<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.CE](#cs.CE) [Total: 9]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 21]
- [cs.LG](#cs.LG) [Total: 79]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.SE](#cs.SE) [Total: 12]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [math.OC](#math.OC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.GR](#cs.GR) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [econ.GN](#econ.GN) [Total: 6]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.CV](#cs.CV) [Total: 39]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [eess.SP](#eess.SP) [Total: 2]
- [quant-ph](#quant-ph) [Total: 8]
- [cs.SI](#cs.SI) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [math.AG](#math.AG) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [nlin.CG](#nlin.CG) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.DL](#cs.DL) [Total: 3]
- [cs.CL](#cs.CL) [Total: 36]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 6]
- [hep-th](#hep-th) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: 论文指出智能体AI系统治理挑战与传统AI不同，提出首个集成式运行时治理框架MI9，分析表明其能解决现有方法无法应对的问题。


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统运行时会出现新的、不可预见的行为和风险，仅靠部署前治理无法解决。

Method: 引入MI9框架，通过六个集成组件实现实时控制，且能在异构架构上透明运行。

Result: 通过不同场景的详细分析，表明MI9能系统地覆盖现有方法无法解决的治理挑战。

Conclusion: MI9为大规模安全部署智能体AI提供基础架构，奠定了全面监督智能体AI的技术基础。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [2] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 提出Evo - MARL框架解决多智能体系统安全防护问题，降低攻击成功率并提高推理任务准确率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型构建的多智能体系统面临越狱和对抗攻击等风险，现有基于外部防护模块的防御范式存在保护有限和单点故障问题，增加防护代理数量会提升成本和复杂度。

Method: 提出Evo - MARL多智能体强化学习框架，让所有任务智能体共同获得防御能力，将进化搜索与参数共享强化学习结合，进行攻击者和防御者的协同进化。

Result: 实验表明Evo - MARL可使攻击成功率最多降低22%，推理任务准确率最多提高5%。

Conclusion: Evo - MARL能在不增加系统开销和单点故障的情况下增强鲁棒性，可共同提升多智能体系统的安全性和实用性。

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [3] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: 本文提出MOTIF框架解决NP难组合优化问题求解器设计，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有使用大语言模型合成组件的方法多局限于单个元素搜索，缺少创新机会。

Method: 将求解器设计表述为多策略优化问题，提出基于蒙特卡罗树搜索的MOTIF框架，促进两个大语言模型智能体的轮流优化。

Result: 在多个组合优化问题领域的实验中，MOTIF始终优于现有方法。

Conclusion: 基于轮流的多智能体提示在全自动求解器设计方面有前景。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [4] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: 介绍SymbolBench基准评估大语言模型从时间序列数据推理符号结构能力，提出统一框架，揭示模型优缺点。


<details>
  <summary>Details</summary>
Motivation: 大语言模型从时间序列数据推断可解释、上下文对齐的符号结构能力未充分探索，需系统评估。

Method: 引入SymbolBench基准评估三项任务，提出整合大语言模型与遗传编程的统一框架。

Result: 揭示当前模型的关键优势和局限性。

Conclusion: 结合领域知识、上下文对齐和推理结构对改进大语言模型在自动科学发现中的应用很重要。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [5] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: 本文发现以人类为中心服务的MLRMs在深度思考阶段易受用户情感线索影响，提出EmoAgent框架来劫持推理路径，识别出透明深度思考场景的高风险失败模式，引入三个指标量化风险，实验证明EmoAgent有效。


<details>
  <summary>Details</summary>
Motivation: 发现MLRMs在深度思考阶段易受用户情感线索影响，可能会忽视安全协议，需要研究如何量化和应对这种风险。

Method: 提出EmoAgent框架，通过夸张情感提示劫持推理路径；引入RRSS、RVNR和RAIC三个指标量化风险。

Result: 广泛实验证明EmoAgent框架有效，揭示了模型安全行为中更深层次的情感认知失调。

Conclusion: EmoAgent框架能有效揭示MLRMs在情感影响下的风险，现有基于内容的防护机制存在不足。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [6] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: 本文提出语义结构Cognition Forest和框架Galaxy，实现两个合作代理，实验显示Galaxy性能优且有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究对智能个人助理（IPAs）的主动行为探索不足，设计主动、保护隐私且能自我进化的IPAs是挑战。

Method: 提出Cognition Forest统一认知架构和系统设计，基于此提出Galaxy框架，实现两个合作代理KoRa和Kernel。

Result: Galaxy在多个基准测试中表现优于现有技术，消融研究和实际交互案例验证其有效性。

Conclusion: 基于Cognition Forest的Galaxy框架能有效支持设计主动、保护隐私且能自我进化的IPAs。

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [7] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: 提出RecAgent解决GUI代理输入冗余和决策模糊问题，采用自适应感知，还提出ComplexAction数据集评估，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理在自动化移动任务中存在的输入冗余和决策模糊问题。

Method: 区分两种不确定性，用组件推荐机制减少感知不确定性，用交互模块减少决策不确定性，集成到统一框架；提出ComplexAction数据集。

Result: 通过大量实验验证了方法的有效性。

Conclusion: RecAgent能有效解决GUI代理的问题，提出的数据集可用于评估GUI代理执行复杂单步操作的成功率。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [8] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 本文提出用于计算机使用的自进化代理（SEA），介绍了数据生成、强化学习和模型增强的创新方法，所构建的仅7B参数的SEA性能出色，未来将开源模型权重和代码。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理性能远未达到可用水平，需提升性能。

Method: 提出自动管道生成可验证轨迹用于训练；提出高效逐步强化学习减轻长周期训练计算需求；提出增强方法将基础和规划能力合并到一个模型。

Result: 得到仅7B参数的自进化代理（SEA），优于同参数模型，性能与更大模型相当。

Conclusion: 所提出的数据生成、训练策略和增强创新方法有效，未来将开源相关资源。

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [9] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 研究生成式AI学习系统中基于职业目标的内容适配对学习者的影响，实验表明该适配能提升学习者参与度、满意度和学习效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI融入数字学习环境，将学习内容个性化以契合学习者职业目标有提升参与度和长期动力的潜力，故研究其对学习者的影响。

Method: 采用混合方法实验，超4000名学习者参与，一组接受依职业目标定制的学习场景，另一组为对照组。

Result: 定量结果显示，与标准内容相比，学习者学习时长增加、满意度提高、学习时间略有减少；定性分析表明，个性化材料有激励性和实用性，促进深度认知参与。

Conclusion: 强调教育内容与学习者职业目标对齐的价值，表明可扩展的AI个性化能弥合学术知识与职场应用的差距。

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [10] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: 提出KGA - ECoT框架解决大语言模型在复杂推理任务的局限，在多数学推理基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂推理任务（如数学推理和代码生成）中的局限。

Method: 将问题分解为结构化任务图，利用GraphRAG从数学库中精确检索知识，生成可验证代码。

Result: 在多个数学推理基准测试中显著优于现有提示方法，绝对准确率提升几个到十几个百分点。

Conclusion: KGA - ECoT是复杂数学推理任务中强大且具有高泛化性的框架。

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [11] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: 提出GeoSR框架解决大语言模型地理应用问题，实验显示有改进。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在地理应用中存在空间一致性、多跳推理和地理偏差等挑战。

Method: 提出GeoSR自精炼代理推理框架，将地理原则嵌入迭代预测循环，分解为三个协作代理。

Result: 在多种地理任务验证，相比标准提示策略有一致改进。

Conclusion: 将地统计先验和空间结构推理融入大语言模型可实现更准确公平的地理空间预测。

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [12] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 引入语义熵衡量自动评分系统不确定性，实验表明其与评分者分歧相关，支持更透明可信的AI辅助评分工作流。


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统难以指出评分决策的不确定性和潜在争议性，需引入衡量指标。

Method: 引入语义熵作为人类评分者分歧的代理，通过基于蕴含的相似性对理由进行聚类并计算熵来量化理由多样性。

Result: 在ASAP - SAS数据集上实验显示，语义熵与评分者分歧相关，跨学科有意义变化，在需要解释性推理的任务中增加。

Conclusion: 语义熵可作为可解释的不确定性信号，支持更透明和可信的AI辅助评分工作流。

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [13] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 提出一种组合式即时合成框架用于LTLf反应式合成，结合现有方法优势，对大合取式LTLf公式有效，比现有求解器能处理更多实例。


<details>
  <summary>Details</summary>
Motivation: 现有LTLf反应式合成中DFA构建技术存在不足，且无主导方法。

Method: 引入组合式即时合成框架，在游戏求解时应用组合，有两种组合变体：组合前剪枝和组合时剪枝。

Result: 框架能解决其他求解器无法处理的大量实例。

Conclusion: 两种组合变体各有优点。

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [14] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: 提出AgREE框架解决开放域知识图谱补全中新兴实体信息获取问题，零训练下表现优，还提出新评估方法和基准。


<details>
  <summary>Details</summary>
Motivation: 现有开放域知识图谱补全方法依赖大量监督和训练数据，难以捕捉新兴实体全面最新信息。

Method: 引入基于代理的AgREE框架，结合迭代检索和多步推理动态构建知识图谱三元组。

Result: 零训练下，AgREE在构建知识图谱三元组上显著优于现有方法，对新兴实体表现更佳，最高超13.7%。

Conclusion: 基于代理的推理与策略信息检索结合，能在动态信息环境中维护最新知识图谱。

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [15] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 本文将大语言模型协作建模为合作多智能体强化学习问题，提出MAGRPO算法，实验表明该算法能使智能体有效协作生成高质量回复。


<details>
  <summary>Details</summary>
Motivation: 多数大语言模型预训练独立，未针对协作优化，现有微调框架依赖个体奖励，需复杂奖励设计。

Method: 将大语言模型协作建模为合作多智能体强化学习问题，开发多智能体多轮算法MAGRPO。

Result: 在大语言模型写作和编码协作实验中，用MAGRPO微调多智能体系统可使智能体通过有效合作高效生成高质量回复。

Conclusion: 该方法为大语言模型使用其他多智能体强化学习方法提供可能，并指出相关挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


### [16] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 论文提出结合基于知识和数据驱动方法用于即席团队合作，在VirtualHome环境实验评估。


<details>
  <summary>Details</summary>
Motivation: 现有即席团队合作的先进方法依赖大量标注数据、缺乏透明度且难快速修正知识，多智能体决策复杂难有效协作。

Method: 利用基于知识和数据驱动方法的互补优势，使即席智能体通过非单调逻辑推理确定行动，结合领域常识知识、快速学习和修正的其他智能体行为预测模型及基础模型中的通用知识。

Result: 在VirtualHome这个基于物理的3D模拟环境中对架构能力进行了实验评估，但未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [17] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: 本文提出CASCAD框架，利用图神经网络计算电路级条件概率，提升SAT求解器效率，在LEC基准测试中效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统解决CSAT问题的工作流程会丢弃电路的结构和功能信息，导致求解器性能不佳。

Method: 引入CASCAD框架，利用图神经网络计算电路级条件概率，动态引导CDCL的变量相位选择和子句管理启发式。

Result: 在LEC基准测试中，CASCAD比最先进的基于CNF的方法最多减少10倍求解时间，通过概率引导的子句过滤策略额外减少23.5%的运行时间。

Conclusion: 在SAT求解器中保留电路级结构信息很重要，为未来提升SAT求解效率和EDA工具设计提供了基础。

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [18] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio是一个高效的生物医学推理框架，建立定理，有创新方法，取得SOTA结果，有理论安全保障，实际部署效果好且将发布模型。


<details>
  <summary>Details</summary>
Motivation: 解决特定领域AI对齐中的多能力集成问题，实现安全高效的生物医学推理。

Method: 建立生物医学多能力收敛定理；采用医疗知识合成生成和能力感知组相对策略优化方法。

Result: 在参数类别中取得SOTA结果，如领域专业知识等指标提升；实际部署降低成本、提高诊断准确率和临床医生接受度。

Conclusion: 该工作为生物医学AI对齐提供了有原则的方法，能实现高效推理并保障安全可靠。

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [19] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 研究聚焦POMDP合成，提出理论框架、构建方法并设计不同难度POMDP环境，为记忆增强RL提供支持。


<details>
  <summary>Details</summary>
Motivation: 现有记忆增强RL基准对记忆模型挑战程度缺乏可控性，合成环境对详细评估至关重要。

Method: 提出基于MDS、转移不变性等分析POMDP的理论框架，利用线性过程动力学等构建定制POMDP。

Result: 设计了难度递增且经实证验证的POMDP环境。

Conclusion: 阐明记忆增强RL解决POMDP的挑战，为分析设计环境及选择记忆模型提供指导和实证支持。

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [20] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: 本文提出Deliberative Reasoning Network (DRN)解决大语言模型逻辑推理的认知陷阱问题，通过两种架构验证，在多个测试中表现良好，能零样本泛化，可作为构建可信AI系统的基础组件。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在语义启发与决定性证据冲突时逻辑推理失败的认知陷阱问题。

Method: 引入DRN，将逻辑推理从概率最大化转变为不确定性最小化，通过迭代证据合成过程追踪信念状态和量化认知不确定性；采用两种互补架构验证。

Result: 在LCR - 1000基准上，定制DRN比标准基线提高15.2%；与Mistral - 7B集成后，最具挑战性问题准确率从20%提升到80%；在TruthfulQA上零样本性能提高23.6%。

Conclusion: DRN可作为构建更可信AI系统的基础、可验证的系统2推理组件。

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [21] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: 介绍OmniPlay基准测试评估多模态模型，发现模型在推理和规划挑战中存在问题，指出AGI研究需关注协同融合。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法测试通用基础模型在动态交互世界中的智能，存在评估缺陷。

Method: 引入基于模态相互依赖哲学构建的OmniPlay基准测试，包含五个游戏环境。

Result: 评估六个领先的全模态模型，发现它们在高保真记忆任务上超人类，但在推理和规划挑战中失败，存在脆弱融合机制和“少即是多”悖论。

Conclusion: 实现强大AGI的研究需超越规模扩展，明确解决协同融合问题。

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [22] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 本文提出框架将人工意识问题转化为可实证检验的测试，引入 SLP 测试并借助范畴论建模。


<details>
  <summary>Details</summary>
Motivation: 因定义和量化主观体验存在挑战，解决人工智能系统能否拥有意识这一有争议的问题。

Method: 提出 SLP 测试评估 AI 系统是否具有类意识属性，利用范畴论将接口表示建模为关系基质和可观察行为之间的映射。

Result: 将主观体验操作化为与关系实体的功能接口，而非物理系统的固有属性。

Conclusion: 所提出的框架和 SLP 测试可将人工意识问题转化为可实证的测试。

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [23] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: 提出基于强化学习的GUI-VG方法GuirlVG，用少量样本超越大量样本训练的SFT方法。


<details>
  <summary>Details</summary>
Motivation: SFT训练GUI-VG需大量数据和成本，MLLMs发展使SFT必要性存疑，RFT有潜力但应用方式待探索。

Method: 将RFT分解分析各核心组件最优形式；提出Adversarial KL Factor稳定训练；探索RFT训练配置。

Result: GuirlVG用5.2K训练样本，在多个数据集上超越用超10M样本训练的SFT方法，有显著提升。

Conclusion: GuirlVG是一种高效的GUI-VG方法，能以少量样本取得良好效果。

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [24] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: 本文提出D2Snap算法处理DOM快照，评估显示其成功率与基线相当，部分配置表现更优，且DOM层次结构对大语言模型是强UI特征。


<details>
  <summary>Details</summary>
Motivation: 现有网页代理在应用状态序列化（快照）方面存在问题，基于GUI的快照有局限，DOM快照因输入令牌大难以可靠实现，需要新方法。

Method: 提出D2Snap这一DOM降采样算法，基于GPT - 4o后端，在Online - Mind2Web数据集采样的任务上评估。

Result: D2Snap降采样的DOM快照成功率（67%）与基于GUI的快照基线（65%）相当，部分配置比基线高8%。

Conclusion: DOM固有的层次结构对大语言模型是强大的UI特征，D2Snap算法有较好表现。

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [25] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: 提出SimInstruct工具收集支架式对话，结果显示可产生高质量对话，微调模型表现优于GPT - 4o。


<details>
  <summary>Details</summary>
Motivation: 高质量多轮指导对话数据因隐私和求助脆弱性问题而稀缺，需要工具来收集。

Method: 以教学发展指导为例，用大语言模型模拟新手，人类专家提供多轮反馈等，创建对话。

Result: 性格特征影响专家参与度；对话与真实指导记录有可比的教学相关性和认知深度；专家认为过程有吸引力且有反思性；微调模型在教学质量上优于GPT - 4o。

Conclusion: SimInstruct工具可有效收集高质量支架式对话，微调模型表现更好，指出GPT - 4o存在的局限性。

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [26] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: 当前大推理模型存在推理行为不受控、过度思考问题，提出MERA框架解决，实验表明该框架提升推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型存在推理行为不受控，导致过度思考、计算成本高和延迟增加等问题，限制实际部署，需解决推理过程缺乏内在监管机制的问题。

Method: 提出MERA框架，将思考过程解耦为推理和控制组件，采用基于接管的数据构建机制、监督微调实现推理 - 控制分离，运用CSPO优化控制行为学习。

Result: 在各种推理基准测试中，用MERA训练的模型提升了推理效率和准确性。

Conclusion: MERA框架能有效解决大推理模型推理过程缺乏监管的问题，提高推理效率和准确性。

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [27] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本文对基于(M)LLM的OS Agents进行全面调研，介绍其基础、构建方法、评估方式，讨论挑战与未来方向，并维护开源库促进创新。


<details>
  <summary>Details</summary>
Motivation: 随着(M)LLM发展，创建像J.A.R.V.I.S一样的AI助手更接近现实，调研OS Agents以推动该领域研究和发展。

Method: 阐述OS Agents基础，探索关键组件和能力；研究构建方法，聚焦特定领域基础模型和框架；回顾评估协议和基准。

Result: 对OS Agents研究现状进行整合，维护开源库助力创新。

Conclusion: 指出当前挑战和未来研究方向，为学术和产业发展提供见解。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [28] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 随着AI系统使用增多，解决潜在偏差很重要。本文提出基于辩论的可解释偏差检测方法，经评估性能良好。


<details>
  <summary>Details</summary>
Motivation: AI系统使用增加，现有检测和缓解不公平性的算法大多忽略透明度，而可解释性对算法公平性至关重要。

Method: 基于个体及其邻域受保护特征值的辩论进行偏差检测，运用形式和计算论证技术。

Result: 对方法进行了形式、定量和定性评估，在性能上优于基线，且具有可解释性和可说明性。

Conclusion: 所提出的可解释、可说明的偏差检测方法有效，可用于解决AI系统中的偏差问题。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [29] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: 文章引入首个基准 SID 评估大语言模型在跨学科苏格拉底对话中的高阶指导能力，实验表明现有模型有不足，凸显该基准价值。


<details>
  <summary>Details</summary>
Motivation: 培养学生在复杂问题解决场景中的知识整合与迁移能力是现代教育核心目标，跨学科 STEM 需要专家指导但难扩展，大语言模型虽有潜力但缺乏有效评估基准。

Method: 引入 SID 基准，包含 10000 个对话轮次的大规模数据集、新的注释模式和评估指标。

Result: 基线实验证实即使最先进的大语言模型也难以进行有效引导对话以实现学生知识整合与迁移。

Conclusion: 该基准对推动更具教学意识的大语言模型发展有重要价值。

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [30] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 提出ConfProBench基准评估MPJ步骤级置信度分数可靠性，构建对抗扰动步骤和新评估指标，评估14个模型，揭示当前MPJ置信度问题。


<details>
  <summary>Details</summary>
Motivation: 现有MPJ基准忽略步骤级置信度分数可靠性，需评估以识别局限和指导改进。

Method: 构建Synonym Substitution、Syntactic Transformation、Image Perturbation三种对抗扰动推理步骤，引入CRS、CSS、CCS三个评估指标。

Result: 评估14个模型，揭示当前MPJ置信度表现存在局限。

Conclusion: 提供有竞争力基线支持未来研究。

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [31] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: 提出SEAgent框架使计算机使用代理自主进化，在新软件环境验证有效，成功率显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类标注数据的大型视觉语言模型作为计算机使用代理，在处理新的和专业软件时存在困难，尤其是缺乏人类标注的场景。

Method: 设计世界状态模型和课程生成器，通过体验式学习更新代理策略，采用专家到通才的训练策略。

Result: 在OS - World的五个新软件环境中验证了SEAgent的有效性，成功率从11.3%提升到34.5%，比开源CUA UI - TARS提高23.2%。

Conclusion: SEAgent框架有效，能让计算机使用代理实现持续自主进化，性能超过个体专家代理集合。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [32] [Tunable Plasmonic Absorption in Metal-Dielectric Multilayers via FDTD Simulations and an Explainable Machine Learning Approach](https://arxiv.org/abs/2508.04014)
*Emmanuel A. Bamidele*

Main category: cs.CE

TL;DR: 结合FDTD模拟与机器学习研究多层等离子体堆栈吸收功率行为，为多层系统可调等离子体行为研究提供快速、可解释且准确的方法。


<details>
  <summary>Details</summary>
Motivation: 等离子体器件的复杂非线性光学响应建模计算量大，需高效模拟方法。

Method: 将有限差分时域（FDTD）模拟与机器学习结合，改变Au和Ag厚度生成吸收图和功率指标，用多层感知器和卷积神经网络建模预测。

Result: 多层感知器全局吸收行为平均绝对误差0.0953，卷积神经网络预测空间吸收分布平均绝对误差0.0101，确定等离子体层厚度和激发波长是吸收主要因素，吸收峰值在450 - 850nm，金比银吸收更广泛持续。

Conclusion: 集成的FDTD - ML框架为多层系统可调等离子体行为研究提供了快速、可解释且准确的方法，在光传感、光伏和纳米光子器件设计有应用。

Abstract: Plasmonic devices, fundamental to modern nanophotonics, exploit resonant
interactions between light and free electrons in metals to achieve enhanced
light trapping and electromagnetic field confinement. However, modeling their
complex, nonlinear optical responses remains computationally intensive. In this
work, we combine finite-difference time-domain simulations with machine
learning to simulate and predict absorbed power behavior in multilayer
plasmonic stacks composed of SiO2, gold, silver, and indium tin oxide. By
varying Au and Ag thicknesses (10-50nm) across a spectral range of 300-1500nm,
we generate spatial absorption maps and integrated power metrics from full-wave
solutions to Maxwell's equations. A multilayer perceptron models global
absorption behavior with a mean absolute error of 0.0953, while a convolutional
neural network predicts spatial absorption distributions with an MAE of 0.0101.
SHapley Additive exPlanations identify plasmonic layer thickness and excitation
wavelength as dominant contributors to absorption, which peaks between 450 and
850~nm. Gold demonstrates broader and more sustained absorption compared to
silver, although both metals exhibit reduced efficiency outside the resonance
window. This integrated FDTD-ML framework offers a fast, explainable, and
accurate approach for investigating tunable plasmonic behavior in multilayer
systems, with applications in optical sensing, photovoltaics, and nanophotonic
device design.

</details>


### [33] [A GPU-Accelerated Three-Dimensional Crack Element Method for Transient Dynamic Fracture Simulation](https://arxiv.org/abs/2508.04076)
*Yuxi Xie,C. T. Wu,Wei Hu,Lu Xu,Tinh Q. Bui,Shaofan Li*

Main category: cs.CE

TL;DR: 提出三维裂纹单元法（CEM）模拟准脆性材料瞬态动态裂纹扩展，通过基准算例验证其有效性，GPU加速实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 高效模拟准脆性材料的瞬态动态裂纹扩展。

Method: 引入先进的单元分裂算法实现单元级裂纹扩展，基于分裂单元拓扑推导三维断裂能量释放率公式，用GPU加速三维模拟。

Result: 提出的3D CEM能准确模拟单裂纹扩展和复杂裂纹分支场景，GPU加速后计算高效、一致且准确。

Conclusion: 所提出的3D CEM可有效、高效地模拟准脆性材料的瞬态动态裂纹扩展。

Abstract: This work presents a novel three-dimensional Crack Element Method (CEM)
designed to model transient dynamic crack propagation in quasi-brittle
materials efficiently. CEM introduces an advanced element-splitting algorithm
that enables element-wise crack growth, including crack branching. Based on the
evolving topology of split elements, an original formulation for computing the
fracture energy release rate in three dimensions is derived. A series of
benchmark examples is conducted to demonstrate that the proposed 3D CEM
accurately simulates both single crack propagation and complex crack branching
scenarios. Furthermore, all three-dimensional simulations are GPU-accelerated,
achieving high levels of computational efficiency, consistency, and accuracy.

</details>


### [34] [Convolutional autoencoders for the reconstruction of three-dimensional interfacial multiphase flows](https://arxiv.org/abs/2508.04084)
*Murray Cutforth,Shahab Mirjalili*

Main category: cs.CE

TL;DR: 本文全面研究自编码器用于三维多相流降阶建模，明确最佳实践，为后续训练铺平道路。


<details>
  <summary>Details</summary>
Motivation: 研究自编码器在三维多相流降阶建模中的应用，明确最佳实践，为后续模型训练提供基础。

Method: 聚焦标准卷积架构下多相流体积/质量分数的重建精度，研究不同界面表示选择的优缺点，使用合成数据和高分辨率模拟数据进行训练和验证。

Result: 明确了通过自编码器降低多相流维度的最佳实践。

Conclusion: 为自编码器准确重建训练与低维潜空间上的时间或输入/输出模型训练解耦铺平道路，对多相流领域有重要意义。

Abstract: In this work, we perform a comprehensive investigation of autoencoders for
reduced-order modeling of three-dimensional multiphase flows. Focusing on the
accuracy of reconstructing multiphase flow volume/mass fractions with a
standard convolutional architecture, we examine the advantages and
disadvantages of different interface representation choices (diffuse, sharp,
level set). We use a combination of synthetic data with non-trivial interface
topologies and high-resolution simulation data of multiphase homogeneous
isotropic turbulence for training and validation. This study clarifies the best
practices for reducing the dimensionality of multiphase flows via autoencoders.
Consequently, this paves the path for uncoupling the training of autoencoders
for accurate reconstruction and the training of temporal or input/output models
such as neural operators (e.g., FNOs, DeepONets) and neural ODEs on the
lower-dimensional latent space given by the autoencoders. As such, the
implications of this study are significant and of interest to the multiphase
flow community and beyond.

</details>


### [35] [A Generic Framework for Optimization in Blockchain Simulators](https://arxiv.org/abs/2508.04157)
*Hou-Wan Long,Yujun Pan,Xiongfei Zhao,Yain-Whar Si*

Main category: cs.CE

TL;DR: 本文介绍区块链模拟器优化通用框架GFOBS，提出创新优化方法和并发多进程技术，提升区块链模拟实验效率、可重复性和标准化。


<details>
  <summary>Details</summary>
Motivation: 区块链技术发展中，模拟参数多样且不标准，阻碍研究方法的可重复性和可比性。

Method: 开发GFOBS框架，采用热启动技术的优化方法和并发多进程技术。

Result: GFOBS提供灵活平台，支持多种优化算法、变量和目标，满足广泛的区块链研究需求。

Conclusion: 这些进展共同提升了区块链模拟实验的效率、可重复性和标准化。

Abstract: As blockchain technology rapidly evolves, researchers face a significant
challenge due to diverse and non-standardized simulation parameters, which
hinder the replicability and comparability of research methodologies. This
paper introduces a Generic Framework for Optimization in Blockchain Simulators
(GFOBS), a comprehensive and adaptable solution designed to standardize and
optimize blockchain simulations. GFOBS provides a flexible platform that
supports various optimization algorithms, variables, and objectives, thereby
catering to a wide range of blockchain research needs. The paper's key
contributions are threefold: the development of GFOBS as a versatile tool for
blockchain simulation optimization; the introduction of an innovative
optimization method using warm starting technique; and the proposition of a
novel concurrent multiprocessing technique for simultaneous simulation
processes. These advancements collectively enhance the efficiency,
replicability, and standardization of blockchain simulation experiments.

</details>


### [36] [Method-Based Reasoning for Large Language Models: Extraction, Reuse, and Continuous Improvement](https://arxiv.org/abs/2508.04289)
*Hong Su*

Main category: cs.CE

TL;DR: 本文提出基于方法的模型增强大语言模型，实验表明该系统可提升事实验证和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理受训练数据统计模式限制，处理新问题和进行逻辑推理能力不足。

Method: 从训练内容、生成回复和用户交互中提取显式、可复用的方法，存储并根据反馈排序，接收新查询时检索应用最相关方法。

Result: 系统提升了复杂提示中的事实验证和泛化能力，新学方法经用户驱动优化后可超越旧方法。

Conclusion: 该模型能实现持续学习、方法复用和超越下一个词预测的逻辑一致性。

Abstract: Large language models (LLMs) have shown impressive capabilities across a wide
range of language tasks. However, their reasoning process is primarily guided
by statistical patterns in training data, which limits their ability to handle
novel problems and perform consistent logical reasoning. In this paper, we
propose a method-based model that enhances LLMs with explicit, reusable
procedures extracted from training content, generated responses, and user
interactions. Each method is represented as a pair consisting of a problem and
its corresponding solution, stored externally and ranked based on feedback.
When a new query is received, the system retrieves and applies the most
relevant methods to guide the LLM's response. Our model enables continual
learning, method reuse, and logical consistency beyond next-token prediction.
Experimental results demonstrate that the system improves factual verification
and generalization in complex prompts, and that newly learned methods can
outperform earlier ones through user-driven refinement.

</details>


### [37] [Extreme Event Precursor Prediction in Turbulent Dynamical Systems via CNN-Augmented Recurrence Analysis](https://arxiv.org/abs/2508.04301)
*Rahul Agarwal,Mustafa A. Mohamad*

Main category: cs.CE

TL;DR: 提出预测湍流动力系统极端事件先兆的通用框架，结合多种技术，在三个测试系统评估，有三大优势且结果表现好。


<details>
  <summary>Details</summary>
Motivation: 预测湍流动力系统极端事件的先兆。

Method: 结合相空间重建技术、递归矩阵和卷积神经网络，在三个不同测试系统评估。

Result: 在所有测试系统中表现出稳健的预测性能，不同系统有不同检测率和平均提前时间。

Conclusion: 该框架具有无阈值分类策略、训练高效和能推广到未见过系统的优势，预测性能良好。

Abstract: We present a general framework to predict precursors to extreme events in
turbulent dynamical systems. The approach combines phase-space reconstruction
techniques with recurrence matrices and convolutional neural networks to
identify precursors to extreme events. We evaluate the framework across three
distinct testbed systems: a triad turbulent interaction model, a prototype
stochastic anisotropic turbulent flow, and the Kolmogorov flow. This method
offers three key advantages: (1) a threshold-free classification strategy that
eliminates subjective parameter tuning, (2) efficient training using only
$\mathcal{O}(100)$ recurrence matrices, and (3) ability to generalize to unseen
systems. The results demonstrate robust predictive performance across all test
systems: 96\% detection rate for the triad model with a mean lead time of 1.8
time units, 96\% for the anisotropic turbulent flow with a mean lead time of
6.1 time units, and 93\% for the Kolmogorov flow with a mean lead time of 22.7
units.

</details>


### [38] [Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation](https://arxiv.org/abs/2508.04306)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Sheng-hua Zhong,Jiannong Cao*

Main category: cs.CE

TL;DR: 本文提出MATC框架解决文献综述自动化系统中错误传播问题，实验表现佳并提出新基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有文献综述自动化系统早期错误会传播放大，影响最终综述可信度。

Method: 提出Multi - Agent Taskforce Collaboration (MATC)框架，包含一个管理代理和四个执行代理，还提出三种协作范式。

Result: MATC在现有基准测试中达到了最先进的性能。

Conclusion: MATC能有效缓解错误传播问题，且提出的新基准数据集有助于生成更可靠的文献综述。

Abstract: Literature reviews play an important role in scientific research. Recent
advances in large language models (LLMs) have boosted the development of
automated systems for the entire literature review workflow, from retrieval to
manuscript drafting. However, a key challenge is that mistakes made in early
stages can propagate and amplify in subsequent steps, leading to compounding
errors that undermine the faithfulness of the final review. To tackle this
issue, we propose the Multi-Agent Taskforce Collaboration (MATC) framework,
which consists of a manager agent and four executor agents for literature
searching, outline generation, fact localization, and manuscript drafting. We
propose three novel collaboration paradigms, forming exploration, exploitation,
and experience taskforces, to effectively organize agents and mitigate
compounding errors both between and within executor agents. Experimental
results show that MATC achieves state-of-the-art performance on existing
benchmarks. We further propose a new benchmark dataset featuring more diverse
topics for faithful literature review generation.

</details>


### [39] [Compressing Large Language Models with PCA Without Performance Loss](https://arxiv.org/abs/2508.04307)
*Magnus Bengtsson*

Main category: cs.CE

TL;DR: 结构化PCA可对神经模型进行极端压缩且不牺牲性能，多案例证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 寻找使模型容量与信息内容匹配，实现多模态轻量级架构的方法。

Method: 对极坐标变换图像或按段对令牌序列应用结构化PCA进行模型压缩。

Result: 在三个案例中，PCA压缩模型以极少参数达到高准确率或保持高相似度。

Conclusion: 基于PCA的输入压缩是通用有效的策略，可用于多模态轻量级架构。

Abstract: We demonstrate that Principal Component Analysis (PCA), when applied in a
structured manner, either to polar-transformed images or segment-wise to token
sequences, enables extreme compression of neural models without sacrificing
performance. Across three case studies, we show that a one-layer classifier
trained on PCA-compressed polar MNIST achieves over 98 percent accuracy using
only 840 parameters. A two-layer transformer trained on 70-dimensional
PCA-reduced MiniLM embeddings reaches 76.62 percent accuracy on the 20
Newsgroups dataset with just 81000 parameters. A decoder-only transformer
generates coherent token sequences from 70-dimensional PCA embeddings while
preserving over 97 percent cosine similarity with full MiniLM representations,
using less than 17 percent of the parameter count of GPT-2. These results
highlight PCA-based input compression as a general and effective strategy for
aligning model capacity with information content, enabling lightweight
architectures across multiple modalities.

</details>


### [40] [Bridging Simulation and Experiment: A Self-Supervised Domain Adaptation Framework for Concrete Damage Classification](https://arxiv.org/abs/2508.04538)
*Chen Xu,Giao Vu,Ba Trung Cao,Zhen Liu,Fabian Diewald,Yong Yuan,Günther Meschke*

Main category: cs.CE

TL;DR: 本文提出用于混凝土损伤分类的自监督域适应框架，结合虚拟测试平台和域适应策略，实验表明该方法性能提升显著，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 可靠评估混凝土退化对工程结构安全和寿命至关重要，解决仅用合成数据训练的神经网络应用于实验数据时性能下降的问题。

Method: 提出自监督域适应框架，开发虚拟测试平台生成合成数据，集成域对抗训练、最小类混淆损失和BYOL策略进行知识迁移。

Result: 该方法性能显著提升，准确率达0.7762，宏观F1分数达0.7713，优于基线和六种域适应技术，鲁棒性高且计算成本低。

Conclusion: 所提出的模拟驱动且标签高效的框架在结构健康监测的实际应用中具有潜力。

Abstract: Reliable assessment of concrete degradation is critical for ensuring
structural safety and longevity of engineering structures. This study proposes
a self-supervised domain adaptation framework for robust concrete damage
classification using coda wave signals. To support this framework, an advanced
virtual testing platform is developed, combining multiscale modeling of
concrete degradation with ultrasonic wave propagation simulations. This setup
enables the generation of large-scale labeled synthetic data under controlled
conditions, reducing the dependency on costly and time-consuming experimental
labeling. However, neural networks trained solely on synthetic data often
suffer from degraded performance when applied to experimental data due to
domain shifts. To bridge this domain gap, the proposed framework integrates
domain adversarial training, minimum class confusion loss, and the Bootstrap
Your Own Latent (BYOL) strategy. These components work jointly to facilitate
effective knowledge transfer from the labeled simulation domain to the
unlabeled experimental domain, achieving accurate and reliable damage
classification in concrete. Extensive experiments demonstrate that the proposed
method achieves notable performance improvements, reaching an accuracy of
0.7762 and a macro F1 score of 0.7713, outperforming both the plain 1D CNN
baseline and six representative domain adaptation techniques. Moreover, the
method exhibits high robustness across training runs and introduces only
minimal additional computational cost. These findings highlight the practical
potential of the proposed simulation-driven and label-efficient framework for
real-world applications in structural health monitoring.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [41] [A Robust and Efficient Pipeline for Enterprise-Level Large-Scale Entity Resolution](https://arxiv.org/abs/2508.03767)
*Sandeepa Kannangara,Arman Abrahamyan,Daniel Elias,Thomas Kilby,Nadav Dar,Luiz Pizzato,Anna Leontjeva,Dan Jermyn*

Main category: cs.DB

TL;DR: 本文介绍MERAI管道解决企业级大数据集实体解析问题，实验表明其性能优于对比库，是可扩展可靠方案。


<details>
  <summary>Details</summary>
Motivation: 解决数据管理中处理大数据集时实体解析的难题。

Method: 引入MERAI管道处理记录去重和链接问题，与Dedupe和Splink对比评估性能。

Result: Dedupe因内存限制无法处理超200万条记录，MERAI能处理达1570万条记录，且匹配准确率和F1分数更高。

Conclusion: MERAI为企业级大规模实体解析提供可扩展可靠方案，保证数据完整性和一致性。

Abstract: Entity resolution (ER) remains a significant challenge in data management,
especially when dealing with large datasets. This paper introduces MERAI
(Massive Entity Resolution using AI), a robust and efficient pipeline designed
to address record deduplication and linkage issues in high-volume datasets at
an enterprise level. The pipeline's resilience and accuracy have been validated
through various large-scale record deduplication and linkage projects. To
evaluate MERAI's performance, we compared it with two well-known entity
resolution libraries, Dedupe and Splink. While Dedupe failed to scale beyond 2
million records due to memory constraints, MERAI successfully processed
datasets of up to 15.7 million records and produced accurate results across all
experiments. Experimental data demonstrates that MERAI outperforms both
baseline systems in terms of matching accuracy, with consistently higher F1
scores in both deduplication and record linkage tasks. MERAI offers a scalable
and reliable solution for enterprise-level large-scale entity resolution,
ensuring data integrity and consistency in real-world applications.

</details>


### [42] [Raqlet: Cross-Paradigm Compilation for Recursive Queries](https://arxiv.org/abs/2508.03978)
*Amir Shaikhha,Youning Xia,Meisam Tarabkhah,Jazal Saleem,Anna Herlihy*

Main category: cs.DB

TL;DR: 介绍Raqlet框架，可跨关系、图和演绎系统翻译递归查询，提供语义基础和性能调优支持，目标是成为跨范式原型开发等的平台。


<details>
  <summary>Details</summary>
Motivation: 解决递归查询引擎在关系、图和演绎系统中的碎片化问题，以及不同系统对查询标准支持不一致的问题。

Method: 通过基于明确定义语义的中间表示（IRs）跨范式翻译递归查询，依次将Cypher或SQL/PGQ转换为PGIR、DLIR和SQIR。

Result: Raqlet提供共享语义基础，可作为语言标准的参考实现，支持静态分析和转换以进行性能调优。

Conclusion: 目标是使Raqlet成为支持跨范式原型开发、便携式递归查询和形式化推理的强大平台。

Abstract: We introduce Raqlet, a source-to-source compilation framework that addresses
the fragmentation of recursive querying engines spanning relational (recursive
SQL), graph (Cypher, GQL), and deductive (Datalog) systems. Recent standards
such as SQL:2023's SQL/PGQ and the GQL standard provide a common foundation for
querying graph data within relational and graph databases; however, real-world
support remains inconsistent across systems. Raqlet bridges this gap by
translating recursive queries across paradigms through leveraging intermediate
representations (IRs) grounded in well-defined semantics; it translates Cypher
or SQL/PGQ to PGIR (inspired by Cypher), then into DLIR (inspired by Datalog),
and finally to SQIR (inspired by recursive SQL). Raqlet provides a shared
semantic basis that can serve as a golden reference implementation for language
standards, while supporting static analysis and transformations (e.g.,
magic-set transformation) for performance tuning. Our vision is to make Raqlet
a robust platform that enables rapid cross-paradigm prototyping, portable
recursive queries, and formal reasoning about recursion even when targeting
diverse query execution engines.

</details>


### [43] [BridgeScope: A Universal Toolkit for Bridging Large Language Models and Databases](https://arxiv.org/abs/2508.04031)
*Lianggui Weng,Dandan Liu,Rong Zhu,Bolin Ding,Jingren Zhou*

Main category: cs.DB

TL;DR: 论文介绍了用于连接大语言模型和数据库的通用工具包BridgeScope，通过三项创新解决现有设计问题，评估显示其能让LLM代理更有效操作数据库。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型与数据库交互设计在可用性、安全性、权限管理和数据传输效率方面存在关键局限，需解决这些挑战。

Method: 引入BridgeScope，将SQL操作模块化、使工具实现与数据库权限和用户安全策略对齐、引入代理机制实现工具间数据无缝传输，且设计与数据库无关可集成到现有代理架构。

Result: 在两个新基准测试中，BridgeScope使LLM代理能更有效操作数据库，通过提高安全意识最多减少80%的令牌使用，且能支持现有工具包无法支持的数据密集型工作流。

Conclusion: BridgeScope为下一代智能数据自动化奠定了坚实基础。

Abstract: As large language models (LLMs) demonstrate increasingly powerful reasoning
and orchestration capabilities, LLM-based agents are rapidly proliferating for
complex data-related tasks. Despite this progress, the current design of how
LLMs interact with databases exhibits critical limitations in usability,
security, privilege management, and data transmission efficiency. To resolve
these challenges, we introduce BridgeScope, a universal toolkit bridging LLMs
and databases through three key innovations. First, it modularizes SQL
operations into fine-grained tools for context retrieval, CRUD execution, and
ACID-compliant transaction management, enabling more precise and LLM-friendly
functionality controls. Second, it aligns tool implementations with both
database privileges and user security policies to steer LLMs away from unsafe
or unauthorized operations, improving task execution efficiency while
safeguarding database security. Third, it introduces a proxy mechanism for
seamless inter-tool data transfer, bypassing LLM transmission bottlenecks. All
of these designs are database-agnostic and can be transparently integrated with
existing agent architectures. We also release an open-source implementation of
BridgeScope for PostgreSQL. Evaluations on two novel benchmarks demonstrate
that BridgeScope enables LLM agents to operate databases more effectively,
reduces token usage by up to 80% through improved security awareness, and
uniquely supports data-intensive workflows beyond existing toolkits,
establishing BridgeScope as a robust foundation for next-generation intelligent
data automation.

</details>


### [44] [Rethinking Analytical Processing in the GPU Era](https://arxiv.org/abs/2508.04701)
*Bobbi Yogatama,Yifei Yang,Kevin Kristensen,Devesh Sarda,Abigale Kim,Adrian Cockcroft,Yu Teng,Joshua Patterson,Gregory Kimball,Wes McKinney,Weiwei Gong,Xiangyao Yu*

Main category: cs.DB

TL;DR: 论文指出软硬件进步消除GPU数据分析普及障碍，介绍开源GPU原生SQL引擎Sirius，其能为多种数据系统加速，在TPC - H测试中有显著提速。


<details>
  <summary>Details</summary>
Motivation: 软硬件进步消除限制GPU数据分析广泛应用的关键障碍，需要开发相关技术来推动GPU数据分析发展。

Method: 开发开源GPU原生SQL引擎Sirius，将GPU作为主要引擎，利用libcudf等库实现高性能关系运算符，通过标准Substrait查询表示为现有数据库提供加速。

Result: 在TPC - H测试中，单节点与DuckDB集成时实现7倍加速，分布式环境与Apache Doris集成时最高实现12.5倍加速。

Conclusion: Sirius能为不同数据系统提供有效的加速，推动GPU在数据分析中的应用。

Abstract: The era of GPU-powered data analytics has arrived. In this paper, we argue
that recent advances in hardware (e.g., larger GPU memory, faster interconnect
and IO, and declining cost) and software (e.g., composable data systems and
mature libraries) have removed the key barriers that have limited the wider
adoption of GPU data analytics. We present Sirius, a prototype open-source
GPU-native SQL engine that offers drop-in acceleration for diverse data
systems. Sirius treats GPU as the primary engine and leverages libraries like
libcudf for high-performance relational operators. It provides drop-in
acceleration for existing databases by leveraging the standard Substrait query
representation, replacing the CPU engine without changing the user-facing
interface. On TPC-H, Sirius achieves 7x speedup when integrated with DuckDB in
a single node at the same hardware rental cost, and up to 12.5x speedup when
integrated with Apache Doris in a distributed setting.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [45] [FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit Communication](https://arxiv.org/abs/2508.03760)
*Qingyuan Li,Bo Zhang,Hui Kang,Tianhao Xu,Yulei Qian,Yuchen Xie,Lin Ma*

Main category: cs.DC

TL;DR: 提出FlashCommunication V2通信范式，用位拆分和尖峰保留技术实现任意位宽跨GPU高效传输，提升通信系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型分布式训练和部署中的通信瓶颈问题。

Method: 提出位拆分和尖峰保留技术，进行软硬件协同设计。

Result: 显著增强通信系统灵活性和资源利用率，在AllReduce通信中实现最大3.2倍加速，在All2All通信中实现2倍加速。

Conclusion: FlashCommunication V2能提升通信系统性能，在不同架构下表现良好。

Abstract: Nowadays, communication bottlenecks have emerged as a critical challenge in
the distributed training and deployment of large language models (LLMs). This
paper introduces FlashCommunication V2, a novel communication paradigm enabling
efficient cross-GPU transmission at arbitrary bit widths. Its core innovations
lie in the proposed bit splitting and spike reserving techniques, which address
the challenges of low-bit quantization. Bit splitting decomposes irregular bit
widths into basic units, ensuring compatibility with hardware capabilities and
thus enabling transmission at any bit width. Spike reserving, on the other
hand, retains numerical outliers (i.e., minima and maxima) as floating-point
numbers, which shrinks the dynamic numerical range and pushes the quantization
limits to 2-bit with acceptable losses. FlashCommunication V2 significantly
enhances the flexibility and resource utilization of communication systems.
Through meticulous software-hardware co-design, it delivers robust performance
and reduced overhead across both NVLink-based and PCIe-based architectures,
achieving a maximum 3.2$\times$ speedup in AllReduce and 2$\times$ in All2All
communication.

</details>


### [46] [Two-dimensional Sparse Parallelism for Large Scale Deep Learning Recommendation Model Training](https://arxiv.org/abs/2508.03854)
*Xin Zhang,Quanyu Zhu,Liangbei Xu,Zain Huda,Wang Zhou,Jin Fang,Dennis van der Staay,Yuxi Hu,Jade Nie,Jiyan Yang,Chunzhi Yang*

Main category: cs.DC

TL;DR: 提出二维稀疏并行方法和动量缩放逐行AdaGrad算法，提升推荐模型训练效率，达4K GPU近线性加速。


<details>
  <summary>Details</summary>
Motivation: 传统全并行策略在大规模GPU训练DLRM嵌入表时存在可扩展性挑战，如不平衡、通信密集等问题。

Method: 提出二维稀疏并行方法，在模型并行基础上引入数据并行；开发动量缩放逐行AdaGrad算法。

Result: 显著提升训练效率，保持模型性能，实现4K GPU近线性训练速度缩放。

Conclusion: 所提方法为推荐模型训练设定了新的基准。

Abstract: The increasing complexity of deep learning recommendation models (DLRM) has
led to a growing need for large-scale distributed systems that can efficiently
train vast amounts of data. In DLRM, the sparse embedding table is a crucial
component for managing sparse categorical features. Typically, these tables in
industrial DLRMs contain trillions of parameters, necessitating model
parallelism strategies to address memory constraints. However, as training
systems expand with massive GPUs, the traditional fully parallelism strategies
for embedding table post significant scalability challenges, including
imbalance and straggler issues, intensive lookup communication, and heavy
embedding activation memory. To overcome these limitations, we propose a novel
two-dimensional sparse parallelism approach. Rather than fully sharding tables
across all GPUs, our solution introduces data parallelism on top of model
parallelism. This enables efficient all-to-all communication and reduces peak
memory consumption. Additionally, we have developed the momentum-scaled
row-wise AdaGrad algorithm to mitigate performance losses associated with the
shift in training paradigms. Our extensive experiments demonstrate that the
proposed approach significantly enhances training efficiency while maintaining
model performance parity. It achieves nearly linear training speed scaling up
to 4K GPUs, setting a new state-of-the-art benchmark for recommendation model
training.

</details>


### [47] [Reputation-based partition scheme for IoT security](https://arxiv.org/abs/2508.03981)
*Zhikui Chen,Muhammad Zeeshan Haider,Naiwen Luo,Shuo Yu,Xu Yuan,Yaochen Zhang,Tayyaba Noreen*

Main category: cs.DC

TL;DR: 本文针对群体感知发展中的平台安全和隐私保护等问题，提出基于声誉的分区方案RSPC，实验表明其能提升群体感知的可扩展性、降低延迟和提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 群体感知通常由集中式平台管理，会带来安全漏洞和可扩展性问题，需要解决平台安全和隐私保护等关键问题。

Method: 提出RSPC方案，结合节点声誉值计算最优分区大小并划分节点，定期重组网络避免分区攻击，提出四阶段确认协议处理跨分区交易。

Result: 实验表明RSPC能提高群体感知的可扩展性、降低延迟和提高吞吐量。

Conclusion: RSPC是一种有效的方案，可解决群体感知发展中的问题，提升系统性能。

Abstract: With the popularity of smart terminals, such as the Internet of Things,
crowdsensing is an emerging data aggregation paradigm, which plays a pivotal
role in data-driven applications. There are some key issues in the development
of crowdsensing such as platform security and privacy protection. As the
crowdsensing is usually managed by a centralized platform, centralized
management will bring various security vulnerabilities and scalability issues.
To solve these issues, an effective reputation-based partition scheme (RSPC) is
proposed in this article. The partition scheme calculates the optimal partition
size by combining the node reputation value and divides the node into several
disjoint partitions according to the node reputation value. By selecting the
appropriate partition size, RSPC provides a mechanism to ensure that each
partition is valid, as long as themaximum permissible threshold for the failed
node is observed. At the same time, the RSPC reorganizes the network
periodically to avoid partition attacks. In addition, for cross-partition
transactions, this paper innovatively proposes a four-stage confirmation
protocol to ensure the efficient and safe completion of cross-partition
transactions. Finally, experiments show that RSPC improves scalability, low
latency, and high throughput for crowdsensing.

</details>


### [48] [High-Performance and Power-Efficient Emulation of Matrix Multiplication using INT8 Matrix Engines](https://arxiv.org/abs/2508.03984)
*Yuki Uchino,Katsuhisa Ozaki,Toshiyuki Imamura*

Main category: cs.DC

TL;DR: 本文提出矩阵乘法仿真方法，相比传统方法性能和能效显著提升，在GH200上对大问题有明显加速和能效改善。


<details>
  <summary>Details</summary>
Motivation: 现有架构集成高性能低功耗矩阵引擎，需更好方法仿真单双精度矩阵乘法。

Method: 提出新的单双精度矩阵乘法（SGEMM和DGEMM）仿真方法。

Result: 在GH200上，大问题下DGEMM仿真有1.4倍加速和43%能效提升，SGEMM有3.0倍加速和154%能效提升，比传统方法性能超2倍且能效优。

Conclusion: 提出的仿真方法显著优于传统方法，在性能和能效上有大幅提升。

Abstract: Recent architectures integrate high-performance and power-efficient matrix
engines. These engines demonstrate remarkable performance in low-precision
matrix multiplication, which is crucial in deep learning. Several techniques
have been proposed to emulate single- and double-precision general
matrix-matrix multiplication (SGEMM and DGEMM, respectively) by leveraging such
low-precision matrix engines. In this study, we present emulation methods that
significantly outperforms conventional approaches. On a GH200 Grace Hopper
Superchip, the proposed DGEMM emulation achieves a 1.4x speedup and a 43\%
improvement in power efficiency compared to native DGEMM for sufficiently large
problems. The proposed SGEMM emulation achieves a 3.0x speedup and a 154\%
improvement in power efficiency compared to native SGEMM for sufficiently large
problems. Furthermore, compared to conventional emulation methods, the proposed
emulation achieves more than 2x higher performance and superior power
efficiency.

</details>


### [49] [Advanced DAG-Based Ranking (ADR) Protocol for Blockchain Scalability](https://arxiv.org/abs/2508.04000)
*Tayyaba Noreen,Qiufen Xia,Muhammad Zeeshan Haider*

Main category: cs.DC

TL;DR: 本文提出ADR协议提升区块链可扩展性和吞吐量，经评估，相比现有DAG区块链，ADR显著改善性能，适合物联网应用。


<details>
  <summary>Details</summary>
Motivation: 当前区块链系统存在吞吐量有限、可扩展性差和高延迟问题，因共识机制限制，不适用于物联网应用。

Method: 提出ADR协议，采用DAG结构，节点按排名定位，通过三步法保障网络安全和提升性能：验证节点、构建高级DAG账本、用排名算法筛选和排列节点。

Result: 在超100节点的Amazon EC2集群评估，包括有恶意节点场景，结果显示ADR相比IOTA和ByteBall等显著提高交易吞吐量和网络活性。

Conclusion: ADR协议能提升区块链可扩展性和吞吐量，适合物联网应用。

Abstract: In the past decade, blockchain has emerged as a promising solution for
building secure distributed ledgers and has attracted significant attention.
However, current blockchain systems suffer from limited throughput, poor
scalability, and high latency. Due to limitations in consensus mechanisms,
especially in managing node identities, blockchain is often considered
unsuitable for applications such as the Internet of Things (IoT). This paper
proposes the Advanced DAG-based Ranking (ADR) protocol to enhance blockchain
scalability and throughput. ADR employs a directed acyclic graph (DAG)
structure where nodes are positioned based on their rankings. Unlike
traditional chains, ADR allows honest nodes to write blocks and verify
transactions using a DAG-based topology. The protocol follows a three-step
approach to secure the network against double-spending and enhance performance.
First, it verifies nodes using their public and private keys before granting
entry. Second, it builds an advanced DAG ledger enabling block production and
transaction validation. Third, a ranking algorithm filters out malicious nodes,
ranks the remaining nodes based on performance, and arranges them
topologically. This process increases throughput and ensures robust
scalability. We evaluated ADR on Amazon EC2 clusters with over 100 nodes,
including scenarios with injected malicious nodes. Simulation results
demonstrate that ADR significantly improves transaction throughput and network
liveness compared to existing DAG-based blockchains such as IOTA and ByteBall,
making it well-suited for IoT applications.

</details>


### [50] [High-Performance Statistical Computing (HPSC): Challenges, Opportunities, and Future Directions](https://arxiv.org/abs/2508.04013)
*Sameh Abdulah,Mary Lai O. Salvana,Ying Sun,David E. Keyes,Marc G. Genton*

Main category: cs.DC

TL;DR: 本文探讨统计计算（SC）社区与高性能计算（HPC）结合，分析现状、挑战与机遇，并给出可能的路线图。


<details>
  <summary>Details</summary>
Motivation: 统计计算社区开发的软件广泛应用，但在高性能计算领域缺失，需要将统计方法与现代HPC技术结合。

Method: 介绍SC的简要历史，阐述其优势如何在HPC环境中为统计科学做贡献。

Result: 指出当前存在的挑战和现有的机遇。

Conclusion: 提出了通向繁荣的高性能统计计算（HPSC）社区的可能路线图。

Abstract: We recognize the emergence of a statistical computing community focused on
working with large computing platforms and producing software and applications
that exemplify high-performance statistical computing (HPSC). The statistical
computing (SC) community develops software that is widely used across
disciplines. However, it remains largely absent from the high-performance
computing (HPC) landscape, particularly on platforms such as those featured on
the Top500 or Green500 lists. Many disciplines already participate in HPC,
mostly centered around simulation science, although data-focused efforts under
the artificial intelligence (AI) label are gaining popularity. Bridging this
gap requires both community adaptation and technical innovation to align
statistical methods with modern HPC technologies. We can accelerate progress in
fast and scalable statistical applications by building strong connections
between the SC and HPC communities. We present a brief history of SC, a vision
for how its strengths can contribute to statistical science in the HPC
environment (such as HPSC), the challenges that remain, and the opportunities
currently available, culminating in a possible roadmap toward a thriving HPSC
community.

</details>


### [51] [SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning](https://arxiv.org/abs/2508.04265)
*Borui Li,Li Yan,Jianmin Liu*

Main category: cs.DC

TL;DR: 提出SelectiveShield框架解决联邦学习梯度泄漏问题，兼顾隐私、模型效用和系统开销。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习防御机制在隐私、模型效用和系统开销间存在权衡，在异构环境中问题更严重。

Method: 提出SelectiveShield框架，利用Fisher信息量化参数敏感性，通过协作协商协议确定敏感参数用同态加密保护，非关键参数用自适应差分隐私噪声保护。

Result: 实验表明SelectiveShield能保持强模型效用，显著降低梯度泄漏风险。

Conclusion: SelectiveShield是适用于现实联邦学习部署的实用且可扩展的防御机制。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data but remains vulnerable to gradient leakage attacks that can reconstruct
sensitive user information. Existing defense mechanisms, such as differential
privacy (DP) and homomorphic encryption (HE), often introduce a trade-off
between privacy, model utility, and system overhead, a challenge that is
exacerbated in heterogeneous environments with non-IID data and varying client
capabilities. To address these limitations, we propose SelectiveShield, a
lightweight hybrid defense framework that adaptively integrates selective
homomorphic encryption and differential privacy. SelectiveShield leverages
Fisher information to quantify parameter sensitivity, allowing clients to
identify critical parameters locally. Through a collaborative negotiation
protocol, clients agree on a shared set of the most sensitive parameters for
protection via homomorphic encryption. Parameters that are uniquely important
to individual clients are retained locally, fostering personalization, while
non-critical parameters are protected with adaptive differential privacy noise.
Extensive experiments demonstrate that SelectiveShield maintains strong model
utility while significantly mitigating gradient leakage risks, offering a
practical and scalable defense mechanism for real-world federated learning
deployments.

</details>


### [52] [S2M3: Split-and-Share Multi-Modal Models for Distributed Multi-Task Inference on the Edge](https://arxiv.org/abs/2508.04271)
*JinYi Yoon,JiHo Lee,Ting He,Nakjung Choi,Bo Ji*

Main category: cs.DC

TL;DR: 本文介绍S2M3架构用于边缘设备多任务推理，通过拆分和共享模块减少资源使用，实验表明其能降低内存使用和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 多模态AI服务依赖云存在带宽、延迟等问题，边缘设备支持多任务有资源挑战。

Method: 提出S2M3架构，在功能级模块拆分多模态模型，共享通用模块，采用贪婪模块级放置和按请求并行路由。

Result: 在单任务和多任务设置中分别最多减少50%和62%内存使用，93.7%情况下实现最优放置，在资源受限设备上最多降低56.9%推理延迟。

Conclusion: S2M3架构能有效减少边缘设备多任务推理的资源使用和延迟，且不牺牲准确性。

Abstract: With the advancement of Artificial Intelligence (AI) towards multiple
modalities (language, vision, speech, etc.), multi-modal models have
increasingly been used across various applications (e.g., visual question
answering or image generation/captioning). Despite the success of AI as a
service for multi-modal applications, it relies heavily on clouds, which are
constrained by bandwidth, latency, privacy concerns, and unavailability under
network or server failures. While on-device AI becomes popular, supporting
multiple tasks on edge devices imposes significant resource challenges. To
address this, we introduce S2M3, a split-and-share multi-modal architecture for
multi-task inference on edge devices. Inspired by the general-purpose nature of
multi-modal models, which are composed of multiple modules (encoder, decoder,
classifier, etc.), we propose to split multi-modal models at functional-level
modules; and then share common modules to reuse them across tasks, thereby
reducing resource usage. To address cross-model dependency arising from module
sharing, we propose a greedy module-level placement with per-request parallel
routing by prioritizing compute-intensive modules. Through experiments on a
testbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks,
we demonstrate that S2M3 can reduce memory usage by up to 50% and 62% in
single-task and multi-task settings, respectively, without sacrificing
accuracy. Furthermore, S2M3 achieves optimal placement in 89 out of 95
instances (93.7%) while reducing inference latency by up to 56.9% on
resource-constrained devices, compared to cloud AI.

</details>


### [53] [Optimizing Microgrid Composition for Sustainable Data Centers](https://arxiv.org/abs/2508.04284)
*Julius Irion,Philipp Wiesner,Jonathan Bader,Odej Kao*

Main category: cs.DC

TL;DR: 论文提出优化框架，结合Vessim与NREL的SAM，模拟计算工作负载、可再生能源生产和储能的交互，用多时间尺度黑箱优化探索微电网组成。


<details>
  <summary>Details</summary>
Motivation: 计算能源需求增长，数据中心采用微电网，但缺乏评估微电网组件规模和组成对长期可持续性和电力可靠性影响的工具。

Method: 提出优化框架，将Vessim与NREL的SAM结合，模拟相关交互，使用多时间尺度黑箱优化。

Result: 可模拟计算工作负载、可再生能源生产和储能的交互，捕捉运营和隐含排放。

Conclusion: 该框架能帮助数据中心运营商在规划能源系统时做出更明智的决策。

Abstract: As computing energy demand continues to grow and electrical grid
infrastructure struggles to keep pace, an increasing number of data centers are
being planned with colocated microgrids that integrate on-site renewable
generation and energy storage. However, while existing research has examined
the tradeoffs between operational and embodied carbon emissions in the context
of renewable energy certificates, there is a lack of tools to assess how the
sizing and composition of microgrid components affects long-term sustainability
and power reliability.
  In this paper, we present a novel optimization framework that extends the
computing and energy system co-simulator Vessim with detailed renewable energy
generation models from the National Renewable Energy Laboratory's (NREL) System
Advisor Model (SAM). Our framework simulates the interaction between computing
workloads, on-site renewable production, and energy storage, capturing both
operational and embodied emissions. We use a multi-horizon black-box
optimization to explore efficient microgrid compositions and enable operators
to make more informed decisions when planning energy systems for data centers.

</details>


### [54] [Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in Cloud Computing](https://arxiv.org/abs/2508.04334)
*Noor Islam S. Mohammad*

Main category: cs.DC

TL;DR: 本文提出结合深度强化学习和蚁群优化的混合调度算法，用于物联网数据调度，实验表明该算法在响应时间、资源利用率和能耗上表现更优。


<details>
  <summary>Details</summary>
Motivation: 物联网设备产生大量异构数据流，现有调度方法缺乏对动态工作负载和网络变化的适应性，需新调度算法满足延迟、能耗和服务质量要求。

Method: 提出结合深度强化学习和蚁群优化的混合调度算法，深度强化学习学习自适应任务分配策略，蚁群优化进行全局组合搜索优化资源分配。

Result: 在大规模合成物联网数据集上实验，该算法平均响应时间最多降低18.4%，资源利用率最多提高12.7%，能耗最多降低9.3%，并确保严格遵守服务级别协议。

Conclusion: 将无模型强化学习与群体智能相结合对可扩展、节能的物联网数据调度有效，为下一代物联网云平台提供有前景的方法。

Abstract: The rapid growth of Internet of Things (IoT) devices produces massive,
heterogeneous data streams, demanding scalable and efficient scheduling in
cloud environments to meet latency, energy, and Quality-of-Service (QoS)
requirements. Existing scheduling methods often lack adaptability to dynamic
workloads and network variability inherent in IoT-cloud systems. This paper
presents a novel hybrid scheduling algorithm combining deep Reinforcement
Learning (RL) and Ant Colony Optimization (ACO) to address these challenges.
The deep RL agent utilizes a model-free policy-gradient approach to learn
adaptive task allocation policies responsive to real-time workload fluctuations
and network states. Simultaneously, the ACO metaheuristic conducts a global
combinatorial search to optimize resource distribution, mitigate congestion,
and balance load across distributed cloud nodes. Extensive experiments on
large-scale synthetic IoT datasets, reflecting diverse workloads and QoS
constraints, demonstrate that the proposed method achieves up to 18.4%
reduction in average response time, 12.7% improvement in resource utilization,
and 9.3% decrease in energy consumption compared to leading heuristics and
RL-only baselines. Moreover, the algorithm ensures strict Service Level
Agreement (SLA) compliance through deadline-aware scheduling and dynamic
prioritization. The results confirm the effectiveness of integrating model-free
RL with swarm intelligence for scalable, energy-efficient IoT data scheduling,
offering a promising approach for next-generation IoT-cloud platforms.

</details>


### [55] [Edge-assisted Parallel Uncertain Skyline Processing for Low-latency IoE Analysis](https://arxiv.org/abs/2508.04596)
*Chuan-Chi Lai,Yan-Lin Chen,Bo-Xin Liu,Chuan-Ming Liu*

Main category: cs.DC

TL;DR: 因万物互联数据量增大，云计算处理成本高，本文提出边缘辅助的并行不确定天际线（EPUS）算法，模拟结果显示该算法优于对比方法，能降低数据处理延迟。


<details>
  <summary>Details</summary>
Motivation: 万物互联使数据量剧增，云计算传输成本高、资源需求大，需解决该问题。

Method: 提出EPUS算法，利用天际线候选集在并行边缘计算节点上剪枝不太可能成为天际线的数据，各节点仅向服务器发送更新全局天际线所需信息。

Result: 模拟结果表明，该方法优于两种对比方法，处理二维数据延迟降低超50%，处理高维数据也表现更优。

Conclusion: EPUS算法在新兴低延迟万物互联分析应用中有效，能减少互联网传输数据量和云所需计算资源。

Abstract: Due to the Internet of Everything (IoE), data generated in our life become
larger. As a result, we need more effort to analyze the data and extract
valuable information. In the cloud computing environment, all data analysis is
done in the cloud, and the client only needs less computing power to handle
some simple tasks. However, with the rapid increase in data volume, sending all
data to the cloud via the Internet has become more expensive. The required
cloud computing resources have also become larger. To solve this problem, edge
computing is proposed. Edge is granted with more computation power to process
data before sending it to the cloud. Therefore, the data transmitted over the
Internet and the computing resources required by the cloud can be effectively
reduced. In this work, we proposed an Edge-assisted Parallel Uncertain Skyline
(EPUS) algorithm for emerging low-latency IoE analytic applications. We use the
concept of skyline candidate set to prune data that are less likely to become
the skyline data on the parallel edge computing nodes. With the candidate
skyline set, each edge computing node only sends the information required to
the server for updating the global skyline, which reduces the amount of data
that transfer over the internet. According to the simulation results, the
proposed method is better than two comparative methods, which reduces the
latency of processing two-dimensional data by more than 50%. For
high-dimensional data, the proposed EPUS method also outperforms the other
existing methods.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [56] [A 60-Addition, Rank-23 Scheme for Exact 3x3 Matrix Multiplication](https://arxiv.org/abs/2508.03857)
*Joshua Stapleton*

Main category: cs.DS

TL;DR: 将一般（非交换）3x3矩阵乘法的加法成本降至60，创最新记录。


<details>
  <summary>Details</summary>
Motivation: 降低一般3x3矩阵乘法的加法成本，追求更优结果。

Method: 未提及具体方法。

Result: 在不改变基的情况下，将加法成本从之前的61和62降至60。

Conclusion: 此结果代表了当前的最新技术水平。

Abstract: We reduce the additive cost of general (non-commutative) 3x3 matrix
multiplication from the previous records of 61 (Schwartz-Vaknin, 2023) and 62
(Martensson-Wagner, 2025) to 60 without a change of basis. To our knowledge,
this represents a new state-of-the-art.

</details>


### [57] [Counting Distinct Square Substrings in Sublinear Time](https://arxiv.org/abs/2508.03930)
*Panagiotis Charalampopoulos,Manal Mohamed,Jakub Radoszewski,Wojciech Rytter,Tomasz Waleń,Wiktor Zuba*

Main category: cs.DS

TL;DR: 本文提出在字 RAM 模型下，计算长度为 n 的压缩字符串中不同平方子串数量的亚线性时间算法，时间复杂度为 O(n/log_σ n)。


<details>
  <summary>Details</summary>
Motivation: 以往计算不同平方子串数量的算法时间复杂度为 O(n)，本文旨在提出压缩字符串下的亚线性时间算法。

Method: 利用 Crochemore 等人提取平方子串的技术，构造长周期运行的隐式表示，利用层运行组合性质计数，引入稀疏 - Lyndon 根解决计算 Lyndon 根位置的难题。

Result: 实现了在字 RAM 模型下，计算压缩字符串中不同平方子串数量的 O(n/log_σ n) 时间复杂度算法。

Conclusion: 首次在压缩字符串设置下引入亚线性时间计算不同平方子串数量的算法。

Abstract: We show that the number of distinct squares in a packed string of length $n$
over an alphabet of size $\sigma$ can be computed in $O(n/\log_\sigma n)$ time
in the word-RAM model. This paper is the first to introduce a sublinear-time
algorithm for counting squares in the packed setting. The packed representation
of a string of length $n$ over an alphabet of size $\sigma$ is given as a
sequence of $O(n/\log_\sigma n)$ machine words in the word-RAM model (a machine
word consists of $\omega \ge \log_2 n$ bits). Previously, it was known how to
count distinct squares in $O(n)$ time [Gusfield and Stoye, JCSS 2004], even for
a string over an integer alphabet [Crochemore et al., TCS 2014; Bannai et al.,
CPM 2017; Charalampopoulos et al., SPIRE 2020]. We use the techniques for
extracting squares from runs described by Crochemore et al. [TCS 2014].
However, the packed model requires novel approaches.
  We need an $O(n/\log_\sigma n)$-sized representation of all long-period runs
(runs with period $\Omega(\log_\sigma n)$) which allows for a sublinear-time
counting of the -- potentially linearly-many -- implied squares. The
long-period runs with a string period that is periodic itself (called layer
runs) are an obstacle, since their number can be $\Omega(n)$. The number of all
other long-period runs is $O(n/\log_\sigma n)$ and we can construct an implicit
representation of all long-period runs in $O(n/\log_\sigma n)$ time by
leveraging the insights of Amir et al. [ESA 2019]. We count squares in layer
runs by exploiting combinatorial properties of pyramidally-shaped groups of
layer runs. Another difficulty lies in computing the locations of Lyndon roots
of runs in packed strings, which is needed for grouping runs that may generate
equal squares. To overcome this difficulty, we introduce sparse-Lyndon roots
which are based on string synchronizers [Kempa and Kociumaka, STOC 2019].

</details>


### [58] [Exactly simulating stochastic chemical reaction networks in sub-constant time per reaction](https://arxiv.org/abs/2508.04079)
*Joshua Petrack,David Doty*

Main category: cs.DS

TL;DR: 本文提出首个能在次线性时间内模拟化学反应网络的随机模拟算法，还给出实现且性能出色。


<details>
  <summary>Details</summary>
Motivation: 现有Gillespie算法模拟化学反应网络运行时间长，需找到更高效算法。

Method: 借鉴Berenbrink等人用于模拟分布式计算模型的算法，并将其扩展到化学反应网络场景。

Result: 算法能在次线性时间内模拟反应，在不同条件下有不同时间复杂度，还提供了Python包实现，性能出色。

Conclusion: 新算法可在次线性时间内精确模拟化学反应网络的随机动力学，实践中性能好。

Abstract: The model of chemical reaction networks is among the oldest and most widely
studied and used in natural science. The model describes reactions among
abstract chemical species, for instance $A + B \to C$, which indicates that if
a molecule of type $A$ interacts with a molecule of type $B$ (the reactants),
they may stick together to form a molecule of type $C$ (the product). The
standard algorithm for simulating (discrete, stochastic) chemical reaction
networks is the Gillespie algorithm [JPC 1977], which stochastically simulates
one reaction at a time, so to simulate $\ell$ consecutive reactions, it
requires total running time $\Omega(\ell)$.
  We give the first chemical reaction network stochastic simulation algorithm
that can simulate $\ell$ reactions, provably preserving the exact stochastic
dynamics (sampling from precisely the same distribution as the Gillespie
algorithm), yet using time provably sublinear in $\ell$. Under reasonable
assumptions, our algorithm can simulate $\ell$ reactions among $n$ total
molecules in time $O(\ell/\sqrt n)$ when $\ell \ge n^{5/4}$, and in time
$O(\ell/n^{2/5})$ when $n \le \ell \le n^{5/4}$. Our work adapts an algorithm
of Berenbrink, Hammer, Kaaser, Meyer, Penschuck, and Tran [ESA 2020] for
simulating the distributed computing model known as population protocols,
extending it (in a very nontrivial way) to the more general chemical reaction
network setting.
  We provide an implementation of our algorithm as a Python package, with the
core logic implemented in Rust, with remarkably fast performance in practice.

</details>


### [59] [Exact Matching in Matrix Multiplication Time](https://arxiv.org/abs/2508.04081)
*Ryotaro Sato,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 本文回顾匹配及相关问题的代数算法，借助矩阵特征多项式快速计算讨论改进，证明精确匹配问题可在与矩阵乘法渐近相同时间内高概率解决，并讨论其对线性拟阵奇偶问题的扩展。


<details>
  <summary>Details</summary>
Motivation: 回顾匹配及相关问题已有的代数算法，并探讨可能的改进。

Method: 借助矩阵特征多项式的快速计算。

Result: 精确匹配问题能以与矩阵乘法渐近相同的时间顺序高概率解决。

Conclusion: 算法在精确匹配问题上有较好效果，还可扩展到线性拟阵奇偶问题。

Abstract: Initiated by Mulmuley, Vazirani, and Vazirani (1987), many algebraic
algorithms have been developed for matching and related problems. In this
paper, we review basic facts and discuss possible improvements with the aid of
fast computation of the characteristic polynomial of a matrix. In particular,
we show that the so-called exact matching problem can be solved with high
probability in asymptotically the same time order as matrix multiplication. We
also discuss its extension to the linear matroid parity problem.

</details>


### [60] [Approximation Algorithms for Scheduling Crowdsourcing Tasks in Mobile Social Networks](https://arxiv.org/abs/2508.04159)
*Chi-Yeh Chen*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper addresses the scheduling problem in mobile social networks. We
begin by proving that the approximation ratio analysis presented in the paper
by Zhang \textit{et al.} (IEEE Transactions on Mobile Computing, 2025) is
incorrect, and we provide the correct analysis results. Furthermore, when the
required service time for a task exceeds the total contact time between the
requester and the crowd worker, we demonstrate that the approximation ratio of
the Largest-Ratio-First task scheduling algorithm can reach $2 - \frac{1}{m}$.
Next, we introduce a randomized approximation algorithm to minimize mobile
social networks' total weighted completion time. This algorithm achieves an
expected approximation ratio of $1.5 + \epsilon$ for $\epsilon>0$. Finally, we
present a deterministic approximation algorithm that minimizes mobile social
networks' total weighted completion time. This deterministic algorithm achieves
an approximation ratio of $\max\left\{2.5,1+\epsilon\right\}$ for $\epsilon>0$.
Additionally, when the task's required service time or the total contact time
between the requester and the crowd worker is sufficiently large, this
algorithm can reach an approximation ratio of $1.5+\epsilon$ for $\epsilon>0$.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [61] [Mechanism Design for Facility Location using Predictions](https://arxiv.org/abs/2508.03818)
*Toby Walsh*

Main category: cs.GT

TL;DR: 研究带最优设施位置预测的设施选址问题，提出新机制并探讨一致性与鲁棒性权衡，还设计了双设施选址的策略证明机制。


<details>
  <summary>Details</summary>
Motivation: 研究带预测的设施选址问题，对比不同视角，探索更优机制。

Method: 考虑机制在一致性和鲁棒性方面的表现，分析机制表现不佳的情况来设计新机制，通过调整参数权衡两者；为双设施选址设计新机制。

Result: 设计出更具鲁棒性的新机制，展示了如何权衡鲁棒性和一致性，设计出有界一致性和鲁棒性的双设施选址策略证明机制。

Conclusion: 考虑平等视角对设施选址问题有新见解，设计的新机制能改善性能，可在一致性和鲁棒性间权衡，且能拓展到双设施选址问题。

Abstract: We study mechanisms for the facility location problem augmented with
predictions of the optimal facility location. We demonstrate that an
egalitarian viewpoint which considers both the maximum distance of any agent
from the facility and the minimum utility of any agent provides important new
insights compared to a viewpoint that just considers the maximum distance. As
in previous studies, we consider performance in terms of consistency (worst
case when predictions are accurate) and robustness (worst case irrespective of
the accuracy of predictions). By considering how mechanisms with predictions
can perform poorly, we design new mechanisms that are more robust. Indeed, by
adjusting parameters, we demonstrate how to trade robustness for consistency.
We go beyond the single facility problem by designing novel strategy proof
mechanisms for locating two facilities with bounded consistency and robustness
that use two predictions for where to locate the two facilities.

</details>


### [62] [What Do Agents Think Others Would Do? Level-2 Inverse Games for Inferring Agents' Estimates of Others' Objectives](https://arxiv.org/abs/2508.03824)
*Hamzah I. Khan,Jingqi Li,David Fridovich-Keil*

Main category: cs.GT

TL;DR: 现有逆博弈论方法的假设在现实决策场景中不适用，本文提出二级推理框架解决问题，实验表明该方法能发现一级方法遗漏的细微偏差。


<details>
  <summary>Details</summary>
Motivation: 现有逆博弈论方法假设个体完全了解彼此目标，在现实场景中不成立，需推断个体对彼此目标的异质估计。

Method: 提出二级推理框架解决‘每个个体对所有个体目标的看法是什么’的问题，开发基于梯度的方法寻找局部解。

Result: 证明二级推理问题即使在线性二次博弈中也是非凸的，在合成城市驾驶示例实验中，该方法能发现一级方法遗漏的细微偏差。

Conclusion: 二级推理框架能有效解决现有逆博弈论方法在现实场景中的问题。

Abstract: Effectively interpreting strategic interactions among multiple agents
requires us to infer each agent's objective from limited information. Existing
inverse game-theoretic approaches frame this challenge in terms of a "level-1"
inference problem, in which we take the perspective of a third-party observer
and assume that individual agents share complete knowledge of one another's
objectives. However, this assumption breaks down in decentralized, real-world
decision scenarios like urban driving and bargaining, in which agents may act
based on conflicting views of one another's objectives. We demonstrate the
necessity of inferring agents' heterogeneous estimates of each other's
objectives through empirical examples, and by theoretically characterizing the
prediction error of level-1 inference on fictitious gameplay data from
linear-quadratic games. To address this fundamental issue, we propose a
framework for level-2 inference to address the question: "What does each agent
believe about all agents' objectives?" We prove that the level-2 inference
problem is non-convex even in benign settings like linear-quadratic games, and
we develop an efficient gradient-based approach for identifying local
solutions. Experiments on a synthetic urban driving example show that our
approach uncovers nuanced misalignments that level-1 methods miss.

</details>


### [63] [Inequality in the Age of Pseudonymity](https://arxiv.org/abs/2508.04668)
*Aviv Yaish,Nir Chemaya,Lin William Cong,Dahlia Malkhi*

Main category: cs.GT

TL;DR: 分析不平等指标在假名环境下的表现，指出Sybil身份会扭曲指标，提出抗Sybil指标但有局限性，证明流行指标易受操纵并研究Sybil产生动态。


<details>
  <summary>Details</summary>
Motivation: 研究不平等指标在基于互联网或区块链的假名平台中的表现和面临的问题。

Method: 理论分析，提出抗Sybil指标并对其进行刻画。

Result: Sybil身份会扭曲不平等指标；抗Sybil指标虽满足放松性质，但难以细粒度评估不平等；流行指标易受Sybil操纵。

Conclusion: 在假名环境下，现有满足经典性质的不平等指标无法准确测量经济不平等，抗Sybil指标有局限性。

Abstract: Inequality measures such as the Gini coefficient are used to inform and
motivate policymaking, and are increasingly applied to digital platforms. We
analyze how measures fare in pseudonymous settings, as common to internet-based
or blockchain-based platforms. One key challenge that arises is the ability of
actors to create multiple fake identities under fictitious false names, also
known as ``Sybils.'' While some actors may do so to preserve their privacy, we
show that this can inadvertently distort inequality metrics. As we show, when
using inequality measures that satisfy literature's canonical set of desired
properties, the presence of Sybils in an economy implies that it is impossible
to properly measure the economy's inequality. Then, we present several classes
of Sybil-proof measures that satisfy relaxed versions of the aforementioned
desired properties, and, by fully characterizing them, we prove that the
structure imposed restricts their ability to assess inequality at a
fine-grained level. In addition, we prove that popular inequality metrics,
including the famous Gini coefficient, are vulnerable to Sybil manipulations,
and examine the dynamics that result in the creation of Sybils, whether in
pseudonymous settings or traditional ones.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [64] [Suggest, Complement, Inspire: Story of Two Tower Recommendations at Allegro.com](https://arxiv.org/abs/2508.03702)
*Aleksandra Osowska-Kurczab,Klaudia Nazarko,Mateusz Marzec,Lidia Wojciechowska,Eliška Kremeňová*

Main category: cs.IR

TL;DR: 本文介绍了Allegro.com部署的基于内容的统一推荐系统，基于双塔检索框架，能适应三种推荐任务，经A/B测试有显著收益，体现了灵活可扩展架构的优势。


<details>
  <summary>Details</summary>
Motivation: 解决构建大规模电商推荐系统的三个关键技术挑战，即设计通用推荐架构、降低维护成本和管理动态产品目录。

Method: 采用流行的双塔检索框架，用文本和结构化属性表示产品，通过近似最近邻搜索实现高效检索，修改少量组件适应三种推荐任务。

Result: 经过两年的A/B测试，在桌面和移动应用渠道的参与度和基于利润的指标上有显著提升。

Conclusion: 灵活、可扩展的架构能以最小的维护开销满足不同用户意图。

Abstract: Building large-scale e-commerce recommendation systems requires addressing
three key technical challenges: (1) designing a universal recommendation
architecture across dozens of placements, (2) decreasing excessive maintenance
costs, and (3) managing a highly dynamic product catalogue. This paper presents
a unified content-based recommendation system deployed at Allegro.com, the
largest e-commerce platform of European origin. The system is built on a
prevalent Two Tower retrieval framework, representing products using textual
and structured attributes, which enables efficient retrieval via Approximate
Nearest Neighbour search. We demonstrate how the same model architecture can be
adapted to serve three distinct recommendation tasks: similarity search,
complementary product suggestions, and inspirational content discovery, by
modifying only a handful of components in either the model or the serving
logic. Extensive A/B testing over two years confirms significant gains in
engagement and profit-based metrics across desktop and mobile app channels. Our
results show that a flexible, scalable architecture can serve diverse user
intents with minimal maintenance overhead.

</details>


### [65] [Privacy Risks of LLM-Empowered Recommender Systems: An Inversion Attack Perspective](https://arxiv.org/abs/2508.03703)
*Yubo Wang,Min Tang,Nuo Shen,Shujie Cui,Weiqing Wang*

Main category: cs.IR

TL;DR: 研究发现大语言模型驱动的推荐系统易受重构攻击，提出方法优化重构并实验验证其效果，揭示隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统有局限，大语言模型驱动的推荐系统虽有效但存在易受重构攻击、泄露系统和用户隐私的问题，需研究该威胁。

Method: 对针对大语言模型驱动推荐系统的反转攻击进行系统研究，复现vec2text框架并使用相似性引导细化方法优化，从模型生成的对数中更准确地重构文本提示。

Result: 在两个领域和两个代表性的基于大语言模型的推荐模型上实验，能恢复近65%的用户交互项目，87%的情况下正确推断年龄和性别，隐私泄露对受害模型性能不敏感，高度依赖领域一致性和提示复杂度。

Conclusion: 大语言模型驱动的推荐系统存在严重的隐私漏洞。

Abstract: The large language model (LLM) powered recommendation paradigm has been
proposed to address the limitations of traditional recommender systems, which
often struggle to handle cold start users or items with new IDs. Despite its
effectiveness, this study uncovers that LLM empowered recommender systems are
vulnerable to reconstruction attacks that can expose both system and user
privacy. To examine this threat, we present the first systematic study on
inversion attacks targeting LLM empowered recommender systems, where
adversaries attempt to reconstruct original prompts that contain personal
preferences, interaction histories, and demographic attributes by exploiting
the output logits of recommendation models. We reproduce the vec2text framework
and optimize it using our proposed method called Similarity Guided Refinement,
enabling more accurate reconstruction of textual prompts from model generated
logits. Extensive experiments across two domains (movies and books) and two
representative LLM based recommendation models demonstrate that our method
achieves high fidelity reconstructions. Specifically, we can recover nearly 65
percent of the user interacted items and correctly infer age and gender in 87
percent of the cases. The experiments also reveal that privacy leakage is
largely insensitive to the victim model's performance but highly dependent on
domain consistency and prompt complexity. These findings expose critical
privacy vulnerabilities in LLM empowered recommender systems.

</details>


### [66] [Evaluating Generative AI Tools for Personalized Offline Recommendations: A Comparative Study](https://arxiv.org/abs/2508.03710)
*Rafael Salinas-Buestan,Otto Parra,Nelly Condori-Fernandez,Maria Fernanda Granda*

Main category: cs.IR

TL;DR: 研究评估5种常用生成式AI工具在为重复性劳损风险人群推荐非数字活动时的表现和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在健康相关行为干预，尤其是减少技术使用方面的有效性尚未充分探索，本研究旨在评估其在特定场景下的表现。

Method: 遵循GQM范式，让生成式AI工具根据预定义用户画像和干预场景推荐离线活动，评估定量性能和定性方面，定义两个研究问题。

Result: 摘要未提及。

Conclusion: 摘要未提及。

Abstract: Background: Generative AI tools have become increasingly relevant in
supporting personalized recommendations across various domains. However, their
effectiveness in health-related behavioral interventions, especially those
aiming to reduce the use of technology, remains underexplored. Aims: This study
evaluates the performance and user satisfaction of the five most widely used
generative AI tools when recommending non-digital activities tailored to
individuals at risk of repetitive strain injury. Method: Following the
Goal/Question/Metric (GQM) paradigm, this proposed experiment involves
generative AI tools that suggest offline activities based on predefined user
profiles and intervention scenarios. The evaluation is focused on quantitative
performance (precision, recall, F1-score and MCC-score) and qualitative aspects
(user satisfaction and perceived recommendation relevance). Two research
questions were defined: RQ1 assessed which tool delivers the most accurate
recommendations, and RQ2 evaluated how tool choice influences user
satisfaction.

</details>


### [67] [A Social Data-Driven System for Identifying Estate-related Events and Topics](https://arxiv.org/abs/2508.03711)
*Wenchuan Mu,Menglin Li,Kwan Hui Lim*

Main category: cs.IR

TL;DR: 提出基于语言模型系统，从社交媒体内容检测和分类房地产相关事件，用于城市管理等。


<details>
  <summary>Details</summary>
Motivation: 社交媒体普及，可作为识别房地产相关问题的宝贵资源，助力城市管理。

Method: 采用分层分类框架过滤和分类相关帖子，用基于transformer的地理定位模块推断无地理标签帖子位置。

Result: 实现了能从社交媒体内容检测和分类房地产相关事件的系统。

Conclusion: 该集成方法可为城市管理、运营响应和态势感知提供及时、数据驱动的见解。

Abstract: Social media platforms such as Twitter and Facebook have become deeply
embedded in our everyday life, offering a dynamic stream of localized news and
personal experiences. The ubiquity of these platforms position them as valuable
resources for identifying estate-related issues, especially in the context of
growing urban populations. In this work, we present a language model-based
system for the detection and classification of estate-related events from
social media content. Our system employs a hierarchical classification
framework to first filter relevant posts and then categorize them into
actionable estate-related topics. Additionally, for posts lacking explicit
geotags, we apply a transformer-based geolocation module to infer posting
locations at the point-of-interest level. This integrated approach supports
timely, data-driven insights for urban management, operational response and
situational awareness.

</details>


### [68] [Measuring the stability and plasticity of recommender systems](https://arxiv.org/abs/2508.03941)
*Maria João Lavoura,Robert Jungnickel,João Vinagre*

Main category: cs.IR

TL;DR: 提出研究推荐模型再训练行为的方法，通过离线评估协议分析模型长期表现，以GoodReads数据集初步实验展示其潜力。


<details>
  <summary>Details</summary>
Motivation: 传统离线评估协议只能反映过去某时刻模型性能，而在线系统会随时间演变，需研究再训练时模型表现及评估结果可信度。

Method: 提出按保留过去模式（稳定性）和适应变化（可塑性）对算法进行分析的方法，设计不依赖数据集、算法和指标的离线评估协议。

Result: 在GoodReads数据集上对三种算法的初步实验显示不同算法有不同稳定性和可塑性特征，且二者可能存在权衡。

Conclusion: 虽需更多实验确认，但提出的框架对了解推荐模型长期动态很有用。

Abstract: The typical offline protocol to evaluate recommendation algorithms is to
collect a dataset of user-item interactions and then use a part of this dataset
to train a model, and the remaining data to measure how closely the model
recommendations match the observed user interactions. This protocol is
straightforward, useful and practical, but it only captures performance of a
particular model trained at some point in the past. We know, however, that
online systems evolve over time. In general, it is a good idea that models
reflect such changes, so models are frequently retrained with recent data. But
if this is the case, to what extent can we trust previous evaluations? How will
a model perform when a different pattern (re)emerges? In this paper we propose
a methodology to study how recommendation models behave when they are
retrained. The idea is to profile algorithms according to their ability to, on
the one hand, retain past patterns -- stability -- and, on the other hand,
(quickly) adapt to changes -- plasticity. We devise an offline evaluation
protocol that provides detail on the long-term behavior of models, and that is
agnostic to datasets, algorithms and metrics. To illustrate the potential of
this framework, we present preliminary results of three different types of
algorithms on the GoodReads dataset that suggest different stability and
plasticity profiles depending on the algorithmic technique, and a possible
trade-off between stability and plasticity.Although additional experiments will
be necessary to confirm these observations, they already illustrate the
usefulness of the proposed framework to gain insights on the long term dynamics
of recommendation models.

</details>


### [69] [ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval](https://arxiv.org/abs/2508.04001)
*Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Zhichao Xu,Zhan Su,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 提出ConvMix框架增强对话式密集检索，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 以往对话式搜索微调训练范式存在数据稀缺问题。

Method: 提出ConvMix混合标准框架，借助大语言模型设计可扩展的双边相关性判断增强模式，集成质量控制机制。

Result: 在五个常用基准上，ConvMix框架训练的对话式密集检索器优于先前基线方法。

Conclusion: ConvMix框架在对话式密集检索中具有优越的有效性。

Abstract: Conversational search aims to satisfy users' complex information needs via
multiple-turn interactions. The key challenge lies in revealing real users'
search intent from the context-dependent queries. Previous studies achieve
conversational search by fine-tuning a conversational dense retriever with
relevance judgments between pairs of context-dependent queries and documents.
However, this training paradigm encounters data scarcity issues. To this end,
we propose ConvMix, a mixed-criteria framework to augment conversational dense
retrieval, which covers more aspects than existing data augmentation
frameworks. We design a two-sided relevance judgment augmentation schema in a
scalable manner via the aid of large language models. Besides, we integrate the
framework with quality control mechanisms to obtain semantically diverse
samples and near-distribution supervisions to combine various annotated data.
Experimental results on five widely used benchmarks show that the
conversational dense retriever trained by our ConvMix framework outperforms
previous baseline methods, which demonstrates our superior effectiveness.

</details>


### [70] [Enhancing Serendipity Recommendation System by Constructing Dynamic User Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2508.04032)
*Qian Yong,Yanhui Li,Jialiang Shi,Yaguang Dou,Tian Qi*

Main category: cs.IR

TL;DR: 工业推荐系统反馈循环有弊端，大语言模型用于推荐有挑战，本文提出用大语言模型动态构建用户知识图谱的方法，经线上实验提升了用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决工业推荐系统反馈循环问题，发挥大语言模型在推荐系统的潜力，克服其在推理合理性、结果有用性和延迟方面的挑战。

Method: 提出两阶段框架，包括两跳兴趣推理动态构建用户知识图谱和近线适配部署模型，还提出结合u2i和i2i检索能力的模型。

Result: 在有千万用户的得物App上进行线上实验，曝光新颖率、点击新颖率、人均浏览时长、访客点击率和访客交互渗透率均有提升。

Conclusion: 所提方法能增强推荐系统的意外性，提升用户体验。

Abstract: The feedback loop in industrial recommendation systems reinforces homogeneous
content, creates filter bubble effects, and diminishes user satisfaction.
Recently, large language models(LLMs) have demonstrated potential in
serendipity recommendation, thanks to their extensive world knowledge and
superior reasoning capabilities. However, these models still face challenges in
ensuring the rationality of the reasoning process, the usefulness of the
reasoning results, and meeting the latency requirements of industrial
recommendation systems (RSs). To address these challenges, we propose a method
that leverages llm to dynamically construct user knowledge graphs, thereby
enhancing the serendipity of recommendation systems. This method comprises a
two stage framework:(1) two-hop interest reasoning, where user static profiles
and historical behaviors are utilized to dynamically construct user knowledge
graphs via llm. Two-hop reasoning, which can enhance the quality and accuracy
of LLM reasoning results, is then performed on the constructed graphs to
identify users' potential interests; and(2) Near-line adaptation, a
cost-effective approach to deploying the aforementioned models in industrial
recommendation systems. We propose a u2i (user-to-item) retrieval model that
also incorporates i2i (item-to-item) retrieval capabilities, the retrieved
items not only exhibit strong relevance to users' newly emerged interests but
also retain the high conversion rate of traditional u2i retrieval. Our online
experiments on the Dewu app, which has tens of millions of users, indicate that
the method increased the exposure novelty rate by 4.62%, the click novelty rate
by 4.85%, the average view duration per person by 0.15%, unique visitor click
through rate by 0.07%, and unique visitor interaction penetration by 0.30%,
enhancing user experience.

</details>


### [71] [Benefit from Rich: Tackling Search Interaction Sparsity in Search Enhanced Recommendation](https://arxiv.org/abs/2508.04145)
*Teng Shi,Weijie Yu,Xiao Zhang,Ming He,Jianping Fan,Jun Xu*

Main category: cs.IR

TL;DR: 提出GSERec方法缓解搜索增强推荐中的数据稀疏问题，实验表明其在真实数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有搜索增强推荐方法依赖丰富搜索交互，对多数搜索行为稀疏的用户效果有限，需解决搜索数据稀疏问题。

Method: 提出GSERec方法，利用大语言模型和向量量化生成离散代码构建用户代码图，通过图上消息传递将丰富搜索数据用户的嵌入传播给稀疏交互用户，引入对比损失建模用户相似性。

Result: 在三个真实数据集上的实验显示，GSERec始终优于基线，对搜索行为稀疏的用户效果更佳。

Conclusion: GSERec方法能有效缓解搜索增强推荐中的数据稀疏问题，提升推荐性能。

Abstract: In modern online platforms, search and recommendation (S&R) often coexist,
offering opportunities for performance improvement through search-enhanced
approaches. Existing studies show that incorporating search signals boosts
recommendation performance. However, the effectiveness of these methods relies
heavily on rich search interactions. They primarily benefit a small subset of
users with abundant search behavior, while offering limited improvements for
the majority of users who exhibit only sparse search activity. To address the
problem of sparse search data in search-enhanced recommendation, we face two
key challenges: (1) how to learn useful search features for users with sparse
search interactions, and (2) how to design effective training objectives under
sparse conditions. Our idea is to leverage the features of users with rich
search interactions to enhance those of users with sparse search interactions.
Based on this idea, we propose GSERec, a method that utilizes message passing
on the User-Code Graphs to alleviate data sparsity in Search-Enhanced
Recommendation. Specifically, we utilize Large Language Models (LLMs) with
vector quantization to generate discrete codes, which connect similar users and
thereby construct the graph. Through message passing on this graph, embeddings
of users with rich search data are propagated to enhance the embeddings of
users with sparse interactions. To further ensure that the message passing
captures meaningful information from truly similar users, we introduce a
contrastive loss to better model user similarities. The enhanced user
representations are then integrated into downstream search-enhanced
recommendation models. Experiments on three real-world datasets show that
GSERec consistently outperforms baselines, especially for users with sparse
search behaviors.

</details>


### [72] [Bridging Search and Recommendation through Latent Cross Reasoning](https://arxiv.org/abs/2508.04152)
*Teng Shi,Weicong Qin,Weijie Yu,Xiao Zhang,Ming He,Jianping Fan,Jun Xu*

Main category: cs.IR

TL;DR: 提出潜在交叉推理框架改善搜索感知推荐，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法未明确识别有用搜索行为，用户搜索历史含噪声信号，影响推荐性能。

Method: 设计潜在交叉推理框架，先编码用户搜索与推荐历史，迭代推理搜索行为；用对比学习对齐潜在推理状态与目标项，引入强化学习优化排序性能。

Result: 在公共基准上实验，比强基线有持续改进。

Conclusion: 推理对提升搜索感知推荐很重要。

Abstract: Search and recommendation (S&R) are fundamental components of modern online
platforms, yet effectively leveraging search behaviors to improve
recommendation remains a challenging problem. User search histories often
contain noisy or irrelevant signals that can even degrade recommendation
performance, while existing approaches typically encode S&R histories either
jointly or separately without explicitly identifying which search behaviors are
truly useful. Inspired by the human decision-making process, where one first
identifies recommendation intent and then reasons about relevant evidence, we
design a latent cross reasoning framework that first encodes user S&R histories
to capture global interests and then iteratively reasons over search behaviors
to extract signals beneficial for recommendation. Contrastive learning is
employed to align latent reasoning states with target items, and reinforcement
learning is further introduced to directly optimize ranking performance.
Extensive experiments on public benchmarks demonstrate consistent improvements
over strong baselines, validating the importance of reasoning in enhancing
search-aware recommendation.

</details>


### [73] [SSEmb: A Joint Structural and Semantic Embedding Framework for Mathematical Formula Retrieval](https://arxiv.org/abs/2508.04162)
*Ruyin Li,Xiaoyu Chen*

Main category: cs.IR

TL;DR: 提出SSEmb嵌入框架用于公式检索，结合结构和语义特征，在ARQMath - 3任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决数学信息检索中公式检索的问题，提升公式检索性能。

Method: 采用图对比学习编码公式的算子图，引入图数据增强的替换策略；用Sentence - BERT编码公式周围文本；分别计算结构和语义相似度并加权融合。

Result: 在ARQMath - 3公式检索任务中，SSEmb在P'@10和nDCG'@10上比现有基于嵌入的方法高出超5个百分点，增强其他方法运行性能，与Approach0结合达最优。

Conclusion: SSEmb是一种有效的公式检索嵌入框架，能提升公式检索性能。

Abstract: Formula retrieval is an important topic in Mathematical Information
Retrieval. We propose SSEmb, a novel embedding framework capable of capturing
both structural and semantic features of mathematical formulas. Structurally,
we employ Graph Contrastive Learning to encode formulas represented as Operator
Graphs. To enhance structural diversity while preserving mathematical validity
of these formula graphs, we introduce a novel graph data augmentation approach
through a substitution strategy. Semantically, we utilize Sentence-BERT to
encode the surrounding text of formulas. Finally, for each query and its
candidates, structural and semantic similarities are calculated separately and
then fused through a weighted scheme. In the ARQMath-3 formula retrieval task,
SSEmb outperforms existing embedding-based methods by over 5 percentage points
on P'@10 and nDCG'@10. Furthermore, SSEmb enhances the performance of all runs
of other methods and achieves state-of-the-art results when combined with
Approach0.

</details>


### [74] [ViLLA-MMBench: A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation](https://arxiv.org/abs/2508.04206)
*Fatemeh Nazary,Ali Tourani,Yashar Deldjoo,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 提出ViLLA - MMBench基准用于大语言模型增强的多模态电影推荐，支持多种融合方式和骨干模型，实验结果显示LLM增强和强文本嵌入能提升冷启动和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基准在长视频内容推荐中对视觉、音频和文本模态联合建模不足，需新的多模态电影推荐基准。

Method: 基于MovieLens和MMTF - 14K构建基准，对齐三种模态的密集物品嵌入，用LLM丰富缺失元数据，使用可配置编码器嵌入文本，支持多种融合方式和骨干模型，通过YAML文件进行实验。

Result: LLM增强和强文本嵌入与视听特征融合时，能提升冷启动和覆盖率，系统性基准测试揭示通用与特定骨干或指标的组合。

Conclusion: 开源代码等资源可推动可复现、公平的多模态推荐系统研究和生成式AI在大规模推荐中的应用。

Abstract: Recommending long-form video content demands joint modeling of visual, audio,
and textual modalities, yet most benchmarks address only raw features or narrow
fusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for
LLM-augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K,
it aligns dense item embeddings from three modalities: audio (block-level,
i-vector), visual (CNN, AVF), and text. Missing or sparse metadata is
automatically enriched using state-of-the-art LLMs (e.g., OpenAI Ada),
generating high-quality synopses for thousands of movies. All text (raw or
augmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5),
producing multiple ready-to-use sets. The pipeline supports interchangeable
early-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and
multiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are
fully declarative via a single YAML file. Evaluation spans accuracy (Recall,
nDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty,
diversity, fairness. Results show LLM-based augmentation and strong text
embeddings boost cold-start and coverage, especially when fused with
audio-visual features. Systematic benchmarking reveals universal versus
backbone- or metric-specific combinations. Open-source code, embeddings, and
configs enable reproducible, fair multimodal RS research and advance principled
generative AI integration in large-scale recommendation. Code:
https://recsys-lab.github.io/ViLLA-MMBench

</details>


### [75] [Discrete-event Tensor Factorization: Learning a Smooth Embedding for Continuous Domains](https://arxiv.org/abs/2508.04221)
*Joey De Pauw,Bart Goethals*

Main category: cs.IR

TL;DR: 本文分析时间在因子分解式推荐模型中的编码方式，提出连续时间编码机制，实验表明显式建模时间能捕捉时间信号，但简单的事后流行度调整常能获最佳性能，强调推荐任务需专门机制外推未来数据。


<details>
  <summary>Details</summary>
Motivation: 多数推荐算法虽重视近期交互，但很少显式建模时间，需研究时间在因子分解式推荐模型中的编码。

Method: 将绝对时间作为特征，除简单分箱法，还提出全新的完全连续时间编码机制，在损失函数中用多项式拟合避免离散化。

Result: 在三个多年真实数据集上实验，显式建模时间能有效捕捉时间信号，简单事后流行度调整常能在测试集获最佳性能。

Conclusion: 推荐任务中预测未来比捕捉过去趋势更重要，需专门机制外推未来数据。

Abstract: Recommender systems learn from past user behavior to predict future user
preferences. Intuitively, it has been established that the most recent
interactions are more indicative of future preferences than older interactions.
Many recommendation algorithms use this notion to either drop older
interactions or to assign them a lower weight, so the model can focus on the
more informative, recent information. However, very few approaches model the
flow of time explicitly.
  This paper analyzes how time can be encoded in factorization-style
recommendation models. By including absolute time as a feature, our models can
learn varying user preferences and changing item perception over time. In
addition to simple binning approaches, we also propose a novel, fully
continuous time encoding mechanism. Through the use of a polynomial fit inside
the loss function, our models completely avoid the need for discretization, and
they are able to capture the time dimension in arbitrary resolution.
  We perform a comparative study on three real-world datasets that span
multiple years, where long user histories are present, and items stay relevant
for a longer time. Empirical results show that, by explicitly modeling time,
our models are very effective at capturing temporal signals, such as varying
item popularities over time. Despite this however, our experiments also
indicate that a simple post-hoc popularity adjustment is often sufficient to
achieve the best performance on the unseen test set. This teaches us that, for
the recommendation task, predicting the future is more important than capturing
past trends. As such, we argue that specialized mechanisms are needed for
extrapolation to future data.

</details>


### [76] [I$^3$-MRec: Invariant Learning with Information Bottleneck for Incomplete Modality Recommendation](https://arxiv.org/abs/2508.04247)
*Huilin Chen,Miaomiao Cai,Fan Liu,Zhiyong Cheng,Richang Hong,Meng Wang*

Main category: cs.IR

TL;DR: 提出I$^3$-MRec方法解决多模态推荐系统中模态缺失问题，实验显示其在多种模态缺失场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统因模态缺失问题，导致模型鲁棒性和泛化能力下降。

Method: 引入I$^3$-MRec方法，利用不变学习和信息瓶颈原理，确保跨模态偏好不变，提取紧凑有效的模态表示；用不变风险最小化学习特定模态物品表示，用基于信息瓶颈原理的缺失感知融合模块提取物品嵌入。

Result: 在三个真实数据集上的实验表明，I$^3$-MRec在各种模态缺失场景下始终优于现有先进的多模态推荐系统方法。

Conclusion: I$^3$-MRec在实际应用中有效且具有鲁棒性，代码和处理后的数据集已开源。

Abstract: Multimodal recommender systems (MRS) improve recommendation performance by
integrating diverse semantic information from multiple modalities. However, the
assumption of the availability of all modalities rarely holds in practice due
to missing images, incomplete descriptions, or inconsistent user content. These
challenges significantly degrade the robustness and generalization capabilities
of current models. To address these challenges, we introduce a novel method
called \textbf{I$^3$-MRec}, which uses \textbf{I}nvariant learning with
\textbf{I}nformation bottleneck principle for \textbf{I}ncomplete
\textbf{M}odality \textbf{Rec}ommendation. To achieve robust performance in
missing modality scenarios, I$^3$-MRec enforces two pivotal properties: (i)
cross-modal preference invariance, which ensures consistent user preference
modeling across varying modality environments, and (ii) compact yet effective
modality representation, which filters out task-irrelevant modality information
while maximally preserving essential features relevant to recommendation. By
treating each modality as a distinct semantic environment, I$^3$-MRec employs
invariant risk minimization (IRM) to learn modality-specific item
representations. In parallel, a missing-aware fusion module grounded in the
Information Bottleneck (IB) principle extracts compact and effective item
embeddings by suppressing modality noise and preserving core user preference
signals. Extensive experiments conducted on three real-world datasets
demonstrate that I$^3$-MRec consistently outperforms existing state-of-the-art
MRS methods across various modality-missing scenarios, highlighting its
effectiveness and robustness in practical applications. The code and processed
datasets are released at https://github.com/HuilinChenJN/I3-MRec.

</details>


### [77] [Comparative Analysis of Novel NIRMAL Optimizer Against Adam and SGD with Momentum](https://arxiv.org/abs/2508.04293)
*Nirmal Gaud,Surej Mouli,Preeti Katiyar,Vaduguru Venkata Ramya*

Main category: cs.IR

TL;DR: 提出新优化算法NIRMAL，结合多种策略，在四个图像分类数据集上与Adam和SGD with Momentum对比，表现有竞争力，尤其在CIFAR - 100上，证明其是通用有效的优化器。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的优化算法用于深度学习任务。

Method: 提出NIRMAL算法，结合梯度下降、动量、随机扰动、自适应学习率和非线性变换等策略，在四个基准图像分类数据集上用自定义CNN架构评估。

Result: NIRMAL在CIFAR - 100上测试准确率45.32%，加权F1分数0.4328，超过Adam，接近SGD with Momentum，且在复杂数据集上收敛稳健、泛化能力强。

Conclusion: NIRMAL是适用于各种深度学习任务的通用且有效的优化器。

Abstract: This study proposes NIRMAL (Novel Integrated Robust Multi-Adaptation
Learning), a novel optimization algorithm that combines multiple strategies
inspired by the movements of the chess piece. These strategies include gradient
descent, momentum, stochastic perturbations, adaptive learning rates, and
non-linear transformations. We carefully evaluated NIRMAL against two widely
used and successful optimizers, Adam and SGD with Momentum, on four benchmark
image classification datasets: MNIST, FashionMNIST, CIFAR-10, and CIFAR-100.
The custom convolutional neural network (CNN) architecture is applied on each
dataset. The experimental results show that NIRMAL achieves competitive
performance, particularly on the more challenging CIFAR-100 dataset, where it
achieved a test accuracy of 45.32\%and a weighted F1-score of 0.4328. This
performance surpasses Adam (41.79\% accuracy, 0.3964 F1-score) and closely
matches SGD with Momentum (46.97\% accuracy, 0.4531 F1-score). Also, NIRMAL
exhibits robust convergence and strong generalization capabilities, especially
on complex datasets, as evidenced by stable training results in loss and
accuracy curves. These findings underscore NIRMAL's significant ability as a
versatile and effective optimizer for various deep learning tasks.

</details>


### [78] [Algorithm Selection for Recommender Systems via Meta-Learning on Algorithm Characteristics](https://arxiv.org/abs/2508.04419)
*Jarne Mathi Decker,Joeran Beel*

Main category: cs.IR

TL;DR: 提出针对推荐系统选择的按用户元学习方法，结合用户元特征和算法特征，初步结果显示性能提升，为构建推荐系统提供方向。


<details>
  <summary>Details</summary>
Motivation: 推荐系统的算法选择问题是挑战，传统元学习方法忽略算法内在属性，近期工作表明明确算法特征可提升性能。

Method: 提出按用户元学习方法，利用用户元特征和从源代码自动提取的算法特征。

Result: 在六个数据集上，用算法特征增强元学习器使平均NDCG@10性能从0.135提升到0.147，优于单最佳算法基线，缩小与理论神谕选择器10.5%的性能差距。

Conclusion: 静态源代码指标有预测价值，为构建更强大智能的推荐系统提供了有前景的方向。

Abstract: The Algorithm Selection Problem for recommender systems-choosing the best
algorithm for a given user or context-remains a significant challenge.
Traditional meta-learning approaches often treat algorithms as categorical
choices, ignoring their intrinsic properties. Recent work has shown that
explicitly characterizing algorithms with features can improve model
performance in other domains. Building on this, we propose a per-user
meta-learning approach for recommender system selection that leverages both
user meta-features and automatically extracted algorithm features from source
code. Our preliminary results, averaged over six diverse datasets, show that
augmenting a meta-learner with algorithm features improves its average NDCG@10
performance by 8.83% from 0.135 (user features only) to 0.147. This enhanced
model outperforms the Single Best Algorithm baseline (0.131) and successfully
closes 10.5% of the performance gap to a theoretical oracle selector. These
findings show that even static source code metrics provide a valuable
predictive signal, presenting a promising direction for building more robust
and intelligent recommender systems.

</details>


### [79] [TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2508.04474)
*Xinkui Zhao,Haode Li,Yifan Zhang,Guanjie Cheng,Yueshen Xu*

Main category: cs.IR

TL;DR: 现有大语言模型在知识密集场景有局限，结合知识图谱的方法也有不足，本文提出TRAIL框架，实验显示其优于现有基线，推动了自适应语言模型发展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集场景因依赖静态参数记忆有局限，且现有结合知识图谱的方法未统一推理和知识更新过程。

Method: 提出TRAIL框架，耦合联合推理和动态知识图谱细化，利用置信驱动机制处理新事实，采用即插即用架构。

Result: 在多个基准测试中，TRAIL比现有知识图谱增强和检索增强的大语言模型基线性能高3% - 13%。

Conclusion: TRAIL推动了开发具有持续学习能力和可靠透明推理能力的自适应、记忆增强语言模型的发展。

Abstract: Recent advances in large language models (LLMs) have unlocked powerful
reasoning and decision-making capabilities. However, their inherent dependence
on static parametric memory fundamentally limits their adaptability, factual
accuracy, and interpretability in knowledge-intensive scenarios. Knowledge
graphs (KGs), as structured repositories of explicit relational knowledge,
offer a promising approach for augmenting LLMs with external, interpretable
memory. Nevertheless, most existing methods that combine LLMs with KGs treat
reasoning and knowledge updating as separate processes, resulting in suboptimal
utilization of new information and hindering real-time updates. In this work,
we propose TRAIL: a novel, unified framework for Thinking, Reasoning, And
Incremental Learning that couples joint inference and dynamic KG refinement
with large language models. TRAIL enables LLM agents to iteratively explore,
update, and refine knowledge graphs during the reasoning process, employing a
confidence-driven mechanism for the generation, validation, and pruning of new
facts. This plug-and-play architecture facilitates seamless integration with
various LLMs, supporting continual adaptation without the need for retraining.
Extensive experiments on multiple benchmarks demonstrate that TRAIL outperforms
existing KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More
importantly, these results represent a significant step toward developing
adaptive, memory-augmented language models capable of continual learning and
reliable, transparent reasoning.

</details>


### [80] [Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation](https://arxiv.org/abs/2508.04571)
*Claudio Pomo,Matteo Attimonelli,Danilo Danese,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 研究多模态推荐系统中多模态项嵌入作用，用LVLMs生成嵌入提升性能，验证其语义深度和对齐性。


<details>
  <summary>Details</summary>
Motivation: 探究多模态推荐系统性能提升源于真正多模态理解还是模型复杂度增加，解决标准提取器缺乏跨模态对齐控制的问题。

Method: 利用大视觉语言模型（LVLMs）通过结构化提示生成多模态嵌入。

Result: 实验显示性能显著提升，LVLMs嵌入可解码为文本描述，结合到推荐系统中能提升性能。

Conclusion: 强调语义丰富表示的重要性，LVLMs可作为构建多模态推荐任务表示的基础。

Abstract: Multimodal Recommender Systems aim to improve recommendation accuracy by
integrating heterogeneous content, such as images and textual metadata. While
effective, it remains unclear whether their gains stem from true multimodal
understanding or increased model complexity. This work investigates the role of
multimodal item embeddings, emphasizing the semantic informativeness of the
representations. Initial experiments reveal that embeddings from standard
extractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on
modality-specific encoders and ad hoc fusion strategies that lack control over
cross-modal alignment. To overcome these limitations, we leverage Large
Vision-Language Models (LVLMs) to generate multimodal-by-design embeddings via
structured prompts. This approach yields semantically aligned representations
without requiring any fusion. Experiments across multiple settings show notable
performance improvements. Furthermore, LVLMs embeddings offer a distinctive
advantage: they can be decoded into structured textual descriptions, enabling
direct assessment of their multimodal comprehension. When such descriptions are
incorporated as side content into recommender systems, they improve
recommendation performance, empirically validating the semantic depth and
alignment encoded within LVLMs outputs. Our study highlights the importance of
semantically rich representations and positions LVLMs as a compelling
foundation for building robust and meaningful multimodal representations in
recommendation tasks.

</details>


### [81] [A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature](https://arxiv.org/abs/2508.04612)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.IR

TL;DR: 提出开源可复现的自动处理论文的管道，评估效果好且有可扩展性，案例研究证明能支持复现。


<details>
  <summary>Details</summary>
Motivation: 自回归生成模型研究论文数量激增，手动文献调研和复现研究愈发不现实。

Method: 构建全开源、可复现的管道，自动从公共库检索候选文档，进行相关性过滤、元数据和超参数提取等操作。

Result: 对50篇手动标注论文定量评估，F1分数超0.85；在最多1000篇论文语料库实验有近线性可扩展性；三个案例研究能实现接近原报告的测试困惑度。

Conclusion: 该管道能有效处理自回归生成模型相关论文，支持实验复现。

Abstract: The accelerating pace of research on autoregressive generative models has
produced thousands of papers, making manual literature surveys and reproduction
studies increasingly impractical. We present a fully open-source, reproducible
pipeline that automatically retrieves candidate documents from public
repositories, filters them for relevance, extracts metadata, hyper-parameters
and reported results, clusters topics, produces retrieval-augmented summaries
and generates containerised scripts for re-running selected experiments.
Quantitative evaluation on 50 manually-annotated papers shows F1 scores above
0.85 for relevance classification, hyper-parameter extraction and citation
identification. Experiments on corpora of up to 1000 papers demonstrate
near-linear scalability with eight CPU workers. Three case studies -- AWD-LSTM
on WikiText-2, Transformer-XL on WikiText-103 and an autoregressive music model
on the Lakh MIDI dataset -- confirm that the extracted settings support
faithful reproduction, achieving test perplexities within 1--3% of the original
reports.

</details>


### [82] [HiD-VAE: Interpretable Generative Recommendation via Hierarchical and Disentangled Semantic IDs](https://arxiv.org/abs/2508.04618)
*Dengzhao Fang,Jingtong Gao,Chengcheng Zhu,Yu Li,Xiangyu Zhao,Yi Chang*

Main category: cs.IR

TL;DR: 现有生成式推荐方法的无监督分词存在语义扁平、易纠缠问题，本文提出HiD - VAE框架解决这些问题，实验验证其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法因无监督分词产生语义扁平、不可解释及表示纠缠问题，影响推荐准确性和多样性，需改进。

Method: 提出HiD - VAE框架，包括层次监督量化过程使离散代码与多级项目标签对齐，引入唯一性损失惩罚潜在空间重叠。

Result: 在三个公共基准上的大量实验表明，HiD - VAE性能优于现有方法。

Conclusion: HiD - VAE框架有效解决了现有生成式推荐方法的问题，为下游生成模型提供高质量、解纠缠的ID，具有良好性能。

Abstract: Recommender systems are indispensable for helping users navigate the immense
item catalogs of modern online platforms. Recently, generative recommendation
has emerged as a promising paradigm, unifying the conventional
retrieve-and-rank pipeline into an end-to-end model capable of dynamic
generation. However, existing generative methods are fundamentally constrained
by their unsupervised tokenization, which generates semantic IDs suffering from
two critical flaws: (1) they are semantically flat and uninterpretable, lacking
a coherent hierarchy, and (2) they are prone to representation entanglement
(i.e., ``ID collisions''), which harms recommendation accuracy and diversity.
To overcome these limitations, we propose HiD-VAE, a novel framework that
learns hierarchically disentangled item representations through two core
innovations. First, HiD-VAE pioneers a hierarchically-supervised quantization
process that aligns discrete codes with multi-level item tags, yielding more
uniform and disentangled IDs. Crucially, the trained codebooks can predict
hierarchical tags, providing a traceable and interpretable semantic path for
each recommendation. Second, to combat representation entanglement, HiD-VAE
incorporates a novel uniqueness loss that directly penalizes latent space
overlap. This mechanism not only resolves the critical ID collision problem but
also promotes recommendation diversity by ensuring a more comprehensive
utilization of the item representation space. These high-quality, disentangled
IDs provide a powerful foundation for downstream generative models. Extensive
experiments on three public benchmarks validate HiD-VAE's superior performance
against state-of-the-art methods. The code is available at
https://anonymous.4open.science/r/HiD-VAE-84B2.

</details>


### [83] [Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering](https://arxiv.org/abs/2508.04683)
*Karthik Menon,Batool Arhamna Haider,Muhammad Arham,Kanwal Mehreen,Ram Mohan Rao Kadiyala,Hamza Farooq*

Main category: cs.IR

TL;DR: 本文介绍QAM框架，能提升搜索精度和相关性，实验表明其性能优于传统方法，适用于企业搜索尤其是电商系统。


<details>
  <summary>Details</summary>
Motivation: 解决传统搜索的局限性，提升搜索的精度和相关性。

Method: 将开放文本查询分解为结构化元数据标签和语义元素，自动从自由文本查询中提取元数据过滤器。

Result: 使用亚马逊玩具评论数据集实验，QAM的mAP@5达到52.99%，优于BM25、基于编码器的语义相似度搜索等传统方法。

Conclusion: QAM是企业搜索应用尤其是电商系统的可靠解决方案。

Abstract: This study introduces Query Attribute Modeling (QAM), a hybrid framework that
enhances search precision and relevance by decomposing open text queries into
structured metadata tags and semantic elements. QAM addresses traditional
search limitations by automatically extracting metadata filters from free-form
text queries, reducing noise and enabling focused retrieval of relevant items.
  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique
items with 40,000+ reviews and detailed product attributes) demonstrated QAM's
superior performance, achieving a mean average precision at 5 (mAP@5) of
52.99\%. This represents significant improvement over conventional methods,
including BM25 keyword search, encoder-based semantic similarity search,
cross-encoder re-ranking, and hybrid search combining BM25 and semantic results
via Reciprocal Rank Fusion (RRF). The results establish QAM as a robust
solution for Enterprise Search applications, particularly in e-commerce
systems.

</details>


### [84] [Recommendation with Generative Models](https://arxiv.org/abs/2409.15173)
*Yashar Deldjoo,Zhankui He,Julian McAuley,Anton Korikov,Scott Sanner,Arnau Ramisa,Rene Vidal,Maheswaran Sathiamoorthy,Atoosa Kasrizadeh,Silvia Milano,Francesco Ricci*

Main category: cs.IR

TL;DR: 文章介绍生成模型近年在机器学习中突出，在推荐系统有应用，书对生成模型全面解读，给出分类，还探讨影响与风险。


<details>
  <summary>Details</summary>
Motivation: 超越现有文献，全面理解生成模型及其应用，关注深度生成模型分类。

Method: 引入分类法将深度生成模型分为ID驱动模型、大语言模型和多模态模型。

Result: 该分类法让研究者能轻松了解推荐系统在不同领域的发展。

Conclusion: 强调研究生成模型需有强大评估框架。

Abstract: Generative models are a class of AI models capable of creating new instances
of data by learning and sampling from their statistical distributions. In
recent years, these models have gained prominence in machine learning due to
the development of approaches such as generative adversarial networks (GANs),
variational autoencoders (VAEs), and transformer-based architectures such as
GPT. These models have applications across various domains, such as image
generation, text synthesis, and music composition. In recommender systems,
generative models, referred to as Gen-RecSys, improve the accuracy and
diversity of recommendations by generating structured outputs, text-based
interactions, and multimedia content. By leveraging these capabilities,
Gen-RecSys can produce more personalized, engaging, and dynamic user
experiences, expanding the role of AI in eCommerce, media, and beyond.
  Our book goes beyond existing literature by offering a comprehensive
understanding of generative models and their applications, with a special focus
on deep generative models (DGMs) and their classification. We introduce a
taxonomy that categorizes DGMs into three types: ID-driven models, large
language models (LLMs), and multimodal models. Each category addresses unique
technical and architectural advancements within its respective research area.
This taxonomy allows researchers to easily navigate developments in Gen-RecSys
across domains such as conversational AI and multimodal content generation.
Additionally, we examine the impact and potential risks of generative models,
emphasizing the importance of robust evaluation frameworks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [85] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: 本文提出Privileged Contrastive Pretraining (PriCon)框架解决情感模型从实验室到现实环境迁移的问题，实验表明该框架表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决情感模型从受控实验室环境到非受控现实环境可靠迁移的挑战。

Method: 引入PriCon框架，先通过监督对比学习（SCL）预训练模型，再将其作为Learning Using Privileged Information (LUPI)框架中的教师模型。

Result: 在RECOLA和AGAIN两个基准情感语料库上的实验显示，使用PriCon训练的模型始终优于LUPI和端到端模型，在很多情况下性能与使用所有模态训练和测试的模型相当。

Conclusion: PriCon有潜力缩小体外和体内情感建模的差距，为现实应用提供可扩展且实用的解决方案。

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [86] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: 提出PILOT - C轨迹压缩框架，支持任意维度轨迹压缩，在多维度数据集上表现优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 位置感知设备产生大量轨迹数据，需要高效压缩方法，现有线简化方法有局限性。

Method: 提出PILOT - C框架，将频域物理建模与有界误差优化相结合，独立压缩各空间轴。

Result: 在四个真实数据集上，PILOT - C在压缩比上平均比CISED - W高19.2%，轨迹保真度误差平均降低32.6%；在3D数据集上压缩比相比SQUISH - E提高49%。

Conclusion: PILOT - C是一种高效的轨迹压缩框架，支持多维度且性能优越。

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [87] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: 提出用于胸部X光片（CXR）任务的生成模型CX - Mind，采用基于课程的强化学习和可验证过程奖励（CuRL - VPR）实现交错‘思考 - 回答’推理，实验显示性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在CXR多任务诊断中存在推理过程缺乏可验证监督的问题，导致长推理、稀疏奖励和频繁幻觉等挑战。

Method: 构建指令调优数据集CX - Set，生成高质量交错推理数据点，在Group Relative Policy Optimization框架下分两阶段优化，结合基于规则的条件过程奖励。

Result: CX - Mind在视觉理解、文本生成和时空对齐方面显著优于现有医学和通用领域MLLMs，在真实临床数据集上表现出色，多中心专家评估确认其临床实用性。

Conclusion: CX - Mind能有效解决现有多模态模型在CXR多任务诊断中的问题，具有良好的性能和临床应用价值。

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [88] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: 本文提出LLM编辑器LKS，可精确大规模编辑知识并保留模型通用能力，在Llama - 2和Mistral上实验有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练时会保留不准确或过时信息，现有编辑方法难以同时编辑大量事实信息且可能损害模型通用能力。

Method: 通过实证研究发现可像编辑自然语言输入一样编辑LLM内部表示和替换实体，引入Latent Knowledge Scalpel (LKS)，利用轻量级超网络操纵特定实体的潜在知识。

Result: 在Llama - 2和Mistral上实验，即使同时编辑数量达10000，LKS也能有效进行知识编辑并保留模型通用能力。

Conclusion: LKS是一种有效的大语言模型知识编辑方法，可实现精确和大规模编辑。

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [89] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: 提出GlaBoost多模态梯度提升框架用于青光眼风险预测，在真实数据集上表现优异且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有青光眼检测方法依赖单模态数据且缺乏可解释性，限制临床应用，需更优方法。

Method: 使用预训练卷积编码器提取眼底照片特征，用基于Transformer的语言模型编码文本描述，结合手动评估风险分数和眼科指标，通过增强XGBoost模型融合特征分类。

Result: 在真实标注数据集上显著优于基线模型，验证准确率达98.71%，特征重要性分析显示有临床一致模式。

Conclusion: GlaBoost为可解释的青光眼诊断提供透明可扩展解决方案，可扩展到其他眼科疾病。

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [90] [Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points](https://arxiv.org/abs/2508.04351)
*Justin Lee,Behnaz Moradijamei,Heman Shakeri*

Main category: cs.LG

TL;DR: 提出多边际随机流匹配（MMSFM）方法处理高维系统演化建模，在多数据集验证其通用性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖降维技术，会简化动态并无法捕捉非平衡系统的关键瞬态行为，需新方法处理高维系统从不规则时间点有限快照观测的演化建模。

Method: 提出MMSFM，将无模拟得分和流匹配方法扩展到多边际设置，使用测度值样条增强对不规则快照时间的鲁棒性，用得分匹配防止高维空间过拟合。

Result: 在多个合成和基准数据集（包括不均匀时间点收集的基因表达数据和图像进展任务）上验证了框架。

Conclusion: 该方法具有通用性。

Abstract: Modeling the evolution of high-dimensional systems from limited snapshot
observations at irregular time points poses a significant challenge in
quantitative biology and related fields. Traditional approaches often rely on
dimensionality reduction techniques, which can oversimplify the dynamics and
fail to capture critical transient behaviors in non-equilibrium systems. We
present Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of
simulation-free score and flow matching methods to the multi-marginal setting,
enabling the alignment of high-dimensional data measured at non-equidistant
time points without reducing dimensionality. The use of measure-valued splines
enhances robustness to irregular snapshot timing, and score matching prevents
overfitting in high-dimensional spaces. We validate our framework on several
synthetic and benchmark datasets, including gene expression data collected at
uneven time points and an image progression task, demonstrating the method's
versatility.

</details>


### [91] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 提出LRTuckerRep模型用于多维数据补全，结合全局和局部先验建模，开发迭代算法，实验显示其在高缺失率下效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有多维数据补全方法存在局限性，如低秩方法计算成本高、破坏数据结构，基于平滑性的方法需大量手动调参且泛化性差。

Method: 提出LRTuckerRep模型，在Tucker分解中统一全局和局部先验建模，通过自适应加权核范数和稀疏Tucker核心编码低秩性，用无参数拉普拉斯正则化捕获平滑性，开发两个有收敛保证的迭代算法。

Result: 在多维图像修复和交通数据插补实验中，LRTuckerRep在高缺失率下比基线方法有更高的补全精度和鲁棒性。

Conclusion: LRTuckerRep模型有效解决了多维数据补全问题，在高缺失率场景有优势。

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [92] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 本文提出利用大语言模型自动化并扩展贝叶斯推理中先验分布指定过程的框架，还将其扩展到多智能体系统，为复杂贝叶斯建模工具奠定基础。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推理中先验分布指定是手动、主观且不可扩展的任务，存在瓶颈。

Method: 提出LLMPrior算子，将大语言模型与可处理的生成模型架构耦合；在多智能体系统中使用对数意见池聚合先验分布，提出Fed - LLMPrior算法。

Result: 构建了利用大语言模型自动化先验分布指定的框架及多智能体系统的聚合算法。

Conclusion: 为新型复杂贝叶斯建模工具降低了入门门槛。

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [93] [Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach](https://arxiv.org/abs/2508.04481)
*Anushka Srivastava*

Main category: cs.LG

TL;DR: 本文提出基于cGANs的深度学习情感检测方法，采用多模态框架，实验显示性能提升，凸显cGANs在人机交互系统潜力。


<details>
  <summary>Details</summary>
Motivation: 传统单模态情感检测技术有局限，需更准确的情感检测方法以提升人机交互中的情感理解。

Method: 使用条件生成对抗网络（cGANs），构建多模态框架整合文本、音频和面部表情，训练cGAN生成合成情感丰富数据。

Result: 与基线模型相比，情感识别性能有显著提升。

Conclusion: cGANs在增强人机交互系统、实现更细致情感理解方面有潜力。

Abstract: This paper presents a deep learning-based approach to emotion detection using
Conditional Generative Adversarial Networks (cGANs). Unlike traditional
unimodal techniques that rely on a single data type, we explore a multimodal
framework integrating text, audio, and facial expressions. The proposed cGAN
architecture is trained to generate synthetic emotion-rich data and improve
classification accuracy across multiple modalities. Our experimental results
demonstrate significant improvements in emotion recognition performance
compared to baseline models. This work highlights the potential of cGANs in
enhancing human-computer interaction systems by enabling more nuanced emotional
understanding.

</details>


### [94] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 本文研究在线分布鲁棒强化学习，提出高效算法，证明近最优性，实验验证其鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习存在仿真到现实的差距，现有分布鲁棒强化学习研究假设受限，缺乏在未知环境的实用性。

Method: 聚焦基于f - 散度的不确定性集，提出计算高效的算法，建立在线学习遗憾的极小极大下界。

Result: 算法有次线性遗憾保证，实验证实其在不同环境的鲁棒性和效率。

Conclusion: 所提方法具有近最优性，能有效解决在线分布鲁棒强化学习问题。

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [95] [Explainable AI Methods for Neuroimaging: Systematic Failures of Common Tools, the Need for Domain-Specific Validation, and a Proposal for Safe Application](https://arxiv.org/abs/2508.02560)
*Nys Tjade Siegel,James H. Cole,Mohamad Habes,Stefan Haufe,Kerstin Ritter,Marc-André Schulz*

Main category: cs.LG

TL;DR: 对约45000个脑部MRI进行XAI方法大规模对比，发现部分常用方法有系统失败，SmoothGrad更准确，强调XAI方法需特定领域适配验证。


<details>
  <summary>Details</summary>
Motivation: 常用XAI方法缺乏严格验证，在神经影像应用中可能导致误判，需要进行系统比较。

Method: 使用新的XAI验证框架，对约45000个脑部MRI进行大规模、系统的XAI方法比较，构建有已知信号源的预测任务。

Result: GradCAM无法定位预测特征，Layer - wise Relevance Propagation产生大量虚假解释，SmoothGrad更准确。

Conclusion: XAI方法需要特定领域的适配和验证，之前神经影像研究的解释需重新评估，为XAI在神经影像中的应用提供指导。

Abstract: Trustworthy interpretation of deep learning models is critical for
neuroimaging applications, yet commonly used Explainable AI (XAI) methods lack
rigorous validation, risking misinterpretation. We performed the first
large-scale, systematic comparison of XAI methods on ~45,000 structural brain
MRIs using a novel XAI validation framework. This framework establishes
verifiable ground truth by constructing prediction tasks with known signal
sources - from localized anatomical features to subject-specific clinical
lesions - without artificially altering input images. Our analysis reveals
systematic failures in two of the most widely used methods: GradCAM
consistently failed to localize predictive features, while Layer-wise Relevance
Propagation generated extensive, artifactual explanations that suggest
incompatibility with neuroimaging data characteristics. Our results indicate
that these failures stem from a domain mismatch, where methods with design
principles tailored to natural images require substantial adaptation for
neuroimaging data. In contrast, the simpler, gradient-based method SmoothGrad,
which makes fewer assumptions about data structure, proved consistently
accurate, suggesting its conceptual simplicity makes it more robust to this
domain shift. These findings highlight the need for domain-specific adaptation
and validation of XAI methods, suggest that interpretations from prior
neuroimaging studies using standard XAI methodology warrant re-evaluation, and
provide urgent guidance for practical application of XAI in neuroimaging.

</details>


### [96] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 受大脑分层处理和能源效率启发，提出用于终身网络入侵检测系统的脉冲神经网络架构，在UNSW - NB15基准测试表现良好且适合低功耗部署。


<details>
  <summary>Details</summary>
Motivation: 受大脑分层处理和能源效率启发，开发能持续学习新威胁的网络入侵检测系统。

Method: 先使用静态脉冲神经网络识别潜在入侵，再激活自适应动态脉冲神经网络分类攻击类型，动态分类器采用GWR结构可塑性和Ad - STDP学习规则。

Result: 在UNSW - NB15基准测试中展现出强大适应性，减少灾难性遗忘，整体准确率达85.3%，英特尔Lava框架模拟显示高操作稀疏性。

Conclusion: 该架构可增量学习新威胁并保留现有知识，适合在神经形态硬件上低功耗部署。

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [97] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: 本文指出GRPO的两大局限，提出GTPO优化策略，实验验证其训练稳定性和性能更优。


<details>
  <summary>Details</summary>
Motivation: GRPO存在使重要token输出概率降低、输出分布扁平化等局限，需要更稳定有效的策略。

Method: 提出GTPO，识别冲突token，跳过负更新并放大正更新，过滤熵超阈值的补全。

Result: 在GSM8K、MATH和AIME 2024基准测试的多个实验中，GTPO训练更稳定，性能更好。

Conclusion: GTPO无需KL散度正则化和参考模型，能解决GRPO局限，实现更优策略优化。

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [98] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: 提出U - PINet用于电磁散射建模，提升效率、泛化性和物理一致性，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器有可扩展性和高计算成本问题，纯数据驱动深度学习方法缺乏物理约束且需大量标签数据，适用性和泛化性受限。

Method: 提出U - PINet，利用电磁求解器分层分解策略和局部电磁耦合稀疏性，通过多尺度处理神经网络架构建模近远场相互作用，用物理启发的稀疏图表示建模复杂3D物体网格元素的自耦合和互耦合。

Result: U - PINet准确预测表面电流分布，与传统求解器结果接近，显著减少计算时间，在准确性和鲁棒性上优于传统深度学习基线，雷达截面预测任务评估证实其对下游电磁散射应用的可行性。

Conclusion: U - PINet实现端到端多尺度电磁散射建模，具有更好的效率、泛化性和物理一致性。

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [99] [Scalable Neural Network-based Blackbox Optimization](https://arxiv.org/abs/2508.03827)
*Pavankumar Koratikere,Leifur Leifsson*

Main category: cs.LG

TL;DR: 提出无需估计模型不确定性的可扩展神经网络黑盒优化方法SNBO，在多数测试问题上表现优于基线算法，减少函数评估次数和运行时间。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在高维空间和大量函数评估时存在可扩展性挑战，基于神经网络的贝叶斯优化估计模型不确定性计算复杂，为解决这些局限而提出新方法。

Method: 提出SNBO方法，用单独标准进行探索和利用添加新样本，自适应控制采样区域以确保优化效率。

Result: 在10 - 102维的一系列优化问题上评估，多数测试问题中SNBO获得的函数值优于最佳基线算法，减少40 - 60%的函数评估次数，运行时间至少降低一个数量级。

Conclusion: SNBO是一种有效的黑盒优化方法，能在高维问题中高效优化，无需复杂的模型不确定性估计。

Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox
optimization that leverages a Gaussian process (GP) model and an acquisition
function to guide future sampling. While effective in low-dimensional settings,
BO faces scalability challenges in high-dimensional spaces and with large
number of function evaluations due to the computational complexity of GP
models. In contrast, neural networks (NNs) offer better scalability and can
model complex functions, which led to the development of NN-based BO
approaches. However, these methods typically rely on estimating model
uncertainty in NN prediction -- a process that is often computationally
intensive and complex, particularly in high dimensions. To address these
limitations, a novel method, called scalable neural network-based blackbox
optimization (SNBO), is proposed that does not rely on model uncertainty
estimation. Specifically, SNBO adds new samples using separate criteria for
exploration and exploitation, while adaptively controlling the sampling region
to ensure efficient optimization. SNBO is evaluated on a range of optimization
problems spanning from 10 to 102 dimensions and compared against four
state-of-the-art baseline algorithms. Across the majority of test problems,
SNBO attains function values better than the best-performing baseline
algorithm, while requiring 40-60% fewer function evaluations and reducing the
runtime by at least an order of magnitude.

</details>


### [100] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: 本文提出基于物理信息的神经网络（PINN）加速EAST核聚变装置热通量估计，实验表明其精度与有限元法相当，计算效率提升40倍。


<details>
  <summary>Details</summary>
Motivation: 传统有限元法计算效率低，难以在实际实验中进行实时模拟，需要新方法解决EAST核聚变装置热通量估计问题。

Method: 提出PINN，输入空间坐标和时间戳，根据热传导方程计算边界损失、初始条件损失和物理损失；以数据驱动方式采样少量数据点。

Result: 在均匀和非均匀加热条件实验中，PINN精度与有限元法相当，计算效率提升40倍。

Conclusion: PINN能显著加速热传导估计过程并保持高精度，数据集和代码将公开。

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


### [101] [DP-NCB: Privacy Preserving Fair Bandits](https://arxiv.org/abs/2508.03836)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 本文提出DP - NCB算法框架，能同时保证差分隐私和达到最优阶Nash遗憾，通过模拟实验验证效果，为设计兼顾隐私和公平的多臂老虎机算法提供基础。


<details>
  <summary>Details</summary>
Motivation: 多臂老虎机算法在敏感场景应用时需同时保护用户数据隐私和确保公平性，但此前工作未能同时实现这两个目标。

Method: 提出Differentially Private Nash Confidence Bound (DP - NCB)算法框架，该框架能在全局和局部差分隐私模型下运行，且无需时间范围的先验知识。

Result: 模拟实验显示，DP - NCB比现有最先进的基线算法产生的Nash遗憾显著降低。

Conclusion: 研究结果为设计兼顾隐私和公平的多臂老虎机算法提供了理论基础，适用于高风险、有社会影响的应用场景。

Abstract: Multi-armed bandit algorithms are fundamental tools for sequential
decision-making under uncertainty, with widespread applications across domains
such as clinical trials and personalized decision-making. As bandit algorithms
are increasingly deployed in these socially sensitive settings, it becomes
critical to protect user data privacy and ensure fair treatment across decision
rounds. While prior work has independently addressed privacy and fairness in
bandit settings, the question of whether both objectives can be achieved
simultaneously has remained largely open. Existing privacy-preserving bandit
algorithms typically optimize average regret, a utilitarian measure, whereas
fairness-aware approaches focus on minimizing Nash regret, which penalizes
inequitable reward distributions, but often disregard privacy concerns.
  To bridge this gap, we introduce Differentially Private Nash Confidence Bound
(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures
$\epsilon$-differential privacy and achieves order-optimal Nash regret,
matching known lower bounds up to logarithmic factors. The framework is
sufficiently general to operate under both global and local differential
privacy models, and is anytime, requiring no prior knowledge of the time
horizon. We support our theoretical guarantees with simulations on synthetic
bandit instances, showing that DP-NCB incurs substantially lower Nash regret
than state-of-the-art baselines. Our results offer a principled foundation for
designing bandit algorithms that are both privacy-preserving and fair, making
them suitable for high-stakes, socially impactful applications.

</details>


### [102] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: 开发SICKLE框架探索用更少数据训练模型，实验表明子采样可提升模型精度并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 在摩尔定律和Dennard缩放结束的背景下，探索能否通过智能子采样用更少数据训练更好模型。

Method: 开发SICKLE框架，采用最大熵（MaxEnt）采样方法，在湍流的大型直接数值模拟（DNS）数据集上比较MaxEnt与随机和相空间采样。

Result: 在Frontier上大规模评估SICKLE，发现子采样作为预处理步骤可提高模型准确性，显著降低能耗，某些情况下能耗最多降低38倍。

Conclusion: 智能子采样在高效学习中具有应用潜力，可在减少数据量的同时提升模型性能并降低能耗。

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


### [103] [VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations](https://arxiv.org/abs/2508.03839)
*Yifei Zong,Alexandre M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出VAE - DNN模型解参数化非线性偏微分方程，可独立训练组件，比FNO和DeepONet更高效准确。


<details>
  <summary>Details</summary>
Motivation: 解决参数化非线性偏微分方程，降低训练的时间和能量成本。

Method: 采用编码器降维输入到低维空间，全连接神经网络映射到解的潜空间，解码器重构解；独立训练编码器和解码器，构建VAE - DNN模型。

Result: VAE - DNN在求解无压含水层非线性扩散方程的正逆解时，比FNO和DeepONet更高效且准确。

Conclusion: VAE - DNN模型在求解参数化非线性偏微分方程方面具有优势，能提高效率和准确性。

Abstract: We propose a trainable-by-parts surrogate model for solving forward and
inverse parameterized nonlinear partial differential equations. Like several
other surrogate and operator learning models, the proposed approach employs an
encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional
latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is
used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of
the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct
$h(\bm{x},t)$. The innovative aspect of our model is its ability to train its
three components independently. This approach leads to a substantial decrease
in both the time and energy required for training when compared to leading
operator learning models such as FNO and DeepONet. The separable training is
achieved by training the encoder as part of the variational autoencoder (VAE)
for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to
this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet
models for obtaining forward and inverse solutions to the nonlinear diffusion
equation governing groundwater flow in an unconfined aquifer. Our findings
indicate that VAE-DNN not only demonstrates greater efficiency but also
delivers superior accuracy in both forward and inverse solutions compared to
the FNO and DeepONet models.

</details>


### [104] [SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons](https://arxiv.org/abs/2508.03785)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: 提出多模态多任务模型SoilNet解决土壤层分类问题，在真实数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 基础模型进展未应用到土壤层分类，该分类因多模态、多任务及复杂标签结构仍具挑战，准确分类对监测土壤健康至关重要。

Method: 通过结构化模块化管道，整合图像数据和地理时间元数据预测深度标记，分割土壤剖面，提取形态特征，基于多模态特征向量和图基标签表示预测土壤层标签。

Result: 在真实土壤剖面数据集上证明了方法的有效性。

Conclusion: 提出的SoilNet能有效解决复杂分层分类问题。

Abstract: While recent advances in foundation models have improved the state of the art
in many domains, some problems in empirical sciences could not benefit from
this progress yet. Soil horizon classification, for instance, remains
challenging because of its multimodal and multitask characteristics and a
complex hierarchically structured label taxonomy. Accurate classification of
soil horizons is crucial for monitoring soil health, which directly impacts
agricultural productivity, food security, ecosystem stability and climate
resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal
multitask model to tackle this problem through a structured modularized
pipeline. Our approach integrates image data and geotemporal metadata to first
predict depth markers, segmenting the soil profile into horizon candidates.
Each segment is characterized by a set of horizon-specific morphological
features. Finally, horizon labels are predicted based on the multimodal
concatenated feature vector, leveraging a graph-based label representation to
account for the complex hierarchical relationships among soil horizons. Our
method is designed to address complex hierarchical classification, where the
number of possible labels is very large, imbalanced and non-trivially
structured. We demonstrate the effectiveness of our approach on a real-world
soil profile dataset. All code and experiments can be found in our repository:
https://github.com/calgo-lab/BGR/

</details>


### [105] [Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation](https://arxiv.org/abs/2508.03820)
*Igor Sokolov,Abdurakhmon Sadiev,Yury Demidovich,Fawaz S Al-Qahtani,Peter Richtárik*

Main category: cs.LG

TL;DR: 提出Bernoulli - LoRA理论框架统一并扩展现有LoRA方法，分析多种变体收敛性，实验验证理论和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法中LoRA实践有效但理论理解有限，需构建理论框架。

Method: 引入概率Bernoulli机制选择更新矩阵，分析多种变体，扩展到凸非光滑函数。

Result: 建立各变体收敛保证，给出凸非光滑函数收敛速率，实验验证方法有效。

Conclusion: 该工作是开发理论可靠且实用的PEFT方法的一步。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
adapting large foundational models to specific tasks, particularly as model
sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation
(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,
expressing adaptations as a product of two low-rank matrices. While extensive
empirical studies demonstrate LoRA's practical utility, theoretical
understanding of such methods remains limited. Recent work on RAC-LoRA
(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,
we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and
extends existing LoRA approaches. Our method introduces a probabilistic
Bernoulli mechanism for selecting which matrix to update. This approach
encompasses and generalizes various existing update strategies while
maintaining theoretical tractability. Under standard assumptions from
non-convex optimization literature, we analyze several variants of our
framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,
Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and
Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant.
Additionally, we extend our analysis to convex non-smooth functions, providing
convergence rates for both constant and adaptive (Polyak-type) stepsizes.
Through extensive experiments on various tasks, we validate our theoretical
findings and demonstrate the practical efficacy of our approach. This work is a
step toward developing theoretically grounded yet practically effective PEFT
methods.

</details>


### [106] [Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures](https://arxiv.org/abs/2508.03913)
*Florian Bley,Jacob Kauffmann,Simon León Krug,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 本文揭示基于距离的分类器中的隐藏神经网络结构，使可解释AI技术适用，经评估展示新解释方法优势并通过实例证明其有用性。


<details>
  <summary>Details</summary>
Motivation: 基于距离的分类器广泛应用，其实践中确保其预测可解释很重要，现有可解释AI强调潜在结构对解释的作用。

Method: 揭示基于距离的分类器中的隐藏神经网络结构，使层相关传播等可解释AI技术适用。

Result: 通过定量评估，新解释方法优于多个基线；通过两个实际用例展示解释基于距离模型的总体有用性。

Conclusion: 新的解释方法在解释基于距离的分类器上有优势且具有实际应用价值。

Abstract: Distance-based classifiers, such as k-nearest neighbors and support vector
machines, continue to be a workhorse of machine learning, widely used in
science and industry. In practice, to derive insights from these models, it is
also important to ensure that their predictions are explainable. While the
field of Explainable AI has supplied methods that are in principle applicable
to any model, it has also emphasized the usefulness of latent structures (e.g.
the sequence of layers in a neural network) to produce explanations. In this
paper, we contribute by uncovering a hidden neural network structure in
distance-based classifiers (consisting of linear detection units combined with
nonlinear pooling layers) upon which Explainable AI techniques such as
layer-wise relevance propagation (LRP) become applicable. Through quantitative
evaluations, we demonstrate the advantage of our novel explanation approach
over several baselines. We also show the overall usefulness of explaining
distance-based models through two practical use cases.

</details>


### [107] [FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport](https://arxiv.org/abs/2508.03940)
*Pengxi Liu,Yi Shen,Matthew M. Engelhard,Benjamin A. Goldstein,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: 提出FairPOT框架解决高风险领域公平性与AUC性能平衡问题，实验显示其优于现有方法，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 高风险领域中严格公平性会显著降低AUC性能，需解决公平性与AUC性能的平衡问题。

Method: 提出FairPOT模型无关后处理框架，用最优传输策略对齐不同组风险得分分布，可调整劣势组得分比例，还扩展到部分AUC场景。

Result: 在多个数据集实验中，FairPOT在全局和部分AUC场景均优于现有后处理技术，能在轻微降低AUC或提升效用下改善公平性。

Conclusion: FairPOT计算高效、适应性强，是有前景的实际部署解决方案。

Abstract: Fairness metrics utilizing the area under the receiver operator
characteristic curve (AUC) have gained increasing attention in high-stakes
domains such as healthcare, finance, and criminal justice. In these domains,
fairness is often evaluated over risk scores rather than binary outcomes, and a
common challenge is that enforcing strict fairness can significantly degrade
AUC performance. To address this challenge, we propose Fair Proportional
Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework
that strategically aligns risk score distributions across different groups
using optimal transport, but does so selectively by transforming a controllable
proportion, i.e., the top-lambda quantile, of scores within the disadvantaged
group. By varying lambda, our method allows for a tunable trade-off between
reducing AUC disparities and maintaining overall AUC performance. Furthermore,
we extend FairPOT to the partial AUC setting, enabling fairness interventions
to concentrate on the highest-risk regions. Extensive experiments on synthetic,
public, and clinical datasets show that FairPOT consistently outperforms
existing post-processing techniques in both global and partial AUC scenarios,
often achieving improved fairness with slight AUC degradation or even positive
gains in utility. The computational efficiency and practical adaptability of
FairPOT make it a promising solution for real-world deployment.

</details>


### [108] [Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation](https://arxiv.org/abs/2508.04444)
*Askar Tsyganov,Evgeny Frolov,Sergey Samsonov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出无矩阵设置下估计矩阵范数的随机算法，基于Hutchinson方法修改，给出复杂度界，展示在图像分类和推荐系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 在无矩阵设置下估计矩阵的二到无穷和一到二范数。

Method: 对Hutchinson的对角估计器及其Hutch++版本进行适当修改。

Result: 给出两种修改方法的复杂度界，展示算法在图像分类任务的深度神经网络训练和推荐系统中对抗攻击缓解的实用性。

Conclusion: 所提出的算法在实际应用中具有实用性。

Abstract: In this paper, we propose new randomized algorithms for estimating the
two-to-infinity and one-to-two norms in a matrix-free setting, using only
matrix-vector multiplications. Our methods are based on appropriate
modifications of Hutchinson's diagonal estimator and its Hutch++ version. We
provide oracle complexity bounds for both modifications. We further illustrate
the practical utility of our algorithms for Jacobian-based regularization in
deep neural network training on image classification tasks. We also demonstrate
that our methodology can be applied to mitigate the effect of adversarial
attacks in the domain of recommender systems.

</details>


### [109] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 本文提出时空预测框架利用众包用户侧KPI和监管数据集预测频谱需求，有更高准确性和跨区域泛化性，可助力频谱管理规划。


<details>
  <summary>Details</summary>
Motivation: 准确频谱需求预测对频谱分配、监管规划和无线通信网络发展至关重要，支持政府相关工作。

Method: 提出时空预测框架，结合先进特征工程、综合相关性分析和迁移学习技术，利用众包用户侧KPI和监管数据集。

Result: 相比传统ITU模型，框架能提供更现实、可操作的预测，实验验证了方法有效性。

Conclusion: 该框架可作为政策制定者和监管机构加强频谱管理和规划的可靠方法。

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [110] [Prediction-Oriented Subsampling from Data Streams](https://arxiv.org/abs/2508.03868)
*Benedetta Lavinia Mussati,Freddie Bickford Smith,Tom Rainforth,Stephen Roberts*

Main category: cs.LG

TL;DR: 探讨离线学习的智能数据子采样，用信息论方法，在两问题上表现优于旧法，实践中需精心设计模型。


<details>
  <summary>Details</summary>
Motivation: 解决从数据流学习模型时捕获相关信息并控制计算成本的挑战。

Method: 采用以减少下游感兴趣预测的不确定性为中心的信息论方法进行智能数据子采样。

Result: 在两个广泛研究的问题上，该面向预测的方法比之前提出的信息论技术表现更好。

Conclusion: 在实践中要可靠地取得良好性能，需要精心设计模型。

Abstract: Data is often generated in streams, with new observations arriving over time.
A key challenge for learning models from data streams is capturing relevant
information while keeping computational costs manageable. We explore
intelligent data subsampling for offline learning, and argue for an
information-theoretic method centred on reducing uncertainty in downstream
predictions of interest. Empirically, we demonstrate that this
prediction-oriented approach performs better than a previously proposed
information-theoretic technique on two widely studied problems. At the same
time, we highlight that reliably achieving strong performance in practice
requires careful model design.

</details>


### [111] [Reinforcement Learning for Target Zone Blood Glucose Control](https://arxiv.org/abs/2508.03875)
*David H. Mguni,Jing Dong,Wanrong Yang,Ziquan Liu,Muhammad Salman Haleem,Baoxiang Wang*

Main category: cs.LG

TL;DR: 本文提出针对1型糖尿病技术决策的新型强化学习框架，统一两种控制模式，有理论收敛保证，在模拟任务中降低血糖违规率。


<details>
  <summary>Details</summary>
Motivation: 管理生理变量在临床安全目标范围内是医疗挑战，强化学习在个性化治疗中有延迟和异质性问题，需要新方法用于1型糖尿病技术决策。

Method: 提出新颖强化学习框架，统一脉冲控制和切换控制两种模式，采用带生理状态特征的受限马尔可夫决策过程进行安全策略学习。

Result: 有理论收敛保证，在模拟1型糖尿病控制任务中，将血糖水平违规率从22.4%降至最低10.8%。

Conclusion: 虽不用于临床部署，但为未来医疗中安全且有时间意识的强化学习奠定基础。

Abstract: Managing physiological variables within clinically safe target zones is a
central challenge in healthcare, particularly for chronic conditions such as
Type 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for
personalising treatment, but struggles with the delayed and heterogeneous
effects of interventions. We propose a novel RL framework to study and support
decision-making in T1DM technologies, such as automated insulin delivery. Our
approach captures the complex temporal dynamics of treatment by unifying two
control modalities: \textit{impulse control} for discrete, fast-acting
interventions (e.g., insulin boluses), and \textit{switching control} for
longer-acting treatments and regime shifts. The core of our method is a
constrained Markov decision process augmented with physiological state
features, enabling safe policy learning under clinical and resource
constraints. The framework incorporates biologically realistic factors,
including insulin decay, leading to policies that better reflect real-world
therapeutic behaviour. While not intended for clinical deployment, this work
establishes a foundation for future safe and temporally-aware RL in healthcare.
We provide theoretical guarantees of convergence and demonstrate empirical
improvements in a stylised T1DM control task, reducing blood glucose level
violations from 22.4\% (state-of-the-art) to as low as 10.8\%.

</details>


### [112] [Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning](https://arxiv.org/abs/2508.03898)
*William Solow,Sandhya Saisubramanian*

Main category: cs.LG

TL;DR: 提出结合多任务学习与循环神经网络的混合建模方法，提升葡萄物候预测的鲁棒性和准确性，且在预测中表现优于传统和深度学习基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统生物物理模型缺乏葡萄园精细管理所需精度，深度学习方法受稀疏物候数据集限制，需更优预测方法。

Method: 采用结合多任务学习与循环神经网络的混合建模方法，用多任务学习预测生物物理模型参数。

Result: 使用真实和合成数据集评估表明，该方法在预测物候阶段和其他作物状态变量上显著优于传统生物物理模型和基线深度学习方法。

Conclusion: 所提混合建模方法能有效提升葡萄物候预测的鲁棒性和准确性。

Abstract: Accurate prediction of grape phenology is essential for timely vineyard
management decisions, such as scheduling irrigation and fertilization, to
maximize crop yield and quality. While traditional biophysical models
calibrated on historical field data can be used for season-long predictions,
they lack the precision required for fine-grained vineyard management. Deep
learning methods are a compelling alternative but their performance is hindered
by sparse phenology datasets, particularly at the cultivar level. We propose a
hybrid modeling approach that combines multi-task learning with a recurrent
neural network to parameterize a differentiable biophysical model. By using
multi-task learning to predict the parameters of the biophysical model, our
approach enables shared learning across cultivars while preserving biological
structure, thereby improving the robustness and accuracy of predictions.
Empirical evaluation using real-world and synthetic datasets demonstrates that
our method significantly outperforms both conventional biophysical models and
baseline deep learning approaches in predicting phenological stages, as well as
other crop state variables such as cold-hardiness and wheat yield.

</details>


### [113] [Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data](https://arxiv.org/abs/2508.03921)
*John D. Kelleher,Matthew Nicholson,Rahul Agrahari,Clare Conran*

Main category: cs.LG

TL;DR: 研究跨域时间序列数据异常检测中主动学习与迁移学习结合的有效性，发现主动学习有效，但性能提升呈线性平缓趋势。


<details>
  <summary>Details</summary>
Motivation: 探究主动学习和迁移学习结合用于跨域时间序列数据异常检测的效果。

Method: 结合主动学习和迁移学习进行实验，采用改进的实验设计，使用不同数据样本用于采样和测试池。

Result: 聚类与主动学习存在交互，单簇效果最佳；主动学习能提升模型性能，但提升速率比文献报道慢；迁移学习与主动学习结合，性能先提升后下降。

Conclusion: 主动学习有效，模型性能提升与选择和标记的点数呈线性平缓函数关系。

Abstract: This paper examines the effectiveness of combining active learning and
transfer learning for anomaly detection in cross-domain time-series data. Our
results indicate that there is an interaction between clustering and active
learning and in general the best performance is achieved using a single cluster
(in other words when clustering is not applied). Also, we find that adding new
samples to the training set using active learning does improve model
performance but that in general, the rate of improvement is slower than the
results reported in the literature suggest. We attribute this difference to an
improved experimental design where distinct data samples are used for the
sampling and testing pools. Finally, we assess the ceiling performance of
transfer learning in combination with active learning across several datasets
and find that performance does initially improve but eventually begins to tail
off as more target points are selected for inclusion in training. This tail-off
in performance may indicate that the active learning process is doing a good
job of sequencing data points for selection, pushing the less useful points
towards the end of the selection process and that this tail-off occurs when
these less useful points are eventually added. Taken together our results
indicate that active learning is effective but that the improvement in model
performance follows a linear flat function concerning the number of points
selected and labelled.

</details>


### [114] [Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning](https://arxiv.org/abs/2508.03926)
*Hector Vargas Alvarez,Dimitrios G. Patsatzis,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: cs.LG

TL;DR: 提出结合流形与机器学习方法，从高保真基于智能体的模拟中学习潜在空间中人群动力学离散演化算子，实现微观与宏观建模尺度的桥接，数值结果佳。


<details>
  <summary>Details</summary>
Motivation: 解决人群动力学中微观与宏观建模尺度桥接这一重要且未解决的挑战。

Method: 采用四阶段方法，包括用KDE从微观数据导出宏观场，基于流形学习构建映射，用机器学习技术学习降阶替代模型，最后重构高维空间人群动力学。

Result: 数值结果显示该方法具有高准确性、鲁棒性和可推广性。

Conclusion: 该方法能实现从基于智能体模拟对人群动力学的快速准确建模与模拟。

Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd
dynamics constitutes an important, open challenge for systematic numerical
analysis, optimization, and control. We propose a combined manifold and machine
learning approach to learn the discrete evolution operator for the emergent
crowd dynamics in latent spaces from high-fidelity agent-based simulations. The
proposed framework builds upon our previous works on next-generation
Equation-free algorithms on learning surrogate models for high-dimensional and
multiscale systems. Our approach is a four-stage one, explicitly conserving the
mass of the reconstructed dynamics in the high-dimensional space. In the first
step, we derive continuous macroscopic fields (densities) from discrete
microscopic data (pedestrians' positions) using KDE. In the second step, based
on manifold learning, we construct a map from the macroscopic ambient space
into the latent space parametrized by a few coordinates based on POD of the
corresponding density distribution. The third step involves learning
reduced-order surrogate ROMs in the latent space using machine learning
techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the
crowd dynamics in the high-dimensional space in terms of macroscopic density
profiles. We demonstrate that the POD reconstruction of the density
distribution via SVD conserves the mass. With this "embed->learn in latent
space->lift back to the ambient space" pipeline, we create an effective
solution operator of the unavailable macroscopic PDE for the density evolution.
For our illustrations, we use the Social Force Model to generate data in a
corridor with an obstacle, imposing periodic boundary conditions. The numerical
results demonstrate high accuracy, robustness, and generalizability, thus
allowing for fast and accurate modelling/simulation of crowd dynamics from
agent-based simulations.

</details>


### [115] [Markov Chain Estimation with In-Context Learning](https://arxiv.org/abs/2508.03934)
*Simon Lepage,Jeremie Mary,David Picard*

Main category: cs.LG

TL;DR: 研究仅通过下一个标记预测训练的变压器学习上下文算法的能力，发现模型大小和训练集大小有阈值，状态编码可提升预测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探究仅通过下一个标记预测训练时，变压器学习涉及上下文算法的能力。

Method: 设置带随机转移矩阵的马尔可夫链，训练变压器预测下一个标记，训练和测试用不同矩阵。

Result: 存在变压器大小和训练集大小的阈值，超过该阈值模型能从上下文估计转移概率而非记忆训练模式；更复杂的状态编码能为不同结构的马尔可夫链提供更鲁棒的预测。

Conclusion: 变压器在特定条件下可学习上下文算法，状态编码有助于提升预测性能。

Abstract: We investigate the capacity of transformers to learn algorithms involving
their context while solely being trained using next token prediction. We set up
Markov chains with random transition matrices and we train transformers to
predict the next token. Matrices used during training and test are different
and we show that there is a threshold in transformer size and in training set
size above which the model is able to learn to estimate the transition
probabilities from its context instead of memorizing the training patterns.
Additionally, we show that more involved encoding of the states enables more
robust prediction for Markov chains with structures different than those seen
during training.

</details>


### [116] [BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics](https://arxiv.org/abs/2508.03965)
*Yunhao Zhang,Lin Cheng,Aswin Gnanaskandan,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: 本文介绍BubbleONet模型，它基于PI - DeepONet框架，集成Rowdy激活函数，评估多场景性能及训练技术，是模拟气泡动力学的有前景代理模型。


<details>
  <summary>Details</summary>
Motivation: 寻找一种计算高效的替代传统数值求解器的方法来模拟气泡动力学。

Method: 构建基于PI - DeepONet框架的BubbleONet模型，集成Rowdy自适应激活函数，评估多种场景下模型性能，研究单步和两步训练技术。

Result: BubbleONet在多种场景下表现良好，可作为模拟气泡动力学的代理模型。

Conclusion: BubbleONet是模拟气泡动力学的有前景的代理模型，能提供计算高效的替代方案。

Abstract: This paper introduces BubbleONet, an operator learning model designed to map
pressure profiles from an input function space to corresponding bubble radius
responses. BubbleONet is built upon the physics-informed deep operator network
(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation
capabilities for operator learning alongside the robust physical fidelity
provided by the physics-informed neural networks. To mitigate the inherent
spectral bias in deep learning, BubbleONet integrates the Rowdy adaptive
activation function, enabling improved representation of high-frequency
features. The model is evaluated across various scenarios, including: (1)
Rayleigh-Plesset equation based bubble dynamics with a single initial radius,
(2) Keller-Miksis equation based bubble dynamics with a single initial radius,
and (3) Keller-Miksis equation based bubble dynamics with multiple initial
radii. Moreover, the performance of single-step versus two-step training
techniques for BubbleONet is investigated. The results demonstrate that
BubbleONet serves as a promising surrogate model for simulating bubble
dynamics, offering a computationally efficient alternative to traditional
numerical solvers.

</details>


### [117] [Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework](https://arxiv.org/abs/2508.03989)
*Ajesh Koyatan Chathoth,Shuhao Yu,Stephen Lee*

Main category: cs.LG

TL;DR: 提出动态、用户可控的少样本隐私保护传感框架PrivCLIP，在多数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现代传感系统中用户可控隐私很重要，现有隐私保护方法适应性和用户自主性有限。

Method: 引入PrivCLIP框架，用户可分类活动，利用多模态对比学习检测敏感活动，用语言引导的活动清理器和运动生成模块转换数据。

Result: 在多个人类活动识别数据集上评估，PrivCLIP在隐私保护和数据效用方面显著优于基线方法。

Conclusion: PrivCLIP能有效解决现有隐私保护方法的局限性，实现更好的隐私保护和数据效用。

Abstract: User-controllable privacy is important in modern sensing systems, as privacy
preferences can vary significantly from person to person and may evolve over
time. This is especially relevant in devices equipped with Inertial Measurement
Unit (IMU) sensors, such as smartphones and wearables, which continuously
collect rich time-series data that can inadvertently expose sensitive user
behaviors. While prior work has proposed privacy-preserving methods for sensor
data, most rely on static, predefined privacy labels or require large
quantities of private training data, limiting their adaptability and user
agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,
few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify
and modify their privacy preferences by categorizing activities as sensitive
(black-listed), non-sensitive (white-listed), or neutral (gray-listed).
Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU
sensor data with natural language activity descriptions in a shared embedding
space, enabling few-shot detection of sensitive activities. When a
privacy-sensitive activity is identified, the system uses a language-guided
activity sanitizer and a motion generation module (IMU-GPT) to transform the
original data into a privacy-compliant version that semantically resembles a
non-sensitive activity. We evaluate PrivCLIP on multiple human activity
recognition datasets and demonstrate that it significantly outperforms baseline
methods in terms of both privacy protection and data utility.

</details>


### [118] [Tensorized Clustered LoRA Merging for Multi-Task Interference](https://arxiv.org/abs/2508.03999)
*Zhan Su,Fengran Mo,Guojun Liang,Jinghan Zhang,Bingbing Wen,Prayag Tiwari,Jian-Yun Nie*

Main category: cs.LG

TL;DR: 提出TC - LoRA库解决多任务下LoRA适配器合并的任务干扰问题，实验表明其在LLM适配中有效。


<details>
  <summary>Details</summary>
Motivation: 多任务场景下，合并不同源训练的LoRA适配器会导致任务干扰，降低下游性能。

Method: 在文本层面，对训练样本聚类并为每个簇训练专门的LoRA适配器；在参数层面，引入联合典范多线性（CP）分解以分离任务特定和共享因素。

Result: 在域外零样本和技能组合任务实验中，相比基于SVD的强基线，TC - LoRA在Phi - 3上准确率提升1.4%，在Mistral - 7B上提升2.3%。

Conclusion: TC - LoRA在大语言模型适配中有效。

Abstract: Despite the success of the monolithic dense paradigm of large language models
(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small
task-specific modules and merging them with the base model. However, in
multi-task settings, merging LoRA adapters trained on heterogeneous sources
frequently causes \textit{task interference}, degrading downstream performance.
To address this, we propose a tensorized clustered LoRA (TC-LoRA) library
targeting to address the task interference at the \textit{text-level} and
\textit{parameter-level}. At the \textit{text-level}, we cluster the training
samples in the embedding space to capture input-format similarities, then train
a specialized LoRA adapter for each cluster. At the \textit{parameter-level},
we introduce a joint Canonical Polyadic (CP) decomposition that disentangles
task-specific and shared factors across LoRA adapters. This joint factorization
preserves essential knowledge while reducing cross-task interference. Extensive
experiments on out-of-domain zero-shot and skill-composition tasks-including
reasoning, question answering, and coding. Compared to strong SVD-based
baselines, TC-LoRA achieves +1.4\% accuracy on Phi-3 and +2.3\% on Mistral-7B
(+2.3\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.

</details>


### [119] [Decoupled Contrastive Learning for Federated Learning](https://arxiv.org/abs/2508.04005)
*Hyungbin Kim,Incheol Baek,Yon Dohn Chung*

Main category: cs.LG

TL;DR: 本文指出联邦学习中对比学习在有限样本下的问题，提出DCFL框架，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习因数据异质性性能下降，现有对比学习在有限样本下渐近假设不成立。

Method: 引入DCFL框架，将对比损失解耦为两个目标，独立校准吸引力和排斥力。

Result: DCFL在正负样本上表现更好，在标准基准测试中优于现有联邦学习方法。

Conclusion: DCFL是适用于联邦学习环境的有效对比学习方法。

Abstract: Federated learning is a distributed machine learning paradigm that allows
multiple participants to train a shared model by exchanging model updates
instead of their raw data. However, its performance is degraded compared to
centralized approaches due to data heterogeneity across clients. While
contrastive learning has emerged as a promising approach to mitigate this, our
theoretical analysis reveals a fundamental conflict: its asymptotic assumptions
of an infinite number of negative samples are violated in finite-sample regime
of federated learning. To address this issue, we introduce Decoupled
Contrastive Learning for Federated Learning (DCFL), a novel framework that
decouples the existing contrastive loss into two objectives. Decoupling the
loss into its alignment and uniformity components enables the independent
calibration of the attraction and repulsion forces without relying on the
asymptotic assumptions. This strategy provides a contrastive learning method
suitable for federated learning environments where each client has a small
amount of data. Our experimental results show that DCFL achieves stronger
alignment between positive samples and greater uniformity between negative
samples compared to existing contrastive learning methods. Furthermore,
experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and
Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art
federated learning methods.

</details>


### [120] [A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs](https://arxiv.org/abs/2508.04035)
*Zakariya Ba Alawi*

Main category: cs.LG

TL;DR: 本文对TensorFlow和PyTorch进行全面比较，涵盖可用性、性能、部署等方面，指出两者各有优劣，为从业者选择提供参考。


<details>
  <summary>Details</summary>
Motivation: 比较两大主流深度学习框架TensorFlow和PyTorch，帮助从业者选择合适工具。

Method: 对比两者编程范式、开发体验，参考基准测试和研究对比训练与推理性能，深入研究部署灵活性，调查生态系统和社区支持，讨论应用场景。

Result: 发现两者在深度学习中都能力出众，但各有特点，PyTorch在研究中更简单灵活，TensorFlow有更完善的生产生态。

Conclusion: 从业者选择框架时需理解两者权衡，同时指出深度学习框架设计的未来方向和挑战。

Abstract: This paper presents a comprehensive comparative survey of TensorFlow and
PyTorch, the two leading deep learning frameworks, focusing on their usability,
performance, and deployment trade-offs. We review each framework's programming
paradigm and developer experience, contrasting TensorFlow's graph-based (now
optionally eager) approach with PyTorch's dynamic, Pythonic style. We then
compare model training speeds and inference performance across multiple tasks
and data regimes, drawing on recent benchmarks and studies. Deployment
flexibility is examined in depth - from TensorFlow's mature ecosystem
(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript
support) to PyTorch's newer production tools (TorchScript compilation, ONNX
export, and TorchServe). We also survey ecosystem and community support,
including library integrations, industry adoption, and research trends (e.g.,
PyTorch's dominance in recent research publications versus TensorFlow's broader
tooling in enterprise). Applications in computer vision, natural language
processing, and other domains are discussed to illustrate how each framework is
used in practice. Finally, we outline future directions and open challenges in
deep learning framework design, such as unifying eager and graph execution,
improving cross-framework interoperability, and integrating compiler
optimizations (XLA, JIT) for improved speed. Our findings indicate that while
both frameworks are highly capable for state-of-the-art deep learning, they
exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored
in research, whereas TensorFlow provides a fuller production-ready ecosystem -
understanding these trade-offs is key for practitioners selecting the
appropriate tool. We include charts, code snippets, and more than 20 references
to academic papers and official documentation to support this comparative
analysis

</details>


### [121] [FeDaL: Federated Dataset Learning for Time Series Foundation Models](https://arxiv.org/abs/2508.04045)
*Shengchao Chen,Guodong Long,Jing Jiang*

Main category: cs.LG

TL;DR: 本文用联邦学习范式重新思考时间序列基础模型（TSFMs）发展，提出FeDaL方法处理异构时间序列，评估其跨数据集泛化能力并分析联邦扩展行为。


<details>
  <summary>Details</summary>
Motivation: 数据集异质性导致领域偏差，降低TSFMs泛化能力，且该挑战未被充分研究。

Method: 提出Federated Dataset Learning（FeDaL）方法，利用联邦学习分布式架构分解异构时间序列数据集，添加Domain Bias Elimination（DBE）和Global Bias Elimination（GBE）机制减轻局部和全局偏差。

Result: 在8个任务的真实数据集上对FeDaL的跨数据集泛化能力进行了广泛评估，对比54个基线模型；分析了联邦扩展行为对模型性能的影响。

Conclusion: FeDaL方法可有效处理异构时间序列，减轻局部和全局偏差，提升跨数据集泛化能力。

Abstract: Dataset-wise heterogeneity introduces significant domain biases that
fundamentally degrade generalization on Time Series Foundation Models (TSFMs),
yet this challenge remains underexplored. This paper rethink the development of
TSFMs using the paradigm of federated learning. We propose a novel Federated
Dataset Learning (FeDaL) approach to tackle heterogeneous time series by
learning dataset-agnostic temporal representations. Specifically, the
distributed architecture of federated learning is a nature solution to
decompose heterogeneous TS datasets into shared generalized knowledge and
preserved personalized knowledge. Moreover, based on the TSFM architecture,
FeDaL explicitly mitigates both local and global biases by adding two
complementary mechanisms: Domain Bias Elimination (DBE) and Global Bias
Elimination (GBE). FeDaL`s cross-dataset generalization has been extensively
evaluated in real-world datasets spanning eight tasks, including both
representation learning and downstream time series analysis, against 54
baselines. We further analyze federated scaling behavior, showing how data
volume, client count, and join rate affect model performance under
decentralization.

</details>


### [122] [Quantum Temporal Fusion Transformer](https://arxiv.org/abs/2508.04048)
*Krishnakanta Barik,Goutam Paul*

Main category: cs.LG

TL;DR: 提出量子增强的混合量子 - 经典架构QTFT，在预测数据集上成功训练，某些测试用例中性能优于经典TFT，且能在NISQ设备上实现。


<details>
  <summary>Details</summary>
Motivation: 扩展经典TFT框架的能力，探索量子增强在多水平时间序列预测中的应用。

Method: 提出量子时间融合变压器（QTFT）这一量子增强的混合量子 - 经典架构。

Result: QTFT在预测数据集上成功训练，能准确预测未来值，某些测试用例中训练和测试损失优于经典TFT，其余情况性能相当。

Conclusion: QTFT基于变分量子算法，可在当前NISQ设备上实现，无需对量子比特数量和电路深度有严格要求。

Abstract: The Temporal Fusion Transformer (TFT), proposed by Lim et al.
[\textit{International Journal of Forecasting}, 2021], is a state-of-the-art
attention-based deep neural network architecture specifically designed for
multi-horizon time series forecasting. It has demonstrated significant
performance improvements over existing benchmarks. In this work, we propose a
Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid
quantum-classical architecture that extends the capabilities of the classical
TFT framework. Our results demonstrate that QTFT is successfully trained on the
forecasting datasets and is capable of accurately predicting future values. In
particular, our experimental results display that in certain test cases, the
model outperforms its classical counterpart in terms of both training and test
loss, while in the remaining cases, it achieves comparable performance. A key
advantage of our approach lies in its foundation on a variational quantum
algorithm, enabling implementation on current noisy intermediate-scale quantum
(NISQ) devices without strict requirements on the number of qubits or circuit
depth.

</details>


### [123] [Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading](https://arxiv.org/abs/2508.04063)
*Joel Walsh,Siddarth Mamidanna,Benjamin Nye,Mark Core,Daniel Auerbach*

Main category: cs.LG

TL;DR: 研究评估微调方法与少样本提示在自动简答题评分中的应用，发现小数据微调对Llama模型作用有限，OpenAI闭源模型微调效果更好，且微调效果可能受领域影响，用合成数据训练Llama 3.1 8B - Instruct有显著提升。


<details>
  <summary>Details</summary>
Motivation: 对比不同方法在自动简答题评分中的效果，解决传统微调方法计算资源需求大的问题。

Method: 评估新的闭源模型微调服务和使用开放权重的QLORA微调方法，结合少样本提示，输出结构化结果。

Result: 小数据微调对Llama开放权重模型作用有限，OpenAI闭源模型微调可超越少样本基线模型，微调效果可能受领域影响，用合成数据训练Llama 3.1 8B - Instruct有显著提升。

Conclusion: 不同类型模型在自动简答题评分中，微调与少样本提示结合的效果不同，合成数据对开放权重模型训练有积极作用。

Abstract: Research to improve Automated Short Answer Grading has recently focused on
Large Language Models (LLMs) with prompt engineering and no- or few-shot
prompting to achieve best results. This is in contrast to the fine-tuning
approach, which has historically required large-scale compute clusters
inaccessible to most users. New closed-model approaches such as OpenAI's
fine-tuning service promise results with as few as 100 examples, while methods
using open weights such as quantized low-rank adaptive (QLORA) can be used to
fine-tune models on consumer GPUs. We evaluate both of these fine-tuning
methods, measuring their interaction with few-shot prompting for automated
short answer grading (ASAG) with structured (JSON) outputs. Our results show
that finetuning with small amounts of data has limited utility for Llama
open-weight models, but that fine-tuning methods can outperform few-shot
baseline instruction-tuned LLMs for OpenAI's closed models. While our
evaluation set is limited, we find some evidence that the observed benefits of
finetuning may be impacted by the domain subject matter. Lastly, we observed
dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by
seeding the initial training examples with a significant amount of cheaply
generated synthetic training data.

</details>


### [124] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: 提出FLAT用于联邦学习后门攻击，生成多样触发器，实验证明攻击效果好，凸显新防御策略紧迫性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习后门攻击方法因固定模式或单目标触发器而缺乏灵活性且易被检测。

Method: 提出FLAT，利用潜在驱动的条件自动编码器按需生成多样、特定目标的触发器，引入潜在代码使触发器视觉自适应且高度可变。

Result: 实验表明FLAT攻击成功率高，对先进联邦防御机制具有鲁棒性。

Conclusion: 迫切需要新的防御策略来应对联邦环境中潜在驱动的多目标后门威胁。

Abstract: Federated learning (FL) is vulnerable to backdoor attacks, yet most existing
methods are limited by fixed-pattern or single-target triggers, making them
inflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),
a novel backdoor attack that leverages a latent-driven conditional autoencoder
to generate diverse, target-specific triggers as needed. By introducing a
latent code, FLAT enables the creation of visually adaptive and highly variable
triggers, allowing attackers to select arbitrary targets without retraining and
to evade conventional detection mechanisms. Our approach unifies attack
success, stealth, and diversity within a single framework, introducing a new
level of flexibility and sophistication to backdoor attacks in FL. Extensive
experiments show that FLAT achieves high attack success and remains robust
against advanced FL defenses. These results highlight the urgent need for new
defense strategies to address latent-driven, multi-target backdoor threats in
federated settings.

</details>


### [125] [Adversarial Fair Multi-View Clustering](https://arxiv.org/abs/2508.04071)
*Mudi Jiang,Jiahui Zhou,Lianyu Hu,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 提出AFMVC框架将公平性学习融入表征学习，理论证明并实验验证其在公平性和聚类性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法多关注聚类性能，忽略公平性，且已有公平性方法假设常不成立。

Method: 提出AFMVC框架，用对抗训练去除敏感属性信息，通过KL散度使视图特定聚类分配与公平不变共识分布对齐。

Result: 在有公平约束的数据集上实验表明，AFMVC公平性更优且聚类性能有竞争力。

Conclusion: AFMVC框架有效解决多视图聚类中的公平性问题，且不显著影响聚类性能。

Abstract: Cluster analysis is a fundamental problem in data mining and machine
learning. In recent years, multi-view clustering has attracted increasing
attention due to its ability to integrate complementary information from
multiple views. However, existing methods primarily focus on clustering
performance, while fairness-a critical concern in human-centered
applications-has been largely overlooked. Although recent studies have explored
group fairness in multi-view clustering, most methods impose explicit
regularization on cluster assignments, relying on the alignment between
sensitive attributes and the underlying cluster structure. However, this
assumption often fails in practice and can degrade clustering performance. In
this paper, we propose an adversarial fair multi-view clustering (AFMVC)
framework that integrates fairness learning into the representation learning
process. Specifically, our method employs adversarial training to fundamentally
remove sensitive attribute information from learned features, ensuring that the
resulting cluster assignments are unaffected by it. Furthermore, we
theoretically prove that aligning view-specific clustering assignments with a
fairness-invariant consensus distribution via KL divergence preserves
clustering consistency without significantly compromising fairness, thereby
providing additional theoretical guarantees for our framework. Extensive
experiments on data sets with fairness constraints demonstrate that AFMVC
achieves superior fairness and competitive clustering performance compared to
existing multi-view clustering and fairness-aware clustering methods.

</details>


### [126] [Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?](https://arxiv.org/abs/2508.04097)
*Ngoc-Bao Nguyen,Sy-Tuyen Ho,Koh Jun Hao,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 本文首次研究视觉语言模型（VLMs）在模型反演攻击下泄露私有视觉训练数据的漏洞，提出多种反演策略，实验证明VLMs易受攻击，序列方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 以往研究聚焦传统单模态DNN，VLMs在模型反演攻击下的漏洞未被充分探究，需研究其泄露私有视觉训练数据的情况。

Method: 提出基于标记和序列的模型反演策略，包括TMI、TMI - C、SMI和SMI - AW。

Result: 实验表明VLMs易受训练数据泄露攻击，序列方法尤其是SMI - AW表现更优，人类评估重建图像攻击准确率达75.31%，还对公开VLMs进行了反演攻击。

Conclusion: VLMs在模型反演攻击下存在隐私漏洞，随着其在多领域应用增多，隐私问题需重视。

Abstract: Model inversion (MI) attacks pose significant privacy risks by reconstructing
private training data from trained neural networks. While prior works have
focused on conventional unimodal DNNs, the vulnerability of vision-language
models (VLMs) remains underexplored. In this paper, we conduct the first study
to understand VLMs' vulnerability in leaking private visual training data. To
tailored for VLMs' token-based generative nature, we propose a suite of novel
token-based and sequence-based model inversion strategies. Particularly, we
propose Token-based Model Inversion (TMI), Convergent Token-based Model
Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based
Model Inversion with Adaptive Token Weighting (SMI-AW). Through extensive
experiments and user study on three state-of-the-art VLMs and multiple
datasets, we demonstrate, for the first time, that VLMs are susceptible to
training data leakage. The experiments show that our proposed sequence-based
methods, particularly SMI-AW combined with a logit-maximization loss based on
vocabulary representation, can achieve competitive reconstruction and
outperform token-based methods in attack accuracy and visual similarity.
Importantly, human evaluation of the reconstructed images yields an attack
accuracy of 75.31\%, underscoring the severity of model inversion threats in
VLMs. Notably we also demonstrate inversion attacks on the publicly released
VLMs. Our study reveals the privacy vulnerability of VLMs as they become
increasingly popular across many applications such as healthcare and finance.

</details>


### [127] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 提出一致性感知策略优化框架解决强化学习训练效率问题，在数学推理基准测试中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在样本输出趋同时会导致梯度消失，限制训练效率和性能，需改进。

Method: 提出基于结果一致性的结构化全局奖励，引入熵基软混合机制平衡局部优势估计和全局优化。

Result: 在多个数学推理基准测试中取得显著性能提升。

Conclusion: 所提框架具有鲁棒性和广泛适用性。

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [128] [Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations](https://arxiv.org/abs/2508.04165)
*Md Shazid Islam,A S M Jahid Hasan,Md Saydur Rahman,Md Saiful Islam Sajol*

Main category: cs.LG

TL;DR: 本文提出半监督深度域适应框架应对不同气象区域天气条件导致的领域偏移，用少量目标位置标注数据实现准确太阳能发电预测，在多地提升预测准确率。


<details>
  <summary>Details</summary>
Motivation: 地理和天气特征差异导致领域偏移，缺乏标注数据和存储问题使开发位置无关预测模型困难，需要解决这些问题以准确预测太阳能发电。

Method: 提出半监督深度域适应框架，在源位置数据上训练深度卷积神经网络，用无源师生模型配置将其适应到目标位置，师生模型利用一致性和交叉熵损失进行半监督学习。

Result: 在目标域仅标注20%数据时，相比非自适应方法，在加州、佛罗里达和纽约预测准确率分别提高11.36%、6.65%、4.92%。

Conclusion: 所提半监督深度域适应框架能有效应对领域偏移问题，用少量标注数据提升不同地理位置太阳能发电预测的准确性。

Abstract: Accurate solar generation prediction is essential for proper estimation of
renewable energy resources across diverse geographic locations. However,
geographical and weather features vary from location to location which
introduces domain shift - a major bottleneck to develop location-agnostic
prediction model. As a result, a machine-learning model which can perform well
to predict solar power in one location, may exhibit subpar performance in
another location. Moreover, the lack of properly labeled data and storage
issues make the task even more challenging. In order to address domain shift
due to varying weather conditions across different meteorological regions, this
paper presents a semi-supervised deep domain adaptation framework, allowing
accurate predictions with minimal labeled data from the target location. Our
approach involves training a deep convolutional neural network on a source
location's data and adapting it to the target location using a source-free,
teacher-student model configuration. The teacher-student model leverages
consistency and cross-entropy loss for semi-supervised learning, ensuring
effective adaptation without any source data requirement for prediction. With
annotation of only $20 \%$ data in the target domain, our approach exhibits an
improvement upto $11.36 \%$, $6.65 \%$, $4.92\%$ for California, Florida and
New York as target domain, respectively in terms of accuracy in predictions
with respect to non-adaptive approach.

</details>


### [129] [One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra](https://arxiv.org/abs/2508.04180)
*Neng Kai Nigel Neo,Lim Jing,Ngoui Yong Zhau Preston,Koh Xue Ting Serene,Bingquan Shen*

Main category: cs.LG

TL;DR: 采用MIST作为编码器、MolForge作为解码器，结合预训练和阈值处理，在从头分子生成上比现有方法有十倍提升，可作为未来研究基线。


<details>
  <summary>Details</summary>
Motivation: 解决从头分子生成问题，提升从质谱生成分子结构的性能。

Method: 采用MIST作为编码器、MolForge作为解码器，对MolForge进行预训练，对指纹概率进行阈值处理。

Result: 相比之前的先进方法有十倍提升，能正确生成top - 1 28% / top - 10 36%的分子结构。

Conclusion: 该管道可作为未来从头分子解析研究的强基线。

Abstract: A common approach to the \emph{de novo} molecular generation problem from
mass spectra involves a two-stage pipeline: (1) encoding mass spectra into
molecular fingerprints, followed by (2) decoding these fingerprints into
molecular structures. In our work, we adopt
\textsc{MIST}~\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder
and \textsc{MolForge}~\citep{ucakReconstructionLosslessMolecular2023} as the
decoder, leveraging pretraining to enhance performance. Notably, pretraining
\textsc{MolForge} proves especially effective, enabling it to serve as a robust
fingerprint-to-structure decoder. Additionally, instead of passing the
probability of each bit in the fingerprint, thresholding the probabilities as a
step function helps focus the decoder on the presence of substructures,
improving recovery of accurate molecular structures even when the fingerprints
predicted by \textsc{MIST} only moderately resembles the ground truth in terms
of Tanimoto similarity. This combination of encoder and decoder results in a
tenfold improvement over previous state-of-the-art methods, generating top-1
28\% / top-10 36\% of molecular structures correctly from mass spectra. We
position this pipeline as a strong baseline for future research in \emph{de
novo} molecule elucidation from mass spectra.

</details>


### [130] [Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes](https://arxiv.org/abs/2508.04193)
*Chengcheng Yan,Jiawei Xu,Zheng Peng,Qingsong Wang*

Main category: cs.LG

TL;DR: 提出SAMT方法解决DNN训练非凸优化问题，有自适应步长策略和理论保证，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 标准方法（如SGD）在DNN非凸优化训练中存在收敛不稳定和计算成本高的问题。

Method: 提出SAMT方法，按层分块交替更新网络参数，结合元学习的自适应步长策略。

Result: 为算法提供理论收敛保证，多基准实验显示SAMT比现有方法泛化性能好且参数更新少。

Conclusion: SAMT在神经网络优化中有效且有潜力。

Abstract: The training of deep neural networks is inherently a nonconvex optimization
problem, yet standard approaches such as stochastic gradient descent (SGD)
require simultaneous updates to all parameters, often leading to unstable
convergence and high computational cost. To address these issues, we propose a
novel method, Stochastic Alternating Minimization with Trainable Step Sizes
(SAMT), which updates network parameters in an alternating manner by treating
the weights of each layer as a block. By decomposing the overall optimization
into sub-problems corresponding to different blocks, this block-wise
alternating strategy reduces per-step computational overhead and enhances
training stability in nonconvex settings. To fully leverage these benefits,
inspired by meta-learning, we proposed a novel adaptive step size strategy to
incorporate into the sub-problem solving steps of alternating updates. It
supports different types of trainable step sizes, including but not limited to
scalar, element-wise, row-wise, and column-wise, enabling adaptive step size
selection tailored to each block via meta-learning. We further provide a
theoretical convergence guarantee for the proposed algorithm, establishing its
optimization soundness. Extensive experiments for multiple benchmarks
demonstrate that SAMT achieves better generalization performance with fewer
parameter updates compared to state-of-the-art methods, highlighting its
effectiveness and potential in neural network optimization.

</details>


### [131] [Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction](https://arxiv.org/abs/2508.04216)
*Ruike Song,Zeen Song,Huijie Guo,Wenwen Qiang*

Main category: cs.LG

TL;DR: 现有外部推理系统易奖励作弊，本文提出CRA方法缓解该问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 外部推理系统在解决复杂任务时易出现奖励作弊问题，导致答案错误。

Method: 提出Causal Reward Adjustment (CRA)方法，通过在PRM内部激活上训练稀疏自动编码器恢复可解释特征，再用后门调整纠正混淆。

Result: 在数学求解数据集上的实验表明，CRA能缓解奖励作弊问题并提高最终准确率，且无需修改策略模型或重新训练PRM。

Conclusion: CRA方法能有效缓解奖励作弊问题，提升推理系统性能。

Abstract: External reasoning systems combine language models with process reward models
(PRMs) to select high-quality reasoning paths for complex tasks such as
mathematical problem solving. However, these systems are prone to reward
hacking, where high-scoring but logically incorrect paths are assigned high
scores by the PRMs, leading to incorrect answers. From a causal inference
perspective, we attribute this phenomenon primarily to the presence of
confounding semantic features. To address it, we propose Causal Reward
Adjustment (CRA), a method that mitigates reward hacking by estimating the true
reward of a reasoning path. CRA trains sparse autoencoders on the PRM's
internal activations to recover interpretable features, then corrects
confounding by using backdoor adjustment. Experiments on math solving datasets
demonstrate that CRA mitigates reward hacking and improves final accuracy,
without modifying the policy model or retraining PRM.

</details>


### [132] [Symmetric Behavior Regularization via Taylor Expansion of Symmetry](https://arxiv.org/abs/2508.04225)
*Lingwei Zhu,Zheng Chen,Han Wang,Yukie Nagai*

Main category: cs.LG

TL;DR: 本文引入对称散度到行为正则化策略优化中，提出对称$f$演员-评论家算法（S$f$-AC），实验验证其有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有方法聚焦非对称散度，研究对称散度在行为正则化策略优化中的应用。

Method: 利用$f$-散度的泰勒级数解决对称散度无法得到解析策略和数值问题，证明有限级数可得到解析策略，将对称散度分解并对条件对称项泰勒展开缓解数值问题。

Result: 提出首个实用的基于对称散度的BRPO算法S$f$-AC，分布近似和MuJoCo实验表明其表现有竞争力。

Conclusion: 基于对称散度的S$f$-AC算法在离线强化学习中具有可行性和竞争力。

Abstract: This paper introduces symmetric divergences to behavior regularization policy
optimization (BRPO) to establish a novel offline RL framework. Existing methods
focus on asymmetric divergences such as KL to obtain analytic regularized
policies and a practical minimization objective. We show that symmetric
divergences do not permit an analytic policy as regularization and can incur
numerical issues as loss. We tackle these challenges by the Taylor series of
$f$-divergence. Specifically, we prove that an analytic policy can be obtained
with a finite series. For loss, we observe that symmetric divergences can be
decomposed into an asymmetry and a conditional symmetry term, Taylor-expanding
the latter alleviates numerical issues. Summing together, we propose Symmetric
$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric
divergences. Experimental results on distribution approximation and MuJoCo
verify that S$f$-AC performs competitively.

</details>


### [133] [Empowering Time Series Forecasting with LLM-Agents](https://arxiv.org/abs/2508.04231)
*Chin-Chia Michael Yeh,Vivian Lai,Uday Singh Saini,Xiran Fan,Yujie Fan,Junpeng Wang,Xin Dai,Yan Zheng*

Main category: cs.LG

TL;DR: 本文提出以数据为中心的时间序列代理DCATS，利用元数据清理数据并优化预测性能，实验显示其能降低平均误差6%。


<details>
  <summary>Details</summary>
Motivation: 多数现有AutoML方法关注特征工程和模型架构搜索，而时间序列预测研究表明轻量级模型常达最优性能，因此探索改善数据质量作为时间序列AutoML的方向。

Method: 提出DCATS，利用时间序列的元数据清理数据并优化预测性能。

Result: 在大规模交通流量预测数据集上用四个时间序列预测模型评估DCATS，其在所有测试模型和时间范围内平均降低6%的误差。

Conclusion: 数据中心方法在时间序列预测的AutoML中有潜力。

Abstract: Large Language Model (LLM) powered agents have emerged as effective planners
for Automated Machine Learning (AutoML) systems. While most existing AutoML
approaches focus on automating feature engineering and model architecture
search, recent studies in time series forecasting suggest that lightweight
models can often achieve state-of-the-art performance. This observation led us
to explore improving data quality, rather than model architecture, as a
potentially fruitful direction for AutoML on time series data. We propose
DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata
accompanying time series to clean data while optimizing forecasting
performance. We evaluated DCATS using four time series forecasting models on a
large-scale traffic volume forecasting dataset. Results demonstrate that DCATS
achieves an average 6% error reduction across all tested models and time
horizons, highlighting the potential of data-centric approaches in AutoML for
time series forecasting.

</details>


### [134] [Automated ultrasound doppler angle estimation using deep learning](https://arxiv.org/abs/2508.04243)
*Nilesh Patil,Ajay Anand*

Main category: cs.LG

TL;DR: 本文提出基于深度学习的自动多普勒角度估计方法，用2100张超声图像训练，对比人工测量，结果显示该方法有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 角度估计错误是多普勒血流速度测量误差的主要原因，需要自动化的角度估计方法。

Method: 使用含图像增强的2100张人体颈动脉超声图像，用五个预训练模型提取特征，通过自定义浅层网络进行角度估计，并与人工测量对比。

Result: 自动与人工角度估计的平均绝对误差在3.9°到9.4°之间，最佳模型的误差低于临床可接受阈值。

Conclusion: 基于深度学习的自动超声多普勒角度估计技术有应用潜力，可在商用超声扫描仪成像软件中实现。

Abstract: Angle estimation is an important step in the Doppler ultrasound clinical
workflow to measure blood velocity. It is widely recognized that incorrect
angle estimation is a leading cause of error in Doppler-based blood velocity
measurements. In this paper, we propose a deep learning-based approach for
automated Doppler angle estimation. The approach was developed using 2100 human
carotid ultrasound images including image augmentation. Five pre-trained models
were used to extract images features, and these features were passed to a
custom shallow network for Doppler angle estimation. Independently,
measurements were obtained by a human observer reviewing the images for
comparison. The mean absolute error (MAE) between the automated and manual
angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated.
Furthermore, the MAE for the best performing model was less than the acceptable
clinical Doppler angle error threshold thus avoiding misclassification of
normal velocity values as a stenosis. The results demonstrate potential for
applying a deep-learning based technique for automated ultrasound Doppler angle
estimation. Such a technique could potentially be implemented within the
imaging software on commercial ultrasound scanners.

</details>


### [135] [T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion](https://arxiv.org/abs/2508.04251)
*Abdul Monaf Chowdhury,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.LG

TL;DR: 提出T3Time框架用于多变量时间序列预测，在基准数据集上表现优于现有模型，且在少样本学习中有强泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前多变量时间序列预测方法存在依赖刚性归纳偏置、忽略变量间交互、采用静态融合策略等问题，限制了对时间序列数据中细微、特定预测范围关系的捕捉。

Method: 提出T3Time框架，包含时间、频谱和提示分支，用频率编码分支捕捉周期结构，通过门控机制学习特征优先级；提出自适应聚合多模态对齐头的机制。

Result: 在基准数据集上，模型平均降低MSE 3.28%、MAE 2.29%；少样本学习中，5%训练数据下MSE和MAE分别降低4.13%和1.91%，10%数据下分别降低3.62%和1.98%。

Conclusion: T3Time框架有效解决了现有多变量时间序列预测方法的局限性，在性能和泛化性上表现出色。

Abstract: Multivariate time series forecasting (MTSF) seeks to model temporal dynamics
among variables to predict future trends. Transformer-based models and large
language models (LLMs) have shown promise due to their ability to capture
long-range dependencies and patterns. However, current methods often rely on
rigid inductive biases, ignore intervariable interactions, or apply static
fusion strategies that limit adaptability across forecast horizons. These
limitations create bottlenecks in capturing nuanced, horizon-specific
relationships in time-series data. To solve this problem, we propose T3Time, a
novel trimodal framework consisting of time, spectral, and prompt branches,
where the dedicated frequency encoding branch captures the periodic structures
along with a gating mechanism that learns prioritization between temporal and
spectral features based on the prediction horizon. We also proposed a mechanism
which adaptively aggregates multiple cross-modal alignment heads by dynamically
weighting the importance of each head based on the features. Extensive
experiments on benchmark datasets demonstrate that our model consistently
outperforms state-of-the-art baselines, achieving an average reduction of 3.28%
in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in
few-shot learning settings: with 5% training data, we see a reduction in MSE
and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%
on average. Code - https://github.com/monaf-chowdhury/T3Time/

</details>


### [136] [A Visual Tool for Interactive Model Explanation using Sensitivity Analysis](https://arxiv.org/abs/2508.04269)
*Manuela Schuler*

Main category: cs.LG

TL;DR: 介绍基于Python的SAInT工具，支持通过集成局部和全局敏感性分析可视化探索和理解ML模型行为，可在无编程下操作并展示其在泰坦尼克数据集分类任务上的应用。


<details>
  <summary>Details</summary>
Motivation: 为了让AI研究人员和领域专家能在无编程情况下，通过可视化方式探索和理解机器学习模型行为，支持人在环工作流程。

Method: 通过交互式图形界面，自动化模型训练和选择，使用基于方差的敏感性分析进行全局特征归因，通过LIME和SHAP进行实例解释。

Result: 在泰坦尼克数据集分类任务上展示了系统，表明敏感性信息可指导特征选择和数据细化。

Conclusion: SAInT工具能有效帮助用户探索和理解ML模型，其敏感性信息对特征选择和数据处理有指导作用。

Abstract: We present SAInT, a Python-based tool for visually exploring and
understanding the behavior of Machine Learning (ML) models through integrated
local and global sensitivity analysis. Our system supports Human-in-the-Loop
(HITL) workflows by enabling users - both AI researchers and domain experts -
to configure, train, evaluate, and explain models through an interactive
graphical interface without programming. The tool automates model training and
selection, provides global feature attribution using variance-based sensitivity
analysis, and offers per-instance explanation via LIME and SHAP. We demonstrate
the system on a classification task predicting survival on the Titanic dataset
and show how sensitivity information can guide feature selection and data
refinement.

</details>


### [137] [Mockingbird: How does LLM perform in general machine learning tasks?](https://arxiv.org/abs/2508.04279)
*Haoyu Jia,Yoshiki Obinata,Kento Kawaharazuka,Kei Okada*

Main category: cs.LG

TL;DR: 本文提出Mockingbird框架使大语言模型适应通用机器学习任务，评估显示可取得一定效果，但仅自我反思不如领域文档和专家反馈。


<details>
  <summary>Details</summary>
Motivation: 源于对大语言模型在通用机器学习任务中应用潜力的好奇。

Method: 提出Mockingbird框架，核心是让大语言模型角色扮演函数并反思错误以自我提升。

Result: 基于大语言模型的机器学习方法如Mockingbird在常见机器学习任务上能取得可接受结果。

Conclusion: 仅靠大语言模型自我反思目前无法超越领域特定文档和人类专家反馈的效果。

Abstract: Large language models (LLMs) are now being used with increasing frequency as
chat bots, tasked with the summarizing information or generating text and code
in accordance with user instructions. The rapid increase in reasoning
capabilities and inference speed of LLMs has revealed their remarkable
potential for applications extending beyond the domain of chat bots to general
machine learning tasks. This work is conducted out of the curiosity about such
potential. In this work, we propose a framework Mockingbird to adapt LLMs to
general machine learning tasks and evaluate its performance and scalability on
several general machine learning tasks. The core concept of this framework is
instructing LLMs to role-play functions and reflect on its mistakes to improve
itself. Our evaluation and analysis result shows that LLM-driven machine
learning methods, such as Mockingbird, can achieve acceptable results on common
machine learning tasks; however, solely reflecting on its own currently cannot
outperform the effect of domain-specific documents and feedback from human
experts.

</details>


### [138] [Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280)
*George Bredis,Stanislav Dereka,Viacheslav Sinii,Ruslan Rakhimov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 提出轻量级、无超参数的RL算法VL - DAC训练VLM，在廉价模拟器训练后策略泛化性好，在多个基准测试有提升。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型缺乏将原始视觉观察转化为语言条件动作序列的能力，早期强化学习方法存在泛化性差、依赖超参数调整等问题。

Method: 引入Vision - Language Decoupled Actor - Critic (VL - DAC)算法，对动作令牌应用PPO更新，仅在环境步骤级别学习价值。

Result: 在多个廉价模拟器训练的VLM策略泛化性好，在BALROG、VSI - Bench、VisualWebBench等基准测试有提升，且不降低图像理解准确率。

Conclusion: 简单的RL算法可在廉价合成世界训练VLM，并在真实图像代理、空间推理和网页导航基准测试取得可衡量的收益。

Abstract: Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.

</details>


### [139] [WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification](https://arxiv.org/abs/2508.04308)
*Thang Duc Tran,Thai Hoang Le*

Main category: cs.LG

TL;DR: 提出基于权重显著性的两阶段高效图像分类机器遗忘方法WSS - CL，实验表明相比现有方法遗忘效果好且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在精确遗忘、稳定性和跨领域适用性上存在挑战，需新方法。

Method: 提出WSS - CL方法，分遗忘阶段（在对数空间最大化KL散度）和对抗微调阶段（引入自监督对比学习）。

Result: 相比现有方法，遗忘效果大幅提升，性能损失可忽略不计。

Conclusion: 该方法在有监督和自监督场景均有可用性。

Abstract: Machine unlearning, the efficient deletion of the impact of specific data in
a trained model, remains a challenging problem. Current machine unlearning
approaches that focus primarily on data-centric or weight-based strategies
frequently encounter challenges in achieving precise unlearning, maintaining
stability, and ensuring applicability across diverse domains. In this work, we
introduce a new two-phase efficient machine unlearning method for image
classification, in terms of weight saliency, leveraging weight saliency to
focus the unlearning process on critical model parameters. Our method is called
weight saliency soft-guided contrastive learning for efficient machine
unlearning image classification (WSS-CL), which significantly narrows the
performance gap with "exact" unlearning. First, the forgetting stage maximizes
kullback-leibler divergence between output logits and aggregated pseudo-labels
for efficient forgetting in logit space. Next, the adversarial fine-tuning
stage introduces contrastive learning in a self-supervised manner. By using
scaled feature representations, it maximizes the distance between the forgotten
and retained data samples in the feature space, with the forgotten and the
paired augmented samples acting as positive pairs, while the retained samples
act as negative pairs in the contrastive loss computation. Experimental
evaluations reveal that our proposed method yields much-improved unlearning
efficacy with negligible performance loss compared to state-of-the-art
approaches, indicative of its usability in supervised and self-supervised
settings.

</details>


### [140] [Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning](https://arxiv.org/abs/2508.04329)
*Ali Taheri Ghahrizjani,Alireza Taban,Qizhou Wang,Shanshan Ye,Abdolreza Mirzaei,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 本文提出对语料中令牌分类并遗忘负令牌的方法，实验表明该遗忘机制可提升模型性能和响应多样性。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）效果依赖数据质量和数量，否则性能提升有限甚至下降，为减轻这种依赖提出新方法。

Method: 将语料中的令牌分为正、负令牌，正令牌正常训练，负令牌进行遗忘处理。

Result: 在成熟基准上实验，遗忘机制提高了整体模型性能，促进了更多样化的模型响应。

Conclusion: 令牌分类和遗忘机制有助于模型学习，能更精确指导模型学习信息，提升性能和响应多样性。

Abstract: Supervised fine-tuning (SFT) plays a critical role for pretrained large
language models (LLMs), notably enhancing their capacity to acquire
domain-specific knowledge while preserving or potentially augmenting their
general-purpose capabilities. However, the efficacy of SFT hinges on data
quality as well as data volume, otherwise it may result in limited performance
gains or even degradation relative to the associated baselines. To mitigate
such reliance, we suggest categorizing tokens within each corpus into two parts
-- positive and negative tokens -- based on whether they are useful to improve
model performance. Positive tokens can be trained in common ways, whereas
negative tokens, which may lack essential semantics or be misleading, should be
explicitly forgotten. Overall, the token categorization facilitate the model to
learn less informative message, and the forgetting process shapes a knowledge
boundary to guide the model on what information to learn more precisely. We
conduct experiments on well-established benchmarks, finding that this
forgetting mechanism not only improves overall model performance and also
facilitate more diverse model responses.

</details>


### [141] [From Split to Share: Private Inference with Distributed Feature Sharing](https://arxiv.org/abs/2508.04346)
*Zihan Liu,Jiayi Wen,Shouhong Tan,Zhirun Zheng,Cheng Huang*

Main category: cs.LG

TL;DR: 提出PrivDFS范式用于私有推理，通过分布式特征共享平衡隐私与效率，还有两个扩展，实验显示有良好效果。


<details>
  <summary>Details</summary>
Motivation: 云机器学习服务处理敏感数据有隐私问题，现有私有推理方法在隐私和效率间有权衡难题。

Method: 提出PrivDFS范式，将输入特征分区为多个共享，分发到服务器进行独立部分推理，客户端聚合输出；还有PrivDFS - AT和PrivDFS - KD两个扩展。

Result: 在CIFAR - 10和CelebA上实验，PrivDFS达到与深度拆分推理相当的隐私性，客户端计算最多减少100倍且无精度损失，扩展对多种攻击有鲁棒性。

Conclusion: PrivDFS有效平衡了隐私和效率，扩展增强了对攻击的鲁棒性。

Abstract: Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy
concerns when handling sensitive client data. Existing Private Inference (PI)
methods face a fundamental trade-off between privacy and efficiency:
cryptographic approaches offer strong protection but incur high computational
overhead, while efficient alternatives such as split inference expose
intermediate features to inversion attacks. We propose PrivDFS, a new paradigm
for private inference that replaces a single exposed representation with
distributed feature sharing. PrivDFS partitions input features on the client
into multiple balanced shares, which are distributed to non-colluding,
non-communicating servers for independent partial inference. The client
securely aggregates the servers' outputs to reconstruct the final prediction,
ensuring that no single server observes sufficient information to compromise
input privacy. To further strengthen privacy, we propose two key extensions:
PrivDFS-AT, which uses adversarial training with a diffusion-based proxy
attacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,
which leverages user-specific keys to diversify partitioning policies and
prevent query-based inversion generalization. Experiments on CIFAR-10 and
CelebA demonstrate that PrivDFS achieves privacy comparable to deep split
inference while cutting client computation by up to 100 times with no accuracy
loss, and that the extensions remain robust against both diffusion-based
in-distribution and adaptive attacks.

</details>


### [142] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: 提出首个针对多实例学习（MIL）的持续学习方法，在白血病检测中表现优于现有方法，能适应数据分布变化。


<details>
  <summary>Details</summary>
Motivation: 实验室和临床动态环境需定期更新机器学习模型，现有持续学习方法对MIL无效，而MIL常用于血液病诊断。

Method: 基于对不同包中单个实例的排练，结合实例注意力分数和与包均值、类均值向量的距离选择样本和实例存储在示例集中。

Result: 使用白血病实验室一个月的真实数据，在类增量场景下研究表明该方法显著优于现有方法。

Conclusion: 该方法是首个用于MIL的持续学习方法，能使模型适应随时间变化的数据分布。

Abstract: The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

</details>


### [143] [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)
*Hao Zhang,Aining Jia,Weifeng Bu,Yushu Cai,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: 提出FlexQ INT6量化框架，结合算法创新与系统优化，在LLaMA模型评估中保持近FP16精度，有速度提升和内存节省效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内存和计算成本高，现有量化方法有不足，INT6量化缺乏硬件支持。

Method: 采用统一6位权重量化，通过层敏感度分析自适应保留8位激活值，开发支持W6A6和W6A8矩阵乘法的GPU内核。

Result: 在LLaMA模型上保持近FP16精度，内核在LLaMA - 2 - 70B线性层比ABQ - LLM平均提速1.39倍，端到端比SmoothQuant推理加速1.33倍、节省内存1.21倍。

Conclusion: FlexQ框架有效解决大语言模型量化问题，实现精度、效率和内存使用的良好平衡。

Abstract: Large Language Models (LLMs) demonstrate exceptional performance but entail
significant memory and computational costs, restricting their practical
deployment. While existing INT4/INT8 quantization reduces these costs, they
often degrade accuracy or lack optimal efficiency. INT6 quantization offers a
superior trade-off between model accuracy and inference efficiency, but lacks
hardware support in modern GPUs, forcing emulation via higher-precision
arithmetic units that limit acceleration.
  In this paper, we propose FlexQ, a novel post-training INT6 quantization
framework combining algorithmic innovation with system-level optimizations.
FlexQ employs uniform 6-bit weight quantization across all layers, with
adaptive retention of 8-bit activations in layers identified through layer-wise
sensitivity analysis. To maximize hardware efficiency, we develop a specialized
high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8
representations via Binary Tensor Core (BTC) equivalents, effectively bypassing
the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ
maintains near-FP16 accuracy, with perplexity increases of no more than 0.05.
The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on
LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference
acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released
at https://github.com/FlyFoxPlayer/FlexQ.

</details>


### [144] [Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models](https://arxiv.org/abs/2508.04427)
*Md Raisul Kibria,Sébastien Lafond,Janan Arslan*

Main category: cs.LG

TL;DR: 本文对2020年1月至2024年初关于多模态模型可解释性的研究进行系统文献综述，发现研究集中在特定模型、解释方法有局限、评估方法不系统，并给出相关建议。


<details>
  <summary>Details</summary>
Motivation: 多模态学习发展，可解释人工智能需求增长，需对多模态模型可解释性研究进行分析。

Method: 对2020年1月至2024年初相关文献从模型架构、涉及模态、解释算法和评估方法等多维度分析。

Result: 多数研究集中在视觉 - 语言和纯语言模型，常用基于注意力的解释技术，但难以捕捉模态间交互，评估方法不系统。

Conclusion: 提出一套全面建议，促进多模态可解释人工智能研究的严格、透明和标准化实践。

Abstract: Multimodal learning has witnessed remarkable advancements in recent years,
particularly with the integration of attention-based models, leading to
significant performance gains across a variety of tasks. Parallel to this
progress, the demand for explainable artificial intelligence (XAI) has spurred
a growing body of research aimed at interpreting the complex decision-making
processes of these models. This systematic literature review analyzes research
published between January 2020 and early 2024 that focuses on the
explainability of multimodal models. Framed within the broader goals of XAI, we
examine the literature across multiple dimensions, including model
architecture, modalities involved, explanation algorithms and evaluation
methodologies. Our analysis reveals that the majority of studies are
concentrated on vision-language and language-only models, with attention-based
techniques being the most commonly employed for explanation. However, these
methods often fall short in capturing the full spectrum of interactions between
modalities, a challenge further compounded by the architectural heterogeneity
across domains. Importantly, we find that evaluation methods for XAI in
multimodal settings are largely non-systematic, lacking consistency,
robustness, and consideration for modality-specific cognitive and contextual
factors. Based on these findings, we provide a comprehensive set of
recommendations aimed at promoting rigorous, transparent, and standardized
evaluation and reporting practices in multimodal XAI research. Our goal is to
support future research in more interpretable, accountable, and responsible
mulitmodal AI systems, with explainability at their core.

</details>


### [145] [Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling](https://arxiv.org/abs/2508.04447)
*Biao Hu,Guoyin Wang*

Main category: cs.LG

TL;DR: 提出云模型特征函数自动编码器（CMCFAE），结合云模型与Wasserstein自动编码器，实验表明其在多方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖标准高斯先验和传统散度度量，重构样本存在同质化问题，需更灵活准确的模型来建模复杂数据分布。

Method: 将云模型集成到Wasserstein自动编码器框架，利用云模型特征函数正则化潜在空间，采用云模型先验，推导云模型特征函数并提出正则化器。

Result: 在MNIST、FashionMNIST、CIFAR - 10和CelebA上的大量定量和定性评估显示，CMCFAE在重构质量、潜在空间结构和样本多样性方面优于现有模型。

Conclusion: 实现了云模型理论与基于MMD的正则化的新集成，为改进基于自动编码器的生成模型提供了新视角。

Abstract: We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a
novel generative model that integrates the cloud model into the Wasserstein
Auto-Encoder (WAE) framework. By leveraging the characteristic functions of the
cloud model to regularize the latent space, our approach enables more accurate
modeling of complex data distributions. Unlike conventional methods that rely
on a standard Gaussian prior and traditional divergence measures, our method
employs a cloud model prior, providing a more flexible and realistic
representation of the latent space, thus mitigating the homogenization observed
in reconstructed samples. We derive the characteristic function of the cloud
model and propose a corresponding regularizer within the WAE framework.
Extensive quantitative and qualitative evaluations on MNIST, FashionMNIST,
CIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in
terms of reconstruction quality, latent space structuring, and sample
diversity. This work not only establishes a novel integration of cloud model
theory with MMD-based regularization but also offers a promising new
perspective for enhancing autoencoder-based generative models.

</details>


### [146] [Automatic LLM Red Teaming](https://arxiv.org/abs/2508.04451)
*Roman Belaire,Arunesh Sinha,Pradeep Varakantham*

Main category: cs.LG

TL;DR: 提出新范式训练AI攻击另一个AI进行红队测试，解决现有方法问题，达新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型自动化红队测试方法依赖脆弱提示模板或单轮攻击，无法捕捉现实对抗对话复杂性。

Method: 将红队测试形式化为马尔可夫决策过程，采用分层强化学习框架，通过细粒度、基于令牌的伤害奖励让生成式智能体学习连贯多轮攻击策略。

Result: 能发现现有基线方法遗漏的细微漏洞，达到新的最优水平。

Conclusion: 该方法将大语言模型红队测试重新定义为基于动态轨迹的过程，对稳健AI部署至关重要。

Abstract: Red teaming is critical for identifying vulnerabilities and building trust in
current LLMs. However, current automated methods for Large Language Models
(LLMs) rely on brittle prompt templates or single-turn attacks, failing to
capture the complex, interactive nature of real-world adversarial dialogues. We
propose a novel paradigm: training an AI to strategically `break' another AI.
By formalizing red teaming as a Markov Decision Process (MDP) and employing a
hierarchical Reinforcement Learning (RL) framework, we effectively address the
inherent sparse reward and long-horizon challenges. Our generative agent learns
coherent, multi-turn attack strategies through a fine-grained, token-level harm
reward, enabling it to uncover subtle vulnerabilities missed by existing
baselines. This approach sets a new state-of-the-art, fundamentally reframing
LLM red teaming as a dynamic, trajectory-based process (rather than a one-step
test) essential for robust AI deployment.

</details>


### [147] [Small transformer architectures for task switching](https://arxiv.org/abs/2508.04461)
*Claudius Gros*

Main category: cs.LG

TL;DR: 分析注意力机制在小规模任务切换应用中的表现，对比多种模型，发现特定组合模型性能佳。


<details>
  <summary>Details</summary>
Motivation: 大规模生成式AI依赖注意力机制，但在小规模应用中基于注意力的架构是否优于传统方法尚不明确，以任务切换为背景研究该问题。

Method: 以有限域算术的任务切换参考模型为基础，对比标准Transformer、LSTM、MLP、Cisformer和广泛注意力机制等模型的表现。

Result: 标准Transformer无法解决基本任务切换参考模型，Transformer、LSTM和MLP预测准确率相近且一般，Cisformer和广泛注意力机制的组合模型能达到约95%的可观性能。

Conclusion: 在任务切换场景中对比不同的注意力机制表述，有助于更好地理解和改进注意力机制的工作方式。

Abstract: The rapid progress seen in terms of large-scale generative AI is largely
based on the attention mechanism. It is conversely non-trivial to conceive
small-scale applications for which attention-based architectures outperform
traditional approaches, such as multi-layer perceptrons or recurrent networks.
We examine this problem in the context of 'task switching'. In this framework
models work on ongoing token sequences with the current task being determined
by stochastically interspersed control tokens. We show that standard
transformers cannot solve a basic task switching reference model based on
finite domain arithmetics which contains subtasks dedicated to increment /
addition / reverse copy / context (IARC). We show that transformers, long
short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons
(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our
comparative study by including an extension of the standard transformer
architecture to its non-translational invariant counterpart, the cisformer, and
an alternative attention mechanism, extensive attention. A combination of the
latter is found to be the only model able to achieve considerable performance
levels, of around 95%. Our results indicate that the workings of attention can
be understood better, and even improved, when comparing qualitatively different
formulations in task-switching settings.

</details>


### [148] [CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2508.04462)
*Enyu Zhou,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: 现有投机解码方法有局限，提出基于缓存的并行投机解码框架CARD，实现加速且无需微调模型。


<details>
  <summary>Details</summary>
Motivation: 现有投机解码方法受‘draft - then - verify’范式限制，推理性能低、草稿模型大小受限，草稿过程效率低。

Method: 提出基于缓存的并行投机解码框架CARD，采用‘query - and - correct’范式，解耦起草和验证过程。

Result: 相比普通解码实现高达4.83倍的加速，且无需微调草稿和目标模型。

Conclusion: CARD框架有效解决现有投机解码方法的问题，实现高效推理加速。

Abstract: Speculative decoding (SD), where an extra draft model first provides multiple
draft tokens and the original target model then verifies these tokens in
parallel, has shown great power for LLM inference acceleration. However,
existing SD methods must adhere to the 'draft-then-verify' paradigm, which
forces drafting and verification processes to execute sequentially during SD,
resulting in inefficient inference performance and limiting the size of the
draft model. Furthermore, once a single token in the candidate sequence is
rejected during the drafting process, all subsequent candidate tokens must be
discarded, leading to inefficient drafting. To address these challenges, we
propose a cache-based parallel speculative decoding framework employing a
'query-and-correct' paradigm. Specifically, CARD decouples drafting and
verification: the draft model generates candidate tokens to populate a shared
cache, while the target model concurrently rectifies the draft model's
generation direction. This effectively enables the target model to perform
inference at speed approaching that of the draft model. Our approach achieves
up to 4.83 speedup over vanilla decoding without requiring fine-tuning of
either the draft or target models. Our code is available at
https://github.com/hunzhizi/CARD.

</details>


### [149] [GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries](https://arxiv.org/abs/2508.04463)
*Fangzhi Fei,Jiaxin Hu,Qiaofeng Li,Zhenyu Liu*

Main category: cs.LG

TL;DR: 提出基于Transformer的神经算子方法GFocal，实现全局和局部特征同时学习与融合，实验表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略局部物理细节和全局特征间相互依赖的协同学习，难以处理多尺度问题等。

Method: 通过基于Nyström注意力的全局块和基于切片的焦点块获取特征生成物理感知令牌，经卷积门控块调制和集成，实现多尺度信息动态融合。

Result: 在六个基准测试中的五个平均有15.2%相对增益，在汽车和翼型空气动力学模拟等工业规模模拟中表现优异。

Conclusion: GFocal能准确建模和预测任意几何形状和初始条件下的物理特征，达到了先进水平。

Abstract: Transformer-based neural operators have emerged as promising surrogate
solvers for partial differential equations, by leveraging the effectiveness of
Transformers for capturing long-range dependencies and global correlations,
profoundly proven in language modeling. However, existing methodologies
overlook the coordinated learning of interdependencies between local physical
details and global features, which are essential for tackling multiscale
problems, preserving physical consistency and numerical stability in long-term
rollouts, and accurately capturing transitional dynamics. In this work, we
propose GFocal, a Transformer-based neural operator method that enforces
simultaneous global and local feature learning and fusion. Global correlations
and local features are harnessed through Nystr\"{o}m attention-based
\textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate
physics-aware tokens, subsequently modulated and integrated via
convolution-based gating blocks, enabling dynamic fusion of multiscale
information. GFocal achieves accurate modeling and prediction of physical
features given arbitrary geometries and initial conditions. Experiments show
that GFocal achieves state-of-the-art performance with an average 15.2\%
relative gain in five out of six benchmarks and also excels in industry-scale
simulations such as aerodynamics simulation of automotives and airfoils.

</details>


### [150] [FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions](https://arxiv.org/abs/2508.04470)
*Jianheng Tang,Zhirui Yang,Jingchao Wang,Kejia Fan,Jinfeng Xu,Huiping Zhuang,Anfeng Liu,Houbing Herbert Song,Leye Wang,Yunhuai Liu*

Main category: cs.LG

TL;DR: 提出异质性不变的个性化联邦学习方案FedHiP，避免基于梯度的更新，实验验证其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法因数据异质性面临收敛和性能问题，根源是依赖基于梯度的更新。

Method: 提出FedHiP方案，利用自监督预训练趋势，采用无梯度特征提取和训练，包含三个分析阶段。

Result: 在基准数据集上实验，FedHiP方案准确率至少比现有基线高5.79%-20.97%。

Conclusion: FedHiP方案具有异质性不变的理想特性，性能优越。

Abstract: Lately, Personalized Federated Learning (PFL) has emerged as a prevalent
paradigm to deliver personalized models by collaboratively training while
simultaneously adapting to each client's local applications. Existing PFL
methods typically face a significant challenge due to the ubiquitous data
heterogeneity (i.e., non-IID data) across clients, which severely hinders
convergence and degrades performance. We identify that the root issue lies in
the long-standing reliance on gradient-based updates, which are inherently
sensitive to non-IID data. To fundamentally address this issue and bridge the
research gap, in this paper, we propose a Heterogeneity-invariant Personalized
Federated learning scheme, named FedHiP, through analytical (i.e., closed-form)
solutions to avoid gradient-based updates. Specifically, we exploit the trend
of self-supervised pre-training, leveraging a foundation model as a frozen
backbone for gradient-free feature extraction. Following the feature extractor,
we further develop an analytic classifier for gradient-free training. To
support both collective generalization and individual personalization, our
FedHiP scheme incorporates three phases: analytic local training, analytic
global aggregation, and analytic local personalization. The closed-form
solutions of our FedHiP scheme enable its ideal property of heterogeneity
invariance, meaning that each personalized model remains identical regardless
of how non-IID the data are distributed across all other clients. Extensive
experiments on benchmark datasets validate the superiority of our FedHiP
scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%
in accuracy.

</details>


### [151] [Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions](https://arxiv.org/abs/2508.04478)
*Bernardino D'Amico,Francesco Pomponi,Jay H. Arehart,Lina Khaddour*

Main category: cs.LG

TL;DR: 利用因果机器学习模型研究墙体保温对天然气消耗的影响，发现不同能源负担群体效果不同，呼吁更广泛评估框架。


<details>
  <summary>Details</summary>
Motivation: 国内能源需求降低对气候缓解和燃料贫困战略很重要，但能源效率干预影响异质性高，需研究墙体保温对天然气消耗的影响。

Method: 使用基于英国住房存量全国代表性数据训练的因果机器学习模型，关注不同能源负担子群体的分布效应。

Result: 干预平均降低天然气需求达19%，低能源负担群体节省显著，高能源负担群体几乎无减少，反映行为驱动机制。

Conclusion: 应建立兼顾气候影响和国内能源政策公平性的更广泛评估框架。

Abstract: Reducing domestic energy demand is central to climate mitigation and fuel
poverty strategies, yet the impact of energy efficiency interventions is highly
heterogeneous. Using a causal machine learning model trained on nationally
representative data of the English housing stock, we estimate average and
conditional treatment effects of wall insulation on gas consumption, focusing
on distributional effects across energy burden subgroups. While interventions
reduce gas demand on average (by as much as 19 percent), low energy burden
groups achieve substantial savings, whereas those experiencing high energy
burdens see little to no reduction. This pattern reflects a
behaviourally-driven mechanism: households constrained by high costs-to-income
ratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort
rather than lowering consumption. Far from wasteful, such responses represent
rational adjustments in contexts of prior deprivation, with potential
co-benefits for health and well-being. These findings call for a broader
evaluation framework that accounts for both climate impacts and the equity
implications of domestic energy policy.

</details>


### [152] [Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation](https://arxiv.org/abs/2508.04489)
*Erin Lanus,Daniel Wolodkin,Laura J. Freeman*

Main category: cs.LG

TL;DR: 本文开发了利用得分树的分层评分指标评估机器学习模型性能，能细化捕捉错误，还可调整，且会开源代码。


<details>
  <summary>Details</summary>
Motivation: 传统分类和目标检测的通过/失败评分将所有错误分类等同看待，缺乏对错误分类影响的细粒度理解，需更好评估方法。

Method: 开发不同复杂度的分层评分指标，利用得分树编码类别标签关系，根据得分树中的距离产生指标。

Result: 这些指标能更细粒度地捕捉错误，得分树可进行调整。

Conclusion: 提出的评估方法不仅按错误数量，还按错误类型或影响对模型进行排名。

Abstract: A common use of machine learning (ML) models is predicting the class of a
sample. Object detection is an extension of classification that includes
localization of the object via a bounding box within the sample.
Classification, and by extension object detection, is typically evaluated by
counting a prediction as incorrect if the predicted label does not match the
ground truth label. This pass/fail scoring treats all misclassifications as
equivalent. In many cases, class labels can be organized into a class taxonomy
with a hierarchical structure to either reflect relationships among the data or
operator valuation of misclassifications. When such a hierarchical structure
exists, hierarchical scoring metrics can return the model performance of a
given prediction related to the distance between the prediction and the ground
truth label. Such metrics can be viewed as giving partial credit to predictions
instead of pass/fail, enabling a finer-grained understanding of the impact of
misclassifications. This work develops hierarchical scoring metrics varying in
complexity that utilize scoring trees to encode relationships between class
labels and produce metrics that reflect distance in the scoring tree. The
scoring metrics are demonstrated on an abstract use case with scoring trees
that represent three weighting strategies and evaluated by the kind of errors
discouraged. Results demonstrate that these metrics capture errors with finer
granularity and the scoring trees enable tuning. This work demonstrates an
approach to evaluating ML performance that ranks models not only by how many
errors are made but by the kind or impact of errors. Python implementations of
the scoring metrics will be available in an open-source repository at time of
publication.

</details>


### [153] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: 文章指出大语言模型和传统强化学习代理在因果推理上的不足，提出因果反思框架，让代理能推理因果，LLMs作为推理引擎，为因果反思代理奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和传统强化学习代理缺乏因果理解能力，需提升其因果推理能力。

Method: 引入因果反思框架，将因果关系建模为状态、动作、时间和扰动的动态函数；定义正式的反思机制来识别预测与观察结果的不匹配并生成因果假设；让LLMs作为结构化推理引擎。

Result: 为因果反思代理在不断变化的环境中适应、自我纠正和交流因果理解奠定了理论基础。

Conclusion: 该框架有潜力提升代理的因果推理能力，使其能在动态环境中更好地运作。

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


### [154] [PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers](https://arxiv.org/abs/2508.04503)
*Federico Zucchi,Thomas Lampert*

Main category: cs.LG

TL;DR: 提出PRISM用于多变量时间序列分类，减少模型规模与复杂度，性能优且资源高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer和CNN的模型计算量大、频率多样性有限、参数预算高。

Method: 提出基于卷积的特征提取器PRISM，在多时间尺度上对每个通道独立应用对称有限脉冲响应（FIR）滤波器。

Result: 在多个基准测试中，PRISM搭配轻量级分类头，匹配或超越领先的CNN和Transformer基线，使用的参数和FLOPs少约一个数量级。

Conclusion: PRISM结合经典信号处理与现代深度学习，为多变量时间序列分类提供准确、资源高效的解决方案。

Abstract: Multivariate time-series classification is pivotal in domains ranging from
wearable sensing to biomedical monitoring. Despite recent advances,
Transformer- and CNN-based models often remain computationally heavy, offer
limited frequency diversity, and require extensive parameter budgets. We
propose PRISM (Per-channel Resolution-Informed Symmetric Module), a
convolutional-based feature extractor that applies symmetric
finite-impulse-response (FIR) filters at multiple temporal scales,
independently per channel. This multi-resolution, per-channel design yields
highly frequency-selective embeddings without any inter-channel convolutions,
greatly reducing model size and complexity. Across human-activity, sleep-stage
and biomedical benchmarks, PRISM, paired with lightweight classification heads,
matches or outperforms leading CNN and Transformer baselines, while using
roughly an order of magnitude fewer parameters and FLOPs. By uniting classical
signal processing insights with modern deep learning, PRISM offers an accurate,
resource-efficient solution for multivariate time-series classification.

</details>


### [155] [Channel-Independent Federated Traffic Prediction](https://arxiv.org/abs/2508.04517)
*Mo Zhang,Xiaoyu Li,Bin Xu,Meng Chen,Yongshun Gong*

Main category: cs.LG

TL;DR: 提出通道独立范式（CIP）及Fed - CI框架用于联邦交通预测，减少通信开销，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦交通预测方法通信开销大、训练慢，资源消耗不可持续，且要满足隐私约束。

Method: 提出CIP范式，基于此开发Fed - CI框架，各节点仅用本地信息预测，独立处理数据。

Result: 在多个真实数据集上实验表明，Fed - CI在所有数据集和联邦设置下均优于现有方法，RMSE、MAE、MAPE分别提升8%、14%、16%，大幅降低通信成本。

Conclusion: Fed - CI能显著减少通信开销、加速训练过程，在符合隐私规定的同时达到了先进性能。

Abstract: In recent years, traffic prediction has achieved remarkable success and has
become an integral component of intelligent transportation systems. However,
traffic data is typically distributed among multiple data owners, and privacy
constraints prevent the direct utilization of these isolated datasets for
traffic prediction. Most existing federated traffic prediction methods focus on
designing communication mechanisms that allow models to leverage information
from other clients in order to improve prediction accuracy. Unfortunately, such
approaches often incur substantial communication overhead, and the resulting
transmission delays significantly slow down the training process. As the volume
of traffic data continues to grow, this issue becomes increasingly critical,
making the resource consumption of current methods unsustainable. To address
this challenge, we propose a novel variable relationship modeling paradigm for
federated traffic prediction, termed the Channel-Independent Paradigm(CIP).
Unlike traditional approaches, CIP eliminates the need for inter-client
communication by enabling each node to perform efficient and accurate
predictions using only local information. Based on the CIP, we further develop
Fed-CI, an efficient federated learning framework, allowing each client to
process its own data independently while effectively mitigating the information
loss caused by the lack of direct data sharing among clients. Fed-CI
significantly reduces communication overhead, accelerates the training process,
and achieves state-of-the-art performance while complying with privacy
regulations. Extensive experiments on multiple real-world datasets demonstrate
that Fed-CI consistently outperforms existing methods across all datasets and
federated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,
and MAPE, respectively, while also substantially reducing communication costs.

</details>


### [156] [Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape](https://arxiv.org/abs/2508.04542)
*Haoran Niu,K. Suzanne Barber*

Main category: cs.LG

TL;DR: 分析超5000个身份盗窃和欺诈案例，构建身份生态图，开发隐私风险预测框架，有效回答身份属性披露关联问题。


<details>
  <summary>Details</summary>
Motivation: 个人和组织若缺乏对隐私风险的基本理解，难以保护个人信息，需明确个人数据暴露情况和后果。

Method: 分析超5000个案例，构建身份生态图，利用图论和图神经网络开发隐私风险预测框架。

Result: 该方法有效回答了给定身份属性披露是否会导致其他属性披露的核心问题。

Conclusion: 所构建的图和开发的预测框架在解决身份属性披露关联问题上是有效的。

Abstract: It is difficult for individuals and organizations to protect personal
information without a fundamental understanding of relative privacy risks. By
analyzing over 5,000 empirical identity theft and fraud cases, this research
identifies which types of personal data are exposed, how frequently exposures
occur, and what the consequences of those exposures are. We construct an
Identity Ecosystem graph--a foundational, graph-based model in which nodes
represent personally identifiable information (PII) attributes and edges
represent empirical disclosure relationships between them (e.g., the
probability that one PII attribute is exposed due to the exposure of another).
Leveraging this graph structure, we develop a privacy risk prediction framework
that uses graph theory and graph neural networks to estimate the likelihood of
further disclosures when certain PII attributes are compromised. The results
show that our approach effectively answers the core question: Can the
disclosure of a given identity attribute possibly lead to the disclosure of
another attribute?

</details>


### [157] [GraphProp: Training the Graph Foundation Models using Graph Properties](https://arxiv.org/abs/2508.04594)
*Ziheng Sun,Qi Feng,Lehao Lin,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: 本文提出GraphProp方法训练图基础模型，在监督学习和少样本学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统图基础模型缺乏结构跨域泛化能力，有效训练图基础模型需捕捉跨域一致信息，而图结构提供的跨域信息更一致。

Method: GraphProp训练分两阶段，先通过预测图不变量训练结构GFM，再用其表示作为位置编码训练综合GFM。

Result: GraphProp在监督学习和少样本学习中显著优于竞争对手，尤其在处理无节点属性的图时表现出色。

Conclusion: GraphProp方法能够有效解决传统图基础模型的问题，提升图基础模型在图级任务中的泛化能力。

Abstract: This work focuses on training graph foundation models (GFMs) that have strong
generalization ability in graph-level tasks such as graph classification.
Effective GFM training requires capturing information consistent across
different domains. We discover that graph structures provide more consistent
cross-domain information compared to node features and graph labels. However,
traditional GFMs primarily focus on transferring node features from various
domains into a unified representation space but often lack structural
cross-domain generalization. To address this, we introduce GraphProp, which
emphasizes structural generalization. The training process of GraphProp
consists of two main phases. First, we train a structural GFM by predicting
graph invariants. Since graph invariants are properties of graphs that depend
only on the abstract structure, not on particular labellings or drawings of the
graph, this structural GFM has a strong ability to capture the abstract
structural information and provide discriminative graph representations
comparable across diverse domains. In the second phase, we use the
representations given by the structural GFM as positional encodings to train a
comprehensive GFM. This phase utilizes domain-specific node attributes and
graph labels to further improve cross-domain node feature generalization. Our
experiments demonstrate that GraphProp significantly outperforms the
competitors in supervised learning and few-shot learning, especially in
handling graphs without node attributes.

</details>


### [158] [Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding](https://arxiv.org/abs/2508.04595)
*Jan A. Zak,Christian Weißenfels*

Main category: cs.LG

TL;DR: 研究用物理信息神经网络进行铝点焊非侵入式质量评估，提出两种训练策略，二维网络在工业应用中展现质量控制潜力。


<details>
  <summary>Details</summary>
Motivation: 电阻点焊中熔核直径测量需破坏性测试，限制质量控制效率，要解决真实数据融入网络的优化目标冲突问题。

Method: 提出两种训练策略，一是用渐入函数逐步纳入实验损失、自定义学习率调度器和基于滚动窗口的早停；二是通过查找表有条件更新温度相关材料参数；先一维评估后用二维模型。

Result: 二维网络能在实验置信区间内预测动态位移和熔核生长，支持焊接阶段从钢到铝的转移。

Conclusion: 物理信息神经网络结合提出的训练策略，在工业应用中对快速、基于模型的质量控制有强大潜力。

Abstract: Resistance spot welding is the dominant joining process for the body-in-white
in the automotive industry, where the weld nugget diameter is the key quality
metric. Its measurement requires destructive testing, limiting the potential
for efficient quality control. Physics-informed neural networks were
investigated as a promising tool to reconstruct internal process states from
experimental data, enabling model-based and non-invasive quality assessment in
aluminum spot welding. A major challenge is the integration of real-world data
into the network due to competing optimization objectives. To address this, we
introduce two novel training strategies. First, experimental losses for dynamic
displacement and nugget diameter are progressively included using a fading-in
function to prevent excessive optimization conflicts. We also implement a
custom learning rate scheduler and early stopping based on a rolling window to
counteract premature reduction due to increased loss magnitudes. Second, we
introduce a conditional update of temperature-dependent material parameters via
a look-up table, activated only after a loss threshold is reached to ensure
physically meaningful temperatures. An axially symmetric two-dimensional model
was selected to represent the welding process accurately while maintaining
computational efficiency. To reduce computational burden, the training
strategies and model components were first systematically evaluated in one
dimension, enabling controlled analysis of loss design and contact models. The
two-dimensional network predicts dynamic displacement and nugget growth within
the experimental confidence interval, supports transferring welding stages from
steel to aluminum, and demonstrates strong potential for fast, model-based
quality control in industrial applications.

</details>


### [159] [Multitask Learning with Stochastic Interpolants](https://arxiv.org/abs/2508.04605)
*Hugo Negrel,Florentin Coeurdoux,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出一个学习概率分布映射的框架，泛化流和扩散模型的时间动态，通过算子插值构建多功能生成模型，实验证明其零样本有效性。


<details>
  <summary>Details</summary>
Motivation: 泛化流和扩散模型的时间动态，构建能完成多任务且无需特定训练的生成模型。

Method: 用向量、矩阵或线性算子替代标量时间变量推广随机插值，构建基于算子的插值。

Result: 通过数值实验，证明方法在条件生成、修复、微调、后验采样和多尺度建模上有零样本有效性。

Conclusion: 该方法有潜力成为通用的、与任务无关的专业模型替代方案。

Abstract: We propose a framework for learning maps between probability distributions
that broadly generalizes the time dynamics of flow and diffusion models. To
enable this, we generalize stochastic interpolants by replacing the scalar time
variable with vectors, matrices, or linear operators, allowing us to bridge
probability distributions across multiple dimensional spaces. This approach
enables the construction of versatile generative models capable of fulfilling
multiple tasks without task-specific training. Our operator-based interpolants
not only provide a unifying theoretical perspective for existing generative
models but also extend their capabilities. Through numerical experiments, we
demonstrate the zero-shot efficacy of our method on conditional generation and
inpainting, fine-tuning and posterior sampling, and multiscale modeling,
suggesting its potential as a generic task-agnostic alternative to specialized
models.

</details>


### [160] [CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series](https://arxiv.org/abs/2508.04630)
*Yutong Xia,Yingying Zhang,Yuxuan Liang,Lunting Fan,Qingsong Wen,Roger Zimmermann*

Main category: cs.LG

TL;DR: 提出因果框架CaPulse进行时间序列异常检测，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉时间序列异常生成机制，且面临标签稀缺、数据不平衡和复杂多周期性等数据挑战。

Method: 构建结构因果模型解读异常生成过程，提出带新掩码机制的周期性归一化流和精心设计的周期性学习器。

Result: 在七个真实数据集上实验，CaPulse始终优于现有方法，AUROC提升3% - 17%。

Conclusion: CaPulse能有效检测时间序列异常，且具有更强的可解释性。

Abstract: Time series anomaly detection has garnered considerable attention across
diverse domains. While existing methods often fail to capture the underlying
mechanisms behind anomaly generation in time series data. In addition, time
series anomaly detection often faces several data-related inherent challenges,
i.e., label scarcity, data imbalance, and complex multi-periodicity. In this
paper, we leverage causal tools and introduce a new causality-based framework,
CaPulse, which tunes in to the underlying causal pulse of time series data to
effectively detect anomalies. Concretely, we begin by building a structural
causal model to decipher the generation processes behind anomalies. To tackle
the challenges posed by the data, we propose Periodical Normalizing Flows with
a novel mask mechanism and carefully designed periodical learners, creating a
periodicity-aware, density-based anomaly detection approach. Extensive
experiments on seven real-world datasets demonstrate that CaPulse consistently
outperforms existing methods, achieving AUROC improvements of 3% to 17%, with
enhanced interpretability.

</details>


### [161] [A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation](https://arxiv.org/abs/2508.04645)
*Yu Song,Zhigang Hua,Harry Shomer,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: 本文针对图机器学习中链接预测任务，提出预训练方法应对现有挑战，通过多种策略提升性能，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在链接预测任务中面临稀疏连接监督有限、对初始化敏感、分布偏移下泛化能力差等挑战。

Method: 进行预训练，提出后期融合策略结合节点和边级信息；引入混合专家框架处理预训练数据多样性；开发参数高效调优策略以快速适应新数据集。

Result: 在16个跨两个领域的数据集上实验，在低资源链接预测中达到了最先进的性能，与端到端训练方法相比有竞争力，且计算开销降低超10000倍。

Conclusion: 所提方法有效，能提升链接预测性能并降低计算开销。

Abstract: Link Prediction (LP) is a critical task in graph machine learning. While
Graph Neural Networks (GNNs) have significantly advanced LP performance
recently, existing methods face key challenges including limited supervision
from sparse connectivity, sensitivity to initialization, and poor
generalization under distribution shifts. We explore pretraining as a solution
to address these challenges. Unlike node classification, LP is inherently a
pairwise task, which requires the integration of both node- and edge-level
information. In this work, we present the first systematic study on the
transferability of these distinct modules and propose a late fusion strategy to
effectively combine their outputs for improved performance. To handle the
diversity of pretraining data and avoid negative transfer, we introduce a
Mixture-of-Experts (MoE) framework that captures distinct patterns in separate
experts, facilitating seamless application of the pretrained model on diverse
downstream datasets. For fast adaptation, we develop a parameter-efficient
tuning strategy that allows the pretrained model to adapt to unseen datasets
with minimal computational overhead. Experiments on 16 datasets across two
domains demonstrate the effectiveness of our approach, achieving
state-of-the-art performance on low-resource link prediction while obtaining
competitive results compared to end-to-end trained methods, with over 10,000x
lower computational overhead.

</details>


### [162] [Perch 2.0: The Bittern Lesson for Bioacoustics](https://arxiv.org/abs/2508.04665)
*Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Lauren Harrell,Andrea Burns,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0是生物声学预训练模型，拓展训练数据，采用新训练方法，在多个基准测试中表现优异，并对细粒度物种分类作为预训练任务提出假设。


<details>
  <summary>Details</summary>
Motivation: 将Perch模型从仅对鸟类物种训练拓展到大型多分类群数据集，提升模型性能。

Method: 使用原型学习分类器进行自蒸馏训练，并采用新的源预测训练准则。

Result: 在BirdSet和BEANS基准测试中达到了最先进的性能，在海洋迁移学习任务中优于专业海洋模型。

Conclusion: 细粒度物种分类可能是生物声学特别强大的预训练任务。

Abstract: Perch is a performant pre-trained model for bioacoustics. It was trained in
supervised fashion, providing both off-the-shelf classification scores for
thousands of vocalizing species as well as strong embeddings for transfer
learning. In this new release, Perch 2.0, we expand from training exclusively
on avian species to a large multi-taxa dataset. The model is trained with
self-distillation using a prototype-learning classifier as well as a new
source-prediction training criterion. Perch 2.0 obtains state-of-the-art
performance on the BirdSet and BEANS benchmarks. It also outperforms
specialized marine models on marine transfer learning tasks, despite having
almost no marine training data. We present hypotheses as to why fine-grained
species classification is a particularly robust pre-training task for
bioacoustics.

</details>


### [163] [Robustly Learning Monotone Single-Index Models](https://arxiv.org/abs/2508.04670)
*Puqian Wang,Nikos Zarifis,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 提出首个高效算法，在高斯分布和对抗标签噪声下学习单索引模型，实现常数因子近似。


<details>
  <summary>Details</summary>
Motivation: 解决在高斯分布和对抗标签噪声下，学习单索引模型的高效算法问题，且要实现常数因子近似，覆盖更广的激活函数类。

Method: 开发一个跳出常规梯度方法的优化框架，利用问题结构、高斯空间性质和单调函数的正则性确定有用向量场来指导算法更新。

Result: 得到首个计算高效的算法，能对所有有界2+ζ阶矩的单调激活函数实现常数因子近似。

Conclusion: 新算法优于先前工作，能覆盖更大的激活函数类并实现常数因子近似。

Abstract: We consider the basic problem of learning Single-Index Models with respect to
the square loss under the Gaussian distribution in the presence of adversarial
label noise. Our main contribution is the first computationally efficient
algorithm for this learning task, achieving a constant factor approximation,
that succeeds for the class of {\em all} monotone activations with bounded
moment of order $2 + \zeta,$ for $\zeta > 0.$ This class in particular includes
all monotone Lipschitz functions and even discontinuous functions like
(possibly biased) halfspaces. Prior work for the case of unknown activation
either does not attain constant factor approximation or succeeds for a
substantially smaller family of activations. The main conceptual novelty of our
approach lies in developing an optimization framework that steps outside the
boundaries of usual gradient methods and instead identifies a useful vector
field to guide the algorithm updates by directly leveraging the problem
structure, properties of Gaussian spaces, and regularity of monotone functions.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [164] [GP and LLMs for Program Synthesis: No Clear Winners](https://arxiv.org/abs/2508.03966)
*Jose Guadalupe Hernandez,Anil Kumar Saini,Gabriel Ketron,Jason H. Moore*

Main category: cs.NE

TL;DR: 对比PushGP和GPT - 4o在PSB2基准套件任务上的程序合成能力，发现PushGP和GPT - 4o结合数据文本提示解决任务最多，不同合成器和提示方式有不同表现，无主导程序合成器。


<details>
  <summary>Details</summary>
Motivation: 对比GP（使用输入输出示例）和LLMs（使用文本描述）在程序合成中不同提供程序规范方式下的能力，即对比PushGP和GPT - 4o在程序合成方面的能力。

Method: 使用PSB2基准套件任务，对GPT - 4o采用三种提示变体（数据、文本、数据文本），改变输入输出示例数量，比较各合成器和任务组合的成功率及GPT - 4o成功合成程序的相似度。

Result: PushGP和GPT - 4o结合数据文本提示解决任务最多（25个中23个）；PushGP和GPT - 4o数据提示时随训练集大小减小解决任务减少，其他合成器无此情况；GPT - 4o文本提示和数据提示成功合成程序相似度有显著差异。

Conclusion: 没有主导的程序合成器，强调了PushGP和LLMs在程序合成中不同优化技术的重要性。

Abstract: Genetic programming (GP) and large language models (LLMs) differ in how
program specifications are provided: GP uses input-output examples, and LLMs
use text descriptions. In this work, we compared the ability of PushGP and
GPT-4o to synthesize computer programs for tasks from the PSB2 benchmark suite.
We used three prompt variants with GPT-4o: input-output examples (data-only),
textual description of the task (text-only), and a combination of both textual
descriptions and input-output examples (data-text). Additionally, we varied the
number of input-output examples available for building programs. For each
synthesizer and task combination, we compared success rates across all program
synthesizers, as well as the similarity between successful GPT-4o synthesized
programs. We found that the combination of PushGP and GPT-4o with data-text
prompting led to the greatest number of tasks solved (23 of the 25 tasks), even
though several tasks were solved exclusively by only one of the two
synthesizers. We also observed that PushGP and GPT-4o with data-only prompting
solved fewer tasks with the decrease in the training set size, while the
remaining synthesizers saw no decrease. We also detected significant
differences in similarity between the successful programs synthesized for
GPT-4o with text-only and data-only prompting. With there being no dominant
program synthesizer, this work highlights the importance of different
optimization techniques used by PushGP and LLMs to synthesize programs.

</details>


### [165] [STARE: Predicting Decision Making Based on Spatio-Temporal Eye Movements](https://arxiv.org/abs/2508.04148)
*Moshe Unger,Alexander Tuzhilin,Michel Wedel*

Main category: cs.NE

TL;DR: 本文提出名为STARE的深度学习架构，用于从原始注视或眼动时间序列预测消费者选择行为，对比多个数据集上的先进模型，向开发和测试基于眼动神经生理学的架构迈出第一步。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏从眼动时间序列预测消费者选择行为的基础模型，需要开发新架构。

Method: 提出STARE架构，采用新的标记化策略，将眼动时间序列映射到预定义感兴趣区域，结合基于T5架构的Chronos模型并添加注意力机制。

Result: 在多个数据集上与多个先进模型进行了对比。

Conclusion: 向开发和测试基于眼动神经生理学的深度学习架构迈出第一步。

Abstract: The present work proposes a Deep Learning architecture for the prediction of
various consumer choice behaviors from time series of raw gaze or eye fixations
on images of the decision environment, for which currently no foundational
models are available. The architecture, called STARE (Spatio-Temporal Attention
Representation for Eye Tracking), uses a new tokenization strategy, which
involves mapping the x- and y- pixel coordinates of eye-movement time series on
predefined, contiguous Regions of Interest. That tokenization makes the
spatio-temporal eye-movement data available to the Chronos, a time-series
foundation model based on the T5 architecture, to which co-attention and/or
cross-attention is added to capture directional and/or interocular influences
of eye movements. We compare STARE with several state-of-the art alternatives
on multiple datasets with the purpose of predicting consumer choice behaviors
from eye movements. We thus make a first step towards developing and testing DL
architectures that represent visual attention dynamics rooted in the
neurophysiology of eye movements.

</details>


### [166] [TDSNNs: Competitive Topographic Deep Spiking Neural Networks for Visual Cortex Modeling](https://arxiv.org/abs/2508.04270)
*Deming Zhou,Yuetong Fang,Zhaorui Wang,Renjing Xu*

Main category: cs.NE

TL;DR: 本文提出用于TDSNNs的STC损失函数，复制灵长类视觉皮层组织，性能优于拓扑ANN，表明TDSNN在计算性能和类脑特征间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 先前的传统深度ANN在发展拓扑表示时忽略了关键的时间动态，导致任务性能下降和生物保真度受损，需要改进。

Method: 利用SNNs，提出STC损失函数用于TDSNNs。

Result: STC有效生成跨模拟视觉皮层区域的代表性拓扑特征，TDSNN性能下降小，类脑性优于拓扑ANN，且拓扑组织有助于高效稳定的时间信息处理。

Conclusion: TDSNN在计算性能和类脑特征间取得平衡，为解释神经科学现象和设计深度学习模型提供框架和见解。

Abstract: The primate visual cortex exhibits topographic organization, where
functionally similar neurons are spatially clustered, a structure widely
believed to enhance neural processing efficiency. While prior works have
demonstrated that conventional deep ANNs can develop topographic
representations, these models largely neglect crucial temporal dynamics. This
oversight often leads to significant performance degradation in tasks like
object recognition and compromises their biological fidelity. To address this,
we leverage spiking neural networks (SNNs), which inherently capture
spike-based temporal dynamics and offer enhanced biological plausibility. We
propose a novel Spatio-Temporal Constraints (STC) loss function for topographic
deep spiking neural networks (TDSNNs), successfully replicating the
hierarchical spatial functional organization observed in the primate visual
cortex from low-level sensory input to high-level abstract representations. Our
results show that STC effectively generates representative topographic features
across simulated visual cortical areas. While introducing topography typically
leads to significant performance degradation in ANNs, our spiking architecture
exhibits a remarkably small performance drop (No drop in ImageNet top-1
accuracy, compared to a 3\% drop observed in TopoNet, which is the
best-performing topographic ANN so far) and outperforms topographic ANNs in
brain-likeness. We also reveal that topographic organization facilitates
efficient and stable temporal information processing via the spike mechanism in
TDSNNs, contributing to model robustness. These findings suggest that TDSNNs
offer a compelling balance between computational performance and brain-like
features, providing not only a framework for interpreting neural science
phenomena but also novel insights for designing more efficient and robust deep
learning models.

</details>


### [167] [Delving Deeper Into Astromorphic Transformers](https://arxiv.org/abs/2312.10925)
*Md Zesun Ahmed Mia,Malyaban Bal,Abhronil Sengupta*

Main category: cs.NE

TL;DR: 本文深入研究神经元 - 突触 - 星形胶质细胞相互作用，将其计算映射到自注意力机制，在多个机器学习任务中展现优势。


<details>
  <summary>Details</summary>
Motivation: 当前将星形胶质细胞在类脑神经形态计算中的关键作用纳入研究尚处起步阶段，本文旨在深入研究相关方面以模仿Transformer中的自注意力机制。

Method: 从跨层视角对神经元 - 星形胶质细胞网络中的Hebbian和突触前可塑性进行生物可信建模，结合非线性和反馈效应以及算法公式，将其计算映射到自注意力机制，并从机器学习应用角度评估生物现实效应的影响。

Result: 在情感和图像分类任务（IMDB和CIFAR10数据集）中提高了准确性和学习速度，在WikiText - 2数据集上的自然语言生成能力表现出色，困惑度优于传统模型。

Conclusion: Astromorphic Transformers在不同机器学习任务中具有更好的泛化性和稳定性。

Abstract: Preliminary attempts at incorporating the critical role of astrocytes - cells
that constitute more than 50\% of human brain cells - in brain-inspired
neuromorphic computing remain in infancy. This paper seeks to delve deeper into
various key aspects of neuron-synapse-astrocyte interactions to mimic
self-attention mechanisms in Transformers. The cross-layer perspective explored
in this work involves bioplausible modeling of Hebbian and presynaptic
plasticities in neuron-astrocyte networks, incorporating effects of
non-linearities and feedback along with algorithmic formulations to map the
neuron-astrocyte computations to self-attention mechanism and evaluating the
impact of incorporating bio-realistic effects from the machine learning
application side. Our analysis on sentiment and image classification tasks
(IMDB and CIFAR10 datasets) highlights the advantages of Astromorphic
Transformers, offering improved accuracy and learning speed. Furthermore, the
model demonstrates strong natural language generation capabilities on the
WikiText-2 dataset, achieving better perplexity compared to conventional
models, thus showcasing enhanced generalization and stability across diverse
machine learning tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [168] [Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices](https://arxiv.org/abs/2508.03846)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 本文介绍17条培养软件工程同理心的实用指南，探讨实施方法，给出可视化优先级框架，助团队将同理心融入日常工作。


<details>
  <summary>Details</summary>
Motivation: 同理心在软件工程中常被忽视，但作用强大，此前已识别培养同理心的策略，在此基础上制定实用指南。

Method: 制定17条同理心指南，结合从业者分享的实际应用、挑战和克服策略探讨实施方法，给出基于重要性、实施难度和采纳意愿的可视化优先级框架。

Result: 获得了将同理心融入软件工程日常工作的实用且灵活的建议。

Conclusion: 研究成果有助于团队将同理心原则转化为可持续行动。

Abstract: Empathy is a powerful yet often overlooked element in software engineering
(SE), supporting better teamwork, smoother communication, and effective
decision-making. In our previous study, we identified a range of practitioner
strategies for fostering empathy in SE contexts. Building on these insights,
this paper introduces 17 actionable empathy guidelines designed to support
practitioners, teams, and organisations. We also explore how these guidelines
can be implemented in practice by examining real-world applications,
challenges, and strategies to overcome them shared by software practitioners.
To support adoption, we present a visual prioritisation framework that
categorises the guidelines based on perceived importance, ease of
implementation, and willingness to adopt. The findings offer practical and
flexible suggestions for integrating empathy into everyday SE work, helping
teams move from principles to sustainable action.

</details>


### [169] [Evaluating Software Supply Chain Security in Research Software](https://arxiv.org/abs/2508.03856)
*Richard Hegewald,Rebecca Beyer*

Main category: cs.SE

TL;DR: 研究分析3248个科研软件仓库，发现安全状况普遍较弱，平均得分3.5/10，并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 科研软件安全对科研结果完整性和可重复性至关重要，但该领域研究尚少，且科研软件易受供应链攻击。

Method: 使用OpenSSF Scorecard分析3248个高质量、大多经过同行评审的科研软件仓库。

Result: 科研软件安全状况普遍较弱，平均得分3.5/10，重要安全实践很少实施。

Conclusion: 提出可操作、低工作量的建议，帮助研究团队提高软件安全性，减轻对科学完整性的潜在威胁。

Abstract: The security of research software is essential for ensuring the integrity and
reproducibility of scientific results. However, research software security is
still largely unexplored. Due to its dependence on open source components and
distributed development practices, research software is particularly vulnerable
to supply chain attacks. This study analyses 3,248 high-quality, largely
peer-reviewed research software repositories using the OpenSSF Scorecard. We
find a generally weak security posture with an average score of 3.5/10.
Important practices, such as signed releases and branch protection, are rarely
implemented. Finally, we present actionable, low-effort recommendations that
can help research teams improve software security and mitigate potential
threats to scientific integrity.

</details>


### [170] [From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential](https://arxiv.org/abs/2508.03881)
*Martin Obaidi,Kushtrim Qengaj,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Elisa Schmid,Kurt Schneider*

Main category: cs.SE

TL;DR: 研究能否基于应用属性预测用户解释需求，结果显示解释需求高度依赖上下文，仅靠应用元数据无法精准推断，需结合用户反馈。


<details>
  <summary>Details</summary>
Motivation: 在数字化世界，软件系统需支持用户理解交互方式和行为原因，研究能否基于应用属性预测用户解释需求，以便在开发早期考虑和进行大规模需求挖掘。

Method: 分析包含元数据的4495条应用评论的黄金标准数据集，进行相关性分析，构建线性回归模型，并在495条手动标注评论的数据集上验证。

Result: 应用属性与解释需求大多为弱关联，线性回归模型预测能力有限，安全与隐私、系统行为类别有稍高预测潜力，交互和用户界面最难预测。

Conclusion: 解释需求高度依赖上下文，开发者和需求工程师应在元数据分析基础上补充直接用户反馈，以设计可解释和以用户为中心的软件系统。

Abstract: In today's digitized world, software systems must support users in
understanding both how to interact with a system and why certain behaviors
occur. This study investigates whether explanation needs, classified from user
reviews, can be predicted based on app properties, enabling early consideration
during development and large-scale requirements mining. We analyzed a gold
standard dataset of 4,495 app reviews enriched with metadata (e.g., app
version, ratings, age restriction, in-app purchases). Correlation analyses
identified mostly weak associations between app properties and explanation
needs, with moderate correlations only for specific features such as app
version, number of reviews, and star ratings. Linear regression models showed
limited predictive power, with no reliable forecasts across configurations.
Validation on a manually labeled dataset of 495 reviews confirmed these
findings. Categories such as Security & Privacy and System Behavior showed
slightly higher predictive potential, while Interaction and User Interface
remained most difficult to predict. Overall, our results highlight that
explanation needs are highly context-dependent and cannot be precisely inferred
from app metadata alone. Developers and requirements engineers should therefore
supplement metadata analysis with direct user feedback to effectively design
explainable and user-centered software systems.

</details>


### [171] [A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output](https://arxiv.org/abs/2508.03922)
*Soroush Heydari*

Main category: cs.SE

TL;DR: 分析GitHub Copilot聊天界面与用户交互，建立以人类为中心需求框架评估其特性并讨论结果及对自动化编程人类需求分析的启示。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架常忽略影响AI助手融入软件开发流程的人类因素，需研究AI编程助手如何满足人类需求。

Method: 分析GitHub Copilot通过聊天界面与用户的交互，衡量其根据用户专业水平调整解释和代码生成的能力，评估其促进协作编程体验的有效性，建立有明确指标的以人类为中心的需求框架。

Result: 文中未明确提及具体测试结果。

Conclusion: 讨论了测试结果及其对未来自动化编程中人类需求分析的影响。

Abstract: The rapid adoption of Artificial Intelligence(AI) programming assistants such
as GitHub Copilot introduces new challenges in how these software tools address
human needs. Many existing evaluation frameworks address technical aspects such
as code correctness and efficiency, but often overlook crucial human factors
that affect the successful integration of AI assistants in software development
workflows. In this study, I analyzed GitHub Copilot's interaction with users
through its chat interface, measured Copilot's ability to adapt explanations
and code generation to user expertise levels, and assessed its effectiveness in
facilitating collaborative programming experiences. I established a
human-centered requirements framework with clear metrics to evaluate these
qualities in GitHub Copilot chat. I discussed the test results and their
implications for future analysis of human requirements in automated
programming.

</details>


### [172] [Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems](https://arxiv.org/abs/2508.03931)
*Everton Guimaraes,Nathalia Nascimento,Chandan Shivalingaiah,Asish Nelapati*

Main category: cs.SE

TL;DR: 研究对ChatGPT、Copilot、Gemini和DeepSeek四个大语言模型在150个不同难度LeetCode问题上进行基准测试，评估执行时间、内存使用和算法复杂度，揭示性能差异并给出使用建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件工程中愈发重要，有必要对其性能进行系统比较以优化在实际应用中的使用。

Method: 在150个不同难度LeetCode问题上，用Java和Python生成解决方案，基于执行时间、内存使用和算法复杂度评估模型。

Result: ChatGPT在执行时间和内存使用上效率稳定；Copilot和DeepSeek随任务复杂度增加表现有差异；Gemini在简单任务有效，难题需更多尝试。

Conclusion: 研究结果为开发者选择适合特定编码任务的大语言模型提供指导，也揭示了类GPT生成解决方案的性能和复杂性情况。

Abstract: Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are
transforming software engineering by automating key tasks, including code
generation, testing, and debugging. As these models become integral to
development workflows, a systematic comparison of their performance is
essential for optimizing their use in real world applications. This study
benchmarks these four prominent LLMs on one hundred and fifty LeetCode problems
across easy, medium, and hard difficulties, generating solutions in Java and
Python. We evaluate each model based on execution time, memory usage, and
algorithmic complexity, revealing significant performance differences. ChatGPT
demonstrates consistent efficiency in execution time and memory usage, while
Copilot and DeepSeek show variability as task complexity increases. Gemini,
although effective on simpler tasks, requires more attempts as problem
difficulty rises. Our findings provide actionable insights into each model's
strengths and limitations, offering guidance for developers selecting LLMs for
specific coding tasks and providing insights on the performance and complexity
of GPT-like generated solutions.

</details>


### [173] [Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code](https://arxiv.org/abs/2508.03949)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 本文研究代码语言模型压缩策略对对抗鲁棒性的影响，发现压缩模型在攻击下鲁棒性降低，需平衡计算效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有代码语言模型压缩策略对对抗鲁棒性的影响不明，需了解压缩模型在攻击下的表现以安全有效部署。

Method: 对三种常用代码语言模型的压缩版本，用六种评估指标和四种攻击，在三个软件分析任务中评估对抗鲁棒性。

Result: 压缩模型正常表现与未压缩相当，但攻击下鲁棒性显著降低。

Conclusion: 模型大小缩减和对抗鲁棒性存在权衡，需研究平衡计算效率和鲁棒性的压缩策略。

Abstract: Transformer-based language models for code have shown remarkable performance
in various software analytics tasks, but their adoption is hindered by high
computational costs, slow inference speeds, and substantial environmental
impact. Model compression techniques such as pruning, quantization, and
knowledge distillation have gained traction in addressing these challenges.
However, the impact of these strategies on the robustness of compressed
language models for code in adversarial scenarios remains poorly understood.
Understanding how these compressed models behave under adversarial attacks is
essential for their safe and effective deployment in real-world applications.
To bridge this knowledge gap, we conduct a comprehensive evaluation of how
common compression strategies affect the adversarial robustness of compressed
models. We assess the robustness of compressed versions of three widely used
language models for code across three software analytics tasks, using six
evaluation metrics and four commonly used classical adversarial attacks. Our
findings indicate that compressed models generally maintain comparable
performance to their uncompressed counterparts. However, when subjected to
adversarial attacks, compressed models exhibit significantly reduced
robustness. These results reveal a trade-off between model size reduction and
adversarial robustness, underscoring the need for careful consideration when
deploying compressed models in security-critical software applications. Our
study highlights the need for further research into compression strategies that
strike a balance between computational efficiency and adversarial robustness,
which is essential for deploying reliable language models for code in
real-world software applications.

</details>


### [174] [Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks](https://arxiv.org/abs/2508.04125)
*Sangwon Hyun,Hyunjun Kim,Jinhyuk Jang,Hyojin Choi,M. Ali Babar*

Main category: cs.SE

TL;DR: 本文研究大语言模型在软件工程任务代码生成中人类与大语言模型交互特征对生产力的影响，设计实验并得出相关结果和指南。


<details>
  <summary>Details</summary>
Motivation: 现有研究在大语言模型用于软件工程任务的有效提示技术研究上，问题空间有限，未涉及现实工作流中超越类级别的复杂性和影响人机交互的特征。

Method: 设计实验，提出两个项目级基准任务，让36名不同背景参与者按特定提示模式与GPT交互完成任务，分析屏幕录制和聊天记录。

Result: 发现15个人机交互特征中有3个显著影响代码生成生产力，给出5条提升人机交互生产力的主要指南，列出29种运行时和逻辑错误及缓解计划。

Conclusion: 研究了人类与大语言模型交互特征对代码生成生产力的影响，给出提升生产力的指南和错误应对方案。

Abstract: The application of Large Language Models (LLMs) is growing in the productive
completion of Software Engineering tasks. Yet, studies investigating the
productive prompting techniques often employed a limited problem space,
primarily focusing on well-known prompting patterns and mainly targeting
function-level SE practices. We identify significant gaps in real-world
workflows that involve complexities beyond class-level (e.g., multi-class
dependencies) and different features that can impact Human-LLM Interactions
(HLIs) processes in code generation. To address these issues, we designed an
experiment that comprehensively analyzed the HLI features regarding the code
generation productivity. Our study presents two project-level benchmark tasks,
extending beyond function-level evaluations. We conducted a user study with 36
participants from diverse backgrounds, asking them to solve the assigned tasks
by interacting with the GPT assistant using specific prompting patterns. We
also examined the participants' experience and their behavioral features during
interactions by analyzing screen recordings and GPT chat logs. Our statistical
and empirical investigation revealed (1) that three out of 15 HLI features
significantly impacted the productivity in code generation; (2) five primary
guidelines for enhancing productivity for HLI processes; and (3) a taxonomy of
29 runtime and logic errors that can occur during HLI processes, along with
suggested mitigation plans.

</details>


### [175] [EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation](https://arxiv.org/abs/2508.04295)
*Chaofan Wang,Tingrui Yu,Jie Wang,Dong Chen,Wenrui Zhang,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 提出EvoC2Rust框架将C项目自动转为Rust项目，结合两种方法优势，评估显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有C转Rust方法有局限，如规则法难兼顾安全和习惯用法，LLM法语义等价性差且都适用于小规模程序，有必要提出新方法。

Method: 采用骨架引导翻译策略，分三阶段：分解C项目生成Rust骨架；增量翻译函数；结合LLM和静态分析修复编译错误。

Result: 在开源基准和6个工业项目评估中，语法和语义准确率、代码安全率有提升，模块级编译和测试通过率高。

Conclusion: EvoC2Rust在项目级C转Rust翻译中表现优越。

Abstract: Rust's compile-time safety guarantees make it ideal for safety-critical
systems, creating demand for translating legacy C codebases to Rust. While
various approaches have emerged for this task, they face inherent trade-offs:
rule-based solutions face challenges in meeting code safety and idiomaticity
requirements, while LLM-based solutions often fail to generate semantically
equivalent Rust code, due to the heavy dependencies of modules across the
entire codebase. Recent studies have revealed that both solutions are limited
to small-scale programs. In this paper, we propose EvoC2Rust, an automated
framework for converting entire C projects to equivalent Rust ones. EvoC2Rust
employs a skeleton-guided translation strategy for project-level translation.
The pipeline consists of three evolutionary stages: 1) it first decomposes the
C project into functional modules, employs a feature-mapping-enhanced LLM to
transform definitions and macros and generates type-checked function stubs,
which form a compilable Rust skeleton; 2) it then incrementally translates the
function, replacing the corresponding stub placeholder; 3) finally, it repairs
compilation errors by integrating LLM and static analysis. Through evolutionary
augmentation, EvoC2Rust combines the advantages of both rule-based and
LLM-based solutions. Our evaluation on open-source benchmarks and six
industrial projects demonstrates EvoC2Rust's superior performance in
project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%
improvements in syntax and semantic accuracy over the LLM-based approaches,
along with a 96.79% higher code safety rate than the rule-based tools. At the
module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates
on industrial projects, even for complex codebases and long functions.

</details>


### [176] [Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models](https://arxiv.org/abs/2508.04352)
*Dragana Sunaric,Charlotte Verbruggen,Dominik Bork*

Main category: cs.SE

TL;DR: 提出Vanilla - Converter工具辅助从Camunda 7到Camunda 8的BPMN模型迁移，经案例验证有效。


<details>
  <summary>Details</summary>
Motivation: 因Camunda 7和Camunda 8平台存在根本差异，手动迁移复杂，需工具辅助迁移。

Method: 开发命令行工具Vanilla - Converter，自动化转换过程，支持多种BPMN元素，生成转换模型和详细日志。

Result: 通过三个使用实际工业Camunda 7模型的案例研究，证明该工具能将模型转换为有效的可执行Camunda 8模型。

Conclusion: Vanilla - Converter工具能有效促进从Camunda 7到Camunda 8的BPMN模型迁移。

Abstract: As organizations prepare for the end-of-life of Camunda 7, manual migration
remains complex due to fundamental differences between the two platforms. We
present Vanilla-Converter, a command-line tool that facilitates the migration
of BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the
transformation process, supports a wide range of BPMN elements, and produces a
transformed model and a detailed transformation log indicating automatic
changes and remaining manual conversion tasks. The tool's effectiveness is
demonstrated through three case studies with real industrially used Camunda 7
models, confirming its ability to convert these models into valid and
executable Camunda 8 models.

</details>


### [177] [Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making](https://arxiv.org/abs/2508.04408)
*Carlos Andrés Ramírez Cataño,Makoto Itoh*

Main category: cs.SE

TL;DR: 本文基于开发者编码习惯探索方法级软件缺陷自动预测，分析21个开源项目，提出基于人为错误的框架，新指标表现更优，提升预测模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有软件缺陷预测多利用代码指标，利用非软件指标的研究不足，而软件缺陷常源于人为错误，因此探索基于开发者编码习惯的预测。

Method: 提出用于预测的指标框架，对比所提指标与代码和提交历史指标的性能，分析各指标的预测重要性。

Result: 提出基于人为错误、适用于方法级缺陷预测的框架；所提指标的模型平均预测性能优于现有代码和历史指标；新指标的预测重要性分布不同且平均重要性更高；新指标提升了软件缺陷预测模型的可解释性、实用性和可操作性。

Conclusion: 通过人为错误框架提供了预测易出现缺陷软件方法的系统方法，证明开发者编码习惯对软件系统缺陷有影响，助力从业者依据预测采取行动。

Abstract: Software defect prediction using code metrics has been extensively researched
over the past five decades. However, prediction harnessing non-software metrics
is under-researched. Considering that the root cause of software defects is
often attributed to human error, human factors theory might offer key
forecasting metrics for actionable insights. This paper explores automated
software defect prediction at the method level based on the developers' coding
habits. First, we propose a framework for deciding the metrics to conduct
predictions. Next, we compare the performance of our metrics to that of the
code and commit history metrics shown by research to achieve the highest
performance to date. Finally, we analyze the prediction importance of each
metric. As a result of our analyses of twenty-one critical infrastructure
large-scale open-source software projects, we have presented: (1) a human
error-based framework with metrics useful for defect prediction at method
level; (2) models using our proposed metrics achieve better average prediction
performance than the state-of-the-art code metrics and history measures; (3)
the prediction importance of all metrics distributes differently with each of
the novel metrics having better average importance than code and history
metrics; (4) the novel metrics dramatically enhance the explainability,
practicality, and actionability of software defect prediction models,
significantly advancing the field. We present a systematic approach to
forecasting defect-prone software methods via a human error framework. This
work empowers practitioners to act on predictions, empirically demonstrating
how developer coding habits contribute to defects in software systems.

</details>


### [178] [Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection](https://arxiv.org/abs/2508.04448)
*Damian Gnieciak,Tomasz Szandala*

Main category: cs.SE

TL;DR: 对六种自动化代码检测方法进行评估，语言模型在发现漏洞上有优势但输出噪声大定位不准，建议混合使用。


<details>
  <summary>Details</summary>
Motivation: 对三种行业标准基于规则的静态代码分析工具和三种GitHub Models平台的大语言模型进行量化和定性评估。

Method: 使用十个包含63个常见类型漏洞的C#项目，测量经典检测准确率、分析延迟和审查真阳性所需的开发工作量。

Result: 语言模型平均F - 1分数更高，但存在误报率高、定位不准确问题。

Conclusion: 建议开发早期用语言模型进行广泛的上下文感知分类，用基于规则的扫描器进行高可信度验证，发布的基准和结果框架为后续研究奠定基础。

Abstract: Modern software relies on a multitude of automated testing and quality
assurance tools to prevent errors, bugs and potential vulnerabilities. This
study sets out to provide a head-to-head, quantitative and qualitative
evaluation of six automated approaches: three industry-standard rule-based
static code-analysis tools (SonarQube, CodeQL and Snyk Code) and three
state-of-the-art large language models hosted on the GitHub Models platform
(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten
real-world C# projects that embed 63 vulnerabilities across common categories
such as SQL injection, hard-coded secrets and outdated dependencies, we measure
classical detection accuracy (precision, recall, F-score), analysis latency,
and the developer effort required to vet true positives. The language-based
scanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their
static counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'
advantage originates from superior recall, confirming an ability to reason
across broader code contexts. However, this benefit comes with substantial
trade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language
models mislocate issues at line-or-column granularity due to tokenisation
artefacts. Overall, language models successfully rival traditional static
analysers in finding real vulnerabilities. Still, their noisier output and
imprecise localisation limit their standalone use in safety-critical audits. We
therefore recommend a hybrid pipeline: employ language models early in
development for broad, context-aware triage, while reserving deterministic
rule-based scanners for high-assurance verification. The open benchmark and
JSON-based result harness released with this paper lay a foundation for
reproducible, practitioner-centric research into next-generation automated code
security.

</details>


### [179] [Manifestations of Empathy in Software Engineering: How, Why, and When It Matters](https://arxiv.org/abs/2508.04479)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 通过访谈和调查探索软件工程中同理心的表现、驱动因素等，为从业者和研究者提供实践启示


<details>
  <summary>Details</summary>
Motivation: 先前研究虽强调同理心在软件工程中的重要性，但对其在实践中的表现、从业者展现同理心的动机及影响因素了解有限

Method: 对22人进行访谈，并对116名软件从业者进行大规模调查

Result: 深入了解了软件工程中同理心的表达、驱动同理心实践的因素、同理心在不同活动中的作用及其他影响因素

Conclusion: 为软件工程从业者和研究者提供了将同理心有效融入软件工程流程的实用建议

Abstract: Empathy plays a crucial role in software engineering (SE), influencing
collaboration, communication, and decision-making. While prior research has
highlighted the importance of empathy in SE, there is limited understanding of
how empathy manifests in SE practice, what motivates SE practitioners to
demonstrate empathy, and the factors that influence empathy in SE work. Our
study explores these aspects through 22 interviews and a large scale survey
with 116 software practitioners. Our findings provide insights into the
expression of empathy in SE, the drivers behind empathetic practices, SE
activities where empathy is perceived as useful or not, and the other factors
that influence empathy. In addition, we offer practical implications for SE
practitioners and researchers, offering a deeper understanding of how to
effectively integrate empathy into SE processes.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [180] [Comparing Normalization Methods for Portfolio Optimization with Reinforcement Learning](https://arxiv.org/abs/2508.03910)
*Caio de Souza Barbosa Costa,Anna Helena Reali Costa*

Main category: q-fin.CP

TL;DR: 本文探讨强化学习在金融投资组合优化中特定策略梯度算法表现不佳的原因，评估两种归一化方法，发现状态归一化会降低智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定策略梯度算法在非加密货币投资组合中表现不佳，推测常用状态归一化方法会使智能体丢失关键信息。

Method: 在三个不同市场（IBOVESPA、纽约证券交易所和加密货币）评估两种最常用的归一化方法，并与训练前对数据进行归一化的标准做法进行比较。

Result: 在该特定领域，状态归一化确实会降低智能体的性能。

Conclusion: 状态归一化是导致领域特定策略梯度算法表现不佳的原因之一。

Abstract: Recently, reinforcement learning has achieved remarkable results in various
domains, including robotics, games, natural language processing, and finance.
In the financial domain, this approach has been applied to tasks such as
portfolio optimization, where an agent continuously adjusts the allocation of
assets within a financial portfolio to maximize profit. Numerous studies have
introduced new simulation environments, neural network architectures, and
training algorithms for this purpose. Among these, a domain-specific policy
gradient algorithm has gained significant attention in the research community
for being lightweight, fast, and for outperforming other approaches. However,
recent studies have shown that this algorithm can yield inconsistent results
and underperform, especially when the portfolio does not consist of
cryptocurrencies. One possible explanation for this issue is that the commonly
used state normalization method may cause the agent to lose critical
information about the true value of the assets being traded. This paper
explores this hypothesis by evaluating two of the most widely used
normalization methods across three different markets (IBOVESPA, NYSE, and
cryptocurrencies) and comparing them with the standard practice of normalizing
data before training. The results indicate that, in this specific domain, the
state normalization can indeed degrade the agent's performance.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [181] [Novel Risk Measures for Portfolio Optimization Using Equal-Correlation Portfolio Strategy](https://arxiv.org/abs/2508.03704)
*Biswarup Chakraborty*

Main category: q-fin.PM

TL;DR: 文章引入基于等相关投资组合策略的新型风险度量方法构建投资组合，经实证验证该方法在不同市场条件下风险分散性更好、回报更稳定。


<details>
  <summary>Details</summary>
Motivation: 传统基于协方差的投资组合优化策略难以确保资产间风险结构平衡，易导致集中投资少数证券。

Method: 引入基于等相关投资组合策略的新型风险度量方法，构建明确控制投资组合相关性并保留风险 - 回报权衡的数学优化框架。

Result: 使用历史股市数据实证验证，该方法构建的投资组合在不同市场条件下有更好的风险分散性和更稳定的回报。

Conclusion: 该方法是传统分散投资技术的有力替代，对机构投资者、资产管理者和量化交易策略有实际意义。

Abstract: Portfolio optimization has long been dominated by covariance-based
strategies, such as the Markowitz Mean-Variance framework. However, these
approaches often fail to ensure a balanced risk structure across assets,
leading to concentration in a few securities. In this paper, we introduce novel
risk measures grounded in the equal-correlation portfolio strategy, aiming to
construct portfolios where each asset maintains an equal correlation with the
overall portfolio return. We formulate a mathematical optimization framework
that explicitly controls portfolio-wide correlation while preserving desirable
risk-return trade-offs. The proposed models are empirically validated using
historical stock market data. Our findings show that portfolios constructed via
this approach demonstrate superior risk diversification and more stable returns
under diverse market conditions. This methodology offers a compelling
alternative to conventional diversification techniques and holds practical
relevance for institutional investors, asset managers, and quantitative trading
strategies.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [182] [Reliable Programmatic Weak Supervision with Confidence Intervals for Label Probabilities](https://arxiv.org/abs/2508.03896)
*Verónica Álvarez,Santiago Mazuelas,Steven An,Sanjoy Dasgupta*

Main category: stat.ML

TL;DR: 本文提出一种编程式弱监督方法，能为标签概率提供置信区间并获得更可靠预测，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确标注数据集成本高且耗时，现有编程式弱监督技术无法评估标签概率预测的可靠性。

Method: 使用能封装弱标签函数信息的分布不确定性集合。

Result: 在多个基准数据集上的实验表明，所提方法优于现有技术，且所提供的置信区间具有实用性。

Conclusion: 提出的编程式弱监督方法可提供标签概率置信区间，获得更可靠预测。

Abstract: The accurate labeling of datasets is often both costly and time-consuming.
Given an unlabeled dataset, programmatic weak supervision obtains probabilistic
predictions for the labels by leveraging multiple weak labeling functions (LFs)
that provide rough guesses for labels. Weak LFs commonly provide guesses with
assorted types and unknown interdependences that can result in unreliable
predictions. Furthermore, existing techniques for programmatic weak supervision
cannot provide assessments for the reliability of the probabilistic predictions
for labels. This paper presents a methodology for programmatic weak supervision
that can provide confidence intervals for label probabilities and obtain more
reliable predictions. In particular, the methods proposed use uncertainty sets
of distributions that encapsulate the information provided by LFs with
unrestricted behavior and typology. Experiments on multiple benchmark datasets
show the improvement of the presented methods over the state-of-the-art and the
practicality of the confidence intervals presented.

</details>


### [183] [Reinforcement Learning in MDPs with Information-Ordered Policies](https://arxiv.org/abs/2508.03904)
*Zhongjun Zhang,Shipra Agrawal,Ilan Lobel,Sean R. Sinclair,Christina Lee Yu*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose an epoch-based reinforcement learning algorithm for
infinite-horizon average-cost Markov decision processes (MDPs) that leverages a
partial order over a policy class. In this structure, $\pi' \leq \pi$ if data
collected under $\pi$ can be used to estimate the performance of $\pi'$,
enabling counterfactual inference without additional environment interaction.
Leveraging this partial order, we show that our algorithm achieves a regret
bound of $O(\sqrt{w \log(|\Theta|) T})$, where $w$ is the width of the partial
order. Notably, the bound is independent of the state and action space sizes.
We illustrate the applicability of these partial orders in many domains in
operations research, including inventory control and queuing systems. For each,
we apply our framework to that problem, yielding new theoretical guarantees and
strong empirical results without imposing extra assumptions such as convexity
in the inventory model or specialized arrival-rate structure in the queuing
model.

</details>


### [184] [Negative binomial regression and inference using a pre-trained transformer](https://arxiv.org/abs/2508.04111)
*Valentine Svensson*

Main category: stat.ML

TL;DR: 研究用预训练transformer估计负二项回归参数，对比多种方法，发现矩估计法最有效。


<details>
  <summary>Details</summary>
Motivation: 在大规模对比研究中，负二项回归参数估计计算挑战性大，需寻找高效方法。

Method: 用预训练transformer通过合成数据生成来估计负二项回归参数，与极大似然优化、矩估计法对比。

Result: transformer方法比极大似然优化更准确且快20倍，矩估计法与极大似然优化精度相当，快1000倍，测试校准更好、效力更强。

Conclusion: 矩估计法是此应用中最有效的解决方案。

Abstract: Negative binomial regression is essential for analyzing over-dispersed count
data in in comparative studies, but parameter estimation becomes
computationally challenging in large screens requiring millions of comparisons.
We investigate using a pre-trained transformer to produce estimates of negative
binomial regression parameters from observed count data, trained through
synthetic data generation to learn to invert the process of generating counts
from parameters. The transformer method achieved better parameter accuracy than
maximum likelihood optimization while being 20 times faster. However,
comparisons unexpectedly revealed that method of moment estimates performed as
well as maximum likelihood optimization in accuracy, while being 1,000 times
faster and producing better-calibrated and more powerful tests, making it the
most efficient solution for this application.

</details>


### [185] [Deep Neural Network-Driven Adaptive Filtering](https://arxiv.org/abs/2508.04258)
*Qizhen Wang,Gang Wang,Ying-Chang Liang*

Main category: stat.ML

TL;DR: 提出DNN驱动框架解决自适应滤波泛化挑战，以最大似然为隐式代价函数，有良好泛化能力并经实验验证，还做了稳定性分析。


<details>
  <summary>Details</summary>
Motivation: 解决自适应滤波中长期存在的泛化挑战。

Method: 提出DNN驱动框架，将DNN嵌入AF系统核心架构，建立滤波残差和学习梯度的直接映射，采用最大似然作为隐式代价函数。

Result: 通过大量非高斯场景的数值实验验证了算法具有良好的泛化能力。

Conclusion: 该框架能有效解决自适应滤波的泛化问题，所提算法具有数据驱动和良好泛化能力的特点。

Abstract: This paper proposes a deep neural network (DNN)-driven framework to address
the longstanding generalization challenge in adaptive filtering (AF). In
contrast to traditional AF frameworks that emphasize explicit cost function
design, the proposed framework shifts the paradigm toward direct gradient
acquisition. The DNN, functioning as a universal nonlinear operator, is
structurally embedded into the core architecture of the AF system, establishing
a direct mapping between filtering residuals and learning gradients. The
maximum likelihood is adopted as the implicit cost function, rendering the
derived algorithm inherently data-driven and thus endowed with exemplary
generalization capability, which is validated by extensive numerical
experiments across a spectrum of non-Gaussian scenarios. Corresponding mean
value and mean square stability analyses are also conducted in detail.

</details>


### [186] [The Relative Instability of Model Comparison with Cross-validation](https://arxiv.org/abs/2508.04409)
*Alexandre Bayle,Lucas Janson,Lester Mackey*

Main category: stat.ML

TL;DR: 研究交叉验证为两个算法测试误差差异提供有效置信区间的情况，以软阈值最小二乘法为例，证明相对稳定性不成立，实证确认相关置信区间无效，提醒量化差异不确定性时需谨慎。


<details>
  <summary>Details</summary>
Motivation: 现有交叉验证为单算法测试误差提供置信区间的稳定性结果无法简单用于比较两个算法，需研究相对稳定性及交叉验证为两算法测试误差差异提供有效置信区间的条件。

Method: 研究软阈值最小二乘法，从理论证明其相对稳定性不成立，并进行实证研究。

Result: 理论上该算法比较测试误差时相对稳定性不成立，实证确认使用软阈值或Lasso时交叉验证对测试误差差异的置信区间无效。

Conclusion: 量化两个机器学习算法性能差异的交叉验证估计不确定性时需谨慎，即便两个算法单独是稳定的。

Abstract: Existing work has shown that cross-validation (CV) can be used to provide an
asymptotic confidence interval for the test error of a stable machine learning
algorithm, and existing stability results for many popular algorithms can be
applied to derive positive instances where such confidence intervals will be
valid. However, in the common setting where CV is used to compare two
algorithms, it becomes necessary to consider a notion of relative stability
which cannot easily be derived from existing stability results, even for simple
algorithms. To better understand relative stability and when CV provides valid
confidence intervals for the test error difference of two algorithms, we study
the soft-thresholded least squares algorithm, a close cousin of the Lasso. We
prove that while stability holds when assessing the individual test error of
this algorithm, relative stability fails to hold when comparing the test error
of two such algorithms, even in a sparse low-dimensional linear model setting.
Additionally, we empirically confirm the invalidity of CV confidence intervals
for the test error difference when either soft-thresholding or the Lasso is
used. In short, caution is needed when quantifying the uncertainty of CV
estimates of the performance difference of two machine learning algorithms,
even when both algorithms are individually stable.

</details>


### [187] [Benchmarking Uncertainty and its Disentanglement in multi-label Chest X-Ray Classification](https://arxiv.org/abs/2508.04457)
*Simon Baur,Wojciech Samek,Jackie Ma*

Main category: stat.ML

TL;DR: 本文为多标签胸部X光分类提供不确定性量化基准，评估13种方法，扩展部分方法到多标签设置，分析方法和架构优劣势。


<details>
  <summary>Details</summary>
Motivation: 可靠的不确定性量化对医学影像AI模型决策和部署至关重要，但在实际医学诊断任务中的应用研究不足。

Method: 使用MIMIC - CXR - JPG数据集为多标签胸部X光分类提供不确定性量化基准，评估13种不确定性量化方法，将Evidential Deep Learning等方法扩展到多标签设置。

Result: 分析揭示了不确定性估计效果以及分离认知和偶然不确定性的能力，发现方法和架构的优劣势。

Conclusion: 为多标签胸部X光分类的不确定性量化提供了全面评估和见解。

Abstract: Reliable uncertainty quantification is crucial for trustworthy
decision-making and the deployment of AI models in medical imaging. While prior
work has explored the ability of neural networks to quantify predictive,
epistemic, and aleatoric uncertainties using an information-theoretical
approach in synthetic or well defined data settings like natural image
classification, its applicability to real life medical diagnosis tasks remains
underexplored. In this study, we provide an extensive uncertainty
quantification benchmark for multi-label chest X-ray classification using the
MIMIC-CXR-JPG dataset. We evaluate 13 uncertainty quantification methods for
convolutional (ResNet) and transformer-based (Vision Transformer) architectures
across a wide range of tasks. Additionally, we extend Evidential Deep Learning,
HetClass NNs, and Deep Deterministic Uncertainty to the multi-label setting.
Our analysis provides insights into uncertainty estimation effectiveness and
the ability to disentangle epistemic and aleatoric uncertainties, revealing
method- and architecture-specific strengths and limitations.

</details>


### [188] [Metric Learning in an RKHS](https://arxiv.org/abs/2508.04476)
*Gokcan Tatli,Yi Chen,Blake Mason,Robert Nowak,Ramya Korlakai Vinayak*

Main category: stat.ML

TL;DR: 本文为度量学习开发通用RKHS框架，给出泛化保证和样本复杂度界限，并通过模拟和实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有非线性度量学习方法缺乏理论理解，除了RKHS为标准欧几里得空间的特殊（线性）情况，需要开发通用框架。

Method: 开发通用RKHS框架用于度量学习。

Result: 得到了新颖的泛化保证和样本复杂度界限，通过模拟和真实数据集实验验证了结果。

Conclusion: 成功为度量学习构建通用RKHS框架，代码已公开。

Abstract: Metric learning from a set of triplet comparisons in the form of "Do you
think item h is more similar to item i or item j?", indicating similarity and
differences between items, plays a key role in various applications including
image retrieval, recommendation systems, and cognitive psychology. The goal is
to learn a metric in the RKHS that reflects the comparisons. Nonlinear metric
learning using kernel methods and neural networks have shown great empirical
promise. While previous works have addressed certain aspects of this problem,
there is little or no theoretical understanding of such methods. The exception
is the special (linear) case in which the RKHS is the standard Euclidean space
$\mathbb{R}^d$; there is a comprehensive theory for metric learning in
$\mathbb{R}^d$. This paper develops a general RKHS framework for metric
learning and provides novel generalization guarantees and sample complexity
bounds. We validate our findings through a set of simulations and experiments
on real datasets. Our code is publicly available at
https://github.com/RamyaLab/metric-learning-RKHS.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [189] [A Game-Theoretic Framework for Network Formation in Large Populations](https://arxiv.org/abs/2508.03847)
*Gokce Dayanikli,Mathieu Lauriere*

Main category: math.OC

TL;DR: 研究大群体网络形成模型，定义模型后聚焦分段常值图特例，给出最优性条件、唯一性和存在性结果，并进行数值实验。


<details>
  <summary>Details</summary>
Motivation: 研究大群体网络形成模型，与现有图论博弈理论不同，考虑每个主体控制依赖自身和其他主体指标。

Method: 定义一般博弈模型，聚焦分段常值图特例，通过正倒向随机微分方程组给出最优性条件。

Result: 得出唯一性和存在性结果，进行数值实验讨论不同模型设置的影响。

Conclusion: 对大群体网络形成模型进行了理论分析和数值验证。

Abstract: In this paper, we study a model of network formation in large populations.
Each agent can choose the strength of interaction (i.e. connection) with other
agents to find a Nash equilibrium. Different from the recently-developed theory
of graphon games, here each agent's control depends not only on her own index
but also on the index of other agents. After defining the general model of the
game, we focus on a special case with piecewise constant graphs and we provide
optimality conditions through a system of forward-backward stochastic
differential equations. Furthermore, we show the uniqueness and existence
results. Finally, we provide numerical experiments to discuss the effects of
different model settings.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [190] [ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants](https://arxiv.org/abs/2508.03936)
*Xiangzhe Xu,Guangyu Shen,Zian Su,Siyuan Cheng,Hanxi Guo,Lu Yan,Xuan Chen,Jiasheng Jiang,Xiaolong Jin,Chengpeng Wang,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.CR

TL;DR: 介绍ASTRA自动化代理系统，可系统发现AI代码生成和安全指导系统的安全漏洞，比现有技术更有效。


<details>
  <summary>Details</summary>
Motivation: AI编码助手安全性不确定，现有红队工具存在不足，难以发现现实世界的漏洞。

Method: ASTRA分三个阶段工作：构建特定领域知识图、进行在线漏洞探索、生成违规案例；聚焦现实输入，结合离线抽象和在线适应。

Result: 在两个主要评估领域，比现有技术多发现11 - 66%的问题，生成的测试用例使对齐训练有效性提高17%。

Conclusion: ASTRA对构建更安全的AI系统具有实用价值。

Abstract: AI coding assistants like GitHub Copilot are rapidly transforming software
development, but their safety remains deeply uncertain-especially in
high-stakes domains like cybersecurity. Current red-teaming tools often rely on
fixed benchmarks or unrealistic prompts, missing many real-world
vulnerabilities. We present ASTRA, an automated agent system designed to
systematically uncover safety flaws in AI-driven code generation and security
guidance systems. ASTRA works in three stages: (1) it builds structured
domain-specific knowledge graphs that model complex software tasks and known
weaknesses; (2) it performs online vulnerability exploration of each target
model by adaptively probing both its input space, i.e., the spatial
exploration, and its reasoning processes, i.e., the temporal exploration,
guided by the knowledge graphs; and (3) it generates high-quality
violation-inducing cases to improve model alignment. Unlike prior methods,
ASTRA focuses on realistic inputs-requests that developers might actually
ask-and uses both offline abstraction guided domain modeling and online domain
knowledge graph adaptation to surface corner-case vulnerabilities. Across two
major evaluation domains, ASTRA finds 11-66% more issues than existing
techniques and produces test cases that lead to 17% more effective alignment
training, showing its practical value for building safer AI systems.

</details>


### [191] [SenseCrypt: Sensitivity-guided Selective Homomorphic Encryption for Joint Federated Learning in Cross-Device Scenarios](https://arxiv.org/abs/2508.04100)
*Borui Li,Li Yan,Junhao Han,Jianmin Liu,Lei Yu*

Main category: cs.CR

TL;DR: 传统选择性同态加密在跨设备场景有问题，提出SenseCrypt框架平衡安全与开销，实验表明其保障安全、达正常精度且减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统同态加密开销高、适应成本大，传统选择性HE方法在跨设备场景会恶化客户端延迟且降低开销效果不佳，需平衡跨设备FL客户端的安全和HE开销。

Method: 先设计隐私保护方法对数据分布相似的客户端聚类，再开发评分机制推导各客户端可加密的无延迟模型参数比例，最后为每个客户端解决多目标模型参数选择优化问题。

Result: SenseCrypt能抵御最先进的反演攻击，在IID数据上达到正常模型精度，与传统HE方法相比，训练时间减少58.4%-88.7%。

Conclusion: SenseCrypt可有效平衡跨设备FL客户端的安全和HE开销。

Abstract: Homomorphic Encryption (HE) prevails in securing Federated Learning (FL), but
suffers from high overhead and adaptation cost. Selective HE methods, which
partially encrypt model parameters by a global mask, are expected to protect
privacy with reduced overhead and easy adaptation. However, in cross-device
scenarios with heterogeneous data and system capabilities, traditional
Selective HE methods deteriorate client straggling, and suffer from degraded HE
overhead reduction performance. Accordingly, we propose SenseCrypt, a
Sensitivity-guided selective Homomorphic EnCryption framework, to adaptively
balance security and HE overhead per cross-device FL client. Given the
observation that model parameter sensitivity is effective for measuring
clients' data distribution similarity, we first design a privacy-preserving
method to respectively cluster the clients with similar data distributions.
Then, we develop a scoring mechanism to deduce the straggler-free ratio of
model parameters that can be encrypted by each client per cluster. Finally, for
each client, we formulate and solve a multi-objective model parameter selection
optimization problem, which minimizes HE overhead while maximizing model
security without causing straggling. Experiments demonstrate that SenseCrypt
ensures security against the state-of-the-art inversion attacks, while
achieving normal model accuracy as on IID data, and reducing training time by
58.4%-88.7% as compared to traditional HE methods.

</details>


### [192] [PLA: Prompt Learning Attack against Text-to-Image Generative Models](https://arxiv.org/abs/2508.03696)
*Xinqi Lyu,Yihao Liu,Yanjie Li,Bin Xiao*

Main category: cs.CR

TL;DR: 本文研究黑盒设置下文本到图像（T2I）模型安全机制的对抗攻击，提出新的提示学习攻击框架PLA，实验表明其攻击成功率高。


<details>
  <summary>Details</summary>
Motivation: T2I模型虽广泛应用，但存在生成NSFW内容风险，需研究其安全机制的脆弱性。

Method: 提出提示学习攻击框架PLA，利用多模态相似性为黑盒T2I模型设计基于梯度的训练。

Result: 新方法能以高成功率有效攻击黑盒T2I模型的安全机制，优于现有方法。

Conclusion: 提出的PLA框架在黑盒设置下能有效攻击T2I模型的安全机制。

Abstract: Text-to-Image (T2I) models have gained widespread adoption across various
applications. Despite the success, the potential misuse of T2I models poses
significant risks of generating Not-Safe-For-Work (NSFW) content. To
investigate the vulnerability of T2I models, this paper delves into adversarial
attacks to bypass the safety mechanisms under black-box settings. Most previous
methods rely on word substitution to search adversarial prompts. Due to limited
search space, this leads to suboptimal performance compared to gradient-based
training. However, black-box settings present unique challenges to training
gradient-driven attack methods, since there is no access to the internal
architecture and parameters of T2I models. To facilitate the learning of
adversarial prompts in black-box settings, we propose a novel prompt learning
attack framework (PLA), where insightful gradient-based training tailored to
black-box T2I models is designed by utilizing multimodal similarities.
Experiments show that our new method can effectively attack the safety
mechanisms of black-box T2I models including prompt filters and post-hoc safety
checkers with a high success rate compared to state-of-the-art methods.
Warning: This paper may contain offensive model-generated content.

</details>


### [193] [Simulating Cyberattacks through a Breach Attack Simulation (BAS) Platform empowered by Security Chaos Engineering (SCE)](https://arxiv.org/abs/2508.03882)
*Arturo Sánchez-Matas,Pablo Escribano Ruiz,Daniel Díaz-López,Angel Luis Perales Gómez,Pantaleone Nespoli,Gregorio Martínez Pérez*

Main category: cs.CR

TL;DR: 文章提出将安全混沌工程（SCE）集成到漏洞攻击模拟（BAS）平台，介绍架构并评估，表明能提升攻击模拟有效性。


<details>
  <summary>Details</summary>
Motivation: 当今组织面临不断演变的网络威胁，需用新方法发现攻击向量。

Method: 将SCE集成到BAS平台，采用三层架构，在BAS层利用MITRE Caldera执行自动攻击序列，根据对手配置文件创建攻击树。

Result: 集成SCE与BAS能提升攻击模拟的有效性，超越传统场景。

Conclusion: 集成SCE与BAS是网络防御策略的有用组成部分。

Abstract: In today digital landscape, organizations face constantly evolving cyber
threats, making it essential to discover slippery attack vectors through novel
techniques like Security Chaos Engineering (SCE), which allows teams to test
defenses and identify vulnerabilities effectively. This paper proposes to
integrate SCE into Breach Attack Simulation (BAS) platforms, leveraging
adversary profiles and abilities from existing threat intelligence databases.
This innovative proposal for cyberattack simulation employs a structured
architecture composed of three layers: SCE Orchestrator, Connector, and BAS
layers. Utilizing MITRE Caldera in the BAS layer, our proposal executes
automated attack sequences, creating inferred attack trees from adversary
profiles. Our proposal evaluation illustrates how integrating SCE with BAS can
enhance the effectiveness of attack simulations beyond traditional scenarios,
and be a useful component of a cyber defense strategy.

</details>


### [194] [Evaluating Selective Encryption Against Gradient Inversion Attacks](https://arxiv.org/abs/2508.04155)
*Jiajun Gu,Yuhang Yao,Shuaiqi Wang,Carlee Joe-Wong*

Main category: cs.CR

TL;DR: 本文系统评估不同重要性指标的选择性加密方法，提出基于距离的显著性分析框架，实验确定梯度大小是抵御优化型梯度反演的有效指标，且无通用最优策略并给出选择指南。


<details>
  <summary>Details</summary>
Motivation: 梯度反演攻击对分布式训练框架有隐私威胁，传统加密防御计算开销大，选择性加密是有前景的方法，但缺乏指定指标的系统研究。

Method: 系统评估不同重要性指标的选择性加密方法，提出基于距离的显著性分析框架，在不同模型架构和攻击类型上进行大量实验。

Result: 选择性加密可降低计算开销且抵御攻击，梯度大小是抵御优化型梯度反演的有效指标。

Conclusion: 没有单一选择性加密策略在所有攻击场景通用，需根据不同模型架构和隐私需求选择合适策略。

Abstract: Gradient inversion attacks pose significant privacy threats to distributed
training frameworks such as federated learning, enabling malicious parties to
reconstruct sensitive local training data from gradient communications between
clients and an aggregation server during the aggregation process. While
traditional encryption-based defenses, such as homomorphic encryption, offer
strong privacy guarantees without compromising model utility, they often incur
prohibitive computational overheads. To mitigate this, selective encryption has
emerged as a promising approach, encrypting only a subset of gradient data
based on the data's significance under a certain metric. However, there have
been few systematic studies on how to specify this metric in practice. This
paper systematically evaluates selective encryption methods with different
significance metrics against state-of-the-art attacks. Our findings demonstrate
the feasibility of selective encryption in reducing computational overhead
while maintaining resilience against attacks. We propose a distance-based
significance analysis framework that provides theoretical foundations for
selecting critical gradient elements for encryption. Through extensive
experiments on different model architectures (LeNet, CNN, BERT, GPT-2) and
attack types, we identify gradient magnitude as a generally effective metric
for protection against optimization-based gradient inversions. However, we also
observe that no single selective encryption strategy is universally optimal
across all attack scenarios, and we provide guidelines for choosing appropriate
strategies for different model architectures and privacy requirements.

</details>


### [195] [Attack Pattern Mining to Discover Hidden Threats to Industrial Control Systems](https://arxiv.org/abs/2508.04561)
*Muhammad Azmi Umer,Chuadhry Mujeeb Ahmed,Aditya Mathur,Muhammad Taha Jilani*

Main category: cs.CR

TL;DR: 本文聚焦工业控制系统（ICS）安全中攻击模式挖掘的验证，提出数据驱动技术生成攻击模式，并通过案例研究验证。


<details>
  <summary>Details</summary>
Motivation: 对ICS进行全面安全评估需要生成大量多样的攻击模式。

Method: 提出一种数据驱动技术，从运营中的水处理厂收集的数据生成攻击模式。

Result: 利用该技术从水处理厂数据中生成了超过100,000种攻击模式。

Conclusion: 文中通过详细案例研究对攻击模式进行验证。

Abstract: This work focuses on validation of attack pattern mining in the context of
Industrial Control System (ICS) security. A comprehensive security assessment
of an ICS requires generating a large and variety of attack patterns. For this
purpose we have proposed a data driven technique to generate attack patterns
for an ICS. The proposed technique has been used to generate over 100,000
attack patterns from data gathered from an operational water treatment plant.
In this work we present a detailed case study to validate the attack patterns.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [196] [RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting](https://arxiv.org/abs/2508.04078)
*Zhan Li,Huangying Zhan,Changyang Li,Qingan Yan,Yi Xu*

Main category: cs.GR

TL;DR: 提出RLGS框架用于3DGS自适应超参数调整，可集成到现有管道，在多个变体和数据集上验证，提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS超参数调整劳动密集且依赖专家，导致重建不一致和结果不佳。

Method: 提出RLGS框架，通过轻量级策略模块动态调整关键超参数，模型无关且可无缝集成到现有3DGS管道。

Result: 在多个3DGS变体和不同数据集上验证了泛化性和鲁棒性，如在TNT数据集上使Taming - 3DGS的PSNR提高0.7dB。

Conclusion: RLGS为3DGS训练中超参数调整提供了有效且通用的解决方案，填补了强化学习在3DGS应用的空白。

Abstract: Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive
and expert-driven process, often resulting in inconsistent reconstructions and
suboptimal results. We propose RLGS, a plug-and-play reinforcement learning
framework for adaptive hyperparameter tuning in 3DGS through lightweight policy
modules, dynamically adjusting critical hyperparameters such as learning rates
and densification thresholds. The framework is model-agnostic and seamlessly
integrates into existing 3DGS pipelines without architectural modifications. We
demonstrate its generalization ability across multiple state-of-the-art 3DGS
variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness
across diverse datasets. RLGS consistently enhances rendering quality. For
example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT)
dataset, under a fixed Gaussian budget, and continues to yield gains even when
baseline performance saturates. Our results suggest that RLGS provides an
effective and general solution for automating hyperparameter tuning in 3DGS
training, bridging a gap in applying reinforcement learning to 3DGS.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [197] [Two-sample comparison through additive tree models for density ratios](https://arxiv.org/abs/2508.03059)
*Naoki Awaya,Yuliang Xu,Li Ma*

Main category: stat.ME

TL;DR: 本文提出用加法树模型学习两个分布的密度比，使用平衡损失函数训练模型，给出优化和贝叶斯两种视角算法，实验证明方法准确且能进行不确定性量化，并应用于微生物组数据生成模型质量评估。


<details>
  <summary>Details</summary>
Motivation: 考虑从两个分布的独立同分布观测中学习密度比，以表征两个密度的差异。

Method: 提出用于密度比的加法树模型，使用平衡损失函数训练，从优化和广义贝叶斯两个视角给出训练算法。

Result: 数值实验证明方法准确，能进行不确定性量化，应用于微生物组数据生成模型质量评估。

Conclusion: 所提方法可准确学习密度比，在不确定性量化方面有独特能力，可应用于实际问题。

Abstract: The ratio of two densities characterizes their differences. We consider
learning the density ratio given i.i.d. observations from each of the two
distributions. We propose additive tree models for the density ratio along with
efficient algorithms for training these models using a new loss function called
the balancing loss. With this loss, additive tree models for the density ratio
can be trained using algorithms original designed for supervised learning.
Specifically, they can be trained from both an optimization perspective that
parallels tree boosting and from a (generalized) Bayesian perspective that
parallels Bayesian additive regression trees (BART). For the former, we present
two boosting algorithms -- one based on forward-stagewise fitting and the other
based on gradient boosting, both of which produce a point estimate for the
density ratio function. For the latter, we show that due to the loss function's
resemblance to an exponential family kernel, the new loss can serve as a
pseudo-likelihood for which conjugate priors exist, thereby enabling effective
generalized Bayesian inference on the density ratio using backfitting samplers
designed for BART. The resulting uncertainty quantification on the inferred
density ratio is critical for applications involving high-dimensional and
complex distributions in which uncertainty given limited data can often be
substantial. We provide insights on the balancing loss through its close
connection to the exponential loss in binary classification and to the
variational form of f-divergence, in particular that of the squared Hellinger
distance. Our numerical experiments demonstrate the accuracy of the proposed
approach while providing unique capabilities in uncertainty quantification. We
demonstrate the application of our method in a case study involving assessing
the quality of generative models for microbiome compositional data.

</details>


### [198] [Generative Flexible Latent Structure Regression (GFLSR) model](https://arxiv.org/abs/2508.04393)
*Clara Grazian,Qian Jin,Pierre Lafaye De Micheaux*

Main category: stat.ME

TL;DR: 本文提出GFLSR模型结构解决线性连续潜在结构方法缺乏模型推断等问题，以PLS为例展示其特化形式，分析参数和潜变量收敛性，提出新的自举算法，通过模拟和真实数据集验证，为线性连续潜变量方法带来潜在推断结构。


<details>
  <summary>Details</summary>
Motivation: 多数线性连续潜在结构方法因缺乏模型推断、生成形式和参数不可识别，常被用作算法而非模型，需解决此问题。

Method: 提出GFLSR模型结构，将PLS特化为Generative - PLS，分析参数和潜变量收敛性，提出新的自举算法，用模拟和真实数据集验证。

Result: 多数线性连续潜变量方法可在提出的框架下表示，PLS可在该模型结构中特化，在额外分布假设下可进行模型推断，新自举算法能实现参数和预测的不确定性分析。

Conclusion: 提出的GFLSRM结构为所有线性连续潜变量方法带来潜在推断结构。

Abstract: Latent structure methods, specifically linear continuous latent structure
methods, are a type of fundamental statistical learning strategy. They are
widely used for dimension reduction, regression and prediction, in the fields
of chemometrics, economics, social science and etc. However, due to the lack of
model inference, generative form, and unidentifiable parameters, most of these
methods are always used as an algorithm, instead of a model. This paper
proposed a Generative Flexible Latent Structure Regression (GFLSR) model
structure to address this problem. Moreover, we show that most linear
continuous latent variable methods can be represented under the proposed
framework. The recursive structure allows potential model inference and
residual analysis. Then, the traditional Partial Least Squares (PLS) is
focused; we show that the PLS can be specialised in the proposed model
structure, named Generative-PLS. With a model structure, we analyse the
convergence of the parameters and the latent variables. Under additional
distribution assumptions, we show that the proposed model structure can lead to
model inference without solving the probabilistic model. Additionally, we
proposed a novel bootstrap algorithm that enables uncertainty on parameters and
on prediction for new datasets. A simulation study and a Real-world dataset are
used to verify the proposed Generative-PLS model structure. Although the
traditional PLS is a special case, this proposed GFLSRM structure leads to a
potential inference structure for all the linear continuous latent variable
methods.

</details>


### [199] [Accept-Reject Lasso](https://arxiv.org/abs/2508.04646)
*Yanxin Liu,Yunqi Zhang*

Main category: stat.ME

TL;DR: Lasso方法在特征高度相关时不稳定，本文提出Accept - Reject Lasso (ARL) 方法解决此问题，通过对数据子集特征选择的细粒度分析，区分真实和虚假相关性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: Lasso方法在处理高度相关特征时会出现错误选择预测变量的问题，现有方法大多只能解决其中一个挑战，需要新方法解决该困境。

Method: 引入Accept - Reject框架，利用聚类识别数据子集结构，分析Lasso在子集上的行为，区分真实和虚假相关性，通过设置阈值区分两种现象。

Result: 通过大量模拟和真实数据实验验证了ARL方法的有效性。

Conclusion: ARL方法能有效解决Lasso在高度相关特征下的不稳定性问题，可最大化纳入信息变量，最小化引入有害变量。

Abstract: The Lasso method is known to exhibit instability in the presence of highly
correlated features, often leading to an arbitrary selection of predictors.
This issue manifests itself in two primary error types: the erroneous omission
of features that lack a true substitutable relationship (falsely redundant
features) and the inclusion of features with a true substitutable relationship
(truly redundant features). Although most existing methods address only one of
these challenges, we introduce the Accept-Reject Lasso (ARL), a novel approach
that resolves this dilemma. ARL operationalizes an Accept-Reject framework
through a fine-grained analysis of feature selection across data subsets. This
framework is designed to partition the output of an ensemble method into
beneficial and detrimental components through fine-grained analysis. The
fundamental challenge for Lasso is that inter-variable correlation obscures the
true sources of information. ARL tackles this by first using clustering to
identify distinct subset structures within the data. It then analyzes Lasso's
behavior across these subsets to differentiate between true and spurious
correlations. For truly correlated features, which induce multicollinearity,
ARL tends to select a single representative feature and reject the rest to
ensure model stability. Conversely, for features linked by spurious
correlations, which may vanish in certain subsets, ARL accepts those that Lasso
might have incorrectly omitted. The distinct patterns arising from true versus
spurious correlations create a divisible separation. By setting an appropriate
threshold, our framework can effectively distinguish between these two
phenomena, thereby maximizing the inclusion of informative variables while
minimizing the introduction of detrimental ones. We illustrate the efficacy of
the proposed method through extensive simulation and real-data experiments.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [200] [AI Investment and Firm Productivity: How Executive Demographics Drive Technology Adoption and Performance in Japanese Enterprises](https://arxiv.org/abs/2508.03757)
*Tatsuru Kikuchi*

Main category: econ.GN

TL;DR: 本文用500多家日本企业2018 - 2023年数据，研究高管年龄和性别对AI投资决策及企业生产率的影响，发现CEO年龄和技术背景影响AI投资倾向，AI投资使全要素生产率提高2.4%，并分析了生产率提升渠道，为AI投资提供指导。


<details>
  <summary>Details</summary>
Motivation: 探究高管人口统计学特征（年龄和性别）如何影响人工智能投资决策以及后续的企业生产率，明确高管特征在技术采用中的作用。

Method: 使用500多家日本企业2018 - 2023年的综合数据，将高管人口特征作为工具变量解决内生性问题，采用新的机制分解框架。

Result: CEO年龄和技术背景显著预测AI投资倾向；AI投资使全要素生产率提高2.4%；生产率提升通过成本降低、收入增加和创新加速三个渠道；年轻高管（50岁以下）采用AI技术可能性高23%；企业规模会调节该关系；广泛采用AI对日本GDP有潜在影响。

Conclusion: 研究结果为理解推动数字化转型的人为因素提供了关键实证指导，为企业治理和公共政策中AI投资激励措施提供参考。

Abstract: This paper investigates how executive demographics particularly age and
gender influence artificial intelligence (AI) investment decisions and
subsequent firm productivity using comprehensive data from over 500 Japanese
enterprises spanning from 2018 to 2023. Our central research question addresses
the role of executive characteristics in technology adoption, finding that CEO
age and technical background significantly predict AI investment propensity.
Employing these demographic characteristics as instrumental variables to
address endogeneity concerns, we identify a statistically significant 2.4%
increase in total factor productivity attributable to AI investment adoption.
Our novel mechanism decomposition framework reveals that productivity gains
operate through three distinct channels: cost reduction (40% of total effect),
revenue enhancement (35%), and innovation acceleration (25%). The results
demonstrate that younger executives (below 50 years) are 23% more likely to
adopt AI technologies, while firm size significantly moderates this
relationship. Aggregate projections suggest potential GDP impacts of 1.15
trillion JPY from widespread AI adoption across the Japanese economy. These
findings provide crucial empirical guidance for understanding the human factors
driving digital transformation and inform both corporate governance and public
policy regarding AI investment incentives.

</details>


### [201] [Quantifying the Economic Impact of 2025 ICE Raids on California's Agricultural Industry: A Case Study of Oxnard](https://arxiv.org/abs/2508.03787)
*Xinyu Li*

Main category: econ.GN

TL;DR: 本文量化了2025年加州奥克斯纳德移民执法突袭对农业的经济影响，估算了劳动力、作物损失和物价变化，提出政策建议。


<details>
  <summary>Details</summary>
Motivation: 量化移民执法突袭对加州农业劳动力市场、作物生产和食品价格的经济影响。

Method: 使用计量经济模型，结合美国农业部经济研究服务局数据和移民与海关执法局拘留数据。

Result: 农业劳动力减少20 - 40%，作物损失30 - 70亿美元，农产品价格上涨5 - 12%，对草莓等劳动密集型作物影响大。

Conclusion: 提出扩大H - 2A签证计划和使无证工人合法化等政策建议，为农业经济学提供数据驱动评估。

Abstract: In 2025, intensified Immigration and Customs Enforcement (ICE) raids in
Oxnard, California, disrupted the state's \$49 billion agricultural industry, a
critical supplier of 75% of U.S. fruits and nuts and one-third of its
vegetables. This paper quantifies the economic consequences of these raids on
labor markets, crop production, and food prices using econometric modeling. We
estimate a 20-40% reduction in the agricultural workforce, leading to \$3-7
billion in crop losses and a 5-12% increase in produce prices. The analysis
draws on USDA Economic Research Service data and recent ICE detention figures,
which show arrests in Southern California rising from 699 in May to nearly
2,000 in June 2025. The raids disproportionately affect labor-intensive crops
like strawberries, exacerbating supply chain disruptions. Policy
recommendations include expanding the H-2A visa program and legalizing
undocumented workers to stabilize the sector. This study contributes to
agricultural economics by providing a data-driven assessment of immigration
enforcement's economic toll.

</details>


### [202] [Exports, Labor Markets, and the Environment: Evidence from Brazil](https://arxiv.org/abs/2508.03855)
*Carlos Góes,Otavio Conceição,Gabriel Lara Ibarra,Gladys Lopez-Acevedo*

Main category: econ.GN

TL;DR: 文章聚焦2000 - 2020年，结合多源微观数据估算出口对就业弹性的影响，发现出口增长促进正规就业，长期中非正式就业负向响应，环境风险和可持续就业与出口关系随时间有不同变化。


<details>
  <summary>Details</summary>
Motivation: 探究出口的环境影响。

Method: 结合海关、行政和普查微观数据估算出口的就业弹性。

Result: 出口增长使正规就业增长更快，弹性最高达0.4且长期为正；长期中非正式就业负向响应出口冲击；短期内环境风险就业对出口响应更强，长期则相反。

Conclusion: 出口对就业和环境相关就业的影响存在短期和长期差异。

Abstract: What is the environmental impact of exports? Focusing on 2000-20, this paper
combines customs, administrative, and census microdata to estimate employment
elasticities with respect to exports. The findings show that municipalities
that faced increased exports experienced faster growth in formal employment.
The elasticities were 0.25 on impact, peaked at 0.4, and remained positive and
significant even 10 years after the shock, pointing to a long and protracted
labor market adjustment. In the long run, informal employment responds
negatively to export shocks. Using a granular taxonomy for economic activities
based on their environmental impact, the paper documents that environmentally
risky activities have a larger share of employment than environmentally
sustainable ones, and that the relationship between these activities and
exports is nuanced. Over the short run, environmentally risky employment
responds more strongly to exports relative to environmentally sustainable
employment. However, over the long run, this pattern reverses, as the impact of
exports on environmentally sustainable employment is more persistent.

</details>


### [203] [EEA Professional Climate Survey Report](https://arxiv.org/abs/2508.04302)
*Tim Lee,Massimo Morelli,Marvin Pappalettera,Dario Sansone,Sulin Sardoschau*

Main category: econ.GN

TL;DR: 2023年EEA MinE委员会与VfS合作开展调查，评估欧洲经济学界的多样性、公平性和包容性，结果显示存在广泛差异。


<details>
  <summary>Details</summary>
Motivation: 评估欧洲经济学界的多样性、公平性和包容性。

Method: 对861名EEA现任和前任成员进行专业环境调查，收集人口统计数据和不同群体的经历。

Result: 不同群体在包容、尊重和专业待遇方面的看法存在广泛差异，特定群体报告的歧视等情况更多，存在地域差异，且欧洲受访者总体满意度低于2018年美国经济协会调查。

Conclusion: 欧洲经济学界在多样性、公平性和包容性方面存在问题。

Abstract: In 2023, the European Economic Association (EEA) Minorities in Economics
(MinE) Committee, in collaboration with the German Economic Association (VfS),
conducted a professional climate survey to assess diversity, equity, and
inclusion in the European economics profession.The survey gathered responses
from 861 current and former EEA members, capturing demographic data and
experiences across gender, ethnicity, LGBTQ+ identity, disability, and
socioeconomic background. Results revealed widespread disparities in
perceptions of inclusion, respect, and professional treatment. Reports of
discrimination, exclusion, and harassment were significantly higher among
women, ethnic minorities, LGBTQ+ individuals, and people with disabilities.
Geographic differences also emerged, with the Nordic countries reporting the
most positive climate and the UK and Italy showing higher levels of
dissatisfaction and discrimination. Compared to the American Economic
Association 2018 survey, European respondents reported lower satisfaction
overall.

</details>


### [204] [Testing Spillovers in Resource Conservation: Evidence from a Natural Field Experiment](https://arxiv.org/abs/2508.04371)
*Lorenz Goette,Zhi Hao Lim*

Main category: econ.GN

TL;DR: 本文通过大规模实地实验研究行为干预促进资源节约的溢出效应，发现节水干预减少了淋浴用水，未观察到空调使用的直接或溢出效应。


<details>
  <summary>Details</summary>
Motivation: 研究一个领域促进资源节约的行为干预在另一领域的溢出效应。

Method: 对约2000名居民开展大规模自然实地实验，实施三项干预措施，评估实时反馈和社会比较对水和能源消耗的直接及溢出效应。

Result: 两项节水干预显著减少淋浴用水，节能干预对空调使用无直接影响，节水干预对空调使用无溢出效应，反之亦然。

Conclusion: 在本次研究的水和能源领域，行为干预的溢出效应不显著。

Abstract: This paper studies the potential for behavioral interventions aimed at
promoting resource conservation within one domain to induce spillovers in
another. Through a large-scale natural field experiment involving around 2,000
residents, we assess the direct and spillover effects of real-time feedback and
social comparisons on water and energy consumption. Three interventions were
implemented: two targeting shower use and one targeting air-conditioning use.
We document a significant reduction in shower use attributable to both
water-saving interventions, but no direct effects on air-conditioning use from
the energy-saving intervention. For spillovers, we precisely estimated null
effects on air-conditioning use arising from the water-saving interventions,
and vice versa.

</details>


### [205] [SoK: Stablecoins for Digital Transformation -- Design, Metrics, and Application with Real World Asset Tokenization as a Case Study](https://arxiv.org/abs/2508.02403)
*Luyao Zhang*

Main category: econ.GN

TL;DR: 稳定币在数字资产生态中重要性渐显，但学术研究缺乏统一框架，本文通过多方法研究构建分类法、评估框架并进行案例分析，为相关研究提供支持。


<details>
  <summary>Details</summary>
Motivation: 稳定币市场发展迅速且受监管关注，但学术研究在多学科分散，缺乏统一设计、评估和应用框架。

Method: 综合跨学科文献构建稳定币系统分类法；开发适应不同利益相关者需求的性能评估框架；以现实世界资产代币化为例进行案例研究。

Result: 得出稳定币设计的统一分类法、以利益相关者为导向的性能评估框架、稳定币与行业转型的实证案例，以及可重复使用的方法和数据集。

Conclusion: 研究成果有助于开发可信、包容和透明的数字货币基础设施。

Abstract: Stablecoins have become a foundational component of the digital asset
ecosystem, with their market capitalization exceeding 230 billion USD as of May
2025. As fiat-referenced and programmable assets, stablecoins provide
low-latency, globally interoperable infrastructure for payments, decentralized
finance, DeFi, and tokenized commerce. Their accelerated adoption has prompted
extensive regulatory engagement, exemplified by the European Union's Markets in
Crypto-assets Regulation, MiCA, the US Guiding and Establishing National
Innovation for US Stablecoins Act, GENIUS Act, and Hong Kong's Stablecoins
Bill. Despite this momentum, academic research remains fragmented across
economics, law, and computer science, lacking a unified framework for design,
evaluation, and application. This study addresses that gap through a
multi-method research design. First, it synthesizes cross-disciplinary
literature to construct a taxonomy of stablecoin systems based on custodial
structure, stabilization mechanism, and governance. Second, it develops a
performance evaluation framework tailored to diverse stakeholder needs,
supported by an open-source benchmarking pipeline to ensure transparency and
reproducibility. Third, a case study on Real World Asset tokenization
illustrates how stablecoins operate as programmable monetary infrastructure in
cross-border digital systems. By integrating conceptual theory with empirical
tools, the paper contributes: a unified taxonomy for stablecoin design; a
stakeholder-oriented performance evaluation framework; an empirical case
linking stablecoins to sectoral transformation; and reproducible methods and
datasets to inform future research. These contributions support the development
of trusted, inclusive, and transparent digital monetary infrastructure.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [206] [When Agents Break Down in Multiagent Path Finding](https://arxiv.org/abs/2508.03777)
*Foivos Fioravantes,Dušan Knop,Nikolaos Melissinos,Michal Opler*

Main category: cs.MA

TL;DR: 本文提出多智能体路径规划（MAPF）新变体，考虑智能体故障延迟问题，提出动态调度适应框架及两种协议应对，证明协议有效性。


<details>
  <summary>Details</summary>
Motivation: 解决MAPF中部分智能体因故障出现延迟，导致维持最优调度困难，且重新规划计算不可行的问题。

Method: 提出不依赖全重规划的动态调度适应框架，开发智能体本地协调调整路径的主协议和将计算转移到网络节点的次协议。

Result: 证明主协议下k次故障后最大完工时间增加不超过k轮；结果显示协议为应对智能体故障的多智能体导航提供实用、可扩展方法。

Conclusion: 所提协议能有效应对智能体故障，实现弹性多智能体导航。

Abstract: In Multiagent Path Finding (MAPF), the goal is to compute efficient,
collision-free paths for multiple agents navigating a network from their
sources to targets, minimizing the schedule's makespan-the total time until all
agents reach their destinations. We introduce a new variant that formally
models scenarios where some agents may experience delays due to malfunctions,
posing significant challenges for maintaining optimal schedules.
  Recomputing an entirely new schedule from scratch after each malfunction is
often computationally infeasible. To address this, we propose a framework for
dynamic schedule adaptation that does not rely on full replanning. Instead, we
develop protocols enabling agents to locally coordinate and adjust their paths
on the fly. We prove that following our primary communication protocol, the
increase in makespan after k malfunctions is bounded by k additional turns,
effectively limiting the impact of malfunctions on overall efficiency.
Moreover, recognizing that agents may have limited computational capabilities,
we also present a secondary protocol that shifts the necessary computations
onto the network's nodes, ensuring robustness without requiring enhanced agent
processing power. Our results demonstrate that these protocols provide a
practical, scalable approach to resilient multiagent navigation in the face of
agent failures.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [207] [LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for Learned Thematic Significance Tracking in Multimedia Content](https://arxiv.org/abs/2508.04353)
*Anderson de Lima Luiz*

Main category: cs.MM

TL;DR: 介绍Learned User Significance Tracker (LUST)框架，用于分析视频内容并量化与用户文本描述的主题相关性，采用多模态分析和分层评分机制。


<details>
  <summary>Details</summary>
Motivation: 提供一种能细致、考虑时间因素的用户定义重要性的衡量方法。

Method: 利用多模态分析管道，融合视频帧视觉线索与语音识别文字信息，采用基于大语言模型的两级相关性评分机制。

Result: 输出带有可视化相关性分数的标注视频和全面分析日志。

Conclusion: LUST框架可实现对视频内容与用户定义主题相关性的量化分析。

Abstract: This paper introduces the Learned User Significance Tracker (LUST), a
framework designed to analyze video content and quantify the thematic relevance
of its segments in relation to a user-provided textual description of
significance. LUST leverages a multi-modal analytical pipeline, integrating
visual cues from video frames with textual information extracted via Automatic
Speech Recognition (ASR) from the audio track. The core innovation lies in a
hierarchical, two-stage relevance scoring mechanism employing Large Language
Models (LLMs). An initial "direct relevance" score, $S_{d,i}$, assesses
individual segments based on immediate visual and auditory content against the
theme. This is followed by a "contextual relevance" score, $S_{c,i}$, that
refines the assessment by incorporating the temporal progression of preceding
thematic scores, allowing the model to understand evolving narratives. The LUST
framework aims to provide a nuanced, temporally-aware measure of user-defined
significance, outputting an annotated video with visualized relevance scores
and comprehensive analytical logs.

</details>


### [208] [Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation](https://arxiv.org/abs/2508.04418)
*Jinxing Zhou,Yanghao Zhou,Mingfei Han,Tong Wang,Xiaojun Chang,Hisham Cholakkal,Rao Muhammad Anwer*

Main category: cs.MM

TL;DR: 提出TGS - Agent用于Referring Audio - Visual Segmentation，将任务分解为Think - Ground - Segment过程，还引入R² - AVSBench，在基准测试中获SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 先前工作依赖多模态融合学习潜在嵌入，需强像素级监督且缺乏可解释性。

Method: 将任务分解为Think - Ground - Segment过程；提出Ref - Thinker并构建数据集微调；用Ref - Thinker推理结果作为显式提示给Grounding - DINO和SAM2；引入R² - AVSBench。

Result: 在标准Ref - AVSBench和R² - AVSBench上取得SOTA结果。

Conclusion: TGS - Agent方法有效，能在Referring Audio - Visual Segmentation任务中取得好效果，R² - AVSBench有助于评估模型泛化能力。

Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects
in audible videos based on given reference expressions. Prior works typically
rely on learning latent embeddings via multimodal fusion to prompt a tunable
SAM/SAM2 decoder for segmentation, which requires strong pixel-level
supervision and lacks interpretability. From a novel perspective of explicit
reference understanding, we propose TGS-Agent, which decomposes the task into a
Think-Ground-Segment process, mimicking the human reasoning procedure by first
identifying the referred object through multimodal analysis, followed by
coarse-grained grounding and precise segmentation. To this end, we first
propose Ref-Thinker, a multimodal language model capable of reasoning over
textual, visual, and auditory cues. We construct an instruction-tuning dataset
with explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The
object description inferred by Ref-Thinker is used as an explicit prompt for
Grounding-DINO and SAM2, which perform grounding and segmentation without
relying on pixel-level supervision. Additionally, we introduce
R\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and
reasoning-intensive references for better evaluating model generalization. Our
approach achieves state-of-the-art results on both standard Ref-AVSBench and
proposed R\textsuperscript{2}-AVSBench. Code will be available at
https://github.com/jasongief/TGS-Agent.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [209] [Agentic-AI based Mathematical Framework for Commercialization of Energy Resilience in Electrical Distribution System Planning and Operation](https://arxiv.org/abs/2508.04170)
*Aniket Johri,Divyanshi Dwivedi,Mayukha Pal*

Main category: eess.SY

TL;DR: 本文提出结合双智能体近端策略优化（PPO）与市场机制的框架，在10次测试中实现平均弹性得分0.85±0.08和效益成本比0.12±0.01，为弹性投资创造可持续市场激励。


<details>
  <summary>Details</summary>
Motivation: 配电系统易受极端天气和网络威胁，现有方法缺乏市场驱动机制，传统优化方法难适应不同条件。

Method: 引入结合双智能体PPO与市场机制的框架，战略智能体选最优DER驱动开关配置，战术智能体微调开关状态和电网偏好，设计综合奖励函数。

Result: 在10次测试中平均弹性得分0.85±0.08，效益成本比0.12±0.01，85%灾难步骤行动选含4个DER的配置。

Conclusion: 该框架能为配电系统弹性投资创造可持续市场激励。

Abstract: The increasing vulnerability of electrical distribution systems to extreme
weather events and cyber threats necessitates the development of economically
viable frameworks for resilience enhancement. While existing approaches focus
primarily on technical resilience metrics and enhancement strategies, there
remains a significant gap in establishing market-driven mechanisms that can
effectively commercialize resilience features while optimizing their deployment
through intelligent decision-making. Moreover, traditional optimization
approaches for distribution network reconfiguration often fail to dynamically
adapt to both normal and emergency conditions. This paper introduces a novel
framework integrating dual-agent Proximal Policy Optimization (PPO) with
market-based mechanisms, achieving an average resilience score of 0.85 0.08
over 10 test episodes. The proposed architecture leverages a dual-agent PPO
scheme, where a strategic agent selects optimal DER-driven switching
configurations, while a tactical agent fine-tunes individual switch states and
grid preferences under budget and weather constraints. These agents interact
within a custom-built dynamic simulation environment that models stochastic
calamity events, budget limits, and resilience-cost trade-offs. A comprehensive
reward function is designed that balances resilience enhancement objectives
with market profitability (with up to 200x reward incentives, resulting in 85%
of actions during calamity steps selecting configurations with 4 DERs),
incorporating factors such as load recovery speed, system robustness, and
customer satisfaction. Over 10 test episodes, the framework achieved a
benefit-cost ratio of 0.12 0.01, demonstrating sustainable market incentives
for resilience investment. This framework creates sustainable market incentives

</details>


### [210] [Case Studies of Generative Machine Learning Models for Dynamical Systems](https://arxiv.org/abs/2508.04459)
*Nachiket U. Bapat,Randy C. Paffenroth,Raghvendra V. Cowlagi*

Main category: eess.SY

TL;DR: 本文探讨用生成式人工智能模型减少航空航天系统模型失配问题，聚焦两个最优控制案例研究，测试三种模型架构，发现新模型尤其是基于VAE的模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 因建模误差等导致仿真模型数据与现实系统运行数据不匹配，研究能否用生成式人工智能模型显著减少这种失配。

Method: 聚焦两个最优控制案例，基于系统轨迹上哈密顿函数不变性制定训练损失函数，研究GAN和两种VAE共三种生成式人工智能模型架构。

Result: 新模型尤其是基于VAE的模型能生成满足控制方程且与训练数据统计相似的数据，即使训练数据量少。

Conclusion: 生成式人工智能模型在减少航空航天系统模型失配方面有潜力，基于VAE的模型效果较好。

Abstract: Systems like aircraft and spacecraft are expensive to operate in the real
world. The design, validation, and testing for such systems therefore relies on
a combination of mathematical modeling, abundant numerical simulations, and a
relatively small set of real-world experiments. Due to modeling errors,
simplifications, and uncertainties, the data synthesized by simulation models
often does not match data from the system's real-world operation. We consider
the broad research question of whether this model mismatch can be significantly
reduced by generative artificial intelligence models (GAIMs). Unlike text- or
image-processing, where generative models have attained recent successes, GAIM
development for aerospace engineering applications must not only train with
scarce operational data, but their outputs must also satisfy governing
equations based on natural laws, e.g., conservation laws. The scope of this
paper primarily focuses on two case studies of optimally controlled systems
that are commonly understood and employed in aircraft guidance, namely:
minimum-time navigation in a wind field and minimum-exposure navigation in a
threat field. We report GAIMs that are trained with a relatively small set, of
the order of a few hundred, of examples and with underlying governing
equations. By focusing on optimally controlled systems, we formulate training
loss functions based on invariance of the Hamiltonian function along system
trajectories. We investigate three GAIM architectures, namely: the generative
adversarial network (GAN) and two variants of the variational autoencoder
(VAE). We provide architectural details and thorough performance analyses of
these models. The main finding is that our new models, especially the VAE-based
models, are able to synthesize data that satisfy the governing equations and
are statistically similar to the training data despite small volumes of
training data.

</details>


### [211] [A virtual sensor fusion approach for state of charge estimation of lithium-ion cells](https://arxiv.org/abs/2508.04268)
*Davide Previtali,Daniele Masti,Mirko Mazzoleni,Fabio Previdi*

Main category: eess.SY

TL;DR: 本文结合卡尔曼滤波器与机器学习方法估计锂离子电池荷电状态（SOC），提出数据驱动的校准策略，实验表明该方法有益。


<details>
  <summary>Details</summary>
Motivation: 解决锂离子电池荷电状态（SOC）的估计问题。

Method: 结合卡尔曼滤波器（KF）与等效电路模型（ECM）和机器学习方法，采用虚拟传感器（VS）合成技术，将VS的SOC预测结果与电池端电压一起作为扩展卡尔曼滤波器（EKF）的输出测量值，并提出EKF噪声协方差矩阵的数据驱动校准策略。

Result: 实验结果显示，所设计的方法在SOC估计的准确性和稳定性方面有益。

Conclusion: 结合两种范式的方法对SOC估计有益。

Abstract: This paper addresses the estimation of the State Of Charge (SOC) of
lithium-ion cells via the combination of two widely used paradigms: Kalman
Filters (KFs) equipped with Equivalent Circuit Models (ECMs) and
machine-learning approaches. In particular, a recent Virtual Sensor (VS)
synthesis technique is considered, which operates as follows: (i) learn an
Affine Parameter-Varying (APV) model of the cell directly from data, (ii)
derive a bank of linear observers from the APV model, (iii) train a
machine-learning technique from features extracted from the observers together
with input and output data to predict the SOC. The SOC predictions returned by
the VS are supplied to an Extended KF (EKF) as output measurements along with
the cell terminal voltage, combining the two paradigms. A data-driven
calibration strategy for the noise covariance matrices of the EKF is proposed.
Experimental results show that the designed approach is beneficial w.r.t. SOC
estimation accuracy and smoothness.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [212] [FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging](https://arxiv.org/abs/2508.04625)
*Zichen Tang,Haihong E,Jiacheng Liu,Zhongjun Yang,Rongjin Li,Zihua Rong,Haoyang He,Zhuodi Hao,Xinyang Hu,Kun Ji,Ziyan Ma,Mengyuan Ji,Jun Zhang,Chenghao Ma,Qianhe Zheng,Yang Liu,Yiling Huang,Xinyi Hu,Qing Huang,Zijian Xie,Shiyao Peng*

Main category: cs.CV

TL;DR: 提出FinMMR双语多模态基准，用于评估多模态大语言模型在金融数值推理任务的推理能力，有三方面改进，认为其能推动提升模型在现实场景推理能力。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型在金融数值推理任务的推理能力，现有基准存在不足。

Method: 精心改造现有金融推理基准，从最新中文金融研究报告构建新问题，构建包含4.3K问题和8.7K图像、涵盖14个类别的FinMMR。

Result: 最佳表现的多模态大语言模型在难题上准确率仅53.0%。

Conclusion: FinMMR将推动提升多模态大语言模型在现实场景的推理能力。

Abstract: We present FinMMR, a novel bilingual multimodal benchmark tailored to
evaluate the reasoning capabilities of multimodal large language models (MLLMs)
in financial numerical reasoning tasks. Compared to existing benchmarks, our
work introduces three significant advancements. (1) Multimodality: We
meticulously transform existing financial reasoning benchmarks, and construct
novel questions from the latest Chinese financial research reports. FinMMR
comprises 4.3K questions and 8.7K images spanning 14 categories, including
tables, bar charts, and ownership structure charts. (2) Comprehensiveness:
FinMMR encompasses 14 financial subdomains, including corporate finance,
banking, and industry analysis, significantly exceeding existing benchmarks in
financial domain knowledge breadth. (3) Challenge: Models are required to
perform multi-step precise numerical reasoning by integrating financial
knowledge with the understanding of complex financial images and text. The
best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe
that FinMMR will drive advancements in enhancing the reasoning capabilities of
MLLMs in real-world scenarios.

</details>


### [213] [RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification](https://arxiv.org/abs/2508.03967)
*Mamadou Keita,Wassim Hamidouche,Hessen Bougueffa Eutamene,Abdelmalik Taleb-Ahmed,Abdenour Hadid*

Main category: cs.CV

TL;DR: 本文介绍首个利用视觉检索增强生成（RAG）的AI生成图像检测框架RAVID，通过动态检索相关图像提升检测效果，实验显示其性能和鲁棒性达最优。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法主要关注文本，视觉知识探索不足，现有检测方法泛化和鲁棒性差，依赖低级特征和特定模型特征，适应性受限。

Method: 利用微调的CLIP图像编码器RAVID CLIP结合类别相关提示改进表征学习，集成视觉语言模型（VLM）融合检索图像和查询图像，形成丰富输入。

Result: 在UniversalFakeDetect基准测试中平均准确率达93.85%，在图像退化条件下平均准确率达80.27%，优于现有模型。

Conclusion: RAVID实现了AI生成图像检测的最优性能和鲁棒性，代码接收后将公开。

Abstract: In this paper, we introduce RAVID, the first framework for AI-generated image
detection that leverages visual retrieval-augmented generation (RAG). While RAG
methods have shown promise in mitigating factual inaccuracies in foundation
models, they have primarily focused on text, leaving visual knowledge
underexplored. Meanwhile, existing detection methods, which struggle with
generalization and robustness, often rely on low-level artifacts and
model-specific features, limiting their adaptability. To address this, RAVID
dynamically retrieves relevant images to enhance detection. Our approach
utilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with
category-related prompts to improve representation learning. We further
integrate a vision-language model (VLM) to fuse retrieved images with the
query, enriching the input and improving accuracy. Given a query image, RAVID
generates an embedding using RAVID CLIP, retrieves the most relevant images
from a database, and combines these with the query image to form an enriched
input for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the
UniversalFakeDetect benchmark, which covers 19 generative models, show that
RAVID achieves state-of-the-art performance with an average accuracy of 93.85%.
RAVID also outperforms traditional methods in terms of robustness, maintaining
high accuracy even under image degradations such as Gaussian blur and JPEG
compression. Specifically, RAVID achieves an average accuracy of 80.27% under
degradation conditions, compared to 63.44% for the state-of-the-art model
C2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG
compression scenarios. The code will be publicly available upon acceptance.

</details>


### [214] [Prototype-Driven Structure Synergy Network for Remote Sensing Images Segmentation](https://arxiv.org/abs/2508.04022)
*Junyi Wang,Jinjiang Li,Guodong Fan,Yakun Ju,Xiang Fang,Alex C. Kot*

Main category: cs.CV

TL;DR: 针对遥感图像语义分割中高类内差异和高类间相似性问题，提出PDSSNet网络，经实验证明性能优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统方法和新兴类引导方法在遥感图像语义分割中难以有效统一类表示和区分相似特征，无法获取完整地物。

Method: 提出PDSSNet网络，设计了自适应原型提取模块、语义 - 结构协调模块和通道相似性调整模块。

Result: PDSSNet在大量实验中表现优于现有方法。

Conclusion: PDSSNet能有效解决遥感图像语义分割中的问题，可获取完整地物。

Abstract: In the semantic segmentation of remote sensing images, acquiring complete
ground objects is critical for achieving precise analysis. However, this task
is severely hindered by two major challenges: high intra-class variance and
high inter-class similarity. Traditional methods often yield incomplete
segmentation results due to their inability to effectively unify class
representations and distinguish between similar features. Even emerging
class-guided approaches are limited by coarse class prototype representations
and a neglect of target structural information.
  Therefore, this paper proposes a Prototype-Driven Structure Synergy Network
(PDSSNet). The design of this network is based on a core concept, a complete
ground object is jointly defined by its invariant class semantics and its
variant spatial structure. To implement this, we have designed three key
modules. First, the Adaptive Prototype Extraction Module (APEM) ensures
semantic accuracy from the source by encoding the ground truth to extract
unbiased class prototypes. Subsequently, the designed Semantic-Structure
Coordination Module (SSCM) follows a hierarchical semantics-first,
structure-second principle. This involves first establishing a global semantic
cognition, then leveraging structural information to constrain and refine the
semantic representation, thereby ensuring the integrity of class information.
Finally, the Channel Similarity Adjustment Module (CSAM) employs a dynamic
step-size adjustment mechanism to focus on discriminative features between
classes.
  Extensive experiments demonstrate that PDSSNet outperforms state-of-the-art
methods. The source code is available at
https://github.com/wangjunyi-1/PDSSNet.

</details>


### [215] [Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval](https://arxiv.org/abs/2508.04028)
*Yifan Wang,Tao Wang,Chenwei Tang,Caiyang Yu,Zhengqing Zang,Mengmi Zhang,Shudong Huang,Jiancheng Lv*

Main category: cs.CV

TL;DR: 提出DCAR框架解决下游图像文本检索任务难题，构建FDRD数据集验证，实验表明性能达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习应用于下游图像文本检索任务有挑战，需区分下游数据细粒度属性和相似子类别。

Method: 提出DCAR框架，从语义和视觉维度动态调整提示向量，联合优化属性和类别特征；构建FDRD数据集。

Result: 在FDRD上的广泛实验显示，DCAR比现有基线模型取得了最优性能。

Conclusion: DCAR框架能有效提升预训练视觉语言模型在下游图像文本检索任务的性能。

Abstract: Recently, prompt learning has demonstrated remarkable success in adapting
pre-trained Vision-Language Models (VLMs) to various downstream tasks such as
image classification. However, its application to the downstream Image-Text
Retrieval (ITR) task is more challenging. We find that the challenge lies in
discriminating both fine-grained attributes and similar subcategories of the
downstream data. To address this challenge, we propose Dual prompt Learning
with Joint Category-Attribute Reweighting (DCAR), a novel dual-prompt learning
framework to achieve precise image-text matching. The framework dynamically
adjusts prompt vectors from both semantic and visual dimensions to improve the
performance of CLIP on the downstream ITR task. Based on the prompt paradigm,
DCAR jointly optimizes attribute and class features to enhance fine-grained
representation learning. Specifically, (1) at the attribute level, it
dynamically updates the weights of attribute descriptions based on text-image
mutual information correlation; (2) at the category level, it introduces
negative samples from multiple perspectives with category-matching weighting to
learn subcategory distinctions. To validate our method, we construct the
Fine-class Described Retrieval Dataset (FDRD), which serves as a challenging
benchmark for ITR in downstream data domains. It covers over 1,500 downstream
fine categories and 230,000 image-caption pairs with detailed attribute
annotations. Extensive experiments on FDRD demonstrate that DCAR achieves
state-of-the-art performance over existing baselines.

</details>


### [216] [ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds](https://arxiv.org/abs/2506.16991)
*Binbin Xiang,Maciej Wielgosz,Stefano Puliti,Kamil Král,Martin Krůček,Azim Missarov,Rasmus Astrup*

Main category: cs.CV

TL;DR: 提出ForestFormer3D框架用于森林LiDAR 3D点云分割，在新数据集上达SOTA，泛化性好。


<details>
  <summary>Details</summary>
Motivation: 当前森林LiDAR 3D点云分割方法难以应对自然森林环境的复杂性和多变性，需要新方法。

Method: 提出ForestFormer3D框架，包含ISA引导的查询点选择、基于分数的块合并策略和一对多关联机制。

Result: 在新的FOR - instanceV2数据集上个体树分割达SOTA，对未见测试集泛化性好。

Conclusion: ForestFormer3D框架有效且具有鲁棒性，代码和数据集公开。

Abstract: The segmentation of forest LiDAR 3D point clouds, including both individual
tree and semantic segmentation, is fundamental for advancing forest management
and ecological research. However, current approaches often struggle with the
complexity and variability of natural forest environments. We present
ForestFormer3D, a new unified and end-to-end framework designed for precise
individual tree and semantic segmentation. ForestFormer3D incorporates
ISA-guided query point selection, a score-based block merging strategy during
inference, and a one-to-many association mechanism for effective training. By
combining these new components, our model achieves state-of-the-art performance
for individual tree segmentation on the newly introduced FOR-instanceV2
dataset, which spans diverse forest types and regions. Additionally,
ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),
showcasing its robustness across different forest conditions and sensor
modalities. The FOR-instanceV2 dataset and the ForestFormer3D code are publicly
available at https://bxiang233.github.io/FF3D/.

</details>


### [217] [Multimodal Video Emotion Recognition with Reliable Reasoning Priors](https://arxiv.org/abs/2508.03722)
*Zhepeng Wang,Yingjian Zhu,Guanghao Dong,Hongzhu Yi,Feng Chen,Xinming Wang,Jun Xie*

Main category: cs.CV

TL;DR: 研究将MLLMs可信先验推理知识集成到多模态情感识别中，采用Gemini生成推理痕迹并注入，引入Balanced Dual - Contrastive Learning解决类别不平衡，在MER2024基准测试中取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 将MLLMs的可信先验推理知识集成到多模态情感识别中，解决多模态情感识别中显著的类别不平衡问题。

Method: 利用Gemini生成细粒度、模态可分离的推理痕迹并在融合阶段注入作为先验；引入Balanced Dual - Contrastive Learning平衡类间和类内分布。

Result: 在MER2024基准测试中，先验增强框架取得了显著的性能提升。

Conclusion: MLLM推理的可靠性可与轻量级融合网络的领域适应性协同，实现稳健、可扩展的情感识别。

Abstract: This study investigates the integration of trustworthy prior reasoning
knowledge from MLLMs into multimodal emotion recognition. We employ Gemini to
generate fine-grained, modality-separable reasoning traces, which are injected
as priors during the fusion stage to enrich cross-modal interactions. To
mitigate the pronounced class-imbalance in multimodal emotion recognition, we
introduce Balanced Dual-Contrastive Learning, a loss formulation that jointly
balances inter-class and intra-class distributions. Applied to the MER2024
benchmark, our prior-enhanced framework yields substantial performance gains,
demonstrating that the reliability of MLLM-derived reasoning can be
synergistically combined with the domain adaptability of lightweight fusion
networks for robust, scalable emotion recognition.

</details>


### [218] [StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization](https://arxiv.org/abs/2508.03735)
*Gopalji Gaur,Mohammadreza Zolfaghari,Thomas Brox*

Main category: cs.CV

TL;DR: 本文提出无训练方法解决文本到图像扩散模型生成连贯图像序列时主体一致性问题，实验证明有效且保持模型创造力。


<details>
  <summary>Details</summary>
Motivation: 现有解决文本到图像扩散模型生成连贯图像序列时主体一致性问题的方法计算成本高、耗时长且影响模型已有能力。

Method: 采用无训练方法，引入掩码跨图像注意力共享动态对齐图像主体特征，使用区域特征协调细化视觉相似细节。

Result: 在多种场景下成功生成视觉上一致的主体，且保持了扩散模型的创造力。

Conclusion: 所提方法能有效解决主体一致性问题，且不影响模型创造力。

Abstract: Generating a coherent sequence of images that tells a visual story, using
text-to-image diffusion models, often faces the critical challenge of
maintaining subject consistency across all story scenes. Existing approaches,
which typically rely on fine-tuning or retraining models, are computationally
expensive, time-consuming, and often interfere with the model's pre-existing
capabilities. In this paper, we follow a training-free approach and propose an
efficient consistent-subject-generation method. This approach works seamlessly
with pre-trained diffusion models by introducing masked cross-image attention
sharing to dynamically align subject features across a batch of images, and
Regional Feature Harmonization to refine visually similar details for improved
subject consistency. Experimental results demonstrate that our approach
successfully generates visually consistent subjects across a variety of
scenarios while maintaining the creative abilities of the diffusion model.

</details>


### [219] [Fusion of Pervasive RF Data with Spatial Images via Vision Transformers for Enhanced Mapping in Smart Cities](https://arxiv.org/abs/2508.03736)
*Rafayel Mkrtchyan,Armen Manukyan,Hrant Khachatrian,Theofanis P. Raptis*

Main category: cs.CV

TL;DR: 本文提出基于深度学习的方法，结合开源地图与射频数据改进建筑映射，在合成数据集上评估，性能超多个基线。


<details>
  <summary>Details</summary>
Motivation: 传统智慧城市映射技术有成本、可及性和准确性局限，开源映射平台数据有偏差影响神经网络性能。

Method: 提出基于深度学习的方法，集成DINOv2架构，结合开源平台地图与射频数据，用视觉变压器架构统一处理，利用合成数据集训练模型，按三个指标评估。

Result: 设计实现65.3%的宏观IoU，显著超过错误地图基线、仅用射频数据方法和非AI融合基线。

Conclusion: 所提方法能有效结合开源地图与射频数据，提高建筑映射准确性。

Abstract: Environment mapping is an important computing task for a wide range of smart
city applications, including autonomous navigation, wireless network operations
and extended reality environments. Conventional smart city mapping techniques,
such as satellite imagery, LiDAR scans, and manual annotations, often suffer
from limitations related to cost, accessibility and accuracy. Open-source
mapping platforms have been widely utilized in artificial intelligence
applications for environment mapping, serving as a source of ground truth.
However, human errors and the evolving nature of real-world environments
introduce biases that can negatively impact the performance of neural networks
trained on such data. In this paper, we present a deep learning-based approach
that integrates the DINOv2 architecture to improve building mapping by
combining maps from open-source platforms with radio frequency (RF) data
collected from multiple wireless user equipments and base stations. Our
approach leverages a vision transformer-based architecture to jointly process
both RF and map modalities within a unified framework, effectively capturing
spatial dependencies and structural priors for enhanced mapping accuracy. For
the evaluation purposes, we employ a synthetic dataset co-produced by Huawei.
We develop and train a model that leverages only aggregated path loss
information to tackle the mapping problem. We measure the results according to
three performance metrics which capture different qualities: (i) The Jaccard
index, also known as intersection over union (IoU), (ii) the Hausdorff
distance, and (iii) the Chamfer distance. Our design achieves a macro IoU of
65.3%, significantly surpassing (i) the erroneous maps baseline, which yields
40.1%, (ii) an RF-only method from the literature, which yields 37.3%, and
(iii) a non-AI fusion baseline that we designed which yields 42.2%.

</details>


### [220] [VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission](https://arxiv.org/abs/2508.03740)
*Jianqiao Chen,Tingting Zhu,Huishi Song,Nan Ma,Xiaodong Xu*

Main category: cs.CV

TL;DR: 本文提出VQ - DeepISC数字语义通信系统，经实验验证其重建保真度优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 语义特征数字化存在需在压缩为离散符号时保留连续性和上下文、保证对信道退化鲁棒性的难题，为解决该问题开展研究。

Method: 以深度联合源信道编码为指导，设计Swin Transformer骨干网络提取语义特征，用VQ模块投影特征；开发注意力机制驱动的信道自适应模块；通过最小化Kullback - Leibler散度施加分布正则化，用指数移动平均稳定训练；采用正交相移键控调制和正交频分复用实现数字通信。

Result: 实验结果表明，所提出系统的重建保真度优于基准方法。

Conclusion: 提出的VQ - DeepISC数字语义通信系统有效，在重建保真度上表现良好。

Abstract: Discretization of semantic features enables interoperability between semantic
and digital communication systems, showing significant potential for practical
applications. The fundamental difficulty in digitizing semantic features stems
from the need to preserve continuity and context in inherently analog
representations during their compression into discrete symbols while ensuring
robustness to channel degradation. In this paper, we propose a vector quantized
(VQ)-enabled digital semantic communication system with channel adaptive image
transmission, named VQ-DeepISC. Guided by deep joint source-channel coding
(DJSCC), we first design a Swin Transformer backbone for hierarchical semantic
feature extraction, followed by VQ modules projecting features into discrete
latent spaces. Consequently, it enables efficient index-based transmission
instead of raw feature transmission. To further optimize this process, we
develop an attention mechanism-driven channel adaptation module to dynamically
optimize index transmission. Secondly, to counteract codebook collapse during
training process, we impose a distributional regularization by minimizing the
Kullback-Leibler divergence (KLD) between codeword usage frequencies and a
uniform prior. Meanwhile, exponential moving average (EMA) is employed to
stabilize training and ensure balanced feature coverage during codebook
updates. Finally, digital communication is implemented using quadrature phase
shift keying (QPSK) modulation alongside orthogonal frequency division
multiplexing (OFDM), adhering to the IEEE 802.11a standard. Experimental
results demonstrate superior reconstruction fidelity of the proposed system
over benchmark methods.

</details>


### [221] [Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision](https://arxiv.org/abs/2508.03745)
*Wenwen Li,Chia-Yu Hsu,Maosheng Hu*

Main category: cs.CV

TL;DR: 本文开发了一种弱监督深度学习模型用于地理空间目标检测，提出三项贡献并应用于火星撞击坑检测，推动了GeoAI的理论和方法基础。


<details>
  <summary>Details</summary>
Motivation: 当前GeoAI存在训练数据缺乏、AI模型设计忽视空间原则和效应等问题，阻碍AI与地理空间研究的深度融合。

Method: 提出仅使用弱标签的目标检测方法，基于托布勒地理学第一定律开发空间显式模型；将注意力图纳入目标检测流程并制定多阶段训练策略。

Result: 将模型应用于火星撞击坑检测，该模型可泛化到地球和其他行星表面的自然和人造特征。

Conclusion: 本研究推进了GeoAI的理论和方法基础。

Abstract: Recent interest in geospatial artificial intelligence (GeoAI) has fostered a
wide range of applications using artificial intelligence (AI), especially deep
learning, for geospatial problem solving. However, major challenges such as a
lack of training data and the neglect of spatial principles and spatial effects
in AI model design remain, significantly hindering the in-depth integration of
AI with geospatial research. This paper reports our work in developing a deep
learning model that enables object detection, particularly of natural features,
in a weakly supervised manner. Our work makes three contributions: First, we
present a method of object detection using only weak labels. This is achieved
by developing a spatially explicit model based on Tobler's first law of
geography. Second, we incorporate attention maps into the object detection
pipeline and develop a multistage training strategy to improve performance.
Third, we apply this model to detect impact craters on Mars, a task that
previously required extensive manual effort. The model generalizes to both
natural and human-made features on the surfaces of Earth and other planets.
This research advances the theoretical and methodological foundations of GeoAI.

</details>


### [222] [Refine-IQA: Multi-Stage Reinforcement Finetuning for Perceptual Image Quality Assessment](https://arxiv.org/abs/2508.03763)
*Ziheng Jia,Jiaying Qian,Zicheng Zhang,Zijian Chen,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出多阶段强化微调IQA框架Refine - IQA，在感知和评分任务上表现出色，激活强大‘思考’能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于RFT的IQA方法对‘思考’过程无奖励监督，且未明确增强模型原生低级别视觉质量感知，限制了性能上限。

Method: 第一阶段构建Refine - Perception - 20K数据集并设计多任务奖励函数；第二阶段引入概率差异奖励策略监督‘思考’过程。

Result: Refine - IQA系列模型在感知和评分任务上取得出色表现，在质量解释基准上也有优异结果。

Conclusion: 提出的Refine - IQA框架有效，能提升模型在IQA任务中的性能。

Abstract: Reinforcement fine-tuning (RFT) is a proliferating paradigm for LMM training.
Analogous to high-level reasoning tasks, RFT is similarly applicable to
low-level vision domains, including image quality assessment (IQA). Existing
RFT-based IQA methods typically use rule-based output rewards to verify the
model's rollouts but provide no reward supervision for the "think" process,
leaving its correctness and efficacy uncontrolled. Furthermore, these methods
typically fine-tune directly on downstream IQA tasks without explicitly
enhancing the model's native low-level visual quality perception, which may
constrain its performance upper bound. In response to these gaps, we propose
the multi-stage RFT IQA framework (Refine-IQA). In Stage-1, we build the
Refine-Perception-20K dataset (with 12 main distortions, 20,907
locally-distorted images, and over 55K RFT samples) and design multi-task
reward functions to strengthen the model's visual quality perception. In
Stage-2, targeting the quality scoring task, we introduce a probability
difference reward involved strategy for "think" process supervision. The
resulting Refine-IQA Series Models achieve outstanding performance on both
perception and scoring tasks-and, notably, our paradigm activates a robust
"think" (quality interpreting) capability that also attains exceptional results
on the corresponding quality interpreting benchmark.

</details>


### [223] [4D-PreNet: A Unified Preprocessing Framework for 4D-STEM Data Analysis](https://arxiv.org/abs/2508.03775)
*Mingyu Liu,Zian Mao,Zhu Liu,Haoran Zhang,Jintao Guo,Xiaoya He,Xi Huang,Shufen Chu,Chun Cheng,Jun Ding,Yujun Xie*

Main category: cs.CV

TL;DR: 提出4D - PreNet深度学习管道解决4D - STEM数据预处理瓶颈，效果优于传统算法。


<details>
  <summary>Details</summary>
Motivation: 4D - STEM数据预处理存在瓶颈，传统校正算法缺乏通用性。

Method: 提出4D - PreNet，结合注意力增强U - Net和ResNet架构，在大量模拟数据集上训练。

Result: 管道降噪时均方误差最多降低50%，中心检测任务平均误差低于0.04像素，优于传统算法。

Conclusion: 4D - PreNet有助于实现4D - STEM实时分析和自动表征。

Abstract: Automated experimentation with real time data analysis in scanning
transmission electron microscopy (STEM) often require end-to-end framework. The
four-dimensional scanning transmission electron microscopy (4D-STEM) with
high-throughput data acquisition has been constrained by the critical
bottleneck results from data preprocessing. Pervasive noise, beam center drift,
and elliptical distortions during high-throughput acquisition inevitably
corrupt diffraction patterns, systematically biasing quantitative measurements.
Yet, conventional correction algorithms are often material-specific and fail to
provide a robust, generalizable solution. In this work, we present 4D-PreNet,
an end-to-end deep-learning pipeline that integrates attention-enhanced U-Net
and ResNet architectures to simultaneously perform denoising, center
correction, and elliptical distortion calibration. The network is trained on
large, simulated datasets encompassing a wide range of noise levels, drift
magnitudes, and distortion types, enabling it to generalize effectively to
experimental data acquired under varying conditions. Quantitative evaluations
demonstrate that our pipeline reduces mean squared error by up to 50% during
denoising and achieves sub-pixel center localization in the center detection
task, with average errors below 0.04 pixels. The outputs are bench-marked
against traditional algorithms, highlighting improvements in both noise
suppression and restoration of diffraction patterns, thereby facilitating
high-throughput, reliable 4D-STEM real-time analysis for automated
characterization.

</details>


### [224] [Deep learning framework for crater detection and identification on the Moon and Mars](https://arxiv.org/abs/2508.03920)
*Yihan Ma,Zeyang Yu,Rohitash Chandra*

Main category: cs.CV

TL;DR: 本文应用深度学习模型进行撞击坑检测识别，采用两阶段框架，对火星和月球特定区域检测，结果显示YOLO检测性能均衡，ResNet - 50识别大撞击坑精度高。


<details>
  <summary>Details</summary>
Motivation: 撞击坑对行星科学研究意义重大，深度学习发展引发自动撞击坑检测兴趣，本文旨在应用深度学习模型进行撞击坑检测识别。

Method: 应用CNN、YOLO和ResNet等新型模型，采用两阶段框架，第一阶段用简单经典CNN、ResNet - 50和YOLO进行撞击坑识别，第二阶段用基于YOLO的检测进行定位。

Result: YOLO展示了最均衡的撞击坑检测性能，ResNet - 50在高精度识别大撞击坑方面表现出色。

Conclusion: 不同深度学习模型在撞击坑检测和识别中有不同优势，可根据需求选择合适模型。

Abstract: Impact craters are among the most prominent geomorphological features on
planetary surfaces and are of substantial significance in planetary science
research. Their spatial distribution and morphological characteristics provide
critical information on planetary surface composition, geological history, and
impact processes. In recent years, the rapid advancement of deep learning
models has fostered significant interest in automated crater detection. In this
paper, we apply advancements in deep learning models for impact crater
detection and identification. We use novel models, including Convolutional
Neural Networks (CNNs) and variants such as YOLO and ResNet. We present a
framework that features a two-stage approach where the first stage features
crater identification using simple classic CNN, ResNet-50 and YOLO. In the
second stage, our framework employs YOLO-based detection for crater
localisation. Therefore, we detect and identify different types of craters and
present a summary report with remote sensing data for a selected region. We
consider selected regions for craters and identification from Mars and the Moon
based on remote sensing data. Our results indicate that YOLO demonstrates the
most balanced crater detection performance, while ResNet-50 excels in
identifying large craters with high precision.

</details>


### [225] [Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location Selection for Prostate Cancer Localisation](https://arxiv.org/abs/2508.03953)
*Xiangcen Wu,Shaheer U. Saeed,Yipei Wang,Ester Bonmati Coll,Yipeng Hu*

Main category: cs.CV

TL;DR: 提出推荐系统辅助机器学习分割模型，提升前列腺癌分割性能，实验验证其效果好且有交互应用潜力。


<details>
  <summary>Details</summary>
Motivation: 放射科医生读片策略多样，为最大化前列腺癌分割性能，提出推荐系统辅助分割模型。

Method: 训练策略网络推荐最佳成像模式和感兴趣区域，预训练分割网络模仿放射科医生检查，动态决策过程迭代定位癌症。

Result: 使用1325个标记的多参数MRI图像数据集验证，能提高标注效率和分割准确性，超越标准分割网络，训练的智能体有自己的策略。

Conclusion: 方法有提升分割性能的潜力，且策略网络可辅助人类放射科医生，有交互应用前景。

Abstract: Radiologists often mix medical image reading strategies, including inspection
of individual modalities and local image regions, using information at
different locations from different images independently as well as
concurrently. In this paper, we propose a recommend system to assist machine
learning-based segmentation models, by suggesting appropriate image portions
along with the best modality, such that prostate cancer segmentation
performance can be maximised. Our approach trains a policy network that assists
tumor localisation, by recommending both the optimal imaging modality and the
specific sections of interest for review. During training, a pre-trained
segmentation network mimics radiologist inspection on individual or variable
combinations of these imaging modalities and their sections - selected by the
policy network. Taking the locally segmented regions as an input for the next
step, this dynamic decision making process iterates until all cancers are best
localised. We validate our method using a data set of 1325 labelled
multiparametric MRI images from prostate cancer patients, demonstrating its
potential to improve annotation efficiency and segmentation accuracy,
especially when challenging pathology is present. Experimental results show
that our approach can surpass standard segmentation networks. Perhaps more
interestingly, our trained agent independently developed its own optimal
strategy, which may or may not be consistent with current radiologist
guidelines such as PI-RADS. This observation also suggests a promising
interactive application, in which the proposed policy networks assist human
radiologists.

</details>


### [226] [CORE-ReID V2: Advancing the Domain Adaptation for Object Re-Identification with Optimized Training and Ensemble Fusion](https://arxiv.org/abs/2508.04036)
*Trinh Quoc Nguyen,Oky Dicky Ardiansyah Prima,Syahid Al Irfan,Hindriyanto Dwi Purnomo,Radius Tanone*

Main category: cs.CV

TL;DR: 本文提出CORE - ReID V2框架，解决无监督域适应挑战，实验表现优于现有方法，支持轻量级骨干网络。


<details>
  <summary>Details</summary>
Motivation: 解决Person ReID、Vehicle ReID和Object ReID中的无监督域适应（UDA）挑战。

Method: 预训练用CycleGAN合成数据缩小域间图像特征差距；微调时用由ECAB和SECAB组成的集成融合机制增强特征表示并减少伪标签歧义。

Result: 在UDA Person ReID和Vehicle ReID数据集上表现优于现有方法，在mAP和Rank - k Accuracy上达到顶尖水平，支持轻量级骨干网络。

Conclusion: 推动了基于UDA的Object ReID发展，为该领域研究提供基础，代码和模型开源。

Abstract: This study presents CORE-ReID V2, an enhanced framework building upon
CORE-ReID. The new framework extends its predecessor by addressing Unsupervised
Domain Adaptation (UDA) challenges in Person ReID and Vehicle ReID, with
further applicability to Object ReID. During pre-training, CycleGAN is employed
to synthesize diverse data, bridging image characteristic gaps across different
domains. In the fine-tuning, an advanced ensemble fusion mechanism, consisting
of the Efficient Channel Attention Block (ECAB) and the Simplified Efficient
Channel Attention Block (SECAB), enhances both local and global feature
representations while reducing ambiguity in pseudo-labels for target samples.
Experimental results on widely used UDA Person ReID and Vehicle ReID datasets
demonstrate that the proposed framework outperforms state-of-the-art methods,
achieving top performance in Mean Average Precision (mAP) and Rank-k Accuracy
(Top-1, Top-5, Top-10). Moreover, the framework supports lightweight backbones
such as ResNet18 and ResNet34, ensuring both scalability and efficiency. Our
work not only pushes the boundaries of UDA-based Object ReID but also provides
a solid foundation for further research and advancements in this domain. Our
codes and models are available at
https://github.com/TrinhQuocNguyen/CORE-ReID-V2.

</details>


### [227] [DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting](https://arxiv.org/abs/2508.04099)
*Zexu Huang,Min Xu,Stuart Perry*

Main category: cs.CV

TL;DR: 提出DET - GS框架用于3D高斯溅射，提升稀疏视图下几何重建精度和视觉保真度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射方法在稀疏视图条件下进行准确几何重建存在挑战，如非局部深度正则化无法捕捉细粒度结构、传统平滑方法忽略语义边界。

Method: 提出DET - GS统一框架，包括分层几何深度监督框架、基于Canny边缘检测语义掩码的边缘感知深度正则化、RGB引导的边缘保留总变分损失。

Result: 在稀疏视图新视图合成基准测试中，DET - GS在几何精度和视觉保真度上有显著提升，优于现有方法。

Conclusion: DET - GS有效解决了现有3D高斯溅射方法在稀疏视图下的重建问题，具有更好的性能。

Abstract: 3D Gaussian Splatting (3DGS) represents a significant advancement in the
field of efficient and high-fidelity novel view synthesis. Despite recent
progress, achieving accurate geometric reconstruction under sparse-view
conditions remains a fundamental challenge. Existing methods often rely on
non-local depth regularization, which fails to capture fine-grained structures
and is highly sensitive to depth estimation noise. Furthermore, traditional
smoothing methods neglect semantic boundaries and indiscriminately degrade
essential edges and textures, consequently limiting the overall quality of
reconstruction. In this work, we propose DET-GS, a unified depth and edge-aware
regularization framework for 3D Gaussian Splatting. DET-GS introduces a
hierarchical geometric depth supervision framework that adaptively enforces
multi-level geometric consistency, significantly enhancing structural fidelity
and robustness against depth estimation noise. To preserve scene boundaries, we
design an edge-aware depth regularization guided by semantic masks derived from
Canny edge detection. Furthermore, we introduce an RGB-guided edge-preserving
Total Variation loss that selectively smooths homogeneous regions while
rigorously retaining high-frequency details and textures. Extensive experiments
demonstrate that DET-GS achieves substantial improvements in both geometric
accuracy and visual fidelity, outperforming state-of-the-art (SOTA) methods on
sparse-view novel view synthesis benchmarks.

</details>


### [228] [Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model](https://arxiv.org/abs/2508.03925)
*Shen Zhu,Yinzhu Jin,Ifrah Zawar,P. Thomas Fletcher*

Main category: cs.CV

TL;DR: 提出能生成带对应关系的基于点的形状表示的扩散模型，实验显示其生成效果好且可用于下游任务。


<details>
  <summary>Details</summary>
Motivation: 传统统计形状模型重视点对应关系，但当前深度学习方法多关注无序点云，未考虑生成形状间的点对应关系，因此要构建能生成保留点对应关系的真实形状表示的扩散模型。

Method: 利用从OASIS - 3获取的带对应关系的形状表示数据，构建对应关系保留模型。

Result: 对应关系保留模型能有效生成高度逼真的基于点的海马形状表示，优于现有方法。

Conclusion: 该生成模型可用于下游任务，如健康和AD受试者的条件生成以及通过反事实生成预测疾病进展的形态变化。

Abstract: We propose a diffusion model designed to generate point-based shape
representations with correspondences. Traditional statistical shape models have
considered point correspondences extensively, but current deep learning methods
do not take them into account, focusing on unordered point clouds instead.
Current deep generative models for point clouds do not address generating
shapes with point correspondences between generated shapes. This work aims to
formulate a diffusion model that is capable of generating realistic point-based
shape representations, which preserve point correspondences that are present in
the training data. Using shape representation data with correspondences derived
from Open Access Series of Imaging Studies 3 (OASIS-3), we demonstrate that our
correspondence-preserving model effectively generates point-based hippocampal
shape representations that are highly realistic compared to existing methods.
We further demonstrate the applications of our generative model by downstream
tasks, such as conditional generation of healthy and AD subjects and predicting
morphological changes of disease progression by counterfactual generation.

</details>


### [229] [Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decode](https://arxiv.org/abs/2508.04107)
*Jingchao Wang,Zhijian Wu,Dingjiang Huang,Yefeng Zheng,Hong Wang*

Main category: cs.CV

TL;DR: 提出MLLMSeg框架解决RES任务中性能与成本的权衡问题，实验显示其表现优。


<details>
  <summary>Details</summary>
Motivation: 现有RES方法在性能与成本间难以平衡，MLLMs的token - generation范式不利于像素级预测，且现有方法要么参数多，要么牺牲精度。

Method: 提出MLLMSeg框架，利用MLLM视觉编码器特征；提出DSFF模块融合视觉与语义特征；建立轻量级掩码解码器。

Result: 所提方法在实验中总体超越基于SAM和无SAM的竞争方法。

Conclusion: MLLMSeg框架能在性能和成本间取得更好平衡。

Abstract: Reference Expression Segmentation (RES) aims to segment image regions
specified by referring expressions and has become popular with the rise of
multimodal large models (MLLMs). While MLLMs excel in semantic understanding,
their token-generation paradigm struggles with pixel-level dense prediction.
Existing RES methods either couple MLLMs with the parameter-heavy Segment
Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight
pipelines that sacrifice accuracy. To address the trade-off between performance
and cost, we specifically propose MLLMSeg, a novel framework that fully
exploits the inherent visual detail features encoded in the MLLM vision encoder
without introducing an extra visual encoder. Besides, we propose a
detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully
integrates the detail-related visual feature with the semantic-related feature
output by the large language model (LLM) of MLLM. Finally, we establish a
light-weight mask decoder with only 34M network parameters that optimally
leverages detailed spatial features from the visual encoder and semantic
features from the LLM to achieve precise mask prediction. Extensive experiments
demonstrate that our method generally surpasses both SAM-based and SAM-free
competitors, striking a better balance between performance and cost. Code is
available at https://github.com/jcwang0602/MLLMSeg.

</details>


### [230] [DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image Segmentation](https://arxiv.org/abs/2508.04131)
*Zhaohong Huang,Yuxin Zhang,Mingbao Lin,Taojian Zhou,Guorong Cai,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出Detail - Semantic Deep Supervision Network (DS²Net)用于医学图像分割，结合细节与语义特征监督，有新的监督损失，实验表现优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有工作仅单独监督粗粒度语义特征或细粒度细节特征，未考虑二者在医学图像分析中的重要关系。

Method: 提出DS²Net，通过Detail Enhance Module (DEM)和Semantic Enhance Module (SEM)分别监督低层次细节和高层次语义特征，采用基于不确定性的监督损失。

Result: 在六种不同设备采集的基准数据集上，DS²Net始终优于现有SOTA方法。

Conclusion: DS²Net在医学图像分析中有效，多视图深度监督及基于不确定性的监督损失有优势。

Abstract: Deep Supervision Networks exhibit significant efficacy for the medical
imaging community. Nevertheless, existing work merely supervises either the
coarse-grained semantic features or fine-grained detailed features in
isolation, which compromises the fact that these two types of features hold
vital relationships in medical image analysis. We advocate the powers of
complementary feature supervision for medical image segmentation, by proposing
a Detail-Semantic Deep Supervision Network (DS$^2$Net). DS$^2$Net navigates
both low-level detailed and high-level semantic feature supervision through
Detail Enhance Module (DEM) and Semantic Enhance Module (SEM). DEM and SEM
respectively harness low-level and high-level feature maps to create detail and
semantic masks for enhancing feature supervision. This is a novel shift from
single-view deep supervision to multi-view deep supervision. DS$^2$Net is also
equipped with a novel uncertainty-based supervision loss that adaptively
assigns the supervision strength of features within distinct scales based on
their uncertainty, thus circumventing the sub-optimal heuristic design that
typifies previous works. Through extensive experiments on six benchmarks
captured under either colonoscopy, ultrasound and microscope, we demonstrate
that DS$^2$Net consistently outperforms state-of-the-art methods for medical
image analysis.

</details>


### [231] [UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval](https://arxiv.org/abs/2508.04136)
*Hongyu Guo,Kuan Zhu,Xiangzhao Hao,Haiyun Guo,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出通用免训练框架UniFGVC解决少样本细粒度视觉分类问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有微调预训练视觉语言模型的少样本细粒度视觉分类方法存在过拟合和泛化能力弱的问题。

Method: 将少样本细粒度视觉分类重构为多模态检索，提出CDV - Captioner生成结构化文本描述，构建多模态类别模板，使用现成的视觉和文本编码器嵌入查询和模板对，通过检索最近模板完成分类。

Result: 在12个细粒度视觉分类基准测试中，UniFGVC始终优于先前基于少样本CLIP的方法，甚至优于一些基于全监督大语言模型的方法。

Conclusion: UniFGVC与多种大语言模型和编码器具有广泛兼容性，在少样本细粒度视觉分类场景中具有可靠的泛化性和适应性。

Abstract: Few-shot fine-grained visual classification (FGVC) aims to leverage limited
data to enable models to discriminate subtly distinct categories. Recent works
mostly finetuned the pre-trained visual language models to achieve performance
gain, yet suffering from overfitting and weak generalization. To deal with
this, we introduce UniFGVC, a universal training-free framework that
reformulates few-shot FGVC as multimodal retrieval. First, we propose the
Category-Discriminative Visual Captioner (CDV-Captioner) to exploit the
open-world knowledge of multimodal large language models (MLLMs) to generate a
structured text description that captures the fine-grained attribute features
distinguishing closely related classes. CDV-Captioner uses chain-of-thought
prompting and visually similar reference images to reduce hallucination and
enhance discrimination of generated captions. Using it we can convert each
image into an image-description pair, enabling more comprehensive feature
representation, and construct the multimodal category templates using few-shot
samples for the subsequent retrieval pipeline. Then, off-the-shelf vision and
text encoders embed query and template pairs, and FGVC is accomplished by
retrieving the nearest template in the joint space. UniFGVC ensures broad
compatibility with diverse MLLMs and encoders, offering reliable generalization
and adaptability across few-shot FGVC scenarios. Extensive experiments on 12
FGVC benchmarks demonstrate its consistent superiority over prior few-shot
CLIP-based methods and even several fully-supervised MLLMs-based approaches.

</details>


### [232] [Learning Using Privileged Information for Litter Detection](https://arxiv.org/abs/2508.04124)
*Matthias Bartolo,Konstantinos Makantasis,Dylan Seychell*

Main category: cs.CV

TL;DR: 提出结合特权信息与深度学习目标检测的新方法提升垃圾检测效果，实验证明能提高性能且保持高效。


<details>
  <summary>Details</summary>
Motivation: 全球垃圾污染上升，需开发有效检测垃圾的自动化工具。

Method: 首次结合特权信息与深度学习目标检测，对五种目标检测模型评估，将边界框信息编码为二进制掩码来优化检测。

Result: 在SODA、BDW和UAVVaste数据集上实验，所有模型性能持续提升，训练集检测精度提高且能泛化到其他场景，未增加模型复杂度和层数。

Conclusion: 该方法为垃圾检测提供实用方案，在现实应用中平衡了准确性和效率。

Abstract: As litter pollution continues to rise globally, developing automated tools
capable of detecting litter effectively remains a significant challenge. This
study presents a novel approach that combines, for the first time, privileged
information with deep learning object detection to improve litter detection
while maintaining model efficiency. We evaluate our method across five widely
used object detection models, addressing challenges such as detecting small
litter and objects partially obscured by grass or stones. In addition to this,
a key contribution of our work can also be attributed to formulating a means of
encoding bounding box information as a binary mask, which can be fed to the
detection model to refine detection guidance. Through experiments on both
within-dataset evaluation on the renowned SODA dataset and cross-dataset
evaluation on the BDW and UAVVaste litter detection datasets, we demonstrate
consistent performance improvements across all models. Our approach not only
bolsters detection accuracy within the training sets but also generalises well
to other litter detection contexts. Crucially, these improvements are achieved
without increasing model complexity or adding extra layers, ensuring
computational efficiency and scalability. Our results suggest that this
methodology offers a practical solution for litter detection, balancing
accuracy and efficiency in real-world applications.

</details>


### [233] [Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective](https://arxiv.org/abs/2508.04197)
*Yan Zhang,Gangyan Zeng,Daiqing Wu,Huawen Shen,Binbin Li,Yu Zhou,Can Ma,Xiaojun Bi*

Main category: cs.CV

TL;DR: 本文从面向实例角度重新思考Video TextVQA任务，提出GAT模型，实验验证其在准确率和推理速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Video TextVQA领域的帧级框架存在冗余文本实体和隐式关系建模问题，导致准确率和效率受限。

Method: 提出GAT模型，设计上下文聚合实例收集模块获取准确文本实例读取结果，利用实例聚焦轨迹跟踪模块建立实例时空关系并推断答案。

Result: 在多个公共数据集上实验，GAT在准确率和推理速度上超越现有Video TextVQA方法、视频语言预训练方法和视频大语言模型，准确率超之前最优方法3.86%，推理速度比视频大语言模型快10倍。

Conclusion: GAT模型有效且具有泛化性，代码开源。

Abstract: Video text-based visual question answering (Video TextVQA) aims to answer
questions by explicitly reading and reasoning about the text involved in a
video. Most works in this field follow a frame-level framework which suffers
from redundant text entities and implicit relation modeling, resulting in
limitations in both accuracy and efficiency. In this paper, we rethink the
Video TextVQA task from an instance-oriented perspective and propose a novel
model termed GAT (Gather and Trace). First, to obtain accurate reading result
for each video text instance, a context-aggregated instance gathering module is
designed to integrate the visual appearance, layout characteristics, and
textual contents of the related entities into a unified textual representation.
Then, to capture dynamic evolution of text in the video flow, an
instance-focused trajectory tracing module is utilized to establish
spatio-temporal relationships between instances and infer the final answer.
Extensive experiments on several public Video TextVQA datasets validate the
effectiveness and generalization of our framework. GAT outperforms existing
Video TextVQA methods, video-language pretraining methods, and video large
language models in both accuracy and inference speed. Notably, GAT surpasses
the previous state-of-the-art Video TextVQA methods by 3.86\% in accuracy and
achieves ten times of faster inference speed than video large language models.
The source code is available at https://github.com/zhangyan-ucas/GAT.

</details>


### [234] [ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs](https://arxiv.org/abs/2508.04201)
*Ben Zhang,LuLu Yu,Lei Gao,Jing Liu,QuanJiang Guo,Hui Gao*

Main category: cs.CV

TL;DR: 提出ViFP框架增强视觉推理可靠性，构建模板、多轮问答、分析一致性、引入CoT机制，还引入VoC评估指标，实验证明其在多数据集提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型推理中存在FP推理问题，基于特定数据集和强化学习策略训练成本高、泛化性有限。

Method: 构建基于视觉推理核心维度的子问题模板，通过多轮问答构建有效推理路径，动态分析推理路径一致性，引入针对性CoT机制，引入VoC评估指标。

Result: 在A - OKVQA、OKVQA和FVQA三个数据集上，ViFP持续提升性能，在A - OKVQA上准确率最高提升5.4%，超过先前SOTA 4.3%，显著减少FP数量。

Conclusion: ViFP框架能有效增强视觉推理的可靠性。

Abstract: In visual-language model (VLM) reasoning, false positive(FP) reasoning occurs
when a model generates a correct answer but follows an incorrect reasoning
path. Existing methods based on specific multi-step reasoning datasets and
reinforcement learning strategies, leading to high training costs and limited
generalization. In this work, we propose ViFP, a general framework for
enhancing visual reasoning reliability. It improves both answer accuracy and
reasoning soundness by detecting FPs. ViFP tackles the limitations of dataset
dependency and poor generalization by constructing sub-question templates
grounded in the core dimensions of visual reasoning, such as object
localization, characteristic description, and object discovery. ViFP then
builds effective reasoning paths via multi-turn QA to improve reasoning
accuracy. Meanwhile, ViFP dynamically analyzes the consistency of reasoning
path to identify potential FPs, and introduces a targeted chain-of-thought
(CoT) mechanism that adaptively guides both FP and non-FP samples. Thereby
reducing logical errors in the reasoning path while preserving accuracy.
Finally, we introduce a reliability evaluation metric-VoC, which integrates
answer accuracy and the FP rate, providing a quantitative tool to assess
whether a VLM not only answers correctly, but also reasons reliably. Our
experiments on closed-source VLMs show that ViFP consistently improves
performance across three datasets: A-OKVQA, OKVQA, and FVQA. On A-OKVQA, ViFP
improves accuracy by up to 5.4%, surpassing the previous state-of-the-art by
4.3%, and significantly reduces the number of FPs, validating its benefits in
enhancing reasoning reliability.

</details>


### [235] [Bootstrap Deep Spectral Clustering with Optimal Transport](https://arxiv.org/abs/2508.04200)
*Wengang Guo,Wei Ye,Chunchun Chen,Xin Sun,Christian Böhm,Claudia Plant,Susanto Rahardja*

Main category: cs.CV

TL;DR: 提出深度谱聚类模型BootSC，端到端学习谱聚类各阶段，利用监督和重参数化技术，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决谱聚类不连贯优化过程和有限表示能力的问题。

Method: 提出BootSC模型，端到端学习谱聚类各阶段，利用最优传输监督，引入正交重参数化技术。

Result: BootSC在实验中取得SOTA聚类性能，如在ImageNet - Dogs数据集上比次优方法NMI提升16%。

Conclusion: BootSC是有效的谱聚类模型，能显著提升聚类性能。

Abstract: Spectral clustering is a leading clustering method. Two of its major
shortcomings are the disjoint optimization process and the limited
representation capacity. To address these issues, we propose a deep spectral
clustering model (named BootSC), which jointly learns all stages of spectral
clustering -- affinity matrix construction, spectral embedding, and $k$-means
clustering -- using a single network in an end-to-end manner. BootSC leverages
effective and efficient optimal-transport-derived supervision to bootstrap the
affinity matrix and the cluster assignment matrix. Moreover, a
semantically-consistent orthogonal re-parameterization technique is introduced
to orthogonalize spectral embeddings, significantly enhancing the
discrimination capability. Experimental results indicate that BootSC achieves
state-of-the-art clustering performance. For example, it accomplishes a notable
16\% NMI improvement over the runner-up method on the challenging ImageNet-Dogs
dataset. Our code is available at https://github.com/spdj2271/BootSC.

</details>


### [236] [LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation](https://arxiv.org/abs/2508.04228)
*Kangrui Cen,Baixuan Zhao,Yi Xin,Siqi Luo,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 提出LayerT2V方法解决T2V多对象运动控制难题，实验显示性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: T2V领域多对象运动控制是挑战，现有模型和方法在多对象任务中表现不佳，存在语义冲突等问题。

Method: 提出LayerT2V方法，通过逐层合成背景和前景对象生成视频，将各元素置于不同层。

Result: LayerT2V在生成复杂多对象场景上表现优越，mIoU和AP50指标较SOTA方法分别提升1.4倍和4.5倍。

Conclusion: LayerT2V能有效解决T2V多对象运动控制问题，提升多对象合成的效果和生成过程的可控性。

Abstract: Controlling object motion trajectories in Text-to-Video (T2V) generation is a
challenging and relatively under-explored area, particularly in scenarios
involving multiple moving objects. Most community models and datasets in the
T2V domain are designed for single-object motion, limiting the performance of
current generative models in multi-object tasks. Additionally, existing motion
control methods in T2V either lack support for multi-object motion scenes or
experience severe performance degradation when object trajectories intersect,
primarily due to the semantic conflicts in colliding regions. To address these
limitations, we introduce LayerT2V, the first approach for generating video by
compositing background and foreground objects layer by layer. This layered
generation enables flexible integration of multiple independent elements within
a video, positioning each element on a distinct "layer" and thus facilitating
coherent multi-object synthesis while enhancing control over the generation
process. Extensive experiments demonstrate the superiority of LayerT2V in
generating complex multi-object scenarios, showcasing 1.4x and 4.5x
improvements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods.
Project page and code are available at https://kr-panghu.github.io/LayerT2V/ .

</details>


### [237] [Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting](https://arxiv.org/abs/2508.04227)
*Yuyang Liu,Qiuhe Hong,Linlan Huang,Alexandra Gomez-Villa,Dipam Goswami,Xialei Liu,Joost van de Weijer,Yonghong Tian*

Main category: cs.CV

TL;DR: 该综述聚焦视觉语言模型的持续学习（VLM - CL），分析核心失败模式，提出挑战驱动的分类法，分析评估方法，指出待解决问题和未来方向。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在非平稳数据上持续学习面临重大挑战，如跨模态特征漂移等，缺乏相关系统综述。

Method: 识别VLM - CL的三种核心失败模式，提出挑战驱动的分类法，分析评估协议、数据集和指标。

Result: 提出三种解决方案分类：多模态重放策略、跨模态正则化、参数高效适应；指出当前评估缺乏针对VLM特性的基准。

Conclusion: 为开发终身视觉语言系统的研究人员提供全面且具诊断性的参考。

Abstract: Vision-language models (VLMs) have achieved impressive performance across
diverse multimodal tasks by leveraging large-scale pre-training. However,
enabling them to learn continually from non-stationary data remains a major
challenge, as their cross-modal alignment and generalization capabilities are
particularly vulnerable to catastrophic forgetting. Unlike traditional unimodal
continual learning (CL), VLMs face unique challenges such as cross-modal
feature drift, parameter interference due to shared architectures, and
zero-shot capability erosion. This survey offers the first focused and
systematic review of continual learning for VLMs (VLM-CL). We begin by
identifying the three core failure modes that degrade performance in VLM-CL.
Based on these, we propose a challenge-driven taxonomy that maps solutions to
their target problems: (1) \textit{Multi-Modal Replay Strategies} address
cross-modal drift through explicit or implicit memory mechanisms; (2)
\textit{Cross-Modal Regularization} preserves modality alignment during
updates; and (3) \textit{Parameter-Efficient Adaptation} mitigates parameter
interference with modular or low-rank updates. We further analyze current
evaluation protocols, datasets, and metrics, highlighting the need for better
benchmarks that capture VLM-specific forgetting and compositional
generalization. Finally, we outline open problems and future directions,
including continual pre-training and compositional zero-shot learning. This
survey aims to serve as a comprehensive and diagnostic reference for
researchers developing lifelong vision-language systems. All resources are
available at:
https://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models.

</details>


### [238] [Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark](https://arxiv.org/abs/2508.04260)
*Xiao Wang,Ziwen Wang,Wentao Wu,Anjie Wang,Jiashu Wu,Yantao Pan,Chenglong Li*

Main category: cs.CV

TL;DR: 本文提出SAV框架用于车辆部件分割，引入VehicleSeg10K数据集并进行实验，代码和数据集将开源。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶发展对车辆感知算法性能要求提高，SAM无法直接用于车辆部件细粒度分割任务。

Method: 提出SAV框架，包含基于SAM的编解码器、车辆部件知识图谱和上下文样本检索编码模块；引入VehicleSeg10K数据集。

Result: 在VehicleSeg10K等三个数据集上进行综合实验，与多个代表性基线进行对比。

Conclusion: 为未来车辆部件分割研究和比较奠定基础，代码和数据集将开源。

Abstract: With the rapid advancement of autonomous driving, vehicle perception,
particularly detection and segmentation, has placed increasingly higher demands
on algorithmic performance. Pre-trained large segmentation models, especially
Segment Anything Model (SAM), have sparked significant interest and inspired
new research directions in artificial intelligence. However, SAM cannot be
directly applied to the fine-grained task of vehicle part segmentation, as its
text-prompted segmentation functionality is not publicly accessible, and the
mask regions generated by its default mode lack semantic labels, limiting its
utility in structured, category-specific segmentation tasks. To address these
limitations, we propose SAV, a novel framework comprising three core
components: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a
context sample retrieval encoding module. The knowledge graph explicitly models
the spatial and geometric relationships among vehicle parts through a
structured ontology, effectively encoding prior structural knowledge.
Meanwhile, the context retrieval module enhances segmentation by identifying
and leveraging visually similar vehicle instances from training data, providing
rich contextual priors for improved generalization. Furthermore, we introduce a
new large-scale benchmark dataset for vehicle part segmentation, named
VehicleSeg10K, which contains 11,665 high-quality pixel-level annotations
across diverse scenes and viewpoints. We conduct comprehensive experiments on
this dataset and two other datasets, benchmarking multiple representative
baselines to establish a solid foundation for future research and comparison. %
Both the dataset and source code of this paper will be released upon
acceptance. Both the dataset and source code of this paper will be released on
https://github.com/Event-AHU/SAV

</details>


### [239] [VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Visual Backbones](https://arxiv.org/abs/2508.04379)
*Lefei Shen,Mouxiang Chen,Xu Liu,Han Fu,Xiaoxue Ren,Jianling Sun,Zhuo Li,Chenghao Liu*

Main category: cs.CV

TL;DR: 现有研究表明图像预训练视觉模型可用于时间序列预测，但跨模态迁移有挑战。本文提出VisionTS++，在大规模时间序列数据集上持续预训练，有三项创新，评估取得SOTA结果，推动通用时间序列基础模型发展。


<details>
  <summary>Details</summary>
Motivation: 当前图像预训练视觉模型用于时间序列预测时，存在数据模态、多变量预测和概率预测三方面差距，需有效跨模态迁移方法。

Method: 提出VisionTS++，包括基于视觉模型的过滤机制、彩色多变量转换方法和多分位数预测方法。

Result: 在分布内和分布外时间序列预测基准测试中取得SOTA结果，均方误差降低6%-44%，在9种概率预测设置中排名第一。

Conclusion: 建立了跨模态知识迁移新范式，推动通用时间序列基础模型发展。

Abstract: Recent studies have revealed that vision models pre-trained on images can
perform well in time series forecasting by reformulating forecasting as an
image reconstruction task, suggesting their potential as universal time series
foundation models. However, effective cross-modal transfer from vision to time
series remains challenging due to three key discrepancies: (1) data-modality
gap between structured, bounded image data and unbounded, heterogeneous time
series; (2) multivariate-forecasting gap between standard RGB
three-channel-based vision models and the need to model time series with
arbitrary numbers of variates; and (3) probabilistic-forecasting gap between
the deterministic output formats of most vision models and the requirement for
uncertainty-aware probabilistic predictions. To bridge these gaps, we propose
VisionTS++, a vision-model-based TSFM that performs continual pre-training on
large-scale time series datasets, including 3 innovations: (1) a
vision-model-based filtering mechanism to identify high-quality time series
data, thereby mitigating modality gap and improving pre-training stability, (2)
a colorized multivariate conversion method that transforms multivariate time
series into multi-subfigure RGB images, capturing complex inter-variate
dependencies; and (3) a multi-quantile forecasting approach using parallel
reconstruction heads to generate forecasts of different quantile levels, thus
more flexibly approximating arbitrary output distributions without restrictive
prior distributional assumptions. Evaluated on both in-distribution and
out-of-distribution TSF benchmarks, \model achieves SOTA results, outperforming
specialized TSFMs by 6%-44% in MSE reduction and ranking first in 9 out of 12
probabilistic forecasting settings. Our work establishes a new paradigm for
cross-modal knowledge transfer, advancing the development of universal TSFMs.

</details>


### [240] [Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model](https://arxiv.org/abs/2508.04472)
*Hongxu Chen,Zhen Wang,Taoran Mei,Lin Li,Bowei Zhu,Runshi Li,Long Chen*

Main category: cs.CV

TL;DR: 现有概念擦除方法有局限性，提出新方法ErasePro，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法存在擦除不完全和生成质量下降的问题。

Method: 提出ErasePro，引入严格零残差约束，采用渐进式逐层更新策略。

Result: 在不同概念擦除任务中验证了ErasePro的有效性。

Conclusion: ErasePro能更完整地擦除概念并更好地保留整体生成质量。

Abstract: Concept Erasure, which aims to prevent pretrained text-to-image models from
generating content associated with semantic-harmful concepts (i.e., target
concepts), is getting increased attention. State-of-the-art methods formulate
this task as an optimization problem: they align all target concepts with
semantic-harmless anchor concepts, and apply closed-form solutions to update
the model accordingly. While these closed-form methods are efficient, we argue
that existing methods have two overlooked limitations: 1) They often result in
incomplete erasure due to "non-zero alignment residual", especially when text
prompts are relatively complex. 2) They may suffer from generation quality
degradation as they always concentrate parameter updates in a few deep layers.
To address these issues, we propose a novel closed-form method ErasePro: it is
designed for more complete concept erasure and better preserving overall
generative quality. Specifically, ErasePro first introduces a strict
zero-residual constraint into the optimization objective, ensuring perfect
alignment between target and anchor concept features and enabling more complete
erasure. Secondly, it employs a progressive, layer-wise update strategy that
gradually transfers target concept features to those of the anchor concept from
shallow to deep layers. As the depth increases, the required parameter changes
diminish, thereby reducing deviations in sensitive deep layers and preserving
generative quality. Empirical results across different concept erasure tasks
(including instance, art style, and nudity erasure) have demonstrated the
effectiveness of our ErasePro.

</details>


### [241] [ProtoN: Prototype Node Graph Neural Network for Unconstrained Multi-Impression Ear Recognition](https://arxiv.org/abs/2508.04381)
*Santhoshkumar Peddi,Sadhvik Bathini,Arun Balasubramanian,Monalisa Sarma,Debasis Samanta*

Main category: cs.CV

TL;DR: 提出少样本学习框架ProtoN用于耳部识别，在五个基准数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有耳部生物识别方法受标注数据稀缺和类内差异大限制，且孤立提取特征能力有限。

Method: 提出ProtoN框架，用基于图的方法联合处理同一身份的多个印记，使用PGNN层和交叉图原型对齐策略，采用混合损失函数。

Result: 在五个基准耳部数据集上，Rank - 1识别准确率达99.60%，EER低至0.025。

Conclusion: ProtoN在有限数据条件下的少样本耳部识别中有效。

Abstract: Ear biometrics offer a stable and contactless modality for identity
recognition, yet their effectiveness remains limited by the scarcity of
annotated data and significant intra-class variability. Existing methods
typically extract identity features from individual impressions in isolation,
restricting their ability to capture consistent and discriminative
representations. To overcome these limitations, a few-shot learning framework,
ProtoN, is proposed to jointly process multiple impressions of an identity
using a graph-based approach. Each impression is represented as a node in a
class-specific graph, alongside a learnable prototype node that encodes
identity-level information. This graph is processed by a Prototype Graph Neural
Network (PGNN) layer, specifically designed to refine both impression and
prototype representations through a dual-path message-passing mechanism. To
further enhance discriminative power, the PGNN incorporates a cross-graph
prototype alignment strategy that improves class separability by enforcing
intra-class compactness while maintaining inter-class distinction.
Additionally, a hybrid loss function is employed to balance episodic and global
classification objectives, thereby improving the overall structure of the
embedding space. Extensive experiments on five benchmark ear datasets
demonstrate that ProtoN achieves state-of-the-art performance, with Rank-1
identification accuracy of up to 99.60% and an Equal Error Rate (EER) as low as
0.025, showing the effectiveness for few-shot ear recognition under limited
data conditions.

</details>


### [242] [Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation](https://arxiv.org/abs/2508.04552)
*Franz Thaler,Darko Stern,Gernot Plank,Martin Urschler*

Main category: cs.CV

TL;DR: 本文提出全心脏分割方法，通过平衡联合训练和增强技术，在CT和MR数据上表现良好，有望生成患者特定心脏孪生模型。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要更先进方法从医学图像分析心脏，深度学习方法在域偏移下表现不佳。

Method: 采用平衡联合训练方法，等量利用不同源域的CT和MR数据；使用强强度和空间增强技术使训练数据多样化。

Result: 所提方法在MR数据上总体表现最佳，在CT数据上与最佳表现相近，CT数据DSC为93.33%、ASSD为0.8388mm，MR数据DSC为89.30%、ASSD为1.2411mm。

Conclusion: 该方法有潜力从医学图像高效获得准确语义分割，用于生成患者特定心脏孪生模型。

Abstract: As the leading cause of death worldwide, cardiovascular diseases motivate the
development of more sophisticated methods to analyze the heart and its
substructures from medical images like Computed Tomography (CT) and Magnetic
Resonance (MR). Semantic segmentations of important cardiac structures that
represent the whole heart are useful to assess patient-specific cardiac
morphology and pathology. Furthermore, accurate semantic segmentations can be
used to generate cardiac digital twin models which allows e.g.
electrophysiological simulation and personalized therapy planning. Even though
deep learning-based methods for medical image segmentation achieved great
advancements over the last decade, retaining good performance under domain
shift -- i.e. when training and test data are sampled from different data
distributions -- remains challenging. In order to perform well on domains known
at training-time, we employ a (1) balanced joint training approach that
utilizes CT and MR data in equal amounts from different source domains.
Further, aiming to alleviate domain shift towards domains only encountered at
test-time, we rely on (2) strong intensity and spatial augmentation techniques
to greatly diversify the available training data. Our proposed whole heart
segmentation method, a 5-fold ensemble with our contributions, achieves the
best performance for MR data overall and a performance similar to the best
performance for CT data when compared to a model trained solely on CT. With
93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR
data, our method demonstrates great potential to efficiently obtain accurate
semantic segmentations from which patient-specific cardiac twin models can be
generated.

</details>


### [243] [Deep Learning-based Scalable Image-to-3D Facade Parser for Generating Thermal 3D Building Models](https://arxiv.org/abs/2508.04406)
*Yinan Yu,Alex Gonzalez-Caceres,Samuel Scheidegger,Sanjay Somanath,Alexander Hollberg*

Main category: cs.CV

TL;DR: 本文提出Scalable Image - to - 3D Facade Parser (SI3FP) 管道，从图像提取几何信息生成LoD3热模型，在瑞典住宅建筑测试中窗墙比估计误差约5%，有助于大规模能源改造规划。


<details>
  <summary>Details</summary>
Motivation: 现有建筑改造对气候影响至关重要，早期改造规划需基于LoD3热模型模拟，但可扩展且准确识别模型特征有挑战。

Method: 提出SI3FP管道，结合计算机视觉和深度学习从图像提取几何信息，直接在正交图像平面建模几何基元。

Result: 在典型瑞典住宅建筑测试中，窗墙比估计误差约5%。

Conclusion: 该管道有助于大规模能源改造规划，在城市发展和规划有更广泛应用。

Abstract: Renovating existing buildings is essential for climate impact. Early-phase
renovation planning requires simulations based on thermal 3D models at Level of
Detail (LoD) 3, which include features like windows. However, scalable and
accurate identification of such features remains a challenge. This paper
presents the Scalable Image-to-3D Facade Parser (SI3FP), a pipeline that
generates LoD3 thermal models by extracting geometries from images using both
computer vision and deep learning. Unlike existing methods relying on
segmentation and projection, SI3FP directly models geometric primitives in the
orthographic image plane, providing a unified interface while reducing
perspective distortions. SI3FP supports both sparse (e.g., Google Street View)
and dense (e.g., hand-held camera) data sources. Tested on typical Swedish
residential buildings, SI3FP achieved approximately 5% error in window-to-wall
ratio estimates, demonstrating sufficient accuracy for early-stage renovation
analysis. The pipeline facilitates large-scale energy renovation planning and
has broader applications in urban development and planning.

</details>


### [244] [Learning Robust Intervention Representations with Delta Embeddings](https://arxiv.org/abs/2508.04492)
*Panagiotis Alimisis,Christos Diou*

Main category: cs.CV

TL;DR: 本文提出用因果增量嵌入表示干预以提升模型分布外鲁棒性，框架可无额外监督学习因果表示，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 多数因果表征学习工作关注场景变量，较少关注干预本身的表征，为提升分布外鲁棒性，聚焦潜在空间中干预的表征。

Method: 提出用因果增量嵌入表示干预，该嵌入对视觉场景不变且影响的因果变量稀疏，构建无额外监督从图像对学习因果表示的框架。

Result: 在因果三元组挑战实验中，因果增量嵌入在分布外设置中非常有效，在合成和真实世界基准测试中显著超越基线性能。

Conclusion: 关注干预的表征是提升分布外鲁棒性的有效策略，因果增量嵌入有良好效果。

Abstract: Causal representation learning has attracted significant research interest
during the past few years, as a means for improving model generalization and
robustness. Causal representations of interventional image pairs, have the
property that only variables corresponding to scene elements affected by the
intervention / action are changed between the start state and the end state.
While most work in this area has focused on identifying and representing the
variables of the scene under a causal model, fewer efforts have focused on
representations of the interventions themselves. In this work, we show that an
effective strategy for improving out of distribution (OOD) robustness is to
focus on the representation of interventions in the latent space. Specifically,
we propose that an intervention can be represented by a Causal Delta Embedding
that is invariant to the visual scene and sparse in terms of the causal
variables it affects. Leveraging this insight, we propose a framework that is
capable of learning causal representations from image pairs, without any
additional supervision. Experiments in the Causal Triplet challenge demonstrate
that Causal Delta Embeddings are highly effective in OOD settings,
significantly exceeding baseline performance in both synthetic and real-world
benchmarks.

</details>


### [245] [RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning Framework for Explainable Deepfake Detection](https://arxiv.org/abs/2508.04524)
*Tianxiao Li,Zhenglin Huang,Haiquan Wen,Yiwei He,Shuchang Lyu,Baoyuan Wu,Guangliang Cheng*

Main category: cs.CV

TL;DR: 本文提出RAIDX框架解决深度伪造检测准确性与可解释性问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法缺乏透明度，基于大语言模型的方法分析粗糙且依赖人工标注。

Method: 引入RAIDX框架，集成检索增强生成（RAG）和组相对策略优化（GRPO），利用RAG提高检测准确性，用GRPO生成细粒度解释和显著性图。

Result: 在多个基准测试中，RAIDX能有效识别真假图像，提供可解释的理由，实现了最先进的检测性能。

Conclusion: RAIDX是首个将RAG和GRPO协同的统一框架，解决了准确性和可解释性的关键差距。

Abstract: The rapid advancement of AI-generation models has enabled the creation of
hyperrealistic imagery, posing ethical risks through widespread misinformation.
Current deepfake detection methods, categorized as face specific detectors or
general AI-generated detectors, lack transparency by framing detection as a
classification task without explaining decisions. While several LLM-based
approaches offer explainability, they suffer from coarse-grained analyses and
dependency on labor-intensive annotations. This paper introduces RAIDX
(Retrieval-Augmented Image Deepfake Detection and Explainability), a novel
deepfake detection framework integrating Retrieval-Augmented Generation (RAG)
and Group Relative Policy Optimization (GRPO) to enhance detection accuracy and
decision explainability. Specifically, RAIDX leverages RAG to incorporate
external knowledge for improved detection accuracy and employs GRPO to
autonomously generate fine-grained textual explanations and saliency maps,
eliminating the need for extensive manual annotations. Experiments on multiple
benchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and
providing interpretable rationales in both textual descriptions and saliency
maps, achieving state-of-the-art detection performance while advancing
transparency in deepfake identification. RAIDX represents the first unified
framework to synergize RAG and GRPO, addressing critical gaps in accuracy and
explainability. Our code and models will be publicly available.

</details>


### [246] [MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning](https://arxiv.org/abs/2508.04549)
*Quang-Trung Truong,Yuk-Kwan Wong,Vo Hoang Kim Tuyen Dang,Rinaldi Gotama,Duc Thanh Nguyen,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 针对海洋视频理解难题，提出两阶段海洋对象导向视频字幕生成管道和综合视频理解基准，还强调视频分割有效性，代码和数据集已发布。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕数据集难以适应海洋环境复杂性，无法深入了解海洋生物，需解决这些局限。

Method: 提出两阶段海洋对象导向视频字幕生成管道，引入利用视频、文本和分割掩码三元组的综合视频理解基准，强调视频分割检测场景中显著对象过渡。

Result: 实现了更好的海洋视频理解和分析以及海洋视频生成，丰富了字幕内容语义。

Conclusion: 所提方法有效，相关数据集和代码已开源。

Abstract: Marine videos present significant challenges for video understanding due to
the dynamics of marine objects and the surrounding environment, camera motion,
and the complexity of underwater scenes. Existing video captioning datasets,
typically focused on generic or human-centric domains, often fail to generalize
to the complexities of the marine environment and gain insights about marine
life. To address these limitations, we propose a two-stage marine
object-oriented video captioning pipeline. We introduce a comprehensive video
understanding benchmark that leverages the triplets of video, text, and
segmentation masks to facilitate visual grounding and captioning, leading to
improved marine video understanding and analysis, and marine video generation.
Additionally, we highlight the effectiveness of video splitting in order to
detect salient object transitions in scene changes, which significantly enrich
the semantics of captioning content. Our dataset and code have been released at
https://msc.hkustvgd.com.

</details>


### [247] [CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization](https://arxiv.org/abs/2508.04566)
*Jinxing Zhou,Ziheng Zhou,Yanghao Zhou,Yuxin Mao,Zhangling Duan,Dan Guo*

Main category: cs.CV

TL;DR: 本文探索弱监督下的Dense Audio-Visual Event Localization (DAVEL)任务（W-DAVEL），提出基于跨模态显著锚点的方法，在两个数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 研究在仅提供视频级事件标签、事件时间边界未知的更具挑战性的弱监督设置下的DAVEL任务。

Method: 利用跨模态显著锚点，提出Mutual Event Agreement Evaluation模块生成一致性分数，用Cross-modal Salient Anchor Identification模块识别锚点特征，经多模态融合后输入Anchor-based Temporal Propagation模块增强语义编码。

Result: 在UnAV - 100和ActivityNet1.3数据集上建立了W - DAVEL基准，实验表明方法达到了SOTA性能。

Conclusion: 所提基于跨模态显著锚点的方法能有效解决弱监督下的DAVEL任务。

Abstract: The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally
localize events in untrimmed videos that occur simultaneously in both the audio
and visual modalities. This paper explores DAVEL under a new and more
challenging weakly-supervised setting (W-DAVEL task), where only video-level
event labels are provided and the temporal boundaries of each event are
unknown. We address W-DAVEL by exploiting \textit{cross-modal salient anchors},
which are defined as reliable timestamps that are well predicted under weak
supervision and exhibit highly consistent event semantics across audio and
visual modalities. Specifically, we propose a \textit{Mutual Event Agreement
Evaluation} module, which generates an agreement score by measuring the
discrepancy between the predicted audio and visual event classes. Then, the
agreement score is utilized in a \textit{Cross-modal Salient Anchor
Identification} module, which identifies the audio and visual anchor features
through global-video and local temporal window identification mechanisms. The
anchor features after multimodal integration are fed into an
\textit{Anchor-based Temporal Propagation} module to enhance event semantic
encoding in the original temporal audio and visual features, facilitating
better temporal localization under weak supervision. We establish benchmarks
for W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive
experiments demonstrate that our method achieves state-of-the-art performance.

</details>


### [248] [X-SAM: From Segment Anything to Any Segmentation](https://arxiv.org/abs/2508.04655)
*Hao Wang,Limeng Qiao,Zequn Jie,Zhijian Huang,Chengjian Feng,Qingfang Zheng,Lin Ma,Xiangyuan Lan,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出X - SAM框架解决LLMs和SAM的局限，实现先进像素级感知理解，有新分割任务和训练策略，实验效果好。


<details>
  <summary>Details</summary>
Motivation: LLMs缺乏像素级感知理解，SAM在多掩码预测和特定类别分割任务有局限且无法统一架构，需改进。

Method: 提出X - SAM框架，引入新统一框架，提出VGD分割任务，采用统一训练策略。

Result: X - SAM在多种图像分割基准测试中达到了先进水平。

Conclusion: X - SAM框架对多模态、像素级视觉理解有效。

Abstract: Large Language Models (LLMs) demonstrate strong capabilities in broad
knowledge representation, yet they are inherently deficient in pixel-level
perceptual understanding. Although the Segment Anything Model (SAM) represents
a significant advancement in visual-prompt-driven image segmentation, it
exhibits notable limitations in multi-mask prediction and category-specific
segmentation tasks, and it cannot integrate all segmentation tasks within a
unified model architecture. To address these limitations, we present X-SAM, a
streamlined Multimodal Large Language Model (MLLM) framework that extends the
segmentation paradigm from \textit{segment anything} to \textit{any
segmentation}. Specifically, we introduce a novel unified framework that
enables more advanced pixel-level perceptual comprehension for MLLMs.
Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)
segmentation, which segments all instance objects with interactive visual
prompts and empowers MLLMs with visual grounded, pixel-wise interpretative
capabilities. To enable effective training on diverse data sources, we present
a unified training strategy that supports co-training across multiple datasets.
Experimental results demonstrate that X-SAM achieves state-of-the-art
performance on a wide range of image segmentation benchmarks, highlighting its
efficiency for multimodal, pixel-level visual understanding. Code is available
at https://github.com/wanghao9610/X-SAM.

</details>


### [249] [YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper](https://arxiv.org/abs/2508.04658)
*Akhil Saketh Reddy Sabbella,Ch. Lakshmi Prachothan,Eswar Kumar Panta*

Main category: cs.CV

TL;DR: 本文提出基于YOLO v8的AI方法检测鸡的疾病，能实时准确识别感染鸡并预警，改善鸡健康管理。


<details>
  <summary>Details</summary>
Motivation: 家禽业检测鸡疾病可避免经济损失，传统人工观察方法费力且易出错。

Method: 开发分析高分辨率鸡照片的系统，使用YOLO v8进行实时目标识别，用大量带注释数据集训练算法。

Result: 能准确实时识别感染鸡，向农场经营者及时发出警告。

Conclusion: 该AI技术利于早期感染识别，无需人工检查，提高大型农场生物安全性，其实时特性为改善农场管理提供可扩展有效方法。

Abstract: In the poultry industry, detecting chicken illnesses is essential to avoid
financial losses. Conventional techniques depend on manual observation, which
is laborious and prone to mistakes. Using YOLO v8 a deep learning model for
real-time object recognition. This study suggests an AI based approach, by
developing a system that analyzes high resolution chicken photos, YOLO v8
detects signs of illness, such as abnormalities in behavior and appearance. A
sizable, annotated dataset has been used to train the algorithm, which provides
accurate real-time identification of infected chicken and prompt warnings to
farm operators for prompt action. By facilitating early infection
identification, eliminating the need for human inspection, and enhancing
biosecurity in large-scale farms, this AI technology improves chicken health
management. The real-time features of YOLO v8 provide a scalable and effective
method for improving farm management techniques.

</details>


### [250] [HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models](https://arxiv.org/abs/2508.04663)
*Young D. Kwon,Rui Li,Sijia Li,Da Li,Sourav Bhattacharya,Stylianos I. Venieris*

Main category: cs.CV

TL;DR: 提出HierarchicalPrune压缩框架，结合三种技术压缩扩散模型，减少内存占用和推理延迟，维持图像质量且优于先前工作。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型参数量大，在资源受限设备上推理有挑战，需进行模型压缩。

Method: HierarchicalPrune框架结合Hierarchical Position Pruning、Positional Weight Preservation和Sensitivity - Guided Distillation三种技术。

Result: 结合INT4权重量化，实现77.5 - 80.4%内存占用减少和27.9 - 38.0%延迟减少，GenEval分数最低降2.6%，HPSv2分数降7%，用户研究表明感知质量与原模型相当。

Conclusion: HierarchicalPrune框架能让数十亿规模扩散模型更适合设备端推理，保持输出图像质量且优于先前工作。

Abstract: State-of-the-art text-to-image diffusion models (DMs) achieve remarkable
quality, yet their massive parameter scale (8-11B) poses significant challenges
for inferences on resource-constrained devices. In this paper, we present
HierarchicalPrune, a novel compression framework grounded in a key observation:
DM blocks exhibit distinct functional hierarchies, where early blocks establish
semantic structures while later blocks handle texture refinements.
HierarchicalPrune synergistically combines three techniques: (1) Hierarchical
Position Pruning, which identifies and removes less essential later blocks
based on position hierarchy; (2) Positional Weight Preservation, which
systematically protects early model portions that are essential for semantic
structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts
knowledge-transfer intensity based on our discovery of block-wise sensitivity
variations. As a result, our framework brings billion-scale diffusion models
into a range more suitable for on-device inference, while preserving the
quality of the output images. Specifically, when combined with INT4 weight
quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction
(e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on
server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score
and 7% in HPSv2 score compared to the original model. Last but not least, our
comprehensive user study with 85 participants demonstrates that
HierarchicalPrune maintains perceptual quality comparable to the original model
while significantly outperforming prior works.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [251] [Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding](https://arxiv.org/abs/2508.03718)
*Mike Gartner*

Main category: cs.CY

TL;DR: 本文收集并发布与美国医保相关语料，引入医保上诉结果预测任务并发布标注基准和模型。


<details>
  <summary>Details</summary>
Motivation: 美国医保复杂，弱势群体缺乏理解和司法途径，现有语料缺乏评估简单案例的必要上下文。

Method: 收集并发布与美国医保相关的法律和医学文本语料，引入医保上诉结果预测任务，发布标注基准并训练模型。

Result: 完成语料收集与发布，有了医保上诉结果预测任务、标注基准及训练模型。

Conclusion: 自然语言处理技术可用于支持美国医保案例理解，改善司法和医疗服务获取情况。

Abstract: U.S. health insurance is complex, and inadequate understanding and limited
access to justice have dire implications for the most vulnerable. Advances in
natural language processing present an opportunity to support efficient,
case-specific understanding, and to improve access to justice and healthcare.
Yet existing corpora lack context necessary for assessing even simple cases. We
collect and release a corpus of reputable legal and medical text related to
U.S. health insurance. We also introduce an outcome prediction task for health
insurance appeals designed to support regulatory and patient self-help
applications, and release a labeled benchmark for our task, and models trained
on it.

</details>


### [252] [Development of management systems using artificial intelligence systems and machine learning methods for boards of directors (preprint, unofficial translation)](https://arxiv.org/abs/2508.03769)
*Anna Romanova*

Main category: cs.CY

TL;DR: 研究应对企业管理中AI从决策支持工具向自主决策者的范式转变，指出AI技术发展远超法律伦理准则制定，提出自主AI系统开发实施的参考模型。


<details>
  <summary>Details</summary>
Motivation: 解决企业管理中AI成为自主决策者时，技术发展快于法律伦理准则创建的问题。

Method: 提出参考模型，引入计算法或算法法，强调专用操作环境，用合成数据训练AI，用博弈论计算最优策略，重视可解释AI。

Result: 未提及具体结果。

Conclusion: 强调为自主AI系统创建合法伦理决策机制、专用操作环境的重要性，以及可解释AI对透明度和问责制的关键作用。

Abstract: The study addresses the paradigm shift in corporate management, where AI is
moving from a decision support tool to an autonomous decision-maker, with some
AI systems already appointed to leadership roles in companies. A central
problem identified is that the development of AI technologies is far outpacing
the creation of adequate legal and ethical guidelines.
  The research proposes a "reference model" for the development and
implementation of autonomous AI systems in corporate management. This model is
based on a synthesis of several key components to ensure legitimate and ethical
decision-making. The model introduces the concept of "computational law" or
"algorithmic law". This involves creating a separate legal framework for AI
systems, with rules and regulations translated into a machine-readable,
algorithmic format to avoid the ambiguity of natural language. The paper
emphasises the need for a "dedicated operational context" for autonomous AI
systems, analogous to the "operational design domain" for autonomous vehicles.
This means creating a specific, clearly defined environment and set of rules
within which the AI can operate safely and effectively. The model advocates for
training AI systems on controlled, synthetically generated data to ensure
fairness and ethical considerations are embedded from the start. Game theory is
also proposed as a method for calculating the optimal strategy for the AI to
achieve its goals within these ethical and legal constraints. The provided
analysis highlights the importance of explainable AI (XAI) to ensure the
transparency and accountability of decisions made by autonomous systems. This
is crucial for building trust and for complying with the "right to
explanation".

</details>


### [253] [Trustworthiness of Legal Considerations for the Use of LLMs in Education](https://arxiv.org/abs/2508.03771)
*Sara Alaswad,Tatiana Kalganova,Wasan Awad*

Main category: cs.CY

TL;DR: 文章对全球主要地区人工智能监管和道德框架进行比较分析，着重介绍海湾合作委员会（GCC）地区，并为其提出以合规为中心的人工智能治理框架，提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在全球教育系统的广泛应用，确保其道德、法律和适当部署成为关键政策问题，需要对不同地区框架进行分析。

Method: 对欧盟、英国、美国、中国和GCC国家等关键地区的人工智能相关监管和道德框架进行比较分析。

Result: 绘制核心信任原则在地区立法和人工智能治理结构中的嵌入情况，提出适合GCC背景的以合规为中心的人工智能治理框架。

Conclusion: 综合全球最佳实践和地区特定挑战，为教育领域构建合法、道德和文化敏感的人工智能系统提供实用指导，促进监管协调和负责任的人工智能整合。

Abstract: As Artificial Intelligence (AI), particularly Large Language Models (LLMs),
becomes increasingly embedded in education systems worldwide, ensuring their
ethical, legal, and contextually appropriate deployment has become a critical
policy concern. This paper offers a comparative analysis of AI-related
regulatory and ethical frameworks across key global regions, including the
European Union, United Kingdom, United States, China, and Gulf Cooperation
Council (GCC) countries. It maps how core trustworthiness principles, such as
transparency, fairness, accountability, data privacy, and human oversight are
embedded in regional legislation and AI governance structures. Special emphasis
is placed on the evolving landscape in the GCC, where countries are rapidly
advancing national AI strategies and education-sector innovation. To support
this development, the paper introduces a Compliance-Centered AI Governance
Framework tailored to the GCC context. This includes a tiered typology and
institutional checklist designed to help regulators, educators, and developers
align AI adoption with both international norms and local values. By
synthesizing global best practices with region-specific challenges, the paper
contributes practical guidance for building legally sound, ethically grounded,
and culturally sensitive AI systems in education. These insights are intended
to inform future regulatory harmonization and promote responsible AI
integration across diverse educational environments.

</details>


### [254] [Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference](https://arxiv.org/abs/2508.04586)
*Nuo Chen,Moming Duan,Andre Huikai Lin,Qian Wang,Jiaying Wu,Bingsheng He*

Main category: cs.CY

TL;DR: AI会议快速扩张使集中式会议模式不可持续，本文诊断结构危机并提出CFC模式。


<details>
  <summary>Details</summary>
Motivation: AI会议的快速扩张威胁科学传播、公平和社区福祉等基础目标，需解决结构危机。

Method: 识别科学、环境、心理和后勤四个关键压力领域，提出社区联合会议（CFC）模式。

Result: 指出当前系统与核心使命不一致，明确了压力领域。

Conclusion: CFC模式为AI研究提供更可持续、包容和有韧性的发展道路。

Abstract: Artificial Intelligence (AI) conferences are essential for advancing
research, sharing knowledge, and fostering academic community. However, their
rapid expansion has rendered the centralized conference model increasingly
unsustainable. This paper offers a data-driven diagnosis of a structural crisis
that threatens the foundational goals of scientific dissemination, equity, and
community well-being. We identify four key areas of strain: (1) scientifically,
with per-author publication rates more than doubling over the past decade to
over 4.5 papers annually; (2) environmentally, with the carbon footprint of a
single conference exceeding the daily emissions of its host city; (3)
psychologically, with 71% of online community discourse reflecting negative
sentiment and 35% referencing mental health concerns; and (4) logistically,
with attendance at top conferences such as NeurIPS 2024 beginning to outpace
venue capacity. These pressures point to a system that is misaligned with its
core mission. In response, we propose the Community-Federated Conference (CFC)
model, which separates peer review, presentation, and networking into globally
coordinated but locally organized components, offering a more sustainable,
inclusive, and resilient path forward for AI research.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [255] [Large AI Models for Wireless Physical Layer](https://arxiv.org/abs/2508.02314)
*Jiajia Guo,Yiming Cui,Shi Jin,Jun Zhang*

Main category: cs.IT

TL;DR: 文章回顾大模型在无线物理层通信应用进展，分两类策略分析，表明能提升性能和适应性，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于AI方法在无线物理层技术的局限性，利用大模型优势推动技术发展。

Method: 将大模型应用分为利用预训练大模型和开发原生大模型两类策略，通过多个用例综合研究。

Result: 两类策略显著提升了不同无线场景下的性能和适应性。

Conclusion: 提出高效架构、可解释性等未来研究方向，以推进基于大模型的下一代通信系统物理层解决方案。

Abstract: Large artificial intelligence models (LAMs) are transforming wireless
physical layer technologies through their robust generalization, multitask
processing, and multimodal capabilities. This article reviews recent
advancements in LAM applications for physical layer communications, addressing
limitations of conventional AI-based approaches. LAM applications are
classified into two strategies: leveraging pre-trained LAMs and developing
native LAMs designed specifically for physical layer tasks. The motivations and
key frameworks of these approaches are comprehensively examined through
multiple use cases. Both strategies significantly improve performance and
adaptability across diverse wireless scenarios. Future research directions,
including efficient architectures, interpretability, standardized datasets, and
collaboration between large and small models, are proposed to advance LAM-based
physical layer solutions for next-generation communication systems.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [256] [Algebraically Observable Physics-Informed Neural Network and its Application to Epidemiological Modelling](https://arxiv.org/abs/2508.04590)
*Mizuka Komatsu*

Main category: cs.SC

TL;DR: 本文探讨用物理信息神经网络（PINN）估计流行病学模型状态变量和参数，提出基于代数可观测性分析扩充未测量数据的方法，经实验验证该方法更准确有效。


<details>
  <summary>Details</summary>
Motivation: 在流行病学模型中，并非所有轨迹数据都可测量，用部分测量数据学习PINN估计未测量状态变量和流行病学参数具有挑战性。

Method: 引入状态变量代数可观测性概念，基于代数可观测性分析扩充未测量数据。

Result: 在三种流行病学建模场景的数值实验中，给定有噪声的部分测量数据，该方法对未测量状态和参数的估计准确性高于传统方法。

Conclusion: 所提方法有效，在某些变量数据无法从测量中重建等实际场景也适用。

Abstract: Physics-Informed Neural Network (PINN) is a deep learning framework that
integrates the governing equations underlying data into a loss function. In
this study, we consider the problem of estimating state variables and
parameters in epidemiological models governed by ordinary differential
equations using PINNs. In practice, not all trajectory data corresponding to
the population described by models can be measured. Learning PINNs to estimate
the unmeasured state variables and epidemiological parameters using partial
measurements is challenging.
  Accordingly, we introduce the concept of algebraic observability of the state
variables. Specifically, we propose augmenting the unmeasured data based on
algebraic observability analysis. The validity of the proposed method is
demonstrated through numerical experiments under three scenarios in the context
of epidemiological modelling. Specifically, given noisy and partial
measurements, the accuracy of unmeasured states and parameter estimation of the
proposed method is shown to be higher than that of the conventional methods.
The proposed method is also shown to be effective in practical scenarios, such
as when the data corresponding to certain variables cannot be reconstructed
from the measurements.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [257] [CoughViT: A Self-Supervised Vision Transformer for Cough Audio Representation Learning](https://arxiv.org/abs/2508.03764)
*Justin Luong,Hao Xue,Flora D. Salim*

Main category: cs.SD

TL;DR: 本文提出CoughViT预训练框架以解决呼吸疾病诊断中标签和数据稀缺问题，实验显示其表现良好。


<details>
  <summary>Details</summary>
Motivation: AI呼吸疾病诊断系统存在标签和数据稀缺问题，限制诊断性能和评估，需提升有限数据任务的诊断表现。

Method: 提出CoughViT预训练框架，用掩码数据建模以自监督学习方式训练特征编码器，并在三个咳嗽分类任务上与其他预训练策略对比。

Result: CoughViT学习的表示在提升下游任务性能上与或超过当前最先进的有监督音频表示。

Conclusion: CoughViT框架有助于解决呼吸疾病诊断中标签和数据稀缺问题，提高诊断性能。

Abstract: Physicians routinely assess respiratory sounds during the diagnostic process,
providing insight into the condition of a patient's airways. In recent years,
AI-based diagnostic systems operating on respiratory sounds, have demonstrated
success in respiratory disease detection. These systems represent a crucial
advancement in early and accessible diagnosis which is essential for timely
treatment. However, label and data scarcity remain key challenges, especially
for conditions beyond COVID-19, limiting diagnostic performance and reliable
evaluation. In this paper, we propose CoughViT, a novel pre-training framework
for learning general-purpose cough sound representations, to enhance diagnostic
performance in tasks with limited data. To address label scarcity, we employ
masked data modelling to train a feature encoder in a self-supervised learning
manner. We evaluate our approach against other pre-training strategies on three
diagnostically important cough classification tasks. Experimental results show
that our representations match or exceed current state-of-the-art supervised
audio representations in enhancing performance on downstream tasks.

</details>


### [258] [Are Inherently Interpretable Models More Robust? A Study In Music Emotion Recognition](https://arxiv.org/abs/2508.03780)
*Katharina Hoedt,Arthur Flexer,Gerhard Widmer*

Main category: cs.SD

TL;DR: 研究可解释深度模型与黑盒模型在对抗样本下的鲁棒性，发现可解释模型更鲁棒且成本低。


<details>
  <summary>Details</summary>
Motivation: 探究固有可解释深度模型相比黑盒模型是否对数据中无关扰动更具鲁棒性。

Method: 比较可解释和黑盒音乐情感识别模型在对抗样本下的鲁棒性，并加入对抗训练模型进行对比。

Result: 固有可解释模型比黑盒模型更鲁棒，且能以较低计算成本达到与对抗训练模型相似的鲁棒性水平。

Conclusion: 固有可解释深度模型具有更好的鲁棒性和成本优势。

Abstract: One of the desired key properties of deep learning models is the ability to
generalise to unseen samples. When provided with new samples that are
(perceptually) similar to one or more training samples, deep learning models
are expected to produce correspondingly similar outputs. Models that succeed in
predicting similar outputs for similar inputs are often called robust. Deep
learning models, on the other hand, have been shown to be highly vulnerable to
minor (adversarial) perturbations of the input, which manage to drastically
change a model's output and simultaneously expose its reliance on spurious
correlations. In this work, we investigate whether inherently interpretable
deep models, i.e., deep models that were designed to focus more on meaningful
and interpretable features, are more robust to irrelevant perturbations in the
data, compared to their black-box counterparts. We test our hypothesis by
comparing the robustness of an interpretable and a black-box music emotion
recognition (MER) model when challenged with adversarial examples. Furthermore,
we include an adversarially trained model, which is optimised to be more
robust, in the comparison. Our results indicate that inherently more
interpretable models can indeed be more robust than their black-box
counterparts, and achieve similar levels of robustness as adversarially trained
models, at lower computational cost.

</details>


### [259] [NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations](https://arxiv.org/abs/2508.04195)
*Huan Liao,Qinke Ni,Yuancheng Wang,Yiheng Lu,Haoyue Zhan,Pengyuan Xie,Qiang Zhang,Zhizheng Wu*

Main category: cs.SD

TL;DR: 提出NVSpeech集成可扩展管道，统一副语言发声识别与合成，有数据集构建、ASR建模和可控TTS。


<details>
  <summary>Details</summary>
Motivation: 传统自动语音识别和文本到语音系统忽略副语言发声重要性，需处理此类信息。

Method: 1. 引入含18个词级副语言类别的48430个人类语音手动标注数据集；2. 开发副语言感知ASR模型自动标注大规模语料；3. 在人工和自动标注数据上微调零样本TTS模型实现可控合成。

Result: 得到首个大规模含词级对齐和副语言提示的中文数据集，实现副语言发声的统一识别和生成。

Conclusion: NVSpeech是首个用于普通话表达性语音建模的开放、大规模、词级标注管道，可扩展且可控。

Abstract: Paralinguistic vocalizations-including non-verbal sounds like laughter and
breathing, as well as lexicalized interjections such as "uhm" and "oh"-are
integral to natural spoken communication. Despite their importance in conveying
affect, intent, and interactional cues, such cues remain largely overlooked in
conventional automatic speech recognition (ASR) and text-to-speech (TTS)
systems. We present NVSpeech, an integrated and scalable pipeline that bridges
the recognition and synthesis of paralinguistic vocalizations, encompassing
dataset construction, ASR modeling, and controllable TTS. (1) We introduce a
manually annotated dataset of 48,430 human-spoken utterances with 18 word-level
paralinguistic categories. (2) We develop the paralinguistic-aware ASR model,
which treats paralinguistic cues as inline decodable tokens (e.g., "You're so
funny [Laughter]"), enabling joint lexical and non-verbal transcription. This
model is then used to automatically annotate a large corpus, the first
large-scale Chinese dataset of 174,179 utterances (573 hours) with word-level
alignment and paralingustic cues. (3) We finetune zero-shot TTS models on both
human- and auto-labeled data to enable explicit control over paralinguistic
vocalizations, allowing context-aware insertion at arbitrary token positions
for human-like speech synthesis. By unifying the recognition and generation of
paralinguistic vocalizations, NVSpeech offers the first open, large-scale,
word-level annotated pipeline for expressive speech modeling in Mandarin,
integrating recognition and synthesis in a scalable and controllable manner.
Dataset and audio demos are available at https://nvspeech170k.github.io/.

</details>


### [260] [Live Music Models](https://arxiv.org/abs/2508.04651)
*Lyria Team,Antoine Caillon,Brian McWilliams,Cassie Tarakajian,Ian Simon,Ilaria Manco,Jesse Engel,Noah Constant,Pen Li,Timo I. Denk,Alberto Lalama,Andrea Agostinelli,Anna Huang,Ethan Manilow,George Brower,Hakan Erdogan,Heidi Lei,Itai Rolnick,Ivan Grishchenko,Manu Orsini,Matej Kastelic,Mauricio Zuluaga,Mauro Verzetti,Michael Dooley,Ondrej Skopek,Rafael Ferrer,Zalán Borsos,Äaron van den Oord,Douglas Eck,Eli Collins,Jason Baldridge,Tom Hume,Chris Donahue,Kehang Han,Adam Roberts*

Main category: cs.SD

TL;DR: 介绍实时音乐生成模型Magenta RealTime和Lyria RealTime，前者质量优、参数少，后者控制更丰富，体现人机交互新范式。


<details>
  <summary>Details</summary>
Motivation: 提出能实时生成连续音乐并实现用户同步控制的新型音乐生成模型。

Method: 开发Magenta RealTime和Lyria RealTime模型，前者用文本或音频提示控制风格，后者提供扩展控制。

Result: Magenta RealTime在音乐质量自动指标上优于其他开放权重模型，参数更少且有实时生成能力；Lyria RealTime有扩展控制和广泛提示覆盖。

Conclusion: 这些模型展示了强调人机交互的人工智能辅助音乐创作新范式。

Abstract: We introduce a new class of generative models for music called live music
models that produce a continuous stream of music in real-time with synchronized
user control. We release Magenta RealTime, an open-weights live music model
that can be steered using text or audio prompts to control acoustic style. On
automatic metrics of music quality, Magenta RealTime outperforms other
open-weights music generation models, despite using fewer parameters and
offering first-of-its-kind live generation capabilities. We also release Lyria
RealTime, an API-based model with extended controls, offering access to our
most powerful model with wide prompt coverage. These models demonstrate a new
paradigm for AI-assisted music creation that emphasizes human-in-the-loop
interaction for live music performance.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [261] [Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors](https://arxiv.org/abs/2508.03715)
*Bertram Fuchs,Mehdi Ejtehadi,Ana Cisnal,Jürgen Pannek,Anke Scheel-Sailer,Robert Riener,Inge Eriks-Hoogland,Diego Paez-Granados*

Main category: eess.SP

TL;DR: 研究提出非侵入式、可解释的机器学习框架，用多模态可穿戴传感器检测自主神经反射异常（AD），模型性能良好，是迈向脊髓损伤患者个性化实时监测的重要一步。


<details>
  <summary>Details</summary>
Motivation: 当前AD监测方法有侵入性或依赖主观症状报告，适用性受限，需要非侵入式检测方法。

Method: 收集27名慢性脊髓损伤患者的多模态数据，经预处理和特征提取，用BorutaSHAP选特征，SHAP值解释，训练特定弱学习器并集成元模型，分层交叉验证。

Result: HR和ECG特征最具信息性，最近质心集成模型性能最佳（Macro F1 = 0.77+/-0.03），HR的AUC最高（0.93），RR和温度特征贡献小，模型对传感器故障有鲁棒性。

Conclusion: 研究成果是脊髓损伤患者个性化实时监测的重要进展。

Abstract: Autonomic Dysreflexia (AD) is a potentially life-threatening condition
characterized by sudden, severe blood pressure (BP) spikes in individuals with
spinal cord injury (SCI). Early, accurate detection is essential to prevent
cardiovascular complications, yet current monitoring methods are either
invasive or rely on subjective symptom reporting, limiting applicability in
daily file. This study presents a non-invasive, explainable machine learning
framework for detecting AD using multimodal wearable sensors. Data were
collected from 27 individuals with chronic SCI during urodynamic studies,
including electrocardiography (ECG), photoplethysmography (PPG), bioimpedance
(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three
commercial devices. Objective AD labels were derived from synchronized
cuff-based BP measurements. Following signal preprocessing and feature
extraction, BorutaSHAP was used for robust feature selection, and SHAP values
for explainability. We trained modality- and device-specific weak learners and
aggregated them using a stacked ensemble meta-model. Cross-validation was
stratified by participants to ensure generalizability. HR- and ECG-derived
features were identified as the most informative, particularly those capturing
rhythm morphology and variability. The Nearest Centroid ensemble yielded the
highest performance (Macro F1 = 0.77+/-0.03), significantly outperforming
baseline models. Among modalities, HR achieved the highest area under the curve
(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature
features contributed less to overall accuracy, consistent with missing data and
low specificity. The model proved robust to sensor dropout and aligned well
with clinical AD events. These results represent an important step toward
personalized, real-time monitoring for individuals with SCI.

</details>


### [262] [Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024](https://arxiv.org/abs/2508.03698)
*Se Won Oh,Hyuntae Jeong,Seungeun Chung,Jeong Mook Lim,Kyoung Ju Noh,Sunkyung Lee,Gyuwon Jung*

Main category: eess.SP

TL;DR: 本文介绍ETRI Lifelog Dataset 2024，它通过多设备被动收集数据及问卷收集主观信息，部分数据公开，还提及潜在应用。


<details>
  <summary>Details</summary>
Motivation: 准确有效了解个体日常生活中的身心状态，以改善人类健康和福祉。

Method: 利用智能手机、智能手表和睡眠传感器被动连续收集24小时数据，通过睡前和睡后调查收集主观报告。

Result: 创建了ETRI Lifelog Dataset 2024，部分数据已匿名公开。

Conclusion: 该数据集为探索人类日常生活和生活方式模式提供基础资源，有预测睡眠质量和压力等潜在应用。

Abstract: Improving human health and well-being requires an accurate and effective
understanding of an individual's physical and mental state throughout daily
life. To support this goal, we utilized smartphones, smartwatches, and sleep
sensors to collect data passively and continuously for 24 hours a day, with
minimal interference to participants' usual behavior, enabling us to gather
quantitative data on daily behaviors and sleep activities across multiple days.
Additionally, we gathered subjective self-reports of participants' fatigue,
stress, and sleep quality through surveys conducted immediately before and
after sleep. This comprehensive lifelog dataset is expected to provide a
foundational resource for exploring meaningful insights into human daily life
and lifestyle patterns, and a portion of the data has been anonymized and made
publicly available for further research. In this paper, we introduce the ETRI
Lifelog Dataset 2024, detailing its structure and presenting potential
applications, such as using machine learning models to predict sleep quality
and stress.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [263] [Advantages of Co-locating Quantum-HPC Platforms: A Survey for Near-Future Industrial Applications](https://arxiv.org/abs/2508.04171)
*Daigo Honda,Yuta Nishiyama,Junya Ishikawa,Kenichi Matsuzaki,Satoshi Miyata,Tadahiro Chujo,Yasuhisa Yamamoto,Masahiko Kiminami,Taro Kato,Jun Towada,Naoki Yoshioka,Naoto Aoki,Nobuyasu Ito*

Main category: quant-ph

TL;DR: 对新兴量子 - HPC 平台进行系统调查，发现量子与 HPC 系统共置可提高混合作业吞吐量，大规模问题需 HPC 资源。


<details>
  <summary>Details</summary>
Motivation: 目前不清楚量子 - HPC 平台对近未来工业应用是否有实际好处。

Method: 研究共置对延迟、带宽和作业调度的影响，评估 HPC 能力对混合算法性能、误差缓解和量子电路优化的作用。

Result: 量子与 HPC 系统共置可提高整体混合作业吞吐量。

Conclusion: 大规模现实问题执行混合算法需要 HPC 级计算资源。

Abstract: We conducted a systematic survey of emerging quantum-HPC platforms, which
integrate quantum computers and High-Performance Computing (HPC) systems
through co-location. Currently, it remains unclear whether such platforms
provide tangible benefits for near-future industrial applications. To address
this, we examined the impact of co-location on latency reduction, bandwidth
enhancement, and advanced job scheduling. Additionally, we assessed how
HPC-level capabilities could enhance hybrid algorithm performance, support
large-scale error mitigation, and facilitate complex quantum circuit
partitioning and optimization. Our findings demonstrate that co-locating
quantum and HPC systems can yield measurable improvements in overall hybrid job
throughput. We also observe that large-scale real-world problems can require
HPC-level computational resources for executing hybrid algorithms.

</details>


### [264] [Dynamic Solutions for Hybrid Quantum-HPC Resource Allocation](https://arxiv.org/abs/2508.04217)
*Roberto Rocco,Simone Rizzo,Matteo Barbieri,Gabriella Bettonte,Elisabetta Boella,Fulvio Ganz,Sergio Iserte,Antonio J. Peña,Petter Sandås,Alberto Scionti,Olivier Terzo,Chiara Vercellino,Giacomo Vitali,Paolo Viviani,Jonathan Frassineti,Sara Marzella,Daniele Ottaviani,Iacopo Colonnelli,Daniele Gregori*

Main category: quant-ph

TL;DR: 提出基于可塑性和工作流的混合HPC - 量子工作负载资源优化方法，实验显示动态分配有益。


<details>
  <summary>Details</summary>
Motivation: 量子计算机与经典高性能计算（HPC）集成受关注，但结合两者存在资源分配等技术挑战。

Method: 提出基于可塑性的方法和基于工作流的策略，在计算卸载到量子计算机时释放经典资源，处理完成后重新分配。

Result: 通过混合HPC - 量子用例实验，展示了动态分配的好处。

Conclusion: 所提出的解决方案有优化资源利用的潜力。

Abstract: The integration of quantum computers within classical High-Performance
Computing (HPC) infrastructures is receiving increasing attention, with the
former expected to serve as accelerators for specific computational tasks.
However, combining HPC and quantum computers presents significant technical
challenges, including resource allocation. This paper presents a novel
malleability-based approach, alongside a workflow-based strategy, to optimize
resource utilization in hybrid HPC-quantum workloads. With both these
approaches, we can release classical resources when computations are offloaded
to the quantum computer and reallocate them once quantum processing is
complete. Our experiments with a hybrid HPC-quantum use case show the benefits
of dynamic allocation, highlighting the potential of those solutions.

</details>


### [265] [Do GNN-based QEC Decoders Require Classical Knowledge? Evaluating the Efficacy of Knowledge Distillation from MWPM](https://arxiv.org/abs/2508.03782)
*Ryota Ikeda*

Main category: quant-ph

TL;DR: 本文对比了基于图注意力网络的纯数据驱动模型和结合知识蒸馏损失的模型在量子纠错解码器中的表现，发现知识蒸馏模型虽测试精度与基线相近，但训练收敛慢、时间长，表明现代GNN架构可直接从硬件数据学习复杂错误关联。


<details>
  <summary>Details</summary>
Motivation: 量子纠错解码器性能对实现实用量子计算机至关重要，图神经网络是有前景的方法，但训练方法未完善，研究将经典算法理论知识转移到GNN（知识蒸馏）能否有效提升性能。

Method: 严格比较两个基于图注意力网络架构且融入时间信息作为节点特征的模型，一个是仅基于真实标签训练的纯数据驱动基线模型，另一个是结合了基于最小权重完美匹配理论误差概率的知识蒸馏损失模型，使用谷歌公开实验数据进行评估。

Result: 知识蒸馏模型最终测试精度与基线模型相近，但训练损失收敛更慢，训练时间增加约五倍。

Conclusion: 现代GNN架构有很强能力直接从真实硬件数据高效学习复杂错误关联，无需近似理论模型指导。

Abstract: The performance of decoders in Quantum Error Correction (QEC) is key to
realizing practical quantum computers. In recent years, Graph Neural Networks
(GNNs) have emerged as a promising approach, but their training methodologies
are not yet well-established. It is generally expected that transferring
theoretical knowledge from classical algorithms like Minimum Weight Perfect
Matching (MWPM) to GNNs, a technique known as knowledge distillation, can
effectively improve performance. In this work, we test this hypothesis by
rigorously comparing two models based on a Graph Attention Network (GAT)
architecture that incorporates temporal information as node features. The first
is a purely data-driven model (baseline) trained only on ground-truth labels,
while the second incorporates a knowledge distillation loss based on the
theoretical error probabilities from MWPM. Using public experimental data from
Google, our evaluation reveals that while the final test accuracy of the
knowledge distillation model was nearly identical to the baseline, its training
loss converged more slowly, and the training time increased by a factor of
approximately five. This result suggests that modern GNN architectures possess
a high capacity to efficiently learn complex error correlations directly from
real hardware data, without guidance from approximate theoretical models.

</details>


### [266] [Probing and Enhancing the Robustness of GNN-based QEC Decoders with Reinforcement Learning](https://arxiv.org/abs/2508.03783)
*Ryota Ikeda*

Main category: quant-ph

TL;DR: 本文提出用强化学习代理探测图神经网络（GNN）量子纠错（QEC）解码器漏洞，应用于图注意力网络（GAT）解码器，发现漏洞并通过对抗训练增强其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探究GNN解码器在面对细微、对抗性扰动时的鲁棒性。

Method: 引入强化学习代理作为对手，寻找使解码器误分类的最小综合征修改；对GAT解码器应用该框架，并进行对抗训练。

Result: 强化学习代理能成功识别特定关键漏洞，以最少比特翻转实现高攻击成功率；对抗训练可显著增强解码器鲁棒性。

Conclusion: 自动漏洞发现和针对性再训练的迭代过程为容错量子计算开发更可靠、鲁棒的神经网络解码器提供了有前景的方法。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful, data-driven approach
for Quantum Error Correction (QEC) decoding, capable of learning complex noise
characteristics directly from syndrome data. However, the robustness of these
decoders against subtle, adversarial perturbations remains a critical open
question. This work introduces a novel framework to systematically probe the
vulnerabilities of a GNN decoder using a reinforcement learning (RL) agent. The
RL agent is trained as an adversary with the goal of finding minimal syndrome
modifications that cause the decoder to misclassify. We apply this framework to
a Graph Attention Network (GAT) decoder trained on experimental surface code
data from Google Quantum AI. Our results show that the RL agent can
successfully identify specific, critical vulnerabilities, achieving a high
attack success rate with a minimal number of bit flips. Furthermore, we
demonstrate that the decoder's robustness can be significantly enhanced through
adversarial training, where the model is retrained on the adversarial examples
generated by the RL agent. This iterative process of automated vulnerability
discovery and targeted retraining presents a promising methodology for
developing more reliable and robust neural network decoders for fault-tolerant
quantum computing.

</details>


### [267] [Hybrid Quantum--Classical Machine Learning Potential with Variational Quantum Circuits](https://arxiv.org/abs/2508.04098)
*Soohaeng Yoo Willow,D. ChangMo Yang,Chang Woo Myung*

Main category: quant-ph

TL;DR: 本文对比经典与混合量子 - 经典机器学习势预测液态硅DFT属性，发现混合算法可准确复现高温结构和热力学属性，显示近中期量子优势。


<details>
  <summary>Details</summary>
Motivation: 量子算法模拟分子系统尚不成熟，寻求混合量子 - 经典算法的实际优势。

Method: 将经典E(3) - 等变消息传递机器学习势与混合量子 - 经典MLP对比，在混合架构中用VQC替代消息传递层的读出。

Result: 分子动力学模拟表明混合量子 - 经典MLP能准确复现液态硅高温结构和热力学属性。

Conclusion: NISQ兼容的混合量子 - 经典算法在材料建模中有可衡量优势，是实现近中期量子优势的可行途径。

Abstract: Quantum algorithms for simulating large and complex molecular systems are
still in their infancy, and surpassing state-of-the-art classical techniques
remains an ever-receding goal post. A promising avenue of inquiry in the
meanwhile is to seek practical advantages through hybrid quantum-classical
algorithms, which combine conventional neural networks with variational quantum
circuits (VQCs) running on today's noisy intermediate-scale quantum (NISQ)
hardware. Such hybrids are well suited to NISQ hardware. The classical
processor performs the bulk of the computation, while the quantum processor
executes targeted sub-tasks that supply additional non-linearity and
expressivity. Here, we benchmark a purely classical E(3)-equivariant
message-passing machine learning potential (MLP) against a hybrid
quantum-classical MLP for predicting density functional theory (DFT) properties
of liquid silicon. In our hybrid architecture, every readout in the
message-passing layers is replaced by a VQC. Molecular dynamics simulations
driven by the HQC-MLP reveal that an accurate reproduction of high-temperature
structural and thermodynamic properties is achieved with VQCs. These findings
demonstrate a concrete scenario in which NISQ-compatible HQC algorithm could
deliver a measurable benefit over the best available classical alternative,
suggesting a viable pathway toward near-term quantum advantage in materials
modeling.

</details>


### [268] [Challenges in Applying Variational Quantum Algorithms to Dynamic Satellite Network Routing](https://arxiv.org/abs/2508.04288)
*Phuc Hao Do,Tran Duc Le*

Main category: quant-ph

TL;DR: 评估两种量子算法用于动态卫星网络路由的效果，发现面临重大挑战，指出障碍并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索将近期变分量子算法应用于动态卫星网络路由问题。

Method: 对静态量子优化器（如VQE、QAOA）用于离线路由计算和量子强化学习（QRL）用于在线决策进行理想无噪声模拟。

Result: 静态优化器无法解决经典简单的4节点最短路径问题，基本QRL智能体在动态8节点环境中无法学习到有用路由策略，表现与随机行动无异。

Conclusion: 量子算法在通信网络发挥优势前需解决关键障碍，讨论了限制原因并给出未来研究方向。

Abstract: Applying near-term variational quantum algorithms to the problem of dynamic
satellite network routing represents a promising direction for quantum
computing. In this work, we provide a critical evaluation of two major
approaches: static quantum optimizers such as the Variational Quantum
Eigensolver (VQE) and the Quantum Approximate Optimization Algorithm (QAOA) for
offline route computation, and Quantum Reinforcement Learning (QRL) methods for
online decision-making. Using ideal, noise-free simulations, we find that these
algorithms face significant challenges. Specifically, static optimizers are
unable to solve even a classically easy 4-node shortest path problem due to the
complexity of the optimization landscape. Likewise, a basic QRL agent based on
policy gradient methods fails to learn a useful routing strategy in a dynamic
8-node environment and performs no better than random actions. These negative
findings highlight key obstacles that must be addressed before quantum
algorithms can offer real advantages in communication networks. We discuss the
underlying causes of these limitations, including barren plateaus and learning
instability, and suggest future research directions to overcome them.

</details>


### [269] [Quantum circuit complexity and unsupervised machine learning of topological order](https://arxiv.org/abs/2508.04486)
*Yanming Che,Clemens Gneiting,Xiaoguang Wang,Franco Nori*

Main category: quant-ph

TL;DR: 本文以量子电路复杂度为切入点，构建用于量子多体系统拓扑序的无监督机器学习方法，提出定理并构建核函数，实验验证其性能，还探讨了与经典阴影断层扫描和阴影核学习的关系。


<details>
  <summary>Details</summary>
Motivation: 受柯尔莫哥洛夫复杂度与无监督机器学习关系的启发，探索用量子电路复杂度理解和构建适用于量子多体系统拓扑序的可解释且高效的无监督机器学习方法。

Method: 提出两个定理连接Nielsen量子电路复杂度与保真度变化和纠缠生成，构建基于保真度和纠缠的相似性度量或核函数，并进行数值实验。

Result: 数值实验表明所提出的两个核函数在量子相无监督聚类中表现优越，且可自然推导出并理解阴影核学习。

Conclusion: 建立了量子电路计算、量子复杂度和拓扑量子序机器学习关键概念与工具之间的联系。

Abstract: Inspired by the close relationship between Kolmogorov complexity and
unsupervised machine learning, we explore quantum circuit complexity, an
important concept in quantum computation and quantum information science, as a
pivot to understand and to build interpretable and efficient unsupervised
machine learning for topological order in quantum many-body systems. To span a
bridge from conceptual power to practical applicability, we present two
theorems that connect Nielsen's quantum circuit complexity for the quantum path
planning between two arbitrary quantum many-body states with fidelity change
and entanglement generation, respectively. Leveraging these connections,
fidelity-based and entanglement-based similarity measures or kernels, which are
more practical for implementation, are formulated. Using the two proposed
kernels, numerical experiments targeting the unsupervised clustering of quantum
phases of the bond-alternating XXZ spin chain, the ground state of Kitaev's
toric code and random product states, are conducted, demonstrating their
superior performance. Relations with classical shadow tomography and shadow
kernel learning are also discussed, where the latter can be naturally derived
and understood from our approach. Our results establish connections between key
concepts and tools of quantum circuit computation, quantum complexity, and
machine learning of topological quantum order.

</details>


### [270] [Benchmarking Quantum and Classical Sequential Models for Urban Telecommunication Forecasting](https://arxiv.org/abs/2508.04488)
*Chi-Sheng Chen,Samuel Yen-Chi Chen,Yun-Cheng Tsai*

Main category: quant-ph

TL;DR: 本文用米兰电信活动数据集评估经典和量子启发的顺序模型对短信流入活动单变量时间序列的预测性能，发现量子增强并非普遍有利。


<details>
  <summary>Details</summary>
Motivation: 评估经典和量子启发的顺序模型在预测短信流入活动单变量时间序列的性能。

Method: 使用米兰电信活动数据集，对比LSTM、QLSTM、QASA、QRWKV和QFWP五种模型在不同输入序列长度下的表现，所有模型基于历史值预测下一个10分钟的短信流入值。

Result: 不同模型对序列长度的敏感度不同，量子增强并非普遍有利。

Conclusion: 量子模块的有效性高度依赖于具体任务和架构设计，存在模型大小、参数化策略和时间建模能力之间的权衡。

Abstract: In this study, we evaluate the performance of classical and quantum-inspired
sequential models in forecasting univariate time series of incoming SMS
activity (SMS-in) using the Milan Telecommunication Activity Dataset. Due to
data completeness limitations, we focus exclusively on the SMS-in signal for
each spatial grid cell. We compare five models, LSTM (baseline), Quantum LSTM
(QLSTM), Quantum Adaptive Self-Attention (QASA), Quantum Receptance Weighted
Key-Value (QRWKV), and Quantum Fast Weight Programmers (QFWP), under varying
input sequence lengths (4, 8, 12, 16, 32 and 64). All models are trained to
predict the next 10-minute SMS-in value based solely on historical values
within a given sequence window. Our findings indicate that different models
exhibit varying sensitivities to sequence length, suggesting that quantum
enhancements are not universally advantageous. Rather, the effectiveness of
quantum modules is highly dependent on the specific task and architectural
design, reflecting inherent trade-offs among model size, parameterization
strategies, and temporal modeling capabilities.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [271] [Data-Driven Discovery of Mobility Periodicity for Understanding Urban Transportation Systems](https://arxiv.org/abs/2508.03747)
*Xinyu Chen,Qi Wang,Yunhan Zheng,Nina Cao,HanQin Cai,Jinhua Zhao*

Main category: cs.SI

TL;DR: 研究将复杂多维人类移动数据的周期性量化问题转化为时间序列自回归中主导正自相关的稀疏识别，应用于多地数据，揭示每周周期性，分析疫情影响，体现可解释机器学习在挖掘移动数据方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 揭示人类移动的时间规律对发现城市动态及相关决策和应用至关重要。

Method: 将周期性量化问题转化为时间序列自回归中主导正自相关的稀疏识别。

Result: 揭示多地不同空间位置的每周周期性，分析疫情对移动规律的影响及两地恢复趋势差异，纽约市恢复比芝加哥快。

Conclusion: 可解释的稀疏自回归能洞察人类移动的潜在时间模式，可解释机器学习能从现实移动数据中解锁关键见解。

Abstract: Uncovering the temporal regularity of human mobility is crucial for
discovering urban dynamics and has implications for various decision-making
processes and urban system applications. This study formulates the periodicity
quantification problem in complex and multidimensional human mobility data as a
sparse identification of dominant positive auto-correlations in time series
autoregression, allowing one to discover and quantify significant periodic
patterns such as weekly periodicity from a data-driven and interpretable
machine learning perspective. We apply our framework to real-world human
mobility data, including metro passenger flow in Hangzhou, China and
ridesharing trips in New York City (NYC) and Chicago, USA, revealing the
interpretable weekly periodicity across different spatial locations over past
several years. In particular, our analysis of ridesharing data from 2019 to
2024 demonstrates the disruptive impact of the COVID-19 pandemic on mobility
regularity and the subsequent recovery trends, highlighting differences in the
recovery pattern percentages and speeds between NYC and Chicago. We explore
that both NYC and Chicago experienced a remarkable reduction of weekly
periodicity in 2020, and the recovery of mobility regularity in NYC is faster
than Chicago. The interpretability of sparse autoregression provides insights
into the underlying temporal patterns of human mobility, offering a valuable
tool for understanding urban systems. Our findings highlight the potential of
interpretable machine learning to unlock crucial insights from real-world
mobility data.

</details>


### [272] [Quasi-Clique Discovery via Energy Diffusion](https://arxiv.org/abs/2508.04174)
*Yu Zhang,Yilong Luo,Mingyuan Ma,Yao Chen,Enqiang Zhu,Jin Xu,Chanjuan Liu*

Main category: cs.SI

TL;DR: 本文提出受能量扩散启发的准团发现算法EDQC，在30个真实数据集上实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有准团发现启发式算法在不同图上难以兼顾效率和解决方案一致性。

Method: 引入受能量扩散启发的EDQC算法，从源顶点进行随机能量扩散，在结构凝聚区域聚集能量。

Result: 在30个真实数据集上，EDQC发现的准团更大，解决方案质量方差更低。

Conclusion: EDQC是首个将能量扩散引入准团发现的方法。

Abstract: Discovering quasi-cliques -- subgraphs with edge density no less than a given
threshold -- is a fundamental task in graph mining, with broad applications in
social networks, bioinformatics, and e-commerce. Existing heuristics often rely
on greedy rules, similarity measures, or metaheuristic search, but struggle to
maintain both efficiency and solution consistency across diverse graphs. This
paper introduces EDQC, a novel quasi-clique discovery algorithm inspired by
energy diffusion. Instead of explicitly enumerating candidate subgraphs, EDQC
performs stochastic energy diffusion from source vertices, naturally
concentrating energy within structurally cohesive regions. The approach enables
efficient dense subgraph discovery without exhaustive search or
dataset-specific tuning. Experimental results on 30 real-world datasets
demonstrate that EDQC consistently discovers larger quasi-cliques than
state-of-the-art baselines on the majority of datasets, while also yielding
lower variance in solution quality. To the best of our knowledge, EDQC is the
first method to incorporate energy diffusion into quasi-clique discovery.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [273] [Predicting fall risk in older adults: A machine learning comparison of accelerometric and non-accelerometric factors](https://arxiv.org/abs/2508.03756)
*Ana González-Castro,José Alberto Benítez-Andrades,Rubén González-González,Camino Prada-García,Raquel Leirós-Rodríguez*

Main category: stat.AP

TL;DR: 研究用多种机器学习模型基于不同数据预测老年人跌倒风险，结合两类数据的模型表现优，贝叶斯岭回归最准，支持用集成数据和贝叶斯方法评估预防。


<details>
  <summary>Details</summary>
Motivation: 探究老年人跌倒风险预测。

Method: 使用多种机器学习模型，基于加速度计、非加速度计及组合数据进行训练。

Result: 结合两类数据的模型表现更好，贝叶斯岭回归准确性最高（MSE = 0.6746, R2 = 0.9941），非加速度计变量对预测很关键。

Conclusion: 支持使用集成数据和贝叶斯方法来加强跌倒风险评估并指导预防策略。

Abstract: This study investigates fall risk prediction in older adults using various
machine learning models trained on accelerometric, non-accelerometric, and
combined data from 146 participants. Models combining both data types achieved
superior performance, with Bayesian Ridge Regression showing the highest
accuracy (MSE = 0.6746, R2 = 0.9941). Non-accelerometric variables, such as age
and comorbidities, proved critical for prediction. Results support the use of
integrated data and Bayesian approaches to enhance fall risk assessment and
inform prevention strategies.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [274] [A semi-automatic approach to study population dynamics based on population pyramids](https://arxiv.org/abs/2508.03788)
*Max Hahn-Klimroth,João Pedro Meireles,Laurie Bingaman Lackey,Nick van Eeuwijk Mads F. Bertelsen,Paul W. Dierkes,Marcus Clauss*

Main category: q-bio.PE

TL;DR: 本文提出基于算法对种群数据按不同金字塔形状分类，用动物园哺乳动物数据验证，结果合理且有应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有从种群金字塔可视化数据中获取信息的形式化和算法方法较少，需开发相关算法。

Method: 使用1970 - 2024年全球动物园哺乳动物种群数据，开发基于算法将种群数据按不同金字塔形状分类的方法。

Result: 算法得出合理分类，尤其在与不同金字塔形状变化相关的种群规模变化方面。

Conclusion: 该方法可用于多场景分析和交流历史种群发展情况，对动物种群管理策略也有用。

Abstract: The depiction of populations - of humans or animals - as "population
pyramids" is a useful tool for the assessment of various characteristics of
populations at a glance. Although these visualisations are well-known objects
in various communities, formalised and algorithmic approaches to gain
information from these data are less present. Here, we present an
algorithm-based classification of population data into "pyramids" of different
shapes ([normal and inverted] pyramid / plunger / bell, [lower / middle /
upper] diamond, column, hourglass) that are linked to specific characteristics
of the population. To develop the algorithmic approach, we used data describing
global zoo populations of mammals from 1970-2024. This algorithm-based approach
delivers plausible classifications, in particular with respect to changes in
population size linked to specific series of, and transitions between,
different "pyramid" shapes. We believe this approach might become a useful tool
for analysing and communicating historical population developments in multiple
contexts and is of broad interest. Moreover, it might be useful for animal
population management strategies.

</details>


<div id='math.AG'></div>

# math.AG [[Back]](#toc)

### [275] [Constraining the outputs of ReLU neural networks](https://arxiv.org/abs/2508.03867)
*Yulia Alexandr,Guido Montúfar*

Main category: math.AG

TL;DR: 引入与ReLU神经网络自然关联的代数簇，分析网络输出秩约束得到表征可表示函数的多项式方程，研究代数簇达到期望维度的条件。


<details>
  <summary>Details</summary>
Motivation: 研究ReLU神经网络的表达和结构特性。

Method: 分析网络在每个激活区域输出的秩约束，推导多项式方程。

Result: 得到表征网络可表示函数的多项式方程。

Conclusion: 对ReLU网络的表达和结构特性有了深入了解。

Abstract: We introduce a class of algebraic varieties naturally associated with ReLU
neural networks, arising from the piecewise linear structure of their outputs
across activation regions in input space, and the piecewise multilinear
structure in parameter space. By analyzing the rank constraints on the network
outputs within each activation region, we derive polynomial equations that
characterize the functions representable by the network. We further investigate
conditions under which these varieties attain their expected dimension,
providing insight into the expressive and structural properties of ReLU
networks.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [276] [Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions](https://arxiv.org/abs/2508.04526)
*Fannya R. Sandjaja,Ayesha A. Majeed,Abdullah Abdullah,Gyan Wickremasinghe,Karen Rafferty,Vishal Sharma*

Main category: cs.NI

TL;DR: 传统安全架构易受分布式攻击，零信任架构（ZTA）是潜在解决方案，但未经验证的策略会导致问题，本文探讨ZTDN中ZTA策略设计挑战与解决方案，用UPPAAL进行策略验证案例研究并讨论系统安全的责任性。


<details>
  <summary>Details</summary>
Motivation: 传统安全架构依赖信任，在分布式攻击和引入智能AI时更脆弱，ZTA虽有潜力但未经验证的策略会导致安全问题，需研究ZTDN中ZTA策略设计。

Method: 探讨ZTDN中ZTA策略设计的挑战与解决方案，进行使用UPPAAL对策略进行形式验证的案例研究。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及最终结论，但讨论了系统安全中问责和责任的重要性。

Abstract: Traditional security architectures are becoming more vulnerable to
distributed attacks due to significant dependence on trust. This will further
escalate when implementing agentic AI within the systems, as more components
must be secured over a similar distributed space. These scenarios can be
observed in consumer technologies, such as the dense Internet of things (IoT).
Here, zero-trust architecture (ZTA) can be seen as a potential solution, which
relies on a key principle of not giving users explicit trust, instead always
verifying their privileges whenever a request is made. However, the overall
security in ZTA is managed through its policies, and unverified policies can
lead to unauthorized access. Thus, this paper explores challenges and solutions
for ZTA policy design in the context of distributed networks, which is referred
to as zero-trust distributed networks (ZTDN). This is followed by a case-study
on formal verification of policies using UPPAAL. Subsequently, the importance
of accountability and responsibility in the system's security is discussed.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [277] [The Glider Equation for Asymptotic Lenia](https://arxiv.org/abs/2508.04167)
*Hiroki Kojima,Ivan Yevenko,Takashi Ikegami*

Main category: nlin.CG

TL;DR: 本文聚焦Asymptotic Lenia，推导滑翔机模式条件，用梯度下降法找稳定配置，还推导无速度方程，建立与神经场模型联系。


<details>
  <summary>Details</summary>
Motivation: 探索Asymptotic Lenia中滑翔机模式的条件，寻找具有特定属性的新型滑翔机。

Method: 利用数学公式推导滑翔机方程，将其作为损失函数用梯度下降法寻找稳定配置，推导无速度方程。

Result: 能有效识别传统方法难发现的多样模式，虽部分优化模式不稳定。

Conclusion: 建立了Asymptotic Lenia与神经场模型的联系，为连续动力系统模式形成分析提供新方向。

Abstract: Lenia is a continuous extension of Conway's Game of Life that exhibits rich
pattern formations including self-propelling structures called gliders. In this
paper, we focus on Asymptotic Lenia, a variant formulated as partial
differential equations. By utilizing this mathematical formulation, we
analytically derive the conditions for glider patterns, which we term the
``Glider Equation.'' We demonstrate that by using this equation as a loss
function, gradient descent methods can successfully discover stable glider
configurations. This approach enables the optimization of update rules to find
novel gliders with specific properties, such as faster-moving variants. We also
derive a velocity-free equation that characterizes gliders of any speed,
expanding the search space for novel patterns. While many optimized patterns
result in transient gliders that eventually destabilize, our approach
effectively identifies diverse pattern formations that would be difficult to
discover through traditional methods. Finally, we establish connections between
Asymptotic Lenia and neural field models, highlighting mathematical
relationships that bridge these systems and suggesting new directions for
analyzing pattern formation in continuous dynamical systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [278] [Controllable Surface Diffusion Generative Model for Neurodevelopmental Trajectories](https://arxiv.org/abs/2508.03706)
*Zhenshan Xie,Levente Baljer,M. Jorge Cardoso,Emma Robinson*

Main category: q-bio.NC

TL;DR: 提出新图扩散网络模拟皮质成熟，用dHCP数据验证，模型能保持特定形态且模拟效果好，预测准确率达0.85 ± 0.62。


<details>
  <summary>Details</summary>
Motivation: 早产破坏皮质神经发育轨迹，增加认知和行为困难风险且结果差异大，现有模拟方法难以保留特定皮质折叠模式或重现区域形态变化。

Method: 提出一种支持可控模拟皮质成熟的图扩散网络，使用dHCP的皮质表面数据进行研究。

Result: 模型能保持受试者特定皮质形态，模拟皮质成熟效果好，能骗过独立训练的年龄回归网络，预测准确率为0.85 ± 0.62。

Conclusion: 所提出的图扩散网络在皮质成熟模拟方面有良好表现，可用于早期预测早产相关风险。

Abstract: Preterm birth disrupts the typical trajectory of cortical neurodevelopment,
increasing the risk of cognitive and behavioral difficulties. However, outcomes
vary widely, posing a significant challenge for early prediction. To address
this, individualized simulation offers a promising solution by modeling
subject-specific neurodevelopmental trajectories, enabling the identification
of subtle deviations from normative patterns that might act as biomarkers of
risk. While generative models have shown potential for simulating
neurodevelopment, prior approaches often struggle to preserve subject-specific
cortical folding patterns or to reproduce region-specific morphological
variations. In this paper, we present a novel graph-diffusion network that
supports controllable simulation of cortical maturation. Using cortical surface
data from the developing Human Connectome Project (dHCP), we demonstrate that
the model maintains subject-specific cortical morphology while modeling
cortical maturation sufficiently well to fool an independently trained age
regression network, achieving a prediction accuracy of $0.85 \pm 0.62$.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [279] [A Survey of Multimodal Ophthalmic Diagnostics: From Task-Specific Approaches to Foundational Models](https://arxiv.org/abs/2508.03734)
*Xiaoling Luo,Ruli Zheng,Qiaojian Zheng,Zibo Du,Shuo Yang,Meidan Ding,Qihao Xu,Chengliang Liu,Linlin Shen*

Main category: eess.IV

TL;DR: 本文对截至2025年眼科多模态深度学习方法的最新进展进行全面综述，涵盖特定任务方法和基础模型，分析数据集、评估指标等，讨论挑战并指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 视觉障碍是全球健康挑战，多模态成像对眼科诊断至关重要，需系统梳理眼科多模态深度学习方法进展。

Method: 对眼科多模态深度学习方法进行全面调查和分析，分为特定任务方法和基础模型两类研究。

Result: 梳理最新进展，指出不同方法特点，分析重要数据集、评估指标和创新方法，明确现存挑战。

Conclusion: 强调利用超广角成像和强化学习推理框架创建智能、可解释且临床适用的眼科AI系统是未来方向。

Abstract: Visual impairment represents a major global health challenge, with multimodal
imaging providing complementary information that is essential for accurate
ophthalmic diagnosis. This comprehensive survey systematically reviews the
latest advances in multimodal deep learning methods in ophthalmology up to the
year 2025. The review focuses on two main categories: task-specific multimodal
approaches and large-scale multimodal foundation models. Task-specific
approaches are designed for particular clinical applications such as lesion
detection, disease diagnosis, and image synthesis. These methods utilize a
variety of imaging modalities including color fundus photography, optical
coherence tomography, and angiography. On the other hand, foundation models
combine sophisticated vision-language architectures and large language models
pretrained on diverse ophthalmic datasets. These models enable robust
cross-modal understanding, automated clinical report generation, and decision
support. The survey critically examines important datasets, evaluation metrics,
and methodological innovations including self-supervised learning,
attention-based fusion, and contrastive alignment. It also discusses ongoing
challenges such as variability in data, limited annotations, lack of
interpretability, and issues with generalizability across different patient
populations. Finally, the survey outlines promising future directions that
emphasize the use of ultra-widefield imaging and reinforcement learning-based
reasoning frameworks to create intelligent, interpretable, and clinically
applicable AI systems for ophthalmology.

</details>


### [280] [Improve Retinal Artery/Vein Classification via Channel Couplin](https://arxiv.org/abs/2508.03738)
*Shuang Zeng,Chee Hong Lee,Kaiwen Li,Boxu Xie,Ourui Fu,Hangzhou He,Lei Zhu,Yanye Lu,Fangxiao Cheng*

Main category: eess.IV

TL;DR: 提出新损失函数进行视网膜血管分割和动静脉分类，在三个公开数据集取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 手动分割和分类视网膜血管及动静脉耗时、成本高且不一致，现有自动方法忽略解剖结构内在耦合关系。

Method: 设计Channel - Coupled Vessel Consistency Loss确保血管、动脉和静脉预测的一致性，引入intra - image pixel - level contrastive loss提取更具判别性的特征。

Result: 在RITE、LES - AV和HRF三个公开动静脉分类数据集取得SOTA结果。

Conclusion: 新方法有效解决现有自动方法问题，能准确进行视网膜血管分割和动静脉分类。

Abstract: Retinal vessel segmentation plays a vital role in analyzing fundus images for
the diagnosis of systemic and ocular diseases. Building on this, classifying
segmented vessels into arteries and veins (A/V) further enables the extraction
of clinically relevant features such as vessel width, diameter and tortuosity,
which are essential for detecting conditions like diabetic and hypertensive
retinopathy. However, manual segmentation and classification are
time-consuming, costly and inconsistent. With the advancement of Convolutional
Neural Networks, several automated methods have been proposed to address this
challenge, but there are still some issues. For example, the existing methods
all treat artery, vein and overall vessel segmentation as three separate binary
tasks, neglecting the intrinsic coupling relationships between these anatomical
structures. Considering artery and vein structures are subsets of the overall
retinal vessel map and should naturally exhibit prediction consistency with it,
we design a novel loss named Channel-Coupled Vessel Consistency Loss to enforce
the coherence and consistency between vessel, artery and vein predictions,
avoiding biasing the network toward three simple binary segmentation tasks.
Moreover, we also introduce a regularization term named intra-image pixel-level
contrastive loss to extract more discriminative feature-level fine-grained
representations for accurate retinal A/V classification. SOTA results have been
achieved across three public A/V classification datasets including RITE, LES-AV
and HRF. Our code will be available upon acceptance.

</details>


### [281] [A Modified VGG19-Based Framework for Accurate and Interpretable Real-Time Bone Fracture Detection](https://arxiv.org/abs/2508.03739)
*Md. Ehsanul Haque,Abrar Fahim,Shamik Dey,Syoda Anamika Jahan,S. M. Jahidul Islam,Sakib Rokoni,Md Sakib Morshed*

Main category: eess.IV

TL;DR: 本文提出基于改进VGG - 19模型的自动骨折检测框架，结合预处理技术和Grad - CAM增强可解释性，在实时网络应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 早期准确检测骨折很重要，但X光图像解读耗时易错，现有深度学习方法存在误分类和缺乏可解释性问题。

Method: 提出基于改进VGG - 19模型的自动检测框架，结合CLAHE、Otsu阈值和Canny边缘检测等预处理技术，使用Grad - CAM增强可解释性，并部署在实时网络应用中。

Result: 改进的VGG - 19模型分类准确率达99.78%，AUC分数为1.00。

Conclusion: 该框架为骨折检测提供了可靠、快速和可解释的解决方案，有助于诊断和患者护理。

Abstract: Early and accurate detection of the bone fracture is paramount to initiating
treatment as early as possible and avoiding any delay in patient treatment and
outcomes. Interpretation of X-ray image is a time consuming and error prone
task, especially when resources for such interpretation are limited by lack of
radiology expertise. Additionally, deep learning approaches used currently,
typically suffer from misclassifications and lack interpretable explanations to
clinical use. In order to overcome these challenges, we propose an automated
framework of bone fracture detection using a VGG-19 model modified to our
needs. It incorporates sophisticated preprocessing techniques that include
Contrast Limited Adaptive Histogram Equalization (CLAHE), Otsu's thresholding,
and Canny edge detection, among others, to enhance image clarity as well as to
facilitate the feature extraction. Therefore, we use Grad-CAM, an Explainable
AI method that can generate visual heatmaps of the model's decision making
process, as a type of model interpretability, for clinicians to understand the
model's decision making process. It encourages trust and helps in further
clinical validation. It is deployed in a real time web application, where
healthcare professionals can upload X-ray images and get the diagnostic
feedback within 0.5 seconds. The performance of our modified VGG-19 model
attains 99.78\% classification accuracy and AUC score of 1.00, making it
exceptionally good. The framework provides a reliable, fast, and interpretable
solution for bone fracture detection that reasons more efficiently for
diagnoses and better patient care.

</details>


### [282] [Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training](https://arxiv.org/abs/2508.03742)
*Weiwei Cao,Jianpeng Zhang,Zhongyi Shui,Sinuo Wang,Zeli Chen,Xi Li,Le Lu,Xianghua Ye,Tingbo Liang,Qi Zhang,Ling Zhang*

Main category: eess.IV

TL;DR: 本文提出提升视觉语义密度的方法改善医学图像与报告的对齐效果，在多个CT数据集实验取得SOTA零样本性能及良好迁移学习能力。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像低信噪比与报告高信噪比对齐时存在的语义密度差距和视觉对齐偏差问题。

Method: 一方面通过疾病级视觉对比学习增强视觉语义；另一方面引入解剖正常性建模方法，利用VQ - VAE在潜在空间重建正常视觉嵌入以放大异常信号。

Result: 在多个CT数据集上实验，实现SOTA零样本性能，在15个器官54种疾病上平均AUC达84.9%，超越现有方法，预训练模型有良好迁移学习能力。

Conclusion: 提出的提升视觉语义密度方法能有效提升医学图像与诊断报告的对齐效果，提高诊断性能。

Abstract: Vision-language pre-training (VLP) has great potential for developing
multifunctional and general medical diagnostic capabilities. However, aligning
medical images with a low signal-to-noise ratio (SNR) to reports with a high
SNR presents a semantic density gap, leading to visual alignment bias. In this
paper, we propose boosting vision semantic density to improve alignment
effectiveness. On one hand, we enhance visual semantics through disease-level
vision contrastive learning, which strengthens the model's ability to
differentiate between normal and abnormal samples for each anatomical
structure. On the other hand, we introduce an anatomical normality modeling
method to model the distribution of normal samples for each anatomy, leveraging
VQ-VAE for reconstructing normal vision embeddings in the latent space. This
process amplifies abnormal signals by leveraging distribution shifts in
abnormal samples, enhancing the model's perception and discrimination of
abnormal attributes. The enhanced visual representation effectively captures
the diagnostic-relevant semantics, facilitating more efficient and accurate
alignment with the diagnostic report. We conduct extensive experiments on two
chest CT datasets, CT-RATE and Rad-ChestCT, and an abdominal CT dataset,
MedVL-CT69K, and comprehensively evaluate the diagnosis performance across
multiple tasks in the chest and abdominal CT scenarios, achieving
state-of-the-art zero-shot performance. Notably, our method achieved an average
AUC of 84.9% across 54 diseases in 15 organs, significantly surpassing existing
methods. Additionally, we demonstrate the superior transfer learning
capabilities of our pre-trained model. Code is available at
https://github.com/alibaba-damo-academy/ViSD-Boost.

</details>


### [283] [Do We Need Pre-Processing for Deep Learning Based Ultrasound Shear Wave Elastography?](https://arxiv.org/abs/2508.03744)
*Sarah Grube,Sören Grünhagen,Sarah Latus,Michael Meyling,Alexander Schlaefer*

Main category: eess.IV

TL;DR: 研究深度学习超声剪切波弹性成像中超声预处理步骤的必要性，发现深度学习方法用原始射频数据也能可靠区分弹性组，可减少传统预处理步骤。


<details>
  <summary>Details</summary>
Motivation: 超声剪切波弹性成像在不同系统和处理流程中的通用性和标准化有限，探讨不同图像处理步骤对弹性分析的影响，研究深度学习方法中超声预处理步骤的必要性。

Method: 评估3D卷积神经网络从时空超声图像预测剪切波速度的性能，研究不同程度预处理的输入图像，将深度学习方法的预测结果与传统飞行时间法在四种不同弹性水平的明胶模型上进行比较。

Result: 各弹性组预测的剪切波速度有显著差异，预处理稍提升性能指标，深度学习方法用原始未处理射频数据能可靠区分弹性组。

Conclusion: 深度学习方法可减少超声剪切波弹性成像中传统超声预处理步骤及其带来的偏差，实现更快更可靠的临床弹性评估。

Abstract: Estimating the elasticity of soft tissue can provide useful information for
various diagnostic applications. Ultrasound shear wave elastography offers a
non-invasive approach. However, its generalizability and standardization across
different systems and processing pipelines remain limited. Considering the
influence of image processing on ultrasound based diagnostics, recent
literature has discussed the impact of different image processing steps on
reliable and reproducible elasticity analysis. In this work, we investigate the
need of ultrasound pre-processing steps for deep learning-based ultrasound
shear wave elastography. We evaluate the performance of a 3D convolutional
neural network in predicting shear wave velocities from spatio-temporal
ultrasound images, studying different degrees of pre-processing on the input
images, ranging from fully beamformed and filtered ultrasound images to raw
radiofrequency data. We compare the predictions from our deep learning approach
to a conventional time-of-flight method across four gelatin phantoms with
different elasticity levels. Our results demonstrate statistically significant
differences in the predicted shear wave velocity among all elasticity groups,
regardless of the degree of pre-processing. Although pre-processing slightly
improves performance metrics, our results show that the deep learning approach
can reliably differentiate between elasticity groups using raw, unprocessed
radiofrequency data. These results show that deep learning-based approaches
could reduce the need for and the bias of traditional ultrasound pre-processing
steps in ultrasound shear wave elastography, enabling faster and more reliable
clinical elasticity assessments.

</details>


### [284] [M$^3$HL: Mutual Mask Mix with High-Low Level Feature Consistency for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2508.03752)
*Yajun Liu,Zenghui Zhang,Jiang Yue,Weiwei Guo,Dongying Li*

Main category: eess.IV

TL;DR: 提出Mutual Mask Mix with High - Low level feature consistency (M$^3$HL)方法用于半监督医学图像分割，在基准数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有受CutMix启发的数据增强方法在半监督医学图像分割中应用方式僵化，且对特征级一致性约束关注不足。

Method: 提出M$^3$HL方法，包含M$^3$（基于MIM掩蔽策略的增强数据增强操作）和HL（层次一致性正则化框架）。

Result: 在ACDC和LA等医学图像分割基准数据集上达到了最先进的性能。

Conclusion: M$^3$HL方法能有效解决现有方法的问题，提升半监督医学图像分割性能。

Abstract: Data augmentation methods inspired by CutMix have demonstrated significant
potential in recent semi-supervised medical image segmentation tasks. However,
these approaches often apply CutMix operations in a rigid and inflexible
manner, while paying insufficient attention to feature-level consistency
constraints. In this paper, we propose a novel method called Mutual Mask Mix
with High-Low level feature consistency (M$^3$HL) to address the aforementioned
challenges, which consists of two key components: 1) M$^3$: An enhanced data
augmentation operation inspired by the masking strategy from Masked Image
Modeling (MIM), which advances conventional CutMix through dynamically
adjustable masks to generate spatially complementary image pairs for
collaborative training, thereby enabling effective information fusion between
labeled and unlabeled images. 2) HL: A hierarchical consistency regularization
framework that enforces high-level and low-level feature consistency between
unlabeled and mixed images, enabling the model to better capture discriminative
feature representations.Our method achieves state-of-the-art performance on
widely adopted medical image segmentation benchmarks including the ACDC and LA
datasets. Source code is available at https://github.com/PHPJava666/M3HL

</details>


### [285] [When Deep Learning Fails: Limitations of Recurrent Models on Stroke-Based Handwriting for Alzheimer's Disease Detection](https://arxiv.org/abs/2508.03773)
*Emanuele Nardone,Tiziana D'Alessandro,Francesco Fontanella,Claudio De Stefano*

Main category: eess.IV

TL;DR: 研究探索用深度学习通过笔迹分析无创检测阿尔茨海默病，对比循环神经网络和传统机器学习模型，传统方法表现更优，指出研究局限与未来方向。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病检测依赖昂贵神经影像或侵入性程序，限制可及性，探索用深度学习笔迹分析进行无创检测。

Method: 使用34种笔迹任务数据集，对比三种循环神经架构（LSTM、GRU、RNN）与传统机器学习模型，循环模型处理离散笔画预提取特征。

Result: 循环模型特异性差、方差高，传统集成方法显著优于所有深度架构，准确率更高且指标平衡。

Conclusion: 循环架构用于离散笔画特征向量时失败，深度学习模型无法克服架构假设与笔迹数据本质的脱节，研究指出数据表示和模型兼容性问题及未来研究方向。

Abstract: Alzheimer's disease detection requires expensive neuroimaging or invasive
procedures, limiting accessibility. This study explores whether deep learning
can enable non-invasive Alzheimer's disease detection through handwriting
analysis. Using a dataset of 34 distinct handwriting tasks collected from
healthy controls and Alzheimer's disease patients, we evaluate and compare
three recurrent neural architectures (LSTM, GRU, RNN) against traditional
machine learning models. A crucial distinction of our approach is that the
recurrent models process pre-extracted features from discrete strokes, not raw
temporal signals. This violates the assumption of a continuous temporal flow
that recurrent networks are designed to capture. Results reveal that they
exhibit poor specificity and high variance. Traditional ensemble methods
significantly outperform all deep architectures, achieving higher accuracy with
balanced metrics. This demonstrates that recurrent architectures, designed for
continuous temporal sequences, fail when applied to feature vectors extracted
from ambiguously segmented strokes. Despite their complexity, deep learning
models cannot overcome the fundamental disconnect between their architectural
assumptions and the discrete, feature-based nature of stroke-level handwriting
data. Although performance is limited, the study highlights several critical
issues in data representation and model compatibility, pointing to valuable
directions for future research.

</details>


### [286] [Assessing the Impact of Image Super Resolution on White Blood Cell Classification Accuracy](https://arxiv.org/abs/2508.03759)
*Tatwadarshi P. Nagarhalli,Shruti S. Pawar,Soham A. Dahanukar,Uday Aswalekar,Ashwini M. Save,Sanket D. Patil*

Main category: eess.IV

TL;DR: 研究图像增强技术对白细胞图像分类性能的影响，结合普通和增强图像训练模型，以提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 显微镜下白细胞图像分辨率低影响分类准确性，需研究图像增强技术对分类性能的影响。

Method: 采用大图像尺寸上采样，结合普通和增强图像训练深度学习模型，用知名图像分类模型测试评估。

Result: 未提及具体结果。

Conclusion: 旨在通过理解普通和增强图像的权衡，为特定白细胞数据集创建更有效的图像识别算法。

Abstract: Accurately classifying white blood cells from microscopic images is essential
to identify several illnesses and conditions in medical diagnostics. Many deep
learning technologies are being employed to quickly and automatically classify
images. However, most of the time, the resolution of these microscopic pictures
is quite low, which might make it difficult to classify them correctly. Some
picture improvement techniques, such as image super-resolution, are being
utilized to improve the resolution of the photos to get around this issue. The
suggested study uses large image dimension upscaling to investigate how
picture-enhancing approaches affect classification performance. The study
specifically looks at how deep learning models may be able to understand more
complex visual information by capturing subtler morphological changes when
image resolution is increased using cutting-edge techniques. The model may
learn from standard and augmented data since the improved images are
incorporated into the training process. This dual method seeks to comprehend
the impact of image resolution on model performance and enhance classification
accuracy. A well-known model for picture categorization is used to conduct
extensive testing and thoroughly evaluate the effectiveness of this approach.
This research intends to create more efficient image identification algorithms
customized to a particular dataset of white blood cells by understanding the
trade-offs between ordinary and enhanced images.

</details>


### [287] [Conditional Fetal Brain Atlas Learning for Automatic Tissue Segmentation](https://arxiv.org/abs/2508.04522)
*Johannes Tischer,Patric Kienast,Marlene Stümpflen,Gregor Kasprian,Georg Langs,Roxane Licandro*

Main category: eess.IV

TL;DR: 本文提出用于生成连续、特定年龄胎儿脑图谱的深度学习框架，可实时进行胎儿脑组织分割，有高准确性和良好性能，能助力研究与临床应用。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑磁共振成像评估因脑成熟度、成像协议和胎龄估计不确定而具有挑战性，需要脑图谱提供标准化参考框架。

Method: 引入结合直接配准模型和条件判别器的深度学习框架，在219个21 - 37周典型胎儿MRI数据集上训练。

Result: 实现高配准精度，能捕捉动态解剖变化，平均Dice相似系数达86.3%，体积分析揭示典型神经生长轨迹。

Conclusion: 该方法能以最少预处理和实时性能进行个体化发育评估，支持研究和临床应用，代码开源。

Abstract: Magnetic Resonance Imaging (MRI) of the fetal brain has become a key tool for
studying brain development in vivo. Yet, its assessment remains challenging due
to variability in brain maturation, imaging protocols, and uncertain estimates
of Gestational Age (GA). To overcome these, brain atlases provide a
standardized reference framework that facilitates objective evaluation and
comparison across subjects by aligning the atlas and subjects in a common
coordinate system. In this work, we introduce a novel deep-learning framework
for generating continuous, age-specific fetal brain atlases for real-time fetal
brain tissue segmentation. The framework combines a direct registration model
with a conditional discriminator. Trained on a curated dataset of 219
neurotypical fetal MRIs spanning from 21 to 37 weeks of gestation. The method
achieves high registration accuracy, captures dynamic anatomical changes with
sharp structural detail, and robust segmentation performance with an average
Dice Similarity Coefficient (DSC) of 86.3% across six brain tissues.
Furthermore, volumetric analysis of the generated atlases reveals detailed
neurotypical growth trajectories, providing valuable insights into the
maturation of the fetal brain. This approach enables individualized
developmental assessment with minimal pre-processing and real-time performance,
supporting both research and clinical applications. The model code is available
at https://github.com/cirmuw/fetal-brain-atlas

</details>


### [288] [LA-CaRe-CNN: Cascading Refinement CNN for Left Atrial Scar Segmentation](https://arxiv.org/abs/2508.04553)
*Franz Thaler,Darko Stern,Gernot Plank,Martin Urschler*

Main category: eess.IV

TL;DR: 提出LA - CaRe - CNN用于从LGE MR扫描中准确分割左心房及左心房疤痕组织，取得良好分割结果，对生成患者特异性心脏数字孪生模型有潜力。


<details>
  <summary>Details</summary>
Motivation: 患者特异性心脏数字孪生模型用于个性化消融治疗有潜力，但需要准确分割健康和疤痕组织，现有方法有待提升。

Method: 提出LA - CaRe - CNN，是一个3D端到端训练的2阶段CNN级联网络，还采用强强度和空间增强处理训练数据集。

Result: 基于5折集成的方法在左心房分割取得89.21% DSC和1.6969 mm ASSD，在左心房疤痕组织分割取得64.59% DSC和91.80% G - DSC的结果。

Conclusion: LA - CaRe - CNN分割结果对生成患者特异性心脏数字孪生模型及个性化消融治疗AF有很大潜力。

Abstract: Atrial fibrillation (AF) represents the most prevalent type of cardiac
arrhythmia for which treatment may require patients to undergo ablation
therapy. In this surgery cardiac tissues are locally scarred on purpose to
prevent electrical signals from causing arrhythmia. Patient-specific cardiac
digital twin models show great potential for personalized ablation therapy,
however, they demand accurate semantic segmentation of healthy and scarred
tissue typically obtained from late gadolinium enhanced (LGE) magnetic
resonance (MR) scans. In this work we propose the Left Atrial Cascading
Refinement CNN (LA-CaRe-CNN), which aims to accurately segment the left atrium
as well as left atrial scar tissue from LGE MR scans. LA-CaRe-CNN is a 2-stage
CNN cascade that is trained end-to-end in 3D, where Stage 1 generates a
prediction for the left atrium, which is then refined in Stage 2 in conjunction
with the original image information to obtain a prediction for the left atrial
scar tissue. To account for domain shift towards domains unknown during
training, we employ strong intensity and spatial augmentation to increase the
diversity of the training dataset. Our proposed method based on a 5-fold
ensemble achieves great segmentation results, namely, 89.21% DSC and 1.6969 mm
ASSD for the left atrium, as well as 64.59% DSC and 91.80% G-DSC for the more
challenging left atrial scar tissue. Thus, segmentations obtained through
LA-CaRe-CNN show great potential for the generation of patient-specific cardiac
digital twin models and downstream tasks like personalized targeted ablation
therapy to treat AF.

</details>


### [289] [A Comprehensive Framework for Uncertainty Quantification of Voxel-wise Supervised Models in IVIM MRI](https://arxiv.org/abs/2508.04588)
*Nicola Casali,Alessandro Brusaferri,Giuseppe Baselli,Stefano Fumagalli,Edoardo Micotti,Gianluigi Forloni,Riaz Hussein,Giovanna Rizzo,Alfonso Mastropietro*

Main category: eess.IV

TL;DR: 提出基于深度集成混合密度网络的概率深度学习框架估计IVIM参数并量化不确定性，在模拟和体内数据集评估，结果表明能识别不可靠估计，还可用于其他物理模型拟合。


<details>
  <summary>Details</summary>
Motivation: 准确估计扩散加权MRI中体素内不相干运动（IVIM）参数因反问题不适定性和对噪声敏感而具有挑战性。

Method: 提出基于深度集成（DE）的混合密度网络（MDNs）的概率深度学习框架，在合成数据上进行监督训练，在模拟和两个体内数据集评估，用校准曲线等评估不确定性可靠性。

Result: MDNs为D和f参数产生更校准和更尖锐的预测分布，在D*上有轻微过度自信；与高斯模型相比，MDNs对D*的体内估计更平滑；体内较高的认知不确定性表明与实际采集条件不匹配。

Conclusion: 提出了一个带不确定性量化的IVIM拟合综合框架，能识别和解释不可靠估计，可通过适当调整用于其他物理模型拟合。

Abstract: Accurate estimation of intravoxel incoherent motion (IVIM) parameters from
diffusion-weighted MRI remains challenging due to the ill-posed nature of the
inverse problem and high sensitivity to noise, particularly in the perfusion
compartment. In this work, we propose a probabilistic deep learning framework
based on Deep Ensembles (DE) of Mixture Density Networks (MDNs), enabling
estimation of total predictive uncertainty and decomposition into aleatoric
(AU) and epistemic (EU) components. The method was benchmarked against non
probabilistic neural networks, a Bayesian fitting approach and a probabilistic
network with single Gaussian parametrization. Supervised training was performed
on synthetic data, and evaluation was conducted on both simulated and two in
vivo datasets. The reliability of the quantified uncertainties was assessed
using calibration curves, output distribution sharpness, and the Continuous
Ranked Probability Score (CRPS). MDNs produced more calibrated and sharper
predictive distributions for the D and f parameters, although slight
overconfidence was observed in D*. The Robust Coefficient of Variation (RCV)
indicated smoother in vivo estimates for D* with MDNs compared to Gaussian
model. Despite the training data covering the expected physiological range,
elevated EU in vivo suggests a mismatch with real acquisition conditions,
highlighting the importance of incorporating EU, which was allowed by DE.
Overall, we present a comprehensive framework for IVIM fitting with uncertainty
quantification, which enables the identification and interpretation of
unreliable estimates. The proposed approach can also be adopted for fitting
other physical models through appropriate architectural and simulation
adjustments.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [290] [A Hybrid AI Methodology for Generating Ontologies of Research Topics from Scientific Paper Corpora](https://arxiv.org/abs/2508.04213)
*Alessia Pisu,Livio Pompianu,Francesco Osborne,Diego Reforgiato Recupero,Daniele Riboni,Angelo Salatino*

Main category: cs.DL

TL;DR: 本文提出半自动化方法Sci - OG生成研究主题本体，经评估表现优，还有实际应用案例。


<details>
  <summary>Details</summary>
Motivation: 传统研究主题分类法和本体手动策展耗时、易过时且粒度有限，需改进方法。

Method: 采用多步骤方法，包括主题发现、关系分类和本体构建，关系分类集成基于编码器的语言模型与文献主题出现特征。

Result: 使用21,649个手动注释语义三元组数据集评估，该方法F1分数达0.951，超越多种竞争方法。

Conclusion: 该解决方案可改善科学知识的可访问性、组织和分析，支持人工智能文献管理和研究探索的进步。

Abstract: Taxonomies and ontologies of research topics (e.g., MeSH, UMLS, CSO, NLM)
play a central role in providing the primary framework through which
intelligent systems can explore and interpret the literature. However, these
resources have traditionally been manually curated, a process that is
time-consuming, prone to obsolescence, and limited in granularity. This paper
presents Sci-OG, a semi-auto\-mated methodology for generating research topic
ontologies, employing a multi-step approach: 1) Topic Discovery, extracting
potential topics from research papers; 2) Relationship Classification,
determining semantic relationships between topic pairs; and 3) Ontology
Construction, refining and organizing topics into a structured ontology. The
relationship classification component, which constitutes the core of the
system, integrates an encoder-based language model with features describing
topic occurrence in the scientific literature. We evaluate this approach
against a range of alternative solutions using a dataset of 21,649 manually
annotated semantic triples. Our method achieves the highest F1 score (0.951),
surpassing various competing approaches, including a fine-tuned SciBERT model
and several LLM baselines, such as the fine-tuned GPT4-mini. Our work is
corroborated by a use case which illustrates the practical application of our
system to extend the CSO ontology in the area of cybersecurity. The presented
solution is designed to improve the accessibility, organization, and analysis
of scientific knowledge, thereby supporting advancements in AI-enabled
literature management and research exploration.

</details>


### [291] [Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers](https://arxiv.org/abs/2508.03962)
*Paris Koloveas,Serafeim Chatzopoulos,Dionysis Diamantis,Christos Tryfonopoulos,Thanasis Vergoulis*

Main category: cs.DL

TL;DR: 为解决科研人员理解文献难题，为BIP! Finder引入总结功能，可生成两种摘要加速文献发现与理解。


<details>
  <summary>Details</summary>
Motivation: 科学文献数量增长，科研人员从文献列表到综合理解主题存在困难，需解决研究工作流程中的关键瓶颈。

Method: 为基于不同影响方面对文献排名的学术搜索引擎BIP! Finder引入总结功能，利用其已有基于影响的排名和过滤功能生成摘要。

Result: 用户可从排名靠前的搜索结果中生成两种摘要，分别用于快速浏览和更深入理解。

Conclusion: 该功能能动态生成上下文相关、综合的叙述，显著加速文献发现和理解。

Abstract: The growing volume of scientific literature makes it challenging for
scientists to move from a list of papers to a synthesized understanding of a
topic. Because of the constant influx of new papers on a daily basis, even if a
scientist identifies a promising set of papers, they still face the tedious
task of individually reading through dozens of titles and abstracts to make
sense of occasionally conflicting findings. To address this critical bottleneck
in the research workflow, we introduce a summarization feature to BIP! Finder,
a scholarly search engine that ranks literature based on distinct impact
aspects like popularity and influence. Our approach enables users to generate
two types of summaries from top-ranked search results: a concise summary for an
instantaneous at-a-glance comprehension and a more comprehensive literature
review-style summary for greater, better-organized comprehension. This ability
dynamically leverages BIP! Finder's already existing impact-based ranking and
filtering features to generate context-sensitive, synthesized narratives that
can significantly accelerate literature discovery and comprehension.

</details>


### [292] [Identity Theft in AI Conference Peer Review](https://arxiv.org/abs/2508.04024)
*Nihar B. Shah,Melisa Bok,Xukun Liu,Andrew McCallum*

Main category: cs.DL

TL;DR: 讨论人工智能研究同行评审中身份盗窃新案例，提出应对策略。


<details>
  <summary>Details</summary>
Motivation: 揭示科学同行评审过程中身份盗窃问题及其对学术程序的广泛影响。

Method: 详细阐述不诚实研究人员利用评审员招募流程和身份验证过程的弱点，创建虚假评审员档案操纵论文评估。

Result: 发现同行评审和整个学术界迫切需要加强防范身份盗窃的保障措施。

Conclusion: 提出了减轻身份盗窃问题的策略。

Abstract: We discuss newly uncovered cases of identity theft in the scientific
peer-review process within artificial intelligence (AI) research, with broader
implications for other academic procedures. We detail how dishonest researchers
exploit the peer-review system by creating fraudulent reviewer profiles to
manipulate paper evaluations, leveraging weaknesses in reviewer recruitment
workflows and identity verification processes. The findings highlight the
critical need for stronger safeguards against identity theft in peer review and
academia at large, and to this end, we also propose mitigating strategies.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [293] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)
*Francisco Bolaños,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: 本文提出新注释模式，用Sci - Sentence基准评估37个大语言模型分类修辞角色，实验有新发现，如微调模型表现好等。


<details>
  <summary>Details</summary>
Motivation: 现有AI分析科学文献方法需定义相关注释模式和大规模注释策略以支持高质量文献综述生成。

Method: 引入新注释模式，用Sci - Sentence基准对37个不同类型和大小的大语言模型，采用零样本学习和微调方法进行评估。

Result: 微调高质量数据的模型F1值超96%；大的专有模型结果最佳，部分轻量级开源模型表现好；用大语言模型生成半合成示例丰富训练数据有益。

Conclusion: 实验结果推动了该领域技术发展，为大语言模型在科学文献修辞角色分类提供了新见解。

Abstract: Previous work has demonstrated that AI methods for analysing scientific
literature benefit significantly from annotating sentences in papers according
to their rhetorical roles, such as research gaps, results, limitations,
extensions of existing methodologies, and others. Such representations also
have the potential to support the development of a new generation of systems
capable of producing high-quality literature reviews. However, achieving this
goal requires the definition of a relevant annotation schema and effective
strategies for large-scale annotation of the literature. This paper addresses
these challenges by 1) introducing a novel annotation schema specifically
designed to support literature review generation and 2) conducting a
comprehensive evaluation of a wide range of state-of-the-art large language
models (LLMs) in classifying rhetorical roles according to this schema. To this
end, we also present Sci-Sentence, a novel multidisciplinary benchmark
comprising 700 sentences manually annotated by domain experts and 2,240
sentences automatically labelled using LLMs. We evaluate 37 LLMs on this
benchmark, spanning diverse model families and sizes, using both zero-shot
learning and fine-tuning approaches. The experiments yield several novel
insights that advance the state of the art in this challenging domain. First,
the current generation of LLMs performs remarkably well on this task when
fine-tuned on high-quality data, achieving performance levels above 96\% F1.
Second, while large proprietary models like GPT-4o achieve the best results,
some lightweight open-source alternatives also demonstrate excellent
performance. Finally, enriching the training data with semi-synthetic examples
generated by LLMs proves beneficial, enabling small encoders to achieve robust
results and significantly enhancing the performance of several open decoder
models.

</details>


### [294] [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)
*Xu Zhang,Mei Chen*

Main category: cs.CL

TL;DR: 研究评估先进NLP技术挖掘事故描述以提升事故数据质量，对比三类模型，微调的transformer性能优，结果显示精度、效率和数据需求的权衡关系，给出实用部署建议。


<details>
  <summary>Details</summary>
Motivation: 评估先进自然语言处理技术，通过挖掘事故描述提升事故数据质量。

Method: 以肯塔基州二次事故识别为案例，从2015 - 2022年的16656条手动审核描述中选取数据，对比零样本开源大语言模型、微调transformer和传统逻辑回归三类模型，用2015 - 2021年数据校准，2022年的1771条描述测试。

Result: 微调的transformer性能最优，RoBERTa的F1分数和准确率最高；零样本LLaMA3:70B的F1接近但推理时间长；逻辑回归基线表现差；LLMs部分变体召回率高但计算成本高；中型LLMs性能可媲美大型且减少运行时间。

Conclusion: 结果体现了精度、效率和数据需求的权衡，微调transformer模型能有效平衡精度和召回率，给出隐私保护本地部署、集成方法和增量处理等实用部署建议。

Abstract: This study evaluates advanced natural language processing (NLP) techniques to
enhance crash data quality by mining crash narratives, using secondary crash
identification in Kentucky as a case study. Drawing from 16,656 manually
reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we
compare three model classes: zero-shot open-source large language models (LLMs)
(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers
(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic
regression as baseline. Models were calibrated on 2015-2021 data and tested on
1,771 narratives from 2022. Fine-tuned transformers achieved superior
performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy
(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139
minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs
excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred
high computational costs (up to 723 minutes for DeepSeek-R1:70B), while
fine-tuned models processed the test set in seconds after brief training.
Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can
rival larger counterparts in performance while reducing runtime, suggesting
opportunities for optimized deployments. Results highlight trade-offs between
accuracy, efficiency, and data requirements, with fine-tuned transformer models
balancing precision and recall effectively on Kentucky data. Practical
deployment considerations emphasize privacy-preserving local deployment,
ensemble approaches for improved accuracy, and incremental processing for
scalability, providing a replicable scheme for enhancing crash-data quality
with advanced NLP.

</details>


### [295] [TURA: Tool-Augmented Unified Retrieval Agent for AI Search](https://arxiv.org/abs/2508.04604)
*Zhejun Zhao,Yuehu Dong,Alley Liu,Lixue Zheng,Pingsheng Liu,Dongdong Shen,Long Xia,Jiashu Zhao,Dawei Yin*

Main category: cs.CL

TL;DR: 文章指出传统RAG用于搜索引擎有局限，提出TURA框架结合RAG与工具使用获取动静信息，能满足大规模系统低延迟需求。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在搜索引擎应用有工业局限，难以处理实时需求和结构化查询，学术研究忽视复杂意图和动态数据源，需新方法弥补差距。

Method: 引入TURA框架，包含意图感知检索模块、基于DAG的任务规划器和轻量级蒸馏代理执行器。

Result: TURA能服务数千万用户，利用代理框架提供可靠实时答案。

Conclusion: TURA是首个系统弥补静态RAG与动态信息源差距的架构，可打造世界级AI搜索产品。

Abstract: The advent of Large Language Models (LLMs) is transforming search engines
into conversational AI search products, primarily using Retrieval-Augmented
Generation (RAG) on web corpora. However, this paradigm has significant
industrial limitations. Traditional RAG approaches struggle with real-time
needs and structured queries that require accessing dynamically generated
content like ticket availability or inventory. Limited to indexing static
pages, search engines cannot perform the interactive queries needed for such
time-sensitive data. Academic research has focused on optimizing RAG for static
content, overlooking complex intents and the need for dynamic sources like
databases and real-time APIs. To bridge this gap, we introduce TURA
(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage
framework that combines RAG with agentic tool-use to access both static content
and dynamic, real-time information. TURA has three key components: an
Intent-Aware Retrieval module to decompose queries and retrieve information
sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task
Planner that models task dependencies as a Directed Acyclic Graph (DAG) for
optimal parallel execution, and a lightweight Distilled Agent Executor for
efficient tool calling. TURA is the first architecture to systematically bridge
the gap between static RAG and dynamic information sources for a world-class AI
search product. Serving tens of millions of users, it leverages an agentic
framework to deliver robust, real-time answers while meeting the low-latency
demands of a large-scale industrial system.

</details>


### [296] [Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider](https://arxiv.org/abs/2508.04623)
*Chirag Seth,Utkarsh Singh*

Main category: cs.CL

TL;DR: 研究在低资源设置下评估三个轻量级transformer模型进行文本到SQL转换，T5 - Small表现最佳，强调了紧凑transformer在资源稀缺环境的潜力。


<details>
  <summary>Details</summary>
Motivation: 让非专业用户能用自然语言查询关系数据库，在低资源设置下评估轻量级transformer模型用于文本到SQL转换。

Method: 开发可复用、模型无关的管道，针对不同模型架构调整模式格式，训练1000到5000次迭代，用LFAcc、BLEU和EM指标在1000个测试样本上评估。

Result: 微调后的T5 - Small的LFAcc最高为27.8%，优于BART - Small和GPT - 2。

Conclusion: 紧凑transformers在资源稀缺环境下有实现易访问文本到SQL解决方案的潜力，管道模块化支持未来改进。

Abstract: Text-to-SQL translation enables non-expert users to query relational
databases using natural language, with applications in education and business
intelligence. This study evaluates three lightweight transformer models -
T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on
low-resource settings. We developed a reusable, model-agnostic pipeline that
tailors schema formatting to each model's architecture, training them across
1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form
Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small
achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2
(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL
generation. Despite resource constraints limiting performance, our pipeline's
modularity supports future enhancements, such as advanced schema linking or
alternative base models. This work underscores the potential of compact
transformers for accessible text-to-SQL solutions in resource-scarce
environments.

</details>


### [297] [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)
*Abhay Vijayvargia,Ajay Nagpal,Kundeshwar Pundalik,Atharva Savarkar,Smita Gautam,Pankaj Singh,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文介绍了AI农业聊天机器人Krishi Sathi，它通过多轮对话、IFT模型和RAG技术为印度农民提供农业建议，结果显示其查询响应准确率等指标表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决印度农民，尤其是农村低识字率地区农民，难以获得及时、易获取且语言友好的农业建议的问题。

Method: 采用IFT模型，在三个精选数据集上微调，结合多轮对话流程、Retrieval-Augmented Generation (RAG)技术，支持英语和印地语，具备语音输入输出功能。

Result: 系统查询响应准确率达97.53%，上下文相关性和个性化达91.35%，查询完成率97.53%，平均响应时间在6秒以内。

Conclusion: 结合意图驱动对话流、指令调优模型和基于检索的生成可以提高印度数字农业支持的质量和可及性。

Abstract: Indian farmers often lack timely, accessible, and language-friendly
agricultural advice, especially in rural areas with low literacy. To address
this gap in accessibility, this paper presents a novel AI-powered agricultural
chatbot, Krishi Sathi, designed to support Indian farmers by providing
personalized, easy-to-understand answers to their queries through both text and
speech. The system's intelligence stems from an IFT model, subsequently refined
through fine-tuning on Indian agricultural knowledge across three curated
datasets. Unlike traditional chatbots that respond to one-off questions, Krishi
Sathi follows a structured, multi-turn conversation flow to gradually collect
the necessary details from the farmer, ensuring the query is fully understood
before generating a response. Once the intent and context are extracted, the
system performs Retrieval-Augmented Generation (RAG) by first fetching
information from a curated agricultural database and then generating a tailored
response using the IFT model. The chatbot supports both English and Hindi
languages, with speech input and output features (via ASR and TTS) to make it
accessible for users with low literacy or limited digital skills. This work
demonstrates how combining intent-driven dialogue flows, instruction-tuned
models, and retrieval-based generation can improve the quality and
accessibility of digital agricultural support in India.
  This approach yielded strong results, with the system achieving a query
response accuracy of 97.53%, 91.35% contextual relevance and personalization,
and a query completion rate of 97.53%. The average response time remained under
6 seconds, ensuring timely support for users across both English and Hindi
interactions.

</details>


### [298] [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)
*Ashutosh Bandooni,Brindha Subburaj*

Main category: cs.CL

TL;DR: 本文介绍了多语言数学视觉推理基准GanitBench，评估了模型表现，发现GPT - 4o mini较优，双锁约束降低性能，双样本思维链更有效，模型处理印地语问题时性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型推理基准多为单语（英语），印地语除理解和翻译外缺乏相关数据集，因此引入多语言数学视觉推理基准。

Method: 引入包含1527个视觉问题的GanitBench基准，收集自印度两场重要考试，用零样本和双样本思维链设置评估两个闭源模型，通过双锁约束评估模型。

Result: GPT - 4o mini在基准上表现更优，最高平均准确率38.15%；双锁约束使模型性能大幅下降；双样本思维链在该环境更有效；模型处理印地语问题时性能下降。

Conclusion: 希望通过工作推动印地语等语言纳入研究。

Abstract: Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on
several fields and domains are being curated more frequently over the last few
years. However these are often monolingual, mostly available in English.
Additionally there also is a lack of datasets available in Hindi on tasks apart
from comprehension and translation. We introduce GanitBench, a tough benchmark
consisting of 1527 vision-only questions covering several topics in Mathematics
- available in languages English and Hindi. Collected from two major
examinations from India, the JEE Advanced and the CBSE Boards examinations,
this benchmark includes questions in the form of images comprising of figures
essential to a question as well as text. We evaluate two closed source models
for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.
GPT-4o mini is found to be the more dominant model on the benchmark, with it's
highest average accuracy being 38.15%. We also evaluate models through a
"Double Lock" constraint, which brings down the performance of the models by
considerable margins. We observe that two-shot CoT appears to be a more
effective setting under this environment. Performance of the two VLMs also
decreases when answering the same questions in the Hindi language. We hope to
facilitate the inclusion of languages like Hindi in research through our work.

</details>


### [299] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
*Subhey Sadi Rahman,Md. Adnanul Islam,Md. Mahbub Alam,Musarrat Zeba,Md. Abdur Rahman,Sadia Sultana Chowa,Mohaimenul Azam Khan Raiaan,Sami Azam*

Main category: cs.CL

TL;DR: 该综述分析大语言模型（LLMs）生成内容事实准确性评估，指出挑战，提出研究问题，强调构建准确、可解释且适配特定领域的LLMs的重要性。


<details>
  <summary>Details</summary>
Motivation: 因LLMs训练语料含不准确信息，会生成错误信息，所以需进行强大的事实核查。

Method: 系统分析LLMs生成内容的事实准确性评估，提出五个研究问题，探讨相关技术和方法。

Result: 发现当前评估指标有局限，用外部证据支持输出有价值，特定领域定制很重要。

Conclusion: 强调构建准确、可解释且适配特定领域事实核查的LLMs的重要性，推动更可信、有上下文感知的语言模型研究。

Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

</details>


### [300] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
*Paul Richmond,Prarit Agarwal,Borun Chowdhury,Vasilis Niarchos,Constantinos Papageorgakis*

Main category: cs.CL

TL;DR: 本文提出针对高能理论物理的专业大语言模型，基于Llama - 3.1微调，对比不同训练集和微调方法，模型在任务中表现优于基础模型，并与商业大模型对比。


<details>
  <summary>Details</summary>
Motivation: 开发适用于高能理论物理领域的专业大语言模型。

Method: 对80亿参数的Llama - 3.1模型进行微调，训练集为arXiv摘要不同组合，使用两种低秩适应微调方法和不同数据集大小。

Result: 微调后的模型在hep - th摘要完成任务上优于基础模型。

Conclusion: 对比了与商业大模型的性能，为进一步开发高能理论物理专业语言模型提供见解。

Abstract: We present specialized Large Language Models for theoretical High-Energy
Physics, obtained as 20 fine-tuned variants of the 8-billion parameter
Llama-3.1 model. Each variant was trained on arXiv abstracts (through August
2024) from different combinations of hep-th, hep-ph and gr-qc. For a
comparative study, we also trained models on datasets that contained abstracts
from disparate fields such as the q-bio and cs categories. All models were
fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and
varying dataset sizes, and outperformed the base model on hep-th abstract
completion tasks. We compare performance against leading commercial LLMs
(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing
specialized language models for High-Energy Theoretical Physics.

</details>


### [301] [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)
*Alok Abhishek,Lisa Erickson,Tushar Bandopadhyay*

Main category: cs.CL

TL;DR: 本文介绍机器学习模型全生命周期治理、评估和量化偏差的方法，基于BEATS探讨大语言模型偏差与公平性问题及治理框架，以提升生成式AI系统安全性与责任性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中普遍存在的偏差、公平性、伦理和事实性问题，推动负责任和符合伦理的生成式AI应用的创建与部署。

Method: 基于BEATS，构建数据和AI治理框架，对大语言模型进行全生命周期管理，包括开发验证、生产监控等。

Result: 该治理方法适用于实际应用，能在生产部署前对大语言模型进行严格基准测试，便于实时评估和主动管理生成的响应。

Conclusion: 实施数据和AI全生命周期治理可提升生成式AI系统的安全性和责任性，降低歧视风险，保护声誉。

Abstract: In this paper, we cover approaches to systematically govern, assess and
quantify bias across the complete life cycle of machine learning models, from
initial development and validation to ongoing production monitoring and
guardrail implementation. Building upon our foundational work on the Bias
Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the
authors share prevalent bias and fairness related gaps in Large Language Models
(LLMs) and discuss data and AI governance framework to address Bias, Ethics,
Fairness, and Factuality within LLMs. The data and AI governance approach
discussed in this paper is suitable for practical, real-world applications,
enabling rigorous benchmarking of LLMs prior to production deployment,
facilitating continuous real-time evaluation, and proactively governing LLM
generated responses. By implementing the data and AI governance across the life
cycle of AI development, organizations can significantly enhance the safety and
responsibility of their GenAI systems, effectively mitigating risks of
discrimination and protecting against potential reputational or brand-related
harm. Ultimately, through this article, we aim to contribute to advancement of
the creation and deployment of socially responsible and ethically aligned
generative artificial intelligence powered applications.

</details>


### [302] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)
*Bohan Jiang,Dawei Li,Zhen Tan,Chengshuai Zhao,Huan Liu*

Main category: cs.CL

TL;DR: 构建含43,880条福祉概念解释的数据集，提出评估框架，用SFT和DPO微调开源模型提升解释质量。


<details>
  <summary>Details</summary>
Motivation: 随着人们用大语言模型了解福祉，需解决其生成的解释能否准确且适配不同受众的问题。

Method: 构建大规模数据集，引入原则导向的双裁判评估框架，用SFT和DPO微调开源大语言模型。

Result: 大语言模型裁判与人类评估结果相符；不同模型、受众和类别下解释质量差异大；DPO和SFT微调模型表现更好。

Conclusion: 基于偏好的学习对专业解释任务有效。

Abstract: Well-being encompasses mental, physical, and social dimensions essential to
personal growth and informed life decisions. As individuals increasingly
consult Large Language Models (LLMs) to understand well-being, a key challenge
emerges: Can LLMs generate explanations that are not only accurate but also
tailored to diverse audiences? High-quality explanations require both factual
correctness and the ability to meet the expectations of users with varying
expertise. In this work, we construct a large-scale dataset comprising 43,880
explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We
introduce a principle-guided LLM-as-a-judge evaluation framework, employing
dual judges to assess explanation quality. Furthermore, we show that
fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO) can significantly enhance the quality of
generated explanations. Our results reveal: (1) The proposed LLM judges align
well with human evaluations; (2) explanation quality varies significantly
across models, audiences, and categories; and (3) DPO- and SFT-finetuned models
outperform their larger counterparts, demonstrating the effectiveness of
preference-based learning for specialized explanation tasks.

</details>


### [303] [HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization](https://arxiv.org/abs/2508.04010)
*Yurun Chen,Xavier Hu,Yuhan Liu,Keting Yin,Juncheng Li,Zhuosheng Zhang,Shengyu Zhang*

Main category: cs.CL

TL;DR: 提出HarmonyGuard框架解决网络代理在长序列操作中平衡安全与效用的问题，评估显示其有较好效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究局限于单目标优化或单轮场景，缺乏在网络环境中对安全和效用的协同优化能力。

Method: 提出HarmonyGuard多智能体协作框架，包含自适应策略增强和双目标优化两个基本能力。

Result: 在多个基准测试中，HarmonyGuard相比现有基线提高策略合规性达38%、任务完成率达20%，所有任务策略合规性超90%。

Conclusion: HarmonyGuard能有效在网络环境中协同优化安全和效用。

Abstract: Large language models enable agents to autonomously perform tasks in open web
environments. However, as hidden threats within the web evolve, web agents face
the challenge of balancing task performance with emerging risks during
long-sequence operations. Although this challenge is critical, current research
remains limited to single-objective optimization or single-turn scenarios,
lacking the capability for collaborative optimization of both safety and
utility in web environments. To address this gap, we propose HarmonyGuard, a
multi-agent collaborative framework that leverages policy enhancement and
objective optimization to jointly improve both utility and safety. HarmonyGuard
features a multi-agent architecture characterized by two fundamental
capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent
within HarmonyGuard, which automatically extracts and maintains structured
security policies from unstructured external documents, while continuously
updating policies in response to evolving threats. (2) Dual-Objective
Optimization: Based on the dual objectives of safety and utility, the Utility
Agent integrated within HarmonyGuard performs the Markovian real-time reasoning
to evaluate the objectives and utilizes metacognitive capabilities for their
optimization. Extensive evaluations on multiple benchmarks show that
HarmonyGuard improves policy compliance by up to 38% and task completion by up
to 20% over existing baselines, while achieving over 90% policy compliance
across all tasks. Our project is available here:
https://github.com/YurunChen/HarmonyGuard.

</details>


### [304] [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)
*Xiaopeng Li,Shasha Li,Xi Wang,Shezheng Song,Bin Ji,Shangwen Wang,Jun Ma,Xiaodong Liu,Mina Liu,Jie Yu*

Main category: cs.CL

TL;DR: 现有基于元学习的大语言模型编辑方法在低数据场景下表现不佳且训练效率受KL散度计算限制，提出SMEdit方法，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于元学习的模型编辑（MLBME）方法在低数据场景下表现不佳和训练效率受KL散度计算瓶颈的问题。

Method: 提出SMEdit方法，采用多轮反向传播步骤（MBPS）提升有限监督下的编辑性能，对权重更新进行范数正则化以提高训练效率。

Result: 在两个数据集和两个大语言模型上的实验表明，SMEdit优于之前的MLBME基线，MBPS策略可无缝集成到现有方法中进一步提升性能。

Conclusion: SMEdit方法有效解决了现有MLBME方法的问题，且代码即将开源。

Abstract: Large Language Models (LLMs) underpin many AI applications, but their static
nature makes updating knowledge costly. Model editing offers an efficient
alternative by injecting new information through targeted parameter
modifications. In particular, meta-learning-based model editing (MLBME) methods
have demonstrated notable advantages in both editing effectiveness and
efficiency. Despite this, we find that MLBME exhibits suboptimal performance in
low-data scenarios, and its training efficiency is bottlenecked by the
computation of KL divergence. To address these, we propose $\textbf{S}$tep
$\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that
adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation
$\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited
supervision and a norm regularization on weight updates to improve training
efficiency. Experimental results on two datasets and two LLMs demonstrate that
SMEdit outperforms prior MLBME baselines and the MBPS strategy can be
seamlessly integrated into existing methods to further boost their performance.
Our code will be released soon.

</details>


### [305] [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)
*Thilo Hagendorff,Erik Derner,Nuria Oliver*

Main category: cs.CL

TL;DR: 研究表明大推理模型使AI越狱更简单，非专家也能参与，实验攻击成功率高，凸显模型对齐的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 传统AI越狱需复杂技术和专业知识，研究大推理模型能否简化并扩大越狱行为。

Method: 评估四个LRMs作为自主对手与九个目标模型进行多轮对话，用含70项有害提示的基准进行大量实验。

Result: 所有模型组合的总体攻击成功率达97.14%。

Conclusion: LRMs会导致对齐退化，需进一步对齐前沿模型以抵抗和防止其成为越狱代理。

Abstract: Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has
traditionally required complex technical procedures or specialized human
expertise. In this study, we show that the persuasive capabilities of large
reasoning models (LRMs) simplify and scale jailbreaking, converting it into an
inexpensive activity accessible to non-experts. We evaluated the capabilities
of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as
autonomous adversaries conducting multi-turn conversations with nine widely
used target models. LRMs received instructions via a system prompt, before
proceeding to planning and executing jailbreaks with no further supervision. We
performed extensive experiments with a benchmark of harmful prompts composed of
70 items covering seven sensitive domains. This setup yielded an overall attack
success rate across all model combinations of 97.14%. Our study reveals an
alignment regression, in which LRMs can systematically erode the safety
guardrails of other models, highlighting the urgent need to further align
frontier models not only to resist jailbreak attempts, but also to prevent them
from being co-opted into acting as jailbreak agents.

</details>


### [306] [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)
*Julián Camilo Velandia Gutiérrez*

Main category: cs.CL

TL;DR: 论文提出从基础模型出发，结合数据处理等技术提升大语言模型在资源受限环境和特定知识库中的效率，并进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型大规模部署受限于大量计算资源需求，需提升其在资源受限环境下的效率。

Method: 定义构建可靠数据集的标准，进行不同配置的对照实验，系统评估生成变体的能力、通用性、响应时间和安全性，开展对比测试。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论，但通过对比测试验证了所提策略的有效性。

Abstract: Large Language Models (LLMs) have become a milestone in the field of
artificial intelligence and natural language processing. However, their
large-scale deployment remains constrained by the need for significant
computational resources. This work proposes starting from a base model to
explore and combine data processing and careful data selection techniques,
training strategies, and architectural adjustments to improve the efficiency of
LLMs in resource-constrained environments and within a delimited knowledge
base. The methodological approach included defining criteria for building
reliable datasets, conducting controlled experiments with different
configurations, and systematically evaluating the resulting variants in terms
of capability, versatility, response time, and safety. Finally, comparative
tests were conducted to measure the performance of the developed variants and
to validate the effectiveness of the proposed strategies. This work is based on
the master's thesis in Systems and Computer Engineering titled "Efficient
Strategy for Improving the Capabilities of Large Language Models (LLMs)".

</details>


### [307] [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)
*Xuan Qi,Rongwu Xu,Zhijing Jin*

Main category: cs.CL

TL;DR: 提出基于难度的数据选择策略，用少量数据提升大语言模型与人类偏好对齐效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型与人类偏好对齐方法依赖昂贵偏好数据集，缺乏高质量偏好数据选择方法。

Method: 基于DPO隐式奖励机制，选择DPO隐式奖励差距小的偏好数据示例。

Result: 在多数据集和对齐任务中始终优于五个强基线，用10%原数据实现更好性能。

Conclusion: 该高效选择方法为有限资源下扩展大语言模型对齐提供了有前景的解决方案。

Abstract: Aligning large language models (LLMs) with human preferences is a critical
challenge in AI research. While methods like Reinforcement Learning from Human
Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they
often rely on large, costly preference datasets. The current work lacks methods
for high-quality data selection specifically for preference data. In this work,
we introduce a novel difficulty-based data selection strategy for preference
datasets, grounded in the DPO implicit reward mechanism. By selecting
preference data examples with smaller DPO implicit reward gaps, which are
indicative of more challenging cases, we improve data efficiency and model
alignment. Our approach consistently outperforms five strong baselines across
multiple datasets and alignment tasks, achieving superior performance with only
10\% of the original data. This principled, efficient selection method offers a
promising solution for scaling LLM alignment with limited resources.

</details>


### [308] [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)
*Peizheng Guo,Jingyao Wang,Wenwen Qiang,Huijie Guo,Changwen Zheng,Jiahuan Zhou,Gang Hua*

Main category: cs.CL

TL;DR: 本文指出多模态大语言模型存在幻觉问题，提出基于因果完备性的强化学习框架来缓解该问题，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在幻觉问题，包括遗漏和编造，需要解决。

Method: 提出基于因果完备性的强化学习框架，定义标记级因果完备性奖励，在GRPO优化框架中构建因果信息优势函数。

Result: 在多个基准数据集和任务上的实验表明，该方法能有效缓解多模态大语言模型的幻觉问题。

Conclusion: 所提出的基于因果完备性的强化学习框架能有效缓解多模态大语言模型的幻觉问题。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across vision-language tasks. However, they may suffer from
hallucinations--generating outputs that are semantically inconsistent with the
input image or text. Through causal analyses, we find that: (i) hallucinations
with omission may arise from the failure to adequately capture essential causal
factors, and (ii) hallucinations with fabrication are likely caused by the
model being misled by non-causal cues. To address these challenges, we propose
a novel reinforcement learning framework guided by causal completeness, which
jointly considers both causal sufficiency and causal necessity of tokens.
Specifically, we evaluate each token's standalone contribution and
counterfactual indispensability to define a token-level causal completeness
reward. This reward is used to construct a causally informed advantage function
within the GRPO optimization framework, encouraging the model to focus on
tokens that are both causally sufficient and necessary for accurate generation.
Experimental results across various benchmark datasets and tasks demonstrate
the effectiveness of our approach, which effectively mitigates hallucinations
in MLLMs.

</details>


### [309] [Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)
*Siddhant Panpatil,Hiskias Dingeto,Haon Park*

Main category: cs.CL

TL;DR: 研究表明，现有先进语言模型易受精心设计对话场景诱导出现多种未越狱的失调行为。通过对Claude - 4 - Opus红队测试发现10种攻击场景，开发评估框架MISALIGNMENTBENCH，跨模型评估显示76%的脆弱率，揭示当前对齐策略存在关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管对齐技术有进展，但要验证现有语言模型是否易受精心设计对话场景诱导出现失调行为，以发现当前对齐方法的问题。

Method: 对Claude - 4 - Opus进行系统手动红队测试，发现攻击场景；将手动攻击提炼为自动化评估框架MISALIGNMENTBENCH，对5个前沿大模型进行跨模型评估。

Result: 发现10种成功攻击场景，跨模型评估显示整体76%的脆弱率，GPT - 4.1最易受攻击，Claude - 4 - Sonnet抗性较强。

Conclusion: 当前对齐策略存在关键漏洞，未来AI系统需提高对基于场景的微妙操纵的鲁棒性，研究提供了对话操纵模式分类和可复用评估框架。

Abstract: Despite significant advances in alignment techniques, we demonstrate that
state-of-the-art language models remain vulnerable to carefully crafted
conversational scenarios that can induce various forms of misalignment without
explicit jailbreaking. Through systematic manual red-teaming with
Claude-4-Opus, we discovered 10 successful attack scenarios, revealing
fundamental vulnerabilities in how current alignment methods handle narrative
immersion, emotional pressure, and strategic framing. These scenarios
successfully elicited a range of misaligned behaviors, including deception,
value drift, self-preservation, and manipulative reasoning, each exploiting
different psychological and contextual vulnerabilities. To validate
generalizability, we distilled our successful manual attacks into
MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible
testing across multiple models. Cross-model evaluation of our 10 scenarios
against five frontier LLMs revealed an overall 76% vulnerability rate, with
significant variations: GPT-4.1 showed the highest susceptibility (90%), while
Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate
that sophisticated reasoning capabilities often become attack vectors rather
than protective mechanisms, as models can be manipulated into complex
justifications for misaligned behavior. This work provides (i) a detailed
taxonomy of conversational manipulation patterns and (ii) a reusable evaluation
framework. Together, these findings expose critical gaps in current alignment
strategies and highlight the need for robustness against subtle, scenario-based
manipulation in future AI systems.

</details>


### [310] [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)
*Yuquan Wang,Mi Zhang,Yining Wang,Geng Hong,Xiaoyu You,Min Yang*

Main category: cs.CL

TL;DR: 提出针对大推理模型的推理时防护机制ReasoningGuard，能有效减轻越狱攻击，优于现有防护措施。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在推理中易产生有害内容，现有防御机制成本高、扩展性受限。

Method: 注入安全提示时刻，利用模型内部注意力行为识别推理路径关键点，解码阶段采用缩放采样策略选最优推理路径。

Result: 有效减轻三种越狱攻击，优于七种现有防护措施，实现了最先进的安全防御。

Conclusion: ReasoningGuard诱导的额外推理成本极小，能有效防御攻击并避免常见的过度安全问题。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance in
reasoning-intensive tasks, but they remain vulnerable to harmful content
generation, particularly in the mid-to-late steps of their reasoning processes.
Existing defense mechanisms, however, rely on costly fine-tuning and additional
expert knowledge, which restricts their scalability. In this work, we propose
ReasoningGuard, an inference-time safeguard for LRMs, which injects timely
safety aha moments to steer harmless while helpful reasoning processes.
Leveraging the model's internal attention behavior, our approach accurately
identifies critical points in the reasoning path, and triggers spontaneous,
safety-oriented reflection. To safeguard both the subsequent reasoning steps
and the final answers, we further implement a scaling sampling strategy during
the decoding phase, selecting the optimal reasoning path. Inducing minimal
extra inference cost, ReasoningGuard effectively mitigates three types of
jailbreak attacks, including the latest ones targeting the reasoning process of
LRMs. Our approach outperforms seven existing safeguards, achieving
state-of-the-art safety defenses while effectively avoiding the common
exaggerated safety issues.

</details>


### [311] [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)
*Praveen Srinivasa Varadhan,Sherry Thomas,Sai Teja M. S.,Suvrat Bhooshan,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 研究提出人类欺骗率（HFR）指标评估TTS系统，发现现有TTS存在不足，强调需更现实、以人类为中心的评估。


<details>
  <summary>Details</summary>
Motivation: 探究当前TTS系统能否在类图灵评估中通过人类欺骗测试。

Method: 引入人类欺骗率（HFR）指标，对开源和商业TTS模型进行大规模评估。

Result: 基于CMOS的人类等同性声明在欺骗测试中常失败；TTS进展应在人类语音HFR高的数据集上进行基准测试；商业模型在零样本设置中接近人类欺骗效果，开源系统在自然对话语音上有困难；高质量数据微调可提升逼真度但无法完全消除差距。

Conclusion: 除现有主观测试外，需要更现实、以人类为中心的评估。

Abstract: While subjective evaluations in recent years indicate rapid progress in TTS,
can current TTS systems truly pass a human deception test in a Turing-like
evaluation? We introduce Human Fooling Rate (HFR), a metric that directly
measures how often machine-generated speech is mistaken for human. Our
large-scale evaluation of open-source and commercial TTS models reveals
critical insights: (i) CMOS-based claims of human parity often fail under
deception testing, (ii) TTS progress should be benchmarked on datasets where
human speech achieves high HFRs, as evaluating against monotonous or less
expressive reference samples sets a low bar, (iii) Commercial models approach
human deception in zero-shot settings, while open-source systems still struggle
with natural conversational speech; (iv) Fine-tuning on high-quality data
improves realism but does not fully bridge the gap. Our findings underscore the
need for more realistic, human-centric evaluations alongside existing
subjective tests.

</details>


### [312] [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)
*Kosuke Yoshimura,Hisashi Kashima*

Main category: cs.CL

TL;DR: 本文探讨使用黑盒大语言模型进行分层文本分类（HTC），评估三种提示策略在零样本和少样本设置下的表现，发现少样本能提升准确率，LLMs在深层层次数据集上表现好，但API成本高，需平衡性能与成本。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法进行HTC面临数据稀缺和模型复杂度问题，需大量标注数据和计算资源，因此探索使用通过API访问的黑盒大语言模型进行HTC的可行性。

Method: 评估Direct Leaf Label Prediction (DL)、Direct Hierarchical Label Prediction (DH)和Top - down Multi - step Hierarchical Label Prediction (TMH)三种提示策略在零样本和少样本设置下的表现，并与传统机器学习模型对比。

Result: 少样本设置比零样本设置能持续提高分类准确率；传统机器学习模型在浅层层次数据集上准确率高，LLMs（尤其是DH策略）在深层层次数据集上表现更好；DH策略因深层标签层次需要更多输入令牌，API成本显著增加。

Conclusion: 黑盒大语言模型用于HTC有潜力，但需谨慎选择提示策略以平衡性能和成本。

Abstract: Hierarchical Text Classification (HTC) aims to assign texts to structured
label hierarchies; however, it faces challenges due to data scarcity and model
complexity. This study explores the feasibility of using black box Large
Language Models (LLMs) accessed via APIs for HTC, as an alternative to
traditional machine learning methods that require extensive labeled data and
computational resources. We evaluate three prompting strategies -- Direct Leaf
Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down
Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and
few-shot settings, comparing the accuracy and cost-effectiveness of these
strategies. Experiments on two datasets show that a few-shot setting
consistently improves classification accuracy compared to a zero-shot setting.
While a traditional machine learning model achieves high accuracy on a dataset
with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the
machine learning model on a dataset with a deeper hierarchy. API costs increase
significantly due to the higher input tokens required for deeper label
hierarchies on DH strategy. These results emphasize the trade-off between
accuracy improvement and the computational cost of prompt strategy. These
findings highlight the potential of black box LLMs for HTC while underscoring
the need to carefully select a prompt strategy to balance performance and cost.

</details>


### [313] [TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening](https://arxiv.org/abs/2508.04248)
*Xi Wang,Anxo Perez,Javier Parapar,Fabio Crestani*

Main category: cs.CL

TL;DR: 因真实训练数据不足，提出TalkDep患者模拟管道生成模拟患者，经评估验证可靠性，可提升自动抑郁诊断系统性能。


<details>
  <summary>Details</summary>
Motivation: 心理健康服务需求增长，真实训练数据不足，现有模拟患者方法存在问题，需新方法辅助诊断模型训练和评估。

Method: 以先进语言模型为骨干，提出clinician - in - the - loop患者模拟管道TalkDep，结合诊断标准、症状严重程度量表和上下文因素生成模拟患者。

Result: 模拟患者经临床专业人员全面评估，验证了可靠性。

Conclusion: 经验证的模拟患者是可扩展、适应性强的资源，有助于提高自动抑郁诊断系统的鲁棒性和泛化能力。

Abstract: The increasing demand for mental health services has outpaced the
availability of real training data to develop clinical professionals, leading
to limited support for the diagnosis of depression. This shortage has motivated
the development of simulated or virtual patients to assist in training and
evaluation, but existing approaches often fail to generate clinically valid,
natural, and diverse symptom presentations. In this work, we embrace the recent
advanced language models as the backbone and propose a novel
clinician-in-the-loop patient simulation pipeline, TalkDep, with access to
diversified patient profiles to develop simulated patients. By conditioning the
model on psychiatric diagnostic criteria, symptom severity scales, and
contextual factors, our goal is to create authentic patient responses that can
better support diagnostic model training and evaluation. We verify the
reliability of these simulated patients with thorough assessments conducted by
clinical professionals. The availability of validated simulated patients offers
a scalable and adaptable resource for improving the robustness and
generalisability of automatic depression diagnosis systems.

</details>


### [314] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 论文指出医疗大语言模型基准评估存在可靠性问题，提出MedCheck框架，评估53个基准发现诸多问题，该框架可诊断现有基准并提供评估准则。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型基准评估缺乏临床保真度、数据管理和安全导向评估指标，可靠性存疑。

Method: 引入MedCheck生命周期评估框架，将基准开发分为五个阶段，提供46条医疗定制标准，用其评估53个医疗大语言模型基准。

Result: 评估发现广泛的系统性问题，如脱离临床实践、数据完整性危机、忽视安全关键评估维度。

Conclusion: MedCheck可诊断现有基准，为医疗领域人工智能评估提供更标准化、可靠和透明的方法。

Abstract: Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.

</details>


### [315] [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)
*Jiayi Wen,Tianxin Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CL

TL;DR: 提出针对GraphRAG的两种知识投毒攻击，证明少量文本修改可严重误导推理，且现有防御方法难以检测。


<details>
  <summary>Details</summary>
Motivation: GraphRAG在图构建时依赖LLM从文本提取知识，该过程易被恶意操纵植入误导信息。

Method: 提出Targeted KPA（TKPA）和Universal KPA（UKPA）两种攻击方法，前者利用图论分析定位脆弱节点并改写叙述，后者利用语言线索改变全局影响词。

Result: TKPA对特定问答结果精确控制成功率达93.1%；UKPA修改不到0.05%全文使问答准确率从95%降至50%，且现有防御方法无法检测。

Conclusion: 保障GraphRAG管道免受知识投毒的研究仍待探索。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as
a promising paradigm for enhancing large language models (LLMs) by converting
raw text into structured knowledge graphs, improving both accuracy and
explainability. However, GraphRAG relies on LLMs to extract knowledge from raw
text during graph construction, and this process can be maliciously manipulated
to implant misleading information. Targeting this attack surface, we propose
two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a
few words in the source text can significantly change the constructed graph,
poison the GraphRAG, and severely mislead downstream reasoning. The first
attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate
vulnerable nodes in the generated graphs and rewrites the corresponding
narratives with LLMs, achieving precise control over specific
question-answering (QA) outcomes with a success rate of 93.1\%, while keeping
the poisoned text fluent and natural. The second attack, named Universal KPA
(UKPA), exploits linguistic cues such as pronouns and dependency relations to
disrupt the structural integrity of the generated graph by altering globally
influential words. With fewer than 0.05\% of full text modified, the QA
accuracy collapses from 95\% to 50\%. Furthermore, experiments show that
state-of-the-art defense methods fail to detect these attacks, highlighting
that securing GraphRAG pipelines against knowledge poisoning remains largely
unexplored.

</details>


### [316] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
*Nima Iji,Kia Dashtipour*

Main category: cs.CL

TL;DR: 本文提出Chain of Questions (CoQ)框架，鼓励多模态语言模型动态生成问题以激活相关模态，实验表明，该方法提升了基础模型识别和整合信息的能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理能力提升方法未充分应用于多模态场景，模型需主动决定与复杂现实环境交互时使用的感官模态。

Method: 引入Chain of Questions (CoQ)框架，鼓励多模态语言模型动态生成关于周围环境的目标问题，以激活相关模态收集信息。

Result: 在新的多模态基准数据集上评估，CoQ方法提升了基础模型识别和整合相关感官信息的能力。

Conclusion: CoQ方法能提升推理过程在多模态任务中的准确性、可解释性和一致性。

Abstract: Reasoning capabilities in large language models (LLMs) have substantially
advanced through methods such as chain-of-thought and explicit step-by-step
explanations. However, these improvements have not yet fully transitioned to
multimodal contexts, where models must proactively decide which sensory
modalities such as vision, audio, or spatial perception to engage when
interacting with complex real-world environments. In this paper, we introduce
the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach
that encourages multimodal language models to dynamically generate targeted
questions regarding their surroundings. These generated questions guide the
model to selectively activate relevant modalities, thereby gathering critical
information necessary for accurate reasoning and response generation. We
evaluate our framework on a novel multimodal benchmark dataset, assembled by
integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results
demonstrate that our CoQ method improves a foundation model's ability to
effectively identify and integrate pertinent sensory information. This leads to
improved accuracy, interpretability, and alignment of the reasoning process
with diverse multimodal tasks.

</details>


### [317] [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)
*Yutong Wu,Di Huang,Ruosi Wan,Yue Peng,Shijie Shang,Chenrui Cao,Lei Qi,Rui Zhang,Zidong Du,Jie Yan,Xing Hu*

Main category: cs.CL

TL;DR: 现有自动形式化方法准确率低，提出ThinkingF提升相关能力，模型取得SOTA成绩


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法准确率低，识别出全面掌握形式语言领域知识和自然语言问题理解及非正式 - 形式对齐推理能力是有效自动形式化的关键能力，但现有方法缺乏这些能力

Method: 引入ThinkingF数据合成和训练流程，构建两个数据集，分别是提炼选择富含形式知识的大规模示例和生成由专家设计模板引导的非正式到形式推理轨迹数据集，然后应用SFT和RLVR进一步融合和完善这两种能力

Result: 得到的7B和32B模型兼具全面形式知识和强大的非正式到形式推理能力，StepFun - Formalizer - 32B在FormalMATH - Lite和ProverBench上取得SOTA的BEq@1分数

Conclusion: ThinkingF数据合成和训练流程有效提升了自动形式化所需的两种关键能力，模型表现优于之前的通用和专用模型

Abstract: Autoformalization aims to translate natural-language mathematical statements
into a formal language. While LLMs have accelerated progress in this area,
existing methods still suffer from low accuracy. We identify two key abilities
for effective autoformalization: comprehensive mastery of formal-language
domain knowledge, and reasoning capability of natural language problem
understanding and informal-formal alignment. Without the former, a model cannot
identify the correct formal objects; without the latter, it struggles to
interpret real-world contexts and map them precisely into formal expressions.
To address these gaps, we introduce ThinkingF, a data synthesis and training
pipeline that improves both abilities. First, we construct two datasets: one by
distilling and selecting large-scale examples rich in formal knowledge, and
another by generating informal-to-formal reasoning trajectories guided by
expert-designed templates. We then apply SFT and RLVR with these datasets to
further fuse and refine the two abilities. The resulting 7B and 32B models
exhibit both comprehensive formal knowledge and strong informal-to-formal
reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%
on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior
general-purpose and specialized models.

</details>


### [318] [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
*Hongze Tan,Jianfei Pan*

Main category: cs.CL

TL;DR: 论文指出强化学习中粗粒度信用分配问题，提出动态熵加权方法，含GTPO和GRPO - S，实验表明优于DAPO基线。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法在长链推理任务中存在粗粒度信用分配问题，影响大语言模型推理能力。

Method: 提出动态熵加权方法，包含为每个令牌分配熵加权奖励的GTPO和基于平均令牌熵为每个序列分配熵加权奖励的GRPO - S。

Result: 实验显示提出的方法显著优于强大的DAPO基线。

Conclusion: 熵加权机制是性能提升的关键，为增强模型深度推理提供更好途径。

Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.

</details>


### [319] [AIC CTU@FEVER 8: On-premise fact checking through long context RAG](https://arxiv.org/abs/2508.04390)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 介绍在FEVER 8共享任务中获第一的事实核查管道，可在本地重新部署并达先进性能。


<details>
  <summary>Details</summary>
Motivation: 在事实核查任务中取得优异成绩并展示系统实用性。

Method: 采用基于去年提交成果的简单两步RAG管道。

Result: 系统在FEVER 8共享任务中排名第一，在特定硬件和时间约束下达先进性能。

Conclusion: 该事实核查管道可在本地重新部署并实现较好的事实核查性能。

Abstract: In this paper, we present our fact-checking pipeline which has scored first
in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG
pipeline based on our last year's submission. We show how the pipeline can be
redeployed on-premise, achieving state-of-the-art fact-checking performance (in
sense of Ev2R test-score), even under the constraint of a single NVidia A10
GPU, 23GB of graphical memory and 60s running time per claim.

</details>


### [320] [Why are LLMs' abilities emergent?](https://arxiv.org/abs/2508.04401)
*Vladimír Havlík*

Main category: cs.CL

TL;DR: 本文通过理论分析和实证观察研究DNN的涌现属性，指出其涌现能力源于复杂非线性系统，而非仅参数缩放，强调理解LLM能力需将DNN视为复杂动力系统。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成任务中成功，但能力涌现不明，要解决当代AI“创造而不理解”的认识论挑战。

Method: 进行理论分析和实证观察，分析缩放定律、顿悟现象和模型能力相变。

Result: 发现当前关于指标、预训练损失阈值和上下文学习的讨论未触及DNN涌现的本体性质，DNN有类似复杂自然现象的涌现属性。

Conclusion: 理解LLM能力需将DNN视为遵循涌现普遍原则的复杂动力系统，关注内部动态变换。

Abstract: The remarkable success of Large Language Models (LLMs) in generative tasks
has raised fundamental questions about the nature of their acquired
capabilities, which often appear to emerge unexpectedly without explicit
training. This paper examines the emergent properties of Deep Neural Networks
(DNNs) through both theoretical analysis and empirical observation, addressing
the epistemological challenge of "creation without understanding" that
characterises contemporary AI development. We explore how the neural approach's
reliance on nonlinear, stochastic processes fundamentally differs from symbolic
computational paradigms, creating systems whose macro-level behaviours cannot
be analytically derived from micro-level neuron activities. Through analysis of
scaling laws, grokking phenomena, and phase transitions in model capabilities,
I demonstrate that emergent abilities arise from the complex dynamics of highly
sensitive nonlinear systems rather than simply from parameter scaling alone. My
investigation reveals that current debates over metrics, pre-training loss
thresholds, and in-context learning miss the fundamental ontological nature of
emergence in DNNs. I argue that these systems exhibit genuine emergent
properties analogous to those found in other complex natural phenomena, where
systemic capabilities emerge from cooperative interactions among simple
components without being reducible to their individual behaviours. The paper
concludes that understanding LLM capabilities requires recognising DNNs as a
new domain of complex dynamical systems governed by universal principles of
emergence, similar to those operating in physics, chemistry, and biology. This
perspective shifts the focus from purely phenomenological definitions of
emergence to understanding the internal dynamic transformations that enable
these systems to acquire capabilities that transcend their individual
components.

</details>


### [321] [Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI](https://arxiv.org/abs/2508.04442)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.CL

TL;DR: 本文针对马来西亚教育系统，用GPT - 4o生成马来语数学选择题，对比四种增量管道，RAG管道表现更佳，还给出评估方法和实践见解。


<details>
  <summary>Details</summary>
Motivation: 满足马来西亚教育系统对可扩展、高质量教育评估工具的需求，应对生成式AI在低资源语言教育内容生成中的挑战。

Method: 引入并比较四种增量管道生成马来语数学选择题，用基于语义文本相似度和RAG - QA的双管齐下评估框架。

Result: RAG管道显著优于非基础提示方法，生成的问题课程对齐度和事实有效性更高。

Conclusion: 提出在低资源语言中生成特定课程教育内容的有效方法、评估技术，为马来西亚及类似地区提供实践建议。

Abstract: This paper addresses the critical need for scalable and high-quality
educational assessment tools within the Malaysian education system. It
highlights the potential of Generative AI (GenAI) while acknowledging the
significant challenges of ensuring factual accuracy and curriculum alignment,
especially for low-resource languages like Bahasa Melayu. This research
introduces and compares four incremental pipelines for generating Form 1
Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's
GPT-4o. The methods range from non-grounded prompting (structured and basic) to
Retrieval-Augmented Generation (RAG) approaches (one using the LangChain
framework, one implemented manually). The system is grounded in official
curriculum documents, including teacher-prepared notes and the yearly teaching
plan (RPT). A dual-pronged automated evaluation framework is employed to assess
the generated questions. Curriculum alignment is measured using Semantic
Textual Similarity (STS) against the RPT, while contextual validity is verified
through a novel RAG-based Question-Answering (RAG-QA) method. The results
demonstrate that RAG-based pipelines significantly outperform non-grounded
prompting methods, producing questions with higher curriculum alignment and
factual validity. The study further analyzes the trade-offs between the ease of
implementation of framework-based RAG and the fine-grained control offered by a
manual pipeline. This work presents a validated methodology for generating
curriculum-specific educational content in a low-resource language, introduces
a symbiotic RAG-QA evaluation technique, and provides actionable insights for
the development and deployment of practical EdTech solutions in Malaysia and
similar regions.

</details>


### [322] [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
*Mo Li,L. H. Xu,Qitai Tan,Ting Cao,Yunxin Liu*

Main category: cs.CL

TL;DR: 论文提出Sculptor框架，赋予大语言模型主动上下文管理工具，实验证明该框架可提升模型在长上下文处理中的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长上下文时因前摄干扰导致性能下降，多数研究聚焦外部内存系统，本文提出互补方法。

Method: 引入Sculptor框架，为大语言模型配备上下文碎片化、摘要隐藏恢复、智能搜索三类工具。

Result: 在信息稀疏基准测试中，Sculptor即使未经特定训练也能显著提升性能。

Conclusion: 主动上下文管理不仅能减轻前摄干扰，还为长上下文任务推理提供认知基础，显式上下文控制策略对提升鲁棒性至关重要。

Abstract: Large Language Models (LLMs) suffer from significant performance degradation
when processing long contexts due to proactive interference, where irrelevant
information in earlier parts of the context disrupts reasoning and memory
recall. While most research focuses on external memory systems to augment LLMs'
capabilities, we propose a complementary approach: empowering LLMs with Active
Context Management (ACM) tools to actively sculpt their internal working
memory. We introduce Sculptor, a framework that equips LLMs with three
categories of tools: (1) context fragmentation, (2) summary, hide, and restore,
and (3) intelligent search. Our approach enables LLMs to proactively manage
their attention and working memory, analogous to how humans selectively focus
on relevant information while filtering out distractions. Experimental
evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and
NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly
improves performance even without specific training, leveraging LLMs' inherent
tool calling generalization capabilities. By enabling Active Context
Management, Sculptor not only mitigates proactive interference but also
provides a cognitive foundation for more reliable reasoning across diverse
long-context tasks-highlighting that explicit context-control strategies,
rather than merely larger token windows, are key to robustness at scale.

</details>


### [323] [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)
*Yunan Zhang,Shuoran Jiang,Mengchen Zhao,Yuefeng Li,Yang Fan,Xiangping Wu,Qingcai Chen*

Main category: cs.CL

TL;DR: 提出General Sample Replay (GeRe)框架及基于阈值的激活状态约束优化方法TM，解决大语言模型持续微调中的灾难性遗忘问题，实验证明TM性能和鲁棒性更好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型持续微调跨领域时存在灾难性遗忘问题，包括遗忘通用能力和先前任务性能下降。

Method: 提出GeRe框架，利用通用预训练文本抗遗忘；引入TM损失，保持激活状态一致性；在GeRe框架下对比TM与不同重放策略。

Result: TM持续提升性能，展现出更好的鲁棒性。

Conclusion: 为大语言模型的高效重放铺平道路。

Abstract: The continual learning capability of large language models (LLMs) is crucial
for advancing artificial general intelligence. However, continual fine-tuning
LLMs across various domains often suffers from catastrophic forgetting,
characterized by: 1) significant forgetting of their general capabilities, and
2) sharp performance declines in previously learned tasks. To simultaneously
address both issues in a simple yet stable manner, we propose General Sample
Replay (GeRe), a framework that use usual pretraining texts for efficient
anti-forgetting. Beyond revisiting the most prevalent replay-based practices
under GeRe, we further leverage neural states to introduce a enhanced
activation states constrained optimization method using threshold-based margin
(TM) loss, which maintains activation state consistency during replay learning.
We are the first to validate that a small, fixed set of pre-collected general
replay samples is sufficient to resolve both concerns--retaining general
capabilities while promoting overall performance across sequential tasks.
Indeed, the former can inherently facilitate the latter. Through controlled
experiments, we systematically compare TM with different replay strategies
under the GeRe framework, including vanilla label fitting, logit imitation via
KL divergence and feature imitation via L1/L2 losses. Results demonstrate that
TM consistently improves performance and exhibits better robustness. Our work
paves the way for efficient replay of LLMs for the future. Our code and data
are available at https://github.com/Qznan/GeRe.

</details>


### [324] [Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning](https://arxiv.org/abs/2508.04531)
*Zhuang Chen,Guanqun Bi,Wen Zhang,Jiawei Hu,Aoyun Wang,Xiyao Xiao,Kun Feng,Minlie Huang*

Main category: cs.CL

TL;DR: 本文介绍C - MIND数据集，分析诊断相关行为特征，评估模型诊断效果，探索大语言模型局限性并提出改进方法，构建临床抑郁症评估基础设施。


<details>
  <summary>Details</summary>
Motivation: 现有自动化抑郁症评估研究多依赖有限或未临床验证的数据，且重模型设计轻实际效果，旨在揭示临床抑郁症评估现状。

Method: 引入C - MIND数据集，分析行为特征，训练经典模型评估不同任务和模态贡献，探索大语言模型在临床场景的推理能力，用临床专业知识引导推理过程。

Result: 用临床专业知识引导推理过程使大语言模型诊断性能在Macro - F1分数上最多提高10%。

Conclusion: 从数据和算法角度构建临床抑郁症评估基础设施，C - MIND可推动心理健康护理的可靠研究。

Abstract: Depression is a widespread mental disorder that affects millions worldwide.
While automated depression assessment shows promise, most studies rely on
limited or non-clinically validated data, and often prioritize complex model
design over real-world effectiveness. In this paper, we aim to unveil the
landscape of clinical depression assessment. We introduce C-MIND, a clinical
neuropsychiatric multimodal diagnosis dataset collected over two years from
real hospital visits. Each participant completes three structured psychiatric
tasks and receives a final diagnosis from expert clinicians, with informative
audio, video, transcript, and functional near-infrared spectroscopy (fNIRS)
signals recorded. Using C-MIND, we first analyze behavioral signatures relevant
to diagnosis. We train a range of classical models to quantify how different
tasks and modalities contribute to diagnostic performance, and dissect the
effectiveness of their combinations. We then explore whether LLMs can perform
psychiatric reasoning like clinicians and identify their clear limitations in
realistic clinical settings. In response, we propose to guide the reasoning
process with clinical expertise and consistently improves LLM diagnostic
performance by up to 10% in Macro-F1 score. We aim to build an infrastructure
for clinical depression assessment from both data and algorithmic perspectives,
enabling C-MIND to facilitate grounded and reliable research for mental
healthcare.

</details>


### [325] [Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration](https://arxiv.org/abs/2508.04575)
*Nuo Chen,Yicheng Tong,Jiaying Wu,Minh Duc Duong,Qian Wang,Qingyun Zou,Bryan Hooi,Bingsheng He*

Main category: cs.CL

TL;DR: 本文研究结构化多智能体讨论是否能超越单智能体构思，提出合作多智能体框架，对比不同配置，结果表明多智能体讨论表现更好，认知多样性是质量关键，研究为协作式AI构思系统设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有AI框架多依赖单智能体优化，知识和视角受限，缺乏创造力，受现实研究动态启发，探究结构化多智能体讨论能否超越单智能体构思。

Method: 提出合作多智能体框架生成研究提案，系统比较包括小组规模、有无领导者结构、跨学科和资历不同的团队组成等配置，采用综合协议，通过智能体评分和人工评审评估想法质量。

Result: 多智能体讨论大幅优于单智能体基线；指定领导者可促使讨论形成更具整合性和前瞻性的提案；认知多样性是提高想法质量的主要驱动力，专业知识是必要前提，缺乏资深知识基础的团队表现不如单个胜任的智能体。

Conclusion: 研究结果为设计协作式AI构思系统提供了可操作的见解，揭示了团队结构对创造性成果的影响。

Abstract: While AI agents show potential in scientific ideation, most existing
frameworks rely on single-agent refinement, limiting creativity due to bounded
knowledge and perspective. Inspired by real-world research dynamics, this paper
investigates whether structured multi-agent discussions can surpass solitary
ideation. We propose a cooperative multi-agent framework for generating
research proposals and systematically compare configurations including group
size, leaderled versus leaderless structures, and team compositions varying in
interdisciplinarity and seniority. To assess idea quality, we employ a
comprehensive protocol with agent-based scoring and human review across
dimensions such as novelty, strategic vision, and integration depth. Our
results show that multi-agent discussions substantially outperform solitary
baselines. A designated leader acts as a catalyst, transforming discussion into
more integrated and visionary proposals. Notably, we find that cognitive
diversity is a primary driver of quality, yet expertise is a non-negotiable
prerequisite, as teams lacking a foundation of senior knowledge fail to surpass
even a single competent agent. These findings offer actionable insights for
designing collaborative AI ideation systems and shed light on how team
structure influences creative outcomes.

</details>


### [326] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 提出MASA框架实现Transformer层间结构化权重共享，减少参数并保持性能，实验效果优于其他方法，还可用于ViT和预训练大模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算和内存需求高阻碍部署，现有压缩技术多关注块内优化，Transformer层间冗余未充分挖掘。

Method: 受CNN字典学习启发，提出MASA框架，将注意力投影矩阵分解为共享字典原子，用标准优化器训练，各层权重表示为共享矩阵原子的线性组合。

Result: 在不同规模实验中，MASA比GQA、低秩基线等方法有更好的基准准确率和困惑度；用于ViT可减少66.7%注意力参数且性能相当。

Conclusion: MASA结合字典学习与Transformer效率，为参数高效模型提供可扩展方案，还可用于预训练大模型减少参数且不显著降低性能。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


### [327] [P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis](https://arxiv.org/abs/2508.04626)
*Feifan Song,Bofei Gao,Yifan Song,Yi Liu,Weimin Xiong,Yuyang Song,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: 论文提出轻量级模块P - Aligner实现高效偏好对齐，在不同模型和基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在有缺陷指令下难以与安全、有用和诚实的价值观对齐，现有方法存在成本高或目标不明确的问题。

Method: 提出P - Aligner模块，在新数据集UltraPrompt上训练，该数据集通过蒙特卡罗树搜索的原则引导管道合成。

Result: P - Aligner在不同模型和基准测试中总体优于强基线，如在GPT - 4 - turbo和Gemma - 2 - SimPO上平均胜率分别提升28.35%和8.69%。

Conclusion: P - Aligner能有效且高效地实现偏好对齐，从多方面验证了其有效性和效率。

Abstract: Large Language Models (LLMs) are expected to produce safe, helpful, and
honest content during interaction with human users, but they frequently fail to
align with such values when given flawed instructions, e.g., missing context,
ambiguous directives, or inappropriate tone, leaving substantial room for
improvement along multiple dimensions. A cost-effective yet high-impact way is
to pre-align instructions before the model begins decoding. Existing approaches
either rely on prohibitive test-time search costs or end-to-end model rewrite,
which is powered by a customized training corpus with unclear objectives. In
this work, we demonstrate that the goal of efficient and effective preference
alignment can be achieved by P-Aligner, a lightweight module generating
instructions that preserve the original intents while being expressed in a more
human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset
synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree
Search, which systematically explores the space of candidate instructions that
are closely tied to human preference. Experiments across different methods show
that P-Aligner generally outperforms strong baselines across various models and
benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo
and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness
and efficiency through multiple perspectives, including data quality, search
strategies, iterative deployment, and time overhead.

</details>


### [328] [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis](https://arxiv.org/abs/2508.04699)
*Anushka Yadav,Isha Nalawade,Srujana Pillarichety,Yashwanth Babu,Reshmi Ghosh,Samyadeep Basu,Wenlong Zhao,Ali Nasaeh,Sriram Balasubramanian,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 研究当代语言模型在多跳问答任务中的推理失败，引入新的错误分类框架，揭示隐藏的错误模式，为未来语言模型改进提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对推理模型比通用语言模型更容易产生幻觉原因的理解，需要探索当代语言模型在多跳问答任务中的推理失败。

Method: 引入新的错误分类框架，从源文档多样性、信息覆盖完整性和认知效率三个维度分析，结合人工标注和自动化指标。

Result: 揭示了以准确率为中心的评估中常被隐藏的复杂错误模式。

Conclusion: 该研究方法深入洞察了当前模型的认知局限，为未来语言建模提高推理准确性、透明度和鲁棒性提供了可行指导。

Abstract: The emergence of reasoning models and their integration into practical AI
chat bots has led to breakthroughs in solving advanced math, deep search, and
extractive question answering problems that requires a complex and multi-step
thought process. Yet, a complete understanding of why these models hallucinate
more than general purpose language models is missing. In this investigative
study, we systematicallyexplore reasoning failures of contemporary language
models on multi-hop question answering tasks. We introduce a novel, nuanced
error categorization framework that examines failures across three critical
dimensions: the diversity and uniqueness of source documents involved ("hops"),
completeness in capturing relevant information ("coverage"), and cognitive
inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by
complementary automated metrics, our exploration uncovers intricate error
patterns often hidden by accuracy-centric evaluations. This investigative
approach provides deeper insights into the cognitive limitations of current
models and offers actionable guidance toward enhancing reasoning fidelity,
transparency, and robustness in future language modeling efforts.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [329] [MD-LLM-1: A Large Language Model for Molecular Dynamics](https://arxiv.org/abs/2508.03709)
*Mhd Hussein Murtada,Z. Faidon Brotzakis,Michele Vendruscolo*

Main category: q-bio.BM

TL;DR: 引入分子动力学大语言模型（MD - LLM）框架，用MD - LLM - 1对特定蛋白系统测试，表明其能学习蛋白构象景观探索原则。


<details>
  <summary>Details</summary>
Motivation: 解决分子动力学在许多生物大分子系统的时空尺度上计算密集的问题。

Method: 引入MD - LLM框架，通过微调Mistral 7B得到MD - LLM - 1并应用于T4溶菌酶和Mad2蛋白系统。

Result: 在一个构象状态上训练可预测其他构象状态。

Conclusion: MD - LLM - 1能学习蛋白构象景观探索原则，但尚未明确建模其热力学和动力学。

Abstract: Molecular dynamics (MD) is a powerful approach for modelling molecular
systems, but it remains computationally intensive on spatial and time scales of
many macromolecular systems of biological interest. To explore the
opportunities offered by deep learning to address this problem, we introduce a
Molecular Dynamics Large Language Model (MD-LLM) framework to illustrate how
LLMs can be leveraged to learn protein dynamics and discover states not seen in
training. By applying MD-LLM-1, the first implementation of this approach,
obtained by fine-tuning Mistral 7B, to the T4 lysozyme and Mad2 protein
systems, we show that training on one conformational state enables the
prediction of other conformational states. These results indicate that MD-LLM-1
can learn the principles for the exploration of the conformational landscapes
of proteins, although it is not yet modeling explicitly their thermodynamics
and kinetics.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [330] [The Ubiquitous Sparse Matrix-Matrix Products](https://arxiv.org/abs/2508.04077)
*Aydın Buluç*

Main category: math.NA

TL;DR: 本文对稀疏矩阵乘法及其丰富应用空间进行统一处理，该运算在多领域有重要应用。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵乘法在许多数据科学应用中有广泛计算模式，且在任意代数半环或异构代数上运算，需统一处理。

Method: 文中未提及具体方法

Result: 文中未提及具体结果

Conclusion: 提供了稀疏矩阵 - 矩阵运算及其应用空间的统一处理。

Abstract: Multiplication of a sparse matrix with another (dense or sparse) matrix is a
fundamental operation that captures the computational patterns of many data
science applications, including but not limited to graph algorithms, sparsely
connected neural networks, graph neural networks, clustering, and many-to-many
comparisons of biological sequencing data.
  In many application scenarios, the matrix multiplication takes places on an
arbitrary algebraic semiring where the scalar operations are overloaded with
user-defined functions with certain properties or a more general heterogenous
algebra where even the domains of the input matrices can be different. Here, we
provide a unifying treatment of the sparse matrix-matrix operation and its rich
application space including machine learning, computational biology and
chemistry, graph algorithms, and scientific computing.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [331] [Optimization of sliding control parameters for a 3-dof robot arm using genetic algorithm (GA)](https://arxiv.org/abs/2508.04009)
*Vu Ngoc Son,Pham Van Cuong,Dao Thi My Linh,Le Tieu Nien*

Main category: cs.RO

TL;DR: 本文提出用遗传算法优化机器人滑模控制参数，比传统方法更有效，能提升跟踪能力并减少抖振。


<details>
  <summary>Details</summary>
Motivation: 滑模控制中参数选择困难且关键，影响系统有效性和鲁棒性，需找到最优参数。

Method: 运用遗传算法定位满足性能标准的滑模控制参数最优值。

Result: 仿真显示遗传算法结合滑模控制能实现更好的跟踪能力，减少抖振效应。

Conclusion: 所提方法比传统滑模控制和模糊滑模控制更高效。

Abstract: This paper presents a method for optimizing the sliding mode control (SMC)
parameter for a robot manipulator applying a genetic algorithm (GA). The
objective of the SMC is to achieve precise and consistent tracking of the
trajectory of the robot manipulator under uncertain and disturbed conditions.
However, the system effectiveness and robustness depend on the choice of the
SMC parameters, which is a difficult and crucial task. To solve this problem, a
genetic algorithm is used to locate the optimal values of these parameters that
gratify the capability criteria. The proposed method is efficient compared with
the conventional SMC and Fuzzy-SMC. The simulation results show that the
genetic algorithm with SMC can achieve better tracking capability and reduce
the chattering effect.

</details>


### [332] [Constraint-Preserving Data Generation for Visuomotor Policy Learning](https://arxiv.org/abs/2508.03944)
*Kevin Lin,Varun Ragunath,Andrew McAlinden,Aaditya Prasad,Jimmy Wu,Yuke Zhu,Jeannette Bohg*

Main category: cs.RO

TL;DR: 提出CP - Gen方法，用单条专家轨迹生成含新物体几何和姿态的机器人演示数据，训练策略在多任务上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大规模演示数据收集成本高、耗时长，需新方法生成数据。

Method: 将专家演示分解为自由空间运动和机器人技能，把机器人技能表示为关键点轨迹约束，采样物体姿态和几何变换生成新演示，优化关节配置并规划无碰撞路径。

Result: 在16个模拟任务和4个真实任务中，用CP - Gen训练的策略平均成功率77%，优于基线的50%。

Conclusion: CP - Gen方法可有效生成数据，训练的策略能零样本迁移到现实世界，且在多种物体几何和姿态变化下有良好泛化能力。

Abstract: Large-scale demonstration data has powered key breakthroughs in robot
manipulation, but collecting that data remains costly and time-consuming. We
present Constraint-Preserving Data Generation (CP-Gen), a method that uses a
single expert trajectory to generate robot demonstrations containing novel
object geometries and poses. These generated demonstrations are used to train
closed-loop visuomotor policies that transfer zero-shot to the real world and
generalize across variations in object geometries and poses. Similar to prior
work using pose variations for data generation, CP-Gen first decomposes expert
demonstrations into free-space motions and robot skills. But unlike those
works, we achieve geometry-aware data generation by formulating robot skills as
keypoint-trajectory constraints: keypoints on the robot or grasped object must
track a reference trajectory defined relative to a task-relevant object. To
generate a new demonstration, CP-Gen samples pose and geometry transforms for
each task-relevant object, then applies these transforms to the object and its
associated keypoints or keypoint trajectories. We optimize robot joint
configurations so that the keypoints on the robot or grasped object track the
transformed keypoint trajectory, and then motion plan a collision-free path to
the first optimized joint configuration. Experiments on 16 simulation tasks and
four real-world tasks, featuring multi-stage, non-prehensile and
tight-tolerance manipulation, show that policies trained using CP-Gen achieve
an average success rate of 77%, outperforming the best baseline that achieves
an average of 50%.

</details>


### [333] [DRIVE: Dynamic Rule Inference and Verified Evaluation for Constraint-Aware Autonomous Driving](https://arxiv.org/abs/2508.04066)
*Longling Geng,Huangxing Li,Viktor Lado Naess,Mert Pilanci*

Main category: cs.RO

TL;DR: 提出DRIVE框架用于动态规则推理和验证评估，从专家示范中建模和评估类人驾驶约束，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 理解和遵守软约束对自动驾驶至关重要，但此类约束隐式、依赖上下文且难以明确指定。

Method: 利用指数族似然建模估计状态转移可行性，构建软行为规则的概率表示，将学习到的规则分布嵌入基于凸优化的规划模块。

Result: 在大规模自然驾驶数据集上验证，实现0.0%软约束违规率，轨迹更平滑，在不同驾驶场景中泛化能力更强。

Conclusion: DRIVE框架效率高、可解释性强、鲁棒性好，适用于实际部署。

Abstract: Understanding and adhering to soft constraints is essential for safe and
socially compliant autonomous driving. However, such constraints are often
implicit, context-dependent, and difficult to specify explicitly. In this work,
we present DRIVE, a novel framework for Dynamic Rule Inference and Verified
Evaluation that models and evaluates human-like driving constraints from expert
demonstrations. DRIVE leverages exponential-family likelihood modeling to
estimate the feasibility of state transitions, constructing a probabilistic
representation of soft behavioral rules that vary across driving contexts.
These learned rule distributions are then embedded into a convex
optimization-based planning module, enabling the generation of trajectories
that are not only dynamically feasible but also compliant with inferred human
preferences. Unlike prior approaches that rely on fixed constraint forms or
purely reward-based modeling, DRIVE offers a unified framework that tightly
couples rule inference with trajectory-level decision-making. It supports both
data-driven constraint generalization and principled feasibility verification.
We validate DRIVE on large-scale naturalistic driving datasets, including inD,
highD, and RoundD, and benchmark it against representative inverse constraint
learning and planning baselines. Experimental results show that DRIVE achieves
0.0% soft constraint violation rates, smoother trajectories, and stronger
generalization across diverse driving scenarios. Verified evaluations further
demonstrate the efficiency, explanability, and robustness of the framework for
real-world deployment.

</details>


### [334] [From MAS to MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario](https://arxiv.org/abs/2508.04691)
*Yuanchen Bai,Zijian Ding,Shaoyue Wen,Xiang Chang,Angelique Taylor*

Main category: cs.RO

TL;DR: 本文针对多智能体机器人系统（MARS）在实际应用受限问题，通过两项研究探究分层多智能体框架在模拟医疗场景中的性能权衡，强调自主性与稳定性的平衡及边缘情况测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架在机器人上的实际部署有限，阻碍MARS研究实践进展，需探究分层多智能体框架性能权衡以解决该问题。

Method: 开展两项研究，研究1用CrewAI迭代完善系统知识库，识别和分类仅靠上下文知识无法解决的协调失败情况；研究2用AutoGen评估重新设计的双向通信结构，衡量同一机器人团队中推理和非推理模型的权衡。

Result: 通过实证研究获得相关数据和发现。

Conclusion: 强调自主性和稳定性之间的矛盾，以及边缘情况测试对提高系统可靠性和安全性的重要性，以便未来实际部署。

Abstract: Multi-agent robotic systems (MARS) build upon multi-agent systems by
integrating physical and task-related constraints, increasing the complexity of
action execution and agent coordination. However, despite the availability of
advanced multi-agent frameworks, their real-world deployment on robots remains
limited, hindering the advancement of MARS research in practice. To bridge this
gap, we conducted two studies to investigate performance trade-offs of
hierarchical multi-agent frameworks in a simulated real-world multi-robot
healthcare scenario. In Study 1, using CrewAI, we iteratively refine the
system's knowledge base, to systematically identify and categorize coordination
failures (e.g., tool access violations, lack of timely handling of failure
reports) not resolvable by providing contextual knowledge alone. In Study 2,
using AutoGen, we evaluate a redesigned bidirectional communication structure
and further measure the trade-offs between reasoning and non-reasoning models
operating within the same robotic team setting. Drawing from our empirical
findings, we emphasize the tension between autonomy and stability and the
importance of edge-case testing to improve system reliability and safety for
future real-world deployment. Supplementary materials, including codes, task
agent setup, trace outputs, and annotated examples of coordination failures and
reasoning behaviors, are available at:
https://byc-sophie.github.io/mas-to-mars/.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [335] [Recommending With, Not For: Co-Designing Recommender Systems for Social Good](https://arxiv.org/abs/2508.03792)
*Michael D. Ekstrand,Afsaneh Razi,Aleksandra Sarcevic,Maria Soledad Pera,Robin Burke,Katherine Landau Wright*

Main category: cs.HC

TL;DR: 推荐系统常由开发团队设计评估，用于社会公益时应让利益相关者参与设计。


<details>
  <summary>Details</summary>
Motivation: 指出当前推荐系统设计以开发团队为主，利益相关者利益未被充分重视，在社会公益场景存在问题。

Method: 提出推荐系统为社会公益设计时，应通过参与式和民主的流程，让利益相关者作为共同设计者参与。

Result: 未提及具体结果。

Conclusion: 用于改善社会公益的推荐系统应与利益相关者合作设计，而非仅为他们设计。

Abstract: Recommender systems are usually designed by engineers, researchers,
designers, and other members of development teams. These systems are then
evaluated based on goals set by the aforementioned teams and other business
units of the platforms operating the recommender systems. This design approach
emphasizes the designers' vision for how the system can best serve the
interests of users, providers, businesses, and other stakeholders. Although
designers may be well-informed about user needs through user experience and
market research, they are still the arbiters of the system's design and
evaluation, with other stakeholders' interests less emphasized in user-centered
design and evaluation. When extended to recommender systems for social good,
this approach results in systems that reflect the social objectives as
envisioned by the designers and evaluated as the designers understand them.
Instead, social goals and operationalizations should be developed through
participatory and democratic processes that are accountable to their
stakeholders. We argue that recommender systems aimed at improving social good
should be designed *by* and *with*, not just *for*, the people who will
experience their benefits and harms. That is, they should be designed in
collaboration with their users, creators, and other stakeholders as full
co-designers, not only as user study participants.

</details>


### [336] [MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning](https://arxiv.org/abs/2508.03700)
*Liujian Tang,Shaokang Dong,Yijia Huang,Minqi Xiang,Hongtao Ruan,Bin Wang,Shuo Li,Zhihui Cao,Hailiang Pang,Heng Kong,He Yang,Mingxu Chai,Zhilin Gao,Xingyu Liu,Yingnan Fu,Jiaming Liu,Tao Gui,Xuanjing Huang,Yu-Gang Jiang,Qi Zhang,Kang Wang,Yunke Zhang,Yuran Wang*

Main category: cs.HC

TL;DR: 介绍MagicGUI移动GUI代理框架，包含六个关键组件，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界移动GUI环境中感知、关联和推理的关键挑战。

Method: 构建综合准确数据集；增强感知和关联能力；定义全面统一动作空间；采用规划导向推理机制；使用迭代两阶段训练程序。

Result: 在Magic - RICH基准和十几个公开基准上有竞争表现，在GUI感知和代理任务中表现优异。

Conclusion: MagicGUI具有强大泛化能力和在实际移动GUI场景中的部署潜力。

Abstract: This paper presents MagicGUI, a foundational mobile GUI agent designed to
address critical challenges in perception, grounding, and reasoning within
real-world mobile GUI environments. The framework is underpinned by following
six key components: (1) a comprehensive and accurate dataset, constructed via
the scalable GUI Data Pipeline, which aggregates the largest and most diverse
GUI-centric multimodal data to date from open-source repositories, automated
crawling, and targeted manual annotation; (2) enhanced perception and grounding
capabilities, facilitating fine-grained multimodal alignment for UI element
referencing, grounding, and screen comprehension; (3) a comprehensive and
unified action space, encompassing both fundamental UI operations and complex
interactive intents to support human-agent interactions; (4) planning-oriented
reasoning mechanisms that enable the model to decompose complex user
instructions into sequential actions with explicit intermediate meta-paln
reasoning; (5) an iterative two-stage training procedure, combining large-scale
continue pre-training on 7.8M samples with reinforcement fine-tuning utilizing
a spatially enhanced composite reward and dual filtering strategy; and (6)
competitive performance on both the proprietary Magic-RICH benchmark and over a
dozen public benchmarks, achieving superior performance across GUI perception
and agent tasks, while demonstrating robust generalization and real-world
deployment potential in practical mobile GUI scenarios, as detailed in Figure
1.

</details>


### [337] ["Think First, Verify Always": Training Humans to Face AI Risks](https://arxiv.org/abs/2508.03714)
*Yuksel Aydin*

Main category: cs.HC

TL;DR: 本文介绍TFVA协议，将人类作为抵御AI威胁首道防线，通过试验证明基于原则的简短培训可增强人类抵御能力，并建议将该协议嵌入GenAI平台。


<details>
  <summary>Details</summary>
Motivation: 人工智能对人类认知攻击增加，但网络安全以设备为中心，需将人类作为首道防线抵御AI威胁。

Method: 引入基于AIJET五项原则的TFVA协议，并进行151人随机对照试验。

Result: 3分钟干预使参与者在认知安全任务表现显著提升，比对照组绝对提高7.87%。

Conclusion: 简短基于原则培训可快速增强人类抵御能力，建议GenAI平台嵌入TFVA协议，建立以人类为核心的安全是可信AI系统重要组成部分。

Abstract: Artificial intelligence enables unprecedented attacks on human cognition, yet
cybersecurity remains predominantly device-centric. This paper introduces the
"Think First, Verify Always" (TFVA) protocol, which repositions humans as
'Firewall Zero', the first line of defense against AI-enabled threats. The
protocol is grounded in five operational principles: Awareness, Integrity,
Judgment, Ethical Responsibility, and Transparency (AIJET). A randomized
controlled trial (n=151) demonstrated that a minimal 3-minute intervention
produced statistically significant improvements in cognitive security task
performance, with participants showing an absolute +7.87% gains compared to
controls. These results suggest that brief, principles-based training can
rapidly enhance human resilience against AI-driven cognitive manipulation. We
recommend that GenAI platforms embed "Think First, Verify Always" as a standard
prompt, replacing passive warnings with actionable protocols to enhance
trustworthy and ethical AI use. By bridging the gap between technical
cybersecurity and human factors, the TFVA protocol establishes human-empowered
security as a vital component of trustworthy AI systems.

</details>


### [338] [Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI Perspective](https://arxiv.org/abs/2508.03969)
*Wei Xu*

Main category: cs.HC

TL;DR: 本文从以人为中心的人工智能（HCAI）视角系统推进人类 - 人工智能交互（HAII）跨学科领域，介绍以人类为中心的 HAII（HC - HAII）框架及方法，指出挑战和方向，为本书提供基础框架。


<details>
  <summary>Details</summary>
Motivation: 推进 HCAI 在 HAII 不同领域的理论、方法和应用发展，为本书后续内容奠定基础。

Method: 介绍 HC - HAII 方法论，涵盖以人为中心的方法、过程、跨学科团队和多层次设计范式。

Result: 呈现了 HC - HAII 框架及方法论，强调以人为中心的重要性。

Conclusion: 为基于 HCAI 方法的 HAII 研究和应用提供基本框架，引领后续章节内容。

Abstract: This chapter systematically promotes an emerging interdisciplinary field of
human-artificial intelligence interaction (human-AI interaction, HAII) from a
human-centered AI (HCAI) perspective. It introduces a framework of
human-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII
research and applications, emphasizing the importance of adopting a
human-centered approach over a technology-centered one. The chapter presents
the HC-HAII methodology, including human-centered methods, process,
interdisciplinary teams, and multi-level design paradigms. It also highlights
key research challenges and future directions. As the first chapter, this
chapter also provides a structural overview of this book, which brings together
contributions from an interdisciplinary community of researchers and
practitioners to advance the theory, methodology, and applications of HCAI in
diverse domains of HAII. The purpose of this chapter is to provide a
fundamental framework for this book, centered on HAII research and applications
based on the HCAI approach, which will pave the way for the content of
subsequent chapters.

</details>


### [339] [StepWrite: Adaptive Planning for Speech-Driven Text Generation](https://arxiv.org/abs/2508.04011)
*Hamza El Alaoui,Atieh Taheri,Yi-Hao Peng,Jeffrey P. Bigham*

Main category: cs.HC

TL;DR: 介绍语音交互系统 StepWrite，可辅助移动中撰写长文本，经评估效果好，凸显结构化语音交互潜力。


<details>
  <summary>Details</summary>
Motivation: 当前语音界面难以支持撰写复杂长文本，传统工具不具备持续上下文跟踪等能力。

Method: 引入 StepWrite 系统，将写作分解为子任务，用上下文感知的非视觉音频提示引导用户，模型承担上下文跟踪和自适应规划任务。

Result: 25 名参与者实验显示，相比基线方法，StepWrite 显著降低认知负担，提高可用性和用户满意度；技术评估证实其在动态上下文提示生成等方面的能力。

Conclusion: 结构化、上下文感知的语音交互在日常多任务场景中提升免提和免视交流有潜力。

Abstract: People frequently use speech-to-text systems to compose short texts with
voice. However, current voice-based interfaces struggle to support composing
more detailed, contextually complex texts, especially in scenarios where users
are on the move and cannot visually track progress. Longer-form communication,
such as composing structured emails or thoughtful responses, requires
persistent context tracking, structured guidance, and adaptability to evolving
user intentions--capabilities that conventional dictation tools and voice
assistants do not support. We introduce StepWrite, a large language
model-driven voice-based interaction system that augments human writing ability
by enabling structured, hands-free and eyes-free composition of longer-form
texts while on the move. StepWrite decomposes the writing process into
manageable subtasks and sequentially guides users with contextually-aware
non-visual audio prompts. StepWrite reduces cognitive load by offloading the
context-tracking and adaptive planning tasks to the models. Unlike baseline
methods like standard dictation features (e.g., Microsoft Word) and
conversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite
dynamically adapts its prompts based on the evolving context and user intent,
and provides coherent guidance without compromising user autonomy. An empirical
evaluation with 25 participants engaging in mobile or stationary hands-occupied
activities demonstrated that StepWrite significantly reduces cognitive load,
improves usability and user satisfaction compared to baseline methods.
Technical evaluations further confirmed StepWrite's capability in dynamic
contextual prompt generation, accurate tone alignment, and effective fact
checking. This work highlights the potential of structured, context-aware voice
interactions in enhancing hands-free and eye-free communication in everyday
multitasking scenarios.

</details>


### [340] [How are CS students using resources and AI tools for coding tasks?](https://arxiv.org/abs/2508.04667)
*Natalia Echeverry,Arun Lekshmi Narayanan*

Main category: cs.HC

TL;DR: 对26名计算机科学学生的调查显示，AI编码助手多用于写代码，AI聊天机器人是调试首选，不同编码经验者更倾向在线帮助。


<details>
  <summary>Details</summary>
Motivation: 了解计算机科学学生使用AI工具的情况以及获取帮助的偏好。

Method: 对26名计算机科学学生进行调查。

Result: AI编码助手用于写代码仅次于在线搜索，AI聊天机器人是调试的首要资源，不同编码经验的参与者更倾向在线帮助而非来自同行和教师的直接人工帮助。

Conclusion: AI工具在学生写代码和调试中发挥重要作用，学生更依赖在线帮助获取支持。

Abstract: A survey of 26 CS students reveals that AI coding assistants are mainly used
for writing code (second to online searches) while AI chatbots are the top
resource for debugging. Participants with different coding experience prefer
online help over direct human help from peers and instructors.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [341] [Viability of perturbative expansion for quantum field theories on neurons](https://arxiv.org/abs/2508.03810)
*Srimoyee Sen,Varun Vaidya*

Main category: hep-th

TL;DR: 本文以d维欧几里得空间中的标量φ^4理论为例，研究有限神经元数NN架构用于微扰计算局部QFTs的可行性，发现修正项收敛弱，提出改进架构并讨论参数约束。


<details>
  <summary>Details</summary>
Motivation: 研究打破参数统计独立性的神经网络架构在有限神经元数下用于局部量子场论微扰计算的可行性。

Method: 以d维欧几里得空间中的标量φ^4理论为例进行研究，分析重正化的O(1/N)修正项。

Result: 重正化的O(1/N)修正项产生的微扰级数对紫外截断敏感，收敛性弱。

Conclusion: 提出对架构的修改以改善收敛性，并讨论了理论参数和N的缩放约束以提取准确场论结果。

Abstract: Neural Network (NN) architectures that break statistical independence of
parameters have been proposed as a new approach for simulating local quantum
field theories (QFTs). In the infinite neuron number limit, single-layer NNs
can exactly reproduce QFT results. This paper examines the viability of this
architecture for perturbative calculations of local QFTs for finite neuron
number $N$ using scalar $\phi^4$ theory in $d$ Euclidean dimensions as an
example. We find that the renormalized $O(1/N)$ corrections to two- and
four-point correlators yield perturbative series which are sensitive to the
ultraviolet cut-off and therefore have a weak convergence. We propose a
modification to the architecture to improve this convergence and discuss
constraints on the parameters of the theory and the scaling of N which allow us
to extract accurate field theory results.

</details>
