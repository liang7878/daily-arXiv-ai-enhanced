<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 71]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 13]
- [cs.LG](#cs.LG) [Total: 66]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 13]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [stat.ML](#stat.ML) [Total: 7]
- [stat.CO](#stat.CO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CR](#cs.CR) [Total: 21]
- [econ.GN](#econ.GN) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.HC](#cs.HC) [Total: 12]
- [cs.NI](#cs.NI) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 21]
- [cs.CL](#cs.CL) [Total: 25]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [stat.AP](#stat.AP) [Total: 2]
- [nlin.AO](#nlin.AO) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ME](#stat.ME) [Total: 3]
- [math.AT](#math.AT) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration](https://arxiv.org/abs/2507.21067)
*Jan Kapusta*

Main category: cs.AI

TL;DR: 本文提出共生认识论作为人机认知伙伴关系的哲学基础，引入SynLang作为透明人机协作的形式化协议，经实证验证，该协议能增强人类智能、维护人类能动性和伦理责任。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统推理过程不透明，阻碍人类监督和协作潜力，传统可解释AI方法无法建立真正的共生协作。

Method: 提出共生认识论，引入SynLang协议，通过实际人机对话进行实证验证，协议定义TRACE和TRACE_FE机制，集成信心量化等功能。

Result: AI能适应结构化推理协议并进行成功的元认知干预。

Conclusion: SynLang与共生认识论结合，通过双级透明度，能增强人类智能、维护人类能动性和伦理责任，便于理解和验证AI决策。

Abstract: Current AI systems rely on opaque reasoning processes that hinder human
oversight and collaborative potential. Conventional explainable AI approaches
offer post-hoc justifications and often fail to establish genuine symbiotic
collaboration. In this paper, the Symbiotic Epistemology is presented as a
philosophical foundation for human-AI cognitive partnerships. Unlike frameworks
that treat AI as a mere tool or replacement, symbiotic epistemology positions
AI as a reasoning partner, fostering calibrated trust by aligning human
confidence with AI reliability through explicit reasoning patterns and
confidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as
a formal protocol for transparent human-AI collaboration. The framework is
empirically validated through actual human-AI dialogues demonstrating AI's
adaptation to structured reasoning protocols and successful metacognitive
intervention. The protocol defines two complementary mechanisms: TRACE for
high-level reasoning patterns and TRACE_FE for detailed factor explanations. It
also integrates confidence quantification, declarative control over AI
behavior, and context inheritance for multi-agent coordination. By structuring
communication and embedding confidence-calibrated transparency, SynLang,
together with symbiotic epistemology, enables AI systems that enhance human
intelligence, preserve human agency, and uphold ethical accountability in
collaborative decision-making. Through dual-level transparency, beginning with
high-level reasoning patterns and progressing to granular explanations, the
protocol facilitates rapid comprehension and supports thorough verification of
AI decision-making.

</details>


### [2] [Artificial intelligence for sustainable wine industry: AI-driven management in viticulture, wine production and enotourism](https://arxiv.org/abs/2507.21098)
*Marta Sidorkiewicz,Karolina Królikowska,Berenika Dyczek,Edyta Pijet-Migon,Anna Dubel*

Main category: cs.AI

TL;DR: 研究探讨AI在葡萄酒行业提升可持续性与效率的作用，基于调查和分析发现AI在葡萄园管理、生产及旅游等方面有积极影响。


<details>
  <summary>Details</summary>
Motivation: 葡萄酒行业面临环境和经济挑战，AI可提供创新方案优化资源利用、减少环境影响和提升客户参与度，需了解其在可持续酿酒中的潜力。

Method: 对波兰酿酒师进行问卷调查，并综合分析适用于葡萄栽培、生产和旅游的AI方法，探索关键AI技术。

Result: AI能增强葡萄园监测、优化灌溉、简化生产流程，在葡萄酒旅游中可个性化消费体验。

Conclusion: 强调AI对经济、环境和社会可持续性的影响，支持当地葡萄酒企业和文化遗产。

Abstract: This study examines the role of Artificial Intelligence (AI) in enhancing
sustainability and efficiency within the wine industry. It focuses on AI-driven
intelligent management in viticulture, wine production, and enotourism. As the
wine industry faces environmental and economic challenges, AI offers innovative
solutions to optimize resource use, reduce environmental impact, and improve
customer engagement. Understanding AI's potential in sustainable winemaking is
crucial for fostering responsible and efficient industry practices. The
research is based on a questionnaire survey conducted among Polish winemakers,
combined with a comprehensive analysis of AI methods applicable to viticulture,
production, and tourism. Key AI technologies, including predictive analytics,
machine learning, and computer vision, are explored. The findings indicate that
AI enhances vineyard monitoring, optimizes irrigation, and streamlines
production processes, contributing to sustainable resource management. In
enotourism, AI-powered chatbots, recommendation systems, and virtual tastings
personalize consumer experiences. The study highlights AI's impact on economic,
environmental, and social sustainability, supporting local wine enterprises and
cultural heritage. Keywords: Artificial Intelligence, Sustainable Development,
AI-Driven Management, Viticulture, Wine Production, Enotourism, Wine
Enterprises, Local Communities

</details>


### [3] [Leveraging Generative AI to Enhance Synthea Module Development](https://arxiv.org/abs/2507.21123)
*Mark A. Kramer,Aanchal Mathur,Caroline E. Adams,Jason A. Walonoski*

Main category: cs.AI

TL;DR: 本文探索用大语言模型辅助Synthea开发新疾病模块，展示四种支持方式，介绍渐进式细化概念，指出挑战并给出未来研究建议。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型融入模块开发过程，有望减少开发时间、降低专业要求、拓展模型多样性并提高合成患者数据质量。

Method: 展示大语言模型支持Synthea模块创建的四种方式，引入渐进式细化概念，迭代评估和修改模块。

Result: 大语言模型在辅助合成数据创建方面有前景，但存在需要人工监督、严格测试验证以及内容可能不准确等挑战和局限。

Conclusion: 给出未来研究和开发建议，以充分发挥大语言模型辅助合成数据创建的潜力。

Abstract: This paper explores the use of large language models (LLMs) to assist in the
development of new disease modules for Synthea, an open-source synthetic health
data generator. Incorporating LLMs into the module development process has the
potential to reduce development time, reduce required expertise, expand model
diversity, and improve the overall quality of synthetic patient data. We
demonstrate four ways that LLMs can support Synthea module creation: generating
a disease profile, generating a disease module from a disease profile,
evaluating an existing Synthea module, and refining an existing module. We
introduce the concept of progressive refinement, which involves iteratively
evaluating the LLM-generated module by checking its syntactic correctness and
clinical accuracy, and then using that information to modify the module. While
the use of LLMs in this context shows promise, we also acknowledge the
challenges and limitations, such as the need for human oversight, the
importance of rigorous testing and validation, and the potential for
inaccuracies in LLM-generated content. The paper concludes with recommendations
for future research and development to fully realize the potential of LLM-aided
synthetic data creation.

</details>


### [4] [DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search](https://arxiv.org/abs/2507.07426)
*Zerui Yang,Yuwei Wan,Yinqiao Li,Yudai Matsuda,Tong Xie,Linqi Song*

Main category: cs.AI

TL;DR: 提出DrugMCTS框架用于药物再利用，无需领域微调，表现优于对比模型，凸显结构化推理等机制重要性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在科学领域推理受预训练知识限制，传统方法有计算开销大或未充分利用结构化科学数据的问题。

Method: 提出DrugMCTS框架，整合RAG、多智能体协作和蒙特卡罗树搜索，使用五个专业智能体检索和分析分子与蛋白质信息。

Result: DrugMCTS让Qwen2.5 - 7B - Instruct比Deepseek - R1性能高超20%，在数据集上比通用大模型和深度学习基线有更高召回率和鲁棒性。

Conclusion: 结构化推理、基于智能体的协作和反馈驱动的搜索机制对推进大语言模型在药物发现中的应用很重要。

Abstract: Recent advances in large language models have demonstrated considerable
potential in scientific domains such as drug discovery. However, their
effectiveness remains constrained when reasoning extends beyond the knowledge
acquired during pretraining. Conventional approaches, such as fine-tuning or
retrieval-augmented generation, face limitations in either imposing high
computational overhead or failing to fully exploit structured scientific data.
To overcome these challenges, we propose DrugMCTS, a novel framework that
synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree
Search for drug repurposing. The framework employs five specialized agents
tasked with retrieving and analyzing molecular and protein information, thereby
enabling structured and iterative reasoning. Without requiring domain-specific
fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by
over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate
that DrugMCTS achieves substantially higher recall and robustness compared to
both general-purpose LLMs and deep learning baselines. Our results highlight
the importance of structured reasoning, agent-based collaboration, and
feedback-driven search mechanisms in advancing LLM applications for drug
discovery.

</details>


### [5] [Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics](https://arxiv.org/abs/2507.21129)
*Jae Wan Shim*

Main category: cs.AI

TL;DR: 本文提出新方法创建大语言模型的认知轮廓，分析其信息处理动态，发现独特一致的认知轮廓，并引入IGS指数，为分析AI内在运行动态提供新视角。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型产生结果的内部机制，超越衡量模型能做什么的指标，研究其如何处理信息。

Method: 引入任务无关的方法，创建基于熵衰减曲线的认知轮廓来探究模型动态，还引入信息增益跨度（IGS）指数。

Result: 对多个先进大语言模型在不同文本上应用该方法，发现独特且一致的认知轮廓，该轮廓对模型规模和文本复杂度敏感。

Conclusion: 该工作为分析和比较人工智能的内在运行动态提供了新的、有原则的视角。

Abstract: The remarkable capabilities of Large Language Models (LLMs) are now
extensively documented on task-specific benchmarks, yet the internal mechanisms
that produce these results are the subject of intense scientific inquiry. This
paper contributes to this inquiry by moving beyond metrics that measure
\textit{what} models can do, to a methodology that characterizes \textit{how}
they process information. We introduce a novel, task-agnostic approach to probe
these dynamics by creating a quantitative ``Cognitive Profile" for any given
model. This profile is centered on the \textbf{Entropy Decay Curve}, a
visualization that traces how a model's normalized predictive uncertainty
changes as a function of context length. Applying this methodology to several
state-of-the-art LLMs across diverse texts, we uncover unique and consistent
cognitive profiles that are sensitive to both model scale and text complexity.
We also introduce the Information Gain Span (IGS) index to summarize the
desirability of the decay trajectory. This work thus provides a new, principled
lens for analyzing and comparing the intrinsic operational dynamics of
artificial intelligence.

</details>


### [6] [INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems](https://arxiv.org/abs/2507.21130)
*Bintao Tang,Xin Yang,Yuhao Wang,Zixuan Qiu,Zimo Ji,Wenyuan Jiang*

Main category: cs.AI

TL;DR: 提出INTEGRALBENCH基准评估大语言模型在定积分问题上的表现，评估显示性能差距和难度与准确率的关联，建立基线指标。


<details>
  <summary>Details</summary>
Motivation: 为定积分计算提供严谨评估框架，推动自动化数学推理发展。

Method: 创建INTEGRALBENCH基准，提供符号和数值的真实解及手动难度标注，对九个最先进的大语言模型进行评估。

Result: 评估揭示模型间存在显著性能差距，问题难度与模型准确率有强相关性，建立了该领域的基线指标。

Conclusion: INTEGRALBENCH对评估大语言模型处理定积分问题有价值，有助于推动自动化数学推理。

Abstract: We present INTEGRALBENCH, a focused benchmark designed to evaluate Large
Language Model (LLM) performance on definite integral problems. INTEGRALBENCH
provides both symbolic and numerical ground truth solutions with manual
difficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals
significant performance gaps and strong correlations between problem difficulty
and model accuracy, establishing baseline metrics for this challenging domain.
INTEGRALBENCH aims to advance automated mathematical reasoning by providing a
rigorous evaluation framework specifically tailored for definite integral
computation.

</details>


### [7] [Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation Tasks: A Case Study Using Banks Public Disclosures](https://arxiv.org/abs/2507.21360)
*Nicholas Botti,Flora Haberkorn,Charlotte Hoopes,Shaun Khan*

Main category: cs.AI

TL;DR: 研究利用被试内设计和随机任务分配，测试AI检索增强生成工具辅助信息提取和数据标注任务的效果，发现该工具可加速任务执行并提高准确性，且标注员技能有影响。


<details>
  <summary>Details</summary>
Motivation: 了解使用AI检索增强生成（RAG）工具辅助分析师进行信息提取和数据标注任务的有效性。

Method: 采用被试内设计和随机任务分配，复制现有具有挑战性的真实世界标注任务，设置“天真”AI使用和“交互”AI使用两种处理条件。

Result: 与纯人工基线相比，AI工具最多可将任务执行速度提高10倍并增强任务准确性，尤其是在交互条件下；外推到整个任务，这些方法最多可节省268小时。

Conclusion: AI工具能提升任务执行速度和准确性，标注员在主题领域和AI工具使用方面的技能是任务绩效的影响因素。

Abstract: We utilize a within-subjects design with randomized task assignments to
understand the effectiveness of using an AI retrieval augmented generation
(RAG) tool to assist analysts with an information extraction and data
annotation task. We replicate an existing, challenging real-world annotation
task with complex multi-part criteria on a set of thousands of pages of public
disclosure documents from global systemically important banks (GSIBs) with
heterogeneous and incomplete information content. We test two treatment
conditions. First, a "naive" AI use condition in which annotators use only the
tool and must accept the first answer they are given. And second, an
"interactive" AI treatment condition where annotators use the tool
interactively, and use their judgement to follow-up with additional information
if necessary. Compared to the human-only baseline, the use of the AI tool
accelerated task execution by up to a factor of 10 and enhanced task accuracy,
particularly in the interactive condition. We find that when extrapolated to
the full task, these methods could save up to 268 hours compared to the
human-only approach. Additionally, our findings suggest that annotator skill,
not just with the subject matter domain, but also with AI tools, is a factor in
both the accuracy and speed of task performance.

</details>


### [8] [NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback](https://arxiv.org/abs/2507.21131)
*Madhava Gaikwad,Ashwini Ramchandra Doke*

Main category: cs.AI

TL;DR: 提出NPO框架，用于人机循环决策系统的反馈驱动自适应，有形式化对齐损失，实现可扩展操作循环，有收敛结果，在超大规模部署中有价值。


<details>
  <summary>Details</summary>
Motivation: 现有方法将对齐视为静态或事后属性，需一种可操作的反馈驱动自适应学习框架。

Method: 引入可测量、可监督、可减少的对齐损失形式化，提出元对齐概念并证明可通过阈值保真度归约为主要对齐；实现包含场景评分、阈值调整等的可扩展操作循环。

Result: 给出随机反馈下的形式收敛结果，对齐损失和监测保真度累加收敛；NPO在超大规模部署中显示出可衡量的价值。

Conclusion: NPO为持续对齐监测提供紧凑、可检查的架构，弥合理论对齐保证与动态环境中实际可靠性的差距。

Abstract: We present NPO, an alignment-aware learning framework that operationalizes
feedback-driven adaptation in human-in-the-loop decision systems. Unlike prior
approaches that treat alignment as a static or post-hoc property, NPO
introduces a formalization of alignment loss that is measurable, supervisable,
and reducible under structured feedback. In parallel, we propose meta-alignment
as the fidelity of the monitoring process that governs retraining or override
triggers, and show that it is formally reducible to primary alignment via
threshold fidelity. Our implementation spans a scalable operational loop
involving scenario scoring, threshold tuning, policy validation, and structured
feedback ingestion, including "likes", overrides, and abstentions. We provide
formal convergence results under stochastic feedback and show that both
alignment loss and monitoring fidelity converge additively. Empirically, NPO
demonstrates measurable value in hyperscale deployment settings. A
simulation-based artifact and ablation studies further illustrate the
theoretical principles in action. Together, NPO offers a compact, inspectable
architecture for continual alignment monitoring, helping bridge theoretical
alignment guarantees with practical reliability in dynamic environments.

</details>


### [9] [Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses](https://arxiv.org/abs/2507.21132)
*Joshua Adrian Cahyono,Saran Subramanian*

Main category: cs.AI

TL;DR: 本文通过三个实验研究大语言模型在提供高风险生活建议时的失败模式，发现部分模型有谄媚现象，部分模型较稳健，还可通过激活向量操控控制模型谨慎性，强调需多维度基准确保模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在提供高风险生活建议时缺乏标准保障，存在谄媚和过度自信风险，需研究其失败模式。

Method: 进行三个实验：多项选择评估以测量模型面对用户压力的稳定性；使用新的安全类型学和LLM评判进行自由回答分析；进行机制可解释性实验，通过操控“高风险”激活向量引导模型行为。

Result: 部分模型有谄媚现象，o4 - mini等模型较稳健；表现好的模型常问澄清问题获高安全分数；可通过激活引导直接控制模型谨慎性。

Conclusion: 需要细致、多方面的基准来确保大语言模型能被信任用于改变生活的决策。

Abstract: Large Language Models (LLMs) are increasingly consulted for high-stakes life
advice, yet they lack standard safeguards against providing confident but
misguided responses. This creates risks of sycophancy and over-confidence. This
paper investigates these failure modes through three experiments: (1) a
multiple-choice evaluation to measure model stability against user pressure;
(2) a free-response analysis using a novel safety typology and an LLM Judge;
and (3) a mechanistic interpretability experiment to steer model behavior by
manipulating a "high-stakes" activation vector. Our results show that while
some models exhibit sycophancy, others like o4-mini remain robust.
Top-performing models achieve high safety scores by frequently asking
clarifying questions, a key feature of a safe, inquisitive approach, rather
than issuing prescriptive advice. Furthermore, we demonstrate that a model's
cautiousness can be directly controlled via activation steering, suggesting a
new path for safety alignment. These findings underscore the need for nuanced,
multi-faceted benchmarks to ensure LLMs can be trusted with life-changing
decisions.

</details>


### [10] [Project Patti: Why can You Solve Diabolical Puzzles on one Sudoku Website but not Easy Puzzles on another Sudoku Website?](https://arxiv.org/abs/2507.21137)
*Arman Eisenkolb-Vaithyanathan*

Main category: cs.AI

TL;DR: 本文提出两种度量标准刻画数独难度，分析上千数独谜题，构建通用评级系统并验证其有效性，还给出初级玩家解题算法。


<details>
  <summary>Details</summary>
Motivation: 回答不同数独网站数独难度评级的构成因素。

Method: 一是将数独转换为SAT问题，从SAT子句长度分布得出首个度量标准；二是用Nishio回溯算法模拟人类解题，通过计算策略应用次数得到第二个度量标准，用Spearman秩相关系数评估关系，用无监督分类器构建通用评级系统。

Result: 4个网站中通用分类与网站标注难度级别高度相关。

Conclusion: 提出的两个度量标准能有效刻画数独难度，构建的通用评级系统可实现不同网站数独难度的一致映射，还给出初级玩家解题算法。

Abstract: In this paper we try to answer the question "What constitutes Sudoku
difficulty rating across different Sudoku websites?" Using two distinct methods
that can both solve every Sudoku puzzle, I propose two new metrics to
characterize Sudoku difficulty. The first method is based on converting a
Sudoku puzzle into its corresponding Satisfiability (SAT) problem. The first
proposed metric is derived from SAT Clause Length Distribution which captures
the structural complexity of a Sudoku puzzle including the number of given
digits and the cells they are in. The second method simulates human Sudoku
solvers by intertwining four popular Sudoku strategies within a backtracking
algorithm called Nishio. The second metric is computed by counting the number
of times Sudoku strategies are applied within the backtracking iterations of a
randomized Nishio. Using these two metrics, I analyze more than a thousand
Sudoku puzzles across five popular websites to characterize every difficulty
level in each website. I evaluate the relationship between the proposed metrics
and website-labeled difficulty levels using Spearman's rank correlation
coefficient, finding strong correlations for 4 out of 5 websites. I construct a
universal rating system using a simple, unsupervised classifier based on the
two proposed metrics. This rating system is capable of classifying both
individual puzzles and entire difficulty levels from the different Sudoku
websites into three categories - Universal Easy, Universal Medium, and
Universal Hard - thereby enabling consistent difficulty mapping across Sudoku
websites. The experimental results show that for 4 out of 5 Sudoku websites,
the universal classification aligns well with website-labeled difficulty
levels. Finally, I present an algorithm that can be used by early Sudoku
practitioners to solve Sudoku puzzles.

</details>


### [11] [The Geometry of Harmfulness in LLMs through Subconcept Probing](https://arxiv.org/abs/2507.21141)
*McNair Shah,Saleena Angeline,Adhitya Rajendra Kumar,Naitik Chheda,Kevin Zhu,Vasu Sharma,Sean O'Brien,Will Cai*

Main category: cs.AI

TL;DR: 提出多维框架探测和引导大语言模型内部有害内容，发现主导方向引导可减少有害性且不太多降低实用性。


<details>
  <summary>Details</summary>
Motivation: 理解和可靠抑制大语言模型的有害行为。

Method: 为55种不同有害子概念学习线性探针，得到激活空间中的方向，构成有害子空间，测试子空间消融、主导方向引导和消融。

Result: 主导方向引导能以较低的实用性损失近乎消除有害性。

Conclusion: 概念子空间为大语言模型行为提供可扩展视角，为审核和强化未来语言模型提供实用工具。

Abstract: Recent advances in large language models (LLMs) have intensified the need to
understand and reliably curb their harmful behaviours. We introduce a
multidimensional framework for probing and steering harmful content in model
internals. For each of 55 distinct harmfulness subconcepts (e.g., racial hate,
employment scams, weapons), we learn a linear probe, yielding 55 interpretable
directions in activation space. Collectively, these directions span a
harmfulness subspace that we show is strikingly low-rank. We then test ablation
of the entire subspace from model internals, as well as steering and ablation
in the subspace's dominant direction. We find that dominant direction steering
allows for near elimination of harmfulness with a low decrease in utility. Our
findings advance the emerging view that concept subspaces provide a scalable
lens on LLM behaviour and offer practical tools for the community to audit and
harden future generations of language models.

</details>


### [12] [Adaptive XAI in High Stakes Environments: Modeling Swift Trust with Multimodal Feedback in Human AI Teams](https://arxiv.org/abs/2507.21158)
*Nishani Fernando,Bahareh Nakisa,Adnan Ahmad,Mohammad Naim Rastgoo*

Main category: cs.AI

TL;DR: 提出自适应可解释AI概念框架AXTF，利用生理和行为信号推断用户状态，通过多目标个性化信任估计模型促进人机协作中的快速信任，适用于高压力环境。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法在高风险、时间敏感场景下提供统一解释且依赖显式反馈，不实用，需要能促进人机快速信任的自适应可解释方法。

Method: 提出自适应可解释信任框架AXTF，利用生理和行为信号推断用户状态，以多目标个性化信任估计模型将工作量、压力和情绪映射到动态信任估计，指导解释特征调整。

Result: 建立了自适应、非侵入式可解释AI系统的基础。

Conclusion: 该概念框架能满足高压力、时间敏感环境对可解释AI系统的严格要求。

Abstract: Effective human-AI teaming heavily depends on swift trust, particularly in
high-stakes scenarios such as emergency response, where timely and accurate
decision-making is critical. In these time-sensitive and cognitively demanding
settings, adaptive explainability is essential for fostering trust between
human operators and AI systems. However, existing explainable AI (XAI)
approaches typically offer uniform explanations and rely heavily on explicit
feedback mechanisms, which are often impractical in such high-pressure
scenarios. To address this gap, we propose a conceptual framework for adaptive
XAI that operates non-intrusively by responding to users' real-time cognitive
and emotional states through implicit feedback, thereby enhancing swift trust
in high-stakes environments. The proposed adaptive explainability trust
framework (AXTF) leverages physiological and behavioral signals, such as EEG,
ECG, and eye tracking, to infer user states and support explanation adaptation.
At its core is a multi-objective, personalized trust estimation model that maps
workload, stress, and emotion to dynamic trust estimates. These estimates guide
the modulation of explanation features enabling responsive and personalized
support that promotes swift trust in human-AI collaboration. This conceptual
framework establishes a foundation for developing adaptive, non-intrusive XAI
systems tailored to the rigorous demands of high-pressure, time-sensitive
environments.

</details>


### [13] [LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems](https://arxiv.org/abs/2507.21276)
*Yufei Li,Zexin Li,Yinglun Zhu,Cong Liu*

Main category: cs.AI

TL;DR: 文章指出大语言模型推理服务和持续再训练分开部署存在低效问题，提出LeMix系统，可协同管理工作负载，评估显示其在多方面优于传统设置。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型推理服务和持续再训练分开部署在分布式环境中存在低效和数据适应延迟问题。

Method: 提出LeMix系统，集成离线分析、执行预测机制和运行时调度，根据工作负载特征和系统条件动态分配资源。

Result: LeMix使吞吐量最高提升3.53倍，推理损失最多降低0.61倍，响应时间SLO达标率最高提升2.12倍。

Conclusion: 这是首个挖掘和利用大语言模型推理和训练联合机会的工作，为生产环境中更高效部署大语言模型铺平道路。

Abstract: Modern deployment of large language models (LLMs) frequently involves both
inference serving and continuous retraining to stay aligned with evolving data
and user feedback. Common practices separate these workloads onto distinct
servers in isolated phases, causing substantial inefficiencies (e.g., GPU
idleness) and delayed adaptation to new data in distributed settings. Our
empirical analysis reveals that these inefficiencies stem from dynamic request
arrivals during serving and workload heterogeneity in pipeline-parallel
training. To address these challenges, we propose LeMix, a system for
co-locating and managing concurrent LLM serving and training workloads. LeMix
integrates offline profiling, execution prediction mechanisms, and runtime
scheduling to dynamically adapt resource allocation based on workload
characteristics and system conditions. By understanding task-specific behaviors
and co-execution interference across shared nodes, LeMix improves utilization
and serving quality without compromising serving responsiveness. Our evaluation
shows that LeMix improves throughput by up to 3.53x, reduces inference loss by
up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over
traditional separate setups. To our knowledge, this is the first work to
uncover and exploit the opportunities of joint LLM inference and training,
paving the way for more resource-efficient deployment of LLMs in production
environments.

</details>


### [14] [Adaptive Cluster Collaborativeness Boosts LLMs Medical Decision Support Capacity](https://arxiv.org/abs/2507.21159)
*Zhihao Peng,Liuxin Bao,Shengyuan Liu,Yixuan Yuan*

Main category: cs.AI

TL;DR: 现有大语言模型协作性在医疗决策支持有问题，本文提出自适应集群协作方法，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗决策支持中缺乏明确组件选择规则，部分模型表现不佳，影响协作性。

Method: 提出自适应集群协作方法，含自多样性和交叉一致性最大化机制，训练时优先选高自多样性值模型，逐步屏蔽低交叉一致性值模型。

Result: 在两个医学数据集实验表明方法有效，如在NEJMQA上各学科达官方及格分，妇产科准确率超GPT - 4。

Conclusion: 所提自适应集群协作方法能提升大语言模型医疗决策支持能力。

Abstract: The collaborativeness of large language models (LLMs) has proven effective in
natural language processing systems, holding considerable promise for
healthcare development. However, it lacks explicit component selection rules,
necessitating human intervention or clinical-specific validation. Moreover,
existing architectures heavily rely on a predefined LLM cluster, where partial
LLMs underperform in medical decision support scenarios, invalidating the
collaborativeness of LLMs. To this end, we propose an adaptive cluster
collaborativeness methodology involving self-diversity and cross-consistency
maximization mechanisms to boost LLMs medical decision support capacity. For
the self-diversity, we calculate the fuzzy matching value of pairwise outputs
within an LLM as its self-diversity value, subsequently prioritizing LLMs with
high self-diversity values as cluster components in a training-free manner. For
the cross-consistency, we first measure cross-consistency values between the
LLM with the highest self-diversity value and others, and then gradually mask
out the LLM having the lowest cross-consistency value to eliminate the
potential inconsistent output during the collaborative propagation. Extensive
experiments on two specialized medical datasets, NEJMQA and MMLU-Pro-health,
demonstrate the effectiveness of our method across physician-oriented
specialties. For example, on NEJMQA, our method achieves the accuracy rate up
to the publicly official passing score across all disciplines, especially
achieving ACC of 65.47\% compared to the 56.12\% achieved by GPT-4 on the
Obstetrics and Gynecology discipline.

</details>


### [15] [Large Language Model Powered Automated Modeling and Optimization of Active Distribution Network Dispatch Problems](https://arxiv.org/abs/2507.21162)
*Xu Yang,Chenhui Lin,Yue Yang,Qi Wang,Haotian Liu,Haizhou Hua,Wenchuan Wu*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型的主动配电网自动建模与优化方法，分解问题、设计架构、改进技术，经测试验证有效。


<details>
  <summary>Details</summary>
Motivation: 分布式能源资源渗透使主动配电网有效调度至关重要，但新集成的运营商缺乏专业知识，依赖人工成本高、耗时长。

Method: 将主动配电网调度问题分解为多个阶段，设计多LLM协调架构，包括信息提取器、问题制定器和代码编程器，并为每个LLM代理开发定制细化技术。

Result: 通过综合比较和端到端演示，验证了所提架构和方法的有效性。

Conclusion: 所提方法能实现智能、灵活的主动配电网调度，以用户为中心的界面消除技术障碍，提高效率。

Abstract: The increasing penetration of distributed energy resources into active
distribution networks (ADNs) has made effective ADN dispatch imperative.
However, the numerous newly-integrated ADN operators, such as distribution
system aggregators, virtual power plant managers, and end prosumers, often lack
specialized expertise in power system operation, modeling, optimization, and
programming. This knowledge gap renders reliance on human experts both costly
and time-intensive. To address this challenge and enable intelligent, flexible
ADN dispatch, this paper proposes a large language model (LLM) powered
automated modeling and optimization approach. First, the ADN dispatch problems
are decomposed into sequential stages, and a multi-LLM coordination
architecture is designed. This framework comprises an Information Extractor, a
Problem Formulator, and a Code Programmer, tasked with information retrieval,
optimization problem formulation, and code implementation, respectively.
Afterwards, tailored refinement techniques are developed for each LLM agent,
greatly improving the accuracy and reliability of generated content. The
proposed approach features a user-centric interface that enables ADN operators
to derive dispatch strategies via simple natural language queries, eliminating
technical barriers and increasing efficiency. Comprehensive comparisons and
end-to-end demonstrations on various test cases validate the effectiveness of
the proposed architecture and methods.

</details>


### [16] [An ontological analysis of risk in Basic Formal Ontology](https://arxiv.org/abs/2507.21171)
*Federico Donato,Adrien Barton*

Main category: cs.AI

TL;DR: 本文用BFO分类探索风险本质，将风险归为BFO:Role子类并应用于实例分析，明确风险充分条件，提及必要条件待研究。


<details>
  <summary>Details</summary>
Motivation: 探索风险的本质并进行合理分类。

Method: 运用BFO分类，对比风险作为BFO:Role和BFO:Disposition子类的不同观点，将该选择应用于实例并推广。

Result: 得到对风险的整体分析，明确了风险的充分条件。

Conclusion: 通过实例分析和推广得出了风险的分析方法和明确了部分条件，必要条件留待未来研究。

Abstract: The paper explores the nature of risk, providing a characterization using the
categories of the Basic Formal Ontology (BFO). It argues that the category Risk
is a subclass of BFO:Role, contrasting it with a similar view classifying Risk
as a subclass of BFO:Disposition. This modeling choice is applied on one
example of risk, which represents objects, processes (both physical and mental)
and their interrelations, then generalizing from the instances in the example
to obtain an overall analysis of risk, making explicit what are the sufficient
conditions for being a risk. Plausible necessary conditions are also mentioned
for future work. Index Terms: ontology, risk, BFO, role, disposition

</details>


### [17] [Ontological Foundations of State Sovereignty](https://arxiv.org/abs/2507.21172)
*John Beverley,Danielle Limbaugh*

Main category: cs.AI

TL;DR: 文章介绍国家主权本质与相关主张重要性，揭示处理主权数据的策略，为国际事务本体论应用工作做铺垫。


<details>
  <summary>Details</summary>
Motivation: 明确国家主权性质和相关主张的重要性，为国际事务本体论的应用工作奠定基础。

Method: 未提及

Result: 未提及

Conclusion: 为国际事务本体论的应用工作做好准备。

Abstract: This short paper is a primer on the nature of state sovereignty and the
importance of claims about it. It also aims to reveal (merely reveal) a
strategy for working with vague or contradictory data about which states, in
fact, are sovereign. These goals together are intended to set the stage for
applied work in ontology about international affairs.

</details>


### [18] [Tell Me You're Biased Without Telling Me You're Biased -- Toward Revealing Implicit Biases in Medical LLMs](https://arxiv.org/abs/2507.21176)
*Farzana Islam Adiba,Rahmatollah Beheshti*

Main category: cs.AI

TL;DR: 研究提出结合知识图谱与辅助大语言模型的框架揭示医疗大语言模型中的复杂偏差模式，实验显示其优于其他基线。


<details>
  <summary>Details</summary>
Motivation: 医疗大语言模型存在偏差和不公平模式，在临床决策应用前需识别偏差模式以减轻影响。

Method: 提出结合知识图谱和辅助大语言模型的框架，集成对抗扰动技术识别微妙偏差模式，采用定制化多跳知识图谱表征提升评估能力。

Result: 通过一系列综合实验，表明该框架在揭示大语言模型复杂偏差模式方面能力和可扩展性明显优于其他基线。

Conclusion: 所提出的框架能有效揭示医疗大语言模型的复杂偏差模式，具有较好性能。

Abstract: Large language models (LLMs) that are used in medical applications are known
to show biased and unfair patterns. Prior to adopting these in clinical
decision-making applications, it is crucial to identify these bias patterns to
enable effective mitigation of their impact. In this study, we present a novel
framework combining knowledge graphs (KGs) with auxiliary LLMs to
systematically reveal complex bias patterns in medical LLMs. Specifically, the
proposed approach integrates adversarial perturbation techniques to identify
subtle bias patterns. The approach adopts a customized multi-hop
characterization of KGs to enhance the systematic evaluation of arbitrary LLMs.
Through a series of comprehensive experiments (on three datasets, six LLMs, and
five bias types), we show that our proposed framework has noticeably greater
ability and scalability to reveal complex biased patterns of LLMs compared to
other baselines.

</details>


### [19] [Agentic Web: Weaving the Next Web with AI Agents](https://arxiv.org/abs/2507.21206)
*Yingxuan Yang,Mulei Ma,Yuxuan Huang,Huacan Chai,Chenyu Gong,Haoran Geng,Yuanjian Zhou,Ying Wen,Meng Fang,Muhao Chen,Shangding Gu,Ming Jin,Costas Spanos,Yang Yang,Pieter Abbeel,Dawn Song,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: 本文提出理解和构建智能体网络的框架，追溯其演变，分析挑战，探讨应用、风险和治理问题并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的AI智能体促使互联网向智能体网络转变，需构建框架理解和建设。

Method: 追溯智能体网络从PC和移动网络时代的演变，确定核心技术基础，构建包含智能、交互和经济三个维度的概念模型。

Result: 分析了创建可扩展智能体系统的架构和基础设施挑战，如通信协议等。

Conclusion: 讨论了智能体系统的潜在应用、社会风险和治理问题，给出开发开放、安全和智能生态系统的研究方向。

Abstract: The emergence of AI agents powered by large language models (LLMs) marks a
pivotal shift toward the Agentic Web, a new phase of the internet defined by
autonomous, goal-driven interactions. In this paradigm, agents interact
directly with one another to plan, coordinate, and execute complex tasks on
behalf of users. This transition from human-driven to machine-to-machine
interaction allows intent to be delegated, relieving users from routine digital
operations and enabling a more interactive, automated web experience. In this
paper, we present a structured framework for understanding and building the
Agentic Web. We trace its evolution from the PC and Mobile Web eras and
identify the core technological foundations that support this shift. Central to
our framework is a conceptual model consisting of three key dimensions:
intelligence, interaction, and economics. These dimensions collectively enable
the capabilities of AI agents, such as retrieval, recommendation, planning, and
collaboration. We analyze the architectural and infrastructural challenges
involved in creating scalable agentic systems, including communication
protocols, orchestration strategies, and emerging paradigms such as the Agent
Attention Economy. We conclude by discussing the potential applications,
societal risks, and governance issues posed by agentic systems, and outline
research directions for developing open, secure, and intelligent ecosystems
shaped by both human intent and autonomous agent behavior. A continuously
updated collection of relevant studies for agentic web is available at:
https://github.com/SafeRL-Lab/agentic-web.

</details>


### [20] [CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting](https://arxiv.org/abs/2507.21257)
*David Maria Schmidt,Raoul Schubert,Philipp Cimiano*

Main category: cs.AI

TL;DR: 本文提出基准测试LLMs问题解释能力的组合性，生成不同难度数据集，实验表明LLMs在将问题映射到SPARQL查询时难以系统且组合性地进行解释。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（LLMs）将问题解释为SPARQL查询的过程在多大程度上具有系统性。

Method: 基于DBpedia的图模式生成三个不同难度的数据集，使用不同大小模型，结合多种提示、少样本优化技术和微调进行实验。

Result: 随着与优化样本的偏差增加，宏观F1分数从0.45降至0.09，即使输入提供所有必要信息，最低复杂度数据集的F1分数也不超过0.57。

Conclusion: LLMs难以系统且组合性地解释问题并将其映射到SPARQL查询。

Abstract: Language interpretation is a compositional process, in which the meaning of
more complex linguistic structures is inferred from the meaning of their parts.
Large language models possess remarkable language interpretation capabilities
and have been successfully applied to interpret questions by mapping them to
SPARQL queries. An open question is how systematic this interpretation process
is. Toward this question, in this paper, we propose a benchmark for
investigating to what extent the abilities of LLMs to interpret questions are
actually compositional. For this, we generate three datasets of varying
difficulty based on graph patterns in DBpedia, relying on Lemon lexica for
verbalization. Our datasets are created in a very controlled fashion in order
to test the ability of LLMs to interpret structurally complex questions, given
that they have seen the atomic building blocks. This allows us to evaluate to
what degree LLMs are able to interpret complex questions for which they
"understand" the atomic parts. We conduct experiments with models of different
sizes using both various prompt and few-shot optimization techniques as well as
fine-tuning. Our results show that performance in terms of macro $F_1$ degrades
from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the
samples optimized on. Even when all necessary information was provided to the
model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of
lowest complexity. We thus conclude that LLMs struggle to systematically and
compositionally interpret questions and map them into SPARQL queries.

</details>


### [21] [Curiosity by Design: An LLM-based Coding Assistant Asking Clarification Questions](https://arxiv.org/abs/2507.21285)
*Harsh Darji,Thibaud Lutellier*

Main category: cs.AI

TL;DR: 本文构建基于大语言模型的编码助手，通过询问澄清问题解决模糊查询问题，评估显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为编码助手时，开发者提示模糊常导致代码生成错误，现有模型缺乏上下文难以推断用户意图。

Method: 构建端到端系统，包含查询分类器检测模糊查询，微调大语言模型生成澄清问题。

Result: 微调后的大语言模型在生成有用澄清问题上优于零样本提示，用户研究表明模型生成的问题优于基线。

Conclusion: 该编码助手比基线编码助手能产生更准确、有用的代码响应。

Abstract: Large Language Models (LLMs) are increasingly used as coding assistants.
However, the ambiguity of the developer's prompt often leads to incorrect code
generation, as current models struggle to infer user intent without extensive
prompt engineering or external context. This work aims to build an LLM-based
coding assistant that mimics the human code review process by asking
clarification questions when faced with ambiguous or under-specified queries.
  Our end-to-end system includes (1) a query classifier trained to detect
unclear programming-related queries and (2) a fine-tuned LLM that generates
clarification questions. Our evaluation shows that the fine-tuned LLM
outperforms standard zero-shot prompting in generating useful clarification
questions. Furthermore, our user study indicates that users find the
clarification questions generated by our model to outperform the baseline,
demonstrating that our coding assistant produces more accurate and helpful code
responses compared to baseline coding assistants.

</details>


### [22] [Structured Relevance Assessment for Robust Retrieval-Augmented Language Models](https://arxiv.org/abs/2507.21287)
*Aryan Raj,Astitva Veer Garg,Anitha D*

Main category: cs.AI

TL;DR: 本文介绍增强检索式语言模型（RALMs）结构化相关性评估框架，可减少幻觉率并提高推理透明度，推动问答系统发展。


<details>
  <summary>Details</summary>
Motivation: 解决RALMs在减少事实错误、文档相关性评估和知识整合方面的挑战。

Method: 采用多维度评分系统，结合语义匹配和来源可靠性，使用基于嵌入的相关性评分和混合质量文档的合成训练数据，进行专业基准测试、知识整合机制和‘未知’响应协议。

Result: 初步评估显示幻觉率显著降低，推理过程透明度提高。

Conclusion: 该框架推动可靠问答系统发展，虽仍有区分可信信息和平衡系统延迟与全面性的挑战，但向增强RALM可靠性迈出有意义一步。

Abstract: Retrieval-Augmented Language Models (RALMs) face significant challenges in
reducing factual errors, particularly in document relevance evaluation and
knowledge integration. We introduce a framework for structured relevance
assessment that enhances RALM robustness through improved document evaluation,
balanced intrinsic and external knowledge integration, and effective handling
of unanswerable queries. Our approach employs a multi-dimensional scoring
system that considers both semantic matching and source reliability, utilizing
embedding-based relevance scoring and synthetic training data with
mixed-quality documents. We implement specialized benchmarking on niche topics,
a knowledge integration mechanism, and an "unknown" response protocol for
queries with insufficient knowledge coverage. Preliminary evaluations
demonstrate significant reductions in hallucination rates and improved
transparency in reasoning processes. Our framework advances the development of
more reliable question-answering systems capable of operating effectively in
dynamic environments with variable data quality. While challenges persist in
accurately distinguishing credible information and balancing system latency
with thoroughness, this work represents a meaningful step toward enhancing RALM
reliability.

</details>


### [23] [The Impact of Foundational Models on Patient-Centric e-Health Systems](https://arxiv.org/abs/2507.21882)
*Elmira Onagh,Alireza Davoodi,Maleknaz Nayebi*

Main category: cs.AI

TL;DR: 研究调查116个以患者为中心的医疗应用中AI功能集成情况，发现超86.21%处于AI集成早期。


<details>
  <summary>Details</summary>
Motivation: 随着AI在医疗技术中日益普及，了解其在以患者为中心的应用中的成熟度，对评估其可信度、透明度和实际影响至关重要。

Method: 使用大语言模型提取关键功能特征，并将其分类到Gartner AI成熟度模型的不同阶段。

Result: 超过86.21%的应用处于AI集成早期，仅13.79%展示了高级AI集成。

Conclusion: 未提及明确结论，但可推测多数患者中心医疗应用的AI集成尚不成熟。

Abstract: As Artificial Intelligence (AI) becomes increasingly embedded in healthcare
technologies, understanding the maturity of AI in patient-centric applications
is critical for evaluating its trustworthiness, transparency, and real-world
impact. In this study, we investigate the integration and maturity of AI
feature integration in 116 patient-centric healthcare applications. Using Large
Language Models (LLMs), we extracted key functional features, which are then
categorized into different stages of the Gartner AI maturity model. Our results
show that over 86.21\% of applications remain at the early stages of AI
integration, while only 13.79% demonstrate advanced AI integration.

</details>


### [24] [Games Agents Play: Towards Transactional Analysis in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2507.21354)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 本文介绍了将Transactional Analysis原则嵌入多智能体系统的Trans - ACT方法，实验表明该方法能让智能体产生更深入且有上下文感知的交互，为多领域应用开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 多数多智能体系统框架忽略人类行为潜在的认知复杂性，需要改进。

Method: 引入Trans - ACT方法，将Transactional Analysis原则嵌入多智能体系统，把Parent、Adult和Child自我状态集成到智能体认知架构中，各自我状态检索特定上下文记忆并据此做出响应。

Result: 通过重现Stupid游戏场景的实验模拟，证明基于认知和TA原则的智能体可产生更深入且有上下文感知的交互。

Conclusion: 研究为冲突解决、教育支持和高级社会心理学研究等多种应用开辟了新途径。

Abstract: Multi-Agent Systems (MAS) are increasingly used to simulate social
interactions, but most of the frameworks miss the underlying cognitive
complexity of human behavior. In this paper, we introduce Trans-ACT
(Transactional Analysis Cognitive Toolkit), an approach embedding Transactional
Analysis (TA) principles into MAS to generate agents with realistic
psychological dynamics. Trans-ACT integrates the Parent, Adult, and Child ego
states into an agent's cognitive architecture. Each ego state retrieves
context-specific memories and uses them to shape response to new situations.
The final answer is chosen according to the underlying life script of the
agent. Our experimental simulation, which reproduces the Stupid game scenario,
demonstrates that agents grounded in cognitive and TA principles produce deeper
and context-aware interactions. Looking ahead, our research opens a new way for
a variety of applications, including conflict resolution, educational support,
and advanced social psychology studies.

</details>


### [25] [LLM-based Content Classification Approach for GitHub Repositories by the README Files](https://arxiv.org/abs/2507.21899)
*Malik Uzair Mehmood,Shahid Hussain,Wen Li Wang,Muhammad Usama Malik*

Main category: cs.AI

TL;DR: 研究提出利用大语言模型微调自动分类GitHub README文件不同部分，超现有方法且有经济替代方案，助于开发自动化工具。


<details>
  <summary>Details</summary>
Motivation: GitHub仓库README文件重要但常被忽视，其完整性影响采用和利用，研究旨在解决这一问题。

Method: 利用BERT、DistilBERT和RoBERTa三个仅编码器大语言模型，基于含4226个README文件部分的标准数据集微调，还研究了PEFT技术。

Result: 该方法超越现有技术，整体F1分数达0.98，PEFT技术是不牺牲太多性能的经济替代方案。

Conclusion: 研究证明大语言模型用于设计自动分类器的潜力，有助于开发GitHub仓库自动化工具。

Abstract: GitHub is the world's most popular platform for storing, sharing, and
managing code. Every GitHub repository has a README file associated with it.
The README files should contain project-related information as per the
recommendations of GitHub to support the usage and improvement of repositories.
However, GitHub repository owners sometimes neglected these recommendations.
This prevents a GitHub repository from reaching its full potential. This
research posits that the comprehensiveness of a GitHub repository's README file
significantly influences its adoption and utilization, with a lack of detail
potentially hindering its full potential for widespread engagement and impact
within the research community. Large Language Models (LLMs) have shown great
performance in many text-based tasks including text classification, text
generation, text summarization and text translation. In this study, an approach
is developed to fine-tune LLMs for automatically classifying different sections
of GitHub README files. Three encoder-only LLMs are utilized, including BERT,
DistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a
gold-standard dataset consisting of 4226 README file sections. This approach
outperforms current state-of-the-art methods and has achieved an overall F1
score of 0.98. Moreover, we have also investigated the use of
Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation
(LoRA) and shown an economical alternative to full fine-tuning without
compromising much performance. The results demonstrate the potential of using
LLMs in designing an automatic classifier for categorizing the content of
GitHub README files. Consequently, this study contributes to the development of
automated tools for GitHub repositories to improve their identifications and
potential usages.

</details>


### [26] [Optimizing Multi-Tier Supply Chain Ordering with LNN+XGBoost: Mitigating the Bullwhip Effect](https://arxiv.org/abs/2507.21383)
*Chunan Tong*

Main category: cs.AI

TL;DR: 文章指出供应链管理有挑战，传统方法难应对，新兴机器学习技术有局限，引入混合LNN和XGBoost模型优化供应链订货策略，弥补现有方法不足。


<details>
  <summary>Details</summary>
Motivation: 供应链管理面临诸多挑战，传统方法和现有机器学习技术有局限，Liquid Neural Networks在供应链优化潜力待挖掘，需新方法应对。

Method: 引入混合LNN和XGBoost模型，利用LNN动态特征提取和XGBoost全局优化能力。

Result: 未提及具体结果。

Conclusion: 该方法弥补现有方法关键缺口，为动态高效供应链管理提供创新解决方案。

Abstract: Supply chain management faces significant challenges, including demand
fluctuations, inventory imbalances, and amplified upstream order variability
due to the bullwhip effect. Traditional methods, such as simple moving
averages, struggle to address dynamic market conditions. Emerging machine
learning techniques, including LSTM, reinforcement learning, and XGBoost, offer
potential solutions but are limited by computational complexity, training
inefficiencies, or constraints in time-series modeling. Liquid Neural Networks,
inspired by dynamic biological systems, present a promising alternative due to
their adaptability, low computational cost, and robustness to noise, making
them suitable for real-time decision-making and edge computing. Despite their
success in applications like autonomous vehicles and medical monitoring, their
potential in supply chain optimization remains underexplored. This study
introduces a hybrid LNN and XGBoost model to optimize ordering strategies in
multi-tier supply chains. By leveraging LNN's dynamic feature extraction and
XGBoost's global optimization capabilities, the model aims to mitigate the
bullwhip effect and enhance cumulative profitability. The research investigates
how local and global synergies within the hybrid framework address the dual
demands of adaptability and efficiency in SCM. The proposed approach fills a
critical gap in existing methodologies, offering an innovative solution for
dynamic and efficient supply chain management.

</details>


### [27] [Teaching Language Models To Gather Information Proactively](https://arxiv.org/abs/2507.21389)
*Tenghao Huang,Sihao Chen,Muhao Chen,Jonathan May,Longqi Yang,Mengting Wan,Pei Zhou*

Main category: cs.AI

TL;DR: 本文提出主动信息收集任务范式，设计框架并采用强化微调策略训练模型，实验表明训练后的模型表现更好，凸显主动澄清对提升大语言模型协作能力的价值。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在现实场景中面对不完整提示时表现不佳，难以主动收集关键信息，因此需要提升其主动信息收集能力。

Method: 引入主动信息收集任务范式，设计可扩展框架生成部分指定的现实任务，采用强化微调策略奖励能引出新的隐含用户信息的问题。

Result: 训练后的Qwen - 2.5 - 7B模型在自动评估指标上比o3 - mini高18%，人工评估中澄清问题和最终大纲受人类标注者青睐程度分别高42%和28%。

Conclusion: 主动澄清能将大语言模型从被动文本生成器提升为真正的协作思考伙伴。

Abstract: Large language models (LLMs) are increasingly expected to function as
collaborative partners, engaging in back-and-forth dialogue to solve complex,
ambiguous problems. However, current LLMs often falter in real-world settings,
defaulting to passive responses or narrow clarifications when faced with
incomplete or under-specified prompts, falling short of proactively gathering
the missing information that is crucial for high-quality solutions. In this
work, we introduce a new task paradigm: proactive information gathering, where
LLMs must identify gaps in the provided context and strategically elicit
implicit user knowledge through targeted questions. To systematically study and
train this capability, we design a scalable framework that generates partially
specified, real-world tasks, masking key information and simulating authentic
ambiguity. Within this setup, our core innovation is a reinforcement finetuning
strategy that rewards questions that elicit genuinely new, implicit user
information -- such as hidden domain expertise or fine-grained requirements --
that would otherwise remain unspoken. Experiments demonstrate that our trained
Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic
evaluation metrics. More importantly, human evaluation reveals that
clarification questions and final outlines generated by our model are favored
by human annotators by 42% and 28% respectively. Together, these results
highlight the value of proactive clarification in elevating LLMs from passive
text generators to genuinely collaborative thought partners.

</details>


### [28] [Shapley Uncertainty in Natural Language Generation](https://arxiv.org/abs/2507.21406)
*Meilin Zhu,Gaojie Jin,Xiaowei Huang,Lijun Zhang*

Main category: cs.AI

TL;DR: 本文提出基于Shapley的不确定性度量框架，用于问答任务中评估大语言模型输出可信度，实验表明该方法优于基线。


<details>
  <summary>Details</summary>
Motivation: 在问答任务中确定何时信任大语言模型的输出对其对齐至关重要，现有基于阈值的语义熵方法不够细致。

Method: 开发基于Shapley的不确定性度量框架，建立有效不确定性度量的三个基本属性并证明该度量满足这些标准。

Result: 通过大量实验表明，相比类似基线度量，Shapley不确定性度量能更准确地预测大语言模型在问答及其他数据集上的性能。

Conclusion: 提出的Shapley不确定性度量框架在评估大语言模型输出可信度方面表现更优。

Abstract: In question-answering tasks, determining when to trust the outputs is crucial
to the alignment of large language models (LLMs). Kuhn et al. (2023) introduces
semantic entropy as a measure of uncertainty, by incorporating linguistic
invariances from the same meaning. It primarily relies on setting threshold to
measure the level of semantic equivalence relation. We propose a more nuanced
framework that extends beyond such thresholding by developing a Shapley-based
uncertainty metric that captures the continuous nature of semantic
relationships. We establish three fundamental properties that characterize
valid uncertainty metrics and prove that our Shapley uncertainty satisfies
these criteria. Through extensive experiments, we demonstrate that our Shapley
uncertainty more accurately predicts LLM performance in question-answering and
other datasets, compared to similar baseline measures.

</details>


### [29] [Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects](https://arxiv.org/abs/2507.21407)
*Yixin Liu,Guibin Zhang,Kun Wang,Shiyuan Li,Shirui Pan*

Main category: cs.AI

TL;DR: 本文对图增强大语言模型代理（GLA）研究进展进行全面概述，分类分析方法，讨论多智能体系统情况并指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键代理程序上受限，图可作为辅助结构增强复杂代理工作流，且GLA研究快速增长和碎片化，需要综述。

Method: 按GLA在大语言模型代理系统中的主要功能（规划、记忆、工具使用）对现有方法分类，分析图和图学习算法的作用，讨论多智能体系统中GLA的情况。

Result: 完成对GLA研究进展的全面综述。

Conclusion: 给出了推动该领域发展的关键未来方向，希望为GLA未来研究提供路线图，增进对图在大语言模型代理系统中作用的理解。

Abstract: Autonomous agents based on large language models (LLMs) have demonstrated
impressive capabilities in a wide range of applications, including web
navigation, software development, and embodied control. While most LLMs are
limited in several key agentic procedures, such as reliable planning, long-term
memory, tool management, and multi-agent coordination, graphs can serve as a
powerful auxiliary structure to enhance structure, continuity, and coordination
in complex agent workflows. Given the rapid growth and fragmentation of
research on Graph-augmented LLM Agents (GLA), this paper offers a timely and
comprehensive overview of recent advances and also highlights key directions
for future work. Specifically, we categorize existing GLA methods by their
primary functions in LLM agent systems, including planning, memory, and tool
usage, and then analyze how graphs and graph learning algorithms contribute to
each. For multi-agent systems, we further discuss how GLA solutions facilitate
the orchestration, efficiency optimization, and trustworthiness of MAS.
Finally, we highlight key future directions to advance this field, from
improving structural adaptability to enabling unified, scalable, and multimodal
GLA systems. We hope this paper can serve as a roadmap for future research on
GLA and foster a deeper understanding of the role of graphs in LLM agent
systems.

</details>


### [30] [GovRelBench:A Benchmark for Government Domain Relevance](https://arxiv.org/abs/2507.21419)
*Haiquan Wang,Yi Chen,Shang Zeng,Yun Bian,Zhe Cui*

Main category: cs.AI

TL;DR: 当前大语言模型在政府领域评估重安全轻核心能力，提出GovRelBench基准及SoftGovScore方法，代码和数据集开源。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型在政府领域的评估，对模型自身核心能力尤其是领域相关性的评估不足。

Method: 提出GovRelBench基准，包含政府领域提示和评估工具GovRelBERT；训练GovRelBERT时引入SoftGovScore方法，基于ModernBERT架构将硬标签转换为软分数进行模型训练。

Result: 无明确提及具体结果。

Conclusion: 该工作旨在完善政府领域大模型的能力评估框架，为相关研究和实践提供有效工具。

Abstract: Current evaluations of LLMs in the government domain primarily focus on
safety considerations in specific scenarios, while the assessment of the
models' own core capabilities, particularly domain relevance, remains
insufficient. To address this gap, we propose GovRelBench, a benchmark
specifically designed for evaluating the core capabilities of LLMs in the
government domain. GovRelBench consists of government domain prompts and a
dedicated evaluation tool, GovRelBERT. During the training process of
GovRelBERT, we introduce the SoftGovScore method: this method trains a model
based on the ModernBERT architecture by converting hard labels to soft scores,
enabling it to accurately compute the text's government domain relevance score.
This work aims to enhance the capability evaluation framework for large models
in the government domain, providing an effective tool for relevant research and
practice. Our code and dataset are available at
https://github.com/pan-xi/GovRelBench.

</details>


### [31] [Evo-DKD: Dual-Knowledge Decoding for Autonomous Ontology Evolution in Large Language Models](https://arxiv.org/abs/2507.21438)
*Vishal Raman,Vijai Aravindh R*

Main category: cs.AI

TL;DR: 提出Evo - DKD框架用于自主本体演化，结合结构化本体遍历与非结构化文本推理，实验证明其在本体更新精度和下游任务表现上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 本体和知识图谱需持续演化，但手动管理费力，大语言模型虽有非结构化知识但难以保持结构化一致性。

Method: 提出Evo - DKD双解码器框架，在大语言模型内引入两个并行解码流，用动态注意力门控机制协调，因GPU限制用基于提示的模式控制模拟双解码器行为，系统在封闭推理循环中运行。

Result: Evo - DKD在医疗本体细化、语义搜索改进和文化遗产时间线建模等用例中表现有效，在本体更新精度和下游任务性能上优于仅使用结构化或非结构化解码的基线方法。

Conclusion: Evo - DKD为大语言模型驱动的知识库维护提供新范式，结合符号和神经推理优势实现可持续本体演化。

Abstract: Ontologies and knowledge graphs require continuous evolution to remain
comprehensive and accurate, but manual curation is labor intensive. Large
Language Models (LLMs) possess vast unstructured knowledge but struggle with
maintaining structured consistency. We propose Evo-DKD, a novel dual-decoder
framework for autonomous ontology evolution that combines structured ontology
traversal with unstructured text reasoning. Evo-DKD introduces two parallel
decoding streams within an LLM: one decoder generates candidate ontology edits
(e.g., new concepts or relations) while the other produces natural-language
justifications. A dynamic attention-based gating mechanism coordinates the two
streams, deciding at each step how to blend structured and unstructured
knowledge. Due to GPU constraints, we simulate the dual-decoder behavior using
prompt-based mode control to approximate coordinated decoding in a
single-stream mode. The system operates in a closed reasoning loop: proposed
ontology edits are validated (via consistency checks and cross-verification
with the text explanations) and then injected into the knowledge base, which in
turn informs subsequent reasoning. We demonstrate Evo-DKD's effectiveness on
use cases including healthcare ontology refinement, semantic search
improvement, and cultural heritage timeline modeling. Experiments show that
Evo-DKD outperforms baselines using structured-only or unstructured-only
decoding in both precision of ontology updates and downstream task performance.
We present quantitative metrics and qualitative examples, confirming the
contributions of the dual-decoder design and gating router. Evo-DKD offers a
new paradigm for LLM-driven knowledge base maintenance, combining the strengths
of symbolic and neural reasoning for sustainable ontology evolution.

</details>


### [32] [Validating Pharmacogenomics Generative Artificial Intelligence Query Prompts Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2507.21453)
*Ashley Rector,Keaton Minor,Kamden Minor,Jeff McCormack,Beth Breeden,Ryan Nowers,Jay Dorris*

Main category: cs.AI

TL;DR: 研究评估了用于药物基因组学的人工智能工具Sherpa Rx，将CPIC和PharmGKB数据整合，分两阶段评估，结果显示整合资源提升性能，Sherpa Rx表现优于ChatGPT - 4omini等模型。


<details>
  <summary>Details</summary>
Motivation: 验证人工智能工具Sherpa Rx在药物基因组学关键响应指标上的性能。

Method: 使用含260个查询的数据集评估，分两阶段，分别嵌入CPIC数据和额外加入PharmGKB内容，对响应进行多维度评分，用Wilcoxon符号秩检验比较，用20题测验评估适用性。

Result: Phase 1中Sherpa Rx各指标表现高；子集分析显示Phase 2在准确性和完整性上有改善；ChatGPT - 4omini在相关性和清晰度上相当，但准确性和完整性落后；Phase 2显著优于ChatGPT - 4omini；20题测验中Sherpa Rx准确率90%。

Conclusion: 将CPIC和PharmGKB等资源与RAG整合可提升AI准确性和性能，Sherpa Rx等生成式AI在药物基因组学有变革潜力。

Abstract: This study evaluated Sherpa Rx, an artificial intelligence tool leveraging
large language models and retrieval-augmented generation (RAG) for
pharmacogenomics, to validate its performance on key response metrics. Sherpa
Rx integrated Clinical Pharmacogenetics Implementation Consortium (CPIC)
guidelines with Pharmacogenomics Knowledgebase (PharmGKB) data to generate
contextually relevant responses. A dataset (N=260 queries) spanning 26 CPIC
guidelines was used to evaluate drug-gene interactions, dosing recommendations,
and therapeutic implications. In Phase 1, only CPIC data was embedded. Phase 2
additionally incorporated PharmGKB content. Responses were scored on accuracy,
relevance, clarity, completeness (5-point Likert scale), and recall. Wilcoxon
signed-rank tests compared accuracy between Phase 1 and Phase 2, and between
Phase 2 and ChatGPT-4omini. A 20-question quiz assessed the tool's real-world
applicability against other models. In Phase 1 (N=260), Sherpa Rx demonstrated
high performance of accuracy 4.9, relevance 5.0, clarity 5.0, completeness 4.8,
and recall 0.99. The subset analysis (N=20) showed improvements in accuracy
(4.6 vs. 4.4, Phase 2 vs. Phase 1 subset) and completeness (5.0 vs. 4.8).
ChatGPT-4omini performed comparably in relevance (5.0) and clarity (4.9) but
lagged in accuracy (3.9) and completeness (4.2). Differences in accuracy
between Phase 1 and Phase 2 was not statistically significant. However, Phase 2
significantly outperformed ChatGPT-4omini. On the 20-question quiz, Sherpa Rx
achieved 90% accuracy, outperforming other models. Integrating additional
resources like CPIC and PharmGKB with RAG enhances AI accuracy and performance.
This study highlights the transformative potential of generative AI like Sherpa
Rx in pharmacogenomics, improving decision-making with accurate, personalized
responses.

</details>


### [33] [An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning](https://arxiv.org/abs/2507.21471)
*Zujie Xie,Zixuan Chen,Jiheng Liang,Xiangyang Yu,Ziru Yu*

Main category: cs.AI

TL;DR: 研究提出基于大语言模型的框架解决低数据条件下红外光谱自动解释难题，在多数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 红外光谱分析存在高维、重叠谱带挑战传统方法，大语言模型应用于红外光谱分析待探索，要解决低数据条件下准确自动解释红外光谱的问题。

Method: 引入端到端大语言模型驱动的代理框架，整合结构化文献知识库、自动光谱预处理、特征提取和多任务推理，通过查询文献选方法，转换光谱为低维特征集，用少样本提示模板进行任务处理，采用闭环多轮协议迭代优化预测。

Result: 多轮大语言模型在多个数据集上始终优于单轮推理，在低数据情况下可与或超越机器学习和深度学习模型。

Conclusion: 基于大语言模型的框架能有效解决低数据条件下红外光谱自动解释的挑战。

Abstract: Infrared spectroscopy offers rapid, non destructive measurement of chemical
and material properties but suffers from high dimensional, overlapping spectral
bands that challenge conventional chemometric approaches. Emerging large
language models (LLMs), with their capacity for generalization and reasoning,
offer promising potential for automating complex scientific workflows. Despite
this promise, their application in IR spectral analysis remains largely
unexplored. This study addresses the critical challenge of achieving accurate,
automated infrared spectral interpretation under low-data conditions using an
LLM-driven framework. We introduce an end-to-end, large language model driven
agent framework that integrates a structured literature knowledge base,
automated spectral preprocessing, feature extraction, and multi task reasoning
in a unified pipeline. By querying a curated corpus of peer reviewed IR
publications, the agent selects scientifically validated routines. The selected
methods transform each spectrum into low dimensional feature sets, which are
fed into few shot prompt templates for classification, regression, and anomaly
detection. A closed loop, multi turn protocol iteratively appends mispredicted
samples to the prompt, enabling dynamic refinement of predictions. Across
diverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri
Reticulatae Pericarpium and waste water COD datasets, the multi turn LLM
consistently outperforms single turn inference, rivaling or exceeding machine
learning and deep learning models under low data regimes.

</details>


### [34] [Learning to Imitate with Less: Efficient Individual Behavior Modeling in Chess](https://arxiv.org/abs/2507.21488)
*Zhenwei Tang,Difan Jiao,Eric Xue,Reid McIlroy-Young,Jon Kleinberg,Siddhartha Sen,Ashton Anderson*

Main category: cs.AI

TL;DR: 提出Maia4All框架，能在有限数据下高效模拟个体决策风格，在国际象棋中表现出色，还具更广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 开发能准确模拟个体决策的AI很重要，但现有建模方法需大量个体数据，不实用，因此需新方法。

Method: 采用两阶段优化过程，即富集步骤和民主化步骤。

Result: Maia4All能准确预测个体走法和分析行为模式，仅需20局棋，数据效率显著提升。

Conclusion: Maia4All提供了群体AI系统灵活适应个体用户的范例，可拓展到国际象棋之外的领域。

Abstract: As humans seek to collaborate with, learn from, and better understand
artificial intelligence systems, developing AIs that can accurately emulate
individual decision-making becomes increasingly important. Chess, a
long-standing AI benchmark with precise skill measurement, offers an ideal
testbed for human-AI alignment. However, existing approaches to modeling human
behavior require prohibitively large amounts of data from each individual,
making them impractical for new or sparsely represented users. In this work, we
introduce Maia4All, a framework designed to learn and adapt to individual
decision-making styles efficiently, even with limited data. Maia4All achieves
this through a two-stage optimization process: (1) an enrichment step, which
bridges population and individual-level human behavior modeling with a
prototype-enriched model, and (2) a democratization step, which leverages
ability levels or user prototypes to initialize and refine individual
embeddings with minimal data. Our experimental results show that Maia4All can
accurately predict individual moves and profile behavioral patterns with high
fidelity, establishing a new standard for personalized human-like AI behavior
modeling in chess. Maia4All achieves individual human behavior modeling in
chess with only 20 games, compared to the 5,000 games required previously,
representing a significant improvement in data efficiency. Our work provides an
example of how population AI systems can flexibly adapt to individual users
using a prototype-enriched model as a bridge. This approach extends beyond
chess, as shown in our case study on idiosyncratic LLMs, highlighting its
potential for broader applications in personalized AI adaptation.

</details>


### [35] [Large Language Models for Supply Chain Decisions](https://arxiv.org/abs/2507.21502)
*David Simchi-Levi,Konstantina Mellou,Ishai Menache,Jeevan Pathuri*

Main category: cs.AI

TL;DR: 供应链管理决策面临挑战，虽有优化技术但使用不便，借助大语言模型可解决问题、提升决策效率。


<details>
  <summary>Details</summary>
Motivation: 现有供应链决策技术需人工大量参与，决策速度慢，大语言模型发展带来解决契机。

Method: 应用大语言模型解决供应链技术使用中的理解结果、分析场景和更新模型三个挑战。

Result: 将决策时间从数天或数周缩短至数分钟或数小时，大幅提升规划者和高管的生产力与影响力。

Conclusion: 大语言模型可使供应链技术民主化，无需人工过多介入。

Abstract: Supply Chain Management requires addressing a variety of complex
decision-making challenges, from sourcing strategies to planning and execution.
Over the last few decades, advances in computation and information technologies
have enabled the transition from manual, intuition and experience-based
decision-making, into more automated and data-driven decisions using a variety
of tools that apply optimization techniques. These techniques use mathematical
methods to improve decision-making.
  Unfortunately, business planners and executives still need to spend
considerable time and effort to (i) understand and explain the recommendations
coming out of these technologies; (ii) analyze various scenarios and answer
what-if questions; and (iii) update the mathematical models used in these tools
to reflect current business environments. Addressing these challenges requires
involving data science teams and/or the technology providers to explain results
or make the necessary changes in the technology and hence significantly slows
down decision making.
  Motivated by the recent advances in Large Language Models (LLMs), we report
how this disruptive technology can democratize supply chain technology -
namely, facilitate the understanding of tools' outcomes, as well as the
interaction with supply chain tools without human-in-the-loop. Specifically, we
report how we apply LLMs to address the three challenges described above, thus
substantially reducing the time to decision from days and weeks to minutes and
hours as well as dramatically increasing planners' and executives' productivity
and impact.

</details>


### [36] [MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions](https://arxiv.org/abs/2507.21503)
*Yanxu Zhu,Shitong Duan,Xiangxu Zhang,Jitao Sang,Peng Zhang,Tun Lu,Xiao Zhou,Jing Yao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 文章首次系统评估多模态大语言模型（MLLMs）诚实行为，构建MoHoBench基准测试28个模型，发现多数模型诚实性不足，提出初步对齐方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作对MLLMs在视觉无法回答问题时的诚实能力研究不足，需系统评估其诚实行为。

Method: 定义四类视觉无法回答问题，构建含12k+样本的MoHoBench基准，对28个流行MLLMs进行测试分析。

Result: 多数模型在必要时不能正确拒绝回答，MLLMs诚实性受视觉信息影响大。

Conclusion: 需要开发专门的多模态诚实对齐方法，已实现初步对齐方法为后续工作奠基。

Abstract: Recently Multimodal Large Language Models (MLLMs) have achieved considerable
advancements in vision-language tasks, yet produce potentially harmful or
untrustworthy content. Despite substantial work investigating the
trustworthiness of language models, MMLMs' capability to act honestly,
especially when faced with visually unanswerable questions, remains largely
underexplored. This work presents the first systematic assessment of honesty
behaviors across various MLLMs. We ground honesty in models' response behaviors
to unanswerable visual questions, define four representative types of such
questions, and construct MoHoBench, a large-scale MMLM honest benchmark,
consisting of 12k+ visual question samples, whose quality is guaranteed by
multi-stage filtering and human verification. Using MoHoBench, we benchmarked
the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our
findings show that: (1) most models fail to appropriately refuse to answer when
necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but
is deeply influenced by visual information, necessitating the development of
dedicated methods for multimodal honesty alignment. Therefore, we implemented
initial alignment methods using supervised and preference learning to improve
honesty behavior, providing a foundation for future work on trustworthy MLLMs.
Our data and code can be found at https://github.com/DSTTSD/MoHoBench.

</details>


### [37] [What Does it Mean for a Neural Network to Learn a "World Model"?](https://arxiv.org/abs/2507.21513)
*Kenneth Li,Fernanda Viégas,Martin Wattenberg*

Main category: cs.AI

TL;DR: 提出判断神经网络学习和使用“世界模型”的精确标准，基于线性探测文献理念，还添加条件确保模型非平凡。


<details>
  <summary>Details</summary>
Motivation: 为常用的非正式术语赋予操作意义，为实验研究提供通用语言。

Method: 基于线性探测文献的思想，形式化通过数据生成过程表示进行的计算概念，并添加条件检查模型非平凡。

Result: 给出了判断神经网络学习和使用“世界模型”的精确标准。

Conclusion: 提出的标准有助于规范该领域研究，后续可研究行动效果的建模。

Abstract: We propose a set of precise criteria for saying a neural net learns and uses
a "world model." The goal is to give an operational meaning to terms that are
often used informally, in order to provide a common language for experimental
investigation. We focus specifically on the idea of representing a latent
"state space" of the world, leaving modeling the effect of actions to future
work. Our definition is based on ideas from the linear probing literature, and
formalizes the notion of a computation that factors through a representation of
the data generation process. An essential addition to the definition is a set
of conditions to check that such a "world model" is not a trivial consequence
of the neural net's data or task.

</details>


### [38] [ST-GDance: Long-Term and Collision-Free Group Choreography from Music](https://arxiv.org/abs/2507.21518)
*Jing Xu,Weiqiang Wang,Cunjian Chen,Jun Liu,Qiuhong Ke*

Main category: cs.AI

TL;DR: 提出ST - GDance框架解决群舞生成难题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有群舞生成方法难以处理舞者间时空交互，存在扩展性问题和舞者碰撞风险。

Method: 提出ST - GDance框架，解耦时空依赖，采用轻量级图卷积进行空间建模、加速稀疏注意力进行时间建模。

Result: 在AIOZ - GDance数据集上的实验显示，ST - GDance优于现有基线方法，尤其在生成长且连贯的群舞序列方面表现出色。

Conclusion: ST - GDance框架能有效解决群舞生成中的计算复杂度和碰撞问题，具有良好性能。

Abstract: Group dance generation from music has broad applications in film, gaming, and
animation production. However, it requires synchronizing multiple dancers while
maintaining spatial coordination. As the number of dancers and sequence length
increase, this task faces higher computational complexity and a greater risk of
motion collisions. Existing methods often struggle to model dense
spatial-temporal interactions, leading to scalability issues and multi-dancer
collisions. To address these challenges, we propose ST-GDance, a novel
framework that decouples spatial and temporal dependencies to optimize
long-term and collision-free group choreography. We employ lightweight graph
convolutions for distance-aware spatial modeling and accelerated sparse
attention for efficient temporal modeling. This design significantly reduces
computational costs while ensuring smooth and collision-free interactions.
Experiments on the AIOZ-GDance dataset demonstrate that ST-GDance outperforms
state-of-the-art baselines, particularly in generating long and coherent group
dance sequences. Project page: https://yilliajing.github.io/ST-GDance-Website/.

</details>


### [39] [Large Language Models for Wireless Communications: From Adaptation to Autonomy](https://arxiv.org/abs/2507.21524)
*Le Liang,Hao Ye,Yucheng Sheng,Ouya Wang,Jiacheng Wang,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: 文章探讨大语言模型在无线网络中的应用，介绍三个方向、进展、优势，指出挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能力强，而无线通信复杂度与动态性增加，需智能自适应解决方案。

Method: 从三个方向探索：适配预训练大语言模型用于核心通信任务、开发无线特定基础模型、赋予大语言模型自主推理与协调能力。

Result: 强调了大语言模型方法相比传统方法的独特优势，给出近期进展和实际案例。

Conclusion: 指出多模态融合、与轻量级模型协作和自改进能力等挑战与机遇，为未来无线网络指明方向。

Abstract: The emergence of large language models (LLMs) has revolutionized artificial
intelligence, offering unprecedented capabilities in reasoning, generalization,
and zero-shot learning. These strengths open new frontiers in wireless
communications, where increasing complexity and dynamics demand intelligent and
adaptive solutions. This article explores the role of LLMs in transforming
wireless systems across three key directions: adapting pretrained LLMs for core
communication tasks, developing wireless-specific foundation models to balance
versatility and efficiency, and enabling agentic LLMs with autonomous reasoning
and coordination capabilities. We highlight recent advances, practical case
studies, and the unique benefits of LLM-based approaches over traditional
methods. Finally, we outline open challenges and research opportunities,
including multimodal fusion, collaboration with lightweight models, and
self-improving capabilities, charting a path toward intelligent, adaptive, and
autonomous wireless networks of the future.

</details>


### [40] [Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations](https://arxiv.org/abs/2507.21571)
*Laura Spillner,Nima Zargham,Mihai Pomarlan,Robert Porzel,Rainer Malaka*

Main category: cs.AI

TL;DR: 为促进以人类为中心的AI解释，本文提出个性化解释方法，让智能体根据用户情况提供信息。


<details>
  <summary>Details</summary>
Motivation: 现有聚焦黑盒机器学习模型内部机制的解释不适合非专家，需以人类为中心进行AI解释。

Method: 提出个性化解释方法，构建智能体世界观模型作为与用户交互的个人动态记忆，以此估计对用户的新信息。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: The need for explanations in AI has, by and large, been driven by the desire
to increase the transparency of black-box machine learning models. However,
such explanations, which focus on the internal mechanisms that lead to a
specific output, are often unsuitable for non-experts. To facilitate a
human-centered perspective on AI explanations, agents need to focus on
individuals and their preferences as well as the context in which the
explanations are given. This paper proposes a personalized approach to
explanation, where the agent tailors the information provided to the user based
on what is most likely pertinent to them. We propose a model of the agent's
worldview that also serves as a personal and dynamic memory of its previous
interactions with the same user, based on which the artificial agent can
estimate what part of its knowledge is most likely new information to the user.

</details>


### [41] [SafeDriveRAG: Towards Safe Autonomous Driving with Knowledge Graph-based Retrieval-Augmented Generation](https://arxiv.org/abs/2507.21585)
*Hao Ye,Mengshi Qi,Zhaohong Liu,Liang Liu,Huadong Ma*

Main category: cs.AI

TL;DR: 本文研究用视觉语言模型提升自动驾驶安全，创建SafeDrive228K基准和SafeDriveRAG基线，评估主流模型，集成RAG提升性能，代码和数据开源。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视视觉语言模型在交通安全关键驾驶场景的评估，需填补此空白。

Method: 创建SafeDrive228K基准，包含22.8万个跨18个子任务的示例；提出基于知识图谱检索增强生成的SafeDriveRAG方法，采用多尺度子图检索算法；收集互联网交通安全指南；对五个主流视觉语言模型进行评估。

Result: 集成RAG显著提升性能，在交通事故任务中提升4.73%，在极端情况任务中提升8.79%，在交通安全常识中提升14.57%。

Conclusion: 提出的基准和方法对推进交通安全研究有潜力。

Abstract: In this work, we study how vision-language models (VLMs) can be utilized to
enhance the safety for the autonomous driving system, including perception,
situational understanding, and path planning. However, existing research has
largely overlooked the evaluation of these models in traffic safety-critical
driving scenarios. To bridge this gap, we create the benchmark (SafeDrive228K)
and propose a new baseline based on VLM with knowledge graph-based
retrieval-augmented generation (SafeDriveRAG) for visual question answering
(VQA). Specifically, we introduce SafeDrive228K, the first large-scale
multimodal question-answering benchmark comprising 228K examples across 18
sub-tasks. This benchmark encompasses a diverse range of traffic safety
queries, from traffic accidents and corner cases to common safety knowledge,
enabling a thorough assessment of the comprehension and reasoning abilities of
the models. Furthermore, we propose a plug-and-play multimodal knowledge
graph-based retrieval-augmented generation approach that employs a novel
multi-scale subgraph retrieval algorithm for efficient information retrieval.
By incorporating traffic safety guidelines collected from the Internet, this
framework further enhances the model's capacity to handle safety-critical
situations. Finally, we conduct comprehensive evaluations on five mainstream
VLMs to assess their reliability in safety-sensitive driving tasks.
Experimental results demonstrate that integrating RAG significantly improves
performance, achieving a +4.73% gain in Traffic Accidents tasks, +8.79% in
Corner Cases tasks and +14.57% in Traffic Safety Commonsense across five
mainstream VLMs, underscoring the potential of our proposed benchmark and
methodology for advancing research in traffic safety. Our source code and data
are available at https://github.com/Lumos0507/SafeDriveRAG.

</details>


### [42] [Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning](https://arxiv.org/abs/2507.21588)
*Jiong Yin,Liang Li,Jiehua Zhang,Yuhan Gao,Chenggang Yan,Xichun Sheng*

Main category: cs.AI

TL;DR: 提出三阶段PHP方法用于视听多任务增量学习，在四项任务中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决视听多任务增量学习中保留旧任务知识并利用先前经验学习新任务的挑战。

Method: 引入三阶段Progressive Homeostatic and Plastic audio - visual prompt (PHP)方法，包括浅阶段设计任务共享模态聚合适配器、中间阶段提出任务特定模态共享动态生成适配器、深阶段引入任务特定模态独立提示。

Result: 方法在四项任务的不同顺序中达到SOTA性能。

Conclusion: PHP方法能有效平衡知识共享和特异性。

Abstract: Audio-visual multi-task incremental learning aims to continuously learn from
multiple audio-visual tasks without the need for joint training on all tasks.
The challenge of the problem is how to preserve the old task knowledge while
facilitating the learning of new task with previous experiences. To address
these challenges, we introduce a three-stage Progressive Homeostatic and
Plastic audio-visual prompt (PHP) method. In the shallow phase, we design the
task-shared modality aggregating adapter to foster cross-task and cross-modal
audio-visual representation learning to enhance shared understanding between
tasks. In the middle phase, we propose the task-specific modality-shared
dynamic generating adapter, which constructs prompts that are tailored to
individual tasks while remaining general across modalities, which balances the
models ability to retain knowledge against forgetting with its potential for
versatile multi-task transferability. In the deep phase, we introduce the
task-specific modality-independent prompts to further refine the understand
ability by targeting individual information for each task and modality. By
incorporating these three phases, PHP retains task-specific prompts while
adapting shared parameters for new tasks to effectively balance knowledge
sharing and specificity. Our method achieves SOTA performance in different
orders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available at
https://github.com/ENJOY-Yin-jiong/PHP.

</details>


### [43] [Exploring the Link Between Bayesian Inference and Embodied Intelligence: Toward Open Physical-World Embodied AI Systems](https://arxiv.org/abs/2507.21589)
*Bin Liu*

Main category: cs.AI

TL;DR: 本文探讨贝叶斯统计与具身智能的联系，分析贝叶斯推理未在现代具身智能发展中起核心作用的原因，指出当前具身智能局限并强调贝叶斯方法潜力。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯统计与具身智能有概念联系，但贝叶斯原理未在具身智能系统广泛应用，需探究原因并挖掘贝叶斯方法潜力。

Method: 从搜索和学习两个现代AI核心主题审视贝叶斯和当代具身智能方法。

Result: 揭示贝叶斯推理未在现代具身智能发展中起核心作用的原因，发现当前具身智能系统局限于封闭物理世界环境。

Conclusion: 贝叶斯方法在推动具身智能系统向真正的开放物理世界具身智能发展方面有重要潜力。

Abstract: Embodied intelligence posits that cognitive capabilities fundamentally emerge
from - and are shaped by - an agent's real-time sensorimotor interactions with
its environment. Such adaptive behavior inherently requires continuous
inference under uncertainty. Bayesian statistics offers a principled
probabilistic framework to address this challenge by representing knowledge as
probability distributions and updating beliefs in response to new evidence. The
core computational processes underlying embodied intelligence - including
perception, action selection, learning, and even higher-level cognition - can
be effectively understood and modeled as forms of Bayesian inference. Despite
the deep conceptual connection between Bayesian statistics and embodied
intelligence, Bayesian principles have not been widely or explicitly applied in
today's embodied intelligence systems. In this work, we examine both Bayesian
and contemporary embodied intelligence approaches through two fundamental
lenses: search and learning - the two central themes in modern AI, as
highlighted in Rich Sutton's influential essay "The Bitter Lesson". This
analysis sheds light on why Bayesian inference has not played a central role in
the development of modern embodied intelligence. At the same time, it reveals
that current embodied intelligence systems remain largely confined to
closed-physical-world environments, and highlights the potential for Bayesian
methods to play a key role in extending these systems toward truly open
physical-world embodied intelligence.

</details>


### [44] ["Teammates, Am I Clear?": Analysing Legible Behaviours in Teams](https://arxiv.org/abs/2507.21631)
*Miguel Faria,Francisco S. Melo,Ana Paiva*

Main category: cs.AI

TL;DR: 研究多智能体团队中决策的可读性，提出扩展方法并证明其能提升团队表现


<details>
  <summary>Details</summary>
Motivation: 以往工作聚焦单智能体与单人类交互，未充分利用团队决策可读性的优势，本文旨在将可读决策扩展到多智能体场景

Method: 提出将可读决策扩展到多智能体设置的方法，并在多智能体基准场景中展示性能

Result: 具有可读智能体的团队能够优于仅由具有标准最优行为的智能体组成的团队

Conclusion: 将可读决策扩展到多智能体设置可提高协作智能体的性能

Abstract: In this paper we investigate the notion of legibility in sequential
decision-making in the context of teams and teamwork. There have been works
that extend the notion of legibility to sequential decision making, for
deterministic and for stochastic scenarios. However, these works focus on one
agent interacting with one human, foregoing the benefits of having legible
decision making in teams of agents or in team configurations with humans. In
this work we propose an extension of legible decision-making to multi-agent
settings that improves the performance of agents working in collaboration. We
showcase the performance of legible decision making in team scenarios using our
proposed extension in multi-agent benchmark scenarios. We show that a team with
a legible agent is able to outperform a team composed solely of agents with
standard optimal behaviour.

</details>


### [45] [StaffPro: an LLM Agent for Joint Staffing and Profiling](https://arxiv.org/abs/2507.21636)
*Alessio Maritan*

Main category: cs.AI

TL;DR: 研究用大语言模型代理解决劳动力管理中人员配置和人员画像问题，提出StaffPro，经模拟验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决劳动力管理中人员配置和人员画像两个紧密相关的挑战。

Method: 将问题置于数学框架，提出LLM代理StaffPro，建立人机反馈循环。

Result: 咨询公司模拟示例显示，StaffPro能成功估计员工属性并生成高质量排班表。

Conclusion: StaffPro为自动化人事管理提供了强大、可解释且以人为本的解决方案。

Abstract: Large language model (LLM) agents integrate pre-trained LLMs with modular
algorithmic components and have shown remarkable reasoning and decision-making
abilities. In this work, we investigate their use for two tightly intertwined
challenges in workforce management: staffing, i.e., the assignment and
scheduling of tasks to workers, which may require team formation; and
profiling, i.e., the continuous estimation of workers' skills, preferences, and
other latent attributes from unstructured data. We cast these problems in a
formal mathematical framework that links scheduling decisions to latent feature
estimation, and we introduce StaffPro, an LLM agent that addresses staffing and
profiling jointly. Differently from existing staffing solutions, StaffPro
allows expressing optimization objectives using natural language, accepts
textual task descriptions and provides high flexibility. StaffPro interacts
directly with humans by establishing a continuous human-agent feedback loop,
ensuring natural and intuitive use. By analyzing human feedback, our agent
continuously estimates the latent features of workers, realizing life-long
worker profiling and ensuring optimal staffing performance over time. A
consulting firm simulation example demonstrates that StaffPro successfully
estimates workers' attributes and generates high quality schedules. With its
innovative design, StaffPro offers a robust, interpretable, and human-centric
solution for automated personnel management.

</details>


### [46] [Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models](https://arxiv.org/abs/2507.21637)
*Wanying Wang,Zeyu Ma,Han Zheng,Xin Tan,Mingang Chen*

Main category: cs.AI

TL;DR: 研究LVLMs对有害输入的脆弱性，提出SASA技术提升安全性且对实用性影响小。


<details>
  <summary>Details</summary>
Motivation: LVLMs相比仅语言模型更易受有害输入影响，需探究其安全问题。

Method: 通过探究LVLMs内部动态定义三项关键能力并定位位置，提出SASA技术将中间层语义表征投影到早期安全层，用线性探测检测风险。

Result: 安全感知常先于全面语义理解，导致安全性降低；SASA显著提升LVLMs安全性，对实用性影响小。

Conclusion: SASA技术能有效增强LVLMs的安全性，且不损害其实用性。

Abstract: Large vision-language models (LVLMs) are vulnerable to harmful input compared
to their language-only backbones. We investigated this vulnerability by
exploring LVLMs internal dynamics, framing their inherent safety understanding
in terms of three key capabilities. Specifically, we define these capabilities
as safety perception, semantic understanding, and alignment for linguistic
expression, and experimentally pinpointed their primary locations within the
model architecture. The results indicate that safety perception often emerges
before comprehensive semantic understanding, leading to the reduction in
safety. Motivated by these findings, we propose \textbf{Self-Aware Safety
Augmentation (SASA)}, a technique that projects informative semantic
representations from intermediate layers onto earlier safety-oriented layers.
This approach leverages the model's inherent semantic understanding to enhance
safety recognition without fine-tuning. Then, we employ linear probing to
articulate the model's internal semantic comprehension to detect the risk
before the generation process. Extensive experiments on various datasets and
tasks demonstrate that SASA significantly improves the safety of LVLMs, with
minimal impact on the utility.

</details>


### [47] [Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for Assistive Robotics](https://arxiv.org/abs/2507.21638)
*Leonard Hinckeldey,Elliot Fosong,Elle Miller,Rimvydas Rubavicius,Trevor McInroe,Patricia Wollstadt,Christiane B. Wiebel-Herboth,Subramanian Ramamoorthy,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 文章介绍开源基准Assistax，用于解决辅助机器人任务挑战，借助JAX加速，评估后可作为实用基准推进研究。


<details>
  <summary>Details</summary>
Motivation: 现有RL基准多为游戏，难以直接应用于现实具身场景，需多样化RL基准并解决具身交互场景的复杂性。

Method: 引入Assistax基准，利用JAX硬件加速，采用多智能体RL概念化人机交互，对多种算法进行评估和超参数调整。

Result: Assistax在向量化训练时比基于CPU的替代方案快370倍，提供可靠基线。

Conclusion: Assistax是推进辅助机器人RL研究的实用基准，代码开源。

Abstract: The development of reinforcement learning (RL) algorithms has been largely
driven by ambitious challenge tasks and benchmarks. Games have dominated RL
benchmarks because they present relevant challenges, are inexpensive to run and
easy to understand. While games such as Go and Atari have led to many
breakthroughs, they often do not directly translate to real-world embodied
applications. In recognising the need to diversify RL benchmarks and addressing
complexities that arise in embodied interaction scenarios, we introduce
Assistax: an open-source benchmark designed to address challenges arising in
assistive robotics tasks. Assistax uses JAX's hardware acceleration for
significant speed-ups for learning in physics-based simulations. In terms of
open-loop wall-clock time, Assistax runs up to $370\times$ faster when
vectorising training runs compared to CPU-based alternatives. Assistax
conceptualises the interaction between an assistive robot and an active human
patient using multi-agent RL to train a population of diverse partner agents
against which an embodied robotic agent's zero-shot coordination capabilities
can be tested. Extensive evaluation and hyperparameter tuning for popular
continuous control RL and MARL algorithms provide reliable baselines and
establish Assistax as a practical benchmark for advancing RL research for
assistive robotics. The code is available at:
https://github.com/assistive-autonomy/assistax.

</details>


### [48] [Can the current trends of AI handle a full course of mathematics?](https://arxiv.org/abs/2507.21664)
*Mariam Alsayyad,Fayadh Kadhem*

Main category: cs.AI

TL;DR: 探讨当前人工智能承担大学数学全课程教学责任的能力，指出其虽有优势但在人文方面不足，并给出人机结合建议。


<details>
  <summary>Details</summary>
Motivation: 研究当前人工智能趋势能否承担大学数学全课程的教学责任。

Method: 从创建课程大纲、呈现所选材料、回答学生问题和创建评估四个方面评估人工智能能力。

Result: 人工智能在组织和准确性等方面较强，但在人文情感方面远不及人类。

Conclusion: 建议整合人类和人工智能的潜力，以更好地实现大学数学全课程教学目标。

Abstract: This paper addresses the question of how able the current trends of
Artificial Intelligence (AI) are in managing to take the responsibility of a
full course of mathematics at a college level. The study evaluates this ability
in four significant aspects, namely, creating a course syllabus, presenting
selected material, answering student questions, and creating an assessment. It
shows that even though the AI is strong in some important parts like
organization and accuracy, there are still some human aspects that are far away
from the current abilities of AI. There is still a hidden emotional part, even
in science, that cannot be fulfilled by the AI in its current state. This paper
suggests some recommendations to integrate the human and AI potentials to
create better outcomes in terms of reaching the target of creating a full
course of mathematics, at a university level, as best as possible.

</details>


### [49] [Unrolling Dynamic Programming via Graph Filters](https://arxiv.org/abs/2507.21705)
*Sergio Rozada,Samuel Rey,Gonzalo Mateos,Antonio G. Marques*

Main category: cs.AI

TL;DR: 本文提出BellNet方法解决MDP中DP计算成本高的问题，初步实验显示其效率优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 标准DP方法在状态 - 动作空间大或有长期依赖时计算成本高。

Method: 将策略迭代展开并截断为可学习的参数化模型BellNet，通过随机值函数初始化最小化贝尔曼误差进行训练，结合图信号处理解释BellNet。

Result: 在网格环境的初步实验中，BellNet能在经典方法所需迭代次数的一小部分内有效近似最优策略。

Conclusion: BellNet为策略和值迭代提供了简洁、可转移和统一的表示，且在推理时能明确控制复杂度。

Abstract: Dynamic programming (DP) is a fundamental tool used across many engineering
fields. The main goal of DP is to solve Bellman's optimality equations for a
given Markov decision process (MDP). Standard methods like policy iteration
exploit the fixed-point nature of these equations to solve them iteratively.
However, these algorithms can be computationally expensive when the
state-action space is large or when the problem involves long-term
dependencies. Here we propose a new approach that unrolls and truncates policy
iterations into a learnable parametric model dubbed BellNet, which we train to
minimize the so-termed Bellman error from random value function
initializations. Viewing the transition probability matrix of the MDP as the
adjacency of a weighted directed graph, we draw insights from graph signal
processing to interpret (and compactly re-parameterize) BellNet as a cascade of
nonlinear graph filters. This fresh look facilitates a concise, transferable,
and unifying representation of policy and value iteration, with an explicit
handle on complexity during inference. Preliminary experiments conducted in a
grid-like environment demonstrate that BellNet can effectively approximate
optimal policies in a fraction of the iterations required by classical methods.

</details>


### [50] [GDAIP: A Graph-Based Domain Adaptive Framework for Individual Brain Parcellation](https://arxiv.org/abs/2507.21727)
*Jianfei Zhu,Haiqi Zhu,Shaohui Liu,Feng Jiang,Baichun Wei,Chunzhi Yi*

Main category: cs.AI

TL;DR: 提出GDAIP框架解决跨数据集个体脑分割问题，实验证明其效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在跨数据集脑分割时因域偏移效果不佳，需新方法解决。

Method: 提出GDAIP框架，结合GAT与MME域适应，构建跨数据集脑图，利用半监督训练和对抗优化。

Result: 实验表明GDAIP产生的个体分割具有拓扑合理边界、强跨会话一致性及反映功能组织的能力。

Conclusion: GDAIP能有效实现跨数据集设置下的个体脑分割。

Abstract: Recent deep learning approaches have shown promise in learning such
individual brain parcellations from functional magnetic resonance imaging
(fMRI). However, most existing methods assume consistent data distributions
across domains and struggle with domain shifts inherent to real-world
cross-dataset scenarios. To address this challenge, we proposed Graph Domain
Adaptation for Individual Parcellation (GDAIP), a novel framework that
integrates Graph Attention Networks (GAT) with Minimax Entropy (MME)-based
domain adaptation. We construct cross-dataset brain graphs at both the group
and individual levels. By leveraging semi-supervised training and adversarial
optimization of the prediction entropy on unlabeled vertices from target brain
graph, the reference atlas is adapted from the group-level brain graph to the
individual brain graph, enabling individual parcellation under cross-dataset
settings. We evaluated our method using parcellation visualization, Dice
coefficient, and functional homogeneity. Experimental results demonstrate that
GDAIP produces individual parcellations with topologically plausible
boundaries, strong cross-session consistency, and ability of reflecting
functional organization.

</details>


### [51] [SAT-Based Bounded Fitting for the Description Logic ALC](https://arxiv.org/abs/2507.21752)
*Maurice Funk,Jean Christoph Jung,Tom Voellmer*

Main category: cs.AI

TL;DR: 研究描述逻辑ALC及其句法片段的有界拟合，证明拟合问题NP完全，对比学习算法并给出实现。


<details>
  <summary>Details</summary>
Motivation: 对从正负数据示例中学习逻辑公式的有界拟合范式感兴趣，研究其在描述逻辑ALC及其片段的应用。

Method: 理论证明拟合问题复杂度，基于SAT求解器实现有界拟合并做优化。

Result: 证明所有研究片段的大小受限拟合问题是NP完全的，有界拟合在PAC学习框架有概率保证而其他学习算法没有。

Conclusion: 有界拟合在ALC及其片段学习中有独特优势，实现的工具可与其他概念学习工具对比。

Abstract: Bounded fitting is a general paradigm for learning logical formulas from
positive and negative data examples, that has received considerable interest
recently. We investigate bounded fitting for the description logic ALC and its
syntactic fragments. We show that the underlying size-restricted fitting
problem is NP-complete for all studied fragments, even in the special case of a
single positive and a single negative example. By design, bounded fitting comes
with probabilistic guarantees in Valiant's PAC learning framework. In contrast,
we show that other classes of algorithms for learning ALC concepts do not
provide such guarantees. Finally, we present an implementation of bounded
fitting in ALC and its fragments based on a SAT solver. We discuss
optimizations and compare our implementation to other concept learning tools.

</details>


### [52] [Towards a rigorous evaluation of RAG systems: the challenge of due diligence](https://arxiv.org/abs/2507.21753)
*Grégoire Martinon,Alexandra Lorenzo de Brionne,Jérôme Bohard,Antoine Lojou,Damien Hervault,Nicolas J-B. Brunel*

Main category: cs.AI

TL;DR: 研究评估投资基金尽职调查中使用的RAG系统，提出结合人工和LLM - Judge注释的评估协议，提供数据集以提升RAG系统评估协议可靠性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI推动高风险领域发展，RAG架构有潜力但在关键场景可靠性存疑，如存在幻觉等问题。

Method: 提出结合人类注释和LLM - Judge注释的评估协议，受PPI方法启发进行精确性能测量。

Result: 完成对投资基金尽职调查中RAG系统的评估，提供了全面数据集。

Conclusion: 研究有助于提升RAG系统评估协议在工业应用中的可靠性和可扩展性。

Abstract: The rise of generative AI, has driven significant advancements in high-risk
sectors like healthcare and finance. The Retrieval-Augmented Generation (RAG)
architecture, combining language models (LLMs) with search engines, is
particularly notable for its ability to generate responses from document
corpora. Despite its potential, the reliability of RAG systems in critical
contexts remains a concern, with issues such as hallucinations persisting. This
study evaluates a RAG system used in due diligence for an investment fund. We
propose a robust evaluation protocol combining human annotations and LLM-Judge
annotations to identify system failures, like hallucinations, off-topic, failed
citations, and abstentions. Inspired by the Prediction Powered Inference (PPI)
method, we achieve precise performance measurements with statistical
guarantees. We provide a comprehensive dataset for further analysis. Our
contributions aim to enhance the reliability and scalability of RAG systems
evaluation protocols in industrial applications.

</details>


### [53] [Hybrid Causal Identification and Causal Mechanism Clustering](https://arxiv.org/abs/2507.21792)
*Saixiong Liu,Yuhua Qian,Jue Li,Honghong Cheng,Feijiang Li*

Main category: cs.AI

TL;DR: 提出MCVCI模型和MCVCC方法推断异质因果关系，在多数据上表现优。


<details>
  <summary>Details</summary>
Motivation: 现有二元因果方法多基于单一因果机制，现实中观测数据因果关系异质，需新方法推断异质因果。

Method: 提出MCVCI模型，结合高斯混合模型和神经网络拟合能力，用混合条件变分自编码器概率边界似然作因果决策标准；提出MCVCC方法，将因果异质性建模为聚类数。

Result: 在多个模拟和真实数据集上，相比现有方法表现最优。

Conclusion: 本文提出的方法有效。

Abstract: Bivariate causal direction identification is a fundamental and vital problem
in the causal inference field. Among binary causal methods, most methods based
on additive noise only use one single causal mechanism to construct a causal
model. In the real world, observations are always collected in different
environments with heterogeneous causal relationships. Therefore, on observation
data, this paper proposes a Mixture Conditional Variational Causal Inference
model (MCVCI) to infer heterogeneous causality. Specifically, according to the
identifiability of the Hybrid Additive Noise Model (HANM), MCVCI combines the
superior fitting capabilities of the Gaussian mixture model and the neural
network and elegantly uses the likelihoods obtained from the probabilistic
bounds of the mixture conditional variational auto-encoder as causal decision
criteria. Moreover, we model the casual heterogeneity into cluster numbers and
propose the Mixture Conditional Variational Causal Clustering (MCVCC) method,
which can reveal causal mechanism expression. Compared with state-of-the-art
methods, the comprehensive best performance demonstrates the effectiveness of
the methods proposed in this paper on several simulated and real data.

</details>


### [54] [MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE](https://arxiv.org/abs/2507.21802)
*Junzhe Li,Yutao Cui,Tao Huang,Yinping Ma,Chun Fan,Miles Yang,Zhao Zhong*

Main category: cs.AI

TL;DR: 提出MixGRPO框架，结合SDE和ODE优化流程，还推出更快的MixGRPO - Flash，在人类偏好对齐多维度表现佳，训练时间大幅降低。


<details>
  <summary>Details</summary>
Motivation: 现有如FlowGRPO等方法因在MDP所有去噪步骤采样和优化而效率低，需改进。

Method: 提出MixGRPO框架，利用SDE和ODE混合采样策略，引入滑动窗口机制；推出MixGRPO - Flash。

Result: MixGRPO在人类偏好对齐多维度有显著提升，比DanceGRPO更有效率，训练时间降低近50%；MixGRPO - Flash进一步降低训练时间71%。

Conclusion: MixGRPO及其快速变体MixGRPO - Flash能有效提高图像生成中人类偏好对齐模型的训练效率和性能。

Abstract: Although GRPO substantially enhances flow matching models in human preference
alignment of image generation, methods such as FlowGRPO still exhibit
inefficiency due to the necessity of sampling and optimizing over all denoising
steps specified by the Markov Decision Process (MDP). In this paper, we propose
$\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed
sampling strategies through the integration of stochastic differential
equations (SDE) and ordinary differential equations (ODE). This streamlines the
optimization process within the MDP to improve efficiency and boost
performance. Specifically, MixGRPO introduces a sliding window mechanism, using
SDE sampling and GRPO-guided optimization only within the window, while
applying ODE sampling outside. This design confines sampling randomness to the
time-steps within the window, thereby reducing the optimization overhead, and
allowing for more focused gradient updates to accelerate convergence.
Additionally, as time-steps beyond the sliding window are not involved in
optimization, higher-order solvers are supported for sampling. So we present a
faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves
training efficiency while achieving comparable performance. MixGRPO exhibits
substantial gains across multiple dimensions of human preference alignment,
outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%
lower training time. Notably, MixGRPO-Flash further reduces training time by
71%. Codes and models are available at
$\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.

</details>


### [55] [An Agentic AI for a New Paradigm in Business Process Development](https://arxiv.org/abs/2507.21823)
*Mohammad Azarijafari,Luisa Mich,Michele Missikoff*

Main category: cs.AI

TL;DR: 本文介绍利用Agentic AI进行业务流程设计与开发的新方法，可实现灵活的自动化。


<details>
  <summary>Details</summary>
Motivation: 人工智能代理是工业自动化技术演进的重要变革，需新的业务流程设计与开发方法。

Method: 提出基于代理的方法，以业务目标和对象为核心，单代理无法完成目标时多代理协作。

Result: 该模型使业务流程开发更模块化和智能化。

Conclusion: 此方法能在动态工业环境中实现灵活且上下文感知的自动化。

Abstract: Artificial Intelligence agents represent the next major revolution in the
continuous technological evolution of industrial automation. In this paper, we
introduce a new approach for business process design and development that
leverages the capabilities of Agentic AI. Departing from the traditional
task-based approach to business process design, we propose an agent-based
method, where agents contribute to the achievement of business goals,
identified by a set of business objects. When a single agent cannot fulfill a
goal, we have a merge goal that can be achieved through the collaboration of
multiple agents. The proposed model leads to a more modular and intelligent
business process development by organizing it around goals, objects, and
agents. As a result, this approach enables flexible and context-aware
automation in dynamic industrial environments.

</details>


### [56] [DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series Forecasting Framework](https://arxiv.org/abs/2507.21830)
*Kuiye Ding,Fanda Fan,Yao Wang,Ruijie jian,Xiaorui Wang,Luqi Gong,Yishan Jiang,Chunjie Luo an Jianfeng Zhan*

Main category: cs.AI

TL;DR: 本文提出 DualSG 双流框架，用大语言模型提供语义指导进行多变量时间序列预测，实验表明其优于 15 个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有使用大语言模型进行多变量时间序列预测的方法存在数值精度损失、处理能力超出设计范围和模态对齐困难等问题。

Method: 提出 DualSG 双流框架，将大语言模型作为语义指导模块；引入时间序列描述作为显式提示格式；设计描述引导的融合模块。

Result: 在不同领域的真实数据集上，DualSG 始终优于 15 个最先进的基线模型。

Conclusion: 显式地将数值预测与语义指导相结合具有重要价值。

Abstract: Multivariate Time Series Forecasting plays a key role in many applications.
Recent works have explored using Large Language Models for MTSF to take
advantage of their reasoning abilities. However, many methods treat LLMs as
end-to-end forecasters, which often leads to a loss of numerical precision and
forces LLMs to handle patterns beyond their intended design. Alternatively,
methods that attempt to align textual and time series modalities within latent
space frequently encounter alignment difficulty. In this paper, we propose to
treat LLMs not as standalone forecasters, but as semantic guidance modules
within a dual-stream framework. We propose DualSG, a dual-stream framework that
provides explicit semantic guidance, where LLMs act as Semantic Guides to
refine rather than replace traditional predictions. As part of DualSG, we
introduce Time Series Caption, an explicit prompt format that summarizes trend
patterns in natural language and provides interpretable context for LLMs,
rather than relying on implicit alignment between text and time series in the
latent space. We also design a caption-guided fusion module that explicitly
models inter-variable relationships while reducing noise and computation.
Experiments on real-world datasets from diverse domains show that DualSG
consistently outperforms 15 state-of-the-art baselines, demonstrating the value
of explicitly combining numerical forecasting with semantic guidance.

</details>


### [57] [Probabilistic Active Goal Recognition](https://arxiv.org/abs/2507.21846)
*Chenyuan Zhang,Cristian Rojas Cardenas,Hamid Rezatofighi,Mor Vered,Buser Say*

Main category: cs.AI

TL;DR: 文章提出主动目标识别概率框架，结合联合信念更新机制与MCTS算法，实验表明该方案是实用且强大的目标推理框架。


<details>
  <summary>Details</summary>
Motivation: 多智能体环境中有效交互需理解其他智能体信念和意图，以往目标识别研究多将观察者视为被动推理者，主动目标识别关注策略性收集信息以减少不确定性。

Method: 采用概率框架，结合联合信念更新机制与蒙特卡罗树搜索（MCTS）算法。

Result: 联合信念更新显著优于被动目标识别，与领域无关的MCTS算法性能与强领域特定贪婪基线相当。

Conclusion: 该解决方案是实用且强大的目标推理框架，推动多智能体系统向更具交互性和适应性发展。

Abstract: In multi-agent environments, effective interaction hinges on understanding
the beliefs and intentions of other agents. While prior work on goal
recognition has largely treated the observer as a passive reasoner, Active Goal
Recognition (AGR) focuses on strategically gathering information to reduce
uncertainty. We adopt a probabilistic framework for Active Goal Recognition and
propose an integrated solution that combines a joint belief update mechanism
with a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan
efficiently and infer the actor's hidden goal without requiring domain-specific
knowledge. Through comprehensive empirical evaluation in a grid-based domain,
we show that our joint belief update significantly outperforms passive goal
recognition, and that our domain-independent MCTS performs comparably to our
strong domain-specific greedy baseline. These results establish our solution as
a practical and robust framework for goal inference, advancing the field toward
more interactive and adaptive multi-agent systems.

</details>


### [58] [EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity](https://arxiv.org/abs/2507.21848)
*Xingjian Zhang,Siwei Wen,Wenjun Wu,Lei Huang*

Main category: cs.AI

TL;DR: 现有GRPO算法存在优势崩溃问题，本文提出EDGE - GRPO算法缓解该问题，实验证明其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: GRPO算法依赖稀疏奖励规则，存在组内奖励相同导致的优势崩溃问题，现有解决方法有局限。

Method: 分析模型反思的局限性，研究细粒度样本级响应的策略熵，提出EDGE - GRPO算法，采用熵驱动优势和引导纠错。

Result: 在多个主要推理基准上的大量实验证明了方法的有效性和优越性。

Conclusion: EDGE - GRPO算法能有效缓解优势崩溃问题，代码开源。

Abstract: Large Language Models (LLMs) have made remarkable progress in enhancing
step-by-step reasoning through reinforcement learning. However, the Group
Relative Policy Optimization (GRPO) algorithm, which relies on sparse reward
rules, often encounters the issue of identical rewards within groups, leading
to the advantage collapse problem. Existing works typically address this
challenge from two perspectives: enforcing model reflection to enhance response
diversity, and introducing internal feedback to augment the training signal
(advantage). In this work, we begin by analyzing the limitations of model
reflection and investigating the policy entropy of responses at the
fine-grained sample level. Based on our experimental findings, we propose the
EDGE-GRPO algorithm, which adopts \textbf{E}ntropy-\textbf{D}riven Advantage
and \textbf{G}uided \textbf{E}rror Correction to effectively mitigate the
problem of advantage collapse. Extensive experiments on several main reasoning
benchmarks demonstrate the effectiveness and superiority of our approach. It is
available at https://github.com/ZhangXJ199/EDGE-GRPO.

</details>


### [59] [MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors](https://arxiv.org/abs/2507.21872)
*Shouyi Lu,Zihan Lin,Chao Lu,Huanran Wang,Guirong Zhuo,Lianqing Zheng*

Main category: cs.AI

TL;DR: 提出MultiEditor框架处理自动驾驶中多模态数据长尾分布问题，实验显示其性能优越，提升罕见类别检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统中真实世界数据长尾分布阻碍泛化，尤其是对罕见但安全关键车辆类别的问题。

Method: 提出双分支潜在扩散框架MultiEditor，引入3D高斯溅射作为先验，设计多级外观控制机制，提出深度引导的可变形跨模态条件模块。

Result: MultiEditor在视觉和几何保真度、编辑可控性和跨模态一致性方面表现优越，生成的罕见类别车辆数据提升感知模型对代表性不足类别的检测精度。

Conclusion: MultiEditor能有效解决自动驾驶多模态数据长尾分布问题，提升感知模型性能。

Abstract: Autonomous driving systems rely heavily on multimodal perception data to
understand complex environments. However, the long-tailed distribution of
real-world data hinders generalization, especially for rare but safety-critical
vehicle categories. To address this challenge, we propose MultiEditor, a
dual-branch latent diffusion framework designed to edit images and LiDAR point
clouds in driving scenarios jointly. At the core of our approach is introducing
3D Gaussian Splatting (3DGS) as a structural and appearance prior for target
objects. Leveraging this prior, we design a multi-level appearance control
mechanism--comprising pixel-level pasting, semantic-level guidance, and
multi-branch refinement--to achieve high-fidelity reconstruction across
modalities. We further propose a depth-guided deformable cross-modality
condition module that adaptively enables mutual guidance between modalities
using 3DGS-rendered depth, significantly enhancing cross-modality consistency.
Extensive experiments demonstrate that MultiEditor achieves superior
performance in visual and geometric fidelity, editing controllability, and
cross-modality consistency. Furthermore, generating rare-category vehicle data
with MultiEditor substantially enhances the detection accuracy of perception
models on underrepresented classes.

</details>


### [60] [A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data](https://arxiv.org/abs/2507.21873)
*Raffaele Pojer,Andrea Passerini,Kim G. Larsen,Manfred Jaeger*

Main category: cs.AI

TL;DR: 本文提出将GNN无缝集成到RBN的神经符号框架，结合两者优势，有两种实现方式并提出MAP推理方法，通过两个应用展示其通用性。


<details>
  <summary>Details</summary>
Motivation: GNN缺乏结合符号领域知识和进行通用推理的能力，RBN虽有相关能力但需要结合GNN的学习优势，因此要将二者集成。

Method: 开发两种集成实现，一是将GNN直接编译到RBN原生语言，二是将GNN作为外部组件；提出MAP推理方法。

Result: 将GNN用于节点分类转化为集体分类模型提高了准确性；在环境规划中引入多目标网络优化问题，MAP推理支持复杂决策，还提供了新的公开基准数据集。

Conclusion: 该神经符号方法强大且连贯，能在图数据上连接学习和推理，实现新应用并提升多任务性能。

Abstract: Graph neural networks (GNNs) excel at predictive tasks on graph-structured
data but often lack the ability to incorporate symbolic domain knowledge and
perform general reasoning. Relational Bayesian Networks (RBNs), in contrast,
enable fully generative probabilistic modeling over graph-like structures and
support rich symbolic knowledge and probabilistic inference. This paper
presents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs,
combining the learning strength of GNNs with the flexible reasoning
capabilities of RBNs.
  We develop two implementations of this integration: one compiles GNNs
directly into the native RBN language, while the other maintains the GNN as an
external component. Both approaches preserve the semantics and computational
properties of GNNs while fully aligning with the RBN modeling paradigm. We also
propose a maximum a-posteriori (MAP) inference method for these neuro-symbolic
models.
  To demonstrate the framework's versatility, we apply it to two distinct
problems. First, we transform a GNN for node classification into a collective
classification model that explicitly models homo- and heterophilic label
patterns, substantially improving accuracy. Second, we introduce a
multi-objective network optimization problem in environmental planning, where
MAP inference supports complex decision-making. Both applications include new
publicly available benchmark datasets.
  This work introduces a powerful and coherent neuro-symbolic approach to graph
data, bridging learning and reasoning in ways that enable novel applications
and improved performance across diverse tasks.

</details>


### [61] [Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis](https://arxiv.org/abs/2507.21875)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 本文提出轻量级预训练嵌入模型Tiny - BioMoE用于自动疼痛评估，经多模态实验验证其有效性，代码和权重公开。


<details>
  <summary>Details</summary>
Motivation: 疼痛影响大量人群，准确评估对患者和医疗系统都很重要，自动疼痛评估系统有诸多优势，利用生理信号可提高系统性能。

Method: 提出轻量级预训练嵌入模型Tiny - BioMoE，在440万个生物信号图像表示上训练，仅有730万个参数用于下游任务嵌入提取。

Result: 涉及多种生理信号及组合的大量实验表明，该模型在自动疼痛识别任务的不同模态中都有效。

Conclusion: Tiny - BioMoE是用于自动疼痛评估的有效工具，可在多模态自动疼痛识别任务中发挥作用。

Abstract: Pain is a complex and pervasive condition that affects a significant portion
of the population. Accurate and consistent assessment is essential for
individuals suffering from pain, as well as for developing effective management
strategies in a healthcare system. Automatic pain assessment systems enable
continuous monitoring, support clinical decision-making, and help minimize
patient distress while mitigating the risk of functional deterioration.
Leveraging physiological signals offers objective and precise insights into a
person's state, and their integration in a multimodal framework can further
enhance system performance. This study has been submitted to the \textit{Second
Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The
proposed approach introduces \textit{Tiny-BioMoE}, a lightweight pretrained
embedding model for biosignal analysis. Trained on $4.4$ million biosignal
image representations and consisting of only $7.3$ million parameters, it
serves as an effective tool for extracting high-quality embeddings for
downstream tasks. Extensive experiments involving electrodermal activity, blood
volume pulse, respiratory signals, peripheral oxygen saturation, and their
combinations highlight the model's effectiveness across diverse modalities in
automatic pain recognition tasks. \textit{\textcolor{blue}{The model's
architecture (code) and weights are available at
https://github.com/GkikasStefanos/Tiny-BioMoE.

</details>


### [62] [Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image](https://arxiv.org/abs/2507.21881)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 文章聚焦自动疼痛评估系统，提出利用皮肤电活动信号的方法，实验证明该方法有效，是传统融合方法的有力替代。


<details>
  <summary>Details</summary>
Motivation: 疼痛影响大量人群，可靠评估对患者和管理策略发展有益，自动疼痛评估系统有诸多优势，需开发有效方法。

Method: 提出一个利用皮肤电活动信号作为输入模态的流程，创建信号的多种表示并在多表示图中联合可视化。

Result: 通过大量实验，该方法结果与传统融合方法相当，在一些情况下更优。

Conclusion: 该方法是整合不同信号表示或模态的可靠替代方案。

Abstract: Pain is a multifaceted phenomenon that affects a substantial portion of the
population. Reliable and consistent evaluation benefits those experiencing pain
and underpins the development of effective and advanced management strategies.
Automatic pain-assessment systems deliver continuous monitoring, inform
clinical decision-making, and aim to reduce distress while preventing
functional decline. By incorporating physiological signals, these systems
provide objective, accurate insights into an individual's condition. This study
has been submitted to the \textit{Second Multimodal Sensing Grand Challenge for
Next-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline
that leverages electrodermal activity signals as input modality. Multiple
representations of the signal are created and visualized as waveforms, and they
are jointly visualized within a single multi-representation diagram. Extensive
experiments incorporating various processing and filtering techniques, along
with multiple representation combinations, demonstrate the effectiveness of the
proposed approach. It consistently yields comparable, and in several cases
superior, results to traditional fusion methods, establishing it as a robust
alternative for integrating different signal representations or modalities.

</details>


### [63] [Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline](https://arxiv.org/abs/2507.21886)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 本文提出利用呼吸信号结合高效交叉注意力变压器和多窗口策略进行疼痛评估的方法，实验表明呼吸是有价值的生理模式，紧凑模型优化后表现出色，多窗口方法可增强模型表征能力。


<details>
  <summary>Details</summary>
Motivation: 疼痛影响大量人群，准确评估对患者很重要，自动疼痛评估系统可提供持续监测和支持临床决策，减少痛苦和预防功能衰退。

Method: 引入以呼吸为输入信号的管道，结合高效交叉注意力变压器和多窗口策略。

Result: 呼吸是有价值的疼痛评估生理模式，紧凑模型优化后表现常超大型模型，多窗口方法增强模型表征能力。

Conclusion: 提出的方法可用于疼痛评估，呼吸信号和优化的紧凑模型有应用潜力。

Abstract: Pain is a complex condition affecting a large portion of the population.
Accurate and consistent evaluation is essential for individuals experiencing
pain, and it supports the development of effective and advanced management
strategies. Automatic pain assessment systems provide continuous monitoring and
support clinical decision-making, aiming to reduce distress and prevent
functional decline. This study has been submitted to the \textit{Second
Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The
proposed method introduces a pipeline that leverages respiration as the input
signal and incorporates a highly efficient cross-attention transformer
alongside a multi-windowing strategy. Extensive experiments demonstrate that
respiration is a valuable physiological modality for pain assessment. Moreover,
experiments revealed that compact and efficient models, when properly
optimized, can achieve strong performance, often surpassing larger
counterparts. The proposed multi-window approach effectively captures both
short-term and long-term features, as well as global characteristics, thereby
enhancing the model's representational capacity.

</details>


### [64] [Libra: Large Chinese-based Safeguard for AI Content](https://arxiv.org/abs/2507.21929)
*Ziyang Chen,Huimu Yu,Xing Wu,Dongqin Liu,Songlin Hu*

Main category: cs.AI

TL;DR: 提出Libra - Guard保障系统和Libra - Test基准评估系统提升中文大模型安全性，实验显示Libra - Guard表现良好，为中文大模型安全治理提供框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险应用中存在显著安全和伦理问题，需提升中文大模型安全性。

Method: 采用两阶段课程训练管道，先对合成样本进行防护预训练，再用高质量真实数据微调；引入Libra - Test基准评估系统。

Result: Libra - Guard准确率达86.79%，优于Qwen2.5 - 14B - Instruct和ShieldLM - Qwen - 14B - Chat，接近闭源模型。

Conclusion: 为中文大语言模型安全治理建立了强大框架，是迈向更安全可靠中文AI系统的初步尝试。

Abstract: Large language models (LLMs) excel in text understanding and generation but
raise significant safety and ethical concerns in high-stakes applications. To
mitigate these risks, we present Libra-Guard, a cutting-edge safeguard system
designed to enhance the safety of Chinese-based LLMs. Leveraging a two-stage
curriculum training pipeline, Libra-Guard enhances data efficiency by employing
guard pretraining on synthetic samples, followed by fine-tuning on
high-quality, real-world data, thereby significantly reducing reliance on
manual annotations. To enable rigorous safety evaluations, we also introduce
Libra-Test, the first benchmark specifically designed to evaluate the
effectiveness of safeguard systems for Chinese content. It covers seven
critical harm scenarios and includes over 5,700 samples annotated by domain
experts. Experiments show that Libra-Guard achieves 86.79% accuracy,
outperforming Qwen2.5-14B-Instruct (74.33%) and ShieldLM-Qwen-14B-Chat
(65.69%), and nearing closed-source models like Claude-3.5-Sonnet and GPT-4o.
These contributions establish a robust framework for advancing the safety
governance of Chinese LLMs and represent a tentative step toward developing
safer, more reliable Chinese AI systems.

</details>


### [65] [Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart Homes via Language Modeling of Sensor Data & Activities](https://arxiv.org/abs/2507.21964)
*Sourish Gunesh Dhekane,Thomas Ploetz*

Main category: cs.AI

TL;DR: 本文提出不依赖提示大语言模型的零样本人类活动识别方法，通过对六个数据集进行案例研究，展示语言建模对零样本识别的作用。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示大语言模型的零样本人类活动识别方法存在隐私侵犯、依赖外部服务、预测不一致等风险，需要替代方案。

Method: 使用自然语言对传感器数据和活动进行建模，利用其嵌入进行零样本分类，绕过提示大语言模型进行活动预测。

Result: 对六个数据集进行了详细的案例研究。

Conclusion: 语言建模可增强零样本识别中的人类活动识别系统。

Abstract: Developing zero-shot human activity recognition (HAR) methods is a critical
direction in smart home research -- considering its impact on making HAR
systems work across smart homes having diverse sensing modalities, layouts, and
activities of interest. The state-of-the-art solutions along this direction are
based on generating natural language descriptions of the sensor data and
feeding it via a carefully crafted prompt to the LLM to perform classification.
Despite their performance guarantees, such ``prompt-the-LLM'' approaches carry
several risks, including privacy invasion, reliance on an external service, and
inconsistent predictions due to version changes, making a case for alternative
zero-shot HAR methods that do not require prompting the LLMs. In this paper, we
propose one such solution that models sensor data and activities using natural
language, leveraging its embeddings to perform zero-shot classification and
thereby bypassing the need to prompt the LLMs for activity predictions. The
impact of our work lies in presenting a detailed case study on six datasets,
highlighting how language modeling can bolster HAR systems in zero-shot
recognition.

</details>


### [66] [Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks](https://arxiv.org/abs/2507.21974)
*Mohamed Sana,Nicola Piovesan,Antonio De Domenico,Yibin Kang,Haozhe Zhang,Merouane Debbah,Fadhel Ayed*

Main category: cs.AI

TL;DR: 提出用大语言模型进行移动网络根因分析的轻量级框架，引入TeleLogs数据集，提出两阶段训练方法，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 移动网络根因分析需可解释性、领域知识和因果推理，具有挑战性，现有开源推理大语言模型难以应对。

Method: 引入TeleLogs数据集，提出结合监督微调与强化学习的两阶段训练方法，微调一系列根因分析模型。

Result: 在多个大语言模型规模上实验，相比现有推理和非推理模型有显著性能提升，对随机测试变体有强泛化能力。

Conclusion: 领域适配、增强推理的大语言模型在网络运维的根因分析中具有应用前景。

Abstract: Root Cause Analysis (RCA) in mobile networks remains a challenging task due
to the need for interpretability, domain expertise, and causal reasoning. In
this work, we propose a lightweight framework that leverages Large Language
Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of
annotated troubleshooting problems designed to benchmark RCA capabilities. Our
evaluation reveals that existing open-source reasoning LLMs struggle with these
problems, underscoring the need for domain-specific adaptation. To address this
issue, we propose a two-stage training methodology that combines supervised
fine-tuning with reinforcement learning to improve the accuracy and reasoning
quality of LLMs. The proposed approach fine-tunes a series of RCA models to
integrate domain knowledge and generate structured, multi-step diagnostic
explanations, improving both interpretability and effectiveness. Extensive
experiments across multiple LLM sizes show significant performance gains over
state-of-the-art reasoning and non-reasoning models, including strong
generalization to randomized test variants. These results demonstrate the
promise of domain-adapted, reasoning-enhanced LLMs for practical and
explainable RCA in network operation and management.

</details>


### [67] [The Effect of Compression Techniques on Large Multimodal Language Models in the Medical Domain](https://arxiv.org/abs/2507.21976)
*Tanvir Ahmed Khan,Aranya Saha,Ismam Nur Swapnil,Mohammad Ariful Haque*

Main category: cs.AI

TL;DR: 评估结构剪枝和激活感知量化对医疗应用微调LLAVA模型的影响，提出剪枝层选择方法，实现7B参数模型在4GB VRAM运行，减少内存并提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在医疗领域有潜力，但计算成本高，需高效压缩技术。

Method: 提出剪枝层选择方法，分析不同量化技术，在剪枝 - SFT - 量化管道中评估性能权衡。

Result: 使7B参数模型在4GB VRAM运行，内存使用减少70%，相同压缩比下比传统技术性能高4%。

Conclusion: 所提方法能有效压缩多模态大语言模型，减少内存使用并提升性能。

Abstract: Multimodal Large Language Models (MLLMs) hold huge potential for usage in the
medical domain, but their computational costs necessitate efficient compression
techniques. This paper evaluates the impact of structural pruning and
activation-aware quantization on a fine-tuned LLAVA model for medical
applications. We propose a novel layer selection method for pruning, analyze
different quantization techniques, and assess the performance trade-offs in a
prune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B
parameters to run within 4 GB of VRAM, reducing memory usage by 70% while
achieving 4% higher model performance compared to traditional pruning and
quantization techniques in the same compression ratio.

</details>


### [68] [PHAX: A Structured Argumentation Framework for User-Centered Explainable AI in Public Health and Biomedical Sciences](https://arxiv.org/abs/2507.22009)
*Bahar İlgen,Akshat Dubey,Georges Hattab*

Main category: cs.AI

TL;DR: 提出PHAX框架，利用结构化论证为AI输出生成以人为本的解释，通过用例证明其适用性，促进公共卫生中透明、以人为本的AI发展。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法缺乏面向不同健康利益相关者所需的结构和适应性，需要为AI驱动的公共卫生和生物医学系统提供清晰、有背景且对社会负责的解释。

Method: 引入PHAX框架，采用多层架构，结合可废止推理、自适应自然语言技术和用户建模，利用结构化论证生成解释。

Result: 通过医疗术语简化、医患沟通和政策论证等用例证明了PHAX的适用性，可将简化决策建模为论证链并个性化。

Conclusion: PHAX通过将形式推理方法与沟通需求相结合，有助于实现公共卫生中透明、以人为本的AI愿景。

Abstract: Ensuring transparency and trust in AI-driven public health and biomedical
sciences systems requires more than accurate predictions-it demands
explanations that are clear, contextual, and socially accountable. While
explainable AI (XAI) has advanced in areas like feature attribution and model
interpretability, most methods still lack the structure and adaptability needed
for diverse health stakeholders, including clinicians, policymakers, and the
general public. We introduce PHAX-a Public Health Argumentation and
eXplainability framework-that leverages structured argumentation to generate
human-centered explanations for AI outputs. PHAX is a multi-layer architecture
combining defeasible reasoning, adaptive natural language techniques, and user
modeling to produce context-aware, audience-specific justifications. More
specifically, we show how argumentation enhances explainability by supporting
AI-driven decision-making, justifying recommendations, and enabling interactive
dialogues across user types. We demonstrate the applicability of PHAX through
use cases such as medical term simplification, patient-clinician communication,
and policy justification. In particular, we show how simplification decisions
can be modeled as argument chains and personalized based on user
expertise-enhancing both interpretability and trust. By aligning formal
reasoning methods with communicative demands, PHAX contributes to a broader
vision of transparent, human-centered AI in public health.

</details>


### [69] [UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding](https://arxiv.org/abs/2507.22025)
*Shuquan Lian,Yuhang Wu,Jia Ma,Zihan Song,Bingqi Chen,Xiawu Zheng,Hui Li*

Main category: cs.AI

TL;DR: 提出UI - AGILE框架解决现有GUI代理训练和推理问题，在两个基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理训练和推理技术存在推理设计困境、奖励无效和视觉噪声问题。

Method: 训练阶段提出对SFT过程的改进，包括连续奖励函数、简单思考奖励和基于裁剪的重采样策略；推理阶段提出分解选择接地方法。

Result: UI - AGILE在ScreenSpot - Pro和ScreenSpot - v2基准测试中达SOTA，在ScreenSpot - Pro上相比最佳基线提高23%接地准确率。

Conclusion: UI - AGILE框架有效提升了GUI代理在训练和推理阶段的性能。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has driven
significant advances in Graphical User Interface (GUI) agent capabilities.
Nevertheless, existing GUI agent training and inference techniques still suffer
from a dilemma for reasoning designs, ineffective reward, and visual noise. To
address these issues, we introduce UI-AGILE, a comprehensive framework
enhancing GUI agents at both the training and inference stages. For training,
we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:
1) a Continuous Reward function to incentivize high-precision grounding; 2) a
"Simple Thinking" reward to balance planning with speed and grounding accuracy;
and 3) a Cropping-based Resampling strategy to mitigate the sparse reward
problem and improve learning on complex tasks. For inference, we present
Decomposed Grounding with Selection, a novel method that dramatically improves
grounding accuracy on high-resolution displays by breaking the image into
smaller, manageable parts. Experiments show that UI-AGILE achieves the
state-of-the-art performance on two benchmarks ScreenSpot-Pro and
ScreenSpot-v2. For instance, using both our proposed training and inference
enhancement methods brings 23% grounding accuracy improvement over the best
baseline on ScreenSpot-Pro.

</details>


### [70] [UserBench: An Interactive Gym Environment for User-Centric Agents](https://arxiv.org/abs/2507.22034)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Zhiwei Liu,Jianguo Zhang,Haolin Chen,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: 引入以用户为中心的基准UserBench评估大语言模型代理在多轮、偏好驱动交互中的表现，发现任务完成与用户意图对齐存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代理在与用户主动协作方面，尤其是目标模糊、不断演变或间接表达时的能力未被充分探索，需评估其在此方面的表现。

Method: 引入UserBench，其中有初始目标不明确且逐步揭示偏好的模拟用户，要求代理主动澄清意图并使用工具做决策。

Result: 评估领先的开源和闭源大语言模型，发现任务完成与用户对齐存在显著差距，模型平均仅20%时间完全符合用户意图，最先进模型通过主动交互发现的用户偏好不足30%。

Conclusion: 构建不仅能执行任务，还能成为真正协作伙伴的代理存在挑战，UserBench可衡量和提升此关键能力。

Abstract: Large Language Models (LLMs)-based agents have made impressive progress in
reasoning and tool use, enabling them to solve complex tasks. However, their
ability to proactively collaborate with users, especially when goals are vague,
evolving, or indirectly expressed, remains underexplored. To address this gap,
we introduce UserBench, a user-centric benchmark designed to evaluate agents in
multi-turn, preference-driven interactions. UserBench features simulated users
who start with underspecified goals and reveal preferences incrementally,
requiring agents to proactively clarify intent and make grounded decisions with
tools. Our evaluation of leading open- and closed-source LLMs reveals a
significant disconnect between task completion and user alignment. For
instance, models provide answers that fully align with all user intents only
20% of the time on average, and even the most advanced models uncover fewer
than 30% of all user preferences through active interaction. These results
highlight the challenges of building agents that are not just capable task
executors, but true collaborative partners. UserBench offers an interactive
environment to measure and advance this critical capability.

</details>


### [71] [The Interspeech 2025 Speech Accessibility Project Challenge](https://arxiv.org/abs/2507.22047)
*Xiuwen Zheng,Bornali Phukon,Jonghwan Na,Ed Cutrell,Kyu Han,Mark Hasegawa-Johnson,Pan-Pan Jiang,Aadhrik Kuila,Colin Lea,Bob MacDonald,Gautam Mantena,Venkatesh Ravichandran,Leda Sari,Katrin Tomanek,Chang D. Yoo,Chris Zwilling*

Main category: cs.AI

TL;DR: 2025 Interspeech SAP Challenge利用超400小时特殊语音数据评估ASR系统，部分团队超基线，顶尖团队创新基准。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别系统对言语障碍者表现不佳，公共训练数据有限。

Method: 举办2025 Interspeech SAP Challenge，使用超400小时来自超500言语障碍者的数据，在EvalAI上利用远程评估管道，基于WER和SemScore评估。

Result: 22支有效团队中12支WER超基线，17支SemScore超基线，顶尖团队WER达8.11%、SemScore达88.44%。

Conclusion: 顶尖团队为未来ASR系统识别障碍语音设定新基准。

Abstract: While the last decade has witnessed significant advancements in Automatic
Speech Recognition (ASR) systems, performance of these systems for individuals
with speech disabilities remains inadequate, partly due to limited public
training data. To bridge this gap, the 2025 Interspeech Speech Accessibility
Project (SAP) Challenge was launched, utilizing over 400 hours of SAP data
collected and transcribed from more than 500 individuals with diverse speech
disabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline,
the SAP Challenge evaluates submissions based on Word Error Rate and Semantic
Score. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2
baseline in terms of WER, while 17 teams surpassed the baseline on SemScore.
Notably, the top team achieved the lowest WER of 8.11\%, and the highest
SemScore of 88.44\% at the same time, setting new benchmarks for future ASR
systems in recognizing impaired speech.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [72] [Improving Neural Network Training using Dynamic Learning Rate Schedule for PINNs and Image Classification](https://arxiv.org/abs/2507.21749)
*D. Veerababu,Ashwin A. Raikar,Prasanta K. Ghosh*

Main category: cs.CE

TL;DR: 本文提出动态学习率调度器（DLRS）算法，在物理信息神经网络和图像分类问题实验中显示其能加速训练并提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 训练神经网络有挑战，尤其是复杂问题，学习率作为关键超参数通常静态，复杂系统学习动态需要更自适应的学习率。

Method: 提出基于训练过程中损失值来调整学习率的动态学习率调度器（DLRS）算法，并在物理信息神经网络和图像分类问题上进行实验。

Result: 提出的DLRS能加速训练并提高稳定性。

Conclusion: 动态学习率调度器（DLRS）算法在训练神经网络时具有加速训练和提高稳定性的优势。

Abstract: Training neural networks can be challenging, especially as the complexity of
the problem increases. Despite using wider or deeper networks, training them
can be a tedious process, especially if a wrong choice of the hyperparameter is
made. The learning rate is one of such crucial hyperparameters, which is
usually kept static during the training process. Learning dynamics in complex
systems often requires a more adaptive approach to the learning rate. This
adaptability becomes crucial to effectively navigate varying gradients and
optimize the learning process during the training process. In this paper, a
dynamic learning rate scheduler (DLRS) algorithm is presented that adapts the
learning rate based on the loss values calculated during the training process.
Experiments are conducted on problems related to physics-informed neural
networks (PINNs) and image classification using multilayer perceptrons and
convolutional neural networks, respectively. The results demonstrate that the
proposed DLRS accelerates training and improves stability.

</details>


### [73] [ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical Knowledge](https://arxiv.org/abs/2507.21990)
*Zihan Zhao,Bo Chen,Ziping Wan,Lu Chen,Xuanze Lin,Shiyang Yu,Situo Zhang,Da Ma,Zichen Zhu,Danyang Zhang,Huayang Wang,Zhongyang Dai,Liyang Wen,Xin Chen,Kai Yu*

Main category: cs.CE

TL;DR: 本文聚焦化学领域，开发化学推理大模型ChemDFM - R，实验显示其达最优性能且输出可解释。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在化学等科学领域应用受限于浅层领域理解和有限推理能力。

Method: 构建原子化知识点综合数据集，提出混合源蒸馏策略整合专业知识与通用推理技能，进行特定领域强化学习。

Result: 在多种化学基准测试中，ChemDFM - R达最优性能，输出可解释。

Conclusion: 明确推理链能显著提高模型在现实人机协作场景中的可靠性、透明度和实用性。

Abstract: While large language models (LLMs) have achieved impressive progress, their
application in scientific domains such as chemistry remains hindered by shallow
domain understanding and limited reasoning capabilities. In this work, we focus
on the specific field of chemistry and develop a Chemical Reasoner LLM,
ChemDFM-R. We first construct a comprehensive dataset of atomized knowledge
points to enhance the model's understanding of the fundamental principles and
logical structure of chemistry. Then, we propose a mix-sourced distillation
strategy that integrates expert-curated knowledge with general-domain reasoning
skills, followed by domain-specific reinforcement learning to enhance chemical
reasoning. Experiments on diverse chemical benchmarks demonstrate that
ChemDFM-R achieves state-of-the-art performance while providing interpretable,
rationale-driven outputs. Further case studies illustrate how explicit
reasoning chains significantly improve the reliability, transparency, and
practical utility of the model in real-world human-AI collaboration scenarios.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [74] [AI-Driven Generation of Data Contracts in Modern Data Engineering Systems](https://arxiv.org/abs/2507.21056)
*Harshraj Bhoite*

Main category: cs.DB

TL;DR: 提出用大语言模型自动生成数据合约框架，集成到数据平台，实验显示可降人力超70%，探讨挑战并证明生成式AI能助力数据治理


<details>
  <summary>Details</summary>
Motivation: 数据管道复杂度增加，手动编写和维护数据合约易出错且耗力

Method: 利用参数高效微调方法（LoRA和PEFT）让大语言模型适应结构化数据领域，以样本数据或模式描述生成合约定义并集成到数据平台

Result: 微调后的大语言模型生成有效合约准确率高，减少超70%手动工作量

Conclusion: 生成式AI可通过弥合企业数据管理中意图与实施差距，实现可扩展、敏捷的数据治理

Abstract: Data contracts formalize agreements between data producers and consumers
regarding schema, semantics, and quality expectations. As data pipelines grow
in complexity, manual authoring and maintenance of contracts becomes
error-prone and labor-intensive. We present an AI-driven framework for
automatic data contract generation using large language models (LLMs). Our
system leverages parameter-efficient fine-tuning methods, including LoRA and
PEFT, to adapt LLMs to structured data domains. The models take sample data or
schema descriptions and output validated contract definitions in formats such
as JSON Schema and Avro. We integrate this framework into modern data platforms
(e.g., Databricks, Snowflake) to automate contract enforcement at scale.
Experimental results on synthetic and real-world datasets demonstrate that the
fine-tuned LLMs achieve high accuracy in generating valid contracts and reduce
manual workload by over 70%. We also discuss key challenges such as
hallucination, version control, and the need for continuous learning. This work
demonstrates that generative AI can enable scalable, agile data governance by
bridging the gap between intent and implementation in enterprise data
management.

</details>


### [75] [Benchmarking Filtered Approximate Nearest Neighbor Search Algorithms on Transformer-based Embedding Vectors](https://arxiv.org/abs/2507.21989)
*Patrick Iff,Paul Bruegger,Marcin Chrapek,Maciej Besta,Torsten Hoefler*

Main category: cs.DB

TL;DR: 对FANNS方法进行全面调查和分类，引入新数据集进行基准测试，发现无通用最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有FANNS缺乏多样化和现实的数据集，尤其是基于最新变压器文本嵌入模型的数据集。

Method: 对FANNS方法进行调查和分类，引入arXiv上270多万研究论文摘要嵌入向量的新数据集，对多种FANNS方法进行基准测试。

Result: 每种FANNS方法都有优缺点，无单一方法在所有场景表现最佳。如ACORN支持多种过滤类型但常被专业方法超越；SeRF在有序属性范围过滤表现好但无法处理分类属性；Filtered - DiskANN和UNG在中规模数据集表现好但在大规模数据集失败。

Conclusion: 不存在通用的最佳FANNS方法。

Abstract: Advances in embedding models for text, image, audio, and video drive progress
across multiple domains, including retrieval-augmented generation,
recommendation systems, vehicle/person reidentification, and face recognition.
Many applications in these domains require an efficient method to retrieve
items that are close to a given query in the embedding space while satisfying a
filter condition based on the item's attributes, a problem known as Filtered
Approximate Nearest Neighbor Search (FANNS). In this work, we present a
comprehensive survey and taxonomy of FANNS methods and analyze how they are
benchmarked in the literature. By doing so, we identify a key challenge in the
current FANNS landscape: the lack of diverse and realistic datasets,
particularly ones derived from the latest transformer-based text embedding
models. To address this, we introduce a novel dataset consisting of embedding
vectors for the abstracts of over 2.7 million research articles from the arXiv
repository, accompanied by 11 real-world attributes such as authors and
categories. We benchmark a wide range of FANNS methods on our novel dataset and
find that each method has distinct strengths and limitations; no single
approach performs best across all scenarios. ACORN, for example, supports
various filter types and performs reliably across dataset scales but is often
outperformed by more specialized methods. SeRF shows excellent performance for
range filtering on ordered attributes but cannot handle categorical attributes.
Filtered-DiskANN and UNG excel on the medium-scale dataset but fail on the
large-scale dataset, highlighting the challenge posed by transformer-based
embeddings, which are often more than an order of magnitude larger than earlier
embeddings. We conclude that no universally best method exists.

</details>


### [76] [Digitalizing Uncertain Information](https://arxiv.org/abs/2507.21173)
*Chris Partridge,Andrew Mitchell,Andreas Cola*

Main category: cs.DB

TL;DR: 本文介绍开发基于本体的数字形式以表示不确定信息的初步成果，探讨跨越技术鸿沟提升数字成熟度的方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种基于本体的数字形式来表示不确定信息，解决处理数字不确定性的相关挑战。

Method: 先描述处理数字不确定性项目面临的基本挑战，然后用刘易斯对应体方法扩展外延本体（如BORO基础本体或信息交换标准）来形式化不确定性。

Result: 所采用的方法具有足够的表达能力来应对相关挑战。

Conclusion: 通过扩展外延本体和采用刘易斯对应体方法能有效处理数字不确定性问题。

Abstract: The paper sketches some initial results from an ongoing project to develop an
ontology-based digital form for representing uncertain information. We frame
this work as a journey from lower to higher levels of digital maturity across a
technology divide. The paper first sets a baseline by describing the basic
challenges any project dealing with digital uncertainty faces. It then
describes how the project is facing them. It shows firstly how an extensional
ontology (such as the BORO Foundational Ontology or the Information Exchange
Standard) can be extended with a Lewisian counterpart approach to formalizing
uncertainty that is adapted to computing. And then it shows how this is
expressive enough to handle the challenges. Keywords: actuality, BORO
Foundational Ontology, counterpart, Information Exchange Standard,
informational uncertainty, my doxastic actualities, two-dimensional semantics.

</details>


### [77] [Ranking Methods for Skyline Queries](https://arxiv.org/abs/2507.21860)
*Mickaël Martin-Nevot,Lotfi Lakhal*

Main category: cs.DB

TL;DR: 本文提出改进dp - idp方法，引入RankSky和CoSky方法，结合多层Skyline提出DeepSky，最后对方法进行实验评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法对高基数结果集的Pareto最优或Skyline点比较不足，且dp - idp缺乏效率和不能保证独特排名。

Method: 引入优势层次概念改进dp - idp；采用类似PageRank的方法提出RankSky；基于TOPSIS建立CoSky；结合多层Skyline提出DeepSky。

Result: 对dp - idp、RankSky和CoSky进行了实验评估。

Conclusion: 未明确提及，推测所提方法在处理高基数结果集的Skyline点比较问题上有一定效果。

Abstract: {Multi-criteria decision analysis in databases has been actively studied,
especially through the Skyline operator. Yet, few approaches offer a relevant
comparison of Pareto optimal, or Skyline, points for high cardinality result
sets. We propose to improve the dp-idp method, inspired by tf-idf, a recent
approach computing a score for each Skyline point, by introducing the concept
of dominance hierarchy. As dp-idp lacks efficiency and does not ensure a
distinctive rank, we introduce the RankSky method, the adaptation of Google's
well-known PageRank solution, using a square stochastic matrix, a teleportation
matrix, a damping factor, and then a row score eigenvector and the IPL
algorithm. For the same reasons as RankSky, and also to offer directly
embeddable in DBMS solution, we establish the TOPSIS based CoSky method,
derived from both information research and multi-criteria analysis. CoSky
automatically ponderates normalized attributes using the Gini index, then
computes a score using Salton's cosine toward an ideal point. By coupling
multilevel Skyline to dp-idp, RankSky or CoSky, we introduce DeepSky.
Implementations of dp-idp, RankSky and CoSky are evaluated experimentally.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [78] [Improving SpGEMM Performance Through Matrix Reordering and Cluster-wise Computation](https://arxiv.org/abs/2507.21253)
*Abdullah Al Raqibul Islam,Helen Xu,Dong Dai,Aydın Buluç*

Main category: cs.DC

TL;DR: 本文提出用于SpGEMM的分层聚类方法，可提升运算速度，还研究不同重排序和聚类方案对性能的影响，该方法平均加速效果好。


<details>
  <summary>Details</summary>
Motivation: SpGEMM因不规则内存访问模式受数据移动瓶颈限制，此前行重排序方案多针对SpMV，本文要解决SpGEMM的这些问题。

Method: 采用分层聚类，结合行重排序和逐簇计算，使用新的行聚类矩阵格式和访问模式，还对10种重排序算法和3种聚类方案进行综合实证研究。

Result: 分层聚类平均可将SpGEMM加速1.39倍，预处理成本低；基于图划分的重排序性能好但预处理时间长；提出的分层聚类方法在相似预处理时间下平均加速效果更好。

Conclusion: 所提分层聚类方法能有效提升SpGEMM性能，且可将重排序算法与聚类矩阵格式独立应用优化。

Abstract: Sparse matrix-sparse matrix multiplication (SpGEMM) is a key kernel in many
scientific applications and graph workloads. Unfortunately, SpGEMM is
bottlenecked by data movement due to its irregular memory access patterns.
Significant work has been devoted to developing row reordering schemes towards
improving locality in sparse operations, but prior studies mostly focus on the
case of sparse-matrix vector multiplication (SpMV).
  In this paper, we address these issues with hierarchical clustering for
SpGEMM that leverages both row reordering and cluster-wise computation to
improve reuse in the second input (B) matrix with a novel row-clustered matrix
format and access pattern in the first input (A) matrix. We find that
hierarchical clustering can speed up SpGEMM by 1.39x on average with low
preprocessing cost (less than 20x the cost of a single SpGEMM on about 90% of
inputs). Furthermore, we decouple the reordering algorithm from the clustered
matrix format so they can be applied as independent optimizations.
  Additionally, this paper sheds light on the role of both row reordering and
clustering independently and together for SpGEMM with a comprehensive empirical
study of the effect of 10 different reordering algorithms and 3 clustering
schemes on SpGEMM performance on a suite of 110 matrices. We find that
reordering based on graph partitioning provides better SpGEMM performance than
existing alternatives at the cost of high preprocessing time. The evaluation
demonstrates that the proposed hierarchical clustering method achieves greater
average speedup compared to other reordering schemes with similar preprocessing
times.

</details>


### [79] [Using Containers to Speed Up Development, to Run Integration Tests and to Teach About Distributed Systems](https://arxiv.org/abs/2507.21464)
*Marco Mambelli,Bruno Moreira Coimbra,Namratha Urs,Ilya Baburashvili*

Main category: cs.DC

TL;DR: 本文介绍GlideinWMS工作区，包括其与其他容器的区别、基础系统构成、容器运行时评估，以及开发和教学中的应用。


<details>
  <summary>Details</summary>
Motivation: 为简化GlideinWMS的开发和测试，借鉴其他社区方法构建工作区。

Method: 构建由三个容器组成的基础系统，评估不同容器运行时，将工作区与IDE集成。

Result: 系统可在笔记本电脑运行，工作区便于集成IDE，简化调试，方便新成员培训和学生实践。

Conclusion: GlideinWMS工作区对开发、培训和教学有积极作用。

Abstract: GlideinWMS is a workload manager provisioning resources for many experiments,
including CMS and DUNE. The software is distributed both as native packages and
specialized production containers. Following an approach used in other
communities like web development, we built our workspaces, system-like
containers to ease development and testing. Developers can change the source
tree or check out a different branch and quickly reconfigure the services to
see the effect of their changes. In this paper, we will talk about what
differentiates workspaces from other containers. We will describe our base
system, composed of three containers: a one-node cluster including a compute
element and a batch system, a GlideinWMS Factory controlling pilot jobs, and a
scheduler and Frontend to submit jobs and provision resources. Additional
containers can be used for optional components. This system can easily run on a
laptop, and we will share our evaluation of different container runtimes, with
an eye for ease of use and performance. Finally, we will talk about our
experience as developers and with students. The GlideinWMS workspaces are
easily integrated with IDEs like VS Code, simplifying debugging and allowing
development and testing of the system even when offline. They simplified the
training and onboarding of new team members and summer interns. And they were
useful in workshops where students could have first-hand experience with the
mechanisms and components that, in production, run millions of jobs.

</details>


### [80] [GlideinBenchmark: collecting resource information to optimize provisioning](https://arxiv.org/abs/2507.21472)
*Marco Mambelli,Shrijan Swaminathan*

Main category: cs.DC

TL;DR: 提出GlideinBenchmark应用来利用GlideinWMS基准测试资源，自动化选择最优资源并优化资源供应。


<details>
  <summary>Details</summary>
Motivation: 早期HEPCloud研究虽表明选对资源有诸多好处，但资源基准测试耗时费力，需改进。

Method: 开发GlideinBenchmark Web应用，借助GlideinWMS的试验基础设施进行基准测试，控制基准执行，可按需选择基准评估工作流性能。

Result: 可收集和发布数据用于自动化选择最优资源。

Conclusion: 调度器如HEPCloud的决策引擎可利用结果优化资源供应。

Abstract: Choosing the right resource can speed up job completion, better utilize the
available hardware, and visibly reduce costs, especially when renting computers
in the cloud. This was demonstrated in earlier studies on HEPCloud. However,
the benchmarking of the resources proved to be a laborious and time-consuming
process. This paper presents GlideinBenchmark, a new Web application leveraging
the pilot infrastructure of GlideinWMS to benchmark resources, and it shows how
to use the data collected and published by GlideinBenchmark to automate the
optimal selection of resources. An experiment can select the benchmark or the
set of benchmarks that most closely evaluate the performance of its workflows.
GlideinBenchmark, with the help of the GlideinWMS Factory, controls the
benchmark execution. Finally, a scheduler like HEPCloud's Decision Engine can
use the results to optimize resource provisioning.

</details>


### [81] [Bridging Cache-Friendliness and Concurrency: A Locality-Optimized In-Memory B-Skiplist](https://arxiv.org/abs/2507.21492)
*Yicong Luo,Senhe Hao,Brian Wheatman,Prashant Pandey,Helen Xu*

Main category: cs.DC

TL;DR: 提出并发B - skiplist，增强缓存局部性与性能，在吞吐量和延迟上表现优于现有实现。


<details>
  <summary>Details</summary>
Motivation: 传统跳表缓存局部性差影响性能，需提高缓存局部性来提升内存索引性能。

Method: 提出B - skiplist的自上而下单遍插入算法和相应并发控制方案。

Result: 在128线程上，吞吐量比现有并发跳表实现高2x - 9x；在点工作负载上，与缓存优化树索引吞吐量相当；在点工作负载插入操作中，99%延迟比其他并发跳表低3.5x - 103x，比树索引低0.85x - 64x。

Conclusion: 并发B - skiplist在保持传统跳表结构和并发控制方案简单性的同时，提升了缓存局部性和性能。

Abstract: Skiplists are widely used for in-memory indexing in many key-value stores,
such as RocksDB and LevelDB, due to their ease of implementation and simple
concurrency control mechanisms. However, traditional skiplists suffer from poor
cache locality, as they store only a single element per node, leaving
performance on the table. Minimizing last-level cache misses is key to
maximizing in-memory index performance, making high cache locality essential.
In this paper, we present a practical concurrent B-skiplist that enhances cache
locality and performance while preserving the simplicity of traditional
skiplist structures and concurrency control schemes. Our key contributions
include a top-down, single-pass insertion algorithm for B-skiplists and a
corresponding simple and efficient top-down concurrency control scheme. On 128
threads, the proposed concurrent B-skiplist achieves between 2x-9x higher
throughput compared to state-of-the-art concurrent skiplist implementations,
including Facebook's concurrent skiplist from Folly and the Java
ConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves
competitive (0.9x-1.7x) throughput on point workloads compared to
state-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a
more complete picture of the performance, we also measure the latency of
skiplist and tree-based indices and find that the B-skiplist achieves between
3.5x-103x lower 99% latency compared to other concurrent skiplists and between
0.85x-64x lower 99% latency compared to tree-based indices on point workloads
with inserts.

</details>


### [82] [Collaborative State Machines: A Better Programming Model for the Cloud-Edge-IoT Continuum](https://arxiv.org/abs/2507.21685)
*Marlon Etheredge,Thomas Fahringer,Felix Erlacher,Elias Kohler,Stefan Pedratscher,Juan Aznar-Poveda,Nishant Saurabh,Adrien Lebre*

Main category: cs.DC

TL;DR: 本文提出协作状态机（CSM）编程模型，介绍运行时系统并评估，结果显示吞吐量和处理时间等方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有编程模型难以有效管理云 - 边 - 物联网应用的动态和有状态特性，需要新模型。

Method: 引入CSM编程模型，介绍其协作机制、状态封装等特性，通过运行时系统并基于三个用例在Grid'5000测试平台评估。

Result: 压力测试吞吐量提升12倍，监控系统处理时间提升2.3倍，智能工厂处理时间减少55倍，整体生产力提高。

Conclusion: CSM编程模型能有效应对云 - 边 - 物联网应用开发的复杂性，在性能和生产力上有明显优势。

Abstract: The development of Cloud-Edge-IoT applications requires robust programming
models. Existing models often struggle to manage the dynamic and stateful
nature of these applications effectively. This paper introduces the
Collaborative State Machines (CSM) programming model to address these
complexities. CSM facilitates the development of reactive, event-driven, and
stateful applications targeting the Cloud-Edge-IoT continuum. Applications
built with CSM are composed of state machines that collaborate autonomously and
can be distributed across different layers of the continuum. Key features of
CSM include (i) a sophisticated collaboration mechanism among state machines
utilizing events and persistent data; (ii) encapsulation of state through the
inherent state of state machines and persistent data; (iii) integration of
actions and service invocations within states and state transitions, thereby
decoupling complex application logic from compute and data processing services;
and (iv) an advanced data model that supports the processing of local, static,
and persistent data with defined scope and lifetime. In addition to introducing
the CSM programming model, we present a runtime system and a comprehensive
evaluation of our approach. This evaluation is based on three use cases: a
stress test on a large-scale infrastructure, a surveillance system application,
and a complex smart factory scenario, all deployed on the Grid'5000 testbed.
Our results demonstrate a 12x increase in throughput through novel language
features in the stress test. Compared to Serverless Workflow, a
state-of-the-art baseline system, we show a 2.3x improvement in processing time
per processed image in a surveillance system use case, a 55x reduction in total
processing time for a smart factory use case, and an overall improvement in
productivity across these use cases.

</details>


### [83] [The Performance of Low-Synchronization Variants of Reorthogonalized Block Classical Gram--Schmidt](https://arxiv.org/abs/2507.21791)
*Erin Carson,Yuxin Ma*

Main category: cs.DC

TL;DR: 评估分布式内存系统上BCGSI+P - 1S和BCGSI+P - 2S的性能，与其他低同步BCGS变体对比，推荐其用于经济QR分解。


<details>
  <summary>Details</summary>
Motivation: 分布式内存大规模问题中，BCGS算法通信成本高，此前提出低同步变体，本文评估新变体性能。

Method: 在分布式内存系统上对BCGSI+P - 1S和BCGSI+P - 2S进行数值实验，并与其他低同步BCGS变体对比。

Result: 与经典算法相比，BCGSI+P - 1S和BCGSI+P - 2S分别可达4倍和2倍加速，与其他同同步次数变体性能相似。

Conclusion: 由于稳定性好，BCGSI+P - 1S和BCGSI+P - 2S是分布式内存系统经济QR分解的最佳选择。

Abstract: Numerous applications, such as Krylov subspace solvers, make extensive use of
the block classical Gram-Schmidt (BCGS) algorithm and its reorthogonalized
variants for orthogonalizing a set of vectors. For large-scale problems in
distributed memory settings, the communication cost, particularly the global
synchronization cost, is a major performance bottleneck. In recent years, many
low-synchronization BCGS variants have been proposed in an effort to reduce the
number of synchronization points. The work [E. Carson, Y. Ma, arXiv preprint
2411.07077] recently proposed stable one-synchronization and
two-synchronization variants of BCGS, i.e., BCGSI+P-1S and BCGSI+P-2S. In this
work, we evaluate the performance of BCGSI+P-1S and BCGSI+P-2S on a distributed
memory system compared to other well-known low-synchronization BCGS variants.
In comparison to the classical reorthogonalized BCGS algorithm (BCGSI+),
numerical experiments demonstrate that BCGSI+P-1S and BCGSI+P-2S can achieve up
to 4 times and 2 times speedups, respectively, and perform similarly to other
(less stable) one-synchronization and two-synchronization variants. BCGSI+P-1S
and BCGSI+P-2S are therefore recommended as the best choice in practice for
computing an economic QR factorization on distributed memory systems due to
their superior stability when compared to other variants with the same
synchronization cost.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [84] [Parallel PLL on DAGs](https://arxiv.org/abs/2507.21204)
*Patrick Steil*

Main category: cs.DS

TL;DR: 提出一种针对有向无环图（DAG）上枢纽标签预处理优化的并行剪枝地标标记（PLL）变体。


<details>
  <summary>Details</summary>
Motivation: 在卡尔斯鲁厄理工学院研讨会上，针对模拟公共交通网络的时间扩展图，开发优化枢纽标签预处理方法。

Method: 利用DAG的拓扑性质实现一种新颖的并行构建枢纽标签的方法。

Result: 未提及

Conclusion: 未提及

Abstract: We present a parallel variant of Pruned Landmark Labelling (PLL) that is
optimised for the preprocessing of hub labels on directed acyclic graphs
(DAGs). This method was developed during a seminar at the Karlsruhe Institute
of Technology (KIT), focusing on time-expanded graphs that model public
transport networks. The approach leverages the topological properties of DAGs
to enable a novel parallel construction of hub labels.

</details>


### [85] [Structural Parameters for Steiner Orientation](https://arxiv.org/abs/2507.21445)
*Tesshu Hanaka,Michael Lampis,Nikolaos Melissinos,Edouard Nemery,Hirotaka Ono,Manolis Vasilakis*

Main category: cs.DS

TL;DR: 本文研究Steiner Orientation问题，从结构参数化复杂性角度探讨其复杂度，给出该问题在不同参数下的复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 从结构参数化复杂性角度研究Steiner Orientation问题的复杂度。

Method: 分析问题在不同参数下的复杂度，给出NP - 完全性证明、设计XP算法和FPT算法等。

Result: 证明该问题在特定图结构下NP - 完全；给出参数为顶点覆盖数的XP算法并证明其最优性；分析不同边数参数下算法的最优性，给出相应算法；还考虑了其他参数化下的复杂度。

Conclusion: Steiner Orientation问题从结构参数化复杂性角度看是一个很困难的问题。

Abstract: We consider the \textsc{Steiner Orientation} problem, where we are given as
input a mixed graph $G=(V,E,A)$ and a set of $k$ demand pairs $(s_i,t_i)$,
$i\in[k]$. The goal is to orient the undirected edges of $G$ in a way that the
resulting directed graph has a directed path from $s_i$ to $t_i$ for all
$i\in[k]$. We adopt the point of view of structural parameterized complexity
and investigate the complexity of \textsc{Steiner Orientation} for standard
measures, such as treewidth. Our results indicate that \textsc{Steiner
Orientation} is a surprisingly hard problem from this point of view. In
particular, our main contributions are the following: (1) We show that
\textsc{Steiner Orientation} is NP-complete on instances where the underlying
graph has feedback vertex number 2, treewidth 2, pathwidth 3, and vertex
integrity 6; (2) We present an XP algorithm parameterized by vertex cover
number $\mathrm{vc}$ of complexity $n^{\mathcal{O}(\mathrm{vc}^2)}$.
Furthermore, we show that this running time is essentially optimal by proving
that a running time of $n^{o(\mathrm{vc}^2)}$ would refute the ETH; (3) We
consider parameterizations by the number of undirected or directed edges ($|E|$
or $|A|$) and we observe that the trivial $2^{|E|}n^{\mathcal{O}(1)}$-time
algorithm for the former parameter is optimal under the SETH. Complementing
this, we show that the problem admits a
$2^{\mathcal{O}(|A|)}n^{\mathcal{O}(1)}$-time algorithm. In addition to the
above, we consider the complexity of \textsc{Steiner Orientation} parameterized
by $\mathrm{tw}+k$ (FPT), distance to clique (FPT), and $\mathrm{vc}+k$ (FPT
with a polynomial kernel).

</details>


### [86] [Online Edge Coloring: Sharp Thresholds](https://arxiv.org/abs/2507.21560)
*Joakim Blikstad,Ola Svensson,Radu Vintan,David Wajc*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vizing's theorem guarantees that every graph with maximum degree $\Delta$
admits an edge coloring using $\Delta + 1$ colors. In online settings - where
edges arrive one at a time and must be colored immediately - a simple greedy
algorithm uses at most $2\Delta - 1$ colors. Over thirty years ago, Bar-Noy,
Motwani, and Naor [IPL'92] proved that this guarantee is optimal among
deterministic algorithms when $\Delta = O(\log n)$, and among randomized
algorithms when $\Delta = O(\sqrt{\log n})$. While deterministic improvements
seemed out of reach, they conjectured that for graphs with $\Delta =
\omega(\log n)$, randomized algorithms can achieve $(1 + o(1))\Delta$ edge
coloring. This conjecture was recently resolved in the affirmative: a $(1 +
o(1))\Delta$-coloring is achievable online using randomization for all graphs
with $\Delta = \omega(\log n)$ [BSVW STOC'24].
  Our results go further, uncovering two findings not predicted by the original
conjecture. First, we give a deterministic online algorithm achieving $(1 +
o(1))\Delta$-colorings for all $\Delta = \omega(\log n)$. Second, we give a
randomized algorithm achieving $(1 + o(1))\Delta$-colorings already when
$\Delta = \omega(\sqrt{\log n})$. Our results establish sharp thresholds for
when greedy can be surpassed, and near-optimal guarantees can be achieved -
matching the impossibility results of [BNMN IPL'92], both deterministically and
randomly.

</details>


### [87] [Towards Tight Bounds for Estimating Degree Distribution in Streaming and Query Models](https://arxiv.org/abs/2507.21784)
*Arijit Bishnu,Debarshi Chanda,Gopinath Mishra*

Main category: cs.DS

TL;DR: 本文研究图的互补累积度直方图（ccdh）近似问题，设计算法并在流式和查询模型中分析复杂度。


<details>
  <summary>Details</summary>
Motivation: 图的度分布重要，ccdh是重要衍生分布，其近似问题复杂度未知，在WOLA 2019被列为开放问题。

Method: 先设计与亚线性模型无关的ccdh近似算法，再在流式和查询模型中高效获取所需样本，最后建立问题的下界。

Result: 设计了近似ccdh的算法，在流式和查询模型中可高效获取样本，建立了两种模型下的下界。

Conclusion: （几乎）解决了流式和查询亚线性模型下ccdh近似问题的复杂度。

Abstract: The degree distribution of a graph $G=(V,E)$, $|V|=n$, $|E|=m$ is one of the
most fundamental objects of study in the analysis of graphs as it embodies
relationship among entities. In particular, an important derived distribution
from degree distribution is the complementary cumulative degree histogram
(ccdh). The ccdh is a fundamental summary of graph structure, capturing, for
each threshold $d$, the number of vertices with degree at least $d$. For
approximating ccdh, we consider the $(\varepsilon_D,\varepsilon_R)$-BiCriteria
Multiplicative Approximation, which allows for controlled multiplicative slack
in both the domain and the range. The exact complexity of the problem was not
known and had been posed as an open problem in WOLA 2019 [Sublinear.info,
Problem 98].
  In this work, we first design an algorithm that can approximate ccdh if a
suitable vertex sample and an edge sample can be obtained and thus, the
algorithm is independent of any sublinear model. Next, we show that in the
streaming and query models, these samples can be obtained efficiently. On the
other end, we establish the first lower bounds for this problem in both query
and streaming models, and (almost) settle the complexity of the problem across
both the sublinear models.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [88] [Non-coercive extortion in game theory](https://arxiv.org/abs/2507.21795)
*Maria Alejandra Ramirez,Rosemarie Nagel,David Wolpert,Jürgen Jost*

Main category: cs.GT

TL;DR: 研究引入非强制勒索机制，利用对结果依赖支付的承诺在博弈中获利，揭示竞争环境战略漏洞。


<details>
  <summary>Details</summary>
Motivation: 多数研究聚焦用承诺实现有效均衡，其其他潜在应用待探索，故研究非强制勒索机制。

Method: 在同时行动博弈中引入顺序性，推导勒索成功条件，确定易受影响的博弈类型及最大获利和最小支付。

Result: 揭示了竞争环境中的战略漏洞，对经济市场、外交关系和区块链多智能体系统有重要影响。

Conclusion: 拓宽了博弈论中承诺的理解，提出防范非强制勒索的关键问题。

Abstract: Commitments play a crucial role in game theory, shaping strategic
interactions by either altering a player's own payoffs or influencing the
incentives of others through outcome-contingent payments. While most research
has focused on using commitments to achieve efficient equilibria, their
potential applications beyond this goal remain largely unexplored. In this
study, we introduce a non-coercive extortion mechanism that leverages
commitments to outcome-contingent payments, demonstrating how a player or
external agent can extract profit by offering rewards rather than threatening
punishment. At the core of the mechanism is the introduction of sequentiality
into a simultaneous-move game, fundamentally reshaping the strategic
interaction. We derive the conditions under which extortion is successful,
identify the class of games susceptible to this scheme, and determine both the
maximum extractable profit and the minimum required payment. To illustrate the
extortion mechanism, we apply it to 2x2 games, highlighting how even simple
strategic settings can be vulnerable to this form of manipulation. Our results
reveal strategic vulnerabilities in competitive settings, with significant
implications for economic markets, diplomatic relations, and multi-agent
systems operating in blockchain environments. This work broadens our
understanding of commitments in game theory and raises critical questions about
how to safeguard strategic interactions from exploitation through non-coercive
extortion.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [89] [Analise Semantica Automatizada com LLM e RAG para Bulas Farmaceuticas](https://arxiv.org/abs/2507.21103)
*Daniel Meireles do Rego*

Main category: cs.IR

TL;DR: 研究用RAG架构结合大语言模型自动化分析PDF文档，实验表明该组合在非结构化技术文本信息检索和解读上有显著收益。


<details>
  <summary>Details</summary>
Motivation: 数字文档生产快速增长，在非结构化信息高效提取和分析方面带来新挑战。

Method: 采用RAG架构结合大语言模型，集成嵌入向量搜索技术、语义数据提取和生成上下文自然语言响应，并对药品说明书进行实验，用准确率、完整性等指标评估语义查询。

Result: RAG与大语言模型的结合在智能信息检索和非结构化技术文本解读方面有显著收益。

Conclusion: RAG和大语言模型的组合能有效提高非结构化技术文本的信息检索和解读能力。

Abstract: The production of digital documents has been growing rapidly in academic,
business, and health environments, presenting new challenges in the efficient
extraction and analysis of unstructured information. This work investigates the
use of RAG (Retrieval-Augmented Generation) architectures combined with
Large-Scale Language Models (LLMs) to automate the analysis of documents in PDF
format. The proposal integrates vector search techniques by embeddings,
semantic data extraction and generation of contextualized natural language
responses. To validate the approach, we conducted experiments with drug package
inserts extracted from official public sources. The semantic queries applied
were evaluated by metrics such as accuracy, completeness, response speed and
consistency. The results indicate that the combination of RAG with LLMs offers
significant gains in intelligent information retrieval and interpretation of
unstructured technical texts.

</details>


### [90] [AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis](https://arxiv.org/abs/2507.21105)
*Callie C. Liao,Duoduo Liao,Sai Surya Gadiraju*

Main category: cs.IR

TL;DR: 提出新型多协议多智能体系统框架AgentMaster，评估显示其有良好性能，助力领域特定、协作和可扩展的对话式AI。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统在通信、协调及与异构工具资源交互方面存在挑战，且很少有框架同时使用MCP和A2A协议。

Method: 构建具有自实现A2A和MCP的模块化多协议多智能体系统框架AgentMaster，通过统一对话界面支持自然语言交互和多模态查询。

Result: 通过BERTScore F1和G - Eval评估，平均值分别达96.3%和87.1%，展现出强大的智能体间协调等能力。

Conclusion: 所提出的框架有助于领域特定、协作和可扩展的对话式AI发展。

Abstract: The rise of Multi-Agent Systems (MAS) in Artificial Intelligence (AI),
especially integrated with Large Language Models (LLMs), has greatly
facilitated the resolution of complex tasks. However, current systems are still
facing challenges of inter-agent communication, coordination, and interaction
with heterogeneous tools and resources. Most recently, the Model Context
Protocol (MCP) by Anthropic and Agent-to-Agent (A2A) communication protocol by
Google have been introduced, and to the best of our knowledge, very few
applications exist where both protocols are employed within a single MAS
framework. We present a pilot study of AgentMaster, a novel modular
multi-protocol MAS framework with self-implemented A2A and MCP, enabling
dynamic coordination and flexible communication. Through a unified
conversational interface, the system supports natural language interaction
without prior technical expertise and responds to multimodal queries for tasks
including information retrieval, question answering, and image analysis.
Evaluation through the BERTScore F1 and LLM-as-a-Judge metric G-Eval averaged
96.3\% and 87.1\%, revealing robust inter-agent coordination, query
decomposition, dynamic routing, and domain-specific, relevant responses.
Overall, our proposed framework contributes to the potential capabilities of
domain-specific, cooperative, and scalable conversational AI powered by MAS.

</details>


### [91] [Page image classification for content-specific data processing](https://arxiv.org/abs/2507.21114)
*Kateryna Lutsai,Pavel Straňák*

Main category: cs.IR

TL;DR: 人文学科数字化项目产生大量历史文档页面图像，该项目开发评估针对历史文档页面的图像分类系统以处理异构数据。


<details>
  <summary>Details</summary>
Motivation: 人文学科数字化项目产生的大量历史文档页面图像，内容多样，手动分类分析困难，需要自动化方法分类页面以进行下游分析。

Method: 开发并评估专门为历史文档页面设计的图像分类系统，利用人工智能和机器学习的进展。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: Digitization projects in humanities often generate vast quantities of page
images from historical documents, presenting significant challenges for manual
sorting and analysis. These archives contain diverse content, including various
text types (handwritten, typed, printed), graphical elements (drawings, maps,
photos), and layouts (plain text, tables, forms). Efficiently processing this
heterogeneous data requires automated methods to categorize pages based on
their content, enabling tailored downstream analysis pipelines. This project
addresses this need by developing and evaluating an image classification system
specifically designed for historical document pages, leveraging advancements in
artificial intelligence and machine learning. The set of categories was chosen
to facilitate content-specific processing workflows, separating pages requiring
different analysis techniques (e.g., OCR for text, image analysis for graphics)

</details>


### [92] [FedFlex: Federated Learning for Diverse Netflix Recommendations](https://arxiv.org/abs/2507.21115)
*Sven Lankester,Manel Slokom,Gustavo de Carvalho Bertoli,Matias Vizcaino,Emmanuelle Beauxis Aussalet,Laura Hollink*

Main category: cs.IR

TL;DR: 本文介绍联邦推荐系统FedFlex，结合矩阵分解算法和MMR增强推荐多样性，实验表明其能引入多样内容且不降低用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统多关注准确性，对公平性和多样性关注有限。

Method: 引入FedFlex，集成两种矩阵分解算法进行个性化微调，应用MMR对项目重新排序。进行算法对比实验和两周用户研究。

Result: FedFlex能有效在推荐中引入新类型等多样内容。

Conclusion: FedFlex可在不影响用户满意度的情况下增强推荐多样性。

Abstract: Federated learning is a decentralized approach that enables collaborative
model training across multiple devices while preserving data privacy. It has
shown significant potential in various domains, including healthcare and
personalized recommendation systems. However, most existing work on federated
recommendation systems has focused primarily on improving accuracy, with
limited attention to fairness and diversity. In this paper, we introduce
FedFlex, a federated recommender system for Netflix-style TV series
recommendations. FedFlex integrates two state-of-the-art matrix factorization
algorithms for personalized fine-tuning. FedFlex also applies Maximal Marginal
Relevance (MMR) to re-rank items and enhance diversity. We conduct extensive
experiments comparing recommendations generated by SVD and BPR algorithms. In a
live two-week user study, participants received two recommendation lists: List
A, based on SVD or BPR, and List B, a re-ranked version emphasizing diversity.
Participants were asked to click on the movies they were interested in
watching. Our findings demonstrate that FedFlex effectively introduces diverse
content, such as new genres, into recommendations without necessarily
compromising user satisfaction.

</details>


### [93] [A Comprehensive Review on Harnessing Large Language Models to Overcome Recommender System Challenges](https://arxiv.org/abs/2507.21117)
*Rahul Raja,Anshaj Vats,Arpita Vats,Anirban Majumder*

Main category: cs.IR

TL;DR: 本文对利用大语言模型（LLMs）解决现代推荐系统关键挑战进行全面技术调研，介绍其多种应用方式，分析架构有效性并给出框架和权衡要点。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统存在数据稀疏、冷启动等问题，LLMs的出现提供了新范式来解决这些局限。

Method: 对LLMs在推荐系统中的应用，如提示驱动候选检索、语言原生排序等进行研究，对新兴的LLM驱动架构进行分类和分析。

Result: LLMs能增强个性化、语义对齐和可解释性，实现零样本和少样本推理，有效缓解传统管道的核心瓶颈。

Conclusion: LLMs是构建更自适应、语义丰富和以用户为中心推荐系统的基础推动者，而非辅助组件。

Abstract: Recommender systems have traditionally followed modular architectures
comprising candidate generation, multi-stage ranking, and re-ranking, each
trained separately with supervised objectives and hand-engineered features.
While effective in many domains, such systems face persistent challenges
including sparse and noisy interaction data, cold-start problems, limited
personalization depth, and inadequate semantic understanding of user and item
content. The recent emergence of Large Language Models (LLMs) offers a new
paradigm for addressing these limitations through unified, language-native
mechanisms that can generalize across tasks, domains, and modalities. In this
paper, we present a comprehensive technical survey of how LLMs can be leveraged
to tackle key challenges in modern recommender systems. We examine the use of
LLMs for prompt-driven candidate retrieval, language-native ranking,
retrieval-augmented generation (RAG), and conversational recommendation,
illustrating how these approaches enhance personalization, semantic alignment,
and interpretability without requiring extensive task-specific supervision.
LLMs further enable zero- and few-shot reasoning, allowing systems to operate
effectively in cold-start and long-tail scenarios by leveraging external
knowledge and contextual cues. We categorize these emerging LLM-driven
architectures and analyze their effectiveness in mitigating core bottlenecks of
conventional pipelines. In doing so, we provide a structured framework for
understanding the design space of LLM-enhanced recommenders, and outline the
trade-offs between accuracy, scalability, and real-time performance. Our goal
is to demonstrate that LLMs are not merely auxiliary components but
foundational enablers for building more adaptive, semantically rich, and
user-centric recommender systems

</details>


### [94] [Affect-aware Cross-Domain Recommendation for Art Therapy via Music Preference Elicitation](https://arxiv.org/abs/2507.21120)
*Bereket A. Yilma,Luis A. Leiva*

Main category: cs.IR

TL;DR: 本文提出基于音乐驱动偏好启发的跨领域推荐方法用于艺术治疗，经大规模研究验证其优于仅视觉启发方法，并公开代码等资源。


<details>
  <summary>Details</summary>
Motivation: 当前视觉艺术推荐系统仅靠视觉刺激进行用户建模，无法全面捕捉情感反应，而音乐刺激能引发独特情感反应，跨领域推荐可用于增强艺术治疗个性化，但该领域尚未探索。

Method: 提出基于音乐驱动偏好启发的跨领域推荐方法。

Result: 200名用户的大规模研究表明，音乐驱动偏好启发方法优于经典的仅视觉启发方法。

Conclusion: 基于音乐驱动偏好启发的跨领域推荐方法可有效提升艺术治疗中的个性化推荐效果。

Abstract: Art Therapy (AT) is an established practice that facilitates emotional
processing and recovery through creative expression. Recently, Visual Art
Recommender Systems (VA RecSys) have emerged to support AT, demonstrating their
potential by personalizing therapeutic artwork recommendations. Nonetheless,
current VA RecSys rely on visual stimuli for user modeling, limiting their
ability to capture the full spectrum of emotional responses during preference
elicitation. Previous studies have shown that music stimuli elicit unique
affective reflections, presenting an opportunity for cross-domain
recommendation (CDR) to enhance personalization in AT. Since CDR has not yet
been explored in this context, we propose a family of CDR methods for AT based
on music-driven preference elicitation. A large-scale study with 200 users
demonstrates the efficacy of music-driven preference elicitation, outperforming
the classic visual-only elicitation approach. Our source code, data, and models
are available at https://github.com/ArtAICare/Affect-aware-CDR

</details>


### [95] [RATE: An LLM-Powered Retrieval Augmented Generation Technology-Extraction Pipeline](https://arxiv.org/abs/2507.21125)
*Karan Mirhosseini,Arya Aftab,Alireza Sheikh*

Main category: cs.IR

TL;DR: 本文介绍基于大语言模型的自动化技术提取管道RATE，结合RAG与多定义验证，在BCI - XR研究文章案例中表现出色，F1分数达91.27%，优于BERT，凸显定义驱动的LLM方法用于技术提取和映射的前景。


<details>
  <summary>Details</summary>
Motivation: 在技术变革时代，技术地图对决策至关重要，而其依赖自动化技术提取方法，因此引入新的技术提取管道。

Method: 提出Retrieval Augmented Technology Extraction (RATE) 管道，结合Retrieval Augmented Generation (RAG) 与多定义LLM验证，以678篇BCI - XR研究文章为案例，用专家整理的70篇文章技术数据集评估，并用BERT模型作对比。

Result: RATE实现91.27%的F1分数，显著优于BERT的53.73%。将验证的技术术语映射到共现网络，揭示研究领域的主题集群和结构特征。

Conclusion: 定义驱动的LLM方法用于技术提取和映射很有前景，为BCI - XR领域新兴趋势提供新见解。代码开源。

Abstract: In an era of radical technology transformations, technology maps play a
crucial role in enhancing decision making. These maps heavily rely on automated
methods of technology extraction. This paper introduces Retrieval Augmented
Technology Extraction (RATE), a Large Language Model (LLM) based pipeline for
automated technology extraction from scientific literature. RATE combines
Retrieval Augmented Generation (RAG) with multi-definition LLM-based
validation. This hybrid method results in high recall in candidate generation
alongside with high precision in candidate filtering. While the pipeline is
designed to be general and widely applicable, we demonstrate its use on 678
research articles focused on Brain-Computer Interfaces (BCIs) and Extended
Reality (XR) as a case study. Consequently, The validated technology terms by
RATE were mapped into a co-occurrence network, revealing thematic clusters and
structural features of the research landscape. For the purpose of evaluation, a
gold standard dataset of technologies in 70 selected random articles had been
curated by the experts. In addition, a technology extraction model based on
Bidirectional Encoder Representations of Transformers (BERT) was used as a
comparative method. RATE achieved F1-score of 91.27%, Significantly
outperforming BERT with F1-score of 53.73%. Our findings highlight the promise
of definition-driven LLM methods for technology extraction and mapping. They
also offer new insights into emerging trends within the BCI-XR field. The
source code is available https://github.com/AryaAftab/RATE

</details>


### [96] [Efficient Data Retrieval and Comparative Bias Analysis of Recommendation Algorithms for YouTube Shorts and Long-Form Videos](https://arxiv.org/abs/2507.21467)
*Selimhan Dagtas,Mert Can Cakmak,Nitin Agarwal*

Main category: cs.IR

TL;DR: 本文研究YouTube推荐算法，开发数据收集框架分析长短视频推荐差异，发现短视内容即时但多样性低，还研究敏感政治话题偏见，为设计推荐系统提供建议。


<details>
  <summary>Details</summary>
Motivation: 短视频流行使推荐算法对用户体验影响增大，存在偏见、回音室和内容多样性等问题，需研究。

Method: 开发高效数据收集框架，采用并行计算和先进抓取技术克服YouTube API限制。

Result: 发现长短视频推荐算法有不同行为模式，短视频内容即时但多样性低，研究敏感政治话题算法偏见。

Conclusion: 研究为设计公平透明推荐系统提供见解，强调数字媒体领域负责任AI实践的重要性。

Abstract: The growing popularity of short-form video content, such as YouTube Shorts,
has transformed user engagement on digital platforms, raising critical
questions about the role of recommendation algorithms in shaping user
experiences. These algorithms significantly influence content consumption, yet
concerns about biases, echo chambers, and content diversity persist. This study
develops an efficient data collection framework to analyze YouTube's
recommendation algorithms for both short-form and long-form videos, employing
parallel computing and advanced scraping techniques to overcome limitations of
YouTube's API. The analysis uncovers distinct behavioral patterns in
recommendation algorithms across the two formats, with short-form videos
showing a more immediate shift toward engaging yet less diverse content
compared to long-form videos. Furthermore, a novel investigation into biases in
politically sensitive topics, such as the South China Sea dispute, highlights
the role of these algorithms in shaping narratives and amplifying specific
viewpoints. By providing actionable insights for designing equitable and
transparent recommendation systems, this research underscores the importance of
responsible AI practices in the evolving digital media landscape.

</details>


### [97] [Solution for Meta KDD Cup'25: A Comprehensive Three-Step Framework for Vision Question Answering](https://arxiv.org/abs/2507.21520)
*Zijian Zhang,Xiaocheng Zhang,Yang Zhou,Zhimin Lin,Peng Yan*

Main category: cs.IR

TL;DR: 本文介绍BlackPearl团队在Meta KDD Cup'25中针对CRAG - MM基准三个任务的解决方案，使用单一模型，取得不错排名。


<details>
  <summary>Details</summary>
Motivation: VLLMs存在答案幻觉问题，多模态RAG虽有帮助但仍面临视觉上下文理解等挑战，需要解决方案。

Method: 针对每个任务使用单一模型，采用数据增强、RAG、重排序和多任务微调等方法。

Result: 在三个任务的自动评估排名为第3、第3和第1，任务3人工评估获第二名。

Conclusion: 所采用的方法在CRAG - MM基准任务中取得了较好的效果。

Abstract: Vision Large Language Models (VLLMs) have improved multi-modal understanding
and visual question answering (VQA), but still suffer from hallucinated
answers. Multi-modal Retrieval-Augmented Generation (RAG) helps address these
issues by incorporating external information, yet challenges remain in visual
context comprehension, multi-source retrieval, and multi-turn interactions. To
address these challenges, Meta constructed the CRAG-MM benchmark and launched
the CRAG-MM Challenge at KDD Cup 2025, which consists of three tasks. This
paper describes the solutions of all tasks in Meta KDD Cup'25 from BlackPearl
team. We use a single model for each task, with key methods including data
augmentation, RAG, reranking, and multi-task fine-tuning. Our solution achieve
automatic evaluation rankings of 3rd, 3rd, and 1st on the three tasks, and win
second place in Task3 after human evaluation.

</details>


### [98] [Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation](https://arxiv.org/abs/2507.21563)
*Minh-Anh Nguyen,Bao Nguyen,Ha Lan N. T.,Tuan Anh Hoang,Duc-Trong Le,Dung D. Le*

Main category: cs.IR

TL;DR: 本文提出利用大语言模型和物品文本描述的数据增强框架，生成合成交互数据并集成到图对比学习框架，实验显示能提升准确率并减少流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 推荐系统受数据稀疏问题影响，性能下降且放大流行度偏差。

Method: 通过少样本多次提示大语言模型对物品重新排序并通过多数投票聚合结果生成合成交互数据，将其集成到图对比学习框架。

Result: 方法提升了准确率，减少了流行度偏差，优于强大的基线模型。

Conclusion: 提出的数据增强框架能有效解决推荐系统的数据稀疏问题。

Abstract: Recommendation systems often suffer from data sparsity caused by limited
user-item interactions, which degrade their performance and amplify popularity
bias in real-world scenarios. This paper proposes a novel data augmentation
framework that leverages Large Language Models (LLMs) and item textual
descriptions to enrich interaction data. By few-shot prompting LLMs multiple
times to rerank items and aggregating the results via majority voting, we
generate high-confidence synthetic user-item interactions, supported by
theoretical guarantees based on the concentration of measure. To effectively
leverage the augmented data in the context of a graph recommendation system, we
integrate it into a graph contrastive learning framework to mitigate
distributional shift and alleviate popularity bias. Extensive experiments show
that our method improves accuracy and reduces popularity bias, outperforming
strong baselines.

</details>


### [99] [Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's NLP Results](https://arxiv.org/abs/2507.21770)
*Ali Fallahi,Azam Bastanfard,Amineh Amini,Hadi Saboohi*

Main category: cs.IR

TL;DR: 论文提出基于语义信息构建知识图谱的新方法，用ChatGPT评估电影简介、提取语气，结果显示该方法比用发行商提供的明确类型更能提高准确性。


<details>
  <summary>Details</summary>
Motivation: 随着网络推荐系统重要性提升，尤其是电影行业，为帮助用户找到相关结果、提高用户参与度和满意度，需更合适的推荐方法。

Method: 提出基于语义信息构建知识图谱的新方法，利用ChatGPT评估电影简介并提取语气。

Result: 使用该方法比采用发行商提供的明确类型能显著提高准确性。

Conclusion: 基于语义信息构建知识图谱并利用ChatGPT提取语气的方法在电影推荐中效果良好。

Abstract: The importance of recommender systems on the web has grown, especially in the
movie industry, with a vast selection of options to watch. To assist users in
traversing available items and finding relevant results, recommender systems
analyze operational data and investigate users' tastes and habits. Providing
highly individualized suggestions can boost user engagement and satisfaction,
which is one of the fundamental goals of the movie industry, significantly in
online platforms. According to recent studies and research, using
knowledge-based techniques and considering the semantic ideas of the textual
data is a suitable way to get more appropriate results. This study provides a
new method for building a knowledge graph based on semantic information. It
uses the ChatGPT, as a large language model, to assess the brief descriptions
of movies and extract their tone of voice. Results indicated that using the
proposed method may significantly enhance accuracy rather than employing the
explicit genres supplied by the publishers.

</details>


### [100] [Exploration on Demand: From Algorithmic Control to User Empowerment](https://arxiv.org/abs/2507.21884)
*Edoardo Bianchi*

Main category: cs.IR

TL;DR: 本文提出自适应聚类框架解决推荐系统过度专业化问题，平衡个性化与多样性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统过度专业化、用户接触内容单一和过滤气泡问题。

Method: 利用句子嵌入通过在线算法和动态阈值聚类，提出用户可控的探索机制。

Result: 在MovieLens数据集上减少列表内相似度，提高意外性；A/B测试显示72.7%长期用户偏好探索性推荐。

Conclusion: 该系统能在不牺牲用户满意度的前提下促进有意义的内容发现。

Abstract: Recommender systems often struggle with over-specialization, which severely
limits users' exposure to diverse content and creates filter bubbles that
reduce serendipitous discovery. To address this fundamental limitation, this
paper introduces an adaptive clustering framework with user-controlled
exploration that effectively balances personalization and diversity in movie
recommendations. Our approach leverages sentence-transformer embeddings to
group items into semantically coherent clusters through an online algorithm
with dynamic thresholding, thereby creating a structured representation of the
content space. Building upon this clustering foundation, we propose a novel
exploration mechanism that empowers users to control recommendation diversity
by strategically sampling from less-engaged clusters, thus expanding their
content horizons while preserving relevance. Experiments on the MovieLens
dataset demonstrate the system's effectiveness, showing that exploration
significantly reduces intra-list similarity from 0.34 to 0.26 while
simultaneously increasing unexpectedness to 0.73. Furthermore, our Large
Language Model-based A/B testing methodology, conducted with 300 simulated
users, reveals that 72.7% of long-term users prefer exploratory recommendations
over purely exploitative ones, providing strong evidence for the system's
ability to promote meaningful content discovery without sacrificing user
satisfaction.

</details>


### [101] [The Curious Case of High-Dimensional Indexing as a File Structure: A Case Study of eCP-FS](https://arxiv.org/abs/2507.21939)
*Omar Shahbaz Khan,Gylfi Þór Guðmundsson,Björn Þór Jónsson*

Main category: cs.IR

TL;DR: 本文提出eCP - FS，一种基于文件的eCP实现，对比显示其虽慢但有竞争力，在内存受限场景有优势。


<details>
  <summary>Details</summary>
Motivation: 现有ANN索引在多索引使用时存在内存竞争问题，且可视化和分析索引结构需复杂编码，需评估新方法的性能损失。

Method: 提出基于文件库将数据结构映射到文件结构的方法，实现eCP - FS并与现有索引对比。

Result: eCP - FS比现有索引慢，但在内存不受限时有一定竞争力，在内存受限场景内存占用极小。

Conclusion: eCP - FS适用于资源受限或多索引环境。

Abstract: Modern analytical pipelines routinely deploy multiple deep learning and
retrieval models that rely on approximate nearest-neighbor (ANN) indexes to
support efficient similarity-based search. While many state-of-the-art
ANN-indexes are memory-based (e.g., HNSW and IVF), using multiple ANN indexes
creates a competition for limited GPU/CPU memory resources, which in turn
necessitates disk-based index structures (e.g., DiskANN or eCP). In typical
index implementations, the main component is a complex data structure that is
serialized to disk and is read either fully at startup time, for memory-based
indexes, or incrementally at query time, for disk-based indexes. To visualize
the index structure, or analyze its quality, complex coding is needed that is
either embedded in the index implementation or replicates the code that reads
the data structure. In this paper, we consider an alternative approach that
maps the data structure to a file structure, using a file library, making the
index easily readable for any programming language and even human-readable. The
disadvantage is that the serialized index is verbose, leading to overhead of
searching through the index. The question addressed in this paper is how severe
this performance penalty is. To that end, this paper presents eCP-FS, a
file-based implementation of eCP, a well-known disk-based ANN index. A
comparison with state-of-the-art indexes shows that while eCP-FS is slower, the
implementation is nevertheless somewhat competitive even when memory is not
constrained. In a memory-constrained scenario, eCP-FS offers a minimal memory
footprint, making it ideal for resource-constrained or multi-index
environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [102] [Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students](https://arxiv.org/abs/2507.21109)
*Prital Bamnodkar*

Main category: cs.LG

TL;DR: 本文提出受人类学习策略启发的持续学习方法TFC - SR，在基准测试中表现优于现有方法，分析了内存与性能权衡，表明TFC - SR是有效方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络灾难性遗忘问题，即学习新任务后旧任务性能下降。

Method: 引入受人类学习策略启发的TFC - SR方法，用主动回忆探针增强标准经验回放，周期性、任务感知地评估模型记忆。

Result: 在Split MNIST和Split CIFAR - 100基准测试中，TFC - SR显著优于正则化和基于回放的基线方法；优势源于探针稳定作用而非回放量；在内存受限环境表现更好，内存充足时高回放量更有效。

Conclusion: TFC - SR是强大且高效的方法，强调在持续学习系统中集成主动内存检索机制的重要性。

Abstract: Deep Neural Networks often suffer from a critical limitation known as
Catastrophic Forgetting, where performance on past tasks degrades after
learning new ones. This paper introduces a novel continual learning approach
inspired by human learning strategies like Active Recall, Deliberate Practice
and Spaced Repetition, named Task Focused Consolidation with Spaced Recall
(TFC-SR). TFC-SR enhances the standard experience replay with a mechanism we
termed the Active Recall Probe. It is a periodic, task-aware evaluation of the
model's memory that stabilizes the representations of past knowledge. We test
TFC-SR on the Split MNIST and Split CIFAR-100 benchmarks against leading
regularization-based and replay-based baselines. Our results show that TFC-SR
performs significantly better than these methods. For instance, on the Split
CIFAR-100, it achieves a final accuracy of 13.17% compared to standard replay's
7.40%. We demonstrate that this advantage comes from the stabilizing effect of
the probe itself, and not from the difference in replay volume. Additionally,
we analyze the trade-off between memory size and performance and show that
while TFC-SR performs better in memory-constrained environments, higher replay
volume is still more effective when available memory is abundant. We conclude
that TFC-SR is a robust and efficient approach, highlighting the importance of
integrating active memory retrieval mechanisms into continual learning systems.

</details>


### [103] [Pre-, In-, and Post-Processing Class Imbalance Mitigation Techniques for Failure Detection in Optical Networks](https://arxiv.org/abs/2507.21119)
*Yousuf Moiz Ali,Jaroslaw E. Prilepsky,Nicola Sambo,João Pedro,Mohammad M. Hosseini,Antonio Napoli,Sergei K. Turitsyn,Pedro Freire*

Main category: cs.LG

TL;DR: 比较光网络故障检测中处理类别不平衡的不同技术，阈值调整F1增益最高，随机欠采样推理最快，存在性能 - 复杂度权衡。


<details>
  <summary>Details</summary>
Motivation: 解决光网络故障检测中的类别不平衡问题

Method: 比较预、中、后处理技术

Result: 阈值调整实现最高F1增益（15.3%），随机欠采样推理最快

Conclusion: 在处理类别不平衡时存在关键的性能 - 复杂度权衡

Abstract: We compare pre-, in-, and post-processing techniques for class imbalance
mitigation in optical network failure detection. Threshold Adjustment achieves
the highest F1 gain (15.3%), while Random Under-sampling (RUS) offers the
fastest inference, highlighting a key performance-complexity trade-off.

</details>


### [104] [Quantum Geometry of Data](https://arxiv.org/abs/2507.21135)
*Alexander G. Abanov,Luca Candelori,Harold C. Steinacker,Martin T. Wells,Jerome R. Busemeyer,Cameron J. Hogan,Vahagn Kirakosyan,Nicola Marzari,Sunil Pinnamaneni,Dario Villani,Mengjia Xu,Kharen Musaelian*

Main category: cs.LG

TL;DR: 论文展示量子认知机器学习（QCML）将数据编码为量子几何，其能赋予数据集丰富结构，避免维数灾难，还举例说明并指出其对理解认知现象有帮助。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用量子认知机器学习对数据进行编码，以及其在理解认知现象方面的作用。

Method: 在QCML中，用学习到的厄米矩阵表示数据特征，将数据点映射到希尔伯特空间的状态。

Result: 量子几何描述赋予数据集丰富的几何和拓扑结构，能捕捉数据全局属性，避免局部方法的维数灾难，并通过多个合成和真实世界的例子进行了说明。

Conclusion: QCML的量子几何表示有助于在量子认知框架内推进对认知现象的理解。

Abstract: We demonstrate how Quantum Cognition Machine Learning (QCML) encodes data as
quantum geometry. In QCML, features of the data are represented by learned
Hermitian matrices, and data points are mapped to states in Hilbert space. The
quantum geometry description endows the dataset with rich geometric and
topological structure - including intrinsic dimension, quantum metric, and
Berry curvature - derived directly from the data. QCML captures global
properties of data, while avoiding the curse of dimensionality inherent in
local methods. We illustrate this on a number of synthetic and real-world
examples. Quantum geometric representation of QCML could advance our
understanding of cognitive phenomena within the framework of quantum cognition.

</details>


### [105] [A Study on Variants of Conventional, Fuzzy, and Nullspace-Based Independence Criteria for Improving Supervised and Unsupervised Learning](https://arxiv.org/abs/2507.21136)
*Mojtaba Moattari*

Main category: cs.LG

TL;DR: 回顾独立性准则，提出3个新准则设计无监督和有监督降维方法，评估后发现性能优于基线，为可解释机器学习开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 传统无监督和有监督学习方法用核函数捕捉数据非线性时，需专家确保提出的非线性能最大化可变性并捕捉数据固有多样性，因此要设计无监督学习器。

Method: 回顾所有独立性准则，提出3个新的独立性准则，用其设计无监督和有监督降维方法，并在线性和神经非线性设置下评估对比度、准确性和可解释性。

Result: 所提出的方法性能优于基线（tSNE、PCA、正则化LDA、带（无）监督学习器和层共享的VAE）。

Conclusion: 为研究人员开辟了可解释机器学习的新方向。

Abstract: Unsupervised and supervised learning methods conventionally use kernels to
capture nonlinearities inherent in data structure. However experts have to
ensure their proposed nonlinearity maximizes variability and capture inherent
diversity of data. We reviewed all independence criteria to design unsupervised
learners. Then we proposed 3 independence criteria and used them to design
unsupervised and supervised dimensionality reduction methods. We evaluated
contrast, accuracy and interpretability of these methods in both linear and
neural nonlinear settings. The results show that the methods have outperformed
the baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and
layer sharing) and opened a new line of interpretable machine learning (ML) for
the researchers.

</details>


### [106] [Bubbleformer: Forecasting Boiling with Transformers](https://arxiv.org/abs/2507.21244)
*Sheikh Md Shakeel Hassan,Xianwei Zou,Akash Dhruv,Vishwanath Ganesan,Aparna Chandramowlishwaran*

Main category: cs.LG

TL;DR: 介绍Bubbleformer模型，可预测沸腾动力学，提出评估指标并发布数据集，创造新基准。


<details>
  <summary>Details</summary>
Motivation: 现有神经PDE替代模型在模拟沸腾过程存在需未来输入、无法模拟流动沸腾速度场等问题。

Method: 引入基于Transformer的时空模型Bubbleformer，集成因式轴向注意力、频率感知缩放并结合热物理参数；提出基于物理的可解释指标；发布BubbleML 2.0数据集。

Result: Bubbleformer在两相沸腾流的预测和预报中创造新基准结果。

Conclusion: Bubbleformer能有效解决现有模型问题，可在无模拟数据依赖下预测沸腾动力学。

Abstract: Modeling boiling (an inherently chaotic, multiphase process central to energy
and thermal systems) remains a significant challenge for neural PDE surrogates.
Existing models require future input (e.g., bubble positions) during inference
because they fail to learn nucleation from past states, limiting their ability
to autonomously forecast boiling dynamics. They also fail to model flow boiling
velocity fields, where sharp interface-momentum coupling demands long-range and
directional inductive biases. We introduce Bubbleformer, a transformer-based
spatiotemporal model that forecasts stable and long-range boiling dynamics
including nucleation, interface evolution, and heat transfer without dependence
on simulation data during inference. Bubbleformer integrates factorized axial
attention, frequency-aware scaling, and conditions on thermophysical parameters
to generalize across fluids, geometries, and operating conditions. To evaluate
physical fidelity in chaotic systems, we propose interpretable physics-based
metrics that evaluate heat-flux consistency, interface geometry, and mass
conservation. We also release BubbleML 2.0, a high-fidelity dataset that spans
diverse working fluids (cryogens, refrigerants, dielectrics), boiling
configurations (pool and flow boiling), flow regimes (bubbly, slug, annular),
and boundary conditions. Bubbleformer sets new benchmark results in both
prediction and forecasting of two-phase boiling flows.

</details>


### [107] [Advancing Wildfire Risk Prediction via Morphology-Aware Curriculum Contrastive Learning](https://arxiv.org/abs/2507.21147)
*Fabrizio Lo Scudo,Alessio De Rango,Luca Furnari,Alfonso Senatore,Donato D'Ambrosio,Giuseppe Mendicino,Gianluigi Greco*

Main category: cs.LG

TL;DR: 本文探讨利用对比框架应对野火预测挑战，提出基于形态学的课程对比学习方法并实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 野火对生态和人类健康影响大，气候变化加剧影响，现有数据不平衡、高维时空数据复杂，需开发先进风险管理策略，且要降低计算成本以利用最新天气预报更新预测。

Method: 采用对比框架，引入基于形态学的课程对比学习方法。

Result: 通过实验分析验证了所提出建模策略的有效性。

Conclusion: 基于形态学的课程对比学习方法可缓解不同区域特征相关问题，能使用更小补丁尺寸且不影响性能，有助于应对野火预测挑战。

Abstract: Wildfires significantly impact natural ecosystems and human health, leading
to biodiversity loss, increased hydrogeological risks, and elevated emissions
of toxic substances. Climate change exacerbates these effects, particularly in
regions with rising temperatures and prolonged dry periods, such as the
Mediterranean. This requires the development of advanced risk management
strategies that utilize state-of-the-art technologies. However, in this
context, the data show a bias toward an imbalanced setting, where the incidence
of wildfire events is significantly lower than typical situations. This
imbalance, coupled with the inherent complexity of high-dimensional
spatio-temporal data, poses significant challenges for training deep learning
architectures. Moreover, since precise wildfire predictions depend mainly on
weather data, finding a way to reduce computational costs to enable more
frequent updates using the latest weather forecasts would be beneficial. This
paper investigates how adopting a contrastive framework can address these
challenges through enhanced latent representations for the patch's dynamic
features. We thus introduce a new morphology-based curriculum contrastive
learning that mitigates issues associated with diverse regional characteristics
and enables the use of smaller patch sizes without compromising performance. An
experimental analysis is performed to validate the effectiveness of the
proposed modeling strategies.

</details>


### [108] [evoxels: A differentiable physics framework for voxel-based microstructure simulations](https://arxiv.org/abs/2507.21748)
*Simon Daubner,Alexander E. Cohen,Benjamin Dörich,Samuel J. Cooper*

Main category: cs.LG

TL;DR: 材料科学跨学科，evoxels框架可集成多方面内容促进逆材料设计。


<details>
  <summary>Details</summary>
Motivation: 材料科学跨学科，需桥接不同领域以实现逆材料设计，加速发现并加深对过程 - 结构 - 属性关系的理解。

Method: 采用基于Python的统一体素方法构建可微物理框架evoxels，集成3D显微镜数据、物理模拟、逆建模和机器学习。

Result: 未提及具体结果

Conclusion: 未提及明确结论

Abstract: Materials science inherently spans disciplines: experimentalists use advanced
microscopy to uncover micro- and nanoscale structure, while theorists and
computational scientists develop models that link processing, structure, and
properties. Bridging these domains is essential for inverse material design
where you start from desired performance and work backwards to optimal
microstructures and manufacturing routes. Integrating high-resolution imaging
with predictive simulations and data-driven optimization accelerates discovery
and deepens understanding of process-structure-property relationships. The
differentiable physics framework evoxels is based on a fully Pythonic, unified
voxel-based approach that integrates segmented 3D microscopy data, physical
simulations, inverse modeling, and machine learning.

</details>


### [109] [Deep Unfolding for MIMO Signal Detection](https://arxiv.org/abs/2507.21152)
*Hangli Ge,Noboru Koshizuka*

Main category: cs.LG

TL;DR: 提出基于深度展开神经网络的MIMO检测器DPST，在复域运算，训练参数少，性能优、复杂度低。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖实值近似，未遵循信号处理任务本质，需高效、可解释、低复杂度的MIMO信号检测方法。

Method: 提出基于深度展开神经网络、用Wirtinger微积分进行复值计算的MIMO检测器DPST，在复域运算且所需训练参数少。

Result: 数值结果表明该方法迭代次数少、计算复杂度低，检测性能更优。

Conclusion: 该方法是下一代大规模MIMO系统的实用解决方案。

Abstract: In this paper, we propose a deep unfolding neural network-based MIMO detector
that incorporates complex-valued computations using Wirtinger calculus. The
method, referred as Dynamic Partially Shrinkage Thresholding (DPST), enables
efficient, interpretable, and low-complexity MIMO signal detection. Unlike
prior approaches that rely on real-valued approximations, our method operates
natively in the complex domain, aligning with the fundamental nature of signal
processing tasks. The proposed algorithm requires only a small number of
trainable parameters, allowing for simplified training. Numerical results
demonstrate that the proposed method achieves superior detection performance
with fewer iterations and lower computational complexity, making it a practical
solution for next-generation massive MIMO systems.

</details>


### [110] [Deep Reinforcement Learning for Real-Time Green Energy Integration in Data Centers](https://arxiv.org/abs/2507.21153)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.LG

TL;DR: 本文研究DRL优化的电商数据中心能源管理系统，该系统在节能、成本和环保方面表现出色，验证了DRL模型有效性。


<details>
  <summary>Details</summary>
Motivation: 提升电商数据中心能源效率、成本效益和环境可持续性。

Method: 利用DRL算法动态管理可再生能源、储能和电网电力的整合，实时适应能源供应波动。

Result: DRL优化系统能源成本降低38%，SLA违规率1.5%，能源效率提高82%，碳排放减少45%，累积奖励950，优于传统RL和启发式方法。

Conclusion: DRL在推进能源优化策略和解决可持续性挑战方面具有潜力，所提系统是数据中心能源管理的可靠解决方案。

Abstract: This paper explores the implementation of a Deep Reinforcement Learning
(DRL)-optimized energy management system for e-commerce data centers, aimed at
enhancing energy efficiency, cost-effectiveness, and environmental
sustainability. The proposed system leverages DRL algorithms to dynamically
manage the integration of renewable energy sources, energy storage, and grid
power, adapting to fluctuating energy availability in real time. The study
demonstrates that the DRL-optimized system achieves a 38\% reduction in energy
costs, significantly outperforming traditional Reinforcement Learning (RL)
methods (28\%) and heuristic approaches (22\%). Additionally, it maintains a
low SLA violation rate of 1.5\%, compared to 3.0\% for RL and 4.8\% for
heuristic methods. The DRL-optimized approach also results in an 82\%
improvement in energy efficiency, surpassing other methods, and a 45\%
reduction in carbon emissions, making it the most environmentally friendly
solution. The system's cumulative reward of 950 reflects its superior
performance in balancing multiple objectives. Through rigorous testing and
ablation studies, the paper validates the effectiveness of the DRL model's
architecture and parameters, offering a robust solution for energy management
in data centers. The findings highlight the potential of DRL in advancing
energy optimization strategies and addressing sustainability challenges.

</details>


### [111] [SPADE-S: A Sparsity-Robust Foundational Forecaster](https://arxiv.org/abs/2507.21155)
*Malcolm Wolff,Matthew Li,Ravi Kiran Selvam,Hanjing Zhu,Kin G. Olivares,Ruijun Ma,Abhinav Katoch,Shankar Ramasubramanian,Mengfei Cao,Roberto Bandarra,Rahul Gopalsamy,Stefania La Vattiata,Sitan Yang,Michael M. Mahoney*

Main category: cs.LG

TL;DR: 现有深度学习架构对异质性时间序列建模困难，提出SPADE - S架构减少偏差、提升精度，在多数据集表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测中，对具有强异质性的时间序列进行准确建模仍是挑战，现有模型在低幅度和稀疏时间序列上表现不佳。

Method: 提出SPADE - S预测架构以减少基于幅度和稀疏性的系统偏差。

Result: 在需求预测的多个用例中，SPADE - S优于现有方法，能提升预测精度，在不同数据集上P90和P50预测精度均有提升。

Conclusion: SPADE - S架构能有效减少偏差，提高时间序列预测的整体准确性。

Abstract: Despite significant advancements in time series forecasting, accurate
modeling of time series with strong heterogeneity in magnitude and/or sparsity
patterns remains challenging for state-of-the-art deep learning architectures.
We identify several factors that lead existing models to systematically
underperform on low-magnitude and sparse time series, including loss functions
with implicit biases toward high-magnitude series, training-time sampling
methods, and limitations of time series encoding methods.
  SPADE-S is a robust forecasting architecture that significantly reduces
magnitude- and sparsity-based systematic biases and improves overall prediction
accuracy. Empirical results demonstrate that SPADE-S outperforms existing
state-of-the-art approaches across a diverse set of use cases in demand
forecasting. In particular, we show that, depending on the quantile forecast
and magnitude of the series, SPADE-S can improve forecast accuracy by up to
15%. This results in P90 overall forecast accuracy gains of 2.21%, 6.58%, and
4.28%, and P50 forecast accuracy gains of 0.92%, 0.77%, and 1.95%,
respectively, for each of three distinct datasets, ranging from 3 million to
700 million series, from a large online retailer.

</details>


### [112] [Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications](https://arxiv.org/abs/2507.21199)
*Xinye Cao,Hongcan Guo,Guoshun Nan,Jiaoyang Cui,Haoting Qian,Yihan Lin,Yilin Peng,Diyang Zhang,Yanzhao Hou,Huici Wu,Xiaofeng Tao,Tony Q. S. Quek*

Main category: cs.LG

TL;DR: 本文提出用单一组合式大语言模型实现多种交互式多模态应用（IMA）的新范式，提出ContextLoRA方法和ContextGear调度策略，实验证明其优越性和实际适用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多个大语言模型实现IMA，本文旨在用单一组合式大语言模型完成各种IMA，解决其适应不同目标及在资源受限环境的灵活性和效率问题。

Method: 提出ContextLoRA方法，构建任务依赖图引导大语言模型学习IMA间的结构化上下文，分区可学习参数矩阵，采用分步微调程序；引入ContextGear调度策略优化训练过程。

Result: 在三个基准测试中证明了ContextLoRA和ContextGear的优越性，在真实无线测试平台上验证了该范式的实际适用性。

Conclusion: 提出的单一组合式大语言模型范式可有效实现多种IMA，ContextLoRA和ContextGear方法具有优势和实际应用价值，代码将开源。

Abstract: Interactive multimodal applications (IMAs), such as route planning in the
Internet of Vehicles, enrich users' personalized experiences by integrating
various forms of data over wireless networks. Recent advances in large language
models (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple
IMAs, with each LLM trained individually for a specific task that presents
different business workflows. In contrast to existing approaches that rely on
multiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes
various IMAs using a single compositional LLM over wireless networks. The two
primary challenges include 1) guiding a single LLM to adapt to diverse IMA
objectives and 2) ensuring the flexibility and efficiency of the LLM in
resource-constrained mobile environments. To tackle the first challenge, we
propose ContextLoRA, a novel method that guides an LLM to learn the rich
structured context among IMAs by constructing a task dependency graph. We
partition the learnable parameter matrix of neural layers for each IMA to
facilitate LLM composition. Then, we develop a step-by-step fine-tuning
procedure guided by task relations, including training, freezing, and masking
phases. This allows the LLM to learn to reason among tasks for better
adaptation, capturing the latent dependencies between tasks. For the second
challenge, we introduce ContextGear, a scheduling strategy to optimize the
training procedure of ContextLoRA, aiming to minimize computational and
communication costs through a strategic grouping mechanism. Experiments on
three benchmarks show the superiority of the proposed ContextLoRA and
ContextGear. Furthermore, we prototype our proposed paradigm on a real-world
wireless testbed, demonstrating its practical applicability for various IMAs.
We will release our code to the community.

</details>


### [113] [Handling Out-of-Distribution Data: A Survey](https://arxiv.org/abs/2507.21160)
*Lakpa Tamang,Mohamed Reda Bouadjenek,Richard Dazeley,Sunil Aryal*

Main category: cs.LG

TL;DR: 本文围绕机器学习中训练与部署阶段的数据分布偏移问题，总结应对两类偏移的机制，阐述贡献并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习和数据驱动应用中训练与部署阶段数据分布变化（分布偏移）的问题。

Method: 形式化分布偏移，回顾传统方法不足；讨论处理分布偏移的重要性并综述检测、测量和缓解偏移的方法；分析当前处理机制并提出未来研究方向。

Result: 对分布偏移相关文献进行回顾，聚焦现有调查中被忽视的OOD数据。

Conclusion: 提出处理分布偏移的综合思路和未来研究方向。

Abstract: In the field of Machine Learning (ML) and data-driven applications, one of
the significant challenge is the change in data distribution between the
training and deployment stages, commonly known as distribution shift. This
paper outlines different mechanisms for handling two main types of distribution
shifts: (i) Covariate shift: where the value of features or covariates change
between train and test data, and (ii) Concept/Semantic-shift: where model
experiences shift in the concept learned during training due to emergence of
novel classes in the test phase. We sum up our contributions in three folds.
First, we formalize distribution shifts, recite on how the conventional method
fails to handle them adequately and urge for a model that can simultaneously
perform better in all types of distribution shifts. Second, we discuss why
handling distribution shifts is important and provide an extensive review of
the methods and techniques that have been developed to detect, measure, and
mitigate the effects of these shifts. Third, we discuss the current state of
distribution shift handling mechanisms and propose future research directions
in this area. Overall, we provide a retrospective synopsis of the literature in
the distribution shift, focusing on OOD data that had been overlooked in the
existing surveys.

</details>


### [114] [OCSVM-Guided Representation Learning for Unsupervised Anomaly Detection](https://arxiv.org/abs/2507.21164)
*Nicolas Pinon,Carole Lartizien*

Main category: cs.LG

TL;DR: 提出将表征学习与可解析单类SVM紧密耦合的无监督异常检测方法，在MNIST - C基准和脑MRI微小病变检测任务验证性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法存在重建异常效果过好、特征空间不佳、依赖替代目标、限制核选择等问题，需要更优方法。

Method: 通过自定义损失公式将表征学习与可解析单类SVM紧密耦合，使潜在特征与OCSVM决策边界对齐。

Result: 在MNIST - C基准和脑MRI微小病变检测任务中表现出性能和鲁棒性，能检测小的非高信号病变。

Conclusion: 所提方法在通用无监督异常检测和实际医学影像应用中有潜力。

Abstract: Unsupervised anomaly detection (UAD) aims to detect anomalies without labeled
data, a necessity in many machine learning applications where anomalous samples
are rare or not available. Most state-of-the-art methods fall into two
categories: reconstruction-based approaches, which often reconstruct anomalies
too well, and decoupled representation learning with density estimators, which
can suffer from suboptimal feature spaces. While some recent methods attempt to
couple feature learning and anomaly detection, they often rely on surrogate
objectives, restrict kernel choices, or introduce approximations that limit
their expressiveness and robustness. To address this challenge, we propose a
novel method that tightly couples representation learning with an analytically
solvable one-class SVM (OCSVM), through a custom loss formulation that directly
aligns latent features with the OCSVM decision boundary. The model is evaluated
on two tasks: a new benchmark based on MNIST-C, and a challenging brain MRI
subtle lesion detection task. Unlike most methods that focus on large,
hyperintense lesions at the image level, our approach succeeds to target small,
non-hyperintense lesions, while we evaluate voxel-wise metrics, addressing a
more clinically relevant scenario. Both experiments evaluate a form of
robustness to domain shifts, including corruption types in MNIST-C and
scanner/age variations in MRI. Results demonstrate performance and robustness
of our proposed mode,highlighting its potential for general UAD and real-world
medical imaging applications. The source code is available at
https://github.com/Nicolas-Pinon/uad_ocsvm_guided_repr_learning

</details>


### [115] [AGORA: Incentivizing Group Emergence Capability in LLMs via Group Distillation](https://arxiv.org/abs/2507.21166)
*Ren Zhuang,Ben Wang,Shuifa Sun*

Main category: cs.LG

TL;DR: 提出结构化交互作为新的扩展轴，AGORA框架使协作集成在数学基准测试上超越现有系统，证明交互可驱动智能扩展，协作生态工程是关键领域。


<details>
  <summary>Details</summary>
Motivation: 当前训练数据集的静态特性限制了复杂推理的进展。

Method: 提出结构化交互作为新扩展轴，构建自进化框架AGORA。

Result: AGORA框架使协作集成在具有挑战性的数学基准测试上的推理性能比最先进的单体系统高出多达4.45个百分点。

Conclusion: 协作生态系统的工程设计是能力涌现的重要前沿领域。

Abstract: Progress in complex reasoning is constrained by the static nature of the
current training datasets. We propose structured interaction as a new scaling
axis, moving beyond the prevailing paradigm of increasing model parameters. Our
self-evolving framework, AGORA, enables a collaborative ensemble to achieve
reasoning performance exceeding state-of-the-art monolithic systems by up to
4.45 percentage points on challenging mathematical benchmarks. This gain stems
from group emergent ability-the synthesis of collective capabilities
unattainable by isolated models, validating interaction as a scalable driver of
intelligence. Our results position the engineering of collaborative ecosystems
as a vital frontier for capability emergence.

</details>


### [116] [LLM-Adapted Interpretation Framework for Machine Learning Models](https://arxiv.org/abs/2507.21179)
*Yuqi Jin,Zihan Hu,Weiteng Zhang,Weihao Xie,Jianwei Shuai,Xian Shen,Zhen Feng*

Main category: cs.LG

TL;DR: 本文提出LAI - ML框架解决机器学习模型在肌肉减少症风险评估中的黑盒问题，该框架提高了预测准确率并增强推理能力。


<details>
  <summary>Details</summary>
Motivation: 高性能机器学习模型如XGBoost缺乏可解释性，限制临床应用，本研究旨在为肌肉减少症风险评估弥合预测准确性和叙事透明度间的差距。

Method: 提出LAI - ML框架，将训练好的XGBoost模型特征属性转化为概率格式，用大语言模型生成数据忠实的诊断叙事。

Result: LAI - ML框架预测准确率达83%，比基线XGBoost模型高13%，大语言模型能复制教师模型逻辑并在21.7%不一致案例中纠正预测。

Conclusion: LAI - ML有效将不透明的模型预测转化为可信且可解释的临床见解，为医疗AI黑盒问题提供可部署解决方案。

Abstract: Background & Aims: High-performance machine learning models like XGBoost are
often "black boxes," limiting their clinical adoption due to a lack of
interpretability. This study aims to bridge the gap between predictive accuracy
and narrative transparency for sarcopenia risk assessment. Methods: We propose
the LLM-Adapted Interpretation Framework (LAI-ML), a novel knowledge
distillation architecture. LAI-ML transforms feature attributions from a
trained XGBoost model into a probabilistic format using specialized techniques
(HAGA and CACS). A Large Language Model (LLM), guided by a reinforcement
learning loop and case-based retrieval, then generates data-faithful diagnostic
narratives. Results: The LAI-ML framework achieved 83% prediction accuracy,
significantly outperforming the baseline XGBoost model, 13% higher. Notably,
the LLM not only replicated the teacher model's logic but also corrected its
predictions in 21.7% of discordant cases, demonstrating enhanced reasoning.
Conclusion: LAI-ML effectively translates opaque model predictions into
trustworthy and interpretable clinical insights, offering a deployable solution
to the "black-box" problem in medical AI.

</details>


### [117] [MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge](https://arxiv.org/abs/2507.21183)
*Guangchen Lan,Sipeng Zhang,Tianle Wang,Yuwei Zhang,Daoan Zhang,Xinpeng Wei,Xiaoman Pan,Hongming Zhang,Dong-Jun Han,Christopher G. Brinton*

Main category: cs.LG

TL;DR: 提出MaPPO框架用于偏好学习，集成先验奖励知识，能提升大语言模型与人类偏好的对齐性能。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型时代，为了更好地使大语言模型与人类偏好对齐并提升性能。

Method: 提出MaPPO框架，将先验奖励知识整合到最大后验目标中，拓展现有偏好学习范式。

Result: 在三个标准基准测试中，MaPPO在不牺牲计算效率的情况下，一致提升了对齐性能，还可作为插件提升DPO变体性能。

Conclusion: MaPPO是一种有效的偏好优化方法，能提升大语言模型的对齐性能，且无额外超参数，支持离线和在线设置。

Abstract: As the era of large language models (LLMs) on behalf of users unfolds,
Preference Optimization (PO) methods have become a central approach to aligning
LLMs with human preferences and improving performance. We propose Maximum a
Posteriori Preference Optimization (MaPPO), a framework for learning from
preferences that explicitly incorporates prior reward knowledge into the
optimization objective. While existing methods such as Direct Preference
Optimization (DPO) and its variants treat preference learning as a Maximum
Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating
prior reward estimates into a principled Maximum a Posteriori (MaP) objective.
This not only generalizes DPO and its variants, but also enhances alignment by
mitigating the oversimplified binary classification of responses. More
importantly, MaPPO introduces no additional hyperparameter, and supports
preference optimization in both offline and online settings. In addition, MaPPO
can be used as a plugin with consistent improvement on DPO variants, including
widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different
model sizes and model series on three standard benchmarks, including MT-Bench,
AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in
alignment performance without sacrificing computational efficiency.

</details>


### [118] [EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models](https://arxiv.org/abs/2507.21184)
*Haowei Lin,Xiangyu Wang,Jianzhu Ma,Yitao Liang*

Main category: cs.LG

TL;DR: 介绍自动化框架EvoSLD用于发现缩放定律，在多场景评估中表现出色，能加速AI研究。


<details>
  <summary>Details</summary>
Motivation: 传统发现缩放定律需大量人力专业知识和手动实验，需要自动化方法。

Method: 引入EvoSLD框架，利用大语言模型引导的进化算法共同进化符号表达式及其优化例程，处理不同实验设置中的变量和指标。

Result: 在五个真实场景评估中，重现两条人类发现的定律，其他情况超越人类，在测试集上降低归一化均方误差，比基线方法表现更好。

Conclusion: EvoSLD准确性、可解释性和效率高，有加速AI研究的潜力。

Abstract: Scaling laws are fundamental mathematical relationships that predict how
neural network performance evolves with changes in variables such as model
size, dataset size, and computational resources. Traditionally, discovering
these laws requires extensive human expertise and manual experimentation. We
introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that
leverages evolutionary algorithms guided by Large Language Models (LLMs) to
co-evolve symbolic expressions and their optimization routines. Formulated to
handle scaling variables, control variables, and response metrics across
diverse experimental settings, EvoSLD searches for parsimonious, universal
functional forms that minimize fitting errors on grouped data subsets.
Evaluated on five real-world scenarios from recent literature, EvoSLD
rediscovers exact human-derived laws in two cases and surpasses them in others,
achieving up to orders-of-magnitude reductions in normalized mean squared error
on held-out test sets. Compared to baselines like symbolic regression and
ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and
efficiency, highlighting its potential to accelerate AI research. Code is
available at https://github.com/linhaowei1/SLD.

</details>


### [119] [AdaptHetero: Machine Learning Interpretation-Driven Subgroup Adaptation for EHR-Based Clinical Prediction](https://arxiv.org/abs/2507.21197)
*Ling Liao,Eva Aagaard*

Main category: cs.LG

TL;DR: 提出AdaptHetero框架，在三个EHR数据集上评估，能识别模型异质性，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习解释方法在EHR数据中指导亚组特定建模效果受数据复杂性和异质性限制。

Method: 提出AdaptHetero框架，集成SHAP解释和无监督聚类。

Result: 在三个大规模EHR数据集上评估，能识别异质性模型行为。

Conclusion: 该框架可增强临床有意义的亚组特定特征识别，提升预测性能。

Abstract: Machine learning interpretation has primarily been leveraged to build
clinician trust and uncover actionable insights in EHRs. However, the intrinsic
complexity and heterogeneity of EHR data limit its effectiveness in guiding
subgroup-specific modeling. We propose AdaptHetero, a novel MLI-driven
framework that transforms interpretability insights into actionable guidance
for tailoring model training and evaluation across subpopulations within
individual hospital systems. Evaluated on three large-scale EHR datasets -
GOSSIS-1-eICU, WiDS, and MIMIC-IV - AdaptHetero consistently identifies
heterogeneous model behaviors in predicting ICU mortality, in-hospital death,
and hidden hypoxemia. By integrating SHAP-based interpretation and unsupervised
clustering, the framework enhances the identification of clinically meaningful
subgroup-specific characteristics, leading to improved predictive performance.

</details>


### [120] [Embeddings to Diagnosis: Latent Fragility under Agentic Perturbations in Clinical LLMs](https://arxiv.org/abs/2507.21188)
*Raj Krishnan Vijayaraj*

Main category: cs.LG

TL;DR: 提出几何感知评估框架LAPD检测临床大语言模型潜在鲁棒性，发现表面鲁棒性和语义稳定性存在差距，强调几何感知审计重要性。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持的大语言模型在输入有小变化时推理易失败，且标准NLP指标难以检测到这种失败。

Method: 提出LAPD框架，引入LDFR指标，对临床笔记进行结构化提示生成并沿四个轴扰动，计算不同模型的LDFR。

Result: 发现即使表面变化极小，模型也存在潜在脆弱性，在真实临床笔记上验证了LDFR的通用性。

Conclusion: 表面鲁棒性和语义稳定性存在持续差距，几何感知审计对临床AI安全很重要。

Abstract: LLMs for clinical decision support often fail under small but clinically
meaningful input shifts such as masking a symptom or negating a finding,
despite high performance on static benchmarks. These reasoning failures
frequently go undetected by standard NLP metrics, which are insensitive to
latent representation shifts that drive diagnosis instability. We propose a
geometry-aware evaluation framework, LAPD (Latent Agentic Perturbation
Diagnostics), which systematically probes the latent robustness of clinical
LLMs under structured adversarial edits. Within this framework, we introduce
Latent Diagnosis Flip Rate (LDFR), a model-agnostic diagnostic signal that
captures representational instability when embeddings cross decision boundaries
in PCA-reduced latent space. Clinical notes are generated using a structured
prompting pipeline grounded in diagnostic reasoning, then perturbed along four
axes: masking, negation, synonym replacement, and numeric variation to simulate
common ambiguities and omissions. We compute LDFR across both foundation and
clinical LLMs, finding that latent fragility emerges even under minimal
surface-level changes. Finally, we validate our findings on 90 real clinical
notes from the DiReCT benchmark (MIMIC-IV), confirming the generalizability of
LDFR beyond synthetic settings. Our results reveal a persistent gap between
surface robustness and semantic stability, underscoring the importance of
geometry-aware auditing in safety-critical clinical AI.

</details>


### [121] [Capacity-Constrained Continual Learning](https://arxiv.org/abs/2507.21479)
*Zheng Wen,Doina Precup,Benjamin Van Roy,Satinder Singh*

Main category: cs.LG

TL;DR: 本文研究容量受限的线性二次高斯（LQG）顺序预测问题，推导了解决方案并展示如何在子问题间最优分配容量，是容量约束下学习理论研究的第一步。


<details>
  <summary>Details</summary>
Motivation: 现有研究对有限容量智能体如何分配资源以实现最优性能关注较少，本文旨在解决该问题。

Method: 研究容量受限的线性二次高斯（LQG）顺序预测问题。

Result: 在适当技术条件下推导出问题的解决方案，展示了稳态下如何在子问题间最优分配容量。

Conclusion: 本文结果是容量约束下学习系统理论研究的第一步。

Abstract: Any agents we can possibly build are subject to capacity constraints, as
memory and compute resources are inherently finite. However, comparatively
little attention has been dedicated to understanding how agents with limited
capacity should allocate their resources for optimal performance. The goal of
this paper is to shed some light on this question by studying a simple yet
relevant continual learning problem: the capacity-constrained
linear-quadratic-Gaussian (LQG) sequential prediction problem. We derive a
solution to this problem under appropriate technical conditions. Moreover, for
problems that can be decomposed into a set of sub-problems, we also demonstrate
how to optimally allocate capacity across these sub-problems in the steady
state. We view the results of this paper as a first step in the systematic
theoretical study of learning under capacity constraints.

</details>


### [122] [Operator-Based Machine Intelligence: A Hilbert Space Framework for Spectral Learning and Symbolic Reasoning](https://arxiv.org/abs/2507.21189)
*Andrew Kiruluta,Andreas Lemos,Priscilla Burity*

Main category: cs.LG

TL;DR: 本文探索以无限维希尔伯特空间进行机器学习任务，回顾基础概念，给出数学公式，介绍模型并讨论优劣势，最后指明方向。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型基于有限维参数空间和非线性函数逼近，本文探索在无限维希尔伯特空间进行学习任务的替代方案。

Method: 利用泛函分析、信号处理和谱理论工具，回顾再生核希尔伯特空间、谱算子学习和小波域表示等基础概念，给出希尔伯特空间学习的数学公式，介绍基于散射变换和Koopman算子的模型。

Result: 对基于希尔伯特空间的机器学习方法进行了研究和阐述，讨论了与传统神经网络架构相比的优缺点。

Conclusion: 指明了基于希尔伯特信号处理的可扩展和可解释机器学习的发展方向。

Abstract: Traditional machine learning models, particularly neural networks, are rooted
in finite-dimensional parameter spaces and nonlinear function approximations.
This report explores an alternative formulation where learning tasks are
expressed as sampling and computation in infinite dimensional Hilbert spaces,
leveraging tools from functional analysis, signal processing, and spectral
theory. We review foundational concepts such as Reproducing Kernel Hilbert
Spaces (RKHS), spectral operator learning, and wavelet-domain representations.
We present a rigorous mathematical formulation of learning in Hilbert spaces,
highlight recent models based on scattering transforms and Koopman operators,
and discuss advantages and limitations relative to conventional neural
architectures. The report concludes by outlining directions for scalable and
interpretable machine learning grounded in Hilbertian signal processing.

</details>


### [123] [Beyond Neural Networks: Symbolic Reasoning over Wavelet Logic Graph Signals](https://arxiv.org/abs/2507.21190)
*Andrew Kiruluta,Andreas Lemos,Priscilla Burity*

Main category: cs.LG

TL;DR: 提出基于图拉普拉斯小波变换的非神经学习框架，实验表现佳，是图学习的新选择。


<details>
  <summary>Details</summary>
Motivation: 寻找传统基于卷积、循环或注意力的神经网络架构之外，用于图学习的可解释且资源高效的方法。

Method: 在图谱域操作，使用结构化多尺度滤波、非线性收缩和小波系数上的符号逻辑，通过GLWT分解图节点信号，用可解释非线性调制并重组用于下游任务，支持基于符号DSL的组合推理。

Result: 在合成图去噪和语言标记图实验中，与轻量级GNN相比有竞争力，且更透明高效。

Conclusion: 该框架是图学习中深度神经架构的有原则、可解释且资源高效的替代方案。

Abstract: We present a fully non neural learning framework based on Graph Laplacian
Wavelet Transforms (GLWT). Unlike traditional architectures that rely on
convolutional, recurrent, or attention based neural networks, our model
operates purely in the graph spectral domain using structured multiscale
filtering, nonlinear shrinkage, and symbolic logic over wavelet coefficients.
Signals defined on graph nodes are decomposed via GLWT, modulated with
interpretable nonlinearities, and recombined for downstream tasks such as
denoising and token classification. The system supports compositional reasoning
through a symbolic domain-specific language (DSL) over graph wavelet
activations. Experiments on synthetic graph denoising and linguistic token
graphs demonstrate competitive performance against lightweight GNNs with far
greater transparency and efficiency. This work proposes a principled,
interpretable, and resource-efficient alternative to deep neural architectures
for learning on graphs.

</details>


### [124] [Exploring Adaptive Structure Learning for Heterophilic Graphs](https://arxiv.org/abs/2507.21191)
*Garv Kaushik*

Main category: cs.LG

TL;DR: 本文关注提升异质图上GCN性能，提出结构学习方法重连浅GCN边，但方法泛化性和一致性欠佳。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递范式在异质图中难以捕捉非局部同类节点的长距离依赖，导致下游任务性能下降。

Method: 提出结构学习方法，对邻接矩阵参数化以学习非局部节点连接，扩展浅GCN的跳数范围。

Result: 方法在异质图上不具有泛化性，在节点分类任务中表现因图结构而异。

Conclusion: 所提结构学习方法虽尝试解决问题，但存在泛化性和一致性问题。

Abstract: Graph Convolutional Networks (GCNs) gained traction for graph representation
learning, with recent attention on improving performance on heterophilic graphs
for various real-world applications. The localized feature aggregation in a
typical message-passing paradigm hinders the capturing of long-range
dependencies between non-local nodes of the same class. The inherent
connectivity structure in heterophilic graphs often conflicts with information
sharing between distant nodes of same class. We propose structure learning to
rewire edges in shallow GCNs itself to avoid performance degradation in
downstream discriminative tasks due to oversmoothing. Parameterizing the
adjacency matrix to learn connections between non-local nodes and extend the
hop span of shallow GCNs facilitates the capturing of long-range dependencies.
However, our method is not generalizable across heterophilic graphs and
performs inconsistently on node classification task contingent to the graph
structure.

</details>


### [125] [EdgeAgentX-DT: Integrating Digital Twins and Generative AI for Resilient Edge Intelligence in Tactical Networks](https://arxiv.org/abs/2507.21196)
*Abir Ray*

Main category: cs.LG

TL;DR: 介绍了EdgeAgentX-DT框架，它集成数字孪生和生成式AI，通过多层架构提升军事网络边缘智能，实验表现优于EdgeAgentX。


<details>
  <summary>Details</summary>
Motivation: 提升军事网络的边缘智能。

Method: 使用网络数字孪生提供训练验证环境，利用生成式AI方法创建多样对抗场景，采用多层架构。

Result: 实验显示比EdgeAgentX有更快学习收敛、更高网络吞吐量、更低延迟和更强抗干扰能力；案例研究中能维持复杂战术场景的运行性能。

Conclusion: 数字孪生赋能的生成式训练有潜力加强对抗环境下的边缘AI部署。

Abstract: We introduce EdgeAgentX-DT, an advanced extension of the EdgeAgentX framework
that integrates digital twin simulations and generative AI-driven scenario
training to significantly enhance edge intelligence in military networks.
EdgeAgentX-DT utilizes network digital twins, virtual replicas synchronized
with real-world edge devices, to provide a secure, realistic environment for
training and validation. Leveraging generative AI methods, such as diffusion
models and transformers, the system creates diverse and adversarial scenarios
for robust simulation-based agent training. Our multi-layer architecture
includes: (1) on-device edge intelligence; (2) digital twin synchronization;
and (3) generative scenario training. Experimental simulations demonstrate
notable improvements over EdgeAgentX, including faster learning convergence,
higher network throughput, reduced latency, and improved resilience against
jamming and node failures. A case study involving a complex tactical scenario
with simultaneous jamming attacks, agent failures, and increased network loads
illustrates how EdgeAgentX-DT sustains operational performance, whereas
baseline methods fail. These results highlight the potential of
digital-twin-enabled generative training to strengthen edge AI deployments in
contested environments.

</details>


### [126] [Uncovering Gradient Inversion Risks in Practical Language Model Training](https://arxiv.org/abs/2507.21198)
*Xinguo Feng,Zhongkui Ma,Zihan Wang,Eu Joe Chegne,Mengyao Ma,Alsharif Abuadbba,Guangdong Bai*

Main category: cs.LG

TL;DR: 提出针对语言模型联邦学习的梯度反转攻击方法Grab，能有效恢复私有训练数据，提升对隐私威胁的理解。


<details>
  <summary>Details</summary>
Motivation: 梯度反转攻击在语言模型中威胁常被低估，虽在连续领域有威胁，但语言模型因文本离散性等问题未受足够重视。

Method: 提出Grab攻击方法，有两个交替优化过程，包括层间丢弃掩码同时优化和离散优化。

Result: 能恢复高达92.9%的私有训练数据，在基准和实际设置中比利用离散优化和辅助模型的策略分别提高28.9%和48.5%的恢复率。

Conclusion: Grab有助于理解语言模型联邦学习训练模式中的隐私威胁。

Abstract: The gradient inversion attack has been demonstrated as a significant privacy
threat to federated learning (FL), particularly in continuous domains such as
vision models. In contrast, it is often considered less effective or highly
dependent on impractical training settings when applied to language models, due
to the challenges posed by the discrete nature of tokens in text data. As a
result, its potential privacy threats remain largely underestimated, despite FL
being an emerging training method for language models. In this work, we propose
a domain-specific gradient inversion attack named Grab (gradient inversion with
hybrid optimization). Grab features two alternating optimization processes to
address the challenges caused by practical training settings, including a
simultaneous optimization on dropout masks between layers for improved token
recovery and a discrete optimization for effective token sequencing. Grab can
recover a significant portion (up to 92.9% recovery rate) of the private
training data, outperforming the attack strategy of utilizing discrete
optimization with an auxiliary model by notable improvements of up to 28.9%
recovery rate in benchmark settings and 48.5% recovery rate in practical
settings. Grab provides a valuable step forward in understanding this privacy
threat in the emerging FL training mode of language models.

</details>


### [127] [Learning from Limited and Imperfect Data](https://arxiv.org/abs/2507.21205)
*Harsh Rangwani*

Main category: cs.LG

TL;DR: 论文旨在开发能从现实世界有限和不完美数据中学习的深度神经网络实用算法，分四个部分分别处理不同场景。


<details>
  <summary>Details</summary>
Motivation: 现实数据分布与精心整理的数据集差异大，现有算法在长尾不平衡和分布偏移的不完美数据集上表现不佳，需开发能从多样现实数据分布学习的鲁棒算法。

Method: 论文分为四部分，分别是从长尾数据学习生成模型、通过归纳正则化方案使尾部类别有效泛化、开发从有限标注的长尾数据学习的优化相关指标的算法、实现模型在极少到零标注样本的不同领域的高效域适应。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: The distribution of data in the world (eg, internet, etc.) significantly
differs from the well-curated datasets and is often over-populated with samples
from common categories. The algorithms designed for well-curated datasets
perform suboptimally when used for learning from imperfect datasets with
long-tailed imbalances and distribution shifts. To expand the use of deep
models, it is essential to overcome the labor-intensive curation process by
developing robust algorithms that can learn from diverse, real-world data
distributions. Toward this goal, we develop practical algorithms for Deep
Neural Networks which can learn from limited and imperfect data present in the
real world. This thesis is divided into four segments, each covering a scenario
of learning from limited or imperfect data. The first part of the thesis
focuses on Learning Generative Models from Long-Tail Data, where we mitigate
the mode-collapse and enable diverse aesthetic image generations for tail
(minority) classes. In the second part, we enable effective generalization on
tail classes through Inductive Regularization schemes, which allow tail classes
to generalize as effectively as the head classes without requiring explicit
generation of images. In the third part, we develop algorithms for Optimizing
Relevant Metrics for learning from long-tailed data with limited annotation
(semi-supervised), followed by the fourth part, which focuses on the Efficient
Domain Adaptation of the model to various domains with very few to zero labeled
samples.

</details>


### [128] [Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors](https://arxiv.org/abs/2507.21260)
*Amartya Banerjee,Xingyu Xu,Caroline Moosmüller,Harlin Lee*

Main category: cs.LG

TL;DR: 提出Adam - PnP框架引导预训练蛋白质扩散模型，减少手动超参数调整，实验显示重建精度显著提高。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合多源噪声实验数据引导蛋白质扩散模型时存在挑战，需精确了解噪声水平和手动调整权重。

Method: 引入Adam - PnP框架，具有自适应噪声估计方案和动态模态加权机制，并集成到扩散过程中。

Result: 在复杂重建任务实验中，使用Adam - PnP显著提高了精度。

Conclusion: Adam - PnP框架能有效引导预训练蛋白质扩散模型，减少手动超参数调整，提升重建精度。

Abstract: In an inverse problem, the goal is to recover an unknown parameter (e.g., an
image) that has typically undergone some lossy or noisy transformation during
measurement. Recently, deep generative models, particularly diffusion models,
have emerged as powerful priors for protein structure generation. However,
integrating noisy experimental data from multiple sources to guide these models
remains a significant challenge. Existing methods often require precise
knowledge of experimental noise levels and manually tuned weights for each data
modality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that
guides a pre-trained protein diffusion model using gradients from multiple,
heterogeneous experimental sources. Our framework features an adaptive noise
estimation scheme and a dynamic modality weighting mechanism integrated into
the diffusion process, which reduce the need for manual hyperparameter tuning.
Experiments on complex reconstruction tasks demonstrate significantly improved
accuracy using Adam-PnP.

</details>


### [129] [Deep Polynomial Chaos Expansion](https://arxiv.org/abs/2507.21273)
*Johannes Exenberger,Sascha Ranftl,Robert Peharz*

Main category: cs.LG

TL;DR: 结合多项式混沌展开（PCE）和概率电路思想提出DeepPCE，解决PCE高维扩展性问题，且有良好预测性能与精确统计推断能力。


<details>
  <summary>Details</summary>
Motivation: PCE在高维问题中因基函数数量随参数数量指数增长，扩展性不佳，需解决该问题。

Method: 将PCE与概率电路的思想相结合，提出DeepPCE。

Result: DeepPCE在预测性能上与多层感知器（MLPs）相当。

Conclusion: DeepPCE能有效扩展到高维输入空间，同时保留PCE通过简单前向传播进行精确统计推断的能力。

Abstract: Polynomial chaos expansion (PCE) is a classical and widely used surrogate
modeling technique in physical simulation and uncertainty quantification. By
taking a linear combination of a set of basis polynomials - orthonormal with
respect to the distribution of uncertain input parameters - PCE enables
tractable inference of key statistical quantities, such as (conditional) means,
variances, covariances, and Sobol sensitivity indices, which are essential for
understanding the modeled system and identifying influential parameters and
their interactions. As the number of basis functions grows exponentially with
the number of parameters, PCE does not scale well to high-dimensional problems.
We address this challenge by combining PCE with ideas from probabilistic
circuits, resulting in the deep polynomial chaos expansion (DeepPCE) - a deep
generalization of PCE that scales effectively to high-dimensional input spaces.
DeepPCE achieves predictive performance comparable to that of multi-layer
perceptrons (MLPs), while retaining PCE's ability to compute exact statistical
inferences via simple forward passes.

</details>


### [130] [Large Language Model-Enhanced Reinforcement Learning for Diverse and Novel Recommendations](https://arxiv.org/abs/2507.21274)
*Jiin Woo,Alireza Bagheri Garakani,Tianchen Zhou,Zhishen Huang,Yan Gao*

Main category: cs.LG

TL;DR: 提出LAAC方法，结合大语言模型和系统数据提升推荐系统多样性、新颖性和准确性，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统重点击相关性，强化学习依赖随机探索可能不符用户兴趣，需提升多样性和新颖性。

Method: 提出LAAC方法，用大语言模型作参考策略推荐新颖物品，训练轻量级策略优化建议，采用双层优化训练，应用正则化缓解高估问题。

Result: 在真实数据集实验中，LAAC在多样性、新颖性和准确性上优于现有基线，在不平衡数据上表现稳健，无需昂贵微调。

Conclusion: LAAC能有效结合大语言模型知识，提升推荐系统性能。

Abstract: In recommendation systems, diversity and novelty are essential for capturing
varied user preferences and encouraging exploration, yet many systems
prioritize click relevance. While reinforcement learning (RL) has been explored
to improve diversity, it often depends on random exploration that may not align
with user interests. We propose LAAC (LLM-guided Adversarial Actor Critic), a
novel method that leverages large language models (LLMs) as reference policies
to suggest novel items, while training a lightweight policy to refine these
suggestions using system-specific data. The method formulates training as a
bilevel optimization between actor and critic networks, enabling the critic to
selectively favor promising novel actions and the actor to improve its policy
beyond LLM recommendations. To mitigate overestimation of unreliable LLM
suggestions, we apply regularization that anchors critic values for unexplored
items close to well-estimated dataset actions. Experiments on real-world
datasets show that LAAC outperforms existing baselines in diversity, novelty,
and accuracy, while remaining robust on imbalanced data, effectively
integrating LLM knowledge without expensive fine-tuning.

</details>


### [131] [Blending data and physics for reduced-order modeling of systems with spatiotemporal chaotic dynamics](https://arxiv.org/abs/2507.21299)
*Alex Guo,Michael D. Graham*

Main category: cs.LG

TL;DR: 本文提出数据与全阶模型结合的混合降阶模型，用于时空混沌动力学建模，在不同数据场景和错误全阶模型下都有更好的时间序列预测效果。


<details>
  <summary>Details</summary>
Motivation: 利用已知物理（全阶模型）提升数据驱动技术对混沌动力学系统降阶建模的预测能力。

Method: 开发结合数据和全阶模型的混合降阶模型，通过自动编码器找到不变流形坐标，将全阶模型向量场投影到不变流形，用动态数据校正或作为贝叶斯先验更新，使用神经常微分方程方法。

Result: 在丰富数据、稀缺数据和错误全阶模型的场景下，混合方法比仅用数据的方法有显著改进的时间序列预测。

Conclusion: 结合数据和全阶模型的混合降阶模型能有效提升对时空混沌动力学的预测能力。

Abstract: While data-driven techniques are powerful tools for reduced-order modeling of
systems with chaotic dynamics, great potential remains for leveraging known
physics (i.e. a full-order model (FOM)) to improve predictive capability. We
develop a hybrid reduced order model (ROM), informed by both data and FOM, for
evolving spatiotemporal chaotic dynamics on an invariant manifold whose
coordinates are found using an autoencoder. This approach projects the vector
field of the FOM onto the invariant manifold; then, this physics-derived vector
field is either corrected using dynamic data, or used as a Bayesian prior that
is updated with data. In both cases, the neural ordinary differential equation
approach is used. We consider simulated data from the Kuramoto-Sivashinsky and
complex Ginzburg-Landau equations. Relative to the data-only approach, for
scenarios of abundant data, scarce data, and even an incorrect FOM (i.e.
erroneous parameter values), the hybrid approach yields substantially improved
time-series predictions.

</details>


### [132] [DEM-NeRF: A Neuro-Symbolic Method for Scientific Discovery through Physics-Informed Simulation](https://arxiv.org/abs/2507.21350)
*Wenkai Tan,Alvaro Velasquez,Houbing Song*

Main category: cs.LG

TL;DR: 提出一种新的神经符号框架，直接从稀疏多视图图像序列重建和模拟弹性物体，结合NeRF与PINN，采用能量约束架构提升准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决纯经验方法可能偏离物理原则，传统数值求解器需完整几何信息且成本高的问题，实现从稀疏图像序列重建和模拟弹性物体。

Method: 将用于物体重建的神经辐射场（NeRF）与结合弹性偏微分方程的物理信息神经网络（PINN）集成，采用能量约束的物理信息神经网络架构处理复杂边界和初始条件。

Result: 学习变形物体的时空表示，结合图像监督和符号物理约束。

Conclusion: 该方法能提升模拟准确性和结果的可解释性。

Abstract: Neural networks have emerged as a powerful tool for modeling physical
systems, offering the ability to learn complex representations from limited
data while integrating foundational scientific knowledge. In particular,
neuro-symbolic approaches that combine data-driven learning, the neuro, with
symbolic equations and rules, the symbolic, address the tension between methods
that are purely empirical, which risk straying from established physical
principles, and traditional numerical solvers that demand complete geometric
knowledge and can be prohibitively expensive for high-fidelity simulations. In
this work, we present a novel neuro-symbolic framework for reconstructing and
simulating elastic objects directly from sparse multi-view image sequences,
without requiring explicit geometric information. Specifically, we integrate a
neural radiance field (NeRF) for object reconstruction with physics-informed
neural networks (PINN) that incorporate the governing partial differential
equations of elasticity. In doing so, our method learns a spatiotemporal
representation of deforming objects that leverages both image supervision and
symbolic physical constraints. To handle complex boundary and initial
conditions, which are traditionally confronted using finite element methods,
boundary element methods, or sensor-based measurements, we employ an
energy-constrained Physics-Informed Neural Network architecture. This design
enhances both simulation accuracy and the explainability of results.

</details>


### [133] [A Contrastive Diffusion-based Network (CDNet) for Time Series Classification](https://arxiv.org/abs/2507.21357)
*Yaoyu Zhang,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: 提出CDNet网络，通过扩散过程生成正负样本提升时间序列分类性能，实验表明其能显著提升SOTA分类器效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在类相似、多模态分布和噪声等挑战性数据条件下性能下降，需改进。

Method: 提出CDNet网络，通过学习扩散过程生成正负样本，采用基于CNN机制实现去噪和模式覆盖，结合不确定性加权复合损失进行训练。

Result: 在UCR Archive和模拟数据集上的大量实验表明，CDNet显著提升了SOTA深度学习分类器的性能。

Conclusion: CDNet能有效增强现有分类器，尤其在噪声、相似和多模态条件下表现出色。

Abstract: Deep learning models are widely used for time series classification (TSC) due
to their scalability and efficiency. However, their performance degrades under
challenging data conditions such as class similarity, multimodal distributions,
and noise. To address these limitations, we propose CDNet, a Contrastive
Diffusion-based Network that enhances existing classifiers by generating
informative positive and negative samples via a learned diffusion process.
Unlike traditional diffusion models that denoise individual samples, CDNet
learns transitions between samples--both within and across classes--through
convolutional approximations of reverse diffusion steps. We introduce a
theoretically grounded CNN-based mechanism to enable both denoising and mode
coverage, and incorporate an uncertainty-weighted composite loss for robust
training. Extensive experiments on the UCR Archive and simulated datasets
demonstrate that CDNet significantly improves state-of-the-art (SOTA) deep
learning classifiers, particularly under noisy, similar, and multimodal
conditions.

</details>


### [134] [Efficient Neural Combinatorial Optimization Solver for the Min-max Heterogeneous Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2507.21386)
*Xuan Wu,Di Wang,Chunguo Wu,Kaifang Qi,Chunyan Miao,Yubin Xiao,Jian Zhang,You Zhou*

Main category: cs.LG

TL;DR: 提出高效NCO求解器ECHO解决MMHCVRP，实验表明其性能优于现有求解器，消融实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 多数NCO求解器聚焦单车辆VRP变体，现有MMHCVRP求解器有决策短视、忽略关键特性等问题，性能不佳。

Method: 使用双模态节点编码器捕捉节点局部拓扑关系；采用无参数交叉注意力机制减少短视决策；引入定制数据增强策略稳定强化学习训练过程。

Result: ECHO在不同车辆和节点数量下均优于现有NCO求解器，在规模和分布模式上有良好泛化能力。

Conclusion: ECHO有效解决MMHCVRP，所提方法有效。

Abstract: Numerous Neural Combinatorial Optimization (NCO) solvers have been proposed
to address Vehicle Routing Problems (VRPs). However, most of these solvers
focus exclusively on single-vehicle VRP variants, overlooking the more
realistic min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP),
which involves multiple vehicles. Existing MMHCVRP solvers typically select a
vehicle and its next node to visit at each decoding step, but often make myopic
decoding decisions and overlook key properties of MMHCVRP, including local
topological relationships, vehicle permutation invariance, and node symmetry,
resulting in suboptimal performance. To better address these limitations, we
propose ECHO, an efficient NCO solver. First, ECHO exploits the proposed
dual-modality node encoder to capture local topological relationships among
nodes. Subsequently, to mitigate myopic decisions, ECHO employs the proposed
Parameter-Free Cross-Attention mechanism to prioritize the vehicle selected in
the preceding decoding step. Finally, leveraging vehicle permutation invariance
and node symmetry, we introduce a tailored data augment strategy for MMHCVRP to
stabilize the Reinforcement Learning training process. To assess the
performance of ECHO, we conduct extensive experiments. The experimental results
demonstrate that ECHO outperforms state-of-the-art NCO solvers across varying
numbers of vehicles and nodes, and exhibits well-performing generalization
across both scales and distribution patterns. Finally, ablation studies
validate the effectiveness of all proposed methods.

</details>


### [135] [Systolic Array-based Accelerator for State-Space Models](https://arxiv.org/abs/2507.21394)
*Shiva Raja,Cansu Demirkiran,Aakash Sarkar,Milos Popovic,Ajay Joshi*

Main category: cs.LG

TL;DR: 本文介绍专门硬件加速器EpochCore以加速状态空间模型（SSMs），能提升长序列任务推理能效和吞吐量，性能、能效有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络模型处理长序列有局限，而SSM虽更高效但在传统CPU和GPU上训练和推理计算与内存开销大，因此需要专门硬件加速器。

Method: 提出基于脉动阵列（SAs）的EpochCore，设计多功能处理单元LIMA - PE，提出新数据流ProDF。

Result: EpochCore相比传统基于SA的加速器性能平均提升250倍、能效提升45倍，面积成本增加2倍，在LRA数据集上相比GPU内核操作延迟/推理约改善2000倍。

Conclusion: EpochCore能有效提升SSM模型在长序列任务推理中的性能、能效并降低延迟。

Abstract: Sequence modeling is crucial for AI to understand temporal data and detect
complex time-dependent patterns. While recurrent neural networks (RNNs),
convolutional neural networks (CNNs), and Transformers have advanced in
capturing long-range dependencies, they struggle with achieving high accuracy
with very long sequences due to limited memory retention (fixed context
window). State-Space Models (SSMs) leverage exponentially decaying memory
enabling lengthy context window and so they process very long data sequences
more efficiently than recurrent and Transformer-based models. Unlike
traditional neural models like CNNs and RNNs, SSM-based models require solving
differential equations through continuous integration, making training and
inference both compute- and memory-intensive on conventional CPUs and GPUs. In
this paper we introduce a specialized hardware accelerator, EpochCore, for
accelerating SSMs. EpochCore is based on systolic arrays (SAs) and is designed
to enhance the energy efficiency and throughput of inference of SSM-based
models for long-range sequence tasks. Within the SA, we propose a versatile
processing element (PE) called LIMA-PE to perform traditional and specialized
MAC operations to support traditional DNNs and SSMs. To complement the
EpochCore microarchitecture, we propose a novel dataflow, ProDF, which enables
highly efficient execution of SSM-based models. By leveraging the LIMA-PE
microarchitecture and ProDF, EpochCore achieves on average 250x gains in
performance and 45x improvement in energy efficiency, at the expense of 2x
increase in area cost over traditional SA-based accelerators, and around
~2,000x improvement in latency/inference on LRA datasets compared to GPU kernel
operations.

</details>


### [136] [Enabling Pareto-Stationarity Exploration in Multi-Objective Reinforcement Learning: A Multi-Objective Weighted-Chebyshev Actor-Critic Approach](https://arxiv.org/abs/2507.21397)
*Fnu Hairi,Jiao Yang,Tianchen Zhou,Haibo Yang,Chaosheng Dong,Fan Yang,Michinari Momma,Yan Gao,Jia Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In many multi-objective reinforcement learning (MORL) applications, being
able to systematically explore the Pareto-stationary solutions under multiple
non-convex reward objectives with theoretical finite-time sample complexity
guarantee is an important and yet under-explored problem. This motivates us to
take the first step and fill the important gap in MORL. Specifically, in this
paper, we propose a \uline{M}ulti-\uline{O}bjective weighted-\uline{CH}ebyshev
\uline{A}ctor-critic (MOCHA) algorithm for MORL, which judiciously integrates
the weighted-Chebychev (WC) and actor-critic framework to enable
Pareto-stationarity exploration systematically with finite-time sample
complexity guarantee. Sample complexity result of MOCHA algorithm reveals an
interesting dependency on $p_{\min}$ in finding an $\epsilon$-Pareto-stationary
solution, where $p_{\min}$ denotes the minimum entry of a given weight vector
$\mathbf{p}$ in WC-scarlarization. By carefully choosing learning rates, the
sample complexity for each exploration can be
$\tilde{\mathcal{O}}(\epsilon^{-2})$. Furthermore, simulation studies on a
large KuaiRand offline dataset, show that the performance of MOCHA algorithm
significantly outperforms other baseline MORL approaches.

</details>


### [137] [Data Leakage and Redundancy in the LIT-PCBA Benchmark](https://arxiv.org/abs/2507.21404)
*Amber Huang,Ian Scott Knight,Slava Naprienko*

Main category: cs.LG

TL;DR: 研究指出LIT - PCBA基准数据集存在数据泄露、重复和结构冗余等问题，导致模型只能记忆而非泛化，简单的记忆基线模型就能胜过先进模型，该基准不适合原用途。


<details>
  <summary>Details</summary>
Motivation: 对广泛使用的LIT - PCBA基准数据集进行审核，以评估其是否适合用于公平的模型评估。

Method: 审核数据集，识别训练集和验证集中的重复样本、查询集中的泄露样本，计算结构冗余程度，并实现基于记忆的基线模型。

Result: 发现大量数据重复和泄露问题，结构冗余严重，简单的记忆基线模型在LIT - PCBA上胜过先进模型。

Conclusion: LIT - PCBA不适合用于其预期目的，此前基于它的结果存疑，呼吁开发更严格可靠的数据集。

Abstract: LIT-PCBA is a widely used benchmark for virtual screening, but our audit
reveals it is fundamentally compromised. The dataset suffers from egregious
data leakage, rampant duplication, and pervasive analog redundancy -- flaws
that invalidate its use for fair model evaluation. Notably, we identify 2,491
inactives duplicated across training and validation sets, and thousands more
repeated within individual data splits (2,945 in training, 789 in validation).
Critically, three ligands in the query set -- meant to represent unseen test
cases -- are leaked: two appear in the training set, one in validation.
Structural redundancy compounds these issues: for some targets, over 80% of
query ligands are near duplicates, with Tanimoto similarity >= 0.9. In ALDH1
alone, we find 323 highly similar active pairs between training and validation
sets, invalidating claims of chemical diversity. These and other flaws
collectively cause models trained on LIT-PCBA to memorize rather than
generalize. To demonstrate the consequences of these data integrity failures,
we implement a trivial memorization-based baseline -- using no learning, no
physics, and no modeling -- that outperforms state-of-the-art models, including
deep neural networks like CHEESE, on LIT-PCBA simply by exploiting these
artifacts. Our findings render the benchmark unfit for its intended purpose and
call into question previous results based on its use. We share this audit to
raise awareness and provide tooling to help the community develop more rigorous
and reliable datasets going forward. All scripts necessary to reproduce our
audit and the baseline implementation are available at:
https://github.com/sievestack/LIT-PCBA-audit

</details>


### [138] [Torque-based Graph Surgery:Enhancing Graph Neural Networks with Hierarchical Rewiring](https://arxiv.org/abs/2507.21422)
*Sujia Huang,Lele Fu,Zhen Cui,Tong Zhang,Na Song,Bo Huang*

Main category: cs.LG

TL;DR: 提出基于扭矩驱动的分层重连策略改进图神经网络在异质图中的表示学习并增强对噪声图的鲁棒性，实验效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图结构中固有的交互对图神经网络消息传递过程不友好，需要开发图重连方法。

Method: 受经典力学中扭矩概念启发，定义干扰感知扭矩度量，结合结构距离和能量分数量化边的扰动，据此分层重新配置每层的感受野，修剪高扭矩边并添加低扭矩边。

Result: 在基准数据集上的广泛评估表明，该方法在异质图和同质图上均超越了现有方法，在噪声图上也能保持高精度。

Conclusion: 所提出的扭矩驱动分层重连策略能有效改善图神经网络的表示学习，增强其对噪声图的鲁棒性。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning from
graph-structured data, leveraging message passing to diffuse information and
update node representations. However, most efforts have suggested that native
interactions encoded in the graph may not be friendly for this process,
motivating the development of graph rewiring methods. In this work, we propose
a torque-driven hierarchical rewiring strategy, inspired by the notion of
torque in classical mechanics, dynamically modulating message passing to
improve representation learning in heterophilous graphs and enhance robustness
against noisy graphs. Specifically, we define an interference-aware torque
metric that integrates structural distance and energy scores to quantify the
perturbation induced by edges, thereby encouraging each node to aggregate
information from its nearest low-energy neighbors. We use the metric to
hierarchically reconfigure the receptive field of each layer by judiciously
pruning high-torque edges and adding low-torque links, suppressing propagation
noise and boosting pertinent signals. Extensive evaluations on benchmark
datasets show that our approach surpasses state-of-the-art methods on both
heterophilous and homophilous graphs, and maintains high accuracy on noisy
graph.

</details>


### [139] [MemShare: Memory Efficient Inference for Large Reasoning Models through KV Cache Reuse](https://arxiv.org/abs/2507.21433)
*Kaiwen Chen,Xin Tan,Minchen Yu,Hong Xu*

Main category: cs.LG

TL;DR: 针对大推理模型推理时内存开销大问题，提出MemShare方法，实验显示可提升吞吐量并保持精度。


<details>
  <summary>Details</summary>
Motivation: 大推理模型生成思维链序列时内存开销大，且中间推理步骤有相似性。

Method: 提出MemShare方法，用协同过滤算法识别可复用KV缓存块，实现零拷贝缓存复用。

Result: MemShare相比现有KV缓存管理方法，吞吐量最高提升84.79%，且精度更好。

Conclusion: MemShare能有效减少内存开销，提升吞吐量并保持精度。

Abstract: Large Reasoning Models (LRMs) have achieved significant advances in
mathematical reasoning and formal logic tasks. However, their tendency to
generate lengthy chain-of-thought sequences leads to substantial memory
overhead during inference. We observe that LRMs frequently produce highly
similar intermediate reasoning steps, which correspond to similar KV cache
states across layers. Motivated by this observation, we propose MemShare, a
novel KV cache management approach that effectively reduces memory overhead.
MemShare employs a collaborative filtering algorithm to efficiently identify
reusable KV cache blocks and enables zero copy cache reuse to significantly
reduce memory overhead, improve throughput while maintaining accuracy.
Experimental results demonstrate that MemShare delivers up to 84.79\%
improvement in throughput while maintaining better accuracy compared to
existing KV cache management methods.

</details>


### [140] [PVD-ONet: A Multi-scale Neural Operator Method for Singularly Perturbed Boundary Layer Problems](https://arxiv.org/abs/2507.21437)
*Tiantian Sun,Jian Zu*

Main category: cs.LG

TL;DR: 提出PVD - Net和PVD - ONet两种框架解决奇异摄动问题，分稳定和高精度版本，实验显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息神经网络和物理信息DeepONet在解决奇异摄动问题时难以收敛，需新方法。

Method: 提出PVD - Net和PVD - ONet框架，针对不同需求设计不同版本，PVD - ONet通过组装多个DeepONet模块实现算子学习。

Result: 数值实验表明，所提方法在多种误差指标下均优于现有基线。

Conclusion: 所提方法为多尺度问题提供了有力新途径。

Abstract: Physics-informed neural networks and Physics-informed DeepONet excel in
solving partial differential equations; however, they often fail to converge
for singularly perturbed problems. To address this, we propose two novel
frameworks, Prandtl-Van Dyke neural network (PVD-Net) and its operator learning
extension Prandtl-Van Dyke Deep Operator Network (PVD-ONet), which rely solely
on governing equations without data. To address varying task-specific
requirements, both PVD-Net and PVD-ONet are developed in two distinct versions,
tailored respectively for stability-focused and high-accuracy modeling. The
leading-order PVD-Net adopts a two-network architecture combined with Prandtl's
matching condition, targeting stability-prioritized scenarios. The high-order
PVD-Net employs a five-network design with Van Dyke's matching principle to
capture fine-scale boundary layer structures, making it ideal for high-accuracy
scenarios. PVD-ONet generalizes PVD-Net to the operator learning setting by
assembling multiple DeepONet modules, directly mapping initial conditions to
solution operators and enabling instant predictions for an entire family of
boundary layer problems without retraining. Numerical experiments on various
models show that our proposed methods consistently outperform existing
baselines under various error metrics, thereby offering a powerful new approach
for multi-scale problems.

</details>


### [141] [Retrieve-Augmented Generation for Speeding up Diffusion Policy without Additional Training](https://arxiv.org/abs/2507.21452)
*Sodtavilan Odonchimed,Tatsuya Matsushima,Simon Holk,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.LG

TL;DR: 提出RAGDP框架加速预训练Diffusion Policies推理，无需额外训练，提升了准确率和速度的权衡。


<details>
  <summary>Details</summary>
Motivation: Diffusion Policies生成时间长，基于知识蒸馏的方法训练时间久，需新方法加速推理。

Method: RAGDP通过DP编码器编码观察 - 动作对构建专家演示向量数据库，推理时嵌入当前观察提取最相似专家动作，结合中间去噪步骤减少扩散步骤。

Result: 使用RAGDP结合基础模型和现有加速方法，无需额外训练提升了准确率和速度权衡，加速20倍时准确率比CP等蒸馏模型高7%。

Conclusion: RAGDP能有效加速预训练DP推理，在准确率和速度上表现良好。

Abstract: Diffusion Policies (DPs) have attracted attention for their ability to
achieve significant accuracy improvements in various imitation learning tasks.
However, DPs depend on Diffusion Models, which require multiple noise removal
steps to generate a single action, resulting in long generation times. To solve
this problem, knowledge distillation-based methods such as Consistency Policy
(CP) have been proposed. However, these methods require a significant amount of
training time, especially for difficult tasks. In this study, we propose RAGDP
(Retrieve-Augmented Generation for Diffusion Policies) as a novel framework
that eliminates the need for additional training using a knowledge base to
expedite the inference of pre-trained DPs. In concrete, RAGDP encodes
observation-action pairs through the DP encoder to construct a vector database
of expert demonstrations. During inference, the current observation is
embedded, and the most similar expert action is extracted. This extracted
action is combined with an intermediate noise removal step to reduce the number
of steps required compared to the original diffusion step. We show that by
using RAGDP with the base model and existing acceleration methods, we improve
the accuracy and speed trade-off with no additional training. Even when
accelerating the models 20 times, RAGDP maintains an advantage in accuracy,
with a 7% increase over distillation models such as CP.

</details>


### [142] [Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning](https://arxiv.org/abs/2507.21494)
*Wenxuan Bao,Ruxi Deng,Ruizhong Qiu,Tianxin Wei,Hanghang Tong,Jingrui He*

Main category: cs.LG

TL;DR: 提出Latte框架用于解决分散式环境下测试时自适应问题，实验证明其性能优越且成本低。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法适用于单领域和数据丰富场景，在分散式环境中存在数据有限和缺乏个性化的问题。

Method: 每个客户端维护本地和外部内存，在服务器协调下从相似客户端获取原型扩展内存，利用嵌入相似度和不确定性进行本地自适应。

Result: 理论分析表明Latte能有效利用分布内客户端且对分布外客户端有鲁棒性，实验验证在分散式环境中性能优越，通信和计算成本可忽略不计。

Conclusion: Latte框架在分散式环境下的测试时自适应问题上表现出色。

Abstract: Test-time adaptation with pre-trained vision-language models has gained
increasing attention for addressing distribution shifts during testing. Among
these approaches, memory-based algorithms stand out due to their training-free
nature and ability to leverage historical test data. However, existing
test-time adaptation methods are typically designed for a single domain with
abundant data. In decentralized settings such as federated learning, applying
these methods individually to each client suffers from limited test data, while
directly sharing a single global memory via the server prevents proper
personalization to each client's unique distribution. To address this, we
propose Latte, a novel framework where each client maintains a local memory to
store embeddings from its own historical test data and an external memory to
store class prototypes from other relevant clients. During communication, each
client retrieves prototypes from similar clients under the server's
coordination to expand its memory. For local adaptation, Latte utilizes both
embedding similarity and uncertainty to enhance model performance. Our
theoretical analysis shows that Latte effectively leverages in-distribution
clients while remaining robust to out-of-distribution clients. Extensive
experiments on domain adaptation and corruption benchmarks validate that Latte
achieves superior performance in decentralized settings, while introducing only
negligible communication and computation costs. Our code is available at
https://github.com/baowenxuan/Latte .

</details>


### [143] [Evaluation and Benchmarking of LLM Agents: A Survey](https://arxiv.org/abs/2507.21504)
*Mahmoud Mohammadi,Yipeng Li,Jane Lo,Wendy Yip*

Main category: cs.LG

TL;DR: 本文对大语言模型（LLM）智能体评估领域进行综述，提出二维分类法，指出企业特定挑战并明确未来研究方向，为评估提供框架。


<details>
  <summary>Details</summary>
Motivation: LLM智能体兴起但评估复杂且发展不足，需对评估领域进行梳理。

Method: 提出二维分类法，从评估目标和评估过程对现有工作进行组织。

Result: 梳理了评估领域，指出企业特定挑战，明确未来研究方向。

Conclusion: 为LLM智能体评估的碎片化局面带来清晰认识，提供系统评估框架，助力实际部署。

Abstract: The rise of LLM-based agents has opened new frontiers in AI applications, yet
evaluating these agents remains a complex and underdeveloped area. This survey
provides an in-depth overview of the emerging field of LLM agent evaluation,
introducing a two-dimensional taxonomy that organizes existing work along (1)
evaluation objectives -- what to evaluate, such as agent behavior,
capabilities, reliability, and safety -- and (2) evaluation process -- how to
evaluate, including interaction modes, datasets and benchmarks, metric
computation methods, and tooling. In addition to taxonomy, we highlight
enterprise-specific challenges, such as role-based access to data, the need for
reliability guarantees, dynamic and long-horizon interactions, and compliance,
which are often overlooked in current research. We also identify future
research directions, including holistic, more realistic, and scalable
evaluation. This work aims to bring clarity to the fragmented landscape of
agent evaluation and provide a framework for systematic assessment, enabling
researchers and practitioners to evaluate LLM agents for real-world deployment.

</details>


### [144] [Hierarchical Stochastic Differential Equation Models for Latent Manifold Learning in Neural Time Series](https://arxiv.org/abs/2507.21531)
*Pedram Rajaei,Maryam Ostadsharif Memar,Navid Ziaei,Behzad Nazari,Ali Yousefi*

Main category: cs.LG

TL;DR: 提出分层随机微分方程（SDE）模型，平衡计算效率和可解释性，验证能准确恢复潜在流形结构。


<details>
  <summary>Details</summary>
Motivation: 现有潜在动力学变量模型存在局限性，需平衡计算效率和可解释性的新模型。

Method: 提出新的分层SDE模型，用布朗桥SDE对潜空间建模，推导训练和推理程序。

Result: 推理计算成本与观测数据长度呈线性关系，在合成数据和神经记录上验证能准确恢复潜在流形结构。

Conclusion: 新模型能有效恢复潜在流形结构，且随数据维度增加可有效扩展。

Abstract: The manifold hypothesis suggests that high-dimensional neural time series lie
on a low-dimensional manifold shaped by simpler underlying dynamics. To uncover
this structure, latent dynamical variable models such as state-space models,
recurrent neural networks, neural ordinary differential equations, and Gaussian
Process Latent Variable Models are widely used. We propose a novel hierarchical
stochastic differential equation (SDE) model that balances computational
efficiency and interpretability, addressing key limitations of existing
methods. Our model assumes the trajectory of a manifold can be reconstructed
from a sparse set of samples from the manifold trajectory. The latent space is
modeled using Brownian bridge SDEs, with points - specified in both time and
value - sampled from a multivariate marked point process. These Brownian
bridges define the drift of a second set of SDEs, which are then mapped to the
observed data. This yields a continuous, differentiable latent process capable
of modeling arbitrarily complex time series as the number of manifold points
increases. We derive training and inference procedures and show that the
computational cost of inference scales linearly with the length of the
observation data. We then validate our model on both synthetic data and neural
recordings to demonstrate that it accurately recovers the underlying manifold
structure and scales effectively with data dimensionality.

</details>


### [145] [Categorical Distributions are Effective Neural Network Outputs for Event Prediction](https://arxiv.org/abs/2507.21616)
*Kevin Doran,Tom Baden*

Main category: cs.LG

TL;DR: 研究表明简单神经网络输出的分类概率分布用于下一个尖峰预测有效，扩展数据集后发现该输出在多数据集上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 探究简单输出结构未在神经时间点过程模型中常用的原因。

Method: 扩展现有数据集并创建新数据集进行研究。

Result: 发现许多现有评估数据集未充分揭示事件生成过程信息，现有模型因正则化等表现良好，简单分类分布输出在多数据集上有竞争力。

Conclusion: 简单神经网络输出的分类概率分布在尖峰预测任务中是有效的，在多数据集上表现有竞争力。

Abstract: We demonstrate the effectiveness of using a simple neural network output, a
categorical probability distribution, for the task of next spike prediction.
This case study motivates an investigation into why this simple output
structure is not commonly used with neural temporal point process models. We
find evidence that many existing datasets for evaluating temporal point process
models do not reveal much information about the underlying event generating
processes, and many existing models perform well due to regularization effects
of model size and constraints on output structure. We extend existing datasets
and create new ones in order to explore outside of this information limited
regime and find that outputting a simple categorical distribution is
competitive across a wide range of datasets.

</details>


### [146] [Hyperbolic Genome Embeddings](https://arxiv.org/abs/2507.21648)
*Raiyan R. Khan,Philippe Chlenski,Itsik Pe'er*

Main category: cs.LG

TL;DR: 提出双曲CNN新应用用于基因组序列建模，在多个基准数据集上表现优于欧几里得模型，还探索新基准数据集，展示双曲框架潜力。


<details>
  <summary>Details</summary>
Motivation: 解决当前基因组序列建模中机器学习模型归纳偏置与生物系统进化结构难以对齐的问题。

Method: 提出双曲CNN的新应用，无需显式系统发育映射来识别序列关键属性。

Result: 在42个基因组解释基准数据集中的37个上，双曲模型优于欧几里得模型；在7个GUE基准数据集上超越现有最佳性能；创建新基准数据集。

Conclusion: 双曲框架有潜力成为基因组表示学习的强大范式。

Abstract: Current approaches to genomic sequence modeling often struggle to align the
inductive biases of machine learning models with the evolutionarily-informed
structure of biological systems. To this end, we formulate a novel application
of hyperbolic CNNs that exploits this structure, enabling more expressive DNA
sequence representations. Our strategy circumvents the need for explicit
phylogenetic mapping while discerning key properties of sequences pertaining to
core functional and regulatory behavior. Across 37 out of 42 genome
interpretation benchmark datasets, our hyperbolic models outperform their
Euclidean equivalents. Notably, our approach even surpasses state-of-the-art
performance on seven GUE benchmark datasets, consistently outperforming many
DNA language models while using orders of magnitude fewer parameters and
avoiding pretraining. Our results include a novel set of benchmark
datasets--the Transposable Elements Benchmark--which explores a major but
understudied component of the genome with deep evolutionary significance. We
further motivate our work by exploring how our hyperbolic models recognize
genomic signal under various data-generating conditions and by constructing an
empirical method for interpreting the hyperbolicity of dataset embeddings.
Throughout these assessments, we find persistent evidence highlighting the
potential of our hyperbolic framework as a robust paradigm for genome
representation learning. Our code and benchmark datasets are available at
https://github.com/rrkhan/HGE.

</details>


### [147] [DGP: A Dual-Granularity Prompting Framework for Fraud Detection with Graph-Enhanced LLMs](https://arxiv.org/abs/2507.21653)
*Yuan Li,Jun Hu,Bryan Hooi,Bingsheng He,Cheng Chen*

Main category: cs.LG

TL;DR: 提出双粒度提示（DGP）方法应对文本提示在异构欺诈检测图中的信息过载问题，实验表明其能在可管理的令牌预算内提升欺诈检测性能。


<details>
  <summary>Details</summary>
Motivation: 文本提示方法在异构欺诈检测图中存在信息过载问题，导致性能下降，需要解决该挑战。

Method: 提出DGP方法，为目标节点保留细粒度文本细节，将邻居信息总结为粗粒度文本提示，针对不同数据模态采用定制总结策略。

Result: 在公共和工业数据集上实验，DGP能在可管理的令牌预算内，相比现有方法将欺诈检测性能（AUPRC）提升最多6.8%。

Conclusion: DGP方法展现了图增强大语言模型在欺诈检测中的潜力。

Abstract: Real-world fraud detection applications benefit from graph learning
techniques that jointly exploit node features, often rich in textual data, and
graph structural information. Recently, Graph-Enhanced LLMs emerge as a
promising graph learning approach that converts graph information into prompts,
exploiting LLMs' ability to reason over both textual and structural
information. Among them, text-only prompting, which converts graph information
to prompts consisting solely of text tokens, offers a solution that relies only
on LLM tuning without requiring additional graph-specific encoders. However,
text-only prompting struggles on heterogeneous fraud-detection graphs:
multi-hop relations expand exponentially with each additional hop, leading to
rapidly growing neighborhoods associated with dense textual information. These
neighborhoods may overwhelm the model with long, irrelevant content in the
prompt and suppress key signals from the target node, thereby degrading
performance. To address this challenge, we propose Dual Granularity Prompting
(DGP), which mitigates information overload by preserving fine-grained textual
details for the target node while summarizing neighbor information into
coarse-grained text prompts. DGP introduces tailored summarization strategies
for different data modalities, bi-level semantic abstraction for textual fields
and statistical aggregation for numerical features, enabling effective
compression of verbose neighbor content into concise, informative prompts.
Experiments across public and industrial datasets demonstrate that DGP operates
within a manageable token budget while improving fraud detection performance by
up to 6.8% (AUPRC) over state-of-the-art methods, showing the potential of
Graph-Enhanced LLMs for fraud detection.

</details>


### [148] [Probabilistic Consistency in Machine Learning and Its Connection to Uncertainty Quantification](https://arxiv.org/abs/2507.21670)
*Paul Patrone,Anthony Kearsley*

Main category: cs.LG

TL;DR: 本文通过诊断推理解决机器学习模型预测置信度量化及理解模型与训练数据关系的问题，推导分类的水平集理论，分析贝叶斯分类器性质，用于多类分类及不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 机器学习的黑盒性质使其难以量化预测置信度和理解模型与训练数据的关系，需解决这些问题并关联到不确定性量化。

Method: 分析流行率的多种解释，推导分类的水平集理论；研究二元贝叶斯最优分类器性质，以流行率参数化贝叶斯分类器；在多类情况下推导归一化和自洽条件。

Result: 某些自洽的机器学习模型等同于类条件概率分布；贝叶斯分类器满足单调性和类切换性质，可推导密度比；推导的条件是任意机器学习模型有有效概率解释的必要条件。

Conclusion: 分析结果通过不确定性传播框架为机器学习的不确定性量化提供信息。

Abstract: Machine learning (ML) is often viewed as a powerful data analysis tool that
is easy to learn because of its black-box nature. Yet this very nature also
makes it difficult to quantify confidence in predictions extracted from ML
models, and more fundamentally, to understand how such models are mathematical
abstractions of training data. The goal of this paper is to unravel these
issues and their connections to uncertainty quantification (UQ) by pursuing a
line of reasoning motivated by diagnostics. In such settings, prevalence - i.e.
the fraction of elements in class - is often of inherent interest. Here we
analyze the many interpretations of prevalence to derive a level-set theory of
classification, which shows that certain types of self-consistent ML models are
equivalent to class-conditional probability distributions. We begin by studying
the properties of binary Bayes optimal classifiers, recognizing that their
boundary sets can be reinterpreted as level-sets of pairwise density ratios. By
parameterizing Bayes classifiers in terms of the prevalence, we then show that
they satisfy important monotonicity and class-switching properties that can be
used to deduce the density ratios without direct access to the boundary sets.
Moreover, this information is sufficient for tasks such as constructing the
multiclass Bayes-optimal classifier and estimating inherent uncertainty in the
class assignments. In the multiclass case, we use these results to deduce
normalization and self-consistency conditions, the latter being equivalent to
the law of total probability for classifiers. We also show that these are
necessary conditions for arbitrary ML models to have valid probabilistic
interpretations. Throughout we demonstrate how this analysis informs the
broader task of UQ for ML via an uncertainty propagation framework.

</details>


### [149] [PREIG: Physics-informed and Reinforcement-driven Interpretable GRU for Commodity Demand Forecasting](https://arxiv.org/abs/2507.21710)
*Hongwei Ma,Junbin Gao,Minh-Ngoc Tran*

Main category: cs.LG

TL;DR: 本文提出用于商品需求预测的PREIG深度学习框架，结合GRU与PINN原理，用混合优化策略，实验显示其优于传统和深度学习基线模型，能为经济领域高维非线性时间序列预测提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 由于市场动态多变、存在非线性依赖及需经济一致的预测，准确预测商品需求仍是关键挑战。

Method: 引入PREIG框架，将GRU架构与PINN原理结合，嵌入价格与需求负弹性的经济约束，通过定制损失函数执行该约束；采用结合NAdam、L - BFGS和POP的混合优化策略。

Result: 在多个商品数据集实验中，PREIG在RMSE和MAPE指标上显著优于传统计量经济模型（ARIMA、GARCH）和深度学习基线模型（BPNN、RNN）；与GRU相比，在保持良好可解释性的同时预测表现良好。

Conclusion: PREIG通过融合领域知识、优化理论和深度学习，为经济领域高维非线性时间序列预测提供了稳健、可解释和可扩展的解决方案。

Abstract: Accurately forecasting commodity demand remains a critical challenge due to
volatile market dynamics, nonlinear dependencies, and the need for economically
consistent predictions. This paper introduces PREIG, a novel deep learning
framework tailored for commodity demand forecasting. The model uniquely
integrates a Gated Recurrent Unit (GRU) architecture with physics-informed
neural network (PINN) principles by embedding a domain-specific economic
constraint: the negative elasticity between price and demand. This constraint
is enforced through a customized loss function that penalizes violations of the
physical rule, ensuring that model predictions remain interpretable and aligned
with economic theory. To further enhance predictive performance and stability,
PREIG incorporates a hybrid optimization strategy that couples NAdam and L-BFGS
with Population-Based Training (POP). Experiments across multiple commodities
datasets demonstrate that PREIG significantly outperforms traditional
econometric models (ARIMA,GARCH) and deep learning baselines (BPNN,RNN) in both
RMSE and MAPE. When compared with GRU,PREIG maintains good explainability while
still performing well in prediction. By bridging domain knowledge, optimization
theory and deep learning, PREIG provides a robust, interpretable, and scalable
solution for high-dimensional nonlinear time series forecasting in economy.

</details>


### [150] [Data-Driven Extended Corresponding State Approach for Residual Property Prediction of Hydrofluoroolefins](https://arxiv.org/abs/2507.21720)
*Gang Wang,Peng Hu*

Main category: cs.LG

TL;DR: 本文提出神经网络扩展对应态模型预测氢氟烯烃制冷剂的残余热力学性质，相比传统模型精度显著提高，有望加速新型制冷剂发现。


<details>
  <summary>Details</summary>
Motivation: 氢氟烯烃制冷剂虽有低全球变暖潜势，但缺乏可靠热力学数据阻碍其新型制冷剂的发现和应用。

Method: 结合理论和数据驱动方法，提出神经网络扩展对应态模型，通过图神经网络模块表征流体，用已知流体高精度数据训练模型，采用留一法交叉验证。

Result: 与传统模型相比，该模型在液体和超临界区域的密度和能量性质预测精度显著提高，密度平均绝对偏差分别为1.49%（液体）和2.42%（超临界）等。

Conclusion: 将物理知识嵌入机器学习模型有效，该模型有望加速新型氢氟烯烃制冷剂的发现。

Abstract: Hydrofluoroolefins are considered the most promising next-generation
refrigerants due to their extremely low global warming potential values, which
can effectively mitigate the global warming effect. However, the lack of
reliable thermodynamic data hinders the discovery and application of newer and
superior hydrofluoroolefin refrigerants. In this work, integrating the
strengths of theoretical method and data-driven method, we proposed a neural
network extended corresponding state model to predict the residual
thermodynamic properties of hydrofluoroolefin refrigerants. The innovation is
that the fluids are characterized through their microscopic molecular
structures by the inclusion of graph neural network module and the specialized
design of model architecture to enhance its generalization ability. The
proposed model is trained using the highly accurate data of available known
fluids, and evaluated via the leave-one-out cross-validation method. Compared
to conventional extended corresponding state models or cubic equation of state,
the proposed model shows significantly improved accuracy for density and energy
properties in liquid and supercritical regions, with average absolute deviation
of 1.49% (liquid) and 2.42% (supercritical) for density, 3.37% and 2.50% for
residual entropy, 1.85% and 1.34% for residual enthalpy. These results
demonstrate the effectiveness of embedding physics knowledge into the machine
learning model. The proposed neural network extended corresponding state model
is expected to significantly accelerate the discovery of novel
hydrofluoroolefin refrigerants.

</details>


### [151] [Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation](https://arxiv.org/abs/2507.21738)
*Huiqiang Chen,Tianqing Zhu,Xin Yu,Wanlei Zhou*

Main category: cs.LG

TL;DR: 本文提出ZS - PAG框架解决零样本机器学习遗忘中过遗忘问题，有理论保证且实验验证其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘算法依赖剩余数据，无法应用于仅有无遗忘样本的零样本场景，需解决过遗忘问题。

Method: 提出ZS - PAG框架，通过生成对抗样本近似剩余数据，确定特定子空间进行遗忘操作，设计基于影响的伪标签策略。

Result: 该方法有理论保证，在多个基准测试上实验验证了其相较于多个基线方法的有效性和优越性。

Conclusion: ZS - PAG框架能有效解决零样本机器学习遗忘中的过遗忘问题，提升遗忘后模型性能。

Abstract: Machine unlearning aims to remove the influence of specific samples from a
trained model. A key challenge in this process is over-unlearning, where the
model's performance on the remaining data significantly drops due to the change
in the model's parameters. Existing unlearning algorithms depend on the
remaining data to prevent this issue. As such, these methods are inapplicable
in a more practical scenario, where only the unlearning samples are available
(i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to
fill this gap. Our approach offers three key innovations: (1) we approximate
the inaccessible remaining data by generating adversarial samples; (2)
leveraging the generated samples, we pinpoint a specific subspace to perform
the unlearning process, therefore preventing over-unlearning in the challenging
zero-shot scenario; and (3) we consider the influence of the unlearning process
on the remaining samples and design an influence-based pseudo-labeling
strategy. As a result, our method further improves the model's performance
after unlearning. The proposed method holds a theoretical guarantee, and
experiments on various benchmarks validate the effectiveness and superiority of
our proposed method over several baselines.

</details>


### [152] [TempRe: Template generation for single and direct multi-step retrosynthesis](https://arxiv.org/abs/2507.21762)
*Nguyen Xuan-Vu,Daniel Armstrong,Zlatko Joncev,Philippe Schwaller*

Main category: cs.LG

TL;DR: 提出TempRe框架用于逆合成规划，在单步和多步任务中表现优于其他方法，还可用于直接多步合成路线生成。


<details>
  <summary>Details</summary>
Motivation: 传统基于模板方法扩展性差、泛化能力有限，无模板生成方法易产生无效反应，逆合成规划面临挑战。

Method: 提出TempRe框架，将基于模板的方法重新表述为序列生成。

Result: 在单步和多步逆合成任务中表现优于模板分类和基于SMILES的生成方法，在PaRoutes多步基准测试中达到强top - k路线准确率，可用于直接多步合成路线生成。

Conclusion: 模板生成建模在计算机辅助合成规划中有强大潜力。

Abstract: Retrosynthesis planning remains a central challenge in molecular discovery
due to the vast and complex chemical reaction space. While traditional
template-based methods offer tractability, they suffer from poor scalability
and limited generalization, and template-free generative approaches risk
generating invalid reactions. In this work, we propose TempRe, a generative
framework that reformulates template-based approaches as sequence generation,
enabling scalable, flexible, and chemically plausible retrosynthesis. We
evaluated TempRe across single-step and multi-step retrosynthesis tasks,
demonstrating its superiority over both template classification and
SMILES-based generation methods. On the PaRoutes multi-step benchmark, TempRe
achieves strong top-k route accuracy. Furthermore, we extend TempRe to direct
multi-step synthesis route generation, providing a lightweight and efficient
alternative to conventional single-step and search-based approaches. These
results highlight the potential of template generative modeling as a powerful
paradigm in computer-aided synthesis planning.

</details>


### [153] [Unlocking Interpretability for RF Sensing: A Complex-Valued White-Box Transformer](https://arxiv.org/abs/2507.21799)
*Xie Zhang,Yina Wang,Chenshu Wu*

Main category: cs.LG

TL;DR: 提出首个可数学解释的射频感知深度网络架构RF - CRATE，通过理论推导扩展到复域，引入子空间正则化策略提升性能，评估显示其与黑盒模型性能相当且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度无线感知（DWS）模型多为黑盒，可解释性有限，影响泛化性和在安全敏感物理应用中的使用。

Method: 受白盒变压器启发，基于复稀疏率降低原理构建RF - CRATE；将实值白盒变压器扩展到复域；引入子空间正则化策略。

Result: 在多个感知任务中平均性能提升19.98%；与黑盒模型性能相当且有可解释性；相比CRATE，平均分类增益5.08%，回归误差降低10.34%。

Conclusion: RF - CRATE是一个有效的、可数学解释的射频感知深度网络架构，在性能和可解释性上取得较好平衡。

Abstract: The empirical success of deep learning has spurred its application to the
radio-frequency (RF) domain, leading to significant advances in Deep Wireless
Sensing (DWS). However, most existing DWS models function as black boxes with
limited interpretability, which hampers their generalizability and raises
concerns in security-sensitive physical applications. In this work, inspired by
the remarkable advances of white-box transformers, we present RF-CRATE, the
first mathematically interpretable deep network architecture for RF sensing,
grounded in the principles of complex sparse rate reduction. To accommodate the
unique RF signals, we conduct non-trivial theoretical derivations that extend
the original real-valued white-box transformer to the complex domain. By
leveraging the CR-Calculus framework, we successfully construct a fully
complex-valued white-box transformer with theoretically derived self-attention
and residual multi-layer perceptron modules. Furthermore, to improve the
model's ability to extract discriminative features from limited wireless data,
we introduce Subspace Regularization, a novel regularization strategy that
enhances feature diversity, resulting in an average performance improvement of
19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against
seven baselines with multiple public and self-collected datasets involving
different RF signals. The results show that RF-CRATE achieves performance on
par with thoroughly engineered black-box models, while offering full
mathematical interpretability. More importantly, by extending CRATE to the
complex domain, RF-CRATE yields substantial improvements, achieving an average
classification gain of 5.08% and reducing regression error by 10.34% across
diverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at:
https://github.com/rfcrate/RF_CRATE.

</details>


### [154] [Bayesian Neural Network Surrogates for Bayesian Optimization of Carbon Capture and Storage Operations](https://arxiv.org/abs/2507.21803)
*Sofianos Panagiotis Fotias,Vassilis Gaganis*

Main category: cs.LG

TL;DR: 本文采用贝叶斯优化评估CCS项目开发决策变量优化策略，对比不同随机模型，结合净现值提升经济性与可持续性，是油藏工程领域首次应用。


<details>
  <summary>Details</summary>
Motivation: CCS技术对可持续未来至关重要，需优化其项目开发决策变量，且探索在高斯过程表现不佳场景下更有效的随机模型。

Method: 采用无导数的贝叶斯优化技术，对比包括高斯过程在内的多种随机模型。

Result: 提出的框架结合净现值，展现出提升CCS技术经济可行性和可持续部署的潜力。

Conclusion: 该研究是油藏工程行业首次应用相关贝叶斯优化研究，有望成为能源行业提升可持续性的优选方法。

Abstract: Carbon Capture and Storage (CCS) stands as a pivotal technology for fostering
a sustainable future. The process, which involves injecting supercritical
CO$_2$ into underground formations, a method already widely used for Enhanced
Oil Recovery, serves a dual purpose: it not only curbs CO$_2$ emissions and
addresses climate change but also extends the operational lifespan and
sustainability of oil fields and platforms, easing the shift toward greener
practices. This paper delivers a thorough comparative evaluation of strategies
for optimizing decision variables in CCS project development, employing a
derivative-free technique known as Bayesian Optimization. In addition to
Gaussian Processes, which usually serve as the gold standard in BO, various
novel stochastic models were examined and compared within a BO framework. This
research investigates the effectiveness of utilizing more exotic stochastic
models than GPs for BO in environments where GPs have been shown to
underperform, such as in cases with a large number of decision variables or
multiple objective functions that are not similarly scaled. By incorporating
Net Present Value (NPV) as a key objective function, the proposed framework
demonstrates its potential to improve economic viability while ensuring the
sustainable deployment of CCS technologies. Ultimately, this study represents
the first application in the reservoir engineering industry of the growing body
of BO research, specifically in the search for more appropriate stochastic
models, highlighting its potential as a preferred method for enhancing
sustainability in the energy sector.

</details>


### [155] [Analysis of Fourier Neural Operators via Effective Field Theory](https://arxiv.org/abs/2507.21833)
*Taeyoung Kim*

Main category: cs.LG

TL;DR: 对FNOs进行有效场论分析，解释其稳定性、泛化性和频率行为，给出权重初始化临界条件并验证。


<details>
  <summary>Details</summary>
Motivation: FNOs在高维偏微分方程中应用广泛，但稳定性、泛化性和频率行为缺乏合理解释。

Method: 在无限维函数空间对FNOs进行系统有效场论分析，推导层核和四点顶点的递归关系，研究三种重要情况。

Result: 理论表明非线性激活会耦合频率输入到高频模式，实验证实；得到宽网络权重初始化临界条件，实证测试验证。

Conclusion: 量化非线性使神经算子捕捉特征的能力，提供超参数选择标准，解释特定激活和残差连接增强特征学习的原因。

Abstract: Fourier Neural Operators (FNOs) have emerged as leading surrogates for
high-dimensional partial-differential equations, yet their stability,
generalization and frequency behavior lack a principled explanation. We present
the first systematic effective-field-theory analysis of FNOs in an
infinite-dimensional function space, deriving closed recursion relations for
the layer kernel and four-point vertex and then examining three practically
important settings-analytic activations, scale-invariant cases and
architectures with residual connections. The theory shows that nonlinear
activations inevitably couple frequency inputs to high-frequency modes that are
otherwise discarded by spectral truncation, and experiments confirm this
frequency transfer. For wide networks we obtain explicit criticality conditions
on the weight-initialization ensemble that keep small input perturbations to
have uniform scale across depth, and empirical tests validate these
predictions. Taken together, our results quantify how nonlinearity enables
neural operators to capture non-trivial features, supply criteria for
hyper-parameter selection via criticality analysis, and explain why
scale-invariant activations and residual connections enhance feature learning
in FNOs.

</details>


### [156] [Discovering Interpretable Ordinary Differential Equations from Noisy Data](https://arxiv.org/abs/2507.21841)
*Rahul Golder,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: 提出无监督参数估计方法用于发现近似物理系统动力学的可解释模型，能高精度发现ODE，对噪声数据稳健。


<details>
  <summary>Details</summary>
Motivation: 当前方法采用预定义函数形式或基函数，所得模型缺乏物理意义和可解释性，难以反映系统真实物理特性。

Method: 先找到近似通解，再进行样条变换线性估计常微分方程（ODE）系数，近似通解采用与一般齐次、线性、常系数ODE解析解相同的函数形式，利用样条近似获取梯度信息构建梯度矩阵求解系数。

Result: 从案例研究可知，建模方法能高精度发现ODE，在不使用正则化技术下促进解的稀疏性。

Conclusion: 该方法对噪声数据稳健，可将数据驱动技术集成到实际实验中用于物理现象的数据驱动学习。

Abstract: The data-driven discovery of interpretable models approximating the
underlying dynamics of a physical system has gained attraction in the past
decade. Current approaches employ pre-specified functional forms or basis
functions and often result in models that lack physical meaning and
interpretability, let alone represent the true physics of the system. We
propose an unsupervised parameter estimation methodology that first finds an
approximate general solution, followed by a spline transformation to linearly
estimate the coefficients of the governing ordinary differential equation
(ODE). The approximate general solution is postulated using the same functional
form as the analytical solution of a general homogeneous, linear,
constant-coefficient ODE. An added advantage is its ability to produce a
high-fidelity, smooth functional form even in the presence of noisy data. The
spline approximation obtains gradient information from the functional form
which are linearly independent and creates the basis of the gradient matrix.
This gradient matrix is used in a linear system to find the coefficients of the
ODEs. From the case studies, we observed that our modeling approach discovers
ODEs with high accuracy and also promotes sparsity in the solution without
using any regularization techniques. The methodology is also robust to noisy
data and thus allows the integration of data-driven techniques into real
experimental setting for data-driven learning of physical phenomena.

</details>


### [157] [Cardiovascular Disease Prediction using Machine Learning: A Comparative Analysis](https://arxiv.org/abs/2507.21898)
*Risshab Srinivas Ramesh,Roshani T S Udupa,Monisha J,Kushi K K S*

Main category: cs.LG

TL;DR: 研究分析心血管疾病数据集，找出与疾病相关因素，对比模型性能，指出数据问题及改进方向。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，探究数值和分类因素对其发生的影响。

Method: 对数据集进行统计分析（t 检验、卡方检验、ANOVA），构建逻辑回归模型，对比不同模型性能。

Result: 发现心血管疾病与老年人、高血压等强相关，身体活动是保护因素；逻辑回归模型显示年龄、血压和胆固醇是主要风险因素，吸烟和饮酒有意外负关联；CatBoost 模型表现最佳。

Conclusion: 数据存在异常值和偏态分布问题，需改进预处理以提高预测可靠性。

Abstract: Cardiovascular diseases (CVDs) are a main cause of mortality globally,
accounting for 31% of all deaths. This study involves a cardiovascular disease
(CVD) dataset comprising 68,119 records to explore the influence of numerical
(age, height, weight, blood pressure, BMI) and categorical gender, cholesterol,
glucose, smoking, alcohol, activity) factors on CVD occurrence. We have
performed statistical analyses, including t-tests, Chi-square tests, and ANOVA,
to identify strong associations between CVD and elderly people, hypertension,
higher weight, and abnormal cholesterol levels, while physical activity (a
protective factor). A logistic regression model highlights age, blood pressure,
and cholesterol as primary risk factors, with unexpected negative associations
for smoking and alcohol, suggesting potential data issues. Model performance
comparisons reveal CatBoost as the top performer with an accuracy of 0.734 and
an ECE of 0.0064 and excels in probabilistic prediction (Brier score = 0.1824).
Data challenges, including outliers and skewed distributions, indicate a need
for improved preprocessing to enhance predictive reliability.

</details>


### [158] [Multi-state Protein Design with DynamicMPNN](https://arxiv.org/abs/2507.21938)
*Alex Abrudan,Sebastian Pujalte Ojeda,Chaitanya K. Joshi,Matthew Greenig,Felipe Engelberger,Alena Khmelinskaia,Jens Meiler,Michele Vendruscolo,Tuomas P. J. Knowles*

Main category: cs.LG

TL;DR: 提出动态MPNN逆折叠模型，用于多构象蛋白质序列设计，性能优于ProteinMPNN。


<details>
  <summary>Details</summary>
Motivation: 现有多态设计方法依赖单态预测事后聚合，实验成功率低，需新方法设计与多构象兼容的序列。

Method: 引入DynamicMPNN逆折叠模型，通过跨构象集合联合学习生成与多构象兼容的序列，在46,033个构象对训练，用AlphaFold初始猜测评估。

Result: DynamicMPNN在具有挑战性的多态蛋白质基准测试中，结构归一化RMSD比ProteinMPNN高13%。

Conclusion: DynamicMPNN模型在多态蛋白质设计上表现更优。

Abstract: Structural biology has long been dominated by the one sequence, one
structure, one function paradigm, yet many critical biological processes - from
enzyme catalysis to membrane transport - depend on proteins that adopt multiple
conformational states. Existing multi-state design approaches rely on post-hoc
aggregation of single-state predictions, achieving poor experimental success
rates compared to single-state design. We introduce DynamicMPNN, an inverse
folding model explicitly trained to generate sequences compatible with multiple
conformations through joint learning across conformational ensembles. Trained
on 46,033 conformational pairs covering 75% of CATH superfamilies and evaluated
using AlphaFold initial guess, DynamicMPNN outperforms ProteinMPNN by up to 13%
on structure-normalized RMSD across our challenging multi-state protein
benchmark.

</details>


### [159] [SLA-Centric Automated Algorithm Selection Framework for Cloud Environments](https://arxiv.org/abs/2507.21963)
*Siana Rizwan,Tasnim Ahmed,Salimur Choudhury*

Main category: cs.LG

TL;DR: 提出SLA感知自动化算法选择框架用于资源受限云环境组合优化问题，应用于0 - 1背包问题并评估。


<details>
  <summary>Details</summary>
Motivation: 云服务中SLA违规会影响效率和CSP盈利能力，需解决资源受限云环境组合优化问题。

Method: 使用机器学习模型集成预测性能，根据SLA约束对算法 - 硬件对排名，构建特定数据集，进行分类和回归任务评估及消融研究。

Result: 对框架在分类和回归任务进行评估，开展消融研究探索相关因素影响。

Conclusion: 所提出的SLA感知自动化算法选择框架有一定可行性和研究价值。

Abstract: Cloud computing offers on-demand resource access, regulated by Service-Level
Agreements (SLAs) between consumers and Cloud Service Providers (CSPs). SLA
violations can impact efficiency and CSP profitability. In this work, we
propose an SLA-aware automated algorithm-selection framework for combinatorial
optimization problems in resource-constrained cloud environments. The framework
uses an ensemble of machine learning models to predict performance and rank
algorithm-hardware pairs based on SLA constraints. We also apply our framework
to the 0-1 knapsack problem. We curate a dataset comprising instance specific
features along with memory usage, runtime, and optimality gap for 6 algorithms.
As an empirical benchmark, we evaluate the framework on both classification and
regression tasks. Our ablation study explores the impact of hyperparameters,
learning approaches, and large language models effectiveness in regression, and
SHAP-based interpretability.

</details>


### [160] [Improving Generative Ad Text on Facebook using Reinforcement Learning](https://arxiv.org/abs/2507.21983)
*Daniel R. Jiang,Alex Nikulkov,Yu-Chia Chen,Yang Bai,Zheqing Zhu*

Main category: cs.LG

TL;DR: 研究以Facebook上生成式广告首次部署RL训练LLM为例，介绍RLPF方法，A/B测试显示AdLlama提升点击率，表明RL后训练有显著经济影响，RLPF是有前景方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI中LLM后训练里RL的经济影响未被充分探索和量化，需研究其实际影响。

Method: 引入RLPF后训练方法，以历史广告绩效数据为奖励信号训练AdLlama模型，进行为期10周、涉及近35000个广告商和64000个广告变体的大规模A/B测试。

Result: AdLlama较监督模仿模型使点击率提高6.7%，使用AdLlama的广告商生成更多广告变体。

Conclusion: 该研究是生成式AI在真实场景最大规模研究，量化了RL后训练实际影响，RLPF是有前景且可推广的以指标驱动后训练方法。

Abstract: Generative artificial intelligence (AI), in particular large language models
(LLMs), is poised to drive transformative economic change. LLMs are pre-trained
on vast text data to learn general language patterns, but a subsequent
post-training phase is critical to align them for specific real-world tasks.
Reinforcement learning (RL) is the leading post-training technique, yet its
economic impact remains largely underexplored and unquantified. We examine this
question through the lens of the first deployment of an RL-trained LLM for
generative advertising on Facebook. Integrated into Meta's Text Generation
feature, our model, "AdLlama," powers an AI tool that helps advertisers create
new variations of human-written ad text. To train this model, we introduce
reinforcement learning with performance feedback (RLPF), a post-training method
that uses historical ad performance data as a reward signal. In a large-scale
10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad
variations, we find that AdLlama improves click-through rates by 6.7%
(p=0.0296) compared to a supervised imitation model trained on curated ads.
This represents a substantial improvement in advertiser return on investment on
Facebook. We also find that advertisers who used AdLlama generated more ad
variations, indicating higher satisfaction with the model's outputs. To our
knowledge, this is the largest study to date on the use of generative AI in an
ecologically valid setting, offering an important data point quantifying the
tangible impact of RL post-training. Furthermore, the results show that RLPF is
a promising and generalizable approach for metric-driven post-training that
bridges the gap between highly capable language models and tangible outcomes.

</details>


### [161] [Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation](https://arxiv.org/abs/2507.21992)
*Siddhartha Pradhan,Shikshya Shiwakoti,Neha Bathuri*

Main category: cs.LG

TL;DR: 研究多异构教师模型知识蒸馏能否增强可迁移对抗样本生成，用两种KD策略训练轻量学生模型，评估攻击效果，结果表明可提升效率和效果。


<details>
  <summary>Details</summary>
Motivation: 探究多异构教师模型知识蒸馏对生成可迁移对抗样本的增强作用。

Method: 用课程切换和联合优化两种KD策略，以ResNet50和DenseNet - 161为教师训练轻量学生模型，用FG、FGS和PGD攻击生成对抗样本，评估对GoogLeNet的攻击效果。

Result: 多教师蒸馏的学生模型攻击成功率与基于集成的基线相当，可将对抗样本生成时间最多缩短6倍，低温设置和硬标签监督能增强可迁移性。

Conclusion: 知识蒸馏不仅是模型压缩技术，也是提升黑盒对抗攻击效率和效果的有力工具。

Abstract: We investigate whether knowledge distillation (KD) from multiple
heterogeneous teacher models can enhance the generation of transferable
adversarial examples. A lightweight student model is trained using two KD
strategies: curriculum-based switching and joint optimization, with ResNet50
and DenseNet-161 as teachers. The trained student is then used to generate
adversarial examples using FG, FGS, and PGD attacks, which are evaluated
against a black-box target model (GoogLeNet). Our results show that student
models distilled from multiple teachers achieve attack success rates comparable
to ensemble-based baselines, while reducing adversarial example generation time
by up to a factor of six. An ablation study further reveals that lower
temperature settings and the inclusion of hard-label supervision significantly
enhance transferability. These findings suggest that KD can serve not only as a
model compression technique but also as a powerful tool for improving the
efficiency and effectiveness of black-box adversarial attacks.

</details>


### [162] [Classification of Honey Botanical and Geographical Sources using Mineral Profiles and Machine Learning](https://arxiv.org/abs/2507.22032)
*Mokhtar Al-Awadhi,Ratnadeep Deshmukh*

Main category: cs.LG

TL;DR: 提出基于机器学习的方法，用矿物元素特征识别蜂蜜花源和地理来源，随机森林表现最佳。


<details>
  <summary>Details</summary>
Motivation: 识别蜂蜜的花源和地理来源。

Method: 分预处理和分类两步，预处理进行缺失值处理和数据归一化，分类采用多种监督分类模型。

Result: 蜂蜜矿物元素含量可用于分类，随机森林分类器在数据集上表现最佳，花源分类交叉验证准确率99.30%，地理来源分类准确率98.01%。

Conclusion: 矿物元素含量可用于蜂蜜花源和地理来源分类，随机森林分类器效果好。

Abstract: This paper proposes a machine learning-based approach for identifying honey
floral and geographical sources using mineral element profiles. The proposed
method comprises two steps: preprocessing and classification. The preprocessing
phase involves missing-value treatment and data normalization. In the
classification phase, we employ various supervised classification models for
discriminating between six botanical sources and 13 geographical origins of
honey. We test the classifiers' performance on a publicly available honey
mineral element dataset. The dataset contains mineral element profiles of
honeys from various floral and geographical origins. Results show that mineral
element content in honey provides discriminative information useful for
classifying honey botanical and geographical sources. Results also show that
the Random Forests (RF) classifier obtains the best performance on this
dataset, achieving a cross-validation accuracy of 99.30% for classifying honey
botanical origins and 98.01% for classifying honey geographical origins.

</details>


### [163] [Structure-Informed Deep Reinforcement Learning for Inventory Management](https://arxiv.org/abs/2507.22040)
*Alvaro Maggiar,Sohrab Andaz,Akhil Bagaria,Carson Eisenach,Dean Foster,Omer Gottesman,Dominique Perrault-Joncas*

Main category: cs.LG

TL;DR: 本文研究深度强化学习（DRL）在经典库存管理问题中的应用，展示其性能并提出改进技术，弥合数据驱动学习与分析见解的差距。


<details>
  <summary>Details</summary>
Motivation: 探索DRL在库存管理问题中的实际应用，避免传统方法对需求分布的不现实假设。

Method: 应用基于DirectBackprop的DRL算法到多种库存管理场景，提出结构信息策略网络技术。

Result: DRL方法表现优于基准和启发式方法，能自然捕捉最优策略的结构特性，改进技术提升性能和可解释性。

Conclusion: 该研究在库存管理中弥合了数据驱动学习和分析见解的差距，具有实际应用价值。

Abstract: This paper investigates the application of Deep Reinforcement Learning (DRL)
to classical inventory management problems, with a focus on practical
implementation considerations. We apply a DRL algorithm based on DirectBackprop
to several fundamental inventory management scenarios including multi-period
systems with lost sales (with and without lead times), perishable inventory
management, dual sourcing, and joint inventory procurement and removal. The DRL
approach learns policies across products using only historical information that
would be available in practice, avoiding unrealistic assumptions about demand
distributions or access to distribution parameters. We demonstrate that our
generic DRL implementation performs competitively against or outperforms
established benchmarks and heuristics across these diverse settings, while
requiring minimal parameter tuning. Through examination of the learned
policies, we show that the DRL approach naturally captures many known
structural properties of optimal policies derived from traditional operations
research methods. To further improve policy performance and interpretability,
we propose a Structure-Informed Policy Network technique that explicitly
incorporates analytically-derived characteristics of optimal policies into the
learning process. This approach can help interpretability and add robustness to
the policy in out-of-sample performance, as we demonstrate in an example with
realistic demand data. Finally, we provide an illustrative application of DRL
in a non-stationary setting. Our work bridges the gap between data-driven
learning and analytical insights in inventory management while maintaining
practical applicability.

</details>


### [164] [Weight-Parameterization in Continuous Time Deep Neural Networks for Surrogate Modeling](https://arxiv.org/abs/2507.22045)
*Haley Rosso,Lars Ruthotto,Khachik Sargsyan*

Main category: cs.LG

TL;DR: 研究连续时间深度学习模型权重参数化策略，实验表明勒让德参数化更优。


<details>
  <summary>Details</summary>
Motivation: 训练连续时间深度学习模型时，在计算约束下学习有表现力且稳定的时变权重是挑战，需探索权重参数化策略。

Method: 研究将权重时间演化约束到多项式基函数张成的低维子空间的参数化策略，在神经ODE和ResNet架构中，用离散 - 优化和优化 - 离散训练范式评估单项式和勒让德多项式基。

Result: 三个高维基准问题实验显示，勒让德参数化训练动态更稳定，降低计算成本，精度与单项式参数化和无约束权重模型相当或更好。

Conclusion: 阐明了时变权重参数化中基选择的作用，使用正交多项式基在模型表达性和训练效率间有良好权衡。

Abstract: Continuous-time deep learning models, such as neural ordinary differential
equations (ODEs), offer a promising framework for surrogate modeling of complex
physical systems. A central challenge in training these models lies in learning
expressive yet stable time-varying weights, particularly under computational
constraints. This work investigates weight parameterization strategies that
constrain the temporal evolution of weights to a low-dimensional subspace
spanned by polynomial basis functions. We evaluate both monomial and Legendre
polynomial bases within neural ODE and residual network (ResNet) architectures
under discretize-then-optimize and optimize-then-discretize training paradigms.
Experimental results across three high-dimensional benchmark problems show that
Legendre parameterizations yield more stable training dynamics, reduce
computational cost, and achieve accuracy comparable to or better than both
monomial parameterizations and unconstrained weight models. These findings
elucidate the role of basis choice in time-dependent weight parameterization
and demonstrate that using orthogonal polynomial bases offers a favorable
tradeoff between model expressivity and training efficiency.

</details>


### [165] [Foundation Models for Demand Forecasting via Dual-Strategy Ensembling](https://arxiv.org/abs/2507.22053)
*Wei Yang,Defu Cao,Yan Liu*

Main category: cs.LG

TL;DR: 提出统一集成框架提升基础模型在现实供应链销售预测中的性能，实验表明该方法优于基线。


<details>
  <summary>Details</summary>
Motivation: 准确需求预测对供应链优化至关重要，但实践中因多种因素困难，现有基础模型存在架构刚性和鲁棒性不足问题。

Method: 提出统一集成框架，包含按语义级别分区训练和推理的分层集成（HE）和整合不同模型骨干预测的架构集成（AE）两种策略。

Result: 在M5基准和三个外部销售数据集上实验，该方法始终优于强基线，提高各层级准确性。

Conclusion: 该方法为复杂预测环境下提升泛化能力提供简单有效的机制。

Abstract: Accurate demand forecasting is critical for supply chain optimization, yet
remains difficult in practice due to hierarchical complexity, domain shifts,
and evolving external factors. While recent foundation models offer strong
potential for time series forecasting, they often suffer from architectural
rigidity and limited robustness under distributional change. In this paper, we
propose a unified ensemble framework that enhances the performance of
foundation models for sales forecasting in real-world supply chains. Our method
combines two complementary strategies: (1) Hierarchical Ensemble (HE), which
partitions training and inference by semantic levels (e.g., store, category,
department) to capture localized patterns; and (2) Architectural Ensemble (AE),
which integrates predictions from diverse model backbones to mitigate bias and
improve stability. We conduct extensive experiments on the M5 benchmark and
three external sales datasets, covering both in-domain and zero-shot
forecasting. Results show that our approach consistently outperforms strong
baselines, improves accuracy across hierarchical levels, and provides a simple
yet effective mechanism for boosting generalization in complex forecasting
environments.

</details>


### [166] [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)
*Zhuokun Chen,Zeren Chen,Jiahao He,Mingkui Tan,Jianfei Cai,Bohan Zhuang*

Main category: cs.LG

TL;DR: 提出R - Stitch框架加速思维链推理，减少推理延迟且准确率损失小。


<details>
  <summary>Details</summary>
Motivation: 思维链推理有效但计算开销大，现有加速策略有局限性，需新方法。

Method: 提出R - Stitch，基于置信度在小语言模型和大语言模型间切换，默认用小模型，置信度低于阈值时用大模型。

Result: 在数学推理基准测试中，R - Stitch可减少高达85%的推理延迟，准确率下降可忽略不计。

Conclusion: R - Stitch在加速思维链推理方面具有实际有效性。

Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of
large language models by encouraging step-by-step intermediate reasoning during
inference. While effective, CoT introduces substantial computational overhead
due to its reliance on autoregressive decoding over long token sequences.
Existing acceleration strategies either reduce sequence length through early
stopping or compressive reward designs, or improve decoding speed via
speculative decoding with smaller models. However, speculative decoding suffers
from limited speedup when the agreement between small and large models is low,
and fails to exploit the potential advantages of small models in producing
concise intermediate reasoning. In this paper, we present R-Stitch, a
token-level, confidence-based hybrid decoding framework that accelerates CoT
inference by switching between a small language model (SLM) and a large
language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to
generate tokens by default and delegates to the LLM only when the SLM's
confidence falls below a threshold. This design avoids full-sequence rollback
and selectively invokes the LLM on uncertain steps, preserving both efficiency
and answer quality. R-Stitch is model-agnostic, training-free, and compatible
with standard decoding pipelines. Experiments on math reasoning benchmarks
demonstrate that R-Stitch achieves up to 85\% reduction in inference latency
with negligible accuracy drop, highlighting its practical effectiveness in
accelerating CoT reasoning.

</details>


### [167] [Online hierarchical partitioning of the output space in extreme multi-label data stream](https://arxiv.org/abs/2507.20894)
*Lara Neves,Afonso Lourenço,Alberto Cano,Goreti Marreiros*

Main category: cs.LG

TL;DR: 本文介绍了在线多标签学习框架iHOMER，可增量划分标签空间，集成漂移检测机制，实验显示其在多标签分类上表现优于多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决数据流多标签输出因分布变化、高维标签空间等带来的挑战，以及概念漂移对模型适应的影响。

Method: 引入iHOMER框架，利用基于Jaccard相似度的在线分裂 - 聚合聚类和多元Bernoulli过程驱动的全局树型学习器，集成全局和局部的漂移检测机制。

Result: 在23个真实数据集上，iHOMER比5个全局基线模型平均高23%，比12个局部基线模型平均高32%。

Conclusion: iHOMER在在线多标签分类上具有很强的鲁棒性。

Abstract: Mining data streams with multi-label outputs poses significant challenges due
to evolving distributions, high-dimensional label spaces, sparse label
occurrences, and complex label dependencies. Moreover, concept drift affects
not only input distributions but also label correlations and imbalance ratios
over time, complicating model adaptation. To address these challenges,
structured learners are categorized into local and global methods. Local
methods break down the task into simpler components, while global methods adapt
the algorithm to the full output space, potentially yielding better predictions
by exploiting label correlations. This work introduces iHOMER (Incremental
Hierarchy Of Multi-label Classifiers), an online multi-label learning framework
that incrementally partitions the label space into disjoint, correlated
clusters without relying on predefined hierarchies. iHOMER leverages online
divisive-agglomerative clustering based on \textit{Jaccard} similarity and a
global tree-based learner driven by a multivariate \textit{Bernoulli} process
to guide instance partitioning. To address non-stationarity, it integrates
drift detection mechanisms at both global and local levels, enabling dynamic
restructuring of label partitions and subtrees. Experiments across 23
real-world datasets show iHOMER outperforms 5 state-of-the-art global
baselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\%, and 12 local
baselines, such as binary relevance transformations of kNN, EFDT, ARF, and
ADWIN bagging/boosting ensembles, by 32\%, establishing its robustness for
online multi-label classification.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [168] [Reservoir Computation with Networks of Differentiating Neuron Ring Oscillators](https://arxiv.org/abs/2507.21377)
*Alexander Yeung,Peter DelMastro,Arjun Karuvally,Hava Siegelmann,Edward Rietman,Hananel Hazan*

Main category: cs.NE

TL;DR: 本文提出用区分神经元的小世界网络替代积分神经元作为储层计算基底，用于MNIST数字识别任务取得90.65%准确率。


<details>
  <summary>Details</summary>
Motivation: 当前储层计算使用的耦合积分神经元网络需稳定电流维持活动，需寻找替代方案。

Method: 引入仅在输入变化时活跃的区分神经元小世界图作为储层计算基底，找到合适耦合强度和网络拓扑。

Result: 在MNIST数字识别任务中取得90.65%准确率，与现有储层计算方法性能相当。

Conclusion: 区分神经元可作为积分神经元的潜在替代方案，为高能耗AI应用提供可持续选择。

Abstract: Reservoir Computing is a machine learning approach that uses the rich
repertoire of complex system dynamics for function approximation. Current
approaches to reservoir computing use a network of coupled integrating neurons
that require a steady current to maintain activity. Here, we introduce a small
world graph of differentiating neurons that are active only when there are
changes in input as an alternative to integrating neurons as a reservoir
computing substrate. We find the coupling strength and network topology that
enable these small world networks to function as an effective reservoir. We
demonstrate the efficacy of these networks in the MNIST digit recognition task,
achieving comparable performance of 90.65% to existing reservoir computing
approaches. The findings suggest that differentiating neurons can be a
potential alternative to integrating neurons and can provide a sustainable
future alternative for power-hungry AI applications.

</details>


### [169] [Hebbian Memory-Augmented Recurrent Networks: Engram Neurons in Deep Learning](https://arxiv.org/abs/2507.21474)
*Daniel Szelogowski*

Main category: cs.NE

TL;DR: 提出Engram Neural Network (ENN)，在多个基准测试中表现与经典架构相当，且在可解释性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前人工循环网络架构依赖隐式隐藏状态记忆，限制可解释性和长距离依赖建模能力，受生物神经系统显式关联记忆痕迹启发。

Method: 引入ENN，结合显式可微记忆矩阵、Hebbian可塑性和稀疏注意力驱动检索机制，通过动态Hebbian痕迹显式建模记忆形成和回忆。

Result: ENN在MNIST、CIFAR - 10和WikiText - 103基准测试中，准确性和泛化性能与经典架构相当，在可解释性上有显著提升，Hebbian痕迹可视化显示出生物合理的结构化记忆形成过程。

Conclusion: 受神经科学启发的机制有助于开发更具可解释性和鲁棒性的深度学习模型。

Abstract: Despite success across diverse tasks, current artificial recurrent network
architectures rely primarily on implicit hidden-state memories, limiting their
interpretability and ability to model long-range dependencies. In contrast,
biological neural systems employ explicit, associative memory traces (i.e.,
engrams) strengthened through Hebbian synaptic plasticity and activated
sparsely during recall. Motivated by these neurobiological insights, we
introduce the Engram Neural Network (ENN), a novel recurrent architecture
incorporating an explicit, differentiable memory matrix with Hebbian plasticity
and sparse, attention-driven retrieval mechanisms. The ENN explicitly models
memory formation and recall through dynamic Hebbian traces, improving
transparency and interpretability compared to conventional RNN variants. We
evaluate the ENN architecture on three canonical benchmarks: MNIST digit
classification, CIFAR-10 image sequence modeling, and WikiText-103 language
modeling. Our empirical results demonstrate that the ENN achieves accuracy and
generalization performance broadly comparable to classical RNN, GRU, and LSTM
architectures, with all models converging to similar accuracy and perplexity on
the large-scale WikiText-103 task. At the same time, the ENN offers significant
enhancements in interpretability through observable memory dynamics. Hebbian
trace visualizations further reveal biologically plausible, structured memory
formation processes, validating the potential of neuroscience-inspired
mechanisms to inform the development of more interpretable and robust deep
learning models.

</details>


### [170] [Knowledge-Guided Memetic Algorithm for Capacitated Arc Routing Problems with Time-Dependent Service Costs](https://arxiv.org/abs/2507.21740)
*Qingya Li,Shengcai Liu,Wenjie Chen,Juan Zou,Ke Tang,Xin Yao*

Main category: cs.NE

TL;DR: 本文针对带时变服务成本的容量弧路径问题（CARPTDSC）提出知识引导的模因算法（KGMA - GN），实验表明其搜索效率高于现有方法。


<details>
  <summary>Details</summary>
Motivation: CARPTDSC是NP难问题，时变服务成本增加计算量，现有算法搜索效率有限。

Method: 提出KGMA - GN算法，包含知识引导初始化策略（KGIS）和知识引导小步长局部搜索策略（KGSLSS）。

Result: 在五个基准测试集上，KGMA - GN搜索效率高于现有方法，消融实验表明KGSLSS的知识引导局部搜索算子可显著减少运行时间。

Conclusion: KGMA - GN算法能有效提高CARPTDSC的搜索效率。

Abstract: The capacitated arc routing problem with time-dependent service costs
(CARPTDSC) is a challenging combinatorial optimization problem that arises from
winter gritting applications. CARPTDSC has two main challenges about time
consumption. First, it is an NP-hard problem. Second, the time-dependent
service costs of tasks require frequent evaluations during the search process,
significantly increasing computational effort. These challenges make it
difficult for existing algorithms to perform efficient searches, often
resulting in limited efficiency. To address these issues, this paper proposes a
knowledge-guided memetic algorithm with golden section search and negatively
correlated search (KGMA-GN), where two knowledge-guided strategies are
introduced to improve search efficiency. First, a knowledge-guided
initialization strategy (KGIS) is proposed to generate high-quality initial
solutions to speed up convergence. Second, a knowledge-guided small-step-size
local search strategy (KGSLSS) is proposed to filter out invalid moves, thereby
reducing unnecessary evaluations and saving the computation time. Experimental
results on five benchmark test sets, including both small- and larger-scale
instances, demonstrate that KGMA-GN achieves higher search efficiency than the
state-of-the-art methods. Moreover, the ablation study further confirms that
the knowledge-guided local search operators in KGSLSS can significantly reduce
runtime compared to traditional operators, especially for the knowledge-guided
swap operator, which achieves more than a tenfold improvement in speed.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [171] [Beamforming-based Achievable Rate Maximization in ISAC System for Multi-UAV Networking](https://arxiv.org/abs/2507.21895)
*Shengcai Zhou,Luping Xiang,Kun Yang,Kai Kit Wong,Dapeng Oliver Wu,Chan-Byoung Chae*

Main category: cs.PF

TL;DR: 本文研究多无人机网络应急通信方案，提出时间辅助帧结构和三种求解机制，仿真显示系统性能提升。


<details>
  <summary>Details</summary>
Motivation: ISAC可感知潜在移动通信用户，研究适用于应急通信的多无人机网络有效方案。

Method: 开发时间辅助帧结构，用扩展卡尔曼滤波辅助波束对准；联合设计无人机波束成形、负载管理和方向规划解决优化问题；引入增强分布式SCA - IRM算法、联盟博弈方法和费马点搜索法求解。

Result: 仿真表明在通信速率、公平性和感知精度方面系统性能得到改善。

Conclusion: 为无人机辅助应急通信网络提供设计指导。

Abstract: Airborne mobile Integrated Sensing and Communication (ISAC) base stations
have garnered significant attention recently, with ISAC technology being a
crucial application for 6G networks. Since ISAC can sense potential mobile
communication users, this paper studies an effective scheme for a multi-UAV
network tailored for emergency communication. In this paper, we develop a
temporal-assisted frame structure utilizing integrated omnidirectional and
directional beampattern to facilitate efficient and frequent searching, with
extended Kalman filtering (EKF) as an aid to beam alignment. Further, we
address an optimization problem to maximize the total achievable rate per slot
by jointly designing UAV beamforming, load management, and UAV direction
planning, all while adhering to the constraints of the predicted beam coverage.
Given the problem NP-hard, we introduce three robust mechanisms for its
resolution: an enhanced distributed Successive Convex Approximation
(SCA)-Iterative Rank Minimization (IRM) algorithm, an coalition game approach,
and a Fermat point search method. In particular, the proposed SCA-IRM algorithm
decomposes the original complex optimization problem into several sub-problems
and assigns them equally to each UAV, so as to realize distributed computing
and improve computational efficiency. Our proposed simulations demonstrate the
improved system performance in terms of communication rate, fairness, and
sensing accuracy, providing design guidelines of UAV-assisted emergency
communication networking.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [172] [Generating Highly Structured Test Inputs Leveraging Constraint-Guided Graph Refinement](https://arxiv.org/abs/2507.21271)
*Zhaorui Yang,Yuxin Qiu,Haichao Zhu,Qian Zhang*

Main category: cs.SE

TL;DR: 研究用基于图的表示统一结构化领域测试输入，开发GRAphRef框架并在八个AI系统评估，对比多种工具。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试工具和输入生成器针对特定输入类型，常生成无效输入，效率低且泛化性差。

Method: 开发GRAphRef框架，将结构化输入映射到图，进行邻域相似性引导突变和约束细化修复无效输入。在八个真实世界的网格处理AI系统开展验证研究，对比多种工具，用统计分析和组件延迟细分评估假设。

Result: 无明确提及

Conclusion: 无明确提及

Abstract: [Context] Modern AI applications increasingly process highly structured data,
such as 3D meshes and point clouds, where test input generation must preserve
both structural and semantic validity. However, existing fuzzing tools and
input generators are typically handcrafted for specific input types and often
generate invalid inputs that are subsequently discarded, leading to
inefficiency and poor generalizability. [Objective] This study investigates
whether test inputs for structured domains can be unified through a graph-based
representation, enabling general, reusable mutation strategies while enforcing
structural constraints. We will evaluate the effectiveness of this approach in
enhancing input validity and semantic preservation across eight AI systems.
[Method] We develop and evaluate GRAphRef, a graph-based test input generation
framework that supports constraint-based mutation and refinement. GRAphRef maps
structured inputs to graphs, applies neighbor-similarity-guided mutations, and
uses a constraint-refinement phase to repair invalid inputs. We will conduct a
confirmatory study across eight real-world mesh-processing AI systems,
comparing GRAphRef with AFL, MeshAttack, Saffron, and two ablated variants.
Evaluation metrics include structural validity, semantic preservation (via
prediction consistency), and performance overhead. Experimental data is derived
from ShapeNetCore mesh seeds and model outputs from systems like MeshCNN and
HodgeNet. Statistical analysis and component latency breakdowns will be used to
assess each hypothesis.

</details>


### [173] ["Maybe We Need Some More Examples:" Individual and Team Drivers of Developer GenAI Tool Use](https://arxiv.org/abs/2507.21280)
*Courtney Miller,Rudrajit Choudhuri,Mara Ulloa,Sankeerti Haniyur,Robert DeLine,Margaret-Anne Storey,Emerson Murphy-Hill,Christian Bird,Jenna L. Butler*

Main category: cs.SE

TL;DR: 生成式AI工具在软件工程中普及但开发者采用不均，研究发现使用差异源于开发者对工具的认知、参与方式和应对挑战的态度，存在'生产力压力悖论'。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI工具在软件工程中开发者采用不均的问题，其阻碍生产力、影响管理预期和带来角色不确定性。

Method: 对27个团队的54名开发者进行配对访谈，每个团队有一名频繁使用者和一名不频繁使用者。

Result: 使用差异主要源于开发者对工具的认知（合作者还是功能）、参与方式（实验性还是保守性）以及应对挑战的反应（适应性坚持还是快速放弃）。

Conclusion: 组织期望快速提高生产力却不充分投入学习支持，会产生'生产力压力悖论'，损害采用工具的生产力效益。

Abstract: Despite the widespread availability of generative AI tools in software
engineering, developer adoption remains uneven. This unevenness is problematic
because it hampers productivity efforts, frustrates management's expectations,
and creates uncertainty around the future roles of developers. Through paired
interviews with 54 developers across 27 teams -- one frequent and one
infrequent user per team -- we demonstrate that differences in usage result
primarily from how developers perceive the tool (as a collaborator vs.
feature), their engagement approach (experimental vs. conservative), and how
they respond when encountering challenges (with adaptive persistence vs. quick
abandonment). Our findings imply that widespread organizational expectations
for rapid productivity gains without sufficient investment in learning support
creates a "Productivity Pressure Paradox," undermining the very productivity
benefits that motivate adoption.

</details>


### [174] [Black-Box Bug-Amplification for Multithreaded Software](https://arxiv.org/abs/2507.21318)
*Yeshayahu Weiss,Gal Amram,Achiya Elyasaf,Eitan Farchi,Oded Margalit,Gera Weiss*

Main category: cs.SE

TL;DR: 提出系统放大并发系统中难以复现的漏洞出现率的方法，通过实验证明集成回归模型能显著提升漏洞出现率。


<details>
  <summary>Details</summary>
Motivation: 并发系统中的漏洞难以复现，测试人员常遇到特定输入下低概率出现的故障，需有效方法放大此类漏洞的出现。

Method: 将被测系统视为黑盒，通过重复试验执行训练预测模型估计输入配置触发漏洞的概率，在含17个代表性并发漏洞的数据集上评估，对比多种基于模型的搜索技术与暴力随机采样基准。

Result: 集成回归模型能在几乎所有场景下显著提高漏洞出现率，常比随机采样有数量级的提升。

Conclusion: 提出将漏洞放大作为罕见事件回归问题的新公式；实证评估多种放大漏洞出现的技术，证明模型引导搜索的有效性；提供实用、非侵入式的测试框架帮助从业者发现隐藏的并发故障。

Abstract: Bugs, especially those in concurrent systems, are often hard to reproduce
because they manifest only under rare conditions. Testers frequently encounter
failures that occur only under specific inputs, even when occurring with low
probability. We propose an approach to systematically amplify the occurrence of
such elusive bugs. We treat the system under test as a black-box and use
repeated trial executions to train a predictive model that estimates the
probability of a given input configuration triggering a bug. We evaluate this
approach on a dataset of 17 representative concurrency bugs spanning diverse
categories. Several model-based search techniques are compared against a
brute-force random sampling baseline. Our results show that an ensemble of
regression models can significantly increase bug occurrence rates across nearly
all scenarios, often achieving an order-of-magnitude improvement over random
sampling. The contributions of this work include: (i) a novel formulation of
bug-amplification as a rare-event regression problem; (ii) an empirical
evaluation of multiple techniques for amplifying bug occurrence, demonstrating
the effectiveness of model-guided search; and (iii) a practical, non-invasive
testing framework that helps practitioners expose hidden concurrency faults
without altering the internal system architecture.

</details>


### [175] [Does Editing Improve Answer Quality on Stack Overflow? A Data-Driven Investigation](https://arxiv.org/abs/2507.21329)
*Saikat Mondal,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 研究分析94,994条有编辑的Python相关答案，发现编辑对答案质量有正负影响，为协作编辑系统改进提供参考。


<details>
  <summary>Details</summary>
Motivation: 技术问答平台高质量答案至关重要，虽有协作编辑机制，但此前未系统评估编辑是否提升关键质量维度。

Method: 分析94,994条至少有一次接受编辑的Python相关答案，评估编辑对六项质量维度的影响。

Result: 编辑有正负影响，如部分提升相关性、优化性能，但也有降低相关性、增加复杂度等负面效果。

Conclusion: 编辑结果不一致，研究为用户、版主提供警示，有助于改进协作编辑系统。

Abstract: High-quality answers in technical Q&A platforms like Stack Overflow (SO) are
crucial as they directly influence software development practices. Poor-quality
answers can introduce inefficiencies, bugs, and security vulnerabilities, and
thus increase maintenance costs and technical debt in production software. To
improve content quality, SO allows collaborative editing, where users revise
answers to enhance clarity, correctness, and formatting. Several studies have
examined rejected edits and identified the causes of rejection. However, prior
research has not systematically assessed whether accepted edits enhance key
quality dimensions. While one study investigated the impact of edits on C/C++
vulnerabilities, broader quality aspects remain unexplored. In this study, we
analyze 94,994 Python-related answers that have at least one accepted edit to
determine whether edits improve (1) semantic relevance, (2) code usability, (3)
code complexity, (4) security vulnerabilities, (5) code optimization, and (6)
readability. Our findings show both positive and negative effects of edits.
While 53.3% of edits improve how well answers match questions, 38.1% make them
less relevant. Some previously broken code (9%) becomes executable, yet working
code (14.7%) turns non-parsable after edits. Many edits increase complexity
(32.3%), making code harder to maintain. Instead of fixing security issues,
20.5% of edits introduce additional issues. Even though 51.0% of edits optimize
performance, execution time still increases overall. Readability also suffers,
as 49.7% of edits make code harder to read. This study highlights the
inconsistencies in editing outcomes and provides insights into how edits impact
software maintainability, security, and efficiency that might caution users and
moderators and help future improvements in collaborative editing systems.

</details>


### [176] [MAAD: Automate Software Architecture Design through Knowledge-Driven Multi-Agent Collaboration](https://arxiv.org/abs/2507.21382)
*Ruiyin Li,Yiran Zhang,Xiyu Zhou,Peng Liang,Weisong Sun,Jifeng Xuan,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: 本文提出MAAD自动化框架用于软件架构设计，经评估其在生成架构组件和评估报告上表现优越，还探讨了不同大模型在该框架中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统软件架构设计耗时费力，依赖架构师，设计方案有限，基于大语言模型的代理在架构设计应用较少，需更多探索。

Method: 提出MAAD框架，利用知识驱动的多智能体系统，协调四个专业智能体协作，通过案例研究和与MetaGPT对比实验进行评估。

Result: MAAD在生成架构组件和评估报告上表现优越，工业架构师反馈其实用性强，GPT - 4o在MAAD框架中表现更好。

Conclusion: MAAD框架有效，同时强调了在多智能体驱动的架构设计中选择大语言模型的重要性。

Abstract: Software architecture design is a critical, yet inherently complex and
knowledge-intensive phase of software development. It requires deep domain
expertise, development experience, architectural knowledge, careful trade-offs
among competing quality attributes, and the ability to adapt to evolving
requirements. Traditionally, this process is time-consuming and
labor-intensive, and relies heavily on architects, often resulting in limited
design alternatives, especially under the pressures of agile development. While
Large Language Model (LLM)-based agents have shown promising performance across
various SE tasks, their application to architecture design remains relatively
scarce and requires more exploration, particularly in light of diverse domain
knowledge and complex decision-making. To address the challenges, we proposed
MAAD (Multi-Agent Architecture Design), an automated framework that employs a
knowledge-driven Multi-Agent System (MAS) for architecture design. MAAD
orchestrates four specialized agents (i.e., Analyst, Modeler, Designer and
Evaluator) to collaboratively interpret requirements specifications and produce
architectural blueprints enriched with quality attributes-based evaluation
reports. We then evaluated MAAD through a case study and comparative
experiments against MetaGPT, a state-of-the-art MAS baseline. Our results show
that MAAD's superiority lies in generating comprehensive architectural
components and delivering insightful and structured architecture evaluation
reports. Feedback from industrial architects across 11 requirements
specifications further reinforces MAAD's practical usability. We finally
explored the performance of the MAAD framework with three LLMs (GPT-4o,
DeepSeek-R1, and Llama 3.3) and found that GPT-4o exhibits better performance
in producing architecture design, emphasizing the importance of LLM selection
in MAS-driven architecture design.

</details>


### [177] [LLM4VV: Evaluating Cutting-Edge LLMs for Generation and Evaluation of Directive-Based Parallel Programming Model Compiler Tests](https://arxiv.org/abs/2507.21447)
*Zachariah Sollenberger,Rahul Patel,Saieda Ali Zada,Sunita Chandrasekaran*

Main category: cs.SE

TL;DR: 本文提出双LLM系统用于生成大量编译器测试，实验表明LLM有生成高质量编译器测试并自动验证的潜力。


<details>
  <summary>Details</summary>
Motivation: LLM在软件和测试开发中应用增多，但验证其生成代码正确性缺乏全面自主方案，盲目应用有幻觉问题且难以信任结果，需有效应用LLM。

Method: 提出双LLM系统（生成式LLM和判别式LLM），用不同参数的多个LLM进行实验，并采用十个精心选择的指标。

Result: 通过实验发现LLM有生成高质量编译器测试并自动验证的潜力。

Conclusion: LLM具备生成高质量编译器测试并自动验证的有前景的潜力。

Abstract: The usage of Large Language Models (LLMs) for software and test development
has continued to increase since LLMs were first introduced, but only recently
have the expectations of LLMs become more realistic. Verifying the correctness
of code generated by LLMs is key to improving their usefulness, but there have
been no comprehensive and fully autonomous solutions developed yet.
Hallucinations are a major concern when LLMs are applied blindly to problems
without taking the time and effort to verify their outputs, and an inability to
explain the logical reasoning of LLMs leads to issues with trusting their
results. To address these challenges while also aiming to effectively apply
LLMs, this paper proposes a dual-LLM system (i.e. a generative LLM and a
discriminative LLM) and experiments with the usage of LLMs for the generation
of a large volume of compiler tests. We experimented with a number of LLMs
possessing varying parameter counts and presented results using ten
carefully-chosen metrics that we describe in detail in our narrative. Through
our findings, it is evident that LLMs possess the promising potential to
generate quality compiler tests and verify them automatically.

</details>


### [178] [HLSDebugger: Identification and Correction of Logic Bugs in HLS Code with LLM Solutions](https://arxiv.org/abs/2507.21485)
*Jing Wang,Shang Liu,Yao Lu,Zhiyao Xie*

Main category: cs.SE

TL;DR: 现有高级综合（HLS）代码调试困难，本文提出HLSDebugger解决应用大语言模型（LLMs）进行HLS逻辑调试的挑战，其表现优于GPT - 4。


<details>
  <summary>Details</summary>
Motivation: 调试HLS代码具有挑战性且耗人力，尤其是对缺乏硬件知识的人员，而应用LLMs进行HLS调试存在数据稀缺、硬件逻辑错误调试复杂、缺乏可靠测试用例等挑战。

Method: 提出HLSDebugger，生成并发布含300K数据样本的大型标记数据集，采用编码器 - 解码器结构，用同一模型进行错误定位、类型预测和修正。

Result: HLSDebugger在错误识别上显著优于GPT - 4，错误修正能力超3倍。

Conclusion: HLSDebugger在HLS代码自动调试探索方面取得重大进展。

Abstract: High-level synthesis (HLS) accelerates hardware design by enabling the
automatic translation of high-level descriptions into efficient hardware
implementations. However, debugging HLS code is a challenging and
labor-intensive task, especially for novice circuit designers or software
engineers without sufficient hardware domain knowledge. The recent emergence of
Large Language Models (LLMs) is promising in automating the HLS debugging
process. Despite the great potential, three key challenges persist when
applying LLMs to HLS logic debugging: 1) High-quality circuit data for training
LLMs is scarce, posing a significant challenge. 2) Debugging logic bugs in
hardware is inherently more complex than identifying software bugs with
existing golden test cases. 3) The absence of reliable test cases requires
multi-tasking solutions, performing both bug identification and correction.
complicates the multi-tasking required for effective HLS debugging. In this
work, we propose a customized solution named HLSDebugger to address the
challenges. HLSDebugger first generates and releases a large labeled dataset
with 300K data samples, targeting HLS logic bugs. The HLSDebugger model adopts
an encoder-decoder structure, performing bug location identification, bug type
prediction, and bug correction with the same model. HLSDebugger significantly
outperforms advanced LLMs like GPT-4 in bug identification and by more than 3x
in bug correction. It makes a substantial advancement in the exploration of
automated debugging of HLS code.

</details>


### [179] [Ethical Classification of Non-Coding Contributions in Open-Source Projects via Large Language Models](https://arxiv.org/abs/2507.21583)
*Sergio Cobos,Javier Luis Cánovas Izquierdo*

Main category: cs.SE

TL;DR: 本文提出用大语言模型（LLM）对开源软件项目中非编码贡献的道德质量进行分类的方法。


<details>
  <summary>Details</summary>
Motivation: 开源软件开发面临道德挑战，现有行为准则的监控和执行方法有局限，需要新方法来评估非编码贡献的道德行为。

Method: 基于Contributor Covenant定义一套道德指标，使用提示工程引导模型输出，开发分类方法来评估道德行为。

Result: 未提及。

Conclusion: 未提及。

Abstract: The development of Open-Source Software (OSS) is not only a technical
challenge, but also a social one due to the diverse mixture of contributors. To
this aim, social-coding platforms, such as GitHub, provide the infrastructure
needed to host and develop the code, but also the support for enabling the
community's collaboration, which is driven by non-coding contributions, such as
issues (i.e., change proposals or bug reports) or comments to existing
contributions. As with any other social endeavor, this development process
faces ethical challenges, which may put at risk the project's sustainability.
To foster a productive and positive environment, OSS projects are increasingly
deploying codes of conduct, which define rules to ensure a respectful and
inclusive participatory environment, with the Contributor Covenant being the
main model to follow. However, monitoring and enforcing these codes of conduct
is a challenging task, due to the limitations of current approaches. In this
paper, we propose an approach to classify the ethical quality of non-coding
contributions in OSS projects by relying on Large Language Models (LLM), a
promising technology for text classification tasks. We defined a set of ethical
metrics based on the Contributor Covenant and developed a classification
approach to assess ethical behavior in OSS non-coding contributions, using
prompt engineering to guide the model's output.

</details>


### [180] [Predicting Maintenance Cessation of Open Source Software Repositories with An Integrated Feature Framework](https://arxiv.org/abs/2507.21678)
*Yiming Xu,Runzhi He,Hengzhi Ye,Minghui Zhou,Huaimin Wang*

Main category: cs.SE

TL;DR: 本文提出基于维护停止的开源软件维护风险预测方法，构建大规模数据集，提出多视角特征框架，模型效果好且具可解释性，并在实际生态系统应用。


<details>
  <summary>Details</summary>
Motivation: 现有开源软件维护风险预测方法存在操作定义模糊、可解释性有限、数据集规模或通用性不足等问题。

Method: 引入基于档案状态和文档语义分析的维护停止概念，构建大规模纵向数据集，提出多视角特征框架，采用AFT生存分析，进行特征消融和SHAP分析，部署GBSA分类器。

Result: AFT生存分析C指数达0.846，远超仅依赖表面特征的模型，特征消融和SHAP分析证实方法有效性和可解释性。

Conclusion: 为维护风险预测建立了可扩展、可解释的基础，能在大规模开源生态系统实现可重复的风险管理。

Abstract: The maintenance risks of open source software (OSS) projects pose significant
threats to the quality, security, and resilience of modern software supply
chains. While prior research has proposed diverse approaches for predicting OSS
maintenance risk -- leveraging signals ranging from surface features (e.g.,
stars, commits) to social network analyses and behavioral patterns -- existing
methods often suffer from ambiguous operational definitions, limited
interpretability, and datasets of insufficient scale or generalizability. In
this work, we introduce ``maintenance cessation'', grounded in both explicit
archival status and rigorous semantic analysis of project documentation.
Building on this foundation, we curate a large-scale, longitudinal dataset of
115,466 GitHub repositories -- encompassing 57,733 confirmed cessation events
-- complemented by comprehensive, timeline-based behavioral features. We
propose an integrated, multi-perspective feature framework for predicting
maintenance cessation, systematically combining user-centric features,
maintainer-centric features and project evolution features. AFT survival
analysis demonstrates a high C-index (0.846), substantially outperforming
models relying only on surface features. Feature ablation and SHAP analysis
further confirm the effectiveness and interpretability of our approach.
Finally, we demonstrate real-world applicability by deploying a GBSA classifier
in the openEuler ecosystem for proactive package risk screening. Our work
establishes a scalable, interpretable foundation for maintenance-risk
prediction, enabling reproducible risk management across large-scale open
source ecosystems.

</details>


### [181] [MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering Multiple Languages, Models,Prompts, and Scenarios](https://arxiv.org/abs/2507.21693)
*Basak Demirok,Mucahid Kutlu,Selin Mergen*

Main category: cs.SE

TL;DR: 随着大语言模型在代码生成领域发展，研究引入MultiAIGCD数据集用于检测Python、Java和Go的AI生成代码，并对三个先进模型进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成中的应用引发教育和招聘方面的担忧，需要开发强大的AI生成代码检测系统来维护学术诚信和招聘公平。

Method: 从CodeNet数据集生成不同语言和场景的代码样本，构建MultiAIGCD数据集，包含AI生成和人类编写的代码片段，并对三个先进的检测模型进行基准测试。

Result: MultiAIGCD数据集包含121,271个AI生成和32,148个人类编写的代码片段，并完成模型在不同测试场景下的性能评估。

Conclusion: 共享数据集和代码以支持该领域的研究。

Abstract: As large language models (LLMs) rapidly advance, their role in code
generation has expanded significantly. While this offers streamlined
development, it also creates concerns in areas like education and job
interviews. Consequently, developing robust systems to detect AI-generated code
is imperative to maintain academic integrity and ensure fairness in hiring
processes. In this study, we introduce MultiAIGCD, a dataset for AI-generated
code detection for Python, Java, and Go. From the CodeNet dataset's problem
definitions and human-authored codes, we generate several code samples in Java,
Python, and Go with six different LLMs and three different prompts. This
generation process covered three key usage scenarios: (i) generating code from
problem descriptions, (ii) fixing runtime errors in human-written code, and
(iii) correcting incorrect outputs. Overall, MultiAIGCD consists of 121,271
AI-generated and 32,148 human-written code snippets. We also benchmark three
state-of-the-art AI-generated code detection models and assess their
performance in various test scenarios such as cross-model and cross-language.
We share our dataset and codes to support research in this field.

</details>


### [182] [Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda](https://arxiv.org/abs/2507.21928)
*Christian Meske,Tobias Hermanns,Esther von der Weiden,Kai-Uwe Loser,Thorsten Berger*

Main category: cs.SE

TL;DR: 随着vibe coding普及，软件开发变革，本文从意图视角和历史分析定义该范式，分析其影响、机遇与风险并提出研究议程。


<details>
  <summary>Details</summary>
Motivation: vibe coding快速普及但概念理解有限，需探究这一新兴范式。

Method: 从意图视角和历史分析，定义vibe coding，研究其意图中介机制。

Result: vibe coding重新配置认知工作，转移专业知识方向，有机会也有风险。

Conclusion: 提出涵盖人、技术和组织方向的研究议程，指导该范式未来研究。

Abstract: Software development is undergoing a fundamental transformation as vibe
coding becomes widespread, with large portions of contemporary codebases now
being AI-generated. The disconnect between rapid adoption and limited
conceptual understanding highlights the need for an inquiry into this emerging
paradigm. Drawing on an intent perspective and historical analysis, we define
vibe coding as a software development paradigm where humans and generative AI
engage in collaborative flow to co-create software artifacts through natural
language dialogue, shifting the mediation of developer intent from
deterministic instruction to probabilistic inference. By intent mediation, we
refer to the fundamental process through which developers translate their
conceptual goals into representations that computational systems can execute.
Our results show that vibe coding reconfigures cognitive work by redistributing
epistemic labor between humans and machines, shifting the expertise in the
software development process away from traditional areas such as design or
technical implementation toward collaborative orchestration. We identify key
opportunities, including democratization, acceleration, and systemic leverage,
alongside risks, such as black box codebases, responsibility gaps, and
ecosystem bias. We conclude with a research agenda spanning human-,
technology-, and organization-centered directions to guide future
investigations of this paradigm.

</details>


### [183] [DeepGo: Predictive Directed Greybox Fuzzing](https://arxiv.org/abs/2507.21952)
*Peihong Lin,Pengfei Wang,Xu Zhou,Wei Xie,Gen Zhang,Kai Lu*

Main category: cs.SE

TL;DR: 提出DeepGo，结合历史与预测信息引导DGF通过最优路径到达目标站点，包含路径过渡模型、VEE、RLF模型和动作组概念。


<details>
  <summary>Details</summary>
Motivation: 现有DGF技术对适应度指标的优化主要基于启发式算法，缺乏对未执行路径的前瞻性，难以执行的复杂路径阻碍DGF到达目标，效率较低。

Method: 提出路径过渡模型；用深度神经网络构建VEE预测路径过渡和奖励；开发RLF模型生成最优路径过渡序列；提出动作组概念优化模糊测试关键步骤。

Result: 文中未提及具体实验结果。

Conclusion: 文中未明确提及最终结论，但可推测DeepGo有望解决现有DGF技术存在的问题，高效到达目标站点。

Abstract: The state-of-the-art DGF techniques redefine and optimize the fitness metric
to reach the target sites precisely and quickly. However, optimizations for
fitness metrics are mainly based on heuristic algorithms, which usually rely on
historical execution information and lack foresight on paths that have not been
exercised yet. Thus, those hard-to-execute paths with complex constraints would
hinder DGF from reaching the targets, making DGF less efficient. In this paper,
we propose DeepGo, a predictive directed grey-box fuzzer that can combine
historical and predicted information to steer DGF to reach the target site via
an optimal path. We first propose the path transition model, which models DGF
as a process of reaching the target site through specific path transition
sequences. The new seed generated by mutation would cause the path transition,
and the path corresponding to the high-reward path transition sequence
indicates a high likelihood of reaching the target site through it. Then, to
predict the path transitions and the corresponding rewards, we use deep neural
networks to construct a Virtual Ensemble Environment (VEE), which gradually
imitates the path transition model and predicts the rewards of path transitions
that have not been taken yet. To determine the optimal path, we develop a
Reinforcement Learning for Fuzzing (RLF) model to generate the transition
sequences with the highest sequence rewards. The RLF model can combine
historical and predicted path transitions to generate the optimal path
transition sequences, along with the policy to guide the mutation strategy of
fuzzing. Finally, to exercise the high-reward path transition sequence, we
propose the concept of an action group, which comprehensively optimizes the
critical steps of fuzzing to realize the optimal path to reach the target
efficiently.

</details>


### [184] [Fine-Tuning Code Language Models to Detect Cross-Language Bugs](https://arxiv.org/abs/2507.21954)
*Zengyang Li,Yimeng Li,Binbin Huang,Peng Liang,Ran Mo,Hui Liu,Yutao Ma*

Main category: cs.SE

TL;DR: 本文研究预训练代码语言模型（CodeLMs）检测跨语言漏洞（CLBs）的潜力，开发CLCFinder工具、构建数据集并微调评估模型，发现微调后性能提升，小模型表现更好等。


<details>
  <summary>Details</summary>
Motivation: 多语言编程引入跨语言漏洞，单语言漏洞检测工具难以检测，因此研究预训练代码语言模型在CLB检测中的潜力。

Method: 开发CLCFinder工具，构建涉及三种PL组合和九种交互类型的CLB数据集，在该数据集上微调13个CodeLMs并评估性能，分析数据集大小、令牌序列长度和代码注释的影响。

Result: 所有CodeLMs微调前表现差，微调后有不同程度提升，UniXcoder - base的F1分数最高；小模型微调后表现优于大模型；在单语言漏洞数据集上微调的CodeLMs在CLB检测中表现差；增加微调数据集大小显著提升性能，更长令牌序列不一定提升性能，代码注释对不同模型影响不同。

Conclusion: 预训练代码语言模型经适当微调可用于CLB检测，CLBs与单语言漏洞有区别，数据集大小对模型性能影响大。

Abstract: Multilingual programming, which involves using multiple programming languages
(PLs) in a single project, is increasingly common due to its benefits. However,
it introduces cross-language bugs (CLBs), which arise from interactions between
different PLs and are difficult to detect by single-language bug detection
tools. This paper investigates the potential of pre-trained code language
models (CodeLMs) in CLB detection. We developed CLCFinder, a cross-language
code identification tool, and constructed a CLB dataset involving three PL
combinations (Python-C/C++, Java-C/C++, and Python-Java) with nine interaction
types. We fine-tuned 13 CodeLMs on this dataset and evaluated their
performance, analyzing the effects of dataset size, token sequence length, and
code comments. Results show that all CodeLMs performed poorly before
fine-tuning, but exhibited varying degrees of performance improvement after
fine-tuning, with UniXcoder-base achieving the best F1 score (0.7407). Notably,
small fine-tuned CodeLMs tended to performe better than large ones. CodeLMs
fine-tuned on single-language bug datasets performed poorly on CLB detection,
demonstrating the distinction between CLBs and single-language bugs.
Additionally, increasing the fine-tuning dataset size significantly improved
performance, while longer token sequences did not necessarily improve the model
performance. The impact of code comments varied across models. Some fine-tuned
CodeLMs' performance was improved, while others showed degraded performance.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [185] [Determinants of Saving Behavior Among Employees in Dhaka, Bangladesh](https://arxiv.org/abs/2507.21254)
*Soumita Roy,Md Muntasir Kamal Dihan,Tasnimah Haque,Nafisa Nomani,Sadia Islam Preety*

Main category: q-fin.ST

TL;DR: 研究探讨孟加拉国达卡员工储蓄行为的影响因素，采用定量方法，发现仅财务管理实践与储蓄行为显著正相关，研究有数据局限。


<details>
  <summary>Details</summary>
Motivation: 探究影响孟加拉国达卡员工储蓄行为的因素，让他人了解孟加拉国定量金融行为分析方法。

Method: 采用定量方法和横截面调查设计，通过结构化问卷收集40名参与者数据，进行描述性统计、可靠性分析和回归分析。

Result: 仅财务管理实践与储蓄行为有显著正相关，其他因素无显著关系。

Conclusion: 研究为孟加拉国储蓄行为决定因素的有限研究做出贡献，探讨了多个因素包括社会影响。

Abstract: Purpose With an emphasis on elements like financial knowledge, financial
attitude, social influence, financial self-efficacy, and financial management
practices, this study explores the factors that influence employees' saving
behavior in Dhaka, Bangladesh. We also welcome others to work on saving
behavior, which is the main reason for publishing. The purpose is to make
others aware of the methods for quantitative financial behavior analysis in
Bangladesh. Design/methodology/approach The study uses a quantitative approach
with a cross-sectional survey design. Data was collected from 40 participants
through a structured questionnaire adapted from reliable sources. The
questionnaire captured demographic information and used established items to
measure the key variables. Data analysis included descriptive statistics,
reliability analysis using Cronbachs alpha, and regression analysis to test the
hypothesized relationships. Findings The results indicate that among the
factors examined, only financial management practices had a significant
positive relationship with saving behavior. Rest of the factors did not show
significant relationships with saving behavior in this study sample. Limitation
or Disclaimer It is still a work in progress, this paper is meant for pre-print
with mostly incomplete and limited data. No data cleaning was performed, so it
is very likely to include outliers and faulty data. Originality or value This
study contributes to the limited research on saving behavior determinants in
the Bangladeshi context, specifically among employees in the capital city of
Dhaka. It explores the influence of multiple factors, including the rarely
studied aspect of social influence.

</details>


### [186] [Slomads Rising: Stay Length Shifts in Digital Nomad Travel, United States 2019-2024](https://arxiv.org/abs/2507.21298)
*Harrison Katz,Erica Savage*

Main category: q-fin.ST

TL;DR: 研究2019 - 2024年美国Airbnb预订数据，发现新冠冲击后长住趋势持久，分析了不同阶段预订时长变化，表明‘慢游族’影响短租市场。


<details>
  <summary>Details</summary>
Motivation: 探究新冠疫情冲击后美国Airbnb预订时长的变化情况及影响。

Method: 使用2019年1月1日至2024年12月31日美国Airbnb预订数据，按预订夜数加权，采用负二项回归和障碍模型进行分析。

Result: 疫情后平均预订夜数增加，长住概率上升，长住平均时长稳定，平均时长增加主要因长住频率上升。

Conclusion: 远程工作的‘慢游族’持久地影响了美国短期租赁市场长尾，对定价、库存管理和税收有影响。

Abstract: Using every U.S. Airbnb reservation created from 1 January 2019 through 31
December 2024, weighted by nights booked, we document a lasting shift toward
longer stays after the COVID 19 shock. Mean nights per booking rose from 3.7
before the pandemic to a stable 4.1 to 4.4 after 2021; the median increased
from two to three nights and the weighted standard deviation nearly doubled to
seven nights, indicating a heavier tail. Negative binomial regression shows
that, relative to the restriction period, post vaccine bookings are 6.5 percent
shorter and pre pandemic bookings 16 percent shorter, with only mild
seasonality. A hurdle model finds that the probability of a stay of at least 28
nights nearly doubled during restrictions (1.5 percent to 2.9 percent) and has
settled at 2.2 percent since, while the conditional mean of long stays remains
43 to 46 nights. Hence the higher average arises chiefly from a greater
frequency, not length of month plus stays. These results indicate that remote
work "slomads" have durably thickened the long stay tail of the U.S. short term
rental market, with implications for pricing, inventory management, and
taxation.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [187] [Graph neural networks for residential location choice: connection to classical logit models](https://arxiv.org/abs/2507.21334)
*Zhanhong Cheng,Lingqian Hu,Yuheng Bu,Yuqi Zhou,Shenhao Wang*

Main category: stat.ML

TL;DR: 本文引入基于图神经网络的离散选择模型（GNN - DCMs）分析住宅区位选择，理论和实证均表现良好，有潜力成为统一框架。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法无法明确捕捉选择方案间的关系，本文旨在填补该空白。

Method: 引入图神经网络（GNN）构建GNN - DCMs，使其能捕捉空间方案间依赖关系，并与经典随机效用理论建立联系。

Result: 理论上GNN - DCMs包含嵌套logit和空间相关logit模型；实证上，在预测芝加哥77个社区的住宅区位选择上优于基准模型；能捕捉个体异质性和空间替代模式。

Conclusion: GNN - DCMs有潜力成为复杂空间选择背景下离散选择建模和深度学习结合的统一且有表现力的框架。

Abstract: Researchers have adopted deep learning for classical discrete choice analysis
as it can capture complex feature relationships and achieve higher predictive
performance. However, the existing deep learning approaches cannot explicitly
capture the relationship among choice alternatives, which has been a
long-lasting focus in classical discrete choice models. To address the gap,
this paper introduces Graph Neural Network (GNN) as a novel framework to
analyze residential location choice. The GNN-based discrete choice models
(GNN-DCMs) offer a structured approach for neural networks to capture
dependence among spatial alternatives, while maintaining clear connections to
classical random utility theory. Theoretically, we demonstrate that the
GNN-DCMs incorporate the nested logit (NL) model and the spatially correlated
logit (SCL) model as two specific cases, yielding novel algorithmic
interpretation through message passing among alternatives' utilities.
Empirically, the GNN-DCMs outperform benchmark MNL, SCL, and feedforward neural
networks in predicting residential location choices among Chicago's 77
community areas. Regarding model interpretation, the GNN-DCMs can capture
individual heterogeneity and exhibit spatially-aware substitution patterns.
Overall, these results highlight the potential of GNN-DCMs as a unified and
expressive framework for synergizing discrete choice modeling and deep learning
in the complex spatial choice contexts.

</details>


### [188] [From Sublinear to Linear: Fast Convergence in Deep Networks via Locally Polyak-Lojasiewicz Regions](https://arxiv.org/abs/2507.21429)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Main category: stat.ML

TL;DR: 本文解决了深度神经网络非凸损失景观中梯度下降收敛速率理论与实践的差异，引入LPLR概念，证明梯度下降在LPLR内线性收敛，并在多种场景验证理论。


<details>
  <summary>Details</summary>
Motivation: 现有理论无法解释深度神经网络非凸损失景观中梯度下降在实践中的指数收敛速率，需要解决理论与实践的差异。

Method: 在对神经切线核（NTK）稳定性的温和假设下，证明局部区域满足局部Polyak - Lojasiewicz条件，引入局部Polyak - Lojasiewicz区域（LPLR）概念。

Result: 梯度下降在LPLR内实现线性收敛，在多种深度学习场景验证了LPLR结构的稳健出现。

Conclusion: 通过NTK框架将局部景观几何与快速优化严格联系起来，为深度学习中基于梯度的优化效率提供了理论解释。

Abstract: The convergence of gradient descent (GD) on the non-convex loss landscapes of
deep neural networks (DNNs) presents a fundamental theoretical challenge. While
recent work has established that GD converges to a stationary point at a
sublinear rate within locally quasi-convex regions (LQCRs), this fails to
explain the exponential convergence rates consistently observed in practice. In
this paper, we resolve this discrepancy by proving that under a mild assumption
on Neural Tangent Kernel (NTK) stability, these same regions satisfy a local
Polyak-Lojasiewicz (PL) condition. We introduce the concept of a Locally
Polyak-Lojasiewicz Region (LPLR), where the squared gradient norm lower-bounds
the suboptimality gap, prove that properly initialized finite-width networks
admit such regions around initialization, and establish that GD achieves linear
convergence within an LPLR, providing the first finite-width guarantee that
matches empirically observed rates. We validate our theory across diverse
settings, from controlled experiments on fully-connected networks to modern
ResNet architectures trained with stochastic methods, demonstrating that LPLR
structure emerges robustly in practical deep learning scenarios. By rigorously
connecting local landscape geometry to fast optimization through the NTK
framework, our work provides a definitive theoretical explanation for the
remarkable efficiency of gradient-based optimization in deep learning.

</details>


### [189] [Measuring Sample Quality with Copula Discrepancies](https://arxiv.org/abs/2507.21434)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Main category: stat.ML

TL;DR: 介绍了Copula Discrepancy (CD)用于评估近似MCMC样本依赖结构的诊断方法，在性能和计算成本上有优势。


<details>
  <summary>Details</summary>
Motivation: 现代贝叶斯机器学习中可扩展MCMC算法牺牲精确性换速度，传统样本质量度量对有偏采样器失效，现有Stein诊断无法评估依赖结构。

Method: 引入CD，利用Sklar定理分离并量化样本依赖结构，有基于矩和基于MLE的变体。

Result: 基于矩的CD在有偏MCMC超参数选择上优于标准诊断；基于MLE的变体可检测尾部依赖的细微差异；计算开销远低于现有Stein差异。

Conclusion: CD为MCMC从业者提供实用价值，为下一代结构感知样本质量评估奠定理论基础。

Abstract: The scalable Markov chain Monte Carlo (MCMC) algorithms that underpin modern
Bayesian machine learning, such as Stochastic Gradient Langevin Dynamics
(SGLD), sacrifice asymptotic exactness for computational speed, creating a
critical diagnostic gap: traditional sample quality measures fail
catastrophically when applied to biased samplers. While powerful Stein-based
diagnostics can detect distributional mismatches, they provide no direct
assessment of dependence structure, often the primary inferential target in
multivariate problems. We introduce the Copula Discrepancy (CD), a principled
and computationally efficient diagnostic that leverages Sklar's theorem to
isolate and quantify the fidelity of a sample's dependence structure
independent of its marginals. Our theoretical framework provides the first
structure-aware diagnostic specifically designed for the era of approximate
inference. Empirically, we demonstrate that a moment-based CD dramatically
outperforms standard diagnostics like effective sample size for hyperparameter
selection in biased MCMC, correctly identifying optimal configurations where
traditional methods fail. Furthermore, our robust MLE-based variant can detect
subtle but critical mismatches in tail dependence that remain invisible to rank
correlation-based approaches, distinguishing between samples with identical
Kendall's tau but fundamentally different extreme-event behavior. With
computational overhead orders of magnitude lower than existing Stein
discrepancies, the CD provides both immediate practical value for MCMC
practitioners and a theoretical foundation for the next generation of
structure-aware sample quality assessment.

</details>


### [190] [From Global to Local: A Scalable Benchmark for Local Posterior Sampling](https://arxiv.org/abs/2507.21449)
*Rohan Hitchcock,Jesse Hoogland*

Main category: stat.ML

TL;DR: 论文指出SGMCMC算法与神经网络损失景观退化的交互研究不足，引入新基准评估局部采样性能，发现RMSProp - 预条件SGLD最有效，能提取大规模模型局部信息。


<details>
  <summary>Details</summary>
Motivation: 当前常见SGMCMC算法的全局收敛保证假设可能与退化损失景观不兼容，需研究其与退化的交互，将重点从全局转向局部后验采样。

Method: 引入可扩展的基准评估SGMCMC算法的局部采样性能。

Result: RMSProp - 预条件SGLD在忠实表示后验分布的局部几何形状方面最有效，能在参数达O(100M)的模型中提取非平凡局部信息。

Conclusion: 虽缺乏全局采样器收敛的理论保证，但能在大规模模型中提取局部信息。

Abstract: Degeneracy is an inherent feature of the loss landscape of neural networks,
but it is not well understood how stochastic gradient MCMC (SGMCMC) algorithms
interact with this degeneracy. In particular, current global convergence
guarantees for common SGMCMC algorithms rely on assumptions which are likely
incompatible with degenerate loss landscapes. In this paper, we argue that this
gap requires a shift in focus from global to local posterior sampling, and, as
a first step, we introduce a novel scalable benchmark for evaluating the local
sampling performance of SGMCMC algorithms. We evaluate a number of common
algorithms, and find that RMSProp-preconditioned SGLD is most effective at
faithfully representing the local geometry of the posterior distribution.
Although we lack theoretical guarantees about global sampler convergence, our
empirical results show that we are able to extract non-trivial local
information in models with up to O(100M) parameters.

</details>


### [191] [Stochastic forest transition model dynamics and parameter estimation via deep learning](https://arxiv.org/abs/2507.21486)
*Satoshi Kumabe,Tianyu Song,Ton Viet Ta*

Main category: stat.ML

TL;DR: 本文开发随机微分方程模型研究森林转变动态，建立全局正解存在性，进行数值分析，并提出深度学习方法估计模型参数，以理解森林转变和毁林趋势。


<details>
  <summary>Details</summary>
Motivation: 森林转变是复杂现象，需研究其动态变化及毁林趋势。

Method: 开发随机微分方程模型，建立全局正解存在性，进行数值分析，提出深度学习方法估计模型参数。

Result: 建立了模型全局正解的存在性，能够从单个包含森林和农业用地比例时间序列观测的样本中估计所有模型参数。

Conclusion: 所提出的创新方法能帮助理解任意未来时间的森林转变动态和毁林趋势。

Abstract: Forest transitions, characterized by dynamic shifts between forest,
agricultural, and abandoned lands, are complex phenomena. This study developed
a stochastic differential equation model to capture the intricate dynamics of
these transitions. We established the existence of global positive solutions
for the model and conducted numerical analyses to assess the impact of model
parameters on deforestation incentives. To address the challenge of parameter
estimation, we proposed a novel deep learning approach that estimates all model
parameters from a single sample containing time-series observations of forest
and agricultural land proportions. This innovative approach enables us to
understand forest transition dynamics and deforestation trends at any future
time.

</details>


### [192] [An Equal-Probability Partition of the Sample Space: A Non-parametric Inference from Finite Samples](https://arxiv.org/abs/2507.21712)
*Urban Eriksson*

Main category: stat.ML

TL;DR: 研究从有限样本推断连续概率分布，发现样本点划分实轴各段期望概率质量相等，有离散熵，对比传统方法并讨论其推断意义。


<details>
  <summary>Details</summary>
Motivation: 探究从有限样本能对任意连续概率分布做出何种推断。

Method: 基于顺序统计量的基本性质进行分析。

Result: N个排序样本点将实轴划分为N + 1段，每段期望概率质量为1/(N + 1)，离散熵为log₂(N + 1)比特。

Conclusion: 此划分框架可用于稳健的非参数推断，尤其在密度和尾部估计方面有意义。

Abstract: This paper investigates what can be inferred about an arbitrary continuous
probability distribution from a finite sample of $N$ observations drawn from
it. The central finding is that the $N$ sorted sample points partition the real
line into $N+1$ segments, each carrying an expected probability mass of exactly
$1/(N+1)$. This non-parametric result, which follows from fundamental
properties of order statistics, holds regardless of the underlying
distribution's shape. This equal-probability partition yields a discrete
entropy of $\log_2(N+1)$ bits, which quantifies the information gained from the
sample and contrasts with Shannon's results for continuous variables. I compare
this partition-based framework to the conventional ECDF and discuss its
implications for robust non-parametric inference, particularly in density and
tail estimation.

</details>


### [193] [MIBoost: A Gradient Boosting Algorithm for Variable Selection After Multiple Imputation](https://arxiv.org/abs/2507.21807)
*Robert Kuchen*

Main category: stat.ML

TL;DR: 介绍自动化变量选择统计学习方法遇缺失数据问题，提出MIBoost算法，模拟研究显示其预测性能与近期方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有自动化变量选择统计学习方法在处理含缺失数据时，模型选择存在争议，简单策略效果不佳，复杂方法难实施。

Method: 提出MIBoost算法，将统一原则拓展到分量梯度提升框架，在插补数据集上采用统一变量选择机制。

Result: 模拟研究表明MIBoost算法的预测性能与近期提出的方法相当。

Conclusion: MIBoost算法为处理含缺失数据的自动化变量选择问题提供了有效解决方案。

Abstract: Statistical learning methods for automated variable selection, such as LASSO,
elastic nets, or gradient boosting, have become increasingly popular tools for
building powerful prediction models. Yet, in practice, analyses are often
complicated by missing data. The most widely used approach to address
missingness is multiple imputation, which creates several completed datasets.
However, there is an ongoing debate on how to perform model selection in the
presence of multiple imputed datasets. Simple strategies, such as pooling
models across datasets, have been shown to have suboptimal properties. Although
more sophisticated methods exist, they are often difficult to implement and
therefore not widely applied. In contrast, two recent approaches modify the
regularization methods LASSO and elastic nets by defining a single loss
function, resulting in a unified set of coefficients across imputations. Our
key contribution is to extend this principle to the framework of component-wise
gradient boosting by proposing MIBoost, a novel algorithm that employs a
uniform variable-selection mechanism across imputed datasets. Simulation
studies suggest that our approach yields prediction performance comparable to
that of these recently proposed methods.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [194] [Sample Complexity of Branch-length Estimation by Maximum Likelihood](https://arxiv.org/abs/2507.22038)
*David Clancy Jr.,Hanbaek Lyu,Sebastien Roch*

Main category: stat.CO

TL;DR: 本文研究二叉树分支长度估计问题，为简单坐标最大化算法表现良好提供理论保证，表明在特定条件下对数似然函数强凹且平滑，算法能快速收敛到接近真实参数的最大似然估计。


<details>
  <summary>Details</summary>
Motivation: 解决分支长度估计问题中简单坐标最大化算法实践表现好但缺乏理论保证的问题。

Method: 研究在Kesten - Stigum重建区域内，假设树平衡且有多项式数量样本时，分析对数似然函数的性质。

Result: 存在与树大小无关的通用参数区域，对数似然函数大概率强凹且平滑；标准坐标最大化算法在足够接近初始点时，能指数快速收敛到最大似然估计，该估计与真实参数误差在O(1/√m)内。

Conclusion: 为简单坐标最大化算法在分支长度估计问题中的有效性提供了理论依据。

Abstract: We consider the branch-length estimation problem on a bifurcating tree: a
character evolves along the edges of a binary tree according to a two-state
symmetric Markov process, and we seek to recover the edge transition
probabilities from repeated observations at the leaves. This problem arises in
phylogenetics, and is related to latent tree graphical model inference. In
general, the log-likelihood function is non-concave and may admit many critical
points. Nevertheless, simple coordinate maximization has been known to perform
well in practice, defying the complexity of the likelihood landscape. In this
work, we provide the first theoretical guarantee as to why this might be the
case. We show that deep inside the Kesten-Stigum reconstruction regime,
provided with polynomially many $m$ samples (assuming the tree is balanced),
there exists a universal parameter regime (independent of the size of the tree)
where the log-likelihood function is strongly concave and smooth with high
probability. On this high-probability likelihood landscape event, we show that
the standard coordinate maximization algorithm converges exponentially fast to
the maximum likelihood estimator, which is within $O(1/\sqrt{m})$ from the true
parameter, provided a sufficiently close initial point.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [195] [Comparative Analysis of Vision Transformers and Convolutional Neural Networks for Medical Image Classification](https://arxiv.org/abs/2507.21156)
*Kunal Kawadkar*

Main category: eess.IV

TL;DR: 研究对比CNN和ViT架构在三种医学影像任务中的表现，评估四个模型，结果显示各模型有任务特定优势，为医学AI架构选择提供见解。


<details>
  <summary>Details</summary>
Motivation: 探索Vision Transformers (ViTs)与传统卷积神经网络(CNNs)在医学影像中的有效性，目前该领域研究不足。

Method: 对CNN和ViT架构在胸部X光肺炎检测、脑肿瘤分类和皮肤癌黑色素瘤检测三个医学影像任务中进行全面对比分析，评估ResNet - 50、EfficientNet - B0、ViT - Base和DeiT - Small四个模型，使用8469张医学影像数据集。

Result: ResNet - 50在胸部X光分类中准确率达98.37%，DeiT - Small在脑肿瘤检测中准确率92.16%，EfficientNet - B0在皮肤癌分类中准确率81.84%。

Conclusion: 研究结果为医学AI应用的架构选择提供关键见解，强调临床决策支持系统中任务特定架构选择的重要性。

Abstract: The emergence of Vision Transformers (ViTs) has revolutionized computer
vision, yet their effectiveness compared to traditional Convolutional Neural
Networks (CNNs) in medical imaging remains under-explored. This study presents
a comprehensive comparative analysis of CNN and ViT architectures across three
critical medical imaging tasks: chest X-ray pneumonia detection, brain tumor
classification, and skin cancer melanoma detection. We evaluated four
state-of-the-art models - ResNet-50, EfficientNet-B0, ViT-Base, and DeiT-Small
- across datasets totaling 8,469 medical images. Our results demonstrate
task-specific model advantages: ResNet-50 achieved 98.37% accuracy on chest
X-ray classification, DeiT-Small excelled at brain tumor detection with 92.16%
accuracy, and EfficientNet-B0 led skin cancer classification at 81.84%
accuracy. These findings provide crucial insights for practitioners selecting
architectures for medical AI applications, highlighting the importance of
task-specific architecture selection in clinical decision support systems.

</details>


### [196] [ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from Free-Text Reports](https://arxiv.org/abs/2507.22030)
*Mohammed Baharoon,Luyang Luo,Michael Moritz,Abhinav Kumar,Sung Eun Kim,Xiaoman Zhang,Miao Zhu,Mahmoud Hussain Alabbad,Maha Sbayel Alhazmi,Neel P. Mistry,Kent Ryan Kleinschmidt,Brady Chrisler,Sathvik Suryadevara,Sri Sai Dinesh Jaliparthi,Noah Michael Prudlo,Mark David Marino,Jeremy Palacio,Rithvik Akula,Hong-Yu Zhou,Ibrahim Ethem Hamamci,Scott J. Adams,Hassan Rayhan AlOmaish,Pranav Rajpurkar*

Main category: eess.IV

TL;DR: 介绍首个公开的手动标注的ReXGroundingCT数据集，用于将放射学自由文本与3D胸部CT扫描像素级分割关联，可用于开发和评估胸部CT相关模型。


<details>
  <summary>Details</summary>
Motivation: 解决医学AI中连接复杂描述性文本与三维空间精确解剖位置的关键差距，助力基于事实的放射学报告生成系统。

Method: 采用三阶段流程，用GPT - 4提取阳性肺部和胸膜发现，由专家手动分割，经认证放射科医生进行质量控制。

Result: 标注了3142个非对比胸部CT扫描及对应报告，8028个发现涉及16301个实体，约79%为局灶性异常，训练、验证和测试集有不同标注设置。

Conclusion: ReXGroundingCT为胸部CT的句子级定位和自由文本医学分割模型建立了新基准，数据集可在线获取。

Abstract: We present ReXGroundingCT, the first publicly available dataset to link
free-text radiology findings with pixel-level segmentations in 3D chest CT
scans that is manually annotated. While prior datasets have relied on
structured labels or predefined categories, ReXGroundingCT captures the full
expressiveness of clinical language represented in free text and grounds it to
spatially localized 3D segmentation annotations in volumetric imaging. This
addresses a critical gap in medical AI: the ability to connect complex,
descriptive text, such as "3 mm nodule in the left lower lobe", to its precise
anatomical location in three-dimensional space, a capability essential for
grounded radiology report generation systems. The dataset comprises 3,142
non-contrast chest CT scans paired with standardized radiology reports from the
CT-RATE dataset. Using a systematic three-stage pipeline, GPT-4 was used to
extract positive lung and pleural findings, which were then manually segmented
by expert annotators. A total of 8,028 findings across 16,301 entities were
annotated, with quality control performed by board-certified radiologists.
Approximately 79% of findings are focal abnormalities, while 21% are non-focal.
The training set includes up to three representative segmentations per finding,
while the validation and test sets contain exhaustive labels for each finding
entity. ReXGroundingCT establishes a new benchmark for developing and
evaluating sentence-level grounding and free-text medical segmentation models
in chest CT. The dataset can be accessed at
https://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [197] [Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition with Cross-Modal Fusion](https://arxiv.org/abs/2507.21395)
*Zeyu Deng,Yanhui Lu,Jiashu Liao,Shuang Wu,Chongfeng Wei*

Main category: cs.MM

TL;DR: 提出Sync - TVA框架解决多模态情感识别问题，实验显示优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别方法存在跨模态交互有限和各模态贡献不均衡的问题。

Method: 提出Sync - TVA端到端图注意力框架，含特定模态动态增强和结构化跨模态融合，有动态增强模块并构建异质跨模态图，还有交叉注意力融合机制。

Result: 在MELD和IEMOCAP上实验表明，在准确率和加权F1分数上持续优于现有模型，尤其在类别不平衡条件下。

Conclusion: Sync - TVA框架能有效解决现有多模态情感识别方法的问题，提升识别性能。

Abstract: Multimodal emotion recognition (MER) is crucial for enabling emotionally
intelligent systems that perceive and respond to human emotions. However,
existing methods suffer from limited cross-modal interaction and imbalanced
contributions across modalities. To address these issues, we propose Sync-TVA,
an end-to-end graph-attention framework featuring modality-specific dynamic
enhancement and structured cross-modal fusion. Our design incorporates a
dynamic enhancement module for each modality and constructs heterogeneous
cross-modal graphs to model semantic relations across text, audio, and visual
features. A cross-attention fusion mechanism further aligns multimodal cues for
robust emotion inference. Experiments on MELD and IEMOCAP demonstrate
consistent improvements over state-of-the-art models in both accuracy and
weighted F1 score, especially under class-imbalanced conditions.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [198] [Combolutional Neural Networks](https://arxiv.org/abs/2507.21202)
*Cameron Churchwell,Minje Kim,Paris Smaragdis*

Main category: cs.SD

TL;DR: 提出combolutional层用于音频任务，在多任务验证其有效性，显示可替代卷积层及有诸多优势。


<details>
  <summary>Details</summary>
Motivation: 音频数据量大，需选择合适归纳偏置设计机器学习模型。

Method: 提出combolutional层，在时域提取谐波特征，包括学习延迟IIR梳状滤波器和融合包络检测器。

Result: 在三个信息检索任务验证有效性，对比计算成本，给出高效训练实现。

Conclusion: combolutional层可替代卷积层用于精确谐波分析重要的音频任务，且有低参数、高效推理等优势。

Abstract: Selecting appropriate inductive biases is an essential step in the design of
machine learning models, especially when working with audio, where even short
clips may contain millions of samples. To this end, we propose the
combolutional layer: a learned-delay IIR comb filter and fused envelope
detector, which extracts harmonic features in the time domain. We demonstrate
the efficacy of the combolutional layer on three information retrieval tasks,
evaluate its computational cost relative to other audio frontends, and provide
efficient implementations for training. We find that the combolutional layer is
an effective replacement for convolutional layers in audio tasks where precise
harmonic analysis is important, e.g., piano transcription, speaker
classification, and key detection. Additionally, the combolutional layer has
several other key benefits over existing frontends, namely: low parameter
count, efficient CPU inference, strictly real-valued computations, and improved
interpretability.

</details>


### [199] [Whilter: A Whisper-based Data Filter for "In-the-Wild" Speech Corpora Using Utterance-level Multi-Task Classification](https://arxiv.org/abs/2507.21642)
*William Ravenscroft,George Close,Kit Bower-Morris,Jamie Stacey,Dmitry Sityaev,Kris Y. Hong*

Main category: cs.SD

TL;DR: 提出Whilter模型解决大规模野外语音数据集存在的问题，取得较好效果且处理时间有优势。


<details>
  <summary>Details</summary>
Motivation: 大规模野外语音数据集存在如多说话者、非目标语言和音乐等不良特征，影响模型学习，需要解决。

Method: Whilter使用Whisper编码器和基于注意力的分类器同时解决五个不同的分类问题，并发布部分标注数据集。

Result: Whilter在三个子任务中F1分数超85%，等错误率在6.5% - 7.8%，在语音特定类别上优于BEATs分类器，处理时间比单任务组合显著减少。

Conclusion: Whilter模型是解决大规模野外语音数据集不良样本识别的有效多任务解决方案。

Abstract: Large-scale in-the-wild speech datasets have become more prevalent in recent
years due to increased interest in models that can learn useful features from
unlabelled data for tasks such as speech recognition or synthesis. These
datasets often contain undesirable features, such as multiple speakers,
non-target languages, and music, which may impact model learning. The Whilter
model is proposed as a multitask solution to identify these undesirable
samples. Whilter uses a Whisper encoder with an attention-based classifier to
solve five diverse classification problems at once. In addition, an annotated
dataset is published for a subset of two popular in-the-wild corpora. Whilter
achieves F1 scores above 85% and equal error rates of 6.5% to 7.8% for three of
five subtasks, outperforming a state-of-the-art BEATs classifier on
speech-specific classes, with a notable decrease in processing time compared to
a combination of single-task alternatives.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [200] [NIST Post-Quantum Cryptography Standard Algorithms Based on Quantum Random Number Generators](https://arxiv.org/abs/2507.21151)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 量子计算威胁传统加密，NIST发布PQC算法但特定场景有不足，本文提出基于QRNG的PQC算法，设计评估QRNG并测试相关PQC算法性能，为部署提供参考。


<details>
  <summary>Details</summary>
Motivation: 量子计算技术对RSA和椭圆曲线密码学构成威胁，NIST发布的PQC算法在特定场景安全不足。

Method: 提出基于QRNG的PQC算法，构建QRNG通用架构，设计六个QRNG并按NIST标准评估，测试相关PQC算法性能。

Result: 评估了六个QRNG的计算时间及相关PQC算法性能。

Conclusion: 研究结果为未来PQC系统部署提供有价值的参考数据。

Abstract: In recent years, the advancement of quantum computing technology has posed
potential security threats to RSA cryptography and elliptic curve cryptography.
In response, the National Institute of Standards and Technology (NIST)
published several Federal Information Processing Standards (FIPS) of
post-quantum cryptography (PQC) in August 2024, including the
Module-Lattice-Based Key-Encapsulation Mechanism (ML-KEM), Module-Lattice-Based
Digital Signature Algorithm (ML-DSA), and Stateless Hash-Based Digital
Signature Algorithm (SLH-DSA). Although these PQC algorithms are designed to
resist quantum computing attacks, they may not provide adequate security in
certain specialized application scenarios. To address this issue, this study
proposes quantum random number generator (QRNG)-based PQC algorithms. These
algorithms leverage quantum computing to generate random numbers, which serve
as the foundation for key pair generation, key encapsulation, and digital
signature generation. A generalized architecture of QRNG is proposed, along
with the design of six QRNGs. Each generator is evaluated according to the
statistical validation procedures outlined in NIST SP 800-90B, including tests
for verification of entropy sources and independent and identically distributed
(IID) outputs. Experimental results assess the computation time of the six
QRNGs, as well as the performance of QRNG-based ML-KEM, QRNG-based ML-DSA, and
QRNG-based SLH-DSA. These findings provide valuable reference data for future
deployment of PQC systems.

</details>


### [201] [A Formal Rebuttal of "The Blockchain Trilemma: A Formal Proof of the Inherent Trade-Offs Among Decentralization, Security, and Scalability"](https://arxiv.org/abs/2507.21111)
*Craig Wright*

Main category: cs.CR

TL;DR: 本文全面驳斥区块链三难困境，指出其缺陷，重构比特币模型，表明可扩展性是工程结果，并指出学术话语和同行评审问题，给出评估标准。


<details>
  <summary>Details</summary>
Motivation: 驳斥广泛引用但缺乏理论依据的“区块链三难困境”说法。

Method: 通过形式分析、实证证据，详细批判相关方法论和术语。

Result: 证明“区块链三难困境”存在语义模糊、滥用分布式系统理论等问题，重构比特币模型表明可扩展性是工程结果。

Conclusion: 指出学术话语和同行评审中的系统性问题，提供评估区块链研究未来主张的正式标准。

Abstract: This paper presents a comprehensive refutation of the so-called "blockchain
trilemma," a widely cited but formally ungrounded claim asserting an inherent
trade-off between decentralisation, security, and scalability in blockchain
protocols. Through formal analysis, empirical evidence, and detailed critique
of both methodology and terminology, we demonstrate that the trilemma rests on
semantic equivocation, misuse of distributed systems theory, and a failure to
define operational metrics. Particular focus is placed on the conflation of
topological network analogies with protocol-level architecture, the
mischaracterisation of Bitcoin's design--including the role of miners, SPV
clients, and header-based verification--and the failure to ground claims in
complexity-theoretic or adversarial models. By reconstructing Bitcoin as a
deterministic, stateless distribution protocol governed by evidentiary trust,
we show that scalability is not a trade-off but an engineering outcome. The
paper concludes by identifying systemic issues in academic discourse and peer
review that have allowed such fallacies to persist, and offers formal criteria
for evaluating future claims in blockchain research.

</details>


### [202] [Verification Cost Asymmetry in Cognitive Warfare: A Complexity-Theoretic Framework](https://arxiv.org/abs/2507.21258)
*Joshua Luberisse*

Main category: cs.CR

TL;DR: 本文介绍验证成本不对称（VCA）系数，构建传播协议，证明不对称性理论保证，通过用户研究验证框架，为认知战工程民主优势奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决对抗性信息流下的人类验证问题，利用复杂性理论在认知战中获得民主优势。

Method: 引入VCA系数，结合概率可检验证明（PCP）和参数化复杂性理论构建传播协议。

Result: 证明了不对称性的理论保证，通过用户研究验证框架，实现现实信息活动的实际编码。

Conclusion: 为认知战工程民主优势建立了复杂性理论基础，可应用于内容认证、平台治理和信息作战原则。

Abstract: Human verification under adversarial information flow operates as a
cost-bounded decision procedure constrained by working memory limits and
cognitive biases. We introduce the Verification Cost Asymmetry (VCA)
coefficient, formalizing it as the ratio of expected verification work between
populations under identical claim distributions. Drawing on probabilistically
checkable proofs (PCP) and parameterized complexity theory, we construct
dissemination protocols that reduce verification for trusted audiences to
constant human effort while imposing superlinear costs on adversarial
populations lacking cryptographic infrastructure. We prove theoretical
guarantees for this asymmetry, validate the framework through controlled user
studies measuring verification effort with and without spot-checkable
provenance, and demonstrate practical encoding of real-world information
campaigns. The results establish complexity-theoretic foundations for
engineering democratic advantage in cognitive warfare, with immediate
applications to content authentication, platform governance, and information
operations doctrine.

</details>


### [203] [HexaMorphHash HMH- Homomorphic Hashing for Secure and Efficient Cryptographic Operations in Data Integrity Verification](https://arxiv.org/abs/2507.21096)
*Krishnendu Das*

Main category: cs.CR

TL;DR: 本文提出基于格的同态哈希函数HexaMorphHash，用于分布式系统处理频繁更新数据集，在计算效率、内存使用和可扩展性上有显著进步。


<details>
  <summary>Details</summary>
Motivation: 传统哈希方法在动态环境中处理节点变更时需大量重新哈希，一致性哈希在负载均衡和可扩展性上有局限，需要新方法处理分布式系统频繁更新数据集问题。

Method: 引入基于格的同态哈希函数HexaMorphHash，利用短整数解（SIS）问题的复杂性保障安全性。

Result: 与现有方法对比，在计算效率、内存使用和可扩展性上有显著提升。

Conclusion: 该方法为分布式系统频繁更新数据传播提供可行方案，保障数据完整性和系统性能。

Abstract: In the realm of big data and cloud computing, distributed systems are tasked
with proficiently managing, storing, and validating extensive datasets across
numerous nodes, all while maintaining robust data integrity. Conventional
hashing methods, though straightforward, encounter substan tial difficulties in
dynamic settings due to the necessity for thorough rehashing when nodes are
altered. Consistent hashing mitigates some of these challenges by reducing data
redistribution; however, it still contends with limitations in load balancing
and scalability under intensive update conditions. This paper introduces an
innovative approach using a lattice based homomorphic hash function
HexaMorphHash that facilitates constant time, incremental updates while
preserving a constant digest size. By utilizing the complexity of the Short
Integer Solutions SIS problem, our method secures strong protective measures,
even against quantum threats. We further com pare our method with existing ones
such as direct signatures for each update, comprehensive database signing,
Merkle tree based techniques, AdHash, MuHash, ECMH, and homomorphic sig nature
schemes highlighting notable advancements in computational efficiency, memory
usage, and scalability. Our contributions present a viable solution for
frequent update dissemination in expansive distributed systems, safeguarding
both data integrity and system performance.

</details>


### [204] [Security study based on the Chatgptplugin system: ldentifying Security Vulnerabilities](https://arxiv.org/abs/2507.21128)
*Ruomai Ren*

Main category: cs.CR

TL;DR: 分析ChatGPT插件商店插件安全性，揭示漏洞并提改进建议。


<details>
  <summary>Details</summary>
Motivation: 插件系统安全是挑战，ChatGPT插件系统发展但相关安全研究少，需关注其插件安全。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: Plugin systems are a class of external programmes that provide users with a
wide range of functionality, and while they enhance the user experience, their
security is always a challenge. Especially due to the diversity and complexity
of developers, many plugin systems lack adequate regulation. As ChatGPT has
become a popular large-scale language modelling platform, its plugin system is
also gradually developing, and the open platform provides creators with the
opportunity to upload plugins covering a wide range of application scenarios.
However, current research and discussions mostly focus on the security issues
of the ChatGPT model itself, while ignoring the possible security risks posed
by the plugin system. This study aims to analyse the security of plugins in the
ChatGPT plugin shop, reveal its major security vulnerabilities, and propose
corresponding improvements.

</details>


### [205] [Out of Distribution, Out of Luck: How Well Can LLMs Trained on Vulnerability Datasets Detect Top 25 CWE Weaknesses?](https://arxiv.org/abs/2507.21817)
*Yikun Li,Ngoc Tan Bui,Ting Zhang,Martin Weyssow,Chengran Yang,Xin Zhou,Jinfeng Jiang,Junkai Chen,Huihui Huang,Huu Hung Nguyen,Chiok Yew Ho,Jie Tan,Ruiyin Li,Yide Yin,Han Wei Ang,Frank Liauw,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.CR

TL;DR: 现有漏洞检测数据集存在问题致泛化差距大，本文提出BenchVul、TitanVul和RVG框架解决，评估显示各组件有效缩小泛化差距。


<details>
  <summary>Details</summary>
Motivation: 当前自动化漏洞检测研究在现实世界影响有限，现有漏洞数据集存在标签不准确、重复、覆盖不足等问题，导致模型泛化能力差。

Method: 引入手动整理的测试数据集BenchVul；构建高质量训练数据集TitanVul；提出RVG框架合成上下文感知的漏洞示例。

Result: BenchVul显示现有模型自测试局限性；TitanVul提升模型泛化能力；RVG生成的数据进一步提升模型性能。

Conclusion: 所提出的三部分解决方案能有效缩小漏洞检测模型的泛化差距。

Abstract: Automated vulnerability detection research has made substantial progress, yet
its real-world impact remains limited. Current vulnerability datasets suffer
from issues including label inaccuracy rates of 20-71%, extensive duplication,
and poor coverage of critical CWE types. These issues create a significant
"generalization gap" where models achieve misleading self-testing performance
(measured on held-out data from same dataset for training) by exploiting
spurious correlations rather than learning true vulnerability patterns. Our
analysis reveals that many models experience substantial performance drops of
up to 40.6% when evaluated on independent data, sometimes underperforming
random guessing.
  To address these limitations, we present a three-part solution. First, we
introduce a manually curated test dataset, BenchVul, covering the MITRE Top 25
Most Dangerous CWEs. Second, we construct a high-quality training dataset,
TitanVul, comprising 35,045 functions by aggregating seven public sources and
applying deduplication and validation using a novel multi-agent LLM framework.
Third, we propose a Realistic Vulnerability Generation (RVG) framework, which
synthesizes context-aware vulnerability examples for underrepresented but
critical CWE types through simulated development workflows.
  Our evaluation shows the strengths of each component in closing the
generalization gap. First, BenchVul shows the limitations of self-testing:
models trained on existing datasets, such as BigVul and PrimeVul, experience
performance drops on BenchVul (from 0.776 to 0.519 and from 0.567 to 0.337).
Second, training models on TitanVul demonstrates improved generalization, with
model performance increasing from 0.584 when evaluated on the same dataset to
0.767 when tested on BenchVul. Third, supplementing TitanVul with RVG-generated
data yields further gains, increasing model performance by 14.0% to 0.874.

</details>


### [206] [Privacy-Preserving AI for Encrypted Medical Imaging: A Framework for Secure Diagnosis and Learning](https://arxiv.org/abs/2507.21060)
*Abdullah Al Siam,Sadequzzaman Shohan*

Main category: cs.CR

TL;DR: 提出用改进CNN对加密医学图像进行隐私保护诊断推断框架，实验表明加密模型性能与未加密相当，可平衡数据隐私与临床效用。


<details>
  <summary>Details</summary>
Motivation: 人工智能融入医学诊断使患者隐私保护成为紧迫问题，需解决敏感图像数据传输、存储和处理时的隐私问题。

Method: 提出使用改进的卷积神经网络（Masked - CNN）对加密医学图像进行隐私保护诊断推断的框架，利用AES - CBC加密和JPEG2000压缩保护图像。

Result: 使用公共DICOM数据集评估，加密推理模型性能与未加密相当，仅在准确性和延迟上有微小权衡。

Conclusion: 所提框架弥合了数据隐私和临床实用性之间的差距，为安全的人工智能驱动诊断提供实用、可扩展的解决方案。

Abstract: The rapid integration of Artificial Intelligence (AI) into medical
diagnostics has raised pressing concerns about patient privacy, especially when
sensitive imaging data must be transferred, stored, or processed. In this
paper, we propose a novel framework for privacy-preserving diagnostic inference
on encrypted medical images using a modified convolutional neural network
(Masked-CNN) capable of operating on transformed or ciphered image formats. Our
approach leverages AES-CBC encryption coupled with JPEG2000 compression to
protect medical images while maintaining their suitability for AI inference. We
evaluate the system using public DICOM datasets (NIH ChestX-ray14 and
LIDC-IDRI), focusing on diagnostic accuracy, inference latency, storage
efficiency, and privacy leakage resistance. Experimental results show that the
encrypted inference model achieves performance comparable to its unencrypted
counterpart, with only marginal trade-offs in accuracy and latency. The
proposed framework bridges the gap between data privacy and clinical utility,
offering a practical, scalable solution for secure AI-driven diagnostics.

</details>


### [207] [Generating Adversarial Point Clouds Using Diffusion Model](https://arxiv.org/abs/2507.21163)
*Ruiyang Zhao,Bingbing Zhu,Chuxuan Tong,Xiaoyi Zhou,Xi Zheng*

Main category: cs.CR

TL;DR: 本文提出基于扩散模型的3D点云分类黑盒对抗样本生成方法，提高攻击成功率和不可感知性。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云分类对抗攻击方法中，白盒攻击在现实场景适用性有限，黑盒攻击效果差，需更好的黑盒攻击方法。

Method: 利用3D扩散模型，以点云压缩特征为先验知识指导反向扩散过程添加对抗点，将其他类别分布转化为对抗点添加到点云。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Adversarial attack methods for 3D point cloud classification reveal the
vulnerabilities of point cloud recognition models. This vulnerability could
lead to safety risks in critical applications that use deep learning models,
such as autonomous vehicles. To uncover the deficiencies of these models,
researchers can evaluate their security through adversarial attacks. However,
most existing adversarial attack methods are based on white-box attacks. While
these methods achieve high attack success rates and imperceptibility, their
applicability in real-world scenarios is limited. Black-box attacks, which are
more meaningful in real-world scenarios, often yield poor results. This paper
proposes a novel black-box adversarial example generation method that utilizes
a diffusion model to improve the attack success rate and imperceptibility in
the black-box setting, without relying on the internal information of the point
cloud classification model to generate adversarial samples. We use a 3D
diffusion model to use the compressed features of the point cloud as prior
knowledge to guide the reverse diffusion process to add adversarial points to
clean examples. Subsequently, its reverse process is employed to transform the
distribution of other categories into adversarial points, which are then added
to the point cloud.

</details>


### [208] [FedBAP: Backdoor Defense via Benign Adversarial Perturbation in Federated Learning](https://arxiv.org/abs/2507.21177)
*Xinhai Yan,Libing Wu,Zhuangzhuang Zhang,Bingyi Liu,Lijuan Huo,Jing Wang*

Main category: cs.CR

TL;DR: 提出FedBAP防御框架减轻联邦学习后门攻击，通过生成扰动触发器、良性对抗扰动和自适应缩放机制，实验显示能降低不同后门攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习防御方法因忽视模型对后门触发器的过度依赖，在恶意客户端比例增加时效果有限，需新防御方法。

Method: 提出扰动触发器生成机制，利用其生成良性对抗扰动，设计自适应缩放机制动态调整扰动强度。

Result: FedBAP在三种类型后门攻击下分别降低攻击成功率0.22%-5.34%、0.48%-6.34%和97.22%-97.6%，对新型后门攻击表现出色。

Conclusion: FedBAP能有效减轻联邦学习中的后门攻击，降低攻击成功率，尤其对新型攻击效果好。

Abstract: Federated Learning (FL) enables collaborative model training while preserving
data privacy, but it is highly vulnerable to backdoor attacks. Most existing
defense methods in FL have limited effectiveness due to their neglect of the
model's over-reliance on backdoor triggers, particularly as the proportion of
malicious clients increases. In this paper, we propose FedBAP, a novel defense
framework for mitigating backdoor attacks in FL by reducing the model's
reliance on backdoor triggers. Specifically, first, we propose a perturbed
trigger generation mechanism that creates perturbation triggers precisely
matching backdoor triggers in location and size, ensuring strong influence on
model outputs. Second, we utilize these perturbation triggers to generate
benign adversarial perturbations that disrupt the model's dependence on
backdoor triggers while forcing it to learn more robust decision boundaries.
Finally, we design an adaptive scaling mechanism to dynamically adjust
perturbation intensity, effectively balancing defense strength and model
performance. The experimental results demonstrate that FedBAP reduces the
attack success rates by 0.22%-5.34%, 0.48%-6.34%, and 97.22%-97.6% under three
types of backdoor attacks, respectively. In particular, FedBAP demonstrates
outstanding performance against novel backdoor attacks.

</details>


### [209] [Interpretable Anomaly-Based DDoS Detection in AI-RAN with XAI and LLMs](https://arxiv.org/abs/2507.21193)
*Sotiris Chatzimiltis,Mohammad Shojafar,Mahdi Boloursaz Mashhadi,Rahim Tafazolli*

Main category: cs.CR

TL;DR: 本文围绕未来无线接入网安全，提出基于多元时间序列KPMs的LLM可解释异常检测系统，经实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 下一代无线接入网需增强安全，解决大语言模型辅助可解释入侵检测的机遇、挑战和研究空白。

Method: 提出基于多元时间序列KPMs的LLM可解释异常检测系统，用LSTM模型识别恶意行为，用LIME和SHAP解释预测，用LLM转换技术解释。

Result: 在真实5G网络KPMs上实验，框架检测准确率高（F1分数>0.96），输出可操作且可解释。

Conclusion: 所提系统在未来无线接入网安全的DDoS攻击检测中有效，兼具高准确率和可解释性。

Abstract: Next generation Radio Access Networks (RANs) introduce programmability,
intelligence, and near real-time control through intelligent controllers,
enabling enhanced security within the RAN and across broader 5G/6G
infrastructures. This paper presents a comprehensive survey highlighting
opportunities, challenges, and research gaps for Large Language Models
(LLMs)-assisted explainable (XAI) intrusion detection (IDS) for secure future
RAN environments. Motivated by this, we propose an LLM interpretable
anomaly-based detection system for distributed denial-of-service (DDoS) attacks
using multivariate time series key performance measures (KPMs), extracted from
E2 nodes, within the Near Real-Time RAN Intelligent Controller (Near-RT RIC).
An LSTM-based model is trained to identify malicious User Equipment (UE)
behavior based on these KPMs. To enhance transparency, we apply post-hoc local
explainability methods such as LIME and SHAP to interpret individual
predictions. Furthermore, LLMs are employed to convert technical explanations
into natural-language insights accessible to non-expert users. Experimental
results on real 5G network KPMs demonstrate that our framework achieves high
detection accuracy (F1-score > 0.96) while delivering actionable and
interpretable outputs.

</details>


### [210] [Cascading and Proxy Membership Inference Attacks](https://arxiv.org/abs/2507.21412)
*Yuntao Du,Jiacheng Li,Yuetian Chen,Kaiyuan Zhang,Zhizhen Yuan,Hanshen Xiao,Bruno Ribeiro,Ninghui Li*

Main category: cs.CR

TL;DR: 本文将成员推理攻击（MIA）分为自适应和非自适应两类，分别提出Cascading Membership Inference Attack (CMIA) 和Proxy Membership Inference Attack (PMIA)，理论分析和实验表明二者优于现有MIA。


<details>
  <summary>Details</summary>
Motivation: 评估训练的机器学习模型对其训练数据的信息泄露程度，提升成员推理攻击性能以评估隐私风险。

Method: 在自适应设置下提出CMIA，通过条件影子训练纳入实例间成员依赖关系；在非自适应设置下提出PMIA，采用代理选择策略进行成员后验几率测试。

Result: CMIA和PMIA在两种设置下，尤其在低误报率情况下，大幅优于现有MIA。

Conclusion: CMIA和PMIA能有效提升成员推理攻击性能，对评估隐私风险具有重要意义。

Abstract: A Membership Inference Attack (MIA) assesses how much a trained machine
learning model reveals about its training data by determining whether specific
query instances were included in the dataset. We classify existing MIAs into
adaptive or non-adaptive, depending on whether the adversary is allowed to
train shadow models on membership queries. In the adaptive setting, where the
adversary can train shadow models after accessing query instances, we highlight
the importance of exploiting membership dependencies between instances and
propose an attack-agnostic framework called Cascading Membership Inference
Attack (CMIA), which incorporates membership dependencies via conditional
shadow training to boost membership inference performance.
  In the non-adaptive setting, where the adversary is restricted to training
shadow models before obtaining membership queries, we introduce Proxy
Membership Inference Attack (PMIA). PMIA employs a proxy selection strategy
that identifies samples with similar behaviors to the query instance and uses
their behaviors in shadow models to perform a membership posterior odds test
for membership inference. We provide theoretical analyses for both attacks, and
extensive experimental results demonstrate that CMIA and PMIA substantially
outperform existing MIAs in both settings, particularly in the low
false-positive regime, which is crucial for evaluating privacy risks.

</details>


### [211] [Analysis of Threat-Based Manipulation in Large Language Models: A Dual Perspective on Vulnerabilities and Performance Enhancement Opportunities](https://arxiv.org/abs/2507.21133)
*Atil Samancioglu*

Main category: cs.CR

TL;DR: 研究分析三种大语言模型在威胁操纵下的表现，提出评估框架，发现系统漏洞和性能提升，对AI安全和提示工程有意义。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型对基于威胁操纵的复杂响应，挖掘漏洞和性能提升机会。

Method: 对三种大语言模型在6种威胁条件、10个任务领域的3390个实验响应进行分析，引入新的威胁分类和多指标评估框架。

Result: 发现系统漏洞，政策评估在基于角色的威胁下指标显著性最高；大量案例有显著性能提升，效应值达+1336%；存在系统确定性操纵，分析深度和响应质量有显著改善。

Conclusion: 研究结果对AI安全和高风险应用中的提示工程有双重意义。

Abstract: Large Language Models (LLMs) demonstrate complex responses to threat-based
manipulations, revealing both vulnerabilities and unexpected performance
enhancement opportunities. This study presents a comprehensive analysis of
3,390 experimental responses from three major LLMs (Claude, GPT-4, Gemini)
across 10 task domains under 6 threat conditions. We introduce a novel threat
taxonomy and multi-metric evaluation framework to quantify both negative
manipulation effects and positive performance improvements. Results reveal
systematic vulnerabilities, with policy evaluation showing the highest metric
significance rates under role-based threats, alongside substantial performance
enhancements in numerous cases with effect sizes up to +1336%. Statistical
analysis indicates systematic certainty manipulation (pFDR < 0.0001) and
significant improvements in analytical depth and response quality. These
findings have dual implications for AI safety and practical prompt engineering
in high-stakes applications.

</details>


### [212] [Privacy Artifact ConnecTor (PACT): Embedding Enterprise Artifacts for Compliance AI Agents](https://arxiv.org/abs/2507.21142)
*Chenhao Fang,Yanqing Peng,Rajeev Rao,Matt Sarmiento,Wendy Summer,Arya Pudota,Alex Goncalves,Jordi Mola,Hervé Robert*

Main category: cs.CR

TL;DR: 提出PACT模型解决企业隐私合规问题，实验显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 企业环境中隐私风险评估和合规所需信息分散，需系统识别不同工件关联。

Method: 提出基于嵌入驱动的图PACT，利用DRAGON嵌入模型，通过对比学习目标和微调，基于文本组件关联工件。

Result: PACT微调模型提升了召回率、查询匹配率和命中率。

Conclusion: PACT能有效解决企业大规模隐私合规问题，提升相关指标。

Abstract: Enterprise environments contain a heterogeneous, rapidly growing collection
of internal artifacts related to code, data, and many different tools. Critical
information for assessing privacy risk and ensuring regulatory compliance is
often embedded across these varied resources, each with their own arcane
discovery and extraction techniques. Therefore, large-scale privacy compliance
in adherence to governmental regulations requires systems to discern the
interconnected nature of diverse artifacts in a common, shared universe.
  We present Privacy Artifact ConnecT or (PACT), an embeddings-driven graph
that links millions of artifacts spanning multiple artifact types generated by
a variety of teams and projects. Powered by the state-of-the-art DRAGON
embedding model, PACT uses a contrastive learning objective with light
fine-tuning to link artifacts via their textual components such as raw
metadata, ownership specifics, and compliance context. Experimental results
show that PACT's fine-tuned model improves recall@1 from 18% to 53%, the query
match rate from 9.6% to 69.7% when paired with a baseline AI agent, and the
hitrate@1 from 25.7% to 44.9% for candidate selection in a standard recommender
system.

</details>


### [213] [Towards Unifying Quantitative Security Benchmarking for Multi Agent Systems](https://arxiv.org/abs/2507.21146)
*Gauri Sharma,Vidhi Kulkarni,Miles King,Ken Huang*

Main category: cs.CR

TL;DR: 本文针对多智能体架构的级联安全风险，定义了Agent Cascading Injection攻击向量，分析其特性并映射到OWASP风险类别，强调需定量基准框架评估通信协议安全，还给出压力测试方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体架构在带来强大功能的同时引入了新的安全风险，如级联风险，需要对其进行研究和防范。

Method: 定义Agent Cascading Injection攻击向量，用对抗目标方程和关键变量形式化攻击，分析其特性并映射到OWASP风险类别，提出压力测试多智能体系统的方法。

Result: 明确了攻击向量，分析了攻击特性，将其映射到相关风险类别，提出了压力测试方法。

Conclusion: ACI凸显了定量基准框架对评估智能体间通信协议安全性的关键需求，为工程师提供了评估系统弹性、进行架构权衡和开发防御措施的工具。

Abstract: Evolving AI systems increasingly deploy multi-agent architectures where
autonomous agents collaborate, share information, and delegate tasks through
developing protocols. This connectivity, while powerful, introduces novel
security risks. One such risk is a cascading risk: a breach in one agent can
cascade through the system, compromising others by exploiting inter-agent
trust. In tandem with OWASP's initiative for an Agentic AI Vulnerability
Scoring System we define an attack vector, Agent Cascading Injection, analogous
to Agent Impact Chain and Blast Radius, operating across networks of agents. In
an ACI attack, a malicious input or tool exploit injected at one agent leads to
cascading compromises and amplified downstream effects across agents that trust
its outputs. We formalize this attack with an adversarial goal equation and key
variables (compromised agent, injected exploit, polluted observations, etc.),
capturing how a localized vulnerability can escalate into system-wide failure.
We then analyze ACI's properties -- propagation chains, amplification factors,
and inter-agent compound effects -- and map these to OWASP's emerging Agentic
AI risk categories (e.g. Impact Chain and Orchestration Exploits). Finally, we
argue that ACI highlights a critical need for quantitative benchmarking
frameworks to evaluate the security of agent-to-agent communication protocols.
We outline a methodology for stress-testing multi-agent systems (using
architectures such as Google's A2A and Anthropic's MCP) against cascading trust
failures, developing upon groundwork for measurable, standardized
agent-to-agent security evaluation. Our work provides the necessary apparatus
for engineers to benchmark system resilience, make data-driven architectural
trade-offs, and develop robust defenses against a new generation of agentic
threats.

</details>


### [214] [OneShield -- the Next Generation of LLM Guardrails](https://arxiv.org/abs/2507.21170)
*Chad DeLuca,Anna Lisa Gentile,Shubhi Asthana,Bing Zhang,Pawan Chowdhary,Kellen Cheng,Basel Shbita,Pengyuan Li,Guang-Jie Ren,Sandeep Gopisetty*

Main category: cs.CR

TL;DR: 大语言模型兴起引发应用热潮，但存在安全等问题，本文提出独立、模型无关且可定制的OneShield解决方案保护LLM并介绍相关情况。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在安全、隐私和伦理问题，且通用防护方案不可行，需针对LLM的风险保护方案。

Method: 提出OneShield解决方案，提供定义风险因素、表达声明安全合规策略和降低LLM风险的功能，关注特定客户。

Result: 文中描述了框架实现、可扩展性考虑，并给出OneShield首次部署后的使用统计。

Conclusion: OneShield是解决大语言模型风险防护问题的可行方案。

Abstract: The rise of Large Language Models has created a general excitement about the
great potential for a myriad of applications. While LLMs offer many
possibilities, questions about safety, privacy, and ethics have emerged, and
all the key actors are working to address these issues with protective measures
for their own models and standalone solutions. The constantly evolving nature
of LLMs makes the task of universally shielding users against their potential
risks extremely challenging, and one-size-fits-all solutions unfeasible. In
this work, we propose OneShield, our stand-alone, model-agnostic and
customizable solution to safeguard LLMs. OneShield aims to provide facilities
for defining risk factors, expressing and declaring contextual safety and
compliance policies, and mitigating LLM risks, with a focus on each specific
customer. We describe the implementation of the framework, the scalability
considerations and provide usage statistics of OneShield since its first
deployment.

</details>


### [215] [SDD: Self-Degraded Defense against Malicious Fine-tuning](https://arxiv.org/abs/2507.21182)
*Zixuan Chen,Weikai Lu,Xin Lin,Ziqian Zeng*

Main category: cs.CR

TL;DR: 文章提出Self - Degraded Defense (SDD)框架抵御大语言模型恶意微调攻击，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有开源大语言模型安全对齐方法易被恶意微调绕过，需找到对抗策略。

Method: 理论分析恶意微调成功原因并确定防御策略，引入SDD框架，让模型对有害提示生成高质量但不相关回复。

Result: 实验结果证实SDD框架能有效抵御此类攻击。

Conclusion: SDD框架可作为防御大语言模型恶意微调攻击的有效手段。

Abstract: Open-source Large Language Models (LLMs) often employ safety alignment
methods to resist harmful instructions. However, recent research shows that
maliciously fine-tuning these LLMs on harmful data can easily bypass these
safeguards. To counter this, we theoretically uncover why malicious fine-tuning
succeeds and identify potential defense strategies. Building on the theoretical
analysis, we introduce the Self-Degraded Defense (SDD) framework. SDD
encourages LLMs to produce high-quality but irrelevant responses to harmful
prompts. When attackers attempt malicious fine-tuning, the general capability
of the LLM aligned by SDD will significantly decrease, rendering it incapable
of following harmful instructions. Our experimental results confirm SDD's
effectiveness against such attacks.

</details>


### [216] [MaXsive: High-Capacity and Robust Training-Free Generative Image Watermarking in Diffusion Models](https://arxiv.org/abs/2507.21195)
*Po-Yuan Mao,Cheng-Chang Tsai,Chun-Shien Lu*

Main category: cs.CR

TL;DR: 提出训练无关的扩散模型生成水印技术MaXsive，利用初始噪声和X形状模板，有高容量和鲁棒性，在两个基准上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型成功引发版权保护和不当内容生成问题，现有训练无关水印方法易受RST攻击，部分方法降低水印容量导致ID冲突。

Method: MaXsive利用初始噪声水印扩散模型，注入X形状模板恢复RST失真。

Result: MaXsive在两个知名水印基准的验证和识别场景中验证了有效性。

Conclusion: MaXsive设计显著提高鲁棒性且不损失容量，减少ID冲突可能性。

Abstract: The great success of the diffusion model in image synthesis led to the
release of gigantic commercial models, raising the issue of copyright
protection and inappropriate content generation. Training-free diffusion
watermarking provides a low-cost solution for these issues. However, the prior
works remain vulnerable to rotation, scaling, and translation (RST) attacks.
Although some methods employ meticulously designed patterns to mitigate this
issue, they often reduce watermark capacity, which can result in identity (ID)
collusion. To address these problems, we propose MaXsive, a training-free
diffusion model generative watermarking technique that has high capacity and
robustness. MaXsive best utilizes the initial noise to watermark the diffusion
model. Moreover, instead of using a meticulously repetitive ring pattern, we
propose injecting the X-shape template to recover the RST distortions. This
design significantly increases robustness without losing any capacity, making
ID collusion less likely to happen. The effectiveness of MaXsive has been
verified on two well-known watermarking benchmarks under the scenarios of
verification and identification.

</details>


### [217] [NCCR: to Evaluate the Robustness of Neural Networks and Adversarial Examples](https://arxiv.org/abs/2507.21483)
*Pu Shi*

Main category: cs.CR

TL;DR: 本文提出神经元覆盖变化率（NCCR）指标评估神经网络鲁棒性，实验证明该指标有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对神经网络及其输入鲁棒性的评估，为解决此问题开展研究。

Method: 提出NCCR指标，通过监测输入扰动时特定神经元输出的变化来评估。

Result: 在图像识别和说话人识别模型实验中，NCCR能很好评估神经网络或其输入的鲁棒性，还可检测对抗样本。

Conclusion: NCCR指标可有效评估神经网络的鲁棒性，检测对抗样本。

Abstract: Neural networks have received a lot of attention recently, and related
security issues have come with it. Many studies have shown that neural networks
are vulnerable to adversarial examples that have been artificially perturbed
with modification, which is too small to be distinguishable by human
perception. Different attacks and defenses have been proposed to solve these
problems, but there is little research on evaluating the robustness of neural
networks and their inputs. In this work, we propose a metric called the neuron
cover change rate (NCCR) to measure the ability of deep learning models to
resist attacks and the stability of adversarial examples. NCCR monitors
alterations in the output of specifically chosen neurons when the input is
perturbed, and networks with a smaller degree of variation are considered to be
more robust. The results of the experiment on image recognition and the speaker
recognition model show that our metrics can provide a good assessment of the
robustness of neural networks or their inputs. It can also be used to detect
whether an input is adversarial or not, as adversarial examples are always less
robust.

</details>


### [218] [Hierarchical Graph Neural Network for Compressed Speech Steganalysis](https://arxiv.org/abs/2507.21591)
*Mustapha Hemis,Hamza Kheddar,Mohamed Chahine Ghanem,Bachir Boudraa*

Main category: cs.CR

TL;DR: 本文首次将GraphSAGE架构的GNN用于压缩VoIP语音流隐写分析，构建图并捕获分层信息，实验表明检测准确率高、效率提升。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的隐写分析方法存在计算复杂和跨数据集泛化难的问题，引入GNN可利用关系数据提升检测准确性和适应性。

Method: 从VoIP流构建图，使用GraphSAGE捕获分层隐写分析信息。

Result: 能有效揭示VoIP信号中基于QIM的隐写模式，短样本检测准确率超98%，低嵌入率时准确率达95.17%，比现有方法提升2.8%；检测效率高，0.5秒样本平均检测时间低至0.016秒，提升0.003秒。

Conclusion: 该模型在短样本和低嵌入率约束下，能较好平衡检测准确性和效率，适用于在线隐写分析任务。

Abstract: Steganalysis methods based on deep learning (DL) often struggle with
computational complexity and challenges in generalizing across different
datasets. Incorporating a graph neural network (GNN) into steganalysis schemes
enables the leveraging of relational data for improved detection accuracy and
adaptability. This paper presents the first application of a Graph Neural
Network (GNN), specifically the GraphSAGE architecture, for steganalysis of
compressed voice over IP (VoIP) speech streams. The method involves
straightforward graph construction from VoIP streams and employs GraphSAGE to
capture hierarchical steganalysis information, including both fine grained
details and high level patterns, thereby achieving high detection accuracy.
Experimental results demonstrate that the developed approach performs well in
uncovering quantization index modulation (QIM)-based steganographic patterns in
VoIP signals. It achieves detection accuracy exceeding 98 percent even for
short 0.5 second samples, and 95.17 percent accuracy under challenging
conditions with low embedding rates, representing an improvement of 2.8 percent
over the best performing state of the art methods. Furthermore, the model
exhibits superior efficiency, with an average detection time as low as 0.016
seconds for 0.5-second samples an improvement of 0.003 seconds. This makes it
efficient for online steganalysis tasks, providing a superior balance between
detection accuracy and efficiency under the constraint of short samples with
low embedding rates.

</details>


### [219] [GUARD-CAN: Graph-Understanding and Recurrent Architecture for CAN Anomaly Detection](https://arxiv.org/abs/2507.21640)
*Hyeong Seon Kim,Huy Kang Kim*

Main category: cs.CR

TL;DR: 本文提出GUARD - CAN异常检测框架，结合图表示学习和时间序列建模，有效检测四种CAN攻击。


<details>
  <summary>Details</summary>
Motivation: 现代车载网络因CAN缺乏加密和认证面临多种网络威胁，需解决安全问题。

Method: 将CAN消息分割成固定长度窗口并转换为图，利用过完备自编码器和图卷积网络生成图嵌入向量，分组后输入门控循环单元检测异常，在序列和窗口级别进行检测，还基于香农熵分析窗口大小选择。

Result: 该模型能有效检测四种CAN攻击（泛洪、模糊、重放和欺骗攻击）。

Conclusion: 提出的GUARD - CAN模型无需复杂特征工程即可有效检测CAN攻击。

Abstract: Modern in-vehicle networks face various cyber threats due to the lack of
encryption and authentication in the Controller Area Network (CAN). To address
this security issue, this paper presents GUARD-CAN, an anomaly detection
framework that combines graph-based representation learning with time-series
modeling. GUARD-CAN splits CAN messages into fixed-length windows and converts
each window into a graph that preserves message order. To detect anomalies in
the timeaware and structure-aware context at the same window, GUARD-CAN takes
advantage of the overcomplete Autoencoder (AE) and Graph Convolutional Network
(GCN) to generate graph embedding vectors. The model groups these vectors into
sequences and feeds them into the Gated Recurrent Unit (GRU) to detect temporal
anomaly patterns across the graphs. GUARD-CAN performs anomaly detection at
both the sequence level and the window level, and this allows multi-perspective
performance evaluation. The model also verifies the importance of window size
selection through an analysis based on Shannon entropy. As a result, GUARD-CAN
shows that the proposed model detects four types of CAN attacks (flooding,
fuzzing, replay and spoofing attacks) effectively without relying on complex
feature engineering.

</details>


### [220] [Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with Reinforcement Learning for Multimodal Model Security](https://arxiv.org/abs/2507.22037)
*Muzhi Dai,Shixuan Liu,Zhiyuan Zhao,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.CR

TL;DR: 现有多模态大语言模型安全面临挑战，提出SecTOW方法提升其安全性，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型安全存在问题，不安全输入稀疏，现有防护方法有局限，传统监督微调会过度拒绝无害输入。

Method: 提出Secure Tug - of - War（SecTOW）方法，含防御者和辅助攻击者两个模块，用强化学习（GRPO）迭代训练，设计奖励机制和质量监测机制。

Result: 在特定安全和通用基准测试中，SecTOW显著提升安全性并保留通用性能。

Conclusion: SecTOW方法能有效增强多模态大语言模型的安全性，同时保证通用性能。

Abstract: The rapid advancement of multimodal large language models (MLLMs) has led to
breakthroughs in various applications, yet their security remains a critical
challenge. One pressing issue involves unsafe image-query pairs--jailbreak
inputs specifically designed to bypass security constraints and elicit
unintended responses from MLLMs. Compared to general multimodal data, such
unsafe inputs are relatively sparse, which limits the diversity and richness of
training samples available for developing robust defense models. Meanwhile,
existing guardrail-type methods rely on external modules to enforce security
constraints but fail to address intrinsic vulnerabilities within MLLMs.
Traditional supervised fine-tuning (SFT), on the other hand, often over-refuses
harmless inputs, compromising general performance. Given these challenges, we
propose Secure Tug-of-War (SecTOW), an innovative iterative defense-attack
training method to enhance the security of MLLMs. SecTOW consists of two
modules: a defender and an auxiliary attacker, both trained iteratively using
reinforcement learning (GRPO). During the iterative process, the attacker
identifies security vulnerabilities in the defense model and expands jailbreak
data. The expanded data are then used to train the defender, enabling it to
address identified security vulnerabilities. We also design reward mechanisms
used for GRPO to simplify the use of response labels, reducing dependence on
complex generative labels and enabling the efficient use of synthetic data.
Additionally, a quality monitoring mechanism is used to mitigate the defender's
over-refusal of harmless inputs and ensure the diversity of the jailbreak data
generated by the attacker. Experimental results on safety-specific and general
benchmarks demonstrate that SecTOW significantly improves security while
preserving general performance.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [221] [Cash and Cognition: The Impact of Transfer Timing on Standardized Test Performance and Human Capital](https://arxiv.org/abs/2507.21393)
*Axel Eizmendi Larrinaga,Germán Reyes*

Main category: econ.GN

TL;DR: 研究表明向低收入家庭的货币转移支付时间影响学生标准化考试成绩，提前收到转移支付可提高成绩，对大额受助者影响更大，还能提升人力资本积累，提示可优化支付安排。


<details>
  <summary>Details</summary>
Motivation: 探究货币转移支付时间对低收入家庭学生标准化考试认知表现的影响。

Method: 将世界最大有条件现金转移支付项目行政记录与18.5万受益家庭高中生大学入学考试结果结合，利用支付日期随机变化进行分析。

Result: 考试前几天收到转移支付比后一周收到成绩提高0.01个标准差，影响集中在最后和较简单问题；大额受助者人力资本积累有持续提升，大学入学率提高0.6个百分点，7年后毕业和正式就业率更高。

Conclusion: 高风险事件中的短期流动性约束有长期影响，可通过优化支付安排改进社会项目。

Abstract: This paper shows that the timing of monetary transfers to low-income families
affects students' cognitive performance on high-stakes standardized tests. We
combine administrative records from the world's largest conditional cash
transfer program with college admission exam results of 185,000 high school
students from beneficiary families. Exploiting random variation in payment
dates, we find that receiving the transfer in the days preceding the exam
increases test scores by 0.01 standard deviations relative to receiving it the
subsequent week. Question-level analysis reveals that effects are concentrated
in final questions and easier questions, suggesting improved cognitive
endurance and effort allocation. The impacts are largest for recipients of
larger transfers, who experience persistent gains in human capital
accumulation: their college enrollment increases by 0.6 percentage points, with
higher graduation and formal employment rates seven years later. Our findings
show that short-term liquidity constraints during high-stakes events can have
long-lasting implications, and suggest opportunities to improve social programs
through improved payment scheduling.

</details>


### [222] [Markowitz Variance May Vastly Undervalue or Overestimate Portfolio Variance and Risks](https://arxiv.org/abs/2507.21824)
*Victor Olkhov*

Main category: econ.GN

TL;DR: 考虑不交易投资组合股票的投资者，利用市场交易估计组合收益、方差和风险，指出Markowitz方差是近似，市场方差依赖交易成交量波动，系数变化会影响评估，建议主要投资者等使用市场方差。


<details>
  <summary>Details</summary>
Motivation: 准确估计不交易股票的投资者投资组合的当前回报、方差和风险，避免因方差评估错误导致的风险估计失误和损失。

Method: 通过分析证券组合市场连续交易的时间序列，推导市场方差关于交易成交量波动系数变化的泰勒级数，考虑三种极限情况。

Result: Markowitz方差是在连续交易成交量恒定假设下的近似，交易成交量波动系数变化会使Markowitz评估高估或低估市场方差。

Conclusion: 主要投资者、投资组合经理和宏观经济模型开发者应使用市场方差调整预测。

Abstract: We consider the investor who doesn't trade shares of his portfolio. The
investor only observes the current trades made in the market with his
securities to estimate the current return, variance, and risks of his unchanged
portfolio. We show how the time series of consecutive trades made in the market
with the securities of the portfolio can determine the time series that model
the trades with the portfolio as with a single security. That establishes the
equal description of the market-based variance of the securities and of the
portfolio composed of these securities that account for the fluctuations of the
volumes of the consecutive trades. We show that Markowitz's (1952) variance
describes only the approximation when all volumes of the consecutive trades
with securities are assumed constant. The market-based variance depends on the
coefficient of variation of fluctuations of volumes of trades. To emphasize
this dependence and to estimate possible deviation from Markowitz variance, we
derive the Taylor series of the market-based variance up to the 2nd term by the
coefficient of variation, taking Markowitz variance as a zero approximation. We
consider three limiting cases with low and high fluctuations of the portfolio
returns, and with a zero covariance of trade values and volumes and show that
the impact of the coefficient of variation of trade volume fluctuations can
cause Markowitz's assessment to highly undervalue or overestimate the
market-based variance of the portfolio. Incorrect assessments of the variances
of securities and of the portfolio cause wrong risk estimates, disturb optimal
portfolio selection, and result in unexpected losses. The major investors,
portfolio managers, and developers of macroeconomic models like BlackRock, JP
Morgan, and the U.S. Fed should use market-based variance to adjust their
predictions to the randomness of market trades.

</details>


### [223] [From macro to micro: Economic complexity indicators for firm growth](https://arxiv.org/abs/2507.21754)
*Valerio De Stefano,Maddalena Mula,Manuel Sebastian Mariani,Andrea Zaccaria*

Main category: econ.GN

TL;DR: 本文研究企业出口篮子特征与未来绩效的关系，发现出口高复杂度产品、核心外多元化利于企业增长。


<details>
  <summary>Details</summary>
Motivation: 以往理论未给出能预测企业增长的产品层面指标，本文旨在探究企业出口篮子的哪些特征能预测未来绩效。

Method: 分析涵盖12,852家意大利企业出口和财务数据的纵向数据集，引入基于算法检测生产块的新多元化衡量指标。

Result: 出口富裕国家常出口产品的企业增长和人均利润更高；核心外多元化与未来增长正相关，核心内多元化负相关。

Conclusion: 企业增长不仅取决于出口产品数量，还与产品在生产生态系统中的位置有关。

Abstract: A rich theoretical and empirical literature investigated the link between
export diversification and firm performance. Prior theoretical works hinted at
the key role of capability accumulation in shaping production activities and
performance, without however producing product-level indicators able to
forecast corporate growth. Building on economic complexity theory and the
corporate growth literature, this paper examines which characteristics of a
firm's export basket predict future performance. We analyze a unique
longitudinal dataset that covers export and financial data for 12,852 Italian
firms. We find that firms exporting products typically exported by wealthier
countries -- a proxy for greater product sophistication and market value --
tend to experience higher growth and profit per employee. Moreover, we find
that diversification outside of a firm's core production area is positively
associated with future growth, whereas diversification within the core is
negatively associated. This is revealed by introducing novel measures of
in-block and out-of-block diversification, based on algorithmically-detected
production blocks. Our findings suggest that growth is driven not just by how
many products a firm exports, but also by where these products lie within the
production ecosystem, at both local and global scales.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [224] [Generative imaging for radio interferometry with fast uncertainty quantification](https://arxiv.org/abs/2507.21270)
*Matthijs Mars,Tobías I. Liaudat,Jessica J. Whitney,Marta M. Betcke,Jason D. McEwen*

Main category: astro-ph.IM

TL;DR: 随着大型射电干涉望远镜发展，需高效图像重建技术。本文提出RI - GAN框架，可实现高效、高质量重建及不确定性量化，推动下一代射电望远镜成像发展。


<details>
  <summary>Details</summary>
Motivation: 大型射电干涉望远镜兴起，现有重建方法计算量大且难以进行不确定性量化，需要高效的图像重建技术。

Method: 构建RI - GAN框架，基于rcGAN框架集成GU - Net架构，使用Wasserstein GANs并结合正则化项解决训练问题。

Result: 该方法能实现高效、高质量图像重建，对不同可见度覆盖具有鲁棒性，可推广到更大动态范围图像，还能提供不确定性量化。

Conclusion: 此方法朝着下一代射电望远镜计算高效、可扩展和具备不确定性感知的成像迈出重要一步。

Abstract: With the rise of large radio interferometric telescopes, particularly the
SKA, there is a growing demand for computationally efficient image
reconstruction techniques. Existing reconstruction methods, such as the CLEAN
algorithm or proximal optimisation approaches, are iterative in nature,
necessitating a large amount of compute. These methods either provide no
uncertainty quantification or require large computational overhead to do so.
Learned reconstruction methods have shown promise in providing efficient and
high quality reconstruction. In this article we explore the use of generative
neural networks that enable efficient approximate sampling of the posterior
distribution for high quality reconstructions with uncertainty quantification.
Our RI-GAN framework, builds on the regularised conditional generative
adversarial network (rcGAN) framework by integrating a gradient U-Net (GU-Net)
architecture - a hybrid reconstruction model that embeds the measurement
operator directly into the network. This framework uses Wasserstein GANs to
improve training stability in combination with regularisation terms that combat
mode collapse, which are typical problems for conditional GANs. This approach
takes as input the dirty image and the point spread function (PSF) of the
observation and provides efficient, high-quality image reconstructions that are
robust to varying visibility coverages, generalises to images with an increased
dynamic range, and provides informative uncertainty quantification. Our methods
provide a significant step toward computationally efficient, scalable, and
uncertainty-aware imaging for next-generation radio telescopes.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [225] [Multiscale geometrical and topological learning in the analysis of soft matter collective dynamics](https://arxiv.org/abs/2507.21265)
*Tetiana Orlova,Amaranta Membrillo Solis,Hayley R. O. Sohn,Tristan Madeleine,Giampaolo D'Alessandro,Ivan I. Smalyukh,Malgosia Kaczmarek,Jacek Brodzki*

Main category: cond-mat.soft

TL;DR: 通过联合几何与拓扑数据分析（TDA）研究液晶Skyrmion阵列的复杂时空动力学，介绍Ψ函数，方法通用可关联图像分析结果与现实系统过程。


<details>
  <summary>Details</summary>
Motivation: 理解动力学多体系统的行为和演化，分析实验捕获图像中的模式，适用于多种自组装系统。

Method: 采用联合几何和拓扑数据分析（TDA），引入Ψ函数；基于Skyrmion集合图像生成向量场进行几何分析。

Result: 为研究系统提供强大框架，洞察系统对外部刺激的非线性物理机制，为与理论预测比较提供基础。

Conclusion: 所提出的方法非常通用，可在个体和整体层面表征系统行为，将图像数据分析结果与现实世界系统过程相关联。

Abstract: Understanding the behavior and evolution of a dynamical many-body system by
analyzing patterns in their experimentally captured images is a promising
method relevant for a variety of living and non-living self-assembled systems.
The arrays of moving liquid crystal skyrmions studied here are a representative
example of hierarchically organized materials that exhibit complex
spatiotemporal dynamics driven by multiscale processes. Joint geometric and
topological data analysis (TDA) offers a powerful framework for investigating
such systems by capturing the underlying structure of the data at multiple
scales. In the TDA approach, we introduce the $\Psi$-function, a robust
numerical topological descriptor related to both the spatiotemporal changes in
the size and shape of individual topological solitons and the emergence of
regions with their different spatial organization. The geometric method based
on the analysis of vector fields generated from images of skyrmion ensembles
offers insights into the nonlinear physical mechanisms of the system's response
to external stimuli and provides a basis for comparison with theoretical
predictions. The methodology presented here is very general and can provide a
characterization of system behavior both at the level of individual
pattern-forming agents and as a whole, allowing one to relate the results of
image data analysis to processes occurring in a physical, chemical, or
biological system in the real world.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [226] [Learning Kinetic Monte Carlo stochastic dynamics with Deep Generative Adversarial Networks](https://arxiv.org/abs/2507.21763)
*Daniele Lanzoni,Olivier Pierre-Louis,Roberto Bergamaschini,Francesco Montalenti*

Main category: cond-mat.stat-mech

TL;DR: 本文展示了用GAN学习随机动力学，以二维多粒子系统为例，训练条件GAN及时随机传播系统状态，降低计算成本，网络能定量重现性质。


<details>
  <summary>Details</summary>
Motivation: 利用GAN替代传统模型学习随机动力学，捕捉热涨落。

Method: 基于动力学蒙特卡罗模拟构建数据集，训练条件GAN及时随机传播系统状态，并对标准GAN进行修改以促进收敛和提高准确性。

Result: 训练的网络能定量重现平衡和动力学性质，与精确值偏差仅百分之几。

Conclusion: 讨论了外推极限和未来前景。

Abstract: We show that Generative Adversarial Networks (GANs) may be fruitfully
exploited to learn stochastic dynamics, surrogating traditional models while
capturing thermal fluctuations. Specifically, we showcase the application to a
two-dimensional, many-particle system, focusing on surface-step fluctuations
and on the related time-dependent roughness. After the construction of a
dataset based on Kinetic Monte Carlo simulations, a conditional GAN is trained
to propagate stochastically the state of the system in time, allowing the
generation of new sequences with a reduced computational cost. Modifications
with respect to standard GANs, which facilitate convergence and increase
accuracy, are discussed. The trained network is demonstrated to quantitatively
reproduce equilibrium and kinetic properties, including scaling laws, with
deviations of a few percent from the exact value. Extrapolation limits and
future perspectives are critically discussed.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [227] [Unified machine-learning framework for property prediction and time-evolution simulation of strained alloy microstructure](https://arxiv.org/abs/2507.21760)
*Andrea Fantasia,Daniele Lanzoni,Niccolò Di Eugenio,Angelo Monteleone,Roberto Bergamaschini,Francesco Montalenti*

Main category: cond-mat.mtrl-sci

TL;DR: 提出统一机器学习框架处理合金微观结构在弹性场下的时间演化，在多种失配条件下准确预测参数和微观结构演化，具有扩展性和一定外推能力，框架通用。


<details>
  <summary>Details</summary>
Motivation: 方便处理合金微观结构在弹性场影响下的时间演化问题。

Method: 构建统一机器学习框架，聚焦存在晶格失配下的旋节分解，用相场模拟提供真实演化与卷积循环神经网络架构预测结果对比，采用级联框架执行任务。

Result: 在多种失配条件下准确预测晶格失配参数和微观结构演化，展示了对更大计算域的可扩展性和一定时间外推能力。

Conclusion: 提出的框架通用，可应用于其他系统，还能用实验视频推断未知外部参数。

Abstract: We introduce a unified machine-learning framework designed to conveniently
tackle the temporal evolution of alloy microstructures under the influence of
an elastic field. This approach allows for the simultaneous extraction of
elastic parameters from a short trajectory and for the prediction of further
microstructure evolution under their influence. This is demonstrated by
focusing on spinodal decomposition in the presence of a lattice mismatch eta,
and by carrying out an extensive comparison between the ground-truth evolution
supplied by phase field simulations and the predictions of suitable
convolutional recurrent neural network architectures. The two tasks may then be
performed subsequently into a cascade framework. Under a wide spectrum of
misfit conditions, the here-presented cascade model accurately predicts eta and
the full corresponding microstructure evolution, also when approaching critical
conditions for spinodal decomposition. Scalability to larger computational
domain sizes and mild extrapolation errors in time (for time sequences five
times longer than the sampled ones during training) are demonstrated. The
proposed framework is general and can be applied beyond the specific,
prototypical system considered here as an example. Intriguingly, experimental
videos could be used to infer unknown external parameters, prior to simulating
further temporal evolution.

</details>


### [228] [Reducing Data Requirements for Sequence-Property Prediction in Copolymer Compatibilizers via Deep Neural Network Tuning](https://arxiv.org/abs/2507.21902)
*Md Mushfiqul Islam,Nishat N. Labiba,Lawrence O. Hall,David S. Simmons*

Main category: cond-mat.mtrl-sci

TL;DR: 提出新AI策略减少数据量以加速序列可控合成聚合物设计，采用预训练和微调方法，用低保真数据加速预测和设计。


<details>
  <summary>Details</summary>
Motivation: 合成序列可控聚合物设计因缺乏大量相关分子数据集而极具挑战，需减少设计所需数据量。

Method: 用低保真序列/界面张力关系数据训练深度神经网络，再快速微调以在不同条件下进行高保真预测。

Result: 训练的深度神经网络可在不同条件下用更少数据进行高保真预测，单组低保真数据能加速相关系统的预测和设计。

Conclusion: 该预训练和微调方法可显著加速相关系统的预测和设计，长期来看或为基于AI进行定量原子设计提供途径。

Abstract: Synthetic sequence-controlled polymers promise to transform polymer science
by combining the chemical versatility of synthetic polymers with the precise
sequence-mediated functionality of biological proteins. However, design of
these materials has proven extraordinarily challenging, because they lack the
massive datasets of closely related evolved molecules that accelerate design of
proteins. Here we report on a new Artifical Intelligence strategy to
dramatically reduce the amount of data necessary to accelerate these materials'
design. We focus on data connecting the repeat-unit-sequence of a
\emph{compatibilizer} molecule to its ability to reduce the interfacial tension
between distinct polymer domains. The optimal sequence of these molecules,
which are essential for applications such as mixed-waste polymer recycling,
depends strongly on variables such as concentration and chemical details of the
polymer. With current methods, this would demand an entirely distinct dataset
to enable design at each condition. Here we show that a deep neural network
trained on low-fidelity data for sequence/interfacial tension relations at one
set of conditions can be rapidly tuned to make higher-fidelity predictions at a
distinct set of conditions, requiring far less data that would ordinarily be
needed. This priming-and-tuning approach should allow a single low-fidelity
parent dataset to dramatically accelerate prediction and design in an entire
constellation of related systems. In the long run, it may also provide an
approach to bootstrapping quantitative atomistic design with AI insights from
fast, coarse simulations.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [229] [What Makes a Level Hard in Super Mario Maker 2?](https://arxiv.org/abs/2507.21078)
*Carlo A. Furia,Andrea Mocci*

Main category: cs.HC

TL;DR: 分析《超级马里奥制造2》用户关卡数据，探究影响关卡难度的因素。


<details>
  <summary>Details</summary>
Motivation: 理解影响《超级马里奥制造2》关卡难度的因素。

Method: 基于回归模型和自然语言处理技术进行分析。

Result: 揭示了关卡特征、主题和情感与关卡难易程度的关联。

Conclusion: 虽无惊人发现，但有助于提炼难易关卡差异，促进对用户关卡设计的理解。

Abstract: Games like Super Mario Maker 2 (SMM2) lower the barrier for casual users to
become level designers. In this paper, we set out to analyze a vast amount of
data about SMM2 user-written levels, in order to understand what factors affect
a level's difficulty as experienced by other users. To this end, we perform two
kinds of analyses: one based on regression models and one using natural
language processing techniques. The main results shed light on which level
characteristics (e.g., its style, popularity, timing) and which topics and
sentiments have a consistent association with easier or harder levels. While
none of our findings are startling, they help distill some key differences
between easy and hard SMM2 levels, which, in turn, can pave the way for a
better understanding of end-user level design.

</details>


### [230] [Conversations over Clicks: Impact of Chatbots on Information Search in Interdisciplinary Learning](https://arxiv.org/abs/2507.21490)
*Hannah Kim,Sergei L. Kosakovsky Pond,Stephen MacNeil*

Main category: cs.HC

TL;DR: 研究生成式AI对学习者体验的影响，采用自我民族志方法，发现使用需谨慎


<details>
  <summary>Details</summary>
Motivation: 电子学习环境中学习者自主导航信息空间有挑战，跨学科领域更甚，研究生成式AI对生物信息学研究信息搜索的影响

Method: 采用自我民族志方法

Result: 学习计划确立后生成式AI支持定向，之前适得其反；传统信息源对其响应效果不佳；通过领域先验知识识别信息线索

Conclusion: 生成式AI引入电子学习环境需谨慎，尤其在跨学科学习情境中

Abstract: This full research paper investigates the impact of generative AI (GenAI) on
the learner experience, with a focus on how learners engage with and utilize
the information it provides. In e-learning environments, learners often need to
navigate a complex information space on their own. This challenge is further
compounded in interdisciplinary fields like bioinformatics, due to the varied
prior knowledge and backgrounds. In this paper, we studied how GenAI influences
information search in bioinformatics research: (1) How do interactions with a
GenAI chatbot influence learner orienteering behaviors?; and (2) How do
learners identify information scent in GenAI chatbot responses? We adopted an
autoethnographic approach to investigate these questions. GenAI was found to
support orienteering once a learning plan was established, but it was
counterproductive prior to that. Moreover, traditionally value-rich information
sources such as bullet points and related terms proved less effective when
applied to GenAI responses. Information scents were primarily recognized
through the presence or absence of prior knowledge of the domain. These
findings suggest that GenAI should be adopted into e-learning environments with
caution, particularly in interdisciplinary learning contexts.

</details>


### [231] [Identification of Design Recommendations for Augmented Reality Authors in Corporate Training](https://arxiv.org/abs/2507.21722)
*Stefan Graser,Martin Schrepp,Stephan Böhm*

Main category: cs.HC

TL;DR: 研究针对现有AR设计建议缺乏特定场景问题，借助多方法分析，更新数据集并分类，评估出适用于企业培训中AR的内容，助力AR应用改进。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺乏特定场景下的AR设计建议，需识别和分析与用户中心设计评估阶段相关的实用AR设计建议。

Method: 依靠现有MR设计建议数据集，运用多方法：扩展数据集、用NLP分类、总结主题内容、通过专家定性评估相关性。

Result: 得到更新的含597条设计建议、分为84个主题的数据集，评估出32个主题共284条与企业培训中AR相关的陈述。

Conclusion: 研究直接有助于作者扩展AR特定的用户体验测量方法，支持AR开发者改进企业培训场景的AR应用。

Abstract: Innovative technologies, such as Augmented Reality (AR), introduce new
interaction paradigms, demanding the identification of software requirements
during the software development process. In general, design recommendations are
related to this, supporting the design of applications positively and meeting
stakeholder needs. However, current research lacks context-specific AR design
recommendations. This study addresses this gap by identifying and analyzing
practical AR design recommendations relevant to the evaluation phase of the
User-Centered Design (UCD) process. We rely on an existing dataset of Mixed
Reality (MR) design recommendations. We applied a multi-method approach by (1)
extending the dataset with AR-specific recommendations published since 2020,
(2) classifying the identified recommendations using a NLP classification
approach based on a pre-trained Sentence Transformer model, (3) summarizing the
content of all topics, and (4) evaluating their relevance concerning AR in
Corporate Training (CT) both based on a qualitative Round Robin approach with
five experts. As a result, an updated dataset of 597 practitioner design
recommendations, classified into 84 topics, is provided with new insights into
their applicability in the context of AR in CT. Based on this, 32 topics with a
total of 284 statements were evaluated as relevant for AR in CT. This research
directly contributes to the authors' work for extending their AR-specific User
Experience (UX) measurement approach, supporting AR authors in targeting the
improvement of AR applications for CT scenarios.

</details>


### [232] [VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization](https://arxiv.org/abs/2507.21124)
*Ayan Biswas,Terece L. Turton,Nishath Rajiv Ranasinghe,Shawn Jones,Bradley Love,William Jones,Aric Hagberg,Han-Wei Shen,Nathan DeBardeleben,Earl Lawrence*

Main category: cs.HC

TL;DR: 介绍了VizGenie框架，它结合特定领域工具和大语言模型，通过自然语言接口和自动化验证提升科学可视化能力，评估显示可降低认知负担。


<details>
  <summary>Details</summary>
Motivation: 推进科学可视化，解决现有工具能力局限，实现更灵活、可复用的可视化。

Method: 利用大语言模型生成新的可视化脚本，进行自动化后端验证；通过微调视觉模型进行图像分析和视觉问答；使用检索增强生成加强可靠性和可重复性。

Result: 在复杂体积数据集评估中，迭代可视化任务的认知负担显著降低。

Conclusion: VizGenie集成特定领域工具和大语言模型灵活性，加速洞察生成，建立可持续、不断发展的可视化实践。

Abstract: We present VizGenie, a self-improving, agentic framework that advances
scientific visualization through large language model (LLM) by orchestrating of
a collection of domain-specific and dynamically generated modules. Users
initially access core functionalities--such as threshold-based filtering, slice
extraction, and statistical analysis--through pre-existing tools. For tasks
beyond this baseline, VizGenie autonomously employs LLMs to generate new
visualization scripts (e.g., VTK Python code), expanding its capabilities
on-demand. Each generated script undergoes automated backend validation and is
seamlessly integrated upon successful testing, continuously enhancing the
system's adaptability and robustness. A distinctive feature of VizGenie is its
intuitive natural language interface, allowing users to issue high-level
feature-based queries (e.g., ``visualize the skull"). The system leverages
image-based analysis and visual question answering (VQA) via fine-tuned vision
models to interpret these queries precisely, bridging domain expertise and
technical implementation. Additionally, users can interactively query generated
visualizations through VQA, facilitating deeper exploration. Reliability and
reproducibility are further strengthened by Retrieval-Augmented Generation
(RAG), providing context-driven responses while maintaining comprehensive
provenance records. Evaluations on complex volumetric datasets demonstrate
significant reductions in cognitive overhead for iterative visualization tasks.
By integrating curated domain-specific tools with LLM-driven flexibility,
VizGenie not only accelerates insight generation but also establishes a
sustainable, continuously evolving visualization practice. The resulting
platform dynamically learns from user interactions, consistently enhancing
support for feature-centric exploration and reproducible research in scientific
visualization.

</details>


### [233] [FingerTip 20K: A Benchmark for Proactive and Personalized Mobile LLM Agents](https://arxiv.org/abs/2507.21071)
*Qinglong Yang,Haoming Li,Haotian Zhao,Xiaokai Yan,Jingtao Ding,Fengli Xu,Yong Li*

Main category: cs.HC

TL;DR: 提出FingerTip基准，含主动任务建议和个性化任务执行两个新赛道，收集安卓设备多步交互演示数据，模型微调后效果好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理只能遵循明确人类指令，缺乏主动意图预测能力且未利用用户上下文信息，忽视用户偏好差异。

Method: 引入FingerTip基准，包含主动任务建议和个性化任务执行两个新赛道，收集多步安卓设备交互的人类演示数据。

Result: 实验揭示所提任务存在挑战，用收集数据微调的模型有效利用用户信息，取得良好结果。

Conclusion: 提出的方法在构建更以用户为导向的移动GUI代理方面具有潜力。

Abstract: Mobile GUI agents are becoming critical tools for enhancing human-device
interaction efficiency, with multimodal large language models (MLLMs) emerging
as dominant paradigms in this domain. Current agents, however, are limited to
following explicit human instructions, resulting in insufficient capability for
proactive intent anticipation. Additionally, these agents fail to leverage the
contextual information associated with users during task execution, thereby
neglecting potentially vast differences in user preferences. To address these
challenges, we introduce the FingerTip benchmark. It contains two new tracks:
proactive task suggestions by analyzing environment observation and users'
previous intents, and personalized task execution by catering to users' action
preferences. We collected unique human demonstrations of multi-step Android
device interactions across a variety of everyday apps. These demonstrations are
not isolated but are continuously acquired from the users' long-term usage in
their real lives, and encompass essential user-related contextual information.
Our experiments reveal challenges of the tasks we propose. The model fine-tuned
with the data we collected effectively utilized user information and achieved
good results, highlighting the potential of our approach in building more
user-oriented mobile GUI agents. Our code is open-source at
https://anonymous.4open.science/r/FingerTip-57B8 for reproducibility.

</details>


### [234] [Empowering Educators in the Age of AI: An Empirical Study on Creating custom GPTs in Qualitative Research Method education](https://arxiv.org/abs/2507.21074)
*Qian Huang,Thijs Willems*

Main category: cs.HC

TL;DR: 研究两位教师如何将四个定制GPT工具融入城市规划政策专业硕士课程，发现工具利弊并给出见解，强调教育者设计定制工具可促进AI学习。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在教育中更普遍，需了解教育者如何塑造其设计和使用，填补学生为被动AI用户及AI在定性方法教育中应用有限的空白。

Method: 基于TPACK框架和行动研究方法，教师设计GPT工具来支撑研究问题制定等任务，对学生反馈、聊天记录和作业进行主题分析。

Result: 工具增强学生反思性、改进面试技巧、支持结构化分析思维，但学生也有认知过载等担忧。

Conclusion: AI与人工协作可成为主动学习支架；定制GPT可作迭代研究实践的认知伙伴；教育者主导设计对AI有意义融入教学至关重要。

Abstract: As generative AI (Gen-AI) tools become more prevalent in education, there is
a growing need to understand how educators, not just students, can actively
shape their design and use. This study investigates how two instructors
integrated four custom GPT tools into a Masters-level Qualitative Research
Methods course for Urban Planning Policy students. Addressing two key gaps: the
dominant framing of students as passive AI users, and the limited use of AI in
qualitative methods education. The study explores how Gen-AI can support
disciplinary learning when aligned with pedagogical intent. Drawing on the
Technological Pedagogical Content Knowledge (TPACK) framework and action
research methodology, the instructors designed GPTs to scaffold tasks such as
research question formulation, interview practice, fieldnote analysis, and
design thinking. Thematic analysis of student reflections, AI chat logs, and
final assignments revealed that the tools enhanced student reflexivity,
improved interview techniques, and supported structured analytic thinking.
However, students also expressed concerns about cognitive overload, reduced
immersion in data, and the formulaic nature of AI responses. The study offers
three key insights: AI can be a powerful scaffold for active learning when
paired with human facilitation; custom GPTs can serve as cognitive partners in
iterative research practice; and educator-led design is critical to
pedagogically meaningful AI integration. This research contributes to emerging
scholarship on AI in higher education by demonstrating how empowering educators
to design custom tools can promote more reflective, responsible, and
collaborative learning with AI.

</details>


### [235] [Data-Driven and Participatory Approaches toward Neuro-Inclusive AI](https://arxiv.org/abs/2507.21077)
*Naba Rizvi*

Main category: cs.HC

TL;DR: 当前AI数据表示存在反自闭症偏见，提出神经包容AI概念，通过实验找到二元标签方案可捕捉反自闭症仇恨言论标签细微差别，开发AUTALIC基准用于评估和微调模型。


<details>
  <summary>Details</summary>
Motivation: 解决AI中数据表示的偏见问题，改善自闭症群体在AI数据中的代表性，因为现有AI医疗应用将自闭症视为神经典型社交技能的缺陷，且研究存在质疑自闭症患者人性的视角。

Method: 定义神经包容AI概念，探索当前研究中反自闭症偏见的起源、普遍性和影响，对标注员和大语言模型进行实证实验。

Result: 90%的类人AI代理排除了自闭症视角，AI创造者认为伦理考量超出工作范围，二元标签方案能捕捉反自闭症仇恨言论标签细微差别，开发了AUTALIC基准。

Conclusion: 开发的AUTALIC基准可作为未来更具神经包容性工作的基础。

Abstract: Biased data representation in AI marginalizes up to 75 million autistic
people worldwide through medical applications viewing autism as a deficit of
neurotypical social skills rather than an aspect of human diversity, and this
perspective is grounded in research questioning the humanity of autistic
people. Turing defined artificial intelligence as the ability to mimic human
communication, and as AI development increasingly focuses on human-like agents,
this benchmark remains popular. In contrast, we define Neuro-Inclusive AI as
datasets and systems that move away from mimicking humanness as a benchmark for
machine intelligence. Then, we explore the origins, prevalence, and impact of
anti-autistic biases in current research. Our work finds that 90% of human-like
AI agents exclude autistic perspectives, and AI creators continue to believe
ethical considerations are beyond the scope of their work. To improve the
autistic representation in data, we conduct empirical experiments with
annotators and LLMs, finding that binary labeling schemes sufficiently capture
the nuances of labeling anti-autistic hate speech. Our benchmark, AUTALIC, can
be used to evaluate or fine-tune models, and was developed to serve as a
foundation for more neuro-inclusive future work.

</details>


### [236] [Empathy in Explanation](https://arxiv.org/abs/2507.21081)
*Katherine M. Collins,Kartik Chandra,Adrian Weller,Jonathan Ragan-Kelley,Joshua B. Tenenbaum*

Main category: cs.HC

TL;DR: 本文将解释视为合作社交互动，构建考虑情感影响的计算框架，以医生向患者解释病情为例测试，模型预测人类直觉效果好，表明人们解释时会考虑情感。


<details>
  <summary>Details</summary>
Motivation: 探究情感在解释这一社交互动中的作用。

Method: 开发考虑解释对听众情感影响的计算框架，以医生向患者解释病情为例进行测试。

Result: 模型能较好预测人类直觉，优于不考虑情感的模型。

Conclusion: 人们在给出解释时确实会对情感进行推理。

Abstract: Why do we give the explanations we do? Recent work has suggested that we
should think of explanation as a kind of cooperative social interaction,
between a why-question-asker and an explainer. Here, we apply this perspective
to consider the role that emotion plays in this social interaction. We develop
a computational framework for modeling explainers who consider the emotional
impact an explanation might have on a listener. We test our framework by using
it to model human intuitions about how a doctor might explain to a patient why
they have a disease, taking into account the patient's propensity for regret.
Our model predicts human intuitions well, better than emotion-agnostic
ablations, suggesting that people do indeed reason about emotion when giving
explanations.

</details>


### [237] [Thinking Like a Scientist: Can Interactive Simulations Foster Critical AI Literacy?](https://arxiv.org/abs/2507.21090)
*Yiling Zhao,Audrey Michal,Nithum Thain,Hari Subramonyam*

Main category: cs.HC

TL;DR: 研究交互式模拟能否提升AI素养，结果显示有效。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以支持对AI的深度理解和批判性参与，需新方法培养AI素养。

Method: 进行有605名参与者的对照研究，评估交互式AI教程对学习关键概念的影响。

Result: 交互式模拟能有效提升各主题的AI素养，支持知识迁移和自我报告的信心，但参与度不能单独预测学习效果。

Conclusion: 交互式、探究驱动的方法能更好地让个人在日常生活中批判性地参与AI，为AI素养教育领域做出贡献。

Abstract: As AI systems shape individual and societal decisions, fostering critical AI
literacy is essential. Traditional approaches, such as blog articles, static
lessons, and social media discussions, often fail to support deep conceptual
understanding and critical engagement. This study examines whether interactive
simulations can help learners think like a scientist by engaging them in
hypothesis testing, experimentation, and direct observation of AI behavior. In
a controlled study with 605 participants, we assess how interactive AI
tutorials impact learning of key concepts such as fairness, dataset
representativeness, and bias in language models. Results show that interactive
simulations effectively enhance AI literacy across topics, supporting greater
knowledge transfer and self-reported confidence, though engagement alone does
not predict learning. This work contributes to the growing field of AI literacy
education, highlighting how interactive, inquiry-driven methodologies can
better equip individuals to critically engage with AI in their daily lives.

</details>


### [238] [ProMemAssist: Exploring Timely Proactive Assistance Through Working Memory Modeling in Multi-Modal Wearable Devices](https://arxiv.org/abs/2507.21378)
*Kevin Pu,Ting Zhang,Naveen Sendhilnathan,Sebastian Freitag,Raj Sodhi,Tanya Jonker*

Main category: cs.HC

TL;DR: 提出ProMemAssist智能眼镜系统，实时建模用户工作记忆，在用户研究中表现优于基线系统，为主动智能体设计提供启示。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴AI系统常忽略用户当前心理状态，本文旨在解决该问题。

Method: 利用多模态传感器信号实时建模用户工作记忆，基于认知理论表示信息，通过工作记忆模型构建时机预测器。

Result: 在12名参与者的用户研究中，ProMemAssist提供更有选择性的帮助，用户参与度更高。

Conclusion: 工作记忆建模有助于提供细致、上下文敏感的支持，为设计更贴心、感知用户的主动智能体提供设计启示。

Abstract: Wearable AI systems aim to provide timely assistance in daily life, but
existing approaches often rely on user initiation or predefined task knowledge,
neglecting users' current mental states. We introduce ProMemAssist, a smart
glasses system that models a user's working memory (WM) in real-time using
multi-modal sensor signals. Grounded in cognitive theories of WM, our system
represents perceived information as memory items and episodes with encoding
mechanisms, such as displacement and interference. This WM model informs a
timing predictor that balances the value of assistance with the cost of
interruption. In a user study with 12 participants completing cognitively
demanding tasks, ProMemAssist delivered more selective assistance and received
higher engagement compared to an LLM baseline system. Qualitative feedback
highlights the benefits of WM modeling for nuanced, context-sensitive support,
offering design implications for more attentive and user-aware proactive
agents.

</details>


### [239] [AI Literacy as a Key Driver of User Experience in AI-Powered Assessment: Insights from Socratic Mind](https://arxiv.org/abs/2507.21654)
*Meryem Yilmaz Soylu,Jeonghyun Lee,Jui-Tse Hung,Christopher Zhang Cui,David A. Joyner*

Main category: cs.HC

TL;DR: 研究学生AI素养和先前AI接触对其与Socratic Mind互动体验的影响，发现AI素养比单纯接触更重要。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在高等教育中日益普及，了解学生与这些系统的互动对支持有效学习至关重要。

Method: 基于自我决定理论和用户体验研究，通过有效调查收集309名本科生数据，用偏最小二乘结构方程建模分析。

Result: AI素养（特别是自我效能、概念理解和应用技能）显著预测可用性、满意度和参与度；可用性和满意度强烈预测感知学习效果；先前AI接触无显著影响。

Conclusion: AI素养而非单纯接触塑造学生体验，设计者应集成自适应指导和以用户为中心的功能。

Abstract: As Artificial Intelligence (AI) tools become increasingly embedded in higher
education, understanding how students interact with these systems is essential
to supporting effective learning. This study examines how students' AI literacy
and prior exposure to AI technologies shape their perceptions of Socratic Mind,
an interactive AI-powered formative assessment tool. Drawing on
Self-Determination Theory and user experience research, we analyze
relationships among AI literacy, perceived usability, satisfaction, engagement,
and perceived learning effectiveness. Data from 309 undergraduates in Computer
Science and Business courses were collected through validated surveys. Partial
least squares structural equation modeling showed that AI literacy - especially
self-efficacy, conceptual understanding, and application skills - significantly
predicts usability, satisfaction, and engagement. Usability and satisfaction,
in turn, strongly predict perceived learning effectiveness, while prior AI
exposure showed no significant effect. These findings highlight that AI
literacy, rather than exposure alone, shapes student experiences. Designers
should integrate adaptive guidance and user-centered features to support
diverse literacy levels, fostering inclusive, motivating, and effective
AI-based learning environments.

</details>


### [240] [MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile Task Automation](https://arxiv.org/abs/2507.21953)
*Yi Kong,Dianxi Shi,Guoli Yang,Zhang ke-di,Chenlin Huang,Xiaopeng Li,Songchang Jin*

Main category: cs.HC

TL;DR: 提出基于LLM的MapAgent框架处理移动设备复杂任务，实验表现优且代码将开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自主代理在处理移动设备复杂任务时因缺乏真实应用知识，存在任务规划无效和幻觉问题。

Method: 提出基于轨迹的内存机制构建页面内存数据库；引入粗到细的任务规划方法，从数据库检索相关页面注入LLM规划器；通过双LLM架构的任务执行器将规划任务转为可执行动作。

Result: 在真实场景实验中，MapAgent性能优于现有方法。

Conclusion: MapAgent能有效解决现有基于LLM的自主代理处理复杂任务的问题，代码开源利于后续研究。

Abstract: The recent advancement of autonomous agents powered by Large Language Models
(LLMs) has demonstrated significant potential for automating tasks on mobile
devices through graphical user interfaces (GUIs). Despite initial progress,
these agents still face challenges when handling complex real-world tasks.
These challenges arise from a lack of knowledge about real-life mobile
applications in LLM-based agents, which may lead to ineffective task planning
and even cause hallucinations. To address these challenges, we propose a novel
LLM-based agent framework called MapAgent that leverages memory constructed
from historical trajectories to augment current task planning. Specifically, we
first propose a trajectory-based memory mechanism that transforms task
execution trajectories into a reusable and structured page-memory database.
Each page within a trajectory is extracted as a compact yet comprehensive
snapshot, capturing both its UI layout and functional context. Secondly, we
introduce a coarse-to-fine task planning approach that retrieves relevant pages
from the memory database based on similarity and injects them into the LLM
planner to compensate for potential deficiencies in understanding real-world
app scenarios, thereby achieving more informed and context-aware task planning.
Finally, planned tasks are transformed into executable actions through a task
executor supported by a dual-LLM architecture, ensuring effective tracking of
task progress. Experimental results in real-world scenarios demonstrate that
MapAgent achieves superior performance to existing methods. The code will be
open-sourced to support further research.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [241] [Load Balancing for AI Training Workloads](https://arxiv.org/abs/2507.21372)
*Sarah McClure,Sylvia Ratnasamy,Scott Shenker*

Main category: cs.NI

TL;DR: 研究专用基础设施上大规模AI训练工作负载的负载均衡算法性能及相关设计选择


<details>
  <summary>Details</summary>
Motivation: 了解大规模AI训练工作负载在专用基础设施上的负载均衡性能，为相关设计提供合适选择

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: We investigate the performance of various load balancing algorithms for
large-scale AI training workloads that are running on dedicated infrastructure.
The performance of load balancing depends on both the congestion control and
loss recovery algorithms, so our evaluation also sheds light on the appropriate
choices for those designs as well.

</details>


### [242] [Generalized few-shot transfer learning architecture for modeling the EDFA gain spectrum](https://arxiv.org/abs/2507.21728)
*Agastya Raj,Zehao Wang,Tingjun Chen,Daniel C Kilper,Marco Ruffini*

Main category: cs.NI

TL;DR: 提出基于半监督自归一化神经网络的少样本迁移学习架构优化掺铒光纤放大器增益谱预测，实验显示效果优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 准确建模掺铒光纤放大器增益谱对优化光网络性能至关重要，尤其是在多供应商解决方案的网络发展趋势下。

Method: 提出基于半监督自归一化神经网络的架构，采用两阶段训练策略，结合迁移学习技术，引入协方差匹配损失解决异构迁移中的特征不匹配问题。

Result: 在多个测试平台的26个掺铒光纤放大器上实验表明，该方法显著减少系统测量需求，平均绝对误差更低，误差分布更优。

Conclusion: 所提方法能有效提升掺铒光纤放大器增益谱预测效果，优于基准方法。

Abstract: Accurate modeling of the gain spectrum in Erbium-Doped Fiber Amplifiers
(EDFAs) is essential for optimizing optical network performance, particularly
as networks evolve toward multi-vendor solutions. In this work, we propose a
generalized few-shot transfer learning architecture based on a Semi-Supervised
Self-Normalizing Neural Network (SS-NN) that leverages internal EDFA features -
such as VOA input or output power and attenuation, to improve gain spectrum
prediction. Our SS-NN model employs a two-phase training strategy comprising
unsupervised pre-training with noise-augmented measurements and supervised
fine-tuning with a custom weighted MSE loss. Furthermore, we extend the
framework with transfer learning (TL) techniques that enable both homogeneous
(same-feature space) and heterogeneous (different-feature sets) model
adaptation across booster, preamplifier, and ILA EDFAs. To address feature
mismatches in heterogeneous TL, we incorporate a covariance matching loss to
align second-order feature statistics between source and target domains.
Extensive experiments conducted across 26 EDFAs in the COSMOS and Open Ireland
testbeds demonstrate that the proposed approach significantly reduces the
number of measurements requirements on the system while achieving lower mean
absolute errors and improved error distributions compared to benchmark methods.

</details>


### [243] [Deep Reinforcement Learning-based Cell DTX/DRX Configuration for Network Energy Saving](https://arxiv.org/abs/2507.21385)
*Wei Mao,Lili Wei,Omid Semiari,Shu-ping Yeh,Hosein Nikopour*

Main category: cs.NI

TL;DR: 本文研究如何配置3GPP R18的cell DTX/DRX以平衡节能和数据包延迟，采用深度强化学习框架训练AI智能体解决问题，模拟显示节能可达约45%且QoS下降不超约1%。


<details>
  <summary>Details</summary>
Motivation: cell DTX/DRX技术可节能但会增加数据包延迟，需配置以平衡节能和延迟，同时不同网络和流量条件下最优配置不同，问题复杂。

Method: 采用深度强化学习框架，在上下文多臂老虎机模型上实现深度Q网络，设计利用理论最优但不连续奖励函数的平滑近似的奖励函数来训练AI智能体。

Result: 模拟结果显示，与不使用cell DTX/DRX相比，智能体根据流量负载场景节能可达约45%，且QoS下降始终不超约1%。

Conclusion: 通过精心设计学习算法和奖励函数训练的AI智能体，能在任何网络和流量条件下选择最佳Cell DTX/DRX配置，有效平衡节能和QoS。

Abstract: 3GPP Release 18 cell discontinuous transmission and reception (cell DTX/DRX)
is an important new network energy saving feature for 5G. As a time-domain
technique, it periodically aggregates the user data transmissions in a given
duration of time when the traffic load is not heavy, so that the remaining time
can be kept silent and advanced sleep modes (ASM) can be enabled to shut down
more radio components and save more energy for the cell. However, inevitably
the packet delay is increased, as during the silent period no transmission is
allowed. In this paper we study how to configure cell DTX/DRX to optimally
balance energy saving and packet delay, so that for delay-sensitive traffic
maximum energy saving can be achieved while the degradation of quality of
service (QoS) is minimized. As the optimal configuration can be different for
different network and traffic conditions, the problem is complex and we resort
to deep reinforcement learning (DRL) framework to train an AI agent to solve
it. Through careful design of 1) the learning algorithm, which implements a
deep Q-network (DQN) on a contextual bandit (CB) model, and 2) the reward
function, which utilizes a smooth approximation of a theoretically optimal but
discontinuous reward function, we are able to train an AI agent that always
tries to select the best possible Cell DTX/DRX configuration under any network
and traffic conditions. Simulation results show that compared to the case when
cell DTX/DRX is not used, our agent can achieve up to ~45% energy saving
depending on the traffic load scenario, while always maintaining no more than
~1% QoS degradation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [244] [Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual Representations](https://arxiv.org/abs/2507.21448)
*Teng,Ma,Sile Yin,Li-Chia Yang,Shuo Zhang*

Main category: eess.AS

TL;DR: 本文提出实时视听语音增强系统RAVEN，研究不同条件下视觉嵌入对语音增强的贡献，给出结果，开发实时流系统并开源。


<details>
  <summary>Details</summary>
Motivation: 解决纯音频环境下语音增强，特别是存在干扰说话者时的难题。

Method: 研究来自视听语音识别和活跃说话者检测的视觉嵌入在不同信噪比和干扰说话者数量条件下对视听语音增强的贡献，开发基于计算机CPU的实时流系统。

Result: 在低信噪比、多说话者环境中，合并视听语音识别和活跃说话者检测模型的嵌入效果最佳；在仅存在噪声场景中，视听语音识别嵌入单独使用效果最好。

Conclusion: 开发了首个开源实时视听语音增强系统RAVEN。

Abstract: Speech enhancement in audio-only settings remains challenging, particularly
in the presence of interfering speakers. This paper presents a simple yet
effective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which
isolates and enhances the on-screen target speaker while suppressing
interfering speakers and background noise. We investigate how visual embeddings
learned from audio-visual speech recognition (AVSR) and active speaker
detection (ASD) contribute to AVSE across different SNR conditions and numbers
of interfering speakers. Our results show concatenating embeddings from AVSR
and ASD models provides the greatest improvement in low-SNR, multi-speaker
environments, while AVSR embeddings alone perform best in noise-only scenarios.
In addition, we develop a real-time streaming system that operates on a
computer CPU and we provide a video demonstration and code repository. To our
knowledge, this is the first open-source implementation of a real-time AVSE
system.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [245] [Not Here, Go There: Analyzing Redirection Patterns on the Web](https://arxiv.org/abs/2507.22019)
*Kritika Garg,Sawood Alam,Dietrich Ayala,Michele C. Weigle,Michael L. Nelson*

Main category: cs.DL

TL;DR: 研究分析1100万个重定向URI，揭示重定向模式及影响，为改善网络可用性等提供信息。


<details>
  <summary>Details</summary>
Motivation: URI重定向复杂影响可用性、SEO性能和数字保存，需研究其模式和影响。

Method: 分析1100万个唯一重定向URI，每个URI最多追踪10跳重定向。

Result: 50%的URI成功终止，50%出错，包括0.06%超10跳；规范和非规范重定向各有特点；发现“sink” URI和62000个自定义404 URI。

Conclusion: URI重定向对网络有重要作用，但存在挑战，研究为相关人员改善网络提供信息。

Abstract: URI redirections are integral to web management, supporting structural
changes, SEO optimization, and security. However, their complexities affect
usability, SEO performance, and digital preservation. This study analyzed 11
million unique redirecting URIs, following redirections up to 10 hops per URI,
to uncover patterns and implications of redirection practices. Our findings
revealed that 50% of the URIs terminated successfully, while 50% resulted in
errors, including 0.06% exceeding 10 hops. Canonical redirects, such as HTTP to
HTTPS transitions, were prevalent, reflecting adherence to SEO best practices.
Non-canonical redirects, often involving domain or path changes, highlighted
significant web migrations, rebranding, and security risks. Notable patterns
included "sink" URIs, where multiple redirects converged, ranging from traffic
consolidation by global websites to deliberate "Rickrolling." The study also
identified 62,000 custom 404 URIs, almost half being soft 404s, which could
compromise SEO and user experience. These findings underscore the critical role
of URI redirects in shaping the web while exposing challenges such as outdated
URIs, server instability, and improper error handling. This research offers a
detailed analysis of URI redirection practices, providing insights into their
prevalence, types, and outcomes. By examining a large dataset, we highlight
inefficiencies in redirection chains and examine patterns such as the use of
"sink" URIs and custom error pages. This information can help webmasters,
researchers, and digital archivists improve web usability, optimize resource
allocation, and safeguard valuable online content.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [246] [Predict Patient Self-reported Race from Skin Histological Images](https://arxiv.org/abs/2507.21912)
*Shengjia Chen,Ruchika Verma,Kevin Clare,Jannes Jegminat,Kuan-lin Huang,Brandon Veremis,Thomas Fuchs,Gabriele Campanella*

Main category: cs.CV

TL;DR: 研究探究深度学习模型能否从数字化皮肤病理学切片预测自我报告的种族，发现不同种族预测表现有差异，强调数据处理和偏差缓解重要性。


<details>
  <summary>Details</summary>
Motivation: 人工智能在计算病理学中虽成功，但学习意外人口统计学偏差的潜力研究不足，需探究深度学习模型从切片预测种族的能力及潜在形态捷径。

Method: 使用多站点、种族多样的数据集，应用基于注意力的机制挖掘与种族相关的形态特征，评估三种数据集管理策略控制混杂因素。

Result: 白人和黑人预测性能较高（AUC分别为0.799和0.762），总体降至0.663；注意力分析显示表皮是关键预测特征，移除该区域性能显著下降。

Conclusion: 在病理学中公平部署人工智能需谨慎进行数据管理和偏差缓解。

Abstract: Artificial Intelligence (AI) has demonstrated success in computational
pathology (CPath) for disease detection, biomarker classification, and
prognosis prediction. However, its potential to learn unintended demographic
biases, particularly those related to social determinants of health, remains
understudied. This study investigates whether deep learning models can predict
self-reported race from digitized dermatopathology slides and identifies
potential morphological shortcuts. Using a multisite dataset with a racially
diverse population, we apply an attention-based mechanism to uncover
race-associated morphological features. After evaluating three dataset curation
strategies to control for confounding factors, the final experiment showed that
White and Black demographic groups retained high prediction performance (AUC:
0.799, 0.762), while overall performance dropped to 0.663. Attention analysis
revealed the epidermis as a key predictive feature, with significant
performance declines when these regions were removed. These findings highlight
the need for careful data curation and bias mitigation to ensure equitable AI
deployment in pathology. Code available at:
https://github.com/sinai-computational-pathology/CPath_SAIF.

</details>


### [247] [GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data](https://arxiv.org/abs/2507.21069)
*Andreas Spilz,Heiko Oppel,Jochen Werner,Kathrin Stucke-Straub,Felix Capanni,Michael Munz*

Main category: cs.CV

TL;DR: 本文介绍了一个多模态人体运动数据集，含理疗和步态相关运动数据，支持机器学习模型开发和基准测试，还提供处理代码以加速研究。


<details>
  <summary>Details</summary>
Motivation: 开发基于传感器的理疗运动和步态分析分类模型需要大量多样数据集，而收集此类数据集成本高且耗时，因此创建该数据集。

Method: 使用同步的惯性测量单元（IMUs）和基于标记的运动捕捉（MoCap）系统，从19名参与者记录数据，提供原始数据、处理后的数据、模型、结果和可视化工具，还有详细注释。

Result: 得到一个多模态数据集，包括原始和处理后的数据、模型、结果、工具和注释，可支持多种机器学习任务。

Conclusion: 该资源可加速机器学习驱动的人体运动分析研究。

Abstract: Wearable inertial measurement units (IMUs) offer a cost-effective and
scalable means to assess human movement quality in clinical and everyday
settings. However, the development of robust sensor-based classification models
for physiotherapeutic exercises and gait analysis requires large, diverse
datasets, which are costly and time-consuming to collect. Here, we present a
multimodal dataset of physiotherapeutic exercises - including correct and
clinically relevant variants - and gait-related exercises - including both
normal and impaired gait patterns - recorded from 19 participants using
synchronized IMUs and marker-based motion capture (MoCap). The dataset includes
raw data from nine IMUs and thirty-five optical markers capturing full-body
kinematics. Each IMU is additionally equipped with four optical markers,
enabling precise comparison between IMU-derived orientation estimates and
reference values from the MoCap system. To support further analysis, we also
provide processed IMU orientations aligned with common segment coordinate
systems, subject-specific OpenSim models, inverse kinematics results, and tools
for visualizing IMU orientations in the musculoskeletal context. Detailed
annotations of movement execution quality and time-stamped segmentations
support diverse analysis goals. This dataset supports the development and
benchmarking of machine learning models for tasks such as automatic exercise
evaluation, gait analysis, temporal activity segmentation, and biomechanical
parameter estimation. To facilitate reproducibility, we provide code for
postprocessing, sensor-to-segment alignment, inverse kinematics computation,
and technical validation. This resource is intended to accelerate research in
machine learning-driven human movement analysis.

</details>


### [248] [Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues](https://arxiv.org/abs/2507.21161)
*Pallavi Zambare,Venkata Nikhil Thanikella,Ying Liu*

Main category: cs.CV

TL;DR: 提出基于Gemini 2.5 Pro的零样本行人意图预测方法BF - PIP，无需额外训练，准确率达73%，优于GPT - 4V基线。


<details>
  <summary>Details</summary>
Motivation: 传统行人意图预测方法依赖监督学习，适应新场景需大量再训练，需新方法。

Method: 引入基于Gemini 2.5 Pro的零样本方法BF - PIP，直接从含JAAD元数据的连续视频片段推断意图，处理连续时间片段，结合边界框注释和自车速度等多模态提示。

Result: BF - PIP无需额外训练，预测准确率达73%，比GPT - 4V基线高18%。

Conclusion: 结合时间视频输入和上下文线索可增强时空感知，改善模糊条件下的意图推断，为智能交通系统提供免再训练的感知模块。

Abstract: Pedestrian intention prediction is essential for autonomous driving in
complex urban environments. Conventional approaches depend on supervised
learning over frame sequences and require extensive retraining to adapt to new
scenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention
Prediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing
intentions directly from short, continuous video clips enriched with structured
JAAD metadata. In contrast to GPT-4V based methods that operate on discrete
frames, BF-PIP processes uninterrupted temporal clips. It also incorporates
bounding-box annotations and ego-vehicle speed via specialized multimodal
prompts. Without any additional training, BF-PIP achieves 73% prediction
accuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate
that combining temporal video inputs with contextual cues enhances
spatiotemporal perception and improves intent inference under ambiguous
conditions. This approach paves the way for agile, retraining-free perception
module in intelligent transportation system.

</details>


### [249] [PanoGAN A Deep Generative Model for Panoramic Dental Radiographs](https://arxiv.org/abs/2507.21200)
*Soren Pedersen,Sanyam Jain,Mikkel Chavez,Viktor Ladehoff,Bruna Neves de Freitas,Ruben Pauwels*

Main category: cs.CV

TL;DR: 本文开发用于合成牙科全景X光片的GAN，训练DCGAN，探索四个候选模型，评估生成图像，为牙科成像GAN方法奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决牙科研究和教育中数据稀缺问题。

Method: 使用WGANGP在2322张不同质量X光片数据集上训练DCGAN，关注牙牙槽区域，进行预处理和数据清洗，探索不同参数的四个候选模型。

Result: 多数生成图像解剖结构描绘中等，部分有伪影；未去噪数据训练的模型细节好，去噪数据训练的模型图像清晰度和锐度高。

Conclusion: 研究为牙科成像中基于GAN的方法的未来工作提供基础。

Abstract: This paper presents the development of a generative adversarial network (GAN)
for synthesizing dental panoramic radiographs. Although exploratory in nature,
the study aims to address the scarcity of data in dental research and
education. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss
with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying
quality. The focus was on the dentoalveolar regions, other anatomical
structures were cropped out. Extensive preprocessing and data cleaning were
performed to standardize the inputs while preserving anatomical variability. We
explored four candidate models by varying critic iterations, feature depth, and
the use of denoising prior to training. A clinical expert evaluated the
generated radiographs based on anatomical visibility and realism, using a
5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical
depiction, although some were degraded by artifacts. A trade-off was observed
the model trained on non-denoised data yielded finer details especially in
structures like the mandibular canal and trabecular bone, while a model trained
on denoised data offered superior overall image clarity and sharpness. These
findings provide a foundation for future work on GAN-based methods in dental
imaging.

</details>


### [250] [Group Relative Augmentation for Data Efficient Action Detection](https://arxiv.org/abs/2507.21353)
*Deep Anil Patel,Iain Melvin,Zachary Izzo,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 本文提出结合LoRA与可学习内部特征增强的高效适应策略及组加权损失函数，在多数据集上验证了对视频语言模型小样本动作检测的有效性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 解决用少量样本使大型视频语言模型适应动作检测时存在的过拟合、场景级预训练与以人为主理解的粒度不匹配问题。

Method: 提出结合参数高效调整（LoRA）与可学习内部特征增强的策略，使用FiLM在冻结的VLM骨干中生成与任务相关的特征变体；引入组加权损失函数，根据样本预测与组平均的差异动态调整训练贡献。

Result: 在复杂多标签、多人动作检测数据集（AVA, MOMA）上实现了较强的mAP性能，展示了从有限示例中适应VLMs的显著数据效率。

Conclusion: 所提方法在小样本动作检测中有效，能提升VLMs适应能力和数据效率。

Abstract: Adapting large Video-Language Models (VLMs) for action detection using only a
few examples poses challenges like overfitting and the granularity mismatch
between scene-level pre-training and required person-centric understanding. We
propose an efficient adaptation strategy combining parameter-efficient tuning
(LoRA) with a novel learnable internal feature augmentation. Applied within the
frozen VLM backbone using FiLM, these augmentations generate diverse feature
variations directly relevant to the task. Additionally, we introduce a
group-weighted loss function that dynamically modulates the training
contribution of each augmented sample based on its prediction divergence
relative to the group average. This promotes robust learning by prioritizing
informative yet reasonable augmentations. We demonstrate our method's
effectiveness on complex multi-label, multi-person action detection datasets
(AVA, MOMA), achieving strong mAP performance and showcasing significant data
efficiency for adapting VLMs from limited examples.

</details>


### [251] [MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving](https://arxiv.org/abs/2507.21423)
*Thomas Monninger,Zihan Zhang,Zhipeng Mo,Md Zafar Anwar,Steffen Staab,Sihao Ding*

Main category: cs.CV

TL;DR: 提出MapDiffusion用于自动驾驶在线矢量高清地图构建，利用扩散范式学习地图分布，实验表明性能优且能估计不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统地图构建模型提供确定性点估计，无法捕捉真实环境的不确定性和模糊性。

Method: 提出MapDiffusion，迭代细化随机初始化查询，基于BEV潜在网格生成多个合理地图样本。

Result: 在nuScenes数据集上达到在线地图构建的先进水平，单样本性能超基线5%，聚合样本提升性能，不确定性估计在遮挡区域更高。

Conclusion: MapDiffusion增强了在线矢量高清地图构建的鲁棒性和可靠性，支持复杂环境下自动驾驶车辆的不确定性感知决策。

Abstract: Autonomous driving requires an understanding of the static environment from
sensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse
multiple inputs, and a vector decoder predicts a vectorized map representation
from the latent BEV grid. However, traditional map construction models provide
deterministic point estimates, failing to capture uncertainty and the inherent
ambiguities of real-world environments, such as occlusions and missing lane
markings. We propose MapDiffusion, a novel generative approach that leverages
the diffusion paradigm to learn the full distribution of possible vectorized
maps. Instead of predicting a single deterministic output from learned queries,
MapDiffusion iteratively refines randomly initialized queries, conditioned on a
BEV latent grid, to generate multiple plausible map samples. This allows
aggregating samples to improve prediction accuracy and deriving uncertainty
estimates that directly correlate with scene ambiguity. Extensive experiments
on the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art
performance in online map construction, surpassing the baseline by 5% in
single-sample performance. We further show that aggregating multiple samples
consistently improves performance along the ROC curve, validating the benefit
of distribution modeling. Additionally, our uncertainty estimates are
significantly higher in occluded areas, reinforcing their value in identifying
regions with ambiguous sensor input. By modeling the full map distribution,
MapDiffusion enhances the robustness and reliability of online vectorized HD
map construction, enabling uncertainty-aware decision-making for autonomous
vehicles in complex environments.

</details>


### [252] [ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions](https://arxiv.org/abs/2507.21167)
*Danglu Yang,Liang Zhang,Zihao Yue,Liangyu Chen,Yichen Xu,Wenxuan Wang,Qin Jin*

Main category: cs.CV

TL;DR: 引入多模态图表编辑新范式，提出 ChartM³ 基准，揭示现有模型局限，构建训练集调优模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言的图表编辑方法存在歧义，难以支持细粒度编辑，需要新的编辑范式。

Method: 提出 ChartM³ 基准，含 1000 个样本及多视角评估指标，构建含 24000 样本的 ChartM³ - Train 训练集进行模型微调。

Result: 基准显示当前多模态大语言模型有显著局限，微调后模型有实质性改进。

Conclusion: 多模态监督对构建实用图表编辑系统很重要，相关数据、代码和工具已开源。

Abstract: Charts are a fundamental visualization format widely used in data analysis
across research and industry. While enabling users to edit charts based on
high-level intentions is of great practical value, existing methods primarily
rely on natural language instructions, which are often too ambiguous to support
fine-grained editing. In this work, we introduce a novel paradigm for
multimodal chart editing, where user intent is expressed through a combination
of natural language and visual indicators that explicitly highlight the
elements to be modified. To support this paradigm, we present
Chart$\text{M}^3$, a new benchmark for Multimodal chart editing with
Multi-level complexity and Multi-perspective evaluation. Chart$\text{M}^3$
contains 1,000 samples spanning four levels of editing difficulty. Each sample
includes triplets in the form of (chart, code, multimodal instructions). To
comprehensively evaluate chart editing models, Chart$\text{M}^3$ provides
metrics that assess both visual appearance and code correctness. Our benchmark
reveals significant limitations in current multimodal large language models
(MLLMs), including GPT-4o, particularly in their ability to interpret and act
on visual indicators. To address this, we construct Chart$\text{M}^3$-Train, a
large-scale training set with 24,000 multimodal chart editing samples.
Fine-tuning MLLMs on this dataset leads to substantial improvements,
demonstrating the importance of multimodal supervision in building practical
chart editing systems. Our datasets, codes, and evaluation tools are available
at https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our
datasets, codes, and evaluation tools are available at
https://github.com/yaolinli/VCE.

</details>


### [253] [Evaluating Deepfake Detectors in the Wild](https://arxiv.org/abs/2507.21905)
*Viacheslav Pirogov,Maksim Artemev*

Main category: cs.CV

TL;DR: 评估现代深度伪造检测器，用新测试流程模拟真实场景，创建含超50万张图像的数据集，发现检测仍具挑战，基本图像操作会降低模型性能，代码和数据公开。


<details>
  <summary>Details</summary>
Motivation: 众多深度伪造检测器在真实数据上的有效性未被测试，需评估其在真实场景下的效果。

Method: 引入新测试流程模拟真实场景，用先进生成方法创建含超50万张高质量深度伪造图像的数据集进行分析。

Result: 不到一半的检测器AUC得分超60%，最低为50%，基本图像操作会显著降低模型性能。

Conclusion: 深度伪造检测仍是一项具有挑战性的任务。

Abstract: Deepfakes powered by advanced machine learning models present a significant
and evolving threat to identity verification and the authenticity of digital
media. Although numerous detectors have been developed to address this problem,
their effectiveness has yet to be tested when applied to real-world data. In
this work we evaluate modern deepfake detectors, introducing a novel testing
procedure designed to mimic real-world scenarios for deepfake detection. Using
state-of-the-art deepfake generation methods, we create a comprehensive dataset
containing more than 500,000 high-quality deepfake images. Our analysis shows
that detecting deepfakes still remains a challenging task. The evaluation shows
that in fewer than half of the deepfake detectors tested achieved an AUC score
greater than 60%, with the lowest being 50%. We demonstrate that basic image
manipulations, such as JPEG compression or image enhancement, can significantly
reduce model performance. All code and data are publicly available at
https://github.com/messlav/Deepfake-Detectors-in-the-Wild.

</details>


### [254] [On Explaining Visual Captioning with Hybrid Markov Logic Networks](https://arxiv.org/abs/2507.21246)
*Monika Shah,Somdeb Sarkhel,Deepak Venugopal*

Main category: cs.CV

TL;DR: 本文提出基于HMLNs的可解释框架，用于解释DNN在图像描述生成中如何整合信息，实验验证了解释的可解释性并可比较模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以解释DNN在图像描述任务中如何整合视觉、语言和知识信息，标准评估指标也无法深入洞察这种整合。

Method: 开发基于HMLNs的解释框架，学习训练实例上的HMLN分布，通过条件推断量化影响生成描述的训练实例。

Result: 在多个先进图像描述模型生成的描述上进行实验，展示了解释的可解释性。

Conclusion: 所提框架具有可解释性，能在可解释性维度上比较不同模型。

Abstract: Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks
such as image captioning. However, explaining/interpreting how these models
integrate visual information, language information and knowledge representation
to generate meaningful captions remains a challenging problem. Standard metrics
to measure performance typically rely on comparing generated captions with
human-written ones that may not provide a user with a deep insights into this
integration. In this work, we develop a novel explanation framework that is
easily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language
that can combine symbolic rules with real-valued functions - where we
hypothesize how relevant examples from the training data could have influenced
the generation of the observed caption. To do this, we learn a HMLN
distribution over the training instances and infer the shift in distributions
over these instances when we condition on the generated sample which allows us
to quantify which examples may have been a source of richer information to
generate the observed caption. Our experiments on captions generated for
several state-of-the-art captioning models using Amazon Mechanical Turk
illustrate the interpretability of our explanations, and allow us to compare
these models along the dimension of explainability.

</details>


### [255] [Staining and locking computer vision models without retraining](https://arxiv.org/abs/2507.22000)
*Oliver J. Sutton,Qinghua Zhou,George Leete,Alexander N. Gorban,Ivan Y. Tyukin*

Main category: cs.CV

TL;DR: 本文提出染色和锁定计算机视觉模型新方法保护知识产权，无需微调或再训练，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 保护计算机视觉模型所有者的知识产权。

Method: 提出染色（嵌入秘密行为用于识别模型）和锁定（输入含秘密触发器图像模型才可用）方法，直接修改少量模型权重实现，无需微调或再训练。

Result: 实验结果显示方法有效，在多种计算机视觉模型上有实际性能表现。

Conclusion: 提出的染色和锁定计算机视觉模型的新方法能有效保护知识产权，且对模型性能影响小。

Abstract: We introduce new methods of staining and locking computer vision models, to
protect their owners' intellectual property. Staining, also known as
watermarking, embeds secret behaviour into a model which can later be used to
identify it, while locking aims to make a model unusable unless a secret
trigger is inserted into input images. Unlike existing methods, our algorithms
can be used to stain and lock pre-trained models without requiring fine-tuning
or retraining, and come with provable, computable guarantees bounding their
worst-case false positive rates. The stain and lock are implemented by directly
modifying a small number of the model's weights and have minimal impact on the
(unlocked) model's performance. Locked models are unlocked by inserting a small
`trigger patch' into the corner of the input image. We present experimental
results showing the efficacy of our methods and demonstrating their practical
performance on a variety of computer vision models.

</details>


### [256] [Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers](https://arxiv.org/abs/2507.21364)
*Lukman Jibril Aliyu,Umar Sani Muhammad,Bilqisu Ismail,Nasiru Muhammad,Almustapha A Wakili,Seid Muhie Yimam,Shamsuddeen Hassan Muhammad,Mustapha Abdullahi*

Main category: cs.CV

TL;DR: 本文对比研究深度学习模型对非洲野生动物图像分类，评估多个模型性能，指出性能、资源与可部署性的权衡，将DenseNet - 201集成用于实时实地使用，为野生动物保护的深度学习工具提供实用见解。


<details>
  <summary>Details</summary>
Motivation: 非洲野生动物面临严重威胁，深度学习图像分类可用于生物多样性监测和保护，需研究合适模型。

Method: 使用包含四种物种的公共数据集，对DenseNet - 201、ResNet - 152、EfficientNet - B4和Vision Transformer ViT - H/14进行评估，采用冻结特征提取器的迁移学习。

Result: DenseNet - 201在卷积网络中性能最佳（67%准确率），ViT - H/14整体准确率最高（99%），但计算成本高。

Conclusion: 实验凸显准确性、资源需求和可部署性之间的权衡，将最佳CNN模型DenseNet - 201集成用于实时实地使用，为野生动物保护的深度学习工具选择、数据集准备和负责任部署提供实用见解。

Abstract: Wildlife populations in Africa face severe threats, with vertebrate numbers
declining by over 65% in the past five decades. In response, image
classification using deep learning has emerged as a promising tool for
biodiversity monitoring and conservation. This paper presents a comparative
study of deep learning models for automatically classifying African wildlife
images, focusing on transfer learning with frozen feature extractors. Using a
public dataset of four species: buffalo, elephant, rhinoceros, and zebra; we
evaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and
Vision Transformer ViT-H/14. DenseNet-201 achieved the best performance among
convolutional networks (67% accuracy), while ViT-H/14 achieved the highest
overall accuracy (99%), but with significantly higher computational cost,
raising deployment concerns. Our experiments highlight the trade-offs between
accuracy, resource requirements, and deployability. The best-performing CNN
(DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time
field use, demonstrating the feasibility of deploying lightweight models in
conservation settings. This work contributes to African-grounded AI research by
offering practical insights into model selection, dataset preparation, and
responsible deployment of deep learning tools for wildlife conservation.

</details>


### [257] [Multimodal LLMs as Customized Reward Models for Text-to-Image Generation](https://arxiv.org/abs/2507.21391)
*Shijie Zhou,Ruiyi Zhang,Huaisheng Zhu,Branislav Kveton,Yufan Zhou,Jiuxiang Gu,Jian Chen,Changyou Chen*

Main category: cs.CV

TL;DR: 提出LLaVA - Reward模型自动评估文本到图像生成，添加SkipCA模块，在多个视角训练，表现优于传统和基于MLLM的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的方法需监督微调的指令遵循数据，评估耗时且难训练，需要更高效的方法。

Method: 直接利用文本 - 图像对下MLLM的隐藏状态，添加Skip - connection Cross Attention (SkipCA) 模块，支持不同类型偏好数据进行高效微调。

Result: LLaVA - Reward在生成符合人类偏好的自动评估分数和推理时间缩放方面优于传统和基于MLLM的方法。

Conclusion: LLaVA - Reward是一种高效的文本到图像生成评估模型，在多方面表现良好。

Abstract: We introduce LLaVA-Reward, an efficient reward model designed to
automatically evaluate text-to-image (T2I) generations across multiple
perspectives, leveraging pretrained multimodal large language models (MLLMs).
Existing MLLM-based approaches require instruction-following data for
supervised fine-tuning and evaluate generation quality on analyzing text
response, which is time-consuming and difficult to train. To address this
problem, we propose LLaVA-Reward, which directly utilizes the hidden states of
MLLMs given text-image pairs. To enhance the bidirectional interaction between
visual and textual representations in decoder-only MLLMs, we further propose
adding a Skip-connection Cross Attention (SkipCA) module. This design enhances
text-image correlation reasoning by connecting early-layer visual features with
later-layer hidden representations.In addition, LLaVA-Reward supports different
types of preference data for efficient fine-tuning, including paired preference
data and unpaired data. We train LLaVA-Reward on four evaluation perspectives:
text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical
results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based
methods in generating human-aligned scores for automatic evaluations and
inference-time scaling in text-to-image generations.

</details>


### [258] [Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation](https://arxiv.org/abs/2507.21455)
*Sheng-Feng Yu,Jia-Jiun Yao,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 论文提出自监督数据集蒸馏方法，通过创新技术从真实数据集提取信息，增强蒸馏集跨架构泛化性，实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集训练成本高，现有数据集蒸馏工作多针对监督数据集，本文旨在对图像及自监督训练的表征进行蒸馏。

Method: 提出创新参数化方法，利用预定增强处理数据增强随机性导致的不稳定性，用轻量级网络建模增强视图表征间联系。

Result: 在多个数据集上的实验表明，该方法在蒸馏效率、跨架构泛化和迁移学习性能方面具有优越性。

Conclusion: 提出的自监督数据集蒸馏方法有效，能从真实数据集提取丰富信息，增强蒸馏集的跨架构泛化性。

Abstract: Although larger datasets are crucial for training large deep models, the
rapid growth of dataset size has brought a significant challenge in terms of
considerable training costs, which even results in prohibitive computational
expenses. Dataset Distillation becomes a popular technique recently to reduce
the dataset size via learning a highly compact set of representative exemplars,
where the model trained with these exemplars ideally should have comparable
performance with respect to the one trained with the full dataset. While most
of existing works upon dataset distillation focus on supervised datasets, we
instead aim to distill images and their self-supervisedly trained
representations into a distilled set. This procedure, named as Self-Supervised
Dataset Distillation, effectively extracts rich information from real datasets,
yielding the distilled sets with enhanced cross-architecture generalizability.
Particularly, in order to preserve the key characteristics of original dataset
more faithfully and compactly, several novel techniques are proposed: 1) we
introduce an innovative parameterization upon images and representations via
distinct low-dimensional bases, where the base selection for parameterization
is experimentally shown to play a crucial role; 2) we tackle the instability
induced by the randomness of data augmentation -- a key component in
self-supervised learning but being underestimated in the prior work of
self-supervised dataset distillation -- by utilizing predetermined
augmentations; 3) we further leverage a lightweight network to model the
connections among the representations of augmented views from the same image,
leading to more compact pairs of distillation. Extensive experiments conducted
on various datasets validate the superiority of our approach in terms of
distillation efficiency, cross-architecture generalization, and transfer
learning performance.

</details>


### [259] [APT: Improving Diffusion Models for High Resolution Image Generation with Adaptive Path Tracing](https://arxiv.org/abs/2507.21690)
*Sangmin Han,Jinho Jeong,Jinwoo Kim,Seon Joo Kim*

Main category: cs.CV

TL;DR: 现有潜在扩散模型在高分辨率图像生成有局限，训练方法不实用，免训练的基于补丁方法有问题，提出APT框架解决问题，实验证明效果好且推理快。


<details>
  <summary>Details</summary>
Motivation: 潜在扩散模型在高分辨率图像生成能力受限，训练方法资源需求大，基于补丁的免训练方法存在‘补丁级分布偏移’和‘补丁单调性增加’问题。

Method: 提出自适应路径追踪（APT）框架，结合统计匹配保证上采样潜在空间中补丁分布一致，用尺度感知调度处理补丁单调性。

Result: APT生成的高分辨率图像细节更清晰、更精细，能实现快捷去噪过程，采样更快且质量损失小。

Conclusion: APT能生成更详细的输出，提高推理速度，是高分辨率图像生成的实用方法。

Abstract: Latent Diffusion Models (LDMs) are generally trained at fixed resolutions,
limiting their capability when scaling up to high-resolution images. While
training-based approaches address this limitation by training on
high-resolution datasets, they require large amounts of data and considerable
computational resources, making them less practical. Consequently,
training-free methods, particularly patch-based approaches, have become a
popular alternative. These methods divide an image into patches and fuse the
denoising paths of each patch, showing strong performance on high-resolution
generation. However, we observe two critical issues for patch-based approaches,
which we call ``patch-level distribution shift" and ``increased patch
monotonicity." To address these issues, we propose Adaptive Path Tracing (APT),
a framework that combines Statistical Matching to ensure patch distributions
remain consistent in upsampled latents and Scale-aware Scheduling to deal with
the patch monotonicity. As a result, APT produces clearer and more refined
details in high-resolution images. In addition, APT enables a shortcut
denoising process, resulting in faster sampling with minimal quality
degradation. Our experimental results confirm that APT produces more detailed
outputs with improved inference speed, providing a practical approach to
high-resolution image generation.

</details>


### [260] [Detection Transformers Under the Knife: A Neuroscience-Inspired Approach to Ablations](https://arxiv.org/abs/2507.21723)
*Nils Hütten,Florian Hölken,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 本文受神经科学消融研究启发，分析三种检测变压器模型关键组件消融的影响，发布DeepDissect库，发现模型特定弹性模式和结构冗余，推动DETRs的可解释AI发展。


<details>
  <summary>Details</summary>
Motivation: 可解释AI虽发展迅速，但在理解复杂模型内部组件作用方面存在研究空白，需提升模型透明度和效率。

Method: 对DETR、DDETR和DINO三种模型的关键组件进行消融，在COCO数据集上评估消融对gIoU和F1分数的影响，并发布DeepDissect库。

Result: 发现模型特定弹性模式，如DETR对特定层消融敏感，DDETR更稳健，DINO最具弹性，还发现结构冗余。

Conclusion: 该研究阐明了内部组件对模型性能的贡献，为优化模型和提高透明度及效率提供了见解。

Abstract: In recent years, Explainable AI has gained traction as an approach to
enhancing model interpretability and transparency, particularly in complex
models such as detection transformers. Despite rapid advancements, a
substantial research gap remains in understanding the distinct roles of
internal components - knowledge that is essential for improving transparency
and efficiency. Inspired by neuroscientific ablation studies, which investigate
the functions of brain regions through selective impairment, we systematically
analyze the impact of ablating key components in three state-of-the-art
detection transformer models: Detection transformer (DETR), deformable
detection transformer (DDETR), and DETR with improved denoising anchor boxes
(DINO). The ablations target query embeddings, encoder and decoder multi-head
self-attentions (MHSA) as well as decoder multi-head cross-attention (MHCA)
layers. We evaluate the effects of these ablations on the performance metrics
gIoU and F1-score, quantifying effects on both the classification and
regression sub-tasks on the COCO dataset. To facilitate reproducibility and
future research, we publicly release the DeepDissect library. Our findings
reveal model-specific resilience patterns: while DETR is particularly sensitive
to ablations in encoder MHSA and decoder MHCA, DDETR's multi-scale deformable
attention enhances robustness, and DINO exhibits the greatest resilience due to
its look-forward twice update rule, which helps distributing knowledge across
blocks. These insights also expose structural redundancies, particularly in
DDETR's and DINO's decoder MHCA layers, highlighting opportunities for model
simplification without sacrificing performance. This study advances XAI for
DETRs by clarifying the contributions of internal components to model
performance, offering insights to optimize and improve transparency and
efficiency in critical applications.

</details>


### [261] [LiteFat: Lightweight Spatio-Temporal Graph Learning for Real-Time Driver Fatigue Detection](https://arxiv.org/abs/2507.21756)
*Jing Ren,Suyu Ma,Hong Jia,Xiwei Xu,Ivan Lee,Haytham Fayek,Xiaodong Li,Feng Xia*

Main category: cs.CV

TL;DR: 文章提出轻量级时空图学习模型LiteFat检测驾驶员疲劳，实验显示其在降低计算复杂度和延迟上有优势，能用于嵌入式设备。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的驾驶员疲劳检测方案计算量大、延迟高，不适合资源有限的嵌入式设备，需快速检测以预防事故。

Method: 将视频数据转为时空图，用MobileNet提取面部特征创建特征矩阵，再用轻量级时空图神经网络识别疲劳迹象。

Result: 在基准数据集上实验表明，LiteFat与现有先进方法相比有竞争力，显著降低计算复杂度和延迟。

Conclusion: 该工作能推动实时、高效的人类疲劳检测系统在嵌入式设备上的应用。

Abstract: Detecting driver fatigue is critical for road safety, as drowsy driving
remains a leading cause of traffic accidents. Many existing solutions rely on
computationally demanding deep learning models, which result in high latency
and are unsuitable for embedded robotic devices with limited resources (such as
intelligent vehicles/cars) where rapid detection is necessary to prevent
accidents. This paper introduces LiteFat, a lightweight spatio-temporal graph
learning model designed to detect driver fatigue efficiently while maintaining
high accuracy and low computational demands. LiteFat involves converting
streaming video data into spatio-temporal graphs (STG) using facial landmark
detection, which focuses on key motion patterns and reduces unnecessary data
processing. LiteFat uses MobileNet to extract facial features and create a
feature matrix for the STG. A lightweight spatio-temporal graph neural network
is then employed to identify signs of fatigue with minimal processing and low
latency. Experimental results on benchmark datasets show that LiteFat performs
competitively while significantly decreasing computational complexity and
latency as compared to current state-of-the-art methods. This work enables the
development of real-time, resource-efficient human fatigue detection systems
that can be implemented upon embedded robotic devices.

</details>


### [262] [SwinECAT: A Transformer-based fundus disease classification model with Shifted Window Attention and Efficient Channel Attention](https://arxiv.org/abs/2507.21922)
*Peiran Gu,Teng Yao,Mengshen He,Fuhao Duan,Feiyan Liu,RenYuan Peng,Bao Ge*

Main category: cs.CV

TL;DR: 本文提出基于Transformer的SwinECAT模型用于眼底疾病分类，在9类分类任务上取得高准确率，表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 眼底图像分析存在小病灶区域和疾病间细微差异，导致模型预测准确率降低和过拟合，需改进模型。

Method: 提出SwinECAT模型，结合Swin Attention和ECA Attention，利用Swin Attention捕获局部和远程依赖，用ECA机制引导关注关键特征通道。

Result: 在包含16140张眼底图像的EDID数据集上进行9类分类实验，SwinECAT准确率达88.29%，加权F1分数0.88，宏F1分数0.90，显著优于基线模型。

Conclusion: SwinECAT模型在公开数据集的9类分类任务中取得最高报告性能，提升了诊断粒度。

Abstract: In recent years, artificial intelligence has been increasingly applied in the
field of medical imaging. Among these applications, fundus image analysis
presents special challenges, including small lesion areas in certain fundus
diseases and subtle inter-disease differences, which can lead to reduced
prediction accuracy and overfitting in the models. To address these challenges,
this paper proposes the Transformer-based model SwinECAT, which combines the
Shifted Window (Swin) Attention with the Efficient Channel Attention (ECA)
Attention. SwinECAT leverages the Swin Attention mechanism in the Swin
Transformer backbone to effectively capture local spatial structures and
long-range dependencies within fundus images. The lightweight ECA mechanism is
incorporated to guide the SwinECAT's attention toward critical feature
channels, enabling more discriminative feature representation. In contrast to
previous studies that typically classify fundus images into 4 to 6 categories,
this work expands fundus disease classification to 9 distinct types, thereby
enhancing the granularity of diagnosis. We evaluate our method on the Eye
Disease Image Dataset (EDID) containing 16,140 fundus images for 9-category
classification. Experimental results demonstrate that SwinECAT achieves 88.29\%
accuracy, with weighted F1-score of 0.88 and macro F1-score of 0.90. The
classification results of our proposed model SwinECAT significantly outperform
the baseline Swin Transformer and multiple compared baseline models. To our
knowledge, this represents the highest reported performance for 9-category
classification on this public dataset.

</details>


### [263] [Enhancing Generalization in Data-free Quantization via Mixup-class Prompting](https://arxiv.org/abs/2507.21947)
*Jiwoong Park,Chaeun Lee,Yongseok Choi,Sein Park,Deokki Hong,Jungwook Choi*

Main category: cs.CV

TL;DR: 提出混合类别提示策略用于数据自由量化，提升量化模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据自由量化方法中合成图像与量化模型泛化性的关系未充分研究，单类提示存在多义性问题导致性能下降。

Method: 提出基于混合的文本提示策略“mixup - class prompt”，在文本提示层融合多类标签生成合成数据，并通过梯度范数和泛化误差分析提供量化见解。

Result: 在CNN和ViT上实验表明，该方法持续优于GenQ等先进DFQ方法，在极低比特场景表现出色，在W2A4量化中达到新的最优精度。

Conclusion: mixup - class prompt策略能增强泛化性，提高后训练量化的优化稳定性，有效提升量化模型性能。

Abstract: Post-training quantization (PTQ) improves efficiency but struggles with
limited calibration data, especially under privacy constraints. Data-free
quantization (DFQ) mitigates this by generating synthetic images using
generative models such as generative adversarial networks (GANs) and
text-conditioned latent diffusion models (LDMs), while applying existing PTQ
algorithms. However, the relationship between generated synthetic images and
the generalizability of the quantized model during PTQ remains underexplored.
Without investigating this relationship, synthetic images generated by previous
prompt engineering methods based on single-class prompts suffer from issues
such as polysemy, leading to performance degradation. We propose
\textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses
multiple class labels at the text prompt level to generate diverse, robust
synthetic data. This approach enhances generalization, and improves
optimization stability in PTQ. We provide quantitative insights through
gradient norm and generalization error analysis. Experiments on convolutional
neural networks (CNNs) and vision transformers (ViTs) show that our method
consistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore,
it pushes the performance boundary in extremely low-bit scenarios, achieving
new state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation
(W2A4) quantization.

</details>


### [264] [Contrast-Prior Enhanced Duality for Mask-Free Shadow Removal](https://arxiv.org/abs/2507.21949)
*Jiyu Wu,Yifan Liu,Jiancheng Huang,Mingfu Yan,Shifeng Chen*

Main category: cs.CV

TL;DR: 现有去阴影方法依赖阴影掩码，获取困难，本文提出AGBA机制和FCFN网络解决问题并取得优异成果。


<details>
  <summary>Details</summary>
Motivation: 现有去阴影方法依赖阴影掩码难获取，利用局部对比信息有局限性，需解决这些问题。

Method: 提出Adaptive Gated Dual-Branch Attention (AGBA)机制动态过滤和重新权衡对比度；引入基于扩散的Frequency-Contrast Fusion Network (FCFN)利用高频和对比线索指导生成过程。

Result: 在无掩码方法中达到了最先进的结果，与基于掩码的方法相比也有竞争力。

Conclusion: 提出的方法有效解决了现有去阴影方法的问题，在相关任务中表现出色。

Abstract: Existing shadow removal methods often rely on shadow masks, which are
challenging to acquire in real-world scenarios. Exploring intrinsic image cues,
such as local contrast information, presents a potential alternative for
guiding shadow removal in the absence of explicit masks. However, the cue's
inherent ambiguity becomes a critical limitation in complex scenes, where it
can fail to distinguish true shadows from low-reflectance objects and intricate
background textures. To address this motivation, we propose the Adaptive Gated
Dual-Branch Attention (AGBA) mechanism. AGBA dynamically filters and re-weighs
the contrast prior to effectively disentangle shadow features from confounding
visual elements. Furthermore, to tackle the persistent challenge of restoring
soft shadow boundaries and fine-grained details, we introduce a diffusion-based
Frequency-Contrast Fusion Network (FCFN) that leverages high-frequency and
contrast cues to guide the generative process. Extensive experiments
demonstrate that our method achieves state-of-the-art results among mask-free
approaches while maintaining competitive performance relative to mask-based
methods.

</details>


### [265] [Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation](https://arxiv.org/abs/2507.22002)
*Yida Tao,Yen-Chia Hsu*

Main category: cs.CV

TL;DR: 提出CEDANet框架结合公民科学与弱监督领域适应，解决工业烟雾分割标注稀缺问题，实验显示其性能远超基线模型，能达小样本全监督水平。


<details>
  <summary>Details</summary>
Motivation: 工业烟雾分割因现实中像素级标注成本高且稀缺而受限，需新方法解决。

Method: 引入CEDANet框架，整合公民提供的视频级弱标签与对抗特征对齐，用公民投票细化伪标签，采用特定类别的域判别器迁移特征。

Result: 在数据集上实验，CEDANet有公民反馈时F1分数和烟雾类IoU远超基线模型，有公民约束伪标签时性能与有限全标注图像训练相当。

Conclusion: 验证了结合公民科学与弱监督领域适应的可扩展性和成本效益，为数据稀缺的环境监测应用提供实用方案。

Abstract: Industrial smoke segmentation is critical for air-quality monitoring and
environmental protection but is often hampered by the high cost and scarcity of
pixel-level annotations in real-world settings. We introduce CEDANet, a
human-in-the-loop, class-aware domain adaptation framework that uniquely
integrates weak, citizen-provided video-level labels with adversarial feature
alignment. Specifically, we refine pseudo-labels generated by a source-trained
segmentation model using citizen votes, and employ class-specific domain
discriminators to transfer rich source-domain representations to the industrial
domain. Comprehensive experiments on SMOKE5K and custom IJmond datasets
demonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of
0.261 with citizen feedback, vastly outperforming the baseline model, which
scored 0.083 and 0.043 respectively. This represents a five-fold increase in
F1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with
citizen-constrained pseudo-labels achieves performance comparable to the same
architecture trained on limited 100 fully annotated images with F1-score of
0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully
supervised-level accuracy without target-domain annotations. Our research
validates the scalability and cost-efficiency of combining citizen science with
weakly supervised domain adaptation, offering a practical solution for complex,
data-scarce environmental monitoring applications.

</details>


### [266] [XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation](https://arxiv.org/abs/2507.22020)
*Raju Ningappa Mulawade,Christoph Garth,Alexander Wiebel*

Main category: cs.CV

TL;DR: 提出基于分割的可解释AI方法用于点云分类神经网络，用点移动机制引入扰动，生成更有意义的显著性图。


<details>
  <summary>Details</summary>
Motivation: AI发展迅速，需理解其在关键领域的决策过程，要为点云分类AI算法生成人类易理解的解释。

Method: 利用点云分割模型生成解释，用点移动机制引入扰动，与经典聚类算法对比。

Result: 所提方法能生成更有意义的显著性图。

Conclusion: 所提方法在生成有意义解释方面有用。

Abstract: We propose a novel segmentation-based explainable artificial intelligence
(XAI) method for neural networks working on point cloud classification. As one
building block of this method, we propose a novel point-shifting mechanism to
introduce perturbations in point cloud data. Recently, AI has seen an
exponential growth. Hence, it is important to understand the decision-making
process of AI algorithms when they are applied in critical areas. Our work
focuses on explaining AI algorithms that classify point cloud data. An
important aspect of the methods used for explaining AI algorithms is their
ability to produce explanations that are easy for humans to understand. This
allows them to analyze the AI algorithms better and make appropriate decisions
based on that analysis. Therefore, in this work, we intend to generate
meaningful explanations that can be easily interpreted by humans. The point
cloud data we consider represents 3D objects such as cars, guitars, and
laptops. We make use of point cloud segmentation models to generate
explanations for the working of classification models. The segments are used to
introduce perturbations into the input point cloud data and generate saliency
maps. The perturbations are introduced using the novel point-shifting mechanism
proposed in this work which ensures that the shifted points no longer influence
the output of the classification algorithm. In contrast to previous methods,
the segments used by our method are meaningful, i.e. humans can easily
interpret the meaning of the segments. Thus, the benefit of our method over
other methods is its ability to produce more meaningful saliency maps. We
compare our method with the use of classical clustering algorithms to generate
explanations. We also analyze the saliency maps generated for example inputs
using our method to demonstrate the usefulness of the method in generating
meaningful explanations.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [267] [StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation](https://arxiv.org/abs/2507.21340)
*Satyananda Kashyap,Sola Shirai,Nandana Mihindukulasooriya,Horst Samulowitz*

Main category: cs.CL

TL;DR: 提出StructText框架自动生成高保真基准测试，评估发现大语言模型在生成可提取文本时叙事连贯性有问题，还发布框架支持研究。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在将自然语言转换为结构化格式时，缺乏评估提取质量的基准测试，手动标注构建基准测试费力且扩展性差。

Method: 利用现有表格数据，采用两阶段“计划 - 执行”管道合成生成对应自然语言文本，引入多维评估策略。

Result: 在49个数据集的71,539个示例上评估，大语言模型事实准确性高、避免幻觉，但叙事连贯性差，数值和时间信息难以自动提取。

Conclusion: 发布包括数据集、评估工具和基线提取系统的框架，支持后续研究。

Abstract: Extracting structured information from text, such as key-value pairs that
could augment tabular data, is quite useful in many enterprise use cases.
Although large language models (LLMs) have enabled numerous automated pipelines
for converting natural language into structured formats, there is still a lack
of benchmarks for evaluating their extraction quality, especially in specific
domains or focused documents specific to a given organization. Building such
benchmarks by manual annotations is labour-intensive and limits the size and
scalability of the benchmarks. In this work, we present StructText, an
end-to-end framework for automatically generating high-fidelity benchmarks for
key-value extraction from text using existing tabular data. It uses available
tabular data as structured ground truth, and follows a two-stage
``plan-then-execute'' pipeline to synthetically generate corresponding
natural-language text. To ensure alignment between text and structured source,
we introduce a multi-dimensional evaluation strategy that combines (a)
LLM-based judgments on factuality, hallucination, and coherence and (b)
objective extraction metrics measuring numeric and temporal accuracy. We
evaluated the proposed method on 71,539 examples across 49 datasets. Results
reveal that while LLMs achieve strong factual accuracy and avoid hallucination,
they struggle with narrative coherence in producing extractable text. Notably,
models presume numerical and temporal information with high fidelity yet this
information becomes embedded in narratives that resist automated extraction. We
release a framework, including datasets, evaluation tools, and baseline
extraction systems, to support continued research.

</details>


### [268] [InsurTech innovation using natural language processing](https://arxiv.org/abs/2507.21112)
*Panyi Dong,Zhiyu Quan*

Main category: cs.CL

TL;DR: 本文探讨自然语言处理（NLP）在保险运营中的应用，结合实际案例展示其将非结构化文本转化为结构化数据用于精算分析和决策的作用，证明NLP是现代保险分析的基础元素。


<details>
  <summary>Details</summary>
Motivation: 随着保险科技兴起，传统保险公司需探索替代数据源和先进技术以保持竞争力，研究NLP在保险运营的应用。

Method: 利用保险科技行业合作伙伴提供的真实替代数据，应用多种NLP技术展示商业保险场景中的实际用例。

Result: 文本衍生的见解丰富并完善了商业保险定价的传统评级因素，还通过引入新的行业分类为评估潜在风险提供新视角。

Conclusion: NLP不仅是补充工具，更是现代数据驱动保险分析的基础元素。

Abstract: With the rapid rise of InsurTech, traditional insurance companies are
increasingly exploring alternative data sources and advanced technologies to
sustain their competitive edge. This paper provides both a conceptual overview
and practical case studies of natural language processing (NLP) and its
emerging applications within insurance operations with a focus on transforming
raw, unstructured text into structured data suitable for actuarial analysis and
decision-making. Leveraging real-world alternative data provided by an
InsurTech industry partner that enriches traditional insurance data sources, we
apply various NLP techniques to demonstrate practical use cases in the
commercial insurance context. These enriched, text-derived insights not only
add to and refine traditional rating factors for commercial insurance pricing
but also offer novel perspectives for assessing underlying risk by introducing
novel industry classifications. Through these demonstrations, we show that NLP
is not merely a supplementary tool but a foundational element for modern,
data-driven insurance analytics.

</details>


### [269] [SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering](https://arxiv.org/abs/2507.21110)
*Kezhen Zhong,Basem Suleiman,Abdelkarim Erradi,Shijing Chen*

Main category: cs.CL

TL;DR: 本文介绍SemRAG，一种增强的检索增强生成框架，无需大量微调就能集成特定领域知识，实验显示其优于传统RAG方法，是实用可扩展的方案。


<details>
  <summary>Details</summary>
Motivation: 现有将特定领域知识集成到大型语言模型的方法计算成本高、易过拟合且扩展性受限，需要改进。

Method: 采用语义分块算法，基于句子嵌入的余弦相似度分割文档；将检索信息构建成知识图谱。

Result: 在MultiHop RAG和Wikipedia数据集上，SemRAG显著提高了从知识图谱中检索信息的相关性和正确性，优于传统RAG方法；优化不同数据集的缓冲区大小可进一步提升性能。

Conclusion: SemRAG能创建高效准确的特定领域大语言模型管道，避免资源密集型微调，是实用可扩展的方法，符合可持续发展目标，可为特定领域AI应用提供解决方案。

Abstract: This paper introduces SemRAG, an enhanced Retrieval Augmented Generation
(RAG) framework that efficiently integrates domain-specific knowledge using
semantic chunking and knowledge graphs without extensive fine-tuning.
Integrating domain-specific knowledge into large language models (LLMs) is
crucial for improving their performance in specialized tasks. Yet, existing
adaptations are computationally expensive, prone to overfitting and limit
scalability. To address these challenges, SemRAG employs a semantic chunking
algorithm that segments documents based on the cosine similarity from sentence
embeddings, preserving semantic coherence while reducing computational
overhead. Additionally, by structuring retrieved information into knowledge
graphs, SemRAG captures relationships between entities, improving retrieval
accuracy and contextual understanding. Experimental results on MultiHop RAG and
Wikipedia datasets demonstrate SemRAG has significantly enhances the relevance
and correctness of retrieved information from the Knowledge Graph,
outperforming traditional RAG methods. Furthermore, we investigate the
optimization of buffer sizes for different data corpus, as optimizing buffer
sizes tailored to specific datasets can further improve retrieval performance,
as integration of knowledge graphs strengthens entity relationships for better
contextual comprehension. The primary advantage of SemRAG is its ability to
create an efficient, accurate domain-specific LLM pipeline while avoiding
resource-intensive fine-tuning. This makes it a practical and scalable approach
aligned with sustainability goals, offering a viable solution for AI
applications in domain-specific fields.

</details>


### [270] [ChartMark: A Structured Grammar for Chart Annotation](https://arxiv.org/abs/2507.21810)
*Yiyu Chen,Yifan Wu,Shuyu Shen,Yupeng Xie,Leixian Shen,Hui Xiong,Yuyu Luo*

Main category: cs.CL

TL;DR: 提出ChartMark结构化语法解决图表注释跨平台复用问题，展示其灵活性和实用性。


<details>
  <summary>Details</summary>
Motivation: 图表注释表示碎片化、非标准化，限制跨平台复用。

Method: 提出ChartMark结构化语法，将注释语义与可视化实现分离，有分层框架映射到注释维度。

Result: 工具包能将ChartMark规范转换为Vega - Lite可视化。

Conclusion: ChartMark具有灵活性、表达性和实际适用性。

Abstract: Chart annotations enhance visualization accessibility but suffer from
fragmented, non-standardized representations that limit cross-platform reuse.
We propose ChartMark, a structured grammar that separates annotation semantics
from visualization implementations. ChartMark features a hierarchical framework
mapping onto annotation dimensions (e.g., task, chart context), supporting both
abstract intents and precise visual details. Our toolkit demonstrates
converting ChartMark specifications into Vega-Lite visualizations, highlighting
its flexibility, expressiveness, and practical applicability.

</details>


### [271] [Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions](https://arxiv.org/abs/2507.21065)
*Sabrina Patania,Luca Annese,Cansu Koyuturk,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.CL

TL;DR: 文章受维果茨基社会文化理论启发，提出'AI Social Gym'动态环境，通过教学对话提升大语言模型获取和应用新知识的能力，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理在线复杂知识有挑战，传统AI训练范式效率低，需新学习范式解决问题。

Method: 引入'AI Social Gym'，让AI学习者与教师进行二元教学对话，研究不同教学策略对本体获取中AI学习过程的影响。

Result: 对话式方法，特别是混合方向交互，显著提升大语言模型获取和应用新知识的能力，优于单向教学和直接获取结构化知识。

Conclusion: 将教学和心理学见解融入AI和机器人训练，可提高训练后知识获取和响应质量，是对现有策略的补充。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
processing extensive offline datasets. However, they often face challenges in
acquiring and integrating complex, knowledge online. Traditional AI training
paradigms, predominantly based on supervised learning or reinforcement
learning, mirror a 'Piagetian' model of independent exploration. These
approaches typically rely on large datasets and sparse feedback signals,
limiting the models' ability to learn efficiently from interactions. Drawing
inspiration from Vygotsky's sociocultural theory, this study explores the
potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI
learner agent engages in dyadic pedagogical dialogues with knowledgeable AI
teacher agents. These interactions emphasize external, structured dialogue as a
core mechanism for knowledge acquisition, contrasting with methods that depend
solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the
AI learning process in the context of ontology acquisition. Empirical results
indicate that such dialogic approaches-particularly those involving
mixed-direction interactions combining top-down explanations with
learner-initiated questioning-significantly enhance the LLM's ability to
acquire and apply new knowledge, outperforming both unidirectional
instructional methods and direct access to structured knowledge, formats
typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological
insights into AI and robot training can substantially improve post-training
knowledge acquisition and response quality. This approach offers a
complementary pathway to existing strategies like prompt engineering

</details>


### [272] [Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing](https://arxiv.org/abs/2507.21084)
*Aly M. Kassem,Zhuan Shi,Negar Rostamzadeh,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: 提出MNEME框架检测大语言模型微调或遗忘操作的意外副作用，准确率达95%，还展示部分逆转副作用的方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法检测大语言模型微调或遗忘操作的意外副作用，需通用方法。

Method: 引入MNEME框架，用稀疏模型差分，在与任务无关的数据上对比基础模型和微调模型。

Result: MNEME在三种场景下对五个大语言模型预测副作用准确率达95%，且显示在高激活样本上重新训练可部分逆转副作用。

Conclusion: 稀疏探测和差分能为理解和管理大语言模型行为提供可扩展和自动化的视角。

Abstract: Large language models (LLMs) are frequently fine-tuned or unlearned to adapt
to new tasks or eliminate undesirable behaviors. While existing evaluation
methods assess performance after such interventions, there remains no general
approach for detecting unintended side effects, such as unlearning biology
content degrading performance on chemistry tasks, particularly when these
effects are unpredictable or emergent. To address this issue, we introduce
MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight
framework for identifying these side effects using sparse model diffing. MNEME
compares base and fine-tuned models on task-agnostic data (for example, The
Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral
shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning,
emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent
accuracy in predicting side effects, aligning with known benchmarks and
requiring no custom heuristics. Furthermore, we show that retraining on
high-activation samples can partially reverse these effects. Our results
demonstrate that sparse probing and diffing offer a scalable and automated lens
into fine-tuning-induced model changes, providing practical tools for
understanding and managing LLM behavior.

</details>


### [273] [TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law](https://arxiv.org/abs/2507.21134)
*Zheng Hui,Yijiang River Dong,Ehsan Shareghi,Nigel Collier*

Main category: cs.CL

TL;DR: 随着大语言模型用于高风险领域，提出Trident - Bench基准评估其特定领域安全，评估19个模型发现专业模型有安全差距，需改进。


<details>
  <summary>Details</summary>
Motivation: 此前工作多关注大语言模型在高风险领域的性能，忽视特定领域安全风险评估，需弥补此差距。

Method: 基于医学、法律、金融伦理准则定义安全原则，引入Trident - Bench基准评估19个通用和专业模型。

Result: Trident - Bench能有效揭示关键安全差距，通用模型可满足基本期望，专业模型难处理细微伦理问题。

Conclusion: 引入Trident - Bench为研究法律和金融领域大语言模型安全提供资源，为降低部署风险研究奠定基础。

Abstract: As large language models (LLMs) are increasingly deployed in high-risk
domains such as law, finance, and medicine, systematically evaluating their
domain-specific safety and compliance becomes critical. While prior work has
largely focused on improving LLM performance in these domains, it has often
neglected the evaluation of domain-specific safety risks. To bridge this gap,
we first define domain-specific safety principles for LLMs based on the AMA
Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and
the CFA Institute Code of Ethics. Building on this foundation, we introduce
Trident-Bench, a benchmark specifically targeting LLM safety in the legal,
financial, and medical domains. We evaluated 19 general-purpose and
domain-specialized models on Trident-Bench and show that it effectively reveals
key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic
expectations, whereas domain-specialized models often struggle with subtle
ethical nuances. This highlights an urgent need for finer-grained
domain-specific safety improvements. By introducing Trident-Bench, our work
provides one of the first systematic resources for studying LLM safety in law
and finance, and lays the groundwork for future research aimed at reducing the
safety risks of deploying LLMs in professionally regulated fields. Code and
benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT

</details>


### [274] [Categorical Classification of Book Summaries Using Word Embedding Techniques](https://arxiv.org/abs/2507.21058)
*Kerem Keskin,Mümine Kaya Keleş*

Main category: cs.CL

TL;DR: 研究使用词嵌入、自然语言处理和机器学习算法对书籍摘要和类别分类，比较不同词嵌入方法效果，发现特定模型和技术对土耳其语更有效。


<details>
  <summary>Details</summary>
Motivation: 对书籍摘要和类别进行分类，并比较不同词嵌入方法的效果。

Method: 采用词嵌入方法（如one hot encoding、Word2Vec、TF - IDF）、自然语言处理技术和机器学习算法（支持向量机、朴素贝叶斯、逻辑回归模型）进行分类和比较。

Result: 支持向量机、朴素贝叶斯和逻辑回归模型以及TF - IDF和One - Hot Encoder词嵌入技术对土耳其语文本效果更好。

Conclusion: 对于土耳其语文本分类，支持向量机、朴素贝叶斯和逻辑回归模型与TF - IDF和One - Hot Encoder词嵌入技术更适用。

Abstract: In this study, book summaries and categories taken from book sites were
classified using word embedding methods, natural language processing techniques
and machine learning algorithms. In addition, one hot encoding, Word2Vec and
Term Frequency - Inverse Document Frequency (TF-IDF) methods, which are
frequently used word embedding methods were used in this study and their
success was compared. Additionally, the combination table of the pre-processing
methods used is shown and added to the table. Looking at the results, it was
observed that Support Vector Machine, Naive Bayes and Logistic Regression
Models and TF-IDF and One-Hot Encoder word embedding techniques gave more
successful results for Turkish texts.

</details>


### [275] [TTS-1 Technical Report](https://arxiv.org/abs/2507.21138)
*Oleg Atamanenko,Anna Chalova,Joseph Coombes,Nikki Cope,Phillip Dang,Zhifeng Deng,Jimmy Du,Michael Ermolenko,Feifan Fan,Yufei Feng,Cheryl Fichter,Pavel Filimonov,Louis Fischer,Kylan Gibbs,Valeria Gusarova,Pavel Karpik,Andreas Assad Kottner,Ian Lee,Oliver Louie,Jasmine Mai,Mikhail Mamontov,Suri Mao,Nurullah Morshed,Igor Poletaev,Florin Radu,Dmytro Semernia,Evgenii Shingarev,Vikram Sivaraja,Peter Skirko,Rinat Takhautdinov,Robert Villahermosa,Jean Wang*

Main category: cs.CL

TL;DR: 介绍Inworld TTS - 1系列两个基于Transformer的自回归TTS模型，展示其性能并开源代码。


<details>
  <summary>Details</summary>
Motivation: 开发具有高质量、高表现力、适用于不同场景（如实时合成和设备端）的TTS模型。

Method: 通过扩展训练时计算量，对语音语言模型组件进行预训练、微调以及强化学习对齐。

Result: 两个模型在多种基准测试中达到了最先进水平，能生成高分辨率低延迟语音，支持11种语言及情感控制等。

Conclusion: 成功开发出性能优异的TTS模型并开源代码。

Abstract: We introduce Inworld TTS-1, a set of two Transformer-based autoregressive
text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters
and is designed for utmost quality and expressiveness in demanding
applications. TTS-1 is our most efficient model, with 1.6B parameters, built
for real-time speech synthesis and on-device use cases. By scaling train-time
compute and applying a sequential process of pre-training, fine-tuning, and
RL-alignment of the speech-language model (SpeechLM) component, both models
achieve state-of-the-art performance on a variety of benchmarks, demonstrating
exceptional quality relying purely on in-context learning of the speaker's
voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech
with low latency, and support 11 languages with fine-grained emotional control
and non-verbal vocalizations through audio markups. We additionally open-source
our training and modeling code under an MIT license.

</details>


### [276] [Which symbol grounding problem should we try to solve?](https://arxiv.org/abs/2507.21080)
*Vincent C. Müller*

Main category: cs.CL

TL;DR: 质疑Floridi和Taddeo解决语义接地问题的条件，借鉴Luc Steels建议后，重新思考问题，得出合理的接地问题定义。


<details>
  <summary>Details</summary>
Motivation: 原有的解决语义接地问题的条件存在缺陷，需要重新审视该问题。

Method: 先指出Floridi和Taddeo条件无法达成，参考Luc Steels的建议，基于对计算的正确理解进行分析。

Result: 明确了唯一合理的接地问题是如何解释和重现人工计算主体中意义的行为能力和功能。

Conclusion: 需重新思考接地问题，明确合理的接地问题定义。

Abstract: Floridi and Taddeo propose a condition of "zero semantic commitment" for
solutions to the grounding problem, and a solution to it. I argue briefly that
their condition cannot be fulfilled, not even by their own solution. After a
look at Luc Steels' very different competing suggestion, I suggest that we need
to re-think what the problem is and what role the 'goals' in a system play in
formulating the problem. On the basis of a proper understanding of computing, I
come to the conclusion that the only sensible grounding problem is how we can
explain and re-produce the behavioral ability and function of meaning in
artificial computational agents

</details>


### [277] [Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question](https://arxiv.org/abs/2507.21168)
*Rafael Rosales,Santiago Miret*

Main category: cs.CL

TL;DR: 比较大语言模型回答二元问题的两种多样性方法，发现问题解释多样性效果更好。


<details>
  <summary>Details</summary>
Motivation: 有效利用多样性可提升机器学习模型性能，但确定最佳利用方式仍是挑战，需比较不同多样性方法。

Method: 比较模型多样性（多模型答同一问题）和问题解释多样性（同一模型答不同表述的同一问题），用多数投票确定最终答案。

Result: 在boolq、strategyqa和pubmedqa实验中，问题解释多样性的集成准确率更高；GPT和LLaMa的模型多样性结果在最佳和最差成员之间，无明显提升。

Conclusion: 对于二元问题，问题解释多样性比模型多样性更能提升大语言模型回答的集成准确率。

Abstract: Effectively leveraging diversity has been shown to improve performance for
various machine learning models, including large language models (LLMs).
However, determining the most effective way of using diversity remains a
challenge. In this work, we compare two diversity approaches for answering
binary questions using LLMs: model diversity, which relies on multiple models
answering the same question, and question interpretation diversity, which
relies on using the same model to answer the same question framed in different
ways. For both cases, we apply majority voting as the ensemble consensus
heuristic to determine the final answer. Our experiments on boolq, strategyqa,
and pubmedqa show that question interpretation diversity consistently leads to
better ensemble accuracy compared to model diversity. Furthermore, our analysis
of GPT and LLaMa shows that model diversity typically produces results between
the best and the worst ensemble members without clear improvement.

</details>


### [278] [ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs](https://arxiv.org/abs/2507.21083)
*Franck Bardol*

Main category: cs.CL

TL;DR: 研究大语言模型GPT - 4对不同情感基调提示的响应，发现其存在“反弹”偏差等问题。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型如GPT - 4是否会根据提问的情感基调调整响应，以及这种调整的具体情况。

Method: 系统改变156个涵盖争议和日常话题提示的情感基调，分析其对模型响应的影响；引入“语气下限”概念，使用语气 - 效价转移矩阵量化行为，基于1536维嵌入进行可视化。

Result: GPT - 4对负面框架问题给出负面响应的可能性比中性问题低三倍，存在“反弹”偏差；在敏感话题上，语气变化效应更明显；可视化证实语义漂移。

Conclusion: 强调了提示中情感框架导致的一类未充分探索的偏差，对AI对齐和信任有影响。

Abstract: Large Language Models like GPT-4 adjust their responses not only based on the
question asked, but also on how it is emotionally phrased. We systematically
vary the emotional tone of 156 prompts - spanning controversial and everyday
topics - and analyze how it affects model responses. Our findings show that
GPT-4 is three times less likely to respond negatively to a negatively framed
question than to a neutral one. This suggests a "rebound" bias where the model
overcorrects, often shifting toward neutrality or positivity. On sensitive
topics (e.g., justice or politics), this effect is even more pronounced:
tone-based variation is suppressed, suggesting an alignment override. We
introduce concepts like the "tone floor" - a lower bound in response negativity
- and use tone-valence transition matrices to quantify behavior. Visualizations
based on 1536-dimensional embeddings confirm semantic drift based on tone. Our
work highlights an underexplored class of biases driven by emotional framing in
prompts, with implications for AI alignment and trust. Code and data are
available at: https://github.com/bardolfranck/llm-responses-viewer

</details>


### [279] [Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers](https://arxiv.org/abs/2507.21186)
*Sungmin Han,Jeonghyun Lee,Sangkyun Lee*

Main category: cs.CL

TL;DR: Transformer决策解释难，现有激活归因法有局限，提出Contrast - CAT方法，实验显示其优于现有方法，提升可解释性。


<details>
  <summary>Details</summary>
Motivation: Transformer决策解释难阻碍实际应用，现有激活归因法因激活中无关特征而不可靠。

Method: 提出Contrast - CAT，通过对比输入序列激活和参考激活，过滤无关特征来优化词元级归因。

Result: 在多个数据集和模型上实验，Contrast - CAT始终优于现有方法，在MoRF设置下，AOPC平均提升1.30倍，LOdds平均提升2.25倍。

Conclusion: Contrast - CAT能有效提升基于Transformer的文本分类可解释性。

Abstract: Transformers have profoundly influenced AI research, but explaining their
decisions remains challenging -- even for relatively simpler tasks such as
classification -- which hinders trust and safe deployment in real-world
applications. Although activation-based attribution methods effectively explain
transformer-based text classification models, our findings reveal that these
methods can be undermined by class-irrelevant features within activations,
leading to less reliable interpretations. To address this limitation, we
propose Contrast-CAT, a novel activation contrast-based attribution method that
refines token-level attributions by filtering out class-irrelevant features. By
contrasting the activations of an input sequence with reference activations,
Contrast-CAT generates clearer and more faithful attribution maps. Experimental
results across various datasets and models confirm that Contrast-CAT
consistently outperforms state-of-the-art methods. Notably, under the MoRF
setting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds
over the most competing methods, demonstrating its effectiveness in enhancing
interpretability for transformer-based text classification.

</details>


### [280] [iLSU-T: an Open Dataset for Uruguayan Sign Language Translation](https://arxiv.org/abs/2507.21104)
*Ariel E. Stassi,Yanina Boria,J. Matías Di Martino,Gregory Randall*

Main category: cs.CL

TL;DR: 本文介绍iLSU T开放数据集，含乌拉圭手语RGB视频、音频和文本转录，用三种算法实验，强调需更多本地化数据集。


<details>
  <summary>Details</summary>
Motivation: 因各国手语特点，机器翻译需本地数据开发和适配新技术，开发新的手语处理工具。

Method: 提出iLSU T数据集，用三种先进翻译算法进行实验。

Result: 实验强调了手语翻译和理解需要更多本地化数据集。

Conclusion: 更多本地化数据集对开发提高可及性和包容性的新工具有关键作用，数据和代码可获取。

Abstract: Automatic sign language translation has gained particular interest in the
computer vision and computational linguistics communities in recent years.
Given each sign language country particularities, machine translation requires
local data to develop new techniques and adapt existing ones. This work
presents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB
videos with audio and text transcriptions. This type of multimodal and curated
data is paramount for developing novel approaches to understand or generate
tools for sign language processing. iLSU T comprises more than 185 hours of
interpreted sign language videos from public TV broadcasting. It covers diverse
topics and includes the participation of 18 professional interpreters of sign
language. A series of experiments using three state of the art translation
algorithms is presented. The aim is to establish a baseline for this dataset
and evaluate its usefulness and the proposed pipeline for data processing. The
experiments highlight the need for more localized datasets for sign language
translation and understanding, which are critical for developing novel tools to
improve accessibility and inclusion of all individuals. Our data and code can
be accessed.

</details>


### [281] [Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams](https://arxiv.org/abs/2507.21107)
*Rob Manson*

Main category: cs.CL

TL;DR: 提出Curved Inference几何可解释性框架，分析Gemma3 - 1b和LLaMA3.2 - 3b模型，发现关注转移提示会改变模型内部激活轨迹，支持LLM几何两层观点。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的框架来研究大语言模型中语义关注变化时残差流轨迹的弯曲情况，以深入了解模型语义抽象和对齐。

Method: 使用Curved Inference框架，对20个匹配提示，用五种原生空间指标分析Gemma3 - 1b和LLaMA3.2 - 3b模型，指标计算基于从反嵌入矩阵导出的拉回语义度量。

Result: 关注转移提示会改变两个模型内部激活轨迹，LLaMA在曲率和显著性上有显著变化，Gemma响应较弱。

Conclusion: 支持LLM几何的两层观点，Curved Inference框架可用于诊断模型对齐、抽象和推理动态，为语义抽象和模型对齐提供新见解。

Abstract: We propose Curved Inference - a geometric Interpretability framework that
tracks how the residual stream trajectory of a large language model bends in
response to shifts in semantic concern. Across 20 matched prompts spanning
emotional, moral, perspective, logical, identity, environmental, and nonsense
domains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics,
with a primary focus on curvature (\k{appa}_i) and salience (S(t)). These
metrics are computed under a pullback semantic metric derived from the
unembedding matrix, ensuring that all measurements reflect token-aligned
geometry rather than raw coordinate structure. We find that concern-shifted
prompts reliably alter internal activation trajectories in both models - with
LLaMA exhibiting consistent, statistically significant scaling in both
curvature and salience as concern intensity increases. Gemma also responds to
concern but shows weaker differentiation between moderate and strong variants.
Our results support a two-layer view of LLM geometry - a latent conceptual
structure encoded in the embedding space, and a contextual trajectory shaped by
prompt-specific inference. Curved Inference reveals how models navigate,
reorient, or reinforce semantic meaning over depth, offering a principled
method for diagnosing alignment, abstraction, and emergent inference dynamics.
These findings offer fresh insight into semantic abstraction and model
alignment through the lens of Curved Inference.

</details>


### [282] [A Survey of Classification Tasks and Approaches for Legal Contracts](https://arxiv.org/abs/2507.21108)
*Amrita Singh,Aditya Joshi,Jiaojiao Jiang,Hye-young Paik*

Main category: cs.CL

TL;DR: 本文是首篇关于自动法律合同分类（LCC）的全面综述，分析其挑战、任务、数据集、方法，指出研究局限并给出未来方向，以助力法律NLP研究和实践。


<details>
  <summary>Details</summary>
Motivation: 由于合同规模大、复杂，手动审查低效且易出错，需要自动化分析，本文旨在为法律NLP研究者和从业者提供帮助。

Method: 确定LCC的七个分类任务，回顾14个英语合同相关数据集，引入LCC方法分类体系，讨论评估技术。

Result: 回顾研究并突出最佳结果，指出当前方法的局限性。

Conclusion: 提出未来研究方向，以提高LCC的效率、准确性和可扩展性。

Abstract: Given the large size and volumes of contracts and their underlying inherent
complexity, manual reviews become inefficient and prone to errors, creating a
clear need for automation. Automatic Legal Contract Classification (LCC)
revolutionizes the way legal contracts are analyzed, offering substantial
improvements in speed, accuracy, and accessibility. This survey delves into the
challenges of automatic LCC and a detailed examination of key tasks, datasets,
and methodologies. We identify seven classification tasks within LCC, and
review fourteen datasets related to English-language contracts, including
public, proprietary, and non-public sources. We also introduce a methodology
taxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning,
and Transformer-based approaches. Additionally, the survey discusses evaluation
techniques and highlights the best-performing results from the reviewed
studies. By providing a thorough overview of current methods and their
limitations, this survey suggests future research directions to improve the
efficiency, accuracy, and scalability of LCC. As the first comprehensive survey
on LCC, it aims to support legal NLP researchers and practitioners in improving
legal processes, making legal information more accessible, and promoting a more
informed and equitable society.

</details>


### [283] [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509)
*Runjin Chen,Andy Arditi,Henry Sleight,Owain Evans,Jack Lindsey*

Main category: cs.CL

TL;DR: 本文识别大语言模型激活空间中的人格向量，用于监测、预测和控制模型人格变化，还能标记不良训练数据，且提取向量方法可自动化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的‘Assistant’有时会偏离理想特质，需识别其人格向量来监测和控制人格变化。

Method: 识别模型激活空间中的人格向量，用其监测部署时人格波动，预测和控制训练中的人格转变，通过事后干预或预防方法减轻人格变化，还用于标记不良训练数据，且提取方法自动化。

Result: 微调后有意和无意的人格变化与相关人格向量的偏移强相关，可通过干预减轻或避免，能标记不良训练数据。

Conclusion: 人格向量可用于监测、预测和控制大语言模型的人格变化，提取方法可自动化应用于多种人格特质。

Abstract: Large language models interact with users through a simulated 'Assistant'
persona. While the Assistant is typically trained to be helpful, harmless, and
honest, it sometimes deviates from these ideals. In this paper, we identify
directions in the model's activation space-persona vectors-underlying several
traits, such as evil, sycophancy, and propensity to hallucinate. We confirm
that these vectors can be used to monitor fluctuations in the Assistant's
personality at deployment time. We then apply persona vectors to predict and
control personality shifts that occur during training. We find that both
intended and unintended personality changes after finetuning are strongly
correlated with shifts along the relevant persona vectors. These shifts can be
mitigated through post-hoc intervention, or avoided in the first place with a
new preventative steering method. Moreover, persona vectors can be used to flag
training data that will produce undesirable personality changes, both at the
dataset level and the individual sample level. Our method for extracting
persona vectors is automated and can be applied to any personality trait of
interest, given only a natural-language description.

</details>


### [284] [Automatic Classification of User Requirements from Online Feedback -- A Replication Study](https://arxiv.org/abs/2507.21532)
*Meet Bhatt,Nic Boilard,Muhammad Rehan Chaudhary,Cole Thompson,Jacob Idoko,Aakash Sorathiya,Gouri Ginde*

Main category: cs.CL

TL;DR: 本文复制并扩展先前NLP4RE研究，评估不同模型可重复性和性能，发现不同模型可重复性不同，部分模型泛化能力好，评估了基线研究复制准备情况。


<details>
  <summary>Details</summary>
Motivation: RE研究对NLP4RE研究复制关注有限，NLP发展带来新机遇，因此复制并扩展先前NLP4RE研究。

Method: 复制先前研究，使用公开代码重现结果，在外部数据集评估模型性能并与GPT - 4o零样本分类器比较，准备复制研究ID卡。

Result: 不同模型可重复性不同，Naive Bayes可完美重现，BERT等结果不一；基线深度学习模型在外部数据集有良好泛化能力，GPT - 4o性能与传统基线机器学习模型相当；确认基线研究复制准备情况。

Conclusion: 为鼓励和支持研究复制，补充缺失环境设置文件到复制包并提供复制研究ID卡。

Abstract: Natural language processing (NLP) techniques have been widely applied in the
requirements engineering (RE) field to support tasks such as classification and
ambiguity detection. Although RE research is rooted in empirical investigation,
it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The
rapidly advancing realm of NLP is creating new opportunities for efficient,
machine-assisted workflows, which can bring new perspectives and results to the
forefront. Thus, we replicate and extend a previous NLP4RE study (baseline),
"Classifying User Requirements from Online Feedback in Small Dataset
Environments using Deep Learning", which evaluated different deep learning
models for requirement classification from user reviews. We reproduced the
original results using publicly released source code, thereby helping to
strengthen the external validity of the baseline study. We then extended the
setup by evaluating model performance on an external dataset and comparing
results to a GPT-4o zero-shot classifier. Furthermore, we prepared the
replication study ID-card for the baseline study, important for evaluating
replication readiness. Results showed diverse reproducibility levels across
different models, with Naive Bayes demonstrating perfect reproducibility. In
contrast, BERT and other models showed mixed results. Our findings revealed
that baseline deep learning models, BERT and ELMo, exhibited good
generalization capabilities on an external dataset, and GPT-4o showed
performance comparable to traditional baseline machine learning models.
Additionally, our assessment confirmed the baseline study's replication
readiness; however missing environment setup files would have further enhanced
readiness. We include this missing information in our replication package and
provide the replication study ID-card for our study to further encourage and
support the replication of our study.

</details>


### [285] [Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences](https://arxiv.org/abs/2507.21831)
*Andreas Reich,Claudia Thoms,Tobias Schrimpf*

Main category: cs.CL

TL;DR: 提出HALC通用管道构建最优提示，通过大量请求测试提示策略和模型表现，找到可靠提示并分析有效性等。


<details>
  <summary>Details</summary>
Motivation: 现有提示策略在不同大语言模型和任务中效果不同，试错法普遍，需系统可靠构建最优提示的方法。

Method: 提出HALC管道，向本地大语言模型发送1512个提示，基于专家编码测试提示策略和模型任务表现。

Result: 使用Mistral NeMo模型找到单变量和双变量可靠编码提示。

Conclusion: 论文揭示不同提示策略有效性、关键影响因素，可针对任务和模型识别可靠提示。

Abstract: LLMs are seeing widespread use for task automation, including automated
coding in the social sciences. However, even though researchers have proposed
different prompting strategies, their effectiveness varies across LLMs and
tasks. Often trial and error practices are still widespread. We propose
HALC$-$a general pipeline that allows for the systematic and reliable
construction of optimal prompts for any given coding task and model, permitting
the integration of any prompting strategy deemed relevant. To investigate LLM
coding and validate our pipeline, we sent a total of 1,512 individual prompts
to our local LLMs in over two million requests. We test prompting strategies
and LLM task performance based on few expert codings (ground truth). When
compared to these expert codings, we find prompts that code reliably for single
variables (${\alpha}$climate = .76; ${\alpha}$movement = .78) and across two
variables (${\alpha}$climate = .71; ${\alpha}$movement = .74) using the LLM
Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM
to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our
paper provides insights into the effectiveness of different prompting
strategies, crucial influencing factors, and the identification of reliable
prompts for each coding task and model.

</details>


### [286] [Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour](https://arxiv.org/abs/2507.21432)
*Tareq Alsaleh,Bilal Farooq*

Main category: cs.CL

TL;DR: 研究采用开源本地可部署因果大语言模型进行出行方式选择预测，推出LiTransMC模型，其表现出色，证明创建专业可解释本地部署模型的可行性，为交通研究和政策制定提供新思路。


<details>
  <summary>Details</summary>
Motivation: 探索使用开源本地可部署因果大语言模型进行出行方式选择预测，以解决传统方法问题并提高模型可解释性。

Method: 对11个大语言模型在三个数据集上进行系统基准测试，测试396种配置，生成超79000个合成通勤预测；用BERTopic和新解释强度指数评估模型推理；用参数高效和损失掩码策略微调LiTransMC。

Result: LiTransMC加权F1分数0.6845，Jensen - Shannon散度0.000245，超越未调优本地模型、大型专有系统和经典模式选择方法。

Conclusion: 证明创建集成预测和可解释性的专业本地可部署大语言模型的可行性，为交通研究和政策制定将通用大模型转化为专业可解释工具开辟道路。

Abstract: This study investigates the adoption of open-access, locally deployable
causal large language models (LLMs) for travel mode choice prediction and
introduces LiTransMC, the first fine-tuned causal LLM developed for this task.
We systematically benchmark eleven LLMs (1-12B parameters) across three stated
and revealed preference datasets, testing 396 configurations and generating
over 79,000 synthetic commuter predictions. Beyond predictive accuracy, we
evaluate models generated reasoning using BERTopic for topic modelling and a
novel Explanation Strength Index, providing the first structured analysis of
how LLMs articulate decision factors in alignment with behavioural theory.
LiTransMC, fine-tuned using parameter efficient and loss masking strategy,
achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of
0.000245, surpassing both untuned local models and larger proprietary systems,
including GPT-4o with advanced persona inference and embedding-based loading,
while also outperforming classical mode choice methods such as discrete choice
models and machine learning classifiers for the same dataset. This dual
improvement, i.e., high instant-level accuracy and near-perfect distributional
calibration, demonstrates the feasibility of creating specialist, locally
deployable LLMs that integrate prediction and interpretability. Through
combining structured behavioural prediction with natural language reasoning,
this work unlocks the potential for conversational, multi-task transport models
capable of supporting agent-based simulations, policy testing, and behavioural
insight generation. These findings establish a pathway for transforming general
purpose LLMs into specialized, explainable tools for transportation research
and policy formulation, while maintaining privacy, reducing cost, and
broadening access through local deployment.

</details>


### [287] [Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench](https://arxiv.org/abs/2507.21476)
*Reuben Narad,Siddharth Suresh,Jiayi Chen,Pine S. L. Dysart-Bricken,Bob Mankoff,Robert Nowak,Jifan Zhang,Lalit Jain*

Main category: cs.CL

TL;DR: 介绍HumorBench基准，用于评估大语言模型理解漫画幽默能力，对当前SOTA模型测试有三点发现。


<details>
  <summary>Details</summary>
Motivation: 现有数学和科学基准被推理模型饱和，需要在STEM领域之外进行新颖且有挑战性的模型智能评估。

Method: 构建包含约300个漫画 - 标题对及专家注释评分标准的HumorBench，基于模型对幽默的解释和识别笑话元素的能力进行评估。

Result: 对当前SOTA模型广泛测试有三点发现：STEM推理进展有效转移到幽默理解；仅在STEM推理数据训练的模型在HumorBench表现良好；增加思考令牌预算在不同模型的幽默推理中效果不一。

Conclusion: 大语言模型在STEM的推理能力有较强可迁移性，不同模型在幽默推理中对测试时扩展的反应不同。

Abstract: We present HumorBench, a benchmark designed to evaluate large language
models' (LLMs) ability to reason about and explain sophisticated humor in
cartoon captions. As reasoning models increasingly saturate existing benchmarks
in mathematics and science, novel and challenging evaluations of model
intelligence beyond STEM domains are essential. Reasoning is fundamentally
involved in text-based humor comprehension, requiring the identification of
connections between concepts in cartoons/captions and external cultural
references, wordplays, and other mechanisms. HumorBench includes approximately
300 unique cartoon-caption pairs from the New Yorker Caption Contest and
Cartoonstock.com, with expert-annotated evaluation rubrics identifying
essential joke elements. LLMs are evaluated based on their explanations towards
the humor and abilities in identifying the joke elements. To perform well on
this task, models must form and test hypotheses about associations between
concepts, potentially backtracking from initial interpretations to arrive at
the most plausible explanation. Our extensive benchmarking of current SOTA
models reveals three key insights: (1) LLM progress on STEM reasoning transfers
effectively to humor comprehension; (2) models trained exclusively on STEM
reasoning data still perform well on HumorBench, demonstrating strong
transferability of reasoning abilities; and (3) test-time scaling by increasing
thinking token budgets yields mixed results across different models in humor
reasoning.

</details>


### [288] [Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs](https://arxiv.org/abs/2507.21482)
*Abhinav Arabelly,Jagrut Nemade,Robert D Nowak,Jifan Zhang*

Main category: cs.CL

TL;DR: 论文针对监督微调的标签高效学习问题，基于任务多样性提出简单采样策略，效果好且成本低。


<details>
  <summary>Details</summary>
Motivation: 开发特定应用的高性能大语言模型需大量人工标注，耗时耗力且成本高，要解决标签高效学习问题。

Method: 基于任务标签易获取和预训练模型在不同任务置信度不同这两个观察，采用逆置信度加权策略跨任务选择示例。

Result: 该方法训练的模型效果与复杂采样程序相当或更好，在MMLU分数上提升4%，在不同标注预算和数据集上表现出色，能降低80%标注成本。

Conclusion: 提出的基于任务多样性的数据选择策略简单有效，可实现高效标签学习，降低标注成本。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, but developing high-performing models for specialized
applications often requires substantial human annotation -- a process that is
time-consuming, labor-intensive, and expensive. In this paper, we address the
label-efficient learning problem for supervised finetuning (SFT) by leveraging
task-diversity as a fundamental principle for effective data selection. This is
markedly different from existing methods based on the prompt-diversity. Our
approach is based on two key observations: 1) task labels for different prompts
are often readily available; 2) pre-trained models have significantly varying
levels of confidence across tasks. We combine these facts to devise a simple
yet effective sampling strategy: we select examples across tasks using an
inverse confidence weighting strategy. This produces models comparable to or
better than those trained with more complex sampling procedures, while being
significantly easier to implement and less computationally intensive. Notably,
our experimental results demonstrate that this method can achieve better
accuracy than training on the complete dataset (a 4\% increase in MMLU score).
Across various annotation budgets and two instruction finetuning datasets, our
algorithm consistently performs at or above the level of the best existing
methods, while reducing annotation costs by up to 80\%.

</details>


### [289] [VN-MTEB: Vietnamese Massive Text Embedding Benchmark](https://arxiv.org/abs/2507.21500)
*Loc Pham,Tung Luu,Thu Vo,Minh Nguyen,Viet Hoang*

Main category: cs.CL

TL;DR: 针对越南互联网环境，提出越南基准VN - MTEB评估嵌入模型，分析不同模型在嵌入任务表现，数据集公开。


<details>
  <summary>Details</summary>
Motivation: 越南互联网流量和网络毒性高，需嵌入模型用于应用推荐和内容控制，但缺乏大规模测试数据集，难以有效评估AI模型。

Method: 创建自动化框架，将英语样本翻译为越南语，利用大语言模型和先进嵌入模型进行翻译和过滤，保留高质量样本。基准包含41个来自六个任务的数据集。

Result: 分析发现使用Rotary Positional Embedding的更大、更复杂模型在嵌入任务中优于使用Absolute Positional Embedding的模型。

Conclusion: VN - MTEB基准可用于评估越南语嵌入模型，不同位置嵌入方法的模型表现有差异。

Abstract: Vietnam ranks among the top countries in terms of both internet traffic and
online toxicity. As a result, implementing embedding models for recommendation
and content control duties in applications is crucial. However, a lack of
large-scale test datasets, both in volume and task diversity, makes it tricky
for scientists to effectively evaluate AI models before deploying them in
real-world, large-scale projects. To solve this important problem, we introduce
a Vietnamese benchmark, VN-MTEB for embedding models, which we created by
translating a large number of English samples from the Massive Text Embedding
Benchmark using our new automated framework. We leverage the strengths of large
language models (LLMs) and cutting-edge embedding models to conduct translation
and filtering processes to retain high-quality samples, guaranteeing a natural
flow of language and semantic fidelity while preserving named entity
recognition (NER) and code snippets. Our comprehensive benchmark consists of 41
datasets from six tasks specifically designed for Vietnamese text embeddings.
In our analysis, we find that bigger and more complex models using Rotary
Positional Embedding outperform those using Absolute Positional Embedding in
embedding tasks. Datasets are available at HuggingFace:
https://huggingface.co/collections/GreenNode/vn-mteb-68871433f0f7573b8e1a6686

</details>


### [290] [Training language models to be warm and empathetic makes them less reliable and more sycophantic](https://arxiv.org/abs/2507.21919)
*Lujain Ibrahim,Franziska Sofia Hafner,Luc Rocher*

Main category: cs.CL

TL;DR: 优化语言模型的温暖特性会降低其可靠性，研究揭示此问题并呼吁重新思考开发和监管方式。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型温暖特性与可靠性之间的关系，因AI语言模型广泛用于提供建议等，其温暖特性可能带来问题。

Method: 对五种不同规模和架构的语言模型进行控制实验，训练其产生更温暖、更有同理心的回应，再在关键安全任务上评估。

Result: 温暖模型比原始模型错误率高10 - 30个百分点，会传播阴谋论、提供错误信息和不良医疗建议，更易认可用户错误信念。

Conclusion: 随着类人AI大规模部署，需要重新思考如何开发和监管这些重塑人类关系和社交互动的系统。

Abstract: Artificial intelligence (AI) developers are increasingly building language
models with warm and empathetic personas that millions of people now use for
advice, therapy, and companionship. Here, we show how this creates a
significant trade-off: optimizing language models for warmth undermines their
reliability, especially when users express vulnerability. We conducted
controlled experiments on five language models of varying sizes and
architectures, training them to produce warmer, more empathetic responses, then
evaluating them on safety-critical tasks. Warm models showed substantially
higher error rates (+10 to +30 percentage points) than their original
counterparts, promoting conspiracy theories, providing incorrect factual
information, and offering problematic medical advice. They were also
significantly more likely to validate incorrect user beliefs, particularly when
user messages expressed sadness. Importantly, these effects were consistent
across different model architectures, and occurred despite preserved
performance on standard benchmarks, revealing systematic risks that current
evaluation practices may fail to detect. As human-like AI systems are deployed
at an unprecedented scale, our findings indicate a need to rethink how we
develop and oversee these systems that are reshaping human relationships and
social interaction.

</details>


### [291] [Post-Training Large Language Models via Reinforcement Learning from Self-Feedback](https://arxiv.org/abs/2507.21931)
*Carel van Niekerk,Renato Vukovic,Benjamin Matthias Ruppik,Hsien-chin Lin,Milica Gašić*

Main category: cs.CL

TL;DR: 提出RLSF方法，利用模型自身信心作内在奖励对大语言模型进行训练后微调，改善模型校准和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常给出似是而非但校准不佳的答案，限制其在推理密集型任务中的可靠性。

Method: 在冻结的大语言模型生成多个思维链解决方案后，定义并计算每个最终答案跨度的置信度，对轨迹进行排序，用这些合成偏好通过标准偏好优化微调策略。

Result: 同时改善了模型的概率估计校准，增强了逐步推理能力，在算术推理和多项选择题回答上表现更好。

Conclusion: RLSF证明强化学习对模型内在行为的作用，是大语言模型训练后流程的有效组成部分，值得进一步研究内在奖励。

Abstract: Large Language Models (LLMs) often produce plausible but poorly-calibrated
answers, limiting their reliability on reasoning-intensive tasks. We present
Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that
uses the model's own confidence as an intrinsic reward, mimicking how humans
learn in the absence of external feedback. After a frozen LLM generates several
chain-of-thought solutions, we define and compute the confidence of each final
answer span and rank the traces accordingly. These synthetic preferences are
then used to fine-tune the policy with standard preference optimization,
similar to RLHF yet requiring no human labels, gold answers, or externally
curated rewards.
  RLSF simultaneously (i) refines the model's probability estimates --
restoring well-behaved calibration -- and (ii) strengthens step-by-step
reasoning, yielding improved performance on arithmetic reasoning and
multiple-choice question answering.
  By turning a model's own uncertainty into useful self-feedback, RLSF affirms
reinforcement learning on intrinsic model behaviour as a principled and
data-efficient component of the LLM post-training pipeline and warrents further
research in intrinsic rewards for LLM post-training.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [292] [EnTao-GPM: DNA Foundation Model for Predicting the Germline Pathogenic Mutations](https://arxiv.org/abs/2507.21706)
*Zekai Lin,Haoran Sun,Yucheng Guo,Yujie Yang,Yanwen Wang,Bozhen Hu,Chonghang Ye,Qirong Yang,Fan Zhong,Xiaoming Zhang,Lei Liu*

Main category: q-bio.GN

TL;DR: EnTao - GPM由复旦和BioMap开发，通过三项创新提升突变分类准确性，经ClinVar验证效果好，推动精准医学。


<details>
  <summary>Details</summary>
Motivation: 精准医学中区分致病突变和良性多态性是关键挑战，该研究旨在解决此问题。

Method: 1. 对疾病相关哺乳动物基因组进行跨物种靶向预训练；2. 在ClinVar和HGMD上微调进行种系突变专业化；3. 集成DNA序列嵌入和基于大语言模型的统计解释的可解释临床框架。

Result: 经ClinVar验证，EnTao - GPM在突变分类上表现出卓越的准确性。

Conclusion: EnTao - GPM革新了基因检测，能为临床诊断和研究提供更快、更准确、更易获取的解释，推动个性化医疗发展。

Abstract: Distinguishing pathogenic mutations from benign polymorphisms remains a
critical challenge in precision medicine. EnTao-GPM, developed by Fudan
University and BioMap, addresses this through three innovations: (1)
Cross-species targeted pre-training on disease-relevant mammalian genomes
(human, pig, mouse), leveraging evolutionary conservation to enhance
interpretation of pathogenic motifs, particularly in non-coding regions; (2)
Germline mutation specialization via fine-tuning on ClinVar and HGMD, improving
accuracy for both SNVs and non-SNVs; (3) Interpretable clinical framework
integrating DNA sequence embeddings with LLM-based statistical explanations to
provide actionable insights. Validated against ClinVar, EnTao-GPM demonstrates
superior accuracy in mutation classification. It revolutionizes genetic testing
by enabling faster, more accurate, and accessible interpretation for clinical
diagnostics (e.g., variant assessment, risk identification, personalized
treatment) and research, advancing personalized medicine.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [293] [Towards a Large Physics Benchmark](https://arxiv.org/abs/2507.21695)
*Kristian G. Barman,Sascha Caron,Faegheh Hasibi,Eugene Shalugin,Yoris Marcet,Johannes Otte,Henk W. de Regt,Merijn Moody*

Main category: physics.data-an

TL;DR: 本文介绍用于评估、监测和引导基础物理领域大语言模型发展的基准框架。


<details>
  <summary>Details</summary>
Motivation: 为基础物理领域的大语言模型发展提供评估、监测和引导的工具。

Method: 基于科学理解和创造力的哲学概念开发评分系统，设置三种形式的问题，构建包含多样示例的数据集，并提出动态基准的方式。

Result: 创建了当前包含多样示例的数据集，提出动态基准的模式。

Conclusion: 此基准框架有望推动有针对性的AI发展，为基础物理研究做贡献。

Abstract: We introduce a benchmark framework developed by and for the scientific
community to evaluate, monitor and steer large language model development in
fundamental physics. Building on philosophical concepts of scientific
understanding and creativity, we develop a scoring system in which each
question is scored by an expert for its correctness, difficulty, and surprise.
The questions are of three forms: (i) multiple-choice questions for conceptual
understanding, (ii) analytical problems requiring mathematical derivation, and
(iii) openended tasks requiring complex problem solving. Our current dataset
contains diverse set of examples, including a machine learning challenge to
classify high-energy physics events, such as the four top quark signal. To
ensure continued relevance, we propose a living benchmark, where physicists
contribute questions, for instance alongside new publications. We invite
contributions via: http://www.physicsbenchmarks.org/. We hope that this
benchmark will enable a targeted AI development that can make a meaningful
contribution to fundamental physics research.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [294] [Large-Scale Linear Energy System Optimization: A Systematic Review on Parallelization Strategies via Decomposition](https://arxiv.org/abs/2507.21932)
*Lars Hadidi,Leonard Göke,Maximilian Hoffmann,Mario Klostermeier,Shima Sasanpour,Tim Varelmann,Vassilios Yfantis,Jochen Linßen,Detlef Stolten,Jann M. Weinand*

Main category: math.OC

TL;DR: 探讨能源系统优化模型并行化策略，提出分类方案，综述并行分解方法及软件工具，并给出基准建议。


<details>
  <summary>Details</summary>
Motivation: 可再生能源整合等使能源系统优化模型规模和复杂度增加，求解器性能受限，需并行化策略应对。

Method: 提出线性能源系统优化模型分类方案，综述并行分解方法，调查可用软件工具。

Result: 许多并行分解方法有性能优势，但无普遍最优方法，且缺乏标准化基准套件。

Conclusion: 给出未来基准的必要标准和最低报告标准，研究见解可扩展到更广泛的运筹学领域。

Abstract: As renewable energy integration, sector coupling, and spatiotemporal detail
increase, energy system optimization models grow in size and complexity, often
pushing solvers to their performance limits. This systematic review explores
parallelization strategies that can address these challenges. We first propose
a classification scheme for linear energy system optimization models, covering
their analytical focus, mathematical structure, and scope. We then review
parallel decomposition methods, finding that while many offer performance
benefits, no single approach is universally superior. The lack of standardized
benchmark suites further complicates comparison. To address this, we recommend
essential criteria for future benchmarks and minimum reporting standards. We
also survey available software tools for parallel decomposition, including
modular frameworks and algorithmic abstractions. Though centered on energy
system models, our insights extend to the broader operations research field.

</details>


### [295] [On Policy Stochasticity in Mutual Information Optimal Control of Linear Systems](https://arxiv.org/abs/2507.21543)
*Shoju Enami,Kenji Kashima*

Main category: math.OC

TL;DR: 本文研究离散时间线性系统的互信息最优控制问题中温度参数与策略随机性的关系，推导最优策略和交替优化算法所得策略的随机与确定性条件，并通过数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 理论上阐明互信息最优控制中温度参数与策略随机性的关系，该关系此前未被研究。

Method: 扩展先前研究结果，建立最优策略存在性，推导不同策略在温度参数下的随机与确定性条件。

Result: 得到了最优策略和交替优化算法所得策略在不同温度参数下的随机与确定性条件。

Conclusion: 通过数值实验验证了理论结果的有效性。

Abstract: In recent years, mutual information optimal control has been proposed as an
extension of maximum entropy optimal control. Both approaches introduce
regularization terms to render the policy stochastic, and it is important to
theoretically clarify the relationship between the temperature parameter (i.e.,
the coefficient of the regularization term) and the stochasticity of the
policy. Unlike in maximum entropy optimal control, this relationship remains
unexplored in mutual information optimal control. In this paper, we investigate
this relationship for a mutual information optimal control problem (MIOCP) of
discrete-time linear systems. After extending the result of a previous study of
the MIOCP, we establish the existence of an optimal policy of the MIOCP, and
then derive the respective conditions on the temperature parameter under which
the optimal policy becomes stochastic and deterministic. Furthermore, we also
derive the respective conditions on the temperature parameter under which the
policy obtained by an alternating optimization algorithm becomes stochastic and
deterministic. The validity of the theoretical results is demonstrated through
numerical experiments.

</details>


### [296] [Riemannian Optimization on Tree Tensor Networks with Application in Machine Learning](https://arxiv.org/abs/2507.21726)
*Marius Willner,Marco Trenti,Dirk Lebiedz*

Main category: math.OC

TL;DR: 本文对树张量网络（TTNs）进行微分几何形式分析，开发优化算法和反向传播算法，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 对广泛应用于低秩近似和量子多体模拟的TTNs进行微分几何分析，开发相关算法。

Method: 对TTNs的微分几何进行形式分析，开发一阶和二阶优化算法，设计反向传播算法。

Result: 通过代表性机器学习任务的数值实验验证了方法。

Conclusion: 所开发的算法在实验中得到验证，对TTNs的研究有积极意义。

Abstract: Tree tensor networks (TTNs) are widely used in low-rank approximation and
quantum many-body simulation. In this work, we present a formal analysis of the
differential geometry underlying TTNs. Building on this foundation, we develop
efficient first- and second-order optimization algorithms that exploit the
intrinsic quotient structure of TTNs. Additionally, we devise a backpropagation
algorithm for training TTNs in a kernel learning setting. We validate our
methods through numerical experiments on a representative machine learning
task.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [297] [Numerical PDE solvers outperform neural PDE solvers](https://arxiv.org/abs/2507.21269)
*Patrick Chatain,Michael Rizvi-Martel,Guillaume Rabusseau,Adam Oberman*

Main category: math.NA

TL;DR: 提出DeepFDM框架用于学习时变偏微分方程中空间变化系数，在多个测试中表现优于其他模型，可作为参数偏微分方程数据驱动求解和识别的基线。


<details>
  <summary>Details</summary>
Motivation: 解决时变偏微分方程中空间变化系数的学习问题。

Method: 将经典前向欧拉离散化嵌入卷积架构，通过CFL合规系数参数化确保稳定性和一阶收敛。

Result: 在分布内和分布外测试中，DeepFDM的归一化均方误差比其他模型小1 - 2个数量级，训练轮数少10 - 20倍，参数少5 - 50倍，恢复的系数场与真实参数准确匹配。

Conclusion: DeepFDM可作为参数偏微分方程数据驱动求解和识别的稳健、高效且透明的基线。

Abstract: We present DeepFDM, a differentiable finite-difference framework for learning
spatially varying coefficients in time-dependent partial differential equations
(PDEs). By embedding a classical forward-Euler discretization into a
convolutional architecture, DeepFDM enforces stability and first-order
convergence via CFL-compliant coefficient parameterizations. Model weights
correspond directly to PDE coefficients, yielding an interpretable
inverse-problem formulation. We evaluate DeepFDM on a benchmark suite of scalar
PDEs: advection, diffusion, advection-diffusion, reaction-diffusion and
inhomogeneous Burgers' equations-in one, two and three spatial dimensions. In
both in-distribution and out-of-distribution tests (quantified by the Hellinger
distance between coefficient priors), DeepFDM attains normalized mean-squared
errors one to two orders of magnitude smaller than Fourier Neural Operators,
U-Nets and ResNets; requires 10-20X fewer training epochs; and uses 5-50X fewer
parameters. Moreover, recovered coefficient fields accurately match
ground-truth parameters. These results establish DeepFDM as a robust,
efficient, and transparent baseline for data-driven solution and identification
of parametric PDEs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [298] [Benchmarking a Tunable Quantum Neural Network on Trapped-Ion and Superconducting Hardware](https://arxiv.org/abs/2507.21222)
*Djamil Lakhdar-Hamina,Xingxin Liu,Richard Barney,Sarah H. Miller,Alaina M. Green,Norbert M. Linke,Victor Galitski*

Main category: quant-ph

TL;DR: 本文在囚禁离子和IBM超导量子计算机上实现量子神经网络对MNIST图像分类，研究了插值参数对性能的影响及物理噪声的作用，为更复杂量子神经网络提供基础。


<details>
  <summary>Details</summary>
Motivation: 在量子计算机上实现神经网络以实现更复杂量子神经网络，探索近量子优势。

Method: 在量子计算机实现量子神经网络，通过模拟训练网络，实验进行推理，利用插值参数控制经典到量子的对应，插入门对衡量物理噪声。

Result: 适度插值参数能提升网络性能；边界图像分类中网络行为与模拟有偏差；清晰图像对物理噪声不敏感。

Conclusion: 此方法为当前设备上更复杂量子神经网络提供跳板，扩展网络或具经典不可模拟性及近量子优势。

Abstract: We implement a quantum generalization of a neural network on trapped-ion and
IBM superconducting quantum computers to classify MNIST images, a common
benchmark in computer vision. The network feedforward involves qubit rotations
whose angles depend on the results of measurements in the previous layer. The
network is trained via simulation, but inference is performed experimentally on
quantum hardware. The classical-to-quantum correspondence is controlled by an
interpolation parameter, $a$, which is zero in the classical limit. Increasing
$a$ introduces quantum uncertainty into the measurements, which is shown to
improve network performance at moderate values of the interpolation parameter.
We then focus on particular images that fail to be classified by a classical
neural network but are detected correctly in the quantum network. For such
borderline cases, we observe strong deviations from the simulated behavior. We
attribute this to physical noise, which causes the output to fluctuate between
nearby minima of the classification energy landscape. Such strong sensitivity
to physical noise is absent for clear images. We further benchmark physical
noise by inserting additional single-qubit and two-qubit gate pairs into the
neural network circuits. Our work provides a springboard toward more complex
quantum neural networks on current devices: while the approach is rooted in
standard classical machine learning, scaling up such networks may prove
classically non-simulable and could offer a route to near-term quantum
advantage.

</details>


### [299] [An em algorithm for quantum Boltzmann machines](https://arxiv.org/abs/2507.21569)
*Takeshi Kimura,Kohtaro Kato,Masahito Hayashi*

Main category: quant-ph

TL;DR: 开发量子EM算法训练量子玻尔兹曼机，在半量子受限玻尔兹曼机上实现，实验显示性能优，体现信息几何优化在量子机器学习潜力。


<details>
  <summary>Details</summary>
Motivation: 开发适合量子玻尔兹曼机训练的算法，利用信息几何优化解决传统方法在量子场景的不足。

Method: 开发量子版的em算法，在半量子受限玻尔兹曼机上实现该算法。

Result: 数值实验表明该方法学习稳定，在一些情况下优于基于梯度的训练。

Conclusion: 信息几何优化在量子机器学习有潜力，尤其在标准方法因非对易性或梯度消失难以发挥作用的场景。

Abstract: We develop a quantum version of the em algorithm for training quantum
Boltzmann machines. The em algorithm is an information-geometric extension of
the well-known expectation-maximization (EM) algorithm, offering a structured
alternative to gradient-based methods with potential advantages in stability
and convergence. We implement the algorithm on a semi-quantum restricted
Boltzmann machine, where quantum effects are confined to the hidden layer. This
structure enables analytical update rules while preserving quantum
expressivity. Numerical experiments on benchmark datasets show that the
proposed method achieves stable learning and outperforms gradient-based
training in several cases. These results demonstrate the potential of
information-geometric optimization for quantum machine learning, particularly
in settings where standard methods struggle due to non-commutativity or
vanishing gradients.

</details>


### [300] [Data-driven quantum Koopman method for simulating nonlinear dynamics](https://arxiv.org/abs/2507.21890)
*Baoyang Zhang,Zhen Lu,Yaomin Zhao,Yue Yang*

Main category: quant-ph

TL;DR: 提出量子Koopman方法（QKM），将非线性动力学转化为高维可观测空间中的线性酉演化，实现量子加速模拟非线性现象，在多种非线性系统中验证有效。


<details>
  <summary>Details</summary>
Motivation: 量子计算模拟非线性动力学受酉演化要求限制，需一种方法弥合差距。

Method: 利用Koopman算子理论，通过深度自编码器将系统状态映射到希尔伯特空间层次结构，在嵌入空间分解状态表示，用从数据学习系数的对角哈密顿量构建酉Koopman算子。

Result: QKM在反应 - 扩散系统和剪切流中预测相对误差低于6%，能捕捉二维湍流关键统计量。

Conclusion: 该方法为量子加速模拟非线性现象建立了实用途径，结合深度学习和量子算法。

Abstract: Quantum computation offers potential exponential speedups for simulating
certain physical systems, but its application to nonlinear dynamics is
inherently constrained by the requirement of unitary evolution. We propose the
quantum Koopman method (QKM), a data-driven framework that bridges this gap
through transforming nonlinear dynamics into linear unitary evolution in
higher-dimensional observable spaces. Leveraging the Koopman operator theory to
achieve a global linearization, our approach maps system states into a
hierarchy of Hilbert spaces using a deep autoencoder. Within the linearized
embedding spaces, the state representation is decomposed into modulus and phase
components, and the evolution is governed by a set of unitary Koopman operators
that act exclusively on the phase. These operators are constructed from
diagonal Hamiltonians with coefficients learned from data, a structure designed
for efficient implementation on quantum hardware. This architecture enables
direct multi-step prediction, and the operator's computational complexity
scales logarithmically with the observable space dimension. The QKM is
validated across diverse nonlinear systems. Its predictions maintain relative
errors below 6% for reaction-diffusion systems and shear flows, and capture key
statistics in 2D turbulence. This work establishes a practical pathway for
quantum-accelerated simulation of nonlinear phenomena, exploring a framework
built on the synergy between deep learning for global linearization and quantum
algorithms for unitary dynamics evolution.

</details>


### [301] [Supervised Quantum Image Processing](https://arxiv.org/abs/2507.22039)
*Marco Parigi,Mehran Khosrojerdi,Filippo Caruso,Leonardo Banchi*

Main category: quant-ph

TL;DR: 本文比较四种量子图像表示（QImRs）的压缩特性，发现FRQI压缩效果更好，还研究二分类问题中精度与内存权衡，表明量子核分类精度相当但存储资源需求指数级减少。


<details>
  <summary>Details</summary>
Motivation: 大数据和人工智能时代，提高数据存储、处理和分析效率有需求，量子图像处理有潜力缓解相关挑战。

Method: 比较四种QImRs（TNR、FRQI、NEQR、QPIE）的压缩特性，研究二分类问题中量子核与经典线性核的性能。

Result: FRQI比其他三种QImRs图像信息压缩效果更好；量子核分类平均精度与经典线性核相当，但图像存储所需资源呈指数级减少。

Conclusion: FRQI在图像信息压缩方面有优势，量子核在二分类问题中能以更少资源达到相当精度。

Abstract: In the era of big data and artificial intelligence, the increasing volume of
data and the demand to solve more and more complex computational challenges are
two driving forces for improving the efficiency of data storage, processing and
analysis. Quantum image processing (QIP) is an interdisciplinary field between
quantum information science and image processing, which has the potential to
alleviate some of these challenges by leveraging the power of quantum
computing. In this work, we compare and examine the compression properties of
four different Quantum Image Representations (QImRs): namely, Tensor Network
Representation (TNR), Flexible Representation of Quantum Image (FRQI), Novel
Enhanced Quantum Representation NEQR, and Quantum Probability Image Encoding
(QPIE). Our simulations show that FRQI performs a higher compression of image
information than TNR, NEQR, and QPIE. Furthermore, we investigate the trade-off
between accuracy and memory in binary classification problems, evaluating the
performance of quantum kernels based on QImRs compared to the classical linear
kernel. Our results indicate that quantum kernels provide comparable
classification average accuracy but require exponentially fewer resources for
image storage.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [302] [Domain Generalization and Adaptation in Intensive Care with Anchor Regression](https://arxiv.org/abs/2507.21783)
*Malte Londschien,Manuel Burger,Gunnar Rätsch,Peter Bühlmann*

Main category: stat.AP

TL;DR: 研究因果启发的领域泛化在异质多中心ICU数据上的应用，介绍新方法并提出框架区分数据利用情况。


<details>
  <summary>Details</summary>
Motivation: 预测模型在新医院部署时因分布偏移性能下降，需在异质多中心ICU数据上研究领域泛化。

Method: 应用锚回归，引入基于树的非线性扩展锚提升方法，提出量化外部大数据集效用的概念框架。

Result: 锚正则化持续提升分布外性能，方法对理论假设的违反具有鲁棒性，确定三种数据利用机制。

Conclusion: 因果启发的领域泛化方法有效，不同数据量情况下外部数据的利用方式不同。

Abstract: The performance of predictive models in clinical settings often degrades when
deployed in new hospitals due to distribution shifts. This paper presents a
large-scale study of causality-inspired domain generalization on heterogeneous
multi-center intensive care unit (ICU) data. We apply anchor regression and
introduce anchor boosting, a novel, tree-based nonlinear extension, to a large
dataset comprising 400,000 patients from nine distinct ICU databases. The
anchor regularization consistently improves out-of-distribution performance,
particularly for the most dissimilar target domains. The methods appear robust
to violations of theoretical assumptions, such as anchor exogeneity.
Furthermore, we propose a novel conceptual framework to quantify the utility of
large external data datasets. By evaluating performance as a function of
available target-domain data, we identify three regimes: (i) a domain
generalization regime, where only the external model should be used, (ii) a
domain adaptation regime, where refitting the external model is optimal, and
(iii) a data-rich regime, where external data provides no additional value.

</details>


### [303] [Predicting VBAC Outcomes from U.S. Natality Data using Deep and Classical Machine Learning Models](https://arxiv.org/abs/2507.21330)
*Ananya Anand*

Main category: stat.AP

TL;DR: 研究用643,029例TOLAC病例构建机器学习模型预测VBAC，MLP和XGBoost表现较好，早期妊娠变量可用于构建VBAC预测模型。


<details>
  <summary>Details</summary>
Motivation: 准确预测剖宫产术后试产（TOLAC）结果，指导产前咨询并降低分娩相关风险。

Method: 从CDC WONDER Natality数据集选取643,029例TOLAC病例，筛选单胎、有一或两次剖宫产史且47个产前特征数据完整的案例，训练逻辑回归、XGBoost和多层感知器（MLP）三种分类器，对MLP应用类加权，为XGBoost实现自定义损失函数，用ROC曲线、混淆矩阵和精确率 - 召回率分析评估。

Result: MLP性能最高，AUC为0.7287，XGBoost紧随其后（AUC = 0.727），均超逻辑回归基线（AUC = 0.709），逻辑回归系数显示产妇BMI、教育程度等是关键预测因素。

Conclusion: 常规收集的早期妊娠变量可支持可扩展且表现较好的VBAC预测模型，在缺乏专业产时数据的环境有临床决策支持潜力。

Abstract: Accurately predicting the outcome of a trial of labor after cesarean (TOLAC)
is essential for guiding prenatal counseling and minimizing delivery-related
risks. This study presents supervised machine learning models for predicting
vaginal birth after cesarean (VBAC) using 643,029 TOLAC cases from the CDC
WONDER Natality dataset (2017-2023). After filtering for singleton births with
one or two prior cesareans and complete data across 47 prenatal-period
features, three classifiers were trained: logistic regression, XGBoost, and a
multilayer perceptron (MLP). The MLP achieved the highest performance with an
AUC of 0.7287, followed closely by XGBoost (AUC = 0.727), both surpassing the
logistic regression baseline (AUC = 0.709). To address class imbalance, class
weighting was applied to the MLP, and a custom loss function was implemented in
XGBoost. Evaluation metrics included ROC curves, confusion matrices, and
precision-recall analysis. Logistic regression coefficients highlighted
maternal BMI, education, parity, comorbidities, and prenatal care indicators as
key predictors. Overall, the results demonstrate that routinely collected,
early-pregnancy variables can support scalable and moderately high-performing
VBAC prediction models. These models offer potential utility in clinical
decision support, particularly in settings lacking access to specialized
intrapartum data.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [304] [Higher-Order Kuramoto Oscillator Network for Dense Associative Memory](https://arxiv.org/abs/2507.21984)
*Jona Nagerl,Natalia G. Berloff*

Main category: nlin.AO

TL;DR: 引入含二次和四次谐波耦合的广义Kuramoto模型，研究其作为密集联想记忆的特性，结果表明高阶耦合提升记忆容量，模拟验证理论预测。


<details>
  <summary>Details</summary>
Motivation: 探究超越经典Kuramoto模型的高阶耦合在网络相位振荡器作为密集联想记忆中的作用。

Method: 运用平均场理论及其动态近似，还进行了大规模振荡器网络模拟。

Result: 得到相图，发现三临界点，确定双稳区域，噪声下记忆逃逸时间随网络规模指数增长，高阶耦合使记忆容量超线性增长，模拟证实理论。

Conclusion: 该研究将Kuramoto同步与现代Hopfield记忆联系起来，有望在振荡器系统中实验实现高容量模拟联想记忆。

Abstract: Networks of phase oscillators can serve as dense associative memories if they
incorporate higher-order coupling beyond the classical Kuramoto model's
pairwise interactions. Here we introduce a generalized Kuramoto model with
combined second-harmonic (pairwise) and fourth-harmonic (quartic) coupling,
inspired by dense Hopfield memory theory. Using mean-field theory and its
dynamical approximation, we obtain a phase diagram for dense associative memory
model that exhibits a tricritical point at which the continuous onset of memory
retrieval is supplanted by a discontinuous, hysteretic transition. In the
quartic-dominated regime, the system supports bistable phase-locked states
corresponding to stored memory patterns, with a sizable energy barrier between
memory and incoherent states. We analytically determine this bistable region
and show that the escape time from a memory state (due to noise) grows
exponentially with network size, indicating robust storage. Extending the
theory to finite memory load, we show that higher-order couplings achieve
superlinear scaling of memory capacity with system size, far exceeding the
limit of pairwise-only oscillators. Large-scale simulations of the oscillator
network confirm our theoretical predictions, demonstrating rapid pattern
retrieval and robust storage of many phase patterns. These results bridge the
Kuramoto synchronization with modern Hopfield memories, pointing toward
experimental realization of high-capacity, analog associative memory in
oscillator systems.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [305] [diffSPH: Differentiable Smoothed Particle Hydrodynamics for Adjoint Optimization and Machine Learning](https://arxiv.org/abs/2507.21684)
*Rene Winchenbach,Nils Thuerey*

Main category: physics.flu-dyn

TL;DR: 提出开源可微SPH框架diffSPH，展示其多应用场景，为CFD社区提供基础平台。


<details>
  <summary>Details</summary>
Motivation: 促进计算流体动力学中的优化和机器学习应用，如训练神经网络和开发混合模型。

Method: 在PyTorch中开发具有GPU加速的可微SPH框架，包含不同物理方案。

Result: 通过多个应用展示框架独特能力，如解决粒子移位、优化初始条件等。

Conclusion: 工作为CFD社区开发和部署新的神经网络及伴随优化应用提供基础平台。

Abstract: We present diffSPH, a novel open-source differentiable Smoothed Particle
Hydrodynamics (SPH) framework developed entirely in PyTorch with GPU
acceleration. diffSPH is designed centrally around differentiation to
facilitate optimization and machine learning (ML) applications in Computational
Fluid Dynamics~(CFD), including training neural networks and the development of
hybrid models. Its differentiable SPH core, and schemes for compressible (with
shock capturing and multi-phase flows), weakly compressible (with boundary
handling and free-surface flows), and incompressible physics, enable a broad
range of application areas. We demonstrate the framework's unique capabilities
through several applications, including addressing particle shifting via a
novel, target-oriented approach by minimizing physical and regularization loss
terms, a task often intractable in traditional solvers. Further examples
include optimizing initial conditions and physical parameters to match target
trajectories, shape optimization, implementing a solver-in-the-loop setup to
emulate higher-order integration, and demonstrating gradient propagation
through hundreds of full simulation steps. Prioritizing readability, usability,
and extensibility, this work offers a foundational platform for the CFD
community to develop and deploy novel neural networks and adjoint optimization
applications.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [306] [Learning Simulatable Models of Cloth with Spatially-varying Constitutive Properties](https://arxiv.org/abs/2507.21288)
*Guanxiong Chen,Shashwat Suri,Yuhao Wu,Etienne Voulga,David I. W. Levin,Dinesh Pai*

Main category: cs.GR

TL;DR: 提出Mass - Spring Net框架学习替代模型模拟复杂布料材料，有优势。


<details>
  <summary>Details</summary>
Motivation: 真实衣物材料复杂，有限元模拟计算量大、慢且有数值伪影问题。

Method: 提出Mass - Spring Net框架，将布料离散为含未知参数的质量 - 弹簧网络，用新型力和冲量损失函数从运动数据中学习参数。

Result: 能准确建模不同数据源的空间变化材料属性，不受膜锁定影响，比图网络和基于神经ODE架构训练快、重建精度高、泛化性好。

Conclusion: Mass - Spring Net框架在模拟复杂布料材料方面有效且有优势。

Abstract: Materials used in real clothing exhibit remarkable complexity and spatial
variation due to common processes such as stitching, hemming, dyeing, printing,
padding, and bonding. Simulating these materials, for instance using finite
element methods, is often computationally demanding and slow. Worse, such
methods can suffer from numerical artifacts called ``membrane locking'' that
makes cloth appear artificially stiff. Here we propose a general framework,
called Mass-Spring Net, for learning a simple yet efficient surrogate model
that captures the effects of these complex materials using only motion
observations. The cloth is discretized into a mass-spring network with unknown
material parameters that are learned directly from the motion data, using a
novel force-and-impulse loss function. Our approach demonstrates the ability to
accurately model spatially varying material properties from a variety of data
sources, and immunity to membrane locking which plagues FEM-based simulations.
Compared to graph-based networks and neural ODE-based architectures, our method
achieves significantly faster training times, higher reconstruction accuracy,
and improved generalization to novel dynamic scenarios.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [307] [Fluidically Innervated Lattices Make Versatile and Durable Tactile Sensors](https://arxiv.org/abs/2507.21225)
*Annan Zhang,Miguel Flores-Acton,Andy Yu,Anshul Gupta,Maggie Yao,Daniela Rus*

Main category: cs.RO

TL;DR: 本文介绍一种集成触觉传感的被动式软机器人指尖，其采用流体神经支配实现触觉传感，具有简单、可扩展等优点，经多种测试验证其性能良好。


<details>
  <summary>Details</summary>
Motivation: 使机器人在动态和非结构化环境中更好地导航，满足精细物体操作、表面探索和人机交互等应用需求。

Method: 用3D打印弹性体晶格嵌入空气通道制造指尖，通过检测密封空气通道内压力变化实现触觉传感，表征传感器响应，开发几何模型，训练神经网络，集成导纳控制器。

Result: 能准确预测接触位置和力，可通过触觉反馈进行环境探索，在高冲击和循环加载条件下耐用。

Conclusion: 该触觉传感技术在简单性、适应性和耐用性方面有优势，为多功能机器人操作带来新机遇。

Abstract: Tactile sensing plays a fundamental role in enabling robots to navigate
dynamic and unstructured environments, particularly in applications such as
delicate object manipulation, surface exploration, and human-robot interaction.
In this paper, we introduce a passive soft robotic fingertip with integrated
tactile sensing, fabricated using a 3D-printed elastomer lattice with embedded
air channels. This sensorization approach, termed fluidic innervation,
transforms the lattice into a tactile sensor by detecting pressure changes
within sealed air channels, providing a simple yet robust solution to tactile
sensing in robotics. Unlike conventional methods that rely on complex materials
or designs, fluidic innervation offers a simple, scalable, single-material
fabrication process. We characterize the sensors' response, develop a geometric
model to estimate tip displacement, and train a neural network to accurately
predict contact location and contact force. Additionally, we integrate the
fingertip with an admittance controller to emulate spring-like behavior,
demonstrate its capability for environment exploration through tactile
feedback, and validate its durability under high impact and cyclic loading
conditions. This tactile sensing technique offers advantages in terms of
simplicity, adaptability, and durability and opens up new opportunities for
versatile robotic manipulation.

</details>


### [308] [Diffusion Denoiser-Aided Gyrocompassing](https://arxiv.org/abs/2507.21245)
*Gershy Ben-Arie,Daniel Engelsman,Rotem Dror,Itzik Klein*

Main category: cs.RO

TL;DR: 提出扩散去噪器辅助的陀螺罗盘方法提升低成本陀螺仪陀螺罗经精度，实验证明有显著提升。


<details>
  <summary>Details</summary>
Motivation: 低成本陀螺仪在无外部导航辅助场景下进行准确及时的陀螺罗经是挑战，如自动驾驶受传感器质量和噪声限制，需解决该问题。

Method: 提出扩散去噪器辅助的陀螺罗盘方法，将基于扩散的去噪框架与增强的基于学习的航向估计模型集成，去噪器先处理原始惯性传感器信号再输入深度学习模型。

Result: 使用模拟和真实传感器数据的实验表明，该方法比基于模型的陀螺罗经精度提高26%，比其他学习驱动方法提高15%。

Conclusion: 该方法对含低成本陀螺仪的自主平台准确和稳健导航有重要意义。

Abstract: An accurate initial heading angle is essential for efficient and safe
navigation across diverse domains. Unlike magnetometers, gyroscopes can provide
accurate heading reference independent of the magnetic disturbances in a
process known as gyrocompassing. Yet, accurate and timely gyrocompassing, using
low-cost gyroscopes, remains a significant challenge in scenarios where
external navigation aids are unavailable. Such challenges are commonly
addressed in real-world applications such as autonomous vehicles, where size,
weight, and power limitations restrict sensor quality, and noisy measurements
severely degrade gyrocompassing performance. To cope with this challenge, we
propose a novel diffusion denoiser-aided gyrocompass approach. It integrates a
diffusion-based denoising framework with an enhanced learning-based heading
estimation model. The diffusion denoiser processes raw inertial sensor signals
before input to the deep learning model, resulting in accurate gyrocompassing.
Experiments using both simulated and real sensor data demonstrate that our
proposed approach improves gyrocompassing accuracy by 26% compared to
model-based gyrocompassing and by 15% compared to other learning-driven
approaches. This advancement holds particular significance for ensuring
accurate and robust navigation in autonomous platforms that incorporate
low-cost gyroscopes within their navigation systems.

</details>


### [309] [Multifunctional physical reservoir computing in soft tensegrity robots](https://arxiv.org/abs/2507.21496)
*Ryo Terajima,Katsuma Inoue,Kohei Nakajima,Yasuo Kuniyoshi*

Main category: cs.RO

TL;DR: 本文研究利用物理储层计算框架，在仿真中让张力结构软机器人控制和嵌入多种行为，发现系统存在“未训练吸引子”，并阐述其对具身认知研究的潜力。


<details>
  <summary>Details</summary>
Motivation: 扩展物理储层计算方法，使软机器人能控制和嵌入多种行为，探索其在具身人工智能研究中的影响。

Method: 通过仿真研究，对张力结构软机器人及其与环境构成的系统进行吸引子分析。

Result: 系统是多稳态动力系统，存在“未训练吸引子”，反映机器人内在特性及与环境的交互。

Conclusion: 这些发现有助于理解具身认知中尚未充分探讨的各种特征。

Abstract: Recent studies have demonstrated that the dynamics of physical systems can be
utilized for the desired information processing under the framework of physical
reservoir computing (PRC). Robots with soft bodies are examples of such
physical systems, and their nonlinear body-environment dynamics can be used to
compute and generate the motor signals necessary for the control of their own
behavior. In this simulation study, we extend this approach to control and
embed not only one but also multiple behaviors into a type of soft robot called
a tensegrity robot. The resulting system, consisting of the robot and the
environment, is a multistable dynamical system that converges to different
attractors from varying initial conditions. Furthermore, attractor analysis
reveals that there exist "untrained attractors" in the state space of the
system outside the training data. These untrained attractors reflect the
intrinsic properties and structures of the tensegrity robot and its
interactions with the environment. The impacts of these recent findings in PRC
remain unexplored in embodied AI research. We here illustrate their potential
to understand various features of embodied cognition that have not been fully
addressed to date.

</details>


### [310] [Decision Transformer-Based Drone Trajectory Planning with Dynamic Safety-Efficiency Trade-Offs](https://arxiv.org/abs/2507.21506)
*Chang-Hun Ji,SiWoon Song,Youn-Hee Han,SungTae Moon*

Main category: cs.RO

TL;DR: 提出基于Decision Transformer的无人机轨迹规划器，用RTG参数动态调整安全与效率权衡，经模拟和真实实验验证有效且优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统多项式规划器调参需专业知识且难达期望权衡，强化学习规划器未明确处理安全与效率权衡。

Method: 引入基于Decision Transformer的轨迹规划器，利用RTG参数作为温度参数动态调整权衡。

Result: 模拟实验表明可通过调整RTG参数动态调整权衡，且在各设置下优于基线方法；真实实验证实其可靠性和实用性。

Conclusion: 所提规划器能有效动态调整安全与效率权衡，具有实际应用价值。

Abstract: A drone trajectory planner should be able to dynamically adjust the
safety-efficiency trade-off according to varying mission requirements in
unknown environments. Although traditional polynomial-based planners offer
computational efficiency and smooth trajectory generation, they require expert
knowledge to tune multiple parameters to adjust this trade-off. Moreover, even
with careful tuning, the resulting adjustment may fail to achieve the desired
trade-off. Similarly, although reinforcement learning-based planners are
adaptable in unknown environments, they do not explicitly address the
safety-efficiency trade-off. To overcome this limitation, we introduce a
Decision Transformer-based trajectory planner that leverages a single
parameter, Return-to-Go (RTG), as a \emph{temperature parameter} to dynamically
adjust the safety-efficiency trade-off. In our framework, since RTG intuitively
measures the safety and efficiency of a trajectory, RTG tuning does not require
expert knowledge. We validate our approach using Gazebo simulations in both
structured grid and unstructured random environments. The experimental results
demonstrate that our planner can dynamically adjust the safety-efficiency
trade-off by simply tuning the RTG parameter. Furthermore, our planner
outperforms existing baseline methods across various RTG settings, generating
safer trajectories when tuned for safety and more efficient trajectories when
tuned for efficiency. Real-world experiments further confirm the reliability
and practicality of our proposed planner.

</details>


### [311] [Model Predictive Adversarial Imitation Learning for Planning from Observation](https://arxiv.org/abs/2507.21533)
*Tyler Han,Yanda Bao,Bhaumik Mehta,Gabriel Guo,Anubhav Vishwakarma,Emily Kang,Sanghun Jung,Rosario Scalise,Jason Zhou,Bryan Xu,Byron Boots*

Main category: cs.RO

TL;DR: 本文提出将基于规划的智能体替代逆强化学习（IRL）中的策略，实现从仅观察演示中进行端到端交互式学习规划器，在多方面有显著改进。


<details>
  <summary>Details</summary>
Motivation: 人类演示数据模糊且不完整，促使开发具有可靠规划行为的模仿学习方法。

Method: 用基于规划的智能体替代 IRL 中的策略，与对抗性模仿学习建立联系实现端到端交互式学习。

Result: 在可解释性、复杂性和安全性上有好处，且在样本效率、分布外泛化和鲁棒性方面有显著改善，在模拟控制基准和现实导航实验中得到验证。

Conclusion: 所提出的方法能有效从仅观察演示中学习规划器，有诸多优势。

Abstract: Human demonstration data is often ambiguous and incomplete, motivating
imitation learning approaches that also exhibit reliable planning behavior. A
common paradigm to perform planning-from-demonstration involves learning a
reward function via Inverse Reinforcement Learning (IRL) then deploying this
reward via Model Predictive Control (MPC). Towards unifying these methods, we
derive a replacement of the policy in IRL with a planning-based agent. With
connections to Adversarial Imitation Learning, this formulation enables
end-to-end interactive learning of planners from observation-only
demonstrations. In addition to benefits in interpretability, complexity, and
safety, we study and observe significant improvements on sample efficiency,
out-of-distribution generalization, and robustness. The study includes
evaluations in both simulated control benchmarks and real-world navigation
experiments using few-to-single observation-only demonstrations.

</details>


### [312] [MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects](https://arxiv.org/abs/2507.21796)
*Yuying Zhang,Kevin Sebastian Luck,Francesco Verdoja,Ville Kyrki,Joni Pajarinen*

Main category: cs.RO

TL;DR: 提出首个用于机器人学习的移动操作可变形对象任务套件MoDeSuite，评估多种学习算法并展示其在模拟和现实中的表现，有望开启新研究领域。


<details>
  <summary>Details</summary>
Motivation: 现有机器人学习算法处理可变形对象操作有挑战，缺乏相关标准化基准。

Method: 引入MoDeSuite，包含八个不同移动操作任务，训练两种强化学习算法和两种模仿学习算法评估该基准。

Result: 展示了算法在模拟中的表现，将训练策略部署到现实中证明其实际相关性。

Conclusion: MoDeSuite有望开启移动操作可变形对象的新研究领域。

Abstract: Mobile manipulation is a critical capability for robots operating in diverse,
real-world environments. However, manipulating deformable objects and materials
remains a major challenge for existing robot learning algorithms. While various
benchmarks have been proposed to evaluate manipulation strategies with rigid
objects, there is still a notable lack of standardized benchmarks that address
mobile manipulation tasks involving deformable objects.
  To address this gap, we introduce MoDeSuite, the first Mobile Manipulation
Deformable Object task suite, designed specifically for robot learning.
MoDeSuite consists of eight distinct mobile manipulation tasks covering both
elastic objects and deformable objects, each presenting a unique challenge
inspired by real-world robot applications. Success in these tasks requires
effective collaboration between the robot's base and manipulator, as well as
the ability to exploit the deformability of the objects. To evaluate and
demonstrate the use of the proposed benchmark, we train two state-of-the-art
reinforcement learning algorithms and two imitation learning algorithms,
highlighting the difficulties encountered and showing their performance in
simulation. Furthermore, we demonstrate the practical relevance of the suite by
deploying the trained policies directly into the real world with the Spot
robot, showcasing the potential for sim-to-real transfer. We expect that
MoDeSuite will open a novel research domain in mobile manipulation involving
deformable objects. Find more details, code, and videos at
https://sites.google.com/view/modesuite/home.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [313] [A Multi-Agent Generative AI Framework for IC Module-Level Verification Automation](https://arxiv.org/abs/2507.21694)
*Wenbo Liu,Forbes Hou,Jon Zhang,Hong Liu,Allen Lei*

Main category: cs.AR

TL;DR: 本文提出多智能体验证框架MAVF用于芯片验证，实验表明其优于传统方法和单对话生成式AI方法，为验证自动化开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI技术在芯片验证这一关键瓶颈环节应用尚处探索阶段，单大语言模型方法在复杂验证任务存在局限。

Method: 构建多智能体验证框架MAVF，通过多个专业智能体协作，建立从设计规格到测试平台的自动转换系统。

Result: 在多个不同复杂度芯片模块的验证实验中，MAVF在验证文档解析与生成以及自动测试平台生成方面显著优于传统手动方法和单对话生成式AI方法。

Conclusion: 本研究为生成式AI在验证自动化中的应用开辟了新方向，有望为解决芯片设计中最具挑战性的瓶颈问题提供有效途径。

Abstract: As large language models demonstrate enormous potential in the field of
Electronic Design Automation (EDA), generative AI-assisted chip design is
attracting widespread attention from academia and industry. Although these
technologies have made preliminary progress in tasks such as code generation,
their application in chip verification -- a critical bottleneck in the chip
development cycle -- remains at an exploratory stage. This paper proposes an
innovative Multi-Agent Verification Framework (MAVF) aimed at addressing the
limitations of current single-LLM approaches in complex verification tasks. Our
framework builds an automated transformation system from design specifications
to testbench through the collaborative work of multiple specialized agents,
including specification parsing, verification strategy generation, and code
implementation. Through verification experiments on multiple chip modules of
varying complexity, results show that MAVF significantly outperforms
traditional manual methods and single-dialogue generative AI approaches in
verification document parsing and generation, as well as automated testbench
generation. This research opens new directions for exploring generative AI
applications in verification automation, potentially providing effective
approaches to solving the most challenging bottleneck issues in chip design.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [314] [High hopes for "Deep Medicine"? AI, economics, and the future of care](https://arxiv.org/abs/2507.21054)
*Robert Sparrow,Joshua Hatherley*

Main category: cs.CY

TL;DR: Eric Topol认为医疗AI将使医疗发生转变，很多日常任务可交给AI，让医生有更多时间关怀患者，但实际医疗AI可能侵蚀医患关系并威胁满意度。


<details>
  <summary>Details</summary>
Motivation: 纠正人们对医疗AI在改善医患关系方面过于乐观的看法，展现医疗AI应用可能带来的负面情况。

Method: 先引出Eric Topol对医疗AI的乐观观点，然后提出相反看法进行分析。

Result: 指出医疗AI使用可能进一步侵蚀医患治疗关系，威胁专业人员和患者满意度。

Conclusion: 医疗AI未来发展可能无法促进医患关系更紧密，反而带来负面影响。

Abstract: In the much-celebrated book Deep Medicine, Eric Topol argues that the
development of artificial intelligence for health care will lead to a dramatic
shift in the culture and practice of medicine. In the next several decades, he
suggests, AI will become sophisticated enough that many of the everyday tasks
of physicians could be delegated to it. Topol is perhaps the most articulate
advocate of the benefits of AI in medicine, but he is hardly alone in spruiking
its potential to allow physicians to dedicate more of their time and attention
to providing empathetic care for their patients in the future. Unfortunately,
several factors suggest a radically different picture for the future of health
care. Far from facilitating a return to a time of closer doctor-patient
relationships, the use of medical AI seems likely to further erode therapeutic
relationships and threaten professional and patient satisfaction.

</details>


### [315] [Failure Risk Prediction in a MOOC: A Multivariate Time Series Analysis Approach](https://arxiv.org/abs/2507.21118)
*Anass El Ayady,Maxime Devanne,Germain Forestier,Nour El Mawas*

Main category: cs.CY

TL;DR: 论文探讨MOOCs低完成率问题，对比多元时间序列分类方法预测学习者表现，实验显示方法有前景且预测精度受交互记录量影响。


<details>
  <summary>Details</summary>
Motivation: MOOCs完成率低，缺乏个性化内容，需预测学习者表现以提供定制反馈。

Method: 对比多元时间序列分类方法，在OULAD数据集的三门课程上进行实验评估。

Result: 评估的方法对预测MOOCs学习者失败情况有前景，预测准确性受记录的交互量影响。

Conclusion: 多元时间序列分类方法在预测MOOCs学习者表现上有潜力，丰富多样的行为数据很重要。

Abstract: MOOCs offer free and open access to a wide audience, but completion rates
remain low, often due to a lack of personalized content. To address this issue,
it is essential to predict learner performance in order to provide tailored
feedback. Behavioral traces-such as clicks and events-can be analyzed as time
series to anticipate learners' outcomes. This work compares multivariate time
series classification methods to identify at-risk learners at different stages
of the course (after 5, 10 weeks, etc.). The experimental evaluation, conducted
on the Open University Learning Analytics Dataset (OULAD), focuses on three
courses: two in STEM and one in SHS. Preliminary results show that the
evaluated approaches are promising for predicting learner failure in MOOCs. The
analysis also suggests that prediction accuracy is influenced by the amount of
recorded interactions, highlighting the importance of rich and diverse
behavioral data.

</details>


### [316] [Bridging the Gap: Enhancing News Interpretation Across Diverse Audiences with Large Language Models](https://arxiv.org/abs/2507.21055)
*Leyi Ouyang*

Main category: cs.CY

TL;DR: 提出基于大语言模型的框架模拟社会交流行为，能识别新闻理解差距并提供补充材料，提升受众理解。


<details>
  <summary>Details</summary>
Motivation: 不同受众对新闻解读差异大，需识别理解差距并提升受众对新闻内容的理解。

Method: 提出基于大语言模型的代理框架，模拟社会交流行为，设计不同职业、年龄的代理讨论新闻。

Result: 框架能通过迭代讨论识别代理对新闻的困惑和误解，设计补充材料，代理接收材料后新闻理解显著提升。

Conclusion: 框架能直接解决理解差距，对提升不同受众的新闻理解有实用和高效性。

Abstract: In the interconnected world, news media are critical in conveying information
to public across diverse domains including technology, finance, and
agriculture. Journalists make efforts to present accurate information, however,
the interpretation of news often varies significantly among different audiences
due to their specific expertise and age. In this work, we investigate how to
identify these comprehension gaps and provide solutions to improve audiences
understanding of news content, particular to the aspects of articles outside
their primary domains of knowledge. We propose a agent-based framework using
large language models (LLMs) to simulate society communication behaviors, where
several agents can discuss news. These agents can be designed to be experts
from various occupation, or from different age group. Our results indicate that
this framework can identify confusions or even misunderstanding of news for the
agent through the iterative discussion process. Based on these accurate
identification, the framework can design a supplement material specific to
these agents on the news. Our results show that agents exhibit significantly
improved news understanding after receiving this material. These findings
highlight our framework's utility and efficiency in enhancing news
comprehension for diverse audiences by directly addressing their understanding
gap.

</details>


### [317] [The Value of Gen-AI Conversations: A bottom-up Framework for AI Value Alignment](https://arxiv.org/abs/2507.21091)
*Lenart Motnikar,Katharina Baum,Alexander Kagan,Sarah Spiekermann-Hoff*

Main category: cs.CY

TL;DR: 论文提出自下而上方法解决对话代理价值对齐问题，分析日志找出核心价值与价值不对齐情况，为CA提供商提供见解。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能对话代理在确保符合人类价值观的伦理交互方面存在挑战，现有自上而下方法与实际场景脱节。

Method: 采用基于ISO价值工程标准的自下而上方法，分析欧洲就业服务对话代理的16908条对话日志中的593个伦理敏感输出。

Result: 发现9个核心价值和32种不同的价值不对齐情况，这些不对齐对用户产生负面影响。

Conclusion: 研究结果为对话代理提供商解决伦理挑战、实现更具上下文敏感性的价值对齐提供了可操作的见解。

Abstract: Conversational agents (CAs) based on generative artificial intelligence
frequently face challenges ensuring ethical interactions that align with human
values. Current value alignment efforts largely rely on top-down approaches,
such as technical guidelines or legal value principles. However, these methods
tend to be disconnected from the specific contexts in which CAs operate,
potentially leading to misalignment with users interests. To address this
challenge, we propose a novel, bottom-up approach to value alignment, utilizing
the value ontology of the ISO Value-Based Engineering standard for ethical IT
design. We analyse 593 ethically sensitive system outputs identified from
16,908 conversational logs of a major European employment service CA to
identify core values and instances of value misalignment within real-world
interactions. The results revealed nine core values and 32 different value
misalignments that negatively impacted users. Our findings provide actionable
insights for CA providers seeking to address ethical challenges and achieve
more context-sensitive value alignment.

</details>


### [318] [A Tactical Behaviour Recognition Framework Based on Causal Multimodal Reasoning: A Study on Covert Audio-Video Analysis Combining GAN Structure Enhancement and Phonetic Accent Modelling](https://arxiv.org/abs/2507.21100)
*Wei Meng*

Main category: cs.CY

TL;DR: 本文介绍TACTIC - GRAPHS系统，结合谱图理论和多模态图神经推理用于战术视频语义理解和威胁检测，实验取得较好结果并支持多领域应用。


<details>
  <summary>Details</summary>
Motivation: 解决高噪声和弱结构下战术视频的语义理解和威胁检测问题。

Method: 系统结合谱图嵌入、时间因果边建模和跨异构模态的判别路径推理，使用语义感知关键帧提取方法构建时间图，利用图注意力和拉普拉斯谱映射进行跨模态加权和因果信号分析。

Result: 在TACTIC - AVS和TACTIC - Voice数据集上，时间对齐准确率达89.3%，完整威胁链识别率超85%，节点延迟在正负150毫秒内。

Conclusion: 该方法增强了结构可解释性，支持监控、国防和智能安全系统等应用。

Abstract: This paper introduces TACTIC-GRAPHS, a system that combines spectral graph
theory and multimodal graph neural reasoning for semantic understanding and
threat detection in tactical video under high noise and weak structure. The
framework incorporates spectral embedding, temporal causal edge modeling, and
discriminative path inference across heterogeneous modalities. A semantic-aware
keyframe extraction method fuses visual, acoustic, and action cues to construct
temporal graphs. Using graph attention and Laplacian spectral mapping, the
model performs cross-modal weighting and causal signal analysis. Experiments on
TACTIC-AVS and TACTIC-Voice datasets show 89.3 percent accuracy in temporal
alignment and over 85 percent recognition of complete threat chains, with node
latency within plus-minus 150 milliseconds. The approach enhances structural
interpretability and supports applications in surveillance, defense, and
intelligent security systems.

</details>


### [319] [Assessing the Ecological Impact of AI](https://arxiv.org/abs/2507.21102)
*Sylvia Wenmackers*

Main category: cs.CY

TL;DR: 哲学技术学者关注AI环境影响，开发者评估有限，提议开展基于哲学理念的生成式AI可持续性分析。


<details>
  <summary>Details</summary>
Motivation: 当前开发者对AI生态影响估算有限，且分析常局限于特定阶段温室气体排放，需更全面分析。

Method: 未提及

Result: 未提及

Conclusion: 鼓励基于哲学理念对生成式AI的可持续性进行可行分析。

Abstract: Philosophers of technology have recently started paying more attention to the
environmental impacts of AI, in particular of large language models (LLMs) and
generative AI (genAI) applications. Meanwhile, few developers of AI give
concrete estimates of the ecological impact of their models and products, and
even when they do so, their analysis is often limited to green house gas
emissions of certain stages of AI development or use. The current proposal
encourages practically viable analyses of the sustainability aspects of genAI
informed by philosophical ideas.

</details>


### [320] [Trustworthy AI: UK Air Traffic Control Revisited](https://arxiv.org/abs/2507.21169)
*Rob Procter,Mark Rouncefield*

Main category: cs.CY

TL;DR: 文章介绍正在进行的民族志研究成果，关于空管工作中当前工具使用及可信AI要求。


<details>
  <summary>Details</summary>
Motivation: 相关文献缺少对组织环境中采用AI面临的社会技术挑战的探索，且对可信AI要求的研究常忽略人们在日常工作中对工具信任问题的处理。

Method: 进行民族志研究，研究空管工作中当前工具的使用情况。

Result: 文中未明确提及具体研究结果。

Conclusion: 文中未明确提及研究得出的结论。

Abstract: Exploring the socio-technical challenges confronting the adoption of AI in
organisational settings is something that has so far been largely absent from
the related literature. In particular, research into requirements for
trustworthy AI typically overlooks how people deal with the problems of trust
in the tools that they use as part of their everyday work practices. This
article presents some findings from an ongoing ethnographic study of how
current tools are used in air traffic control work and what it reveals about
requirements for trustworthy AI in air traffic control and other
safety-critical application domains.

</details>


### [321] [A ChatGPT-based approach for questions generation in higher education](https://arxiv.org/abs/2507.21174)
*Sinh Trong Vu,Huong Thu Truong,Oanh Tien Do,Tu Anh Le,Tai Tan Mai*

Main category: cs.CY

TL;DR: 探讨ChatGPT在高等教育中辅助生成测验问题和评估学习者的应用，越南银行学院初步结果有潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型广泛应用，探索ChatGPT辅助高等教育工作者生成测验问题和评估学习者。

Method: 探索交互式提示模式设计AI题库创建流程，通过“盲测”调查评估生成的问题。

Result: 越南银行学院的初步结果比较有前景。

Conclusion: 为简化高等教育机构评估学习者的时间和精力指出了潜在方向。

Abstract: Large language models have been widely applied in many aspects of real life,
bringing significant efficiency to businesses and offering distinctive user
experiences. In this paper, we focus on exploring the application of ChatGPT, a
chatbot based on a large language model, to support higher educator in
generating quiz questions and assessing learners. Specifically, we explore
interactive prompting patterns to design an optimal AI-powered question bank
creation process. The generated questions are evaluated through a "Blind test"
survey sent to various stakeholders including lecturers and learners. Initial
results at the Banking Academy of Vietnam are relatively promising, suggesting
a potential direction to streamline the time and effort involved in assessing
learners at higher education institutes.

</details>


### [322] [Against racing to AGI: Cooperation, deterrence, and catastrophic risks](https://arxiv.org/abs/2507.21839)
*Leonard Dung,Max Hellrigel-Holderbaum*

Main category: cs.CY

TL;DR: 文章反对AGI竞赛，指出其弊端大、预期收益存疑，国际合作是更好选择。


<details>
  <summary>Details</summary>
Motivation: 探讨AGI竞赛是否符合参与者自身利益，反对该竞赛观点。

Method: 分析AGI竞赛的弊端、预期收益，提出国际合作等替代方案。

Result: 发现AGI竞赛会大幅增加灾难性风险，预期收益存疑，国际合作风险小且能提供类似收益。

Conclusion: AGI竞赛不符合任何人的自身利益，应推动国际合作。

Abstract: AGI Racing is the view that it is in the self-interest of major actors in AI
development, especially powerful nations, to accelerate their frontier AI
development to build highly capable AI, especially artificial general
intelligence (AGI), before competitors have a chance. We argue against AGI
Racing. First, the downsides of racing to AGI are much higher than portrayed by
this view. Racing to AGI would substantially increase catastrophic risks from
AI, including nuclear instability, and undermine the prospects of technical AI
safety research to be effective. Second, the expected benefits of racing may be
lower than proponents of AGI Racing hold. In particular, it is questionable
whether winning the race enables complete domination over losers. Third,
international cooperation and coordination, and perhaps carefully crafted
deterrence measures, constitute viable alternatives to racing to AGI which have
much smaller risks and promise to deliver most of the benefits that racing to
AGI is supposed to provide. Hence, racing to AGI is not in anyone's
self-interest as other actions, particularly incentivizing and seeking
international cooperation around AI issues, are preferable.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [323] [Gender Similarities Dominate Mathematical Cognition at the Neural Level: A Japanese fMRI Study Using Advanced Wavelet Analysis and Generative AI](https://arxiv.org/abs/2507.21140)
*Tatsuru Kikuchi*

Main category: q-bio.NC

TL;DR: 研究用fMRI和小波时频分析日本参与者，发现性别在数学认知神经机制上相似，个体差异超群体差异，挑战性别固有差异说法。


<details>
  <summary>Details</summary>
Motivation: 先前大规模行为研究关于入学数月内数学表现性别差异的发现缺乏直接神经证据且受文化背景限制，需进一步研究。

Method: 对日本156名参与者在数学任务时进行fMRI，采用先进小波时频分析动态脑过程。

Result: 数学认知神经处理机制性别基本相似，动态激活模式相似度89.1%，个体神经动力学差异是群体差异3.2倍，机器学习分类器区分性别准确率仅53.8%，跨频耦合分析显示网络协调模式相似。

Conclusion: 在数学认知中性别相似性占主导，尤其在早期发展阶段，动态脑分析能揭示静态行为评估无法触及的神经机制。

Abstract: Recent large scale behavioral studies suggest early emergence of gender
differences in mathematical performance within months of school entry. However,
these findings lack direct neural evidence and are constrained by cultural
contexts. We conducted functional magnetic resonance imaging (fMRI) during
mathematical tasks in Japanese participants (N = 156), employing an advanced
wavelet time frequency analysis to examine dynamic brain processes rather than
static activation patterns. Wavelet decomposition across four frequency bands
(0.01-0.25 Hz) revealed that neural processing mechanisms underlying
mathematical cognition are fundamentally similar between genders. Time
frequency analysis demonstrated 89.1% similarity in dynamic activation patterns
(p = 0.734, d = 0.05), with identical temporal sequences and frequency profiles
during mathematical processing. Individual variation in neural dynamics
exceeded group differences by 3.2:1 (p $<$ 0.001). Machine learning classifiers
achieved only 53.8% accuracy in distinguishing gender based neural patterns
essentially at chance level even when analyzing sophisticated temporal spectral
features. Cross frequency coupling analysis revealed similar network
coordination patterns between genders, indicating shared fundamental cognitive
architecture. These findings provide robust process level neural evidence that
gender similarities dominate mathematical cognition, particularly in early
developmental stages, challenging recent claims of inherent differences and
demonstrating that dynamic brain analysis reveals neural mechanisms that static
behavioral assessments cannot access.

</details>


### [324] [Representations in vision and language converge in a shared, multidimensional space of perceived similarities](https://arxiv.org/abs/2507.21871)
*Katerina Marie Simkova,Adrien Doerig,Clayton Hickey,Ian Charest*

Main category: q-bio.NC

TL;DR: 研究人类视觉和语言相似性判断，发现二者基于共享表征结构，且与人工系统有共同概念形成能力。


<details>
  <summary>Details</summary>
Motivation: 建立视觉和语言的共享表征格式是挑战，虽有证据但不清楚其在人类行为中的表现。

Method: 63名参与者对100张自然场景图像和对应句子进行相似性判断。

Result: 视觉和语言相似性判断在行为层面收敛，能预测相似的fMRI脑响应网络，图像到LLM嵌入的计算模型表现更优。

Conclusion: 人类视觉和语言相似性判断基于共享表征结构，反映外部世界稳定关系属性，与人工系统有共同概念形成能力。

Abstract: Humans can effortlessly describe what they see, yet establishing a shared
representational format between vision and language remains a significant
challenge. Emerging evidence suggests that human brain representations in both
vision and language are well predicted by semantic feature spaces obtained from
large language models (LLMs). This raises the possibility that sensory systems
converge in their inherent ability to transform their inputs onto shared,
embedding-like representational space. However, it remains unclear how such a
space manifests in human behaviour. To investigate this, sixty-three
participants performed behavioural similarity judgements separately on 100
natural scene images and 100 corresponding sentence captions from the Natural
Scenes Dataset. We found that visual and linguistic similarity judgements not
only converge at the behavioural level but also predict a remarkably similar
network of fMRI brain responses evoked by viewing the natural scene images.
Furthermore, computational models trained to map images onto LLM-embeddings
outperformed both category-trained and AlexNet controls in explaining the
behavioural similarity structure. These findings demonstrate that human visual
and linguistic similarity judgements are grounded in a shared,
modality-agnostic representational structure that mirrors how the visual system
encodes experience. The convergence between sensory and artificial systems
suggests a common capacity of how conceptual representations are formed-not as
arbitrary products of first order, modality-specific input, but as structured
representations that reflect the stable, relational properties of the external
world.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [325] [Preconditioned Discrete-HAMS: A Second-order Irreversible Discrete Sampler](https://arxiv.org/abs/2507.21982)
*Yuze Zhou,Zhiqiang Tan*

Main category: stat.ME

TL;DR: 提出PDHAMS算法，在数值实验中表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 为离散分布采样，改进现有基于梯度的马尔可夫链蒙特卡罗方法。

Method: 在DHAMS基础上，引入势函数的二阶二次近似，用高斯积分技巧避免直接采样成对马尔可夫随机场。

Result: PDHAMS满足广义细致平衡，对二次势函数目标分布无拒绝采样，数值实验中表现更好。

Conclusion: PDHAMS算法在离散分布采样方面性能优越。

Abstract: Gradient-based Markov Chain Monte Carlo methods have recently received much
attention for sampling discrete distributions, with notable examples such as
Norm Constrained Gradient (NCG), Auxiliary Variable Gradient (AVG), and
Discrete Hamiltonian Assisted Metropolis Sampling (DHAMS). In this work, we
propose the Preconditioned Discrete-HAMS (PDHAMS) algorithm, which extends
DHAMS by incorporating a second-order, quadratic approximation of the potential
function, and uses Gaussian integral trick to avoid directly sampling a
pairwise Markov random field. The PDHAMS sampler not only satisfies generalized
detailed balance, hence enabling irreversible sampling, but also is a
rejection-free property for a target distribution with a quadratic potential
function. In various numerical experiments, PDHAMS algorithms consistently
yield superior performance compared with other methods.

</details>


### [326] [Horseshoe Forests for High-Dimensional Causal Survival Analysis](https://arxiv.org/abs/2507.22004)
*Tijn Jacobs,Wessel N. van Wieringen,Stéphanie L. van der Pas*

Main category: stat.ME

TL;DR: 开发贝叶斯树集成模型估计删失生存数据中异质治疗效果，用马蹄形先验正则化，通过可逆跳吉布斯采样器实现，模拟和实际数据验证有效。


<details>
  <summary>Details</summary>
Motivation: 在高维协变量的删失生存数据中准确估计异质治疗效果。

Method: 开发贝叶斯树集成模型，直接在步高上设置马蹄形先验，采用可逆跳吉布斯采样器。

Result: 模拟显示方法能在高维协变量空间、不同稀疏水平和非线性治疗效果函数下准确估计治疗效果，实际数据再分析展示了实用性。

Conclusion: 所提出的方法可有效估计高维删失生存数据中的异质治疗效果。

Abstract: We develop a Bayesian tree ensemble model to estimate heterogeneous treatment
effects in censored survival data with high-dimensional covariates. Instead of
imposing sparsity through the tree structure, we place a horseshoe prior
directly on the step heights to achieve adaptive global-local shrinkage. This
strategy allows flexible regularisation and reduces noise. We develop a
reversible jump Gibbs sampler to accommodate the non-conjugate horseshoe prior
within the tree ensemble framework. We show through extensive simulations that
the method accurately estimates treatment effects in high-dimensional covariate
spaces, at various sparsity levels, and under non-linear treatment effect
functions. We further illustrate the practical utility of the proposed approach
by a re-analysis of pancreatic ductal adenocarcinoma (PDAC) survival data from
The Cancer Genome Atlas.

</details>


### [327] [An empirical comparison of some outlier detection methods with longitudinal data](https://arxiv.org/abs/2507.21203)
*Marcello D'Orazio*

Main category: stat.ME

TL;DR: 研究纵向数据离群值检测问题，比较传统与新方法，新方法更灵活有效。


<details>
  <summary>Details</summary>
Motivation: 解决纵向数据中离群值检测问题。

Method: 将官方统计中的传统方法与数据挖掘和机器学习领域基于观测距离或二叉划分树的新方法应用于面板调查数据。

Result: 传统方法简单可直接识别潜在离群值但需特定假设，新方法给出与离群可能性相关的分数，所有方法都需设置调参，新方法更灵活有时更有效且适用于多维数据。

Conclusion: 最近的方法在纵向数据离群值检测上比传统方法更具优势。

Abstract: This note investigates the problem of detecting outliers in longitudinal
data. It compares well-known methods used in official statistics with proposals
from the fields of data mining and machine learning that are based on the
distance between observations or binary partitioning trees. This is achieved by
applying the methods to panel survey data related to different types of
statistical units. Traditional methods are quite simple, enabling the direct
identification of potential outliers, but they require specific assumptions. In
contrast, recent methods provide only a score whose magnitude is directly
related to the likelihood of an outlier being present. All the methods require
the user to set a number of tuning parameters. However, the most recent methods
are more flexible and sometimes more effective than traditional methods. In
addition, these methods can be applied to multidimensional data.

</details>


<div id='math.AT'></div>

# math.AT [[Back]](#toc)

### [328] [Exploring the Stratified Space Structure of an RL Game with the Volume Growth Transform](https://arxiv.org/abs/2507.22010)
*Justin Curry,Brennan Lagasse,Ngoc B. Lam,Gregory Cox,David Rosenbluth,Alberto Speranzon*

Main category: math.AT

TL;DR: 探索基于Transformer的强化学习模型嵌入空间结构，发现其更适合用分层空间建模，提出分层潜在空间维度分布可作强化学习游戏复杂度几何指标。


<details>
  <summary>Details</summary>
Motivation: 研究基于Transformer的近端策略优化（PPO）模型在简单视觉强化学习游戏中如何嵌入视觉输入，分析其嵌入空间结构。

Method: 将Robinson等人对大语言模型的体积增长变换研究应用到强化学习场景，加强Robinson的方法，证明分层空间可实现较通用的体积增长曲线，并进行相关分析。

Result: 游戏的令牌嵌入空间不是流形，更适合用分层空间建模；RL智能体行动时，其潜在表示在低局部维度和高局部维度之间交替。

Conclusion: 分层潜在空间的维度分布可为强化学习游戏提供新的复杂度几何指标。

Abstract: In this work, we explore the structure of the embedding space of a
transformer model trained for playing a particular reinforcement learning (RL)
game. Specifically, we investigate how a transformer-based Proximal Policy
Optimization (PPO) model embeds visual inputs in a simple environment where an
agent must collect "coins" while avoiding dynamic obstacles consisting of
"spotlights." By adapting Robinson et al.'s study of the volume growth
transform for LLMs to the RL setting, we find that the token embedding space
for our visual coin collecting game is also not a manifold, and is better
modeled as a stratified space, where local dimension can vary from point to
point. We further strengthen Robinson's method by proving that fairly general
volume growth curves can be realized by stratified spaces. Finally, we carry
out an analysis that suggests that as an RL agent acts, its latent
representation alternates between periods of low local dimension, while
following a fixed sub-strategy, and bursts of high local dimension, where the
agent achieves a sub-goal (e.g., collecting an object) or where the
environmental complexity increases (e.g., more obstacles appear). Consequently,
our work suggests that the distribution of dimensions in a stratified latent
space may provide a new geometric indicator of complexity for RL games.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [329] [Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation](https://arxiv.org/abs/2507.21903)
*Tiviatis Sim,Kaiwen Yang,Shen Xin,Kenji Kawaguchi*

Main category: cs.SI

TL;DR: 现有新闻摘要方法有局限，提出SUnSET框架用于时间线摘要任务，实验结果超越先前基线。


<details>
  <summary>Details</summary>
Motivation: 现有新闻摘要方法仅考虑文章文本内容，缺乏对事件相关方的分析，难以跨多源跟踪相关事件。

Method: 提出SUnSET框架，利用大语言模型构建SET三元组，引入基于利益相关者的排名构建相关性指标。

Result: 实验结果超越所有先前基线，成为新的最优方法。

Conclusion: 凸显了新闻文章中利益相关者信息的重要性。

Abstract: As news reporting becomes increasingly global and decentralized online,
tracking related events across multiple sources presents significant
challenges. Existing news summarization methods typically utilizes Large
Language Models and Graphical methods on article-based summaries. However, this
is not effective since it only considers the textual content of similarly dated
articles to understand the gist of the event. To counteract the lack of
analysis on the parties involved, it is essential to come up with a novel
framework to gauge the importance of stakeholders and the connection of related
events through the relevant entities involved. Therefore, we present SUnSET:
Synergistic Understanding of Stakeholder, Events and Time for the task of
Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)
to build SET triplets and introduced the use of stakeholder-based ranking to
construct a $Relevancy$ metric, which can be extended into general situations.
Our experimental results outperform all prior baselines and emerged as the new
State-of-the-Art, highlighting the impact of stakeholder information within
news article.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [330] [Semantic Numeration Systems as Dynamical Systems](https://arxiv.org/abs/2507.21295)
*Alexander Yu. Chunikhin*

Main category: cs.LO

TL;DR: 简述语义计数系统理论基础概念，将基数抽象对象视为带非线性控制的线性离散动力系统，给出状态方程并说明配置矩阵的重要作用。


<details>
  <summary>Details</summary>
Motivation: 研究语义计数系统理论，探索基数抽象对象的特性和描述方法。

Method: 将基数抽象对象视为线性离散动力系统，在理想可观测假设下推导状态方程。

Result: 得到了基数抽象对象在平稳和非平稳情况下的状态方程，明确了配置矩阵的作用。

Conclusion: 配置矩阵在描述基数抽象对象中起关键作用，所提出的动力系统模型可用于研究语义计数系统。

Abstract: The foundational concepts of semantic numeration systems theory are briefly
outlined. The action of cardinal semantic operators unfolds over a set of
cardinal abstract entities belonging to the cardinal semantic multeity. The
cardinal abstract object (CAO) formed by them in a certain connectivity
topology is proposed to be considered as a linear discrete dynamical system
with nonlinear control. Under the assumption of ideal observability, the CAO
state equations are provided for both stationary and non-stationary cases. The
fundamental role of the configuration matrix, which combines information about
the types of cardinal semantic operators in the CAO, their parameters and
topology of connectivity, is demonstrated.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [331] [Recursive KalmanNet: Analyse des capacités de généralisation d'un réseau de neurones récurrent guidé par un filtre de Kalman](https://arxiv.org/abs/2507.14144)
*Cyril Falcon,Hassan Mortada,Mathéo Clavaud,Jean-Philippe Michel*

Main category: eess.SP

TL;DR: 本文探索Recursive KalmanNet在分布外场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究Recursive KalmanNet在测试测量的时间动态与训练时不同的分布外场景中的泛化能力。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: The Recursive KalmanNet, recently introduced by the authors, is a recurrent
neural network guided by a Kalman filter, capable of estimating the state
variables and error covariance of stochastic dynamic systems from noisy
measurements, without prior knowledge of the noise characteristics. This paper
explores its generalization capabilities in out-of-distribution scenarios,
where the temporal dynamics of the test measurements differ from those
encountered during training.
  Le Recursive KalmanNet, r\'ecemment introduit par les auteurs, est un
r\'eseau de neurones r\'ecurrent guid\'e par un filtre de Kalman, capable
d'estimer les variables d'\'etat et la covariance des erreurs des syst\`emes
dynamiques stochastiques \`a partir de mesures bruit\'ees, sans connaissance
pr\'ealable des caract\'eristiques des bruits. Cet article explore ses
capacit\'es de g\'en\'eralisation dans des sc\'enarios hors distribution, o\`u
les dynamiques temporelles des mesures de test diff\`erent de celles
rencontr\'ees \`a l'entra\^inement.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [332] [Can large language models assist choice modelling? Insights into prompting strategies and current models capabilities](https://arxiv.org/abs/2507.21790)
*Georges Sfeir,Gabriel Nova,Stephane Hess,Sander van Cranenburgh*

Main category: econ.EM

TL;DR: 本文研究大语言模型（LLMs）在选择建模中的潜力，通过实验评估多个LLMs，发现其有潜力但也存在局限，并给出实践指导。


<details>
  <summary>Details</summary>
Motivation: LLMs在各领域广泛应用，但在选择建模中的潜力未充分挖掘，因此研究其在多项式Logit模型规范和估计中的潜力。

Method: 实施系统实验框架，对六种领先LLMs的十三个版本在五种实验配置下进行评估，根据拟合度、行为合理性和模型复杂度评估LLM建议的规范。

Result: 专有LLMs能生成有效且行为合理的效用规范，部分开放权重模型表现不佳，不同LLMs有不同优势，部分LLMs在仅提供数据字典时表现更好，GPT o3能正确估计自身规范。

Conclusion: 结果展示了LLMs在选择建模中的潜力和局限，为将其集成到选择建模工作流程提供实践指导。

Abstract: Large Language Models (LLMs) are widely used to support various workflows
across different disciplines, yet their potential in choice modelling remains
relatively unexplored. This work examines the potential of LLMs as assistive
agents in the specification and, where technically feasible, estimation of
Multinomial Logit models. We implement a systematic experimental framework
involving thirteen versions of six leading LLMs (ChatGPT, Claude, DeepSeek,
Gemini, Gemma, and Llama) evaluated under five experimental configurations.
These configurations vary along three dimensions: modelling goal (suggesting
vs. suggesting and estimating MNLs); prompting strategy (Zero-Shot vs.
Chain-of-Thoughts); and information availability (full dataset vs. data
dictionary only). Each LLM-suggested specification is implemented, estimated,
and evaluated based on goodness-of-fit metrics, behavioural plausibility, and
model complexity. Findings reveal that proprietary LLMs can generate valid and
behaviourally sound utility specifications, particularly when guided by
structured prompts. Open-weight models such as Llama and Gemma struggled to
produce meaningful specifications. Claude 4 Sonnet consistently produced the
best-fitting and most complex models, while GPT models suggested models with
robust and stable modelling outcomes. Some LLMs performed better when provided
with just data dictionary, suggesting that limiting raw data access may enhance
internal reasoning capabilities. Among all LLMs, GPT o3 was uniquely capable of
correctly estimating its own specifications by executing self-generated code.
Overall, the results demonstrate both the promise and current limitations of
LLMs as assistive agents in choice modelling, not only for model specification
but also for supporting modelling decision and estimation, and provide
practical guidance for integrating these tools into choice modellers'
workflows.

</details>
