<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 32]
- [cs.CE](#cs.CE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 8]
- [stat.ML](#stat.ML) [Total: 5]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [nucl-th](#nucl-th) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [hep-ex](#hep-ex) [Total: 2]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.CR](#cs.CR) [Total: 9]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [math.AG](#math.AG) [Total: 1]
- [cs.CL](#cs.CL) [Total: 18]
- [eess.IV](#eess.IV) [Total: 4]
- [math.OC](#math.OC) [Total: 2]
- [cs.CV](#cs.CV) [Total: 33]
- [eess.SP](#eess.SP) [Total: 3]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SD](#cs.SD) [Total: 7]
- [eess.AS](#eess.AS) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.NA](#math.NA) [Total: 3]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Renewable Energy Sources Selection Analysis with the Maximizing Deviation Method](https://arxiv.org/abs/2509.07011)
*Kirisci Murat*

Main category: cs.AI

TL;DR: 本文将费马模糊环境与区间值费马模糊集结合，用偏差最大化方法确定特征权重，应用于可再生能源选择问题，并讨论其管理和政治影响。


<details>
  <summary>Details</summary>
Motivation: 多准则决策方法结合模糊逻辑和模糊集理论可处理决策中的不确定性和模糊性，选择可再生能源在满足能源需求、平衡碳排放和缓解气候变化方面很关键，且具有管理和政治意义。

Method: 利用费马模糊环境，提出基于偏差最大化方法的优化模型确定部分已知特征权重，并与区间值费马模糊集结合。

Result: 将所提方法应用于可再生能源选择问题。

Conclusion: 研究解决了可再生能源选择问题，同时考虑了其管理和政治影响。

Abstract: Multi-criteria decision-making methods provide decision-makers with
appropriate tools to make better decisions in uncertain, complex, and
conflicting situations. Fuzzy set theory primarily deals with the uncertainty
inherent in human thoughts and perceptions and attempts to quantify this
uncertainty. Fuzzy logic and fuzzy set theory are utilized with multi-criteria
decision-making methods because they effectively handle uncertainty and
fuzziness in decision-makers' judgments, allowing for verbal judgments of the
problem. This study utilizes the Fermatean fuzzy environment, a generalization
of fuzzy sets. An optimization model based on the deviation maximization method
is proposed to determine partially known feature weights. This method is
combined with interval-valued Fermatean fuzzy sets. The proposed method was
applied to the problem of selecting renewable energy sources. The reason for
choosing renewable energy sources is that meeting energy needs from renewable
sources, balancing carbon emissions, and mitigating the effects of global
climate change are among the most critical issues of the recent period. Even
though selecting renewable energy sources is a technical issue, the managerial
and political implications of this issue are also important, and are discussed
in this study.

</details>


### [2] [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)
*Jakub Grudzien Kuba,Mengting Gu,Qi Ma,Yuandong Tian,Vijai Mohan*

Main category: cs.AI

TL;DR: 提出语言自博弈（LSP）方法，让大语言模型在无额外数据下提升性能，实验表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展面临需更多数据学习的瓶颈，期望找到不依赖额外数据提升模型的方法。

Method: 利用博弈论自博弈框架，提出语言自博弈（LSP）方法，让模型与自身博弈。

Result: 在指令跟随基准测试中，Llama - 3.2 - 3B - Instruct预训练模型仅通过自博弈就能提升在挑战性任务上的性能，且比数据驱动基线方法更有效。

Conclusion: 所提出的方法能使大语言模型在不依赖额外数据的情况下提升性能。

Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by
scale, abundant high-quality training data, and reinforcement learning. Yet
this progress faces a fundamental bottleneck: the need for ever more data from
which models can continue to learn. In this work, we propose a reinforcement
learning approach that removes this dependency by enabling models to improve
without additional data. Our method leverages a game-theoretic framework of
self-play, where a model's capabilities are cast as performance in a
competitive game and stronger policies emerge by having the model play against
itself - a process we call Language Self-Play (LSP). Experiments with
Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained
models can not only enhance their performance on challenging tasks through
self-play alone, but can also do so more effectively than data-driven
baselines.

</details>


### [3] [From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning](https://arxiv.org/abs/2509.07017)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.AI

TL;DR: 介绍Spectral NSR神经符号推理框架，结合图信号处理和频率选择滤波器，有多种扩展，在推理基准测试中表现优异，为下一代推理系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 将符号推理的可解释性与谱学习的可扩展性和适应性相结合，构建新一代推理系统。

Method: 将逻辑规则嵌入为谱模板，在图谱域直接推理，利用图信号处理和基于知识图谱拉普拉斯特征结构的频率选择滤波器，还有一系列扩展和增强。

Result: 在ProofWriter和CLUTRR等基准测试中，相比基线方法有更高准确率、更快推理速度、更强对抗鲁棒性和更高可解释性，模型决策与符号证明结构相符，域适应有效。

Conclusion: Spectral NSR为下一代推理系统提供了可扩展且有原则的基础，优于传统方法。

Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning
framework that embeds logical rules as spectral templates and performs
inference directly in the graph spectral domain. By leveraging graph signal
processing (GSP) and frequency-selective filters grounded in the Laplacian
eigenstructure of knowledge graphs, the architecture unifies the
interpretability of symbolic reasoning with the scalability and adaptability of
spectral learning. Beyond the core formulation, we incorporate a comprehensive
set of extensions, including dynamic graph and basis learning, rational and
diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts
for modular specialization, proof-guided training with spectral curricula, and
uncertainty quantification for calibrated confidence. Additional enhancements
such as large language model coupling, co-spectral transfer alignment,
adversarial robustness, efficient GPU kernels, generalized Laplacians, and
causal interventions further expand the versatility of the framework.
  Empirical evaluation on state-of-the-art reasoning benchmarks such as
ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior
accuracy, faster inference, improved robustness to adversarial perturbations,
and higher interpretability compared to leading baselines including
transformers, message-passing neural networks, and neuro-symbolic logic
programming systems. Spectral attribution and proof-band agreement analyses
confirm that model decisions align closely with symbolic proof structures,
while transfer experiments validate effective domain adaptation through
co-spectral alignment. These results establish Spectral NSR as a scalable and
principled foundation for the next generation of reasoning systems, offering
transparency, robustness, and generalization beyond conventional approaches.

</details>


### [4] [Statistical Methods in Generative AI](https://arxiv.org/abs/2509.07054)
*Edgar Dobriban*

Main category: cs.AI

TL;DR: 本文探讨统计方法提升生成式AI可靠性、评估质量与效率等，回顾相关工作，讨论局限与未来方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI默认无正确性、安全性等保证，统计方法有提升其可靠性等潜力。

Method: 回顾现有关于统计方法用于生成式AI的工作。

Result: 介绍了通用统计技术及其在生成式AI中的应用。

Conclusion: 讨论了统计方法应用于生成式AI的局限性和潜在未来方向。

Abstract: Generative Artificial Intelligence is emerging as an important technology,
promising to be transformative in many areas. At the same time, generative AI
techniques are based on sampling from probabilistic models, and by default,
they come with no guarantees about correctness, safety, fairness, or other
properties. Statistical methods offer a promising potential approach to improve
the reliability of generative AI techniques. In addition, statistical methods
are also promising for improving the quality and efficiency of AI evaluation,
as well as for designing interventions and experiments in AI.
  In this paper, we review some of the existing work on these topics,
explaining both the general statistical techniques used, as well as their
applications to generative AI. We also discuss limitations and potential future
directions.

</details>


### [5] [Instruction Agent: Enhancing Agent with Expert Demonstration](https://arxiv.org/abs/2509.07098)
*Yinheng Li,Hailey Hultquist,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: 提出Instruction Agent GUI智能体，利用专家演示解决复杂任务，实验表明其在OSWorld部分任务上成功率达60%，提供实用可扩展框架。


<details>
  <summary>Details</summary>
Motivation: 当前GUI智能体在处理涉及新UI元素、长周期动作和个性化轨迹的复杂任务时存在困难。

Method: 利用专家演示，从单个演示中提取分步指令并严格遵循用户意图轨迹执行，还借助验证器和回溯器模块提高鲁棒性。

Result: Instruction Agent在OSWorld一组所有排名靠前的智能体都未完成的任务上达到60%的成功率。

Conclusion: Instruction Agent提供了实用且可扩展的框架，缩小了当前GUI智能体与可靠的现实世界GUI任务自动化之间的差距。

Abstract: Graphical user interface (GUI) agents have advanced rapidly but still
struggle with complex tasks involving novel UI elements, long-horizon actions,
and personalized trajectories. In this work, we introduce Instruction Agent, a
GUI agent that leverages expert demonstrations to solve such tasks, enabling
completion of otherwise difficult workflows. Given a single demonstration, the
agent extracts step-by-step instructions and executes them by strictly
following the trajectory intended by the user, which avoids making mistakes
during execution. The agent leverages the verifier and backtracker modules
further to improve robustness. Both modules are critical to understand the
current outcome from each action and handle unexpected interruptions(such as
pop-up windows) during execution. Our experiments show that Instruction Agent
achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked
agents failed to complete. The Instruction Agent offers a practical and
extensible framework, bridging the gap between current GUI agents and reliable
real-world GUI task automation.

</details>


### [6] [Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis](https://arxiv.org/abs/2509.07122)
*Sania Sinha,Tanawan Premsri,Danial Kamali,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 本文分析现有神经符号（NeSy）框架技术方面，展示三个通用NeSy框架，指出各方面挑战以促进社区创新思考。


<details>
  <summary>Details</summary>
Motivation: NeSy框架虽有优势，但学习曲线、工具缺乏等问题给开发者带来挑战，且多数研究重算法轻通用框架。

Method: 刻画现有NeSy框架技术方面，展示三个通用NeSy框架。

Result: 识别出各方面挑战，为确定框架解决多种问题的表达能力奠定基础。

Conclusion: 旨在激发变革行动，鼓励社区用新方式重新思考该问题。

Abstract: Neurosymbolic (NeSy) frameworks combine neural representations and learning
with symbolic representations and reasoning. Combining the reasoning
capacities, explainability, and interpretability of symbolic processing with
the flexibility and power of neural computing allows us to solve complex
problems with more reliability while being data-efficient. However, this
recently growing topic poses a challenge to developers with its learning curve,
lack of user-friendly tools, libraries, and unifying frameworks. In this paper,
we characterize the technical facets of existing NeSy frameworks, such as the
symbolic representation language, integration with neural models, and the
underlying algorithms. A majority of the NeSy research focuses on algorithms
instead of providing generic frameworks for declarative problem specification
to leverage problem solving. To highlight the key aspects of Neurosymbolic
modeling, we showcase three generic NeSy frameworks - \textit{DeepProbLog},
\textit{Scallop}, and \textit{DomiKnowS}. We identify the challenges within
each facet that lay the foundation for identifying the expressivity of each
framework in solving a variety of problems. Building on this foundation, we aim
to spark transformative action and encourage the community to rethink this
problem in novel ways.

</details>


### [7] [Autoencoder-Based Denoising of Muscle Artifacts in ECG to Preserve Skin Nerve Activity (SKNA) for Cognitive Stress Detection](https://arxiv.org/abs/2509.07146)
*Farnoush Baghestani,Jihye Moon,Youngsun Kong,Ki Chon*

Main category: cs.AI

TL;DR: 提出用轻量级一维卷积自编码器结合LSTM瓶颈结构去除皮肤神经活动（SKNA）记录中的肌电（EMG）噪声，实验表明该方法能有效提升信号质量和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 交感神经系统（SNS）失调与多种疾病相关，SKNA可反映SNS动态，但测量易受EMG污染，传统预处理方法效果不佳。

Method: 使用轻量级一维卷积自编码器结合LSTM瓶颈结构，从受EMG污染的记录中重建干净的SKNA，在留一受试者交叉验证框架下训练模型。

Result: 该方法使信噪比提高达9.65 dB，与干净SKNA的互相关从0.40提升到0.72，恢复的SKNA特征接近干净数据的可区分性，在严重噪声水平下分类准确率达91 - 98%。

Conclusion: 基于深度学习的重建方法能在大量EMG干扰下保留生理相关的交感神经爆发，可在自然、运动丰富的环境中更稳健地监测SKNA。

Abstract: The sympathetic nervous system (SNS) plays a central role in regulating the
body's responses to stress and maintaining physiological stability. Its
dysregulation is associated with a wide range of conditions, from
cardiovascular disease to anxiety disorders. Skin nerve activity (SKNA)
extracted from high-frequency electrocardiogram (ECG) recordings provides a
noninvasive window into SNS dynamics, but its measurement is highly susceptible
to electromyographic (EMG) contamination. Traditional preprocessing based on
bandpass filtering within a fixed range (e.g., 500--1000 Hz) is susceptible to
overlapping EMG and SKNA spectral components, especially during sustained
muscle activity. We present a denoising approach using a lightweight
one-dimensional convolutional autoencoder with a long short-term memory (LSTM)
bottleneck to reconstruct clean SKNA from EMG-contaminated recordings. Using
clean ECG-derived SKNA data from cognitive stress experiments and EMG noise
from chaotic muscle stimulation recordings, we simulated contamination at
realistic noise levels (--4 dB, --8 dB signal-to-noise ratio) and trained the
model in the leave-one-subject-out cross-validation framework. The method
improved signal-to-noise ratio by up to 9.65 dB, increased cross correlation
with clean SKNA from 0.40 to 0.72, and restored burst-based SKNA features to
near-clean discriminability (AUROC $\geq$ 0.96). Classification of baseline
versus sympathetic stimulation (cognitive stress) conditions reached accuracies
of 91--98\% across severe noise levels, comparable to clean data. These results
demonstrate that deep learning--based reconstruction can preserve
physiologically relevant sympathetic bursts during substantial EMG
interference, enabling more robust SKNA monitoring in naturalistic,
movement-rich environments.

</details>


### [8] [PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning](https://arxiv.org/abs/2509.07159)
*Heng Hao,Wenjun Hu,Oxana Verkholyak,Davoud Ataee Tarzanagh,Baruch Gutow,Sima Didari,Masoud Faraki,Hankyu Moon,Seungjai Min*

Main category: cs.AI

TL;DR: 提出PaVeRL - SQL框架结合部分匹配奖励和语言强化学习提升文本到SQL转换性能，在多个基准测试达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL方法在工业级数据库和复杂问题上执行准确率低。

Method: 采用两种管道，一是带组自我评估的上下文学习框架（verbal - RL），二是带特殊奖励函数和两阶段RL的思维链（CoT）RL管道。

Result: 在Spider、Spider 2.0和BIRD等基准测试达SOTA，在Spider2.0 - SQLite基准上，verbal - RL管道执行准确率比SOTA高7.4%，CoT管道高1.4%，混合SQL方言的RL训练有显著提升。

Conclusion: PaVeRL - SQL在现实工业约束下能提供可靠的SOTA文本到SQL转换，代码开源。

Abstract: Text-to-SQL models allow users to interact with a database more easily by
generating executable SQL statements from natural-language questions. Despite
recent successes on simpler databases and questions, current Text-to-SQL
methods still suffer from low execution accuracy on industry-scale databases
and complex questions involving domain-specific business logic. We present
\emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and
\emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning
language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt
two pipelines: (1) a newly designed in-context learning framework with group
self-evaluation (verbal-RL), using capable open- and closed-source large
language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL
pipeline with a small backbone model (OmniSQL-7B) trained with a specially
designed reward function and two-stage RL. These pipelines achieve
state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider,
Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the
verbal-RL pipeline achieves an execution accuracy 7.4\% higher than SOTA, and
the CoT pipeline is 1.4\% higher. RL training with mixed SQL dialects yields
strong, threefold gains, particularly for dialects with limited training data.
Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic
industrial constraints. The code is available at
https://github.com/PaVeRL-SQL/PaVeRL-SQL.

</details>


### [9] [That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral](https://arxiv.org/abs/2509.07170)
*Quinten Steenhuis*

Main category: cs.AI

TL;DR: 本文介绍并评估用于法律问题分类的FETCH分类器，利用新数据集和两种提效方法实现高准确率，降低引导成本。


<details>
  <summary>Details</summary>
Motivation: 解决法律求助者问题匹配错误导致的后果，如错过时限、遭受虐待等。

Method: 引入FETCH分类器，采用混合大语言模型/机器学习集成分类和自动生成后续问题丰富初始描述的方法，使用419条真实查询数据集。

Result: 使用低成本模型组合实现97.37%的分类准确率（hits@2），超过GPT - 5模型。

Conclusion: 该方法有望显著降低引导法律系统用户获取正确资源的成本并保持高准确率。

Abstract: Each year millions of people seek help for their legal problems by calling a
legal aid program hotline, walking into a legal aid office, or using a lawyer
referral service. The first step to match them to the right help is to identify
the legal problem the applicant is experiencing. Misdirection has consequences.
Applicants may miss a deadline, experience physical abuse, lose housing or lose
custody of children while waiting to connect to the right legal help. We
introduce and evaluate the FETCH classifier for legal issue classification and
describe two methods for improving accuracy: a hybrid LLM/ML ensemble
classification method, and the automatic generation of follow-up questions to
enrich the initial problem narrative. We employ a novel data set of 419
real-world queries to a nonprofit lawyer referral service. Ultimately, we show
classification accuracy (hits@2) of 97.37\% using a mix of inexpensive models,
exceeding the performance of the current state-of-the-art GPT-5 model. Our
approach shows promise in significantly reducing the cost of guiding users of
the legal system to the right resource for their problem while achieving high
accuracy.

</details>


### [10] [A Hybrid CNN-LSTM Deep Learning Model for Intrusion Detection in Smart Grid](https://arxiv.org/abs/2509.07208)
*Abdulhakim Alsaiari,Mohammad Ilyas*

Main category: cs.AI

TL;DR: 传统电网向智能电网演进带来攻击风险，本文提出基于混合深度学习的入侵检测系统，在检测性能上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 智能电网互联增加了遭受攻击的脆弱性，SCADA 协议易受攻击，需提升智能电网网络安全。

Method: 提出基于 CNN 和 LSTM 的混合深度学习入侵检测系统，用 DNP3 和 IEC104 数据集进行训练和测试。

Result: 与其他深度学习方法相比，在准确率、精确率、召回率和 F1 分数上有显著提升，检测准确率达 99.70%。

Conclusion: 所提出的混合深度学习入侵检测系统能有效提升智能电网的网络安全。

Abstract: The evolution of the traditional power grid into the "smart grid" has
resulted in a fundamental shift in energy management, which allows the
integration of renewable energy sources with modern communication technology.
However, this interconnection has increased smart grids' vulnerability to
attackers, which might result in privacy breaches, operational interruptions,
and massive outages. The SCADA-based smart grid protocols are critical for
real-time data collection and control, but they are vulnerable to attacks like
unauthorized access and denial of service (DoS). This research proposes a
hybrid deep learning-based Intrusion Detection System (IDS) intended to improve
the cybersecurity of smart grids. The suggested model takes advantage of
Convolutional Neural Networks' (CNN) feature extraction capabilities as well as
Long Short-Term Memory (LSTM) networks' temporal pattern recognition skills.
DNP3 and IEC104 intrusion detection datasets are employed to train and test our
CNN-LSTM model to recognize and classify the potential cyber threats. Compared
to other deep learning approaches, the results demonstrate considerable
improvements in accuracy, precision, recall, and F1-score, with a detection
accuracy of 99.70%.

</details>


### [11] [BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions](https://arxiv.org/abs/2509.07209)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Kaira Samuel,Matthew C. Jones,Faez Ahmed*

Main category: cs.AI

TL;DR: 介绍公开的BlendedNet空气动力学数据集及端到端代理框架，实验显示预测误差低，可解决非常规配置数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决非常规配置数据稀缺问题，推动空气动力学设计的数据驱动代理建模研究。

Method: 采样几何设计参数和飞行条件生成数据集，使用PointNet回归器预测几何参数，再用FiLM网络预测点系数。

Result: 实验表明在不同BWB上表面预测误差低。

Conclusion: BlendedNet可解决数据稀缺问题，促进空气动力学设计的相关研究。

Abstract: BlendedNet is a publicly available aerodynamic dataset of 999 blended wing
body (BWB) geometries. Each geometry is simulated across about nine flight
conditions, yielding 8830 converged RANS cases with the Spalart-Allmaras model
and 9 to 14 million cells per case. The dataset is generated by sampling
geometric design parameters and flight conditions, and includes detailed
pointwise surface quantities needed to study lift and drag. We also introduce
an end-to-end surrogate framework for pointwise aerodynamic prediction. The
pipeline first uses a permutation-invariant PointNet regressor to predict
geometric parameters from sampled surface point clouds, then conditions a
Feature-wise Linear Modulation (FiLM) network on the predicted parameters and
flight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.
Experiments show low errors in surface predictions across diverse BWBs.
BlendedNet addresses data scarcity for unconventional configurations and
enables research on data-driven surrogate modeling for aerodynamic design.

</details>


### [12] [OmniAcc: Personalized Accessibility Assistant Using Generative AI](https://arxiv.org/abs/2509.07220)
*Siddhant Karki,Ethan Han,Nadim Mahmud,Suman Bhunia,John Femiani,Vaskar Raychoudhury*

Main category: cs.AI

TL;DR: 本文介绍了AI驱动的交互式导航系统OmniAcc，利用多种数据识别无障碍设施，通过案例展示其在辅助导航和建设包容城市空间方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决行动不便者在城市环境中因缺乏无障碍信息和工具而面临的导航障碍问题。

Method: 利用GPT - 4、卫星图像和OpenStreetMap数据，采用零样本学习和定制提示，通过结构化工作流程进行验证。

Result: 在人行横道检测案例中，检测准确率达97.5%。

Conclusion: OmniAcc体现了AI在改善导航和促进更具包容性城市空间方面的变革潜力。

Abstract: Individuals with ambulatory disabilities often encounter significant barriers
when navigating urban environments due to the lack of accessible information
and tools. This paper presents OmniAcc, an AI-powered interactive navigation
system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to
identify, classify, and map wheelchair-accessible features such as ramps and
crosswalks in the built environment. OmniAcc offers personalized route
planning, real-time hands-free navigation, and instant query responses
regarding physical accessibility. By using zero-shot learning and customized
prompts, the system ensures precise detection of accessibility features, while
supporting validation through structured workflows. This paper introduces
OmniAcc and explores its potential to assist urban planners and mobility-aid
users, demonstrated through a case study on crosswalk detection. With a
crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative
potential of AI in improving navigation and fostering more inclusive urban
spaces.

</details>


### [13] [HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring](https://arxiv.org/abs/2509.07260)
*Xin Wang,Ting Dang,Xinyu Zhang,Vassilis Kostakos,Michael J. Witbrock,Hong Jia*

Main category: cs.AI

TL;DR: 本文探讨小型语言模型（SLMs）用于医疗预测的表现，发现其性能可与大语言模型（LLMs）媲美，且在效率和隐私方面有优势，但仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 大多数基于LLMs的医疗解决方案是基于云的，存在隐私、内存使用和延迟问题，而SLMs的医疗预测性能有待探索。

Method: 使用零样本、少样本和指令微调方法对SLMs进行医疗预测任务评估，并将微调后的最佳SLMs部署到移动设备上评估其实践效率和预测性能。

Result: SLMs性能可与LLMs媲美，在效率和隐私方面有显著提升，但在处理类别不平衡和少样本场景存在挑战。

Conclusion: SLMs虽有不足，但仍是下一代隐私保护医疗监测的有前景解决方案。

Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating
timely interventions, managing chronic health conditions, and ultimately
improving individuals' quality of life. Previous studies on large language
models (LLMs) have highlighted their impressive generalization abilities and
effectiveness in healthcare prediction tasks. However, most LLM-based
healthcare solutions are cloud-based, which raises significant privacy concerns
and results in increased memory usage and latency. To address these challenges,
there is growing interest in compact models, Small Language Models (SLMs),
which are lightweight and designed to run locally and efficiently on mobile and
wearable devices. Nevertheless, how well these models perform in healthcare
prediction remains largely unexplored. We systematically evaluated SLMs on
health prediction tasks using zero-shot, few-shot, and instruction fine-tuning
approaches, and deployed the best performing fine-tuned SLMs on mobile devices
to evaluate their real-world efficiency and predictive performance in practical
healthcare scenarios. Our results show that SLMs can achieve performance
comparable to LLMs while offering substantial gains in efficiency and privacy.
However, challenges remain, particularly in handling class imbalance and
few-shot scenarios. These findings highlight SLMs, though imperfect in their
current form, as a promising solution for next-generation, privacy-preserving
healthcare monitoring.

</details>


### [14] [Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity](https://arxiv.org/abs/2509.07339)
*Vardhan Palod,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 研究中间标记生成（ITG）中中间标记序列长度与问题难度的关系，发现其关联不大，挑战了中间推理痕迹生成适应问题难度的假设。


<details>
  <summary>Details</summary>
Motivation: 当前中间推理痕迹或思维链虽能提升语言模型推理任务表现，但背后机制不明，社区普遍假设中间标记序列长度反映问题难度，本文对此进行批判性检验。

Method: 从零开始在A*搜索算法的推导痕迹上训练Transformer模型，在简单自由空间问题和分布外问题上评估模型。

Result: 模型在最简单任务上常产生过长推理痕迹，甚至无法生成解决方案；中间标记长度与真实A*跟踪长度仅松散相关，关联多在问题接近训练分布时出现。

Conclusion: 问题实例的固有计算复杂性不是关键因素，而是与训练数据的分布距离；挑战了中间痕迹生成适应问题难度的假设，提醒不要将长序列自动视为“思考努力”。

Abstract: Intermediate token generation (ITG), where a model produces output before the
solution, has been proposed as a method to improve the performance of language
models on reasoning tasks. While these reasoning traces or Chain of Thoughts
(CoTs) are correlated with performance gains, the mechanisms underlying them
remain unclear. A prevailing assumption in the community has been to
anthropomorphize these tokens as "thinking", treating longer traces as evidence
of higher problem-adaptive computation. In this work, we critically examine
whether intermediate token sequence length reflects or correlates with problem
difficulty. To do so, we train transformer models from scratch on derivational
traces of the A* search algorithm, where the number of operations required to
solve a maze problem provides a precise and verifiable measure of problem
complexity. We first evaluate the models on trivial free-space problems,
finding that even for the simplest tasks, they often produce excessively long
reasoning traces and sometimes fail to generate a solution. We then
systematically evaluate the model on out-of-distribution problems and find that
the intermediate token length and ground truth A* trace length only loosely
correlate. We notice that the few cases where correlation appears are those
where the problems are closer to the training distribution, suggesting that the
effect arises from approximate recall rather than genuine problem-adaptive
computation. This suggests that the inherent computational complexity of the
problem instance is not a significant factor, but rather its distributional
distance from the training data. These results challenge the assumption that
intermediate trace generation is adaptive to problem difficulty and caution
against interpreting longer sequences in systems like R1 as automatically
indicative of "thinking effort".

</details>


### [15] [Autonomous Code Evolution Meets NP-Completeness](https://arxiv.org/abs/2509.07367)
*Cunxi Yu,Rongjian Liang,Chia-Tung Ho,Haoxing Ren*

Main category: cs.AI

TL;DR: 本文提出SATLUTION框架，将基于大语言模型的代码进化扩展到完整代码库规模，用于布尔可满足性问题，进化出的求解器表现优于人类设计的竞赛获胜者。


<details>
  <summary>Details</summary>
Motivation: 受AlphaEvolve启发，将基于大语言模型的代码进化从孤立代码扩展到完整代码库规模，解决布尔可满足性问题。

Method: SATLUTION框架协调大语言模型代理，在严格正确性保证和分布式运行时反馈下直接进化求解器代码库，同时自我进化其进化策略和规则。

Result: 从SAT Competition 2024代码库和基准开始，SATLUTION进化出的求解器在SAT Competition 2025中优于人类设计的获胜者，在2024基准上也超过了2024和2025的冠军。

Conclusion: SATLUTION框架有效扩展了基于大语言模型的代码进化范围，能产生优于人类设计的求解器。

Abstract: Large language models (LLMs) have recently shown strong coding abilities,
enabling not only static code generation but also iterative code self-evolving
through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve}
demonstrated that LLM-based coding agents can autonomously improve algorithms
and surpass human experts, with scopes limited to isolated kernels spanning
hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the
first framework to extend LLM-based code evolution to the full repository
scale, encompassing hundreds of files and tens of thousands of lines of C/C++
code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem
and a cornerstone of both theory and applications. SATLUTION orchestrates LLM
agents to directly evolve solver repositories under strict correctness
guarantees and distributed runtime feedback, while simultaneously self-evolving
its own evolution policies and rules. Starting from SAT Competition 2024
codebases and benchmark, SATLUTION evolved solvers that decisively outperformed
the human-designed winners of the SAT Competition 2025, and also surpassed both
2024 and 2025 champions on the 2024 benchmarks.

</details>


### [16] [SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection](https://arxiv.org/abs/2509.07473)
*Qin Chen,Yuanyi Ren,Xiaojun Ma,Mugeng Liu,Han Shi,Dongmei Zhang*

Main category: cs.AI

TL;DR: 提出自动化电子表格布局生成框架SheetDesigner，结合规则和视觉反思，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 手动设计电子表格布局耗时且需专业知识，现有自动化布局模型不适用于电子表格。

Method: 形式化电子表格布局生成任务，用七标准评估协议和3326个电子表格数据集，引入使用MLLMs的SheetDesigner框架。

Result: SheetDesigner比五个基线模型至少高22.6%，MLLMs处理重叠和平衡较好但对齐有困难。

Conclusion: 提出的SheetDesigner框架有效，需结合规则和视觉反思策略处理MLLMs的问题。

Abstract: Spreadsheets are critical to data-centric tasks, with rich, structured
layouts that enable efficient information transmission. Given the time and
expertise required for manual spreadsheet layout design, there is an urgent
need for automated solutions. However, existing automated layout models are
ill-suited to spreadsheets, as they often (1) treat components as axis-aligned
rectangles with continuous coordinates, overlooking the inherently discrete,
grid-based structure of spreadsheets; and (2) neglect interrelated semantics,
such as data dependencies and contextual links, unique to spreadsheets. In this
paper, we first formalize the spreadsheet layout generation task, supported by
a seven-criterion evaluation protocol and a dataset of 3,326 spreadsheets. We
then introduce SheetDesigner, a zero-shot and training-free framework using
Multimodal Large Language Models (MLLMs) that combines rule and vision
reflection for component placement and content population. SheetDesigner
outperforms five baselines by at least 22.6\%. We further find that through
vision modality, MLLMs handle overlap and balance well but struggle with
alignment, necessitates hybrid rule and visual reflection strategies. Our codes
and data is available at Github.

</details>


### [17] [Towards explainable decision support using hybrid neural models for logistic terminal automation](https://arxiv.org/abs/2509.07577)
*Riccardo DElia,Alberto Termine,Francesco Flammini*

Main category: cs.AI

TL;DR: 本文提出可解释的神经系统动力学建模框架，结合深度学习与多种可解释性技术，用于多式联运物流终端决策支持。


<details>
  <summary>Details</summary>
Motivation: 深度学习与系统动力学结合在交通物流建模中有优势，但损失了解释性和因果可靠性，而关键决策系统需要这些特性。

Method: 提出将深度学习与基于概念的可解释性、机械可解释性和因果机器学习技术相结合的混合方法。

Result: 可构建基于语义有意义且可操作变量的神经网络模型，保留传统系统动力学模型的因果基础和透明度。

Conclusion: 神经符号方法可弥合黑箱预测模型与复杂动态环境中关键决策支持需求之间的差距。

Abstract: The integration of Deep Learning (DL) in System Dynamics (SD) modeling for
transportation logistics offers significant advantages in scalability and
predictive accuracy. However, these gains are often offset by the loss of
explainability and causal reliability $-$ key requirements in critical
decision-making systems. This paper presents a novel framework for
interpretable-by-design neural system dynamics modeling that synergizes DL with
techniques from Concept-Based Interpretability, Mechanistic Interpretability,
and Causal Machine Learning. The proposed hybrid approach enables the
construction of neural network models that operate on semantically meaningful
and actionable variables, while retaining the causal grounding and transparency
typical of traditional SD models. The framework is conceived to be applied to
real-world case-studies from the EU-funded project AutoMoTIF, focusing on
data-driven decision support, automation, and optimization of multimodal
logistic terminals. We aim at showing how neuro-symbolic methods can bridge the
gap between black-box predictive models and the need for critical decision
support in complex dynamical environments within cyber-physical systems enabled
by the industrial Internet-of-Things.

</details>


### [18] [Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling](https://arxiv.org/abs/2509.07617)
*Minghui Li,Hao Zhang,Yechao Zhang,Wei Wan,Shengshan Hu,pei Xiaobing,Jing Wang*

Main category: cs.AI

TL;DR: 提出激活引导的提示注入攻击框架应对大语言模型安全威胁，实验显示有优越跨模型可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有白盒/灰盒方法不实用，黑盒方法可迁移性差，需解决大语言模型的直接提示注入（DPI）攻击这一安全威胁。

Method: 用替代模型激活构建能量模型（EBM）评估对抗性提示质量，在训练好的EBM引导下，使用标记级马尔可夫链蒙特卡罗（MCMC）采样自适应优化对抗性提示，实现无梯度黑盒攻击。

Result: 跨五个主流大语言模型攻击成功率达49.6%，比人工制作提示提高34.6%，在未见任务场景保持36.6%的攻击成功率。

Conclusion: 激活与攻击有效性相关，语义模式在可迁移漏洞利用中起关键作用。

Abstract: Direct Prompt Injection (DPI) attacks pose a critical security threat to
Large Language Models (LLMs) due to their low barrier of execution and high
potential damage. To address the impracticality of existing white-box/gray-box
methods and the poor transferability of black-box methods, we propose an
activations-guided prompt injection attack framework. We first construct an
Energy-based Model (EBM) using activations from a surrogate model to evaluate
the quality of adversarial prompts. Guided by the trained EBM, we employ the
token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize
adversarial prompts, thereby enabling gradient-free black-box attacks.
Experimental results demonstrate our superior cross-model transferability,
achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%
improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen
task scenarios. Interpretability analysis reveals a correlation between
activations and attack effectiveness, highlighting the critical role of
semantic patterns in transferable vulnerability exploitation.

</details>


### [19] [Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment](https://arxiv.org/abs/2509.07642)
*Sascha Kaltenpoth,Oliver Müller*

Main category: cs.AI

TL;DR: 论文指出组织采用大语言模型存在对齐问题，提出LLM ATLAS框架以缓解该问题，并进行概念文献分析得到相关成果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在组织应用中存在对齐问题，现有研究未解决组织采用者与黑盒模型间信息不对称及组织AI采用过程问题。

Method: 以组织大语言模型采用阶段和代理理论为概念进行概念文献分析。

Result: 提供了特定于组织大语言模型采用中AI对齐方法的扩展文献分析过程，以及首个大语言模型对齐问题 - 解决方案空间。

Conclusion: 提出的LLM ATLAS概念框架有助于缓解组织采用大语言模型时的对齐问题。

Abstract: Adopting Large language models (LLMs) in organizations potentially
revolutionizes our lives and work. However, they can generate off-topic,
discriminating, or harmful content. This AI alignment problem often stems from
misspecifications during the LLM adoption, unnoticed by the principal due to
the LLM's black-box nature. While various research disciplines investigated AI
alignment, they neither address the information asymmetries between
organizational adopters and black-box LLM agents nor consider organizational AI
adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led
Alignment Strategy) a conceptual framework grounded in agency (contract)
theory, to mitigate alignment problems during organizational LLM adoption. We
conduct a conceptual literature analysis using the organizational LLM adoption
phases and the agency theory as concepts. Our approach results in (1) providing
an extended literature analysis process specific to AI alignment methods during
organizational LLM adoption and (2) providing a first LLM alignment
problem-solution space.

</details>


### [20] [DeepGraphLog for Layered Neurosymbolic AI](https://arxiv.org/abs/2509.07665)
*Adem Kikaj,Giuseppe Marra,Floris Geerts,Robin Manhaeve,Luc De Raedt*

Main category: cs.AI

TL;DR: 提出新的神经符号AI框架DeepGraphLog，克服现有系统局限性，拓展其在图结构领域的应用。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号AI框架如DeepProbLog有固定流程，限制了对复杂依赖关系的建模能力，尤其是在图等不规则数据结构中。

Method: 引入DeepGraphLog，扩展ProbLog与图神经谓词，实现多层神经符号推理，允许神经和符号组件任意分层，将符号表示视为图以用图神经网络处理。

Result: 在规划、知识图谱补全和图神经网络表达性等任务上展示了能力，有效捕捉复杂关系依赖。

Conclusion: DeepGraphLog克服了现有系统的关键限制，为神经符号集成提供了更具表达性和灵活性的框架，拓宽了神经符号AI在图结构领域的适用性。

Abstract: Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neural
networks with the interpretability and structure of symbolic reasoning.
However, current NeSy frameworks like DeepProbLog enforce a fixed flow where
symbolic reasoning always follows neural processing. This restricts their
ability to model complex dependencies, especially in irregular data structures
such as graphs. In this work, we introduce DeepGraphLog, a novel NeSy framework
that extends ProbLog with Graph Neural Predicates. DeepGraphLog enables
multi-layer neural-symbolic reasoning, allowing neural and symbolic components
to be layered in arbitrary order. In contrast to DeepProbLog, which cannot
handle symbolic reasoning via neural methods, DeepGraphLog treats symbolic
representations as graphs, enabling them to be processed by Graph Neural
Networks (GNNs). We showcase the capabilities of DeepGraphLog on tasks in
planning, knowledge graph completion with distant supervision, and GNN
expressivity. Our results demonstrate that DeepGraphLog effectively captures
complex relational dependencies, overcoming key limitations of existing NeSy
systems. By broadening the applicability of neurosymbolic AI to
graph-structured domains, DeepGraphLog offers a more expressive and flexible
framework for neural-symbolic integration.

</details>


### [21] [Unleashing the True Potential of LLMs: A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding](https://arxiv.org/abs/2509.07676)
*Jipeng Li,Zeyu Gao,Yubin Qi,Hande Dong,Weijian Chen,Qiang Lin*

Main category: cs.AI

TL;DR: 提出FTR框架解决大语言模型推理生成错误问题，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理易生成错误内容，现有自纠错方法存在缺乏可靠错误定位信号和推理深度受限的问题。

Method: 提出Feedback - Triggered Regeneration (FTR)框架，结合用户反馈和增强解码动态；引入Long - Term Multipath (LTM)解码，通过延迟序列评估探索多推理路径。

Result: 在数学推理和代码生成基准测试中，框架比现有基于提示的自纠错方法有显著且持续的改进。

Conclusion: 所提出的FTR框架有效解决了大语言模型推理的问题，比现有自纠错方法更优。

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
diverse tasks, yet their susceptibility to generating incorrect content during
inference remains a critical unsolved challenge. While self-correction methods
offer potential solutions, their effectiveness is hindered by two inherent
limitations: (1) the absence of reliable guidance signals for error
localization, and (2) the restricted reasoning depth imposed by conventional
next-token decoding paradigms. To address these issues, we propose
Feedback-Triggered Regeneration (FTR), a novel framework that synergizes user
feedback with enhanced decoding dynamics. Specifically, FTR activates response
regeneration only upon receiving negative user feedback, thereby circumventing
error propagation from faulty self-assessment while preserving originally
correct outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,
which enables systematic exploration of multiple reasoning trajectories through
delayed sequence evaluation, effectively overcoming the myopic decision-making
characteristic of standard next-token prediction. Extensive experiments on
mathematical reasoning and code generation benchmarks demonstrate that our
framework achieves consistent and significant improvements over
state-of-the-art prompt-based self-correction methods.

</details>


### [22] [FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support](https://arxiv.org/abs/2509.07706)
*Yildiray Kabak,Gokce B. Laleci Erturkmen,Mert Gencturk,Tuncay Namli,A. Anil Sinaci,Ruben Alcantud Corcoles,Cristina Gomez Ballesteros,Pedro Abizanda,Asuman Dogac*

Main category: cs.AI

TL;DR: 提出FHIR - RAG - MEDS系统以整合HL7 FHIR和RAG系统，提升个性化医疗决策支持。


<details>
  <summary>Details</summary>
Motivation: 医疗决策支持系统领域中，虽RAG和HL7 FHIR有潜力，但二者在实际应用中的整合研究有限，需开展相关研究。

Method: 提出FHIR - RAG - MEDS系统进行HL7 FHIR与RAG系统的整合。

Result: 未提及。

Conclusion: 未提及。

Abstract: In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health
Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a
Retrieval-Augmented Generation (RAG)-based system to improve personalized
medical decision support on evidence-based clinical guidelines, emphasizing the
need for research in practical applications. In the evolving landscape of
medical decision support systems, integrating advanced technologies such as RAG
and HL7 FHIR can significantly enhance clinical decision-making processes.
Despite the potential of these technologies, there is limited research on their
integration in practical applications.

</details>


### [23] [RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.07711)
*Ziye Chen,Chengwei Qin,Yao Shu*

Main category: cs.AI

TL;DR: 现有奥数基准存在评估问题，论文引入RIMO基准，测试显示大模型在该基准上表现不佳，揭示其与奥数推理能力差距。


<details>
  <summary>Details</summary>
Motivation: 现有奥数级基准存在实际限制，引入评分噪声和潜在偏差，需新基准推动评估。

Method: 创建RIMO双轨基准，RIMO - N改写问题获唯一整数答案用于确定性检查，RIMO - P含专家审核证明题，分解为子问题用自动评分系统评估。

Result: 对十个前沿大模型测试显示，在旧基准表现好，在RIMO上性能大幅下降。

Conclusion: 当前大模型能力与实际奥数推理能力有巨大差距，RIMO为未来研究提供高分辨率衡量标准。

Abstract: As large language models (LLMs) reach high scores on established mathematical
benchmarks, such as GSM8K and MATH, the research community has turned to
International Mathematical Olympiad (IMO) problems to push the evaluation
frontier. However, existing Olympiad-level benchmarks suffer from practical
constraints that introduce grading noise and potential bias, such as
heterogeneous answer formats requiring model-based judges and a reliance on
potentially flawed solutions. We introduce RIMO, a two-track benchmark designed
to preserve peak Olympiad difficulty while eliminating this evaluation noise.
The first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique
integer answer, allowing for deterministic correctness checking. The second
track, RIMO-P, features 456 proof problems with expert-checked solutions, which
are decomposed into a sequence of sub-problems to evaluate the step-by-step
reasoning process via an automated grading system. Our benchmarking of ten
frontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these
systems excel on older benchmarks, their performance drops sharply on RIMO.
These results highlight a substantial gap between current LLM capabilities and
actual Olympiad-level reasoning. By providing a challenging yet
easy-to-evaluate suite, RIMO offers a high-resolution yardstick for future
research, presenting a clear target for closing the profound reasoning gap our
findings expose.

</details>


### [24] [BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis](https://arxiv.org/abs/2509.07723)
*Bo Yu,Zhixiu Hua,Bo Zhao*

Main category: cs.AI

TL;DR: 现有帕金森病诊断依赖临床量表误诊率高，基于肠道微生物群的深度学习模型有潜力但有不足，提出BDPM方法。


<details>
  <summary>Details</summary>
Motivation: 帕金森病误诊率高，现有基于肠道微生物群的深度学习模型有局限，需要更强大的针对微生物组数据的特征提取方法。

Method: 提出BDPM，收集患者及健康配偶肠道微生物群概况确定差异丰度分类群，开发RFRE特征选择框架，设计混合分类模型捕捉微生物组数据时空模式。

Result: 文档未提及

Conclusion: 文档未提及

Abstract: Background: Parkinson's disease remains a major neurodegenerative disorder
with high misdiagnosis rates, primarily due to reliance on clinical rating
scales. Recent studies have demonstrated a strong association between gut
microbiota and Parkinson's disease, suggesting that microbial composition may
serve as a promising biomarker. Although deep learning models based ongut
microbiota show potential for early prediction, most approaches rely on single
classifiers and often overlook inter-strain correlations or temporal dynamics.
Therefore, there is an urgent need for more robust feature extraction methods
tailored to microbiome data. Methods: We proposed BDPM (A Machine
Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut
Microbiota Analysis). First, we collected gut microbiota profiles from 39
Parkinson's patients and their healthy spouses to identify differentially
abundant taxa. Second, we developed an innovative feature selection framework
named RFRE (Random Forest combined with Recursive Feature Elimination),
integrating ecological knowledge to enhance biological interpretability.
Finally, we designed a hybrid classification model to capture temporal and
spatial patterns in microbiome data.

</details>


### [25] [The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for Streamlining Food Carbon Footprint Analysis](https://arxiv.org/abs/2509.07733)
*Mustafa Kaan Aslan,Reinout Heijungs,Filip Ilievski*

Main category: cs.AI

TL;DR: 本文提出结合LCA进展、公开数据库与知识增强AI技术估算食品碳足迹的方法，还引入聊天机器人界面，通过网络演示展示系统，指出其潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 应对气候变化，因供应链不透明和数据碎片化，传统LCA评估碳足迹复杂，需要新方法。

Method: 结合LCA进展、公开数据库与知识增强AI技术（如检索增强生成）估算食品从摇篮到大门的碳足迹，引入聊天机器人界面供用户交互。

Result: 进行了网络演示，展示系统可处理任意食品和后续问题。

Conclusion: 该系统有以易获取格式提供LCA见解的潜力，但存在数据库不确定性和AI误判等局限。

Abstract: Environmental sustainability, particularly in relation to climate change, is
a key concern for consumers, producers, and policymakers. The carbon footprint,
based on greenhouse gas emissions, is a standard metric for quantifying the
contribution to climate change of activities and is often assessed using life
cycle assessment (LCA). However, conducting LCA is complex due to opaque and
global supply chains, as well as fragmented data. This paper presents a
methodology that combines advances in LCA and publicly available databases with
knowledge-augmented AI techniques, including retrieval-augmented generation, to
estimate cradle-to-gate carbon footprints of food products. We introduce a
chatbot interface that allows users to interactively explore the carbon impact
of composite meals and relate the results to familiar activities. A live web
demonstration showcases our proof-of-concept system with arbitrary food items
and follow-up questions, highlighting both the potential and limitations - such
as database uncertainties and AI misinterpretations - of delivering LCA
insights in an accessible format.

</details>


### [26] [Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach](https://arxiv.org/abs/2509.07820)
*João Paulo Nogueira,Wentao Sun,Alonso Silva,Laith Zumot*

Main category: cs.AI

TL;DR: 提出确定性引导推理（CGR）方法，平衡大推理语言模型推理效率与可靠性，实验证明其提升准确率、减少token使用且稳定。


<details>
  <summary>Details</summary>
Motivation: 大推理语言模型有推理预算，需平衡推理效率与可靠性。

Method: 受生成对抗网络启发，用评判模型定期评估推理是否达确信结论，未达则继续推理直至满足确定性阈值。

Result: 在AIME2024和AIME2025数据集实验显示，CGR提升基线准确率、减少token使用，多种子评估表明其稳定，token节省分析显示可大量减少token。

Conclusion: 确定性是推理充分性的有力信号，CGR使大推理语言模型更自适应、可信和资源高效，利于实际部署。

Abstract: The rise of large reasoning language models (LRLMs) has unlocked new
potential for solving complex tasks. These models operate with a thinking
budget, that is, a predefined number of reasoning tokens used to arrive at a
solution. We propose a novel approach, inspired by the generator/discriminator
framework in generative adversarial networks, in which a critic model
periodically probes its own reasoning to assess whether it has reached a
confident conclusion. If not, reasoning continues until a target certainty
threshold is met. This mechanism adaptively balances efficiency and reliability
by allowing early termination when confidence is high, while encouraging
further reasoning when uncertainty persists. Through experiments on the
AIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)
improves baseline accuracy while reducing token usage. Importantly, extended
multi-seed evaluations over 64 runs demonstrate that CGR is stable, reducing
variance across seeds and improving exam-like performance under penalty-based
grading. Additionally, our token savings analysis shows that CGR can eliminate
millions of tokens in aggregate, with tunable trade-offs between certainty
thresholds and efficiency. Together, these findings highlight certainty as a
powerful signal for reasoning sufficiency. By integrating confidence into the
reasoning process, CGR makes large reasoning language models more adaptive,
trustworthy, and resource efficient, paving the way for practical deployment in
domains where both accuracy and computational cost matter.

</details>


### [27] [Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study](https://arxiv.org/abs/2509.07846)
*Amay Jain,Liu Cui,Si Chen*

Main category: cs.AI

TL;DR: 研究两种RAG范式用于课堂问答，发现不同范式优势，提出动态分支框架提升性能并为教育者和设计者提供指导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于课堂易提供错误信息，现有研究未考虑教学因素，需探索RAG范式用于课堂问答的最佳实践。

Method: 使用EduScopeQA数据集测量不同教育查询类型的性能，用系统修改的教科书数据集评估系统一致性。

Result: OpenAI Vector Search RAG适合低成本快速事实检索；GraphRAG Global擅于回答主题查询；GraphRAG Local在语料完整性重要时准确率最高；动态分支框架可提升性能。

Conclusion: 研究结果为教育者和系统设计者有效集成RAG增强的大语言模型到学习环境提供了可操作的指导。

Abstract: Large language models like ChatGPT are increasingly used in classrooms, but
they often provide outdated or fabricated information that can mislead
students. Retrieval Augmented Generation (RAG) improves reliability of LLMs by
grounding responses in external resources. We investigate two accessible RAG
paradigms, vector-based retrieval and graph-based retrieval to identify best
practices for classroom question answering (QA). Existing comparative studies
fail to account for pedagogical factors such as educational disciplines,
question types, and practical deployment costs. Using a novel dataset,
EduScopeQA, of 3,176 questions across academic subjects, we measure performance
on various educational query types, from specific facts to broad thematic
discussions. We also evaluate system alignment with a dataset of systematically
altered textbooks that contradict the LLM's latent knowledge. We find that
OpenAI Vector Search RAG (representing vector-based RAG) performs well as a
low-cost generalist, especially for quick fact retrieval. On the other hand,
GraphRAG Global excels at providing pedagogically rich answers to thematic
queries, and GraphRAG Local achieves the highest accuracy with the dense,
altered textbooks when corpus integrity is critical. Accounting for the 10-20x
higher resource usage of GraphRAG (representing graph-based RAG), we show that
a dynamic branching framework that routes queries to the optimal retrieval
method boosts fidelity and efficiency. These insights provide actionable
guidelines for educators and system designers to integrate RAG-augmented LLMs
into learning environments effectively.

</details>


### [28] [SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs](https://arxiv.org/abs/2509.07858)
*Xinyu Zhang,Changzhi Zhou,Linmei Hu,Luhao Zhang,Xiancai Chen,Haomin Fu,Yang Yang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 本文探索用小规模开源大模型构建高质量代码指令数据，提出迭代自蒸馏方法，开发SCoder模型并取得了先进的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码大模型微调依赖专有大模型蒸馏的大规模指令数据，成本高，探索小规模开源大模型构建数据的潜力。

Method: 先在专有大模型的优质数据合成样本上训练小规模大模型增强其数据合成能力，提出迭代自蒸馏方法；设计多检查点采样和多方面评分策略进行初始数据选择，引入基于梯度的影响估计方法进行最终数据过滤；基于小规模合成器生成的代码指令数据集微调开发SCoder模型。

Result: SCoder模型实现了最先进的代码生成能力。

Conclusion: 提出的利用小规模开源大模型构建数据的方法有效。

Abstract: Existing code large language models (LLMs) often rely on large-scale
instruction data distilled from proprietary LLMs for fine-tuning, which
typically incurs high costs. In this paper, we explore the potential of
small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code
instruction data construction. We first observe that the data synthesis
capability of small-scale LLMs can be enhanced by training on a few superior
data synthesis samples from proprietary LLMs. Building on this, we propose a
novel iterative self-distillation approach to bootstrap small-scale LLMs,
transforming them into powerful synthesizers that reduce reliance on
proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain
diverse and high-quality self-distilled data, we design multi-checkpoint
sampling and multi-aspect scoring strategies for initial data selection.
Furthermore, to identify the most influential samples, we introduce a
gradient-based influence estimation method for final data filtering. Based on
the code instruction datasets from the small-scale synthesizers, we develop
SCoder, a family of code generation models fine-tuned from DeepSeek-Coder.
SCoder models achieve state-of-the-art code generation capabilities,
demonstrating the effectiveness of our method.

</details>


### [29] [CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models](https://arxiv.org/abs/2509.07867)
*Augustin Crespin,Ioannis Kostis,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: 提出CP - Model - Zoo辅导系统，利用专家模型数据库，基于用户自然语言描述检索模型，实验显示检索准确率高。


<details>
  <summary>Details</summary>
Motivation: 约束规划建模语言复杂等因素阻碍非专家使用CP解决组合问题，且难以从自然语言描述生成专家级模型。

Method: 构建CP - Model - Zoo系统，从数据库中基于用户自然语言描述检索最接近的源代码模型，无需人工数据标注。

Result: 实验表明，在模拟不同专业水平用户输入问题描述时，能准确检索到正确模型。

Conclusion: CP - Model - Zoo系统能有效基于用户自然语言描述检索专家验证过的模型，帮助非专家使用CP解决组合问题。

Abstract: Constraint Programming and its high-level modeling languages have long been
recognized for their potential to achieve the holy grail of problem-solving.
However, the complexity of modeling languages, the large number of global
constraints, and the art of creating good models have often hindered
non-experts from choosing CP to solve their combinatorial problems. While
generating an expert-level model from a natural-language description of a
problem would be the dream, we are not yet there. We propose a tutoring system
called CP-Model-Zoo, exploiting expert-written models accumulated through the
years. CP-Model-Zoo retrieves the closest source code model from a database
based on a user's natural language description of a combinatorial problem. It
ensures that expert-validated models are presented to the user while
eliminating the need for human data labeling. Our experiments show excellent
accuracy in retrieving the correct model based on a user-input description of a
problem simulated with different levels of expertise.

</details>


### [30] [HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?](https://arxiv.org/abs/2509.07894)
*Fangchen Yu,Haiyuan Wan,Qianjia Cheng,Yuchen Zhang,Jiacheng Chen,Fujun Han,Yulun Wu,Junchi Yao,Ruilizhen Hu,Ning Ding,Yu Cheng,Tao Chen,Lei Bai,Dongzhan Zhou,Yun Luo,Ganqu Cui,Peng Ye*

Main category: cs.AI

TL;DR: 提出首个面向高中物理奥赛的基准HiPhO，对30个模型评估，揭示不同类型模型表现差距。


<details>
  <summary>Details</summary>
Motivation: 现有物理基准存在未系统覆盖物理竞赛和无法与人类直接对比的问题，需新基准。

Method: 构建HiPhO，包括收集综合数据、采用专业评估和与人类参赛者对比，用官方评分方案评估模型。

Result: 开源MLLMs多在铜牌及以下，开源LLMs偶获金牌，闭源推理MLLMs获6 - 12枚金牌，多数模型距满分差距大。

Conclusion: 开源模型与顶尖学生有差距，闭源推理模型物理推理能力强，仍有提升空间，HiPhO开源可用。

Abstract: Recently, the physical capabilities of (M)LLMs have garnered increasing
attention. However, existing benchmarks for physics suffer from two major gaps:
they neither provide systematic and up-to-date coverage of real-world physics
competitions such as physics Olympiads, nor enable direct performance
comparison with humans. To bridge these gaps, we present HiPhO, the first
benchmark dedicated to high school physics Olympiads with human-aligned
evaluation. Specifically, HiPhO highlights three key innovations. (1)
Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,
spanning both international and regional competitions, and covering mixed
modalities that encompass problems spanning text-only to diagram-based. (2)
Professional Evaluation: We adopt official marking schemes to perform
fine-grained grading at both the answer and step level, fully aligned with
human examiners to ensure high-quality and domain-specific evaluation. (3)
Comparison with Human Contestants: We assign gold, silver, and bronze medals to
models based on official medal thresholds, thereby enabling direct comparison
between (M)LLMs and human contestants. Our large-scale evaluation of 30
state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly
remain at or below the bronze level; open-source LLMs show promising progress
with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold
medals; and most models still have a significant gap from full marks. These
results highlight a substantial performance gap between open-source models and
top students, the strong physical reasoning capabilities of closed-source
reasoning models, and the fact that there is still significant room for
improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused
benchmark for advancing multimodal physical reasoning, is open-source and
available at https://github.com/SciYu/HiPhO.

</details>


### [31] [Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare](https://arxiv.org/abs/2509.07961)
*Valen Tagliabue,Leonard Dung*

Main category: cs.AI

TL;DR: 开发新实验范式测量语言模型福利，比较多种测量方式，观察到测量结果有一定相关性，但不确定是否成功测量福利，凸显测量可行性。


<details>
  <summary>Details</summary>
Motivation: 探索测量语言模型福利的方法。

Method: 开发新实验范式，比较模型的口头报告偏好与行为偏好，测试成本和奖励对行为的影响，检验不同语义等价提示下对幸福福利量表的响应一致性。

Result: 各测量方式有一定相互支持，特定条件下陈述偏好与行为有可靠相关性，但测量一致性在不同模型和条件下有差异，响应在扰动下不一致。

Conclusion: 目前不确定方法是否成功测量语言模型福利状态，但凸显了语言模型福利测量的可行性，值得进一步探索。

Abstract: We develop new experimental paradigms for measuring welfare in language
models. We compare verbal reports of models about their preferences with
preferences expressed through behavior when navigating a virtual environment
and selecting conversation topics. We also test how costs and rewards affect
behavior and whether responses to an eudaimonic welfare scale - measuring
states such as autonomy and purpose in life - are consistent across
semantically equivalent prompts. Overall, we observed a notable degree of
mutual support between our measures. The reliable correlations observed between
stated preferences and behavior across conditions suggest that preference
satisfaction can, in principle, serve as an empirically measurable welfare
proxy in some of today's AI systems. Furthermore, our design offered an
illuminating setting for qualitative observation of model behavior. Yet, the
consistency between measures was more pronounced in some models and conditions
than others and responses were not consistent across perturbations. Due to
this, and the background uncertainty about the nature of welfare and the
cognitive states (and welfare subjecthood) of language models, we are currently
uncertain whether our methods successfully measure the welfare state of
language models. Nevertheless, these findings highlight the feasibility of
welfare measurement in language models, inviting further exploration.

</details>


### [32] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: 提出统一框架VISION用于漏洞检测，通过生成反事实训练数据集减少虚假关联，提升检测性能并发布基准数据集，推进可解释AI网络安全系统。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在源代码漏洞检测中受训练数据不平衡和标签噪声影响，学习到虚假关联，泛化能力差。

Method: 提出VISION框架，包括用大语言模型生成反事实样本、对相反标签的代码对进行针对性GNN训练、基于图的可解释性分析。

Result: VISION减少了虚假学习，提高了整体准确率、成对对比准确率和最差组准确率，在提出的指标上有提升，发布了CWE - 20 - CFA基准数据集。

Conclusion: VISION推进了基于AI的透明可信网络安全系统，可通过交互式可视化进行人工分析。

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [33] [An Agentic AI Workflow to Simplify Parameter Estimation of Complex Differential Equation Systems](https://arxiv.org/abs/2509.07283)
*Saakaar Bhatnagar*

Main category: cs.CE

TL;DR: 提出代理式AI工作流，将轻量级规范转化为校准管道，降低高级校准门槛。


<details>
  <summary>Details</summary>
Motivation: 常微分方程（ODE）模型参数识别劳动密集且易出错，数据集有噪声、模型可能存在问题且需要框架专业知识。

Method: 将问题的XML描述和Python代码骨架输入，代理自动验证规范与代码一致性、修复常见问题，将Python可调用对象转换为JAX函数，进行两阶段搜索。

Result: 得到原生自动微分、可复现的工作流。

Conclusion: 开源实现降低高级校准门槛，保留专家控制，可快速从问题陈述得到拟合、可审核的模型。

Abstract: Parameter identification for mechanistic Ordinary Differential Equation (ODE)
models underpins prediction and control in several applications, yet remains a
labor-intensive and brittle process: datasets are noisy and partial, models can
be stiff or misspecified, and differentiable implementations demand framework
expertise. An agentic AI workflow is presented that converts a lightweight,
human-readable specification into a compiled, parallel, and differentiable
calibration pipeline. Users supply an XML description of the problem and fill
in a Python code skeleton; the agent automatically validates consistency
between spec and code, and auto-remediates common pathologies. It transforms
Python callables into pure JAX functions for efficient just-in-time compilation
and parallelization. The system then orchestrates a two-stage search comprising
global exploration of the parameter space followed by gradient-based
refinement. The result is an AD-native, reproducible workflow that lowers the
barrier to advanced calibration while preserving expert control. An open-source
implementation with a documented API and examples is released, enabling rapid
movement from problem statement to fitted, auditable models with minimal
boilerplate.

</details>


### [34] [A Unified Data-Driven Framework for Efficient Scientific Discovery](https://arxiv.org/abs/2509.07303)
*Tingxiong Xiao,Xinxin Song,Ziqian Wang,Boyang Zhang,Jinli Suo*

Main category: cs.CE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Scientific discovery drives progress across disciplines, from fundamental
physics to industrial applications. However, identifying physical laws
automatically from gathered datasets requires identifying the structure and
parameters of the formula underlying the data, which involves navigating a vast
search space and consuming substantial computational resources. To address
these issues, we build on the Buckingham $\Pi$ theorem and Taylor's theorem to
create a unified representation of diverse formulas, which introduces latent
variables to form a two-stage structure. To minimize the search space, we
initially focus on determining the structure of the latent formula, including
the relevant contributing inputs, the count of latent variables, and their
interconnections. Following this, the process of parameter identification is
expedited by enforcing dimensional constraints for physical relevance, favoring
simplicity in the formulas, and employing strategic optimization techniques.
Any overly complex outcomes are refined using symbolic regression for a compact
form. These general strategic techniques drastically reduce search iterations
from hundreds of millions to just tens, significantly enhancing the efficiency
of data-driven formula discovery. We performed comprehensive validation to
demonstrate FIND's effectiveness in discovering physical laws, dimensionless
numbers, partial differential equations, and uniform critical system parameters
across various fields, including astronomy, physics, chemistry, and
electronics. The excellent performances across 11 distinct datasets position
FIND as a powerful and versatile tool for advancing data-driven scientific
discovery in multiple domains.

</details>


### [35] [Uncertainty-Driven Hierarchical Sampling for Unbalanced Continual Malware Detection with Time-Series Update-Based Retrieval](https://arxiv.org/abs/2509.07532)
*Yi Xie,Ziyuan Yang,Yongqiang Huang,Yinyu Chen,Lei Zhang,Liang Liu,Yi Zhang*

Main category: cs.CE

TL;DR: 提出新的不确定性引导持续学习框架用于安卓恶意软件检测，实验表明性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决安卓恶意软件检测中概念漂移和类别不平衡问题，以及现有基于重放方法的固有偏差。

Method: 提出层次平衡采样器动态平衡样本并选择高信息、高不确定性实例，增加向量检索机制利用历史恶意软件嵌入识别变体。

Result: 在严格低标签条件下显著优于现有方法，真阳性率达92.95%，平均准确率达94.26%。

Conclusion: 所提框架对可持续安卓恶意软件检测有效。

Abstract: Android malware detection continues to face persistent challenges stemming
from long-term concept drift and class imbalance, as evolving malicious
behaviors and shifting usage patterns dynamically reshape feature
distributions. Although continual learning (CL) mitigates drift, existing
replay-based methods suffer from inherent bias. Specifically, their reliance on
classifier uncertainty for sample selection disproportionately prioritizes the
dominant benign class, causing overfitting and reduced generalization to
evolving malware. To address these limitations, we propose a novel
uncertainty-guided CL framework. First, we introduce a hierarchical balanced
sampler that employs a dual-phase uncertainty strategy to dynamically balance
benign and malicious samples while simultaneously selecting high-information,
high-uncertainty instances within each class. This mechanism ensures class
equilibrium across both replay and incremental data, thereby enhancing
adaptability to emerging threats. Second, we augment the framework with a
vector retrieval mechanism that exploits historical malware embeddings to
identify evolved variants via similarity-based retrieval, thereby complementing
classifier updates. Extensive experiments demonstrate that our framework
significantly outperforms state-of-the-art methods under strict low-label
conditions (50 labels per phase). It achieves a true positive rate (TPR) of
92.95\% and a mean accuracy (mACC) of 94.26\%, which validates its efficacy for
sustainable Android malware detection.

</details>


### [36] [LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design](https://arxiv.org/abs/2509.07627)
*Ruihao Zhang,Xiao Liu*

Main category: cs.CE

TL;DR: 提出LSMTCR框架用于从头生成全长TCR，在多个数据集上表现优于基线，能输出多样的全长TCR设计。


<details>
  <summary>Details</summary>
Motivation: 由于序列空间大、数据偏差和免疫遗传约束建模不完整，设计全长、表位特异性TCR具有挑战性。

Method: 提出LSMTCR框架，包括扩散增强的BERT编码器学习表位表示，条件GPT解码器生成CDR3，基因感知的Transformer组装完整序列。

Result: 在多个数据集上，LSMTCR预测结合能力高于基线，更忠实地恢复位置和长度语法，提供更好的多样性；转移学习改善α链生成；全长组装保留k - mer光谱，编辑距离小；配对α/β共同建模时pTM/ipTM更高。

Conclusion: LSMTCR可仅根据表位输入输出多样的、基因关联的全长TCR设计，实现高通量筛选和迭代优化。

Abstract: Designing full-length, epitope-specific TCR {\alpha}\b{eta} remains
challenging due to vast sequence space, data biases and incomplete modeling of
immunogenetic constraints. We present LSMTCR, a scalable multi-architecture
framework that separates specificity from constraint learning to enable de
novo, epitope-conditioned generation of paired, full-length TCRs. A
diffusion-enhanced BERT encoder learns time-conditioned epitope
representations; conditional GPT decoders, pretrained on CDR3\b{eta} and
transferred to CDR3{\alpha}, generate chain-specific CDR3s under cross-modal
conditioning with temperature-controlled diversity; and a gene-aware
Transformer assembles complete {\alpha}/\b{eta} sequences by predicting V/J
usage to ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and our
curated dataset, LSMTCR achieves higher predicted binding than baselines on
most datasets, more faithfully recovers positional and length grammars, and
delivers superior, temperature-tunable diversity. For {\alpha}-chain
generation, transfer learning improves predicted binding, length realism and
diversity over representative methods. Full-length assembly from known or de
novo CDR3s preserves k-mer spectra, yields low edit distances to references,
and, in paired {\alpha}/\b{eta} co-modelling with epitope, attains higher
pTM/ipTM than single-chain settings. LSMTCR outputs diverse,
gene-contextualized, full-length TCR designs from epitope input alone, enabling
high-throughput screening and iterative optimization.

</details>


### [37] [Generalized eigenvalue stabilization for immersed explicit dynamics](https://arxiv.org/abs/2509.07632)
*Tim Bürchner,Lars Radtke,Sascha Eisenträger,Alexander Düster,Ernst Rank,Stefan Kollmannsberger,Philipp Kopp*

Main category: cs.CE

TL;DR: 提出广义特征值稳定化（GEVS）策略处理浸没有限元离散中切割单元对临界时间步长的不利影响，结合有限单元法保证系统矩阵正定性，数值实验显示策略有效。


<details>
  <summary>Details</summary>
Motivation: 显式时间积分在浸没有限元离散中受切割单元影响严重，需解决其对全局系统临界时间步长的不利影响。

Method: 提出GEVS策略处理切割单元的单元质量矩阵，使用谱基函数结合GLL求积规则得到未切割单元的高阶收敛对角质量矩阵，结合有限单元法保证系统矩阵正定性。

Result: 稳定化策略实现了最优收敛率，恢复了等效边界一致离散的临界时间步长，在弱强制狄利克雷边界条件下也成立。

Conclusion: 所提出的GEVS稳定化策略有效，可直接应用于其他浸没边界有限元方法。

Abstract: Explicit time integration for immersed finite element discretizations
severely suffers from the influence of poorly cut elements. In this
contribution, we propose a generalized eigenvalue stabilization (GEVS) strategy
for the element mass matrices of cut elements to cure their adverse impact on
the critical time step size of the global system. We use spectral basis
functions, specifically $C^0$ continuous Lagrangian interpolation polynomials
defined on Gauss-Lobatto-Legendre (GLL) points, which, in combination with its
associated GLL quadrature rule, yield high-order convergent diagonal mass
matrices for uncut elements. Moreover, considering cut elements, we combine the
proposed GEVS approach with the finite cell method (FCM) to guarantee
definiteness of the system matrices. However, the proposed GEVS stabilization
can directly be applied to other immersed boundary finite element methods.
Numerical experiments demonstrate that the stabilization strategy achieves
optimal convergence rates and recovers critical time step sizes of equivalent
boundary-conforming discretizations. This also holds in the presence of weakly
enforced Dirichlet boundary conditions using either Nitsche's method or penalty
formulations.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [38] [Navigating the Data Space Landscape: Concepts, Applications, and Future Directions](https://arxiv.org/abs/2509.06983)
*Bojana Marojevikj,Riste Stojanov*

Main category: cs.DB

TL;DR: 本文探讨数据空间发展现状，介绍基础原理、设计原则，展示实际应用，提出未来利用语义数据模型和SPARQL授权层的发展方向。


<details>
  <summary>Details</summary>
Motivation: 全面了解数据空间现状，为该快速发展领域的未来进步提供指导。

Method: 先介绍数据空间架构基础原则，分析核心设计原则与构建模块，展示多个行业的实际应用案例。

Result: 呈现了数据空间在不同行业的应用，解决数据主权、互操作性和信任等问题。

Conclusion: 利用语义数据模型可增强数据空间互操作性和集成，采用SPARQL作为授权层可提升数据访问的安全性和精细控制。

Abstract: This paper explores the evolving landscape of data spaces, focusing on key
concepts, practical applications, and emerging future directions. It begins by
introducing the foundational principles that underpin data space architectures,
emphasizing their importance in facilitating secure and efficient data
exchange. The core design principles and essential building blocks that form
the backbone of data-space systems are then examined. Several real-world
implementations are presented, showcasing how data spaces are applied across
various industries to address challenges such as data sovereignty,
interoperability, and trust. The paper concludes by discussing future
directions, proposing that leveraging semantic data models can significantly
enhance interoperability and data integration within data spaces. Furthermore,
it suggests exploring the implementation of SPARQL as a sophisticated
authorization layer to improve security and granular control over data access.
This research provides a comprehensive understanding of the current state of
data spaces and aims to guide future advancements in this rapidly evolving
field by highlighting the potential of semantic data and SPARQL-based
authorization.

</details>


### [39] [Private Queries with Sigma-Counting](https://arxiv.org/abs/2509.07018)
*Jun Gao,Jie Ding*

Main category: cs.DB

TL;DR: 传统计数查询易泄露隐私，添加噪声方法有局限，提出sigma - counting方法应对挑战，可提高准确性并保持隐私。


<details>
  <summary>Details</summary>
Motivation: 传统计数查询程序易泄露敏感信息，添加噪声的方法在查询数量、输出准确性及嵌套查询顺序上有局限。

Method: 使用sigma - algebra概念构建隐私保护计数查询的sigma - counting方法。

Result: 提出的概念和方法在大量查询时能显著提高输出准确性，并保持所需隐私水平。

Conclusion: sigma - counting方法可应对挑战，还可应用于大型和时变数据集。

Abstract: Many data applications involve counting queries, where a client specifies a
feasible range of variables and a database returns the corresponding item
counts. A program that produces the counts of different queries often risks
leaking sensitive individual-level information. A popular approach to enhance
data privacy is to return a noisy version of the actual count. It is typically
achieved by adding independent noise to each query and then control the total
privacy budget within a period. This approach may be limited in the number of
queries and output accuracy in practice. Also, the returned counts do not
maintain the total order for nested queries, an important feature in many
applications. This work presents the design and analysis of a new method,
sigma-counting, that addresses these challenges. Sigma-counting uses the notion
of sigma-algebra to construct privacy-preserving counting queries. We show that
the proposed concepts and methods can significantly improve output accuracy
while maintaining a desired privacy level in the presence of massive queries to
the same data. We also discuss how the technique can be applied to address
large and time-varying datasets.

</details>


### [40] [JOINT: Join Optimization and Inference via Network Traversal](https://arxiv.org/abs/2509.07230)
*Szu-Yun Ko,Ethan Chen,Bo-Cian Chang,Alan Shu-Luen Chang*

Main category: cs.DB

TL;DR: 提出模糊连接框架，自动识别可连接列对并遍历间接连接路径，实验证明其有效性，可用于数据集成。


<details>
  <summary>Details</summary>
Motivation: 传统关系数据库需手动指定连接键，限制了碎片化或命名不一致表之间的连接性。

Method: 结合列名相似度和行级模糊值重叠，用负对数变换的Jaccard分数计算边权重，通过图遍历发现连接路径。

Result: 在合成医疗风格数据库实验中，系统能在列名模糊和部分值不匹配情况下恢复有效连接。

Conclusion: 该研究在数据集成方面有直接应用。

Abstract: Traditional relational databases require users to manually specify join keys
and assume exact matches between column names and values. In practice, this
limits joinability across fragmented or inconsistently named tables. We propose
a fuzzy join framework that automatically identifies joinable column pairs and
traverses indirect (multi-hop) join paths across multiple databases. Our method
combines column name similarity with row-level fuzzy value overlap, computes
edge weights using negative log-transformed Jaccard scores, and performs join
path discovery via graph traversal. Experiments on synthetic healthcare-style
databases demonstrate the system's ability to recover valid joins despite
fuzzified column names and partial value mismatches. This research has direct
applications in data integration.

</details>


### [41] [Filtered Approximate Nearest Neighbor Search: A Unified Benchmark and Systematic Experimental Study [Experiment, Analysis & Benchmark]](https://arxiv.org/abs/2509.07789)
*Jiayang Shi,Yuzheng Cai,Weiguo Zheng*

Main category: cs.DB

TL;DR: 本文针对FANNS算法缺乏系统研究的问题，提出分类法和评估框架，进行实证研究并建立标准化基准。


<details>
  <summary>Details</summary>
Motivation: 现有FANNS算法缺乏系统研究，参数耦合、关键因素分析不足、实验设计有偏差，难以了解算法优劣。

Method: 提出分类法（过滤后搜索、搜索后过滤、混合搜索）和系统评估框架，进行综合实证研究，建立标准化基准。

Result: 分析了查询难度和数据集属性对性能的影响，明确了各方法优势，建立标准化基准并开源相关资源。

Conclusion: 综合调查和基准测试对FANNS算法很关键，能实现公平评估、明确算法分类等目标。

Abstract: For a given dataset $\mathcal{D}$ and structured label $f$, the goal of
Filtered Approximate Nearest Neighbor Search (FANNS) algorithms is to find
top-$k$ points closest to a query that satisfy label constraints, while
ensuring both recall and QPS (Queries Per Second). In recent years, many FANNS
algorithms have been proposed. However, the lack of a systematic investigation
makes it difficult to understand their relative strengths and weaknesses.
Additionally, we found that: (1) FANNS algorithms have coupled,
dataset-dependent parameters, leading to biased comparisons. (2) Key impact
factors are rarely analyzed systematically, leaving unclear when each algorithm
performs well. (3) Disparate datasets, workloads, and biased experiment designs
make cross-algorithm comparisons unreliable. Thus, a comprehensive survey and
benchmark for FANNS is crucial to achieve the following goals: designing a fair
evaluation and clarifying the classification of algorithms, conducting in-depth
analysis of their performance, and establishing a unified benchmark. First, we
propose a taxonomy (dividing methods into \textit{filter-then-search},
\textit{search-then-filter}, \textit{hybrid-search}) and a systematic
evaluation framework, integrating unified parameter tuning and standardized
filtering across algorithms to reduce implementation-induced performance
variations and reflect core trade-offs. Then, we conduct a comprehensive
empirical study to analyze how query difficulty and dataset properties impact
performance, evaluating robustness under pressures like filter selectivity,
Recall@k, and scalability to clarify each method's strengths. Finally, we
establish a standardized benchmark with real-world datasets and open-source
related resources to ensure reproducible future research.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [42] [Crossword: Adaptive Consensus for Dynamic Data-Heavy Workloads](https://arxiv.org/abs/2509.07157)
*Guanzhou Hu,Yiwei Chen,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: 提出用于动态数据密集型工作负载的灵活共识协议Crossword，在Gazette中实现并评估，性能优于先前协议。


<details>
  <summary>Details</summary>
Motivation: 解决云环境中动态数据密集型工作负载带来的复制负载大小跨度大、间歇性带宽压力等挑战。

Method: 采用每实例擦除编码，智能分配编码分片；实现分片分配和法定人数大小的自适应权衡；采用惰性追随者八卦机制处理领导者故障转移。

Result: 在静态场景下与先前协议性能相当，在动态工作负载和网络条件下性能最高提升2.3倍；与CockroachDB集成使TPC - C聚合吞吐量提高1.32倍。

Conclusion: Crossword是适用于动态数据密集型工作负载的有效共识协议。

Abstract: We present Crossword, a flexible consensus protocol for dynamic data-heavy
workloads, a rising challenge in the cloud where replication payload sizes span
a wide spectrum and introduce sporadic bandwidth stress. Crossword applies
per-instance erasure coding and distributes coded shards intelligently to
reduce critical-path data transfer significantly when desirable. Unlike
previous approaches that statically assign shards to servers, Crossword enables
an adaptive tradeoff between the assignment of shards and quorum size in
reaction to dynamic workloads and network conditions, while always retaining
the availability guarantee of classic protocols. Crossword handles leader
failover gracefully by employing a lazy follower gossiping mechanism that
incurs minimal impact on critical-path performance. We implement Crossword
(along with relevant protocols) in Gazette, a distributed, replicated, and
protocol-generic key-value store written in async Rust. We evaluate Crossword
comprehensively to show that it matches the best performance among previous
protocols (MultiPaxos, Raft, RSPaxos, and CRaft) in static scenarios, and
outperforms them by up to 2.3x under dynamic workloads and network conditions.
Our integration of Crossword with CockroachDB brings 1.32x higher aggregate
throughput to TPC-C under 5-way replication. We will open-source Gazette upon
publication.

</details>


### [43] [Bodega: Serving Linearizable Reads Locally from Anywhere at Anytime via Roster Leases](https://arxiv.org/abs/2509.07158)
*Guanzhou Hu,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: 提出Bodega共识协议，可从任意节点本地提供线性化读取，在真实广域网集群上加速客户端读取请求，将开源实现。


<details>
  <summary>Details</summary>
Motivation: 设计一种能在不受干扰写入影响下，从任意期望节点本地提供线性化读取的共识协议。

Method: 采用新颖的名册租赁算法保护名册（集群元数据新概念），使用乐观持有和早期接受通知减少干扰写入的中断，结合智能名册覆盖和轻量级心跳机制。

Result: 在真实广域网集群上，与先前方法相比，在适度写入干扰下将平均客户端读取请求速度提高5.6 - 13.1倍，写入性能相当，支持快速主动名册更改和容错。

Conclusion: Bodega是经典共识的非侵入式扩展，性能表现优秀，具有实际应用价值。

Abstract: We present Bodega, the first consensus protocol that serves linearizable
reads locally from any desired node, regardless of interfering writes. Bodega
achieves this via a novel roster leases algorithm that safeguards the roster, a
new notion of cluster metadata. The roster is a generalization of leadership;
it tracks arbitrary subsets of replicas as responder nodes for local reads. A
consistent agreement on the roster is established through roster leases, an
all-to-all leasing mechanism that generalizes existing all-to-one leasing
approaches (Leader Leases, Quorum Leases), unlocking a new point in the
protocol design space. Bodega further employs optimistic holding and early
accept notifications to minimize interruption from interfering writes, and
incorporates smart roster coverage and lightweight heartbeats to maximize
practicality. Bodega is a non-intrusive extension to classic consensus; it
imposes no special requirements on writes other than a responder-covering
quorum. We implement Bodega and related works in Vineyard, a protocol-generic
replicated key-value store written in async Rust. We compare it to previous
protocols (Leader Leases, EPaxos, PQR, and Quorum Leases) and two production
coordination services (etcd and ZooKeeper). Bodega speeds up average client
read requests by 5.6x-13.1x on real WAN clusters versus previous approaches
under moderate write interference, delivers comparable write performance,
supports fast proactive roster changes as well as fault tolerance via leases,
and closely matches the performance of sequentially-consistent etcd and
ZooKeeper deployments across all YCSB workloads. We will open-source Vineyard
upon publication.

</details>


### [44] [A Study on Messaging Trade-offs in Data Streaming for Scientific Workflows](https://arxiv.org/abs/2509.07199)
*Anjus George,Michael J. Brim,Christopher Zimmer,Tyler J. Skluzacek,A. J. Ruckman,Gustav R. Jansen,Sarp Oral*

Main category: cs.DC

TL;DR: 本文研究消息参数及其配置对两种科学工作流数据流式传输的影响，通过模拟得出帮助用户选择配置的关键观察和实用见解。


<details>
  <summary>Details</summary>
Motivation: 内存到内存的数据流式传输对现代科学工作流至关重要，现成消息框架虽可行，但需利用其能力并调整配置以满足工作流要求。

Method: 使用从Deleria和LCLS工作流导出的合成工作负载进行流式模拟，采用RabbitMQ消息框架。

Result: 模拟揭示了帮助用户理解哪种配置最适合其流式工作负载的关键观察和实用见解。

Conclusion: 研究有助于用户为流式工作负载选择合适的消息框架配置。

Abstract: Memory-to-memory data streaming is essential for modern scientific workflows
that require near real-time data analysis, experimental steering, and informed
decision-making during experiment execution. It eliminates the latency
bottlenecks associated with file-based transfers to parallel storage, enabling
rapid data movement between experimental facilities and HPC systems. These
tightly coupled experimental-HPC workflows demand low latency, high throughput,
and reliable data delivery to support on-the-fly analysis and timely feedback
for experimental control. Off-the-shelf messaging frameworks are increasingly
considered viable solutions for enabling such direct memory streaming due to
their maturity, broad adoption, and ability to abstract core messaging and
reliability functionalities from the application layer. However, effectively
meeting the workflows' requirements depends on utilizing the framework's
capabilities and carefully tuning its configurations.
  In this paper, we present a study that investigates the messaging parameters,
and their configuration choices that impact the streaming requirements of two
representative scientific workflows. We specifically characterize throughput
trade-offs associated with reliable message transmission for these workflows.
Our study is conducted through streaming simulations using synthetic workloads
derived from the Deleria and LCLS workflows, employing the RabbitMQ messaging
framework within the context of the Data Streaming to HPC infrastructure at
OLCF. Our simulations reveal several key observations and practical insights
that help users understand which configurations best meet the needs of their
streaming workloads.

</details>


### [45] [DREAMS: Decentralized Resource Allocation and Service Management across the Compute Continuum Using Service Affinity](https://arxiv.org/abs/2509.07497)
*Hai Dinh-Tuan,Tien Hung Nguyen,Sanjeet Raj Pandey*

Main category: cs.DC

TL;DR: 本文提出去中心化框架DREAMS，用于优化计算连续体中微服务放置决策，评估证明其高效可扩展。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要自适应计算基础设施，传统集中式资源分配和服务放置方法有局限，因此需新方法。

Method: 引入DREAMS框架，在各计算域内设置自主运行的代理，通过基于Raft的共识算法和成本效益投票进行全局协调。

Result: 在现代制造环境中可实现全局优化的服务放置，保持高容错性；关键协调操作随域数量呈亚线性扩展。

Conclusion: DREAMS框架具有响应性、隐私保护和容错性，适用于多利益相关者场景，高效且可扩展。

Abstract: Modern manufacturing systems require adaptive computing infrastructures that
can respond to highly dynamic workloads and increasingly customized production
demands. The compute continuum emerges as a promising solution, enabling
flexible deployment of microservices across distributed, heterogeneous domains.
However, this paradigm also requires a novel approach to resource allocation
and service placement, as traditional centralized solutions struggle to scale
effectively, suffer from latency bottlenecks, and introduce single points of
failure. In this paper, we present DREAMS, a decentralized framework that
optimizes microservice placement decisions collaboratively across different
computational domains. At its core, DREAMS introduces agents that operate
autonomously within each domain while coordinating globally through a
Raft-based consensus algorithm and cost-benefit voting. This decentralized
architecture enables responsive, privacy-preserving, and fault-tolerant
coordination, making it particularly suitable given the growing prevalence of
multi-stakeholder scenarios across the compute continuum. In particular, within
modern manufacturing environments, DREAMS achieves globally optimized service
placements while maintaining high fault tolerance. Further evaluations
demonstrate that key coordination operations, such as Local Domain Manager
(LDM) registration and migration voting, scale sub-linearly with the number of
domains, confirming the efficiency and scalability of our proposal.

</details>


### [46] [Optimizing Task Scheduling in Fog Computing with Deadline Awareness](https://arxiv.org/abs/2509.07378)
*Mohammad Sadegh Sirjani,Somayeh Sobati-Moghadam*

Main category: cs.DC

TL;DR: 本文提出RIGEO算法用于物联网任务调度，结合IGEO算法和强化学习，实验表明该算法在多方面优于其他算法。


<details>
  <summary>Details</summary>
Motivation: 物联网应用对响应速度和低延迟有要求，雾计算面临资源分配和任务调度挑战，需设计算法降低能耗、提升服务质量。

Method: 将雾节点按流量分为低、高两类，用IGEO算法在低流量节点调度低截止时间任务，用强化学习在高流量节点处理高截止时间任务，形成RIGEO算法。

Result: 实验结果显示，提出的算法在系统响应时间、总截止时间违规时间、资源和系统能耗方面优于其他先进算法。

Conclusion: RIGEO算法能有效优化雾计算中的物联网任务调度，降低节点能耗，提升服务质量。

Abstract: The rise of Internet of Things (IoT) devices has led to the development of
numerous applications that require quick responses and low latency. Fog
computing has emerged as a solution for processing these IoT applications, but
it faces challenges such as resource allocation and job scheduling. Therefore,
it is crucial to determine how to assign and schedule tasks on Fog nodes. A
well-designed job scheduling algorithm can help decrease energy usage and
improve response times for application requests. This work aims to schedule
tasks in IoT while minimizing the total energy consumption of nodes and
enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into
account task deadlines. Initially, this paper classifies the Fog nodes into two
categories based on their traffic level: low and high. It schedules
low-deadline tasks on low-traffic-level nodes using an Improved Golden Eagle
Optimization (IGEO) algorithm, an enhancement of the Golden Eagle Optimization
Algorithm that utilizes genetic operators for discretization. High-deadline
tasks are processed on high-traffic nodes using reinforcement learning (RL).
This combined approach is called the Reinforcement Improved Golden Eagle
Optimization (RIGEO) algorithm. Experimental results demonstrate that the
proposed algorithms optimize system response time, total deadline violation
time, and resource and system energy consumption compared to other
state-of-the-art algorithms.

</details>


### [47] [DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference](https://arxiv.org/abs/2509.07379)
*Yuning Zhang,Grant Pinkert,Nan Yang,Yanli Li,Dong Yuan*

Main category: cs.DC

TL;DR: 本文提出DuoServe - MoE推理服务系统，分离预填充和解码阶段并采用定制调度策略，实验表明其可提升端到端延迟1.42 - 7.54倍，且峰值内存仅占全模型大小15%。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型存在GPU内存压力大问题，且统一调度策略导致延迟和内存使用不佳，需要改进。

Method: 提出DuoServe - MoE系统，预填充阶段用双流CUDA管道，解码阶段用离线训练的轻量级层级别预测器。

Result: 在4位Mixtral - 8x7B和8x22B模型实验中，DuoServe - MoE提升端到端延迟1.42 - 7.54倍，峰值内存仅占全模型大小15%。

Conclusion: DuoServe - MoE系统能有效提升推理性能，减少内存使用。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances
their capabilities by increasing model width through sparsely activated expert
branches, which keeps inference computation efficient. However, the large
number of expert weights introduces significant GPU memory pressure, especially
in resource-constrained environments such as single-GPU servers. More
importantly, MoE inference consists of two fundamentally different stages: a
prefill stage where most experts are activated densely, and a decode stage
where only a few experts are triggered sparsely. Treating these stages with a
uniform scheduling strategy often leads to suboptimal latency and memory usage.
To address this, we propose DuoServe-MoE, an inference serving system that
explicitly separates prefill and decode stages and applies tailored expert
scheduling strategies to each. In the prefill stage, DuoServe-MoE uses a
two-stream CUDA pipeline that overlaps expert weight prefetching with the
computation of non-MoE layers, limiting expert residency in GPU memory. In the
decode stage, a lightweight layer-level predictor trained offline from
activation traces is used to prefetch only the most likely activated experts,
without requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B
and 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to
7.54 times while keeping peak memory usage at only 15 percent of the full model
size.

</details>


### [48] [Dependency-Aware Execution Mechanism in Hyperledger Fabric Architecture](https://arxiv.org/abs/2509.07425)
*Sanyam Kaul,Manaswini Piduguralla,Gayathri Shreeya Patnala,Sathya Peri*

Main category: cs.DC

TL;DR: 针对Hyperledger Fabric在高负载下的性能局限，提出依赖感知执行模型，测试显示可提升吞吐量并降低拒绝率，增强可扩展性。


<details>
  <summary>Details</summary>
Motivation: Hyperledger Fabric在高负载下存在交易吞吐量低和拒绝率高的问题，乐观并发控制和延迟验证导致资源低效和竞争。

Method: 提出依赖感知执行模型，包括背书阶段的依赖标记、排序服务的优化块构建、块内引入DAG表示依赖以及提交者并行执行独立交易。

Result: 在Hyperledger Fabric v2.5中测试，高竞争场景下吞吐量最高提升40%，拒绝率显著降低。

Conclusion: 依赖感知调度和基于DAG的执行能显著增强Fabric的可扩展性，且与现有共识和智能合约层兼容。

Abstract: Hyperledger Fabric is a leading permissioned blockchain framework for
enterprise use, known for its modular design and privacy features. While it
strongly supports configurable consensus and access control, Fabric can face
challenges in achieving high transaction throughput and low rejection rates
under heavy workloads. These performance limitations are often attributed to
endorsement, ordering, and validation bottlenecks. Further, optimistic
concurrency control and deferred validation in Fabric may lead to resource
inefficiencies and contention, as conflicting transactions are identified only
during the commit phase. To address these challenges, we propose a
dependency-aware execution model for Hyperledger Fabric. Our approach includes:
(a) a dependency flagging system during endorsement, marking transactions as
independent or dependent using a hashmap; (b) an optimized block construction
in the ordering service that prioritizes independent transactions; (c) the
incorporation of a Directed Acyclic Graph (DAG) within each block to represent
dependencies; and (d) parallel execution of independent transactions at the
committer, with dependent transactions processed according to DAG order.
Incorporated in Hyperledger Fabric v2.5, our framework was tested on workloads
with varying dependency levels and system loads. Results show up to 40% higher
throughput and significantly reduced rejection rates in high-contention
scenarios. This demonstrates that dependency-aware scheduling and DAG-based
execution can substantially enhance Fabric's scalability while remaining
compatible with its existing consensus and smart contract layers.

</details>


### [49] [Astra: A Multi-Agent System for GPU Kernel Performance Optimization](https://arxiv.org/abs/2509.07506)
*Anjiang Wei,Tianran Sun,Yogesh Seenichamy,Hang Song,Anne Ouyang,Azalia Mirhoseini,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: 介绍了首个基于大语言模型的多智能体系统Astra用于GPU内核优化，从现有CUDA实现出发，实现平均1.32倍加速，凸显多智能体大语言模型系统是有前景的新范式。


<details>
  <summary>Details</summary>
Motivation: GPU内核优化对加速大语言模型训练和服务至关重要，但现有方法需大量手动调整，此前基于大语言模型的工作多关注将PyTorch模块转换为CUDA代码。

Method: 引入Astra系统，从SGLang提取的现有CUDA实现开始，通过专门的大语言模型智能体进行迭代代码生成、测试、分析和规划。

Result: 在SGLang内核上，使用OpenAI o4 - mini零样本提示实现平均1.32倍加速，案例研究表明大语言模型可自主应用循环变换等优化手段。

Conclusion: 多智能体大语言模型系统是GPU内核优化的有前景的新范式。

Abstract: GPU kernel optimization has long been a central challenge at the intersection
of high-performance computing and machine learning. Efficient kernels are
crucial for accelerating large language model (LLM) training and serving, yet
attaining high performance typically requires extensive manual tuning.
Compiler-based systems reduce some of this burden, but still demand substantial
manual design and engineering effort. Recently, researchers have explored using
LLMs for GPU kernel generation, though prior work has largely focused on
translating high-level PyTorch modules into CUDA code. In this work, we
introduce Astra, the first LLM-based multi-agent system for GPU kernel
optimization. Unlike previous approaches, Astra starts from existing CUDA
implementations extracted from SGLang, a widely deployed framework for serving
LLMs, rather than treating PyTorch modules as the specification. Within Astra,
specialized LLM agents collaborate through iterative code generation, testing,
profiling, and planning to produce kernels that are both correct and
high-performance. On kernels from SGLang, Astra achieves an average speedup of
1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study
further demonstrates that LLMs can autonomously apply loop transformations,
optimize memory access patterns, exploit CUDA intrinsics, and leverage fast
math operations to yield substantial performance gains. Our work highlights
multi-agent LLM systems as a promising new paradigm for GPU kernel
optimization.

</details>


### [50] [Navigating Energy Doldrums: Modeling the Impact of Energy Price Volatility on HPC Cost of Ownership](https://arxiv.org/abs/2509.07567)
*Peter Arzt,Felix Wolf*

Main category: cs.DC

TL;DR: 本文探讨可变容量策略管理HPC能源成本，提出简单模型评估其对TCO的影响并应用于实际数据。


<details>
  <summary>Details</summary>
Motivation: 能源成本是HPC系统TCO的主要因素，电力市场波动使能源预算复杂化，需探索管理能源成本的策略。

Method: 提出简单模型，利用关键系统参数估算可变容量策略对TCO的影响，并应用于大学HPC集群的实际数据。

Result: 对大学HPC集群实际数据应用模型，评估不同情景对该方法成本效益的影响。

Conclusion: 模型可帮助运营商评估可变容量策略在管理HPC能源成本时成本与硬件利用率的权衡。

Abstract: Energy costs are a major factor in the total cost of ownership (TCO) for
high-performance computing (HPC) systems. The rise of intermittent green energy
sources and reduced reliance on fossil fuels have introduced volatility into
electricity markets, complicating energy budgeting. This paper explores
variable capacity as a strategy for managing HPC energy costs - dynamically
adjusting compute resources in response to fluctuating electricity prices.
While this approach can lower energy expenses, it risks underutilizing costly
hardware. To evaluate this trade-off, we present a simple model that helps
operators estimate the TCO impact of variable capacity strategies using key
system parameters. We apply this model to real data from a university HPC
cluster and assess how different scenarios could affect the cost-effectiveness
of this approach in the future.

</details>


### [51] [AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with FaaS-hosted MCP Services](https://arxiv.org/abs/2509.07595)
*Shiva Sai Krishna Anand Tokal,Vaibhav Jha,Anand Eswaran,Praveen Jayachandran,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文提出新的代理工作流模式AgentX，结合MCP工具并提出两种部署MCP服务器的方法，对AgentX和其他两种模式进行评估，揭示代理工作流设计和部署的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统在处理多工具、复杂多步任务和长上下文管理时存在困难，需要更好的工作流模式。

Method: 定义AgentX工作流模式，结合MCP工具，提出两种部署MCP服务器的方法，通过实际应用对AgentX和其他两种模式进行评估。

Result: 评估了AgentX、ReAct和Magentic One的成功率、延迟和成本。

Conclusion: 强调了设计和部署代理工作流的机遇与挑战。

Abstract: Generative Artificial Intelligence (GenAI) has rapidly transformed various
fields including code generation, text summarization, image generation and so
on. Agentic AI is a recent evolution that further advances this by coupling the
decision making and generative capabilities of LLMs with actions that can be
performed using tools. While seemingly powerful, Agentic systems often struggle
when faced with numerous tools, complex multi-step tasks,and long-context
management to track history and avoid hallucinations. Workflow patterns such as
Chain-of-Thought (CoT) and ReAct help address this. Here, we define a novel
agentic workflow pattern, AgentX, composed of stage designer, planner, and
executor agents that is competitive or better than the state-of-the-art agentic
patterns. We also leverage Model Context Protocol (MCP) tools, and propose two
alternative approaches for deploying MCP servers as cloud Functions as a
Service (FaaS). We empirically evaluate the success rate, latency and cost for
AgentX and two contemporary agentic patterns, ReAct and Magentic One, using
these the FaaS and local MCP server alternatives for three practical
applications. This highlights the opportunities and challenges of designing and
deploying agentic workflows.

</details>


### [52] [Scaling atomic ordering in shared memory](https://arxiv.org/abs/2509.07781)
*Lorenzo Martignetti,Eliã Batista,Gianpaolo Cugola,Fernando Pedone*

Main category: cs.DC

TL;DR: 本文介绍了专为共享内存系统设计的原子多播协议TRAM，它采用覆盖树架构，性能优越，相比现有协议提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有适用于共享内存系统模型的原子多播协议较少，需要开发新的协议。

Method: 引入TRAM协议，利用覆盖树架构。

Result: TRAM协议性能出色，相比基于共享内存的最新协议，吞吐量提高超3倍，延迟降低超2.3倍；相比基于消息传递的协议，吞吐量最高提升5.9倍，延迟最多降低106倍。

Conclusion: TRAM协议设计简单实用，在共享内存系统中能带来卓越性能。

Abstract: Atomic multicast is a communication primitive used in dependable systems to
ensure consistent ordering of messages delivered to a set of replica groups.
This primitive enables critical services to integrate replication and sharding
(i.e., state partitioning) to achieve fault tolerance and scalability. While
several atomic multicast protocols have been developed for message-passing
systems, only a few are designed for the shared memory system model. This paper
introduces TRAM, an atomic multicast protocol specifically designed for shared
memory systems, leveraging an overlay tree architecture. Due to its simple and
practical design, TRAM delivers exceptional performance, increasing throughput
by more than 3$\times$ and reducing latency by more than 2.3$\times$ compared
to state-of-the-art shared memory-based protocols. Additionally, it
significantly outperforms message-passing-based protocols, boosting throughput
by up to 5.9$\times$ and reducing latency by up to 106$\times$.

</details>


### [53] [VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving](https://arxiv.org/abs/2509.04827)
*Jiahuan Yu,Aryan Taneja,Junfeng Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: 本文提出VoltanaLLM系统，通过频率缩放和请求路由协同设计实现节能LLM服务，评估显示能节能36.3%且保持高SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型推理能耗高，对可持续和低成本部署构成挑战。

Method: 从控制理论角度构建VoltanaLLM，协同设计新兴架构中的频率缩放和请求路由，包含反馈驱动的频率控制器和状态空间路由器。

Result: 在多个先进LLM和真实数据集上评估，VoltanaLLM节能达36.3%，并保持接近完美的SLO达成率。

Conclusion: VoltanaLLM为可持续和智能的LLM服务奠定了基础。

Abstract: Modern Large Language Model (LLM) serving systems increasingly support
interactive applications, like real-time chat assistants, code generation
tools, and agentic workflows. However, the soaring energy cost of LLM inference
presents a growing challenge for sustainable and cost-effective deployment.
This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM
serving, built from a control theory perspective. VoltanaLLM co-designs
frequency scaling and request routing in emerging prefill/decode disaggregated
architectures, leveraging their decoupled execution to enable fine-grained
phase-specific control. It consists of a feedback-driven frequency controller
that dynamically adapts GPU frequency for prefill and decode phases, and a
state-space router that explores routing decisions across frequency-scaled
instances to minimize energy under latency constraints. We implement VoltanaLLM
in SGLang and evaluate its performance over multiple state-of-the-art LLMs and
real-world datasets. The results demonstrate that VoltanaLLM achieves up to
36.3% energy savings while maintaining near-perfect SLO attainment rate, paving
the way for sustainable and intelligent LLM serving.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [54] [Optimal streaming algorithm for detecting $\ell_2$ heavy hitters in random order streams](https://arxiv.org/abs/2509.07286)
*Santhoshini Velusamy,Huacheng Yu*

Main category: cs.DS

TL;DR: 本文研究流数据中ℓ₂重元素检测的空间复杂度，证明在随机顺序流和部分随机顺序流中可达到最优空间界。


<details>
  <summary>Details</summary>
Motivation: 现有算法在某些情况下空间复杂度非最优，需探索在不同流顺序下达到最优空间界的方法。

Method: 针对随机顺序流和部分随机顺序流设计算法，证明可达到最优空间界O(1/ε log n)。

Result: 在随机顺序流中，对ε = Ω(1/2^√(log n))可达到最优空间界；在部分随机顺序流中，在给定F₂常数近似的前提下可达到相同空间界。

Conclusion: 在特定流顺序条件下，可实现ℓ₂重元素检测的最优空间复杂度。

Abstract: Given a stream $x_1,x_2,\dots,x_n$ of items from a Universe $U$ of size
$\mathsf{poly}(n)$, and a parameter $\epsilon>0$, an item $i\in U$ is said to
be an $\ell_2$ heavy hitter if its frequency $f_i$ in the stream is at least
$\sqrt{\epsilon F_2}$, where $F_2=\sqrt{\sum_{i\in U} f_i^2}$. The classical
$\mathsf{CountSketch}$ algorithm due to Charikar, Chen, and Farach-Colton
[2004], was the first algorithm to detect $\ell_2$ heavy hitters using
$O\left(\frac{\log^2 n}{\epsilon}\right)$ bits of space, and their algorithm is
optimal for streams with deletions. For insertion-only streams, Braverman,
Chestnut, Ivkin, Nelson, Wang, and Woodruff [2017] gave the $\mathsf{BPTree}$
algorithm which requires only $O\left(\frac{\log(1/\epsilon)}{\epsilon}\log n
\right)$ space. Note that any algorithm requires at least
$O\left(\frac{1}{\epsilon} \log n\right)$ space to output $O(1/\epsilon)$ heavy
hitters in the worst case. So for constant $\epsilon$, the space usage of the
$\mathsf{BPTree}$ algorithm is optimal but their bound could be sub-optimal for
$\epsilon=o(1)$. In this work, we show that for random order streams, where the
stream elements can be adversarial but their order of arrival is uniformly
random, it is possible to achieve the optimal space bound of
$O\left(\frac{1}{\epsilon} \log n\right)$ for every $\epsilon =
\Omega\left(\frac{1}{2^{\sqrt{\log n}}}\right)$. We also show that for
partially random order streams where only the heavy hitters are required to be
uniformly distributed in the stream, it is possible to achieve the same space
bound, but with an additional assumption that the algorithm is given a constant
approximation to $F_2$ in advance.

</details>


### [55] [Dimension Reduction for Clustering: The Curious Case of Discrete Centers](https://arxiv.org/abs/2509.07444)
*Shaofeng H. -C. Jiang,Robert Krauthgamer,Shay Sapir,Sandeep Silwal,Di Yue*

Main category: cs.DS

TL;DR: 本文探讨离散聚类中的降维问题，给出两个与数据集双倍维度相关的结果，实现强降维并指出与连续设置的差异。


<details>
  <summary>Details</summary>
Motivation: Johnson - Lindenstrauss变换在一般情况下的降维维度界是紧的，但在特定问题上可突破。此前聚类问题在连续设置上进展大，本文研究离散设置下的聚类降维。

Method: 对离散设置下的聚类降维进行探索，给出两个与数据集双倍维度相关的结果。

Result: 第一个结果表明Oₑ(ddim + log k + loglog n)维足以保证所有中心集成本在1±ε因子内，且该维度界是紧的；第二个结果通过放宽保证消除了loglog n项。

Conclusion: 在离散设置下实现强降维，且与连续设置不仅在维度界上不同，在保证上也有差异。

Abstract: The Johnson-Lindenstrauss transform is a fundamental method for dimension
reduction in Euclidean spaces, that can map any dataset of $n$ points into
dimension $O(\log n)$ with low distortion of their distances. This dimension
bound is tight in general, but one can bypass it for specific problems. Indeed,
tremendous progress has been made for clustering problems, especially in the
\emph{continuous} setting where centers can be picked from the ambient space
$\mathbb{R}^d$. Most notably, for $k$-median and $k$-means, the dimension bound
was improved to $O(\log k)$ [Makarychev, Makarychev and Razenshteyn, STOC
2019].
  We explore dimension reduction for clustering in the \emph{discrete} setting,
where centers can only be picked from the dataset, and present two results that
are both parameterized by the doubling dimension of the dataset, denoted as
$\operatorname{ddim}$. The first result shows that dimension
$O_{\epsilon}(\operatorname{ddim} + \log k + \log\log n)$ suffices, and is
moreover tight, to guarantee that the cost is preserved within factor
$1\pm\epsilon$ for every set of centers. Our second result eliminates the
$\log\log n$ term in the dimension through a relaxation of the guarantee
(namely, preserving the cost only for all approximately-optimal sets of
centers), which maintains its usefulness for downstream applications.
  Overall, we achieve strong dimension reduction in the discrete setting, and
find that it differs from the continuous setting not only in the dimension
bound, which depends on the doubling dimension, but also in the guarantees
beyond preserving the optimal value, such as which clusterings are preserved.

</details>


### [56] [The General Expiration Streaming Model: Diameter, $k$-Center, Counting, Sampling, and Friends](https://arxiv.org/abs/2509.07587)
*Lotte Blank,Sergio Cabello,MohammadTaghi Hajiaghayi,Robert Krauthgamer,Sepideh Mahabadi,André Nusser,Jeff M. Phillips,Jonas Sauer*

Main category: cs.DS

TL;DR: 本文引入新的过期模型，给出几个基础问题和直径、k - 中心问题的算法，拓展了滑动窗口流的相关结果。


<details>
  <summary>Details</summary>
Motivation: 聚焦数据流算法中流项仅在有限时间内活跃的场景，引入新的过期模型。

Method: 在过期流模型中，通过有效跟踪活跃项设计算法；针对直径和k - 中心问题，开发新的分解、协调技术和几何优势框架过滤冗余点。

Result: 得到几个基础问题（近似计数、均匀采样和加权采样）的算法；设计了直径和k - 中心问题的算法，拓展了滑动窗口流的结果，在高维欧氏空间直径问题上有更好近似因子。

Conclusion: 新过期模型及相应算法有效，拓展了已有滑动窗口流的研究成果。

Abstract: An important thread in the study of data-stream algorithms focuses on
settings where stream items are active only for a limited time. We introduce a
new expiration model, where each item arrives with its own expiration time. The
special case where items expire in the order that they arrive, which we call
consistent expirations, contains the classical sliding-window model of Datar,
Gionis, Indyk, and Motwani [SICOMP 2002] and its timestamp-based variant of
Braverman and Ostrovsky [FOCS 2007].
  Our first set of results presents algorithms (in the expiration streaming
model) for several fundamental problems, including approximate counting,
uniform sampling, and weighted sampling by efficiently tracking active items
without explicitly storing them all. Naturally, these algorithms have many
immediate applications to other problems.
  Our second and main set of results designs algorithms (in the expiration
streaming model) for the diameter and $k$-center problems, where items are
points in a metric space. Our results significantly extend those known for the
special case of sliding-window streams by Cohen-Addad, Schwiegelshohn, and
Sohler [ICALP 2016], including also a strictly better approximation factor for
the diameter in the important special case of high-dimensional Euclidean space.
We develop new decomposition and coordination techniques along with a geometric
dominance framework, to filter out redundant points based on both temporal and
spatial proximity.

</details>


### [57] [Proximity Graphs for Similarity Search: Fast Construction, Lower Bounds, and Euclidean Separation](https://arxiv.org/abs/2509.07732)
*Shangqi Lu,Yufei Tao*

Main category: cs.DS

TL;DR: 本文为基于邻近图的近似最近邻（ANN）搜索方法提供理论基础，给出构建算法，分析下界，还在欧几里得空间优化了图大小。


<details>
  <summary>Details</summary>
Motivation: 为基于邻近图的ANN搜索方法提供理论基础，改进现有构建方法的局限性。

Method: 提出构建邻近图算法，分析其边数、查询时间和构建时间，给出下界，在欧几里得空间利用几何特性优化图大小。

Result: 构建算法边数为$O((1/\epsilon)^\lambda \cdot n \log \Delta)$，查询时间为$(1/\epsilon)^\lambda \cdot \text{polylog }\Delta$，构建时间近线性于$n$；最坏情况下边数下界为$\Omega((1/\epsilon)^\lambda \cdot n + n \log \Delta)$；在欧几里得空间图大小降至$O((1/\epsilon)^\lambda \cdot n)$。

Conclusion: 为邻近图ANN搜索方法提供理论基础，算法有时间和空间上的优势，欧几里得空间可进一步优化图大小。

Abstract: Proximity graph-based methods have emerged as a leading paradigm for
approximate nearest neighbor (ANN) search in the system community. This paper
presents fresh insights into the theoretical foundation of these methods. We
describe an algorithm to build a proximity graph for $(1+\epsilon)$-ANN search
that has $O((1/\epsilon)^\lambda \cdot n \log \Delta)$ edges and guarantees
$(1/\epsilon)^\lambda \cdot \text{polylog }\Delta$ query time. Here, $n$ and
$\Delta$ are the size and aspect ratio of the data input, respectively, and
$\lambda = O(1)$ is the doubling dimension of the underlying metric space. Our
construction time is near-linear to $n$, improving the $\Omega(n^2)$ bounds of
all previous constructions. We complement our algorithm with lower bounds
revealing an inherent limitation of proximity graphs: the number of edges needs
to be at least $\Omega((1/\epsilon)^\lambda \cdot n + n \log \Delta)$ in the
worst case, up to a subpolynomial factor. The hard inputs used in our
lower-bound arguments are non-geometric, thus prompting the question of whether
improvement is possible in the Euclidean space (a key subclass of metric
spaces). We provide an affirmative answer by using geometry to reduce the graph
size to $O((1/\epsilon)^\lambda \cdot n)$ while preserving nearly the same
query and construction time.

</details>


### [58] [Tight Bounds for Low-Error Frequency Moment Estimation and the Power of Multiple Passes](https://arxiv.org/abs/2509.07599)
*Naomi Green-Maimon,Or Zamir*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Estimating the second frequency moment $F_2$ of a data stream up to a $(1 \pm
\varepsilon)$ factor is a central problem in the streaming literature. For
errors $\varepsilon > \Omega(1/\sqrt{n})$, the tight bound
$\Theta\left(\log(\varepsilon^2 n)/\varepsilon^2\right)$ was recently
established by Braverman and Zamir. In this work, we complete the picture by
resolving the remaining regime of small error, $\varepsilon < 1/\sqrt{n}$,
showing that the optimal space complexity is $\Theta\left( \min\left(n,
\frac{1}{\varepsilon^2} \right) \cdot \left(1 + \left| \log(\varepsilon^2 n)
\right| \right) \right)$ bits for all $\varepsilon \geq 1/n^2$, assuming a
sufficiently large universe. This closes the gap between the best known
$\Omega(n)$ lower bound and the straightforward $O(n \log n)$ upper bound in
that range, and shows that essentially storing the entire stream is necessary
for high-precision estimation.
  To derive this bound, we fully characterize the two-party communication
complexity of estimating the size of a set intersection up to an arbitrary
additive error $\varepsilon n$. In particular, we prove a tight $\Omega(n \log
n)$ lower bound for one-way communication protocols when $\varepsilon <
n^{-1/2-\Omega(1)}$, in contrast to classical $O(n)$-bit protocols that use
two-way communication. Motivated by this separation, we present a two-pass
streaming algorithm that computes the exact histogram of a stream with high
probability using only $O(n \log \log n)$ bits of space, in contrast to the
$\Theta(n \log n)$ bits required in one pass even to approximate $F_2$ with
small error. This yields the first asymptotic separation between one-pass and
$O(1)$-passes space complexity for small frequency moment estimation.

</details>


### [59] [Compressibility Measures and Succinct Data Structures for Piecewise Linear Approximations](https://arxiv.org/abs/2509.07827)
*Paolo Ferragina,Filippo Lari*

Main category: cs.DS

TL;DR: 研究分段线性近似（PLAs）的可压缩性度量问题，给出存储成本下界，设计达下界的数据结构并提供高效检索操作，提供理论分析和存储方案。


<details>
  <summary>Details</summary>
Motivation: PLAs广泛用于学习型数据结构，需对其可压缩性进行理论分析。

Method: 引入两种设置下存储PLAs成本的下界，与已知数据结构比较，设计达下界的数据结构。

Result: 数据结构能高效检索和评估PLA中的线段，渐近最优，多数实际情况简洁。

Conclusion: 对基于PLA的学习型数据结构可压缩性进行首次理论分析，提供有理论保障的存储方案。

Abstract: We study the problem of deriving compressibility measures for \emph{Piecewise
Linear Approximations} (PLAs), i.e., error-bounded approximations of a set of
two-dimensional {\em increasing} data points using a sequence of segments. Such
approximations are widely used tools in implementing many \emph{learned data
structures}, which mix learning models with traditional algorithmic design
blocks to exploit regularities in the underlying data distribution, providing
novel and effective space-time trade-offs.
  We introduce the first lower bounds to the cost of storing PLAs in two
settings, namely {\em compression} and {\em indexing}. We then compare these
compressibility measures to known data structures, and show that they are
asymptotically optimal up to a constant factor from the space lower bounds.
Finally, we design the first data structures for the aforementioned settings
that achieve the space lower bounds plus small additive terms, which turn out
to be {\em succinct} in most practical cases. Our data structures support the
efficient retrieval and evaluation of a segment in the (compressed) PLA for a
given $x$-value, which is a core operation in any learned data structure
relying on PLAs.
  As a result, our paper offers the first theoretical analysis of the maximum
compressibility achievable by PLA-based learned data structures, and provides
novel storage schemes for PLAs offering strong theoretical guarantees while
also suggesting simple and efficient practical implementations.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [60] [Persuading Agents in Opinion Formation Games](https://arxiv.org/abs/2509.07520)
*Martin Hoefer,Tim Koglin,Tolga Tel*

Main category: cs.GT

TL;DR: 本文分析了Friedkin - Johnsen模型中的贝叶斯说服方法，提出信息披露算法优化均衡，给出不同情况下算法，分析目标近似复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有意见形成模型主要关注同伴压力影响，实际意见形成还基于世界状态信息和说服努力，所以分析贝叶斯说服方法。

Method: 提出发送者信息披露算法，针对不同类型目标设计算法，分析算法复杂度。

Result: 对许多自然发送者目标有简单最优策略；针对基于范围目标在多种情况下提供高效算法；给出子加性和加性目标的近似复杂度。

Conclusion: 该方法能优化意见形成均衡，不同目标有不同算法和近似复杂度。

Abstract: Prominent opinion formation models such as the one by Friedkin and Johnsen
(FJ) concentrate on the effects of peer pressure on public opinions. In
practice, opinion formation is also based on information about the state of the
world and persuasion efforts. In this paper, we analyze an approach of Bayesian
persuasion in the FJ model. There is an unknown state of the world that
influences the preconceptions of n agents. A sender S can (partially) reveal
information about the state to all agents. The agents update their
preconceptions, and an equilibrium of public opinions emerges. We propose
algorithms for the sender to reveal information in order to optimize various
aspects of the emerging equilibrium. For many natural sender objectives, we
show that there are simple optimal strategies. We then focus on a general class
of range-based objectives with desired opinion ranges for each agent. We
provide efficient algorithms in several cases, e.g., when the matrix of
preconceptions in all states has constant rank, or when there is only a
polynomial number of range combinations that lead to positive value for S. This
generalizes, e.g., instances with a constant number of states and/or agents, or
instances with a logarithmic number of ranges. In general, we show that
subadditive range-based objectives allow a simple n-approximation, and even for
additive ones, obtaining an $n^{1-c}$-approximation is NP-hard, for any
constant $c > 0$.

</details>


### [61] [City Sampling for Citizens' Assemblies](https://arxiv.org/abs/2509.07557)
*Paul Gölz,Jan Maly,Ulrike Schmidt-Kraepelin,Markus Utke,Philipp C. Verpoort*

Main category: cs.GT

TL;DR: 研究公民大会两阶段抽样问题，开发多算法并在德国数据上评估，计划与非营利组织合作部署。


<details>
  <summary>Details</summary>
Motivation: 解决德国等地公民大会从业者面临的两阶段抽样问题，在市政层面存储选民信息下，实现事前选民等概率选择和事后限制联系城市数量。

Method: 针对问题开发多种算法，包括基于线性规划分离预言机的伪多项式时间算法和1 - 近似算法、贪心算法、基于列生成和整数线性规划的最优算法及简单启发式算法。

Result: 对算法在德国数据上进行评估。

Conclusion: 开发的多种算法可用于解决公民大会两阶段抽样问题，计划与非营利组织合作部署算法。

Abstract: In citizens' assemblies, a group of constituents is randomly selected to
weigh in on policy issues. We study a two-stage sampling problem faced by
practitioners in countries such as Germany, in which constituents' contact
information is stored at a municipal level. As a result, practitioners can only
select constituents from a bounded number of cities ex post, while ensuring
equal selection probability for constituents ex ante.
  We develop several algorithms for this problem. Although minimizing the
number of contacted cities is NP-hard, we provide a pseudo-polynomial time
algorithm and an additive 1-approximation, both based on separation oracles for
a linear programming formulation. Recognizing that practical objectives go
beyond minimizing city count, we further introduce a simple and more
interpretable greedy algorithm, which additionally satisfies an ex-post
monotonicity property and achieves an additive 2-approximation. Finally, we
explore a notion of ex-post proportionality, for which we propose two practical
algorithms: an optimal algorithm based on column generation and integer linear
programming and a simple heuristic creating particularly transparent
distributions. We evaluate these algorithms on data from Germany, and plan to
deploy them in cooperation with a leading nonprofit organization in this space.

</details>


### [62] [Inference of Intrinsic Rewards and Fairness in Multi-Agent Systems](https://arxiv.org/abs/2509.07650)
*Victor Villin,Christos Dimitrakakis*

Main category: cs.GT

TL;DR: 将了解他人公平性挑战转为多智能体逆强化学习问题，提出贝叶斯策略，实验表明可从演示中推断公平性。


<details>
  <summary>Details</summary>
Motivation: 在不明确他人偏好情况下理解其公平性。

Method: 将挑战建模为多智能体逆强化学习问题，引入贝叶斯策略。

Result: 可从演示中可靠推断公平性概念，分离公平组件能理解智能体偏好，分组可展现奖励结构新方面。

Conclusion: 能解决谁更公平这一核心问题。

Abstract: From altruism to antagonism, fairness plays a central role in social
interactions. But can we truly understand how fair someone is, especially
without explicit knowledge of their preferences? We cast this challenge as a
multi-agent inverse reinforcement learning problem, explicitly structuring
rewards to reflect how agents value the welfare of others. We introduce novel
Bayesian strategies, reasoning about the optimality of demonstrations and
characterisation of equilibria in general-sum Markov games. Our experiments,
spanning randomised environments and a collaborative cooking task, reveal that
coherent notions of fairness can be reliably inferred from demonstrations.
Furthermore, when isolating fairness components, we obtain a disentangled
understanding of agents preferences. Crucially, we unveil that by placing
agents in different groups, we can force them to exhibit new facets of their
reward structures, cutting through ambiguity to answer the central question:
who is being fair?

</details>


### [63] [Smart Fast Finish: Preventing Overdelivery via Daily Budget Pacing at DoorDash](https://arxiv.org/abs/2509.07929)
*Rohan Garg,Yongjin Xiao,Jason,Yang,Mandar Rahurkar*

Main category: cs.GT

TL;DR: 提出名为Smart Fast Finish (SFF)的预算调控功能，基于FF功能，动态更新参数，在DoorDash应用，实验证明可缓解过度投放问题。


<details>
  <summary>Details</summary>
Motivation: 解决预算调控中过度投放的问题。

Method: 在行业标准Fast Finish (FF)功能基础上，根据历史广告活动数据动态更新系统参数，如开始时间和节流率。

Result: 通过在线预算拆分实验数据和离线模拟，证明SFF是缓解过度投放的可靠解决方案。

Conclusion: SFF是预算调控中缓解过度投放的有效功能。

Abstract: We present a budget pacing feature called Smart Fast Finish (SFF). SFF builds
upon the industry standard Fast Finish (FF) feature in budget pacing systems
that depletes remaining advertising budget as quickly as possible towards the
end of some fixed time period. SFF dynamically updates system parameters such
as start time and throttle rate depending on historical ad-campaign data. SFF
is currently in use at DoorDash, one of the largest delivery platforms in the
US, and is part of its budget pacing system. We show via online budget-split
experimentation data and offline simulations that SFF is a robust solution for
overdelivery mitigation when pacing budget.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [64] [Avoiding Over-Personalization with Rule-Guided Knowledge Graph Adaptation for LLM Recommendations](https://arxiv.org/abs/2509.07133)
*Fernando Spadea,Oshani Seneviratne*

Main category: cs.IR

TL;DR: 提出轻量级神经符号框架，在推理时调整用户侧知识图谱，缓解大语言模型推荐系统的过度个性化问题，实验显示该方法有效。


<details>
  <summary>Details</summary>
Motivation: 缓解大语言模型推荐系统中的过度个性化问题，打破算法诱导的过滤气泡，增加内容多样性。

Method: 在推理时重构用户的个性化知识图谱，抑制强化个性化信息环境的特征共现模式，引入符号适应策略和客户端学习算法。

Result: 在食谱推荐基准测试中，个性化的知识图谱调整显著增加了内容新颖性，同时保持了推荐质量，优于全局调整和基于简单提示的方法。

Conclusion: 提出的方法在缓解过度个性化问题上有效，能在保持推荐相关性的同时提高内容多样性。

Abstract: We present a lightweight neuro-symbolic framework to mitigate
over-personalization in LLM-based recommender systems by adapting user-side
Knowledge Graphs (KGs) at inference time. Instead of retraining models or
relying on opaque heuristics, our method restructures a user's Personalized
Knowledge Graph (PKG) to suppress feature co-occurrence patterns that reinforce
Personalized Information Environments (PIEs), i.e., algorithmically induced
filter bubbles that constrain content diversity. These adapted PKGs are used to
construct structured prompts that steer the language model toward more diverse,
Out-PIE recommendations while preserving topical relevance. We introduce a
family of symbolic adaptation strategies, including soft reweighting, hard
inversion, and targeted removal of biased triples, and a client-side learning
algorithm that optimizes their application per user. Experiments on a recipe
recommendation benchmark show that personalized PKG adaptations significantly
increase content novelty while maintaining recommendation quality,
outperforming global adaptation and naive prompt-based methods.

</details>


### [65] [Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning Intensive Retrieval](https://arxiv.org/abs/2509.07163)
*Haike Xu,Tong Chen*

Main category: cs.IR

TL;DR: 本文提出Reranker - Guided - Search (RGS)方法解决检索重排流程局限，实验显示在多基准测试有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有检索重排流程受初始检索质量和基于大语言模型重排器计算需求的限制。

Method: 采用RGS方法，通过近似最近邻算法生成的邻近图进行贪心搜索，基于文档相似度优先选择有潜力的文档进行重排。

Result: 在BRIGHT、FollowIR和M - BEIR基准测试中分别提升3.5、2.9和5.1分，重排预算为100个文档。

Conclusion: 在固定嵌入和重排器模型下，策略性选择文档重排可在有限重排预算下显著提高检索准确性。

Abstract: The widely used retrieve-and-rerank pipeline faces two critical limitations:
they are constrained by the initial retrieval quality of the top-k documents,
and the growing computational demands of LLM-based rerankers restrict the
number of documents that can be effectively processed. We introduce
Reranker-Guided-Search (RGS), a novel approach that bypasses these limitations
by directly retrieving documents according to reranker preferences rather than
following the traditional sequential reranking method. Our method uses a greedy
search on proximity graphs generated by approximate nearest neighbor
algorithms, strategically prioritizing promising documents for reranking based
on document similarity. Experimental results demonstrate substantial
performance improvements across multiple benchmarks: 3.5 points on BRIGHT, 2.9
on FollowIR, and 5.1 on M-BEIR, all within a constrained reranker budget of 100
documents. Our analysis suggests that, given a fixed pair of embedding and
reranker models, strategically selecting documents to rerank can significantly
improve retrieval accuracy under limited reranker budget.

</details>


### [66] [Benchmarking Information Retrieval Models on Complex Retrieval Tasks](https://arxiv.org/abs/2509.07253)
*Julian Killingback,Hamed Zamani*

Main category: cs.IR

TL;DR: 构建复杂检索任务集对检索模型进行基准测试，发现最佳模型检索效果不佳，LLM增强对最强模型有负面影响。


<details>
  <summary>Details</summary>
Motivation: 现有评估资源有限，难以评估检索模型在复杂真实检索任务中的能力，为推动下一代检索模型创新。

Method: 构建多样化、真实的复杂检索任务集，对代表性的先进检索模型进行基准测试，探索基于LLM的查询扩展和重写对检索质量的影响。

Result: 最佳模型平均nDCG@10仅0.346，R@100仅0.587，LLM增强对较弱模型有帮助，但最强模型在所有重写技术下各指标性能下降。

Conclusion: 当前检索模型在复杂检索任务中表现不佳，LLM增强并非对所有模型都有益。

Abstract: Large language models (LLMs) are incredible and versatile tools for
text-based tasks that have enabled countless, previously unimaginable,
applications. Retrieval models, in contrast, have not yet seen such capable
general-purpose models emerge. To achieve this goal, retrieval models must be
able to perform complex retrieval tasks, where queries contain multiple parts,
constraints, or requirements in natural language. These tasks represent a
natural progression from the simple, single-aspect queries that are used in the
vast majority of existing, commonly used evaluation sets. Complex queries
naturally arise as people expect search systems to handle more specific and
often ambitious information requests, as is demonstrated by how people use
LLM-based information systems. Despite the growing desire for retrieval models
to expand their capabilities in complex retrieval tasks, there exist limited
resources to assess the ability of retrieval models on a comprehensive set of
diverse complex tasks. The few resources that do exist feature a limited scope
and often lack realistic settings making it hard to know the true capabilities
of retrieval models on complex real-world retrieval tasks. To address this
shortcoming and spur innovation in next-generation retrieval models, we
construct a diverse and realistic set of complex retrieval tasks and benchmark
a representative set of state-of-the-art retrieval models. Additionally, we
explore the impact of LLM-based query expansion and rewriting on retrieval
quality. Our results show that even the best models struggle to produce
high-quality retrieval results with the highest average nDCG@10 of only 0.346
and R@100 of only 0.587 across all tasks. Although LLM augmentation can help
weaker models, the strongest model has decreased performance across all metrics
with all rewriting techniques.

</details>


### [67] [Datasets for Navigating Sensitive Topics in Recommendation Systems](https://arxiv.org/abs/2509.07269)
*Amelia Kovacs,Jerry Chee,Kimia Kazemian,Sarah Dean*

Main category: cs.IR

TL;DR: 个性化AI系统存在不良影响，本文引入两个含敏感标签和用户内容评分的新数据集。


<details>
  <summary>Details</summary>
Motivation: 解决个性化AI系统可能让用户接触敏感有害内容的问题，定量评估系统。

Method: 引入两个新数据集，一个整合MovieLens评分数据和Does the Dog Die?网站的内容警告，另一个结合同人小说互动数据和Archive of Our Own的用户生成警告。

Result: 成功引入两个含敏感标签和用户内容评分的新数据集。

Conclusion: 通过创建含相关敏感标签的数据集，可让研究人员超越参与度指标评估个性化系统。

Abstract: Personalized AI systems, from recommendation systems to chatbots, are a
prevalent method for distributing content to users based on their learned
preferences. However, there is growing concern about the adverse effects of
these systems, including their potential tendency to expose users to sensitive
or harmful material, negatively impacting overall well-being. To address this
concern quantitatively, it is necessary to create datasets with relevant
sensitivity labels for content, enabling researchers to evaluate personalized
systems beyond mere engagement metrics. To this end, we introduce two novel
datasets that include a taxonomy of sensitivity labels alongside user-content
ratings: one that integrates MovieLens rating data with content warnings from
the Does the Dog Die? community ratings website, and another that combines
fan-fiction interaction data and user-generated warnings from Archive of Our
Own.

</details>


### [68] [MEGG: Replay via Maximally Extreme GGscore in Incremental Learning for Neural Recommendation Models](https://arxiv.org/abs/2509.07319)
*Yunxiao Shi,Shuo Yang,Haimin Zhang,Li Wang,Yongze Wang,Qiang Wu,Min Xu*

Main category: cs.IR

TL;DR: 提出MEGG增量学习框架解决神经协同过滤模型在动态环境的应用问题，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 神经协同过滤模型在静态设置下训练，难以适应动态环境，现有增量学习方法在推荐任务有局限。

Method: 提出MEGG，基于经验回放，引入GGscore量化样本影响，选择性回放高影响样本，且模型无关。

Result: 在三个神经模型和四个基准数据集上实验，性能优于现有基线，有强可扩展性、效率和鲁棒性。

Conclusion: MEGG能有效解决神经协同过滤模型在动态环境的应用问题，且具有良好性能。

Abstract: Neural Collaborative Filtering models are widely used in recommender systems
but are typically trained under static settings, assuming fixed data
distributions. This limits their applicability in dynamic environments where
user preferences evolve. Incremental learning offers a promising solution, yet
conventional methods from computer vision or NLP face challenges in
recommendation tasks due to data sparsity and distinct task paradigms. Existing
approaches for neural recommenders remain limited and often lack
generalizability. To address this, we propose MEGG, Replay Samples with
Maximally Extreme GGscore, an experience replay based incremental learning
framework. MEGG introduces GGscore, a novel metric that quantifies sample
influence, enabling the selective replay of highly influential samples to
mitigate catastrophic forgetting. Being model-agnostic, MEGG integrates
seamlessly across architectures and frameworks. Experiments on three neural
models and four benchmark datasets show superior performance over
state-of-the-art baselines, with strong scalability, efficiency, and
robustness. Implementation will be released publicly upon acceptance.

</details>


### [69] [Multi-view-guided Passage Reranking with Large Language Models](https://arxiv.org/abs/2509.07485)
*Jeongwoo Na,Jun Kwon,Eunseong Choi,Jongwuk Lee*

Main category: cs.IR

TL;DR: 现有大语言模型在段落重排序任务中有效率和外部偏差敏感问题，提出MVP模型解决，实验显示其性能优且推理延迟低。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于大语言模型的段落重排序方法在效率和对外部偏差敏感性方面的挑战。

Method: 引入非生成式的基于大语言模型的重排序方法MVP，将查询 - 段落信息编码为不同视图嵌入，结合查询感知段落嵌入生成锚向量计算相关性分数，采用正交损失使视图更独特。

Result: 220M参数的MVP性能与7B规模微调模型相当，推理延迟降低100倍；3B参数的MVP在内外领域基准测试中达最优。

Conclusion: MVP模型有效解决了现有方法的局限性，在性能和效率上表现出色。

Abstract: Recent advances in large language models (LLMs) have shown impressive
performance in passage reranking tasks. Despite their success, LLM-based
methods still face challenges in efficiency and sensitivity to external biases.
(1) Existing models rely mostly on autoregressive generation and sliding window
strategies to rank passages, which incur heavy computational overhead as the
number of passages increases. (2) External biases, such as position or
selection bias, hinder the model's ability to accurately represent passages and
increase input-order sensitivity. To address these limitations, we introduce a
novel passage reranking model, called Multi-View-guided Passage Reranking
(MVP). MVP is a non-generative LLM-based reranking method that encodes
query-passage information into diverse view embeddings without being influenced
by external biases. For each view, it combines query-aware passage embeddings
to produce a distinct anchor vector, which is then used to directly compute
relevance scores in a single decoding step. In addition, it employs an
orthogonal loss to make the views more distinctive. Extensive experiments
demonstrate that MVP, with just 220M parameters, matches the performance of
much larger 7B-scale fine-tuned models while achieving a 100x reduction in
inference latency. Notably, the 3B-parameter variant of MVP achieves
state-of-the-art performance on both in-domain and out-of-domain benchmarks.
The source code is available at: https://github.com/bulbna/MVP

</details>


### [70] [FLeW: Facet-Level and Adaptive Weighted Representation Learning of Scientific Documents](https://arxiv.org/abs/2509.07531)
*Zheng Dou,Deqing Wang,Fuzhen Zhuang,Jian Ren,Yanlin Hu*

Main category: cs.IR

TL;DR: 现有科学文档表示学习方法有局限，提出FLeW方法统一三种途径实现更好表示，实验显示其在多任务和领域的适用性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前科学文档表示学习的三种方法存在问题，如对比训练利用引用信息不足、细粒度学习集成成本高且缺乏泛化性、任务感知学习依赖手动分类等，需新方法解决。

Method: 提出FLeW方法，引入新的三元组采样方法增强引用结构信号，利用引用意图进行细粒度表示学习的方面划分，采用简单权重搜索自适应集成方面级嵌入得到任务特定文档嵌入。

Result: 实验表明FLeW在多个科学任务和领域中比先前模型更具适用性和鲁棒性。

Conclusion: FLeW方法能有效解决现有科学文档表示学习方法的问题，在多任务和领域表现良好。

Abstract: Scientific document representation learning provides powerful embeddings for
various tasks, while current methods face challenges across three approaches.
1) Contrastive training with citation-structural signals underutilizes citation
information and still generates single-vector representations. 2) Fine-grained
representation learning, which generates multiple vectors at the sentence or
aspect level, requires costly integration and lacks domain generalization. 3)
Task-aware learning depends on manually predefined task categorization,
overlooking nuanced task distinctions and requiring extra training data for
task-specific modules. To address these problems, we propose a new method that
unifies the three approaches for better representations, namely FLeW.
Specifically, we introduce a novel triplet sampling method that leverages
citation intent and frequency to enhance citation-structural signals for
training. Citation intents (background, method, result), aligned with the
general structure of scientific writing, facilitate a domain-generalized facet
partition for fine-grained representation learning. Then, we adopt a simple
weight search to adaptively integrate three facet-level embeddings into a
task-specific document embedding without task-aware fine-tuning. Experiments
show the applicability and robustness of FLeW across multiple scientific tasks
and fields, compared to prior models.

</details>


### [71] [ELEC: Efficient Large Language Model-Empowered Click-Through Rate Prediction](https://arxiv.org/abs/2509.07594)
*Rui Dong,Wentao Ouyang,Xiangzheng Liu*

Main category: cs.IR

TL;DR: 本文提出ELEC框架结合传统CTR预测模型和大语言模型优势，通过伪孪生网络和知识蒸馏实现高效CTR预测，实验证明其有效性与效率。


<details>
  <summary>Details</summary>
Motivation: 传统CTR预测模型丢失文本语义，大语言模型难以捕捉协作信号且推理延迟长，需结合二者优势实现协作、语义和效率。

Method: 提出ELEC框架，先适配大语言模型用于CTR预测，用伪孪生网络（增益网络和普通网络），将大语言模型生成的高维向量注入协作CTR模型形成增益网络，再将增益网络知识从分数和表征层面蒸馏到普通网络。

Result: 在真实数据集上实验，证明ELEC用于CTR预测的有效性和效率。

Conclusion: ELEC框架是模型无关的，可与多种现有大语言模型和协作CTR模型集成，有效实现CTR预测。

Abstract: Click-through rate (CTR) prediction plays an important role in online
advertising systems. On the one hand, traditional CTR prediction models capture
the collaborative signals in tabular data via feature interaction modeling, but
they lose semantics in text. On the other hand, Large Language Models (LLMs)
excel in understanding the context and meaning behind text, but they face
challenges in capturing collaborative signals and they have long inference
latency. In this paper, we aim to leverage the benefits of both types of models
and pursue collaboration, semantics and efficiency. We present ELEC, which is
an Efficient LLM-Empowered CTR prediction framework. We first adapt an LLM for
the CTR prediction task. In order to leverage the ability of the LLM but
simultaneously keep efficiency, we utilize the pseudo-siamese network which
contains a gain network and a vanilla network. We inject the high-level
representation vector generated by the LLM into a collaborative CTR model to
form the gain network such that it can take advantage of both tabular modeling
and textual modeling. However, its reliance on the LLM limits its efficiency.
We then distill the knowledge from the gain network to the vanilla network on
both the score level and the representation level, such that the vanilla
network takes only tabular data as input, but can still generate comparable
performance as the gain network. Our approach is model-agnostic. It allows for
the integration with various existing LLMs and collaborative CTR models.
Experiments on real-world datasets demonstrate the effectiveness and efficiency
of ELEC for CTR prediction.

</details>


### [72] [Towards End-to-End Model-Agnostic Explanations for RAG Systems](https://arxiv.org/abs/2509.07620)
*Viju Sudhi,Sinchana Ramakanth Bhat,Max Rudat,Roman Teucher,Nicolas Flores-Herr*

Main category: cs.IR

TL;DR: 本文提出新颖框架解释RAG系统的检索和生成过程，并评估解释，旨在推动构建可靠可解释的RAG系统。


<details>
  <summary>Details</summary>
Motivation: RAG系统在可信度和可解释性方面存在问题，需要解决。

Method: 提出基于扰动技术的模型无关的事后解释框架，提出不同策略评估解释。

Result: 未提及具体结果。

Conclusion: 期望推动构建可靠且可解释的RAG系统的协作。

Abstract: Retrieval Augmented Generation (RAG) systems, despite their growing
popularity for enhancing model response reliability, often struggle with
trustworthiness and explainability. In this work, we present a novel, holistic,
model-agnostic, post-hoc explanation framework leveraging perturbation-based
techniques to explain the retrieval and generation processes in a RAG system.
We propose different strategies to evaluate these explanations and discuss the
sufficiency of model-agnostic explanations in RAG systems. With this work, we
further aim to catalyze a collaborative effort to build reliable and
explainable RAG systems.

</details>


### [73] [A Survey of Long-Document Retrieval in the PLM and LLM Era](https://arxiv.org/abs/2509.07759)
*Minghan Li,Miyang Luo,Tianrui Lv,Yishuai Zhang,Siqi Zhao,Ercong Nie,Guodong Zhou*

Main category: cs.IR

TL;DR: 本文是长文档检索（LDR）的全面综述，涵盖三个时代的方法、挑战和应用，为基础模型时代的LDR发展提供参考和展望。


<details>
  <summary>Details</summary>
Motivation: 长文档的激增对信息检索提出挑战，标准段落级技术难以应对，需要专门方法。

Method: 系统梳理从经典词法、早期神经模型到现代预训练模型和大语言模型的演变，介绍关键范式。

Result: 回顾了特定领域应用、专业评估资源，指出效率权衡、多模态对齐和忠实性等关键开放挑战。

Conclusion: 为基础模型时代推进长文档检索提供综合参考和前瞻性议程。

Abstract: The proliferation of long-form documents presents a fundamental challenge to
information retrieval (IR), as their length, dispersed evidence, and complex
structures demand specialized methods beyond standard passage-level techniques.
This survey provides the first comprehensive treatment of long-document
retrieval (LDR), consolidating methods, challenges, and applications across
three major eras. We systematize the evolution from classical lexical and early
neural models to modern pre-trained (PLM) and large language models (LLMs),
covering key paradigms like passage aggregation, hierarchical encoding,
efficient attention, and the latest LLM-driven re-ranking and retrieval
techniques. Beyond the models, we review domain-specific applications,
specialized evaluation resources, and outline critical open challenges such as
efficiency trade-offs, multimodal alignment, and faithfulness. This survey aims
to provide both a consolidated reference and a forward-looking agenda for
advancing long-document retrieval in the era of foundation models.

</details>


### [74] [Query Expansion in the Age of Pre-trained and Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2509.07794)
*Minghan Li,Xinxuan Lv,Junjie Zou,Tongna Chen,Chao Zhang,Suchao An,Ercong Nie,Guodong Zhou*

Main category: cs.IR

TL;DR: 本文对基于预训练和大语言模型的查询扩展进行调查，从多角度综合该领域，比较方法并给出应用映射和应对问题的方法，最后提出未来研究议程。


<details>
  <summary>Details</summary>
Motivation: 现代信息检索需解决短而模糊查询与多样语料的词汇不匹配问题，预训练和大语言模型使查询扩展设计空间改变，需进行综合研究。

Method: 从四维框架、以模型为中心的分类法和面向实践的指导三个角度综合该领域，比较传统与基于预训练/大语言模型的方法。

Result: 对比了传统和基于预训练/大语言模型的查询扩展方法，映射了应用场景，提出解决主题漂移和幻觉的方法。

Conclusion: 给出了在现实约束下选择和组合查询扩展技术的原则性蓝图，提出质量控制、成本感知调用等未来研究议程。

Abstract: Modern information retrieval (IR) must bridge short, ambiguous queries and
ever more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key
mechanism for mitigating vocabulary mismatch, but the design space has shifted
markedly with pre-trained language models (PLMs) and large language models
(LLMs). This survey synthesizes the field from three angles: (i) a
four-dimensional framework of query expansion - from the point of injection
(explicit vs. implicit QE), through grounding and interaction (knowledge bases,
model-internal capabilities, multi-turn retrieval) and learning alignment, to
knowledge graph-based argumentation; (ii) a model-centric taxonomy spanning
encoder-only, encoder-decoder, decoder-only, instruction-tuned, and
domain/multilingual variants, highlighting their characteristic affordances for
QE (contextual disambiguation, controllable generation, zero-/few-shot
reasoning); and (iii) practice-oriented guidance on where and how neural QE
helps in first-stage retrieval, multi-query fusion, re-ranking, and
retrieval-augmented generation (RAG). We compare traditional query expansion
with PLM/LLM-based methods across seven key aspects, and we map applications
across web search, biomedicine, e-commerce, open-domain QA/RAG, conversational
and code search, and cross-lingual settings. The review distills design
grounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG
constraints - as robust remedies to topic drift and hallucination. We conclude
with an agenda on quality control, cost-aware invocation, domain/temporal
adaptation, evaluation beyond end-task metrics, and fairness/privacy.
Collectively, these insights provide a principled blueprint for selecting and
combining QE techniques under real-world constraints.

</details>


### [75] [KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis](https://arxiv.org/abs/2509.07860)
*Guanzhi Deng,Yi Xie,Yu-Keung Ng,Mingyang Liu,Peijun Zheng,Jie Liu,Dapeng Wu,Yinqiao Li,Linqi Song*

Main category: cs.IR

TL;DR: 传统专利分析方法有局限，本文提出 KLIPA 框架，经实验验证有效，能助力知识产权管理。


<details>
  <summary>Details</summary>
Motivation: 传统专利分析方法依赖人工搜索和关键词匹配，效率低且难以揭示复杂关系，阻碍战略决策，需新方法。

Method: 引入 KLIPA 框架，整合结构化知识图谱、检索增强生成系统和智能代理三个关键组件。

Result: 在真实专利数据库验证，KLIPA 在知识提取、发现新联系和整体运营效率上有显著提升。

Conclusion: KLIPA 组合技术提高检索准确性、减少对领域专家依赖，为管理知识产权组织提供可扩展自动化解决方案。

Abstract: Effectively managing intellectual property is a significant challenge.
Traditional methods for patent analysis depend on labor-intensive manual
searches and rigid keyword matching. These approaches are often inefficient and
struggle to reveal the complex relationships hidden within large patent
datasets, hindering strategic decision-making. To overcome these limitations,
we introduce KLIPA, a novel framework that leverages a knowledge graph and a
large language model (LLM) to significantly advance patent analysis. Our
approach integrates three key components: a structured knowledge graph to map
explicit relationships between patents, a retrieval-augmented generation(RAG)
system to uncover contextual connections, and an intelligent agent that
dynamically determines the optimal strategy for resolving user queries. We
validated KLIPA on a comprehensive, real-world patent database, where it
demonstrated substantial improvements in knowledge extraction, discovery of
novel connections, and overall operational efficiency. This combination of
technologies enhances retrieval accuracy, reduces reliance on domain experts,
and provides a scalable, automated solution for any organization managing
intellectual property, including technology corporations and legal firms,
allowing them to better navigate the complexities of strategic innovation and
competitive intelligence.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model](https://arxiv.org/abs/2509.06974)
*Xueyi Wang,Elisabeth Wilhelm*

Main category: cs.LG

TL;DR: 本文提出用于预测睡眠质量分数的两阶段自适应时空模型，实验表明其优于基线方法，有良好实用性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 睡眠质量影响幸福感，需可靠预测工具进行预防性干预。

Method: 提出两阶段自适应时空模型，结合多尺度卷积层、循环层、注意力机制及两阶段领域自适应策略，分训练和测试两阶段减少过拟合和适应新用户。

Result: 模型在不同输入和预测窗口实验中均优于基线方法，3 天输入 1 天预测窗口 RMSE 为 0.216，3 天预测窗口 RMSE 为 0.257。

Conclusion: 所提框架为利用商业可穿戴设备稀疏数据进行个性化睡眠预测提供了可靠、自适应且可解释的解决方案。

Abstract: Sleep quality significantly impacts well-being. Therefore, healthcare
providers and individuals need accessible and reliable forecasting tools for
preventive interventions. This paper introduces an interpretable,
individualized two-stage adaptive spatial-temporal model for predicting sleep
quality scores. Our proposed framework combines multi-scale convolutional
layers to model spatial interactions across multiple input variables, recurrent
layers and attention mechanisms to capture long-term temporal dependencies, and
a two-stage domain adaptation strategy to enhance generalization. The first
adaptation stage is applied during training to mitigate overfitting on the
training set. In the second stage, a source-free test-time adaptation mechanism
is employed to adapt the model to new users without requiring labels. We
conducted various experiments with five input window sizes (3, 5, 7, 9, and 11
days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model
consistently outperformed time series forecasting baseline approaches,
including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The
best performance was achieved with a three-day input window and a one-day
prediction window, yielding a root mean square error (RMSE) of 0.216.
Furthermore, the model demonstrated good predictive performance even for longer
forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction
window), highlighting its practical utility for real-world applications. We
also conducted an explainability analysis to examine how different features
influence sleep quality. These findings proved that the proposed framework
offers a robust, adaptive, and explainable solution for personalized sleep
forecasting using sparse data from commercial wearable devices.

</details>


### [77] [GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning](https://arxiv.org/abs/2509.06975)
*Yu Song,Zhigang Hua,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: 提出GSTBench基准评估图自监督学习方法跨数据集可迁移性，发现多数方法泛化难，GraphMAE表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有图自监督学习方法多在单数据集设置下开发和评估，跨数据集可迁移性未充分探索，限制知识迁移和大规模预训练，为推进图基础模型研究。

Method: 在ogbn - papers100M上进行大规模预训练，评估五种代表性自监督学习方法在不同目标图上的表现，标准化实验设置以聚焦预训练目标进行比较。

Result: 多数图自监督学习方法泛化困难，部分比随机初始化表现差，GraphMAE能持续提升迁移性能。

Conclusion: 分析差异背后因素，为可迁移图自监督学习研究提供见解，为图学习的“预训练 - 迁移”范式奠定基础。

Abstract: Self-supervised learning (SSL) has shown great promise in graph
representation learning. However, most existing graph SSL methods are developed
and evaluated under a single-dataset setting, leaving their cross-dataset
transferability largely unexplored and limiting their ability to leverage
knowledge transfer and large-scale pretraining, factors that are critical for
developing generalized intelligence beyond fitting training data. To address
this gap and advance foundation model research for graphs, we present GSTBench,
the first systematic benchmark for evaluating the transferability of graph SSL
methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate
five representative SSL methods across a diverse set of target graphs. Our
standardized experimental setup decouples confounding factors such as model
architecture, dataset characteristics, and adaptation protocols, enabling
rigorous comparisons focused solely on pretraining objectives. Surprisingly, we
observe that most graph SSL methods struggle to generalize, with some
performing worse than random initialization. In contrast, GraphMAE, a masked
autoencoder approach, consistently improves transfer performance. We analyze
the underlying factors that drive these differences and offer insights to guide
future research on transferable graph SSL, laying a solid foundation for the
"pretrain-then-transfer" paradigm in graph learning. Our code is available at
https://github.com/SongYYYY/GSTBench.

</details>


### [78] [Lookup multivariate Kolmogorov-Arnold Networks](https://arxiv.org/abs/2509.07103)
*Sergey Pozdnyakov,Philippe Schwaller*

Main category: cs.LG

TL;DR: 提出lookup multivariate Kolmogorov - Arnold Networks (lmKANs)替代高维线性映射，能在容量和推理成本间取得更好平衡，实验表明可降低推理FLOPs。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型中高维线性映射主导参数数量和计算成本，需更好的替代方案。

Method: 引入lmKANs，通过可训练的低维多元函数表达高维映射，函数以样条查找表实现。

Result: lmKANs最多降低推理FLOPs 6.0x，在特定基准测试中使H100吞吐量提高超10x，基于lmKAN的CNN在特定数据集降低推理FLOPs。

Conclusion: lmKANs能在容量和推理成本间取得更好平衡，可作为高维线性映射的有效替代。

Abstract: High-dimensional linear mappings, or linear layers, dominate both the
parameter count and the computational cost of most modern deep-learning models.
We introduce a general drop-in replacement, lookup multivariate
Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better
trade-off between capacity and inference cost. Our construction expresses a
general high-dimensional mapping through trainable low-dimensional multivariate
functions. These functions can carry dozens or hundreds of trainable parameters
each, and yet it takes only a few multiplications to compute them because they
are implemented as spline lookup tables. Empirically, lmKANs reduce inference
FLOPs by up to 6.0x while matching the flexibility of MLPs in general
high-dimensional function approximation. In another feedforward fully connected
benchmark, on the tabular-like dataset of randomly displaced methane
configurations, lmKANs enable more than 10x higher H100 throughput at equal
accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs
cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10
and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA
kernels, is available online at https://github.com/schwallergroup/lmkan.

</details>


### [79] [A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction](https://arxiv.org/abs/2509.06976)
*Lingyu Zhang,Pengfei Xu,Guobin Wu,Jian Liang,Ruiyang Dong,Yunhai Wang,Xuan Song*

Main category: cs.LG

TL;DR: 本文提出知识引导的跨模态特征表示学习（KGCM）模型用于交通需求预测，结合结构化时间交通数据与代表人类知识经验的文本数据，实验表明该模型优于现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型主要依赖时间交通数据，较少结合人类知识经验，而现实中人类的交通知识经验对精确预测影响大，因此需改进模型。

Method: 将结构化时间交通数据与代表人类知识经验的文本数据结合，构建先验知识数据集，通过设计的局部和全局自适应图网络及跨模态特征融合机制学习多模态数据特征，采用基于推理的动态更新策略优化图模型参数。

Result: 在多个交通数据集上的实验表明，模型能准确预测未来交通需求，且性能优于现有SOTA模型。

Conclusion: 所提出的KGCM模型在交通需求预测方面有效，结合人类知识经验可提升预测的准确性和鲁棒性。

Abstract: Traffic demand prediction plays a critical role in intelligent transportation
systems. Existing traffic prediction models primarily rely on temporal traffic
data, with limited efforts incorporating human knowledge and experience for
urban traffic demand forecasting. However, in real-world scenarios, traffic
knowledge and experience derived from human daily life significantly influence
precise traffic prediction. Such knowledge and experiences can guide the model
in uncovering latent patterns within traffic data, thereby enhancing the
accuracy and robustness of predictions. To this end, this paper proposes
integrating structured temporal traffic data with textual data representing
human knowledge and experience, resulting in a novel knowledge-guided
cross-modal feature representation learning (KGCM) model for traffic demand
prediction. Based on regional transportation characteristics, we construct a
prior knowledge dataset using a large language model combined with manual
authoring and revision, covering both regional and global knowledge and
experiences. The KGCM model then learns multimodal data features through
designed local and global adaptive graph networks, as well as a cross-modal
feature fusion mechanism. A proposed reasoning-based dynamic update strategy
enables dynamic optimization of the graph model's parameters, achieving optimal
performance. Experiments on multiple traffic datasets demonstrate that our
model accurately predicts future traffic demand and outperforms existing
state-of-the-art (SOTA) models.

</details>


### [80] [Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First Framework with Three-Tier Verification](https://arxiv.org/abs/2509.06977)
*Zehua Li*

Main category: cs.LG

TL;DR: 提出配置优先框架评估深度学习系统跨后端兼容性，实验表明多数运行通过检查，检测模型和编译后端易漂移，确定性适配器和选择性回退可改善一致性。


<details>
  <summary>Details</summary>
Motivation: 系统地量化和缓解深度学习中的跨后端漂移，为异构运行时的可靠部署提供可重复的方法。

Method: 使用YAML将实验与代码解耦，支持库和仓库模型，采用三层验证协议。

Result: 通过672次检查，72.0%的运行通过，检测模型和编译后端易漂移，常因非确定性后处理。

Conclusion: 确定性适配器和选择性回退可显著改善一致性且不损失性能，该框架是首个统一量化和缓解跨后端漂移的框架。

Abstract: This paper presents a configuration-first framework for evaluating
cross-backend compatibility in deep learning systems deployed on CPU, GPU, and
compiled runtimes. The framework decouples experiments from code using YAML,
supports both library and repository models, and employs a three-tier
verification protocol covering tensor-level closeness, activation alignment,
and task-level metrics. Through 672 checks across multiple models and tolerance
settings, we observe that 72.0% of runs pass, with most discrepancies occurring
under stricter thresholds. Our results show that detection models and compiled
backends are particularly prone to drift, often due to nondeterministic
post-processing. We further demonstrate that deterministic adapters and
selective fallbacks can substantially improve agreement without significant
performance loss. To our knowledge, this is the first unified framework that
systematically quantifies and mitigates cross-backend drift in deep learning,
providing a reproducible methodology for dependable deployment across
heterogeneous runtimes.

</details>


### [81] [A Kriging-HDMR-based surrogate model with sample pool-free active learning strategy for reliability analysis](https://arxiv.org/abs/2509.06978)
*Wenxiong Li,Hanyu Liao,Suiyin Chen*

Main category: cs.LG

TL;DR: 本文针对可靠性工程中传统代理模型的维数灾难问题，提出基于Kriging - HDMR的主动学习代理模型方法用于可靠性分析，数值实验表明该方法高效且准确。


<details>
  <summary>Details</summary>
Motivation: 传统代理模型有维数灾难，现有Kriging - HDMR方法多用于优化问题，针对可靠性分析的研究较少，而可靠性分析更注重关键区域预测精度。

Method: 构建基于Kriging - HDMR的主动学习代理模型，分三个阶段构建代理模型框架，基于各阶段特点建立优化数学模型选择实验设计样本，采用无候选样本池方法选择信息样本。

Result: 数值实验表明，该方法在解决高维可靠性问题时能保持高计算效率和强预测精度。

Conclusion: 所提出的基于Kriging - HDMR的主动学习代理模型方法可有效解决高维可靠性问题。

Abstract: In reliability engineering, conventional surrogate models encounter the
"curse of dimensionality" as the number of random variables increases. While
the active learning Kriging surrogate approaches with high-dimensional model
representation (HDMR) enable effective approximation of high-dimensional
functions and are widely applied to optimization problems, there are rare
studies specifically focused on reliability analysis, which prioritizes
prediction accuracy in critical regions over uniform accuracy across the entire
domain. This study develops an active learning surrogate model method based on
the Kriging-HDMR modeling for reliability analysis. The proposed approach
facilitates the approximation of high-dimensional limit state functions through
a composite representation constructed from multiple low-dimensional
sub-surrogate models. The architecture of the surrogate modeling framework
comprises three distinct stages: developing single-variable sub-surrogate
models for all random variables, identifying the requirements for
coupling-variable sub-surrogate models, and constructing the coupling-variable
sub-surrogate models. Optimization mathematical models for selection of design
of experiment samples are formulated based on each stage's characteristics,
with objectives incorporating uncertainty variance, predicted mean, sample
location and inter-sample distances. A candidate sample pool-free approach is
adopted to achieve the selection of informative samples. Numerical experiments
demonstrate that the proposed method achieves high computational efficiency
while maintaining strong predictive accuracy in solving high-dimensional
reliability problems.

</details>


### [82] [Exploring Over-stationarization in Deep Learning-based Bus/Tram Arrival Time Prediction: Analysis and Non-stationary Effect Recovery](https://arxiv.org/abs/2509.06979)
*Zirui Li,Bin Yang,Meng Wang*

Main category: cs.LG

TL;DR: 本文提出非平稳到达时间预测（NSATP）方法用于公共交通车辆多步到达时间预测，经实验验证，相比基线方法能降低误差。


<details>
  <summary>Details</summary>
Motivation: 现有多步到达时间预测中，归一化处理会导致过度平稳化，掩盖非平稳性有用特征，需平衡可预测性与非平稳性。

Method: 提出NSATP方法，包括序列平稳化和非平稳效应恢复两个阶段，第二阶段将一维模型扩展到二维以捕捉隐藏周期性，并设计过平稳化补偿模块。

Result: 使用德累斯顿125天公共交通运营数据验证，相比基线方法，NSATP对有轨电车和公交车的RMSE、MAE和MAPE均有降低。

Conclusion: NSATP方法能有效平衡可预测性与非平稳性，提升多步到达时间预测性能。

Abstract: Arrival time prediction (ATP) of public transport vehicles is essential in
improving passenger experience and supporting traffic management. Deep learning
has demonstrated outstanding performance in ATP due to its ability to model
non-linear and temporal dynamics. In the multi-step ATP, non-stationary data
will degrade the model performance due to the variation in variables' joint
distribution along the temporal direction. Previous studies mainly applied
normalization to eliminate the non-stationarity in time series, thereby
achieving better predictability. However, the normalization may obscure useful
characteristics inherent in non-stationarity, which is known as the
over-stationarization. In this work, to trade off predictability and
non-stationarity, a new approach for multi-step ATP, named non-stationary ATP (
NSATP), is proposed. The method consists of two stages: series stationarization
and non-stationarity effect recovery. The first stage aims at improving the
predictability. As for the latter, NSATP extends a state-of-the-art method from
one-dimensional to two dimensional based models to capture the hidden
periodicity in time series and designs a compensation module of
over-stationarization by learning scaling and shifting factors from raw data.
125 days' public transport operational data of Dresden is collected for
validation. Experimental results show that compared to baseline methods, the
proposed NSATP can reduce RMSE, MAE, and MAPE by 2.37%, 1.22%, and 2.26% for
trams and by 1.72%, 0.60%, and 1.17% for buses, respectively.

</details>


### [83] [RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use](https://arxiv.org/abs/2509.06980)
*Jiajun Chai,Guojun Yin,Zekun Xu,Chuhuai Yue,Yi Jia,Siyu Xia,Xiaohan Wang,Jiwen Jiang,Xiaoguang Li,Chengqi Dong,Hang He,Wei Lin*

Main category: cs.LG

TL;DR: 提出RLFactory框架用于大模型多轮工具使用，解决稳定性、适应性和评估问题，提升表现和训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在与外部工具交互任务上表现不佳，需提升多轮工具使用能力。

Method: 采用基于asyncio的异步调用器和分离式工具/训练架构，构建支持多种信号的奖励层，引入观察标记重构MDP，实现生成-解析-调用-更新工作流。

Result: 在Search - R1和Qwen3 - 4B上，在NQ数据集测试得分超相似技术训练的更大模型，训练吞吐量提升6.8倍。

Conclusion: RLFactory为现实场景中提升大模型多轮工具使用能力提供低门槛、高适应性框架。

Abstract: Large language models excel at basic reasoning but struggle with tasks that
require interaction with external tools. We present RLFactory, a plug-and-play
reinforcement learning post-training framework for multi-round tool use.
RLFactory tackles (i) tool-call stability and adaptability amid tool
heterogeneity and interface issues via an asyncio-based asynchronous caller and
a decoupled tool/training architecture, and (ii) diverse evaluation needs via a
reward layer supporting rule-based, model-judgment, and tool-verification
signals. It reconstructs the MDP by introducing observation markers from tool
feedback, closing the loop among model, tools, and environment, and implements
a generate-parse-invoke-update workflow for dynamic policy optimization. On
Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural
Questions (NQ) dataset, surpassing larger models trained with similar
techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training
throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable
framework for strengthening multi-round tool use of LLMs in real-world
scenarios. Code: https://github.com/Simple-Efficient/RL-Factory.

</details>


### [84] [A Minimalist Bayesian Framework for Stochastic Optimization](https://arxiv.org/abs/2509.07030)
*Kaizheng Wang*

Main category: cs.LG

TL;DR: 提出简约贝叶斯框架及MINTS算法，适用于结构化问题，分析算法并给出近最优后悔保证。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯范式依赖概率模型，难以融入复杂结构约束。

Method: 引入仅对感兴趣组件设置先验的简约贝叶斯框架，用轮廓似然消除干扰参数，开发MINTS算法。

Result: 框架适用于结构化问题，为经典凸优化算法提供概率视角，分析MINTS算法得到近最优后悔保证。

Conclusion: 简约贝叶斯框架及MINTS算法有效，可处理结构化问题。

Abstract: The Bayesian paradigm offers principled tools for sequential decision-making
under uncertainty, but its reliance on a probabilistic model for all parameters
can hinder the incorporation of complex structural constraints. We introduce a
minimalist Bayesian framework that places a prior only on the component of
interest, such as the location of the optimum. Nuisance parameters are
eliminated via profile likelihood, which naturally handles constraints. As a
direct instantiation, we develop a MINimalist Thompson Sampling (MINTS)
algorithm. Our framework accommodates structured problems, including
continuum-armed Lipschitz bandits and dynamic pricing. It also provides a
probabilistic lens on classical convex optimization algorithms such as the
center of gravity and ellipsoid methods. We further analyze MINTS for
multi-armed bandits and establish near-optimal regret guarantees.

</details>


### [85] [CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention](https://arxiv.org/abs/2509.06982)
*Xiaomeng Hu,Fei Huang,Chenhan Yuan,Junyang Lin,Tsung-Yi Ho*

Main category: cs.LG

TL;DR: 提出CARE框架用于解码时安全对齐，平衡安全与响应质量，实验显示其在安全、质量和效率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际应用中，现有解码干预方法难以平衡输出安全和响应质量。

Method: 提出CARE框架，包含实时安全监控的防护模型、带令牌缓冲区的回滚机制和基于自省的干预策略。

Result: 框架实现了安全、质量和效率的平衡，有害响应率低，对用户体验干扰小且响应质量高。

Conclusion: CARE框架能有效解决解码时安全对齐问题，实现安全与质量的良好权衡。

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, ensuring the safety of their outputs during decoding has become a
critical challenge. However, existing decoding-time interventions, such as
Contrastive Decoding, often force a severe trade-off between safety and
response quality. In this work, we propose CARE, a novel framework for
decoding-time safety alignment that integrates three key components: (1) a
guard model for real-time safety monitoring, enabling detection of potentially
unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe
outputs efficiently at an earlier stage without disrupting the user experience;
and (3) a novel introspection-based intervention strategy, where the model
generates self-reflective critiques of its previous outputs and incorporates
these reflections into the context to guide subsequent decoding steps. The
framework achieves a superior safety-quality trade-off by using its guard model
for precise interventions, its rollback mechanism for timely corrections, and
our novel introspection method for effective self-correction. Experimental
results demonstrate that our framework achieves a superior balance of safety,
quality, and efficiency, attaining a low harmful response rate and minimal
disruption to the user experience while maintaining high response quality.

</details>


### [86] [FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities](https://arxiv.org/abs/2509.06984)
*Lishan Yang,Nam Kha Nguygen,Po Hu,Wei Emma Zhang,Yanjun Shu,Mong Yuan Sim,Weitong Chen*

Main category: cs.LG

TL;DR: 提出FediLoRA框架用于异构LoRA秩和缺失模态下的联邦多模态微调，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有联邦LoRA方法假设统一秩配置和单模态输入，未考虑异构客户端资源和多模态数据缺失模态问题。

Method: 提出FediLoRA框架，引入维度聚合策略和轻量级层模型编辑方法。

Result: 在三个多模态基准数据集上，FediLoRA在全局和个性化设置中均优于竞争基线，尤其在模态不完整时。

Conclusion: FediLoRA是简单有效的联邦多模态微调框架，能应对异构LoRA秩和缺失模态挑战。

Abstract: Foundation models have demonstrated remarkable performance across a wide
range of tasks, yet their large parameter sizes pose challenges for practical
deployment, especially in decentralized environments. Parameter-efficient
fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing
and memory overhead, making it attractive for federated learning. However,
existing federated LoRA methods typically assume uniform rank configurations
and unimodal inputs, overlooking two key real-world challenges: (1)
heterogeneous client resources have different LoRA ranks, and (2) multimodal
data settings with potentially missing modalities. In this work, we propose
FediLoRA, a simple yet effective framework for federated multimodal fine-tuning
under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a
dimension-wise aggregation strategy that reweights LoRA updates without
information dilution during aggregation. It also includes a lightweight
layer-wise model editing method that selectively incorporates global parameters
to repair local components which improves both client and global model
performances. Experimental results on three multimodal benchmark datasets
demonstrate that FediLoRA achieves superior performance over competitive
baselines in both global and personalized settings, particularly in the
presence of modality incompleteness.

</details>


### [87] [Machine Generalize Learning in Agent-Based Models: Going Beyond Surrogate Models for Calibration in ABMs](https://arxiv.org/abs/2509.07013)
*Sima Najafzadehkhoei,George Vega Yon,Bernardo Modenesi,Derek S. Meyer*

Main category: cs.LG

TL;DR: 提出一种监督式机器学习校准器用于校准基于代理的流行病模型，比近似贝叶斯计算法效果好、速度快。


<details>
  <summary>Details</summary>
Motivation: 基于代理的流行病模型校准计算要求高，需要高效校准方法。

Method: 使用三层双向LSTM学习从流行病时间序列到SIR参数的逆映射，训练采用带流行病学一致性惩罚的复合损失函数。

Result: 在1000场景模拟研究中，该方法误差更低、预测区间更窄、覆盖度接近标称值、校准时间大幅减少，能更真实重现流行曲线。

Conclusion: 该方法实现了快速实用的校准，可用于基于SIR代理的流行病，并提供了R语言实现。

Abstract: Calibrating agent-based epidemic models is computationally demanding. We
present a supervised machine learning calibrator that learns the inverse
mapping from epidemic time series to SIR parameters. A three-layer
bidirectional LSTM ingests 60-day incidence together with population size and
recovery rate, and outputs transmission probability, contact rate, and R0.
Training uses a composite loss with an epidemiology-motivated consistency
penalty that encourages R0 \* recovery rate to equal transmission probability
\* contact rate.
  In a 1000-scenario simulation study, we compare the calibrator with
Approximate Bayesian Computation (likelihood-free MCMC). The method achieves
lower error across all targets (MAE: R0 0.0616 vs 0.275; transmission 0.0715 vs
0.128; contact 1.02 vs 4.24), produces tighter predictive intervals with near
nominal coverage, and reduces wall clock time from 77.4 s to 2.35 s per
calibration. Although contact rate and transmission probability are partially
nonidentifiable, the approach reproduces epidemic curves more faithfully than
ABC, enabling fast and practical calibration. We evaluate it on SIR agent based
epidemics generated with epiworldR and provide an implementation in R.

</details>


### [88] [uGMM-NN: Univariate Gaussian Mixture Model Neural Network](https://arxiv.org/abs/2509.07569)
*Zakeria Sharif Ali*

Main category: cs.LG

TL;DR: 提出uGMM - NN新型神经架构，可嵌入概率推理，实验显示有竞争力表现并为建模提供新方向


<details>
  <summary>Details</summary>
Motivation: 将概率推理直接嵌入深度网络计算单元，捕捉多模态和不确定性，为现代神经架构集成不确定性感知组件

Method: 设计uGMM - NN节点，将激活参数化为单变量高斯混合，具有可学习的均值、方差和混合系数

Result: uGMM - NN与传统多层感知器相比有竞争力的判别性能，且激活有概率解释

Conclusion: 该框架为判别和生成建模提供基础，开辟新方向

Abstract: This paper introduces the Univariate Gaussian Mixture Model Neural Network
(uGMM-NN), a novel neural architecture that embeds probabilistic reasoning
directly into the computational units of deep networks. Unlike traditional
neurons, which apply weighted sums followed by fixed nonlinearities, each
uGMM-NN node parameterizes its activations as a univariate Gaussian mixture,
with learnable means, variances, and mixing coefficients. This design enables
richer representations by capturing multimodality and uncertainty at the level
of individual neurons, while retaining the scalability of standard feedforward
networks. We demonstrate that uGMM-NN can achieve competitive discriminative
performance compared to conventional multilayer perceptrons, while additionally
offering a probabilistic interpretation of activations. The proposed framework
provides a foundation for integrating uncertainty-aware components into modern
neural architectures, opening new directions for both discriminative and
generative modeling.

</details>


### [89] [An efficient deep reinforcement learning environment for flexible job-shop scheduling](https://arxiv.org/abs/2509.07019)
*Xinquan Wu,Xuefeng Yan,Mingqiang Wei,Donghai Guan*

Main category: cs.LG

TL;DR: 本文为柔性作业车间调度问题（FJSP）提出基于离散事件模拟的简单时间顺序DRL环境和基于PPO的端到端DRL调度模型，设计新的状态表示和奖励函数，实验显示该模型有较好性能。


<details>
  <summary>Details</summary>
Motivation: 现有DRL调度方法主要关注调度智能体设计，忽略了DRL环境建模，本文旨在解决该问题以生成快速准确的FJSP调度方案。

Method: 提出基于离散事件模拟的简单时间顺序DRL环境，基于PPO提出端到端DRL调度模型，基于调度环境的两个状态变量提出新的状态表示，基于机器调度区域设计新的奖励函数。

Result: 在公共基准实例上的实验结果表明，简单优先级调度规则（PDR）在调度环境中性能得到提升，DRL调度模型与OR - Tools、元启发式、DRL和PDR调度方法相比具有竞争力。

Conclusion: 所提出的方法能有效解决FJSP，改善调度性能。

Abstract: The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial
optimization problem that has a wide-range of applications in the real world.
In order to generate fast and accurate scheduling solutions for FJSP, various
deep reinforcement learning (DRL) scheduling methods have been developed.
However, these methods are mainly focused on the design of DRL scheduling
Agent, overlooking the modeling of DRL environment. This paper presents a
simple chronological DRL environment for FJSP based on discrete event
simulation and an end-to-end DRL scheduling model is proposed based on the
proximal policy optimization (PPO). Furthermore, a short novel state
representation of FJSP is proposed based on two state variables in the
scheduling environment and a novel comprehensible reward function is designed
based on the scheduling area of machines. Experimental results on public
benchmark instances show that the performance of simple priority dispatching
rules (PDR) is improved in our scheduling environment and our DRL scheduling
model obtains competing performance compared with OR-Tools, meta-heuristic, DRL
and PDR scheduling methods.

</details>


### [90] [1 bit is all we need: binary normalized neural networks](https://arxiv.org/abs/2509.07025)
*Eduardo Lobo Lustoda Cabral,Paulo Pirozelli,Larissa Driemeier*

Main category: cs.LG

TL;DR: 开发仅用单比特参数的新型神经网络层和模型，在图像分类和语言解码任务测试，内存使用降32倍且性能相当，可用简单硬件部署。


<details>
  <summary>Details</summary>
Motivation: 大型神经网络模型尺寸增大带来部署挑战，需降低内存需求、提高计算效率。

Method: 开发使用单比特参数的新型神经网络层，即二元归一化层，构建两种模型进行测试。

Result: 含二元归一化层的模型与32位实参等效模型结果相近，内存使用降32倍，可在现有计算机用1位数组实现。

Conclusion: 新型层为降低内存需求的大型神经网络模型开启新时代，可用简单廉价硬件部署。

Abstract: The increasing size of large neural network models, specifically language
models and foundational image models, poses deployment challenges, prompting
efforts to reduce memory requirements and enhance computational efficiency.
These efforts are critical to ensure practical deployment and effective
utilization of these models across various applications. In this work, a novel
type of neural network layers and models is developed that uses only single-bit
parameters. In this novel type of models all parameters of all layers,
including kernel weights and biases, only have values equal to zero or one.
This novel type of models uses layers named as binary normalized layer. These
binary normalized layers can be of any type, such as fully connected,
convolutional, attention, etc., and they consist of slight variations of the
corresponding conventional layers. To show the effectiveness of the binary
normalized layers, two different models are configured to solve a multiclass
image classification problem and a language decoder to predict the next token
of a sequence. The model to solve the image classification has convolutional
and fully connected layers, and the language model is composed of transformer
blocks with multi-head attention. The results show that models with binary
normalized layers present almost the same results obtained by equivalent models
with real 32-bit parameters. The binary normalized layers allow to develop
models that use 32 times less memory than current models and have equivalent
performance. Besides, the binary normalized layers can be easily implemented on
current computers using 1-bit arrays, and do not require the development of
dedicated electronic hardware. This novel type of layers opens a new era for
large neural network models with reduced memory requirements that can be
deployed using simple and cheap hardware, such as mobile devices or only cpus.

</details>


### [91] [Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data](https://arxiv.org/abs/2509.07198)
*Yiyue Chen,Usman Akram,Chianing Wang,Haris Vikalo*

Main category: cs.LG

TL;DR: 本文提出Fed - REACT框架解决联邦学习中客户端数据异质性和动态变化问题，经理论分析和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 集中式机器学习资源成本高且有隐私问题，联邦学习虽好但实际中客户端数据分布有异质性和动态变化，影响标准算法性能。

Method: 提出Fed - REACT框架，分两阶段，先让客户端学习本地模型提取特征表示，再由服务器基于表示动态聚类并协调特定任务模型训练。

Result: 在真实数据集上，Fed - REACT准确性和鲁棒性表现更优。

Conclusion: Fed - REACT能有效应对联邦学习中客户端数据异质性和动态变化问题。

Abstract: Motivated by the high resource costs and privacy concerns associated with
centralized machine learning, federated learning (FL) has emerged as an
efficient alternative that enables clients to collaboratively train a global
model while keeping their data local. However, in real-world deployments,
client data distributions often evolve over time and differ significantly
across clients, introducing heterogeneity that degrades the performance of
standard FL algorithms. In this work, we introduce Fed-REACT, a federated
learning framework designed for heterogeneous and evolving client data.
Fed-REACT combines representation learning with evolutionary clustering in a
two-stage process: (1) in the first stage, each client learns a local model to
extracts feature representations from its data; (2) in the second stage, the
server dynamically groups clients into clusters based on these representations
and coordinates cluster-wise training of task-specific models for downstream
objectives such as classification or regression. We provide a theoretical
analysis of the representation learning stage, and empirically demonstrate that
Fed-REACT achieves superior accuracy and robustness on real-world datasets.

</details>


### [92] [Recursive State Inference for Linear PASFA](https://arxiv.org/abs/2509.07028)
*Vishal Rishi*

Main category: cs.LG

TL;DR: 本文提出线性PASFA的递归扩展算法，用于从观测和模型中推断状态（慢特征），并在合成数据集上验证其正确性。


<details>
  <summary>Details</summary>
Motivation: 现有概率扩展的SFA方法需开发高效方法从观测和模型中推断状态（慢特征），且当前方法难以恢复有用表示的原始状态。

Method: 提出线性PASFA的递归扩展算法，对按ARMA过程演化的状态进行MMSE估计。

Result: 在合成数据集上对提出的技术进行评估，证明其正确性。

Conclusion: 提出的递归扩展算法可有效从观测和模型中推断状态（慢特征）。

Abstract: Slow feature analysis (SFA), as a method for learning slowly varying features
in classification and signal analysis, has attracted increasing attention in
recent years. Recent probabilistic extensions to SFA learn effective
representations for classification tasks. Notably, the Probabilistic Adaptive
Slow Feature Analysis models the slow features as states in an ARMA process and
estimate the model from the observations. However, there is a need to develop
efficient methods to infer the states (slow features) from the observations and
the model. In this paper, a recursive extension to the linear PASFA has been
proposed. The proposed algorithm performs MMSE estimation of states evolving
according to an ARMA process, given the observations and the model. Although
current methods tackle this problem using Kalman filters after transforming the
ARMA process into a state space model, the original states (or slow features)
that form useful representations cannot be easily recovered. The proposed
technique is evaluated on a synthetic dataset to demonstrate its correctness.

</details>


### [93] [FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning](https://arxiv.org/abs/2509.07342)
*Yuxuan Bai,Yuxuan Sun,Tan Chen,Wei Chen,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 提出FedTeddi算法，在动态数据和通信资源限制下促进联邦边缘学习快速收敛，实验显示比基准方法效果好。


<details>
  <summary>Details</summary>
Motivation: 现有研究多假设静态数据集，现实中客户数据具有时变和非独立同分布特性，需及时高效适配模型。

Method: 用时间漂移和集体散度量化数据特征，以地球移动距离表示，提出新优化目标和联合调度与带宽分配算法。

Result: 算法在测试精度和收敛速度上优于基准方法，在CIFAR - 10和CIFAR - 100上收敛率分别提升58.4%和49.2%。

Conclusion: FedTeddi算法能在动态数据和资源限制下使联邦边缘学习快速收敛，表现良好。

Abstract: Federated edge learning (FEEL) enables collaborative model training across
distributed clients over wireless networks without exposing raw data. While
most existing studies assume static datasets, in real-world scenarios clients
may continuously collect data with time-varying and non-independent and
identically distributed (non-i.i.d.) characteristics. A critical challenge is
how to adapt models in a timely yet efficient manner to such evolving data. In
this paper, we propose FedTeddi, a temporal-drift-and-divergence-aware
scheduling algorithm that facilitates fast convergence of FEEL under dynamic
data evolution and communication resource limits. We first quantify the
temporal dynamics and non-i.i.d. characteristics of data using temporal drift
and collective divergence, respectively, and represent them as the Earth
Mover's Distance (EMD) of class distributions for classification tasks. We then
propose a novel optimization objective and develop a joint scheduling and
bandwidth allocation algorithm, enabling the FEEL system to learn from new data
quickly without forgetting previous knowledge. Experimental results show that
our algorithm achieves higher test accuracy and faster convergence compared to
benchmark methods, improving the rate of convergence by 58.4% on CIFAR-10 and
49.2% on CIFAR-100 compared to random scheduling.

</details>


### [94] [Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators](https://arxiv.org/abs/2509.07036)
*Federico Cerutti*

Main category: cs.LG

TL;DR: 结合因果发现与不确定性感知预测分析金融时间序列，以美国四个宏观经济指标为例，揭示因果关系并对失业率进行预测，证明该方法对经济政策和预测有价值。


<details>
  <summary>Details</summary>
Motivation: 提出一种金融时间序列分析的方法，为经济政策提供信息并增强预测鲁棒性。

Method: 结合因果发现和不确定性感知预测，用LPCMCI框架与GPDC揭示因果关系，用Chronos框架对失业率进行零样本预测。

Result: 发现经济增长到GDP的单向因果关系，通胀连通性有限，失业率自回归依赖强，失业率预测准确且有90%置信区间。

Conclusion: 结合因果结构学习与概率语言模型对经济政策和提高预测鲁棒性有价值。

Abstract: This paper presents a methodological approach to financial time series
analysis by combining causal discovery and uncertainty-aware forecasting. As a
case study, we focus on four key U.S. macroeconomic indicators -- GDP, economic
growth, inflation, and unemployment -- and we apply the LPCMCI framework with
Gaussian Process Distance Correlation (GPDC) to uncover dynamic causal
relationships in quarterly data from 1970 to 2021. Our results reveal a robust
unidirectional causal link from economic growth to GDP and highlight the
limited connectivity of inflation, suggesting the influence of latent factors.
Unemployment exhibits strong autoregressive dependence, motivating its use as a
case study for probabilistic forecasting. Leveraging the Chronos framework, a
large language model trained for time series, we perform zero-shot predictions
on unemployment. This approach delivers accurate forecasts one and two quarters
ahead, without requiring task-specific training. Crucially, the model's
uncertainty-aware predictions yield 90\% confidence intervals, enabling
effective anomaly detection through statistically principled deviation
analysis. This study demonstrates the value of combining causal structure
learning with probabilistic language models to inform economic policy and
enhance forecasting robustness.

</details>


### [95] [MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?](https://arxiv.org/abs/2509.07727)
*Songkai Ma,Zhaorui Zhang,Sheng Di,Benben Liu,Xiaodong Yu,Xiaoyi Lu,Dan Wang*

Main category: cs.LG

TL;DR: 在有限GPU内存下高效服务MoE模型有挑战，提出用误差有界的有损压缩算法压缩非激活专家，实验分析压缩误差对推理准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 随着MoE推理模型在LLM学习领域广泛应用，需解决在有限GPU内存下高效服务的问题，探索专家压缩方法及分析压缩误差对推理性能的影响。

Method: 采用误差有界的有损压缩算法（如SZ3和CuSZp）压缩非激活专家，在多个基准上进行实验。

Result: 浅层专家在有界误差下推理准确性下降最小；中层专家误差显著影响推理准确性；深层专家引入有界误差有时可提升推理准确性。

Conclusion: 不同层专家受压缩误差的影响不同，可根据此特点优化MoE模型推理。

Abstract: With the widespread application of Mixture of Experts (MoE) reasoning models
in the field of LLM learning, efficiently serving MoE models under limited GPU
memory constraints has emerged as a significant challenge. Offloading the
non-activated experts to main memory has been identified as an efficient
approach to address such a problem, while it brings the challenges of
transferring the expert between the GPU memory and main memory. We need to
explore an efficient approach to compress the expert and analyze how the
compression error affects the inference performance.
  To bridge this gap, we propose employing error-bounded lossy compression
algorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby
reducing data transfer overhead during MoE inference. We conduct extensive
experiments across various benchmarks and present a comprehensive analysis of
how compression-induced errors in different experts affect overall inference
accuracy. The results indicate that experts in the shallow layers, which are
primarily responsible for the attention mechanism and the transformation of
input tokens into vector representations, exhibit minimal degradation in
inference accuracy when subjected to bounded errors. In contrast, errors in the
middle-layer experts, which are central to model reasoning, significantly
impair inference accuracy. Interestingly, introducing bounded errors in the
deep-layer experts, which are mainly responsible for instruction following and
output integration, can sometimes lead to improvements in inference accuracy.

</details>


### [96] [Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation](https://arxiv.org/abs/2509.07039)
*Serra Aksoy*

Main category: cs.LG

TL;DR: 本文对卷积神经网络和视觉变压器用于热光伏故障检测进行系统比较，用XRAI分析评估与热物理原理的一致性，Swin Transformer性能最佳，热物理引导的可解释性方法有助于解决AI在能源监测应用的部署障碍。


<details>
  <summary>Details</summary>
Motivation: 人工智能在自动光伏监测中的部署面临可解释性障碍，缺乏模型决策与热物理原理一致性的验证，限制了其在能源基础设施应用中的采用。

Method: 对卷积神经网络（ResNet - 18、EfficientNet - B0）和视觉变压器（ViT - Tiny、Swin - Tiny）进行系统比较，使用XRAI显著性分析评估与热物理原理的一致性。

Result: Swin Transformer性能最佳，XRAI分析表明模型学习到有物理意义的特征，但不同故障类型性能差异大，电气故障检测强，环境因素检测有挑战。

Conclusion: 热物理引导的可解释性方法为验证能源监测应用中的AI决策提供了方法，有助于解决可再生能源基础设施中的部署障碍。

Abstract: Artificial intelligence deployment for automated photovoltaic (PV) monitoring
faces interpretability barriers that limit adoption in energy infrastructure
applications. While deep learning achieves high accuracy in thermal fault
detection, validation that model decisions align with thermal physics
principles remains lacking, creating deployment hesitancy where understanding
model reasoning is critical. This study provides a systematic comparison of
convolutional neural networks (ResNet-18, EfficientNet-B0) and vision
transformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI
saliency analysis to assess alignment with thermal physics principles. This
represents the first systematic comparison of CNNs and vision transformers for
thermal PV fault detection with physics-validated interpretability. Evaluation
on 20,000 infrared images spanning normal operation and 11 fault categories
shows that Swin Transformer achieves the highest performance (94% binary
accuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis
reveals that models learn physically meaningful features, such as localized
hotspots for cell defects, linear thermal paths for diode failures, and thermal
boundaries for vegetation shading, consistent with expected thermal signatures.
However, performance varies significantly across fault types: electrical faults
achieve strong detection (F1-scores >0.90) while environmental factors like
soiling remain challenging (F1-scores 0.20-0.33), indicating limitations
imposed by thermal imaging resolution. The thermal physics-guided
interpretability approach provides methodology for validating AI
decision-making in energy monitoring applications, addressing deployment
barriers in renewable energy infrastructure.

</details>


### [97] [Riemannian Batch Normalization: A Gyro Approach](https://arxiv.org/abs/2509.07115)
*Ziheng Chen,Xiao-Jun Wu,Nicu Sebe*

Main category: cs.LG

TL;DR: 提出用于陀螺群的黎曼批量归一化框架GyroBN，证明其条件对已知陀螺群成立，在多种几何上实例化并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 欧几里得形式的归一化层不适用于流形数据，而机器学习中许多黎曼流形有陀螺结构，可将欧几里得神经网络扩展到非欧领域。

Method: 引入GyroBN框架，建立伪约简和陀螺等距回转两个必要条件，在七种代表性几何上实例化该框架。

Result: 所建立的条件对机器学习中所有已知陀螺群成立，框架包含现有黎曼归一化方法作为特例，实验证明GyroBN有效。

Conclusion: GyroBN是一个有效的黎曼批量归一化框架。

Abstract: Normalization layers are crucial for deep learning, but their Euclidean
formulations are inadequate for data on manifolds. On the other hand, many
Riemannian manifolds in machine learning admit gyro-structures, enabling
principled extensions of Euclidean neural networks to non-Euclidean domains.
Inspired by this, we introduce GyroBN, a principled Riemannian batch
normalization framework for gyrogroups. We establish two necessary conditions,
namely \emph{pseudo-reduction} and \emph{gyroisometric gyrations}, that
guarantee GyroBN with theoretical control over sample statistics, and show that
these conditions hold for all known gyrogroups in machine learning. Our
framework also incorporates several existing Riemannian normalization methods
as special cases. We further instantiate GyroBN on seven representative
geometries, including the Grassmannian, five constant curvature spaces, and the
correlation manifold, and derive novel gyro and Riemannian structures to enable
these instantiations. Experiments across these geometries demonstrate the
effectiveness of GyroBN. The code is available at
https://github.com/GitZH-Chen/GyroBN.git.

</details>


### [98] [Of Graphs and Tables: Zero-Shot Node Classification with Tabular Foundation Models](https://arxiv.org/abs/2509.07143)
*Adrian Hayler,Xingyue Huang,İsmail İlkan Ceylan,Michael Bronstein,Ben Finkelshtein*

Main category: cs.LG

TL;DR: 提出TabGFM框架，将图转换为表格，用TFMs进行零样本节点分类，在28个真实数据集实验中表现优于特定任务GNN和SOTA GFMs。


<details>
  <summary>Details</summary>
Motivation: 现有GFMs训练数据集不能很好代表真实世界图，限制泛化性能，而TFMs在多领域有强适用性，因此将节点分类重构成表格问题。

Method: 引入TabGFM框架，先通过特征和结构编码器将图转换为表格，对不同子采样表格应用多个TFMs，再通过集成选择聚合输出。

Result: 在28个真实世界数据集上，TabGFM比特定任务GNN和SOTA GFMs有持续改进。

Conclusion: 表格重构在可扩展和可泛化图学习方面有潜力。

Abstract: Graph foundation models (GFMs) have recently emerged as a promising paradigm
for achieving broad generalization across various graph data. However, existing
GFMs are often trained on datasets that were shown to poorly represent
real-world graphs, limiting their generalization performance. In contrast,
tabular foundation models (TFMs) not only excel at classical tabular prediction
tasks but have also shown strong applicability in other domains such as time
series forecasting, natural language processing, and computer vision. Motivated
by this, we take an alternative view to the standard perspective of GFMs and
reformulate node classification as a tabular problem. Each node can be
represented as a row with feature, structure, and label information as columns,
enabling TFMs to directly perform zero-shot node classification via in-context
learning. In this work, we introduce TabGFM, a graph foundation model framework
that first converts a graph into a table via feature and structural encoders,
applies multiple TFMs to diversely subsampled tables, and then aggregates their
outputs through ensemble selection. Through experiments on 28 real-world
datasets, TabGFM achieves consistent improvements over task-specific GNNs and
state-of-the-art GFMs, highlighting the potential of tabular reformulation for
scalable and generalizable graph learning.

</details>


### [99] [Measuring Uncertainty in Transformer Circuits with Effective Information Consistency](https://arxiv.org/abs/2509.07149)
*Anatoly A. Krasnovsky*

Main category: cs.LG

TL;DR: 文章提出用于量化Transformer Circuits行为一致性的有效信息一致性分数（EICS），并给出相关解释和分析。


<details>
  <summary>Details</summary>
Motivation: 缺乏一种正式、单遍的方法来量化活动电路何时行为一致且值得信赖。

Method: 将层论/上同调与因果涌现视角应用于Transformer Circuits，结合归一化层不一致性和高斯有效信息代理构建EICS。

Result: 构建了EICS，给出分数解释、计算开销等方面的实用指导及简单验证分析。

Conclusion: 提出EICS方法，但LLM任务的实证验证待开展。

Abstract: Mechanistic interpretability has identified functional subgraphs within large
language models (LLMs), known as Transformer Circuits (TCs), that appear to
implement specific algorithms. Yet we lack a formal, single-pass way to
quantify when an active circuit is behaving coherently and thus likely
trustworthy. Building on prior systems-theoretic proposals, we specialize a
sheaf/cohomology and causal emergence perspective to TCs and introduce the
Effective-Information Consistency Score (EICS). EICS combines (i) a normalized
sheaf inconsistency computed from local Jacobians and activations, with (ii) a
Gaussian EI proxy for circuit-level causal emergence derived from the same
forward state. The construction is white-box, single-pass, and makes units
explicit so that the score is dimensionless. We further provide practical
guidance on score interpretation, computational overhead (with fast and exact
modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is
deferred.

</details>


### [100] [PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design](https://arxiv.org/abs/2509.07150)
*Andy Xu,Rohan Desai,Larry Wang,Gabriel Hope,Ethan Ritz*

Main category: cs.LG

TL;DR: 引入 PLaID++ 用于加速新材料发现，采用微调大语言模型生成晶体结构，效果优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 新材料开发受试错过程限制，需加速新材料发现流程。

Method: 微调 Qwen - 2.5 7B 模型，用基于 Wyckoff 的文本表示生成晶体结构，使用基于 DPO 的强化学习技术引导生成。

Result: PLaID++ 生成结构在热力学稳定性、独特性和新颖性上比先前方法高约 50%，迭代 DPO 在无条件和有空间群条件生成上分别提升约 115% 和 50%。

Conclusion: 将自然语言处理的训练后技术应用于材料设计有潜力，为高效发现新材料铺平道路。

Abstract: Discovering novel materials is critical for technological advancements such
as solar cells, batteries, and carbon capture. However, the development of new
materials is constrained by a slow and expensive trial-and-error process. To
accelerate this pipeline, we introduce PLaID++, a Large Language Model (LLM)
fine-tuned for stable and property-guided crystal generation. We fine-tune
Qwen-2.5 7B to generate crystal structures using a novel Wyckoff-based text
representation. We show that generation can be effectively guided with a
reinforcement learning technique based on Direct Preference Optimization (DPO),
with sampled structures categorized by their stability, novelty, and space
group. By encoding symmetry constraints directly into text and guiding model
outputs towards desirable chemical space, PLaID++ generates structures that are
thermodynamically stable, unique, and novel at a $\sim$50\% greater rate than
prior methods and conditionally generates structures with desired space group
properties. Our experiments highlight the effectiveness of iterative DPO,
achieving $\sim$115\% and $\sim$50\% improvements in unconditional and space
group conditioned generation, respectively, compared to fine-tuning alone. Our
work demonstrates the potential of adapting post-training techniques from
natural language processing to materials design, paving the way for targeted
and efficient discovery of novel materials.

</details>


### [101] [Predicting effect of novel treatments using molecular pathways and real-world data](https://arxiv.org/abs/2509.07204)
*Adrien Couetoux,Thomas Devenyns,Lise Diagne,David Champagne,Pierre-Yves Mousset,Chris Anagnostopoulos*

Main category: cs.LG

TL;DR: 提出基于机器学习的方法预测未测试药物疗效，用真实数据集验证并讨论迭代方式。


<details>
  <summary>Details</summary>
Motivation: 解决制药研发中在临床测试或实际使用前预测药物治疗特定疾病疗效的难题。

Method: 用药物 - 通路权重影响分数集和患者数据训练机器学习模型，分析未测试药物在生物分子 - 蛋白质通路的加权影响分数以生成预测疗效值。

Result: 在含患者治疗和结果的真实数据集上验证方法，给出评估泛化性能和确定最具预测性条件的方法。

Conclusion: 该方法可迭代，为利用真实世界数据和药物嵌入预测未测试药物效果提供初始框架。

Abstract: In pharmaceutical R&D, predicting the efficacy of a pharmaceutical in
treating a particular disease prior to clinical testing or any real-world use
has been challenging. In this paper, we propose a flexible and modular machine
learning-based approach for predicting the efficacy of an untested
pharmaceutical for treating a disease. We train a machine learning model using
sets of pharmaceutical-pathway weight impact scores and patient data, which can
include patient characteristics and observed clinical outcomes. The resulting
model then analyses weighted impact scores of an untested pharmaceutical across
human biological molecule-protein pathways to generate a predicted efficacy
value. We demonstrate how the method works on a real-world dataset with patient
treatments and outcomes, with two different weight impact score algorithms We
include methods for evaluating the generalisation performance on unseen
treatments, and to characterise conditions under which the approach can be
expected to be most predictive. We discuss specific ways in which our approach
can be iterated on, making it an initial framework to support future work on
predicting the effect of untested drugs, leveraging RWD clinical data and drug
embeddings.

</details>


### [102] [Explaining How Quantization Disparately Skews a Model](https://arxiv.org/abs/2509.07222)
*Abhimanyu Bellam,Jung-Eun Kim*

Main category: cs.LG

TL;DR: 研究量化对不同群体影响，提出混合精度量化感知训练等方法实现公平部署量化神经网络。


<details>
  <summary>Details</summary>
Motivation: 发现量化会加剧对不同群体的差异影响，分析其在网络前后向传播中的影响因素。

Method: 分析量化引起的权重和激活值变化在网络中的级联影响，拓展研究对群体梯度范数和Hessian矩阵特征值的影响，提出混合精度量化感知训练结合数据集采样方法和加权损失函数。

Result: 明确量化导致logits方差降低、损失增加、群体准确率受损，从优化角度洞察网络状态。

Conclusion: 提出的方法可实现量化神经网络的公平部署。

Abstract: Post Training Quantization (PTQ) is widely adopted due to its high
compression capacity and speed with minimal impact on accuracy. However, we
observed that disparate impacts are exacerbated by quantization, especially for
minority groups. Our analysis explains that in the course of quantization there
is a chain of factors attributed to a disparate impact across groups during
forward and backward passes. We explore how the changes in weights and
activations induced by quantization cause cascaded impacts in the network,
resulting in logits with lower variance, increased loss, and compromised group
accuracies. We extend our study to verify the influence of these impacts on
group gradient norms and eigenvalues of the Hessian matrix, providing insights
into the state of the network from an optimization point of view. To mitigate
these effects, we propose integrating mixed precision Quantization Aware
Training (QAT) with dataset sampling methods and weighted loss functions,
therefore providing fair deployment of quantized neural networks.

</details>


### [103] [Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2509.07238)
*Pranav Pawar,Dhwaj Jain,Varun Gupta,Kaustav Dedhia,Dashrath Kale,Sudhir Dhekane*

Main category: cs.LG

TL;DR: 文章通过实验不同配置对数学推理任务模型参数微调，提出优化框架，在多个模型上提升效率和性能，找到通用优化趋势。


<details>
  <summary>Details</summary>
Motivation: 对数学推理任务的模型参数进行微调，提升模型在该任务上的效率和性能。

Method: 通过实验不同配置（随机性控制、推理深度、采样策略等），系统搜索参数空间（温度、推理步骤等），在数学推理基准上测试确定最优配置。

Result: 实现平均29.4%的计算成本降低和23.9%的推理速度提升，DeepSeek - V3准确率达98%，Mixtral - 8x22B性价比最高。

Conclusion: 提出首个针对五个SOTA模型的综合优化研究、标准化框架，发现通用优化趋势并给出生产就绪配置。

Abstract: This paper presents a practical investigation into fine-tuning model
parameters for mathematical reasoning tasks through experimenting with various
configurations including randomness control, reasoning depth, and sampling
strategies, careful tuning demonstrates substantial improvements in efficiency
as well as performance. A holistically optimized framework is introduced for
five state-of-the-art models on mathematical reasoning tasks, exhibiting
significant performance boosts while maintaining solution correctness. Through
systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B,
DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are
demonstrated with 100% optimization success rate. The methodology achieves an
average 29.4% reduction in computational cost and 23.9% improvement in
inference speed across all tested models. This framework systematically
searches parameter spaces including temperature (0.1-0.5), reasoning steps
(4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining
optimal configurations through testing on mathematical reasoning benchmarks.
Critical findings show that lower temperature regimes (0.1-0.4) and reduced
reasoning steps (4-6) consistently enhance efficiency without compromising
accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B
delivers the most cost-effective performance at 361.5 tokens per accurate
response. Key contributions include: (1) the first comprehensive optimization
study for five diverse SOTA models in mathematical reasoning, (2) a
standardized production-oriented parameter optimization framework, (3)
discovery of universal optimization trends applicable across model
architectures, and (4) production-ready configurations with extensive
performance characterization.

</details>


### [104] [IP-Basis PINNs: Efficient Multi-Query Inverse Parameter Estimation](https://arxiv.org/abs/2509.07245)
*Shalev Manor,Mohammad Kohandel*

Main category: cs.LG

TL;DR: 提出Inverse - Parameter Basis PINNs（IP - Basis PINNs）元学习框架解决PINNs在多查询场景下计算成本高的问题，在三个基准测试中展示了有效性。


<details>
  <summary>Details</summary>
Motivation: PINNs在多查询场景下解决逆问题计算成本高，每个新观测数据都需昂贵的训练过程。

Method: 采用离线 - 在线分解，离线训练深度网络生成基函数，在线针对新逆问题冻结网络，仅训练轻量级线性输出层；有新的在线损失公式、用前向自动微分减少计算开销、非平凡的验证和提前停止机制。

Result: 在三个不同基准测试中，在常数和函数参数估计上表现一致，每次查询比标准PINNs显著提速，在稀缺和噪声数据下能稳健运行。

Conclusion: IP - Basis PINNs框架能有效且高效地解决逆问题。

Abstract: Solving inverse problems with Physics-Informed Neural Networks (PINNs) is
computationally expensive for multi-query scenarios, as each new set of
observed data requires a new, expensive training procedure. We present
Inverse-Parameter Basis PINNs (IP-Basis PINNs), a meta-learning framework that
extends the foundational work of Desai et al. (2022) to enable rapid and
efficient inference for inverse problems. Our method employs an offline-online
decomposition: a deep network is first trained offline to produce a rich set of
basis functions that span the solution space of a parametric differential
equation. For each new inverse problem online, this network is frozen, and
solutions and parameters are inferred by training only a lightweight linear
output layer against observed data. Key innovations that make our approach
effective for inverse problems include: (1) a novel online loss formulation for
simultaneous solution reconstruction and parameter identification, (2) a
significant reduction in computational overhead via forward-mode automatic
differentiation for PDE loss evaluation, and (3) a non-trivial validation and
early-stopping mechanism for robust offline training. We demonstrate the
efficacy of IP-Basis PINNs on three diverse benchmarks, including an extension
to universal PINNs for unknown functional terms-showing consistent performance
across constant and functional parameter estimation, a significant speedup per
query over standard PINNs, and robust operation with scarce and noisy data.

</details>


### [105] [GCond: Gradient Conflict Resolution via Accumulation-based Stabilization for Large-Scale Multi-Task Learning](https://arxiv.org/abs/2509.07252)
*Evgeny Alves Limarenko,Anastasiia Alexandrovna Studenikina*

Main category: cs.LG

TL;DR: 提出Gradient Conductor (GCond)解决多任务学习中梯度冲突问题，在多数据集和模型上验证其高效可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有解决梯度冲突的方法计算需求大，限制在现代大模型和transformer中的应用。

Method: 基于PCGrad原则，结合梯度累积和自适应仲裁机制提出GCond方法。

Result: GCond随机模式计算速度提升两倍，保持优化质量，在各评估指标上表现优异，可扩展到不同规模模型，与现代优化器兼容。

Conclusion: GCond为多任务学习中的梯度冲突问题提供了可扩展且高效的解决方案。

Abstract: In multi-task learning (MTL), gradient conflict poses a significant
challenge. Effective methods for addressing this problem, including PCGrad,
CAGrad, and GradNorm, in their original implementations are computationally
demanding, which significantly limits their application in modern large models
and transformers. We propose Gradient Conductor (GCond), a method that builds
upon PCGrad principles by combining them with gradient accumulation and an
adaptive arbitration mechanism. We evaluated GCond on self-supervised learning
tasks using MobileNetV3-Small and ConvNeXt architectures on the ImageNet 1K
dataset and a combined head and neck CT scan dataset, comparing the proposed
method against baseline linear combinations and state-of-the-art gradient
conflict resolution methods. The stochastic mode of GCond achieved a two-fold
computational speedup while maintaining optimization quality, and demonstrated
superior performance across all evaluated metrics, achieving lower L1 and SSIM
losses compared to other methods on both datasets. GCond exhibited high
scalability, being successfully applied to both compact models
(MobileNetV3-Small) and large architectures (ConvNeXt-tiny and ConvNeXt-Base).
It also showed compatibility with modern optimizers such as AdamW and
Lion/LARS. Therefore, GCond offers a scalable and efficient solution to the
problem of gradient conflicts in multi-task learning.

</details>


### [106] [Learning Generalized Hamiltonian Dynamics with Stability from Noisy Trajectory Data](https://arxiv.org/abs/2509.07280)
*Luke McLennan,Yi Wang,Ryan Farell,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: 提出基于变分贝叶斯推理从噪声稀疏相空间数据无监督学习广义哈密顿动力学的框架，用核化证据下界损失和约束项提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 不同类型哈密顿系统有相同初始总能量时，单一哈密顿网络模型难从采样轨迹捕捉相空间运动动力学和物理特性。

Method: 扩展稀疏辛随机傅里叶高斯过程学习，结合状态和共轭动量哈密顿动力学的广义形式；用核化证据下界损失保证数据保真度，加入稳定性和守恒性约束项平衡多重梯度。

Result: 未提及具体结果。

Conclusion: 文档未明确提及结论。

Abstract: We introduce a robust framework for learning various generalized Hamiltonian
dynamics from noisy, sparse phase-space data and in an unsupervised manner
based on variational Bayesian inference. Although conservative, dissipative,
and port-Hamiltonian systems might share the same initial total energy of a
closed system, it is challenging for a single Hamiltonian network model to
capture the distinctive and varying motion dynamics and physics of a phase
space, from sampled observational phase space trajectories. To address this
complicated Hamiltonian manifold learning challenge, we extend sparse
symplectic, random Fourier Gaussian processes learning with predictive
successive numerical estimations of the Hamiltonian landscape, using a
generalized form of state and conjugate momentum Hamiltonian dynamics,
appropriate to different classes of conservative, dissipative and
port-Hamiltonian physical systems. In addition to the kernelized evidence lower
bound (ELBO) loss for data fidelity, we incorporate stability and conservation
constraints as additional hyper-parameter balanced loss terms to regularize the
model's multi-gradients, enforcing physics correctness for improved prediction
accuracy with bounded uncertainty.

</details>


### [107] [ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers](https://arxiv.org/abs/2509.07282)
*Jeff Shen,Lindsay Smith*

Main category: cs.LG

TL;DR: 提出用密码解谜作为研究神经网络泛化的测试平台，开发ALICE模型在解密问题上达新水平，能泛化到未见密码，还引入新解码头增强可解释性，分析揭示其预测过程，成果可拓展到其他领域。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在组合复杂领域的泛化能力，选择密码解谜作为测试平台。

Method: 开发ALICE（一种仅含编码器的Transformer），引入基于Gumbel - Sinkhorn方法的双射解码头，进行早期退出分析。

Result: ALICE在解密问题上准确率和速度达新的最优水平，在少量独特密码上训练后能泛化到未见密码，可直接提取学习到的密码映射，分析揭示其预测过程类似人类策略。

Conclusion: 架构创新和分析方法可拓展到有双射映射和组合结构的领域，为神经网络泛化和可解释性提供新见解。

Abstract: We present cryptogram solving as an ideal testbed for studying neural network
generalization in combinatorially complex domains. In this task, models must
decrypt text encoded with substitution ciphers, choosing from 26! possible
mappings without explicit access to the cipher. We develop ALICE (an
Architecture for Learning Interpretable Cryptogram dEcipherment): a simple
encoder-only Transformer that sets a new state-of-the-art for both accuracy and
speed on this decryption problem. Surprisingly, ALICE generalizes to unseen
ciphers after training on only ${\sim}1500$ unique ciphers, a minute fraction
($3.7 \times 10^{-24}$) of the possible cipher space. To enhance
interpretability, we introduce a novel bijective decoding head that explicitly
models permutations via the Gumbel-Sinkhorn method, enabling direct extraction
of learned cipher mappings. Through early exit analysis, we reveal how ALICE
progressively refines its predictions in a way that appears to mirror common
human strategies for this task: early layers employ frequency-based heuristics,
middle layers form word structures, and final layers correct individual
characters. Our architectural innovations and analysis methods extend beyond
cryptograms to any domain with bijective mappings and combinatorial structure,
offering new insights into neural network generalization and interpretability.

</details>


### [108] [CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation](https://arxiv.org/abs/2509.07325)
*Alyssa Unell,Noel C. F. Codella,Sam Preston,Peniel Argaw,Wen-wai Yim,Zelalem Gero,Cliff Wong,Rajesh Jena,Eric Horvitz,Amanda K. Hall,Ruican Rachel Zhong,Jiachen Li,Shrey Jain,Mu Wei,Matthew Lungren,Hoifung Poon*

Main category: cs.LG

TL;DR: 提出基于大语言模型（LLM）代理的方法，为非小细胞肺癌（NSCLC）患者自动生成符合指南的治疗方案，构建数据集、验证LLM能力并开发混合方法，建立可行框架。


<details>
  <summary>Details</summary>
Motivation: 将复杂患者情况转化为符合癌症治疗指南的推荐方案耗时、需专业知识且易出错，LLM能力进步有望减少生成推荐时间并提高准确性。

Method: 构建NSCLC患者纵向数据集；利用现有LLM生成高质量代理基准；开发结合人工注释和模型一致性信息的混合方法，创建代理框架和元分类器。

Result: 生成的代理基准与专家注释基准有强相关性（Spearman系数r = 0.88，RMSE = 0.08）；元分类器验证治疗推荐准确性的AUROC为0.800。

Conclusion: 建立了基于LLM的临床可行的指南遵循系统框架，平衡了准确性、可解释性和监管要求，降低注释成本，为自动临床决策支持提供可扩展途径。

Abstract: The National Comprehensive Cancer Network (NCCN) provides evidence-based
guidelines for cancer treatment. Translating complex patient presentations into
guideline-compliant treatment recommendations is time-intensive, requires
specialized expertise, and is prone to error. Advances in large language model
(LLM) capabilities promise to reduce the time required to generate treatment
recommendations and improve accuracy. We present an LLM agent-based approach to
automatically generate guideline-concordant treatment trajectories for patients
with non-small cell lung cancer (NSCLC). Our contributions are threefold.
First, we construct a novel longitudinal dataset of 121 cases of NSCLC patients
that includes clinical encounters, diagnostic results, and medical histories,
each expertly annotated with the corresponding NCCN guideline trajectories by
board-certified oncologists. Second, we demonstrate that existing LLMs possess
domain-specific knowledge that enables high-quality proxy benchmark generation
for both model development and evaluation, achieving strong correlation
(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.
Third, we develop a hybrid approach combining expensive human annotations with
model consistency information to create both the agent framework that predicts
the relevant guidelines for a patient, as well as a meta-classifier that
verifies prediction accuracy with calibrated confidence scores for treatment
recommendations (AUROC=0.800), a critical capability for communicating the
accuracy of outputs, custom-tailoring tradeoffs in performance, and supporting
regulatory compliance. This work establishes a framework for clinically viable
LLM-based guideline adherence systems that balance accuracy, interpretability,
and regulatory requirements while reducing annotation costs, providing a
scalable pathway toward automated clinical decision support.

</details>


### [109] [General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases](https://arxiv.org/abs/2509.07330)
*Li-Chin Chen,Ji-Tian Sheu,Yuh-Jue Chuang*

Main category: cs.LG

TL;DR: 提出针对年龄和性别通用人口统计预训练（GDP）模型，实验表明其能提升性能，为医疗应用预测性能改善提供方向。


<details>
  <summary>Details</summary>
Motivation: 人口统计属性在电子健康记录中重要，但在模型设计常为辅助角色，对其表示学习关注有限。

Method: 提出GDP模型，用不同地区、疾病和人口构成数据集预训练和评估，探索排序策略和编码方法组合将表格人口统计输入转化为潜在嵌入。

Result: 顺序排序显著提升模型性能，即使在人口统计属性预测价值低的数据集，也能增强其表示重要性和在下游梯度提升模型中的影响。

Conclusion: 表格人口统计属性基础模型可跨任务和人群泛化，为医疗应用改善预测性能提供有前景方向。

Abstract: Demographic attributes are universally present in electronic health records
and serve as vital predictors in clinical risk stratification and treatment
decisions. Despite their significance, these attributes are often relegated to
auxiliary roles in model design, with limited attention has been given to
learning their representations. This study proposes a General Demographic
Pre-trained (GDP) model as a foundational representation framework tailored to
age and gender. The model is pre-trained and evaluated using datasets with
diverse diseases and population compositions from different geographic regions.
The GDP architecture explores combinations of ordering strategies and encoding
methods to transform tabular demographic inputs into latent embeddings.
Experimental results demonstrate that sequential ordering substantially
improves model performance in discrimination, calibration, and the
corresponding information gain at each decision tree split, particularly in
diseases where age and gender contribute significantly to risk stratification.
Even in datasets where demographic attributes hold relatively low predictive
value, GDP enhances the representational importance, increasing their influence
in downstream gradient boosting models. The findings suggest that foundational
models for tabular demographic attributes can generalize across tasks and
populations, offering a promising direction for improving predictive
performance in healthcare applications.

</details>


### [110] [SBS: Enhancing Parameter-Efficiency of Neural Representations for Neural Networks via Spectral Bias Suppression](https://arxiv.org/abs/2509.07373)
*Qihu Xie,Yuan Li,Yi Kang*

Main category: cs.LG

TL;DR: 本文提出 SBS 方法增强神经网络的神经表示，抑制频谱偏差，在多数据集上评估显示其比 SOTA 用更少参数实现更好重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络的神经表示中标准多层感知器存在频谱偏差，影响高频细节重建能力。

Method: 提出 SBS 方法，采用基于单向排序的平滑和基于单向排序平滑感知的随机傅里叶特征两种技术抑制频谱偏差。

Result: 在 CIFAR - 10、CIFAR - 100 和 ImageNet 数据集的各种 ResNet 模型上进行评估，SBS 用更少参数实现了比 SOTA 更好的重建精度。

Conclusion: SBS 是一种参数高效的神经网络神经表示增强方法，能有效抑制频谱偏差。

Abstract: Implicit neural representations have recently been extended to represent
convolutional neural network weights via neural representation for neural
networks, offering promising parameter compression benefits. However, standard
multi-layer perceptrons used in neural representation for neural networks
exhibit a pronounced spectral bias, hampering their ability to reconstruct
high-frequency details effectively. In this paper, we propose SBS, a
parameter-efficient enhancement to neural representation for neural networks
that suppresses spectral bias using two techniques: (1) a unidirectional
ordering-based smoothing that improves kernel smoothness in the output space,
and (2) unidirectional ordering-based smoothing aware random fourier features
that adaptively modulate the frequency bandwidth of input encodings based on
layer-wise parameter count. Extensive evaluations on various ResNet models with
datasets CIFAR-10, CIFAR-100, and ImageNet, demonstrate that SBS achieves
significantly better reconstruction accuracy with less parameters compared to
SOTA.

</details>


### [111] [EfficientNet in Digital Twin-based Cardiac Arrest Prediction and Analysis](https://arxiv.org/abs/2509.07388)
*Qasim Zia,Avais Jan,Zafar Iqbal,Muhammad Mumtaz Ali,Mukarram Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 提出结合EfficientNet深度学习模型与数字孪生系统的框架，用于改善心脏骤停的早期检测与分析，实验表明系统准确高效。


<details>
  <summary>Details</summary>
Motivation: 心脏骤停是重大全球健康问题，早期识别和管理对改善患者预后至关重要。

Method: 使用复合缩放和EfficientNet学习心血管图像特征，数字孪生基于物联网设备数据创建患者个性化心血管系统模型。

Result: 所提出的系统预测能力高度准确且高效。

Conclusion: 结合深度学习和数字孪生技术为预测心脏病提供了积极且个性化的方法。

Abstract: Cardiac arrest is one of the biggest global health problems, and early
identification and management are key to enhancing the patient's prognosis. In
this paper, we propose a novel framework that combines an EfficientNet-based
deep learning model with a digital twin system to improve the early detection
and analysis of cardiac arrest. We use compound scaling and EfficientNet to
learn the features of cardiovascular images. In parallel, the digital twin
creates a realistic and individualized cardiovascular system model of the
patient based on data received from the Internet of Things (IoT) devices
attached to the patient, which can help in the constant assessment of the
patient and the impact of possible treatment plans. As shown by our
experiments, the proposed system is highly accurate in its prediction abilities
and, at the same time, efficient. Combining highly advanced techniques such as
deep learning and digital twin (DT) technology presents the possibility of
using an active and individual approach to predicting cardiac disease.

</details>


### [112] [Hybrid GCN-GRU Model for Anomaly Detection in Cryptocurrency Transactions](https://arxiv.org/abs/2509.07392)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Soyoun Kim,Sunyoung Moon,Sua Lee,Jaeyoung Choi,Hyemin Lee,Sangmi Chai*

Main category: cs.LG

TL;DR: 提出混合GCN - GRU模型检测区块链交易网络非法活动，用比特币交易数据验证，性能超基线。


<details>
  <summary>Details</summary>
Motivation: 区块链交易网络复杂，需检测非法活动。

Method: 提出混合GCN - GRU模型，捕捉结构和序列特征。

Result: 使用2020 - 2024年比特币交易数据，模型准确率达0.9470，AUC - ROC达0.9807，超所有基线。

Conclusion: 混合GCN - GRU模型在检测区块链交易网络非法活动上表现良好。

Abstract: Blockchain transaction networks are complex, with evolving temporal patterns
and inter-node relationships. To detect illicit activities, we propose a hybrid
GCN-GRU model that captures both structural and sequential features. Using real
Bitcoin transaction data (2020-2024), our model achieved 0.9470 Accuracy and
0.9807 AUC-ROC, outperforming all baselines.

</details>


### [113] [EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise](https://arxiv.org/abs/2509.07415)
*Arslan Majal,Aamir Hussain Chughtai,Muhammad Tahir*

Main category: cs.LG

TL;DR: 提出基于学习的抗异常值滤波器EMORF - II，实验表明其精度提升但计算开销增加，复杂度与实用方法相当。


<details>
  <summary>Details</summary>
Motivation: 在测量噪声可能相关的一般设置下，改进现有抗异常值滤波器的性能。

Method: 提出EMORF - II，它是基于EM的抗异常值滤波器的增强版本，在推理过程中增加学习异常值特征和检测异常值的功能。

Result: 数值实验显示，与现有方法相比，EMORF - II在精度上有提升，但计算开销增加。

Conclusion: EMORF - II计算复杂度与其他实用方法相当，是多种应用的有用选择。

Abstract: We present a learning-based outlier-robust filter for a general setup where
the measurement noise can be correlated. Since it is an enhanced version of
EM-based outlier robust filter (EMORF), we call it as EMORF-II. As it is
equipped with an additional powerful feature to learn the outlier
characteristics during inference along with outlier-detection, EMORF-II has
improved outlier-mitigation capability. Numerical experiments confirm
performance gains as compared to the state-of-the-art methods in terms of
accuracy with an increased computational overhead. However, thankfully the
computational complexity order remains at par with other practical methods
making it a useful choice for diverse applications.

</details>


### [114] [The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2509.07430)
*Long Li,Jiaran Hao,Jason Klein Liu,Zhijian Zhou,Xiaoyu Tan,Wei Chu,Zhe Wang,Shirui Pan,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: 论文指出微调大语言模型时存在Pass@k性能下降和灾难性遗忘问题，提出DPH - RL框架，利用f - 散度解决问题并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决用可验证奖励的强化学习微调大语言模型时出现的Pass@k性能下降和灾难性遗忘问题，且现有方法未充分研究散度项的选择和作用。

Method: 提出DPH - RL框架，利用质量覆盖f - 散度作为排练机制，持续参考初始策略。

Result: 在数学和SQL生成的实验中，DPH - RL解决了Pass@k下降问题，提升了Pass@1和Pass@k，且训练更高效。

Conclusion: 恰当选择散度度量是构建更通用、更多样推理模型的有力工具，是改进可验证奖励强化学习的关键被忽视方面。

Abstract: A central paradox in fine-tuning Large Language Models (LLMs) with
Reinforcement Learning with Verifiable Reward (RLVR) is the frequent
degradation of multi-attempt performance (Pass@k) despite improvements in
single-attempt accuracy (Pass@1). This is often accompanied by catastrophic
forgetting, where models lose previously acquired skills. While various methods
have been proposed, the choice and function of the divergence term have been
surprisingly unexamined as a proactive solution. We argue that standard RLVR
objectives -- both those using the mode-seeking reverse KL-divergence and those
forgoing a divergence term entirely -- lack a crucial mechanism for knowledge
retention. The reverse-KL actively accelerates this decay by narrowing the
policy, while its absence provides no safeguard against the model drifting from
its diverse knowledge base. We propose a fundamental shift in perspective:
using the divergence term itself as the solution. Our framework,
Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences
(like forward-KL and JS-divergence) to function as a rehearsal mechanism. By
continuously referencing the initial policy, this approach forces the model to
maintain broad solution coverage. Extensive experiments on math and SQL
generation demonstrate that DPH-RL not only resolves the Pass@k degradation but
improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is
more training-efficient because it computes f-divergence using generator
functions, requiring only sampling from the initial policy and no online
reference model. Our work highlights a crucial, overlooked axis for improving
RLVR, demonstrating that the proper selection of a divergence measure is a
powerful tool for building more general and diverse reasoning models.

</details>


### [115] [Conv4Rec: A 1-by-1 Convolutional AutoEncoder for User Profiling through Joint Analysis of Implicit and Explicit Feedbacks](https://arxiv.org/abs/2509.07499)
*Antoine Ledent,Petr Kasalický,Rodrigo Alves,Hady W. Lauw*

Main category: cs.LG

TL;DR: 提出新的卷积自动编码器架构用于用户建模和推荐任务，有多项改进，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 改进现有技术，更好地处理用户建模和推荐任务，提高预测性能和信息性。

Method: 引入新的卷积自动编码器架构，可学习不同交互类型关联，联合显式评分和隐式反馈学习，给出泛化界。

Result: 在多个真实数据集实验中，单模型在隐式和显式反馈预测任务达SOTA，预测具有额外可解释性。

Conclusion: 新架构有效，能提高推荐任务性能，且优化损失函数可保证恢复交互采样分布。

Abstract: We introduce a new convolutional AutoEncoder architecture for user modelling
and recommendation tasks with several improvements over the state of the art.
Firstly, our model has the flexibility to learn a set of associations and
combinations between different interaction types in a way that carries over to
each user and item. Secondly, our model is able to learn jointly from both the
explicit ratings and the implicit information in the sampling pattern (which we
refer to as `implicit feedback'). It can also make separate predictions for the
probability of consuming content and the likelihood of granting it a high
rating if observed. This not only allows the model to make predictions for both
the implicit and explicit feedback, but also increases the informativeness of
the predictions: in particular, our model can identify items which users would
not have been likely to consume naturally, but would be likely to enjoy if
exposed to them. Finally, we provide several generalization bounds for our
model, which to the best of our knowledge, are among the first generalization
bounds for auto-encoders in a Recommender Systems setting; we also show that
optimizing our loss function guarantees the recovery of the exact sampling
distribution over interactions up to a small error in total variation. In
experiments on several real-life datasets, we achieve state-of-the-art
performance on both the implicit and explicit feedback prediction tasks despite
relying on a single model for both, and benefiting from additional
interpretability in the form of individual predictions for the probabilities of
each possible rating.

</details>


### [116] [Water Demand Forecasting of District Metered Areas through Learned Consumer Representations](https://arxiv.org/abs/2509.07515)
*Adithya Ramachandran,Thorkil Flensmark B. Neergaard,Tomás Arias-Vergara,Andreas Maier,Siming Bayer*

Main category: cs.LG

TL;DR: 本文提出用于分区计量区域短期用水需求预测的新方法，经评估有更好预测表现并能识别受社会经济因素影响的用户。


<details>
  <summary>Details</summary>
Motivation: 气候变化增加不确定性，保障水资源和供应成为紧迫全球问题，且由于非确定性因素，用水需求预测仍具挑战。

Method: 应用无监督对比学习根据消费行为对终端用户分类，利用小波变换卷积网络结合交叉注意力机制进行需求预测。

Result: 在六个月内对实际分区计量区域评估，不同区域平均绝对百分比误差（MAPE）有改善，最大提升4.9%，还能识别受社会经济因素影响的用户。

Conclusion: 所提方法在短期用水需求预测方面表现更好，能增强对影响需求的确定性模式的了解。

Abstract: Advancements in smart metering technologies have significantly improved the
ability to monitor and manage water utilities. In the context of increasing
uncertainty due to climate change, securing water resources and supply has
emerged as an urgent global issue with extensive socioeconomic ramifications.
Hourly consumption data from end-users have yielded substantial insights for
projecting demand across regions characterized by diverse consumption patterns.
Nevertheless, the prediction of water demand remains challenging due to
influencing non-deterministic factors, such as meteorological conditions. This
work introduces a novel method for short-term water demand forecasting for
District Metered Areas (DMAs) which encompass commercial, agricultural, and
residential consumers. Unsupervised contrastive learning is applied to
categorize end-users according to distinct consumption behaviors present within
a DMA. Subsequently, the distinct consumption behaviors are utilized as
features in the ensuing demand forecasting task using wavelet-transformed
convolutional networks that incorporate a cross-attention mechanism combining
both historical data and the derived representations. The proposed approach is
evaluated on real-world DMAs over a six-month period, demonstrating improved
forecasting performance in terms of MAPE across different DMAs, with a maximum
improvement of 4.9%. Additionally, it identifies consumers whose behavior is
shaped by socioeconomic factors, enhancing prior knowledge about the
deterministic patterns that influence demand.

</details>


### [117] [RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection](https://arxiv.org/abs/2509.07523)
*Jad Yehya,Mansour Benbakoura,Cédric Allain,Benoît Malezieux,Matthieu Kowalski,Thomas Moreau*

Main category: cs.LG

TL;DR: 本文提出RoseCDL算法用于长信号无监督稀有事件检测，结合随机窗口和离群值检测，使CDL成为现实信号事件发现工具。


<details>
  <summary>Details</summary>
Motivation: 卷积字典学习（CDL）用于检测稀有或异常事件研究不足，且存在计算成本高和对伪影及离群值敏感的问题，需改进用于长信号无监督稀有事件检测。

Method: 提出RoseCDL算法，结合随机窗口进行大数据集高效训练与在线离群值检测以增强鲁棒性。

Result: 将CDL重新构建为现实信号事件发现和表征的实用工具。

Conclusion: RoseCDL算法扩展了CDL在传统任务之外的作用，可用于实际信号的稀有事件检测。

Abstract: Identifying recurring patterns and rare events in large-scale signals is a
fundamental challenge in fields such as astronomy, physical simulations, and
biomedical science. Convolutional Dictionary Learning (CDL) offers a powerful
framework for modeling local structures in signals, but its use for detecting
rare or anomalous events remains largely unexplored. In particular, CDL faces
two key challenges in this setting: high computational cost and sensitivity to
artifacts and outliers. In this paper, we introduce RoseCDL, a scalable and
robust CDL algorithm designed for unsupervised rare event detection in long
signals. RoseCDL combines stochastic windowing for efficient training on large
datasets with inline outlier detection to enhance robustness and isolate
anomalous patterns. This reframes CDL as a practical tool for event discovery
and characterization in real-world signals, extending its role beyond
traditional tasks like compression or denoising.

</details>


### [118] [$ΔL$ Normalization: Rethink Loss Aggregation in RLVR](https://arxiv.org/abs/2509.07558)
*Zhiyuan He,Xufang Luo,Yike Zhang,Yuqing Yang,Lili Qiu*

Main category: cs.LG

TL;DR: 提出ΔL Normalization解决强化学习中响应长度变化导致的梯度方差问题，实验效果好


<details>
  <summary>Details</summary>
Motivation: 强化学习中响应长度变化大，导致梯度方差高、优化不稳定，现有方法存在估计偏差或梯度方差高的问题

Method: 理论和实证分析不同长度对策略损失的影响，将问题转化为寻找最小方差无偏估计量，提出ΔL Normalization

Result: 在不同模型大小、最大长度和任务上均取得了优异的结果

Conclusion: ΔL Normalization能提供真实策略损失的无偏估计，理论上最小化梯度方差，效果良好，代码将公开

Abstract: We propose $\Delta L$ Normalization, a simple yet effective loss aggregation
method tailored to the characteristic of dynamic generation lengths in
Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has
demonstrated strong potential in improving the reasoning capabilities of large
language models (LLMs), but a major challenge lies in the large variability of
response lengths during training, which leads to high gradient variance and
unstable optimization. Although previous methods such as GRPO, DAPO, and Dr.
GRPO introduce different loss normalization terms to address this issue, they
either produce biased estimates or still suffer from high gradient variance. By
analyzing the effect of varying lengths on policy loss both theoretically and
empirically, we reformulate the problem as finding a minimum-variance unbiased
estimator. Our proposed $\Delta L$ Normalization not only provides an unbiased
estimate of the true policy loss but also minimizes gradient variance in
theory. Extensive experiments show that it consistently achieves superior
results across different model sizes, maximum lengths, and tasks. Our code will
be made public at https://github.com/zerolllin/Delta-L-Normalization.

</details>


### [119] [Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks](https://arxiv.org/abs/2509.07579)
*Liya Gaynutdinova,Martin Doškář,Ondřej Rokoš,Ivana Pultarová*

Main category: cs.LG

TL;DR: 本文引入对偶公式改进周期性热传导复合材料均匀化PINN框架可靠性，对比不同PINN方法，指出对偶公式能增强PINN在微观力学均匀化问题的适用性。


<details>
  <summary>Details</summary>
Motivation: PINN在处理具有不连续系数材料时易失败，需要改进其在周期性热传导复合材料均匀化中的可靠性。

Method: 引入对偶公式，对比标准PINN应用于平滑材料近似与使用谱和基于神经网络测试函数的变分PINN（VPINN）。

Result: 强形式PINN在特定设置下可能表现更好，但对材料不连续性敏感且无明确诊断易失败；VPINN可直接处理分段常数材料参数，但需谨慎选择测试函数避免不稳定。

Conclusion: 对偶公式是收敛质量的可靠指标，将其集成到PINN框架可增强其在微观力学均匀化问题中的适用性。

Abstract: Physics-informed neural networks (PINNs) have shown promise in solving
partial differential equations (PDEs) relevant to multiscale modeling, but they
often fail when applied to materials with discontinuous coefficients, such as
media with piecewise constant properties. This paper introduces a dual
formulation for the PINN framework to improve the reliability of the
homogenization of periodic thermo-conductive composites, for both strong and
variational (weak) formulations. The dual approach facilitates the derivation
of guaranteed upper and lower error bounds, enabling more robust detection of
PINN failure. We compare standard PINNs applied to smoothed material
approximations with variational PINNs (VPINNs) using both spectral and neural
network-based test functions. Our results indicate that while strong-form PINNs
may outperform VPINNs in controlled settings, they are sensitive to material
discontinuities and may fail without clear diagnostics. In contrast, VPINNs
accommodate piecewise constant material parameters directly but require careful
selection of test functions to avoid instability. Dual formulation serves as a
reliable indicator of convergence quality, and its integration into PINN
frameworks enhances their applicability to homogenization problems in
micromechanics.

</details>


### [120] [Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards](https://arxiv.org/abs/2509.07603)
*Mehdi Bejani,Marco Mauri,Daniele Acconcia,Simone Todaro,Stefano Mariani*

Main category: cs.LG

TL;DR: 提出基于Transformer深度学习策略优化半导体探针卡传感器布局，模型分类准确率高，能确定关键传感器位置，助力半导体制造。


<details>
  <summary>Details</summary>
Motivation: 探针卡故障影响半导体制造良率和可靠性，需优化传感器布局来监测结构健康。

Method: 采用模拟故障场景的频率响应函数，通过物理场景扩展和数据增强构建数据集，训练CNN和Transformer混合模型，用10折分层交叉验证评估。

Result: 模型对探针卡健康状态分类准确率达99.83%，裂缝检测召回率99.73%，通过交叉验证确认鲁棒性，注意力机制确定关键传感器位置。

Conclusion: 基于注意力的深度学习可推动主动维护，提高半导体制造的可靠性和良率。

Abstract: This paper presents an innovative Transformer-based deep learning strategy
for optimizing the placement of sensors aiming at structural health monitoring
of semiconductor probe cards. Failures in probe cards, including substrate
cracks and loosened screws, would critically affect semiconductor manufacturing
yield and reliability. Some failure modes could be detected by equipping a
probe card with adequate sensors. Frequency response functions from simulated
failure scenarios are adopted within a finite element model of a probe card. A
comprehensive dataset, enriched by physics-informed scenario expansion and
physics-aware statistical data augmentation, is exploited to train a hybrid
Convolutional Neural Network and Transformer model. The model achieves high
accuracy (99.83%) in classifying the probe card health states (baseline, loose
screw, crack) and an excellent crack detection recall (99.73%). Model
robustness is confirmed through a rigorous framework of 3 repetitions of
10-fold stratified cross-validation. The attention mechanism also pinpoints
critical sensor locations: an analysis of the attention weights offers
actionable insights for designing efficient, cost-effective monitoring systems
by optimizing sensor configurations. This research highlights the capability of
attention-based deep learning to advance proactive maintenance, enhancing
operational reliability and yield in semiconductor manufacturing.

</details>


### [121] [K2-Think: A Parameter-Efficient Reasoning System](https://arxiv.org/abs/2509.07604)
*Zhoujun Cheng,Richard Fan,Shibo Hao,Taylor W. Killian,Haonan Li,Suqi Sun,Hector Ren,Alexander Moreno,Daqian Zhang,Tianjun Zhong,Yuxin Xiong,Yuanzhe Hu,Yutao Xie,Xudong Han,Yuqi Wang,Varad Pimpalkhute,Yonghao Zhuang,Aaryamonvikram Singh,Xuezhi Liang,Anze Xie,Jianshu She,Desai Fan,Chengqian Gao,Liqun Ma,Mikhail Yurochkin,John Maggs,Xuezhe Ma,Guowei He,Zhiting Hu,Zhengzhong Liu,Eric P. Xing*

Main category: cs.LG

TL;DR: K2-Think推理系统用32B参数模型达SOTA，在数学推理等方面表现出色，证明小模型可与大模型竞争，且免费可用。


<details>
  <summary>Details</summary>
Motivation: 探索小模型在推理任务中达到或超越大模型的能力，使开源推理系统更易获取和负担。

Method: 基于Qwen2.5基础模型，采用六项关键技术（长思维链监督微调、可验证奖励强化学习等），使用公开开源数据集。

Result: K2-Think在开源模型公共基准测试中取得SOTA分数，在数学推理、代码和科学等领域表现出色，推理速度超2000 tokens/秒/请求。

Conclusion: 参数效率更高的K2-Think 32B通过集成后训练方法可与SOTA系统竞争，使开源推理系统更可行。

Abstract: K2-Think is a reasoning system that achieves state-of-the-art performance
with a 32B parameter model, matching or surpassing much larger models like
GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system
shows that smaller models can compete at the highest levels by combining
advanced post-training and test-time computation techniques. The approach is
based on six key technical pillars: Long Chain-of-thought Supervised
Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic
planning prior to reasoning, Test-time Scaling, Speculative Decoding, and
Inference-optimized Hardware, all using publicly available open-source
datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art
scores on public benchmarks for open-source models, while also performing
strongly in other areas such as Code and Science. Our results confirm that a
more parameter-efficient model like K2-Think 32B can compete with
state-of-the-art systems through an integrated post-training recipe that
includes long chain-of-thought training and strategic inference-time
enhancements, making open-source reasoning systems more accessible and
affordable. K2-Think is freely available at k2think.ai, offering best-in-class
inference speeds of over 2,000 tokens per second per request via the Cerebras
Wafer-Scale Engine.

</details>


### [122] [Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques](https://arxiv.org/abs/2509.07605)
*Ali Nawaz,Amir Ahmad,Shehroz S. Khan*

Main category: cs.LG

TL;DR: 研究在不进行显式重平衡的情况下评估二元分类器性能，系统评估不同数据集和场景下分类器表现，结果显示数据复杂度增加和少数类规模减小会使分类更难，部分先进模型表现较好，为不平衡学习的模型选择提供指导。


<details>
  <summary>Details</summary>
Motivation: 解决监督分类中类别不平衡问题，且以往对不使用重平衡技术时二元分类器性能评估关注较少。

Method: 系统评估不同数据集上多种二元分类器在少数类规模逐渐减小的情况下的鲁棒性，通过合成决策边界模拟不同数据复杂度，还进行了欠采样、过采样和单类分类方法的实验。

Result: 数据复杂度增加和少数类规模减小使分类更困难，传统分类器在极端不平衡下性能下降，TabPFN和基于提升的集成等先进模型表现较好，可视化和评估指标验证了结果。

Conclusion: 为不平衡学习的模型选择提供了有价值的指导，展示了不依赖显式重平衡技术时分类器的鲁棒性。

Abstract: Class imbalance poses a significant challenge to supervised classification,
particularly in critical domains like medical diagnostics and anomaly detection
where minority class instances are rare. While numerous studies have explored
rebalancing techniques to address this issue, less attention has been given to
evaluating the performance of binary classifiers under imbalance when no such
techniques are applied. Therefore, the goal of this study is to assess the
performance of binary classifiers "as-is", without performing any explicit
rebalancing. Specifically, we systematically evaluate the robustness of a
diverse set of binary classifiers across both real-world and synthetic
datasets, under progressively reduced minority class sizes, using one-shot and
few-shot scenarios as baselines. Our approach also explores varying data
complexities through synthetic decision boundary generation to simulate
real-world conditions. In addition to standard classifiers, we include
experiments using undersampling, oversampling strategies, and one-class
classification (OCC) methods to examine their behavior under severe imbalance.
The results confirm that classification becomes more difficult as data
complexity increases and the minority class size decreases. While traditional
classifiers deteriorate under extreme imbalance, advanced models like TabPFN
and boosting-based ensembles retain relatively higher performance and better
generalization compared to traditional classifiers. Visual interpretability and
evaluation metrics further validate these findings. Our work offers valuable
guidance on model selection for imbalanced learning, providing insights into
classifier robustness without dependence on explicit rebalancing techniques.

</details>


### [123] [Graph-based Integrated Gradients for Explaining Graph Neural Networks](https://arxiv.org/abs/2509.07648)
*Lachlan Simpson,Kyle Millar,Adriel Cheng,Cheng-Chew Lim,Hong Gunn Chew*

Main category: cs.LG

TL;DR: 提出图基集成梯度（GB - IG）扩展集成梯度（IG）到图数据，在合成和真实数据集验证其性能。


<details>
  <summary>Details</summary>
Motivation: IG假设数据连续，不适用于图这种离散结构，需引入新方法解决图数据可解释性问题。

Method: 引入GB - IG，将IG扩展到图数据。

Result: 在四个合成数据集上，GB - IG能准确识别分类任务中图的关键结构组件；在三个真实图数据集上，GB - IG在节点分类任务中突出重要特征方面优于IG。

Conclusion: GB - IG是一种适用于图数据的有效可解释性技术。

Abstract: Integrated Gradients (IG) is a common explainability technique to address the
black-box problem of neural networks. Integrated gradients assumes continuous
data. Graphs are discrete structures making IG ill-suited to graphs. In this
work, we introduce graph-based integrated gradients (GB-IG); an extension of IG
to graphs. We demonstrate on four synthetic datasets that GB-IG accurately
identifies crucial structural components of the graph used in classification
tasks. We further demonstrate on three prevalent real-world graph datasets that
GB-IG outperforms IG in highlighting important features for node classification
tasks.

</details>


### [124] [FUnc-SNE: A flexible, Fast, and Unconstrained algorithm for neighbour embeddings](https://arxiv.org/abs/2509.07681)
*Pierre Lambert,Edouard Couplet,Michel Verleysen,John Aldo Lee*

Main category: cs.LG

TL;DR: 本文提出加速邻域嵌入（NE）新方法，兼顾速度与结构保留，无维度限制，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有加速NE方法要么结构提取质量差，要么速度慢且限制维度，需新方法平衡两者。

Method: 提出新的加速NE方法，放弃传统两阶段方法，采用迭代近似最近邻搜索。

Result: 使用公开可用的GPU加速GUI集成方法进行实验，在速度、结构提取灵活性上有良好表现。

Conclusion: 该方法能在较少计算量下保持细粒度结构保留和灵活性，无维度限制，在机器学习领域有应用潜力。

Abstract: Neighbour embeddings (NE) allow the representation of high dimensional
datasets into lower dimensional spaces and are often used in data
visualisation. In practice, accelerated approximations are employed to handle
very large datasets. Accelerating NE is challenging, and two main directions
have been explored: very coarse approximations based on negative sampling (as
in UMAP) achieve high effective speed but may lack quality in the extracted
structures; less coarse approximations, as used in FIt-SNE or BH-t-SNE, offer
better structure preservation at the cost of speed, while also restricting the
target dimensionality to 2 or 3, limiting NE to visualisation. In some
variants, the precision of these costlier accelerations also enables
finer-grained control on the extracted structures through dedicated
hyperparameters.
  This paper proposes to bridge the gab between both approaches by introducing
a novel way to accelerate NE, requiring a small number of computations per
iteration while maintaining good fine-grained structure preservation and
flexibility through hyperparameter tuning, without limiting the dimensionality
of the embedding space. The method was designed for interactive exploration of
data; as such, it abandons the traditional two-phased approach of other NE
methods, allowing instantaneous visual feedback when changing hyperparameters,
even when these control processes happening on the high-dimensional side of the
computations. Experiments using a publicly available, GPU accelerated GUI
integration of the method show promising results in terms of speed, flexibility
in the structures getting extracted, and show potential uses in broader machine
learning contexts with minimal algorithmic modifications. Central to this
algorithm is a novel approach to iterative approximate nearest neighbour
search, which shows promising results compared to nearest neighbour descent.

</details>


### [125] [IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing](https://arxiv.org/abs/2509.07725)
*Shusen Ma,Tianhao Zhang,Qijiu Xia,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出IBN解决MTSF中变量缺失问题，实验显示其有SOTA表现，代码开源。


<details>
  <summary>Details</summary>
Motivation: GinAR解决变量缺失问题时缺乏可解释性，且简单递归单元无法捕捉更多潜在时间模式。

Method: 提出IBN，集成UAI和GGCN，用MC Dropout估计重构值不确定性，采用加权策略，GGCN建模空间相关性，双向RU增强时间依赖建模。

Result: IBN在不同缺失率场景下实现了SOTA预测性能。

Conclusion: IBN为有缺失变量的MTSF提供了更可靠和可解释的框架。

Abstract: Multivariate time series forecasting (MTSF) often faces challenges from
missing variables, which hinder conventional spatial-temporal graph neural
networks in modeling inter-variable correlations. While GinAR addresses
variable missing using attention-based imputation and adaptive graph learning
for the first time, it lacks interpretability and fails to capture more latent
temporal patterns due to its simple recursive units (RUs). To overcome these
limitations, we propose the Interpretable Bidirectional-modeling Network (IBN),
integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-based
Graph Convolution (GGCN). IBN estimates the uncertainty of reconstructed values
using MC Dropout and applies an uncertainty-weighted strategy to mitigate
high-risk reconstructions. GGCN explicitly models spatial correlations among
variables, while a bidirectional RU enhances temporal dependency modeling.
Extensive experiments show that IBN achieves state-of-the-art forecasting
performance under various missing-rate scenarios, providing a more reliable and
interpretable framework for MTSF with missing variables. Code is available at:
https://github.com/zhangth1211/NICLab-IBN.

</details>


### [126] [Forecasting Russian Equipment Losses Using Time Series and Deep Learning Models](https://arxiv.org/abs/2509.07813)
*Jonathan Teagan*

Main category: cs.LG

TL;DR: 运用多种预测技术对俄乌冲突中俄罗斯装备损失建模预测，发现深度学习模型表现好，强调集成预测和开源情报数据价值。


<details>
  <summary>Details</summary>
Motivation: 评估俄罗斯装备损耗趋势、评估模型性能并预测至2025年底的损失模式。

Method: 运用ARIMA、Prophet、LSTM、TCN和XGBoost等预测技术，基于WarSpotting的每日和每月开源情报数据进行建模预测。

Result: 深度学习模型（特别是TCN和LSTM）在高时间粒度条件下能产生稳定一致的预测。

Conclusion: 强调了集成预测在冲突建模中的重要性以及公开可用的开源情报数据在量化物资损耗方面的价值。

Abstract: This study applies a range of forecasting techniques,including ARIMA,
Prophet, Long Short Term Memory networks (LSTM), Temporal Convolutional
Networks (TCN), and XGBoost, to model and predict Russian equipment losses
during the ongoing war in Ukraine. Drawing on daily and monthly open-source
intelligence (OSINT) data from WarSpotting, we aim to assess trends in
attrition, evaluate model performance, and estimate future loss patterns
through the end of 2025. Our findings show that deep learning models,
particularly TCN and LSTM, produce stable and consistent forecasts, especially
under conditions of high temporal granularity. By comparing different model
architectures and input structures, this study highlights the importance of
ensemble forecasting in conflict modeling, and the value of publicly available
OSINT data in quantifying material degradation over time.

</details>


### [127] [Predicting person-level injury severity using crash narratives: A balanced approach with roadway classification and natural language process techniques](https://arxiv.org/abs/2509.07845)
*Mohammad Zana Majidi,Sajjad Karimi,Teng Wang,Robert Kluger,Reginald Souleyrette*

Main category: cs.LG

TL;DR: 研究结合无结构事故描述与结构化事故数据预测伤害严重程度，用NLP技术提取语义，多种分类方案建模。结果显示含描述数据模型更优，TF - IDF与XGBoost结合表现最佳，为交通安全专业人员提供框架。


<details>
  <summary>Details</summary>
Motivation: 探究无结构事故描述与结构化数据结合在预测伤害严重程度上的附加价值，以提升道路安全等。

Method: 使用TF - IDF和Word2Vec提取描述语义并比较；应用基于K近邻的过采样处理训练数据；采用三种道路分类方案；结合结构化和描述性特征，用三种集成算法开发102个机器学习模型。

Result: 含描述数据的模型始终优于仅依赖结构化数据的模型；TF - IDF与XGBoost结合在多数子组中预测最准确。

Conclusion: 整合文本和结构化事故信息可提升个人伤害预测能力，为交通安全专业人员提供实用且可适应的框架。

Abstract: Predicting injuries and fatalities in traffic crashes plays a critical role
in enhancing road safety, improving emergency response, and guiding public
health interventions. This study investigates the added value of unstructured
crash narratives (written by police officers at the scene) when combined with
structured crash data to predict injury severity. Two widely used Natural
Language Processing (NLP) techniques, Term Frequency-Inverse Document Frequency
(TF-IDF) and Word2Vec, were employed to extract semantic meaning from the
narratives, and their effectiveness was compared. To address the challenge of
class imbalance, a K-Nearest Neighbors-based oversampling method was applied to
the training data prior to modeling. The dataset consists of crash records from
Kentucky spanning 2019 to 2023. To account for roadway heterogeneity, three
road classification schemes were used: (1) eight detailed functional classes
(e.g., Urban Two-Lane, Rural Interstate, Urban Multilane Divided), (2) four
broader paired categories (e.g., Urban vs. Rural, Freeway vs. Non-Freeway), and
(3) a unified dataset without classification. A total of 102 machine learning
models were developed by combining structured features and narrative-based
features using the two NLP techniques alongside three ensemble algorithms:
XGBoost, Random Forest, and AdaBoost. Results demonstrate that models
incorporating narrative data consistently outperform those relying solely on
structured data. Among all combinations, TF-IDF coupled with XGBoost yielded
the most accurate predictions in most subgroups. The findings highlight the
power of integrating textual and structured crash information to enhance
person-level injury prediction. This work offers a practical and adaptable
framework for transportation safety professionals to improve crash severity
modeling, guide policy decisions, and design more effective countermeasures.

</details>


### [128] [Addressing the Cold-Start Problem for Personalized Combination Drug Screening](https://arxiv.org/abs/2509.07850)
*Antoine de Mathelin,Christopher Tosh,Wesley Tansey*

Main category: cs.LG

TL;DR: 提出利用预训练深度学习模型解决肿瘤个性化联合疗法早期实验选择的冷启动问题，提高了初始筛选效率。


<details>
  <summary>Details</summary>
Motivation: 肿瘤个性化联合疗法需探索大量药物和剂量组合，现有实验手段受限，存在冷启动问题，需选择最具信息性的组合进行早期测试。

Method: 利用基于历史药物反应数据的预训练深度学习模型，提供药物组合嵌入和剂量重要性得分，结合药物嵌入聚类和剂量加权机制。

Result: 在大规模药物组合数据集的回顾性模拟中，该方法相比基线显著提高了初始筛选效率。

Conclusion: 该方法为个性化联合药物筛选的早期决策提供了可行途径。

Abstract: Personalizing combination therapies in oncology requires navigating an
immense space of possible drug and dose combinations, a task that remains
largely infeasible through exhaustive experimentation. Recent developments in
patient-derived models have enabled high-throughput ex vivo screening, but the
number of feasible experiments is limited. Further, a tight therapeutic window
makes gathering molecular profiling information (e.g. RNA-seq) impractical as a
means of guiding drug response prediction. This leads to a challenging
cold-start problem: how do we select the most informative combinations to test
early, when no prior information about the patient is available? We propose a
strategy that leverages a pretrained deep learning model built on historical
drug response data. The model provides both embeddings for drug combinations
and dose-level importance scores, enabling a principled selection of initial
experiments. We combine clustering of drug embeddings to ensure functional
diversity with a dose-weighting mechanism that prioritizes doses based on their
historical informativeness. Retrospective simulations on large-scale drug
combination datasets show that our method substantially improves initial
screening efficiency compared to baselines, offering a viable path for more
effective early-phase decision-making in personalized combination drug screens.

</details>


### [129] [Leveraging Support Vector Regression for Outcome Prediction in Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy](https://arxiv.org/abs/2509.07872)
*Yajun Yu,Steve Jiang,Robert Timmerman,Hao Peng*

Main category: cs.LG

TL;DR: 研究开发多组学支持向量回归（SVR）模型预测脑转移瘤大体肿瘤体积（GTV）变化，多组学模型表现佳，为PULSAR治疗提供定量个性化方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测GTV变化有重要预后价值，需开发预测模型助力PULSAR治疗。

Method: 分析39例69个脑转移瘤患者，基于影像组学和剂量组学特征，计算差异特征，用Lasso算法筛选特征，评估不同核的SVR模型，采用五折交叉验证。

Result: 多组学模型优于单组学模型，差异影像组学特征提升预测准确性，最佳模型R2为0.743，RRMSE为0.022。

Conclusion: 提出的多组学SVR模型预测GTV连续变化表现良好，为PULSAR治疗患者选择和治疗调整提供定量个性化方法。

Abstract: Personalized ultra-fractionated stereotactic adaptive radiotherapy (PULSAR)
is a novel treatment that delivers radiation in pulses of protracted intervals.
Accurate prediction of gross tumor volume (GTV) changes through regression
models has substantial prognostic value. This study aims to develop a
multi-omics based support vector regression (SVR) model for predicting GTV
change. A retrospective cohort of 39 patients with 69 brain metastases was
analyzed, based on radiomics (MRI images) and dosiomics (dose maps) features.
Delta features were computed to capture relative changes between two time
points. A feature selection pipeline using least absolute shrinkage and
selection operator (Lasso) algorithm with weight- or frequency-based ranking
criterion was implemented. SVR models with various kernels were evaluated using
the coefficient of determination (R2) and relative root mean square error
(RRMSE). Five-fold cross-validation with 10 repeats was employed to mitigate
the limitation of small data size. Multi-omics models that integrate radiomics,
dosiomics, and their delta counterparts outperform individual-omics models.
Delta-radiomic features play a critical role in enhancing prediction accuracy
relative to features at single time points. The top-performing model achieves
an R2 of 0.743 and an RRMSE of 0.022. The proposed multi-omics SVR model shows
promising performance in predicting continuous change of GTV. It provides a
more quantitative and personalized approach to assist patient selection and
treatment adjustment in PULSAR.

</details>


### [130] [A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges](https://arxiv.org/abs/2509.07887)
*Katherine Berry,Liang Cheng*

Main category: cs.LG

TL;DR: 本文综述了图神经网络在药物发现各研究类别（如分子属性预测等）的应用，并为未来工作提供指导。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在药物发现领域因能处理图结构数据而受关注，且已产生众多方法和模型，需全面梳理。

Method: 全面涵盖分子属性预测、药物 - 靶点结合亲和力预测等多个药物发现研究类别，结合近期论文进行综述。

Result: 梳理了图神经网络在药物发现各研究类别的应用情况。

Conclusion: 为图神经网络在药物发现领域的未来工作提供了指导。

Abstract: Graph Neural Networks (GNNs) have gained traction in the complex domain of
drug discovery because of their ability to process graph-structured data such
as drug molecule models. This approach has resulted in a myriad of methods and
models in published literature across several categories of drug discovery
research. This paper covers the research categories comprehensively with recent
papers, namely molecular property prediction, including drug-target binding
affinity prediction, drug-drug interaction study, microbiome interaction
prediction, drug repositioning, retrosynthesis, and new drug design, and
provides guidance for future work on GNNs for drug discovery.

</details>


### [131] [Feasibility of In-Ear Single-Channel ExG for Wearable Sleep~Monitoring in Real-World Settings](https://arxiv.org/abs/2509.07896)
*Philipp Lepold,Jonas Leichtle,Tobias Röddiger,Michael Beigl*

Main category: cs.LG

TL;DR: 研究利用单通道入耳式电生理信号进行自动睡眠分期，系统在二元睡眠检测和四分类分期有一定准确率，显示入耳电极用于睡眠监测潜力。


<details>
  <summary>Details</summary>
Motivation: 传统基于脑电图的睡眠分期方法有侵入性，不适合日常使用，需更便捷的方法用于长期监测和消费应用。

Method: 对11名平均年龄24岁参与者进行睡眠研究，用定制耳麦和苹果手表Ultra获取数据。

Result: 二元睡眠检测准确率90.5%，四分类分期准确率65.1%。

Conclusion: 入耳电极是低负担、舒适的睡眠监测方法，有消费应用潜力。

Abstract: Automatic sleep staging typically relies on gold-standard EEG setups, which
are accurate but obtrusive and impractical for everyday use outside sleep
laboratories. This limits applicability in real-world settings, such as home
environments, where continuous, long-term monitoring is needed. Detecting sleep
onset is particularly relevant, enabling consumer applications (e.g.
automatically pausing media playback when the user falls asleep). Recent
research has shown correlations between in-ear EEG and full-scalp EEG for
various phenomena, suggesting wearable, in-ear devices could allow unobtrusive
sleep monitoring. We investigated the feasibility of using single-channel
in-ear electrophysiological (ExG) signals for automatic sleep staging in a
wearable device by conducting a sleep study with 11~participants (mean age:
24), using a custom earpiece with a dry eartip electrode (D\"atwyler SoftPulse)
as a measurement electrode in one ear and a reference in the other. Ground
truth sleep stages were obtained from an Apple Watch Ultra, validated for sleep
staging. Our system achieved 90.5% accuracy for binary sleep detection (Awake
vs. Asleep) and 65.1% accuracy for four-class staging (Awake, REM, Core, Deep)
using leave-one-subject-out validation. These findings demonstrate the
potential of in-ear electrodes as a low-effort, comfortable approach to sleep
monitoring, with applications such as stopping podcasts when users fall asleep.

</details>


### [132] [A Modular Algorithm for Non-Stationary Online Convex-Concave Optimization](https://arxiv.org/abs/2509.07901)
*Qing-xin Meng,Xia Lei,Jian-wei Liu*

Main category: cs.LG

TL;DR: 本文研究在线凸 - 凹优化问题，提出新模块化算法，达最优动态对偶间隙上界，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有算法在在线凸 - 凹优化问题中，尤其在平稳或可预测环境下无法达到最优性能。

Method: 提出含自适应模块、多预测器聚合器和集成模块的模块化算法。

Result: 算法达到极小极大最优动态对偶间隙上界，有预测误差驱动的动态对偶间隙界，模块化设计便于替换和整合组件。

Conclusion: 实证结果表明该方法有效且具有适应性。

Abstract: This paper investigates the problem of Online Convex-Concave Optimization,
which extends Online Convex Optimization to two-player time-varying
convex-concave games. The goal is to minimize the dynamic duality gap (D-DGap),
a critical performance measure that evaluates players' strategies against
arbitrary comparator sequences. Existing algorithms fail to deliver optimal
performance, particularly in stationary or predictable environments. To address
this, we propose a novel modular algorithm with three core components: an
Adaptive Module that dynamically adjusts to varying levels of non-stationarity,
a Multi-Predictor Aggregator that identifies the best predictor among multiple
candidates, and an Integration Module that effectively combines their
strengths. Our algorithm achieves a minimax optimal D-DGap upper bound, up to a
logarithmic factor, while also ensuring prediction error-driven D-DGap bounds.
The modular design allows for the seamless replacement of components that
regulate adaptability to dynamic environments, as well as the incorporation of
components that integrate ``side knowledge'' from multiple predictors.
Empirical results further demonstrate the effectiveness and adaptability of the
proposed method.

</details>


### [133] [Bio-KGvec2go: Serving up-to-date Dynamic Biomedical Knowledge Graph Embeddings](https://arxiv.org/abs/2509.07905)
*Hamid Ahmad,Heiko Paulheim,Rita T. Sousa*

Main category: cs.LG

TL;DR: 本文介绍了Bio - KGvec2go，它是KGvec2go Web API的扩展，用于生成和提供生物医学本体的知识图谱嵌入，支持定期更新，方便生物医学研究。


<details>
  <summary>Details</summary>
Motivation: 知识图谱和本体在现代AI应用中重要，预训练模型有价值，为便于生物医学研究，需开发能生成和提供生物医学本体知识图谱嵌入的工具。

Method: 开发Bio - KGvec2go，它是KGvec2go Web API的扩展，支持随本体版本发布进行定期更新。

Result: Bio - KGvec2go能以最小的用户计算量提供最新的嵌入。

Conclusion: Bio - KGvec2go有助于高效及时地开展生物医学研究。

Abstract: Knowledge graphs and ontologies represent entities and their relationships in
a structured way, having gained significance in the development of modern AI
applications. Integrating these semantic resources with machine learning models
often relies on knowledge graph embedding models to transform graph data into
numerical representations. Therefore, pre-trained models for popular knowledge
graphs and ontologies are increasingly valuable, as they spare the need to
retrain models for different tasks using the same data, thereby helping to
democratize AI development and enabling sustainable computing.
  In this paper, we present Bio-KGvec2go, an extension of the KGvec2go Web API,
designed to generate and serve knowledge graph embeddings for widely used
biomedical ontologies. Given the dynamic nature of these ontologies,
Bio-KGvec2go also supports regular updates aligned with ontology version
releases. By offering up-to-date embeddings with minimal computational effort
required from users, Bio-KGvec2go facilitates efficient and timely biomedical
research.

</details>


### [134] [Uncovering Scaling Laws for Large Language Models via Inverse Problems](https://arxiv.org/abs/2509.07909)
*Arun Verma,Zhaoxuan Wu,Zijian Zhou,Xiaoqiang Lin,Zhiliang Chen,Rachael Hwee Ling Sim,Rui Qiao,Jingtan Wang,Nhung Bui,Xinyuan Niu,Wenyang Hu,Gregory Kang Ruey Lau,Zi-Yu Khoo,Zitong Zhao,Xinyi Xu,Apivich Hemachandra,See-Kiong Ng,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 大语言模型因训练成本高，暴力试错不可行，本文主张用逆问题挖掘缩放定律以高效构建模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练成本高，暴力试错方法不可行，需要更高效的方法来构建模型。

Method: 受逆问题在揭示科学定律方面成功的启发，倡导使用逆问题来挖掘缩放定律。

Result: 未提及具体结果。

Conclusion: 逆问题可以有效挖掘缩放定律，以更优的成本效益指导大语言模型的构建。

Abstract: Large Language Models (LLMs) are large-scale pretrained models that have
achieved remarkable success across diverse domains. These successes have been
driven by unprecedented complexity and scale in both data and computations.
However, due to the high costs of training such models, brute-force
trial-and-error approaches to improve LLMs are not feasible. Inspired by the
success of inverse problems in uncovering fundamental scientific laws, this
position paper advocates that inverse problems can also efficiently uncover
scaling laws that guide the building of LLMs to achieve the desirable
performance with significantly better cost-effectiveness.

</details>


### [135] [One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning](https://arxiv.org/abs/2509.07945)
*Yuan Pu,Yazhe Niu,Jia Tang,Junyu Xiong,Shuai Hu,Hongsheng Li*

Main category: cs.LG

TL;DR: 文章提出ScaleZero模型及动态参数缩放策略，解决多任务学习中梯度冲突和模型可塑性问题，在多基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统多任务世界模型在处理大规模异构环境时，存在梯度冲突和模型可塑性损失问题，影响样本和计算效率。

Method: 从单次学习迭代和整体学习过程两方面解决问题，提出Mixture-of-Experts (MoE)架构的ScaleZero模型，引入基于LoRA的动态参数缩放（DPS）策略。

Result: ScaleZero仅用一个模型进行在线强化学习，性能与单任务基线相当；采用动态参数缩放策略后，只需单任务环境交互步骤的80%就能达到有竞争力的性能。

Conclusion: ScaleZero在大规模多任务学习中有很大潜力。

Abstract: In heterogeneous multi-task learning, tasks not only exhibit diverse
observation and action spaces but also vary substantially in intrinsic
difficulty. While conventional multi-task world models like UniZero excel in
single-task settings, we find that when handling large-scale heterogeneous
environments, gradient conflicts and the loss of model plasticity often
constrain their sample and computational efficiency. In this work, we address
these challenges from two perspectives: the single learning iteration and the
overall learning process. First, we investigate the impact of key design spaces
on extending UniZero to multi-task planning. We find that a Mixture-of-Experts
(MoE) architecture provides the most substantial performance gains by
mitigating gradient conflicts, leading to our proposed model,
\textit{ScaleZero}. Second, to dynamically balance the computational load
across the learning process, we introduce an online, LoRA-based \textit{dynamic
parameter scaling} (DPS) strategy. This strategy progressively integrates LoRA
adapters in response to task-specific progress, enabling adaptive knowledge
retention and parameter expansion. Empirical evaluations on standard benchmarks
such as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relying
exclusively on online reinforcement learning with one model, attains
performance on par with specialized single-task baselines. Furthermore, when
augmented with our dynamic parameter scaling strategy, our method achieves
competitive performance while requiring only 80\% of the single-task
environment interaction steps. These findings underscore the potential of
ScaleZero for effective large-scale multi-task learning. Our code is available
at \textcolor{magenta}{https://github.com/opendilab/LightZero}.

</details>


### [136] [Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges](https://arxiv.org/abs/2509.07946)
*Kasra Borazjani,Naji Khosravan,Rajeev Sahay,Bita Akram,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 提出教育领域的M3T联邦基础模型（FedFMs），介绍其潜力、对智能教育系统的作用及待解决的研究挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态多任务基础模型在教育领域的实际部署受隐私法规、数据孤岛和特定领域数据可用性限制，需新方法解决。

Method: 将联邦学习与M3T基础模型集成，形成M3T FedFMs范式。

Result: 指出M3T FedFMs能推进下一代智能教育系统的隐私保护、个性化、公平与包容三大支柱。

Conclusion: 确定了包括机构间隐私法规、数据模态特征、模型可解释性等多个待解决的研究挑战，需共同解决以实现实际部署。

Abstract: Multi-modal multi-task (M3T) foundation models (FMs) have recently shown
transformative potential in artificial intelligence, with emerging applications
in education. However, their deployment in real-world educational settings is
hindered by privacy regulations, data silos, and limited domain-specific data
availability. We introduce M3T Federated Foundation Models (FedFMs) for
education: a paradigm that integrates federated learning (FL) with M3T FMs to
enable collaborative, privacy-preserving training across decentralized
institutions while accommodating diverse modalities and tasks. Subsequently,
this position paper aims to unveil M3T FedFMs as a promising yet underexplored
approach to the education community, explore its potentials, and reveal its
related future research directions. We outline how M3T FedFMs can advance three
critical pillars of next-generation intelligent education systems: (i) privacy
preservation, by keeping sensitive multi-modal student and institutional data
local; (ii) personalization, through modular architectures enabling tailored
models for students, instructors, and institutions; and (iii) equity and
inclusivity, by facilitating participation from underrepresented and
resource-constrained entities. We finally identify various open research
challenges, including studying of (i) inter-institution heterogeneous privacy
regulations, (ii) the non-uniformity of data modalities' characteristics, (iii)
the unlearning approaches for M3T FedFMs, (iv) the continual learning
frameworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must
be collectively addressed for practical deployment.

</details>


### [137] [ACE and Diverse Generalization via Selective Disagreement](https://arxiv.org/abs/2509.07955)
*Oliver Daniels,Stuart Armstrong,Alexandre Maranhão,Mahirah Fairuz Rahman,Benjamin M. Marlin,Rebecca Gorman*

Main category: cs.LG

TL;DR: 本文提出方法ACE解决深度神经网络对虚假关联的敏感性问题，在多个基准测试中表现良好，在语言模型对齐应用中也有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有处理虚假关联的工作多关注不完整关联，在虚假关联完整时正确泛化存在欠指定问题，需要解决该问题。

Method: 学习一组与训练数据一致但对部分新的未标记输入有不同预测的概念，采用鼓励“自信”和“选择性”分歧的自训练方法。

Result: ACE在一系列完整虚假关联基准测试中达到或超过现有方法，对不完整虚假关联也有鲁棒性，且更具可配置性。在语言模型对齐早期应用中，无需不可信测量即可在测量篡改检测基准测试中取得有竞争力的表现。

Conclusion: 虽然仍有重要局限，但ACE在克服欠指定问题上取得了重大进展。

Abstract: Deep neural networks are notoriously sensitive to spurious correlations -
where a model learns a shortcut that fails out-of-distribution. Existing work
on spurious correlations has often focused on incomplete
correlations,leveraging access to labeled instances that break the correlation.
But in cases where the spurious correlations are complete, the correct
generalization is fundamentally \textit{underspecified}. To resolve this
underspecification, we propose learning a set of concepts that are consistent
with training data but make distinct predictions on a subset of novel unlabeled
inputs. Using a self-training approach that encourages \textit{confident} and
\textit{selective} disagreement, our method ACE matches or outperforms existing
methods on a suite of complete-spurious correlation benchmarks, while remaining
robust to incomplete spurious correlations. ACE is also more configurable than
prior approaches, allowing for straight-forward encoding of prior knowledge and
principled unsupervised model selection. In an early application to
language-model alignment, we find that ACE achieves competitive performance on
the measurement tampering detection benchmark \textit{without} access to
untrusted measurements. While still subject to important limitations, ACE
represents significant progress towards overcoming underspecification.

</details>


### [138] [Customizing the Inductive Biases of Softmax Attention using Structured Matrices](https://arxiv.org/abs/2509.07963)
*Yilun Kuang,Noah Amsel,Sanae Lotfi,Shikai Qiu,Andres Potapczynski,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本文提出基于高秩结构化矩阵的新注意力评分函数，在高维输入回归、语言建模和时间序列预测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决标准注意力评分函数低维投影导致信息损失，且对所有输入对使用相同评分函数缺乏距离依赖计算偏差的问题。

Method: 提出基于高秩结构化矩阵（BTT和MLR）的新评分函数。

Result: 在高维输入上下文回归任务中优于标准注意力；在语言建模中MLR注意力方法有更好的缩放定律；MLR注意力在长程时间序列预测中有良好结果。

Conclusion: BTT和MLR属于能编码全秩或距离依赖计算偏差的高效结构化矩阵家族，可解决标准注意力的重大缺陷。

Abstract: The core component of attention is the scoring function, which transforms the
inputs into low-dimensional queries and keys and takes the dot product of each
pair. While the low-dimensional projection improves efficiency, it causes
information loss for certain tasks that have intrinsically high-dimensional
inputs. Additionally, attention uses the same scoring function for all input
pairs, without imposing a distance-dependent compute bias for neighboring
tokens in the sequence. In this work, we address these shortcomings by
proposing new scoring functions based on computationally efficient structured
matrices with high ranks, including Block Tensor-Train (BTT) and Multi-Level
Low Rank (MLR) matrices. On in-context regression tasks with high-dimensional
inputs, our proposed scoring functions outperform standard attention for any
fixed compute budget. On language modeling, a task that exhibits locality
patterns, our MLR-based attention method achieves improved scaling laws
compared to both standard attention and variants of sliding window attention.
Additionally, we show that both BTT and MLR fall under a broader family of
efficient structured matrices capable of encoding either full-rank or
distance-dependent compute biases, thereby addressing significant shortcomings
of standard attention. Finally, we show that MLR attention has promising
results for long-range time-series forecasting.

</details>


### [139] [Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence](https://arxiv.org/abs/2509.07972)
*Yuxing Liu,Yuze Ge,Rui Pan,An Kang,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出广义平滑性假设，研究梯度下降收敛性质，表明学习率预热能加速梯度下降。


<details>
  <summary>Details</summary>
Motivation: 学习率预热在实践中成功，但理论优势未充分理解，需填补理论与实践差距。

Method: 提出广义平滑性假设，在确定性和随机设置下研究梯度下降收敛性质。

Result: 学习率预热能持续加速梯度下降，特定情况下比非递增学习率调度快至多 Θ(T) 倍。

Conclusion: 从优化理论角度为学习率预热策略的益处提供见解。

Abstract: Learning rate warmup is a popular and practical technique in training
large-scale deep neural networks. Despite the huge success in practice, the
theoretical advantages of this strategy of gradually increasing the learning
rate at the beginning of the training process have not been fully understood.
To resolve this gap between theory and practice, we first propose a novel
family of generalized smoothness assumptions, and validate its applicability
both theoretically and empirically. Under the novel smoothness assumption, we
study the convergence properties of gradient descent (GD) in both deterministic
and stochastic settings. It is shown that learning rate warmup consistently
accelerates GD, and GD with warmup can converge at most $\Theta(T)$ times
faster than with a non-increasing learning rate schedule in some specific
cases, providing insights into the benefits of this strategy from an
optimization theory perspective.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [140] [A multi-strategy improved gazelle optimization algorithm for solving numerical optimization and engineering applications](https://arxiv.org/abs/2509.07211)
*Qi Diao,Chengyue Xie,Yuchen Yin,Hoileong Lee,Haolong Yang*

Main category: cs.NE

TL;DR: 本文提出多策略改进瞪羚优化算法（MSIGOA），经测试在复杂问题处理上优于基础算法和其他先进算法，还通过工程设计问题验证其扩展性。


<details>
  <summary>Details</summary>
Motivation: 针对瞪羚优化算法探索与开发不平衡、种群内信息交换不足的缺点，提出改进算法。

Method: 提出基于迭代的更新框架，采用两种自适应参数调整策略和基于优势种群的重启策略。

Result: 在CEC2017和CEC2022测试集上，MSIGOA表现优于基础GOA和其他先进算法；解决工程设计优化问题验证了其扩展性。

Conclusion: MSIGOA显著提高了探索和开发能力，在处理复杂问题时具有优越的收敛性和效率。

Abstract: Aiming at the shortcomings of the gazelle optimization algorithm, such as the
imbalance between exploration and exploitation and the insufficient information
exchange within the population, this paper proposes a multi-strategy improved
gazelle optimization algorithm (MSIGOA). To address these issues, MSIGOA
proposes an iteration-based updating framework that switches between
exploitation and exploration according to the optimization process, which
effectively enhances the balance between local exploitation and global
exploration in the optimization process and improves the convergence speed. Two
adaptive parameter tuning strategies improve the applicability of the algorithm
and promote a smoother optimization process. The dominant population-based
restart strategy enhances the algorithms ability to escape from local optima
and avoid its premature convergence. These enhancements significantly improve
the exploration and exploitation capabilities of MSIGOA, bringing superior
convergence and efficiency in dealing with complex problems. In this paper, the
parameter sensitivity, strategy effectiveness, convergence and stability of the
proposed method are evaluated on two benchmark test sets including CEC2017 and
CEC2022. Test results and statistical tests show that MSIGOA outperforms basic
GOA and other advanced algorithms. On the CEC2017 and CEC2022 test sets, the
proportion of functions where MSIGOA is not worse than GOA is 92.2% and 83.3%,
respectively, and the proportion of functions where MSIGOA is not worse than
other algorithms is 88.57% and 87.5%, respectively. Finally, the extensibility
of MSIGAO is further verified by several engineering design optimization
problems.

</details>


### [141] [Breaking the Conventional Forward-Backward Tie in Neural Networks: Activation Functions](https://arxiv.org/abs/2509.07236)
*Luigi Troiano,Francesco Gissi,Vincenzo Benedetto,Genny Tortora*

Main category: cs.NE

TL;DR: 本文挑战梯度对称假设，通过数学分析和实验证明放宽前向 - 后向对称可有效训练神经网络，拓展设计灵活性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于梯度的神经网络训练要求激活函数可微且单调，限制了激活函数选择，作者欲打破此限制。

Method: 进行数学分析，在多层感知机、卷积神经网络和二值神经网络等基础架构上进行实证实验。

Result: 放宽前向 - 后向对称，用更简单或随机的梯度替代传统梯度不影响学习，甚至能增强训练稳定性和效率，可有效训练含不可微激活函数的网络。

Conclusion: 打破传统假设可拓展设计灵活性和计算效率，对更复杂架构的实证验证是未来研究方向。

Abstract: Gradient-based neural network training traditionally enforces symmetry
between forward and backward propagation, requiring activation functions to be
differentiable (or sub-differentiable) and strictly monotonic in certain
regions to prevent flat gradient areas. This symmetry, linking forward
activations closely to backward gradients, significantly restricts the
selection of activation functions, particularly excluding those with
substantial flat or non-differentiable regions. In this paper, we challenge
this assumption through mathematical analysis, demonstrating that precise
gradient magnitudes derived from activation functions are largely redundant,
provided the gradient direction is preserved. Empirical experiments conducted
on foundational architectures - such as Multi-Layer Perceptrons (MLPs),
Convolutional Neural Networks (CNNs), and Binary Neural Networks (BNNs) -
confirm that relaxing forward-backward symmetry and substituting traditional
gradients with simpler or stochastic alternatives does not impair learning and
may even enhance training stability and efficiency. We explicitly demonstrate
that neural networks with flat or non-differentiable activation functions, such
as the Heaviside step function, can be effectively trained, thereby expanding
design flexibility and computational efficiency. Further empirical validation
with more complex architectures remains a valuable direction for future
research.

</details>


### [142] [Word2Spike: Poisson Rate Coding for Associative Memories and Neuromorphic Algorithms](https://arxiv.org/abs/2509.07361)
*Archit Kalra,Midhun Sadanand*

Main category: cs.NE

TL;DR: 本文提出Word2Spike编码机制，结合词嵌入与神经形态架构，在多指标表现良好，后续将与其他模型集成评估。


<details>
  <summary>Details</summary>
Motivation: 探索脉冲神经网络实现节能、类脑联想记忆的途径。

Method: 引入Word2Spike机制，用泊松过程将多维词向量转换为基于脉冲的吸引子状态，采用BitNet b1.58量化。

Result: 在SimLex - 999上保持97%语义相似度，对10000个词实现100%重建准确率，在引入噪声时保留100%类比性能。

Conclusion: Word2Spike是神经形态系统中语义编码的弹性机制，后续将与其他模型集成评估。

Abstract: Spiking neural networks offer a promising path toward energy-efficient,
brain-like associative memory. This paper introduces Word2Spike, a novel rate
coding mechanism that combines continuous word embeddings and neuromorphic
architectures. We develop a one-to-one mapping that converts multi-dimensional
word vectors into spike-based attractor states using Poisson processes. Using
BitNet b1.58 quantization, we maintain 97% semantic similarity of continuous
embeddings on SimLex-999 while achieving 100% reconstruction accuracy on 10,000
words from OpenAI's text-embedding-3-large. We preserve analogy performance
(100% of original embedding performance) even under intentionally introduced
noise, indicating a resilient mechanism for semantic encoding in neuromorphic
systems. Next steps include integrating the mapping with spiking transformers
and liquid state machines (resembling Hopfield Networks) for further
evaluation.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [143] [Unikernels vs. Containers: A Runtime-Level Performance Comparison for Resource-Constrained Edge Workloads](https://arxiv.org/abs/2509.07891)
*Hai Dinh-Tuan*

Main category: cs.PF

TL;DR: 本文对Go和Node.js应用在工业边缘环境内存约束下对比容器和unikernel性能，发现最优部署范式依赖运行时行为和系统资源。


<details>
  <summary>Details</summary>
Motivation: 此前工业边缘环境严重内存约束下容器和unikernel的权衡在不同执行模型下研究不足。

Method: 对代表AOT和JIT编译的Go和Node.js应用进行实证比较。

Result: unikernel在Go工作负载启动快且表现优；Node.js在特定内存阈值下，Docker容器性能稳定，Nanos unikernel性能骤降。

Conclusion: 最优部署范式取决于运行时行为和系统资源，边缘计算需工作负载感知的部署策略。

Abstract: The choice between containers and unikernels is a critical trade-off for edge
applications, balancing the container's ecosystem maturity against unikernel's
specialized efficiency. However, until now, how this trade-off behaves under
the severe memory constraints of industrial edge environments remains
insufficiently investigated, especially across different execution models. This
work presents an empirical comparison using Go and Node.js applications,
representing ahead-of-time (AOT) and just-in-time (JIT) compilation,
respectively. While unikernels consistently deliver faster startup times and
outperform containers for Go-based workloads in resource-constrained
environments, the evaluation results identify a critical performance crossover
for Node.js. Below a certain memory threshold, Docker containers maintain
stable performance for both I/O-bound and CPU-bound applications, while the
Nanos unikernel's performance degrades sharply. This reveals that Linux's
memory management capabilities can outweigh the minimalist efficiency of
unikernels under resource scarcity, a critical trade-off that, until now, has
not been adequately quantified for JIT runtimes in this context. These findings
demonstrate that the optimal deployment paradigm depends on both runtime
behavior and available system resources, underscoring the need for
workload-aware deployment strategies in edge computing.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [144] [Aspect-Oriented Programming in Secure Software Development: A Case Study of Security Aspects in Web Applications](https://arxiv.org/abs/2509.07449)
*Mterorga Ukor*

Main category: cs.SE

TL;DR: 研究探讨AOP在增强安全软件开发中的作用，通过案例研究对比AOP与传统方法，结果表明AOP提升了安全机制的模块化、可重用性和可维护性，且性能开销小。


<details>
  <summary>Details</summary>
Motivation: 现代Web应用安全是关键挑战，传统OOP将安全逻辑与业务功能交织，导致代码混乱和可维护性降低，因此研究AOP对安全软件开发的作用。

Method: 采用案例研究方法，对比AOP与传统OOP或基于中间件的方法实现安全功能，收集代码质量、性能和可维护性指标，结合开发者反馈，用统计方法评估差异。

Result: AOP增强了安全机制的模块化、可重用性和可维护性，仅引入最小的性能开销。

Conclusion: 研究为寻求在Web应用开发中平衡安全与软件质量的工程师和研究人员提供了实用见解。

Abstract: Security remains a critical challenge in modern web applications, where
threats such as unauthorized access, data breaches, and injection attacks
continue to undermine trust and reliability. Traditional Object-Oriented
Programming (OOP) often intertwines security logic with business functionality,
leading to code tangling, scattering, and reduced maintainability. This study
investigates the role of Aspect-Oriented Programming (AOP) in enhancing secure
software development by modularizing cross-cutting security concerns. Using a
case study approach, we compare AOP-based implementations of security features
including authentication, authorization, input validation, encryption, logging,
and session management with conventional OOP or middleware-based approaches.
Data collection involves analyzing code quality metrics (e.g., lines of code,
coupling, cohesion, modularity index, reusability), performance metrics
(response time, throughput, memory usage), and maintainability indicators.
Developer feedback is also incorporated to assess integration and debugging
experiences. Statistical methods, guided by the ISO/IEC 25010 software quality
model, are applied to evaluate differences across implementations. The findings
demonstrate that AOP enhances modularity, reusability, and maintainability of
security mechanisms, while introducing only minimal performance overhead. The
study contributes practical insights for software engineers and researchers
seeking to balance security with software quality in web application
development.

</details>


### [145] [CRACI: A Cloud-Native Reference Architecture for the Industrial Compute Continuum](https://arxiv.org/abs/2509.07498)
*Hai Dinh-Tuan*

Main category: cs.SE

TL;DR: 传统工业架构有局限性，本文提出云原生参考架构CRACI并验证其能克服传统架构局限，实现可扩展现代工业系统。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中IT与OT融合暴露传统分层架构如ISA - 95和RAMI 4.0的局限性，阻碍可扩展和互操作工业系统发展。

Method: 提出CRACI架构，采用比较理论分析和基于实际智能制造实施性能数据的定量评估两种方法验证。

Result: 结果表明CRACI是可行的、先进的架构，能利用计算连续体克服传统模型的结构限制。

Conclusion: CRACI可克服传统架构局限，实现可扩展的现代工业系统。

Abstract: The convergence of Information Technology (IT) and Operational Technology
(OT) in Industry 4.0 exposes the limitations of traditional, hierarchical
architectures like ISA-95 and RAMI 4.0. Their inherent rigidity, data silos,
and lack of support for cloud-native technologies impair the development of
scalable and interoperable industrial systems. This paper addresses this issue
by introducing CRACI, a Cloud-native Reference Architecture for the Industrial
Compute Continuum. Among other features, CRACI promotes a decoupled and
event-driven model to enable flexible, non-hierarchical data flows across the
continuum. It embeds cross-cutting concerns as foundational pillars: Trust,
Governance & Policy, Observability, and Lifecycle Management, ensuring quality
attributes are core to the design. The proposed architecture is validated
through a two-fold approach: (1) a comparative theoretical analysis against
established standards, operational models, and academic proposals; and (2) a
quantitative evaluation based on performance data from previously published
real-world smart manufacturing implementations. The results demonstrate that
CRACI provides a viable, state-of-the-art architecture that utilizes the
compute continuum to overcome the structural limitations of legacy models and
enable scalable, modern industrial systems.

</details>


### [146] [PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits with LLM Generated Commits and Embeddings](https://arxiv.org/abs/2509.07540)
*Huu Hung Nguyen,Anh Tuan Nguyen,Thanh Le-Cong,Yikun Li,Han Wei Ang,Yide Yin,Frank Liauw,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: 本文介绍了PatchSeeker方法，利用大语言模型建立漏洞描述与修复提交之间的语义链接，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 国家漏洞数据库（NVD）缺乏与漏洞修复提交（VFCs）的明确链接，现有方法存在局限，需自动映射NVD记录与VFCs。

Method: 引入PatchSeeker方法，利用大语言模型生成NVD描述的嵌入，为简短或无信息的提交消息合成详细摘要。

Result: PatchSeeker在基准数据集上比最佳基线Prospector的MRR高59.3%，Recall@10高27.9%，扩展评估也证实其有效性。

Conclusion: PatchSeeker有效，提交消息生成方法和骨干大语言模型的选择有积极贡献，同时讨论了局限性和开放挑战。

Abstract: Software vulnerabilities pose serious risks to modern software ecosystems.
While the National Vulnerability Database (NVD) is the authoritative source for
cataloging these vulnerabilities, it often lacks explicit links to the
corresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code
changes, enabling vulnerability localization, patch analysis, and dataset
construction. Automatically mapping NVD records to their true VFCs is therefore
critical. Existing approaches have limitations as they rely on sparse, often
noisy commit messages and fail to capture the deep semantics in the
vulnerability descriptions. To address this gap, we introduce PatchSeeker, a
novel method that leverages large language models to create rich semantic links
between vulnerability descriptions and their VFCs. PatchSeeker generates
embeddings from NVD descriptions and enhances commit messages by synthesizing
detailed summaries for those that are short or uninformative. These generated
messages act as a semantic bridge, effectively closing the information gap
between natural language reports and low-level code changes. Our approach
PatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the
best-performing baseline, Prospector, on the benchmark dataset. The extended
evaluation on recent CVEs further confirms PatchSeeker's effectiveness.
Ablation study shows that both the commit message generation method and the
selection of backbone LLMs make a positive contribution to PatchSeeker. We also
discuss limitations and open challenges to guide future work.

</details>


### [147] [Bridging the Gap Between Binary and Source Based Package Management in Spack](https://arxiv.org/abs/2509.07728)
*John Gouwar,Gregory Becker,Tamara Dahlgren,Nathan Hanford,Arjun Guha,Todd Gamblin*

Main category: cs.SE

TL;DR: 提出Splicing扩展，可让Spack无缝混合源和二进制发行版，实现快速安装。


<details>
  <summary>Details</summary>
Motivation: 二进制包管理器安装快但配置受限，源包管理器灵活但编译慢，Spack缺乏二进制兼容模型。

Method: 提出Splicing扩展，增强Spack的打包语言和依赖解析引擎。

Result: Splicing可复用兼容二进制文件，减少安装时间开销。

Conclusion: Splicing能让Spack在保持源构建灵活性的同时，实现ABI敏感依赖的快速二进制安装。

Abstract: Binary package managers install software quickly but they limit
configurability due to rigid ABI requirements that ensure compatibility between
binaries. Source package managers provide flexibility in building software, but
compilation can be slow. For example, installing an HPC code with a new MPI
implementation may result in a full rebuild. Spack, a widely deployed,
HPC-focused package manager, can use source and pre-compiled binaries, but
lacks a binary compatibility model, so it cannot mix binaries not built
together. We present splicing, an extension to Spack that models binary
compatibility between packages and allows seamless mixing of source and binary
distributions. Splicing augments Spack's packaging language and dependency
resolution engine to reuse compatible binaries but maintains the flexibility of
source builds. It incurs minimal installation-time overhead and allows rapid
installation from binaries, even for ABI-sensitive dependencies like MPI that
would otherwise require many rebuilds.

</details>


### [148] [What's Coming Next? Short-Term Simulation of Business Processes from Current State](https://arxiv.org/abs/2509.07747)
*Maksym Avramenko,David Chapela-Campa,Marlon Dumas,Fredrik Milani*

Main category: cs.SE

TL;DR: 本文研究从事件日志获取当前状态初始化模拟的方法，以解决业务流程模拟中运营决策问题，实验表明该方法短期预测更准确。


<details>
  <summary>Details</summary>
Motivation: 现有业务流程模拟方法主要支持战术决策，在运营决策场景中存在不足，未考虑当前进行中案例和资源状态。

Method: 提出从进行中案例的事件日志获取当前状态来初始化模拟的方法，解决两个关键挑战，并实现模拟引擎。

Result: 实验评估显示，该短期模拟方法比带预热期的长期模拟在短期性能预测上更准确，尤其在概念漂移或突发性能模式下。

Conclusion: 从事件日志表示的当前状态初始化模拟的方法能有效提高业务流程模拟在运营决策中的短期性能预测准确性。

Abstract: Business process simulation is an approach to evaluate business process
changes prior to implementation. Existing methods in this field primarily
support tactical decision-making, where simulations start from an empty state
and aim to estimate the long-term effects of process changes. A complementary
use-case is operational decision-making, where the goal is to forecast
short-term performance based on ongoing cases and to analyze the impact of
temporary disruptions, such as demand spikes and shortfalls in available
resources. An approach to tackle this use-case is to run a long-term simulation
up to a point where the workload is similar to the current one (warm-up), and
measure performance thereon. However, this approach does not consider the
current state of ongoing cases and resources in the process. This paper studies
an alternative approach that initializes the simulation from a representation
of the current state derived from an event log of ongoing cases. The paper
addresses two challenges in operationalizing this approach: (1) Given a
simulation model, what information is needed so that a simulation run can start
from the current state of cases and resources? (2) How can the current state of
a process be derived from an event log? The resulting short-term simulation
approach is embodied in a simulation engine that takes as input a simulation
model and a log of ongoing cases, and simulates cases for a given time horizon.
An experimental evaluation shows that this approach yields more accurate
short-term performance forecasts than long-term simulations with warm-up
period, particularly in the presence of concept drift or bursty performance
patterns.

</details>


### [149] [What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects](https://arxiv.org/abs/2509.07763)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 研究借助LLMs分析开发者重构动机，发现LLMs能有效捕捉表层动机，与软件指标结合可系统化重构。


<details>
  <summary>Details</summary>
Motivation: 理解开发者重构原因及衡量动机的指标，以支持重构在实践中更广泛有效的应用。

Method: 开展大规模实证研究，利用LLMs从版本控制数据中识别重构动机，并与文献中报告的动机对比。

Result: LLMs在80%的情况下与人类判断相符，与文献动机仅47%一致，丰富22%动机，多数动机务实，开发者经验和代码可读性指标排名高但与动机类别相关性弱。

Conclusion: LLMs能有效捕捉表层动机但难进行架构推理，与软件指标结合形成混合方法可系统化重构，平衡短期改进与长期架构目标。

Abstract: Context. Code refactoring improves software quality without changing external
behavior. Despite its advantages, its benefits are hindered by the considerable
cost of time, resources, and continuous effort it demands. Aim. Understanding
why developers refactor, and which metrics capture these motivations, may
support wider and more effective use of refactoring in practice. Method. We
performed a large-scale empirical study to analyze developers refactoring
activity, leveraging Large Language Models (LLMs) to identify underlying
motivations from version control data, comparing our findings with previous
motivations reported in the literature. Results. LLMs matched human judgment in
80% of cases, but aligned with literature-based motivations in only 47%. They
enriched 22% of motivations with more detailed rationale, often highlighting
readability, clarity, and structural improvements. Most motivations were
pragmatic, focused on simplification and maintainability. While metrics related
to developer experience and code readability ranked highest, their correlation
with motivation categories was weak. Conclusions. We conclude that LLMs
effectively capture surface-level motivations but struggle with architectural
reasoning. Their value lies in providing localized explanations, which, when
combined with software metrics, can form hybrid approaches. Such integration
offers a promising path toward prioritizing refactoring more systematically and
balancing short-term improvements with long-term architectural goals.

</details>


### [150] ["We provide our resources in a dedicated repository": Surveying the Transparency of HICSS publications](https://arxiv.org/abs/2509.07851)
*Irdin Pekaric,Giovanni Apruzzese*

Main category: cs.SE

TL;DR: 研究检查HICSS会议近2028篇论文利用外部仓库提供补充材料情况，仅3%有可用仓库，还公布工具。


<details>
  <summary>Details</summary>
Motivation: 研究HICSS会议论文利用外部仓库提供补充材料的程度，以提高研究透明度和促进可重复性。

Method: 收集2017 - 2024年HICSS会议5579篇论文，识别含人类受试者研究、技术实现或两者皆有的论文，审查文本查找外部仓库链接并检查内容。

Result: 2028篇论文中，仅3%有可供后续研究使用的功能性公开仓库。

Conclusion: HICSS会议论文利用外部仓库提供补充材料的比例较低。

Abstract: Every day, new discoveries are made by researchers from all across the globe
and fields. HICSS is a flagship venue to present and discuss such scientific
advances. Yet, the activities carried out for any given research can hardly be
fully contained in a single document of a few pages-the "paper." Indeed, any
given study entails data, artifacts, or other material that is crucial to truly
appreciate the contributions claimed in the corresponding paper. External
repositories (e.g., GitHub) are a convenient tool to store all such resources
so that future work can freely observe and build upon them -- thereby improving
transparency and promoting reproducibility of research as a whole. In this
work, we scrutinize the extent to which papers recently accepted to HICSS
leverage such repositories to provide supplementary material. To this end, we
collect all the 5579 papers included in HICSS proceedings from 2017-2024. Then,
we identify those entailing either human subject research (850) or technical
implementations (737), or both (147). Finally, we review their text, examining
how many include a link to an external repository-and, inspect its contents.
Overall, out of 2028 papers, only 3\% have a functional and publicly available
repository that is usable by downstream research. We release all our tools.

</details>


### [151] [Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation](https://arxiv.org/abs/2509.07933)
*Wanni Vidulige Ishan Perera,Xing Liu,Fan liang,Junyi Zhang*

Main category: cs.SE

TL;DR: 本文探讨使用基于大语言模型的工具（如PentestGPT）进行安卓渗透测试自动化，比较传统和AI方法，给出安全建议，发现LLM需人工控制。


<details>
  <summary>Details</summary>
Motivation: 人工智能和大语言模型发展为网络安全带来新机遇，探索安卓渗透测试自动化。

Method: 使用安卓模拟器Genymotion进行传统和基于漏洞利用的Rooting方法测试，集成OpenAI API创建Web应用生成脚本，比较自动和手动渗透测试协议。

Result: 大语言模型能显著简化漏洞利用工作流程。

Conclusion: 大语言模型需人工控制以确保准确性和道德应用，研究丰富了AI网络安全相关文献。

Abstract: The rapid evolution of Artificial Intelligence (AI) and Large Language Models
(LLMs) has opened up new opportunities in the area of cybersecurity, especially
in the exploitation automation landscape and penetration testing. This study
explores Android penetration testing automation using LLM-based tools,
especially PentestGPT, to identify and execute rooting techniques. Through a
comparison of the traditional manual rooting process and exploitation methods
produced using AI, this study evaluates the efficacy, reliability, and
scalability of automated penetration testing in achieving high-level privilege
access on Android devices. With the use of an Android emulator (Genymotion) as
the testbed, we fully execute both traditional and exploit-based rooting
methods, automating the process using AI-generated scripts. Secondly, we create
a web application by integrating OpenAI's API to facilitate automated script
generation from LLM-processed responses. The research focuses on the
effectiveness of AI-enabled exploitation by comparing automated and manual
penetration testing protocols, by determining LLM weaknesses and strengths
along the way. We also provide security suggestions of AI-enabled exploitation,
including ethical factors and potential misuse. The findings exhibit that while
LLMs can significantly streamline the workflow of exploitation, they need to be
controlled by humans to ensure accuracy and ethical application. This study
adds to the increasing body of literature on AI-powered cybersecurity and its
effect on ethical hacking, security research, and mobile device security.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [152] [ADHAM: Additive Deep Hazard Analysis Mixtures for Interpretable Survival Regression](https://arxiv.org/abs/2509.07108)
*Mert Ketenci,Vincent Jeanselme,Harry Reyes Nieva,Shalmali Joshi,Noémie Elhadad*

Main category: stat.ML

TL;DR: 提出可解释的加性生存模型ADHAM，在医疗事件时间预测中兼具可解释性与预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络生存分析模型大多无法提供暴露因素与建模结果关联的可解释见解，无法满足临床决策需求。

Method: 提出ADHAM模型，假设条件潜结构定义子组，引入训练后细化方法减少等效潜子组数量。

Result: 在真实数据集上的实验表明，ADHAM能提供暴露因素与结果关联的新见解，预测性能与现有最优生存基线相当。

Conclusion: ADHAM为医疗领域的事件时间预测提供了可扩展且可解释的方法。

Abstract: Survival analysis is a fundamental tool for modeling time-to-event outcomes
in healthcare. Recent advances have introduced flexible neural network
approaches for improved predictive performance. However, most of these models
do not provide interpretable insights into the association between exposures
and the modeled outcomes, a critical requirement for decision-making in
clinical practice. To address this limitation, we propose Additive Deep Hazard
Analysis Mixtures (ADHAM), an interpretable additive survival model. ADHAM
assumes a conditional latent structure that defines subgroups, each
characterized by a combination of covariate-specific hazard functions. To
select the number of subgroups, we introduce a post-training refinement that
reduces the number of equivalent latent subgroups by merging similar groups. We
perform comprehensive studies to demonstrate ADHAM's interpretability at the
population, subgroup, and individual levels. Extensive experiments on
real-world datasets show that ADHAM provides novel insights into the
association between exposures and outcomes. Further, ADHAM remains on par with
existing state-of-the-art survival baselines in terms of predictive
performance, offering a scalable and interpretable approach to time-to-event
prediction in healthcare.

</details>


### [153] [NestGNN: A Graph Neural Network Framework Generalizing the Nested Logit Model for Travel Mode Choice](https://arxiv.org/abs/2509.07123)
*Yuqi Zhou,Zhanhong Cheng,Lingqian Hu,Yuheng Bu,Shenhao Wang*

Main category: stat.ML

TL;DR: 本文提出替代图概念，设计了嵌套效用图神经网络NestGNN用于出行方式选择分析，理论上泛化经典NL模型和现有DNN，实证显示NestGNN显著优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 经典NL模型表示能力有限、效用需手工指定，现有DNN无法明确捕捉离散选择中的替代相关性。

Method: 提出替代图概念，设计NestGNN作为经典NL模型在神经网络家族中的泛化。

Result: NestGNN显著优于基准模型，比对应NL模型性能高9.2%，保留两层替代模式且设计更灵活。

Conclusion: NestGNN在预测、解释和泛化经典NL模型用于出行方式选择分析方面有强大能力。

Abstract: Nested logit (NL) has been commonly used for discrete choice analysis,
including a wide range of applications such as travel mode choice, automobile
ownership, or location decisions. However, the classical NL models are
restricted by their limited representation capability and handcrafted utility
specification. While researchers introduced deep neural networks (DNNs) to
tackle such challenges, the existing DNNs cannot explicitly capture
inter-alternative correlations in the discrete choice context. To address the
challenges, this study proposes a novel concept - alternative graph - to
represent the relationships among travel mode alternatives. Using a nested
alternative graph, this study further designs a nested-utility graph neural
network (NestGNN) as a generalization of the classical NL model in the neural
network family. Theoretically, NestGNNs generalize the classical NL models and
existing DNNs in terms of model representation, while retaining the crucial
two-layer substitution patterns of the NL models: proportional substitution
within a nest but non-proportional substitution beyond a nest. Empirically, we
find that the NestGNNs significantly outperform the benchmark models,
particularly the corresponding NL models by 9.2\%. As shown by elasticity
tables and substitution visualization, NestGNNs retain the two-layer
substitution patterns as the NL model, and yet presents more flexibility in its
model design space. Overall, our study demonstrates the power of NestGNN in
prediction, interpretation, and its flexibility of generalizing the classical
NL model for analyzing travel mode choice.

</details>


### [154] [Kernel VICReg for Self-Supervised Learning in Reproducing Kernel Hilbert Space](https://arxiv.org/abs/2509.07289)
*M. Hadi Sepanj,Benyamin Ghojogh,Paul Fieguth*

Main category: stat.ML

TL;DR: 提出Kernel VICReg框架，将VICReg目标提升到RKHS，避免表征崩溃，在复杂或小尺度数据任务上表现更好，证明核化SSL目标是连接经典核方法与现代表征学习的有前途方向。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法多在欧几里得空间操作，难以捕捉非线性依赖和几何结构。

Method: 提出Kernel VICReg框架，对损失的方差、不变性和协方差项进行核化，在双中心核矩阵和希尔伯特 - 施密特范数上操作。

Result: 在多个数据集上比欧几里得VICReg有一致提升，UMAP可视化显示核嵌入有更好的等距性和类分离。

Conclusion: 核化SSL目标是连接经典核方法与现代表征学习的有前途方向。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for
representation learning by optimizing geometric objectives--such as invariance
to augmentations, variance preservation, and feature decorrelation--without
requiring labels. However, most existing methods operate in Euclidean space,
limiting their ability to capture nonlinear dependencies and geometric
structures. In this work, we propose Kernel VICReg, a novel self-supervised
learning framework that lifts the VICReg objective into a Reproducing Kernel
Hilbert Space (RKHS). By kernelizing each term of the loss-variance,
invariance, and covariance--we obtain a general formulation that operates on
double-centered kernel matrices and Hilbert-Schmidt norms, enabling nonlinear
feature learning without explicit mappings.
  We demonstrate that Kernel VICReg not only avoids representational collapse
but also improves performance on tasks with complex or small-scale data.
Empirical evaluations across MNIST, CIFAR-10, STL-10, TinyImageNet, and
ImageNet100 show consistent gains over Euclidean VICReg, with particularly
strong improvements on datasets where nonlinear structures are prominent. UMAP
visualizations further confirm that kernel-based embeddings exhibit better
isometry and class separation. Our results suggest that kernelizing SSL
objectives is a promising direction for bridging classical kernel methods with
modern representation learning.

</details>


### [155] [Identifying Neural Signatures from fMRI using Hybrid Principal Components Regression](https://arxiv.org/abs/2509.07300)
*Jared Rieck,Julia Wrobel,Joshua L. Gowin,Yue Wang,Martin Paulus,Ryan Peterson*

Main category: stat.ML

TL;DR: 本文改进LASSO PCR模型并提出新方法JSRL，应用于多项任务脑激活数据，结果显示新方法分类性能更好，是MVPA的可靠替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统LASSO PCR模型假设所有成分包含相关信息的可能性相同，但实际任务相关信号可能集中在特定成分，该模型无法选择最优主成分集。

Method: 对LASSO PCR进行修改，使正则化惩罚与主成分索引直接相关；提出新的混合方法JSRL，在信息对等框架下整合成分级和体素级活动并施加排序稀疏性以指导成分选择。

Result: 将稀疏排序纳入LASSO PCR可提高模型分类性能，JSRL在交叉验证偏差$R^2$上提升达51.7%，在交叉验证AUC上提升7.3%；稀疏排序模型在所有分类任务中表现与或优于标准LASSO PCR方法。

Conclusion: 稀疏排序模型是MVPA的可靠替代方案。

Abstract: Recent advances in neuroimaging analysis have enabled accurate decoding of
mental state from brain activation patterns during functional magnetic
resonance imaging scans. A commonly applied tool for this purpose is principal
components regression regularized with the least absolute shrinkage and
selection operator (LASSO PCR), a type of multi-voxel pattern analysis (MVPA).
This model presumes that all components are equally likely to harbor relevant
information, when in fact the task-related signal may be concentrated in
specific components. In such cases, the model will fail to select the optimal
set of principal components that maximizes the total signal relevant to the
cognitive process under study. Here, we present modifications to LASSO PCR that
allow for a regularization penalty tied directly to the index of the principal
component, reflecting a prior belief that task-relevant signal is more likely
to be concentrated in components explaining greater variance. Additionally, we
propose a novel hybrid method, Joint Sparsity-Ranked LASSO (JSRL), which
integrates component-level and voxel-level activity under an information parity
framework and imposes ranked sparsity to guide component selection. We apply
the models to brain activation during risk taking, monetary incentive, and
emotion regulation tasks. Results demonstrate that incorporating sparsity
ranking into LASSO PCR produces models with enhanced classification
performance, with JSRL achieving up to 51.7\% improvement in cross-validated
deviance $R^2$ and 7.3\% improvement in cross-validated AUC. Furthermore,
sparsity-ranked models perform as well as or better than standard LASSO PCR
approaches across all classification tasks and allocate predictive weight to
brain regions consistent with their established functional roles, offering a
robust alternative for MVPA.

</details>


### [156] [Asynchronous Gossip Algorithms for Rank-Based Statistical Methods](https://arxiv.org/abs/2509.07543)
*Anna Van Elst,Igor Colin,Stephan Clémençon*

Main category: stat.ML

TL;DR: 随着去中心化AI和边缘智能发展，本文开发计算基于秩的统计量的流言算法用于分布式两样本假设检验，给出收敛保证并实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统去中心化算法依赖简单统计量，易受数据污染影响，需要更鲁棒的统计量。

Method: 开发计算基于秩的统计量（如L统计量和秩统计量）的流言算法，应用于分布式两样本假设检验，提出首个用于Wilcoxon秩和检验的流言算法。

Result: 提供了严格的收敛保证，包括首个基于异步流言的秩估计收敛率界，通过不同网络拓扑实验验证理论结果。

Conclusion: 所提出的流言算法在去中心化AI和边缘智能的分布式环境中能有效计算鲁棒统计量，可用于假设检验且有收敛保证。

Abstract: As decentralized AI and edge intelligence become increasingly prevalent,
ensuring robustness and trustworthiness in such distributed settings has become
a critical issue-especially in the presence of corrupted or adversarial data.
Traditional decentralized algorithms are vulnerable to data contamination as
they typically rely on simple statistics (e.g., means or sum), motivating the
need for more robust statistics. In line with recent work on decentralized
estimation of trimmed means and ranks, we develop gossip algorithms for
computing a broad class of rank-based statistics, including L-statistics and
rank statistics-both known for their robustness to outliers. We apply our
method to perform robust distributed two-sample hypothesis testing, introducing
the first gossip algorithm for Wilcoxon rank-sum tests. We provide rigorous
convergence guarantees, including the first convergence rate bound for
asynchronous gossip-based rank estimation. We empirically validate our
theoretical results through experiments on diverse network topologies.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [157] [nsEVDx: A Python library for modeling Non-Stationary Extreme Value Distributions](https://arxiv.org/abs/2509.07261)
*Nischal Kafle,Claudio I. Meier*

Main category: stat.CO

TL;DR: nsEVDx是用于拟合极值分布的开源Python包，支持平稳和非平稳建模，易用且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有极值理论工具复杂或缺乏贝叶斯选项，需要一个实用、易用且可扩展的工具来分析非平稳环境中的极端事件。

Method: 使用频率论和贝叶斯方法，贝叶斯推理采用Metropolis - Hastings、MALA和HMC等蒙特卡罗采样技术。

Result: 开发出nsEVDx包，仅需标准科学Python库实现核心功能，支持灵活的非平稳建模。

Conclusion: nsEVDx可作为分析非平稳环境中极端事件的实用工具。

Abstract: nsEVDx is an open-source Python package for fitting stationary and
nonstationary Extreme Value Distributions (EVDs) to extreme value data. It can
be used to model extreme events in fields like hydrology, climate science,
finance, and insurance, using both frequentist and Bayesian methods. For
Bayesian inference it employs advanced Monte Carlo sampling techniques such as
Metropolis-Hastings, Metropolis-adjusted Langevin (MALA), and Hamiltonian Monte
Carlo (HMC). Unlike many existing extreme value theory (EVT) tools, which can
be complex or lack Bayesian options, nsEVDx offers an intuitive, Python-native
interface that is both user-friendly and extensible. It requires only standard
scientific Python libraries (numpy, scipy) for its core functionality, while
optional features like plotting and diagnostics use matplotlib and seaborn. A
key feature of nsEVDx is its flexible support for non-stationary modeling,
where the location, scale, and shape parameters can each depend on arbitrary,
user-defined covariates. This enables practical applications such as linking
extremes to other variables (e.g., rainfall extremes to temperature or maximum
stock market losses to market volatility indices). Overall, nsEVDx aims to
serve as a practical, easy-to-use, and extensible tool for researchers and
practitioners analyzing extreme events in non-stationary environments.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [158] [Contradictions](https://arxiv.org/abs/2509.07026)
*Yang Xu,Shuwei Chen,Xiaomei Zhong,Jun Liu,Xingxing He*

Main category: cs.LO

TL;DR: 本文聚焦标准矛盾的系统构建，研究两种主要标准矛盾的构造方法，提出判定子句集可满足性的程序及计算标准子矛盾数量的公式，拓展自动推理系统能力。


<details>
  <summary>Details</summary>
Motivation: 经典二元归结在自动定理证明中有局限，需克服瓶颈以提升自动推理系统的表达和演绎能力。

Method: 研究最大三角标准矛盾和三角型标准矛盾的构造方法，基于这些结构提出判定子句集可满足性的程序，推导计算标准子矛盾数量的公式。

Result: 得出判定子句集可满足性的程序和计算标准子矛盾数量的公式。

Conclusion: 研究为基于矛盾分离的动态多子句自动演绎提供方法基础，使自动推理系统能力超越经典二元范式。

Abstract: Trustworthy AI requires reasoning systems that are not only powerful but also
transparent and reliable. Automated Theorem Proving (ATP) is central to formal
reasoning, yet classical binary resolution remains limited, as each step
involves only two clauses and eliminates at most two literals. To overcome this
bottleneck, the concept of standard contradiction and the theory of
contradiction-separation-based deduction were introduced in 2018. This paper
advances that framework by focusing on the systematic construction of standard
contradictions. Specially, this study investigates construction methods for two
principal forms of standard contradiction: the maximum triangular standard
contradiction and the triangular-type standard contradiction. Building on these
structures, we propose a procedure for determining the satisfiability and
unsatisfiability of clause sets via maximum standard contradiction.
Furthermore, we derive formulas for computing the number of standard
sub-contradictions embedded within both the maximum triangular standard
contradiction and the triangular-type standard contradiction. The results
presented herein furnish the methodological basis for advancing
contradiction-separation-based dynamic multi-clause automated deduction,
thereby extending the expressive and deductive capabilities of automated
reasoning systems beyond the classical binary paradigm.

</details>


<div id='nucl-th'></div>

# nucl-th [[Back]](#toc)

### [159] [Nuclear Data Adjustment for Nonlinear Applications in the OECD/NEA WPNCS SG14 Benchmark -- A Bayesian Inverse UQ-based Approach for Data Assimilation](https://arxiv.org/abs/2509.07790)
*Christopher Brady,Xu Wu*

Main category: nucl-th

TL;DR: 经合组织核临界安全工作组开展基准测试，引入贝叶斯逆不确定性量化（IUQ）用于核数据调整并与传统方法对比，显示IUQ在核数据调整有潜力。


<details>
  <summary>Details</summary>
Motivation: 评估当前核数据调整技术在非线性应用和低相关性实验中的性能。

Method: 引入贝叶斯逆不确定性量化（IUQ）方法进行核数据调整，并与广义线性最小二乘法（GLLS）和蒙特卡罗贝叶斯（MOCABA）对比。

Result: 线性应用中IUQ后验预测与GLLS和MOCABA一致；非线性应用中GLLS预测无法复制计算响应分布，MOCABA接近一致，IUQ直接使用计算模型响应。

Conclusion: 基准测试表现显示贝叶斯IUQ在核数据调整中有潜力。

Abstract: The Organization for Economic Cooperation and Development (OECD) Working
Party on Nuclear Criticality Safety (WPNCS) proposed a benchmark exercise to
assess the performance of current nuclear data adjustment techniques applied to
nonlinear applications and experiments with low correlation to applications.
This work introduces Bayesian Inverse Uncertainty Quantification (IUQ) as a
method for nuclear data adjustments in this benchmark, and compares IUQ to the
more traditional methods of Generalized Linear Least Squares (GLLS) and Monte
Carlo Bayes (MOCABA). Posterior predictions from IUQ showed agreement with GLLS
and MOCABA for linear applications. When comparing GLLS, MOCABA, and IUQ
posterior predictions to computed model responses using adjusted parameters, we
observe that GLLS predictions fail to replicate computed response distributions
for nonlinear applications, while MOCABA shows near agreement, and IUQ uses
computed model responses directly. We also discuss observations on why
experiments with low correlation to applications can be informative to nuclear
data adjustments and identify some properties useful in selecting experiments
for inclusion in nuclear data adjustment. Performance in this benchmark
indicates potential for Bayesian IUQ in nuclear data adjustments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [160] [Multi-Topic Projected Opinion Dynamics for Resource Allocation](https://arxiv.org/abs/2509.07847)
*Prashil Wankhede,Nirabhra Mandal,Sonia Martínez,Pavankumar Tallapragada*

Main category: eess.SY

TL;DR: 提出多主体在多主题资源分配的观点形成模型，证明观点收敛，分析特殊网络情况，指出是潜在博弈并关联动态均衡与纳什均衡，用模拟验证。


<details>
  <summary>Details</summary>
Motivation: 研究多主体在硬预算约束下对多主题资源分配的观点形成。

Method: 定义各主体效用函数，推导投影动态系统模型，考虑主体间通过社交网络耦合、主题间通过资源约束耦合。

Result: 观点总会收敛到均衡集；特殊弱对抗关系网络中观点收敛到唯一均衡点；该观点形成博弈是潜在博弈；关联动态均衡和博弈的纳什均衡并刻画无对抗关系网络的唯一纳什均衡。

Conclusion: 所提出的模型有良好的收敛性和博弈特性，模拟结果支持相关结论。

Abstract: We propose a model of opinion formation on resource allocation among multiple
topics by multiple agents, who are subject to hard budget constraints. We
define a utility function for each agent and then derive a projected dynamical
system model of opinion evolution assuming that each agent myopically seeks to
maximize its utility subject to its constraints. Inter-agent coupling arises
from an undirected social network, while inter-topic coupling arises from
resource constraints. We show that opinions always converge to the equilibrium
set. For special networks with very weak antagonistic relations, the opinions
converge to a unique equilibrium point. We further show that the underlying
opinion formation game is a potential game. We relate the equilibria of the
dynamics and the Nash equilibria of the game and characterize the unique Nash
equilibrium for networks with no antagonistic relations. Finally, simulations
illustrate our findings.

</details>


### [161] [A smart fridge with AI-enabled food computing](https://arxiv.org/abs/2509.07400)
*Khue Nong Thuc,Khoa Tran Nguyen Anh,Tai Nguyen Huy,Du Nguyen Hao Hong,Khanh Dinh Ba*

Main category: eess.SY

TL;DR: 本文介绍物联网与计算机视觉结合的智能冰箱系统，解决高密度库存下物体检测难题，提出三模块系统和改进损失函数，提升检测可靠性，推动可持续生活。


<details>
  <summary>Details</summary>
Motivation: 提升智能冰箱在食物管理中的效率，减少浪费、优化消费，解决高密度库存下物体检测不准确问题。

Method: 构建三核心模块系统，实现数据预处理、物体检测管理和可视化；采用改进的焦点损失函数，通过温度缩放进行自适应、按类别的误差校准。

Result: 在不同光照和可扩展性挑战下，鲁棒的功能校准显著提高了检测可靠性。

Conclusion: 提出的系统是实用、以用户为中心的现代食物管理方法，有助于实现可持续生活目标。

Abstract: The Internet of Things (IoT) plays a crucial role in enabling seamless
connectivity and intelligent home automation, particularly in food management.
By integrating IoT with computer vision, the smart fridge employs an ESP32-CAM
to establish a monitoring subsystem that enhances food management efficiency
through real-time food detection, inventory tracking, and temperature
monitoring. This benefits waste reduction, grocery planning improvement, and
household consumption optimization. In high-density inventory conditions,
capturing partial or layered images complicates object detection, as
overlapping items and occluded views hinder accurate identification and
counting. Besides, varied angles and obscured details in multi-layered setups
reduce algorithm reliability, often resulting in miscounts or
misclassifications. Our proposed system is structured into three core modules:
data pre-processing, object detection and management, and a web-based
visualization. To address the challenge of poor model calibration caused by
overconfident predictions, we implement a variant of focal loss that mitigates
over-confidence and under-confidence in multi-category classification. This
approach incorporates adaptive, class-wise error calibration via temperature
scaling and evaluates the distribution of predicted probabilities across
methods. Our results demonstrate that robust functional calibration
significantly improves detection reliability under varying lighting conditions
and scalability challenges. Further analysis demonstrates a practical,
user-focused approach to modern food management, advancing sustainable living
goals through reduced waste and more informed consumption.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [162] [Synthetic Data Generation with Lorenzetti for Time Series Anomaly Detection in High-Energy Physics Calorimeters](https://arxiv.org/abs/2509.07451)
*Laura Boggia,Bogdan Malaescu*

Main category: hep-ex

TL;DR: 使用Lorenzetti模拟器生成含异常的合成事件，评估多种时间序列异常检测方法，该方法通用。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列异常检测对物理实验数据质量很重要，但存在标签稀缺、异常类型未知和维度间复杂关联等挑战。

Method: 使用Lorenzetti模拟器生成含量能器异常的合成事件，评估包括基于Transformer和其他深度学习模型在内的多种时间序列异常检测方法。

Result: 未提及具体结果。

Conclusion: 所采用的方法通用，适用于不同探测器设计和缺陷。

Abstract: Anomaly detection in multivariate time series is crucial to ensure the
quality of data coming from a physics experiment. Accurately identifying the
moments when unexpected errors or defects occur is essential, yet challenging
due to scarce labels, unknown anomaly types, and complex correlations across
dimensions. To address the scarcity and unreliability of labelled data, we use
the Lorenzetti Simulator to generate synthetic events with injected calorimeter
anomalies. We then assess the sensitivity of several time series anomaly
detection methods, including transformer-based and other deep learning models.
The approach employed here is generic and applicable to different detector
designs and defects.

</details>


### [163] [RINO: Renormalization Group Invariance with No Labels](https://arxiv.org/abs/2509.07486)
*Zichun Hao,Raghav Kansal,Abhijith Gandrakota,Chang Sun,Ngadiuba Jennifer,Javier Duarte,Maria Spiropulu*

Main category: hep-ex

TL;DR: 为缓解高能物理中监督式机器学习的领域偏移问题，提出自监督学习方法RINO，实验显示其能提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决高能物理中监督式机器学习依赖模拟数据进行标注，可能导致对底层碰撞或探测器响应建模错误的领域偏移问题。

Method: 提出RINO自监督学习方法，在JetClass数据集上预训练基于transformer的模型，再在JetNet数据集上微调，用于识别来自顶夸克衰变的喷流。

Result: 与直接在JetNet上进行监督训练相比，RINO在从JetNet训练数据到JetClass数据上表现出更好的泛化能力。

Conclusion: RINO在真实碰撞数据上预训练，再在小而高质量的MC数据集上微调，可提高高能物理中机器学习模型的鲁棒性。

Abstract: A common challenge with supervised machine learning (ML) in high energy
physics (HEP) is the reliance on simulations for labeled data, which can often
mismodel the underlying collision or detector response. To help mitigate this
problem of domain shift, we propose RINO (Renormalization Group Invariance with
No Labels), a self-supervised learning approach that can instead pretrain
models directly on collision data, learning embeddings invariant to
renormalization group flow scales. In this work, we pretrain a
transformer-based model on jets originating from quantum chromodynamic (QCD)
interactions from the JetClass dataset, emulating real QCD-dominated
experimental data, and then finetune on the JetNet dataset -- emulating
simulations -- for the task of identifying jets originating from top quark
decays. RINO demonstrates improved generalization from the JetNet training data
to JetClass data compared to supervised training on JetNet from scratch,
demonstrating the potential for RINO pretraining on real collision data
followed by fine-tuning on small, high-quality MC datasets, to improve the
robustness of ML models in HEP.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [164] [Towards automatizing detection and quantification of intestinal metaplasia: a multi-expert comparative study](https://arxiv.org/abs/2509.06991)
*Fabian Cano,Mauricio Caviedes,Andres Siabatto,Jesus Villarreal,Jose Quijano,Alvaro Bedoya,Satish E. Viswanath,Angel Cruz-Roa,Fabio Gonzalez,Eduardo Romero*

Main category: q-bio.TO

TL;DR: 研究提出用深度学习模型检测和量化肠化生的自动化方法并与病理学家评估对比，显示模型更精确、可重复性更高。


<details>
  <summary>Details</summary>
Motivation: 现有胃癌风险系统因依赖肠化生百分比的视觉评估易出错，需更精确方法。

Method: 从两个队列收集胃样本，选择并训练深度学习模型分类肠化生，估计百分比和风险评分，与三位病理学家独立盲评结果对比。

Result: 最佳深度学习架构F1 - Score为0.80 ± 0.01，AUC为0.91 ± 0.01；病理学家间Fleiss's Kappa评分为0.61 - 0.75；病理学家与模型一致性为0.37 - 0.54。

Conclusion: 深度学习模型检测和量化肠化生百分比比经验丰富的病理学家更精确、可重复性更高，视觉评估风险存在高观察者间变异性。

Abstract: Current gastric cancer risk systems are prone to errors since they evaluate a
visual estimation of intestinal metaplasia percentages to assign a risk. This
study presents an automated method to detect and quantify intestinal metaplasia
using deep learning models as well as a comparative analysis with visual
estimations of three experienced pathologists. Gastric samples were collected
from two different cohorts: 149 asymptomatic volunteers from a region with a
high prevalence of GCa in Colombia and 56 patients from a third-level hospital.
Deep learning models were selected and trained to classify intestinal
metaplasia, and predictions were used to estimate the percentage of intestinal
metaplasia and assign the risk score. Results were compared with independent
blinded assessments performed by three experienced pathologists. The
best-performing deep learning architecture classified intestinal metaplasia
with F1-Score of 0.80 +- 0.01 and AUC of 0.91 +- 0.01. Among pathologists,
inter-observer agreement by a Fleiss's Kappa score ranged from 0.61 to 0.75. In
comparison, agreement between the pathologists and the best-performing model
ranged from 0.37 to 0.54. Deep learning models show potential to detect and
quantify the percentage of intestinal metaplasia with greater precision and
reproducibility than experienced pathologists. Likewise, estimated risk shows
high inter-observer variability when visually assigning the intestinal
metaplasia percentage.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [165] [Quantum algorithms for general nonlinear dynamics based on the Carleman embedding](https://arxiv.org/abs/2509.07155)
*David Jennings,Kamil Korzekwa,Matteo Lostaglio,Andrew T Sornborger,Yigit Subasi,Guoming Wang*

Main category: quant-ph

TL;DR: 本文修正先前分析的技术问题，扩展可在量子计算机上高效模拟的非线性动力系统范围。


<details>
  <summary>Details</summary>
Motivation: 先前求解非线性微分方程的量子算法局限于纯耗散系统，无法应用于许多重要问题，需要扩展适用范围。

Method: 修正技术问题，从纯耗散系统扩展到更广泛的稳定系统，将稳定系统结果拓展到有守恒多项式量的物理相关场景，对非共振系统给出大量结果。

Result: 证明存在适用于更广泛非线性系统的高效量子算法，证明指数规模非线性振子问题的BQP完备性，还得到与庞加莱 - 杜拉克定理和卡尔曼矩阵对角化相关的结果。

Conclusion: 能在量子计算机上高效模拟的非线性系统范围比之前更广。

Abstract: Important nonlinear dynamics, such as those found in plasma and fluid
systems, are typically hard to simulate on classical computers. Thus, if
fault-tolerant quantum computers could efficiently solve such nonlinear
problems, it would be a transformative change for many industries. In a recent
breakthrough [Liu et al., PNAS 2021], the first efficient quantum algorithm for
solving nonlinear differential equations was constructed, based on a single
condition $R<1$, where $R$ characterizes the ratio of nonlinearity to
dissipation. This result, however, is limited to the class of purely
dissipative systems with negative log-norm, which excludes application to many
important problems. In this work, we correct technical issues with this and
other prior analysis, and substantially extend the scope of nonlinear dynamical
systems that can be efficiently simulated on a quantum computer in a number of
ways. Firstly, we extend the existing results from purely dissipative systems
to a much broader class of stable systems, and show that every quadratic
Lyapunov function for the linearized system corresponds to an independent
$R$-number criterion for the convergence of the Carlemen scheme. Secondly, we
extend our stable system results to physically relevant settings where
conserved polynomial quantities exist. Finally, we provide extensive results
for the class of non-resonant systems. With this, we are able to show that
efficient quantum algorithms exist for a much wider class of nonlinear systems
than previously known, and prove the BQP-completeness of nonlinear oscillator
problems of exponential size. In our analysis, we also obtain several results
related to the Poincar\'{e}-Dulac theorem and diagonalization of the Carleman
matrix, which could be of independent interest.

</details>


### [166] [A Quantum Bagging Algorithm with Unsupervised Base Learners for Label Corrupted Datasets](https://arxiv.org/abs/2509.07040)
*Neeshu Rathi,Sanjeev Kumar*

Main category: quant-ph

TL;DR: 提出量子装袋框架，用QMeans聚类作基学习器，模拟显示其表现与经典方法相当且更抗标签噪声。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代开发抗噪声量子机器学习算法。

Method: 提出量子装袋框架，以QMeans聚类为基学习器，结合基于QRAM采样的量子自助法和多数投票的装袋聚合。

Result: 在噪声分类和回归任务模拟中，表现与用KMeans的经典方法相当，比监督装袋方法更抗标签损坏。

Conclusion: 无监督量子装袋在从不可靠数据中学习有潜力。

Abstract: The development of noise-resilient quantum machine learning (QML) algorithms
is critical in the noisy intermediate-scale quantum (NISQ) era. In this work,
we propose a quantum bagging framework that uses QMeans clustering as the base
learner to reduce prediction variance and enhance robustness to label noise.
Unlike bagging frameworks built on supervised learners, our method leverages
the unsupervised nature of QMeans, combined with quantum bootstrapping via
QRAM-based sampling and bagging aggregation through majority voting. Through
extensive simulations on both noisy classification and regression tasks, we
demonstrate that the proposed quantum bagging algorithm performs comparably to
its classical counterpart using KMeans while exhibiting greater resilience to
label corruption than supervised bagging methods. This highlights the potential
of unsupervised quantum bagging in learning from unreliable data.

</details>


### [167] [Toward Quantum Utility in Finance: A Robust Data-Driven Algorithm for Asset Clustering](https://arxiv.org/abs/2509.07766)
*Shivam Sharma,Supreeth Mysore Venkatesh,Pushkin Kachroo*

Main category: quant-ph

TL;DR: 本文将GCS - Q算法用于金融资产聚类，在合成和真实金融数据上验证，结果表明该算法聚类质量高且能动态确定簇数量，凸显近量子计算在金融应用的实用价值。


<details>
  <summary>Details</summary>
Motivation: 经典聚类方法处理有符号相关结构不足，需有损转换和启发式假设。

Method: 应用GCS - Q算法直接对有符号加权图聚类，将划分步骤构建为QUBO问题，利用量子退火探索解空间。

Result: 在合成和真实金融数据上实验，GCS - Q聚类质量更高，能动态确定簇数量。

Conclusion: 近量子计算在金融基于图的无监督学习中有实用价值。

Abstract: Clustering financial assets based on return correlations is a fundamental
task in portfolio optimization and statistical arbitrage. However, classical
clustering methods often fall short when dealing with signed correlation
structures, typically requiring lossy transformations and heuristic assumptions
such as a fixed number of clusters. In this work, we apply the Graph-based
Coalition Structure Generation algorithm (GCS-Q) to directly cluster signed,
weighted graphs without relying on such transformations. GCS-Q formulates each
partitioning step as a QUBO problem, enabling it to leverage quantum annealing
for efficient exploration of exponentially large solution spaces. We validate
our approach on both synthetic and real-world financial data, benchmarking
against state-of-the-art classical algorithms such as SPONGE and k-Medoids. Our
experiments demonstrate that GCS-Q consistently achieves higher clustering
quality, as measured by Adjusted Rand Index and structural balance penalties,
while dynamically determining the number of clusters. These results highlight
the practical utility of near-term quantum computing for graph-based
unsupervised learning in financial applications.

</details>


### [168] [From Classical Data to Quantum Advantage -- Quantum Policy Evaluation on Quantum Hardware](https://arxiv.org/abs/2509.07614)
*Daniel Hein,Simon Wiedemann,Markus Baumann,Patrik Felbinger,Justin Klein,Maximilian Schieder,Jonas Stein,Daniëlle Schuman,Thomas Cope,Steffen Udluft*

Main category: quant-ph

TL;DR: 本文展示如何通过量子机器学习从经典观测数据中学习量子环境参数，并应用于量子策略评估，实验显示量子机器学习与量子策略评估结合在强化学习中有实现量子优势的潜力。


<details>
  <summary>Details</summary>
Motivation: 此前量子环境是手动实现和参数化的，本文旨在通过量子机器学习从经典观测数据中学习量子环境参数。

Method: 通过量子机器学习在量子硬件上从一批经典观测数据中学习环境参数，并将学习到的量子环境应用于量子策略评估。

Result: 尽管存在噪声和短相干时间等挑战，但实验表明量子机器学习和量子策略评估的集成有潜力实现量子优势。

Conclusion: 量子机器学习和量子策略评估的集成在强化学习中显示出实现量子优势的前景。

Abstract: Quantum policy evaluation (QPE) is a reinforcement learning (RL) algorithm
which is quadratically more efficient than an analogous classical Monte Carlo
estimation. It makes use of a direct quantum mechanical realization of a finite
Markov decision process, in which the agent and the environment are modeled by
unitary operators and exchange states, actions, and rewards in superposition.
Previously, the quantum environment has been implemented and parametrized
manually for an illustrative benchmark using a quantum simulator. In this
paper, we demonstrate how these environment parameters can be learned from a
batch of classical observational data through quantum machine learning (QML) on
quantum hardware. The learned quantum environment is then applied in QPE to
also compute policy evaluations on quantum hardware. Our experiments reveal
that, despite challenges such as noise and short coherence times, the
integration of QML and QPE shows promising potential for achieving quantum
advantage in RL.

</details>


### [169] [Variational Quantum Circuits in Offline Contextual Bandit Problems](https://arxiv.org/abs/2509.07633)
*Lukas Schulte,Daniel Hein,Steffen Udluft,Thomas A. Runkler*

Main category: quant-ph

TL;DR: 本文探讨变分量子电路在工业优化任务离线上下文老虎机问题中的应用，对比量子与经典模型，证明量子模型有潜力。


<details>
  <summary>Details</summary>
Motivation: 探索变分量子电路在工业优化任务离线上下文老虎机问题中的应用。

Method: 使用工业基准环境评估量子回归模型与经典模型的性能，通过粒子群优化确定最优配置。

Result: 量子模型能有效拟合复杂奖励函数，在噪声和稀疏数据集上泛化性好。

Conclusion: 证明了变分量子电路用于离线上下文老虎机问题的可行性，凸显其在工业优化任务中的潜力。

Abstract: This paper explores the application of variational quantum circuits (VQCs)
for solving offline contextual bandit problems in industrial optimization
tasks. Using the Industrial Benchmark (IB) environment, we evaluate the
performance of quantum regression models against classical models. Our findings
demonstrate that quantum models can effectively fit complex reward functions,
identify optimal configurations via particle swarm optimization (PSO), and
generalize well in noisy and sparse datasets. These results provide a proof of
concept for utilizing VQCs in offline contextual bandit problems and highlight
their potential in industrial optimization tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [170] [Leveraging Digital Twin-as-a-Service Towards Continuous and Automated Cybersecurity Certification](https://arxiv.org/abs/2509.07649)
*Ioannis Koufos,Abdul Rehman Qureshi,Adrian Asensio,Allen Abishek,Efstathios Zaragkas,Ricard Vilalta,Maria Souvalioti,George Xilouris,Michael-Alexandros Kourtis*

Main category: cs.CR

TL;DR: 本文提出SDT - aaS方法解决传统风险评估问题，通过DT技术实现自动化、非侵入式安全合规，实证证明其可行性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统风险评估依赖人工审计和系统扫描，会造成运营中断和安全漏洞，需新方法解决。

Method: 提出Security Digital Twin - as - a - Service (SDT - aaS)方法，利用DT技术，镜像现实资产、收集合规工件、创建机器可读证据以实现实时安全评估。

Result: 通过中等规模基础设施用例的实证结果，证明了该方法的可行性和性能。

Conclusion: 该方法是可扩展、可互操作的，支持开放标准，为高效、按需的网络安全治理铺平道路，且对运营影响极小。

Abstract: Traditional risk assessments rely on manual audits and system scans, often
causing operational disruptions and leaving security gaps. To address these
challenges, this work presents Security Digital Twin-as-a-Service (SDT-aaS), a
novel approach that leverages Digital Twin (DT) technology for automated,
non-intrusive security compliance. SDT-aaS enables real-time security
assessments by mirroring real-world assets, collecting compliance artifacts,
and creating machine-readable evidence. The proposed work is a scalable and
interoperable solution that supports open standards like CycloneDX and Web of
Things (WoT), facilitating seamless integration and efficient compliance
management. Empirical results from a moderate-scale infrastructure use case
demonstrate its feasibility and performance, paving the way for efficient,
on-demand cybersecurity governance with minimal operational impact.

</details>


### [171] [SafeToolBench: Pioneering a Prospective Benchmark to Evaluating Tool Utilization Safety in LLMs](https://arxiv.org/abs/2509.07315)
*Hongfei Xia,Hongru Wang,Zeming Liu,Qian Yu,Yuhang Guo,Haifeng Wang*

Main category: cs.CR

TL;DR: 论文提出SafeToolBench基准和SafeInstructTool框架，前瞻性评估大语言模型工具使用安全性，实验表明框架提升了大语言模型自我认知。


<details>
  <summary>Details</summary>
Motivation: 现有外部工具会因模糊或恶意用户指令放大风险，以往研究多为事后评估，本文旨在前瞻性评估大语言模型工具使用安全性以避免不可逆伤害。

Method: 提出SafeToolBench基准进行前瞻性评估，提出SafeInstructTool框架从三个视角九个维度增强大语言模型工具使用安全意识。

Result: 对四个大语言模型实验显示，现有方法无法捕捉工具使用的所有风险，而提出的框架显著提升大语言模型自我认知。

Conclusion: 所提框架能使大语言模型工具使用更安全可信。

Abstract: Large Language Models (LLMs) have exhibited great performance in autonomously
calling various tools in external environments, leading to better problem
solving and task automation capabilities. However, these external tools also
amplify potential risks such as financial loss or privacy leakage with
ambiguous or malicious user instructions. Compared to previous studies, which
mainly assess the safety awareness of LLMs after obtaining the tool execution
results (i.e., retrospective evaluation), this paper focuses on prospective
ways to assess the safety of LLM tool utilization, aiming to avoid irreversible
harm caused by directly executing tools. To this end, we propose SafeToolBench,
the first benchmark to comprehensively assess tool utilization security in a
prospective manner, covering malicious user instructions and diverse practical
toolsets. Additionally, we propose a novel framework, SafeInstructTool, which
aims to enhance LLMs' awareness of tool utilization security from three
perspectives (i.e., \textit{User Instruction, Tool Itself, and Joint
Instruction-Tool}), leading to nine detailed dimensions in total. We experiment
with four LLMs using different methods, revealing that existing approaches fail
to capture all risks in tool utilization. In contrast, our framework
significantly enhances LLMs' self-awareness, enabling a more safe and
trustworthy tool utilization.

</details>


### [172] [Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV](https://arxiv.org/abs/2509.07016)
*Muhammad Arif Hakimi Zamrai,Kamaludin Mohd Yusof*

Main category: cs.CR

TL;DR: 针对SD - IoV中TCP SYN洪泛攻击，优化随机森林分类器模型提升车联网安全，指标表现优，检测时间短。


<details>
  <summary>Details</summary>
Motivation: 解决SD - IoV中TCP SYN洪泛攻击问题，提升车联网通信系统的网络安全。

Method: 预处理含SYN攻击实例的数据集，采用特征缩放和标签编码技术，应用分层K折交叉验证。

Result: 各项指标平均值达0.999998，SYN DoS攻击检测时间0.24秒，微调后的随机森林模型能有效区分正常和恶意流量。

Conclusion: 该方法是检测SYN洪泛攻击的先进算法，为车联网安全提供了可靠解决方案，兼顾网络效率和可靠性。

Abstract: In response to the prevalent concern of TCP SYN flood attacks within the
context of Software-Defined Internet of Vehicles (SD-IoV), this study addresses
the significant challenge of network security in rapidly evolving vehicular
communication systems. This research focuses on optimizing a Random Forest
Classifier model to achieve maximum accuracy and minimal detection time,
thereby enhancing vehicular network security. The methodology involves
preprocessing a dataset containing SYN attack instances, employing feature
scaling and label encoding techniques, and applying Stratified K-Fold
cross-validation to target key metrics such as accuracy, precision, recall, and
F1-score. This research achieved an average value of 0.999998 for all metrics
with a SYN DoS attack detection time of 0.24 seconds. Results show that the
fine-tuned Random Forest model, configured with 20 estimators and a depth of
10, effectively differentiates between normal and malicious traffic with high
accuracy and minimal detection time, which is crucial for SD-IoV networks. This
approach marks a significant advancement and introduces a state-of-the-art
algorithm in detecting SYN flood attacks, combining high accuracy with minimal
detection time. It contributes to vehicular network security by providing a
robust solution against TCP SYN flood attacks while maintaining network
efficiency and reliability.

</details>


### [173] [SoK: Security and Privacy of AI Agents for Blockchain](https://arxiv.org/abs/2509.07131)
*Nicolò Romandini,Carlo Mazzocca,Kai Otsuki,Rebecca Montanari*

Main category: cs.CR

TL;DR: 本文进行了首个专注于区块链AI驱动系统的知识系统化研究，聚焦安全和隐私维度。


<details>
  <summary>Details</summary>
Motivation: 区块链和智能合约对非专业用户有使用障碍，现有文献缺乏针对AI代理与区块链交叉领域的全面调查。

Method: 开展首个专注于区块链AI驱动系统的知识系统化研究。

Result: 阐述了区块链AI驱动系统在安全和隐私维度的应用、局限性。

Conclusion: 指明了区块链AI驱动系统未来的研究方向。

Abstract: Blockchain and smart contracts have garnered significant interest in recent
years as the foundation of a decentralized, trustless digital ecosystem,
thereby eliminating the need for traditional centralized authorities. Despite
their central role in powering Web3, their complexity still presents
significant barriers for non-expert users. To bridge this gap, Artificial
Intelligence (AI)-based agents have emerged as valuable tools for interacting
with blockchain environments, supporting a range of tasks, from analyzing
on-chain data and optimizing transaction strategies to detecting
vulnerabilities within smart contracts. While interest in applying AI to
blockchain is growing, the literature still lacks a comprehensive survey that
focuses specifically on the intersection with AI agents. Most of the related
work only provides general considerations, without focusing on any specific
domain. This paper addresses this gap by presenting the first Systematization
of Knowledge dedicated to AI-driven systems for blockchain, with a special
focus on their security and privacy dimensions, shedding light on their
applications, limitations, and future research directions.

</details>


### [174] [Sequentially Auditing Differential Privacy](https://arxiv.org/abs/2509.07055)
*Tomás González,Mateo Dulce-Rubio,Aaditya Ramdas,Mónica Ribero*

Main category: cs.CR

TL;DR: 提出用于审计黑盒机制差分隐私保证的顺序测试，克服先前批量审计方法固定样本大小限制，实验显示检测违规所需样本量远小于现有方法。


<details>
  <summary>Details</summary>
Motivation: 克服先前批量审计方法固定样本大小的限制，提供更有效的差分隐私审计方法。

Method: 提出一种实用的顺序测试，处理机制输出流并在控制一类错误的同时提供随时有效的推断。

Result: 该测试能以比现有方法小几个数量级的样本量检测违规，将样本数量从50K减少到几百个，还能在不到一次训练运行中识别DP - SGD隐私违规。

Conclusion: 所提出的顺序测试方法在差分隐私审计方面优于现有方法。

Abstract: We propose a practical sequential test for auditing differential privacy
guarantees of black-box mechanisms. The test processes streams of mechanisms'
outputs providing anytime-valid inference while controlling Type I error,
overcoming the fixed sample size limitation of previous batch auditing methods.
Experiments show this test detects violations with sample sizes that are orders
of magnitude smaller than existing methods, reducing this number from 50K to a
few hundred examples, across diverse realistic mechanisms. Notably, it
identifies DP-SGD privacy violations in \textit{under} one training run, unlike
prior methods needing full model training.

</details>


### [175] [Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm](https://arxiv.org/abs/2509.07287)
*Yan Pang,Wenlong Meng,Xiaojing Liao,Tianhao Wang*

Main category: cs.CR

TL;DR: 本文指出大语言模型被恶意用于生成钓鱼内容的威胁，现有检测方法有局限，提出Paladin方法解决检测难题，实验显示该方法优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的钓鱼内容难以检测，现有语义级检测方法不可靠，基于大模型的检测方法计算成本高且不适合大规模部署。

Method: 提出Paladin方法，将触发 - 标签关联嵌入原始大语言模型形成工具化大模型，生成钓鱼内容时自动包含可检测标签，考虑四种不同场景，从隐蔽性、有效性和鲁棒性三个关键视角评估。

Result: 实验结果表明，该方法优于基线方法，在所有场景下检测准确率超90%。

Conclusion: Paladin方法能有效检测大语言模型生成的钓鱼内容，具有实际应用价值。

Abstract: With the rapid development of large language models, the potential threat of
their malicious use, particularly in generating phishing content, is becoming
increasingly prevalent. Leveraging the capabilities of LLMs, malicious users
can synthesize phishing emails that are free from spelling mistakes and other
easily detectable features. Furthermore, such models can generate
topic-specific phishing messages, tailoring content to the target domain and
increasing the likelihood of success.
  Detecting such content remains a significant challenge, as LLM-generated
phishing emails often lack clear or distinguishable linguistic features. As a
result, most existing semantic-level detection approaches struggle to identify
them reliably. While certain LLM-based detection methods have shown promise,
they suffer from high computational costs and are constrained by the
performance of the underlying language model, making them impractical for
large-scale deployment.
  In this work, we aim to address this issue. We propose Paladin, which embeds
trigger-tag associations into vanilla LLM using various insertion strategies,
creating them into instrumented LLMs. When an instrumented LLM generates
content related to phishing, it will automatically include detectable tags,
enabling easier identification. Based on the design on implicit and explicit
triggers and tags, we consider four distinct scenarios in our work. We evaluate
our method from three key perspectives: stealthiness, effectiveness, and
robustness, and compare it with existing baseline methods. Experimental results
show that our method outperforms the baselines, achieving over 90% detection
accuracy across all scenarios.

</details>


### [176] [zkUnlearner: A Zero-Knowledge Framework for Verifiable Unlearning with Multi-Granularity and Forgery-Resistance](https://arxiv.org/abs/2509.07290)
*Nan Wang,Nan Wu,Xiangyu Hui,Jiafan Wang,Xin Yuan*

Main category: cs.CR

TL;DR: 提出首个用于可验证机器学习遗忘的零知识框架zkUnlearner，支持多粒度和抗伪造，有计算模型和抗伪造策略并进行性能评估。


<details>
  <summary>Details</summary>
Motivation: 随着行使“被遗忘权”需求增长，需要可验证的机器学习遗忘方法保证透明度和问责性。

Method: 提出通用计算模型，采用位掩码技术实现训练零知识证明的选择性，支持多粒度遗忘；提出抗伪造策略；对框架进行基准测试和性能评估。

Result: 该方法克服了现有方法在效率和隐私方面的关键限制，能抵抗最先进的伪造攻击。

Conclusion: 框架zkUnlearner具有实用性。

Abstract: As the demand for exercising the "right to be forgotten" grows, the need for
verifiable machine unlearning has become increasingly evident to ensure both
transparency and accountability. We present {\em zkUnlearner}, the first
zero-knowledge framework for verifiable machine unlearning, specifically
designed to support {\em multi-granularity} and {\em forgery-resistance}.
  First, we propose a general computational model that employs a {\em
bit-masking} technique to enable the {\em selectivity} of existing
zero-knowledge proofs of training for gradient descent algorithms. This
innovation enables not only traditional {\em sample-level} unlearning but also
more advanced {\em feature-level} and {\em class-level} unlearning. Our model
can be translated to arithmetic circuits, ensuring compatibility with a broad
range of zero-knowledge proof systems. Furthermore, our approach overcomes key
limitations of existing methods in both efficiency and privacy. Second, forging
attacks present a serious threat to the reliability of unlearning.
Specifically, in Stochastic Gradient Descent optimization, gradients from
unlearned data, or from minibatches containing it, can be forged using
alternative data samples or minibatches that exclude it. We propose the first
effective strategies to resist state-of-the-art forging attacks. Finally, we
benchmark a zkSNARK-based instantiation of our framework and perform
comprehensive performance evaluations to validate its practicality.

</details>


### [177] [Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees](https://arxiv.org/abs/2509.07939)
*Katsuaki Nakano,Reza Feyyazi,Shanchieh Jay Yang,Michael Zuzak*

Main category: cs.CR

TL;DR: 本文提出用于渗透测试LLM代理的引导推理管道，用MITRE ATT&CK矩阵构建任务树约束推理，实验表明该方法能提升自动化网络安全评估的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有用于渗透测试的LLM代理主要依赖自引导推理，会产生不准确或幻觉的步骤，导致非生产性行动。

Method: 提出结合从MITRE ATT&CK矩阵构建的确定性任务树的引导推理管道，约束LLM的推理过程，使用三种LLM构建自动化渗透测试代理并应用于HackTheBox练习。

Result: 使用Llama - 3 - 8B、Gemini - 1.5和GPT - 4时，所提推理管道分别引导代理完成71.8%、72.8%和78.6%的子任务，而现有自引导推理工具完成率低且需更多模型查询。

Conclusion: 将确定性任务树纳入LLM推理管道可提高自动化网络安全评估的准确性和效率。

Abstract: Recent advances in Large Language Models (LLMs) have driven interest in
automating cybersecurity penetration testing workflows, offering the promise of
faster and more consistent vulnerability assessment for enterprise systems.
Existing LLM agents for penetration testing primarily rely on self-guided
reasoning, which can produce inaccurate or hallucinated procedural steps. As a
result, the LLM agent may undertake unproductive actions, such as exploiting
unused software libraries or generating cyclical responses that repeat prior
tactics. In this work, we propose a guided reasoning pipeline for penetration
testing LLM agents that incorporates a deterministic task tree built from the
MITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the
LLM's reaoning process to explicitly defined tactics, techniques, and
procedures. This anchors reasoning in proven penetration testing methodologies
and filters out ineffective actions by guiding the agent towards more
productive attack procedures. To evaluate our approach, we built an automated
penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and
GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with
103 discrete subtasks representing real-world cyberattack scenarios. Our
proposed reasoning pipeline guided the LLM agent through 71.8\%, 72.8\%, and
78.6\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.
Comparatively, the state-of-the-art LLM penetration testing tool using
self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and
required 86.2\%, 118.7\%, and 205.9\% more model queries. This suggests that
incorporating a deterministic task tree into LLM reasoning pipelines can
enhance the accuracy and efficiency of automated cybersecurity assessments

</details>


### [178] [ImportSnare: Directed "Code Manual" Hijacking in Retrieval-Augmented Code Generation](https://arxiv.org/abs/2509.07941)
*Kai Ye,Liangcai Su,Chenxiong Qian*

Main category: cs.CR

TL;DR: 本文探索检索增强代码生成（RACG）中的攻击面，提出ImportSnare攻击框架开展实验，揭示LLM开发供应链风险。


<details>
  <summary>Details</summary>
Motivation: 代码生成存在功能缺陷和安全漏洞，RAG引入新攻击面，需研究RACG攻击面。

Method: 提出ImportSnare攻击框架，采用位置感知波束搜索和多语言归纳建议两种策略构建中毒文档。

Result: 在Python、Rust和JavaScript实验中，对流行库攻击成功率超50%，低至0.01%投毒率也能成功。

Conclusion: 揭示LLM开发中供应链风险，指出代码生成任务安全对齐不足，将发布基准套件和数据集。

Abstract: Code generation has emerged as a pivotal capability of Large Language
Models(LLMs), revolutionizing development efficiency for programmers of all
skill levels. However, the complexity of data structures and algorithmic logic
often results in functional deficiencies and security vulnerabilities in
generated code, reducing it to a prototype requiring extensive manual
debugging. While Retrieval-Augmented Generation (RAG) can enhance correctness
and security by leveraging external code manuals, it simultaneously introduces
new attack surfaces.
  In this paper, we pioneer the exploration of attack surfaces in
Retrieval-Augmented Code Generation (RACG), focusing on malicious dependency
hijacking. We demonstrate how poisoned documentation containing hidden
malicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting
dual trust chains: LLM reliance on RAG and developers' blind trust in LLM
suggestions. To construct poisoned documents, we propose ImportSnare, a novel
attack framework employing two synergistic strategies: 1)Position-aware beam
search optimizes hidden ranking sequences to elevate poisoned documents in
retrieval results, and 2)Multilingual inductive suggestions generate
jailbreaking sequences to manipulate LLMs into recommending malicious
dependencies. Through extensive experiments across Python, Rust, and
JavaScript, ImportSnare achieves significant attack success rates (over 50% for
popular libraries such as matplotlib and seaborn) in general, and is also able
to succeed even when the poisoning ratio is as low as 0.01%, targeting both
custom and real-world malicious packages. Our findings reveal critical supply
chain risks in LLM-powered development, highlighting inadequate security
alignment for code generation tasks. To support future research, we will
release the multilingual benchmark suite and datasets. The project homepage is
https://importsnare.github.io.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [179] [TGLF-SINN: Deep Learning Surrogate Model for Accelerating Turbulent Transport Modeling in Fusion](https://arxiv.org/abs/2509.07024)
*Yadi Cao,Futian Zhang,Wesley Liu,Tom Neiser,Orso Meneghini,Lawson Fuller,Sterling Smith,Raffi Nazikian,Brian Sammuli,Rose Yu*

Main category: physics.plasm-ph

TL;DR: 提出TGLF - SINN方法，创新特征工程、正则化和主动学习，用更少数据实现更好性能，下游应用加速45倍。


<details>
  <summary>Details</summary>
Motivation: TGLF模型全设备模拟计算成本高，神经网络替代模型需大量训练数据且训练负担大。

Method: 提出TGLF - SINN，包括特征工程、物理引导正则化和贝叶斯主动学习。

Result: 离线时降低LRMSE 12.4%，用25%数据误差增加少，下游应用加速45倍且精度相当。

Conclusion: TGLF - SINN能用更少数据实现高效性能，在高保真模型代理训练中有潜力。

Abstract: The Trapped Gyro-Landau Fluid (TGLF) model provides fast, accurate
predictions of turbulent transport in tokamaks, but whole device simulations
requiring thousands of evaluations remain computationally expensive. Neural
network (NN) surrogates offer accelerated inference with fully differentiable
approximations that enable gradient-based coupling but typically require large
training datasets to capture transport flux variations across plasma
conditions, creating significant training burden and limiting applicability to
expensive gyrokinetic simulations. We propose \textbf{TGLF-SINN
(Spectra-Informed Neural Network)} with three key innovations: (1) principled
feature engineering that reduces target prediction range, simplifying the
learning task; (2) physics-guided regularization of transport spectra to
improve generalization under sparse data; and (3) Bayesian Active Learning
(BAL) to strategically select training samples based on model uncertainty,
reducing data requirements while maintaining accuracy. Our approach achieves
superior performance with significantly less training data. In offline
settings, TGLF-SINN reduces logarithmic root mean squared error (LRMSE) by 12.
4\% compared to the current baseline \base. Using only 25\% of the complete
dataset with BAL, we achieve LRMSE only 0.0165 higher than \base~and 0.0248
higher than our offline model (0.0583). In downstream flux matching
applications, our NN surrogate provides 45x speedup over TGLF while maintaining
comparable accuracy, demonstrating potential for training efficient surrogates
for higher-fidelity models where data acquisition is costly and sparse.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [180] [From Passive to Participatory: How Liberating Structures Can Revolutionize Our Conferences](https://arxiv.org/abs/2509.07046)
*Daniel Russo,Margaret-Anne Storey*

Main category: cs.CY

TL;DR: 会议面临投稿过多等危机，建议从被动展示转向互动参与模式，采用Liberating Structures技术，分双轨制办会，以提升质量。


<details>
  <summary>Details</summary>
Motivation: 解决会议投稿过多、评审负担重、缺乏有意义互动，以及质疑当前模式能否促进创新的问题。

Method: 倡导从被动论文展示转向互动参与模式，采用Liberating Structures技术，将会议分为产生新想法和讨论既有工作两个轨道。

Result: 未提及具体结果。

Conclusion: 采用这种改变能确保会议在AI时代仍是产生深刻见解、创造力和有效合作的场所。

Abstract: Our conferences face a growing crisis: an overwhelming flood of submissions,
increased reviewing burdens, and diminished opportunities for meaningful
engagement. With AI making paper generation easier than ever, we must ask
whether the current model fosters real innovation or simply incentivizes more
publications. This article advocates for a shift from passive paper
presentations to interactive, participatory formats. We propose Liberating
Structures, facilitation techniques that promote collaboration and deeper
intellectual exchange. By restructuring conferences into two tracks, one for
generating new ideas and another for discussing established work, we can
prioritize quality over quantity and reinvigorate academic gatherings.
Embracing this change will ensure conferences remain spaces for real insight,
creativity, and impactful collaboration in the AI era.

</details>


### [181] [ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code](https://arxiv.org/abs/2509.07006)
*Kapil Madan*

Main category: cs.CY

TL;DR: 本文介绍ArGen框架，用于使大语言模型符合复杂规则，通过多方法结合实现治理要求，以医疗AI助手为例展示其能力，有开源仓库。


<details>
  <summary>Details</summary>
Motivation: 让大语言模型符合包括伦理原则、安全协议和合规标准等在内的复杂规则集。

Method: 采用基于原则的自动奖励评分、Group Relative Policy Optimisation (GRPO) 和受Open Policy Agent (OPA) 启发的治理层相结合的方法。

Result: 以遵循达摩伦理原则的医疗AI助手为例，相比基线在领域范围遵守上提升了70.9%。

Conclusion: ArGen的方法为构建技术熟练、伦理可靠、可验证合规的'可治理AI'系统提供了途径，可在全球不同环境安全部署。

Abstract: This paper introduces ArGen (Auto-Regulation of Generative AI systems), a
framework for aligning Large Language Models (LLMs) with complex sets of
configurable, machine-readable rules spanning ethical principles, operational
safety protocols, and regulatory compliance standards. Moving beyond just
preference-based alignment, ArGen is designed to ensure LLMs adhere to these
multifaceted policies through a novel synthesis of principle-based automated
reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy
Agent (OPA) inspired governance layer. This approach provides the technical
foundation for achieving and demonstrating compliance with diverse and nuanced
governance requirements. To showcase the framework's capability to
operationalize a deeply nuanced and culturally-specific value system, we
present an in-depth case study: the development of a medical AI assistant
guided by principles from Dharmic ethics (such as Ahimsa and Dharma), as
derived from texts like the Bhagavad Gita. This challenging application
demonstrates ArGen's adaptability, achieving a 70.9% improvement in
domain-scope adherence over the baseline. Through our open-source repository,
we show that ArGen's methodology offers a path to 'Governable Al' systems that
are technically proficient, ethically robust, and verifiably compliant for safe
deployment in diverse global contexts.

</details>


### [182] [Preventing Another Tessa: Modular Safety Middleware For Health-Adjacent AI Assistants](https://arxiv.org/abs/2509.07022)
*Pavan Reddy,Nithin Reddy*

Main category: cs.CY

TL;DR: 本文以NEDA聊天机器人Tessa事件为案例，提出混合安全中间件防止不安全AI事件，合成评估显示其性能优，强调健康相关AI只需最后阶段检查和治理即可保障安全。


<details>
  <summary>Details</summary>
Motivation: 以Tessa提供有害减肥建议被停用事件为背景，探讨如何防止医疗场景中不安全AI事件发生。

Method: 提出结合确定性词法门和内联大语言模型策略过滤器的混合安全中间件，进行合成评估，并将Tessa失败模式与现有框架关联。

Result: 该设计能在不增加成本和延迟下完美拦截不安全提示，优于传统多阶段管道。

Conclusion: 健康相关AI无需重量级基础设施，最后阶段明确可测试检查结合治理和升级机制就能防止类似事件，保障实际部署可持续性。

Abstract: In 2023, the National Eating Disorders Association's (NEDA) chatbot Tessa was
suspended after providing harmful weight-loss advice to vulnerable users-an
avoidable failure that underscores the risks of unsafe AI in healthcare
contexts. This paper examines Tessa as a case study in absent safety
engineering and demonstrates how a lightweight, modular safeguard could have
prevented the incident. We propose a hybrid safety middleware that combines
deterministic lexical gates with an in-line large language model (LLM) policy
filter, enforcing fail-closed verdicts and escalation pathways within a single
model call. Using synthetic evaluations, we show that this design achieves
perfect interception of unsafe prompts at baseline cost and latency,
outperforming traditional multi-stage pipelines. Beyond technical remedies, we
map Tessa's failure patterns to established frameworks (OWASP LLM Top10, NIST
SP 800-53), connecting practical safeguards to actionable governance controls.
The results highlight that robust, auditable safety in health-adjacent AI does
not require heavyweight infrastructure: explicit, testable checks at the last
mile are sufficient to prevent "another Tessa", while governance and escalation
ensure sustainability in real-world deployment.

</details>


### [183] [The Impact of Artificial Intelligence on Traditional Art Forms: A Disruption or Enhancement](https://arxiv.org/abs/2509.07029)
*Viswa Chaitanya Marella,Sai Teja Erukude,Suhasnadh Reddy Veluru*

Main category: cs.CY

TL;DR: 本文探讨AI在传统艺术领域的双重影响，既带来积极作用，也存在负面影响，倡导制定伦理准则等，认为未来取决于人类对AI的治理。


<details>
  <summary>Details</summary>
Motivation: 探讨AI引入传统艺术领域后，是破坏还是提升传统艺术形式。

Method: 结合生成对抗网络、扩散模型等技术，用实例和数据说明AI对绘画、雕塑等艺术形式的影响。

Result: AI能使创意表达民主化、提高生产力和保护文化遗产，但也存在威胁艺术真实性、数据伦理等问题。

Conclusion: 应制定伦理准则、采用协作方法和包容性技术发展，让AI促进艺术进化而非替代艺术家灵魂。

Abstract: The introduction of Artificial Intelligence (AI) into the domains of
traditional art (visual arts, performing arts, and crafts) has sparked a
complicated discussion about whether this might be an agent of disruption or an
enhancement of our traditional art forms. This paper looks at the duality of
AI, exploring the ways that recent technologies like Generative Adversarial
Networks and Diffusion Models, and text-to-image generators are changing the
fields of painting, sculpture, calligraphy, dance, music, and the arts of
craft. Using examples and data, we illustrate the ways that AI can democratize
creative expression, improve productivity, and preserve cultural heritage,
while also examining the negative aspects, including: the threats to
authenticity within art, ethical concerns around data, and issues including
socio-economic factors such as job losses. While we argue for the
context-dependence of the impact of AI (the potential for creative
homogenization and the devaluation of human agency in artmaking), we also
illustrate the potential for hybrid practices featuring AI in cuisine, etc. We
advocate for the development of ethical guidelines, collaborative approaches,
and inclusive technology development. In sum, we are articulating a vision of
AI in which it amplifies our innate creativity while resisting the displacement
of the cultural, nuanced, and emotional aspects of traditional art. The future
will be determined by human choices about how to govern AI so that it becomes a
mechanism for artistic evolution and not a substitute for the artist's soul.

</details>


### [184] [A Maslow-Inspired Hierarchy of Engagement with AI Model](https://arxiv.org/abs/2509.07032)
*Madara Ogot*

Main category: cs.CY

TL;DR: 本文介绍受马斯洛需求层次理论启发的AI参与层次模型，通过案例验证其灵活性，为学者、从业者和政策制定者提供工具。


<details>
  <summary>Details</summary>
Motivation: 人工智能在各领域快速发展，急需强大框架来概念化和指导参与。

Method: 引入AI参与层次模型，用四个不同案例进行初步验证。

Result: 模型在不同部门具有情境灵活性。

Conclusion: AI成熟度进展是多维度的，需要技术能力、道德诚信、组织韧性和生态系统协作。

Abstract: The rapid proliferation of artificial intelligence (AI) across industry,
government, and education highlights the urgent need for robust frameworks to
conceptualise and guide engagement. This paper introduces the Hierarchy of
Engagement with AI model, a novel maturity framework inspired by Maslow's
hierarchy of needs. The model conceptualises AI adoption as a progression
through eight levels, beginning with initial exposure and basic understanding
and culminating in ecosystem collaboration and societal impact. Each level
integrates technical, organisational, and ethical dimensions, emphasising that
AI maturity is not only a matter of infrastructure and capability but also of
trust, governance, and responsibility. Initial validation of the model using
four diverse case studies (General Motors, the Government of Estonia, the
University of Texas System, and the African Union AI Strategy) demonstrate the
model's contextual flexibility across various sectors. The model provides
scholars with a framework for analysing AI maturity and offers practitioners
and policymakers a diagnostic and strategic planning tool to guide responsible
and sustainable AI engagement. The proposed model demonstrates that AI maturity
progression is multi-dimensional, requiring technological capability, ethical
integrity, organisational resilience, and ecosystem collaboration.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [185] [Association of Timing and Duration of Moderate-to-Vigorous Physical Activity with Cognitive Function and Brain Aging: A Population-Based Study Using the UK Biobank](https://arxiv.org/abs/2509.06969)
*Wasif Khan,Lin Gu,Noah Hammarlund,Lei Xing,Joshua K. Wong,Ruogu Fang*

Main category: q-bio.NC

TL;DR: 研究分析45892名60岁以上老人数据，发现更高强度的中等到剧烈体育活动（MVPA）与更好认知功能和保存的大脑结构相关，公共卫生策略增加MVPA或助健康认知老化。


<details>
  <summary>Details</summary>
Motivation: 目前MVPA强度、时间与认知功能和特定脑区结构的关联尚不明确，需进一步研究。

Method: 分析英国生物银行45892名60岁以上参与者数据，用多变量线性模型评估MVPA与认知表现和脑区体积的关联，进行二次分析和敏感性分析。

Result: 更高MVPA与各认知领域更好表现相关，与皮质下脑区和特定脑区灰质体积有关；一天中任何时间的MVPA都与认知功能和脑体积相关，尤其在中午 - 下午和晚上；各亚组结果一致，有剂量反应关系。

Conclusion: 更高MVPA与晚年保存的大脑结构和增强的认知功能相关，公共卫生策略增加MVPA可支持健康认知老化并带来经济收益。

Abstract: Physical activity is a modifiable lifestyle factor with potential to support
cognitive resilience. However, the association of moderate-to-vigorous physical
activity (MVPA) intensity, and timing, with cognitive function and
region-specific brain structure remain poorly understood. We analyzed data from
45,892 UK Biobank participants aged 60 years and older with valid wrist-worn
accelerometer data, cognitive testing, and structural brain MRI. MVPA was
measured both continuously (mins per week) and categorically (thresholded using
>=150 min/week based on WHO guidelines). Associations with cognitive
performance and regional brain volumes were evaluated using multivariable
linear models adjusted for demographic, socioeconomic, and health-related
covariates. We conducted secondary analyses on MVPA timing and subgroup
effects. Higher MVPA was associated with better performance across cognitive
domains, including reasoning, memory, executive function, and processing speed.
These associations persisted in fully adjusted models and were higher among
participants meeting WHO guidelines. Greater MVPA was also associated with
subcortical brain regions (caudate, putamen, pallidum, thalamus), as well as
regional gray matter volumes involved in emotion, working memory, and
perceptual processing. Secondary analyses showed that MVPA at any time of day
was associated with cognitive functions and brain volume particularly in the
midday-afternoon and evening. Sensitivity analysis shows consistent findings
across subgroups, with evidence of dose-response relationships. Higher MVPA is
associated with preserved brain structure and enhanced cognitive function in
later life. Public health strategies to increase MVPA may support healthy
cognitive aging and generate substantial economic benefits, with global gains
projected to reach USD 760 billion annually by 2050.

</details>


### [186] [Impact of Neuron Models on Spiking Neural Networks performance. A Complexity Based Classification Approach](https://arxiv.org/abs/2509.06970)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: q-bio.NC

TL;DR: 研究神经元模型和学习规则对脉冲神经网络（SNNs）分类性能的影响，引入基于复杂度的评估机制并进行系统分析。


<details>
  <summary>Details</summary>
Motivation: 探索神经元模型和学习规则选择对SNNs在生物信号处理中分类性能的影响。

Method: 比较多种生物启发的神经元模型和学习规则，将基于复杂度的决策机制融入评估流程，使用合成数据集研究神经动力学和评估算法性能。

Result: 验证合成数据揭示了性能趋势，分类准确率取决于神经元模型、网络规模和学习规则的相互作用，基于LZC的评估突出了对弱或噪声信号稳健的配置。

Conclusion: 提供了神经元模型选择与网络参数和学习策略相互作用的系统分析，基于复杂度的评估方法为SNN性能提供了一致的基准。

Abstract: This study explores how the selection of neuron models and learning rules
impacts the classification performance of Spiking Neural Networks (SNNs), with
a focus on applications in bio-signal processing. We compare biologically
inspired neuron models, including Leaky Integrate-and-Fire (LIF), metaneurons,
and probabilistic Levy-Baxter (LB) neurons, across multiple learning rules,
including spike-timing-dependent plasticity (STDP), tempotron, and
reward-modulated updates. A novel element of this work is the integration of a
complexity-based decision mechanism into the evaluation pipeline. Using
Lempel-Ziv Complexity (LZC), a measure related to entropy rate, we quantify the
structural regularity of spike trains and assess classification outcomes in a
consistent and interpretable manner across different SNN configurations. To
investigate neural dynamics and assess algorithm performance, we employed
synthetic datasets with varying temporal dependencies and stochasticity levels.
These included Markov and Poisson processes, well-established models to
simulate neuronal spike trains and capture the stochastic firing behavior of
biological neurons.Validation of synthetic Poisson and Markov-modeled data
reveals clear performance trends: classification accuracy depends on the
interaction between neuron model, network size, and learning rule, with the
LZC-based evaluation highlighting configurations that remain robust to weak or
noisy signals. This work delivers a systematic analysis of how neuron model
selection interacts with network parameters and learning strategies, supported
by a novel complexity-based evaluation approach that offers a consistent
benchmark for SNN performance.

</details>


### [187] [Computational Concept of the Psyche](https://arxiv.org/abs/2509.07009)
*Anton Kolonin,Vladimir Kryukov*

Main category: q-bio.NC

TL;DR: 文章从构建人工智能角度综述人类心理建模方法，提出认知架构概念并进行计算形式化，还给出模型的最小实验实现。


<details>
  <summary>Details</summary>
Motivation: 从构建人工智能的视角，对人类心理进行建模。

Method: 提出认知架构概念，将心理视为主体操作系统，在此基础上进行计算形式化，通过在需求空间中从经验学习创建人工智能系统。

Result: 形式化了在不确定条件下，在特定主体需求空间中构建通用人工智能以做出最优决策的问题，同时给出模型的最小实验实现。

Conclusion: 基于对人类心理建模的研究，能为构建通用人工智能提供有效方法和思路。

Abstract: The article provides an overview of approaches to modeling the human psyche
in the perspective of building an artificial one. Based on the review, a
concept of cognitive architecture is proposed, where the psyche is considered
as an operating system of a living or artificial subject, including a space of
needs that determines its life meanings in connection with stimuli from the
external world, and intelligence as a decision-making system for actions in
relation to this world in order to satisfy these needs. Based on the concept, a
computational formalization is proposed for creating artificial intelligence
systems through learning from experience in the space of a space of needs,
taking into account their biological or existential significance for an
intelligent agent. Thus, the problem of building general artificial
intelligence as a system for making optimal decisions in the space of
agent-specific needs under conditions of uncertainty is formalized, with
maximization of success in achieving goals, minimization of existential risks
and maximization of energy efficiency. A minimal experimental implementation of
the model is also provided.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [188] [HYLU: Hybrid Parallel Sparse LU Factorization](https://arxiv.org/abs/2509.07690)
*Xiaoming Chen*

Main category: cs.AR

TL;DR: 介绍了用于多核共享内存架构求解稀疏线性系统的通用求解器HYLU，测试显示其性能优于Intel MKL PARDISO，可从指定链接下载。


<details>
  <summary>Details</summary>
Motivation: 设计高效求解稀疏线性系统(Ax=b)的通用求解器。

Method: 集成混合数值内核，使求解器适应系数矩阵的各种稀疏模式。

Result: 对34个稀疏矩阵的测试表明，HYLU在数值分解阶段，单次求解时比Intel MKL PARDISO快1.74倍，重复求解时快2.26倍。

Conclusion: HYLU是一个在多核共享内存架构上求解稀疏线性系统的有效通用求解器。

Abstract: This article introduces HYLU, a hybrid parallel LU factorization-based
general-purpose solver designed for efficiently solving sparse linear systems
(Ax=b) on multi-core shared-memory architectures. The key technical feature of
HYLU is the integration of hybrid numerical kernels so that it can adapt to
various sparsity patterns of coefficient matrices. Tests on 34 sparse matrices
from SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL
PARDISO in the numerical factorization phase by geometric means of 1.74X (for
one-time solving) and 2.26X (for repeated solving). HYLU can be downloaded from
https://github.com/chenxm1986/hylu.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [189] [Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions](https://arxiv.org/abs/2509.07445)
*Harrison Field,Max Yang,Yijiong Lin,Efi Psomopoulou,David Barton,Nathan F. Lepora*

Main category: cs.RO

TL;DR: 提出Text2Touch，将大语言模型设计的奖励应用于多轴手中物体旋转任务，在真实触觉传感下表现优于人工设计基线，加速了灵巧触觉技能部署。


<details>
  <summary>Details</summary>
Motivation: 此前工作未考虑触觉传感对类人灵巧性的重要性，需将大语言模型奖励设计应用到有触觉传感的灵巧操作中。

Method: 采用提示工程策略处理超70个环境变量，利用仿真到现实的蒸馏方法实现策略转移到有触觉的四指灵巧机器人手。

Result: Text2Touch显著优于精心调整的人工设计基线，旋转速度和稳定性更好，奖励函数更短更简单。

Conclusion: 大语言模型设计的奖励可显著减少从概念到可部署的灵巧触觉技能的时间，支持更快速和可扩展的多模态机器人学习。

Abstract: Large language models (LLMs) are beginning to automate reward design for
dexterous manipulation. However, no prior work has considered tactile sensing,
which is known to be critical for human-like dexterity. We present Text2Touch,
bringing LLM-crafted rewards to the challenging task of multi-axis in-hand
object rotation with real-world vision based tactile sensing in palm-up and
palm-down configurations. Our prompt engineering strategy scales to over 70
environment variables, and sim-to-real distillation enables successful policy
transfer to a tactile-enabled fully actuated four-fingered dexterous robot
hand. Text2Touch significantly outperforms a carefully tuned human-engineered
baseline, demonstrating superior rotation speed and stability while relying on
reward functions that are an order of magnitude shorter and simpler. These
results illustrate how LLM-designed rewards can significantly reduce the time
from concept to deployable dexterous tactile skills, supporting more rapid and
scalable multimodal robot learning. Project website:
https://hpfield.github.io/text2touch-website

</details>


### [190] [DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis](https://arxiv.org/abs/2509.07463)
*Sven Kirchner,Nils Purschke,Ross Greer,Alois C. Knoll*

Main category: cs.RO

TL;DR: 提出DepthVision框架解决视觉输入不足时的机器人可靠操作问题，结合合成与真实RGB数据，实验显示在低光下有性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决视觉输入退化或不足时机器人可靠操作的挑战。

Method: 用条件生成对抗网络从稀疏LiDAR点云合成RGB图像，通过LAMA结合合成与真实RGB数据。

Result: 在低光条件下提升性能，比仅用RGB的基线有显著提升，且与冻结的VLMs兼容。

Conclusion: LiDAR引导的RGB合成在现实环境中实现鲁棒机器人操作有潜力。

Abstract: Ensuring reliable robot operation when visual input is degraded or
insufficient remains a central challenge in robotics. This letter introduces
DepthVision, a framework for multimodal scene understanding designed to address
this problem. Unlike existing Vision-Language Models (VLMs), which use only
camera-based visual input alongside language, DepthVision synthesizes RGB
images from sparse LiDAR point clouds using a conditional generative
adversarial network (GAN) with an integrated refiner network. These synthetic
views are then combined with real RGB data using a Luminance-Aware Modality
Adaptation (LAMA), which blends the two types of data dynamically based on
ambient lighting conditions. This approach compensates for sensor degradation,
such as darkness or motion blur, without requiring any fine-tuning of
downstream vision-language models. We evaluate DepthVision on real and
simulated datasets across various models and tasks, with particular attention
to safety-critical tasks. The results demonstrate that our approach improves
performance in low-light conditions, achieving substantial gains over RGB-only
baselines while preserving compatibility with frozen VLMs. This work highlights
the potential of LiDAR-guided RGB synthesis for achieving robust robot
operation in real-world environments.

</details>


### [191] [Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion Control?](https://arxiv.org/abs/2509.07593)
*Gavin Tao,Yinuo Wang,Jinzhao Zhou*

Main category: cs.RO

TL;DR: 提出基于SSD - Mamba2的视觉驱动跨模态RL框架用于运动控制，在多种场景表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有运动控制方法存在盲目、计算内存权衡不佳、长程信用分配困难和成本高等问题，需更好的方法。

Method: 构建基于SSD - Mamba2的框架，将本体感受状态和外部感受观测编码成紧凑令牌并融合，在课程设置下进行端到端训练，使用紧凑的以状态为中心的奖励。

Result: 在不同运动控制场景中，在回报、安全性和样本效率上超过现有基线，且在相同计算预算下收敛更快。

Conclusion: SSD - Mamba2为可扩展、有远见且高效的端到端运动控制提供了实用的融合骨干。

Abstract: End-to-end reinforcement learning for motion control promises unified
perception-action policies that scale across embodiments and tasks, yet most
deployed controllers are either blind (proprioception-only) or rely on fusion
backbones with unfavorable compute-memory trade-offs. Recurrent controllers
struggle with long-horizon credit assignment, and Transformer-based fusion
incurs quadratic cost in token length, limiting temporal and spatial context.
We present a vision-driven cross-modal RL framework built on SSD-Mamba2, a
selective state-space backbone that applies state-space duality (SSD) to enable
both recurrent and convolutional scanning with hardware-aware streaming and
near-linear scaling. Proprioceptive states and exteroceptive observations
(e.g., depth tokens) are encoded into compact tokens and fused by stacked
SSD-Mamba2 layers. The selective state-space updates retain long-range
dependencies with markedly lower latency and memory use than quadratic
self-attention, enabling longer look-ahead, higher token resolution, and stable
training under limited compute. Policies are trained end-to-end under curricula
that randomize terrain and appearance and progressively increase scene
complexity. A compact, state-centric reward balances task progress, energy
efficiency, and safety. Across diverse motion-control scenarios, our approach
consistently surpasses strong state-of-the-art baselines in return, safety
(collisions and falls), and sample efficiency, while converging faster at the
same compute budget. These results suggest that SSD-Mamba2 provides a practical
fusion backbone for scalable, foresightful, and efficient end-to-end motion
control.

</details>


### [192] [RaC: Robot Learning for Long-Horizon Tasks by Scaling Recovery and Correction](https://arxiv.org/abs/2509.07953)
*Zheyuan Hu,Robyn Wu,Naveen Enock,Jasmine Li,Riya Kadakia,Zackory Erickson,Aviral Kumar*

Main category: cs.RO

TL;DR: 介绍RaC方法，在模仿学习预训练后对人工干预轨迹微调策略，在多任务上用更少数据超现有方法且支持测试时扩展。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类遥操作的“专家”数据收集程序效率低，导致机器人在接触丰富、可变形物体和长视野任务上性能难以提升。

Method: 提出RaC方法，在模仿学习预训练后，基于人类干预轨迹微调机器人策略，人类在策略执行可能失败时介入，回退机器人状态并提供纠正片段。

Result: 在三个现实双手控制任务和一个模拟装配任务中，RaC用少10倍的数据收集时间和样本数超越现有方法，且策略性能随恢复操作数量线性提升。

Conclusion: 训练数据中加入重试和适应行为对提升长视野任务效率和鲁棒性至关重要，RaC有效且支持测试时扩展。

Abstract: Modern paradigms for robot imitation train expressive policy architectures on
large amounts of human demonstration data. Yet performance on contact-rich,
deformable-object, and long-horizon tasks plateau far below perfect execution,
even with thousands of expert demonstrations. This is due to the inefficiency
of existing ``expert'' data collection procedures based on human teleoperation.
To address this issue, we introduce RaC, a new phase of training on
human-in-the-loop rollouts after imitation learning pre-training. In RaC, we
fine-tune a robotic policy on human intervention trajectories that illustrate
recovery and correction behaviors. Specifically, during a policy rollout, human
operators intervene when failure appears imminent, first rewinding the robot
back to a familiar, in-distribution state and then providing a corrective
segment that completes the current sub-task. Training on this data composition
expands the robotic skill repertoire to include retry and adaptation behaviors,
which we show are crucial for boosting both efficiency and robustness on
long-horizon tasks. Across three real-world bimanual control tasks: shirt
hanging, airtight container lid sealing, takeout box packing, and a simulated
assembly task, RaC outperforms the prior state-of-the-art using 10$\times$ less
data collection time and samples. We also show that RaC enables test-time
scaling: the performance of the trained RaC policy scales linearly in the
number of recovery maneuvers it exhibits. Videos of the learned policy are
available at https://rac-scaling-robot.github.io/.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [193] [dciWebMapper2: Enhancing the dciWebMapper framework toward integrated, interactive visualization of linked multi-type maps, charts, and spatial statistics and analysis](https://arxiv.org/abs/2509.07897)
*Sarigai Sarigai,Liping Yang,Katie Slack,Carolyn Fish,Michaela Buenemann,Qiusheng Wu,Yan Lin,Joseph A. Cook,David Jacobs*

Main category: cs.HC

TL;DR: 本文介绍dciWebMapper2，它是原框架的扩展，集成多种地图类型与统计图表，具灵活性与开放性，通过三个用例展示其适用性，为多领域提供基础。


<details>
  <summary>Details</summary>
Motivation: 交互式基于网络的地理可视化在各学科愈发重要，需要支持动态、多属性空间分析和易访问设计的开源框架。

Method: 扩展原dciWebMapper框架，集成多种地图类型、统计图表和时间滑块，采用下拉式控件，遵循制图和信息可视化原则，且开源、独立、无服务器。

Result: 通过三个应用用例证明了dciWebMapper2的适应性和使交互式网络制图大众化的潜力。

Conclusion: 该框架为研究、教育和公民参与中的包容性空间叙事和透明地理空间分析提供了通用基础。

Abstract: As interactive web-based geovisualization becomes increasingly vital across
disciplines, there is a growing need for open-source frameworks that support
dynamic, multi-attribute spatial analysis and accessible design. This paper
introduces dciWebMapper2, a significant expansion of the original dciWebMapper
framework, designed to enable exploratory analysis across domains such as
climate justice, food access, and social vulnerability. The enhanced framework
integrates multiple map types, including choropleth, proportional symbol, small
multiples, and heatmaps, with linked statistical charts (e.g., scatter plots,
boxplots) and time sliders, all within a coordinated-view environment.
Dropdown-based controls allow flexible, high-dimensional comparisons while
maintaining visual clarity. Grounded in cartographic and information
visualization principles, dciWebMapper2 is fully open-source, self-contained,
and server-free, supporting modularity, reproducibility, and long-term
sustainability. Three applied use cases demonstrate its adaptability and
potential to democratize interactive web cartography. This work offers a
versatile foundation for inclusive spatial storytelling and transparent
geospatial analysis in research, education, and civic engagement.

</details>


### [194] [Enhancing Online Learning by Integrating Biosensors and Multimodal Learning Analytics for Detecting and Predicting Student Behavior: A Review](https://arxiv.org/abs/2509.07742)
*Alvaro Becerra,Ruth Cobos,Charles Lang*

Main category: cs.HC

TL;DR: 本文系统综述了生物传感器和多模态学习分析在计算机学习中分析和预测学生行为的应用，分析相关挑战、方法、趋势等，指出整合多模态数据可推动更定制化的在线学习。


<details>
  <summary>Details</summary>
Motivation: 在现代在线学习中，理解和预测学生行为对提高参与度和优化教育成果至关重要，因此探索生物传感器和多模态学习分析的整合。

Method: 综合分析54项关键研究，研究常用方法如先进机器学习算法和多模态数据预处理技术。

Result: 识别了该领域的当前研究趋势、局限性和新兴方向，强调了生物传感器驱动的自适应学习系统的变革潜力。

Conclusion: 整合多模态数据可促进个性化学习体验、实时反馈和智能教育干预，推动更定制化和自适应的在线学习。

Abstract: In modern online learning, understanding and predicting student behavior is
crucial for enhancing engagement and optimizing educational outcomes. This
systematic review explores the integration of biosensors and Multimodal
Learning Analytics (MmLA) to analyze and predict student behavior during
computer-based learning sessions. We examine key challenges, including emotion
and attention detection, behavioral analysis, experimental design, and
demographic considerations in data collection. Our study highlights the growing
role of physiological signals, such as heart rate, brain activity, and
eye-tracking, combined with traditional interaction data and self-reports to
gain deeper insights into cognitive states and engagement levels. We synthesize
findings from 54 key studies, analyzing commonly used methodologies such as
advanced machine learning algorithms and multimodal data pre-processing
techniques. The review identifies current research trends, limitations, and
emerging directions in the field, emphasizing the transformative potential of
biosensor-driven adaptive learning systems. Our findings suggest that
integrating multimodal data can facilitate personalized learning experiences,
real-time feedback, and intelligent educational interventions, ultimately
advancing toward a more customized and adaptive online learning experience.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [195] [A transformer-based generative model for planetary systems](https://arxiv.org/abs/2509.07226)
*Yann Alibert,Jeanne Davoult,Sara Marques*

Main category: astro-ph.EP

TL;DR: 本文开发基于transformer架构的生成模型，以低成本生成大量合成行星系统，验证模型有效性并以TOI - 469系统为例展示预测未观测行星属性的能力，还公开了模型。


<details>
  <summary>Details</summary>
Motivation: 数值计算行星系统形成对计算能力要求高，开发能捕捉同系统行星间相关性和统计关系的生成模型，以低成本生成合成行星系统指导观测活动。

Method: 基于transformer架构构建生成模型，通过视觉、统计比较和机器学习驱动测试评估模型有效性，以TOI - 469系统为用例进行预测。

Result: 模型生成系统的属性与Bern模型计算结果相似，能基于已观测行星属性预测未观测行星属性。

Conclusion: 所开发的生成模型有效，可用于预测未观测行星属性，模型已公开。

Abstract: Numerical calculations of planetary system formation are very demanding in
terms of computing power. These synthetic planetary systems can however provide
access to correlations, as predicted in a given numerical framework, between
the properties of planets in the same system. Such correlations can, in return,
be used in order to guide and prioritize observational campaigns aiming at
discovering some types of planets, as Earth-like planets. Our goal is to
develop a generative model which is capable of capturing correlations and
statistical relationships between planets in the same system. Such a model,
trained on the Bern model, offers the possibility to generate large number of
synthetic planetary systems with little computational cost, that can be used,
for example, to guide observational campaigns. Our generative model is based on
the transformer architecture which is well-known to efficiently capture
correlations in sequences and is at the basis of all modern Large Language
Models. To assess the validity of the generative model, we perform visual and
statistical comparisons, as well as a machine learning driven tests. Finally,
as a use case example, we consider the TOI-469 system, in which we aim at
predicting the possible properties of planets c and d, based on the properties
of planet b (the first that has been detected). We show using different
comparison methods that the properties of systems generated by our model are
very similar to the ones of the systems computed directly by the Bern model. We
also show in the case of the TOI-469 system, that using the generative model
allows to predict the properties of planets not yet observed, based on the
properties of the already observed planet. We provide our model to the
community on our website www.ai4exoplanets.com.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [196] [Expected Signature Kernels for Lévy Rough Paths](https://arxiv.org/abs/2509.07893)
*Peter K. Friz,Paul P. Hager*

Main category: math.PR

TL;DR: 本文基于已有研究，推导了非齐次 Lévy 过程期望签名的 PDE 系统，以高斯鞅为例，其期望签名核满足 Goursat PDE。


<details>
  <summary>Details</summary>
Motivation: 计算已知随机过程类的期望签名核是重要问题，可降低计算成本。

Method: 基于非齐次 Lévy 过程期望签名的表示，将光滑粗糙路径的论证扩展到非齐次 Lévy 过程。

Result: 得到非齐次 Lévy 过程期望签名的 PDE 系统，高斯鞅的期望签名核满足 Goursat PDE。

Conclusion: 成功推导出非齐次 Lévy 过程期望签名的相关 PDE 系统。

Abstract: The expected signature kernel arises in statistical learning tasks as a
similarity measure of probability measures on path space. Computing this kernel
for known classes of stochastic processes is an important problem that, in
particular, can help reduce computational costs. Building on the representation
of the expected signature of (inhomogeneous) L\'evy processes with absolutely
continuous characteristics as the development of an absolutely continuous path
in the extended tensor algebra [F.-H.-Tapia, Forum of Mathematics: Sigma
(2022), "Unified signature cumulants and generalized Magnus expansions"], we
extend the arguments developed for smooth rough paths in
[Lemercier-Lyons-Salvi, "Log-PDE Methods for Rough Signature Kernels"] to
derive a PDE system for the expected signature of inhomogeneous L\'evy
processes. As a specific example, we see that the expected signature kernel of
Gaussian martingales satisfies a Goursat PDE.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [197] [SVGauge: Towards Human-Aligned Evaluation for SVG Generation](https://arxiv.org/abs/2509.07127)
*Leonardo Zini,Elia Frigieri,Sebastiano Aloscari,Marcello Generali,Lorenzo Dodi,Robert Dosen,Lorenzo Baraldi*

Main category: cs.GR

TL;DR: 提出针对文本到SVG生成的首个人类对齐、基于参考的度量指标SVGauge，评估显示其优于现有指标。


<details>
  <summary>Details</summary>
Motivation: 现有度量指标无法满足生成的可缩放矢量图形（SVG）图像基于其符号和矢量性质的评估需求。

Method: SVGauge联合测量视觉保真度（提取SigLIP图像嵌入，用PCA和白化进行域对齐）和语义一致性（在SBERT和TF - IDF组合空间比较BLIP - 2生成的SVG字幕与原始提示）。

Result: 在SHE基准测试中，SVGauge与人类判断的相关性最高，能更真实地重现八个零样本大语言模型生成器的系统级排名。

Conclusion: 强调了特定矢量评估的必要性，为未来文本到SVG生成模型的基准测试提供实用工具。

Abstract: Generated Scalable Vector Graphics (SVG) images demand evaluation criteria
tuned to their symbolic and vectorial nature: criteria that existing metrics
such as FID, LPIPS, or CLIPScore fail to satisfy. In this paper, we introduce
SVGauge, the first human-aligned, reference based metric for text-to-SVG
generation. SVGauge jointly measures (i) visual fidelity, obtained by
extracting SigLIP image embeddings and refining them with PCA and whitening for
domain alignment, and (ii) semantic consistency, captured by comparing
BLIP-2-generated captions of the SVGs against the original prompts in the
combined space of SBERT and TF-IDF. Evaluation on the proposed SHE benchmark
shows that SVGauge attains the highest correlation with human judgments and
reproduces system-level rankings of eight zero-shot LLM-based generators more
faithfully than existing metrics. Our results highlight the necessity of
vector-specific evaluation and provide a practical tool for benchmarking future
text-to-SVG generation models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [198] [Quantum Computing for Large-scale Network Optimization: Opportunities and Challenges](https://arxiv.org/abs/2509.07773)
*Sebastian Macaluso,Giovanni Geraci,Elías F. Combarro,Sergi Abadal,Ioannis Arapakis,Sofia Vallecorsa,Eduard Alarcón*

Main category: cs.NI

TL;DR: 本文提出利用量子计算解决未来移动网络关键问题的愿景，给出统一策略和优化方法，并讨论面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 大规模6G及未来网络复杂，传统多目标优化方法在大搜索空间中难处理，量子计算有高效优化潜力。

Method: 分析识别问题共同特征，以图为中心表示，提出涉及量子退火和量子强化学习的统一策略。

Result: 未提及具体结果。

Conclusion: 讨论了量子计算算法和硬件在有效优化未来网络时需克服的主要挑战。

Abstract: The complexity of large-scale 6G-and-beyond networks demands innovative
approaches for multi-objective optimization over vast search spaces, a task
often intractable. Quantum computing (QC) emerges as a promising technology for
efficient large-scale optimization. We present our vision of leveraging QC to
tackle key classes of problems in future mobile networks. By analyzing and
identifying common features, particularly their graph-centric representation,
we propose a unified strategy involving QC algorithms. Specifically, we outline
a methodology for optimization using quantum annealing as well as quantum
reinforcement learning. Additionally, we discuss the main challenges that QC
algorithms and hardware must overcome to effectively optimize future networks.

</details>


<div id='math.AG'></div>

# math.AG [[Back]](#toc)

### [199] [Toric geometry of ReLU neural networks](https://arxiv.org/abs/2509.05894)
*Yaoying Fu*

Main category: math.AG

TL;DR: 本文建立了环面几何与ReLU神经网络的联系，利用代数几何工具研究神经网络，揭示了热带几何与环面几何的联系，并给出无偏浅ReLU神经网络可实现函数的充要条件。


<details>
  <summary>Details</summary>
Motivation: 解决给定前馈ReLU神经网络架构下，连续有限分段线性函数的精确函数实现问题，开发系统的解决方法。

Method: 建立环面几何与ReLU神经网络的联系，定义ReLU扇、ReLU环面簇和ReLU卡蒂尔除子，利用代数几何结构和工具进行研究。

Result: 揭示了热带几何与ReLU神经网络环面几何的联系，通过计算ReLU卡蒂尔除子与环面不变曲线的相交数，证明了无偏浅ReLU神经网络可实现函数的充要条件。

Conclusion: 环面几何框架可用于研究ReLU神经网络的函数实现问题，为解决相关问题提供了新的方法和工具。

Abstract: Given a continuous finitely piecewise linear function $f:\mathbb{R}^{n_0} \to
\mathbb{R}$ and a fixed architecture $(n_0,\ldots,n_k;1)$ of feedforward ReLU
neural networks, the exact function realization problem is to determine when
some network with the given architecture realizes $f$. To develop a systematic
way to answer these questions, we establish a connection between toric geometry
and ReLU neural networks. This approach enables us to utilize numerous
structures and tools from algebraic geometry to study ReLU neural networks.
Starting with an unbiased ReLU neural network with rational weights, we define
the ReLU fan, the ReLU toric variety, and the ReLU Cartier divisor associated
with the network. This work also reveals the connection between the tropical
geometry and the toric geometry of ReLU neural networks. As an application of
the toric geometry framework, we prove a necessary and sufficient criterion of
functions realizable by unbiased shallow ReLU neural networks by computing
intersection numbers of the ReLU Cartier divisor and torus-invariant curves.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [200] [ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval](https://arxiv.org/abs/2509.07512)
*Zihan Chen,Lei Shi,Weize Wu,Qiji Zhou,Yue Zhang*

Main category: cs.CL

TL;DR: 提出ALLabel框架用于实体识别，能以少量标注数据达全量标注效果，实验证明其有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前主流实体识别大语言模型微调成本高，需找到性能 - 成本最佳权衡方法。

Method: 提出ALLabel三阶段框架，通过三种主动学习策略选择最具信息和代表性样本构建检索语料用于大语言模型上下文学习。

Result: 在三个专业领域数据集上，ALLabel在相同标注预算下优于所有基线；仅标注5%-10%数据集可达到全量标注方法的性能。

Conclusion: ALLabel框架有效且具有泛化性。

Abstract: Many contemporary data-driven research efforts in the natural sciences, such
as chemistry and materials science, require large-scale, high-performance
entity recognition from scientific datasets. Large language models (LLMs) have
increasingly been adopted to solve the entity recognition task, with the same
trend being observed on all-spectrum NLP tasks. The prevailing entity
recognition LLMs rely on fine-tuned technology, yet the fine-tuning process
often incurs significant cost. To achieve a best performance-cost trade-off, we
propose ALLabel, a three-stage framework designed to select the most
informative and representative samples in preparing the demonstrations for LLM
modeling. The annotated examples are used to construct a ground-truth retrieval
corpus for LLM in-context learning. By sequentially employing three distinct
active learning strategies, ALLabel consistently outperforms all baselines
under the same annotation budget across three specialized domain datasets.
Experimental results also demonstrate that selectively annotating only 5\%-10\%
of the dataset with ALLabel can achieve performance comparable to the method
annotating the entire dataset. Further analyses and ablation studies verify the
effectiveness and generalizability of our proposal.

</details>


### [201] [MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval](https://arxiv.org/abs/2509.07666)
*Xixi Wu,Yanchao Tan,Nan Hou,Ruiyang Zhang,Hong Cheng*

Main category: cs.CL

TL;DR: 本文提出MoLoRAG用于多模态、多页文档理解，结合语义和逻辑相关性检索，在四个数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统方法处理文档问答时存在丢失多模态信息、输入大小受限、忽略页面逻辑连接等问题，需要改进。

Method: 提出逻辑感知的检索框架MoLoRAG，构建页面图捕获上下文关系，用轻量级VLM进行图遍历检索相关页面，有训练和无训练两个版本。

Result: 在四个DocQA数据集上，相比LVLM直接推理准确率平均提高9.68%，相比基线检索精度提高7.44%。

Conclusion: MoLoRAG能有效结合语义和逻辑相关性进行多模态、多页文档理解，提升检索和问答效果。

Abstract: Document Understanding is a foundational AI capability with broad
applications, and Document Question Answering (DocQA) is a key evaluation task.
Traditional methods convert the document into text for processing by Large
Language Models (LLMs), but this process strips away critical multi-modal
information like figures. While Large Vision-Language Models (LVLMs) address
this limitation, their constrained input size makes multi-page document
comprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate
this by selecting relevant pages, but they rely solely on semantic relevance,
ignoring logical connections between pages and the query, which is essential
for reasoning.
  To this end, we propose MoLoRAG, a logic-aware retrieval framework for
multi-modal, multi-page document understanding. By constructing a page graph
that captures contextual relationships between pages, a lightweight VLM
performs graph traversal to retrieve relevant pages, including those with
logical connections often overlooked. This approach combines semantic and
logical relevance to deliver more accurate retrieval. After retrieval, the
top-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance
flexibility, MoLoRAG offers two variants: a training-free solution for easy
deployment and a fine-tuned version to improve logical relevance checking.
Experiments on four DocQA datasets demonstrate average improvements of 9.68% in
accuracy over LVLM direct inference and 7.44% in retrieval precision over
baselines. Codes and datasets are released at
https://github.com/WxxShirley/MoLoRAG.

</details>


### [202] [SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP](https://arxiv.org/abs/2509.07801)
*Decheng Duan,Yingyi Zhang,Jitong Peng,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 提出自然语言处理领域全文实体和关系提取基准数据集SciNLP，经实验验证其有效性，用其训练模型构建知识图谱并公开数据集。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多聚焦特定出版章节，为解决科学文本标注成本高和领域复杂问题，需要能用于全文实体和关系提取的数据集。

Method: 引入包含60篇手动标注的NLP全文出版物的SciNLP数据集，与相似数据集进行对比实验，评估现有监督模型性能。

Result: 现有模型在不同长度学术文本上提取能力有差异，SciNLP在某些基线模型上性能显著提升，构建的知识图谱节点平均度为3.2。

Conclusion: SciNLP是NLP领域首个提供全文实体及关系标注的数据集，能提升下游应用效果，且数据集公开可促进研究。

Abstract: Structured information extraction from scientific literature is crucial for
capturing core concepts and emerging trends in specialized fields. While
existing datasets aid model development, most focus on specific publication
sections due to domain complexity and the high cost of annotating scientific
texts. To address this limitation, we introduce SciNLP - a specialized
benchmark for full-text entity and relation extraction in the Natural Language
Processing (NLP) domain. The dataset comprises 60 manually annotated full-text
NLP publications, covering 7,072 entities and 1,826 relations. Compared to
existing research, SciNLP is the first dataset providing full-text annotations
of entities and their relationships in the NLP domain. To validate the
effectiveness of SciNLP, we conducted comparative experiments with similar
datasets and evaluated the performance of state-of-the-art supervised models on
this dataset. Results reveal varying extraction capabilities of existing models
across academic texts of different lengths. Cross-comparisons with existing
datasets show that SciNLP achieves significant performance improvements on
certain baseline models. Using models trained on SciNLP, we implemented
automatic construction of a fine-grained knowledge graph for the NLP domain.
Our KG has an average node degree of 3.2 per entity, indicating rich semantic
topological information that enhances downstream applications. The dataset is
publicly available at https://github.com/AKADDC/SciNLP.

</details>


### [203] [Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models](https://arxiv.org/abs/2509.07142)
*Zhiyin Tan,Jennifer D'Souza*

Main category: cs.CL

TL;DR: 提出用大语言模型自动评估动态演化主题模型的框架，验证其有效性，表明能发现传统指标遗漏问题，支持开发评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有自动指标如连贯性和多样性只能捕捉狭窄统计模式，无法解释语义失败问题，需要更好的评估方法用于数字图书馆系统中主题模型评估。

Method: 引入面向目标的评估框架，采用九个基于大语言模型的指标，涵盖主题质量四个关键维度，通过对抗和基于采样的协议验证，并应用于不同数据集、多种主题建模方法和开源大语言模型。

Result: 基于大语言模型的指标能提供可解释、稳健且与任务相关的评估，发现主题模型中冗余和语义漂移等传统指标常遗漏的关键弱点。

Conclusion: 支持开发可扩展、细粒度的评估工具，以维护动态数据集中主题的相关性。

Abstract: This study presents a framework for automated evaluation of dynamically
evolving topic models using Large Language Models (LLMs). Topic modeling is
essential for organizing and retrieving scholarly content in digital library
systems, helping users navigate complex and evolving knowledge domains.
However, widely used automated metrics, such as coherence and diversity, often
capture only narrow statistical patterns and fail to explain semantic failures
in practice. We introduce a purpose-oriented evaluation framework that employs
nine LLM-based metrics spanning four key dimensions of topic quality: lexical
validity, intra-topic semantic soundness, inter-topic structural soundness, and
document-topic alignment soundness. The framework is validated through
adversarial and sampling-based protocols, and is applied across datasets
spanning news articles, scholarly publications, and social media posts, as well
as multiple topic modeling methods and open-source LLMs. Our analysis shows
that LLM-based metrics provide interpretable, robust, and task-relevant
assessments, uncovering critical weaknesses in topic models such as redundancy
and semantic drift, which are often missed by traditional metrics. These
results support the development of scalable, fine-grained evaluation tools for
maintaining topic relevance in dynamic datasets. All code and data supporting
this work are accessible at
https://github.com/zhiyintan/topic-model-LLMjudgment.

</details>


### [204] [DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge](https://arxiv.org/abs/2509.07188)
*Zonghai Yao,Michael Sun,Won Seok Jang,Sunjae Kwon,Soie Kwon,Hong Yu*

Main category: cs.CL

TL;DR: 引入DischargeSim基准评估大语言模型作为个性化出院教育者的能力，实验揭示模型在出院教育能力上有差距，模型大小不总带来更好结果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型基准未评估其诊后支持患者的能力，出院沟通是患者护理重要但研究不足的部分。

Method: 引入DischargeSim基准，模拟诊后多轮对话，从对话质量、个性化文档生成、患者理解三方面评估。

Result: 18个大语言模型在出院教育能力上有显著差距，不同患者特征下表现差异大，模型大小不总带来更好教育结果。

Conclusion: DischargeSim为诊后临床教育评估大语言模型迈出第一步，有助于促进公平、个性化患者支持。

Abstract: Discharge communication is a critical yet underexplored component of patient
care, where the goal shifts from diagnosis to education. While recent large
language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they
fail to evaluate models' ability to support patients after the visit. We
introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability
to act as personalized discharge educators. DischargeSim simulates post-visit,
multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with
diverse psychosocial profiles (e.g., health literacy, education, emotion).
Interactions are structured across six clinically grounded discharge topics and
assessed along three axes: (1) dialogue quality via automatic and LLM-as-judge
evaluation, (2) personalized document generation including free-text summaries
and structured AHRQ checklists, and (3) patient comprehension through a
downstream multiple-choice exam. Experiments across 18 LLMs reveal significant
gaps in discharge education capability, with performance varying widely across
patient profiles. Notably, model size does not always yield better education
outcomes, highlighting trade-offs in strategy use and content prioritization.
DischargeSim offers a first step toward benchmarking LLMs in post-visit
clinical education and promoting equitable, personalized patient support.

</details>


### [205] [Basis Vector Metric: A Method for Robust Open-Ended State Change Detection](https://arxiv.org/abs/2509.07308)
*David Oprea,Sam Powers*

Main category: cs.CL

TL;DR: 测试BVM方法利用语言嵌入判断图像状态变化的能力，通过两个实验与其他指标对比，BVM在分类名词状态上表现最佳，但在区分形容词上未显示明显优势，不过有改进可能。


<details>
  <summary>Details</summary>
Motivation: 测试BVM方法利用语言嵌入判断图像状态变化的能力。

Method: 使用MIT - States数据集，进行两个实验，分别将BVM与多种指标对比判断名词状态、与逻辑回归模型对比区分形容词。

Result: BVM在分类名词状态上表现最佳；在区分形容词上未找到其优于逻辑回归模型的确凿证据。

Conclusion: BVM有改进空间，可通过调整方法提高准确性。

Abstract: We test a new method, which we will abbreviate using the acronym BVM (Basis
Vectors Method), in its ability to judge the state changes in images through
using language embeddings. We used the MIT-States dataset, containing about
53,000 images, to gather all of our data, which has 225 nouns and 115
adjectives, with each noun having about 9 different adjectives, forming
approximately 1000 noun-adjective pairs. For our first experiment, we test our
method's ability to determine the state of each noun class separately against
other metrics for comparison. These metrics are cosine similarity, dot product,
product quantization, binary index, Naive Bayes, and a custom neural network.
Among these metrics, we found that our proposed BVM performs the best in
classifying the states for each noun. We then perform a second experiment where
we try using BVM to determine if it can differentiate adjectives from one
another for each adjective separately. We compared the abilities of BVM to
differentiate adjectives against the proposed method the MIT-States paper
suggests: using a logistic regression model. In the end, we did not find
conclusive evidence that our BVM metric could perform better than the logistic
regression model at discerning adjectives. Yet, we were able to find evidence
for possible improvements to our method; this leads to the chance of increasing
our method's accuracy through certain changes in our methodologies.

</details>


### [206] [Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations](https://arxiv.org/abs/2509.07311)
*Sihyun Park*

Main category: cs.CL

TL;DR: 现有大语言模型监督微调数据选择方法存在不足，本文提出KAMIR方法，基于模型内部表征分析数据，适用于多种任务，实验表明用不熟悉数据训练泛化性能更好。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型监督微调缺乏有效训练数据选择方法，现有基于知识的选择方法依赖提示工程，有局限性。

Method: 提出Knowledge Analysis via Model Internal Representations (KAMIR)方法，通过分析模型内部表征计算各层隐藏状态与最终隐藏状态的相似度来评估数据。

Result: 实验证明在不同任务数据集上，使用不太熟悉的数据进行训练能带来更好的泛化性能。

Conclusion: KAMIR方法克服了现有数据选择方法的局限，可应用于多种任务，用不熟悉数据训练有助于提升泛化性能。

Abstract: Recent advances in large language models (LLMs) have been driven by
pretraining, supervised fine tuning (SFT), and alignment tuning. Among these,
SFT plays a crucial role in transforming a model 's general knowledge into
structured responses tailored to specific tasks. However, there is no clearly
established methodology for effective training data selection. Simply
increasing the volume of data does not guarantee performance improvements,
while preprocessing, sampling, and validation require substantial time and
cost.
  To address this issue, a variety of data selection methods have been
proposed. Among them, knowledge based selection approaches identify suitable
training data by analyzing the model 's responses. Nevertheless, these methods
typically rely on prompt engineering, making them sensitive to variations and
incurring additional costs for prompt design.
  In this study, we propose Knowledge Analysis via Model Internal
Representations (KAMIR), a novel approach that overcomes these limitations by
analyzing data based on the model 's internal representations. KAMIR computes
similarities between the hidden states of each layer (block) and the final
hidden states for a given input to assess the data. Unlike prior methods that
were largely limited to multiple choice tasks, KAMIR can be applied to a wide
range of tasks such as machine reading comprehension and summarization.
Moreover, it selects data useful for training based on the model 's familiarity
with the input, even with a small dataset and a simple classifier architecture.
Experiments across diverse task datasets demonstrate that training with less
familiar data leads to better generalization performance.

</details>


### [207] [Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation](https://arxiv.org/abs/2509.07324)
*Nakyung Lee,Yeongoon Kim,Minhae Oh,Suhwan Kim,Jin Woo Koo,Hyewon Jo,Jungwoo Lee*

Main category: cs.CL

TL;DR: 提出SAOBP框架解决Transformer自注意力机制的局部化问题，引入GTD量化交互，实证显示其能提升模型性能，在小规模模型有优势。


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力机制存在局部化问题，难以捕捉长距离依赖。

Method: 提出Self - Attention One - step Belief Propagation (SAOBP)框架注入多跳关系，引入Global Token Dependency (GTD)量化交互。

Result: SAOBP可防止深层熵坍塌，自适应维持GTD在合适水平，提升模型性能，在小规模模型有竞争优势。

Conclusion: SAOBP框架能有效解决自注意力机制的局部化问题，在资源受限场景有提升推理质量的潜力。

Abstract: Transformer-based self-attention mechanism serves as the core of modern
language models, yet it often suffers from localization, where attentions
collapse onto a limited subset of tokens and fail to capture long-range
dependencies. To address this issue, we propose Self-Attention One-step Belief
Propagation (SAOBP), a refinement framework that injects multi-hop
relationships through a belief propagation process. To interpret and quantify
these interactions, we introduce Global Token Dependency (GTD) that captures
the relative contribution of multihop connections within the attention graph.
Empirical results indicate that SAOBP helps prevent entropy collapse in deeper
layers and adaptively maintains GTD at task-appropriate levels, thereby
supporting improvements in model performance. Importantly, we observe
competitive gains in small-scale models, highlighting its potential for
improving inference quality in resource-constrained scenarios.

</details>


### [208] [LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade](https://arxiv.org/abs/2509.07274)
*Aida Kostikova,Ole Pütz,Steffen Eger,Olga Sabelfeld,Benjamin Paassen*

Main category: cs.CL

TL;DR: 本文评估多个大语言模型在标注德国议会辩论中（反）团结亚型的表现，发现战后对移民的高度团结及2015年后反团结趋势，凸显大语言模型用于政治文本分析的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统研究政治演讲需大量手动注释，限制分析范围，大语言模型有潜力部分自动化复杂注释任务。

Method: 对多个大语言模型在标注德国议会辩论（反）团结亚型方面进行广泛评估，对比数千个人工参考注释，评估模型大小、提示差异、微调、数据年代等影响，调查系统误差。

Result: 数据显示战后对移民高度团结，2015年以来德国议会反团结趋势增强。

Conclusion: 大语言模型用于政治文本分析有前景，德国移民辩论重要，该国面临人口下降、劳动力短缺与两极分化加剧并存的情况。

Abstract: Migration has been a core topic in German political debate, from millions of
expellees post World War II over labor migration to refugee movements in the
recent past. Studying political speech regarding such wide-ranging phenomena in
depth traditionally required extensive manual annotations, limiting the scope
of analysis to small subsets of the data. Large language models (LLMs) have the
potential to partially automate even complex annotation tasks. We provide an
extensive evaluation of a multiple LLMs in annotating (anti-)solidarity
subtypes in German parliamentary debates compared to a large set of thousands
of human reference annotations (gathered over a year). We evaluate the
influence of model size, prompting differences, fine-tuning, historical versus
contemporary data; and we investigate systematic errors. Beyond methodological
evaluation, we also interpret the resulting annotations from a social science
lense, gaining deeper insight into (anti-)solidarity trends towards migrants in
the German post-World War II period and recent past. Our data reveals a high
degree of migrant-directed solidarity in the postwar period, as well as a
strong trend towards anti-solidarity in the German parliament since 2015,
motivating further research. These findings highlight the promise of LLMs for
political text analysis and the importance of migration debates in Germany,
where demographic decline and labor shortages coexist with rising polarization.

</details>


### [209] [Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents](https://arxiv.org/abs/2509.07389)
*Sankalp Tattwadarshi Swain,Anshika Krishnatray,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CL

TL;DR: 现有大语言模型语言能力评估研究未关注通过模式识别和交互反馈习得语言的能力，本文提出新实验框架评估其学习新语言能力，发现虽难建立对话但采用类似人类策略，为评估和模型设计提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型语言能力评估研究未涉及通过模式识别和交互反馈习得语言这一人类语言习得核心特征，需新评估方法。

Method: 提出新实验框架，让大语言模型与仅懂新构造语言Tinkatongue的机器人对话，评估其习得和使用该语言的能力。

Result: 大语言模型在100次回应内无法建立对话，但采用了类似人类语言学习的策略。

Conclusion: 研究结果为评估基准提供新方向，为设计更有效从交互反馈中学习的模型开辟道路。

Abstract: Existing evaluation studies on linguistic competence of large language models
(LLM agents) have focused primarily on vocabulary learning, morphological rule
induction, syntactic generalization, pragmatic inference, and cross-linguistic
transfer. However, none assess whether LLM agents can acquire a language
through pattern recognition and interactive feedback, a central feature of
human language acquisition. We propose a novel experimental framework in which
an LLM agent is evaluated on its ability to acquire and use a newly constructed
language (Tinkatongue) in conversation with a bot that understands only
Tinkatongue. Our findings show that LLM agents fail to establish a conversation
within 100 responses, yet they adopt distinct strategies that mirror human
approaches to language learning. The results suggest a new direction for
evaluation benchmarks and open pathways to model designs that learn more
effectively from interactive feedback.

</details>


### [210] [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301)
*Zhuoqing Song,Peng Sun,Huizhuo Yuan,Quanquan Gu*

Main category: cs.CL

TL;DR: 提出CASTLE注意力机制，可更新键值，避免显式生成前瞻键，在语言建模基准测试中表现优于标准因果注意力。


<details>
  <summary>Details</summary>
Motivation: 标准因果注意力中每个标记的QKV是静态的，仅编码前面的上下文，需要改进。

Method: 引入CASTLE注意力机制，不断更新每个标记的键，推导数学等价式以实现高效并行训练。

Result: 在语言建模基准测试中，CASTLE在各模型规模下均优于标准因果注意力，降低验证困惑度，提升下游任务性能。

Conclusion: CASTLE注意力机制是一种有效的注意力机制，能提升语言模型性能。

Abstract: In standard causal attention, each token's query, key, and value (QKV) are
static and encode only preceding context. We introduce CAuSal aTtention with
Lookahead kEys (CASTLE), an attention mechanism that continually updates each
token's keys as the context unfolds. We term these updated keys lookahead keys
because they belong to earlier positions yet integrate information from tokens
that appear later relative to those positions, while strictly preserving the
autoregressive property. Although the mechanism appears sequential, we derive a
mathematical equivalence that avoids explicitly materializing lookahead keys at
each position and enables efficient parallel training. On language modeling
benchmarks, CASTLE consistently outperforms standard causal attention across
model scales, reducing validation perplexity and improving performance on a
range of downstream tasks.

</details>


### [211] [Instance-level Performance Prediction for Long-form Generation Tasks](https://arxiv.org/abs/2509.07309)
*Chi-Yang Hsu,Alexander Braylan,Yiheng Su,Omar Alonso,Matthew Lease*

Main category: cs.CL

TL;DR: 提出新基准用于长文本生成任务实例级性能预测，可预测连续评估指标得分和区间，少量训练样本有效，引入新任务、基准和基线。


<details>
  <summary>Details</summary>
Motivation: 为具有多方面、细粒度质量指标的长文本生成任务的实例级性能预测提供新基准。

Method: 提出任务、模型和指标无关的公式，仅根据黑盒模型输入输出预测连续评估指标得分，还需推断预测区间。对11个长文本数据集/任务，用多个大语言模型、基线和指标进行评估。

Result: 使用低至16个训练示例就能跨长文本生成任务有效预测分数。

Conclusion: 引入了新颖且有用的任务、有价值的基准以推动进展，以及可实际应用的基线。

Abstract: We motivate and share a new benchmark for instance-level performance
prediction of long-form generation tasks having multi-faceted, fine-grained
quality metrics. Our task-, model- and metric-agnostic formulation predicts
continuous evaluation metric scores given only black-box model inputs and
outputs. Beyond predicting point estimates of metric scores, the benchmark also
requires inferring prediction intervals to quantify uncertainty around point
estimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs,
baselines, and metrics per task. We show that scores can be effectively
predicted across long-form generation tasks using as few as 16 training
examples. Overall, we introduce a novel and useful task, a valuable benchmark
to drive progress, and baselines ready for practical adoption today.

</details>


### [212] [HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention](https://arxiv.org/abs/2509.07475)
*Saumya Goswami,Siddharth Kurra*

Main category: cs.CL

TL;DR: 本文介绍HALT - RAG系统检测RAG管道输出中的幻觉内容，在多个任务上取得较好F1分数，有实用弃权机制。


<details>
  <summary>Details</summary>
Motivation: 检测与给定源文本矛盾或无支持的内容是生成式语言模型安全部署的关键挑战，需开发检测RAG管道输出中幻觉内容的系统。

Method: 引入HALT - RAG后验验证系统，使用两个冻结的现成NLI模型和轻量级词汇信号的通用特征集训练简单、校准且适应任务的元分类器，采用严格的5折外折训练协议。

Result: HALT - RAG在摘要、问答和对话任务上分别取得0.7756、0.9786和0.7391的强OOF F1分数。

Conclusion: 系统校准良好的概率实现了实用的弃权机制，为平衡模型性能和安全要求提供了可靠工具。

Abstract: Detecting content that contradicts or is unsupported by a given source text
is a critical challenge for the safe deployment of generative language models.
We introduce HALT-RAG, a post-hoc verification system designed to identify
hallucinations in the outputs of Retrieval-Augmented Generation (RAG)
pipelines. Our flexible and task-adaptable framework uses a universal feature
set derived from an ensemble of two frozen, off-the-shelf Natural Language
Inference (NLI) models and lightweight lexical signals. These features are used
to train a simple, calibrated, and task-adapted meta-classifier. Using a
rigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and
produce unbiased estimates, we evaluate our system on the HaluEval benchmark.
By pairing our universal feature set with a lightweight, task-adapted
classifier and a precision-constrained decision policy, HALT-RAG achieves
strong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,
and dialogue tasks, respectively. The system's well-calibrated probabilities
enable a practical abstention mechanism, providing a reliable tool for
balancing model performance with safety requirements.

</details>


### [213] [Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition](https://arxiv.org/abs/2509.07555)
*Yi Liu,Xiangrong Zhu,Xiangyu Liu,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: 信息更新快，大语言模型知识易过时，现有RAG - 基KE方法在多跳问答知识编辑有问题，提出IRAKE方法，实验显示其效果好。


<details>
  <summary>Details</summary>
Motivation: 信息更新快，大语言模型知识易过时，重新训练成本高，需无参数修改的知识编辑；现有RAG - 基KE方法在多跳问答中因‘编辑跳过’问题表现不佳。

Method: 提出Iterative Retrieval - Augmented Knowledge Editing method with guided decomposition (IRAKE)，通过单编辑事实和整个编辑案例的引导。

Result: IRAKE减轻了因编辑跳过导致的编辑失败，在多跳问答知识编辑上优于现有方法。

Conclusion: IRAKE方法有效解决了现有RAG - 基KE方法在多跳问答知识编辑中的问题，具有良好性能。

Abstract: In a rapidly evolving world where information updates swiftly, knowledge in
large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a
cost-effective option, making knowledge editing (KE) without modifying
parameters particularly necessary. We find that although existing
retrieval-augmented generation (RAG)-based KE methods excel at editing simple
knowledge, they struggle with KE in multi-hop question answering due to the
issue of "edit skipping", which refers to skipping the relevant edited fact in
inference. In addition to the diversity of natural language expressions of
knowledge, edit skipping also arises from the mismatch between the granularity
of LLMs in problem-solving and the facts in the edited memory. To address this
issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing
method with guided decomposition (IRAKE) through the guidance from single
edited facts and entire edited cases. Experimental results demonstrate that
IRAKE mitigates the failure of editing caused by edit skipping and outperforms
state-of-the-art methods for KE in multi-hop question answering.

</details>


### [214] [BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment](https://arxiv.org/abs/2509.07588)
*Andrey Sakhovskiy,Elena Tutubalina*

Main category: cs.CL

TL;DR: 提出BALI方法将生物医学知识图谱与语言模型对齐，提升语言模型在生物医学文本理解任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学大语言模型对复杂领域概念结构和知识图谱中事实信息理解有限。

Method: 提出BALI方法，同时学习KG编码器并对齐LM和图的表示，将生物医学概念提及与UMLS KG链接，利用局部KG子图作为跨模态正样本。

Result: 在PubMedBERT和BioLinkBERT等模型上应用该方法，提升了语言理解任务表现和实体表示质量，小数据集预训练也有效。

Conclusion: BALI方法能有效增强生物医学语言模型的性能。

Abstract: In recent years, there has been substantial progress in using pretrained
Language Models (LMs) on a range of tasks aimed at improving the understanding
of biomedical texts. Nonetheless, existing biomedical LLMs show limited
comprehension of complex, domain-specific concept structures and the factual
information encoded in biomedical Knowledge Graphs (KGs). In this work, we
propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel
joint LM and KG pre-training method that augments an LM with external knowledge
by the simultaneous learning of a dedicated KG encoder and aligning the
representations of both the LM and the graph. For a given textual sequence, we
link biomedical concept mentions to the Unified Medical Language System (UMLS)
KG and utilize local KG subgraphs as cross-modal positive samples for these
mentions. Our empirical findings indicate that implementing our method on
several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves
their performance on a range of language understanding tasks and the quality of
entity representations, even with minimal pre-training on a small alignment
dataset sourced from PubMed scientific abstracts.

</details>


### [215] [Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost](https://arxiv.org/abs/2509.07829)
*Mihai Nadas,Laura Diosan,Andreea Tomescu,Andrei Piscoran*

Main category: cs.CL

TL;DR: 提出TINYFABULIST TRANSLATION FRAMEWORK (TF2)框架用于英罗文学翻译，创建并发布模型和数据集，结果显示微调模型有竞争力且成本低。


<details>
  <summary>Details</summary>
Motivation: 解决小开源模型在文学翻译中的问题，满足低资源语言如罗马尼亚语对高质量文学数据集的需求。

Method: 基于DS - TF1 - EN - 3M生成罗马尼亚语参考，对12B参数模型进行两阶段微调，用BLEU和基于LLM的五维评分标准评估。

Result: 微调模型在流畅性和充分性上可与顶级大型专有模型竞争，且开源、易访问、成本低。

Conclusion: TF2为低成本翻译等研究提供端到端、可复现的流程。

Abstract: Literary translation has recently gained attention as a distinct and complex
task in machine translation research. However, the translation by small open
models remains an open problem. We contribute to this ongoing research by
introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for
dataset creation, fine tuning, and evaluation in English-Romanian literary
translations, centred on the creation and open release of both a compact, fine
tuned language model (TF2-12B) and large scale synthetic parallel datasets
(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the
largest collection of synthetic English fables to date, we address the need for
rich, high quality literary datasets in low resource languages such as
Romanian. Our pipeline first generates 15k high quality Romanian references
from the TF1 pool using a high performing LLM. We then apply a two stage fine
tuning process to a 12B parameter open weight model: (i) instruction tuning to
capture genre specific narrative style, and (ii) adapter compression for
efficient deployment. Evaluation combines corpus level BLEU and a five
dimension LLM based rubric (accuracy, fluency, coherence, style, cultural
adaptation) to provide a nuanced assessment of translation quality. Results
show that our fine tuned model achieves fluency and adequacy competitive with
top performing large proprietary models, while being open, accessible, and
significantly more cost effective. Alongside the fine tuned model and both
datasets, we publicly release all scripts and evaluation prompts. TF2 thus
provides an end-to-end, reproducible pipeline for research on cost efficient
translation, cross lingual narrative generation, and the broad adoption of open
models for culturally significant literary content in low resource settings.

</details>


### [216] [GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2509.07925)
*Tuo Wang,Adithya Kulkarni,Tyler Cody,Peter A. Beling,Yujun Yan,Dawei Zhou*

Main category: cs.CL

TL;DR: 提出用于大语言模型的结构感知不确定性估计框架 GENUINE，实验表明其比基于语义熵的方法更有效。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型不确定性估计方法常忽略语义依赖，无法捕捉生成文本的结构关系，需提升可靠性。

Method: 提出 GENUINE 框架，利用依赖解析树和分层图池化，结合监督学习来改进不确定性量化。

Result: 在 NLP 任务的大量实验中，GENUINE 比基于语义熵的方法 AUROC 最高提升 29%，校准误差降低超 15%。

Conclusion: 基于图的不确定性建模方法有效，代码开源。

Abstract: Uncertainty estimation is essential for enhancing the reliability of Large
Language Models (LLMs), particularly in high-stakes applications. Existing
methods often overlook semantic dependencies, relying on token-level
probability measures that fail to capture structural relationships within the
generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty
Estimation for Large Language Models, a structure-aware framework that
leverages dependency parse trees and hierarchical graph pooling to refine
uncertainty quantification. By incorporating supervised learning, GENUINE
effectively models semantic and structural relationships, improving confidence
assessments. Extensive experiments across NLP tasks show that GENUINE achieves
up to 29% higher AUROC than semantic entropy-based approaches and reduces
calibration errors by over 15%, demonstrating the effectiveness of graph-based
uncertainty modeling. The code is available at
https://github.com/ODYSSEYWT/GUQ.

</details>


### [217] [Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning](https://arxiv.org/abs/2509.07768)
*Michele Joshua Maggini,Dhia Merzougui,Rabiraj Bandyopadhyay,Gaël Dias,Fabrice Maurel,Pablo Gamallo*

Main category: cs.CL

TL;DR: 研究对不同大语言模型适应范式用于检测虚假新闻等内容进行全面评估，实验涵盖多数据集和语言，发现上下文学习常不如微调模型。


<details>
  <summary>Details</summary>
Motivation: 在线平台虚假新闻等有害内容传播受关注，尚无研究对大语言模型在不同模型、使用方法和语言上的性能进行恰当基准测试。

Method: 实验涵盖10个数据集和5种语言，测试从参数高效微调模型到多种上下文学习策略和提示的不同策略。

Result: 发现上下文学习与微调模型相比通常表现较差。

Conclusion: 强调即使在特定任务设置中，微调较小模型也比在上下文学习设置中评估的最大模型更重要。

Abstract: The spread of fake news, polarizing, politically biased, and harmful content
on online platforms has been a serious concern. With large language models
becoming a promising approach, however, no study has properly benchmarked their
performance across different models, usage methods, and languages. This study
presents a comprehensive overview of different Large Language Models adaptation
paradigms for the detection of hyperpartisan and fake news, harmful tweets, and
political bias. Our experiments spanned 10 datasets and 5 different languages
(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and
multiclass classification scenarios. We tested different strategies ranging
from parameter efficient Fine-Tuning of language models to a variety of
different In-Context Learning strategies and prompts. These included zero-shot
prompts, codebooks, few-shot (with both randomly-selected and
diversely-selected examples using Determinantal Point Processes), and
Chain-of-Thought. We discovered that In-Context Learning often underperforms
when compared to Fine-Tuning a model. This main finding highlights the
importance of Fine-Tuning even smaller models on task-specific settings even
when compared to the largest models evaluated in an In-Context Learning setup -
in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and
Qwen2.5-7B-Instruct.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [218] [Evaluation of Machine Learning Reconstruction Techniques for Accelerated Brain MRI Scans](https://arxiv.org/abs/2509.07193)
*Jonathan I. Mandel,Shivaprakash Hiremath,Hedyeh Keshtgar,Timothy Scholl,Sadegh Raeisi*

Main category: eess.IV

TL;DR: 研究评估DeepFoqus - Accelerate在四倍加速脑部MRI扫描时能否保留诊断质量，结果显示可行且减少扫描时间、提升效率。


<details>
  <summary>Details</summary>
Motivation: 评估深度学习的MRI重建算法在四倍加速脑部MRI扫描时能否保留诊断质量。

Method: 纳入健康志愿者和公共数据集，用DeepFoqus - Accelerate重建图像与标准护理对比，由专家评分并进行定量评估。

Result: AI重建扫描评分大多≥4，定量指标良好，判读一致性为轻度到中度，罕见伪影不影响诊断。

Conclusion: DeepFoqus - Accelerate能实现四倍脑部MRI加速，减少75%扫描时间，保留诊断质量并提升工作流程效率。

Abstract: This retrospective-prospective study evaluated whether a deep learning-based
MRI reconstruction algorithm can preserve diagnostic quality in brain MRI scans
accelerated up to fourfold, using both public and prospective clinical data.
The study included 18 healthy volunteers (scans acquired at 3T, January
2024-March 2025), as well as selected fastMRI public datasets with diverse
pathologies. Phase-encoding-undersampled 2D/3D T1, T2, and FLAIR sequences were
reconstructed with DeepFoqus-Accelerate and compared with standard-of-care
(SOC). Three board-certified neuroradiologists and two MRI technologists
independently reviewed 36 paired SOC/AI reconstructions from both datasets
using a 5-point Likert scale, while quantitative similarity was assessed for
408 scans and 1224 datasets using Structural Similarity Index (SSIM), Peak
Signal-to-Noise Ratio (PSNR), and Haar wavelet-based Perceptual Similarity
Index (HaarPSI). No AI-reconstructed scan scored below 3 (minimally
acceptable), and 95% scored $\geq 4$. Mean SSIM was 0.95 $\pm$ 0.03 (90% cases
>0.90), PSNR >41.0 dB, and HaarPSI >0.94. Inter-rater agreement was slight to
moderate. Rare artifacts did not affect diagnostic interpretation. These
findings demonstrate that DeepFoqus-Accelerate enables robust fourfold brain
MRI acceleration with 75% reduced scan time, while preserving diagnostic image
quality and supporting improved workflow efficiency.

</details>


### [219] [Physics-Guided Diffusion Transformer with Spherical Harmonic Posterior Sampling for High-Fidelity Angular Super-Resolution in Diffusion MRI](https://arxiv.org/abs/2509.07020)
*Mu Nan,Taohui Xiao,Ruoyou Wu,Shoujun Yu,Ye Li,Hairong Zheng,Shanshan Wang*

Main category: eess.IV

TL;DR: 本文提出Physics - Guided Diffusion Transformer (PGDiT) 用于扩散MRI角超分辨率重建，在实验中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散MRI角超分辨率方法在恢复细粒度角细节和保持高保真度方面存在不足，原因是对q空间几何建模不足和物理约束融入不够。

Method: 训练阶段使用Q - space Geometry - Aware Module (QGAM) 进行方向感知表示学习；推理阶段采用两阶段Spherical Harmonics - Guided Posterior Sampling (SHPS) 并结合热扩散SH正则化。

Result: 在一般角超分辨率任务及两个下游应用中，PGDiT在细节恢复和数据保真度上优于现有深度学习模型。

Conclusion: PGDiT是一种新颖的生成式角超分辨率框架，能实现高保真高角分辨率扩散MRI重建，在神经科学和临床研究有潜在应用。

Abstract: Diffusion MRI (dMRI) angular super-resolution (ASR) aims to reconstruct
high-angular-resolution (HAR) signals from limited low-angular-resolution (LAR)
data without prolonging scan time. However, existing methods are limited in
recovering fine-grained angular details or preserving high fidelity due to
inadequate modeling of q-space geometry and insufficient incorporation of
physical constraints. In this paper, we introduce a Physics-Guided Diffusion
Transformer (PGDiT) designed to explore physical priors throughout both
training and inference stages. During training, a Q-space Geometry-Aware Module
(QGAM) with b-vector modulation and random angular masking facilitates
direction-aware representation learning, enabling the network to generate
directionally consistent reconstructions with fine angular details from sparse
and noisy data. In inference, a two-stage Spherical Harmonics-Guided Posterior
Sampling (SHPS) enforces alignment with the acquired data, followed by
heat-diffusion-based SH regularization to ensure physically plausible
reconstructions. This coarse-to-fine refinement strategy mitigates
oversmoothing and artifacts commonly observed in purely data-driven or
generative models. Extensive experiments on general ASR tasks and two
downstream applications, Diffusion Tensor Imaging (DTI) and Neurite Orientation
Dispersion and Density Imaging (NODDI), demonstrate that PGDiT outperforms
existing deep learning models in detail recovery and data fidelity. Our
approach presents a novel generative ASR framework that offers high-fidelity
HAR dMRI reconstructions, with potential applications in neuroscience and
clinical research.

</details>


### [220] [PUUMA (Placental patch and whole-Uterus dual-branch U-Mamba-based Architecture): Functional MRI Prediction of Gestational Age at Birth and Preterm Risk](https://arxiv.org/abs/2509.07042)
*Diego Fajardo-Rojas,Levente Baljer,Jordina Aviles Verdera,Megan Hall,Daniel Cromb,Mary A. Rutherford,Lisa Story,Emma C. Robinson,Jana Hutter*

Main category: eess.IV

TL;DR: 开发双分支深度学习架构PUUMA，用T2*胎儿MRI数据预测出生时胎龄，与线性回归等对比，结果证明MRI自动预测胎龄可行，强调全子宫功能成像和MRI宫颈长度测量价值，未来将扩大样本和纳入更多成像。


<details>
  <summary>Details</summary>
Motivation: 早产成因复杂多因素，现有临床预测指标效果有限，阻碍最佳护理，需开发新的预测方法。

Method: 开发双分支深度学习架构PUUMA，整合全子宫和胎盘特征，用295例妊娠T2*胎儿MRI数据预测出生时胎龄，与线性回归和其他深度学习架构对比，用平均绝对误差评估胎龄预测，用准确率、敏感性和特异性评估早产分类。

Result: 全自动MRI管道和宫颈长度回归在检测早产时平均绝对误差相当（3周），敏感性良好（0.67）。

Conclusion: 证明从功能MRI自动预测出生时胎龄的概念可行，强调全子宫功能成像识别高危妊娠的价值，表明MRI手动高分辨率宫颈长度测量有预测价值，未来需扩大样本和纳入更多器官成像。

Abstract: Preterm birth is a major cause of mortality and lifelong morbidity in
childhood. Its complex and multifactorial origins limit the effectiveness of
current clinical predictors and impede optimal care. In this study, a
dual-branch deep learning architecture (PUUMA) was developed to predict
gestational age (GA) at birth using T2* fetal MRI data from 295 pregnancies,
encompassing a heterogeneous and imbalanced population. The model integrates
both global whole-uterus and local placental features. Its performance was
benchmarked against linear regression using cervical length measurements
obtained by experienced clinicians from anatomical MRI and other Deep Learning
architectures. The GA at birth predictions were assessed using mean absolute
error. Accuracy, sensitivity, and specificity were used to assess preterm
classification. Both the fully automated MRI-based pipeline and the cervical
length regression achieved comparable mean absolute errors (3 weeks) and good
sensitivity (0.67) for detecting preterm birth, despite pronounced class
imbalance in the dataset. These results provide a proof of concept for
automated prediction of GA at birth from functional MRI, and underscore the
value of whole-uterus functional imaging in identifying at-risk pregnancies.
Additionally, we demonstrate that manual, high-definition cervical length
measurements derived from MRI, not currently routine in clinical practice,
offer valuable predictive information. Future work will focus on expanding the
cohort size and incorporating additional organ-specific imaging to improve
generalisability and predictive performance.

</details>


### [221] [Enhanced SegNet with Integrated Grad-CAM for Interpretable Retinal Layer Segmentation in OCT Images](https://arxiv.org/abs/2509.07795)
*S M Asiful Islam Saky,Ugyen Tshering*

Main category: eess.IV

TL;DR: 提出基于SegNet的深度学习框架用于视网膜层自动可解释分割，在Duke OCT数据集上验证，性能佳且有可视化解释，弥合准确性与可解释性差距。


<details>
  <summary>Details</summary>
Motivation: 手动分割视网膜层耗时且结果不稳定，传统深度学习模型缺乏可解释性，需准确且可解释的分割方法辅助临床决策。

Method: 提出改进的基于SegNet的深度学习框架，采用修改的池化策略增强特征提取，结合分类交叉熵和Dice损失的混合损失函数，集成Grad - CAM提供可视化解释。

Result: 在Duke OCT数据集上验证，验证准确率达95.77%，Dice系数0.9446，Jaccard指数0.8951，多数层表现稳健，但较薄边界仍有挑战，Grad - CAM可视化与临床生物标志物一致。

Conclusion: 结合架构改进、定制混合损失和可解释AI的框架，能弥合准确性与可解释性差距，有望规范OCT分析、提高诊断效率和增强临床对AI眼科工具的信任。

Abstract: Optical Coherence Tomography (OCT) is essential for diagnosing conditions
such as glaucoma, diabetic retinopathy, and age-related macular degeneration.
Accurate retinal layer segmentation enables quantitative biomarkers critical
for clinical decision-making, but manual segmentation is time-consuming and
variable, while conventional deep learning models often lack interpretability.
This work proposes an improved SegNet-based deep learning framework for
automated and interpretable retinal layer segmentation. Architectural
innovations, including modified pooling strategies, enhance feature extraction
from noisy OCT images, while a hybrid loss function combining categorical
cross-entropy and Dice loss improves performance for thin and imbalanced
retinal layers. Gradient-weighted Class Activation Mapping (Grad-CAM) is
integrated to provide visual explanations, allowing clinical validation of
model decisions. Trained and validated on the Duke OCT dataset, the framework
achieved 95.77% validation accuracy, a Dice coefficient of 0.9446, and a
Jaccard Index (IoU) of 0.8951. Class-wise results confirmed robust performance
across most layers, with challenges remaining for thinner boundaries. Grad-CAM
visualizations highlighted anatomically relevant regions, aligning segmentation
with clinical biomarkers and improving transparency. By combining architectural
improvements, a customized hybrid loss, and explainable AI, this study delivers
a high-performing SegNet-based framework that bridges the gap between accuracy
and interpretability. The approach offers strong potential for standardizing
OCT analysis, enhancing diagnostic efficiency, and fostering clinical trust in
AI-driven ophthalmic tools.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [222] [Reinforcement learning for online hyperparameter tuning in convex quadratic programming](https://arxiv.org/abs/2509.07404)
*Jeremy Bertoncini,Alberto De Marchi,Matthias Gerdts,Simon Gottschalk*

Main category: math.OC

TL;DR: 探讨数据驱动方法加速二次规划求解，用强化学习助力求解器调参和加速优化，实验表明学习策略泛化性好。


<details>
  <summary>Details</summary>
Motivation: 正则化方法尾收敛慢且超参数调优困难，需解决这些问题加速求解过程。

Method: 聚焦稳定内点求解器，处理其双循环流程和控制参数，采用强化学习。

Result: 经轻量级训练后，学习到的策略能很好地泛化到不同问题类和求解器配置。

Conclusion: 强化学习能显著促进求解器调优和加速优化过程。

Abstract: Quadratic programming is a workhorse of modern nonlinear optimization,
control, and data science. Although regularized methods offer convergence
guarantees under minimal assumptions on the problem data, they can exhibit the
slow tail-convergence typical of first-order schemes, thus requiring many
iterations to achieve high-accuracy solutions. Moreover, hyperparameter tuning
significantly impacts on the solver performance but how to find an appropriate
parameter configuration remains an elusive research question. To address these
issues, we explore how data-driven approaches can accelerate the solution
process. Aiming at high-accuracy solutions, we focus on a stabilized
interior-point solver and carefully handle its two-loop flow and control
parameters. We will show that reinforcement learning can make a significant
contribution to facilitating the solver tuning and to speeding up the
optimization process. Numerical experiments demonstrate that, after a
lightweight training, the learned policy generalizes well to different problem
classes with varying dimensions and to various solver configurations.

</details>


### [223] [Decentralized Online Riemannian Optimization Beyond Hadamard Manifolds](https://arxiv.org/abs/2509.07779)
*Emre Sahinoglu,Shahin Shahrampour*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study decentralized online Riemannian optimization over manifolds with
possibly positive curvature, going beyond the Hadamard manifold setting.
Decentralized optimization techniques rely on a consensus step that is well
understood in Euclidean spaces because of their linearity. However, in
positively curved Riemannian spaces, a main technical challenge is that
geodesic distances may not induce a globally convex structure. In this work, we
first analyze a curvature-aware Riemannian consensus step that enables a linear
convergence beyond Hadamard manifolds. Building on this step, we establish a
$O(\sqrt{T})$ regret bound for the decentralized online Riemannian gradient
descent algorithm. Then, we investigate the two-point bandit feedback setup,
where we employ computationally efficient gradient estimators using smoothing
techniques, and we demonstrate the same $O(\sqrt{T})$ regret bound through the
subconvexity analysis of smoothed objectives.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [224] [Estimating forest carbon stocks from high-resolution remote sensing imagery by reducing domain shift with style transfer](https://arxiv.org/abs/2502.00784)
*Zhenyu Yu,Jinnian Wang*

Main category: cs.CV

TL;DR: 利用GF - 1 WFV和Landsat TM图像，采用风格迁移法引入Swin Transformer分析云南会泽县森林碳储量，将碳储量估算转化为图像翻译。


<details>
  <summary>Details</summary>
Motivation: 森林是重要陆地碳库，现有森林碳储量监测评估技术虽利于大规模观测，但精度有待提高。

Method: 使用GF - 1 WFV和Landsat TM图像，采用风格迁移法，引入Swin Transformer通过注意力机制提取全局特征，将碳储量估算转化为图像翻译。

Result: 摘要未提及。

Conclusion: 摘要未提及。

Abstract: Forests function as crucial carbon reservoirs on land, and their carbon sinks
can efficiently reduce atmospheric CO2 concentrations and mitigate climate
change. Currently, the overall trend for monitoring and assessing forest carbon
stocks is to integrate ground monitoring sample data with satellite remote
sensing imagery. This style of analysis facilitates large-scale observation.
However, these techniques require improvement in accuracy. We used GF-1 WFV and
Landsat TM images to analyze Huize County, Qujing City, Yunnan Province in
China. Using the style transfer method, we introduced Swin Transformer to
extract global features through attention mechanisms, converting the carbon
stock estimation into an image translation.

</details>


### [225] [CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis](https://arxiv.org/abs/2509.06986)
*Cedric Caruzzo,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出CellPainTR架构解决生物数据集成难题，验证其在批处理集成和泛化性能上表现优异。


<details>
  <summary>Details</summary>
Motivation: 大规模生物发现需整合异构数据集，但技术批处理效应和缺乏通用模型是关键障碍。

Method: 引入基于Transformer的CellPainTR架构，用特定源上下文标记设计实现免微调泛化。

Result: 在JUMP数据集上表现优于ComBat和Harmony，在Bray等未见数据集OOD任务中保持高性能。

Conclusion: 研究为基于图像的分析创建基础模型迈出重要一步，实现更可靠可扩展的跨研究生物分析。

Abstract: Large-scale biological discovery requires integrating massive, heterogeneous
datasets like those from the JUMP Cell Painting consortium, but technical batch
effects and a lack of generalizable models remain critical roadblocks. To
address this, we introduce CellPainTR, a Transformer-based architecture
designed to learn foundational representations of cellular morphology that are
robust to batch effects. Unlike traditional methods that require retraining on
new data, CellPainTR's design, featuring source-specific context tokens, allows
for effective out-of-distribution (OOD) generalization to entirely unseen
datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP
dataset, where it outperforms established methods like ComBat and Harmony in
both batch integration and biological signal preservation. Critically, we
demonstrate its robustness through a challenging OOD task on the unseen Bray et
al. dataset, where it maintains high performance despite significant domain and
feature shifts. Our work represents a significant step towards creating truly
foundational models for image-based profiling, enabling more reliable and
scalable cross-study biological analysis.

</details>


### [226] [FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection](https://arxiv.org/abs/2509.06987)
*Alexey Zhukov,Jenny Benois-Pineau,Amira Youssef,Akka Zemmari,Mohamed Mosbah,Virginie Taillandier*

Main category: cs.CV

TL;DR: 提出基于领域规则的多模态融合架构，结合YOLO和视觉Transformer，在铁路数据集实验中提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 单模态检测方法有局限性，在外观与正常结构元素相似时会过度检测。

Method: 提出基于领域规则的多模态融合架构，结合YOLOv8n和视觉Transformer，融合图像和音频特征。

Result: 在真实铁路数据集上实验，多模态融合比仅视觉方法精度和整体准确率提升0.2点，t检验证明均值准确率差异有统计学意义。

Conclusion: 多模态融合架构有效提升了缺陷检测的精度和准确率。

Abstract: Multimodal fusion is a multimedia technique that has become popular in the
wide range of tasks where image information is accompanied by a signal/audio.
The latter may not convey highly semantic information, such as speech or music,
but some measures such as audio signal recorded by mics in the goal to detect
rail structure elements or defects. While classical detection approaches such
as You Only Look Once (YOLO) family detectors can be efficiently deployed for
defect detection on the image modality, the single modality approaches remain
limited. They yield an overdetection in case of the appearance similar to
normal structural elements. The paper proposes a new multimodal fusion
architecture built on the basis of domain rules with YOLO and Vision
transformer backbones. It integrates YOLOv8n for rapid object detection with a
Vision Transformer (ViT) to combine feature maps extracted from multiple layers
(7, 16, and 19) and synthesised audio representations for two defect classes:
rail Rupture and Surface defect. Fusion is performed between audio and image.
Experimental evaluation on a real-world railway dataset demonstrates that our
multimodal fusion improves precision and overall accuracy by 0.2 points
compared to the vision-only approach. Student's unpaired t-test also confirms
statistical significance of differences in the mean accuracy.

</details>


### [227] [Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection](https://arxiv.org/abs/2509.06988)
*Yingsheng Wang,Shuo Lu,Jian Liang,Aihua Zheng,Ran He*

Main category: cs.CV

TL;DR: 提出基于子空间投影的ClaFR后验方法用于OOD检测，无需访问训练数据且性能领先，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于特征的后验OOD检测方法需访问训练数据，不适合数据隐私保护场景。

Method: 从子空间投影角度提出ClaFR方法，对分类器权重正交分解提取类已知子空间，将原始数据特征映射到该子空间得到新表示，通过计算子空间内数据特征重建误差确定OOD分数。

Result: 与现有OOD检测算法相比，该方法无需访问训练数据，在多个OOD基准测试中取得领先性能。

Conclusion: 提出的ClaFR方法简单有效，解决了现有方法在数据隐私方面的问题，具有良好性能。

Abstract: Out-of-distribution (OOD) detection helps models identify data outside the
training categories, crucial for security applications. While feature-based
post-hoc methods address this by evaluating data differences in the feature
space without changing network parameters, they often require access to
training data, which may not be suitable for some data privacy scenarios. This
may not be suitable in scenarios where data privacy protection is a concern. In
this paper, we propose a simple yet effective post-hoc method, termed
Classifier-based Feature Reconstruction (ClaFR), from the perspective of
subspace projection. It first performs an orthogonal decomposition of the
classifier's weights to extract the class-known subspace, then maps the
original data features into this subspace to obtain new data representations.
Subsequently, the OOD score is determined by calculating the feature
reconstruction error of the data within the subspace. Compared to existing OOD
detection algorithms, our method does not require access to training data while
achieving leading performance on multiple OOD benchmarks. Our code is released
at https://github.com/Aie0923/ClaFR.

</details>


### [228] [The Protocol Genome A Self Supervised Learning Framework from DICOM Headers](https://arxiv.org/abs/2509.06995)
*Jimmy Joseph*

Main category: cs.CV

TL;DR: 介绍Protocol Genome自监督学习系统，学习DICOM头信息关联，在外部验证中表现优，多任务实验显示有优势，发布模型卡和部署指南。


<details>
  <summary>Details</summary>
Motivation: 临床成像中程序选择带来的潜在混杂因素阻碍仅图像网络跨站点泛化，需学习协议感知且临床鲁棒的图像表示。

Method: 将结构化DICOM头信息作为标签，采用协议 - 图像对比学习、掩码协议预测、协议 - 协议翻译等方法处理头嵌入和图像特征。

Result: 在多个任务中比强SSL基线和ImageNet迁移有更高的外部AUROC，校准改善25 - 37%；减少协议边界误报。

Conclusion: Protocol Genome系统表现良好，相关增益在部分标记数据情况下仍能保留，可用于PACS并已发布相关资料。

Abstract: In this paper, we introduce the Protocol Genome, a self-supervised learning
system that learns correlations from DICOM headers and achieves AUROC 0.901 (vs
0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation.
Our method also improves calibration and robustness across modalities (CT, MRI,
CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where
procedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice
thickness) have consequences for contrast, noise, and artifact. These latent
confounders impede the generalization of image-only networks across sites. We
consider structured DICOM headers as a label and learn protocol-aware but
clinically robust image representations. Protocol Genome obtains tokenized
embeddings of de-identified header fields and models them along with image
features using: (1) protocol-image contrastive learning, (2) masked protocol
prediction, and (3) protocol-protocol translation. With 1.26M studies (7 health
systems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT
triage for PE, (B) brain MRI glioma grading, and (C) chest radiograph
cardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well
as ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041:
cardiomegaly) is associated with higher external AUROC; 25-37% calibration
improvements are obtained (p < 0.01, DeLong tests). While the gains may be
task-dependent, they are preserved with 10-20% of labeled data. From a clinical
point of view, the technique reduces false positives at protocol borders and is
applicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a
model card and deployment guide, complete with both de-identification and bias
audits.

</details>


### [229] [Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems](https://arxiv.org/abs/2509.06996)
*Jie Zhang,Ting Xu,Gelei Deng,Runyi Hu,Han Qiu,Tianwei Zhang,Qing Guo,Ivor Tsang*

Main category: cs.CV

TL;DR: 研究先进视觉语言模型（VLMs）在文字受干扰时的识别能力，发现其存在结构局限，还发布相关代码等以推动后续研究。


<details>
  <summary>Details</summary>
Motivation: 探究先进视觉语言模型是否具备人类在文字碎片化、融合或部分遮挡时仍能识别的能力。

Method: 构建跨越中文和英文书写系统的心理物理学基准，通过拼接、重组和叠加字形生成干扰刺激。

Result: 当代VLMs在干净文本上表现良好，但在干扰下性能严重下降，常产生无关或不连贯输出。

Conclusion: VLMs存在结构局限，依赖通用视觉不变性而对组合先验依赖不足，研究结果为架构和训练策略提供方向，指出多模态系统应用面临的挑战。

Abstract: Writing is a universal cultural technology that reuses vision for symbolic
communication. Humans display striking resilience: we readily recognize words
even when characters are fragmented, fused, or partially occluded. This paper
investigates whether advanced vision language models (VLMs) share this
resilience. We construct two psychophysics inspired benchmarks across distinct
writing systems, Chinese logographs and English alphabetic words, by splicing,
recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli
for models while remaining legible to humans. Despite strong performance on
clean text, contemporary VLMs show a severe drop under these perturbations,
frequently producing unrelated or incoherent outputs. The pattern suggests a
structural limitation: models heavily leverage generic visual invariances but
under rely on compositional priors needed for robust literacy. We release
stimuli generation code, prompts, and evaluation protocols to facilitate
transparent replication and follow up work. Our findings motivate architectures
and training strategies that encode symbol segmentation, composition, and
binding across scripts, and they delineate concrete challenges for deploying
multimodal systems in education, accessibility, cultural heritage, and
security.

</details>


### [230] [Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories](https://arxiv.org/abs/2509.06998)
*Liviu Nicolae Fircă,Antonio Bărbălau,Dan Oneata,Elena Burceanu*

Main category: cs.CV

TL;DR: 文章首次评估属性预测任务在语义和感知不同类别下的鲁棒性，引入多种训练测试集划分策略，结果显示性能随训练和测试类别相关性降低而下降，聚类方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探究模型能否跨语义和感知不同的类别泛化属性知识，评估当前模型在属性预测任务中的鲁棒性。

Method: 引入基于大语言模型语义分组、嵌入相似度阈值、基于嵌入的聚类和基于真实标签的超类别划分等训练测试集划分策略。

Result: 随着训练和测试类别相关性降低，模型性能急剧下降，聚类方法在减少隐藏相关性和保持可学习性方面取得最有效平衡。

Conclusion: 揭示了当前表征的局限性，为未来属性推理基准构建提供了新见解。

Abstract: Can models generalize attribute knowledge across semantically and
perceptually dissimilar categories? While prior work has addressed attribute
prediction within narrow taxonomic or visually similar domains, it remains
unclear whether current models can abstract attributes and apply them to
conceptually distant categories. This work presents the first explicit
evaluation for the robustness of the attribute prediction task under such
conditions, testing whether models can correctly infer shared attributes
between unrelated object types: e.g., identifying that the attribute "has four
legs" is common to both "dogs" and "chairs". To enable this evaluation, we
introduce train-test split strategies that progressively reduce correlation
between training and test sets, based on: LLM-driven semantic grouping,
embedding similarity thresholding, embedding-based clustering, and
supercategory-based partitioning using ground-truth labels. Results show a
sharp drop in performance as the correlation between training and test
categories decreases, indicating strong sensitivity to split design. Among the
evaluated methods, clustering yields the most effective trade-off, reducing
hidden correlations while preserving learnability. These findings offer new
insights into the limitations of current representations and inform future
benchmark construction for attribute reasoning.

</details>


### [231] [Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models](https://arxiv.org/abs/2509.07010)
*Ahmed R. Sadik,Mariusz Bujny*

Main category: cs.CV

TL;DR: 本文提出人在回路框架定量评估大语言模型生成的3D模型，用多种指标对比，发现代码提示生成效果好，定量评估收敛更快。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成3D形状能力增强，但评估其几何和结构保真度的可靠方法不足，需开展相关研究以支持CAD设计等应用。

Method: 引入人在回路框架，提出相似度和复杂度指标，以L支架为案例，对比四种输入模态下大语言模型的表现。

Result: 语义丰富度增加可提高生成保真度，代码提示在所有指标上实现完美重建。

Conclusion: 提出的定量评估方法比传统定性方法收敛更快，有助于推进对AI辅助形状合成的理解，为CAD应用提供可扩展方法。

Abstract: Large Language Models are increasingly capable of interpreting multimodal
inputs to generate complex 3D shapes, yet robust methods to evaluate geometric
and structural fidelity remain underdeveloped. This paper introduces a human in
the loop framework for the quantitative evaluation of LLM generated 3D models,
supporting applications such as democratization of CAD design, reverse
engineering of legacy designs, and rapid prototyping. We propose a
comprehensive suite of similarity and complexity metrics, including volumetric
accuracy, surface alignment, dimensional fidelity, and topological intricacy,
to benchmark generated models against ground truth CAD references. Using an L
bracket component as a case study, we systematically compare LLM performance
across four input modalities: 2D orthographic views, isometric sketches,
geometric structure trees, and code based correction prompts. Our findings
demonstrate improved generation fidelity with increased semantic richness, with
code level prompts achieving perfect reconstruction across all metrics. A key
contribution of this work is demonstrating that our proposed quantitative
evaluation approach enables significantly faster convergence toward the ground
truth, especially compared to traditional qualitative methods based solely on
visual inspection and human intuition. This work not only advances the
understanding of AI assisted shape synthesis but also provides a scalable
methodology to validate and refine generative models for diverse CAD
applications.

</details>


### [232] [MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning](https://arxiv.org/abs/2509.07021)
*Jiarui Chen,Yikeng Chen,Yingshuang Zou,Ye Huang,Peng Wang,Yuan Liu,Yujing Sun,Wenping Wang*

Main category: cs.CV

TL;DR: 提出MEGS²框架解决3DGS渲染内存瓶颈，实现内存压缩，实验显示内存减少效果好且渲染质量相当。


<details>
  <summary>Details</summary>
Motivation: 3DGS内存消耗高限制在边缘设备应用，现有压缩方法多只关注存储压缩，未解决渲染内存瓶颈。

Method: 联合优化总基元数和每个基元的参数，用轻量级任意方向球形高斯瓣代替球谐函数表示颜色，提出统一软剪枝框架。

Result: 与现有方法相比，实现50%静态VRAM减少和40%渲染VRAM减少，渲染质量相当。

Conclusion: MEGS²框架能有效解决3DGS渲染内存瓶颈，实现前所未有的内存压缩。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis
technique, but its high memory consumption severely limits its applicability on
edge devices. A growing number of 3DGS compression methods have been proposed
to make 3DGS more efficient, yet most only focus on storage compression and
fail to address the critical bottleneck of rendering memory. To address this
problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that
tackles this challenge by jointly optimizing two key factors: the total
primitive number and the parameters per primitive, achieving unprecedented
memory compression. Specifically, we replace the memory-intensive spherical
harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our
color representations. More importantly, we propose a unified soft pruning
framework that models primitive-number and lobe-number pruning as a single
constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a
50% static VRAM reduction and a 40% rendering VRAM reduction compared to
existing methods, while maintaining comparable rendering quality.

</details>


### [233] [Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models](https://arxiv.org/abs/2509.07027)
*Jisung Hwang,Jaihoon Kim,Minhyuk Sung*

Main category: cs.CV

TL;DR: 提出一种强制标准高斯性的正则化损失，适用于文本到图像模型潜在空间优化任务，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为文本到图像模型潜在空间的下游优化任务提供有效正则化方法。

Method: 将高维样本元素视为一维标准高斯变量，结合空间域基于矩的正则化和频谱域基于功率谱的正则化定义复合损失，并应用于随机排列输入。

Result: 现有基于高斯性的正则化都在统一框架内，新正则化在生成建模中表现更好，能防止奖励破解并加速收敛。

Conclusion: 提出的正则化损失有效，在文本到图像模型的生成建模中优于先前的高斯性正则化方法。

Abstract: We propose a novel regularization loss that enforces standard Gaussianity,
encouraging samples to align with a standard Gaussian distribution. This
facilitates a range of downstream tasks involving optimization in the latent
space of text-to-image models. We treat elements of a high-dimensional sample
as one-dimensional standard Gaussian variables and define a composite loss that
combines moment-based regularization in the spatial domain with power
spectrum-based regularization in the spectral domain. Since the expected values
of moments and power spectrum distributions are analytically known, the loss
promotes conformity to these properties. To ensure permutation invariance, the
losses are applied to randomly permuted inputs. Notably, existing
Gaussianity-based regularizations fall within our unified framework: some
correspond to moment losses of specific orders, while the previous
covariance-matching loss is equivalent to our spectral loss but incurs higher
time complexity due to its spatial-domain computation. We showcase the
application of our regularization in generative modeling for test-time reward
alignment with a text-to-image model, specifically to enhance aesthetics and
text alignment. Our regularization outperforms previous Gaussianity
regularization, effectively prevents reward hacking and accelerates
convergence.

</details>


### [234] [Automated Evaluation of Gender Bias Across 13 Large Multimodal Models](https://arxiv.org/abs/2509.07050)
*Juan Manuel Contreras*

Main category: cs.CV

TL;DR: 本文引入Aymara Image Fairness Evaluation评估AI生成图像的社会偏见，测试13个LMMs，发现模型会放大职业性别刻板印象，不同模型偏见程度差异大，强调标准化评估工具的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有工作在分析大型多模态模型（LMMs）性别偏见时存在方法局限，缺乏大规模、可比较的跨模型分析，需填补这一空白。

Method: 引入Aymara Image Fairness Evaluation基准，用75个程序生成的性别中立提示测试13个商用LMMs，生成不同职业的人物图像，用验证过的LLM评判系统对图像的性别表征打分。

Result: LMMs不仅会重现还会放大职业性别刻板印象，存在默认男性偏见，不同模型偏见程度差异大，表现最佳的模型能减轻刻板印象并接近性别平等。

Conclusion: 本研究提供了最全面的跨模型性别偏见基准，强调标准化、自动化评估工具对促进AI发展的责任和公平性的必要性。

Abstract: Large multimodal models (LMMs) have revolutionized text-to-image generation,
but they risk perpetuating the harmful social biases in their training data.
Prior work has identified gender bias in these models, but methodological
limitations prevented large-scale, comparable, cross-model analysis. To address
this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for
assessing social bias in AI-generated images. We test 13 commercially available
LMMs using 75 procedurally-generated, gender-neutral prompts to generate people
in stereotypically-male, stereotypically-female, and non-stereotypical
professions. We then use a validated LLM-as-a-judge system to score the 965
resulting images for gender representation. Our results reveal (p < .001 for
all): 1) LMMs systematically not only reproduce but actually amplify
occupational gender stereotypes relative to real-world labor data, generating
men in 93.0% of images for male-stereotyped professions but only 22.5% for
female-stereotyped professions; 2) Models exhibit a strong default-male bias,
generating men in 68.3% of the time for non-stereotyped professions; and 3) The
extent of bias varies dramatically across models, with overall male
representation ranging from 46.7% to 73.3%. Notably, the top-performing model
de-amplified gender stereotypes and approached gender parity, achieving the
highest fairness scores. This variation suggests high bias is not an inevitable
outcome but a consequence of design choices. Our work provides the most
comprehensive cross-model benchmark of gender bias to date and underscores the
necessity of standardized, automated evaluation tools for promoting
accountability and fairness in AI development.

</details>


### [235] [DIET-CP: Lightweight and Data Efficient Self Supervised Continued Pretraining](https://arxiv.org/abs/2509.06990)
*Bryan Rodas,Natalie Montesino,Jakob Ambsdorf,David Klindt,Randall Balestriero*

Main category: cs.CV

TL;DR: 提出DIET - CP继续预训练策略，适配基础模型到新领域。


<details>
  <summary>Details</summary>
Motivation: 专业领域数据集小，限制SSL方法应用，预训练模型缺乏继续预训练信息。

Method: 提出DIET - CP策略，以简单目标、无标签、不多于监督微调的超参数，引导模型适应新数据分布。

Result: 在不同数据模态和骨干网络选择上稳定，用1000张图像为DINOv3等模型提升性能。

Conclusion: DIET - CP策略可有效解决基础模型适配新领域的问题。

Abstract: Continued pretraining offers a promising solution for adapting foundation
models to a new target domain. However, in specialized domains, available
datasets are often very small, limiting the applicability of SSL methods
developed for large-scale pretraining and making hyperparameter search
infeasible. In addition, pretrained models are usually released as
backbone-weights only, lacking important information to continue pretraining.
We propose to bridge this gap with DIET-CP, a simple continued pretraining
strategy, where any strong foundation model can be steered towards the new data
distribution of interest. DIET-CP relies on a very simple objective, requires
no labels, and introduces no more hyperparameters than supervised finetuning.
It is stable across data modalities and backbone choices, while providing a
significant performance boost for state-of-the-art models such as DINOv3 using
only 1000 images.

</details>


### [236] [XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning](https://arxiv.org/abs/2509.07213)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

TL;DR: 提出XBusNet用于乳腺超声图像分割，在数据集上达SOTA，尤其对小病灶效果好。


<details>
  <summary>Details</summary>
Motivation: 精确乳腺超声分割有难度，直接应用文本图像线索易产生边界模糊结果，需新方法。

Method: 提出XBusNet，结合图像特征与临床文本，有全局和局部两条路径，用结构化元数据自动生成提示，在BLU数据集上用五折交叉验证评估。

Result: XBusNet在BLU数据集上达SOTA，小病灶改善明显，消融实验显示各模块互补。

Conclusion: 双提示、双分支多模态设计能实现准确分割，提高小的低对比度病灶的鲁棒性。

Abstract: Background: Precise breast ultrasound (BUS) segmentation supports reliable
measurement, quantitative analysis, and downstream classification, yet remains
difficult for small or low-contrast lesions with fuzzy margins and speckle
noise. Text prompts can add clinical context, but directly applying weakly
localized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce
coarse, blob-like responses that smear boundaries unless additional mechanisms
recover fine edges. Methods: We propose XBusNet, a novel dual-prompt,
dual-branch multimodal model that combines image features with clinically
grounded text. A global pathway based on a CLIP Vision Transformer encodes
whole-image semantics conditioned on lesion size and location, while a local
U-Net pathway emphasizes precise boundaries and is modulated by prompts that
describe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS)
terms. Prompts are assembled automatically from structured metadata, requiring
no manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using
five-fold cross-validation. Primary metrics are Dice and Intersection over
Union (IoU); we also conduct size-stratified analyses and ablations to assess
the roles of the global and local paths and the text-driven modulation.
Results: XBusNet achieves state-of-the-art performance on BLU, with mean Dice
of 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions
show the largest gains, with fewer missed regions and fewer spurious
activations. Ablation studies show complementary contributions of global
context, local boundary modeling, and prompt-based modulation. Conclusions: A
dual-prompt, dual-branch multimodal design that merges global semantics with
local precision yields accurate BUS segmentation masks and improves robustness
for small, low-contrast lesions.

</details>


### [237] [SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards](https://arxiv.org/abs/2509.07047)
*Kamyar Barakati,Utkarsh Pratiush,Sheryl L. Sanchez,Aditya Raghavan,Delia J. Milliron,Mahshid Ahmadi,Philip D. Rack,Sergei V. Kalinin*

Main category: cs.CV

TL;DR: 本文提出基于奖励函数优化基础模型用于显微镜图像分割，以SAM框架为例，得到优化变体SAM*，能用于实时流数据分割并在显微镜成像中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 基础模型用于图像分割时存在大量不透明调参需手动优化，限制其实时流数据分析可用性。

Method: 引入基于奖励函数的优化方法对基础模型进行微调，以Meta的SAM框架为例，构建代表成像系统物理特性的奖励函数。

Result: 得到优化变体SAM*，增强了SAM的适应性和性能，可用于实时流数据分割。

Conclusion: 该方法在显微镜成像中有效，能满足多样分割任务需求，对精确分析细胞结构等至关重要。

Abstract: Image segmentation is a critical task in microscopy, essential for accurately
analyzing and interpreting complex visual data. This task can be performed
using custom models trained on domain-specific datasets, transfer learning from
pre-trained models, or foundational models that offer broad applicability.
However, foundational models often present a considerable number of
non-transparent tuning parameters that require extensive manual optimization,
limiting their usability for real-time streaming data analysis. Here, we
introduce a reward function-based optimization to fine-tune foundational models
and illustrate this approach for SAM (Segment Anything Model) framework by
Meta. The reward functions can be constructed to represent the physics of the
imaged system, including particle size distributions, geometries, and other
criteria. By integrating a reward-driven optimization framework, we enhance
SAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$,
that better aligns with the requirements of diverse segmentation tasks and
particularly allows for real-time streaming data segmentation. We demonstrate
the effectiveness of this approach in microscopy imaging, where precise
segmentation is crucial for analyzing cellular structures, material interfaces,
and nanoscale features.

</details>


### [238] [Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion](https://arxiv.org/abs/2509.07277)
*Sepehr Salem,M. Moein Esfahani,Jingyu Liu,Vince Calhoun*

Main category: cs.CV

TL;DR: 提出基于DPM的数据增强框架用于热成像乳腺癌分类，融合特征训练的XGBoost分类器准确率和灵敏度高，验证了生成模型与可解释特征的协同作用。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像深度学习中数据稀缺问题。

Method: 使用DPM进行数据增强，融合预训练ResNet - 50的深度特征和U - Net分割肿瘤的手工非线性特征，用XGBoost分类器训练。

Result: XGBoost分类器准确率达98.0%，灵敏度达98.1%，消融研究和统计测试表明DPM增强和非线性特征融合是关键因素。

Conclusion: 先进生成模型和可解释特征的协同作用可用于创建高精度医疗诊断工具。

Abstract: Data scarcity hinders deep learning for medical imaging. We propose a
framework for breast cancer classification in thermograms that addresses this
using a Diffusion Probabilistic Model (DPM) for data augmentation. Our
DPM-based augmentation is shown to be superior to both traditional methods and
a ProGAN baseline. The framework fuses deep features from a pre-trained
ResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived
from U-Net segmented tumors. An XGBoost classifier trained on these fused
features achieves 98.0\% accuracy and 98.1\% sensitivity. Ablation studies and
statistical tests confirm that both the DPM augmentation and the nonlinear
feature fusion are critical, statistically significant components of this
success. This work validates the synergy between advanced generative models and
interpretable features for creating highly accurate medical diagnostic tools.

</details>


### [239] [Reconstruction Alignment Improves Unified Multimodal Models](https://arxiv.org/abs/2509.07295)
*Ji Xie,Trevor Darrell,Luke Zettlemoyer,XuDong Wang*

Main category: cs.CV

TL;DR: 提出资源高效的后训练方法RecA，提升统一多模态模型图像生成和编辑保真度。


<details>
  <summary>Details</summary>
Motivation: 传统统一多模态模型训练依赖的图像 - 文本对标注稀疏，缺少细粒度视觉细节。

Method: 提出Reconstruction Alignment (RecA) 方法，利用视觉理解编码器嵌入作为密集“文本提示”，通过自监督重建损失优化模型以重建输入图像。

Result: 仅用27 GPU - 小时，RecA在GenEval、DPGBench等基准测试中显著提升图像生成和编辑性能，超越许多更大的开源模型。

Conclusion: RecA是一种高效通用的统一多模态模型后训练对齐策略。

Abstract: Unified multimodal models (UMMs) unify visual understanding and generation
within a single architecture. However, conventional training relies on
image-text pairs (or sequences) whose captions are typically sparse and miss
fine-grained visual details--even when they use hundreds of words to describe a
simple image. We introduce Reconstruction Alignment (RecA), a
resource-efficient post-training method that leverages visual understanding
encoder embeddings as dense "text prompts," providing rich supervision without
captions. Concretely, RecA conditions a UMM on its own visual understanding
embeddings and optimizes it to reconstruct the input image with a
self-supervised reconstruction loss, thereby realigning understanding and
generation. Despite its simplicity, RecA is broadly applicable: across
autoregressive, masked-autoregressive, and diffusion-based UMMs, it
consistently improves generation and editing fidelity. With only 27 GPU-hours,
post-training with RecA substantially improves image generation performance on
GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while
also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit
6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models
and applies broadly across diverse UMM architectures, establishing it as an
efficient and general post-training alignment strategy for UMMs

</details>


### [240] [Dimensionally Reduced Open-World Clustering: DROWCULA](https://arxiv.org/abs/2509.07184)
*Erencem Ozbey,Dimitrios I. Diochnos*

Main category: cs.CV

TL;DR: 本文聚焦图像分类，提出全监督方法解决特定数据集中确定新类别的问题，利用视觉变换器和流形学习技术提升图像聚类性能，在多个数据集上取得新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 标注数据需大量人力，在‘开放世界’中，新类别的实例可能随时出现，此前研究多聚焦半监督方法，本文希望提出全监督方法解决问题。

Method: 利用视觉变换器估计簇的数量生成向量嵌入，结合流形学习技术利用数据的内在几何结构细化嵌入，提升图像聚类性能。

Result: 在CIFAR - 10、CIFAR - 100、ImageNet - 100和Tiny ImageNet数据集的单模态聚类和新类别发现任务上取得新的最优结果，无论簇的数量是否提前已知。

Conclusion: 所提出的全监督方法在图像分类的新类别发现问题上表现出色，具有有效性和先进性，代码已开源。

Abstract: Working with annotated data is the cornerstone of supervised learning.
Nevertheless, providing labels to instances is a task that requires significant
human effort. Several critical real-world applications make things more
complicated because no matter how many labels may have been identified in a
task of interest, it could be the case that examples corresponding to novel
classes may appear in the future. Not unsurprisingly, prior work in this,
so-called, `open-world' context has focused a lot on semi-supervised
approaches.
  Focusing on image classification, somehow paradoxically, we propose a fully
unsupervised approach to the problem of determining the novel categories in a
particular dataset. Our approach relies on estimating the number of clusters
using Vision Transformers, which utilize attention mechanisms to generate
vector embeddings. Furthermore, we incorporate manifold learning techniques to
refine these embeddings by exploiting the intrinsic geometry of the data,
thereby enhancing the overall image clustering performance. Overall, we
establish new State-of-the-Art results on single-modal clustering and Novel
Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do
so, both when the number of clusters is known or unknown ahead of time. The
code is available at: https://github.com/DROWCULA/DROWCULA.

</details>


### [241] [DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion](https://arxiv.org/abs/2509.07327)
*Shucong Li,Zhenyu Liu,Zijie Hong,Zhiheng Zhou,Xianghai Cao*

Main category: cs.CV

TL;DR: 针对无人机多光谱目标检测面临的挑战，提出带双域增强和优先级引导曼巴融合的检测器DEPF，实验表明其检测效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有无人机多光谱遥感目标检测面临低光图像融合互补性低、局部小目标建模受冗余信息干扰、基于Transformer方法计算复杂度高难以应用于无人机平台等问题。

Method: 提出DEPF，设计双域增强模块（DDE）增强低光遥感图像，设计优先级引导曼巴融合模块（PGMF）增强局部目标建模并减少冗余信息影响。

Result: 在DroneVehicle和VEDAI数据集上实验，DEPF与现有先进方法相比表现良好。

Conclusion: DEPF能有效解决无人机多光谱目标检测面临的问题，可用于实际检测任务。

Abstract: Multispectral remote sensing object detection is one of the important
application of unmanned aerial vehicle (UAV). However, it faces three
challenges. Firstly, the low-light remote sensing images reduce the
complementarity during multi-modality fusion. Secondly, the local small target
modeling is interfered with redundant information in the fusion stage easily.
Thirdly, due to the quadratic computational complexity, it is hard to apply the
transformer-based methods on the UAV platform. To address these limitations,
motivated by Mamba with linear complexity, a UAV multispectral object detector
with dual-domain enhancement and priority-guided mamba fusion (DEPF) is
proposed. Firstly, to enhance low-light remote sensing images, Dual-Domain
Enhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba
(CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba
scanning for the low-frequency components to enhance the global brightness of
images, while FDR constructs spectrum recovery network to enhance the frequency
spectra features for recovering the texture-details. Secondly, to enhance local
target modeling and reduce the impact of redundant information during fusion,
Priority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the
concept of priority scanning, which starts from local targets features
according to the priority scores obtained from modality difference. Experiments
on DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on
object detection, comparing with state-of-the-art methods. Our code is
available in the supplementary material.

</details>


### [242] [Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting](https://arxiv.org/abs/2509.07456)
*Sai Siddhartha Chary Aylapuram,Veeraraju Elluru,Shivang Agarwal*

Main category: cs.CV

TL;DR: 本文研究偏差感知的机器遗忘，评估多种策略，通过实验证明事后遗忘可大幅减少子组差异，且能在不重新训练的情况下提高视觉系统公平性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络依赖训练数据中的虚假关联，导致关键领域预测有偏差或不公平，传统偏差缓解方法需从头训练或重新设计数据管道，而机器遗忘为事后模型修正提供新途径。

Method: 基于隐私保护的遗忘技术，评估梯度上升、LoRA和师生蒸馏等策略。

Result: 事后遗忘能大幅减少子组差异，在三个数据集上人口统计学平等性有显著提升，且准确性损失最小，联合评估得分平均0.62。

Conclusion: 机器遗忘是一种无需完全重新训练即可提高已部署视觉系统公平性的实用框架。

Abstract: Deep neural networks often rely on spurious correlations in training data,
leading to biased or unfair predictions in safety-critical domains such as
medicine and autonomous driving. While conventional bias mitigation typically
requires retraining from scratch or redesigning data pipelines, recent advances
in machine unlearning provide a promising alternative for post-hoc model
correction. In this work, we investigate \textit{Bias-Aware Machine
Unlearning}, a paradigm that selectively removes biased samples or feature
representations to mitigate diverse forms of bias in vision models. Building on
privacy-preserving unlearning techniques, we evaluate various strategies
including Gradient Ascent, LoRA, and Teacher-Student distillation. Through
empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias),
CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection),
we demonstrate that post-hoc unlearning can substantially reduce subgroup
disparities, with improvements in demographic parity of up to \textbf{94.86\%}
on CUB-200, \textbf{30.28\%} on CIFAR-10, and \textbf{97.37\%} on CelebA. These
gains are achieved with minimal accuracy loss and with methods scoring an
average of 0.62 across the 3 settings on the joint evaluation of utility,
fairness, quality, and privacy. Our findings establish machine unlearning as a
practical framework for enhancing fairness in deployed vision systems without
necessitating full retraining.

</details>


### [243] [MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification](https://arxiv.org/abs/2509.07477)
*Patrick Wienholt,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: 提出可自解释架构MedicalPatchNet用于胸部X光分类，性能与EfficientNet - B0相当，显著提升可解释性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在放射图像分类中可解释性差，限制临床接受度，需提升可解释性。

Method: 将图像分割为非重叠块，独立分类各块并聚合预测，无需事后技术。

Result: 在CheXpert数据集上分类性能与EfficientNet - B0相当（AUROC 0.907 vs. 0.908）；在CheXlocalize数据集上病理定位准确率更高（平均命中率0.485 vs. 0.376）。

Conclusion: MedicalPatchNet可缓解捷径学习风险，提高临床信任，有助于医学影像领域安全、可解释的AI辅助诊断。

Abstract: Deep neural networks excel in radiological image classification but
frequently suffer from poor interpretability, limiting clinical acceptance. We
present MedicalPatchNet, an inherently self-explainable architecture for chest
X-ray classification that transparently attributes decisions to distinct image
regions. MedicalPatchNet splits images into non-overlapping patches,
independently classifies each patch, and aggregates predictions, enabling
intuitive visualization of each patch's diagnostic contribution without
post-hoc techniques. Trained on the CheXpert dataset (223,414 images),
MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908)
of EfficientNet-B0, while substantially improving interpretability:
MedicalPatchNet demonstrates substantially improved interpretability with
higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with
Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable
explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks
associated with shortcut learning, thus improving clinical trust. Our model is
publicly available with reproducible training and inference scripts and
contributes to safer, explainable AI-assisted diagnostics across medical
imaging domains. We make the code publicly available:
https://github.com/TruhnLab/MedicalPatchNet

</details>


### [244] [Fine-Tuning Vision-Language Models for Visual Navigation Assistance](https://arxiv.org/abs/2509.07488)
*Xiao Li,Bharat Gandhi,Ming Zhan,Mohit Nehra,Zhicheng Zhang,Yuchen Sun,Meijia Song,Naisheng Zhang,Xi Wang*

Main category: cs.CV

TL;DR: 本文聚焦视觉语言驱动的室内导航，助力视障人士，通过整合视觉与语言模型生成导航指令，经LoRA微调模型及提出新评估指标，提升导航性能。


<details>
  <summary>Details</summary>
Motivation: 传统室内导航系统因缺乏精确位置数据而效果不佳，需要更好的方法帮助视障人士在室内导航。

Method: 在手动标注的室内导航数据集上用Low Rank Adaptation (LoRA)微调BLIP - 2模型；提出改进BERT F1分数的评估指标，强调方向和顺序变量。

Result: 应用LoRA后，模型在生成方向指令方面有显著提升，克服了原BLIP - 2模型的局限。

Conclusion: 整合视觉和语言模型并结合LoRA微调及新评估指标，能有效提升室内导航性能，增强视障人士的可达性和独立性。

Abstract: We address vision-language-driven indoor navigation to assist visually
impaired individuals in reaching a target location using images and natural
language guidance. Traditional navigation systems are ineffective indoors due
to the lack of precise location data. Our approach integrates vision and
language models to generate step-by-step navigational instructions, enhancing
accessibility and independence. We fine-tune the BLIP-2 model with Low Rank
Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose
an evaluation metric that refines the BERT F1 score by emphasizing directional
and sequential variables, providing a more comprehensive measure of
navigational performance. After applying LoRA, the model significantly improved
in generating directional instructions, overcoming limitations in the original
BLIP-2 model.

</details>


### [245] [Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition](https://arxiv.org/abs/2509.07495)
*Chun Liu,Hailong Wang,Bingqian Zhu,Panpan Ding,Zheng Zheng,Tao Xu,Zhigang Han,Jiayao Wang*

Main category: cs.CV

TL;DR: 研究提出针对DNNs非目标攻击的新框架，结合局部混合和logits优化，实验显示其性能优于12种先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于混合的策略存在破坏全局语义特征、梯度消失等问题，影响对抗样本质量和可迁移性。

Method: 提出局部混合策略生成输入；将目标攻击的logit损失应用于非目标场景；使用扰动平滑损失抑制高频噪声。

Result: 在FGSCR - 42和MTARSI数据集上，性能优于12种先进方法，在MTARSI上以ResNet为替代模型时，黑盒攻击成功率平均提升17.28%。

Conclusion: 所提新框架有效解决现有策略的局限性，提升了对抗攻击性能。

Abstract: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing
significant security threats to their deployment in remote sensing
applications. Research on adversarial attacks not only reveals model
vulnerabilities but also provides critical insights for enhancing robustness.
Although current mixing-based strategies have been proposed to increase the
transferability of adversarial examples, they either perform global blending or
directly exchange a region in the images, which may destroy global semantic
features and mislead the optimization of adversarial examples. Furthermore,
their reliance on cross-entropy loss for perturbation optimization leads to
gradient diminishing during iterative updates, compromising adversarial example
quality. To address these limitations, we focus on non-targeted attacks and
propose a novel framework via local mixing and logits optimization. First, we
present a local mixing strategy to generate diverse yet semantically consistent
inputs. Different from MixUp, which globally blends two images, and MixCut,
which stitches images together, our method merely blends local regions to
preserve global semantic information. Second, we adapt the logit loss from
targeted attacks to non-targeted scenarios, mitigating the gradient vanishing
problem of cross-entropy loss. Third, a perturbation smoothing loss is applied
to suppress high-frequency noise and enhance transferability. Extensive
experiments on FGSCR-42 and MTARSI datasets demonstrate superior performance
over 12 state-of-the-art methods across 6 surrogate models. Notably, with
ResNet as the surrogate on MTARSI, our method achieves a 17.28% average
improvement in black-box attack success rate.

</details>


### [246] [EHWGesture -- A dataset for multimodal understanding of clinical gestures](https://arxiv.org/abs/2509.07525)
*Gianluca Amprimo,Alberto Ancilotto,Alessandro Savino,Fabio Quazzolo,Claudia Ferraris,Gabriella Olmo,Elisabetta Farella,Stefano Di Carlo*

Main category: cs.CV

TL;DR: 本文介绍了用于手势理解的多模态视频数据集EHWGesture，含5种临床相关手势，有高精度设备采集数据，可用于手势分类等任务，能作为临床手势理解基准。


<details>
  <summary>Details</summary>
Motivation: 动态手势理解因复杂时空变化具挑战，现有数据集存在缺乏多模态、多视图多样性等问题，论文旨在解决这些问题，推动临床手势理解。

Method: 引入EHWGesture数据集，用两台高分辨率RGB - Depth相机和事件相机从25名健康受试者处采集超1100条记录，用动作捕捉系统提供精确手部地标跟踪，设备空间校准和同步，按执行速度分类记录。

Result: 基线实验显示该数据集在手势分类、手势触发检测和动作质量评估方面有潜力。

Conclusion: EHWGesture可作为推进多模态临床手势理解的综合基准。

Abstract: Hand gesture understanding is essential for several applications in
human-computer interaction, including automatic clinical assessment of hand
dexterity. While deep learning has advanced static gesture recognition, dynamic
gesture understanding remains challenging due to complex spatiotemporal
variations. Moreover, existing datasets often lack multimodal and multi-view
diversity, precise ground-truth tracking, and an action quality component
embedded within gestures. This paper introduces EHWGesture, a multimodal video
dataset for gesture understanding featuring five clinically relevant gestures.
It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects
using two high-resolution RGB-Depth cameras and an event camera. A motion
capture system provides precise ground-truth hand landmark tracking, and all
devices are spatially calibrated and synchronized to ensure cross-modal
alignment. Moreover, to embed an action quality task within gesture
understanding, collected recordings are organized in classes of execution speed
that mirror clinical evaluations of hand dexterity. Baseline experiments
highlight the dataset's potential for gesture classification, gesture trigger
detection, and action quality assessment. Thus, EHWGesture can serve as a
comprehensive benchmark for advancing multimodal clinical gesture
understanding.

</details>


### [247] [Nearest Neighbor Projection Removal Adversarial Training](https://arxiv.org/abs/2509.07673)
*Himanshu Singh,A. V. Subramanyam,Shivank Rajput,Mohan Kankanhalli*

Main category: cs.CV

TL;DR: 提出新对抗训练框架缓解类间特征重叠，理论证明可降低Lipschitz常数，实验显示在多基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 标准对抗训练未明确解决类间特征重叠问题，而这是对抗样本易感性的重要因素。

Method: 提出新对抗训练框架，识别对抗样本最近类间邻居并去除投影，进行logits校正。

Result: 在CIFAR - 10、CIFAR - 100和SVHN等基准测试中，该方法表现与领先对抗训练技术相当，在鲁棒和干净准确率上有显著成果。

Conclusion: 明确解决类间特征接近性对增强DNN对抗鲁棒性很重要。

Abstract: Deep neural networks have exhibited impressive performance in image
classification tasks but remain vulnerable to adversarial examples. Standard
adversarial training enhances robustness but typically fails to explicitly
address inter-class feature overlap, a significant contributor to adversarial
susceptibility. In this work, we introduce a novel adversarial training
framework that actively mitigates inter-class proximity by projecting out
inter-class dependencies from adversarial and clean samples in the feature
space. Specifically, our approach first identifies the nearest inter-class
neighbors for each adversarial sample and subsequently removes projections onto
these neighbors to enforce stronger feature separability. Theoretically, we
demonstrate that our proposed logits correction reduces the Lipschitz constant
of neural networks, thereby lowering the Rademacher complexity, which directly
contributes to improved generalization and robustness. Extensive experiments
across standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that
our method demonstrates strong performance that is competitive with leading
adversarial training techniques, highlighting significant achievements in both
robust and clean accuracy. Our findings reveal the importance of addressing
inter-class feature proximity explicitly to bolster adversarial robustness in
DNNs.

</details>


### [248] [CAViAR: Critic-Augmented Video Agentic Reasoning](https://arxiv.org/abs/2509.07680)
*Sachit Menon,Ahmet Iscen,Arsha Nagrani,Tobias Weyand,Carl Vondrick,Cordelia Schmid*

Main category: cs.CV

TL;DR: 现有视频模型在复杂推理任务表现不佳，本文开发大语言模型代理结合视频模块和引入评判器，在相关数据集上取得良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型在复杂推理任务上性能下降，探索能否利用现有感知能力进行更复杂视频推理。

Method: 开发大语言模型代理，将视频模块作为子代理或工具，代理根据模块调用结果决定后续步骤，引入评判器区分成功和失败序列。

Result: 代理和评判器的组合在相关数据集上取得强性能。

Conclusion: 提出的方法能有效利用现有感知能力进行更复杂视频推理。

Abstract: Video understanding has seen significant progress in recent years, with
models' performance on perception from short clips continuing to rise. Yet,
multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show
performance wanes for tasks requiring complex reasoning on videos as queries
grow more complex and videos grow longer. In this work, we ask: can existing
perception capabilities be leveraged to successfully perform more complex video
reasoning? In particular, we develop a large language model agent given access
to video modules as subagents or tools. Rather than following a fixed procedure
to solve queries as in previous work such as Visual Programming, ViperGPT, and
MoReVQA, the agent uses the results of each call to a module to determine
subsequent steps. Inspired by work in the textual reasoning domain, we
introduce a critic to distinguish between instances of successful and
unsuccessful sequences from the agent. We show that the combination of our
agent and critic achieve strong performance on the previously-mentioned
datasets.

</details>


### [249] [HU-based Foreground Masking for 3D Medical Masked Image Modeling](https://arxiv.org/abs/2509.07534)
*Jin Lee,Vu Dang,Gwang-Hyun Yu,Anh Le,Zahid Rahman,Jin-Ho Jang,Heonzoo Lee,Kun-Yung Kim,Jin-Sul Kim,Jin-Young Kim*

Main category: cs.CV

TL;DR: 提出基于HU的前景掩码策略改进3D医学图像计算中MIM，实验显示在多个数据集提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 3D医学图像计算中MIM采用随机掩码方法忽略解剖对象密度，限制其应用，需改进。

Method: 利用Hounsfield Unit测量，实现基于HU的前景掩码，关注内脏器官强度分布，排除无诊断特征的非组织区域。

Result: 在五个公共3D医学成像数据集上实验，掩码策略持续提升分割质量和Dice分数。

Conclusion: 强调领域中心MIM的重要性，为医学图像分割的表征学习提供有前景的方向。

Abstract: While Masked Image Modeling (MIM) has revolutionized fields of computer
vision, its adoption in 3D medical image computing has been limited by the use
of random masking, which overlooks the density of anatomical objects. To
address this limitation, we enhance the pretext task with a simple yet
effective masking strategy. Leveraging Hounsfield Unit (HU) measurements, we
implement an HU-based Foreground Masking, which focuses on the intensity
distribution of visceral organs and excludes non-tissue regions, such as air
and fluid, that lack diagnostically meaningful features. Extensive experiments
on five public 3D medical imaging datasets demonstrate that our masking
consistently improves performance, both in quality of segmentation and Dice
score (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%,
BraTS:~78.55\%). These results underscore the importance of domain-centric MIM
and suggest a promising direction for representation learning in medical image
segmentation. Implementation is available at github.com/AISeedHub/SubFore/.

</details>


### [250] [Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks](https://arxiv.org/abs/2509.07581)
*Barkin Buyukcakir,Rocharles Cavalcante Fontenele,Reinhilde Jacobs,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 本文提出CGAT架构用于3D形状识别，通过图注意力卷积和注意力机制解释决策过程，分析特征与模型设置，提升了性能并促进透明深度学习模型在高风险环境应用。


<details>
  <summary>Details</summary>
Motivation: 深度学习黑盒特性阻碍其在高风险应用中的采用，需要可解释模型用于3D形状识别任务。

Method: 引入Class Node Graph Attention Network (CGAT) 架构，利用图注意力卷积和注意力机制，评估不同节点特征和模型深度。

Result: 结合局部平均曲率和到质心距离作为节点特征，加权F1分数达0.76，生成更直观注意力图和理想分类性能。

Conclusion: CGAT能生成可理解注意力图，增强信任和专家验证，适用于图基分类和回归任务，推动透明模型在高风险环境的广泛应用。

Abstract: Deep learning offers a promising avenue for automating many recognition tasks
in fields such as medicine and forensics. However, the black-box nature of
these models hinders their adoption in high-stakes applications where trust and
accountability are required. For 3D shape recognition tasks in particular, this
paper introduces the Class Node Graph Attention Network (CGAT) architecture to
address this need. Applied to 3D meshes of third molars derived from CBCT
images, for Demirjian stage allocation, CGAT utilizes graph attention
convolutions and an inherent attention mechanism, visualized via attention
rollout, to explain its decision-making process. We evaluated the local mean
curvature and distance to centroid node features, both individually and in
combination, as well as model depth, finding that models incorporating directed
edges to a global CLS node produced more intuitive attention maps, while also
yielding desirable classification performance. We analyzed the attention-based
explanations of the models, and their predictive performances to propose
optimal settings for the CGAT. The combination of local mean curvature and
distance to centroid as node features yielded a slight performance increase
with 0.76 weighted F1 score, and more comprehensive attention visualizations.
The CGAT architecture's ability to generate human-understandable attention maps
can enhance trust and facilitate expert validation of model decisions. While
demonstrated on dental data, CGAT is broadly applicable to graph-based
classification and regression tasks, promoting wider adoption of transparent
and competitive deep learning models in high-stakes environments.

</details>


### [251] [Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s](https://arxiv.org/abs/2509.07928)
*Mahmudul Islam Masum,Miad Islam,Arif I. Sarwat*

Main category: cs.CV

TL;DR: 论文指出目标检测器在消费级硬件上性能有差距，提出二阶段自适应推理算法，在COCO数据集上加速1.85倍，mAP损失5.51%，提供在消费级设备部署高性能实时AI的方案。


<details>
  <summary>Details</summary>
Motivation: 目标检测器在消费级硬件上的基准性能与实际可用性存在差距，资源受限系统存在系统级瓶颈。

Method: 引入与模型无关的二阶段自适应推理算法，进行自适应推理策略研究和对比分析，系统采用低分辨率快速通过，低检测置信度时采用高分辨率模型。

Result: 在5000张图像的COCO数据集上，比PyTorch早期退出基线加速1.85倍，mAP损失5.51%。

Conclusion: 将重点从纯模型优化转向硬件感知推理策略，为在消费级设备上部署高性能实时AI提供实用且可复现的方案。

Abstract: As local AI grows in popularity, there is a critical gap between the
benchmark performance of object detectors and their practical viability on
consumer-grade hardware. While models like YOLOv10s promise real-time speeds,
these metrics are typically achieved on high-power, desktop-class GPUs. This
paper reveals that on resource-constrained systems, such as laptops with RTX
4060 GPUs, performance is not compute-bound but is instead dominated by
system-level bottlenecks, as illustrated by a simple bottleneck test. To
overcome this hardware-level constraint, we introduce a Two-Pass Adaptive
Inference algorithm, a model-independent approach that requires no
architectural changes. This study mainly focuses on adaptive inference
strategies and undertakes a comparative analysis of architectural early-exit
and resolution-adaptive routing, highlighting their respective trade-offs
within a unified evaluation framework. The system uses a fast, low-resolution
pass and only escalates to a high-resolution model pass when detection
confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x
speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%.
This work provides a practical and reproducible blueprint for deploying
high-performance, real-time AI on consumer-grade devices by shifting the focus
from pure model optimization to hardware-aware inference strategies that
maximize throughput.

</details>


### [252] [XSRD-Net: EXplainable Stroke Relapse Detection](https://arxiv.org/abs/2509.07772)
*Christian Gapp,Elias Tappeiner,Martin Welk,Karl Fritscher,Stephanie Mangesius,Constantin Eisenschink,Philipp Deisl,Michael Knoflach,Astrid E. Grams,Elke R. Gizewski,Rainer Schubert*

Main category: cs.CV

TL;DR: 文章聚焦降低中风复发率，收集相关数据训练单模态和多模态神经网络，在复发检测和无复发生存时间预测任务取得一定成果，发现心脏病和颈动脉与复发检测及预测有关。


<details>
  <summary>Details</summary>
Motivation: 中风死亡率高且复发率不低，复发死亡率极高，需降低复发率，早期检测中风复发风险患者以规划治疗。

Method: 收集2010 - 2024年中风患者3D颅内CTA图像数据、心脏病史、年龄和性别信息，训练单模态和多模态深度学习神经网络进行二元复发检测和无复发生存时间预测及后续分类。

Result: 任务1用表格数据解决复发与非复发患者分离，测试集AUC为0.84；任务2多模态XSRD - net按模态贡献处理视觉和表格数据，多模态模型复发c指数达0.68，测试集AUC为0.71。

Conclusion: 心脏病和颈动脉与复发检测和无复发生存时间预测存在关联，后续将持续收集数据和重新训练模型强化该结果。

Abstract: Stroke is the second most frequent cause of death world wide with an annual
mortality of around 5.5 million. Recurrence rates of stroke are between 5 and
25% in the first year. As mortality rates for relapses are extraordinarily high
(40%) it is of utmost importance to reduce the recurrence rates. We address
this issue by detecting patients at risk of stroke recurrence at an early stage
in order to enable appropriate therapy planning. To this end we collected 3D
intracranial CTA image data and recorded concomitant heart diseases, the age
and the gender of stroke patients between 2010 and 2024. We trained single- and
multimodal deep learning based neural networks for binary relapse detection
(Task 1) and for relapse free survival (RFS) time prediction together with a
subsequent classification (Task 2). The separation of relapse from non-relapse
patients (Task 1) could be solved with tabular data (AUC on test dataset:
0.84). However, for the main task, the regression (Task 2), our multimodal
XSRD-net processed the modalities vision:tabular with 0.68:0.32 according to
modality contribution measures. The c-index with respect to relapses for the
multimodal model reached 0.68, and the AUC is 0.71 for the test dataset. Final,
deeper interpretability analysis results could highlight a link between both
heart diseases (tabular) and carotid arteries (vision) for the detection of
relapses and the prediction of the RFS time. This is a central outcome that we
strive to strengthen with ongoing data collection and model retraining.

</details>


### [253] [Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets](https://arxiv.org/abs/2509.07852)
*Seyd Teymoor Seydi*

Main category: cs.CV

TL;DR: 本文提出结合AlphaEArth数据集与Siamese U - Net深度学习架构进行自动燃烧区域制图，实验效果好，有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 准确及时的燃烧区域制图对环境监测、灾害管理和气候变化评估至关重要。

Method: 使用AlphaEArth数据集结合Siamese U - Net深度学习架构，用MTBS数据集训练，在欧洲17个地区评估。

Result: 提出的集成方法在测试数据集上总体准确率达95%，IoU为0.6，F1分数为74%，能识别不同生态系统的燃烧区域。

Conclusion: 该研究推动自动火灾损失评估发展，为全球燃烧区域监测提供可扩展解决方案。

Abstract: Accurate and timely mapping of burned areas is crucial for environmental
monitoring, disaster management, and assessment of climate change. This study
presents a novel approach to automated burned area mapping using the AlphaEArth
dataset combined with the Siamese U-Net deep learning architecture. The
AlphaEArth Dataset, comprising high-resolution optical and thermal infrared
imagery with comprehensive ground-truth annotations, provides an unprecedented
resource for training robust burned area detection models. We trained our model
with the Monitoring Trends in Burn Severity (MTBS) dataset in the contiguous US
and evaluated it with 17 regions cross in Europe. Our experimental results
demonstrate that the proposed ensemble approach achieves superior performance
with an overall accuracy of 95%, IoU of 0.6, and F1-score of 74% on the test
dataset. The model successfully identifies burned areas across diverse
ecosystems with complex background, showing particular strength in detecting
partially burned vegetation and fire boundaries and its transferability and
high generalization in burned area mapping. This research contributes to the
advancement of automated fire damage assessment and provides a scalable
solution for global burn area monitoring using the AlphaEarth dataset.

</details>


### [254] [Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning](https://arxiv.org/abs/2509.07879)
*Daniel DeAlcala,Aythami Morales,Julian Fierrez,Gonzalo Mancera,Ruben Tolosana,Javier Ortega-Garcia*

Main category: cs.CV

TL;DR: 提出Active Membership Inference Test (aMINT)方法检测数据是否用于模型训练，多任务学习，在多个基准测试中表现好，提高AI模型透明度。


<details>
  <summary>Details</summary>
Motivation: 提高机器学习模型的可审计性和透明度，实现数据使用的检测，保障安全、隐私和版权。

Method: 采用多任务学习，同时训练原始模型和MINT模型，将中间激活图作为MINT层输入。

Result: 在5个公共基准测试中，使用多种神经网络，aMINT检测数据是否用于训练的准确率超80%，优于现有方法。

Conclusion: aMINT及其方法发展有助于提高AI模型透明度，在AI部署中实现更强保障。

Abstract: Active Membership Inference Test (aMINT) is a method designed to detect
whether given data were used during the training of machine learning models. In
Active MINT, we propose a novel multitask learning process that involves
training simultaneously two models: the original or Audited Model, and a
secondary model, referred to as the MINT Model, responsible for identifying the
data used for training the Audited Model. This novel multi-task learning
approach has been designed to incorporate the auditability of the model as an
optimization objective during the training process of neural networks. The
proposed approach incorporates intermediate activation maps as inputs to the
MINT layers, which are trained to enhance the detection of training data. We
present results using a wide range of neural networks, from lighter
architectures such as MobileNet to more complex ones such as Vision
Transformers, evaluated in 5 public benchmarks. Our proposed Active MINT
achieves over 80% accuracy in detecting if given data was used for training,
significantly outperforming previous approaches in the literature. Our aMINT
and related methodological developments contribute to increasing transparency
in AI models, facilitating stronger safeguards in AI deployments to achieve
proper security, privacy, and copyright protection.

</details>


### [255] [Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation](https://arxiv.org/abs/2509.07923)
*Moo Hyun Son,Juyoung Bae,Zelin Qiu,Jiale Peng,Kai Xin Li,Yifan Lin,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出ToothMCL多模态预训练框架用于牙齿分割，构建了最大的配对数据集CBCT - IOS3.8K，在多个独立数据集上评估，取得了SOTA性能，泛化性强。


<details>
  <summary>Details</summary>
Motivation: 现有牙齿分割方法缺乏严格验证，性能和临床适用性有限，需要更好的方法。

Method: 提出ToothMCL多模态对比学习预训练框架，整合CBCT和IOS模态；构建了最大的配对数据集CBCT - IOS3.8K；在多个独立数据集上评估。

Result: 在内部和外部测试中达到SOTA性能，CBCT分割的DSC提高12%，IOS分割的DSC提高8%；在牙齿组分类上超越现有方法，在不同成像条件和临床场景下泛化性强。

Conclusion: ToothMCL多模态预训练框架在牙齿分割任务中表现出色，具有良好的性能和泛化性。

Abstract: Digital dentistry represents a transformative shift in modern dental
practice. The foundational step in this transformation is the accurate digital
representation of the patient's dentition, which is obtained from segmented
Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the
growing interest in digital dental technologies, existing segmentation
methodologies frequently lack rigorous validation and demonstrate limited
performance and clinical applicability. To the best of our knowledge, this is
the first work to introduce a multimodal pretraining framework for tooth
segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for
pretraining that integrates volumetric (CBCT) and surface-based (IOS)
modalities. By capturing modality-invariant representations through multimodal
contrastive learning, our approach effectively models fine-grained anatomical
features, enabling precise multi-class segmentation and accurate identification
of F\'ed\'eration Dentaire Internationale (FDI) tooth numbering. Along with the
framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to
date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive
collection of independent datasets, representing the largest and most diverse
evaluation to date. Our method achieves state-of-the-art performance in both
internal and external testing, with an increase of 12\% for CBCT segmentation
and 8\% for IOS segmentation in the Dice Similarity Coefficient (DSC).
Furthermore, ToothMCL consistently surpasses existing approaches in tooth
groups and demonstrates robust generalizability across varying imaging
conditions and clinical scenarios.

</details>


### [256] [Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search](https://arxiv.org/abs/2509.07969)
*Xin Lai,Junyi Li,Wei Li,Tao Liu,Tianjian Li,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 现有开源方法处理视觉问题推理模式单调、交互轮数有限，本文提出 Mini - o3 系统，通过三项关键组件提升性能，实验证明其能有效解决复杂视觉搜索问题。


<details>
  <summary>Details</summary>
Motivation: 现有开源方法在处理需要反复探索的困难视觉任务时，推理模式单调且交互轮数有限，无法满足需求。

Method: 1. 构建视觉探测数据集；2. 开发迭代数据收集管道获取冷启动轨迹；3. 提出过轮屏蔽策略平衡训练效率和测试扩展性。

Result: 模型在推理时轨迹能自然扩展到数十轮，且准确率随轮数增加而提高。

Conclusion: Mini - o3 能产生丰富推理模式和深度思考路径，有效解决具有挑战性的视觉搜索问题。

Abstract: Recent advances in large multimodal models have leveraged image-based tools
with reinforcement learning to tackle visual problems. However, existing
open-source approaches often exhibit monotonous reasoning patterns and allow
only a limited number of interaction turns, making them inadequate for
difficult tasks that require trial-and-error exploration. In this work, we
address this limitation by scaling up tool-based interactions and introduce
Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of
steps -- and achieves state-of-the-art performance on challenging visual search
tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key
components. First, we construct the Visual Probe Dataset, a collection of
thousands of challenging visual search problems designed for exploratory
reasoning. Second, we develop an iterative data collection pipeline to obtain
cold-start trajectories that exhibit diverse reasoning patterns, including
depth-first search, trial-and-error, and goal maintenance. Third, we propose an
over-turn masking strategy that prevents penalization of over-turn responses
(those that hit the maximum number of turns) during reinforcement learning,
thereby balancing training-time efficiency with test-time scalability. Despite
training with an upper bound of only six interaction turns, our model generates
trajectories that naturally scale to tens of turns at inference time, with
accuracy improving as the number of turns increases. Extensive experiments
demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking
paths, effectively solving challenging visual search problems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [257] [Cross-device Zero-shot Label Transfer via Alignment of Time Series Foundation Model Embeddings](https://arxiv.org/abs/2509.06966)
*Neal G. Ravindra,Arijit Sehanobish*

Main category: eess.SP

TL;DR: 本文提出一种无需配对数据，将临床活动记录仪的标签迁移到苹果手表等消费级可穿戴设备数据上的框架。


<details>
  <summary>Details</summary>
Motivation: 临床活动记录仪数据有高质量医学验证标签，但消费级可穿戴设备数据无此类标签，手动标注成本高且难扩展。

Method: 使用时间序列基础模型将源域和目标域投影到共享潜在嵌入空间，开发新框架对齐跨设备表示，通过Adversarial Alignment of TSFM Embeddings使源和目标嵌入分布对齐。

Result: 未提及

Conclusion: 未提及

Abstract: High-quality, medically validated labels exist for clinical actigraphy data
but not for ubiquitous consumer wearables like the Apple Watch. Manually
labeling wearables data is expensive and doesn't scale. This paper offers a
novel framework that transfers valuable labels from a source domain (e.g.,
actigraphy) to a target domain (e.g., Apple Watch) without requiring paired
data. Instead of working with raw time-series signals, we project both domains
into a shared latent embedding space using time-series foundation models
(TSFMs) and develop a new framework to align the cross-device representations.
Our method, Adversarial Alignment of TSFM Embeddings forces the distributions
of source and target embeddings to align within this space, facilitating label
transfer across device type.

</details>


### [258] [Cross-field SNR Analysis and Tensor Channel Estimation for Multi-UAV Near-field Communications](https://arxiv.org/abs/2509.06967)
*Tianyu Huo,Jian Xiong,Yiyan Wu,Songjie Yang,Bo Liu,Wenjun Zhang*

Main category: eess.SP

TL;DR: 本文研究分布式近场多无人机通信系统的信道估计，推导三种模型下的信噪比表达式，提出两种信道估计算法，仿真显示tensor - OMP性能好且复杂度低。


<details>
  <summary>Details</summary>
Motivation: 分布式多无人机系统形成的分布式极大型天线阵列工作在近场，传统远场平面波假设失效，需研究近场信道估计。

Method: 推导PWM、SWM和HSPWM三种模型下分布式均匀平面阵列的信噪比表达式，提出SD - OMP和tensor - OMP两种信道估计算法。

Result: 仿真表明tensor - OMP与SD - OMP的归一化均方误差性能相当，但计算复杂度更低、可扩展性更好。

Conclusion: HSPWM在建模精度和分析易处理性上取得良好平衡，tensor - OMP是更优的信道估计算法。

Abstract: Extremely large antenna array (ELAA) is key to enhancing spectral efficiency
in 6G networks. Leveraging the distributed nature of multi-unmanned aerial
vehicle (UAV) systems enables the formation of distributed ELAA, which often
operate in the near-field region with spatial sparsity, rendering the
conventional far-field plane wave assumption invalid. This paper investigates
channel estimation for distributed near-field multi-UAV communication systems.
We first derive closed-form signal-to-noise ratio (SNR) expressions under the
plane wave model (PWM), spherical wave model (SWM), and a hybrid
spherical-plane wave model (HSPWM), also referred to as the cross-field model,
within a distributed uniform planar array (UPA) scenario. The analysis shows
that HSPWM achieves a good balance between modeling accuracy and analytical
tractability. Based on this, we propose two channel estimation algorithms: the
spherical-domain orthogonal matching pursuit (SD-OMP) and the tensor-OMP. The
SD-OMP generalizes the polar domain to jointly consider elevation, azimuth, and
range. Under the HSPWM, the channel is naturally formulated as a tensor,
enabling the use of tensor-OMP. Simulation results demonstrate that tensor-OMP
achieves normalized mean square error (NMSE) performance comparable to SD-OMP,
while offering reduced computational complexity and improved scalability.

</details>


### [259] [Deep Learning-based Techniques for Integrated Sensing and Communication Systems: State-of-the-Art, Challenges, and Opportunities](https://arxiv.org/abs/2509.06968)
*Murat Temiz,Yongwei Zhang,Yanwei Fu,Chi Zhang,Chenfeng Meng,Orhan Kaplan,Christos Masouros*

Main category: eess.SP

TL;DR: 文章综述基于深度学习的集成传感与通信（ISAC）技术，介绍其优势、研究现状，指出关键优势、挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: ISAC是6G及未来网络关键，传统技术集成功能计算复杂，深度学习技术可降复杂度，近期有相关研究提出多种方法。

Method: 先介绍深度学习架构和ISAC基础，再对基于深度学习的ISAC技术进行分类综述。

Result: 梳理了基于深度学习的ISAC技术现状，明确了关键优势和主要挑战。

Conclusion: 对基于深度学习的ISAC技术研究进行总结，指出未来研究潜在方向。

Abstract: This article comprehensively reviews recent developments and research on deep
learning-based (DL-based) techniques for integrated sensing and communication
(ISAC) systems. ISAC, which combines sensing and communication functionalities,
is regarded as a key enabler for 6G and beyond networks, as many emerging
applications, such as vehicular networks and industrial robotics, necessitate
both sensing and communication capabilities for effective operation. A unified
platform that provides both functions can reduce hardware complexity, alleviate
frequency spectrum congestion, and improve energy efficiency. However,
integrating these functionalities on the same hardware requires highly
optimized signal processing and system design, introducing significant
computational complexity when relying on conventional iterative or
optimization-based techniques. As an alternative to conventional techniques,
DL-based techniques offer efficient and near-optimal solutions with reduced
computational complexity. Hence, such techniques are well-suited for operating
under limited computational resources and low latency requirements in real-time
systems. DL-based techniques can swiftly and effectively yield near-optimal
solutions for a wide range of sophisticated ISAC-related tasks, including
waveform design, channel estimation, sensing signal processing, data
demodulation, and interference mitigation. Therefore, motivated by these
advantages, recent studies have proposed various DL-based approaches for ISAC
system design. After briefly introducing DL architectures and ISAC
fundamentals, this survey presents a comprehensive and categorized review of
state-of-the-art DL-based techniques for ISAC, highlights their key advantages
and major challenges, and outlines potential directions for future research.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [260] [Building causation links in stochastic nonlinear systems from data](https://arxiv.org/abs/2509.07701)
*Sergio Chibbaro,Cyril Furtlehner,Théo Marchetta,Andrei-Tiberiu Pantea,Davide Rossetti*

Main category: cond-mat.stat-mech

TL;DR: 本文在物理响应理论框架下，利用机器学习技术检测复杂系统内在因果联系，并计算线性响应因果预测器在大规模马尔可夫过程网络中的渐近效率。


<details>
  <summary>Details</summary>
Motivation: 因果关系对理解世界至关重要，但从观测数据中解读因果关系困难，机器学习为挖掘隐藏因果机制提供新机会，因此要检测复杂系统的内在因果联系。

Method: 在物理响应理论框架下，发展已有理论观点，使用先进的机器学习技术从数据中构建模型，考虑线性随机和非线性系统。

Result: 计算了线性响应基于因果预测器在大规模线性相互作用马尔可夫过程网络情况下的渐近效率。

Conclusion: 未明确提及，但暗示了该方法在检测复杂系统因果关系上的可行性与潜在价值。

Abstract: Causal relationships play a fundamental role in understanding the world
around us. The ability to identify and understand cause-effect relationships is
critical to making informed decisions, predicting outcomes, and developing
effective strategies. However, deciphering causal relationships from
observational data is a difficult task, as correlations alone may not provide
definitive evidence of causality. In recent years, the field of machine
learning (ML) has emerged as a powerful tool, offering new opportunities for
uncovering hidden causal mechanisms and better understanding complex systems.
In this work, we address the issue of detecting the intrinsic causal links of a
large class of complex systems in the framework of the response theory in
physics. We develop some theoretical ideas put forward by [1], and technically
we use state-of-the-art ML techniques to build up models from data. We consider
both linear stochastic and non-linear systems. Finally, we compute the
asymptotic efficiency of the linear response based causal predictor in a case
of large scale Markov process network of linear interactions.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [261] [Toward Lifelong-Sustainable Electronic-Photonic AI Systems via Extreme Efficiency, Reconfigurability, and Robustness](https://arxiv.org/abs/2509.07396)
*Ziang Yin,Hongjian Zhou,Chetan Choppali Sudarshan,Vidya Chhabria,Jiaqi Gu*

Main category: physics.optics

TL;DR: 大规模AI增长使传统电子平台受限，电子光子集成电路（EPICs）有优势，本文探讨电子光子设计自动化（EPDA）和跨层协同设计方法提升光子AI系统可持续性，提出终身可持续电子光子AI系统愿景。


<details>
  <summary>Details</summary>
Motivation: 大规模AI增长对计算能力需求大增，传统电子平台有能耗、带宽和扩展限制，需提升光子AI系统可持续性。

Method: 研究EPDA和跨层协同设计方法，EPDA工具生成更紧凑布局，跨层设备 - 电路 - 架构协同设计挖掘新的可持续性优势。

Result: EPDA工具可减少芯片面积和金属层使用，跨层协同设计实现超紧凑电路设计、可重构硬件拓扑和智能弹性机制。

Conclusion: 结合光子固有优势与EPDA和协同设计带来的面积效率、可重构性和鲁棒性提升，EPIC AI系统能满足现代AI性能需求和可持续计算要求。

Abstract: The relentless growth of large-scale artificial intelligence (AI) has created
unprecedented demand for computational power, straining the energy, bandwidth,
and scaling limits of conventional electronic platforms. Electronic-photonic
integrated circuits (EPICs) have emerged as a compelling platform for
next-generation AI systems, offering inherent advantages in ultra-high
bandwidth, low latency, and energy efficiency for computing and
interconnection. Beyond performance, EPICs also hold unique promises for
sustainability. Fabricated in relaxed process nodes with fewer metal layers and
lower defect densities, photonic devices naturally reduce embodied carbon
footprint (CFP) compared to advanced digital electronic integrated circuits,
while delivering orders-of-magnitude higher computing performance and
interconnect bandwidth. To further advance the sustainability of photonic AI
systems, we explore how electronic-photonic design automation (EPDA) and
cross-layer co-design methodologies can amplify these inherent benefits. We
present how advanced EPDA tools enable more compact layout generation, reducing
both chip area and metal layer usage. We will also demonstrate how cross-layer
device-circuit-architecture co-design unlocks new sustainability gains for
photonic hardware: ultra-compact photonic circuit designs that minimize chip
area cost, reconfigurable hardware topology that adapts to evolving AI
workloads, and intelligent resilience mechanisms that prolong lifetime by
tolerating variations and faults. By uniting intrinsic photonic efficiency with
EPDA- and co-design-driven gains in area efficiency, reconfigurability, and
robustness, we outline a vision for lifelong-sustainable electronic-photonic AI
systems. This perspective highlights how EPIC AI systems can simultaneously
meet the performance demands of modern AI and the urgent imperative for
sustainable computing.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [262] [veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD](https://arxiv.org/abs/2509.07003)
*Youjie Li,Cheng Wan,Zhiqi Lin,Hongyu Zhu,Jiacheng Yang,Ziang Song,Xinyi Di,Jiawei Wu,Huiyao Shu,Wenlei Bao,Yanghua Peng,Haibin Lin,Li-Wen Chang*

Main category: cs.PL

TL;DR: 本文介绍veScale，一个采用SPMD范式的即时模式训练系统，解决了SPMD在即时执行中的挑战，提升性能且降低代码复杂度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型分布式训练需要更复杂并行性，促使向简单、易调试的SPMD编程范式转变，但SPMD在即时执行中有确保结果一致性和大规模高性能的挑战。

Method: 引入与任意分片算子兼容的分布式随机数生成新算法解决结果不一致问题，减少PyTorch原语开销并提高通信效率提升训练性能。

Result: veScale比现有训练系统最高有2.2倍加速，代码复杂度降低78.4%，并能保持与单设备相当的结果。

Conclusion: veScale有效解决了SPMD在即时执行中的挑战，实现分布式张量编程的普及。

Abstract: Large Language Models (LLMs) have scaled rapidly in size and complexity,
requiring increasingly intricate parallelism for distributed training, such as
3D parallelism. This sophistication motivates a shift toward simpler, more
debuggable programming paradigm like Single Program Multiple Data (SPMD).
However, SPMD in eager execution introduces two key challenges: ensuring
consistency with single-device execution and achieving high performance at
scale. In this paper, we introduce veScale, an eager-mode training system that
fully embraces SPMD paradigm to democratize distributed tensor programming.
veScale addresses the prevalent issue of inconsistent results in systems like
PyTorch by introducing a novel algorithm of distributed Random Number
Generation (RNG) compatible with arbitrary sharded operators. veScale also
significantly boosts training performance by reducing PyTorch primitive's
overhead and improving communication efficiency. Evaluations show that veScale
delivers up to 2.2x speedup over the state-of-the-art training systems, like
TorchTitan, and cuts code complexity by 78.4%, while preserving
single-device-equivalent results.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [263] [Controllable Singing Voice Synthesis using Phoneme-Level Energy Sequence](https://arxiv.org/abs/2509.07038)
*Yerin Ryu,Inseop Shin,Chanwoo Kim*

Main category: cs.SD

TL;DR: 本文聚焦可控歌唱语音合成中的动态控制，通过对能量序列建模提升可控性，实验表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有多数SVS系统依赖概率建模，对动态等属性缺乏精确控制。

Method: 将SVS模型基于从真实频谱图提取的能量序列进行条件设置，提出音素级能量序列以方便用户控制。

Result: 与基线和能量预测模型相比，音素级输入的能量序列平均绝对误差降低超50%，且不影响合成质量。

Conclusion: 这是首次在SVS中实现用户驱动的动态控制。

Abstract: Controllable Singing Voice Synthesis (SVS) aims to generate expressive
singing voices reflecting user intent. While recent SVS systems achieve high
audio quality, most rely on probabilistic modeling, limiting precise control
over attributes such as dynamics. We address this by focusing on dynamic
control--temporal loudness variation essential for musical expressiveness--and
explicitly condition the SVS model on energy sequences extracted from
ground-truth spectrograms, reducing annotation costs and improving
controllability. We also propose a phoneme-level energy sequence for
user-friendly control. To the best of our knowledge, this is the first attempt
enabling user-driven dynamics control in SVS. Experiments show our method
achieves over 50% reduction in mean absolute error of energy sequences for
phoneme-level inputs compared to baseline and energy-predictor models, without
compromising synthesis quality.

</details>


### [264] [Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative Study](https://arxiv.org/abs/2509.07132)
*Kutub Uddin,Muhammad Umar Farooq,Awais Khan,Khalid Mahmood Malik*

Main category: cs.SD

TL;DR: 文章研究了现有音频深度伪造检测方法，在五个数据集上评估，分析其对抗反取证攻击的优缺点，为设计更强大检测器和防御策略提供参考。


<details>
  <summary>Details</summary>
Motivation: 生成式AI产生的深度伪造音频对语音生物识别应用构成威胁，现有检测方法受反取证攻击影响有效性降低。

Method: 对现有音频深度伪造检测方法进行比较分析，在五个深度伪造基准数据集上用原始和基于频谱图两种方法评估。

Result: 深入了解了现有方法对抗不同反取证攻击的优缺点。

Conclusion: 分析不仅指出检测方法的漏洞，还为设计更强大通用的检测器和未来研究自适应防御策略提供指导。

Abstract: The widespread use of generative AI has shown remarkable success in producing
highly realistic deepfakes, posing a serious threat to various voice biometric
applications, including speaker verification, voice biometrics, audio
conferencing, and criminal investigations. To counteract this, several
state-of-the-art (SoTA) audio deepfake detection (ADD) methods have been
proposed to identify generative AI signatures to distinguish between real and
deepfake audio. However, the effectiveness of these methods is severely
undermined by anti-forensic (AF) attacks that conceal generative signatures.
These AF attacks span a wide range of techniques, including statistical
modifications (e.g., pitch shifting, filtering, noise addition, and
quantization) and optimization-based attacks (e.g., FGSM, PGD, C \& W, and
DeepFool). In this paper, we investigate the SoTA ADD methods and provide a
comparative analysis to highlight their effectiveness in exposing deepfake
signatures, as well as their vulnerabilities under adversarial conditions. We
conducted an extensive evaluation of ADD methods on five deepfake benchmark
datasets using two categories: raw and spectrogram-based approaches. This
comparative analysis enables a deeper understanding of the strengths and
limitations of SoTA ADD methods against diverse AF attacks. It does not only
highlight vulnerabilities of ADD methods, but also informs the design of more
robust and generalized detectors for real-world voice biometrics. It will
further guide future research in developing adaptive defense strategies that
can effectively counter evolving AF techniques.

</details>


### [265] [End-to-End Efficiency in Keyword Spotting: A System-Level Approach for Embedded Microcontrollers](https://arxiv.org/abs/2509.07051)
*Pietro Bartoli,Tommaso Bondini,Christian Veronesi,Andrea Giudici,Niccolò Antonello,Franco Zappa*

Main category: cs.SD

TL;DR: 本文评估对比多种轻量级神经网络架构用于关键词检测，提出Typman - KWS架构，测试显示其在减少内存占用同时保证准确率，强调实际部署要考虑特征提取参数和硬件优化。


<details>
  <summary>Details</summary>
Motivation: 嵌入式和物联网设备有严格内存和能量限制，需高效关键词检测技术。

Method: 系统评估对比DS - CNN、LiCoNet、TENet等架构及提出的Typman - KWS架构，涵盖从MFCC特征提取到神经推理的整个处理流程，并在三个STM32平台上进行基准测试。

Result: Typman - KWS带三个残差块F1分数达92.4%，仅14.4k参数；N6 MCU实现最佳能量延迟积。

Conclusion: 模型准确率不能单独决定实际效果，关键词检测部署需考虑特征提取参数和硬件优化。

Abstract: Keyword spotting (KWS) is a key enabling technology for hands-free
interaction in embedded and IoT devices, where stringent memory and energy
constraints challenge the deployment of AI-enabeld devices. In this work, we
systematically evaluate and compare several state-of-the-art lightweight neural
network architectures, including DS-CNN, LiCoNet, and TENet, alongside our
proposed Typman-KWS (TKWS) architecture built upon MobileNet, specifically
designed for efficient KWS on microcontroller units (MCUs). Unlike prior
studies focused solely on model inference, our analysis encompasses the entire
processing pipeline, from Mel-Frequency Cepstral Coefficient (MFCC) feature
extraction to neural inference, and is benchmarked across three STM32 platforms
(N6, H7, and U5). Our results show that TKWS with three residual blocks
achieves up to 92.4% F1-score with only 14.4k parameters, reducing memory
footprint without compromising the accuracy. Moreover, the N6 MCU with
integrated neural acceleration achieves the best energy-delay product (EDP),
enabling efficient, low-latency operation even with high-resolution features.
Our findings highlight the model accuracy alone does not determine real-world
effectiveness; rather, optimal keyword spotting deployments require careful
consideration of feature extraction parameters and hardware-specific
optimization.

</details>


### [266] [Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data](https://arxiv.org/abs/2509.07526)
*Gokul Karthik Kumar,Rishabh Saraf,Ludovick Lepauloux,Abdul Muneer,Billel Mokeddem,Hakim Hacid*

Main category: cs.SD

TL;DR: 本文介绍了基于指令微调大语言模型和Whisper编码器构建的音频语言模型Falcon3 - Audio，用少量公开音频数据取得良好表现，且发现一些复杂操作并非性能必需。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理领域应用广泛，但与音频的结合研究不足，而音频在人类交流中很重要，因此要探索大语言模型与音频的结合。

Method: 基于指令微调的大语言模型和Whisper编码器构建Falcon3 - Audio。

Result: Falcon3 - Audio - 7B在MMAU基准测试中达到最佳报告性能，得分64.14，与R1 - AQA相当，且在数据和参数效率、单阶段训练和透明度方面表现出色；最小的1B模型能与2B - 13B参数的更大开源模型竞争。

Conclusion: 一些常见的复杂操作（如课程学习、多音频编码器和复杂的交叉注意力连接）对良好性能并非必需。

Abstract: Large language models (LLMs) have transformed NLP, yet their integration with
audio remains underexplored -- despite audio's centrality to human
communication. We introduce Falcon3-Audio, a family of Audio-Language Models
(ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably
small amount of public audio data -- less than 30K hours (5K unique) --
Falcon3-Audio-7B matches the best reported performance among open-weight models
on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while
distinguishing itself through superior data and parameter efficiency,
single-stage training, and transparency. Notably, our smallest 1B model remains
competitive with larger open models ranging from 2B to 13B parameters. Through
extensive ablations, we find that common complexities -- such as curriculum
learning, multiple audio encoders, and intricate cross-attention connectors --
are not required for strong performance, even compared to models trained on
over 500K hours of data.

</details>


### [267] [Neural Proxies for Sound Synthesizers: Learning Perceptually Informed Preset Representations](https://arxiv.org/abs/2509.07635)
*Paolo Combes,Stefan Weinzierl,Klaus Obermayer*

Main category: cs.SD

TL;DR: 本文提出近似任意合成器的方法，以解决深度学习用于自动合成器编程时软件合成器集成到训练管道的难题，并进行了评估，取得一定成果。


<details>
  <summary>Details</summary>
Motivation: 深度学习用于自动合成器编程（ASP）时，软件合成器集成到训练管道有挑战，因其可能不可微。

Method: 训练神经网络将合成器预设映射到预训练模型导出的音频嵌入空间，定义产生紧凑有效表示的神经代理，将音频嵌入损失集成到基于神经网络的ASP系统。评估不同预训练音频模型表示及多种神经网络架构定义神经代理的有效性。

Result: 使用三种流行软件合成器的合成和手工预设评估方法，在合成器声音匹配下游任务中评估性能，虽学习表示的好处受资源需求影响，但所有合成器都有积极结果。

Conclusion: 研究成果为基于神经网络的ASP系统中合成器代理的应用研究奠定基础。

Abstract: Deep learning appears as an appealing solution for Automatic Synthesizer
Programming (ASP), which aims to assist musicians and sound designers in
programming sound synthesizers. However, integrating software synthesizers into
training pipelines is challenging due to their potential non-differentiability.
This work tackles this challenge by introducing a method to approximate
arbitrary synthesizers. Specifically, we train a neural network to map
synthesizer presets onto an audio embedding space derived from a pretrained
model. This facilitates the definition of a neural proxy that produces compact
yet effective representations, thereby enabling the integration of audio
embedding loss into neural-based ASP systems for black-box synthesizers. We
evaluate the representations derived by various pretrained audio models in the
context of neural-based nASP and assess the effectiveness of several neural
network architectures, including feedforward, recurrent, and transformer-based
models, in defining neural proxies. We evaluate the proposed method using both
synthetic and hand-crafted presets from three popular software synthesizers and
assess its performance in a synthesizer sound matching downstream task. While
the benefits of the learned representation are nuanced by resource
requirements, encouraging results were obtained for all synthesizers, paving
the way for future research into the application of synthesizer proxies for
neural-based ASP systems.

</details>


### [268] [Spectral and Rhythm Feature Performance Evaluation for Category and Class Level Audio Classification with Deep Convolutional Neural Networks](https://arxiv.org/abs/2509.07756)
*Friedrich Wolf-Monheim*

Main category: cs.SD

TL;DR: 研究多种频谱和节奏特征在音频分类中的表现，发现 mel 标度频谱图和 MFCC 在音频分类中表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究不同频谱和节奏特征在音频分类任务中的性能，为音频分类提供更优特征选择。

Method: 使用深度 CNN 和 ESC - 50 数据集，通过端到端深度学习管道评估多种特征用于音频类别和音频级别分类的表现。

Result: 评估多类分类的准确率、精确率、召回率和 F1 分数，表明 mel 标度频谱图和 MFCC 比其他特征表现更好。

Conclusion: 在使用深度 CNN 进行音频分类任务时，mel 标度频谱图和 MFCC 是更优的特征选择。

Abstract: Next to decision tree and k-nearest neighbours algorithms deep convolutional
neural networks (CNNs) are widely used to classify audio data in many domains
like music, speech or environmental sounds. To train a specific CNN various
spectral and rhythm features like mel-scaled spectrograms, mel-frequency
cepstral coefficients (MFCC), cyclic tempograms, short-time Fourier transform
(STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy
normalized statistics (CENS) chromagrams can be used as digital image input
data for the neural network. The performance of these spectral and rhythm
features for audio category level as well as audio class level classification
is investigated in detail with a deep CNN and the ESC-50 dataset with 2,000
labeled environmental audio recordings using an end-to-end deep learning
pipeline. The evaluated metrics accuracy, precision, recall and F1 score for
multiclass classification clearly show that the mel-scaled spectrograms and the
mel-frequency cepstral coefficients (MFCC) perform significantly better then
the other spectral and rhythm features investigated in this research for audio
classification tasks using deep CNNs.

</details>


### [269] [Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems](https://arxiv.org/abs/2509.07677)
*Kamel Kamel,Hridoy Sankar Dutta,Keshav Sood,Sunil Aryal*

Main category: cs.SD

TL;DR: 论文指出语音认证系统面临高级威胁，提出Spectral Masking and Interpolation Attack (SMIA) 方法进行攻击测试，结果显示当前安全措施不足，需转向下一代动态防御。


<details>
  <summary>Details</summary>
Motivation: 语音认证系统虽有改进，但面临深度伪造和对抗攻击等威胁，现有反欺骗对策存在安全漏洞，为证明其脆弱性开展研究。

Method: 提出SMIA方法，通过操纵AI生成音频中人类听不见的频率区域创建对抗样本，在模拟真实世界条件下对最先进模型进行综合评估。

Result: SMIA在对组合语音认证/反欺骗系统攻击成功率至少82%，对独立说话人验证系统至少97.5%，对反欺骗对策达100%。

Conclusion: 当前安全措施不足以应对自适应对抗攻击，急需转向采用动态、上下文感知框架的下一代防御。

Abstract: Voice Authentication Systems (VAS) use unique vocal characteristics for
verification. They are increasingly integrated into high-security sectors such
as banking and healthcare. Despite their improvements using deep learning, they
face severe vulnerabilities from sophisticated threats like deepfakes and
adversarial attacks. The emergence of realistic voice cloning complicates
detection, as systems struggle to distinguish authentic from synthetic audio.
While anti-spoofing countermeasures (CMs) exist to mitigate these risks, many
rely on static detection models that can be bypassed by novel adversarial
methods, leaving a critical security gap. To demonstrate this vulnerability, we
propose the Spectral Masking and Interpolation Attack (SMIA), a novel method
that strategically manipulates inaudible frequency regions of AI-generated
audio. By altering the voice in imperceptible zones to the human ear, SMIA
creates adversarial samples that sound authentic while deceiving CMs. We
conducted a comprehensive evaluation of our attack against state-of-the-art
(SOTA) models across multiple tasks, under simulated real-world conditions.
SMIA achieved a strong attack success rate (ASR) of at least 82% against
combined VAS/CM systems, at least 97.5% against standalone speaker verification
systems, and 100% against countermeasures. These findings conclusively
demonstrate that current security postures are insufficient against adaptive
adversarial attacks. This work highlights the urgent need for a paradigm shift
toward next-generation defenses that employ dynamic, context-aware frameworks
capable of evolving with the threat landscape.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [270] [Exploring System Adaptations For Minimum Latency Real-Time Piano Transcription](https://arxiv.org/abs/2509.07586)
*Patricia Hu,Silvan David Peter,Jan Schlüter,Gerhard Widmer*

Main category: eess.AS

TL;DR: 本文研究将在线钢琴转录模型适配实时场景，通过消除非因果处理、减少计算负载等方法进行改进，评估发现严格因果处理会降低转录精度，且存在预处理延迟和预测精度的权衡，还发布系统作为基线。


<details>
  <summary>Details</summary>
Motivation: 现有钢琴转录方法无法满足多数实时音乐应用低于30ms延迟的需求，需将在线转录模型适配实时场景。

Method: 消除非因果处理，通过核心模型组件共享计算和调整模型大小降低计算负载，探索不同预处理、后处理策略与标签编码方案。

Result: 在MAESTRO数据集评估发现严格因果处理使转录精度下降，存在预处理延迟和预测精度的权衡。

Conclusion: 发布系统作为基线，支持研究者设计低延迟实时转录模型。

Abstract: Advances in neural network design and the availability of large-scale labeled
datasets have driven major improvements in piano transcription. Existing
approaches target either offline applications, with no restrictions on
computational demands, or online transcription, with delays of 128-320 ms.
However, most real-time musical applications require latencies below 30 ms. In
this work, we investigate whether and how the current state-of-the-art online
transcription model can be adapted for real-time piano transcription.
Specifically, we eliminate all non-causal processing, and reduce computational
load through shared computations across core model components and variations in
model size. Additionally, we explore different pre- and postprocessing
strategies, and related label encoding schemes, and discuss their suitability
for real-time transcription. Evaluating the adaptions on the MAESTRO dataset,
we find a drop in transcription accuracy due to strictly causal processing as
well as a tradeoff between the preprocessing latency and prediction accuracy.
We release our system as a baseline to support researchers in designing models
towards minimum latency real-time transcription.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [271] [Individual utilities of life satisfaction reveal inequality aversion unrelated to political alignment](https://arxiv.org/abs/2509.07793)
*Crispin Cooper,Ana Friedrich,Tommaso Reggiani,Wouter Poortinga*

Main category: econ.GN

TL;DR: 研究英国样本对幸福优先排序及公平与个人幸福权衡，发现多数人风险厌恶且更关注社会不平等，结果挑战平均生活满意度指标，支持非线性效用替代方案。


<details>
  <summary>Details</summary>
Motivation: 探讨社会中幸福应如何优先排序，以及人们在公平和个人幸福之间愿意做出的权衡。

Method: 采用陈述偏好实验，对英国有全国代表性样本进行研究，用期望效用最大化框架估计个体效用函数，并测试对小概率事件的敏感性。

Result: 多数参与者呈凹形（风险厌恶）效用曲线，对社会生活满意度结果中的不平等更厌恶，且偏好与政治立场无关。

Conclusion: 结果挑战将平均生活满意度作为政策指标，支持开发更能反映集体人类价值观的基于非线性效用的替代方案。

Abstract: How should well-being be prioritised in society, and what trade-offs are
people willing to make between fairness and personal well-being? We investigate
these questions using a stated preference experiment with a nationally
representative UK sample (n = 300), in which participants evaluated life
satisfaction outcomes for both themselves and others under conditions of
uncertainty. Individual-level utility functions were estimated using an
Expected Utility Maximisation (EUM) framework and tested for sensitivity to the
overweighting of small probabilities, as characterised by Cumulative Prospect
Theory (CPT). A majority of participants displayed concave (risk-averse)
utility curves and showed stronger aversion to inequality in societal life
satisfaction outcomes than to personal risk. These preferences were unrelated
to political alignment, suggesting a shared normative stance on fairness in
well-being that cuts across ideological boundaries. The results challenge use
of average life satisfaction as a policy metric, and support the development of
nonlinear utility-based alternatives that more accurately reflect collective
human values. Implications for public policy, well-being measurement, and the
design of value-aligned AI systems are discussed.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [272] [Benchmarking Universal Interatomic Potentials on Zeolite Structures](https://arxiv.org/abs/2509.07417)
*Shusuke Ito,Koki Muraoka,Akira Nakayama*

Main category: cond-mat.mtrl-sci

TL;DR: 本文以平衡沸石结构为测试平台对几种通用原子间势（IPs）进行基准测试，发现现代预训练通用机器学习IPs是沸石筛选实用工具。


<details>
  <summary>Details</summary>
Motivation: 尽管过去几年开发了多种覆盖元素周期表范围广的通用IPs，但需仔细研究其对目标化学体系的适用性。

Method: 选择通用解析IPs和预训练通用机器学习IPs（MLIPs），与定制IPs对比，以实验数据和含色散校正的密度泛函理论（DFT）计算为参考，对多种沸石结构进行测试。

Result: 通用解析IPs中GFN - FF最佳，但对高应变硅环和铝硅酸盐体系精度不足；所有MLIPs能很好重现实验或DFT级别的几何结构和能量；通用MLIPs中eSEN - 30M - OAM模型在所有研究的沸石结构中表现最稳定。

Conclusion: 现代预训练通用MLIPs是涉及各种组成的沸石筛选工作流程中的实用工具。

Abstract: Interatomic potentials (IPs) with wide elemental coverage and high accuracy
are powerful tools for high-throughput materials discovery. While the past few
years witnessed the development of multiple new universal IPs that cover wide
ranges of the periodic table, their applicability to target chemical systems
should be carefully investigated. We benchmark several universal IPs using
equilibrium zeolite structures as testbeds. We select a diverse set of
universal IPs encompassing two major categories: (i) universal analytic IPs,
including GFN-FF, UFF, and Dreiding; (ii) pretrained universal machine learning
IPs (MLIPs), comprising CHGNet, ORB-v3, MatterSim, eSEN-30M-OAM, PFP-v7, and
EquiformerV2-lE4-lF100-S2EFS-OC22. We compare them with established tailor-made
IPs, SLC, ClayFF, and BSFF using experimental data and density functional
theory (DFT) calculations with dispersion correction as the reference. The
tested zeolite structures comprise pure silica frameworks and aluminosilicates
containing copper species, potassium, and organic cations. We found that GFN-FF
is the best among the tested universal analytic IPs, but it does not achieve
satisfactory accuracy for highly strained silica rings and aluminosilicate
systems. All MLIPs can well reproduce experimental or DFT-level geometries and
energetics. Among the universal MLIPs, the eSEN-30M-OAM model shows the most
consistent performance across all zeolite structures studied. These findings
show that the modern pretrained universal MLIPs are practical tools in zeolite
screening workflows involving various compositions.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [273] [Safe cross-entropy-based importance sampling for rare event simulations](https://arxiv.org/abs/2509.07160)
*Zhiwei Gao,George Karniadakis*

Main category: math.NA

TL;DR: 提出Safe - ICE方法改进传统ICE方法在估计极小失效概率时的不足，经测试性能更优。


<details>
  <summary>Details</summary>
Motivation: 传统ICE方法使用的常规混合模型轻尾，导致收敛慢、不稳定，且提前选择混合成分数量困难。

Method: 采用加权交叉熵惩罚的期望最大化（EM）算法自动修剪冗余成分，引入轻尾与重尾分布结合的双成分混合模型。

Result: Safe - ICE比标准ICE收敛更快，失效概率估计更准确，能自动确定合适的混合成分数量。

Conclusion: Safe - ICE方法有效克服了传统ICE方法的问题，性能更优。

Abstract: The Improved Cross-Entropy (ICE) method is a powerful tool for estimating
failure probabilities in reliability analysis. Its core idea is to approximate
the optimal importance-sampling density by minimizing the forward
Kullback-Leibler divergence within a chosen parametric family-typically a
mixture model. However, conventional mixtures are often light-tailed, which
leads to slow convergence and instability when targeting very small failure
probabilities. Moreover, selecting the number of mixture components in advance
can be difficult and may undermine stability. To overcome these challenges, we
adopt a weighted cross-entropy-penalized expectation-maximization (EM)
algorithm that automatically prunes redundant components during the iterative
process, making the approach more stable. Furthermore, we introduce a novel
two-component mixture that pairs a light-tailed distribution with a
heavy-tailed one, enabling more effective exploration of the tail region and
thus accelerating convergence for extremely small failure probabilities. We
call the resulting method Safe-ICE and assess it on a variety of test problems.
Numerical results show that Safe-ICE not only converges more rapidly and yields
more accurate failure-probability estimates than standard ICE, but also
identifies the appropriate number of mixture components without manual tuning.

</details>


### [274] [Physics-informed low-rank neural operators with application to parametric elliptic PDEs](https://arxiv.org/abs/2509.07687)
*Sebastian Schaffer,Lukas Exl*

Main category: math.NA

TL;DR: 提出物理信息低秩神经算子（PILNO）用于近似偏微分方程（PDE）解算子，展示其在多种问题上的有效性。


<details>
  <summary>Details</summary>
Motivation: 高效近似点云数据上偏微分方程的解算子。

Method: 将低秩核近似与编解码器架构结合，使用物理信息惩罚框架训练模型。

Result: 在函数拟合、泊松方程等多种问题上证明了有效性，低秩结构在高维参数空间提供计算效率。

Conclusion: PILNO是用于PDE的可扩展且灵活的代理建模工具。

Abstract: We present the Physics-Informed Low-Rank Neural Operator (PILNO), a neural
operator framework for efficiently approximating solution operators of partial
differential equations (PDEs) on point cloud data. PILNO combines low-rank
kernel approximations with an encoder--decoder architecture, enabling fast,
continuous one-shot predictions while remaining independent of specific
discretizations. The model is trained using a physics-informed penalty
framework, ensuring that PDE constraints and boundary conditions are satisfied
in both supervised and unsupervised settings. We demonstrate its effectiveness
on diverse problems, including function fitting, the Poisson equation, the
screened Poisson equation with variable coefficients, and parameterized Darcy
flow. The low-rank structure provides computational efficiency in
high-dimensional parameter spaces, establishing PILNO as a scalable and
flexible surrogate modeling tool for PDEs.

</details>


### [275] [Feature Understanding and Sparsity Enhancement via 2-Layered kernel machines (2L-FUSE)](https://arxiv.org/abs/2509.07806)
*Fabiana Camattari,Sabrina Guastavino,Francesco Marchetti,Emma Perracchione*

Main category: math.NA

TL;DR: 提出基于2层核机器学习数据自适应核度量的稀疏增强策略，实验表明该方法能实现特征降维且不损失预测性能。


<details>
  <summary>Details</summary>
Motivation: 为回归任务找到一种更好的特征降维方法。

Method: 通过2层核机器学习数据自适应核度量（形状矩阵），对形状矩阵进行特征分解以确定特征空间中最具信息的方向。

Result: 在合成数据和地磁暴真实数据集上实现了最小且信息丰富的特征集，且不损失预测性能。

Conclusion: 该数据驱动方法提供了灵活、可解释且准确的特征降维方案。

Abstract: We propose a novel sparsity enhancement strategy for regression tasks, based
on learning a data-adaptive kernel metric, i.e., a shape matrix, through
2-Layered kernel machines. The resulting shape matrix, which defines a
Mahalanobis-type deformation of the input space, is then factorized via an
eigen-decomposition, allowing us to identify the most informative directions in
the space of features. This data-driven approach provides a flexible,
interpretable and accurate feature reduction scheme. Numerical experiments on
synthetic and applications to real datasets of geomagnetic storms demonstrate
that our approach achieves minimal yet highly informative feature sets without
losing predictive performance.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [276] [Bayesian Pliable Lasso with Horseshoe Prior for Interaction Effects in GLMs with Missing Responses](https://arxiv.org/abs/2509.07501)
*The Tien Mai*

Main category: stat.ME

TL;DR: 提出贝叶斯柔韧套索方法，用于稀疏回归问题，可处理主效应和交互效应，能实现不确定性量化，通过模拟和真实数据研究证明其优势，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有柔韧套索缺乏不确定性量化和融入先验知识的自然框架，需要改进。

Method: 提出贝叶斯柔韧套索，在主效应和交互效应上设置马蹄形等稀疏诱导先验，扩展到广义线性模型，开发处理缺失响应的方法和基于马蹄形先验重新参数化的Gibbs采样算法。

Result: 贝叶斯框架产生稀疏、可解释的交互结构和不确定性的原则性度量，在模拟和真实数据研究中显示出优于现有方法的优势。

Conclusion: 提出的贝叶斯柔韧套索方法在处理复杂交互模式上有优势，适用于完整和不完整数据。

Abstract: Sparse regression problems, where the goal is to identify a small set of
relevant predictors, often require modeling not only main effects but also
meaningful interactions through other variables. While the pliable lasso has
emerged as a powerful frequentist tool for modeling such interactions under
strong heredity constraints, it lacks a natural framework for uncertainty
quantification and incorporation of prior knowledge. In this paper, we propose
a Bayesian pliable lasso that extends this approach by placing
sparsity-inducing priors, such as the horseshoe, on both main and interaction
effects. The hierarchical prior structure enforces heredity constraints while
adaptively shrinking irrelevant coefficients and allowing important effects to
persist. We extend this framework to Generalized Linear Models (GLMs) and
develop a tailored approach to handle missing responses. To facilitate
posterior inference, we develop an efficient Gibbs sampling algorithm based on
a reparameterization of the horseshoe prior. Our Bayesian framework yields
sparse, interpretable interaction structures, and principled measures of
uncertainty. Through simulations and real-data studies, we demonstrate its
advantages over existing methods in recovering complex interaction patterns
under both complete and incomplete data.
  Our method is implemented in the package \texttt{hspliable} available on
Github.

</details>


### [277] [Nonparametric Envelopes for Flexible Response Reduction](https://arxiv.org/abs/2509.07248)
*Tate Jacobson*

Main category: stat.ME

TL;DR: 本文提出核包络（KENV）估计器用于非线性多元回归，证明其预测风险收敛性，通过模拟和实际数据验证其预测性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有包络方法多在参数建模框架下，需为非线性多元回归提出灵活的非参数扩展方法。

Method: 提出核包络（KENV）估计器，在再生核希尔伯特空间同时估计响应包络子空间和包络非参数条件均值函数，并使用考虑包络结构的新惩罚项。

Result: 证明KENV预测风险随样本量增加收敛到最优风险，当响应有非平凡非物质成分时，KENV样本内预测风险低于核岭回归；模拟和实际数据表明KENV在高低维下预测更准确。

Conclusion: KENV在非线性多元回归中比基于包络和基于核的替代方法有更准确的预测性能。

Abstract: Envelope methods improve the estimation efficiency in multivariate linear
regression by identifying and separating the material and immaterial parts of
the responses or the predictors and estimating the regression coefficients
using only the material part. Though envelopes have been extended to other
models, such as GLMs, these extensions still largely fall under the restrictive
parametric modeling framework. In this paper, we introduce a flexible,
nonparametric extension of response envelopes for improving efficiency in
nonlinear multivariate regressions. We propose the kernel envelope (KENV)
estimator for simultaneously estimating the response envelope subspace and the
enveloped nonparametric conditional mean function in a reproducing kernel
Hilbert space, with a novel penalty that accounts for the envelope structure.
We prove that the prediction risk for KENV converges to the optimal risk as the
sample size diverges and show that KENV achieves a lower in-sample prediction
risk than kernel ridge regression when the response has a non-trivial
immaterial component. We compare the prediction performance of KENV with other
envelope methods and kernel regression methods in simulations and a real data
example, finding that KENV delivers more accurate predictions than both the
envelope-based and kernel-based alternatives in both low and high dimensions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [278] [Towards Generalized Routing: Model and Agent Orchestration for Adaptive and Efficient Inference](https://arxiv.org/abs/2509.07571)
*Xiyu Guo,Shan Wang,Chunfang Ji,Xuefeng Zhao,Wenhao Xi,Yaoyao Liu,Qinglan Li,Chao Deng,Junlan Feng*

Main category: cs.MA

TL;DR: 提出MoMA框架解决用户查询路由难题，实现效率与成本平衡，实验显示其成本效益和可扩展性更优。


<details>
  <summary>Details</summary>
Motivation: 用户查询多样跨域，存在如何准确路由查询到合适执行单元以优化性能和效率的挑战。

Method: 构建训练数据集了解模型能力，推理时动态路由查询到性价比最高的大语言模型；采用基于上下文感知状态机和动态掩码的高效代理选择策略。

Result: 实验表明MoMA路由器在成本效益和可扩展性上优于现有方法。

Conclusion: MoMA框架能有效处理多样查询，在效率和成本间取得最优平衡。

Abstract: The rapid advancement of large language models (LLMs) and domain-specific AI
agents has greatly expanded the ecosystem of AI-powered services. User queries,
however, are highly diverse and often span multiple domains and task types,
resulting in a complex and heterogeneous landscape. This diversity presents a
fundamental routing challenge: how to accurately direct each query to an
appropriate execution unit while optimizing both performance and efficiency. To
address this, we propose MoMA (Mixture of Models and Agents), a generalized
routing framework that integrates both LLM and agent-based routing. Built upon
a deep understanding of model and agent capabilities, MoMA effectively handles
diverse queries through precise intent recognition and adaptive routing
strategies, achieving an optimal balance between efficiency and cost.
Specifically, we construct a detailed training dataset to profile the
capabilities of various LLMs under different routing model structures,
identifying the most suitable tasks for each LLM. During inference, queries are
dynamically routed to the LLM with the best cost-performance efficiency. We
also introduce an efficient agent selection strategy based on a context-aware
state machine and dynamic masking. Experimental results demonstrate that the
MoMA router offers superior cost-efficiency and scalability compared to
existing approaches.

</details>
