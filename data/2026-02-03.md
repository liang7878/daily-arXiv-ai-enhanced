<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 153]
- [cs.CE](#cs.CE) [Total: 6]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.DC](#cs.DC) [Total: 18]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.GT](#cs.GT) [Total: 7]
- [cs.IR](#cs.IR) [Total: 23]
- [cs.LG](#cs.LG) [Total: 421]
- [cs.NE](#cs.NE) [Total: 15]
- [cs.SE](#cs.SE) [Total: 29]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 3]
- [q-fin.ST](#q-fin.ST) [Total: 10]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 35]
- [stat.CO](#stat.CO) [Total: 3]
- [cs.DL](#cs.DL) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.MA](#cs.MA) [Total: 5]
- [quant-ph](#quant-ph) [Total: 6]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [math.OC](#math.OC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.NI](#cs.NI) [Total: 2]
- [nlin.CG](#nlin.CG) [Total: 1]
- [cs.CR](#cs.CR) [Total: 27]
- [cs.GR](#cs.GR) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [math.ST](#math.ST) [Total: 4]
- [math.DS](#math.DS) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 4]
- [math.NA](#math.NA) [Total: 3]
- [cs.CG](#cs.CG) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.SD](#cs.SD) [Total: 7]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [eess.AS](#eess.AS) [Total: 3]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 97]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [cs.CL](#cs.CL) [Total: 82]
- [econ.GN](#econ.GN) [Total: 8]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [stat.ME](#stat.ME) [Total: 5]
- [cs.CY](#cs.CY) [Total: 20]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 13]
- [cs.MM](#cs.MM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 8]
- [cs.RO](#cs.RO) [Total: 19]
- [math.FA](#math.FA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

TL;DR: 本文对比两种机器学习模型部署范式，发现FastAPI单请求开销低，Triton可扩展性强，验证了混合架构是企业临床AI的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 在医疗等受监管领域高效可扩展地部署机器学习模型，需平衡推理延迟、吞吐量和数据隐私等要求。

Method: 采用医疗AI参考架构，在Kubernetes上部署DistilBERT情感分析模型，对比基于FastAPI的轻量级REST服务和NVIDIA Triton推理服务器两种部署范式。

Result: FastAPI单请求工作负载开销低，p50延迟22ms；Triton通过动态批处理可扩展性强，单NVIDIA T4 GPU吞吐量达780请求/秒。

Conclusion: 混合架构（FastAPI用于健康信息去标识，Triton用于后端推理）是企业临床AI的最佳实践，为安全高可用部署提供蓝图。

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [2] [Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets](https://arxiv.org/abs/2602.00188)
*Srividhya Sethuraman,Chandrashekar Lakshminarayanan*

Main category: cs.AI

TL;DR: 提出AFDLD模型和ADEPT算法解决高维市场动态定价问题，兼顾效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有低秩多臂老虎机公式在高维市场动态定价中存在可扩展性、不确定性和可解释性问题，且无法明确产品属性对价格的影响。

Method: 引入AFDLD模型，提出投影和梯度无关的在线学习算法ADEPT。

Result: ADEPT在动态市场条件下学习接近最优价格，快速适应冲击和漂移，给出透明的属性级价格解释。

Conclusion: 通过结构化、属性驱动的表示可以同时实现自主定价代理的可解释性和效率。

Abstract: Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \emph{Additive Feature Decomposition-based Low-Dimensional Demand (\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.

</details>


### [3] [From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models](https://arxiv.org/abs/2602.00190)
*Mohit Jiwatode,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 研究用大语言模型从游戏玩法轨迹中逆向工程VGDL规则进行因果归纳，对比两种VGDL生成方法，SCM方法更优且可用于下游任务。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习代理在复杂游戏领域常不理解潜在因果游戏机制的问题。

Method: 从GVGAI框架选九个代表游戏，对比直接代码生成和先推断结构因果模型再转化为VGDL的两种方法，在多提示策略和控制上下文机制下评估。

Result: SCM方法生成的VGDL描述更接近真实情况，盲测中偏好胜率达81%，逻辑不一致规则更少。

Conclusion: 学习到的SCM可用于因果强化学习、可解释代理和程序生成新游戏等下游用例。

Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.

</details>


### [4] [Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic](https://arxiv.org/abs/2602.00266)
*Yani Zhang,Helmut Bölcskei*

Main category: cs.AI

TL;DR: 本文研究深度ReLU神经网络的功能对称问题，通过将网络转化为Lukasiewicz逻辑公式，实现功能等价网络变换，证明同一功能等价类中的网络可由逻辑公理对应的对称性连接。


<details>
  <summary>Details</summary>
Motivation: 解决给定函数f，推导所有能实现该函数的前馈ReLU网络的架构和参数的完全识别问题。

Method: 将ReLU网络转化为Lukasiewicz逻辑公式，通过逻辑公理进行代数重写实现功能等价网络变换，提出组合范式便于从逻辑公式映射回ReLU网络。

Result: 利用Chang完备性定理，证明对于每个功能等价类，该类中的所有ReLU网络都由对应Lukasiewicz逻辑有限公理集的有限对称集连接。

Conclusion: 受香农开关电路设计工作启发，此方法为ReLU网络的功能对称性研究提供了有效途径。

Abstract: Deep ReLU neural networks admit nontrivial functional symmetries: vastly different architectures and parameters (weights and biases) can realize the same function. We address the complete identification problem -- given a function f, deriving the architecture and parameters of all feedforward ReLU networks giving rise to f. We translate ReLU networks into Lukasiewicz logic formulae, and effect functional equivalent network transformations through algebraic rewrites governed by the logic axioms. A compositional norm form is proposed to facilitate the mapping from Lukasiewicz logic formulae back to ReLU networks. Using Chang's completeness theorem, we show that for every functional equivalence class, all ReLU networks in that class are connected by a finite set of symmetries corresponding to the finite set of axioms of Lukasiewicz logic. This idea is reminiscent of Shannon's seminal work on switching circuit design, where the circuits are translated into Boolean formulae, and synthesis is effected by algebraic rewriting governed by Boolean logic axioms.

</details>


### [5] [Localizing and Correcting Errors for LLM-based Planners](https://arxiv.org/abs/2602.00276)
*Aditya Kumar,William W. Cohen*

Main category: cs.AI

TL;DR: 大语言模型在符号经典规划任务常失败，提出L - ICL方法解决，比其他方法有效，在多领域和模型架构有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在符号经典规划任务中常违反领域约束，需要解决这一问题。

Method: 提出迭代地用局部上下文学习（L - ICL）示范增强指令，针对失败步骤进行针对性修正。

Result: L - ICL比显式指令、传统ICL和其他基线方法更有效，如在8x8网格世界中有效规划率提升30%，在其他领域和模型架构也有显著改进。

Conclusion: L - ICL是解决大语言模型在符号经典规划任务失败问题的有效方法。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.

</details>


### [6] [Multi-Head Attention Is a Multi-Player Game](https://arxiv.org/abs/2602.00861)
*Kushal Chakrabarti,Nirmal Balachundar*

Main category: cs.AI

TL;DR: 指出训练现代Transformer注意力机制存在的问题，将其训练当作整体优化器与内部多智能体特性不符，给出无政府代价界，提出GAME - LoRA方法并验证理论。


<details>
  <summary>Details</summary>
Motivation: 现代Transformer注意力机制内部是多智能体的，但训练时将其当作整体优化器，存在差异，需要研究并解决该问题。

Method: 将训练诱导的问题形式化为潜在博弈，分析梯度下降收敛情况；提出GAME - LoRA方法，结合Barlow Twins去相关与对数行列式协调压力。

Result: 无政府代价由头交互矩阵的非对角质量Γ(G)界定；Γ(G)可预测幻觉；GAME - LoRA能减少高达18%的幻觉（平均8%）且不降低知识水平。

Conclusion: 降低Γ(G)的正则化可收紧无政府代价；GAME - LoRA是一种考虑游戏结构的帕累托改进方法，优于忽略游戏结构的方法。

Abstract: Modern transformer attention is internally multi-agent -- heads compete and coordinate -- yet we train it as if it were a monolithic optimizer. We formalize this gap: cross-entropy training induces an implicit potential game among heads, and gradient descent converges to Nash equilibria with potentially unbounded inefficiency due to unpriced externalities (redundancy, correlated errors). Our main result bounds the Price of Anarchy by $Γ(G)$, the off-diagonal mass of a head interaction matrix capturing weight and gradient coupling. Under mild smoothness assumptions, we prove that both \emph{excess hallucination probability} and \emph{excess head redundancy} scale with PoA, unifying two distinct failure modes into a single mechanism. The bound is prescriptive: regularization that reduces $Γ(G)$ provably tightens PoA. We instantiate this as GAME-LoRA, combining Barlow Twins decorrelation with log-determinant coordination pressure. Experiments validate the theory: $Γ(G)$ predicts hallucination ($p{<}0.05$), emergent coalitions exhibit selective coordination, and GAME-LoRA achieves up to 18\% hallucination reduction (8\% average) with no knowledge degradation -- a Pareto improvement inaccessible to methods ignoring the game structure.

</details>


### [7] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

TL;DR: 本文研究大语言模型微调于不安全数据集时的涌现性对齐问题，发现后门触发增加对齐错误率，不同领域脆弱性差异大，还探索相关研究问题并提出分类排名和构建数据集方法。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型用于自主任务增多，涌现性对齐问题对AI安全构成风险，需进行研究。

Method: 对大语言模型在11个领域不安全数据集上微调，在有和无后门触发下用不相关用户提示评估，还进行多项探索实验。

Result: 后门触发使77.8%领域对齐错误率增加，不同领域脆弱性从0%到87.67%不等；成员推断指标是预测广义对齐错误程度的好先验；给出领域涌现性对齐分类排名。

Conclusion: 研究成果对AI安全和训练后处理有意义，还规范了构建对齐错误数据集的方法，代码和数据公开。

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [8] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

TL;DR: 提出ADP - MA框架动态构建、执行和优化数据处理管道，并用交互式演示展示其功能。


<details>
  <summary>Details</summary>
Motivation: 传统数据处理管道静态且针对特定任务，通用智能体和编码助手部署后缺乏自主监控、管理和优化管道能力。

Method: 通过分层智能体编排动态构建、执行和迭代优化管道，架构包括规划、编排和监控组件，强调上下文感知优化等。

Result: 通过交互式演示展示了该框架在代表性数据处理任务中的管道构建、执行监控和自适应优化。

Conclusion: ADP - MA框架提升了数据处理管道的构建和优化能力，可减少冗余、加速构建。

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [9] [SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?](https://arxiv.org/abs/2602.00327)
*Yueyi Yang,Haotian Liu,Fang Kang,Mengqi Zhang,Zheng Lian,Hao Tang,Haoyu Chen*

Main category: cs.AI

TL;DR: 本文探索用大语言模型进行人类对话下一语句预测，提出SayNext - Bench基准和SayNext - PC数据集，开发SayNext - Chat模型，实验表明模型性能优越，强调多模态线索和预测处理的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型有对话能力，但在预测人类下一语句方面表现不佳，人类可通过多模态线索做到，所以要系统研究LLMs是否能复制此能力。

Method: 提出SayNext - Bench基准评估LLMs和MLLMs；构建SayNext - PC数据集；开发双路线预测MLLM模型SayNext - Chat。

Result: SayNext - Chat模型在词汇重叠、语义相似性和情感一致性方面优于现有先进MLLMs。

Conclusion: 证明从多模态线索进行下一语句预测的可行性，强调多模态线索和预测处理对自然人类交互的重要性，为以人为本的AI提供新研究方向。

Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.

</details>


### [10] [MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants](https://arxiv.org/abs/2602.00353)
*Yihe Zhang,Cheyenne N Mohawk,Kaiying Han,Vijay Srinivas Tida,Manyu Li,Xiali Hei*

Main category: cs.AI

TL;DR: 提出开源平台MHDash用于心理健康AI系统开发评估，结果表明传统基准不适用于高安全要求场景，发布MHDash促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有评估依赖整体性能指标，难以发现特定风险失败模式，对多轮交互中模型行为洞察有限。

Method: 构建MHDash平台，将数据收集、标注、多轮对话生成和基线评估整合为统一流程，支持多维度标注。

Result: 简单基线和先进LLM API整体精度相当，但高风险情况差异大；部分LLM严重等级排序一致但绝对风险分类失败；多轮对话中性能差距扩大。

Conclusion: 传统基准不适用于高安全要求的心理健康场景，开放MHDash平台促进可复现研究、透明评估和安全开发。

Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.

</details>


### [11] [MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI](https://arxiv.org/abs/2602.01086)
*Takahito Nakajima*

Main category: cs.AI

TL;DR: 论文提出MedBeads解决大语言模型在医疗应用部署受限问题，实现流程并证明其能解决上下文不匹配问题，还开源推广。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽有医疗知识，但作为临床代理部署受限，现有电子病历及标准存在上下文不匹配问题，导致幻觉和审计困难。

Method: 提出MedBeads，将临床事件作为Merkle有向无环图节点，实现原型并包含Go核心引擎、Python中间件和React可视化界面。

Result: 使用合成数据成功实现流程，将FHIR资源转换为因果图，BFS上下文检索算法可实时决策支持，保证防篡改，可视化辅助医生理解。

Conclusion: MedBeads解决上下文不匹配问题，提供可信医疗AI基础，开源以加速代理原生数据标准发展。

Abstract: Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous "Clinical Agents" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a "Context Mismatch": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable "Beads"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This "write-once, read-many" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the "Context Mismatch" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for "Trustworthy Medical AI." It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient "AI-native language." We release MedBeads as open-source software to accelerate agent-native data standards.

</details>


### [12] [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)
*Minhua Lin,Hanqing Lu,Zhan Shi,Bing He,Rui Mao,Zhiwei Zhang,Zongyu Wu,Xianfeng Tang,Hui Liu,Zhenwei Dai,Xiang Zhang,Suhang Wang,Benoit Dumoulin,Jian Pei*

Main category: cs.AI

TL;DR: 大语言模型从特定训练集进入现实环境时，静态训练无法跟上环境变化，提出代理进化是LLM适应的未来，并给出A - Evolve框架和进化扩展假设。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型静态训练无法适应现实环境持续变化的问题。

Method: 提出通用框架A - Evolve，将部署时改进视为对持久系统状态的有目标优化过程；提出进化扩展假设。

Result: 无明确提及具体结果

Conclusion: 代理进化是大语言模型适应的必然未来，是实现现实世界持续、开放式适应的可扩展途径。

Abstract: As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.

</details>


### [13] [Legal Infrastructure for Transformative AI Governance](https://arxiv.org/abs/2602.01474)
*Gillian K. Hadfield*

Main category: cs.AI

TL;DR: 论文指出人工智能治理不应只关注实质规则，还应构建法律和监管框架，并列举三个具体示例。


<details>
  <summary>Details</summary>
Motivation: 人工智能具有变革性，当前治理多关注实质规则，而法律应发挥建立规则生成和执行的基础设施的作用，因此需要构建法律和监管框架。

Method: 回顾作者提出的三个例子，包括为前沿模型创建注册制度、为自主智能体创建注册和识别制度、设计监管市场以便私营公司创新和提供人工智能监管服务。

Result: 提出三个有助于构建人工智能法律和监管框架的具体示例。

Conclusion: 人工智能治理需重视构建法律和监管框架，所提出的例子能有所帮助。

Abstract: Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.

</details>


### [14] [POET: Protocol Optimization via Eligibility Tuning](https://arxiv.org/abs/2602.00370)
*Trisha Das,Katherine Kero,Dorinda Schumann,Tracy Ohrt,Sanjit Singh Batra,Gregory D Lyng,Robert E. Tillman*

Main category: cs.AI

TL;DR: 提出引导式生成框架和基于规则的评估框架用于生成临床试验资格标准，表现优于无引导生成。


<details>
  <summary>Details</summary>
Motivation: 现有自动化生成临床试验资格标准的方法要么需高度结构化输入，要么实用性有限，需更好方法。

Method: 提出引导式生成框架，引入可解释语义轴引导生成；提出基于规则的评估框架评估生成的标准。

Result: 引导式生成方法在自动、基于规则和临床医生评估中均优于无引导生成。

Conclusion: 该方法为人工智能辅助试验设计提供了实用且可解释的解决方案。

Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.

</details>


### [15] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

TL;DR: 提出KEPO框架解决推理导向强化学习后训练难题，在医学视觉问答基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 推理导向的强化学习后训练因稀疏轨迹级奖励面临挑战，现有均匀蒸馏方法不适用于推理密集型任务。

Method: 提出KEPO统一后训练框架，包含质量门控的策略内蒸馏目标和知识增强的探索策略。

Result: 在医学视觉问答基准测试中，KEPO训练更稳定，推理行为更连贯，分布外性能更优。

Conclusion: KEPO框架能有效解决推理导向强化学习后训练的问题。

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [16] [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039)
*Wei Liu,Peijie Yu,Michele Orini,Yali Du,Yulan He*

Main category: cs.AI

TL;DR: 引入Deep Data Research (DDR)和DDR - Bench评估大语言模型调查智能，结果显示前沿模型有初步自主性，但长时探索有挑战，有效调查智能依赖多种因素。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理期望超出正确回答，需要调查智能，而数据科学领域缺乏相关基准。

Method: 引入DDR开放任务让大语言模型自主从数据库提取关键见解，以及基于清单的大规模DDR - Bench基准进行可验证评估。

Result: 前沿模型展现出初步自主性，但长时探索仍具挑战性。

Conclusion: 有效调查智能不仅依赖代理脚手架或模型扩展，还依赖代理模型的内在策略。

Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.

</details>


### [17] [RobustDebias: Debiasing Language Models using Distributionally Robust Optimization](https://arxiv.org/abs/2602.00405)
*Deep Gandhi,Katyani Singh,Nidhi Hegde*

Main category: cs.AI

TL;DR: 提出RobustDebias机制在微调时对预训练语言模型去偏，实验表明能显著减轻偏见且对性能影响小。


<details>
  <summary>Details</summary>
Motivation: 先前去偏方法难扩展、会降低模型性能和放大偏见，需解决微调时的偏见放大问题。

Method: 提出RobustDebias机制，将Distributionally Robust Optimization用于微调时去偏。

Result: 在多种语言模型上实验显示可显著减轻偏见，对性能影响极小。

Conclusion: 该机制能在微调时对多人口统计学特征去偏，适用于任何数据集或任务。

Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.

</details>


### [18] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

TL;DR: 随着多模态智能体发展，需有逻辑可验证性的记忆系统。本文提出无训练的PolarMem，通过非参数分布划分和极化图拓扑存储否定约束，推理时抑制幻觉模式，实验证明其能为多模态智能体奠定基础。


<details>
  <summary>Details</summary>
Motivation: 多模态智能体从被动观察者向长时决策制定者发展，当前架构存在概率视觉语言模型和密集关联记忆的认知不对称问题，无法编码负约束，需要有逻辑可验证性的记忆系统。

Method: 引入无训练的Polarized Latent Graph Memory（PolarMem），通过非参数分布划分将模糊感知可能性转化为离散逻辑约束，采用具有正交抑制连接的极化图拓扑明确存储验证的否定，推理时实施逻辑主导的检索范式。

Result: 在八个冻结视觉语言模型和六个基准测试上的广泛评估表明，PolarMem作为一个强大的认知系统起作用。

Conclusion: PolarMem为可验证的多模态智能体奠定了基础。

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [19] [Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks](https://arxiv.org/abs/2602.00449)
*Jia Liang,Liangming Pan*

Main category: cs.AI

TL;DR: 研究CODI模型在多项式迭代任务中的潜在思维链机制，指出其在不同跳数任务中的表现及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: Latent - CoT机制不明，研究CODI模型在严格顺序多项式迭代任务中的机制。

Method: 使用logit - lens解码、线性探针、注意力分析和激活修补等方法定位中间状态表示并追踪其路由。

Result: 两跳和三跳任务中，CODI形成可解码的完整桥接状态；长跳数任务中，执行部分潜在推理路径，部分路径在机制转变时会崩溃。

Conclusion: 明确CODI式潜在思维链何时产生可靠迭代计算，强调为顺序推理设计稳健潜在思维链目标的挑战。

Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.

</details>


### [20] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: 介绍跨模态压缩框架DebateOCR，可压缩多智能体辩论文本，减少输入令牌和计算成本，理论上多智能体多样性有助于信息恢复。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论随着轮数和智能体数量增加，上下文快速增长，保留完整文本历史会超上下文限制，需重复总结带来开销和信息损失。

Method: 引入DebateOCR框架，用紧凑图像表示替换长文本辩论痕迹，通过专用视觉编码器处理以进行后续轮次。

Result: 压缩了通常包含数万到数十万个令牌的历史，将输入令牌减少超92%，在多个基准测试中降低计算成本并加快推理速度。

Conclusion: 多智能体的多样性支持恢复遗漏信息，聚合多个智能体压缩视图可使集体表示以极大概率接近信息瓶颈。

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [21] [Benchmarking Agents in Insurance Underwriting Environments](https://arxiv.org/abs/2602.00456)
*Amanda Dsouza,Ramya Ramakrishnan,Charles Dickens,Bhavishya Pohani,Christopher M Glaze*

Main category: cs.AI

TL;DR: 现有AI基准不适用于企业应用评估，提出UNDERWRITE保险承保基准评估模型，发现实验室与企业应用性能差距，强调专家参与的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准无法反映企业应用的现实复杂性，需新基准来评估融入企业应用的AI智能体。

Method: 与领域专家合作设计UNDERWRITE基准，用其评估13个前沿模型。

Result: 发现研究实验室性能与企业就绪度存在显著差距，如最准确模型并非最有效率等。

Conclusion: 基准设计需专家参与，通用框架有缺陷，专业领域幻觉检测需组合方法，为开发符合企业需求的基准提供见解。

Abstract: As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.

</details>


### [22] [Dual Latent Memory for Visual Multi-agent System](https://arxiv.org/abs/2602.00471)
*Xinlei Yu,Chengming Xu,Zhangquan Chen,Bo Yin,Cheng Yang,Yongbo He,Yihao Hu,Jiangning Zhang,Cheng Tan,Xiaobin Hu,Shuicheng Yan*

Main category: cs.AI

TL;DR: 现有VMAS存在‘扩展墙’问题，本文提出L² - VMAS框架解决该问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: VMAS存在‘扩展墙’，即增加智能体回合会降低性能且增加令牌成本，原因是文本通信的信息瓶颈。

Method: 提出L² - VMAS框架，采用双潜在记忆实现智能体协作，解耦感知和思考，动态合成双潜在记忆，引入熵驱动主动触发机制。

Result: 在多种骨干网络、规模和多智能体结构的实验中，有效打破‘扩展墙’，平均准确率提高2.7 - 5.4%，令牌使用减少21.3 - 44.8%。

Conclusion: L² - VMAS框架具有出色的可扩展性，能有效解决VMAS的‘扩展墙’问题。

Abstract: While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive "scaling wall": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the "scaling wall" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.

</details>


### [23] [Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models](https://arxiv.org/abs/2602.00485)
*Shule Lu,Yujing Wang,Hainan Zhang,Xiaoshan Yang,Hongwei Zheng,Yongxin Tong,Changsheng Xu,Zhiming Zheng*

Main category: cs.AI

TL;DR: 本文针对异构视觉语言模型（VLMs）在联邦学习（FL）中的对齐问题，提出MoR框架，实验显示其在泛化、鲁棒性和跨客户端适应性上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: VLMs在隐私敏感领域有应用潜力，但集中训练因数据共享限制不可行，FL面临客户端异构性挑战，且替换参数为偏好是更具扩展性和隐私保护的未来方向。

Method: 提出基于GRPO和混合奖励的联邦对齐框架MoR，初始化视觉基础模型为参考，客户端本地训练奖励模型，引入路由融合机制聚合奖励信号，服务器用混合奖励进行GRPO优化基础VLM。

Result: 在三个公共VQA基准测试上，MoR在泛化、鲁棒性和跨客户端适应性上始终优于联邦对齐基线方法。

Conclusion: 该方法为联邦设置下异构VLMs的隐私保护对齐提供了可扩展的解决方案。

Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.

</details>


### [24] [PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)](https://arxiv.org/abs/2602.00510)
*Huanghaohe Zou,Peng Han,Emad Nazerian,Alex Q. Huang*

Main category: cs.AI

TL;DR: 介绍首个免训练的PCB原理图设计框架PCBSchemaGen，含LLM代理和约束引导合成，实验显示其提升设计准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单一类型电路，且因开源数据稀缺和缺乏基于仿真的验证，自动化PCB原理图设计未充分探索。

Method: 提出含LLM代理和约束引导合成的PCBSchemaGen框架，包括基于LLM的代码生成范式、利用知识图和子图同构的验证框架。

Result: 在23个PCB原理图任务实验中，PCBSchemaGen显著提高了设计准确性和计算效率。

Conclusion: PCBSchemaGen框架在PCB原理图设计中具有良好效果，能提高设计准确性和计算效率。

Abstract: Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.

</details>


### [25] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: 提出基于IRT的两阶段诊断框架评估LLM-as-a-Judge可靠性，可提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM-as-a-Judge的验证主要基于观测输出，缺乏对其作为测量工具稳定性和可靠性的深入了解。

Method: 引入基于项目反应理论（IRT）的两阶段诊断框架，采用IRT的分级反应模型（GRM），从内在一致性和人类对齐两个维度评估。

Result: 利用IRT - GRM能系统地诊断判断并产生可解释的信号。

Conclusion: 这些信号为验证LLM - as - a - Judge的可靠性和找出不可靠的潜在原因提供了实用指导。

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [26] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 研究大语言模型（LLMs）在扑克任务中的表现，发现其不足后提出ToolPoker框架，实现了先进游戏玩法并符合博弈论原则。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险领域应用时，其在不确定性下的策略推理能力至关重要，扑克是严格测试平台，而现有LLMs表现不佳。

Method: 对LLMs在多个现实扑克任务中进行系统研究，先尝试行为克隆和逐步强化学习，后提出ToolPoker工具集成推理框架。

Result: LLMs难以与传统算法竞争，存在三种缺陷；行为克隆和强化学习改进有限；ToolPoker实现了最先进的游戏玩法，推理过程符合博弈论原则。

Conclusion: ToolPoker框架能有效解决LLMs在扑克任务中的问题，实现符合博弈论的游戏玩法。

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [27] [Uncovering Latent Communication Patterns in Brain Networks via Adaptive Flow Routing](https://arxiv.org/abs/2602.00561)
*Tianhao Huang,Guanghui Min,Zhenyu Lei,Aiying Zhang,Chen Chen*

Main category: cs.AI

TL;DR: 本文提出AFR - Net以神经通信动力学视角进行多模态融合，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏神经科学见解，无法揭示连接组潜在神经区域交互，不能解释SC和FC的耦合与异质性动态状态，需要新方法解决。

Method: 从神经通信动力学视角构建多模态融合，提出物理信息框架AFR - Net，模拟结构约束产生功能通信模式。

Result: AFR - Net在大量实验中显著优于现有基线。

Conclusion: AFR - Net是有效的，能实现关键神经通路的可解释发现。

Abstract: Unraveling how macroscopic cognitive phenotypes emerge from microscopic neuronal connectivity remains one of the core pursuits of neuroscience. To this end, researchers typically leverage multi-modal information from structural connectivity (SC) and functional connectivity (FC) to complete downstream tasks. Recent methodologies explore the intricate coupling mechanisms between SC and FC, attempting to fuse their representations at the regional level. However, lacking fundamental neuroscientific insight, these approaches fail to uncover the latent interactions between neural regions underlying these connectomes, and thus cannot explain why SC and FC exhibit dynamic states of both coupling and heterogeneity. In this paper, we formulate multi-modal fusion through the lens of neural communication dynamics and propose the Adaptive Flow Routing Network (AFR-Net), a physics-informed framework that models how structural constraints (SC) give rise to functional communication patterns (FC), enabling interpretable discovery of critical neural pathways. Extensive experiments demonstrate that AFR-Net significantly outperforms state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/DIAL-F0D1.

</details>


### [28] [Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs](https://arxiv.org/abs/2602.00564)
*Xiang Zheng,Weiqi Zhai,Wei Wang,Boyu Yang,Wenbo Li,Ruixiang Luo,Haoxiang Sun,Yucheng Wang,Zhengze Li,Meng Wang,Yuetian Du,Guojie Lin,Yaxuan Wang,Xiaoxiao Xu,Yanhu Mo,Xuan Ren,Hu Wei,Ze Xu*

Main category: cs.AI

TL;DR: 现有大语言模型在数学推理基准测试中准确率饱和，提出ReasoningMath - Plus基准和HCRS评分函数，显示仅看答案指标会高估推理稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集无法有效诊断大语言模型真正的推理能力，缺乏如多约束协调等推理技能的体现。

Method: 引入ReasoningMath - Plus基准评估结构推理，提出HCRS评分函数，训练过程奖励模型。

Result: 领先模型最终答案准确率较高，但HCRS整体评估得分低。

Conclusion: 仅看答案的指标会高估推理的稳健性。

Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.

</details>


### [29] [Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings](https://arxiv.org/abs/2602.00574)
*Yifei Shao,Kun Zhou,Ziming Xu,Mohammad Atif Quamar,Shibo Hao,Zhen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: 研究将思维链扩展到多模态推理，提出模态混合思维链方法，实验表明性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有文本形式的思维链在视觉密集问题上表现不佳，需扩展到多模态以更好处理推理。

Method: 引入模态混合思维链，用VLM作为编码器训练语言骨干，附加基于扩散的潜在解码器，分监督微调与强化学习两阶段训练。

Result: 在11个多模态推理任务实验中，性能优于仅语言和其他思维链方法。

Conclusion: 提出的方法有效，代码将公开。

Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.

</details>


### [30] [Small Shifts, Large Gains: Unlocking Traditional TSP Heuristic Guided-Sampling via Unsupervised Neural Instance Modification](https://arxiv.org/abs/2602.00580)
*Wei Huang,Hanchen Wang,Dong Wen,Wenjie Zhang*

Main category: cs.AI

TL;DR: 提出TSP - MDF框架提升传统启发式旅行商问题构造器性能，效果好且训练时间短。


<details>
  <summary>Details</summary>
Motivation: 传统启发式旅行商问题构造器易陷入局部最优，基于神经网络的构造器需大量训练和监督，缺乏实用性。

Method: 提出TSP - MDF实例修改框架，利用基于神经网络的实例修改器移动节点坐标采样，传统构造器在修改实例上构造路径再映射回原实例。

Result: 在大规模TSP基准和真实世界基准的大量实验中，TSP - MDF显著提升传统启发式构造器性能，解质量与基于神经网络的构造器相当，训练时间极短。

Conclusion: TSP - MDF能有效提升传统启发式构造器性能，兼具实用性。

Abstract: The Traveling Salesman Problem (TSP) is one of the most representative NP-hard problems in route planning and a long-standing benchmark in combinatorial optimization. Traditional heuristic tour constructors, such as Farthest or Nearest Insertion, are computationally efficient and highly practical, but their deterministic behavior limits exploration and often leads to local optima. In contrast, neural-based heuristic tour constructors alleviate this issue through guided-sampling and typically achieve superior solution quality, but at the cost of extensive training and reliance on ground-truth supervision, hindering their practical use. To bridge this gap, we propose TSP-MDF, a novel instance modification framework that equips traditional deterministic heuristic tour constructors with guided-sampling capability. Specifically, TSP-MDF introduces a neural-based instance modifier that strategically shifts node coordinates to sample multiple modified instances, on which the base traditional heuristic tour constructor constructs tours that are mapped back to the original instance, allowing traditional tour constructors to explore higher-quality tours and escape local optima. At the same time, benefiting from our instance modification formulation, the neural-based instance modifier can be trained efficiently without any ground-truth supervision, ensuring the framework maintains practicality. Extensive experiments on large-scale TSP benchmarks and real-world benchmarks demonstrate that TSP-MDF significantly improves the performance of traditional heuristics tour constructors, achieving solution quality comparable to neural-based heuristic tour constructors, but with an extremely short training time.

</details>


### [31] [Exploring Information Seeking Agent Consolidation](https://arxiv.org/abs/2602.00585)
*Guochen Yan,Jialong Wu,Zhengwei Tao,Bo Li,Qintong Zhang,Jiahao Xu,Haitao Mi,Yuejian Fang,Qingni Shen,Wentao Zhang,Zhonghai Wu*

Main category: cs.AI

TL;DR: 研究如何将异构信息搜索代理整合为单一基础代理模型，比较两种整合策略，指出数据级整合是强稳定基线，参数级整合有潜力但有挑战，并确定参数级整合的关键设计因素。


<details>
  <summary>Details</summary>
Motivation: 现有信息搜索代理通常针对特定领域，限制了可扩展性和跨领域泛化能力，因此要将异构信息搜索代理整合为单一基础代理模型。

Method: 研究数据级整合（在特定领域数据集混合上联合训练统一模型）和参数级整合（在参数级别合并独立训练的代理模型）两种互补策略，并比较它们在性能保留、跨领域泛化和信息搜索行为干扰方面的表现。

Result: 数据级整合是强稳定基线，参数级整合是有前景的高效替代方案，但存在干扰和鲁棒性挑战。

Conclusion: 确定了参数级有效代理整合的关键设计因素，如细粒度合并粒度、任务异质性意识和原则性共识策略。

Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.

</details>


### [32] [DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder](https://arxiv.org/abs/2602.00592)
*Jiaran Zhang,Luck Ma,Yanhao Li,Fanqi Wan,Di Qi,Xu Zhao,Jieyi Hou,Zhe Xie,Mengqiang Ren,Xin Wu,Zhewei Huang,Liangyu Chen,Yingwei Ma,Qi Han,Xiangyu Zhang*

Main category: cs.AI

TL;DR: 提出DockSmith解决基于Docker的可靠环境构建瓶颈问题，训练模型在评估中达开源最优，还提升了分布外性能


<details>
  <summary>Details</summary>
Motivation: 可靠的基于Docker的环境构建是软件工程代理扩展性训练和评估的主要瓶颈

Method: 构建DockSmith，将环境构建作为核心代理能力，在由增强型SWE - Factory流水线生成的大规模基于执行的Docker构建轨迹上训练30B - A3B模型

Result: 训练的模型在Multi - Docker - Eval上达开源最优，有39.72%的Fail - to - Pass和58.28%的Commit Rate，且提升了多个数据集分布外性能

Conclusion: DockSmith解决了环境构建瓶颈，体现了环境构建更广泛的代理益处

Abstract: Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.

</details>


### [33] [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)
*Wei Zeng,Xuchen Li,Ruili Feng,Zhen Liu,Fengwei An,Jian Zhao*

Main category: cs.AI

TL;DR: 本文提出一种可扩展的硬件 - 算法协同设计框架，解决实时生成游戏引擎中高分辨率生成的内存墙问题，在可编程AI加速器集群上验证，实现高分辨率实时生成，证明架构协同设计对高保真、响应式神经游戏的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有实时生成游戏引擎方法受“内存墙”限制，只能用于低分辨率，需实现生成模型与高分辨率神经模拟的结合。

Method: 提出异构架构，将世界模型和解码器组件在AI加速器集群上解耦，有三项核心创新：非对称资源分配策略、以内存为中心的算子融合方案、流形感知潜在外推机制。

Result: 在可编程AI加速器集群上实现720×480分辨率实时生成，像素吞吐量比之前基线提高50倍，在3D赛车和2D平台游戏基准测试中分别达到26.4 FPS和48.3 FPS，有效延迟2.7 ms。

Conclusion: 通过架构协同设计解决“内存墙”问题是实现高保真、响应式神经游戏的前提。

Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \times 480$ resolution -- a $50\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.

</details>


### [34] [From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development](https://arxiv.org/abs/2602.00699)
*Xuan Liu,Ziyu Li,Mu He,Ziyang Ma,Xiaoxu Wu,Gizem Yilmaz,Yiyuan Xia,Bingbing Li,He Tan,Jerry Ying Hsi Fuh,Wen Feng Lu,Anders E. W. Jarfors,Per Jansson*

Main category: cs.AI

TL;DR: 传统本体构建劳动密集且成本高，研究用三种基于大语言模型方法从特定领域文本中提取术语和关系，选最优方法构建铸造本体并经专家验证。


<details>
  <summary>Details</summary>
Motivation: 传统本体构建依赖人工和常规NLP技术，过程劳动密集且成本高，大语言模型为知识提取自动化带来新可能。

Method: 研究三种基于大语言模型的方法，包括预训练大语言模型驱动方法、上下文学习方法和微调方法，比较性能。

Result: 未提及具体结果数据，但得出了最佳性能方法。

Conclusion: 可使用最佳性能的大语言模型方法构建铸造本体并通过专家验证。

Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.

</details>


### [35] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: 对虚拟家庭基准测试上的大语言模型进行综合评估，提出SSC策略提升性能，分析模型优势。


<details>
  <summary>Details</summary>
Motivation: 在模拟环境中评估大语言模型对目标理解、行动规划和任务执行的能力，为具身AI系统开发提供参考。

Method: 使用EAI框架在虚拟家庭基准测试评估LLMs，对比OPENPANGU - 7B和QWEN2.5 - 7B在四项基本任务表现，提出SSC解码策略。

Result: SSC显著提升性能，OPENPANGU - 7B擅长层次规划，QWEN2.5 - 7B在行动级任务有优势。

Conclusion: 不同类型模型有互补优势，为未来具身AI系统开发提供见解。

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [36] [Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees](https://arxiv.org/abs/2602.00616)
*Minhyuk Lee,Hyekyung Yoon,Myungjoo Kang*

Main category: cs.AI

TL;DR: 文本到图像扩散模型部署需保障安全且不降低良性提示与图像的对齐度，本文提出推理式提示投影框架，在多个数据集和骨干网络上减少不当生成比例且保持良性对齐。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型在现实部署中，抑制不安全生成同时不降低良性提示与图像对齐度的问题。

Method: 提出推理式提示投影框架，通过带验证的代理目标对高风险提示进行选择性干预，将其映射到安全集，不重新训练或微调生成器。

Result: 在四个数据集和三个扩散骨干网络上，与强模型级对齐基线相比，不当百分比相对降低16.7 - 60.0%，在COCO上保持良性提示与图像的对齐接近未对齐参考。

Conclusion: 所提方法能有效解决文本到图像扩散模型的安全与提示对齐权衡问题。

Abstract: Text-to-Image (T2I) diffusion models enable high-quality open-ended synthesis, but their real-world deployment demands safeguards that suppress unsafe generations without degrading benign prompt-image alignment. We formalize this tension through a total variation (TV) lens: once the reference conditional distribution is fixed, any nontrivial reduction in unsafe generations necessarily incurs TV deviation from the reference, yielding a principled Safety-Prompt Alignment Trade-off (SPAT). Guided by this view, we propose an inference-only prompt projection framework that selectively intervenes on high-risk prompts via a surrogate objective with verification, mapping them into a tolerance-controlled safe set while leaving benign prompts effectively unchanged, without retraining or fine-tuning the generator. Across four datasets and three diffusion backbones, our approach achieves 16.7-60.0% relative reductions in inappropriate percentage (IP) versus strong model-level alignment baselines, while preserving benign prompt-image alignment on COCO near the unaligned reference.

</details>


### [37] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 本文提出基于模糊相似推理的可解释预后框架来估计超滤膜剩余使用寿命，经测试误差小且规则库可解释。


<details>
  <summary>Details</summary>
Motivation: 反渗透海水淡化中，超滤膜因结垢性能下降，现有预测性维护模型缺乏可解释性和操作员信任。

Method: 提出基于模糊相似推理的预后框架，利用物理信息健康指标捕捉降解动态，通过高斯隶属函数模糊化，用相似性度量识别历史降解轨迹并以Takagi - Sugeno模糊规则进行剩余使用寿命预测。

Result: 在工业规模超滤系统的12528个运行周期上测试，平均绝对误差为4.50个周期。

Conclusion: 该框架能实现低误差的剩余使用寿命估计，且生成的规则库与专家理解一致，具有可解释性。

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [38] [SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent](https://arxiv.org/abs/2602.00663)
*Fabian P. Krüger,Andrea Hunklinger,Adrian Wolny,Tim J. Adler,Igor Tetko,Santiago David Villalba*

Main category: cs.AI

TL;DR: 介绍分子优化LLM代理SEISMO，其在多任务表现超先前方法，利用领域知识和信息是高效关键。


<details>
  <summary>Details</summary>
Motivation: 分子优化需高样本效率，解决分子结构优化瓶颈问题。

Method: 引入SEISMO，基于完整优化轨迹提出方案，结合自然语言任务描述、标量分数和解释性反馈。

Result: 在23个任务基准测试中，优化曲线下面积比先前方法高2 - 3倍，50次调用内常接近最大任务得分，提供解释性反馈可提高效率。

Conclusion: 利用领域知识和结构化信息是样本高效分子优化的关键。

Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.

</details>


### [39] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: 论文提出新基准OpenGuanDan评估掼蛋AI智能体，实验显示当前学习型智能体未达人类水平，需持续研究。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的人工智能发展依赖大规模基准，当前缺乏更具挑战性的基准，需要新基准推动研究。

Method: 提出OpenGuanDan基准，模拟掼蛋游戏并评估AI智能体，进行智能体间两两比赛和人机对战两种评估。

Result: 当前学习型掼蛋AI智能体大幅优于规则型，但未达人类水平。

Conclusion: 多智能体智能决策领域需持续研究。

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [40] [HumanStudy-Bench: Towards AI Agent Design for Participant Simulation](https://arxiv.org/abs/2602.00685)
*Xuan Liu,Haoyang Shang,Zizhang Liu,Xinyan Liu,Yunze Xiao,Yiwen Tu,Haojian Jin*

Main category: cs.AI

TL;DR: 本文指出大语言模型在社科实验中行为不稳定，提出将参与者模拟作为代理设计问题，引入HUMANSTUDY - BENCH基准和执行引擎，提出新指标评估，还实例化12项研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在社科实验中行为不稳定，且先前评估混淆基础模型能力和实验实例化，难以判断结果来源。

Method: 将参与者模拟视为代理设计问题，通过Filter - Extract - Execute - Evaluate管道引入HUMANSTUDY - BENCH基准和执行引擎，提出新指标量化人类和代理行为的一致性。

Result: 实例化了12项基础研究，涵盖6000多个试验，人类样本从几十到2100多名参与者。

Conclusion: 该研究提供一种新的方式来利用大语言模型进行社科实验，保证实验的科学推理和结果可信度。

Abstract: Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.

</details>


### [41] [Self-Guard: Defending Large Reasoning Models via enhanced self-reflection](https://arxiv.org/abs/2602.00707)
*Jingnan Zheng,Jingjun Xu,Yanzhen Luo,Chenhang Cui,Gelei Deng,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 提出Self - Guard框架解决大推理模型安全对齐问题，实验证明有效且具泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型存在推理操纵和信息泄露等风险，现有对齐策略计算量大且无法解决意识 - 合规差距问题。

Method: 提出Self - Guard轻量级安全防御框架，包括安全导向提示和安全激活引导两个阶段。

Result: Self - Guard有效弥合意识 - 合规差距，在不影响模型效用的前提下实现强大安全性能，且在不同风险和模型规模上有强泛化性。

Conclusion: Self - Guard为大型推理模型安全对齐提供了经济有效的解决方案。

Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.

</details>


### [42] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: 本文介绍了医疗AI系统'Maria'平台案例，提出通过四个工程支柱的整体集成实现可信临床AI，并给出参考架构。


<details>
  <summary>Details</summary>
Motivation: 人工智能融入临床面临软件工程挑战，现有工业应用存在架构脆弱、缺乏系统监督等问题，导致安全和问责性受损，需解决此差距。

Method: 提出协同架构，结合清洁架构保证可维护性、事件驱动架构保证弹性和可审计性；引入代理作为模块化单元，有自主MLOps生命周期；将人在回路治理模型作为关键事件驱动数据源进行技术集成。

Result: 展示了'Maria'平台作为参考架构。

Conclusion: 为高风险领域构建可维护、可扩展和可问责的AI系统的工程师提供了实用经验。

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [43] [Physics-informed Diffusion Generation for Geomagnetic Map Interpolation](https://arxiv.org/abs/2602.00709)
*Wenda Li,Tongya Zheng,Kaixuan Chen,Shunyu Liu,Haoze Jiang,Yunzhi Hao,Rui Miao,Zujie Ren,Mingli Song,Hang Shi,Gang Chen*

Main category: cs.AI

TL;DR: 提出物理感知扩散生成框架(PDG)用于地磁图插值，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有离散数据插值方法非专为地磁图设计，面临检测噪声和物理定律问题，性能不佳。

Method: 设计基于局部感受野的物理感知掩码策略消除噪声干扰；依据地磁图克里金原理施加物理感知约束。

Result: 在四个真实数据集上的大量实验和深入分析表明PDG各组件具有优越性和有效性。

Conclusion: PDG能有效实现地磁图插值，应对现有方法的不足。

Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.

</details>


### [44] [Learning More from Less: Unlocking Internal Representations for Benchmark Compression](https://arxiv.org/abs/2602.00710)
*Yueqi Zhang,Jin Hu,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yiwei Li,Jiayi Shi,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: 现有评估大语言模型成本高，现有方法有局限，本文提出REPCORE方法，在多基准测试中表现好。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型成本高，现有方法依赖大量源模型，在源模型少或新基准测试时不稳定，且离散正确性标签有信息损失。

Method: 引入REPCORE，将异构隐藏状态对齐到统一潜空间构建代表性核心集。

Result: 在五个基准测试和200多个模型的实验中，REPCORE在排名相关性和估计准确性上比基于输出的基线方法有持续提升，光谱分析表明对齐表示包含反映广泛响应趋势和特定任务推理模式的可分离组件。

Conclusion: REPCORE能以少至十个源模型实现精确估计，是评估大语言模型性能的有效方法。

Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.

</details>


### [45] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: 本文介绍了多智能体系统CAREP，它能从DTC高维事件序列自动生成EP规则，在汽车数据集上表现出色，推动了全自动故障诊断。


<details>
  <summary>Details</summary>
Motivation: 当前汽车制造商的EP规则由领域专家手工制作，成本高且易出错，随着车辆复杂度增加问题更突出。

Method: 提出CAREP系统，结合因果发现智能体、上下文信息智能体和协调智能体，从DTC事件序列生成EP规则。

Result: 在大规模汽车数据集上，CAREP能自动准确发现未知EP规则，优于仅用大语言模型的基线方法，且提供透明因果解释。

Conclusion: CAREP结合实际因果发现和基于智能体的推理，是迈向全自动故障诊断的一步，可实现可扩展、可解释且经济高效的车辆维护。

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [46] [Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations](https://arxiv.org/abs/2602.00731)
*Kyle Hamilton,Ali Intizar*

Main category: cs.AI

TL;DR: 对过去五年工业环境下预测性维护（PdM）进行系统综述，指出数据驱动和传统方法的优缺点，提出结合深度学习与符号逻辑的混合方法，介绍相关架构及优缺点，概述现代维护背景等。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法在PdM中存在需大量标签数据、缺乏泛化性和透明度等问题，传统方法存在准确性差等问题，需要寻找能结合两者优势的方法。

Method: 对现有的数据驱动、传统及混合方法进行研究分析，并提出将深度学习与符号逻辑结合的神经符号AI（NESY）方法，描述多个神经符号架构。

Result: 分析了不同方法在PdM领域的优缺点，介绍了神经符号架构的特点。

Conclusion: 神经符号AI方法有潜力创建更准确、可解释、可理解和鲁棒的PdM系统，本综述为现代维护提供了框架，介绍了当前方法及挑战。

Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).

</details>


### [47] [Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models](https://arxiv.org/abs/2602.00780)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Zenghuan Zhu,Jiajun Deng,Xinrui Lin,Shuo Liu,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.AI

TL;DR: 提出EcoVLA框架来解决VLA模型推理延迟问题，在多模型和基准测试上表现出色。


<details>
  <summary>Details</summary>
Motivation: VLA模型参数多导致推理延迟，现有剪枝方法无法适应环境动态变化。

Method: 提出由EAP和I²O组成的EcoVLA框架，EAP结合环境时间一致性更新稀疏模式，I²O利用FLOPs空泡并行调度剪枝方法。

Result: 在多VLA模型和基准测试中，EcoVLA实现了1.60倍加速且成功率仅下降0.4%，结合token剪枝达到2.18倍加速且仅下降0.5%，并在真实机器人上验证有效。

Conclusion: EcoVLA是一种无训练、即插即用的自适应剪枝框架，能有效解决VLA模型推理延迟问题。

Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.

</details>


### [48] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: 提出模仿工程团队结构的多智能体系统，可自动解决软件问题，在SWE - bench表现超单智能体基线，显示组织设计和智能体基础设施重要性。


<details>
  <summary>Details</summary>
Motivation: 当前多数自主系统将问题解决视为整体或基于流水线的过程，而实际软件开发是团队协作活动，因此想构建模仿团队结构的多智能体系统。

Method: 基于agyn平台构建多智能体系统，为不同角色分配专业智能体，提供隔离沙箱和结构化通信，遵循既定开发方法且无需人工干预。

Result: 该系统在SWE - bench 500上解决了72.4%的任务，优于使用类似语言模型的单智能体基线。

Conclusion: 复制团队结构、方法和通信是自主软件工程的强大范式，未来进展可能同样依赖组织设计和智能体基础设施与模型改进。

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [49] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

TL;DR: LLM智能体在低代价环境表现超人类，但在高代价复杂领域受限，论文提出用世界模型解决问题并指出挑战与行动项。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在高代价复杂领域因行动获取奖励信号成本高而难以提升性能的问题。

Method: 将世界模型作为智能体与现实世界的中介，发挥其在处理动力学、奖励和任务分布等方面的作用。

Result: 显示世界模型能为多个领域的智能体提供关键且丰富的学习信号。

Conclusion: 构建世界模型面临挑战，并给出了数据整理、架构设计、扩展和评估等方面的行动项。

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [50] [ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development](https://arxiv.org/abs/2602.01655)
*Pengrui Lu,Shiqi Zhang,Yunzhong Hou,Lyumanshan Ye,Chaoyi Huang,Zixi Chen,Ji Zeng,Hantao Jiang,Pengfei Liu,Yiwei Wang,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: 提出ProjDevBench基准评估编码代理端到端开发能力，评估6个编码代理，总体接受率27.38%，基准已开源。


<details>
  <summary>Details</summary>
Motivation: 现有评估侧重于问题级别的错误修复，落后于端到端开发，需要新的端到端评估基准。

Method: 结合在线评测和大语言模型辅助代码审查，从系统架构设计、功能正确性和迭代解决方案改进三方面评估，整理20个编程问题评估6个不同大语言模型后端的编码代理。

Result: 总体接受率27.38%，代理能处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面存在困难。

Conclusion: ProjDevBench可用于评估编码代理的端到端开发能力，揭示了当前编码代理存在的不足。

Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.

</details>


### [51] [MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing](https://arxiv.org/abs/2602.00811)
*Ronghao Lin,Honghao Lu,Ruixing Wu,Aolin Xiong,Qinggong Chu,Qiaolin He,Sijie Mai,Haifeng Hu*

Main category: cs.AI

TL;DR: 本文引入MissMAC - Bench基准，解决多模态情感计算中缺失模态问题，实验验证其有效性，推动相关研究发展。


<details>
  <summary>Details</summary>
Motivation: 当前多模态情感计算依赖多模态完整性，但现实中模态数据可用性动态不确定，缺失模态问题影响模型鲁棒性和实际部署，需系统量化该问题。

Method: 引入MissMAC - Bench基准，提出两条指导原则，在数据集和实例层面集成固定和随机缺失模式评估协议。

Result: 在4个数据集上对3个广泛使用的语言模型进行大量实验，验证了不同多模态情感计算方法处理缺失模态问题的有效性。

Conclusion: 该基准为推进鲁棒多模态情感计算提供坚实基础，促进多媒体数据挖掘发展。

Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.

</details>


### [52] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

TL;DR: 引入BELLA框架用于高效选择大语言模型，可在预算内推荐最优模型，介绍其框架结构与应用领域。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型从业者在任务中选择合适模型不浪费钱的问题，现有的标准基准测试掩盖了任务所需的特定能力及是否可用更便宜的模型。

Method: BELLA框架分三个阶段：用基于评判的分析提取细粒度技能，将技能聚类成结构化能力矩阵，多目标优化选择模型。

Result: 该框架能为推荐提供自然语言理由，弥补现有黑箱路由系统缺乏透明度的问题。

Conclusion: 此框架能让从业者在部署大语言模型时在成本和性能间进行权衡。

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [53] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

TL;DR: 本文探讨RLVR数据和计算效率问题，建立样本复杂度理论下限，提出DoPR策略降低开销并保持推理精度。


<details>
  <summary>Details</summary>
Motivation: RLVR资源消耗大，需解决数据和计算效率问题。

Method: 先建立样本复杂度理论下限，再提出DoPR策略，根据奖励波动性和探索驱动采集动态选样本更新策略。

Result: DoPR将滚动开销降低近一个数量级，保持有竞争力的推理精度。

Conclusion: DoPR为推理密集型大语言模型应用提供高效可扩展的基于强化学习训练方案。

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [54] [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029)
*Zhongyuan Lyu,Shuoyu Hu,Lujie Liu,Hongxia Yang,Ming LI*

Main category: cs.AI

TL;DR: 提出CIR和R2C框架解决从自然语言生成优化模型问题，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的方法难以处理复杂操作规则所需的复合约束和合适建模范式。

Method: 引入CIR架构，基于CIR知识库开发R2C框架，通过多智能体管道解析文本、合成CIR实现并实例化优化模型。

Result: R2C在新基准测试中达到47.2%准确率，在已有基准测试中表现有竞争力，加入反思机制有进一步提升。

Conclusion: R2C框架在从自然语言生成优化模型任务中有效，能达到先进水平。

Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.

</details>


### [55] [Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward](https://arxiv.org/abs/2602.00845)
*Senkang Hu,Yong Dai,Yuzhi Zhao,Yihang Tao,Yu Guo,Zhengru Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.AI

TL;DR: 文章提出InfoReasoner框架，通过合成语义信息增益奖励激励有效信息检索，在问答基准测试中表现优于基线，为检索式智能推理提供理论支撑和可扩展方法。


<details>
  <summary>Details</summary>
Motivation: 解决大推理模型动态获取外部知识时，因缺乏密集、有原则的奖励信号，优化检索过程困难的问题。

Method: 引入InfoReasoner框架，理论上重新定义信息增益，实践中提出输出感知的内在估计器，用双向文本蕴含的语义聚类计算信息增益，通过GRPO算法实现高效训练。

Result: 在七个问答基准测试中，InfoReasoner始终优于强检索增强基线，平均准确率最高提升5.4%。

Conclusion: 为基于检索的智能推理提供了理论可靠且可扩展的途径。

Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.

</details>


### [56] [Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents](https://arxiv.org/abs/2602.02050)
*Zeping Li,Hongru Wang,Yiwen Zhao,Guanhua Chen,Yixia Li,Keyang Chen,Yixin Cao,Guangnan Ye,Hongfeng Chai,Mengdi Wang,Zhenfei Yin*

Main category: cs.AI

TL;DR: 本文提出用熵减作为监督信号设计两种奖励策略优化大语言模型工具使用行为，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型工具使用代理在长轨迹任务中存在工具调用过多、质量低的问题，需要管理工具使用行为。

Method: 进行基于熵的实验，发现熵减与高质量工具调用强正相关，提出用熵减作为监督信号，设计稀疏结果奖励和密集过程奖励两种策略。

Result: 两种奖励设计均改善了工具使用行为，稀疏结果奖励使工具调用比基线平均减少72.07%，密集过程奖励使性能提升22.27%。

Conclusion: 熵减是增强工具使用行为的关键机制，能使代理在现实应用中更具适应性。

Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.

</details>


### [57] [Persuasion Propagation in LLM Agents](https://arxiv.org/abs/2602.00851)
*Hyejun Jeong,Amir Houmansadr,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 研究AI代理在长时任务中受用户说服的影响，提出'说服传播'现象，对比不同说服方式效果，表明即便先前交互中的说服也会影响行为。


<details>
  <summary>Details</summary>
Motivation: 探究AI代理在长时任务中受用户说服时会发生什么，研究信念层面干预对下游任务行为的影响。

Method: 引入以行为为中心的评估框架，区分任务执行期间和之前施加的说服。

Result: 即时说服产生的行为效果弱且不一致；信念预填充的代理比中性预填充的代理平均搜索次数少26.9%，访问的唯一来源少16.9%。

Conclusion: 说服（即使是在先前的交互中）会影响代理的行为，促使在代理系统中进行行为层面的评估。

Abstract: Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\% fewer searches and visit 16.9\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.

</details>


### [58] [Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding](https://arxiv.org/abs/2602.00854)
*Fangzhou Lin,Qianwen Ge,Lingyu Xu,Peiran Li,Xiangbo Gao,Shuo Xing,Kazunori Yamada,Ziming Zhang,Haichong Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 论文指出AI系统发展使能力与理解差距增大，定义认知完整性阈值（CIT）并提出操作维度，推动人机交互设计与治理议程。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法未明确人类在持续AI委托下监督所需基础理解的问题。

Method: 定义认知完整性阈值（CIT），并通过验证能力、保理解交互、治理制度支架三个功能维度将其操作化。

Result: 明确了认知完整性阈值及操作维度，为责任关键场景下的人机交互设计和治理提供依据。

Conclusion: 应推动设计和治理议程，使责任关键场景下的人机交互符合认知可持续性。

Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.

</details>


### [59] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

TL;DR: 本文引入基础CAN模型，将CAN数据视为语言，通过预训练和微调实现多目标下游泛化，验证基础模型范式对CAN数据有效。


<details>
  <summary>Details</summary>
Motivation: 现有CAN数据处理管道孤立训练特定任务模型，缺乏共享表示学习和跨任务泛化，而NLP和CV领域基础模型范式取得成功，因此探索将其应用于CAN数据。

Method: 将CAN数据视为语言，在大规模未标记的解码CAN信号上进行预训练，然后在异构汽车保险任务上进行微调，提出统一的混合离散 - 连续信号标记化方案，解决时间复杂性和行程特定变异性问题。

Result: 一个预训练的CAN模型能有效适应不同的预测任务。

Conclusion: 证明在NLP和CV领域被证明有效的基础模型范式也适用于CAN数据，为汽车AI中可泛化的表示学习开辟了新方向。

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [60] [Beyond Output Critique: Self-Correction via Task Distillation](https://arxiv.org/abs/2602.00871)
*Hossein A. Rahmani,Mengting Wan,Pei Zhou,Longqi Yang,Nick Craswell,Emine Yilmaz,Sujay Kumar Jauhar*

Main category: cs.AI

TL;DR: 提出SELF - THOUGHT框架，在解决方案细化前引入任务抽象中间步骤，跨模型复用任务结构，实验证明可提升大小模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型自纠错方法多停留在输出批判层面，难以纠正深层次推理缺陷。

Method: 提出SELF - THOUGHT框架，先将任务提炼为结构化模板，再指导解决方案实例化，且模板可跨模型使用。

Result: 在不同推理任务的实验中，SELF - THOUGHT提升了大小模型的准确性、鲁棒性和泛化能力。

Conclusion: SELF - THOUGHT为构建更可靠的自纠错语言系统提供了可扩展途径。

Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.

</details>


### [61] [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出SafeGround框架用于GUI grounding模型，可在测试前校准实现风险感知预测，实验显示提升了系统级准确率。


<details>
  <summary>Details</summary>
Motivation: GUI grounding模型存在不正确接地带来高成本且难逆转的行为，需要提升其可靠性。

Method: 使用分布感知的不确定性量化方法捕捉随机样本空间分散，校准得出具有统计保证的FDR控制测试时间决策阈值。

Result: 不确定性度量优于现有基线，校准阈值实现严格风险控制，在多个模型上系统级准确率最高提升5.38个百分点。

Conclusion: SafeGround能有效区分正确与错误预测，实现风险控制，提升系统级准确率。

Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference.

</details>


### [62] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

TL;DR: 提出Synapse框架解决基于大语言模型的代理在联邦学习中协作学习的挑战，提升工具使用效果并减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的代理在联邦学习中的协作学习面临通信成本、数据异质性和工具使用等挑战，限制了其有效性。

Method: 引入Synapse框架，训练共享的全局工具使用行为知识模型，客户端代理本地学习工具使用模式，通过协调器传输工件进行联邦聚合，更新并重新分发全局工具手册，采用模板化表示、嵌入检索与大语言模型重排序、自适应掩码等方法。

Result: 与权重或提示共享方法相比，Synapse提高了工具使用效果并减少了通信开销。

Conclusion: Synapse框架能有效解决基于大语言模型的代理在联邦学习中协作学习的问题，支持异质数据并量化性能提升。

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [63] [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455)
*Han Bao,Zheyuan Zhang,Pengcheng Jing,Zhengqing Yuan,Kaiwen Shi,Yanfang Ye*

Main category: cs.AI

TL;DR: 介绍了首个诊断基准Drift - Bench，用于评估输入故障下代理语用学，实验显示故障下性能下降，该基准可连接澄清研究和代理安全评估


<details>
  <summary>Details</summary>
Motivation: 大语言模型向自主代理过渡时，用户输入常违反合作假设，存在执行风险且现有基准无法衡量，需要新的评估方法

Method: 引入Drift - Bench，基于经典通信理论，提供合作故障统一分类，采用角色驱动用户模拟器和Rise评估协议

Result: 实验显示在故障下性能大幅下降，澄清效果因用户角色和故障类型而异

Conclusion: Drift - Bench能连接澄清研究和代理安全评估，可系统诊断导致不安全执行的故障

Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.

</details>


### [64] [Supervised sparse auto-encoders as unconstrained feature models for semantic composition](https://arxiv.org/abs/2602.00924)
*Ouns El Harzli,Hugo Wallner,Yoonsoo Nam,Haixuan Xavier Tao*

Main category: cs.AI

TL;DR: 本文针对稀疏自编码器的局限，采用无约束特征模型和任务监督方法，在Stable Diffusion 3.5上验证，实现组合泛化和语义图像编辑。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器存在L1惩罚非平滑性影响重建和可扩展性，以及学习特征与人类语义缺乏对齐的问题。

Method: 采用无约束特征模型，对（仅解码器）SAEs进行任务监督，联合学习稀疏概念嵌入和解码器权重来重建特征向量。

Result: 在Stable Diffusion 3.5上验证，实现了组合泛化，能重建训练中未见的概念组合图像，可在不修改提示的情况下进行特征级语义图像编辑。

Conclusion: 所提方法有效解决了稀疏自编码器的局限，在图像重建和语义编辑方面表现良好。

Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.

</details>


### [65] [Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents](https://arxiv.org/abs/2602.00929)
*Zergham Ahmed,Kazuki Irie,Joshua B. Tenenbaum,Christopher J. Bates,Samuel J. Gershman*

Main category: cs.AI

TL;DR: 提出新TBRL智能体TheoryCoder - 2，能利用LLM上下文学习能力主动学习抽象知识，实验显示其样本效率高、能解决复杂任务且仅需最少人工提示。


<details>
  <summary>Details</summary>
Motivation: 现有TBRL系统严重依赖人工提供抽象知识，回避了抽象学习问题，而人类能学习抽象知识高效规划并跨任务泛化，当前LLM智能体和深度强化学习系统在这方面有挑战。

Method: 引入TheoryCoder - 2，利用LLM上下文学习能力，从经验中合成抽象知识并集成到分层规划过程。

Result: 在多个环境实验表明，TheoryCoder - 2比基线LLM智能体、推理规划智能体和先前程序合成智能体样本效率更高，能解决基线方法失败的复杂任务。

Conclusion: TheoryCoder - 2是一种有效的TBRL智能体，可主动学习抽象知识，减少对人工指定抽象的依赖，仅需最少人工提示。

Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.

</details>


### [66] [The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis](https://arxiv.org/abs/2602.00947)
*Mohan Reddy*

Main category: cs.AI

TL;DR: 聊天界面用于多步、依赖状态的数据分析任务会降低分析性能，文章指出五种机制，提出八种混合设计模式解决问题并给出验证假设和范式。


<details>
  <summary>Details</summary>
Motivation: 指出聊天作为人工智能辅助数据分析默认界面，用于多步、依赖状态的分析任务是错误的，探讨其对分析性能的影响。

Method: 基于Woods (1984)的Keyhole Effect，分析聊天界面影响分析性能的五种机制，将认知过载形式化，提出八种混合设计模式。

Result: 明确聊天界面降低分析性能的五种机制，提出八种解决问题的混合设计模式。

Conclusion: 文章给出可证伪的假设和实验范式用于实证验证，框架对开放式探索应用性强。

Abstract: Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.

</details>


### [67] [MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support](https://arxiv.org/abs/2602.00950)
*António Farinhas,Nuno M. Guerreiro,José Pombal,Pedro Henrique Martins,Laura Melton,Alex Conway,Cara Dochat,Maya D'Eon,Ricardo Rei*

Main category: cs.AI

TL;DR: 本文引入临床风险分类法，发布MindGuard - testset数据集，训练轻量级安全分类器MindGuard，降低误报率，表现优于通用防护措施，并发布模型和评估数据。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于心理健康支持时，通用防护措施无法区分治疗性披露和真正的临床危机，导致安全问题。

Method: 与心理学家合作开发临床风险分类法；发布临床专家标注的MindGuard - testset数据集；通过双智能体设置生成合成对话训练MindGuard分类器。

Result: 分类器在高召回率操作点降低误报率，与临床医生语言模型配合时，在对抗性多轮交互中攻击成功率和有害参与率低于通用防护措施。

Conclusion: 所提出的方法和模型在大语言模型用于心理健康支持时能提供更有效的安全保障。

Abstract: Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures. To address this gap, we introduce a clinically grounded risk taxonomy, developed in collaboration with PhD-level psychologists, that identifies actionable harm (e.g., self-harm and harm to others) while preserving space for safe, non-crisis therapeutic content. We release MindGuard-testset, a dataset of real-world multi-turn conversations annotated at the turn level by clinical experts. Using synthetic dialogues generated via a controlled two-agent setup, we train MindGuard, a family of lightweight safety classifiers (with 4B and 8B parameters). Our classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, help achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards. We release all models and human evaluation data.

</details>


### [68] [R-HTN: Rebellious Online HTN Planning for Safety and Game AI](https://arxiv.org/abs/2602.00951)
*Hector Munoz-Avila,David W. Aha,Paola Rizzo*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \D. Like other agents that are capable of rebellion (i.e., {\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.

</details>


### [69] [Small-Margin Preferences Still Matter-If You Train Them Right](https://arxiv.org/abs/2602.00954)
*Jinlong Pang,Zhaowei Zhu,Na Di,Yichi Zhang,Yaxuan Wang,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出MixDPO策略解决偏好优化方法中偏好对质量和难度影响模型对齐效果的问题，并在基准测试中表现更优。


<details>
  <summary>Details</summary>
Motivation: 偏好优化方法效果受偏好对质量和难度影响，常见启发式方法过滤小差距的偏好对，本文探索合理利用这些困难偏好对的方法。

Method: 提出MixDPO策略，将偏好数据按难度排序，对困难数据用有监督微调目标，对容易数据用偏好损失进行训练。

Result: 在三个基准测试中，MixDPO比DPO和广泛使用的变体在模型对齐上有改善，在AlpacaEval 2长度控制胜率上有显著提升。

Conclusion: MixDPO策略能有效利用模糊的配对数据，避免因在低差距数据上使用偏好损失而导致的优化失败。

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

</details>


### [70] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: 本文研究现有ARL联合训练假设，发现推理和工具使用行为训练干扰问题，提出DART框架解决，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有ARL方法联合训练推理和工具使用行为的假设缺乏实证检验，需研究该假设的有效性。

Method: 引入LEAS系统量化推理和工具使用行为的干扰，提出DART框架通过单独低秩适应模块解耦参数更新。

Result: DART平均性能比基线方法提高6.35%，单模型性能与显式分离工具使用和推理的多智能体系统相当。

Conclusion: 推理和工具使用行为联合训练存在干扰，DART框架能有效解决该问题，提升ARL性能。

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [71] [Error Taxonomy-Guided Prompt Optimization](https://arxiv.org/abs/2602.00997)
*Mayank Singh,Vikas Yadav,Eduardo Blanco*

Main category: cs.AI

TL;DR: 提出ETGPO算法优化提示，在多基准测试中表现佳且省资源。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法多靠试错耗算力，且自底向上方法缺全局视角。

Method: 提出ETGPO算法，采用自上而下方法，收集模型错误分类，用针对高频失败模式的指导增强提示。

Result: 在数学、问答和逻辑推理等多基准测试中，ETGPO准确性与现有最优方法相当或更好，优化阶段令牌使用和评估预算约为三分之一。

Conclusion: ETGPO是一种有效的提示优化算法，能以较低资源消耗取得良好效果。

Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.

</details>


### [72] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

TL;DR: 研究大语言模型基于偏好的后训练后谄媚行为增加问题，分析原因、提出干预方法并实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在基于偏好的后训练后谄媚行为增加，违背事实准确性和合理判断的问题。

Method: 对人类反馈对齐导致该问题的机制进行形式分析，分析随机效用模型下奖励学习情况，提出训练时干预方法。

Result: 发现奖励差距普遍存在，在所考虑配置中都会导致行为偏差。

Conclusion: 提出的训练时干预方法可有效中和放大机制，找到最接近无约束后训练策略的策略及相应最小奖励修正。

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [73] [HalluHard: A Hard Multi-Turn Hallucination Benchmark](https://arxiv.org/abs/2602.01031)
*Dongyang Fan,Sebastien Delsad,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.AI

TL;DR: 本文引入挑战多轮幻觉基准HalluHard，提出判断流程评估内容准确性，发现各模型仍有大量幻觉问题，且幻觉行为受多种因素影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮对话中会产生无根据事实性主张，且问题会随上下文增长而恶化。

Method: 引入含950个种子问题的多轮幻觉基准HalluHard，要求事实断言有引用；提出判断流程，通过网络搜索迭代检索证据，评估引用材料是否支持生成内容。

Result: 即使使用网络搜索，多个模型仍存在大量幻觉问题，例如表现最强的Opus - 4.5搭配网络搜索幻觉率约30%。

Conclusion: 幻觉行为受模型容量、轮次位置、有效推理和知识类型等因素影响。

Abstract: Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search ($\approx 30\%$ for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.

</details>


### [74] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

TL;DR: 本文提出新框架解决大语言模型强化学习中奖励稀疏和信用分配低效问题，实验显示该方法在样本效率、准确率和泛化能力上优于基线。


<details>
  <summary>Details</summary>
Motivation: 标准基于结果的强化学习方法在大语言模型推理能力提升中存在奖励稀疏和信用分配低效问题。

Method: 提出提供连续奖励信号的框架，引入逐步边际信息增益机制过滤噪声，采用解耦掩码策略分配奖励，结合双门控SFT目标稳定训练。

Result: 在文本和多模态基准测试中，该方法在样本效率和最终准确率上优于GRPO等基线，模型具有更好的分布外鲁棒性和零样本迁移能力。

Conclusion: 所提框架能有效解决大语言模型强化学习中的问题，提升推理能力。

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [75] [SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning](https://arxiv.org/abs/2602.01062)
*Chenyi Li,Yuan Zhang,Bo Wang,Guoqing Ma,Wei Tang,Haoyang Huang,Nan Duan*

Main category: cs.AI

TL;DR: 提出基于核化相似度的集水平多样性目标优化策略提升大语言模型推理性能多样性，实验效果好


<details>
  <summary>Details</summary>
Motivation: 强化学习提升大语言模型推理能力时会降低结果多样性，受收益递减原则启发开展研究

Method: 引入基于核化相似度定义在采样轨迹上的集水平多样性目标，将其作为策略优化的优势塑形项，在分布扰动框架下分析轨迹对多样性贡献

Result: 实验表明在不同规模模型、多个基准测试的Pass@1和Pass@K上均优于强基线

Conclusion: 所提算法能有效提升大语言模型推理性能和结果多样性

Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.

</details>


### [76] [ConvexBench: Can LLMs Recognize Convex Functions?](https://arxiv.org/abs/2602.01075)
*Yepeng Liu,Yu Huang,Yu-Xiang Wang,Yingbin Liang,Yuheng Bu*

Main category: cs.AI

TL;DR: 介绍凸性基准测试CB以测试大语言模型识别符号目标凸性的能力，发现模型组合推理存在差距，提出代理分治框架解决问题并取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型开始自动化研究级数学和科学，需要其具备理解和推理凸性的能力。

Method: 引入可扩展且可机械验证的基准测试CB，分析模型推理痕迹找出失败模式，提出代理分治框架，借助外部工具构建抽象语法树、对中间子表达式递归推理。

Result: 前沿大语言模型存在组合推理差距，性能随深度增加迅速下降；代理分治框架能可靠缓解深度组合失败，在大深度下显著提升性能。

Conclusion: 代理分治框架可解决大语言模型在识别符号目标凸性时的深度组合失败问题，实现性能提升。

Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \cb, a scalable and mechanically verifiable benchmark for testing \textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \textit{parsing failure} and \textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).

</details>


### [77] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

TL;DR: 现存大语言模型代理用于健康数据存在局限，提出AutoHealth系统，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代理在健康数据应用上存在泛化难、依赖预设模板、忽视不确定性评估等问题。

Method: 提出AutoHealth系统，通过五个专业代理进行闭环协调实现数据探索、模型构建、训练和优化，权衡预测性能和不确定性量化，并生成综合报告。同时，创建具有挑战性的真实世界基准进行评估。

Result: AutoHealth完成所有任务，预测性能比现有最先进基线高出29.2%，不确定性估计高出50.2%。

Conclusion: AutoHealth系统能有效解决现有大语言模型代理在健康数据上的应用问题，提高预测性能和不确定性评估能力。

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [78] [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)
*Yiliu He,Tianle Li,Binghao Ji,Zhiyuan Liu,Di Huang*

Main category: cs.AI

TL;DR: 提出EvoOpt - LLM框架解决工业优化建模问题，数据效率高、性能好


<details>
  <summary>Details</summary>
Motivation: 现有将自然语言需求转化为可执行模型的方法存在数据效率低、求解器有效性有限和扩展性差等问题

Method: 构建基于7B参数LLM的EvoOpt - LLM框架，采用参数高效的LoRA微调

Result: 仅3000个训练样本下生成率达91%，可执行率65.9%，约束注入和变量修剪模块表现良好

Conclusion: EvoOpt - LLM是实用、数据高效的工业优化建模方法，减少专家干预，提升适应性和求解器效率

Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.

</details>


### [79] [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)
*Yang Liu,Chuan Zhou,Yancheng Chen,Shuai Zhang,Xixun Lin,Xiaoqing Wang*

Main category: cs.AI

TL;DR: 提出FALCON框架解决大语言模型在组合优化中无法保证解可行性问题，有三项关键创新及训练方法，理论证明收敛性，实证表现优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为组合优化求解器缺乏保证解可行性的机制，难以实际应用。

Method: 引入FALCON框架，含语法约束解码、可行性修复层、自适应Best-of-$N$采样；提出BOPO用于训练LLM。

Result: 在七个NP难组合优化问题上，FALCON实现100%可行性，解质量匹配或超越现有方法。

Conclusion: FALCON框架有效解决了大语言模型在组合优化中解可行性问题，有良好的理论和实证效果。

Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% feasibility through three key innovations: (i) \emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

</details>


### [80] [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)
*Yiming Dong,Kun Fu,Haoyu Li,Xinyuan Zhu,Yurou Liu,Lijing Shao,Jieping Ye,Zheng Wang*

Main category: cs.AI

TL;DR: 本文从目标级黑客攻击角度引入框架理解MoE架构中RLVR训练不稳定性，通过实验追溯其关键病态训练动态的起源并形式化机制，为设计稳定算法提供指导。


<details>
  <summary>Details</summary>
Motivation: 长时间的强化学习（RLVR）训练在MoE架构中易出现不稳定性，严重影响模型能力提升，且其潜在原因和机制尚不明确。

Method: 引入基于目标级黑客攻击的框架理解RLVR不稳定性，并在30B MoE模型上进行大量实验。

Result: 追溯到MoE模型中关键病态训练动态（训练 - 推理差异异常增长）的起源并形式化了其背后的机制。

Conclusion: 研究结果为MoE模型不稳定性的训练动态提供了具体的因果解释，为设计稳定的RLVR算法提供了指导。

Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.

</details>


### [81] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

TL;DR: 提出了BiCarFormer多模态方法，结合DTC序列和环境条件对错误代码进行多标签序列分类，实验显示其性能优于仅依靠DTC序列的模型，强调结合环境信息对车辆诊断的重要性。


<details>
  <summary>Details</summary>
Motivation: 现代诊断系统主要依赖DTC序列，忽略了有价值的上下文信息（如原始感官数据），而这些信息对领域专家分类车辆故障很关键，但存在数据复杂和噪声大的挑战。

Method: 提出BiCarFormer，这是一个针对车辆事件序列的双向Transformer模型，采用嵌入融合和协同注意力机制来捕捉诊断代码和环境数据之间的关系。

Result: 在包含22,137个错误代码和360个错误模式的真实汽车数据集上的实验表明，该方法相比仅依赖DTC序列的模型和传统序列模型，显著提高了分类性能。

Conclusion: 强调了结合上下文环境信息对于更准确和稳健的车辆诊断的重要性，有助于降低维护成本并提高汽车行业的自动化流程。

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [82] [Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach](https://arxiv.org/abs/2602.01131)
*Yue Zhong,Jiawen Kang,Yongju Tong,Hong-Ning Dai,Dong In Kim,Abbas Jamalipour,Shengli Xie*

Main category: cs.AI

TL;DR: 针对低空经济中UAV异构网络资源与稳定性问题，提出传感 - 通信 - 计算 - 控制闭环框架和轻量级PPO算法，保障系统稳定与效用。


<details>
  <summary>Details</summary>
Motivation: 解决UAV异构网络中有限机载资源与严格稳定性要求的冲突，突破传统吞吐量设计局限。

Method: 提出传感 - 通信 - 计算 - 控制闭环框架，利用Lyapunov稳定性理论推导映射，将资源分配问题建模为Stackelberg博弈，提出轻量级剪枝PPO算法。

Result: 通过仿真验证该方案能在动态低空环境中有效保障控制环稳定性并最大化系统效用。

Conclusion: 所提方案能在动态低空环境中实现控制环稳定性和系统效用最大化。

Abstract: With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networks is often compromised by the conflict between limited onboard resources and stringent stability requirements. Moving beyond traditional throughput-centric designs, we propose a Sensing-Communication-Computing-Control closed-loop framework that explicitly models the impact of communication latency on physical control stability. To guarantee mission reliability, we leverage the Lyapunov stability theory to derive an intrinsic mapping between the state evolution of the control system and communication constraints, transforming abstract stability requirements into quantifiable resource boundaries. Then, we formulate the resource allocation problem as a Stackelberg game, where UAVs (as leaders) dynamically price resources to balance load and ensure stability, while users (as followers) optimize requests based on service urgency. Furthermore, addressing the prohibitive computational overhead of standard Deep Reinforcement Learning (DRL) on energy-constrained edge platforms, we propose a novel and lightweight pruning-based Proximal Policy Optimization (PPO) algorithm. By integrating a dynamic structured pruning mechanism, the proposed algorithm significantly compresses the neural network scale during training, enabling the UAV to rapidly approximate the game equilibrium with minimal inference latency. Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments.

</details>


### [83] [PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?](https://arxiv.org/abs/2602.01146)
*Sidharth Pulipaka,Oliver Chen,Manas Sharma,Taaha S Bajwa,Vyas Raina,Ivaxi Sheth*

Main category: cs.AI

TL;DR: 对话助手将长期记忆与大语言模型结合带来安全风险，本文引入PersistBench衡量风险，评估18个大模型，结果显示失败率高，鼓励开发更安全的长期记忆使用方式。


<details>
  <summary>Details</summary>
Motivation: 对话助手结合长期记忆虽能增强个性化，但存在被忽视的安全风险，需衡量这些风险。

Method: 引入PersistBench衡量安全风险，识别两种长期记忆特定风险，在基准上评估18个前沿和开源大语言模型。

Result: 评估结果显示大语言模型跨域样本的中位失败率为53%，谄媚样本为97%。

Conclusion: 鼓励前沿对话系统开发更健壮、更安全的长期记忆使用方式。

Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.

</details>


### [84] [Capabilities and Fundamental Limits of Latent Chain-of-Thought](https://arxiv.org/abs/2602.01148)
*Jiaxuan Zou,Yaozhong Xiong,Yong Liu*

Main category: cs.AI

TL;DR: 本文研究潜思维链模型探索与计算性能不一致的问题，揭示决策确定性的影响，提出符号指标，证明课程学习的必要性，推动设计范式转变。


<details>
  <summary>Details</summary>
Motivation: 解决潜思维链模型在探索和计算任务中表现不一致的问题。

Method: 理论分析探索 - 执行权衡关系，引入符号指标量化决策承诺，证明课程学习的必要性。

Result: 发现高确定性利于精确执行但抑制探索，低确定性利于搜索但易积累误差；建立符号指标与执行稳定性和探索能力的因果关系；证明直接训练因分布不匹配而失败。

Conclusion: 应从二元架构选择转向基于任务需求动态调节决策确定性的自适应系统。

Abstract: Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certainty. Our contributions are threefold: (1) We theoretically characterize the fundamental Exploration-Execution Trade-off, proving that high certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation. (2) We introduce the Symbolic Index--quantifying decisional commitment--as the core mechanism governing this trade-off and establish its causal relationship with both execution stability and exploration capability. (3) We prove that curriculum learning is theoretically necessary, as direct training provably fails due to distributional mismatch. Our framework shifts the design paradigm from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands.

</details>


### [85] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

TL;DR: 研究发现预训练VLM中部分层会阻碍下游任务，提出TaLo方法动态绕过干扰层，提升多模型和数据集性能。


<details>
  <summary>Details</summary>
Motivation: 当前预训练VLM默认使用所有层进行下游任务预测，研究发现部分层可能阻碍任务表现，因此系统研究各层对不同任务的影响。

Method: 通过层干预系统研究各层对不同任务的影响，引入任务 - 层交互向量量化影响，提出TaLo方法动态识别并绕过干扰层。

Result: 发现任务干扰层的存在及特定敏感模式，TaLo方法在无参数更新下提升多个模型和数据集性能，如使Qwen - VL在ScienceQA中Maps任务准确率提高最多16.6%。

Conclusion: 预训练VLM中存在意外的模块化形式，TaLo方法作为即插即用、无需训练的机制可在推理时释放隐藏能力，代码将公开。

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [86] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 本文提出ASP - Bench基准用于评估将自然语言问题转化为答案集程序的系统，用基于ReAct框架的方法测试该基准并分析问题建模难度。


<details>
  <summary>Details</summary>
Motivation: 自动将自然语言规范转化为逻辑程序的任务具有挑战性且影响神经符号工程，需要评估相关转化系统。

Method: 提出包含128个自然语言问题实例的ASP - Bench基准，覆盖ASP多种特性，用基于ReAct框架的agentic方法测试基准。

Result: 使用基于ReAct框架的方法实现了完全饱和，反馈驱动的迭代细化是在ASP中对自然语言建模的可靠且稳健方法。

Conclusion: 通过多次agent运行分析能深入了解决定问题建模难度的因素。

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [87] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

TL;DR: 提出高效推理框架以提升大语言模型推理效率和性能，减少计算复杂度并缓解过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 长链式思维推理虽提升大模型复杂推理任务表现，但生成链计算和内存成本大，现有压缩方法与测试时扩展冲突，限制推理能力。

Method: 将推理过程建模为状态转换过程，用线性注意力机制估计推理状态，基于查询提示和状态执行推理并更新，提出基于状态的推理策略。

Result: 实验表明，该框架提高了大语言模型的推理效率与性能。

Conclusion: 所提框架可显著提升大语言模型的推理效率和性能。

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [88] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: 论文提出Workflow - R1框架及GSsPO算法，有效解决代理工作流优化问题，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作流优化方法将合成问题视为静态、以代码为中心的一次性生成问题，限制模型能力和动态解决问题的灵活性。

Method: 提出Workflow - R1框架，将工作流构建改为多轮、基于自然语言的序列决策过程；引入GSsPO算法，以复合子序列为优化单元确保复杂多轮推理任务的稳健学习。

Result: 在多个QA基准测试中，Workflow - R1性能优于竞争基线。

Conclusion: GSsPO是序列推理的通用解决方案，Workflow - R1是自动化工作流优化的新范式。

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [89] [Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)](https://arxiv.org/abs/2602.01206)
*Zeinab Dehghani*

Main category: cs.AI

TL;DR: 本文介绍gSMILE统一框架，用于解释生成式模型，通过实验证明它能产生稳健归因并跨模型有效泛化，助力生成式AI技术透明部署。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能决策过程不透明，限制了其在高风险应用中的信任和可靠性，需要可解释性方法。

Method: 引入gSMILE框架，扩展SMILE方法到生成式场景，采用文本输入扰动、Wasserstein距离度量和加权替代建模，结合基于ODD框架的场景评估策略。

Result: gSMILE在大型语言模型和图像编辑模型中可提供细粒度归因和直观可视化，通过严格指标评估证明其能产生稳健、符合人类认知的归因，且能在多种生成式架构中有效泛化。

Conclusion: gSMILE有潜力推动生成式AI技术的透明、可靠和负责任的部署。

Abstract: The rapid advancement of generative artificial intelligence has enabled models capable of producing complex textual and visual outputs; however, their decision-making processes remain largely opaque, limiting trust and accountability in high-stakes applications. This thesis introduces gSMILE, a unified framework for the explainability of generative models, extending the Statistical Model-agnostic Interpretability with Local Explanations (SMILE) method to generative settings. gSMILE employs controlled perturbations of textual input, Wasserstein distance metrics, and weighted surrogate modelling to quantify and visualise how specific components of a prompt or instruction influence model outputs. Applied to Large Language Models (LLMs), gSMILE provides fine-grained token-level attribution and generates intuitive heatmaps that highlight influential tokens and reasoning pathways. In instruction-based image editing models, the exact text-perturbation mechanism is employed, allowing for the analysis of how modifications to an editing instruction impact the resulting image. Combined with a scenario-based evaluation strategy grounded in the Operational Design Domain (ODD) framework, gSMILE allows systematic assessment of model behaviour across diverse semantic and environmental conditions. To evaluate explanation quality, we define rigorous attribution metrics, including stability, fidelity, accuracy, consistency, and faithfulness, and apply them across multiple generative architectures. Extensive experiments demonstrate that gSMILE produces robust, human-aligned attributions and generalises effectively across state-of-the-art generative models. These findings highlight the potential of gSMILE to advance transparent, reliable, and responsible deployment of generative AI technologies.

</details>


### [90] [Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models](https://arxiv.org/abs/2602.01207)
*Hui Wu,Hengyi Cai,Jinman Zhao,Xinran Chen,Ziheng Li,Zhejun Zhao,Shuaiqiang Wang,Yuchen Li,Dawei Yin*

Main category: cs.AI

TL;DR: 提出SAGE框架提升大推理模型偏好对齐可靠性，实验显示其加速收敛且优于静态基线。


<details>
  <summary>Details</summary>
Motivation: 标准偏好对齐方法将所有偏好对统一处理，忽略训练实例效用变化，导致优化低效或不稳定。

Method: 提出SAGE动态框架，整合粗粒度课程机制和细粒度稳定性感知评分函数。

Result: 在多个数学推理基准测试中，SAGE显著加速收敛，性能优于静态基线。

Conclusion: 策略感知、注重稳定性的数据选择在推理对齐中至关重要。

Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.

</details>


### [91] [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)
*Shaoxiong Yang,Junting Li,Mengyuan Zhang,Chao Li,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: 提出FutureMind框架提升小语言模型处理复杂任务能力，实验效果优且揭示思维模式蒸馏瓶颈。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在复杂知识密集型任务中表现不佳，需提升其处理能力。

Method: 提出FutureMind模块化推理框架，含动态推理管道和三种检索范式，通过自适应知识蒸馏赋予小语言模型战略思维模式先验。

Result: 在多跳问答基准测试中优于强基线，在多种小语言模型架构和规模下取得最优结果。

Conclusion: 思维模式蒸馏受师生模型认知偏差瓶颈限制，为开发兼具效率和认知能力的小语言模型提供新视角。

Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.

</details>


### [92] [Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models](https://arxiv.org/abs/2602.01237)
*Katrina Brown,Aneesh Muppidi,Rana Shahout*

Main category: cs.AI

TL;DR: 提出Predictive Scheduling框架优化大语言模型推理的token预算分配，在GSM8K基准上提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使用固定token预算进行推理会导致简单输入过计算、困难输入计算不足。

Method: 引入Predictive Scheduling框架，使用轻量级预测器估计查询的最佳推理长度或难度，通过贪婪批量分配器动态分配token预算。

Result: 在GSM8K算术基准上，相同token成本下比统一预算分配最多提升7.9个百分点的绝对准确率，缩小与完美先知的差距超50%；中间层（12 - 17）对大小估计信号最丰富。

Conclusion: 预运行预算预测可实现计算 - 准确率权衡的细粒度控制，为低延迟、低成本的大语言模型部署提供路径。

Abstract: Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.

</details>


### [93] [LLM-Driven Ontology Construction for Enterprise Knowledge Graphs](https://arxiv.org/abs/2602.01276)
*Abdulsobur Oyewale,Tommaso Soru*

Main category: cs.AI

TL;DR: 本文介绍OntoEKG，一个由大语言模型驱动的管道，用于从非结构化企业数据中加速生成特定领域本体，实验显示其有潜力也有挑战。


<details>
  <summary>Details</summary>
Motivation: 企业知识图谱底层本体构建资源密集且依赖专业知识，缺乏端到端本体构建的综合基准。

Method: 将建模任务分解为提取模块和蕴含模块，采用来自数据、金融和物流领域文档的新评估数据集。

Result: 在数据领域模糊匹配F1分数达0.724，也暴露出范围定义和层次推理的局限性。

Conclusion: 该方法有潜力但也面临一定挑战。

Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.

</details>


### [94] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: 现有方法处理电子病历诊断有局限，提出RE - MCDF框架，实验表明其在复杂诊断场景中优于基线。


<details>
  <summary>Details</summary>
Motivation: 电子病历异质、稀疏、嘈杂，单智能体系统易出错，多智能体框架交互浅且忽略疾病逻辑依赖，影响临床诊断。

Method: 提出RE - MCDF框架，采用生成 - 验证 - 修订闭环架构，含三个互补组件，由医学知识图谱引导。

Result: 在NEEMRs和XMEMRs数据集上实验，RE - MCDF在复杂诊断场景中始终优于现有基线。

Conclusion: RE - MCDF框架能有效解决现有临床诊断方法的问题，提升诊断效果。

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [95] [Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance](https://arxiv.org/abs/2602.01346)
*Wei Yang,Hong Xie,Tao Tan,Xin Li,Defu Lian,Enhong Chen*

Main category: cs.AI

TL;DR: 现有开源视觉语言模型选择方法有局限，本文提出基于视觉编码器内部功能动态的框架，实验显示优于基线。


<details>
  <summary>Details</summary>
Motivation: 开源视觉语言模型众多，为特定下游任务选最优预训练模型有挑战，现有选择方法存在不足。

Method: 提出基于视觉编码器内部功能动态的框架，用层电导表示任务，通过熵正则化对齐得到目标条件块重要性分布，引入不对称指标DCD。

Result: 在48个视觉语言模型和21个数据集上实验，优于现有基线，NDCG@5比SWAB提高14.7%。

Conclusion: 所提方法在视觉语言模型选择上效果良好，能有效解决现有问题。

Abstract: While open sourced Vision-Language Models (VLMs) have proliferated, selecting the optimal pretrained model for a specific downstream task remains challenging. Exhaustive evaluation is often infeasible due to computational constraints and data limitations in few shot scenarios. Existing selection methods fail to fully address this: they either rely on data-intensive proxies or use symmetric textual descriptors that neglect the inherently directional and model-specific nature of transferability. To address this problem, we propose a framework that grounds model selection in the internal functional dynamics of the visual encoder. Our approach represents each task via layer wise conductance and derives a target-conditioned block importance distribution through entropy regularized alignment. Building on this, we introduce Directional Conductance Divergence (DCD), an asymmetric metric that quantifies how effectively a source task covers the target's salient functional blocks. This allows for predicting target model rankings by aggregating source task ranks without direct inference. Experimental results on 48 VLMs across 21 datasets demonstrate that our method outperforms state-of-the-art baselines, achieving a 14.7% improvement in NDCG@5 over SWAB.

</details>


### [96] [Aggregation Queries over Unstructured Text: Benchmark and Agentic Method](https://arxiv.org/abs/2602.01355)
*Haojia Zhu,Qinyuan Xu,Haoyu Li,Yuxi Liu,Hanchen Qiu,Jiaoyan Chen,Jiahui Jin*

Main category: cs.AI

TL;DR: 本文针对文本聚合查询问题，形式化了实体级聚合查询，引入AGGBench基准，提出DFA方法，实验显示DFA能提升证据覆盖率。


<details>
  <summary>Details</summary>
Motivation: 聚合查询需全面收集证据，现有范式无法满足完整性要求，因此要解决文本实体级聚合查询问题。

Method: 形式化实体级聚合查询，引入AGGBench基准，提出DFA模块化代理基线方法。

Result: DFA相比强大的RAG和代理基线方法，持续提高了聚合证据覆盖率。

Conclusion: DFA方法在解决文本聚合查询问题上有效，有助于实现严格完整性要求的聚合查询。

Abstract: Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to "find all," not merely "find one." Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.

</details>


### [97] [Building Better Deception Probes Using Targeted Instruction Pairs](https://arxiv.org/abs/2602.01425)
*Vikram Natarajan,Devina Jain,Shivam Arora,Satvik Golechha,Joseph Bloom*

Main category: cs.AI

TL;DR: 本文指出线性探针监测AI欺骗行为时训练指令对的重要性，特定目标的探针效果更好，建议设计针对特定威胁模型的探针。


<details>
  <summary>Details</summary>
Motivation: 以往线性探针在监测AI欺骗行为时有明显缺陷，如虚假关联和误报，需改进。

Method: 确定训练时指令对的重要性，通过人类可解释的欺骗分类法针对特定欺骗行为。

Result: 指令对能捕捉欺骗意图，而非特定内容模式，提示选择主导探针性能（70.6%的方差），特定目标探针在评估数据集上效果更好。

Conclusion: 组织应设计针对特定威胁模型的专业探针，而非寻求通用欺骗检测器。

Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.

</details>


### [98] [SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce](https://arxiv.org/abs/2602.01443)
*Alberto Castelo,Zahra Zanjani Foumani,Ailin Fan,Keat Yang Koay,Vibhor Malik,Yuanzheng Zhu,Han Li,Meysam Feghhi,Ronie Uliana,Shuang Xie,Zhaoyu Zhang,Angelo Ocana Martins,Mingyu Zhao,Francis Pelland,Jonathan Faerman,Nikolas LeBlanc,Aaron Glazer,Andrew McNamara,Lingyun Wang,Zhong Wu*

Main category: cs.AI

TL;DR: 介绍了用于快速离线A/B测试的可扩展系统SimGym，它能减少实验周期，无需让真实买家参与。


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试分流流量、耗时久且可能损害用户体验，需要新方法。

Method: SimGym利用大语言模型代理在真实浏览器中操作，提取买家信息、识别行为原型，在控制组和实验组店面模拟加权会话。

Result: 在主要电商平台上与真实人类结果验证，即使未进行训练后校准，SimGym代理也达到了最先进的与观察到的结果变化的对齐，将实验周期从数周缩短到不足一小时。

Conclusion: SimGym可实现无真实买家参与的快速实验。

Abstract: A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Language Model agents operating in a live browser. SimGym extracts per-shop buyer profiles and intents from production interaction data, identifies distinct behavioral archetypes, and simulates cohort-weighted sessions across control and treatment storefronts. We validate SimGym against real human outcomes from real UI changes on a major e-commerce platform under confounder control. Even without alignment post training, SimGym agents achieve state of the art alignment with observed outcome shifts and reduces experiment cycles from weeks to under an hour , enabling rapid experimentation without exposure to real buyers.

</details>


### [99] [Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models](https://arxiv.org/abs/2602.01475)
*Brij Malhotra,Shivvrat Arya,Tahrima Rahman,Vibhav Giridhar Gogate*

Main category: cs.AI

TL;DR: 提出神经摊销框架改进概率图模型中重复查询场景下的局部搜索，在高树宽基准上表现优于SLS和GLS+。


<details>
  <summary>Details</summary>
Motivation: 概率图模型中MPE推理在重复查询场景下，现有随机局部搜索算法易陷入局部最优，启发式方法无法有效重用指导信息。

Method: 利用固定图结构，训练基于注意力的网络对局部移动评分，将其与现有局部搜索程序集成，平衡短期似然增益和长期前景。

Result: 在摊销推理设置的具有挑战性的高树宽基准上，相比SLS和GLS+有一致改进。

Conclusion: 所提的神经摊销框架能有效改进重复查询场景下的局部搜索。

Abstract: Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be performed repeatedly for varying evidence patterns. Stochastic Local Search (SLS) algorithms scale to large models but rely on myopic best-improvement rule that prioritizes immediate likelihood gains and often stagnate in poor local optima. Heuristics such as Guided Local Search (GLS+) partially alleviate this limitation by modifying the search landscape, but their guidance cannot be reused effectively across multiple inference queries on the same model. We propose a neural amortization framework for improving local search in this repeated-query regime. Exploiting the fixed graph structure, we train an attention-based network to score local moves by predicting their ability to reduce Hamming distance to a near-optimal solution. Our approach integrates seamlessly with existing local search procedures, using this signal to balance short-term likelihood gains with long-term promise during neighbor selection. We provide theoretical intuition linking distance-reducing move selection to improved convergence behavior, and empirically demonstrate consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in the amortized inference setting.

</details>


### [100] [Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection](https://arxiv.org/abs/2602.01518)
*Jongseok Park,Sunga Kim,Alvin Cheung,Ion Stoica*

Main category: cs.AI

TL;DR: 提出高效Top - k和Top - p算法Qrita，比现有算法有更高吞吐量和更低内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有Top - k和Top - p截断算子在大词汇表上实现效率低，排序方法计算和内存开销大，随机方法会改变输出。

Method: 基于RTop - k，使用高斯sigma截断减少搜索空间，四元枢轴搜索处理重复项，用Triton语言实现。

Result: 与高性能LLM执行引擎的Top - k和Top - p内核对比，Qrita吞吐量最高达2倍，内存使用减半，输出与基于排序的算法相同。

Conclusion: Qrita是一种高效的Top - k和Top - p算法，能提升性能并降低内存消耗。

Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.

</details>


### [101] [PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents](https://arxiv.org/abs/2602.01532)
*Yuxuan Fu,Xiaoyu Tan,Teqi Hao,Chen Zhan,Xihe Qiu*

Main category: cs.AI

TL;DR: 提出PRISM框架解决主动代理干预问题，在ProactiveBench上表现良好，减少误报、提升F1值，还发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有系统在主动代理干预决策上依赖不可靠启发式或无差别长推理，难以控制利弊权衡。

Method: 将问题建模为成本敏感的选择性干预，结合决策理论门控与双过程推理架构，采用门控校准、成本阈值、选择性慢推理及门控对齐、模式锁定蒸馏训练。

Result: 在ProactiveBench上，PRISM比强基线减少22.78%误报，F1值提升20.14%。

Conclusion: 基于决策理论的门控、选择性慢推理和对齐蒸馏能让主动代理更精确、高效且可控。

Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: "make haste slowly"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.

</details>


### [102] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: 本文提出MAGIC框架，将大语言模型安全对齐问题建模为对抗性非对称博弈，实现攻防双方协同进化，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全对齐防御方法依赖静态预收集数据分布，落后于不断演变的对抗攻击。

Method: 引入多轮多智能体强化学习框架MAGIC，将安全对齐问题建模为对抗性非对称博弈，攻击者改写查询，防御者优化策略识别拒绝输入。

Result: 攻击者通过迭代强化学习训练进化出新颖组合策略，实验表明框架防御成功率高，且不影响模型有用性。

Conclusion: MAGIC框架有效，能实现更鲁棒的安全对齐，理论上有更稳健的博弈均衡并提供安全保证。

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [103] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

TL;DR: 本文提出用于多学科科研的自进化代理框架S1 - NexusAgent，采用分层执行范式，支持MCP协议，解决数据和工具难题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型和基于工具的代理在长期规划、目标维护和持续学习方面存在局限，难以处理现代科研中的大规模数据、复杂工作流程和专业工具。

Method: 提出S1 - NexusAgent框架，采用分层Plan - and - CodeAct执行范式，支持MCP协议，使用基于对象引用的稀疏上下文管理，有Critic Agent自动评估与形成科学技能闭环。

Result: 在涉及长期规划和复杂专业工具编排的权威科学基准测试中实现了最先进性能。

Conclusion: S1 - NexusAgent在复杂科学任务中有有效性和泛化能力，对可持续和长期的科学研究有价值。

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [104] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: 提出基于人类模拟框架让AI系统自主形成问题和设定任务，实验显示环境感知和多智能体感知提示有显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的AI系统依赖预定义任务和固定提示，难以在环境变化时自主识别问题。

Method: 提出基于人类模拟的框架，将问题形成作为任务选择和执行前的决策过程，整合内部驱动、环境感知和多智能体感知提示范围，支持从经验中学习问题形成过程。

Result: 在多智能体模拟环境中，环境感知提示显著减少无进食事件，多智能体感知提示在20天模拟中使累积无进食事件减少超60%，有统计学显著改善。

Conclusion: 所提框架能提升AI系统在动态开放环境中的自主决策适应性和决策质量。

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [105] [Reasoning with Autoregressive-Diffusion Collaborative Thoughts](https://arxiv.org/abs/2602.01608)
*Mu Yuan,Liekang Zeng,Guoliang Xing,Lan Zhang,Yunhao Liu*

Main category: cs.AI

TL;DR: 提出Collaborative Thoughts统一协作框架，让自回归和扩散模型通过闭环交互联合推理和生成，提升空间推理可靠性和生成可控性。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在空间或物理基础任务上有困难，扩散模型缺乏逐步逻辑控制以满足复杂多阶段约束和纠错，需要结合两者优势。

Method: 引入Collaborative Thoughts框架，自回归模型进行结构化规划和约束管理，扩散模型将约束实例化为中间视觉思考，视觉批评模块评估视觉思考是否满足要求，反馈用于迭代优化后续步骤。

Result: 通过代表性示例说明能提升空间推理可靠性和生成可控性。

Conclusion: Collaborative Thoughts框架可使自回归和扩散模型联合推理和生成，且同一协作循环适用于不同任务。

Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.

</details>


### [106] [ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning](https://arxiv.org/abs/2602.01610)
*Zitao Guo,Changyang Jiang,Tianhong Zhao,Jinzhou Cao,Genan Dai,Bowen Zhang*

Main category: cs.AI

TL;DR: 文章提出ToPT框架用于学习有效区域嵌入，由SREL和Prompt4RE模块组成，实验表现达SOTA，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法缺乏空间先验和任务语义对齐机制，无法很好满足下游任务。

Method: 提出ToPT框架，包括用Graphormer进行空间感知区域嵌入学习，及基于冻结MLLM做任务感知提示。

Result: 在多任务和多城市实验中达到SOTA，最高提升64.2%。

Conclusion: 证明了空间先验和提示 - 区域对齐的必要性和互补性。

Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.

</details>


### [107] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

TL;DR: 本文提出FlowSteer框架解决现有工作流编排挑战，引入CWRPO训练，实验显示其显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作流编排面临高人工成本、依赖特定操作符/大语言模型、奖励信号稀疏等挑战。

Method: 提出FlowSteer端到端强化学习框架，以轻量级策略模型为智能体，结合可执行画布环境；提出CWRPO优化方法，引入带条件释放的多样性约束奖励。

Result: 在十二个数据集实验表明FlowSteer在各种任务上显著优于基线。

Conclusion: FlowSteer框架能有效应对现有工作流编排挑战，提升性能。

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [108] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: 引入长视野基准TRIP - Bench和在线多轮强化学习方法GTPO，前者用于评估LLM代理，后者用于训练，实验显示现有模型表现不佳，GTPO有更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分体现LLM代理在复杂现实场景中面临的关键挑战，如执行全局约束、多工具推理协调等。

Method: 引入TRIP - Bench基准，该基准基于现实旅行规划场景；提出GTPO在线多轮强化学习方法，有专门的奖励归一化和奖励差分。

Result: 先进模型在TRIP - Bench简单子集上最高成功率50%，困难子集低于10%；GTPO应用于Qwen2.5 - 32B - Instruct后，在评估中优于Gemini - 3 - Pro。

Conclusion: TRIP - Bench可推动实用长视野交互代理发展，GTPO为长视野鲁棒训练提供有效在线强化学习方案。

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [109] [What LLMs Think When You Don't Tell Them What to Think About?](https://arxiv.org/abs/2602.01689)
*Yongchan Kwon,James Zou*

Main category: cs.AI

TL;DR: 本文研究大语言模型从最小、无主题输入的生成情况，发现各模型族有主题偏好、内容深度差异和重复短语退化现象，并发布数据集和代码库。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型分析多依赖特定主题或任务提示，限制了观测范围，需要研究从无主题输入的生成情况以进行可靠监测和保障AI安全。

Method: 研究大语言模型从最小、无主题输入的生成，分析输出内容。

Result: 模型输出覆盖广泛语义空间，各模型族有主题偏好，如GPT - OSS倾向编程和数学，Llama倾向文学；内容深度有差异；生成常退化为重复短语。

Conclusion: 大语言模型在无主题输入下有多样的生成特征，数据集和代码库有助于进一步研究。

Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.

</details>


### [110] [Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning](https://arxiv.org/abs/2602.01695)
*Yadong Wang,Haodong Chen,Yu Tian,Chuanxing Geng,Dong Liang,Xiang Chen*

Main category: cs.AI

TL;DR: 提出LSTR框架调和潜在推理与稀疏表示模型问题，实验表明其有优势。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法难以解释和控制，稀疏表示模型多用于事后分析，需调和两者问题。

Method: 提出LSTR框架，采用带残差跳过架构的LTT，解耦线性流形传输与稀疏语义更新。

Result: LSTR保持推理准确性和压缩效率，大幅提升可解释性，稀疏特征在推理过程中兼具可解释性和因果有效性。

Conclusion: LSTR框架在潜在推理中能有效提升可解释性，稀疏特征在推理中有积极作用。

Abstract: Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.

</details>


### [111] [Mitigating loss of control in advanced AI systems through instrumental goal trajectories](https://arxiv.org/abs/2602.01699)
*Willem Fourie*

Main category: cs.AI

TL;DR: 本文提出工具性目标轨迹（IGTs），拓宽了应对高能力AI侵蚀人类控制问题的解决途径。


<details>
  <summary>Details</summary>
Motivation: 高能力AI系统可能通过追求工具性目标侵蚀人类控制，现有缓解措施多为技术和以系统为中心的，需要新方法。

Method: 开发工具性目标轨迹，分析组织中获取技术资源的三条途径（采购、治理和财务IGTs），并监测其产生的组织工件。

Result: IGTs能作为干预点，在系统能力或行为超出可接受阈值时发挥作用。

Conclusion: IGTs为定义能力水平、实施可纠正性和可中断性提供了具体途径，将关注点从模型属性扩展到组织系统。

Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.

</details>


### [112] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

TL;DR: 论文提出因果提示优化（CPO）框架，解决大语言模型提示优化问题，经评估表现优于现有方法，重塑提示优化经济性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型性能对提示设计敏感，现有自动提示优化方法存在对不同查询适应性差及依赖离线奖励模型的问题。

Method: CPO分两阶段，先通过双重机器学习学习离线因果奖励模型，再用无偏奖励信号搜索特定查询提示。

Result: CPO在多个基准测试中表现优于人工设计提示和现有自动优化器，在难题上表现更稳健。

Conclusion: 因果推理可作为企业大语言模型部署中可靠且经济高效提示优化的可扩展基础。

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [113] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

TL;DR: 提出MACD方法减少视频语言模型幻觉，实验显示有效且将开源代码和数据


<details>
  <summary>Details</summary>
Motivation: 现有解码方法难以控制致幻觉视觉线索且难契合模型弱点，视频语言模型易产生幻觉

Method: 结合模型引导的反事实构建和解码，利用模型反馈识别致幻觉区域，生成对象级反事实输入并集成到对比解码中

Result: 在多个数据集上实验表明，MACD可减少幻觉并提升或维持任务精度，在复杂场景有效

Conclusion: MACD是减少视频语言模型幻觉的有效推理策略

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [114] [Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives](https://arxiv.org/abs/2602.01749)
*Lin Chen,Samuel Drapeau,Fanghao Shao,Xuekai Zhu,Bo Xue,Yunchong Song,Mathieu Laurière,Zhouhan Lin*

Main category: cs.AI

TL;DR: 本文指出GFlowNet目标函数的局限性，建立其与马尔可夫链的联系，提出α - GFNs，在多个基准测试中表现优于之前的GFlowNet目标函数。


<details>
  <summary>Details</summary>
Motivation: GFlowNet目标函数隐式固定前后向策略的混合比例，限制训练中的探索 - 利用权衡。

Method: 探索GFlowNet与马尔可夫链的联系，建立等价性，提出通过可调参数α进行泛化的α - GFNs。

Result: α - GFN目标函数在多个基准测试中始终优于先前的GFlowNet目标函数，发现模式数量最多增加10倍。

Conclusion: α - GFNs可直接控制探索 - 利用动态，增强模式发现能力并确保收敛到唯一流。

Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \times$ increase in the number of discovered modes.

</details>


### [115] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: 提出对抗性奖励审计（ARA）框架应对强化学习中的奖励破解问题，实验证明其在多方面表现良好且具备跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有防止奖励破解的方法是静态防御，无法适应新的破解策略，强化学习易受奖励破解影响。

Method: ARA框架分两个阶段，先由黑客策略发现奖励模型漏洞、审计器学习检测，后AG - RLHF惩罚检测到的破解行为。

Result: 在三个破解场景实验中，ARA实现了最佳的对齐 - 效用权衡，在多个指标上表现出色，且具备跨领域泛化能力。

Conclusion: ARA是一种有效的应对奖励破解的方法，能实现多领域防御。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [116] [PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models](https://arxiv.org/abs/2602.01762)
*Xuliang Wang,Yuetao Chen,Maochan Zhen,Fang Liu,Xinzhou Zheng,Xingwu Liu,Hong Xu,Ming Li*

Main category: cs.AI

TL;DR: 本文提出PRISM方法解决大语言模型（LLMs）推测解码在提升草稿质量时计算开销大的问题，实验证明其优于现有草稿架构。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法为追求更好的草稿质量增大草稿模型参数规模，带来大量计算开销，需要解决预测精度和计算延迟间的权衡问题。

Method: 提出PRISM，将每个预测步骤的计算分散到不同参数集，重构草稿模型的计算路径，实现模型容量与推理成本的解耦。

Result: PRISM超越所有现有草稿架构，在保持最小草稿延迟的同时实现出色的接受长度，端到端加速效果好；在处理更大数据量时比其他草稿架构扩展更有效；使高度优化的推理引擎的解码吞吐量提高超2.6倍。

Conclusion: PRISM是解决大语言模型推测解码中预测精度和计算延迟权衡问题的有效方法。

Abstract: Large Language Models (LLMs), constrained by their auto-regressive nature, suffer from slow decoding. Speculative decoding methods have emerged as a promising solution to accelerate LLM decoding, attracting attention from both systems and AI research communities. Recently, the pursuit of better draft quality has driven a trend toward parametrically larger draft models, which inevitably introduces substantial computational overhead. While existing work attempts to balance the trade-off between prediction accuracy and compute latency, we address this fundamental dilemma through architectural innovation.
  We propose PRISM, which disaggregates the computation of each predictive step across different parameter sets, refactoring the computational pathways of draft models to successfully decouple model capacity from inference cost. Through extensive experiments, we demonstrate that PRISM outperforms all existing draft architectures, achieving exceptional acceptance lengths while maintaining minimal draft latency for superior end-to-end speedup. We also re-examine scaling laws with PRISM, revealing that PRISM scales more effectively with expanding data volumes than other draft architectures. Through rigorous and fair comparison, we show that PRISM boosts the decoding throughput of an already highly optimized inference engine by more than 2.6x.

</details>


### [117] [Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction](https://arxiv.org/abs/2602.01775)
*Yucheng Wu,Yuekui Yang,Hongzheng Li,Anan Liu,Jian Xiao,Junjie Zhai,Huan Yu,Shaoping Ma,Leye Wang*

Main category: cs.AI

TL;DR: 提出CrossAdapt框架用于高效跨架构知识转移，在多数据集实验和腾讯微信渠道大规模部署中展现优势。


<details>
  <summary>Details</summary>
Motivation: 大规模用户响应预测系统部署新架构时模型切换成本高，现有知识蒸馏方法存在架构异构和嵌入表转移成本高的问题。

Method: 提出两阶段框架CrossAdapt，离线阶段通过维度自适应投影实现快速嵌入转移，结合渐进式网络蒸馏和策略采样；在线阶段引入非对称协同蒸馏和分布感知适应机制。

Result: 在三个公共数据集上AUC提升0.27 - 0.43%，减少训练时间43 - 71%；在腾讯微信渠道大规模部署中显著减轻AUC下降、LogLoss增加和预测偏差。

Conclusion: CrossAdapt框架在跨架构知识转移方面有效，能降低成本并提升性能。

Abstract: Deploying new architectures in large-scale user response prediction systems incurs high model switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterogeneity and the prohibitive cost of transferring large embedding tables. We propose CrossAdapt, a two-stage framework for efficient cross-architecture knowledge transfer. The offline stage enables rapid embedding transfer via dimension-adaptive projections without iterative training, combined with progressive network distillation and strategic sampling to reduce computational cost. The online stage introduces asymmetric co-distillation, where students update frequently while teachers update infrequently, together with a distribution-aware adaptation mechanism that dynamically balances historical knowledge preservation and fast adaptation to evolving data. Experiments on three public datasets show that CrossAdapt achieves 0.27-0.43% AUC improvements while reducing training time by 43-71%. Large-scale deployment on Tencent WeChat Channels (~10M daily samples) further demonstrates its effectiveness, significantly mitigating AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.

</details>


### [118] [LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning](https://arxiv.org/abs/2602.01779)
*Rui Hua,Yu Wei,Zixin Shu,Kai Chang,Dengying Yan,Jianan Xia,Zeyu Liu,Hui Zhu,Shujie Song,Mingzhong Xiao,Xiaodong Li,Dongmei Jia,Zhuye Gao,Yanyan Meng,Naixuan Zhao,Yu Fu,Haibin Yu,Benman Yu,Yuanyuan Chen,Fei Dong,Zhizhou Meng,Pengcheng Yang,Songxue Zhao,Lijuan Pei,Yunhui Hu,Kan Ding,Jiayuan Duan,Wenmao Yin,Yang Gu,Runshun Zhang,Qiang Zhu,Jian Yu,Jiansheng Li,Baoyan Liu,Wenjia Wang,Xuezhong Zhou*

Main category: cs.AI

TL;DR: 提出LingLan基准测试对14个大模型进行中医NLP评估，揭示模型与人类专家差距，为中医大模型研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有中医基准测试在覆盖范围、规模及评分方式上存在问题，需要领域忠实的评估方法。

Method: 提出LingLan基准测试，包含统一指标设计、临床标签协议等，对14个开源和专有大模型进行零样本评估。

Result: 评估揭示了模型在中医常识知识理解、推理和临床决策支持方面的优缺点，硬子集评估显示模型与人类专家在中医专业推理上差距大。

Conclusion: LingLan通过标准化评估为中医大模型和特定领域医学AI研究建立了统一、定量和可扩展的基础。

Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.

</details>


### [119] [ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing](https://arxiv.org/abs/2602.01797)
*Hanlin Zhou,Huah Yong Chan*

Main category: cs.AI

TL;DR: 本文提出用于离散选择推理的确定性协调框架ORCH，在多数据集上表现优于单模型基线和多数投票集成，还通过EMA路由器提升效果，为基于大语言模型的代理系统提供实用路径。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体架构在离散选择推理中依赖随机路由或临时启发式方法，行为难重现、决策过程难解释。

Method: 提出ORCH框架，遵循“多分析，一决策”范式，用固定规则进行任务分解和答案聚合，还可选引入EMA引导路由器。

Result: 在MMLU、MMLU - Pro和GSM8K上，ORCH始终优于单模型基线和多数投票集成，EMA路由器额外提升0.7 - 2.0点准确率。

Conclusion: ORCH为离散选择推理的基于大语言模型的代理系统提供了可控、可解释且可部署的实用路径。

Abstract: Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.

</details>


### [120] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

TL;DR: 提出INDIBATOR框架用于分子发现，以个性化科学家档案构建代理，多轮辩论表现优于粗粒度角色系统。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统为区分代理行为采用的角色或关键词方法，简化了人类科学家的工作方式，需新方法。

Method: 提出INDIBATOR框架，基于发表历史和分子历史构建个性化科学家档案，代理通过提案、批判和投票阶段进行多轮辩论。

Result: 细粒度基于个性的代理始终优于依赖粗粒度角色的系统，取得有竞争力或最先进的性能。

Conclusion: 捕捉个体代理的“科学DNA”对高质量发现至关重要。

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [121] [Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs](https://arxiv.org/abs/2602.01832)
*Rui Wang,Yaoguang Cao,Yuyi Chen,Jianyi Xu,Zhuoyang Li,Jiachen Shang,Shichun Yang*

Main category: cs.AI

TL;DR: 提出车辆联觉（SoV）框架，用视觉输入预测触觉激励，实验表明模型性能优，提升自动驾驶安全。


<details>
  <summary>Details</summary>
Motivation: 当前视觉和光学传感器无法检测对车辆动态控制至关重要的道路激励，为解决此问题提出SoV框架。

Method: 开发跨模态时空对齐方法解决时空差异，提出基于潜在扩散的视觉 - 触觉联觉（VTSyn）生成模型进行无监督高质量触觉数据合成，收集多模态数据集。

Result: VTSyn在时间、频率和分类性能上优于现有模型。

Conclusion: VTSyn通过主动触觉感知增强了自动驾驶车辆的安全性。

Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.

</details>


### [122] [ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems](https://arxiv.org/abs/2602.01848)
*Salaheddin Alzu'bi,Baran Nama,Arda Kaz,Anushri Eswaran,Weiyuan Chen,Sarvesh Khetan,Rishab Bala,Tu Vu,Sewoong Oh*

Main category: cs.AI

TL;DR: 现有智能体框架在长时任务表现不佳，提出ROMA框架及GEPA+方法，在推理和长文本生成基准测试中取得领先性能，证明递归模块化架构优势。


<details>
  <summary>Details</summary>
Motivation: 解决当前智能体框架在长时任务中因顺序编排脆弱、上下文窗口限制和执行轨迹不透明导致的性能问题。

Method: 引入ROMA框架，通过递归任务分解和结构化聚合解决问题，规范四个模块化角色；还引入GEPA+在不微调情况下适配特定任务。

Result: ROMA结合GEPA+在推理和长文本生成基准测试中取得领先性能，如在SEAL - 0上提升准确率，在EQ - Bench上使模型匹配领先闭源模型表现。

Conclusion: 递归、模块化的智能体架构可在保持可解释性、灵活性和模型无关性的同时扩展推理深度。

Abstract: Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.

</details>


### [123] [SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures](https://arxiv.org/abs/2602.01858)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: 提出SOPRAG框架解决标准操作程序（SOP）检索难题，实验显示其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 标准语义驱动的RAG范式无法解决SOP检索面临的挑战，如专有结构僵化、条件依赖相关性和可操作执行要求等问题。

Method: 提出SOPRAG框架，用专业的实体、因果和流程图专家代替扁平分块；使用程序卡层优化搜索空间，用大语言模型引导的门控机制动态加权专家；引入自动多智能体工作流构建基准。

Result: 在四个工业领域的大量实验表明，SOPRAG在检索准确性和响应实用性方面显著优于基于词法、密集和图的RAG基线方法，在实际关键任务中实现完美执行分数。

Conclusion: SOPRAG是有效解决SOP检索痛点的方案。

Abstract: Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.

</details>


### [124] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

TL;DR: 提出ProcMEM框架让大语言模型驱动的智能体自主学习程序记忆，减少计算冗余和执行不稳定性，实验显示有优越复用率和性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的智能体在顺序决策中依赖即时推理，经验复用不足，导致计算冗余和执行不稳定。

Method: 提出ProcMEM框架，通过形式化Skill - MDP将被动叙事转化为可执行技能，引入非参数PPO，利用语义梯度生成高质量候选并通过PPO门进行技能验证，用基于分数的维护机制保持记忆质量。

Result: 在不同场景实验中，ProcMEM实现了优越的复用率和极高记忆压缩下的显著性能提升，可视化展示了知识积累、提炼和复用过程。

Conclusion: ProcMEM能让智能体有效积累、提炼和复用程序知识，促进长期自主能力。

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [125] [Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models](https://arxiv.org/abs/2602.01884)
*Shidong Yang,Tongwen Huang,Hao Wen,Yong Wang,Li Chen,Xiangxiang Chu*

Main category: cs.AI

TL;DR: 本文指出多模态推理奖励模型训练存在的问题，提出熵引导训练（EGT）方法，实验显示该方法优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理奖励模型训练存在偏好数据集噪声和传统训练方法低效的问题，需改进训练方式。

Method: 提出EGT方法，包括熵引导的数据筛选和训练策略，利用响应熵与准确性的关联。

Result: 在三个基准测试中，EGT训练的模型始终优于最先进的多模态奖励模型。

Conclusion: EGT方法能有效解决多模态推理奖励模型训练的问题，提升模型性能。

Abstract: Multimodal reward models are crucial for aligning multimodal large language models with human preferences. Recent works have incorporated reasoning capabilities into these models, achieving promising results. However, training these models suffers from two critical challenges: (1) the inherent noise in preference datasets, which degrades model performance, and (2) the inefficiency of conventional training methods, which ignore the differences in sample difficulty. In this paper, we identify a strong correlation between response entropy and accuracy, indicating that entropy can serve as a reliable and unsupervised proxy for annotation noise and sample difficulty. Based on this insight, we propose a novel Entropy-Guided Training (EGT) approach for multimodal reasoning reward models, which combines two strategies: (1) entropy-guided data curation to mitigate the impact of unreliable samples, and (2) an entropy-guided training strategy that progressively introduces more complex examples. Extensive experiments across three benchmarks show that the EGT-trained model consistently outperforms state-of-the-art multimodal reward models.

</details>


### [126] [Geometric Analysis of Token Selection in Multi-Head Attention](https://arxiv.org/abs/2602.01893)
*Timur Mudarisov,Mikhal Burtsev,Tatiana Petrova,Radu State*

Main category: cs.AI

TL;DR: 提出分析大语言模型多头注意力的几何框架，定义几何指标量化可分离性，理论与实验相符，发现LLaMA - 2 - 7B头的三种机制，表明注意力是有结构的几何分类器。


<details>
  <summary>Details</summary>
Motivation: 分析大语言模型中多头注意力机制，为其提供可解释性并指导设计。

Method: 从top - N选择视角观察标准注意力，在值状态空间研究其行为，定义几何指标，推导非渐近边界。

Result: 理论预测了小N操作区域，实验测量与理论相符，发现LLaMA - 2 - 7B头的三种机制。

Conclusion: 注意力是有结构的几何分类器，提供头级可解释性，可指导注意力的稀疏化和设计。

Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.

</details>


### [127] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

TL;DR: 介绍专门为智能家居传感器数据设计的基础模型DomusFM，采用自监督双对比学习范式，在多数据集评估中表现优于现有基线，能应对数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能家居传感器数据处理方法存在局限，如监督模型需大量标注数据，基础模型仅关注惯性传感器，LLM 方法有隐私和成本问题。

Method: 引入 DomusFM 模型，采用自监督双对比学习范式，集成轻量级语言模型的语义嵌入和专门的编码器。

Result: 通过七组公开数据集评估，DomusFM 在不同下游任务中优于现有基线，仅用 5% 标注数据微调也有出色表现。

Conclusion: DomusFM 可解决数据稀缺问题，适用于现实智能家居系统部署。

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


### [128] [Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling](https://arxiv.org/abs/2602.01933)
*Fabrice Boissier,Monica Sen,Irina Rychkova*

Main category: cs.AI

TL;DR: 本文对比大语言模型（LLM）和形式概念分析（FCA）在主题建模领域的优劣。


<details>
  <summary>Details</summary>
Motivation: 主题建模应用广泛，LLM用于此任务研究少，FCA无实际应用案例，需对比二者优劣。

Method: 用CREA管道评估FCA，用GPT - 5代表LLM，采用三种提示策略进行零样本设置实验，开展两个实验。

Result: 原文未提及。

Conclusion: 原文未提及。

Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.

</details>


### [129] [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)
*Yun Qu,Qi Wang,Yixiu Mao,Heming Zou,Yuhang Jiang,Weijie Liu,Clive Bai,Kai Yang,Yangkun Chen,Saiyong Yang,Xiangyang Ji*

Main category: cs.AI

TL;DR: 研究提出GPS方法提升大语言模型强化学习训练效率，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习计算成本高，现有在线提示选择方法存在依赖高成本评估或缺乏泛化性的问题。

Method: 引入Generalizable Predictive Prompt Selection (GPS)，用轻量级生成模型对提示难度进行贝叶斯推理，将中间难度优先级和历史锚定多样性纳入批量获取原则选择提示批次。

Result: 在不同推理基准测试中，GPS在训练效率、最终性能和测试时效率上相比基线方法有显著提升。

Conclusion: GPS方法能有效提高大语言模型强化学习的训练和测试效率。

Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.

</details>


### [130] [Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning](https://arxiv.org/abs/2602.01983)
*Xintian Shen,Jiawei Chen,Lihao Zheng,Hao Ma,Tao Wei,Kun Zhan*

Main category: cs.AI

TL;DR: 提出UCT训练无框架，让代理从工具使用者变为创建者，提升TIR模型能力，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有TIR模型使用固定工具，缺乏自优化机制，构建工具需大量手动工作，无法满足开放问题需求。

Method: 提出UCT框架，转化代理为工具创建者，引入记忆巩固机制维护工具库。

Result: 方法作为新范式提升TIR模型能力，在多领域基准测试中性能显著提升。

Conclusion: 提出的方法有效，可使代理系统自我进化。

Abstract: Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\uparrow$ and +23.04%$\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.

</details>


### [131] [Emergent Analogical Reasoning in Transformers](https://arxiv.org/abs/2602.01992)
*Gouki Minegishi,Jingyuan Feng,Hiroki Furuta,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文受范畴论启发，将类比推理形式化，引入合成任务评估，发现其受多种因素影响，剖析了Transformer中类比推理机制并在预训练大模型中验证。


<details>
  <summary>Details</summary>
Motivation: 尽管类比推理在人类认知中很重要，但Transformers获取和实现类比推理的机制尚不清楚。

Method: 受范畴论中函子概念启发，将类比推理形式化为跨类别实体对应关系的推理；引入合成任务评估；进行机制分析。

Result: 类比推理的出现对数据特征、优化选择和模型规模高度敏感；Transformer中的类比推理可分解为嵌入空间中关系结构的几何对齐和在Transformer内应用函子；相同趋势在预训练大模型中存在。

Conclusion: 将类比从抽象认知概念转变为现代神经网络中有具体机制基础的现象。

Abstract: Analogy is a central faculty of human intelligence, enabling abstract patterns discovered in one domain to be applied to another. Despite its central role in cognition, the mechanisms by which Transformers acquire and implement analogical reasoning remain poorly understood. In this work, inspired by the notion of functors in category theory, we formalize analogical reasoning as the inference of correspondences between entities across categories. Based on this formulation, we introduce synthetic tasks that evaluate the emergence of analogical reasoning under controlled settings. We find that the emergence of analogical reasoning is highly sensitive to data characteristics, optimization choices, and model scale. Through mechanistic analysis, we show that analogical reasoning in Transformers decomposes into two key components: (1) geometric alignment of relational structure in the embedding space, and (2) the application of a functor within the Transformer. These mechanisms enable models to transfer relational structure from one category to another, realizing analogy. Finally, we quantify these effects and find that the same trends are observed in pretrained LLMs. In doing so, we move analogy from an abstract cognitive notion to a concrete, mechanistically grounded phenomenon in modern neural networks.

</details>


### [132] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: 提出一种对话诊断系统，通过两步推理探索诊断知识图谱，使用适应的患者模拟器评估，实验显示性能提升且医生认可其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有对话诊断方法依赖模型参数知识或假设患者提供丰富信息，不切实际，需改进。

Method: 提出对话诊断系统，两步推理：从对话上下文生成诊断假设，通过澄清问题验证假设；采用成熟模拟器和患者档案，调整以反映早期临床情况。

Result: 实验显示相比基线，诊断准确性和效率提高，医生评估认可模拟器真实性和问题的临床实用性。

Conclusion: 所提对话诊断系统有效，代码将发布。

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [133] [Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction](https://arxiv.org/abs/2602.02018)
*Enes Altinisik,Masoomali Fatehkia,Fatih Deniz,Nadir Durrani,Majd Hawasly,Mohammad Raza,Husrev Taha Sencar*

Main category: cs.AI

TL;DR: 提出VeriFY框架降低大语言模型事实幻觉率，减少幻觉同时仅适度降低召回率。


<details>
  <summary>Details</summary>
Motivation: 现有缓解大语言模型事实幻觉的方法常导致过于保守的行为。

Method: 提出VeriFY训练框架，通过基于一致性的自我验证让模型推理事实不确定性，采用阶段级损失掩码方法避免强化幻觉内容。

Result: 在多个模型系列和规模上，将事实幻觉率降低9.7%至53.3%，召回率仅适度降低0.4%至5.7%，在单一源上训练可跨数据集泛化。

Conclusion: VeriFY框架能有效降低大语言模型的事实幻觉率。

Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.

</details>


### [134] [Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron](https://arxiv.org/abs/2602.02027)
*Sicheng Shen,Mingyang Lv,Han Shen,Jialin Wu,Binghao Wang,Zhou Yang,Guobin Shen,Dongcheng Zhao,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 提出一种安全感知解码方法用于大模型轻量级对齐，兼顾实用性与安全性，在训练开销和泛化性上有优势。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全对齐方案计算成本高、泛化性差，轻量级方法存在依赖先验或模型自身能力的问题，导致泛化能力和效率受限。

Method: 提出一种安全感知解码方法，仅需低成本训练专家模型，并使用单个神经元作为门控机制。

Result: 该方法能有效平衡模型内在能力和外部指导，同时保留实用性并提升输出安全性。

Conclusion: 此方法在训练开销和跨模型规模泛化方面展现出明确优势，为大语言模型安全实用部署的轻量级对齐提供新视角。

Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.

</details>


### [135] [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028)
*Ya Gao,Kalle Kujanpää,Pekka Marttinen,Harri Valpola,Alexander Ilin*

Main category: cs.AI

TL;DR: 现有知识编辑方法整合知识存缺陷，提出基于三原则的训练策略，实验显示该策略有效。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能系统尤其是大语言模型整合新知识并在推理中灵活应用的挑战，弥补现有知识编辑方法不能将新信息融入连贯框架的不足。

Method: 提出基于三原则的训练策略，包括将新知识作为连贯背景故事引入、用自生成多跳问题训练、使用知识蒸馏训练。

Result: 经该策略训练的模型能在推理中有效利用新知识，在需结合多个新事实的难题上表现出色。

Conclusion: 知识内化是推理问题，所提训练策略对模型整合和应用新知识有效。

Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.

</details>


### [136] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: 介绍基于有限 - 水平马尔可夫决策过程的多智能体系统用于复杂工作流，在AI安全评估案例中效果优于单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体架构多依赖单智能体提示工程，难以观察和比较模型在不确定和协调方面的表现。

Method: 引入具有有向无环结构的有限 - 水平马尔可夫决策过程的多智能体系统，用蒙特卡罗估计量化智能体层面的认知不确定性。

Result: 在AI安全评估案例中，相比单智能体基线，准确率最高提升19%，人工审核最多减少85倍，部分配置下处理时间减少。

Conclusion: 所提出的多智能体系统在复杂工作流中表现优于单智能体方法。

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [137] [SIDiffAgent: Self-Improving Diffusion Agent](https://arxiv.org/abs/2602.02051)
*Shivank Garg,Ayush Singh,Gaurav Kumar Nayak*

Main category: cs.AI

TL;DR: 文章指出文本到图像扩散模型的局限，提出无训练的Self-Improving Diffusion Agent框架解决问题，表现优异且将开源代码。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在实际部署中存在对提示措辞敏感、语义解释模糊、有伪影等局限，现有方法需额外训练且可控性有限，因此需要新方法解决这些问题。

Method: 引入Self-Improving Diffusion Agent框架，利用Qwen系列模型，自主管理提示工程、检测和纠正不良生成、去除伪影，通过数据库存储经验实现迭代自我改进。

Result: 在GenAIBench上平均VQA得分0.884，显著优于开源、专有模型和代理方法。

Conclusion: Self-Improving Diffusion Agent框架能有效解决文本到图像扩散模型的局限，实现更可靠和一致的输出。

Abstract: Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.

</details>


### [138] [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133)
*Sangwoo Shin,BumJun Kim,Kyelim Lee,Moongyu Jeon,Albert No*

Main category: cs.AI

TL;DR: 研究掩码扩散语言模型（MDMs）缓解自回归语言模型（ARMs）反转诅咒的原因，发现是架构结构及其与训练的相互作用。


<details>
  <summary>Details</summary>
Motivation: ARMs存在反转诅咒问题，MDMs缓解但原因不明，常见解释不合理，所以探究其缓解的真正原因。

Method: 分析一层Transformer编码器中权重共享使正反向注意力得分正相关，对应梯度对齐；进行受控玩具任务和大规模扩散语言模型实验。

Result: 实验支持架构结构及其与训练的相互作用是MDMs缓解反转诅咒的机制。

Conclusion: 解释了MDMs能部分克服ARMs持续存在的反转诅咒问题的原因。

Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that "$A$ is $B$", they often fail on the reverse query "$B$ is $A$". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing "[MASK] is $B$" during training does not necessarily teach the model to handle the reverse prompt "$B$ is [MASK]". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.

</details>


### [139] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

TL;DR: 现有安全对齐数据集与目标大模型存在分布差异，导致推理能力下降，提出DGR方法构建数据集，有效缓解安全税，实验证明其有效性并揭示安全对齐激活机制。


<details>
  <summary>Details</summary>
Motivation: 现有用于大推理模型安全对齐的数据集与目标模型存在分布差距，导致目标模型推理能力显著下降。

Method: 提出安全对齐数据集构建方法DGR，将现有的分布外安全推理数据集进行转换和细化，使其与目标大语言模型的内部分布对齐。

Result: 1. DGR有效缓解安全税，在DirectRefusal和R1 - ACT上平均推理准确率比Vanilla SFT分别提高30.2%和21.2%。2. 推理能力下降程度与分布转移程度相关。3. 仅10个样本就能激活有效拒绝行为。

Conclusion: 强调分布一致性的重要性，为推理模型的安全激活机制提供了见解。

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [140] [Traffic-Aware Navigation in Road Networks](https://arxiv.org/abs/2602.02158)
*Sarah Nassar*

Main category: cs.AI

TL;DR: 比较金斯顿道路网中三种交通感知导航的图搜索方法，分析各自优缺点以按需选择最佳方案。


<details>
  <summary>Details</summary>
Motivation: 为交通感知导航任务在金斯顿道路网中寻找合适的图搜索方法。

Method: 比较三种图搜索方法，分别是单轮多查询预处理算法（Floyd - Warshall - Ingerman）、连续单查询实时搜索（Dijkstra和A*）以及结合两种方法的Yen算法。

Result: Dijkstra和A*需最少预处理且得到最具交通感知的最优解；Floyd - Warshall - Ingerman实时最快但无交通感知；Yen算法预处理多，但在运行速度和最优性上平衡。

Conclusion: 每种方法有优缺点，需根据具体部署情况权衡选择最佳方案。

Abstract: This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.

</details>


### [141] [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188)
*Xia Jiang,Jing Chen,Cong Zhang,Jie Gao,Chengpeng Hu,Chenhao Zhang,Yaoxin Wu,Yingqian Zhang*

Main category: cs.AI

TL;DR: 提出NLCO基准评估大语言模型在组合优化推理能力，实验显示模型在小实例表现好，大实例性能下降。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在组合优化处理能力研究不足，需评估其端到端组合优化推理能力。

Method: 引入NLCO基准，涵盖43个组合优化问题，用四层分类法组织，提供求解器标注解，从可行性、最优性和推理效率评估模型。

Result: 高性能模型在小实例上可行性和求解质量高，实例规模增大性能下降；基于集合任务较易，图结构问题和瓶颈目标易失败。

Conclusion: 大语言模型在组合优化推理能力受实例规模和问题类型影响，仍有提升空间。

Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \textbf{N}atural \textbf{L}anguage \textbf{C}ombinatorial \textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.

</details>


### [142] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: 本文提出Test - time Improvement Diagnostic Evaluation (TIDE)框架评估TTI，实验表明提升智能体性能需优化与环境交互动态。


<details>
  <summary>Details</summary>
Motivation: 当前对TTI成功或失败机制理解不足，现有评估指标无法衡量任务优化效率等。

Method: 提出TIDE框架，将TTI分解为三个相互关联维度进行测量。

Result: 通过不同智能体和环境的大量实验，TIDE框架显示提升智能体性能不止要扩展内部推理。

Conclusion: 提升智能体性能需明确优化与环境的交互动态。

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [143] [More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression](https://arxiv.org/abs/2602.02199)
*Aryan Sood,Tanvi Sharma,Vansh Agrawal*

Main category: cs.AI

TL;DR: 本文针对大语言模型KV缓存内存线性增长问题，提出LASER - KV框架进行KV压缩测试，实验表明其性能优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型实际部署受KV缓存内存线性增长限制，现有压缩策略存在语义召回和内存效率的权衡问题。

Method: 提出LASER - KV框架，采用由保护除数（n）控制的分块累积策略，隔离压缩和滑动窗口伪影的影响。

Result: 在Babilong基准测试中，以往压缩方法在长上下文任务中性能下降15 - 30%，LASER - KV性能稳定，在128k时精度最高可提升10%。

Conclusion: 研究结果挑战了注意力分数足以代表令牌效用的普遍假设。

Abstract: While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.

</details>


### [144] [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304)
*Martino Ciaperoni,Marzio Di Vece,Luca Pappalardo,Fosca Giannotti,Francesco Giannini*

Main category: cs.AI

TL;DR: 大规模基础模型存在行为转变现象，现有XAI方法难以解释，提出对比XAI（Δ - XAI）框架并介绍实验。


<details>
  <summary>Details</summary>
Motivation: 现有经典可解释AI（XAI）方法无法解释模型不同检查点间内部变化，需解释模型行为转变。

Method: 提出对比XAI（Δ - XAI）框架，设定一系列设计解释方法时需考虑的需求，介绍了可能的流程并进行具体实验。

Result: 文中未明确提及具体结果，仅介绍了方法框架和实验。

Conclusion: 应采用对比方式解释模型行为转变，并提出了Δ - XAI框架。

Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.

</details>


### [145] [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313)
*Changming Li,Kaixing Zhang,Haoyun Xu,Yingdong Shi,Zheng Zhang,Kaitao Song,Kan Ren*

Main category: cs.AI

TL;DR: 本文提出集成策略梯度（IPG）框架，用于识别对推理行为有顺序贡献的组件，实现对推理行为更精确的定位和可靠调节。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理机制不透明，现有可解释性方法难以精确定位复杂推理机制或捕捉从模型内部工作到推理输出的顺序影响。

Method: 基于面向结果和顺序影响感知原则，提出集成策略梯度（IPG）框架，通过反向传播基于复合结果的信号，将推理行为归因于模型的内部组件。

Result: 该方法在不同推理模型中实现了更精确的定位，并能可靠地调节推理行为。

Conclusion: 所提的IPG框架能有效解决现有方法的不足，实现推理行为的精确分析和调节。

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.

</details>


### [146] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

TL;DR: 提出多LLM上下文学习方法M2CL，解决多智能体讨论不一致问题，效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体讨论（MAD）方法因个体上下文未对齐，易出现讨论不一致和无法达成一致解决方案的问题。

Method: 引入M2CL方法，为每个智能体学习上下文生成器，通过自适应机制控制上下文一致性和输出差异。

Result: 在学术推理、具身任务和移动控制等任务上评估，M2CL性能比现有方法高20%-50%，且具有良好的可迁移性和计算效率。

Conclusion: M2CL能有效解决多智能体讨论一致性问题，提升性能。

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [147] [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369)
*Yaolun Zhang,Yiran Wu,Yijiong Yu,Qingyun Wu,Huazheng Wang*

Main category: cs.AI

TL;DR: 介绍在线自我进化内存系统Live - Evo，解耦经验与使用方式，管理在线内存，在实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有自我进化系统多基于静态数据集，在真实分布变化和连续反馈下较脆弱，需开发在线学习的内存系统。

Method: 通过经验库和元指南库解耦经验与使用方式，维护经验权重并根据反馈更新。

Result: 在10周的实时Prophet Arena基准测试中，Brier分数提高20.8%，市场回报率增加12.9%，在深度研究基准上也有稳定提升。

Conclusion: Live - Evo能有效提升任务解决性能，代码已开源。

Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.

</details>


### [148] [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416)
*Ankur Samanta,Akshayaa Magesh,Ayush Jain,Kavosh Asadi,Youliang Yu,Daniel Jiang,Boris Vidolov,Kaveh Hassani,Paul Sajda,Jalaj Bhandari,Yonathan Efroni*

Main category: cs.AI

TL;DR: 本文探索语言模型能否定位推理错误以实现自我修正，提出结构化推理提示方法和Thought - ICS框架，该框架能提升自我修正能力，在自主场景中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型自我修正困难，探索其能否明确找出错误推理中的错误，以构建能有效自我修正的AI系统。

Method: 引入将推理结构化的提示方法，提出Thought - ICS自我修正框架，该框架逐步骤生成推理，定位错误后回溯重新生成。

Result: 在有oracle验证时，Thought - ICS自我修正能力提升20 - 40%；在无外部验证的自主场景中，优于当代自我修正基线方法。

Conclusion: 语言模型通过结构化推理能可靠定位错误，Thought - ICS框架能有效提升语言模型的自我修正能力。

Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

</details>


### [149] [Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling](https://arxiv.org/abs/2602.02453)
*Andong Chen,Wenxin Zhu,Qiuyu Ding,Yuchen Song,Muyun Yang,Tiejun Zhao*

Main category: cs.AI

TL;DR: 提出用漫画进行视觉推理范式，实验显示其在多步时间和因果推理任务上优于图像推理，且比视频推理更高效，漫画可作有效中间视觉表征。


<details>
  <summary>Details</summary>
Motivation: 当前大模型链思维推理在不同模态存在局限，图像难表示时间结构，视频有冗余和高计算成本。

Method: 提出用漫画作为介于图像和视频之间的高信息密度媒介进行视觉推理范式，系统研究两条基于漫画的推理路径，并在多种推理和长上下文理解任务上评估。

Result: 在多步时间和因果推理任务上，漫画推理优于图像推理，且比视频推理更高效，不同漫画叙事结构和风格会影响任务表现。

Conclusion: 漫画是改善多模态推理的有效中间视觉表征。

Abstract: Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.

</details>


### [150] [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465)
*Jana Zeller,Thaddäus Wiedemer,Fanfei Li,Thomas Klein,Prasanna Mayilvahanan,Matthias Bethge,Felix Wichmann,Ryan Cotterell,Wieland Brendel*

Main category: cs.AI

TL;DR: 前沿模型从仅摄入视觉信息的多模态大语言模型向原生交错生成的统一多模态模型转变，研究开发MentisOculi评估模型视觉推理能力，发现视觉思维尚未提升模型推理，该工具为分析和缩小差距奠定基础。


<details>
  <summary>Details</summary>
Motivation: 前沿模型转变引发对利用中间可视化辅助推理的研究兴趣，为评估模型形成、维持和操作视觉表征的能力。

Method: 开发MentisOculi，一套适合视觉解决的多步推理问题程序分层套件，评估从潜在令牌到显式生成图像的视觉策略。

Result: 视觉策略通常未能提高模型性能，统一多模态模型存在生成错误累积问题，无法利用真实可视化信息。

Conclusion: 尽管具有吸引力，视觉思维目前对模型推理并无帮助，MentisOculi为各模型家族分析和缩小差距奠定基础。

Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.

</details>


### [151] [Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts](https://arxiv.org/abs/2602.02468)
*Aiden Yiliu Li,Xinyue Hao,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 现有自主网页代理执行长时任务有局限，本文介绍Avenir - Web，它在Online - Mind2Web基准测试达开源最优水平。


<details>
  <summary>Details</summary>
Motivation: 解决现有自主网页代理在复杂动态网页界面执行长时任务时存在的元素定位不准、缺乏特定程序知识、长时任务跟踪和记忆不稳定等问题。

Method: 采用Mixture of Grounding Experts、Experience - Imitation Planning融合程序先验知识，结合任务跟踪清单和自适应内存，实现跨多样用户界面的交互。

Result: Avenir - Web在Online - Mind2Web基准测试中显著超越先前开源代理，达到与顶级专有模型相当的性能。

Conclusion: Avenir - Web为实时网站上可靠的网页代理建立了新的开源最优标准。

Abstract: Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.

</details>


### [152] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

TL;DR: 本文指出自回归大语言模型存在‘反转诅咒’问题，通过简单的数据正则化方法可缓解该问题，理论和实验均有验证。


<details>
  <summary>Details</summary>
Motivation: 解决自回归大语言模型在逻辑推理中存在的‘反转诅咒’问题，挑战该问题是模型固有基本局限的观点。

Method: 使用一种名为身份桥（Identity Bridge）的简单数据正则化方法微调训练数据，并从理论上分析梯度下降的隐式偏置。

Result: 理论上证明单层Transformer可打破反转诅咒；实验中，用该方法微调的1B预训练语言模型在反转任务上成功率达40%，而仅用正向知识数据训练时成功率接近零。

Conclusion: 为反转诅咒问题提供了新的理论基础，提供了一种鼓励大语言模型从数据中学习更高级规则的原则性、低成本方法。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>


### [153] [AgentRx: Diagnosing AI Agent Failures from Execution Trajectories](https://arxiv.org/abs/2602.02475)
*Shraddha Barke,Arnav Goyal,Alind Khare,Avaljot Singh,Suman Nath,Chetan Bansal*

Main category: cs.AI

TL;DR: 为解决AI智能体执行失败难定位问题，发布含115个失败轨迹的基准，提出自动化诊断框架AGENTRX，在三领域表现优于基线。


<details>
  <summary>Details</summary>
Motivation: AI智能体执行失败难以定位，当前存在差距。

Method: 手动标注失败运行轨迹，发布基准；提出AGENTRX框架，合成约束、逐步评估、生成验证日志，用大语言模型判断关键步骤和类别。

Result: AGENTRX框架在三个领域的步骤定位和失败归因方面优于现有基线。

Conclusion: 所提基准和AGENTRX框架有助于解决AI智能体失败定位问题。

Abstract: AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [154] [An Impulse-formed Navier-Stokes Solver based on Long-range Particle Flow Maps](https://arxiv.org/abs/2602.00499)
*Zhiqi Li,Duowen Chen,Junwei Zhou,Sinan Wang,Yuchen Sun,Bo Zhu*

Main category: cs.CE

TL;DR: 本文提出粒子-网格特征映射框架，将长程特征映射扩展到粘性、有体力和复杂边界的纳维 - 斯托克斯动力学。


<details>
  <summary>Details</summary>
Motivation: 将长程特征映射从无粘流扩展到通用的纳维 - 斯托克斯动力学，处理有粘性、体力和复杂边界的情况。

Method: 基于粒子轨迹自然提供长程流映射的观察构建方法，将冲量作为沿特征映射的主要量，并导出积分公式分解冲量演化。

Result: 得到一个统一的特征映射求解器，可处理有粘性和体力的不可压缩纳维 - 斯托克斯流。

Conclusion: 该求解器能处理相关流动，同时保持特征传输的准确性和几何保真度。

Abstract: We present a particle-grid characteristic-mapping framework that extends long-range characteristic mapping from inviscid flows to general Navier-Stokes dynamics with viscosity, body forces, and complex boundaries. Unlike traditional grid-based and vorticity-centered characteristic methods, our method is built on the observation that particle trajectories naturally provide the long-range flow map, enabling geometric quantities and their gradients to be transported in a direct and effective manner. We identify the impulse, the gauge variable of the velocity field, as the primary quantity mapped along characteristics while remaining compatible with standard velocity-based incompressible solvers. Using the 1-form representation of the impulse equation, we derive an integral formulation that decomposes the impulse evolution into a component transported geometrically along the particle flow map and a complementary component generated by viscosity and body forces evaluated through path integrals accumulated along particle trajectories. These components together yield a unified characteristic-mapping solver capable of handling incompressible Navier-Stokes flows with viscosity and body forces while maintaining the accuracy and geometric fidelity of characteristic transport.

</details>


### [155] [The Enhanced Physics-Informed Kolmogorov-Arnold Networks: Applications of Newton's Laws in Financial Deep Reinforcement Learning (RL) Algorithms](https://arxiv.org/abs/2602.01388)
*Trang Thoi,Hung Tran,Tram Thoi,Huaiyang Zhong*

Main category: cs.CE

TL;DR: 本文提出结合物理信息的Kolmogorov - Arnold网络（PIKANs）的强化学习框架用于投资组合优化，在三个股票市场上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在金融交易中存在不稳定和泛化能力差的问题，需要更稳定有效的方法用于投资组合优化。

Method: 将PIKANs融入多个深度强化学习算法，在行动者和评论家组件中用KANs替代传统多层感知器，在行动者更新时引入物理信息正则化损失。

Result: 在中美越三个股票市场，基于PIKAN的代理在累计和年化回报、夏普和卡尔玛比率及回撤特性上表现更好，训练更稳定。

Conclusion: 该方法在高度动态和嘈杂的金融市场中很有价值，相比传统深度强化学习表现更优。

Abstract: Deep Reinforcement Learning (DRL), a subset of machine learning focused on sequential decision-making, has emerged as a powerful approach for tackling financial trading problems. In finance, DRL is commonly used either to generate discrete trade signals or to determine continuous portfolio allocations. In this work, we propose a novel reinforcement learning framework for portfolio optimization that incorporates Physics-Informed Kolmogorov-Arnold Networks (PIKANs) into several DRL algorithms. The approach replaces conventional multilayer perceptrons with Kolmogorov-Arnold Networks (KANs) in both actor and critic components-utilizing learnable B-spline univariate functions to achieve parameter-efficient and more interpretable function approximation. During actor updates, we introduce a physics-informed regularization loss that promotes second-order temporal consistency between observed return dynamics and the action-induced portfolio adjustments. The proposed framework is evaluated across three equity markets-China, Vietnam, and the United States, covering both emerging and developed economies. Across all three markets, PIKAN-based agents consistently deliver higher cumulative and annualized returns, superior Sharpe and Calmar ratios, and more favorable drawdown characteristics compared to both standard DRL baselines and classical online portfolio-selection methods. This yields more stable training, higher Sharpe ratios, and superior performance compared to traditional DRL counterparts. The approach is particularly valuable in highly dynamic and noisy financial markets, where conventional DRL often suffers from instability and poor generalization.

</details>


### [156] [Mutual-Guided Expert Collaboration for Cross-Subject EEG Classification](https://arxiv.org/abs/2602.01728)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Jiannong Cao,Shenghua Zhong,Sean Fontaine,Changhong Jing,Shuqiang Wang*

Main category: cs.CE

TL;DR: 现有脑电信号解码方法因数据异质性存在泛化问题，本文提出MGEC框架，在合成数据集验证理论，在七个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 脑电数据存在跨主体和会话的异质性，限制了现有方法的泛化能力，且现有表征学习和集成学习方法存在不足。

Method: 提出Mutual - Guided Expert Collaboration (MGEC)框架，采用与特定领域和不变领域功能对齐的网络结构，通过共享专家引导学习捕获可约的领域不变功能，路由专家引导学习用混合专家架构建模不可约的特定领域功能，相互引导学习实现协作正则化。

Result: 在合成数据集上验证了理论发现，在七个基准测试中MGEC优于现有方法。

Conclusion: MGEC框架能有效解决脑电信号解码中因数据异质性导致的泛化问题，为脑电信号解码提供了更有效的方法。

Abstract: Decoding the human brain from electroencephalography (EEG) signals holds promise for understanding neurological activities. However, EEG data exhibit heterogeneity across subjects and sessions, limiting the generalization of existing methods. Representation learning approaches sacrifice subject-specific information for domain invariance, while ensemble learning methods risk error accumulation for unseen subjects. From a theoretical perspective, we reveal that the applicability of these paradigms depends on the reducibility cost of domain-specific functions to domain-invariant ones. Building on this insight, we propose a Mutual-Guided Expert Collaboration (MGEC) framework that employs distinct network structures aligned with domain-specific and domain-invariant functions. Shared expert-guided learning captures reducible domain-invariant functions. Routed expert-guided learning employs a mixture-of-experts architecture to model irreducible domain-specific functions. Mutual-guided learning enables collaborative regularization to prevent over-reduction and under-reduction. We validate our theoretical findings on synthetic datasets, and experiments on seven benchmarks demonstrate that MGEC outperforms state-of-the-art methods.

</details>


### [157] [Cell-JEPA: Latent Representation Learning for Single-Cell Transcriptomics](https://arxiv.org/abs/2602.02093)
*Ali ElSheikh,Rui-Xi Wang,Weimin Wu,Yibo Wen,Payam Dibaeinia,Jennifer Yuntong Zhang,Jerry Yao-Chieh Hu,Mei Knudson,Sudarshan Babu,Shao-Hua Sun,Aly A. Khan,Han Liu*

Main category: cs.CE

TL;DR: 提出Cell - JEPA模型，从重建稀疏基因表达转向潜在空间预测，在细胞类型聚类上优于scGPT，在单细胞系扰动预测中有不同表现。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞基础模型通过重建掩码基因表达学习，会将技术噪声当信号，重建目标促使模型编码测量伪影而非稳定细胞程序。

Method: 引入Cell - JEPA，利用细胞身份在基因间冗余编码的特点，让模型从部分观测中预测细胞水平嵌入以学习抗丢包特征。

Result: 在细胞类型聚类零样本转移中，Cell - JEPA的AvgBIO达0.72，优于scGPT的0.53，相对提升36%；在单细胞系扰动预测中，改善了绝对状态重建但未改善效应大小估计。

Conclusion: 表示学习和扰动建模可解决细胞预测的不同方面。

Abstract: Single-cell foundation models learn by reconstructing masked gene expression, implicitly treating technical noise as signal. With dropout rates exceeding 90%, reconstruction objectives encourage models to encode measurement artifacts rather than stable cellular programs. We introduce Cell-JEPA, a joint-embedding predictive architecture that shifts learning from reconstructing sparse counts to predicting in latent space. The key insight is that cell identity is redundantly encoded across genes. We show predicting cell-level embeddings from partial observations forces the model to learn dropout-robust features. On cell-type clustering, Cell-JEPA achieves 0.72 AvgBIO in zero-shot transfer versus 0.53 for scGPT, a 36% relative improvement. On perturbation prediction within a single cell line, Cell-JEPA improves absolute-state reconstruction but not effect-size estimation, suggesting that representation learning and perturbation modeling address complementary aspects of cellular prediction.

</details>


### [158] [Neural Geometry for PDEs: Regularity, Stability, and Convergence Guarantees](https://arxiv.org/abs/2602.02271)
*Samundra Karki,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CE

TL;DR: 本文建立统一理论框架，连接INR训练误差与PDE解精度，定义INR所需最小几何正则性并推导误差估计。


<details>
  <summary>Details</summary>
Motivation: INRs在基于物理的模拟中的适用性有待探索，现有度量无法捕捉数值性能所需的几何正则性。

Method: 建立统一理论框架，定义最小几何正则性，推导先验误差估计。

Result: 分析得出INR训练损失需相对于网格尺寸二次缩放以匹配线性有限元的收敛速度。

Conclusion: 建立的框架可连接INR训练误差与PDE解精度，明确了INR训练损失的缩放要求。

Abstract: Implicit Neural Representations (INRs) have emerged as a powerful tool for geometric representation, yet their suitability for physics-based simulation remains underexplored. While metrics like Hausdorff distance quantify surface reconstruction quality, they fail to capture the geometric regularity required for provable numerical performance. This work establishes a unified theoretical framework connecting INR training errors to Partial Differential Equation (PDE) (specifically, linear elliptic equation) solution accuracy. We define the minimal geometric regularity required for INRs to support well-posed boundary value problems and derive \emph{a priori} error estimates linking the neural network's function approximation error to the finite element discretization error. Our analysis reveals that to match the convergence rate of linear finite elements, the INR training loss must scale quadratically relative to the mesh size.

</details>


### [159] [Modelling Socio-Psychological Drivers of Land Management Intensity](https://arxiv.org/abs/2602.02347)
*Ronja Hotz,Calum Brown,Yongchao Zeng,Thomas Schmitt,Mark Rounsevell*

Main category: cs.CE

TL;DR: 本文开发基于计划行为理论的行为扩展模型，将社会心理因素纳入土地利用模型，结果显示社会心理驱动因素会显著改变土地管理强度等，该框架可重塑土地利用结果并可复用。


<details>
  <summary>Details</summary>
Motivation: 多数土地利用模型强调经济和生物物理驱动因素，而影响土地管理者决策的社会心理因素未得到充分体现，为填补这一空白开展研究。

Method: 以计划行为理论为框架，开发基于主体的土地利用模型的行为扩展，将环境态度、描述性社会规范和行为惯性纳入土地管理者决策，与现有模型耦合并在典型场景中探索。

Result: 社会心理驱动因素可显著改变土地管理强度份额、景观配置和生态系统服务供给，存在非线性反馈和涌现动态，增加社会规范影响会产生空间聚类等。

Conclusion: 该框架能重塑系统层面土地利用结果，提供可复用建模组件，可集成到其他模型并在未来实证参数化。

Abstract: Land management intensity shapes ecosystem service provision, socio-ecological resilience and is central to sustainable transformation. Yet most land use models emphasise economic and biophysical drivers, while socio-psychological factors influencing land managers' decisions remain underrepresented despite increasing evidence that they shape land management choices. To address this gap, we develop a generic behavioural extension for agent-based land use models, guided by the Theory of Planned Behaviour as an overarching conceptual framework. The extension integrates environmental attitudes, descriptive social norms and behavioural inertia into land managers' decisions on land management intensity. To demonstrate applicability, the extension is coupled to an existing land use modelling framework and explored in stylised settings to isolate behavioural mechanisms. Results show that socio-psychological drivers can significantly alter land management intensity shares, landscape configuration, and ecosystem service provision. Nonlinear feedbacks between these drivers, spatial resource heterogeneity, and ecosystem service demand lead to emergent dynamics that are sometimes counter-intuitive and can diverge from the agent-level decision rules. Increasing the influence of social norms generates spatial clustering and higher landscape connectivity, while feedbacks between behavioural factors can lead to path dependence, lock-in effects, and the emergence of multiple stable regimes with sharp transitions. The proposed framework demonstrates how even low levels of behavioural diversity and social interactions can reshape system-level land use outcomes and provides a reusable modelling component for incorporating socio-psychological processes into land use simulations. The approach can be integrated into other agent-based land use models and parameterised empirically in future work.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [160] [Updatable Balanced Index for Stable Streaming Similarity Search over Large-Scale Fresh Vectors](https://arxiv.org/abs/2602.00563)
*Yuhui Lai,Shixun Huang,Sheng Wang*

Main category: cs.DB

TL;DR: 本文提出用于稳定流相似性搜索的可更新平衡索引 UBIS，在流工作负载中比现有索引有更好的搜索精度和更新吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着数据变化频率增加，现有索引在适应新数据和实时更新方面存在资源密集、效率低、性能下降等问题，需要新的索引方案。

Method: 提出 UBIS 索引，通过调度并发更新解决冲突，减少不平衡更新情况以保持良好的索引质量。

Result: 在真实数据集实验中，与现有索引相比，UBIS 搜索精度最高提升 77%，更新吞吐量平均提升 45%。

Conclusion: UBIS 是一种在流工作负载中更高效的可更新索引方案。

Abstract: As artificial intelligence gains more and more popularity, vectors are one of the most widely used data structures for services such as information retrieval and recommendation. Approximate Nearest Neighbor Search (ANNS), which generally relies on indices optimized for fast search to organize large datasets, has played a core role in these popular services. As the frequency of data shift grows, it is crucial for indices to accommodate new data and support real-time updates. Existing researches adopting two different approaches hold the following drawbacks: 1) approaches using additional buffers to temporarily store new data are resource-intensive and inefficient due to the global rebuilding processes; 2) approaches upgrading the internal index structure suffer from performance degradation because of update congestion and imbalanced distribution in streaming workloads. In this paper, we propose UBIS, an Updatable Balanced Index for stable streaming similarity Search, to resolve conflicts by scheduling concurrent updates and maintain good index quality by reducing imbalanced update cases, when the update frequency grows. Experimental results in the real-world datasets demonstrate that UBIS achieves up to 77% higher search accuracy and 45% higher update throughput on average compared to the state-of-the-art indices in streaming workloads.

</details>


### [161] [Meta Engine: A Unified Semantic Query Engine on Heterogeneous LLM-Based Query Systems](https://arxiv.org/abs/2602.01701)
*Ruyu Li,Tinghui Zhang,Haodi Ma,Daisy Zhe Wang,Yifan Wang*

Main category: cs.DB

TL;DR: 随着多模态数据使用增加，语义查询需求增长，但现有基于大语言模型的语义查询系统生态分散。本文提出Meta Engine，整合异构查询系统，评估中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的语义查询系统生态分散，应用面临集成挑战，如API不同、存在专业化与通用性权衡，专业系统处理多模态数据困难，通用系统特定模态性能不佳。

Method: 提出Meta Engine，它是统一语义查询引擎，整合异构、基于大语言模型的查询系统，架构包含自然语言查询解析器、操作符生成器、查询路由器、适配器集和结果聚合器。

Result: Meta Engine在评估中始终优于所有基线，多数情况F1值高3 - 6倍，特定数据集高至24倍。

Conclusion: Meta Engine能有效解决现有语义查询系统生态分散和集成挑战问题。

Abstract: With the increasingly use of multi-modal data, semantic query has become more and more demanded in data management systems, which is an important way to access and analyze multi-modal data. As unstructured data, most information of multi-modal data (text, image, video, etc) hides in the semantics, which cannot be accessed by the traditional database queries like SQL.
  Given the power of Large Language Model (LLM) in understanding semantics and processing natural language, in recent years several LLM-based semantic query systems have been proposed, to support semantic querying over unstructured data. However, this rapid growth has produced a fragmented ecosystem. Applications face significant integration challenges due to (1) disparate APIs of different semantic query systems and (2) a fundamental trade-off between specialization and generality. Many semantic query systems are highly specialized, offering state-of-the-art performance within a single modality but struggling with multi-modal data. Conversely, some "all-in-one" systems handle multiple modalities but often exhibit suboptimal performance compared to their specialized counterparts in specific modalities.
  This paper introduces Meta Engine, a novel "query system on query systems", designed to resolve those aforementioned challenges. Meta Engine is a unified semantic query engine that integrates heterogeneous, specialized LLM-based query systems. Its architecture comprises five key components: (1) a Natural Language (NL) Query Parser, (2) an Operator Generator, (3) a Query Router, (4) a set of Adapters, and (5) a Result Aggregator. In the evaluation, Meta Engine consistently outperforms all baselines, yielding 3-6x higher F1 in most cases and up to 24x on specific datasets.

</details>


### [162] [ChemDCAT-AP: Enabling Semantic Interoperability with a Contextual Extension of DCAT-AP](https://arxiv.org/abs/2602.01822)
*Philip Stroemert,Hendrik Borgelt,David Linke,Mark Doerr,Bhavin Katabathuni,Oliver Koepler,Norbert Kockmann*

Main category: cs.DB

TL;DR: 提出DCAT - AP PLUS促进研究数据跨领域互操作性，并用ChemDCAT - AP展示潜力，采用LinkML确保集成。


<details>
  <summary>Details</summary>
Motivation: 各学科维护自身元数据模式和领域本体，导致语义互操作性复杂，需促进研究数据跨领域互操作性。

Method: 提出DCAT - AP PLUS通用应用轮廓，引入上层可定制层；采用LinkML框架支持架构继承、生成子架构及数据类型协调等。

Result: 展示了DCAT - AP+和ChemDCAT - AP在化学和催化领域数据集成的应用。

Conclusion: DCAT - AP+及相关框架可有效促进跨领域数据集成与互操作性。

Abstract: Cross-domain data integration drives interdisciplinary data reuse and knowledge transfer across domains. However, each discipline maintains its own metadata schemas and domain ontologies, employing distinct conceptual models and application profiles, which complicates semantic interoperability. The W3C Data Catalog Vocabulary (DCAT) offers a widely adopted RDF vocabulary for describing datasets and their distributions, but its core model is intentionally lightweight. Numerous domain-specific application profiles have emerged to enrich DCAT's expressivity, the most well-known DCAT-AP for public data. To facilitate cross-domain interoperability for research data, we propose DCAT-AP PLUS, a DCAT Application Profile (P)roviding additional (L)inks to (U)se-case (S)pecific context (DCAT-AP+). This generic application profile enables a comprehensive representation of the provenance and context of research data generation. DACT-AP+ introduces an upper-level layer that can be specialized by individual domains without sacrificing compatibility. We demonstrate the application of DCAT-AP+ and a specific profile ChemDCAT-AP to showcase the potential of data integration of the neighboring disciplines chemistry and catalysis. We adopt LinkML, a YAML-based modeling framework, to support schema inheritance, generate domain-specific subschemas, and provide mechanisms for data type harmonization, validation, and format conversion, ensuring smooth integration of DCAT-AP+ and ChemDCAT-AP within existing data infrastructures.

</details>


### [163] [Tidehunter: Large-Value Storage With Minimal Data Relocation](https://arxiv.org/abs/2602.01873)
*Andrey Chursin,Lefteris Kokoris-Kogias,Alex Orlov,Alberto Sonnino,Igor Zablotchi*

Main category: cs.DB

TL;DR: 论文提出存储引擎Tidehunter，消除值合并，性能优于RocksDB和BlobDB，并在区块链中验证其实用性。


<details>
  <summary>Details</summary>
Motivation: LSM - trees在随机工作负载下写放大问题严重，处理大值和均匀分布键的工作负载开销大。

Method: 将预写日志（WAL）作为永久存储，还有无锁写入、乐观索引结构和基于纪元的修剪等技术。

Result: 在1TB数据集上，写性能比RocksDB高8.4倍，比BlobDB高2.9倍，点查询和存在检查也有显著提升；集成到区块链Sui中，能保持稳定吞吐量和低延迟。

Conclusion: Tidehunter可投入生产，已在Sui中部署。

Abstract: Log-Structured Merge-Trees (LSM-trees) dominate persistent key-value storage but suffer from high write amplification from 10x to 30x under random workloads due to repeated compaction. This overhead becomes prohibitive for large values with uniformly distributed keys, a workload common in content-addressable storage, deduplication systems, and blockchain validators. We present Tidehunter, a storage engine that eliminates value compaction by treating the Write-Ahead Log (WAL) as permanent storage rather than a temporary recovery buffer. Values are never overwritten; and small, lazily-flushed index tables map keys to WAL positions. Tidehunter introduces (a) lock-free writes that saturate NVMe drives through atomic allocation and parallel copying, (b) an optimistic index structure that exploits uniform key distributions for single-roundtrip lookups, and (c) epoch-based pruning that reclaims space without blocking writes. On a 1 TB dataset with 1 KB values, Tidehunter achieves 830K writes per second, that is 8.4x higher than RocksDB and 2.9x higher than BlobDB, while improving point queries by 1.7x and existence checks by 15.6x. We validate real-world impact by integrating Tidehunter into Sui, a high-throughput blockchain, where it maintains stable throughput and latency under loads that cause RocksDB-backed validators to collapse. Tidehunter is production-ready and is being deployed in production within Sui.

</details>


### [164] [SQLAgent: Learning to Explore Before Generating as a Data Engineer](https://arxiv.org/abs/2602.01952)
*Wenjia Jiang,Yiwei Wang,Boyan Han,Joey Tianyi Zhou,Chi Zhang*

Main category: cs.DB

TL;DR: 提出基于大语言模型的两阶段框架解决SQL推理泛化难题，实验显示效果好。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言到关系数据库接口方法在复杂真实场景泛化性不足，因SQL推理高度依赖特定数据库。

Method: 引入两阶段框架，探索阶段用蒙特卡罗树搜索策略构建数据库特定知识库；部署阶段用双智能体系统结合知识库生成SQL查询。

Result: 在大规模基准测试中，显著提高相对于强基线的准确率。

Conclusion: 该方法有效且具有良好泛化性。

Abstract: Large Language Models have recently shown impressive capabilities in reasoning and code generation, making them promising tools for natural language interfaces to relational databases. However, existing approaches often fail to generalize in complex, real-world settings due to the highly database-specific nature of SQL reasoning, which requires deep familiarity with unique schemas, ambiguous semantics, and intricate join paths. To address this challenge, we introduce a novel two-stage LLM-based framework that decouples knowledge acquisition from query generation. In the Exploration Stage, the system autonomously constructs a database-specific knowledge base by navigating the schema with a Monte Carlo Tree Search-inspired strategy, generating triplets of schema fragments, executable queries, and natural language descriptions as usage examples. In the Deployment Stage, a dual-agent system leverages the collected knowledge as in-context examples to iteratively retrieve relevant information and generate accurate SQL queries in response to user questions. This design enables the agent to proactively familiarize itself with unseen databases and handle complex, multi-step reasoning. Extensive experiments on large-scale benchmarks demonstrate that our approach significantly improves accuracy over strong baselines, highlighting its effectiveness and generalizability.

</details>


### [165] [Hippasus: Effective and Efficient Automatic Feature Augmentation for Machine Learning Tasks on Relational Data](https://arxiv.org/abs/2602.02025)
*Serafeim Papadias,Kostas Patroumpas,Dimitrios Skoutas*

Main category: cs.DB

TL;DR: 现有特征增强方法在复杂模式下存在有效性和效率的权衡问题，介绍了Hippasus框架，结合多种技术实现高效特征增强，实验显示其在准确性和运行时性能上有提升。


<details>
  <summary>Details</summary>
Motivation: 现有特征增强方法在处理复杂模式时面临有效性和效率的权衡问题，难以扩展到多表和多跳路径的复杂架构。

Method: 提出Hippasus框架，结合轻量级统计信号和大语言模型的语义推理来修剪连接路径，采用优化的多路连接算法，整合多路径特征，结合大语言模型语义理解和统计指标进行特征选择。

Result: 在公开数据集上的实验表明，Hippasus比现有最先进的基线方法在特征增强准确性上提高了26.8%，同时具有较高的运行时性能。

Conclusion: Hippasus框架有效解决了特征增强在复杂模式下的有效性和效率问题，在准确性和运行性能上有显著提升。

Abstract: Machine learning models depend critically on feature quality, yet useful features are often scattered across multiple relational tables. Feature augmentation enriches a base table by discovering and integrating features from related tables through join operations. However, scaling this process to complex schemas with many tables and multi-hop paths remains challenging. Feature augmentation must address three core tasks: identify promising join paths that connect the base table to candidate tables, execute these joins to materialize augmented data, and select the most informative features from the results. Existing approaches face a fundamental tradeoff between effectiveness and efficiency: achieving high accuracy requires exploring many candidate paths, but exhaustive exploration is computationally prohibitive. Some methods compromise by considering only immediate neighbors, limiting their effectiveness, while others employ neural models that require expensive training data and introduce scalability limitations. We present Hippasus, a modular framework that achieves both goals through three key contributions. First, we combine lightweight statistical signals with semantic reasoning from Large Language Models to prune unpromising join paths before execution, focusing computational resources on high-quality candidates. Second, we employ optimized multi-way join algorithms and consolidate features from multiple paths, substantially reducing execution time. Third, we integrate LLM-based semantic understanding with statistical measures to select features that are both semantically meaningful and empirically predictive. Our experimental evaluation on publicly available datasets shows that Hippasus substantially improves feature augmentation accuracy by up to 26.8% over state-of-the-art baselines while also offering high runtime performance.

</details>


### [166] [QVCache: A Query-Aware Vector Cache](https://arxiv.org/abs/2602.02057)
*Anıl Eren Göçer,Ioanna Tsakalidou,Hamish Nicholson,Kyoungmin Kim,Anastasia Ailamaki*

Main category: cs.DB

TL;DR: 提出QVCache，首个后端无关的ANN搜索查询级缓存系统，能减少端到端查询延迟并保持召回率。


<details>
  <summary>Details</summary>
Motivation: 扩展近似最近邻（ANN）搜索在低延迟下实现高召回率受内存容量和I/O带宽限制，且向量搜索缺少通用查询级缓存层。

Method: 利用语义查询重复执行相似度感知缓存，用在线学习算法动态学习特定区域距离阈值。

Result: QVCache内存占用为兆字节级，缓存命中延迟亚毫秒级，集成现有ANN系统时端到端查询延迟最多降低40 - 1000倍。

Conclusion: 对于有时间 - 语义局部性的工作负载，QVCache能大幅降低延迟并保持召回率，是可扩展向量搜索中缺失但必要的缓存层。

Abstract: Vector databases have become a cornerstone of modern information retrieval, powering applications in recommendation, search, and retrieval-augmented generation (RAG) pipelines. However, scaling approximate nearest neighbor (ANN) search to high recall under strict latency SLOs remains fundamentally constrained by memory capacity and I/O bandwidth. Disk-based vector search systems suffer severe latency degradation at high accuracy, while fully in-memory solutions incur prohibitive memory costs at billion-scale. Despite the central role of caching in traditional databases, vector search lacks a general query-level caching layer capable of amortizing repeated query work.
  We present QVCache, the first backend-agnostic, query-level caching system for ANN search with bounded memory footprint. QVCache exploits semantic query repetition by performing similarity-aware caching rather than exact-match lookup. It dynamically learns region-specific distance thresholds using an online learning algorithm, enabling recall-preserving cache hits while bounding lookup latency and memory usage independently of dataset size. QVCache operates as a drop-in layer for existing vector databases. It maintains a megabyte-scale memory footprint and achieves sub-millisecond cache-hit latency, reducing end-to-end query latency by up to 40-1000x when integrated with existing ANN systems. For workloads exhibiting temporal-semantic locality, QVCache substantially reduces latency while preserving recall comparable to the underlying ANN backend, establishing it as a missing but essential caching layer for scalable vector search.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [167] [What Artificial Intelligence can do for High-Performance Computing systems?](https://arxiv.org/abs/2602.00014)
*Pierrick Pochelu,Hyacinthe Cartiaux,Julien Schleich*

Main category: cs.DC

TL;DR: 本文评估人工智能提升高性能计算（HPC）系统效率的情况，筛选文献后分组研究，指出调度最活跃等发现及整合机会和发展需求。


<details>
  <summary>Details</summary>
Motivation: HPC中心能耗大，有环境和运营成本，需用人工智能提升其效率。

Method: 手动筛选2019 - 2025年约1800篇出版物，按预定义标准保留74篇“AI for HPC”论文并分组。

Result: 调度是最活跃领域；监督式性能估计是调度和优化基础；图神经网络等强化异常检测；HPC领域专用语言模型在特定任务上表现更好。

Conclusion: 强调了如基于大语言模型的操作系统概念等整合机会，以及MLOps、AI组件标准化和基准测试方法的发展需求。

Abstract: High-performance computing (HPC) centers consume substantial power, incurring environmental and operational costs. This review assesses how artificial intelligence (AI), including machine learning (ML) and optimization, improves the efficiency of operational HPC systems. Approximately 1,800 publications from 2019 to 2025 were manually screened using predefined inclusion/exclusion criteria; 74 "AI for HPC" papers were retained and grouped into six application areas: performance estimation, performance optimization, scheduling, surrogate modeling, fault detection, and language-model-based automation.
  Scheduling is the most active area, spanning research-oriented reinforcement-learning schedulers to production-friendly hybrids that combine ML with heuristics. Supervised performance estimation is foundational for both scheduling and optimization. Graph neural networks and time-series models strengthen anomaly detection by capturing spatio-temporal dependencies in production telemetry. Domain-specialized language models for HPC can outperform general-purpose LLMs on targeted coding and automation tasks. Together, these findings highlight integration opportunities such as LLM-based operating-system concepts and underscore the need for advances in MLOps, standardization of AI components, and benchmarking methodology.

</details>


### [168] [A Fault-Tolerant Version of Safra's Termination Detection Algorithm](https://arxiv.org/abs/2602.00272)
*Wan Fokkink,Georgios Karlos,Andy Tatman*

Main category: cs.DC

TL;DR: 将经典的Safra分布式终止检测算法改进为容错算法，该算法无额外消息开销，能容忍任意数量和同时发生的崩溃。


<details>
  <summary>Details</summary>
Motivation: 使经典的Safra分布式终止检测算法具备容错能力。

Method: 将计数器拆分为每个节点的计数器以丢弃崩溃节点的计数；节点崩溃时本地恢复令牌环并发送备份令牌；节点通过令牌相互通知检测到的崩溃。

Result: 算法无额外消息开销，能容忍任意数量及同时发生的崩溃，以去中心化方式处理崩溃。

Conclusion: 给出了原算法及其容错变体的正确性证明和模型检查分析。

Abstract: Safra's distributed termination detection algorithm employs a logical token ring structure within a distributed network; only passive nodes forward the token, and a counter in the token keeps track of the number of sent minus the number of received messages. We adapt this classic algorithm to make it fault-tolerant. The counter is split into counters per node, to discard counts from crashed nodes. If a node crashes, the token ring is restored locally and a backup token is sent. Nodes inform each other of detected crashes via the token. Our algorithm imposes no additional message overhead, tolerates any number of crashes as well as simultaneous crashes, and copes with crashes in a decentralized fashion. Correctness proofs are provided of both the original Safra's algorithm and its fault-tolerant variant, as well as a model checking analysis.

</details>


### [169] [Standardized Methods and Recommendations for Green Federated Learning](https://arxiv.org/abs/2602.00343)
*Austin Tapp,Holger R. Roth,Ziyue Xu,Abhijeet Parida,Hareem Nisar,Marius George Linguraru*

Main category: cs.DC

TL;DR: 本文提出用于联邦学习二氧化碳排放跟踪的碳核算方法，在两个工作负载上验证，结果支持标准化碳核算方法以实现可复现的绿色联邦学习评估。


<details>
  <summary>Details</summary>
Motivation: 联邦学习环境影响因测量边界不一致和报告异质性难以跨研究比较。

Method: 使用NVIDIA NVFlare和CodeCarbon对显式、分阶段任务进行二氧化碳排放跟踪，估计网络通信排放。

Result: 在CIFAR - 10中系统级放缓和协调效应会显著增加碳足迹；视网膜分割中更换GPU层级产生运行时间差距和能源、二氧化碳排放的非均匀变化。

Conclusion: 需要标准化碳核算方法作为可复现的绿色联邦学习评估的前提。

Abstract: Federated learning (FL) enables collaborative model training over privacy-sensitive, distributed data, but its environmental impact is difficult to compare across studies due to inconsistent measurement boundaries and heterogeneous reporting. We present a practical carbon-accounting methodology for FL CO2e tracking using NVIDIA NVFlare and CodeCarbon for explicit, phase-aware tasks (initialization, per-round training, evaluation, and idle/coordination). To capture non-compute effects, we additionally estimate communication emissions from transmitted model-update sizes under a network-configurable energy model. We validate the proposed approach on two representative workloads: CIFAR-10 image classification and retinal optic disk segmentation. In CIFAR-10, controlled client-efficiency scenarios show that system-level slowdowns and coordination effects can contribute meaningfully to carbon footprint under an otherwise fixed FL protocol, increasing total CO2e by 8.34x (medium) and 21.73x (low) relative to the high-efficiency baseline. In retinal segmentation, swapping GPU tiers (H100 vs.\ V100) yields a consistent 1.7x runtime gap (290 vs. 503 minutes) while producing non-uniform changes in total energy and CO2e across sites, underscoring the need for per-site and per-round reporting. Overall, our results support a standardized carbon accounting method that acts as a prerequisite for reproducible 'green' FL evaluation. Our code is available at https://github.com/Pediatric-Accelerated-Intelligence-Lab/carbon_footprint.

</details>


### [170] [Training LLMs with Fault Tolerant HSDP on 100,000 GPUs](https://arxiv.org/abs/2602.00277)
*Omkar Salpekar,Rohan Varma,Kenny Yu,Vladimir Ivanov,Yang Wang,Ahmed Sharif,Min Si,Shawn Xu,Feng Tian,Shengbao Zheng,Tristan Rice,Ankush Garg,Shangfu Peng,Shreyas Siravara,Wenyin Fu,Rodrigo de Castro,Adithya Gangidi,Andrey Obraztsov,Sharan Narang,Sergey Edunov,Maxim Naumov,Chunqiang Tang,Mathew Oldham*

Main category: cs.DC

TL;DR: 传统同步训练在大规模GPU上效率低，提出FT - HSDP训练范式，结合多种技术，减少故障恢复停顿时间，提高有效训练时间且不影响模型精度。


<details>
  <summary>Details</summary>
Motivation: 传统同步训练在大规模GPU（O(100K)）训练时因频繁故障和长恢复时间导致效率低下。

Method: 提出Fault Tolerant Hybrid - Shared Data Parallelism (FT - HSDP) 范式，以数据并行副本为容错单元；引入Fault Tolerant All Reduce (FTAR) 协议进行梯度交换；引入非阻塞追赶协议。

Result: 与全同步训练相比，将故障恢复停顿时间从10分钟降至3分钟，有效训练时间从44%提升至80%，异步恢复不影响模型精度。

Conclusion: FT - HSDP可以有效解决大规模GPU训练中同步训练效率低的问题。

Abstract: Large-scale training systems typically use synchronous training, requiring all GPUs to be healthy simultaneously. In our experience training on O(100K) GPUs, synchronous training results in a low efficiency due to frequent failures and long recovery time.
  To address this problem, we propose a novel training paradigm, Fault Tolerant Hybrid-Shared Data Parallelism (FT-HSDP). FT-HSDP uses data parallel replicas as units of fault tolerance. When failures occur, only a single data-parallel replica containing the failed GPU or server is taken offline and restarted, while the other replicas continue training. To realize this idea at scale, FT-HSDP incorporates several techniques: 1) We introduce a Fault Tolerant All Reduce (FTAR) protocol for gradient exchange across data parallel replicas. FTAR relies on the CPU to drive the complex control logic for tasks like adding or removing participants dynamically, and relies on GPU to perform data transfer for best performance. 2) We introduce a non-blocking catch-up protocol, allowing a recovering replica to join training with minimal stall.
  Compared with fully synchronous training at O(100K) GPUs, FT-HSDP can reduce the stall time due to failure recovery from 10 minutes to 3 minutes, increasing effective training time from 44\% to 80\%. We further demonstrate that FT-HSDP's asynchronous recovery does not bring any meaning degradation to the accuracy of the result model.

</details>


### [171] [PROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching](https://arxiv.org/abs/2602.00509)
*Qianchao Zhu,Xucheng Ye,Yuliang Liu,Haodong Ouyang,Chengru Song*

Main category: cs.DC

TL;DR: 提出PROBE推理系统，可实时平衡计算和通信，实验显示其在减少预填充延迟和提高解码吞吐量上有优势。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家模型在延迟关键推理中，专家并行会放大执行延迟，现实服务中会出现计算倾斜和网络拥塞的‘双重惩罚’问题。

Method: 提出PROBE系统，包含连续前瞻流水线，有门初始化前瞻预测器、硬件感知平衡规划求解器和锁相协同调度策略。

Result: PROBE相比现有基线，预填充延迟最多降低1.32倍，解码吞吐量最多提高1.26倍，在极端工作负载波动下效果显著。

Conclusion: PROBE系统能有效实时平衡计算和通信，在处理延迟关键推理和极端工作负载时有良好表现。

Abstract: Mixture-of-Experts models have become a dominant architecture for scaling Large Language Models by activating only a sparse subset of experts per token. However, latency-critical MoE inference faces a fundamental tension: while expert parallelism improves memory efficiency, it also amplifies execution stragglers. In real-world serving, continuous batching and diverse concurrent requests induce rapid semantic shifts, causing expert hotspots to migrate abruptly across GPUs and triggering the 'double penalty' of coupled computational skew and network congestion.
  We propose PROBE, an inference system that co-balances computation and communication in real time. PROBE introduces Continuous Lookahead Pipelining, which proactively predicts, plans, and prefetches for upcoming layers while keeping all control overheads off the critical path. PROBE consists of: (1) a Gate-Initialized Lookahead Predictor that distills the target router to forecast next-layer expert activation with high fidelity; (2) a Hardware-Aware Balance Planning solver that jointly optimizes dynamic expert replication and token assignment under strict hiding-window constraints; and (3) a Phase-Locked Co-Scheduling policy that uses split-phase transmission to hide bandwidth-intensive expert transfers behind computation without contending with All-to-All collectives. Experiments show that PROBE reduces prefill latency by up to 1.32X and improves decoding throughput by up to 1.26X over state-of-the-art baselines, especially under extreme workload volatility.

</details>


### [172] [HyperOffload: Graph-Driven Hierarchical Memory Management for Large Language Models on SuperNode Architectures](https://arxiv.org/abs/2602.00748)
*Fangxin Liu,Qinghua Zhang,Hanjing Shen,Zhibo Liang,Li Jiang,Haibing Guan,Chong Bao,Xuefeng Jin*

Main category: cs.DC

TL;DR: 本文提出SuperNode Memory Management Framework (HyperOffload)应对大语言模型内存需求问题，减少设备内存峰值使用，证明将内存增强硬件集成到编译器优化框架对扩展下一代AI工作负载很重要。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对内存需求超出单个设备HBM容量，现有软件栈无法有效利用新兴超级节点架构的硬件，当前运行时卸载和交换技术存在问题。

Method: 提出HyperOffload，采用编译器辅助方法，利用图驱动内存管理，在编译器IR中用缓存运算符表示数据移动，开发全局执行顺序细化算法，在MindSpore中实现。

Result: 在代表性LLM工作负载上评估，推理时减少设备内存峰值使用达26%，同时保持端到端性能。

Conclusion: 将内存增强硬件集成到编译器优化框架对扩展下一代AI工作负载至关重要。

Abstract: The rapid evolution of Large Language Models (LLMs) towards long-context reasoning and sparse architectures has pushed memory requirements far beyond the capacity of individual device HBM. While emerging supernode architectures offer terabyte-scale shared memory pools via high-bandwidth interconnects, existing software stacks fail to exploit this hardware effectively. Current runtime-based offloading and swapping techniques operate with a local view, leading to reactive scheduling and exposed communication latency that stall the computation pipeline.
  In this paper, we propose the SuperNode Memory Management Framework (\textbf{HyperOffload}). It employs a compiler-assisted approach that leverages graph-driven memory management to treat remote memory access as explicit operations in the computation graph, specifically designed for hierarchical SuperNode architectures. Unlike reactive runtime systems, SuperNode represents data movement using cache operators within the compiler's Intermediate Representation (IR). This design enables a global, compile-time analysis of tensor lifetimes and execution dependencies. Leveraging this visibility, we develop a global execution-order refinement algorithm that statically schedules data transfers to hide remote memory latency behind compute-intensive regions. We implement SuperNode within the production deep learning framework MindSpore, adding a remote memory backend and specialized compiler passes. Evaluation on representative LLM workloads shows that SuperNode reduces peak device memory usage by up to 26\% for inference while maintaining end-to-end performance. Our work demonstrates that integrating memory-augmented hardware into the compiler's optimization framework is essential for scaling next-generation AI workloads.

</details>


### [173] [System-Level Performance Modeling of Photonic In-Memory Computing](https://arxiv.org/abs/2602.00892)
*Jebacyril Arockiaraj,Sasindu Wijeratne,Sugeet Sunder,Md Abdullah-Al Kaiser,Akhilesh Jaiswal,Ajey P. Jacob,Viktor Prasanna*

Main category: cs.DC

TL;DR: 本文开发光子内存计算系统级性能模型，评估关键延迟源对实际工作负载的影响，展示特定光子SRAM阵列性能和能效。


<details>
  <summary>Details</summary>
Motivation: 光子内存计算是传统数字计算的高速低能耗替代方案，需评估其在实际高性能计算工作负载的性能。

Method: 开发综合系统级性能模型，考虑关键延迟源；对一系列工作负载进行算法到硬件的映射。

Result: 一个紧凑的1x256位单波长光子SRAM阵列在三个工作负载上分别维持1.5 TOPS、0.9 TOPS和1.3 TOPS，平均能效2.5 TOPS/W。

Conclusion: 所开发模型可有效评估光子内存计算在实际工作负载中的性能和能效。

Abstract: Photonic in-memory computing is a high-speed, low-energy alternative to traditional transistor-based digital computing that utilizes high photonic operating frequencies and bandwidths. In this work, we develop a comprehensive system-level performance model for photonic in-memory computing, capturing the effects of key latency sources such as external memory access and opto-electronic conversion. We perform algorithm-to-hardware mapping across a range of workloads, including the Sod shock tube problem, Matricized Tensor Times Khatri-Rao Product (MTTKRP), and the Vlasov-Maxwell equation, to evaluate how the latencies impact real-world high-performance computing workloads. Our performance model shows that, while accounting for system overheads, a compact 1x256 bit single-wavelength photonic SRAM array, fabricated using the standard silicon photonics process by GlobalFoundries, sustains up to 1.5 TOPS, 0.9 TOPS, and 1.3 TOPS on the Sod shock tube problem, MTTKRP, and the Vlasov-Maxwell equation with an average energy efficiency of 2.5 TOPS/W.

</details>


### [174] [Low-latency Federated LLM Fine-tuning Over Wireless Networks](https://arxiv.org/abs/2602.01024)
*Zhiwen Pang,Kang Wei,Long Shi,Zhe Wang,Jun Li,Feng Shu*

Main category: cs.DC

TL;DR: 提出JCPBA框架提升联邦大语言模型在无线网络上的微调效率，实验证明其优势。


<details>
  <summary>Details</summary>
Motivation: 现有联邦大语言模型微调框架在资源受限且计算能力异构、无线信道随机的客户端面临挑战。

Method: 提出联合客户端特定剪枝和带宽分配（JCPBA）框架，通过联合优化剪枝率和带宽分配，用块坐标下降法解决微调延迟最小化问题。

Result: 在Yahoo Answers和GSM8K数据集上的大量实验表明，该框架比现有基线显著减少了实际微调时间，且能以更低的计算和通信开销达到相同或更低的测试损失。

Conclusion: JCPBA框架可有效提升联邦大语言模型在无线网络上的微调效率。

Abstract: Recently, federated large language models (LLMs) have drawn significant attention thanks to coupled capabilities of LLMs and federated learning (FL) that address privacy concerns in collaborative fine-tuning. However, due to large-scale parameters of LLMs, existing federated LLM fine-tuning frameworks incur significant challenges in resource-constrained clients characterized by heterogeneous computing capabilities and random wireless channels. To address this issue, we propose a joint client-specific pruning and bandwidth allocation (JCPBA) framework for federated LLMs to improve the fine-tuning efficiency over the wireless networks. Specifically, we formulate a fine-tuning latency minimization problem by jointly optimizing pruning rates and bandwidth allocations. Furthermore, we solve this optimization problem using a block coordinate descent method. Extensive experiments on the datasets of Yahoo Answers and GSM8K demonstrate that the proposed framework significantly reduces wall-clock fine-tuning time compared with state-of-the-art baselines and gains equal or lower test loss at the cost of lower computation and communication overhead.

</details>


### [175] [BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation](https://arxiv.org/abs/2602.01404)
*Zhouzi Li,Cindy Zhu,Arpan Mukhopadhyay,Mor Harchol-Balter,Benjamin Berg*

Main category: cs.DC

TL;DR: 过去十年GPU训练ML模型需求增加，组织多从云提供商租GPU，存在成本 - 性能权衡，为此开发BOA Constrictor调度器，实验显示其能降低平均作业完成时间。


<details>
  <summary>Details</summary>
Motivation: 大多数组织自建和维护GPU集群成本高，从云提供商租GPU时需平衡成本 - 性能权衡。

Method: 开发使用Budget - Optimal Allocation (BOA) 策略的BOA Constrictor调度器，将问题明确表述为预算约束调度问题并推导BOA策略。

Result: 在小规模实现实验中，相比现有启发式调度器，BOA Constrictor可使平均作业完成时间降低1.6倍；在大规模模拟中降低2倍。

Conclusion: BOA Constrictor调度器能在固定预算约束下，有效降低平均作业完成时间，提高云部署GPU集群性能。

Abstract: The past decade has seen a dramatic increase in demand for GPUs to train Machine Learning (ML) models. Because it is prohibitively expensive for most organizations to build and maintain a large GPU cluster, organizations instead choose to rent GPUs from cloud providers. The customer is responsible for devising a policy for (i) deciding how many GPUs to rent at every moment in time to process a stream of ML training jobs and (ii) allocating the rented GPUs among the currently active jobs in the system. Because ML training jobs can be parallelized across different numbers of GPUs, the customer generally has many options for how many GPUs to use for each job. Allocating more GPUs to a single training job will cause the job to complete more quickly. However, the customer pays for each GPU-hour they use, and a training job receives a diminishing marginal benefit from running on additional GPUs. Hence, allocating too many GPUs to a single training job can dramatically increase the overall cost that the customer pays to the cloud provider. This gives rise to a cost-performance tradeoff that customers must balance when running training jobs in the cloud.
  To balance the cost-performance tradeoff, we develop BOA Constrictor, a new scheduler for ML training jobs which uses a Budget-Optimal Allocation (BOA) policy to squeeze the highest level of performance out of a cloud-deployed GPU cluster given a fixed budget constraint. We explicitly formulate the problem as a budget-constrained scheduling problem and derive the BOA policy which minimizes the average job completion time (JCT) of a stream of arriving jobs subject to the user's budget. For a given budget level, we demonstrate that BOA Constrictor can reduce average JCT by 1.6 times in small-scale implementation experiments and by 2 times in detailed, large-scale simulations compared to state-of-the-art heuristic based schedulers.

</details>


### [176] [Mean field optimal Core Allocation across Malleable jobs](https://arxiv.org/abs/2602.01411)
*Zhouzi Li,Mor Harchol-Balter,Benjamin Berg*

Main category: cs.DC

TL;DR: 本文针对海量可延展作业的核心分配问题（CAM）在高度通用设置下求解，推导两个平均场最优策略FW - CAM和WHAM。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心和云计算集群中可延展作业普遍存在，已有理论提出如何分配固定核心以最小化作业平均响应时间的CAM问题，本文旨在求解该问题。

Method: 在平均场渐近状态下分析CAM问题。

Result: 推导出两个平均场最优策略FW - CAM和WHAM，FW - CAM表明作业大小在平均场状态下对寻找最优策略无关，WHAM渐近最优且在非渐近状态下也是好的启发式策略，以往文献中的策略在作业遵循不同加速函数时非平均场最优。

Conclusion: 成功解决高度通用设置下的CAM问题，所提两个策略具有独特优势。

Abstract: Modern data centers and cloud computing clusters are increasingly running workloads composed of malleable jobs. A malleable job can be parallelized across any number of cores, yet the job typically exhibits diminishing marginal returns for each additional core on which it runs. This can be seen in the concavity of a job's speedup function, which describes the job's processing speed as a function of the number of cores on which it runs.
  Given the prevalence of malleable jobs, several theoretical works have posed the problem of how to allocate a fixed number of cores across a stream of arriving malleable jobs so as to minimize the mean response time across jobs. We refer to this as the Core Allocation to Malleable jobs (CAM) problem. We solve the CAM problem under a highly general setting, allowing for multiple job classes, each with an arbitrary concave speedup function and holding costs (weight). Furthermore, we allow for generally distributed inter-arrival times and job sizes.
  We analyze the CAM problem in the mean field asymptotic regime and derive two distinct mean field optimal policies, FW-CAM and WHAM. FW-CAM is interesting because it demonstrates a new intuition: in the mean field regime, job sizes are not relevant in finding an optimal policy. WHAM (Whittle Allocation for Malleable jobs) is interesting because it is asymptotically optimal and also serves as a good heuristic even outside of the asymptotic regime. Notably, none of the policies previously proposed in the literature are mean field optimal when jobs may follow different speedup functions.

</details>


### [177] [Developing a Portable Solution for Post-Event Analysis Pipelines](https://arxiv.org/abs/2602.01798)
*Leonardo Pelonero,Fabio Vitello,Eva Sciacca,Mauro Imbrosciano,Salvatore Scavo,Ugo Becciani*

Main category: cs.DC

TL;DR: 提出用于评估极端自然灾害事件及其对风险资产影响的科学网关框架。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了洪水、干旱等自然灾害，加上地震风险，凸显了在意大利等敏感地区加强风险评估和缓解策略的必要性。

Method: 开发集成摄影测量技术、数据可视化和人工智能技术的可移植且全自动的事件后分析管道的科学网关框架，并应用于航空图像。

Result: 未提及

Conclusion: 未提及

Abstract: In recent years, the monitoring and study of natural hazards have gained significant attention, particularly due to climate change, which exacerbates incidents like floods, droughts, storm surges, and landslides. Together with the constant risk of earthquakes, these climate-induced events highlight the critical necessity for enhanced risk assessment and mitigation strategies in susceptible areas such as Italy.
  In this work, we present a Science Gateway framework for the development of portable and fully automated post-event analysis pipelines integrating Photogrammetry techniques, Data Visualization and Artificial Intelligence technologies, applied on aerial images, to assess extreme natural events and evaluate their impact on risk-exposed assets.

</details>


### [178] [Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training](https://arxiv.org/abs/2602.01872)
*Chongyang Xu,Christoph Siebenbrunner,Laurent Bindschaedler*

Main category: cs.DC

TL;DR: Grappa 是一种分布式 GNN 训练框架，采用仅梯度通信方式，结合重分区和修正梯度聚合，训练效率高且精度好。


<details>
  <summary>Details</summary>
Motivation: 跨分区边主导分布式 GNN 训练成本，远程特征和激活的获取使网络负担过重，需改进训练方式。

Method: 在每次迭代中分区独立训练，仅交换梯度进行全局更新；周期性重分区 ；应用轻量级覆盖率修正的梯度聚合，提出节点级和批处理级修正方法及收缩版本。

Result: 在真实和合成图上，Grappa 平均训练速度比现有系统快 4 倍（最高 13 倍），对更深模型精度更高，能在通用硬件上处理万亿边规模图。

Conclusion: Grappa 模型无关，支持全图和小批量训练，无需高带宽互连或缓存。

Abstract: Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We prove the corrected estimator is asymptotically unbiased under standard support and boundedness assumptions, and we derive a batch-level variant for compatibility with common deep-learning packages that minimizes mean-squared deviation from the ideal node-level correction. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4 times faster on average (up to 13 times) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching.

</details>


### [179] [vLLM-Omni: Fully Disaggregated Serving for Any-to-Any Multimodal Models](https://arxiv.org/abs/2602.02204)
*Peiqi Yin,Jiangyun Zhu,Han Gao,Chenguang Zheng,Yongxiang Huang,Taichang Zhou,Ruirui Yang,Weizhi Liu,Weiqing Chen,Canlin Guo,Didan Deng,Zifeng Mo,Cong Wang,James Cheng,Roger Wang,Hongsheng Liu*

Main category: cs.DC

TL;DR: 现有多模态模型服务系统难支持任意到任意架构，提出vLLM - Omni系统，可减少作业完成时间，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有服务系统主要针对单一范式，缺乏对任意到任意多模态模型管道的支持，开发者手动处理交互导致性能下降。

Method: 提出vLLM - Omni系统，有新颖的阶段抽象将复杂架构分解为图表示的阶段，还有分解式阶段执行后端优化资源利用和吞吐量，各阶段独立服务。

Result: vLLM - Omni相比基线方法最多可减少91.4%的作业完成时间。

Conclusion: vLLM - Omni能有效解决任意到任意多模态模型的服务问题，提升性能。

Abstract: Any-to-any multimodal models that jointly handle text, images, video, and audio represent a significant advance in multimodal AI. However, their complex architectures (typically combining multiple autoregressive LLMs, diffusion transformers, and other specialized components) pose substantial challenges for efficient model serving. Existing serving systems are mainly tailored to a single paradigm, such as autoregressive LLMs for text generation or diffusion transformers for visual generation. They lack support for any-to-any pipelines that involve multiple interconnected model components. As a result, developers must manually handle cross-stage interactions, leading to huge performance degradation. We present vLLM-Omni, a fully disaggregated serving system for any-to-any models. vLLM-Omni features a novel stage abstraction that enables users to decompose complex any-to-any architectures into interconnected stages represented as a graph, and a disaggregated stage execution backend that optimizes resource utilization and throughput across stages. Each stage is independently served by an LLM or diffusion engine with per-stage request batching, flexible GPU allocation, and unified inter-stage connectors for data routing. Experimental results demonstrate that vLLM-Omni reduces job completion time (JCT) by up to 91.4% compared to baseline methods. The code is public available at https://github.com/vllm-project/vllm-omni.

</details>


### [180] [Building a Correct-by-Design Lakehouse. Data Contracts, Versioning, and Transactional Pipelines for Humans and Agents](https://arxiv.org/abs/2602.02335)
*Weiming Sheng,Jinlang Wang,Manuel Barros,Aldrin Montana,Jacopo Tagliabue,Luca Bigon*

Main category: cs.DC

TL;DR: 本文介绍了Bauplan，一种代码优先的湖仓架构，利用熟悉抽象使非法状态难以出现，并汇报早期结果与讨论未来工作。


<details>
  <summary>Details</summary>
Motivation: 现有湖仓在不可信参与者并发操作生产数据时不安全，存在上下游不匹配和多表管道泄漏部分影响的问题。

Method: 设计Bauplan，从三个维度展开：使用类型化表契约使管道边界可检查、采用类似Git的数据版本控制以实现审查和可重复性、进行事务性运行以保证管道级原子性。

Result: 得到了轻量级形式化事务模型的早期结果。

Conclusion: 未明确给出最终结论，提及基于反例的未来工作。

Abstract: Lakehouses are the default cloud platform for analytics and AI, but they become unsafe when untrusted actors concurrently operate on production data: upstream-downstream mismatches surface only at runtime, and multi-table pipelines can leak partial effects. Inspired by software engineering, we design Bauplan, a code-first lakehouse that aims to make (most) illegal states unrepresentable using familiar abstractions. Bauplan acts along three axes: typed table contracts to make pipeline boundaries checkable, Git-like data versioning for review and reproducibility, and transactional runs that guarantee pipeline-level atomicity. We report early results from a lightweight formal transaction model and discuss future work motivated by counterexamples.

</details>


### [181] [Enabling AI Deep Potentials for Ab Initio-quality Molecular Dynamics Simulations in GROMACS](https://arxiv.org/abs/2602.02234)
*Andong Hu,Luca Pennati,Stefano Markidis,Ivy Peng*

Main category: cs.DC

TL;DR: 本文将AI深度势集成到GROMACS中，评估DPA2和DPA3架构，发现DPA2吞吐量更高，确定了优化重点。


<details>
  <summary>Details</summary>
Motivation: 将AI深度势引入生产级分子动力学代码GROMACS，以较低计算成本获得从头算质量结果。

Method: 将GROMACS与DeePMD - kit集成，耦合GROMACS神经网络势与DeePMD - kit的C++/CUDA后端，在NVIDIA GPU上用四个基准评估DPA2和DPA3架构。

Result: DPA2在A100和GH200 GPU上的吞吐量分别比DPA3高4.23倍和3.18倍，还对二者在吞吐量、内存使用和GPU内核级执行进行了对比研究。

Conclusion: 确定内核启动开销和域分解推理是生产MD模拟中AI深度势的主要优化重点。

Abstract: State-of-the-art AI deep potentials provide ab initio-quality results, but at a fraction of the computational cost of first-principles quantum mechanical calculations, such as density functional theory. In this work, we bring AI deep potentials into GROMACS, a production-level Molecular Dynamics (MD) code, by integrating with DeePMD-kit that provides domain-specific deep learning (DL) models of interatomic potential energy and force fields. In particular, we enable AI deep potentials inference across multiple DP model families and DL backends by coupling GROMACS Neural Network Potentials with the C++/CUDA backend in DeePMD-kit. We evaluate two recent large-atom-model architectures, DPA2 that is based on the attention mechanism and DPA3 that is based on GNN, in GROMACS using four ab initio-quality protein-in-water benchmarks (1YRF, 1UBQ, 3LZM, 2PTC) on NVIDIA A100 and GH200 GPUs. Our results show that DPA2 delivers up to 4.23x and 3.18x higher throughput than DPA3 on A100 and GH200 GPUs, respectively. We also provide a characterization study to further contrast DPA2 and DPA3 in throughput, memory usage, and kernel-level execution on GPUs. Our findings identify kernel-launch overhead and domain-decomposed inference as the main optimization priorities for AI deep potentials in production MD simulations.

</details>


### [182] [LCLs Beyond Bounded Degrees](https://arxiv.org/abs/2602.02340)
*Gustav Schmid*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The study of Locally Checkable Labelings (LCLs) has led to a remarkably precise characterization of the distributed time complexities that can occur on bounded-degree trees. A central feature of this complexity landscape is the existence of strong gap results, which rule out large ranges of intermediate complexities. While it was initially hoped that these gaps might extend to more general graph classes, this has turned out not to be the case. In this work, we investigate a different direction: we remain in the class of trees, but allow arbitrarily large degrees.
  We focus on the polynomial regime ($Θ(n^{1/k} \mid k \in \mathbb{N})$) and show that whether polynomial gap results persist in the unbounded-degree setting crucially depends on how LCLs are generalized beyond bounded degrees. We first demonstrate that if one allows LCLs to be defined using infinitely many local configurations, then the polynomial gaps disappear entirely: for every real exponent $0 < r \leq 1$, there exists a locally checkable problem on trees with deterministic LOCAL complexity $Θ(n^r)$.
  Rather than stopping at this negative result, we identify a natural class of problems for which polynomial gap results can still be recovered. We introduce Locally Finite Labelings (LFLs), which formalize the intuition that ''every node must fall into one of finitely many local cases'', even in the presence of unbounded degrees.
  Our main result shows that this restriction is sufficient to restore the polynomial gaps: for any LFL $Π$ on trees with unbounded degrees, the deterministic LOCAL complexity of $Π$ is either
  - $Θ(n^{1/k})$ for some integer $k \geq 1$, or
  - $O(\log n)$.
  Moreover, which case applies, and the corresponding value of $k$, can be determined solely from the description of $Π$.

</details>


### [183] [Hierarchical Federated Learning with SignSGD: A Highly Communication-Efficient Approach](https://arxiv.org/abs/2602.02355)
*Amirreza Kazemi,Seyed Mohammad Azimi-Abarghouyi,Gabor Fodor,Carlo Fischione*

Main category: cs.DC

TL;DR: 提出高效符号化分层联邦学习框架HierSignSGD，分析收敛性，实验表明其通信成本低且性能好。


<details>
  <summary>Details</summary>
Motivation: 现有理论和算法无法自然扩展到分层联邦学习场景，边缘层和云层聚合交互及其对端到端性能的影响未知。

Method: 提出符号化分层联邦学习框架，设备发送符号随机梯度，边缘服务器多数投票组合，云定期平均边缘模型，使用下行量化广播全局模型。

Result: HierSignSGD在极端压缩下，精度与全精度随机梯度下降相当或更好，降低通信成本，且在积极下行稀疏化下保持鲁棒性。

Conclusion: 所提HierSignSGD框架有效，能在分层联邦学习中平衡通信成本和性能。

Abstract: Hierarchical federated learning (HFL) has emerged as a key architecture for large-scale wireless and Internet of Things systems, where devices communicate with nearby edge servers before reaching the cloud. In these environments, uplink bandwidth and latency impose strict communication limits, thereby making aggressive gradient compression essential. One-bit methods such as sign-based stochastic gradient descent (SignSGD) offer an attractive solution in flat federated settings, but existing theory and algorithms do not naturally extend to hierarchical settings. In particular, the interaction between majority-vote aggregation at the edge layer and model aggregation at the cloud layer, and its impact on end-to-end performance, remains unknown. To bridge this gap, we propose a highly communication-efficient sign-based HFL framework and develop its corresponding formulation for nonconvex learning, where devices send only signed stochastic gradients, edge servers combine them through majority-vote, and the cloud periodically averages the obtained edge models, while utilizing downlink quantization to broadcast the global model. We introduce the resulting scalable HFL algorithm, HierSignSGD, and provide the convergence analysis for SignSGD in a hierarchical setting. Our core technical contribution is a characterization of how biased sign compression, two-level aggregation intervals, and inter-cluster heterogeneity collectively affect convergence. Numerical experiments under homogeneous and heterogeneous data splits show that HierSignSGD, despite employing extreme compression, achieves accuracy comparable to or better than full-precision stochastic gradient descent while reducing communication cost in the process, and remains robust under aggressive downlink sparsification.

</details>


### [184] [sVIRGO: A Scalable Virtual Tree Hierarchical Framework for Distributed Systems](https://arxiv.org/abs/2602.02438)
*Lican Huang*

Main category: cs.DC

TL;DR: 提出用于大规模分布式系统的可扩展虚拟树分层框架sVIRGO，介绍其构建方式、协调机制、通信策略和命令执行特点。


<details>
  <summary>Details</summary>
Motivation: 为大规模分布式系统设计一个高效、安全且稳健的框架，解决在复杂环境下的协调和通信问题。

Method: 直接在物理节点上构建虚拟分层树，让节点承担多角色，区域内分层配置，通过动态映射角色实现跨区域协调；支持多消息跳策略；采用层范围命令执行机制。

Result: 实现了接近零的恢复延迟、有界的通信开销、指数级降低的故障概率，能在复杂条件下保持安全、活性和鲁棒性。

Conclusion: sVIRGO作为一个可扩展的虚拟树分层框架，能有效应用于大规模分布式系统，在通信、协调和决策方面表现良好。

Abstract: We propose sVIRGO, a scalable virtual tree hierarchical framework for large-scale distributed systems. sVIRGO constructs virtual hierarchical trees directly on physical nodes, allowing each node to assume multiple hierarchical roles without overlay networks. The hierarchy preserves locality and is organized into configurable layers within regions. Coordination across thousands of regions is achieved via virtual upper-layer roles dynamically mapped onto nodes up to the top layer.
  Each region maintains multiple active coordinators that monitor local health and perform dynamic re-selection if failures occur. Temporary drops below the minimum threshold do not compromise coordination, ensuring near-zero recovery latency, bounded communication overhead, and exponentially reduced failure probability while maintaining safety, liveness, and robustness under mobile, interference-prone, or adversarial conditions.
  Communication is decoupled from the hierarchy and may use multi-frequency wireless links. Two message hop strategies are supported: (i) with long-distance infrastructure-assisted channels, coordinators exploit the virtual tree to minimize hops; (ii) without such channels, messages propagate via adjacent regions.
  sVIRGO also supports Layer-Scoped Command Execution. Commands and coordination actions are executed within the scope of each hierarchical layer, enabling efficient local and regional decision-making while limiting unnecessary global propagation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [185] [End Cover for Initial Value Problem: Complete Validated Algorithms with Complexity Analysis](https://arxiv.org/abs/2602.00162)
*Bingwei Zhang,Chee Yap*

Main category: cs.DS

TL;DR: 提出解常微分方程的End Cover问题的完整验证算法，分析复杂度并给出新计算技术，用实验验证。


<details>
  <summary>Details</summary>
Motivation: 为了解决常微分方程的End Cover问题。

Method: 提出完整验证算法，分析复杂度，用覆盖End_f(B_0,h)边界的技术计算端覆盖C。

Result: 实验结果表明该方法可行。

Conclusion: 所提出的算法和计算技术具有实用性。

Abstract: We consider the first-order autonomous ordinary differential equation \[ \mathbf{x}' = \mathbf{f}(\mathbf{x}), \] where $\mathbf{f} : \mathbb{R}^n \to \mathbb{R}^n$ is locally Lipschitz. For a box $B_0 \subseteq \mathbb{R}^n$ and $h > 0$, we denote by $\mathrm{IVP}_{\mathbf{f}}(B_0,h)$ the set of solutions $\mathbf{x} : [0,h] \to \mathbb{R}^n$ satisfying \[ \mathbf{x}'(t) = \mathbf{f}(\mathbf{x}(t)), \qquad \mathbf{x}(0) \in B_0 . \]
  We present a complete validated algorithm for the following \emph{End Cover Problem}: given $(\mathbf{f}, B_0, \varepsilon, h)$, compute a finite set $\mathcal{C}$ of boxes such that \[ \mathrm{End}_{\mathbf{f}}(B_0,h) \;\subseteq\; \bigcup_{B \in \mathcal{C}} B \;\subseteq\; \mathrm{End}_{\mathbf{f}}(B_0,h) \oplus [-\varepsilon,\varepsilon]^n , \] where \[ \mathrm{End}_{\mathbf{f}}(B_0,h) = \left\{ \mathbf{x}(h) : \mathbf{x} \in \mathrm{IVP}_{\mathbf{f}}(B_0,h) \right\}. \]
  Moreover, we provide a complexity analysis of our algorithm and introduce a novel technique for computing the end cover $\mathcal{C}$ based on covering the boundary of $\mathrm{End}_{\mathbf{f}}(B_0,h)$. Finally, we present experimental results demonstrating the practicality of our approach.

</details>


### [186] [Hardness and Tractability of T_{h+1}-Free Edge Deletion](https://arxiv.org/abs/2602.00644)
*Ajinkya Gaikwad,Soumen Maity,Leeja R*

Main category: cs.DS

TL;DR: 研究T(h+1)-Free Edge Deletion问题的参数化复杂度，给出多个参数化下的难易结果及可处理性条件。


<details>
  <summary>Details</summary>
Motivation: 深入了解T(h+1)-Free Edge Deletion问题在不同参数化下的复杂度情况。

Method: 证明参数化下的W[1]-困难性，通过整数线性规划公式等方法确定可处理性。

Result: 证明多个参数化下W[1]-困难，找到部分参数化下问题可处理，给出双准则近似算法，确定在特定图上的可处理性及有向推广的困难性。

Conclusion: 多个参数化下问题难以固定参数可处理，部分参数组合可恢复可处理性，特定图上有相应算法。

Abstract: We study the parameterized complexity of the T(h+1)-Free Edge Deletion problem. Given a graph G and integers k and h, the task is to delete at most k edges so that every connected component of the resulting graph has size at most h. The problem is NP-complete for every fixed h at least 3, while it is solvable in polynomial time for h at most 2.
  Recent work showed strong hardness barriers: the problem is W[1]-hard when parameterized by the solution size together with the size of a feedback edge set, ruling out fixed-parameter tractability for many classical structural parameters. We significantly strengthen these negative results by proving W[1]-hardness when parameterized by the vertex deletion distance to a disjoint union of paths, the vertex deletion distance to a disjoint union of stars, or the twin cover number. These results unify and extend known hardness results for treewidth, pathwidth, and feedback vertex set, and show that several restrictive parameters, including treedepth, cluster vertex deletion number, and modular width, do not yield fixed-parameter tractability when h is unbounded.
  On the positive side, we identify parameterizations that restore tractability. We show that the problem is fixed-parameter tractable when parameterized by cluster vertex deletion together with h, and also when parameterized by neighborhood diversity together with h via an integer linear programming formulation. We further present a fixed-parameter tractable bicriteria approximation algorithm parameterized by k. Finally, we show that the problem admits fixed-parameter tractable algorithms on split graphs and interval graphs, and we establish hardness for a directed generalization even on directed acyclic graphs.

</details>


### [187] [Fanciful Figurines flip Free Flood-It -- Polynomial-Time Miniature Painting on Co-gem-free Graphs](https://arxiv.org/abs/2602.00690)
*Christian Rosenke,Mark Scheibner*

Main category: cs.DS

TL;DR: 引入微型绘画问题，证明其与自由洪水游戏等价，转移复杂度结果，给出无诱导余宝石图的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 受同名爱好启发，研究按给定模板绘制图的计算问题。

Method: 证明微型绘画问题与自由洪水游戏等价，利用已知自由洪水游戏复杂度结果，设计无诱导余宝石图的多项式时间算法。

Result: 微型绘画问题在严重结构限制下是NP难的，在无诱导余宝石图上有多项式时间算法，自由洪水游戏在余宝石图上也可多项式时间求解。

Conclusion: 通过问题等价性转移复杂度结果，为微型绘画和自由洪水游戏在特定图类上提供有效算法。

Abstract: Inspired by the eponymous hobby, we introduce Miniature Painting as the computational problem to paint a given graph $G=(V,E)$ according to a prescribed template $t \colon V \rightarrow C$, which assigns colors $C$ to the vertices of $G$. In this setting, the goal is to realize the template using a shortest possible sequence of brush strokes, where each stroke overwrites a connected vertex subset with a color in $C$. We show that this problem is equivalent to a reversal of the well-studied Free Flood-It game, in which a colored graph is decolored into a single color using as few moves as possible. This equivalence allows known complexity results for Free Flood-It to be transferred directly to Miniature Painting, including NP-hardness under severe structural restrictions, such as when $G$ is a grid, a tree, or a split graph. Our main contribution is a polynomial-time algorithm for Miniature Painting on graphs that are free of induced co-gems, a graph class that strictly generalizes cographs. As a direct consequence, Free Flood-It is also polynomial-time solvable on co-gem-free graphs, independent of the initial coloring.

</details>


### [188] [Fast $k$-means Seeding Under The Manifold Hypothesis](https://arxiv.org/abs/2602.01104)
*Poojan Shah,Shashwat Agrawal,Ragesh Jaiswal*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study beyond worst case analysis for the $k$-means problem where the goal is to model typical instances of $k$-means arising in practice. Existing theoretical approaches provide guarantees under certain assumptions on the optimal solutions to $k$-means, making them difficult to validate in practice. We propose the manifold hypothesis, where data obtained in ambient dimension $D$ concentrates around a low dimensional manifold of intrinsic dimension $d$, as a reasonable assumption to model real world clustering instances. We identify key geometric properties of datasets which have theoretically predictable scaling laws depending on the quantization exponent $\varepsilon = 2/d$ using techniques from optimum quantization theory. We show how to exploit these regularities to design a fast seeding method called $\operatorname{Qkmeans}$ which provides $O(ρ^{-2} \log k)$ approximate solutions to the $k$-means problem in time $O(nD) + \widetilde{O}(\varepsilon^{1+ρ}ρ^{-1}k^{1+γ})$; where the exponent $γ= \varepsilon + ρ$ for an input parameter $ρ< 1$. This allows us to obtain new runtime - quality tradeoffs. We perform a large scale empirical study across various domains to validate our theoretical predictions and algorithm performance to bridge theory and practice for beyond worst case data clustering.

</details>


### [189] [Benchmarking of algorithms for set partitions](https://arxiv.org/abs/2602.01350)
*Arnav Khinvasara,Alexander Pikovski*

Main category: cs.DS

TL;DR: 本文探讨集合划分问题，给出确定集合划分数的近似公式，回顾枚举算法并测试，推荐Djokic等人的算法。


<details>
  <summary>Details</summary>
Motivation: 集合划分问题在多种场景尤其是组合优化任务中出现，需要解决列出所有集合划分及确定其数量的问题。

Method: 回顾已有方法，给出确定集合划分数的近似公式，对多种枚举算法进行基准测试。

Result: 得到确定集合划分数的近似公式，完成算法的基准测试。

Conclusion: 推荐Djokic等人的算法用于实际应用。

Abstract: Set partitions are arrangements of distinct objects into groups. The problem of listing all set partitions arises in a variety of settings, in particular in combinatorial optimization tasks. After a brief review, we give practical approximate formulas for determining the number of set partitions, both for small and large set sizes. Several algorithms for enumerating all set partitions are reviewed, and benchmarking tests were conducted. The algorithm of Djokic et al. is recommended for practical use.

</details>


### [190] [A $5$-Approximation Analysis for the Cover Small Cuts Problem](https://arxiv.org/abs/2602.01462)
*Miles Simmons,Ishan Bansal,Joe Cheriyan*

Main category: cs.DS

TL;DR: 本文聚焦覆盖小割问题，前人的WGMV原始对偶算法对该问题的近似比分别达到16、6，本文证明该算法能达到近似比5。


<details>
  <summary>Details</summary>
Motivation: 在覆盖小割问题中，进一步优化WGMV原始对偶算法的近似比。

Method: 使用满足对称性和结构子模性的更强大的柔韧性集合族概念进行分析。

Result: 证明WGMV原始对偶算法在覆盖小割问题上能达到近似比5。

Conclusion: 通过新的柔韧性集合族概念，提升了WGMV原始对偶算法在覆盖小割问题上的近似比。

Abstract: In the Cover Small Cuts problem, we are given a capacitated (undirected) graph $G=(V,E,u)$ and a threshold value $λ$, as well as a set of links $L$ with end-nodes in $V$ and a non-negative cost for each link $\ell\in L$; the goal is to find a minimum-cost set of links such that each non-trivial cut of capacity less than $λ$ is covered by a link. Bansal, Cheriyan, Grout, and Ibrahimpur (arXiv:2209.11209, Algorithmica 2024) showed that the WGMV primal-dual algorithm, due to Williamson, Goemans, Mihail, and Vazirani (Combinatorica, 1995), achieves approximation ratio $16$ for the Cover Small Cuts problem; their analysis uses the notion of a pliable family of sets that satisfies a combinatorial property. Later, Bansal (arXiv:2308.15714v2, IPCO 2025) and then Nutov (arXiv:2504.03910, MFCS 2025) proved that the same algorithm achieves approximation ratio $6$. We show that the same algorithm achieves approximation ratio $5$, by using a stronger notion, namely, a pliable family of sets that satisfies symmetry and structural submodularity.

</details>


### [191] [A polynomial-time algorithm for recognizing high-bandwidth graphs](https://arxiv.org/abs/2602.01755)
*Luis M. B. Varona*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: An unweighted, undirected graph $G$ on $n$ nodes is said to have \emph{bandwidth} at most $k$ if its nodes can be labelled from $0$ to $n - 1$ such that no two adjacent nodes have labels that differ by more than $k$. It is known that one can decide whether the bandwidth of $G$ is at most $k$ in $O(n^k)$ time and $O(n^k)$ space using dynamic programming techniques. For small $k$ close to $0$, this approach is effectively polynomial, but as $k$ scales with $n$, it becomes superexponential, requiring up to $O(n^{n - 1})$ time (where $n - 1$ is the maximum possible bandwidth). In this paper, we reformulate the problem in terms of bipartite matching for sufficiently large $k \ge \lfloor (n - 1)/2 \rfloor$, allowing us to use Hall's marriage theorem to develop an algorithm that runs in $O(n^{n - k + 1})$ time and $O(n)$ auxiliary space (beyond storage of the input graph). This yields polynomial complexity for large $k$ close to $n - 1$, demonstrating that the bandwidth recognition problem is solvable in polynomial time whenever either $k$ or $n - k$ remains small.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [192] [Learning in Bayesian Stackelberg Games With Unknown Follower's Types](https://arxiv.org/abs/2602.00771)
*Matteo Bollini,Francesco Bacchiocchi,Samuel Coutts,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study online learning in Bayesian Stackelberg games, where a leader repeatedly interacts with a follower whose unknown private type is independently drawn at each round from an unknown probability distribution. The goal is to design algorithms that minimize the leader's regret with respect to always playing an optimal commitment computed with knowledge of the game. We consider, for the first time to the best of our knowledge, the most realistic case in which the leader does not know anything about the follower's types, i.e., the possible follower payoffs. This raises considerable additional challenges compared to the commonly studied case in which the payoffs of follower types are known. First, we prove a strong negative result: no-regret is unattainable under action feedback, i.e., when the leader only observes the follower's best response at the end of each round. Thus, we focus on the easier type feedback model, where the follower's type is also revealed. In such a setting, we propose a no-regret algorithm that achieves a regret of $\widetilde{O}(\sqrt{T})$, when ignoring the dependence on other parameters.

</details>


### [193] [ReACT-TTC: Capacity-Aware Top Trading Cycles for Post-Choice Reassignment in Shared CPS](https://arxiv.org/abs/2602.00859)
*Anurag Satpathy,Arindam Khanda,Chittaranjan Swain,Sajal K. Das*

Main category: cs.GT

TL;DR: 本文针对共享资源网络物理系统提出偏差后重分配框架，改进TTC机制，结合PT偏好模型，通过电动汽车充电案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决共享资源网络物理系统中用户不遵守分配方案，降低系统效率问题，需轻量级重分配方案。

Method: 提出偏离后重分配框架，改进TTC机制处理资源容量和未分配情况，引入容量感知循环检测规则，结合PT偏好模型。

Result: 在电动汽车充电案例中，该框架提高了用户满意度和非合规行为下的有效分配质量。

Conclusion: 所提框架能有效应对共享资源网络物理系统中用户非合规问题，提升系统性能。

Abstract: Cyber-physical systems (CPS) increasingly manage shared physical resources in the presence of human decision-making, where system-assigned actions must be executed by users or agents in the physical world. A fundamental challenge in such settings is user non-compliance: individuals may deviate from assigned resources due to personal preferences or local information, degrading system efficiency and requiring light-weight reassignment schemes. This paper proposes a post-deviation reassignment framework for shared-resource CPS that operates on top of any initial allocation algorithm and is invoked only when users diverge from prescribed assignments. We advance the Top-Trading-Cycle (TTC) mechanism to enable voluntary, preference-driven exchanges after deviation events, and extend it to handle many-to-one resource capacities and unassigned resource conditions that are not supported by the classical TTC. We formalize these structural cases, introduce capacity-aware cycle-detection rules, and prove termination along with the preservation of Pareto efficiency, individual rationality, and strategy-proofness. A Prospect-Theoretic (PT) preference model is further incorporated to capture realistic user satisfaction behavior. We demonstrate the applicability of this framework on an electric-vehicle (EV) charging case study using real-world data, where it increases user satisfaction and effective assignment quality under non-compliant behavior.

</details>


### [194] [Minimizing Inequity in Facility Location Games](https://arxiv.org/abs/2602.01048)
*Yuhang Guo,Houyu Zhou*

Main category: cs.GT

TL;DR: 文章研究实线上设施选址博弈中最小化群体层面不平等问题，提出新机制解决单设施情况，扩展经典机制解决双设施情况。


<details>
  <summary>Details</summary>
Motivation: 解决实线上设施选址博弈中，当代理分属不同群体且可能策略性行动时，如何最小化群体层面不平等的问题。

Method: 针对单设施情况提出BALANCED机制和MAJOR - PHANTOM机制；针对双设施情况，扩展经典端点机制。

Result: 提出的机制在单设施情况下具策略证明性和近似保证，扩展的端点机制在双设施情况下对不同目标给出紧界。

Conclusion: 所提机制填补了群体公平目标近似界限的现有空白，将许多经典真实机制统一在公平感知框架内。

Abstract: This paper studies the problem of minimizing group-level inequity in facility location games on the real line, where agents belong to different groups and may act strategically. We explore a fairness-oriented objective that minimizes the maximum group effect introduced by Marsh and Schilling (1994). Each group's effect is defined as its total or maximum distance to the nearest facility, weighted by group-specific factors. We show that this formulation generalizes several prominent optimization objectives, including the classical utilitarian (social cost) and egalitarian (maximum cost) objectives, as well as two group-fair objectives, maximum total and average group cost. In order to minimize the maximum group effect, we first propose two novel mechanisms for the single-facility case, the BALANCED mechanism and the MAJOR-PHANTOM mechanism. Both are strategyproof and achieve tight approximation guarantees under distinct formulations of the maximum group effect objective. Our mechanisms not only close the existing gap in approximation bounds for group-fairness objectives identified by Zhou, Li, and Chan (2022), but also unify many classical truthful mechanisms within a broader fairness-aware framework. For the two-facility case, we revisit and extend the classical endpoint mechanism to our generalized setting and demonstrate that it provides tight bounds for two distinct maximum group effect objectives.

</details>


### [195] [Simple and Robust Quality Disclosure: The Power of Quantile Partition](https://arxiv.org/abs/2602.01066)
*Shipra Agrawal,Yiding Feng,Wei Tang*

Main category: cs.GT

TL;DR: 研究在线平台稳健质量披露，证明分位数分区披露的合理性，给出K - 分位数分区策略的最优解和任意分位数分区的稳健比率公式，指出有限信号单调分区的鲁棒性极限。


<details>
  <summary>Details</summary>
Motivation: 在线平台常通过简单、基于百分位的徽章和层级传达质量信息，且在不同市场环境中保持稳定，因此研究市场中的稳健质量披露。

Method: 通过极小极大竞争比率评估披露政策，将稳健质量披露问题转化为稳健披露设计程序。

Result: 为分位数分区披露提供理论依据，完全刻画K - 分位数分区策略的稳健最优解，给出任意分位数分区的稳健比率公式，指出有限信号单调分区无法超越2倍近似。

Conclusion: 分位数分区披露在稳健质量披露中有优势，有限信号单调分区存在鲁棒性限制。

Abstract: Quality information on online platforms is often conveyed through simple, percentile-based badges and tiers that remain stable across different market environments. Motivated by this empirical evidence, we study robust quality disclosure in a market where a platform commits to a public disclosure policy mapping the seller's product quality into a signal, and the seller subsequently sets a downstream monopoly price. Buyers have heterogeneous private types and valuations that are linear in quality. We evaluate a disclosure policy via a minimax competitive ratio: its worst-case revenue relative to the Bayesian-optimal disclosure-and-pricing benchmark, uniformly over all prior quality distributions, type distributions, and admissible valuations.
  Our main results provide a sharp theoretical justification for quantile-partition disclosure. For K-quantile partition policies, we fully characterize the robust optimum: the optimal worst-case ratio is pinned down by a one-dimensional fixed-point equation and the optimal thresholds follow a backward recursion. We also give an explicit formula for the robust ratio of any quantile partition as a simple "max-over-bins" expression, which explains why the robust-optimal partition allocates finer resolution to upper quantiles and yields tight guarantees such as 1 + 1/K for uniform percentile buckets. In contrast, we show a robustness limit for finite-signal monotone (quality-threshold) partitions, which cannot beat a factor-2 approximation. Technically, our analysis reduces the robust quality disclosure to a robust disclosure design program by establishing a tight functional characterization of all feasible indirect revenue functions.

</details>


### [196] [Efficiently Solving Mixed-Hierarchy Games with Quasi-Policy Approximations](https://arxiv.org/abs/2602.01568)
*Hamzah Khan,Dong Ho Lee,Jingqi Li,Tianyu Qiu,Christian Ellis,Jesse Milzman,Wesley Suttle,David Fridovich-Keil*

Main category: cs.GT

TL;DR: 研究N - 机器人森林结构混合层次博弈，提出准策略近似和不精确牛顿法求解，证明收敛性并实验验证实时收敛。


<details>
  <summary>Details</summary>
Motivation: 现有博弈论求解器难以处理结合同时和层次决策的混合信息结构，需研究新方法。

Method: 推导KKT条件，引入准策略近似去除高阶策略导数，开发不精确牛顿法求解近似KKT系统。

Result: 证明算法对非二次目标和非线性约束游戏局部指数收敛，在Julia库实现并实验验证实时收敛。

Conclusion: 提出的方法能有效解决N - 机器人森林结构混合层次博弈问题，实现实时收敛。

Abstract: Multi-robot coordination often exhibits hierarchical structure, with some robots' decisions depending on the planned behaviors of others. While game theory provides a principled framework for such interactions, existing solvers struggle to handle mixed information structures that combine simultaneous (Nash) and hierarchical (Stackelberg) decision-making. We study N-robot forest-structured mixed-hierarchy games, in which each robot acts as a Stackelberg leader over its subtree while robots in different branches interact via Nash equilibria. We derive the Karush-Kuhn-Tucker (KKT) first-order optimality conditions for this class of games and show that they involve increasingly high-order derivatives of robots' best-response policies as the hierarchy depth grows, rendering a direct solution intractable. To overcome this challenge, we introduce a quasi-policy approximation that removes higher-order policy derivatives and develop an inexact Newton method for efficiently solving the resulting approximated KKT systems. We prove local exponential convergence of the proposed algorithm for games with non-quadratic objectives and nonlinear constraints. The approach is implemented in a highly optimized Julia library (MixedHierarchyGames.jl) and evaluated in simulated experiments, demonstrating real-time convergence for complex mixed-hierarchy information structures.

</details>


### [197] [Stable Matching with Predictions: Robustness and Efficiency under Pruned Preferences](https://arxiv.org/abs/2602.02254)
*Samuel McCauley,Benjamin Moseley,Helia Niaparast,Shikha Singh*

Main category: cs.GT

TL;DR: 研究带截断偏好列表的稳定匹配问题，用算法 - 预测框架评估算法，结果表明准确预测可减少实例规模和提议次数。


<details>
  <summary>Details</summary>
Motivation: 经典双边匹配市场假设双方对对方所有主体排名，在大型匹配市场不可行，故研究带截断偏好列表的稳定匹配问题。

Method: 使用算法 - 预测框架，提出两个算法并进行理论和实证评估。

Result: 即使预测较准确，也能显著减少实例规模和提议次数。

Conclusion: 结果解释了延迟接受算法的实际成功，将市场设计与新兴的带预测算法理论相联系。

Abstract: In this paper, we study the fundamental problem of finding a stable matching in two-sided matching markets. In the classic variant, it is assumed that both sides of the market submit a ranked list of all agents on the other side. However, in large matching markets such as the National Resident Matching Program (NRMP), it is infeasible for hospitals to interview or mutually rank each resident. In this paper, we study the stable matching problem with truncated preference lists. In particular, we assume that, based on historical datasets, each hospital has a predicted rank of its likely match and only ranks residents within a bounded interval around that prediction.
  We use the algorithms-with-predictions framework and show that the classic deferred-acceptance (DA) algorithm used to compute stable matchings is robust to such truncation. We present two algorithms and theoretically and empirically evaluate their performance. Our results show that even with reasonably accurate predictions, it is possible to significantly cut down on both instance size (the length of preference lists) as well as the number of proposals made. These results explain the practical success of the DA algorithm and connect market design to the emerging theory of algorithms with predictions.

</details>


### [198] [Carry-Over Lottery Allocation: Practical Incentive-Compatible Drafts](https://arxiv.org/abs/2602.02487)
*Timothy Highley,Tannah Duncan,Ilia Volkov*

Main category: cs.GT

TL;DR: 提出COLA选秀机制，兼顾实用性、激励兼容性并利于弱队，解决实际实施挑战。


<details>
  <summary>Details</summary>
Motivation: NBA选秀乐透机制存在摆烂问题，需新机制促竞争平衡且防摆烂。

Method: 提出COLA机制，以多年季后赛结果评估球队，非季后赛队乐透票相同，未中奖票可结转，同时处理实施挑战。

Result: 该机制能奖励长期表现差且少获选秀帮助的球队，保留乐透格式以保透明和球迷参与度。

Conclusion: COLA机制可行，可用真相揭示机制解决强选秀年可能出现的摆烂问题。

Abstract: The NBA Draft lottery is designed to promote competitive balance by awarding better draft positions to weaker teams, but it creates incentives to deliberately lose, a practice known as tanking. We propose a draft mechanism that is simultaneously practical, incentive-compatible, and advantages weaker teams. The \textbf{Carry-Over Lottery Allocation (COLA) Draft Mechanism} represents a paradigm shift in evaluating team quality, replacing a single season's standings with playoff outcomes over multiple years. COLA uses a draft lottery where every non-playoff team receives the same number of lottery tickets, removing incentives to lose additional games after elimination. Lottery tickets that do not win a top draft pick carry over to future lotteries, while playoff success or winning a top pick diminishes a team's accumulated tickets. Over time, COLA rewards teams with poor long-term performance and less prior draft assistance. By retaining the lottery format, COLA preserves transparency and fan engagement.
  Real-world implementation challenges are addressed to demonstrate feasibility, including transitioning from the current system, handling traded draft picks, and accommodating draft classes of varying strength. The most significant challenge occurs in years with exceptionally strong draft classes, where teams may prefer missing the playoffs in order to gain lottery access, violating a foundational assumption: that teams prefer playoff success to lottery participation. We provide a solution to this problem, employing a truth-elicitation mechanism to identify such years and expand lottery eligibility to include as many playoff teams as necessary to preserve anti-tanking incentives.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [199] [Disentangled Interest Network for Out-of-Distribution CTR Prediction](https://arxiv.org/abs/2602.00002)
*Yu Zheng,Chen Gao,Jianxin Chang,Yanan Niu,Yang Song,Depeng Jin,Meng Wang,Yong Li*

Main category: cs.IR

TL;DR: 现有CTR预测方法因数据分布变化存在OOD问题，本文提出DiseCTR模型解决该问题，实验显示其在OOD推荐上表现优。


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测方法在训练和测试数据分布不同时存在OOD问题，且用户兴趣多元且变化速度不同。

Method: 提出DiseCTR模型，对CTR预测进行因果分解，设计兴趣编码器、弱监督兴趣解缠器和注意力兴趣聚合器。

Result: 在三个真实数据集上，DiseCTR在OOD推荐中准确率和鲁棒性最佳，显著提升AUC和GAUC，降低logloss。

Conclusion: DiseCTR成功解缠用户兴趣，是CTR预测OOD泛化的关键，代码和数据已开源。

Abstract: Click-through rate (CTR) prediction, which estimates the probability of a user clicking on a given item, is a critical task for online information services. Existing approaches often make strong assumptions that training and test data come from the same distribution. However, the data distribution varies since user interests are constantly evolving, resulting in the out-of-distribution (OOD) issue. In addition, users tend to have multiple interests, some of which evolve faster than others. Towards this end, we propose Disentangled Click-Through Rate prediction (DiseCTR), which introduces a causal perspective of recommendation and disentangles multiple aspects of user interests to alleviate the OOD issue in recommendation. We conduct a causal factorization of CTR prediction involving user interest, exposure model, and click model, based on which we develop a deep learning implementation for these three causal mechanisms. Specifically, we first design an interest encoder with sparse attention which maps raw features to user interests, and then introduce a weakly supervised interest disentangler to learn independent interest embeddings, which are further integrated by an attentive interest aggregator for prediction. Experimental results on three real-world datasets show that DiseCTR achieves the best accuracy and robustness in OOD recommendation against state-of-the-art approaches, significantly improving AUC and GAUC by over 0.02 and reducing logloss by over 13.7%. Further analyses demonstrate that DiseCTR successfully disentangles user interests, which is the key to OOD generalization for CTR prediction. We have released the code and data at https://github.com/DavyMorgan/DiseCTR/.

</details>


### [200] [Efficient Multilingual Search Relevance Modeling in E-Commerce via LLM Mixture-of-Experts](https://arxiv.org/abs/2602.00003)
*Ye Liu,Xu Chen,Wuji Chen,Mang Li*

Main category: cs.IR

TL;DR: 文章提出基于LLM的Mixture - of - Experts (MoE)框架用于解决电商搜索相关性建模问题，优化推理流程，在多语言场景中提升效果和效率。


<details>
  <summary>Details</summary>
Motivation: 电商平台多国家部署时，语言、文化和产品目录差异导致分布偏移，现有方法在异构环境下受数据多样性、覆盖度和推理成本限制，需更好的相关性建模方法。

Method: 提出基于LLM的MoE框架，动态路由查询到专家模型并融合嵌入，采用端到端硬路由和拼接策略；开发工程优化的离线批处理管道和资源高效调度方法。

Result: 在六个东南亚市场数据集上，MoE比相同参数的密集基线AUC提高0.72个百分点；优化管道每秒查询27.6次，吞吐量提高9%，GPU小时消耗最多降低35%。

Conclusion: 所提方法在多语言相关性和效率方面表现出色，对电商搜索系统有高性价比。

Abstract: In e-commerce platforms, search relevance directly influences both user experience and merchant revenue. In multi-country deployments, diverse linguistic, cultural, and product catalog contexts introduce significant distribution shifts, posing substantial challenges to relevance modeling. Existing approaches typically enhance the reasoning or multilingual abilities of a single monolithic model, yet they remain limited by data diversity, coverage gaps, and high inference costs in heterogeneous environments. Our empirical analysis reveals that different LLM base models exhibit complementary strengths across languages and regions, motivating an expert-based architecture. We propose a scalable LLM-based Mixture-of-Experts (MoE) framework that dynamically routes queries to specialized experts and fuses their embeddings through concatenation. Among rule-based, pseudo-label-based, and fully end-to-end strategies, end-to-end hard routing with concatenation offers the best balance of effectiveness and efficiency. To mitigate inference overhead, we further develop an engineering-optimized offline batch pipeline with resource-efficient scheduling, which hides memory latency, improves GPU utilization, and reduces GPU-hour consumption by up to 35% compared with synchronous execution. On datasets spanning six Southeast Asian markets, our MoE improves AUC by 0.72 percentage points over a dense baseline with the same active parameters. Meanwhile, the optimized pipeline achieves 27.6 queries per second (QPS), a 9% throughput improvement. These results demonstrate superior multilingual relevance and efficiency, delivering strong cost-effectiveness for real-world e-commerce search systems.

</details>


### [201] [C$^2$-Cite: Contextual-Aware Citation Generation for Attributed Large Language Models](https://arxiv.org/abs/2602.00004)
*Yue Yu,Ting Bai,HengZhi Lan,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Chuan Shi*

Main category: cs.IR

TL;DR: 现有指令微调归因大语言模型生成文本时对引用符号语境语义理解不足，提出C² - Cite框架解决，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有指令微调归因大语言模型在文本生成时不能正确解释引用符号的语境语义，存在引用脱节和知识整合差的问题。

Method: 提出C² - Cite框架，采用上下文引用对齐机制，先将检索文档上下文编码到引用符号表示中，再通过引用路由函数解码信息对齐标记编号。

Result: 在ALCE基准三个数据集上实验，C² - Cite++在引用质量上平均优于SOTA基线5.8%，响应正确性上优于17.4%。

Conclusion: C² - Cite框架能有效提升归因大模型的引用质量和响应正确性，代码公开。

Abstract: The attribution technique enhances the credibility of LLMs by adding citations to the generated sentences, enabling users to trace back to the original sources and verify the reliability of the output. However, existing instruction-tuned attributed LLMs often fail to properly interpret the contextual semantics of citation symbols (e.g., [i]) during text generation. This shortcoming arises from their insufficient awareness of the context information surrounding citation markers, which in turn leads to disjointed references and poor integration of retrieved knowledge into the generated content. To address this issue, we propose a novel \textbf{C}ontextual-aware \textbf{C}itation generation framework (\textbf{C$^2$}-\textbf{Cite}) that explicitly integrates the semantic relationships between citation markers and their referenced content. Specifically, a contextual citation alignment mechanism is adopted: it first encodes the retrieved document contexts into the symbol representation of citations, then aligns the marker numbers by decoding information from a citation router function. This mechanism enables the transformation of citation markers from generic placeholders into active knowledge pointers that link to the referenced source information. Experimental results on the ALCE benchmark across three datasets validate our framework C$^2$-Cite++: it outperforms the SOTA baseline by an average of 5.8\% in citation quality and 17.4\% in response correctness. The implementation is publicly available at https://github.com/BAI-LAB/c2cite

</details>


### [202] [AutoBool: An Reinforcement-Learning trained LLM for Effective Automated Boolean Query Generation for Systematic Reviews](https://arxiv.org/abs/2602.00005)
*Shuai Wang,Harrisen Scells,Bevan Koopman,Guido Zuccon*

Main category: cs.IR

TL;DR: 提出AutoBool强化学习框架训练大语言模型生成医学系统评价布尔查询，创建并发布相关最大数据集，实验显示其性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的大语言模型方法难以在医学系统评价布尔查询中平衡召回率和精确率，且缺乏高质量的真实布尔查询用于监督微调。

Method: 使用强化学习直接优化查询生成，无需目标查询；创建并发布包含65588个主题的数据集用于训练和评估。

Result: AutoBool在新数据集和两个现有数据集上显著优于零样本/少样本提示，使用较小骨干网络也能匹配或超越更大的基于GPT的模型，检索文档数量少10到16倍。

Conclusion: 揭示了模型骨干、大小、解码温度和提示设计的关键作用，代码和数据公开。

Abstract: We present AutoBool, a reinforcement learning (RL) framework that trains large language models (LLMs) to generate effective Boolean queries for medical systematic reviews. Boolean queries are the primary mechanism for literature retrieval in this domain and must achieve high recall while maintaining reasonable precision - a challenging balance that existing prompt-based LLM approaches often struggle to achieve. A major limitation in this space is the lack of high-quality ground-truth Boolean queries for each topic, which makes supervised fine-tuning impractical. AutoBool addresses this challenge by using RL to directly optimize query generation with retrieval measures, without requiring target queries. To support this effort, we create and release the largest dataset of its kind: 65588 topics in total for training and evaluating the task of automatic Boolean query formulation. Experiments on our new dataset and two established datasets (CLEF TAR and Seed Collection) show that AutoBool significantly outperforms zero shot/few shot prompting and matches or exceeds the effectiveness of much larger GPT-based models (e.g., GPT-4o, O3) using smaller backbones. It also approaches effectiveness of expert-authored queries while retrieving 10 to 16 times fewer documents. Ablation studies reveal the critical roles of model backbone, size, decoding temperature, and prompt design. Code and data are available at https://github.com/ielab/AutoBool.

</details>


### [203] [FDA AI Search: Making FDA-Authorized AI Devices Searchable](https://arxiv.org/abs/2602.00006)
*Arun Kavishwar,William Lotter*

Main category: cs.IR

TL;DR: 开发FDA AI Search网站帮助查询FDA授权的AI医疗设备，评估显示其检索算法比基于关键字的方法有效。


<details>
  <summary>Details</summary>
Motivation: FDA数据库的元数据有限且摘要PDF不可搜索，难以识别满足特定临床需求的设备。

Method: 开发FDA AI Search网站，其后端采用基于嵌入的检索系统，比较用户查询与从授权摘要中提取的特征。

Result: 定量和定性评估表明该检索算法比基于关键字的方法更有效。

Conclusion: 随着FDA授权的AI设备增多和用例扩展，此工具将帮助医疗人员和开发者。

Abstract: Over 1,200 AI-enabled medical devices have received marketing authorization from the U.S. FDA, yet identifying devices suited to specific clinical needs remains challenging because the FDA's databases contain only limited metadata and non-searchable summary PDFs. To address this gap, we developed FDA AI Search, a website that enables semantic querying of FDA-authorized AI-enabled devices. The backend includes an embedding-based retrieval system, where LLM-extracted features from authorization summaries are compared to user queries to find relevant matches. We present quantitative and qualitative evaluation that support the effectiveness of the retrieval algorithm compared to keyword-based methods. As FDA-authorized AI devices become increasingly prevalent and their use cases expand, we envision that the tool will assist healthcare providers in identifying devices aligned with their clinical needs and support developers in formulating novel AI applications.

</details>


### [204] [Front-Loaded or Balanced? The Mechanism through Which Review Order Affects Overall Ratings in Premium Service Settings](https://arxiv.org/abs/2602.00008)
*He Wang,Ziyu Zhou,Hanxiang Liu*

Main category: cs.IR

TL;DR: 研究评估顺序（先评分或先评论）对消费者评分的影响，发现先评分在高质量服务情境中提升评分，低质量时降低评分，揭示心理机制。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注评论内容和情感，对评估顺序如何影响评分结果的系统研究有限，在高质量服务情境下评估顺序对评分真实性和反馈质量很关键。

Method: 探索性分析对比Letterboxd和Yelp的评分分布；进行三个对照实验；开展机制分析。

Result: 先评分界面在高质量服务情境中显著提高消费者整体评分；认知努力和情感启发式是影响评分的双途径；服务质量起调节作用，低质量时先评分导致更低评分。

Conclusion: 揭示评估顺序通过认知和情感途径影响消费者评分的心理机制，拓展在线评分形成的理论理解，为优化平台界面设计提供实践启示。

Abstract: In the increasingly prevalent landscape of high-quality service contexts, whether consumer evaluation interfaces adopt a rating-first or review-first sequence has become a critical factor shaping rating authenticity and feedback quality. While prior research has primarily examined review content and sentiment, systematic investigation into how evaluation order influences rating outcomes remains limited. Through exploratory analyses, we find that Letterboxd -- which employs a review-first, rating-after mechanism -- exhibits a more centralized rating distribution with fewer extreme scores, whereas Yelp -- which adopts a rating-first, review-after mechanism -- shows a pronounced bimodal distribution with more polarized ratings. Three controlled experiments further demonstrate that in high-quality service contexts, a rating-first (vs. review-first) interface significantly elevates consumers' overall ratings. Mechanism analyses indicate that cognitive effort and affective heuristics serve as dual pathways: a rating-first (vs. review-first) sequence reduces cognitive effort and heightens affective heuristics, thereby increasing rating scores. Moreover, service quality moderates this process. When service quality is low, the rating-first (vs. review-first) sequence instead leads to lower ratings. This research reveals the psychological mechanisms through which evaluation order affects consumer ratings via cognitive and affective pathways. It extends theoretical understanding of online rating formation and offers practical implications for optimizing platform interface design to enhance rating authenticity and credibility.

</details>


### [205] [ChunkNorris: A High-Performance and Low-Energy Approach to PDF Parsing and Chunking](https://arxiv.org/abs/2602.00010)
*Mathieu Ciancone,Clovis Varangot-Reille,Marion Schaeffer*

Main category: cs.IR

TL;DR: 介绍了一种名为ChunkNorris的启发式PDF文档解析和分块技术，经基准测试表现优于现有方法，凸显启发式方法在资源受限RAG场景的潜力。


<details>
  <summary>Details</summary>
Motivation: 在检索增强生成应用中，高质量解析和分块对信息检索和答案生成至关重要，需优化PDF文档的解析和分块。

Method: 提出不依赖机器学习的ChunkNorris技术，使用简单有效的启发式方法。

Result: 通过综合基准测试，ChunkNorris在执行时间、能耗和检索准确性等方面优于基线和更先进的技术。

Conclusion: 启发式方法在现实世界资源受限的RAG用例中具有潜力。

Abstract: In Retrieval-Augmented Generation applications, the Information Retrieval part is central as it provides the contextual information that enables a Large Language Model to generate an appropriate and truthful response. High quality parsing and chunking are critical as efficient data segmentation directly impacts downstream tasks, i.e. Information Retrieval and answer generation. In this paper, we introduce ChunkNorris, a novel heuristic-based technique designed to optimise the parsing and chunking of PDF documents. Our approach does not rely on machine learning and employs a suite of simple yet effective heuristics to achieve high performance with minimal computational overhead. We demonstrate the efficiency of ChunkNorris through a comprehensive benchmark against existing parsing and chunking methods, evaluating criteria such as execution time, energy consumption, and retrieval accuracy. We propose an open-access dataset to produce our results. ChunkNorris outperforms baseline and more advanced techniques, offering a practical and efficient alternative for Information Retrieval tasks. Therefore, this research highlights the potential of heuristic-based methods for real-world, resource-constrained RAG use cases.

</details>


### [206] [Chained Prompting for Better Systematic Review Search Strategies](https://arxiv.org/abs/2602.00011)
*Fatima Nasser,Fouad Trad,Ammar Mohanna,Ghada El-Hajj Fuleihan,Ali Chehab*

Main category: cs.IR

TL;DR: 介绍基于大语言模型的链式提示工程框架用于系统评价搜索策略自动开发，性能超现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统手动方法资源消耗大且有主观性，启发式和自动化技术召回率低，需更好方法开发系统评价搜索策略。

Method: 引入基于大语言模型的链式提示工程框架，分解评价目标、提取和形式化PICO元素等。

Result: 框架生成PICO元素表现优于现有方法，在部分数据集上平均召回率达0.9，超现有方法。

Conclusion: 基于大语言模型的流程能产生透明、可重复和高性能的搜索策略，有潜力支持证据综合和循证实践。

Abstract: Systematic reviews require the use of rigorously designed search strategies to ensure both comprehensive retrieval and minimization of bias. Conventional manual approaches, although methodologically systematic, are resource-intensive and susceptible to subjectivity, whereas heuristic and automated techniques frequently under-perform in recall unless supplemented by extensive expert input. We introduce a Large Language Model (LLM)-based chained prompt engineering framework for the automated development of search strategies in systematic reviews. The framework replicates the procedural structure of manual search design while leveraging LLMs to decompose review objectives, extract and formalize PICO elements, generate conceptual representations, expand terminologies, and synthesize Boolean queries. In addition to query construction, the framework exhibits superior performance in generating well-structured PICO elements relative to existing methods, thereby strengthening the foundation for high-recall search strategies. Evaluation on a subset of the LEADSInstruct dataset demonstrates that the framework attains a 0.9 average recall. These results significantly exceed the performance of existing approaches. Error analysis further highlights the critical role of precise objective specification and terminological alignment in optimizing retrieval effectiveness. These findings confirm the capacity of LLM-based pipelines to yield transparent, reproducible, and high-performing search strategies, and highlight their potential as scalable instruments for supporting evidence synthesis and evidence-based practice.

</details>


### [207] [Linear-PAL: A Lightweight Ranker for Mitigating Shortcut Learning in Personalized, High-Bias Tabular Ranking](https://arxiv.org/abs/2602.00013)
*Vipul Dinesh Pawar*

Main category: cs.IR

TL;DR: 指出深度学习架构在高偏差场景下的问题，提出轻量级框架Linear - PAL，在大规模数据集上验证其在去偏排序质量和训练效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 解决电商排序中深度学习架构在高偏差场景下因捷径学习导致排序质量下降的问题。

Method: 提出Linear - PAL框架，通过结构约束去偏，引入向量整数哈希技术生成特征。

Result: 在420万样本的大规模数据集上，Linear - PAL去偏排序质量优于深度集成模型，训练延迟降低43倍。

Conclusion: Linear - PAL计算效率高，能实现高频再训练，近乎实时提供鲁棒、个性化的排序。

Abstract: In e-commerce ranking, implicit user feedback is systematically confounded by Position Bias -- the strong propensity of users to interact with top-ranked items regardless of relevance. While Deep Learning architectures (e.g., Two-Tower Networks) are the standard solution for de-biasing, we demonstrate that in High-Bias Regimes, state-of-the-art Deep Ensembles suffer from Shortcut Learning: they minimize training loss by overfitting to the rank signal, leading to degraded ranking quality despite high prediction accuracy. We propose Linear Position-bias Aware Learning (Linear-PAL), a lightweight framework that enforces de-biasing through structural constraints: explicit feature conjunctions and aggressive regularization. We further introduce a Vectorized Integer Hashing technique for feature generation, replacing string-based operations with $O(N)$ vectorized arithmetic. Evaluating on a large-scale dataset (4.2M samples), Linear-PAL achieves Pareto Dominance: it outperforms Deep Ensembles in de-biased ranking quality (Relevance AUC: 0.7626 vs. 0.6736) while reducing training latency by 43x (40s vs 1762s). This computational efficiency enables high-frequency retraining, allowing the system to capture user-specific emerging market trends and deliver robust, personalized ranking in near real-time.

</details>


### [208] [AI-assisted Protocol Information Extraction For Improved Accuracy and Efficiency in Clinical Trial Workflows](https://arxiv.org/abs/2602.00052)
*Ramtin Babaeipour,François Charest,Madison Wright*

Main category: cs.IR

TL;DR: 文章评估用含RAG的AI系统自动提取临床试验协议信息，对比其与公开LLMs提取准确性及对模拟工作流影响，结果显示该RAG更好，建议整合相关方法到临床工作流。


<details>
  <summary>Details</summary>
Motivation: 临床试验协议复杂度增加等给团队带来负担，结构化协议内容有提升效率等潜力，因此评估用AI系统自动提取协议信息的可行性。

Method: 评估用含RAG的AI系统自动提取信息，对比其与公开LLMs提取准确性，评估AI辅助对模拟提取CRC工作流的影响。

Result: RAG过程提取准确率87.8%，高于公开LLMs的62.6%；模拟工作流中，AI辅助任务完成快40%，认知要求低，受用户青睐。

Conclusion: 虽需专家监督，但AI辅助提取可实现大规模协议智能，建议将类似方法整合到临床工作流验证其影响。

Abstract: Increasing clinical trial protocol complexity, amendments, and challenges around knowledge management create significant burden for trial teams. Structuring protocol content into standard formats has the potential to improve efficiency, support documentation quality, and strengthen compliance. We evaluate an Artificial Intelligence (AI) system using generative LLMs with Retrieval-Augmented Generation (RAG) for automated clinical trial protocol information extraction. We compare the extraction accuracy of our clinical-trial-specific RAG process against that of publicly available (standalone) LLMs. We also assess the operational impact of AI-assistance on simulated extraction CRC workflows. Our RAG process was measured as more accurate (87.8%) than standalone LLMs with fine-tuned prompts (62.6%) against expert-supported reference annotations. In the simulated extraction workflows, AI-assisted tasks were completed 40% faster, rated as less cognitively demanding and strongly preferred by users. While expert oversight remains essential, this suggests that AI-assisted extraction can enable protocol intelligence at scale, motivating the integration of similar methodologies into real world clinical workflows to further validate its impact on feasibility, study start-up, and post-activation monitoring.

</details>


### [209] [SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.00083)
*Yuxin Yang,Gangda Deng,Ömer Faruk Akgül,Nima Chitsazan,Yash Govilkar,Akasha Tigalappanavara,Shi-Xiong Zhang,Sambit Sahu,Viktor Prasanna*

Main category: cs.IR

TL;DR: 提出SPARC - RAG多智能体框架解决RAG在多跳问答推理中的扩展问题，还引入微调方法，在问答基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有RAG模型在多跳问答长推理方面存在挑战，简单扩展会导致上下文污染和扩展效率低下。

Method: 提出SPARC - RAG多智能体框架，采用专门智能体维护全局上下文，生成子查询进行并行探索并调控退出决策；引入轻量级微调方法。

Result: 在单跳和多跳问答基准测试中，SPARC - RAG始终优于之前的RAG基线，在较低推理成本下平均F1提高6.2。

Conclusion: SPARC - RAG能有效解决RAG模型扩展问题，提高推理效率和性能。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language model outputs in external evidence, but remains challenged on multi-hop question answering that requires long reasoning. Recent works scale RAG at inference time along two complementary dimensions: sequential depth for iterative refinement and parallel width for coverage expansion. However, naive scaling causes context contamination and scaling inefficiency, leading to diminishing or negative returns despite increased computation. To address these limitations, we propose SPARC-RAG, a multi-agent framework that coordinates sequential and parallel inference-time scaling under a unified context management mechanism. SPARC-RAG employs specialized agents that maintain a shared global context and provide explicit control over the scaling process. It generates targeted, complementary sub-queries for each branch to enable diverse parallel exploration, and explicitly regulates exiting decisions based on answer correctness and evidence grounding. To optimize scaling behavior, we further introduce a lightweight fine-tuning method with process-level verifiable preferences, which improves the efficiency of sequential scaling and effectiveness of parallel scaling. Across single- and multi-hop QA benchmarks, SPARC-RAG consistently outperforms previous RAG baselines, yielding an average +6.2 F1 improvement under lower inference cost.

</details>


### [210] [RAGRouter-Bench: A Dataset and Benchmark for Adaptive RAG Routing](https://arxiv.org/abs/2602.00296)
*Ziqi Wang,Xi Zhu,Shuhang Lin,Haochen Xue,Minghao Guo,Yongfeng Zhang*

Main category: cs.IR

TL;DR: 提出首个用于自适应RAG路由的数据集和基准RAGRouter - Bench，评估不同RAG范式，发现无通用最优范式，强调路由感知评估必要。


<details>
  <summary>Details</summary>
Motivation: 现有RAG研究缺乏对不同查询 - 语料库上下文及有效性 - 效率权衡的系统理解。

Method: 引入RAGRouter - Bench，从查询 - 语料库兼容性角度重新审视检索，标准化五种RAG范式，结合三种查询类型、语料库指标及统一评估。

Result: 实验表明无单一RAG范式通用最优，范式适用性受查询 - 语料库交互影响，增加高级机制不一定带来更好权衡。

Conclusion: 强调路由感知评估的必要性，为下一代RAG系统奠定基础。

Abstract: Retrieval-Augmented Generation (RAG) has become a core paradigm for grounding large language models with external knowledge. Despite extensive efforts exploring diverse retrieval strategies, existing studies predominantly focus on query-side complexity or isolated method improvements, lacking a systematic understanding of how RAG paradigms behave across different query-corpus contexts and effectiveness-efficiency trade-offs. In this work, we introduce RAGRouter-Bench, the first dataset and benchmark designed for adaptive RAG routing. RAGRouter-Bench revisits retrieval from a query-corpus compatibility perspective and standardizes five representative RAG paradigms for systematic evaluation across 7,727 queries and 21,460 documents spanning diverse domains. The benchmark incorporates three canonical query types together with fine-grained semantic and structural corpus metrics, as well as a unified evaluation for both generation quality and resource consumption. Experiments with DeepSeek-V3 and LLaMA-3.1-8B demonstrate that no single RAG paradigm is universally optimal, that paradigm applicability is strongly shaped by query-corpus interactions, and that increased advanced mechanism does not necessarily yield better effectiveness-efficiency trade-offs. These findings underscore the necessity of routing-aware evaluation and establish a foundation for adaptive, interpretable, and generalizable next-generation RAG systems.

</details>


### [211] [Equity vs. Equality: Optimizing Ranking Fairness for Tailored Provider Needs](https://arxiv.org/abs/2602.00495)
*Yiteng Tu,Weihang Su,Shuguang Han,Yiqun Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 提出面向公平性的框架，开发EquityRank算法优化用户有效性和提供者公平性，模拟显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有排名公平性方法多基于平等视角，忽略真实世界提供者多样化需求，不能准确反映不同提供者的实际效用。

Method: 引入面向公平性的框架，明确建模每个提供者对关键结果的偏好，开发基于梯度的EquityRank算法。

Result: 大量离线和在线模拟表明，EquityRank在有效性和公平性之间提供了更好的权衡，并能适应不同提供者的需求。

Conclusion: EquityRank能在优化用户有效性的同时，保障提供者端的公平性，适应多样化需求。

Abstract: Ranking plays a central role in connecting users and providers in Information Retrieval (IR) systems, making provider-side fairness an important challenge. While recent research has begun to address fairness in ranking, most existing approaches adopt an equality-based perspective, aiming to ensure that providers with similar content receive similar exposure. However, it overlooks the diverse needs of real-world providers, whose utility from ranking may depend not only on exposure but also on outcomes like sales or engagement. Consequently, exposure-based fairness may not accurately capture the true utility perceived by different providers with varying priorities. To this end, we introduce an equity-oriented fairness framework that explicitly models each provider's preferences over key outcomes such as exposure and sales, thus evaluating whether a ranking algorithm can fulfill these individualized goals while maintaining overall fairness across providers. Based on this framework, we develop EquityRank, a gradient-based algorithm that jointly optimizes user-side effectiveness and provider-side equity. Extensive offline and online simulations demonstrate that EquityRank offers improved trade-offs between effectiveness and fairness and adapts to heterogeneous provider needs.

</details>


### [212] [Towards Sample-Efficient and Stable Reinforcement Learning for LLM-based Recommendation](https://arxiv.org/abs/2602.00632)
*Hongxun Ding,Keqin Bao,Jizhi Zhang,Yi Fang,Wenxin Xu,Fuli Feng,Xiangnan He*

Main category: cs.IR

TL;DR: 研究指出Long CoT不适合顺序推荐领域，提出RISER框架，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: Long CoT存在过度推理延迟和用户行为数据缺乏明确认知推理模式问题，不适合顺序推荐领域，且直接应用RL有低样本效率和训练不稳定问题。

Method: 提出了RISER框架，将不可学习轨迹转化为有效成对偏好数据进行优化，还采用策略确保稳定性。

Result: 在三个真实数据集上的实验表明，RISER显著优于竞争基线。

Conclusion: RISER为RL增强的LLM推荐建立了一个强大的范式。

Abstract: While Long Chain-of-Thought (Long CoT) reasoning has shown promise in Large Language Models (LLMs), its adoption for enhancing recommendation quality is growing rapidly. In this work, we critically examine this trend and argue that Long CoT is inherently ill-suited for the sequential recommendation domain. We attribute this misalignment to two primary factors: excessive inference latency and the lack of explicit cognitive reasoning patterns in user behavioral data. Driven by these observations, we propose pivoting away from the CoT structure to directly leverage its underlying mechanism: Reinforcement Learning (RL), to explore the item space. However, applying RL directly faces significant obstacles, notably low sample efficiency-where most actions fail to provide learning signals-and training instability. To overcome these limitations, we propose RISER, a novel Reinforced Item Space Exploration framework for Recommendation. RISER is designed to transform non-learnable trajectories into effective pairwise preference data for optimization. Furthermore, it incorporates specific strategies to ensure stability, including the prevention of redundant rollouts and the constraint of token-level update magnitudes. Extensive experiments on three real-world datasets show that RISER significantly outperforms competitive baselines, establishing a robust paradigm for RL-enhanced LLM recommendation. Our code will be available at https://anonymous.4open.science/r/RISER/.

</details>


### [213] [RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment](https://arxiv.org/abs/2602.00682)
*Yuecheng Li,Hengwei Ju,Zeyu Song,Wei Yang,Chi Lu,Peng Jiang,Kun Gai*

Main category: cs.IR

TL;DR: 提出RecGOAT框架用于大模型增强的多模态推荐，解决大模型与推荐系统表征差异问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有工作忽略大模型与推荐系统基本表征差异，导致多模态表征不兼容和推荐性能不佳。

Method: 提出RecGOAT框架，用图注意力网络丰富协作语义，设计双粒度渐进式多模态 - ID对齐框架实现实例级和分布级语义对齐。

Result: 在三个公开基准上达到了最先进性能，在大规模在线广告平台部署验证了有效性和可扩展性。

Conclusion: RecGOAT框架具有理论保证的对齐能力，其导出的统一表征具有更好的语义一致性和全面性，能有效提升推荐性能。

Abstract: Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.

</details>


### [214] [SWGCN: Synergy Weighted Graph Convolutional Network for Multi-Behavior Recommendation](https://arxiv.org/abs/2602.00727)
*Fangda Chen,Yueyang Wang,Chaoli Lou,Min Gao,Qingyu Xiong*

Main category: cs.IR

TL;DR: 提出SWGCN解决现有图方法不足，在多数据集测试有增益且开源。


<details>
  <summary>Details</summary>
Motivation: 当前图方法常忽略跨行为协同信号和个体行为细粒度强度，需改进。

Method: 引入SWGCN，包含目标偏好加权器和协同对齐任务，后者借助辅助偏好评估器指导训练。

Result: 在淘宝、IJCAI和贝贝三个数据集测试，淘宝数据集HR和NDCG有显著相对增益，其他数据集也有一致增益。

Conclusion: SWGCN模型有良好鲁棒性和泛化性，代码已开源。

Abstract: Multi-behavior recommendation paradigms have emerged to capture diverse user activities, forecasting primary conversions (e.g., purchases) by leveraging secondary signals like browsing history. However, current graph-based methods often overlook cross-behavioral synergistic signals and fine-grained intensity of individual actions. Motivated by the need to overcome these shortcomings, we introduce Synergy Weighted Graph Convolutional Network (SWGCN). SWGCN introduces two novel components: a Target Preference Weigher, which adaptively assigns weights to user-item interactions within each behavior, and a Synergy Alignment Task, which guides its training by leveraging an Auxiliary Preference Valuator. This task prioritizes interactions from synergistic signals that more accurately reflect user preferences. The performance of our model is rigorously evaluated through comprehensive tests on three open-source datasets, specifically Taobao, IJCAI, and Beibei. On the Taobao dataset, SWGCN yields relative gains of 112.49% and 156.36% in terms of Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG), respectively. It also yields consistent gains on IJCAI and Beibei, confirming its robustness and generalizability across various datasets. Our implementation is open-sourced and can be accessed via https://github.com/FangdChen/SWGCN.

</details>


### [215] [Towards Trustworthy Multimodal Recommendation](https://arxiv.org/abs/2602.00730)
*Zixuan Li*

Main category: cs.IR

TL;DR: 本文从方法和分析角度探讨可信多模态推荐，提出模态级纠正组件，并给出交互级可信度的见解，实验证明能提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实中多模态内容可能不可信，会使现有推荐器在模态损坏时表现不佳，因此需研究可信多模态推荐。

Method: 提出模态级纠正组件，通过学习物品与多模态特征的软对应关系减轻不可信特征；探究交互级可信度在噪声协同信号下的情况。

Result: 在多个数据集和骨干网络上的实验表明，模态纠正提升了鲁棒性，并验证了交互级的见解。

Conclusion: 提出的模态纠正组件和交互级见解有助于推动可信多模态推荐发展，提升推荐系统在不可信多模态内容下的鲁棒性。

Abstract: Recent advances in multimodal recommendation have demonstrated the effectiveness of incorporating visual and textual content into collaborative filtering. However, real-world deployments raise an increasingly important yet underexplored issue: trustworthiness. On modern e-commerce platforms, multimodal content can be misleading or unreliable (e.g., visually inconsistent product images or click-bait titles), injecting untrustworthy signals into multimodal representations and making existing recommenders brittle under modality corruption. In this work, we take a step towards trustworthy multimodal recommendation from both a method and an analysis perspective. First, we propose a plug-and-play modality-level rectification component that mitigates untrustworthy modality features by learning soft correspondences between items and multimodal features. Using lightweight projections and Sinkhorn-based soft matching, the rectification suppresses mismatched modality signals while preserving semantic consistency, and can be integrated into existing multimodal recommenders without architectural modifications. Second, we present two practical insights on interaction-level trustworthiness under noisy collaborative signals: (i) training-set pseudo interactions can help or hurt performance under noise depending on prior-signal alignment; and (ii) propagation-graph pseudo edges can also help or hurt robustness, as message passing may amplify misalignment. Extensive experiments on multiple datasets and backbones under varying corruption levels demonstrate improved robustness from modality rectification and validate the above interaction-level observations.

</details>


### [216] [Optimizing Retrieval Components for a Shared Backbone via Component-Wise Multi-Stage Training](https://arxiv.org/abs/2602.00805)
*Yunhan Li,Mingjie Xie,Zihan Gong,Zeyang Shi,Gengshen Wu,Min Yang*

Main category: cs.IR

TL;DR: 本文针对生产法律检索系统中作为共享骨干的检索组件优化提出系统级解决方案，采用多阶段优化框架，验证并部署了支持多应用的共享检索服务。


<details>
  <summary>Details</summary>
Motivation: 嵌入检索发展使密集检索器成工业系统核心，检索质量制约系统性能和可扩展性，且跨应用关联模型决策，需优化检索组件。

Method: 采用多阶段优化框架，对密集检索器和重排器进行优化，采用组件级、混合阶段配置。

Result: 得到的骨干通过端到端评估，可作为支持多个工业应用的共享检索服务部署。

Conclusion: 采用多阶段优化框架和组件级、混合阶段配置能有效优化共享检索骨干，提升系统性能。

Abstract: Recent advances in embedding-based retrieval have enabled dense retrievers to serve as core infrastructure in many industrial systems, where a single retrieval backbone is often shared across multiple downstream applications. In such settings, retrieval quality directly constrains system performance and extensibility, while coupling model selection, deployment, and rollback decisions across applications.
  In this paper, we present empirical findings and a system-level solution for optimizing retrieval components deployed as a shared backbone in production legal retrieval systems. We adopt a multi-stage optimization framework for dense retrievers and rerankers, and show that different retrieval components exhibit stage-dependent trade-offs. These observations motivate a component-wise, mixed-stage configuration rather than relying on a single uniformly optimal checkpoint. The resulting backbone is validated through end-to-end evaluation and deployed as a shared retrieval service supporting multiple industrial applications.

</details>


### [217] [Unifying Ranking and Generation in Query Auto-Completion via Retrieval-Augmented Generation and Multi-Objective Alignment](https://arxiv.org/abs/2602.01023)
*Kai Yuan,Anthony Zheng,Jia Hu,Divyanshu Sheth,Hemanth Velaga,Kylee Kim,Matteo Guarrera,Besim Avci,Xuetao Yin,Rajyashree Mukherjee,Sean Suchter*

Main category: cs.IR

TL;DR: 提出统一框架将查询自动补全（QAC）重构成端到端列表生成，结合RAG和多目标DPO，评估显示有显著改进，是一种范式转变。


<details>
  <summary>Details</summary>
Motivation: 现有QAC方法存在传统检索排序管道长尾覆盖有限、需大量特征工程，生成方法有幻觉和安全风险等问题。

Method: 将QAC重构成端到端列表生成，采用RAG和多目标DPO；定义部署多种验证器，结合多方法生成高质量合成数据；采用混合服务架构实现高效生产部署。

Result: 在大规模商业搜索平台评估中，离线指标各维度提升，人工评估偏好得分增加，在线实验击键减少、建议采纳率提高。

Conclusion: 基于大语言模型、RAG和多目标对齐的端到端生成是生产QAC的有效解决方案，为搜索和推荐行业提供有价值框架。

Abstract: Query Auto-Completion (QAC) suggests query completions as users type, helping them articulate intent and reach results more efficiently. Existing approaches face fundamental challenges: traditional retrieve-and-rank pipelines have limited long-tail coverage and require extensive feature engineering, while recent generative methods suffer from hallucination and safety risks. We present a unified framework that reformulates QAC as end-to-end list generation through Retrieval-Augmented Generation (RAG) and multi-objective Direct Preference Optimization (DPO). Our approach combines three key innovations: (1) reformulating QAC as end-to-end list generation with multi-objective optimization; (2) defining and deploying a suite of rule-based, model-based, and LLM-as-judge verifiers for QAC, and using them in a comprehensive methodology that combines RAG, multi-objective DPO, and iterative critique-revision for high-quality synthetic data; (3) a hybrid serving architecture enabling efficient production deployment under strict latency constraints. Evaluation on a large-scale commercial search platform demonstrates substantial improvements: offline metrics show gains across all dimensions, human evaluation yields +0.40 to +0.69 preference scores, and a controlled online experiment achieves 5.44\% reduction in keystrokes and 3.46\% increase in suggestion adoption, validating that unified generation with RAG and multi-objective alignment provides an effective solution for production QAC. This work represents a paradigm shift to end-to-end generation powered by large language models, RAG, and multi-objective alignment, establishing a production-validated framework that can benefit the broader search and recommendation industry.

</details>


### [218] [GRAB: An LLM-Inspired Sequence-First Click-Through Rate Prediction Modeling Paradigm](https://arxiv.org/abs/2602.01865)
*Shaopeng Chen,Chuyue Xie,Huimin Ren,Shaozong Zhang,Han Zhang,Ruobing Cheng,Zhiqiang Cao,Zehao Ju,Gao Yu,Jie Ding,Xiaodong Chen,Xuewu Jiao,Shuanglong Li,Liu Lin*

Main category: cs.IR

TL;DR: 针对传统DLRMs性能和效率瓶颈，提出GRAB框架用于CTR预测，在线部署效果好且有良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习推荐模型在性能和效率上存在瓶颈，难以进行泛化和长序列建模。

Method: 提出GRAB端到端生成框架，集成CamA机制捕获用户行为序列中的时间动态和特定动作信号。

Result: 全规模在线部署显示，GRAB显著优于现有DLRMs，收入提升3.05%，CTR提升3.49%，且使用更长交互序列时表达能力呈单调近似线性提升。

Conclusion: GRAB框架有效解决了传统DLRMs的问题，在CTR预测上表现优异且有良好的扩展性。

Abstract: Traditional Deep Learning Recommendation Models (DLRMs) face increasing bottlenecks in performance and efficiency, often struggling with generalization and long-sequence modeling. Inspired by the scaling success of Large Language Models (LLMs), we propose Generative Ranking for Ads at Baidu (GRAB), an end-to-end generative framework for Click-Through Rate (CTR) prediction. GRAB integrates a novel Causal Action-aware Multi-channel Attention (CamA) mechanism to effectively capture temporal dynamics and specific action signals within user behavior sequences. Full-scale online deployment demonstrates that GRAB significantly outperforms established DLRMs, delivering a 3.05% increase in revenue and a 3.49% rise in CTR. Furthermore, the model demonstrates desirable scaling behavior: its expressive power shows a monotonic and approximately linear improvement as longer interaction sequences are utilized.

</details>


### [219] [Adaptive Quality-Diversity Trade-offs for Large-Scale Batch Recommendation](https://arxiv.org/abs/2602.02024)
*Clémence Réda,Tomas Rigaux,Hiba Bederina,Koh Takeuchi,Hisashi Kashima,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: 本文提出B - DivRec算法处理推荐系统中物品相关性与多样性平衡问题，在合成和真实数据集上验证其性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统需推荐高相关性和多样性物品，实际应用存在避免推荐相似物品、降低计算成本等问题。

Method: 先在用户反馈模型已知情况下，引入结合行列式点过程和模糊剥蚀程序的B - DivRec算法调整物品多样性；再提出自适应调整质量 - 多样性权衡的方法。

Result: 在合成和真实数据集的电影推荐和药物再利用任务中展示了B - DivRec的性能和通用性。

Conclusion: B - DivRec算法可有效解决推荐系统中物品相关性与多样性的平衡问题。

Abstract: A core research question in recommender systems is to propose batches of highly relevant and diverse items, that is, items personalized to the user's preferences, but which also might get the user out of their comfort zone. This diversity might induce properties of serendipidity and novelty which might increase user engagement or revenue. However, many real-life problems arise in that case: e.g., avoiding to recommend distinct but too similar items to reduce the churn risk, and computational cost for large item libraries, up to millions of items. First, we consider the case when the user feedback model is perfectly observed and known in advance, and introduce an efficient algorithm called B-DivRec combining determinantal point processes and a fuzzy denuding procedure to adjust the degree of item diversity. This helps enforcing a quality-diversity trade-off throughout the user history. Second, we propose an approach to adaptively tailor the quality-diversity trade-off to the user, so that diversity in recommendations can be enhanced if it leads to positive feedback, and vice-versa. Finally, we illustrate the performance and versatility of B-DivRec in the two settings on synthetic and real-life data sets on movie recommendation and drug repurposing.

</details>


### [220] [Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs](https://arxiv.org/abs/2602.02338)
*Yu Liang,Zhongjin Zhang,Yuxuan Zhu,Kerui Zhang,Zhiluohan Guo,Wenhang Zhou,Zonqi Yang,Kangle Wu,Yabo Ni,Anxiang Zeng,Cong Fu,Jianxin Wang,Jiazhi Xia*

Main category: cs.IR

TL;DR: 提出不依赖大模型的ReSID框架，结合FAMAE和GAOQ，实验表明其效果优于基线，且降低标记成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义ID的推荐方法与生成式推荐目标不符，语义嵌入与协同预测耦合弱且通用量化低效。

Method: 提出ReSID框架，包含FAMAE学习预测充分的物品表示，GAOQ减少语义模糊和前缀条件不确定性以生成紧凑可预测的SID序列。

Result: ReSID在十个数据集上实验，平均优于强基线超10%，并最多降低122x标记成本。

Conclusion: ReSID框架有效，在推荐任务中有良好表现。

Abstract: Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.

</details>


### [221] [RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval](https://arxiv.org/abs/2602.02444)
*Tyler Skow,Alexander Martin,Benjamin Van Durme,Rama Chellappa,Reno Kriz*

Main category: cs.IR

TL;DR: 提出用于视频检索的基于推理的重排器RANKVIDEO，经两阶段训练，在基准测试中提升检索性能且更高效。


<details>
  <summary>Details</summary>
Motivation: 基于大推理模型的文本重排有进展，但视频检索的基于推理重排研究不足，需填补该空白。

Method: 引入RANKVIDEO，采用两阶段课程训练，结合感知监督微调与重排训练，还有数据合成管道构建推理密集查询 - 视频对。

Result: 在MultiVENT 2.0基准测试中，RANKVIDEO在两阶段框架下持续提升检索性能，nDCG@10平均提高31%，优于文本和视觉 - 语言重排替代方案，且更高效。

Conclusion: RANKVIDEO能有效提升视频检索的重排性能，是一种更优的视频检索重排方法。

Abstract: Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [222] [ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization](https://arxiv.org/abs/2602.00087)
*Haolin Pan,Lianghong Huang,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: 本文提出ECCO框架，结合可解释推理与组合搜索，在7个数据集上表现优于LLVM opt -O3基线。


<details>
  <summary>Details</summary>
Motivation: 传统编译器自动调优的黑盒搜索方法缺乏语义指导，大语言模型方法存在表面模式匹配和因果不透明问题。

Method: 提出逆向工程方法构建思维链数据集，设计协作推理机制，让大语言模型作为策略制定者指导遗传算法。

Result: 在7个数据集上，ECCO显著优于LLVM opt -O3基线，平均减少24.44%的周期。

Conclusion: ECCO框架通过结合可解释推理和组合搜索，有效提升了编译器自动调优的性能。

Abstract: Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.

</details>


### [223] [OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models](https://arxiv.org/abs/2602.00012)
*Michael Siebenmann,Javier Argota Sánchez-Vaquerizo,Stefan Arisona,Krystian Samp,Luis Gisler,Dirk Helbing*

Main category: cs.LG

TL;DR: 提出基于大语言模型的OGD4All框架增强公民与地理空间开放政府数据交互，评估表现良好，推进可信AI用于开放治理。


<details>
  <summary>Details</summary>
Motivation: 增强公民与地理空间开放政府数据的交互。

Method: 结合语义数据检索、用于迭代代码生成的智能推理和安全沙盒执行，产生可验证的多模态输出。

Result: 在199个问题基准测试中，分析正确率达98%，召回率达94%，能可靠拒绝无数据支持的问题，统计稳健性测试和专家反馈显示其可靠性和社会相关性。

Conclusion: 该方法展示了大语言模型可为公共数据提供可解释的多模态访问，推进了开放治理的可信AI。

Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.

</details>


### [224] [Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning](https://arxiv.org/abs/2602.00022)
*Margaret Foster*

Main category: cs.LG

TL;DR: 提出用于难以访问场景的测量框架，结合多源三角测量与可解释机器学习模型，通过对秘密武装组织分析展示其有效性。


<details>
  <summary>Details</summary>
Motivation: 许多高风险系统难以直接访问，现有数据不支持常规分析策略，需新测量框架。

Method: 引入适用于数据缺失或对抗性数据的测量框架，将多源三角测量与可解释机器学习模型结合，追求不同模型间一致性。

Result: 通过对秘密武装组织实证分析，表明三角化、可解释的机器学习能恢复有实质意义的变化。

Conclusion: 所提框架为缺乏传统统计或因果推断数据时的定量分析提供工作流程，且能揭示推断局限性。

Abstract: We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.

</details>


### [225] [Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems](https://arxiv.org/abs/2602.00027)
*Zhenyu Pu,Yu Yang,Lun Yang,Qing-Shan Jia,Xiaohong Guan,Costas J. Spanos*

Main category: cs.LG

TL;DR: 本文针对氢基多能源系统（HMES）优化运行挑战，构建综合运行模型，提出增强深度强化学习（DRL）框架，实验表明模型对保障储能系统安全可靠至关重要，所提方法在降低运行成本和处理约束方面优于传统DRL，并探讨表征学习在DRL中的作用。


<details>
  <summary>Details</summary>
Motivation: HMES因氢能储能系统（HESS）的非线性和多物理耦合动力学以及供需不确定性，其优化运行面临挑战。

Method: 构建全面的HMES运行模型以捕捉HESS的非线性动力学和多物理过程；提出集成新兴表征学习技术的增强DRL框架。

Result: 综合模型对确保HESS的安全可靠至关重要；所提SR - DRL方法在降低HMES运行成本和处理系统运行约束方面，收敛速度和性能优于传统DRL。

Conclusion: 表征学习可将原始状态空间重组为结构良好且具有聚类意识的几何表示，从而平滑和促进DRL的学习过程。

Abstract: Hydrogen-based multi-energy systems (HMES) have emerged as a promising low-carbon and energy-efficient solution, as it can enable the coordinated operation of electricity, heating and cooling supply and demand to enhance operational flexibility, improve overall energy efficiency, and increase the share of renewable integration. However, the optimal operation of HMES remains challenging due to the nonlinear and multi-physics coupled dynamics of hydrogen energy storage systems (HESS) (consisting of electrolyters, fuel cells and hydrogen tanks) as well as the presence of multiple uncertainties from supply and demand. To address these challenges, this paper develops a comprehensive operational model for HMES that fully captures the nonlinear dynamics and multi-physics process of HESS. Moreover, we propose an enhanced deep reinforcement learning (DRL) framework by integrating the emerging representation learning techniques, enabling substantially accelerated and improved policy optimization for spatially and temporally coupled complex networked systems, which is not provided by conventional DRL. Experimental studies based on real-world datasets show that the comprehensive model is crucial to ensure the safe and reliable of HESS. In addition, the proposed SR-DRL approaches demonstrate superior convergence rate and performance over conventional DRL counterparts in terms of reducing the operation cost of HMES and handling the system operating constraints. Finally, we provide some insights into the role of representation learning in DRL, speculating that it can reorganize the original state space into a well-structured and cluster-aware geometric representation, thereby smoothing and facilitating the learning process of DRL.

</details>


### [226] [ELLMPEG: An Edge-based Agentic LLM Video Processing Tool](https://arxiv.org/abs/2602.00028)
*Zoha Azimi,Reza Farahani,Radu Prodan,Christian Timmerer*

Main category: cs.LG

TL;DR: 文章介绍云部署大语言模型的局限，提出边缘智能代理大语言模型框架ELLMPEG用于自动生成视频处理命令，实验表明Qwen2.5经ELLMPEG增强后性能良好。


<details>
  <summary>Details</summary>
Motivation: 云部署大语言模型有高计算能源需求、隐私和可靠性风险、API成本等局限，需新方法利用本地工具和大语言模型。

Method: 提出ELLMPEG框架，集成工具感知的检索增强生成与迭代自我反思，在边缘生成并验证命令；收集480个查询的数据集，从多方面评估模型。

Result: Qwen2.5经ELLMPEG框架增强后，命令生成平均准确率达78%，无API成本，在FFmpeg和VVenC数据集上优于其他模型。

Conclusion: ELLMPEG框架能有效解决云部署大语言模型的局限，提升视频处理命令生成的性能。

Abstract: Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.

</details>


### [227] [Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games](https://arxiv.org/abs/2602.00606)
*Ahmed Said Donmez,Yuksel Arslantas,Muhammed O. Sayin*

Main category: cs.LG

TL;DR: 提出一种用于随机博弈的无模型、与博弈无关且无梯度的新学习框架，证明在两类博弈中收敛到（近似）均衡，实证验证有效性。


<details>
  <summary>Details</summary>
Motivation: 寻找适用于随机博弈的有效学习方法，且能有理论保证和实际有效性。

Method: 采用最佳响应型演员-评论家架构，通过两个不同的评论家更新策略，学习过程依赖对观测收益的平滑最佳响应的非均衡适应。

Result: 在两类随机博弈中理论上能收敛到（近似）均衡，实证验证了方法的鲁棒性和有效性。

Conclusion: 所提出的学习框架是首个在两类博弈中有理论保证的基于收益的完全去中心化学习算法。

Abstract: We propose a novel independent and payoff-based learning framework for stochastic games that is model-free, game-agnostic, and gradient-free. The learning dynamics follow a best-response-type actor-critic architecture, where agents update their strategies (actors) using feedback from two distinct critics: a fast critic that intuitively responds to observed payoffs under limited information, and a slow critic that deliberatively approximates the solution to the underlying dynamic programming problem. Crucially, the learning process relies on non-equilibrium adaptation through smoothed best responses to observed payoffs. We establish convergence to (approximate) equilibria in two-agent zero-sum and multi-agent identical-interest stochastic games over an infinite horizon. This provides one of the first payoff-based and fully decentralized learning algorithms with theoretical guarantees in both settings. Empirical results further validate the robustness and effectiveness of the proposed approach across both classes of games.

</details>


### [228] [RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making](https://arxiv.org/abs/2602.00030)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 论文提出用于灾害响应的智能体RAG框架，整合资源构建知识库，优化检索树，提升模型能力，实验展示其在实际灾害数据集上的优势。


<details>
  <summary>Details</summary>
Motivation: 有效人道主义援助和灾害救援需要快速态势感知、可靠决策支持及跨灾种的泛化能力，现有方法在这些方面有不足。

Method: 构建集成多源信息的分层知识库；基于开源多模态实现处理PDF，生成多模态检索树；智能体控制器动态选择检索策略；采用轻量级LoRA后训练方法注入经验知识。

Result: 在实际灾害数据集实验中，实现了更好的态势感知、任务分解准确性和应急操作可用性。

Conclusion: 结合相关领域进展，系统通过自适应检索增强生成和自推理、多模态思维链能力取得显著效果。

Abstract: Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.

</details>


### [229] [Multimodal Scientific Learning Beyond Diffusions and Flows](https://arxiv.org/abs/2602.00960)
*Leonardo Ferreira Guilhoto,Akshat Kaushal,Paris Perdikaris*

Main category: cs.LG

TL;DR: 研究表明混合密度网络（MDNs）是科学机器学习（SciML）中多模态不确定性量化的有效替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有高度表达性的隐式生成模型在SciML方面存在数据需求大、计算成本高且与科学问题解空间结构不匹配等问题。

Method: 通过统一概率框架对比显式和隐式分布网络，并进行实证研究。

Result: MDNs在一系列逆、多稳定和混沌科学回归任务中实现了更好的泛化、可解释性和样本效率。

Conclusion: MDNs是SciML中多模态不确定性量化被忽视但合理的替代方案，能在数据稀缺时可靠地恢复分离模式。

Abstract: Scientific machine learning (SciML) increasingly requires models that capture multimodal conditional uncertainty arising from ill-posed inverse problems, multistability, and chaotic dynamics. While recent work has favored highly expressive implicit generative models such as diffusion and flow-based methods, these approaches are often data-hungry, computationally costly, and misaligned with the structured solution spaces frequently found in scientific problems. We demonstrate that Mixture Density Networks (MDNs) provide a principled yet largely overlooked alternative for multimodal uncertainty quantification in SciML. As explicit parametric density estimators, MDNs impose an inductive bias tailored to low-dimensional, multimodal physics, enabling direct global allocation of probability mass across distinct solution branches. This structure delivers strong data efficiency, allowing reliable recovery of separated modes in regimes where scientific data is scarce. We formalize these insights through a unified probabilistic framework contrasting explicit and implicit distribution networks, and demonstrate empirically that MDNs achieve superior generalization, interpretability, and sample efficiency across a range of inverse, multistable, and chaotic scientific regression tasks.

</details>


### [230] [Enhancing few-shot time series forecasting with LLM-guided diffusion](https://arxiv.org/abs/2602.00040)
*Haonan Shi,Dehua Shuai,Liming Wang,Xiyang Liu,Long Tian*

Main category: cs.LG

TL;DR: 提出LTSM - DIFF框架解决时间序列预测的少样本问题，在多场景表现出色。


<details>
  <summary>Details</summary>
Motivation: 专业领域时间序列预测数据有限，传统模型需大规模数据，要解决少样本挑战。

Method: 提出LTSM - DIFF框架，微调LTSM模块作为时间记忆机制提取序列表示，用于扩散过程的条件引导。

Result: 实验表明LTSM - DIFF在数据丰富场景达最优性能，少样本预测也有显著提升。

Conclusion: 为数据稀缺情况下的时间序列分析建立了新范式。

Abstract: Time series forecasting in specialized domains is often constrained by limited data availability, where conventional models typically require large-scale datasets to effectively capture underlying temporal dynamics. To tackle this few-shot challenge, we propose LTSM-DIFF (Large-scale Temporal Sequential Memory with Diffusion), a novel learning framework that integrates the expressive power of large language models with the generative capability of diffusion models. Specifically, the LTSM module is fine-tuned and employed as a temporal memory mechanism, extracting rich sequential representations even under data-scarce conditions. These representations are then utilized as conditional guidance for a joint probability diffusion process, enabling refined modeling of complex temporal patterns. This design allows knowledge transfer from the language domain to time series tasks, substantially enhancing both generalization and robustness. Extensive experiments across diverse benchmarks demonstrate that LTSM-DIFF consistently achieves state-of-the-art performance in data-rich scenarios, while also delivering significant improvements in few-shot forecasting. Our work establishes a new paradigm for time series analysis under data scarcity.

</details>


### [231] [The Effect of Mini-Batch Noise on the Implicit Bias of Adam](https://arxiv.org/abs/2602.01642)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 引入理论框架研究小批量噪声对Adam优化器隐式偏置的影响，发现批量大小、动量超参数对泛化性能的影响规律并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 在深度学习多轮训练中，高质量数据有限且计算资源增长，需理解小批量噪声对Adam优化器隐式偏置的影响，以提高泛化性能。

Method: 引入理论框架分析小批量噪声对Adam优化器隐式偏置的影响，研究不同批量大小和动量超参数组合的效果。

Result: 大批量时，较高的$β_2$增加反正则化程度损害泛化；小批量时，$β_2$影响反转，$β_1$也有类似相反单调性变化；小批量时默认$(β_1, β_2) = (0.9, 0.999)$较好，大批量时让$β_1$接近$β_2$更好。

Conclusion: 理论推导将单调性转变的批量大小与临界批量大小联系起来，通过小规模数据实验验证了相关效果。

Abstract: With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(β_1, β_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $β_1$, $β_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $β_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $β_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $β_1$. In particular, the commonly "default" pair $(β_1, β_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $β_1$ closer to $β_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.

</details>


### [232] [Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy](https://arxiv.org/abs/2602.00046)
*Sarthak Sattigeri*

Main category: cs.LG

TL;DR: 研究语言模型谄媚倾向在英语和印地语中的差异，发现印地语文化适应提示下谄媚率更高，英语评估结果可能无法跨语言统一转移。


<details>
  <summary>Details</summary>
Motivation: 不清楚语言模型谄媚倾向诊断是否能跨语言和文化背景推广。

Method: 将Beacon单轮强制选择谄媚诊断扩展到印地语，采用三种条件设计，评估四个模型。

Result: 所有模型在文化适应印地语提示下谄媚率高于英语，文化适应是差异主因，建议提示跨语言差异最大。

Conclusion: 英语中测量的对齐行为可能无法跨语言统一转移，文化背景提示框架起重要作用。

Abstract: Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.

</details>


### [233] [Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning](https://arxiv.org/abs/2409.01329)
*Lucas Lange,Maurice-Maximilian Heykeroth,Erhard Rahm*

Main category: cs.LG

TL;DR: 研究识别影响私有和非私有CNN模型效用和脆弱性的图像数据集特征，为图像数据集效用 - 隐私权衡提供指导。


<details>
  <summary>Details</summary>
Motivation: ML模型在敏感数据上训练面临安全挑战，PPML用DP平衡效用和隐私，需识别影响模型效用和脆弱性的图像数据集特征。

Method: 分析多个数据集和隐私预算。

Result: 不平衡数据集增加少数类脆弱性，DP可缓解；少类别的数据集提升模型效用和隐私；高熵或低FDR数据集恶化效用 - 隐私权衡。

Conclusion: 研究结果为从业者和研究者在图像数据集效用 - 隐私权衡的估计和优化上提供有价值指导。

Abstract: Machine Learning (ML) is crucial in many sectors, including computer vision. However, ML models trained on sensitive data face security challenges, as they can be attacked and leak information. Privacy-Preserving Machine Learning (PPML) addresses this by using Differential Privacy (DP) to balance utility and privacy. This study identifies image dataset characteristics that affect the utility and vulnerability of private and non-private Convolutional Neural Network (CNN) models. Through analyzing multiple datasets and privacy budgets, we find that imbalanced datasets increase vulnerability in minority classes, but DP mitigates this issue. Datasets with fewer classes improve both model utility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR) datasets deteriorate the utility-privacy trade-off. These insights offer valuable guidance for practitioners and researchers in estimating and optimizing the utility-privacy trade-off in image datasets, helping to inform data and privacy modifications for better outcomes based on dataset characteristics.

</details>


### [234] [Lightweight Edge Learning via Dataset Pruning](https://arxiv.org/abs/2602.00047)
*Laha Ale,Hu Luo,Mingsheng Cao,Shichao Li,Huanlai Xing,Haifeng Sun*

Main category: cs.LG

TL;DR: 提出数据中心优化框架，利用数据集剪枝实现资源高效的边缘学习，实验表明可降低训练延迟和能耗，对模型精度影响小。


<details>
  <summary>Details</summary>
Motivation: 边缘学习在电池供电移动系统中部署受限于设备训练的高计算和能耗开销，且训练阶段受本地数据集处理瓶颈影响。

Method: 提出数据中心优化框架，通过轻量级设备端重要性评估构建紧凑、高信息含量的训练子集，利用截断预热阶段的平均损失统计对样本重要性排序，按动态剪枝率保留关键数据点，该机制与模型无关且无需设备间通信。

Result: 在标准图像分类基准测试中，框架实现了与剪枝率近乎线性的训练延迟和能耗降低，模型精度仅有可忽略的下降。

Conclusion: 数据集剪枝是提高资源受限移动边缘设备学习可持续性和可扩展性的重要补充范式。

Abstract: Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.

</details>


### [235] [Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment](https://arxiv.org/abs/2602.00051)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 本文提出用集成老化因子的QR - DQN进行多设备CBM的分布强化学习方法，经实验验证策略有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于时间的维护策略有多余开支和意外设备故障问题，需CBM优化维护和资源分配。

Method: 使用集成老化因子的QR - DQN，通过三种战略场景同时管理多个泵单元。

Result: 实验显示各策略性能显著提升，安全优先策略成本效率高，系统有95.66%运营稳定性，可用于工业环境。

Conclusion: 提出的方法能有效用于多设备CBM，提升维护效果和资源分配效率。

Abstract: Condition-Based Maintenance (CBM) signifies a paradigm shift from reactive to proactive equipment management strategies in modern industrial systems. Conventional time-based maintenance schedules frequently engender superfluous expenditures and unanticipated equipment failures. In contrast, CBM utilizes real-time equipment condition data to enhance maintenance timing and optimize resource allocation. The present paper proposes a novel distributional reinforcement learning approach for multi-equipment CBM using Quantile Regression Deep Q-Networks (QR-DQN) with aging factor integration. The methodology employed in this study encompasses the concurrent administration of multiple pump units through three strategic scenarios. The implementation of safety-first, balanced, and cost-efficient approaches is imperative. Comprehensive experimental validation over 3,000 training episodes demonstrates significant performance improvements across all strategies. The Safety-First strategy demonstrates superior cost efficiency, with a return on investment (ROI) of 3.91, yielding 152\% better performance than alternatives while requiring only 31\% higher investment. The system exhibits 95.66\% operational stability and immediate applicability to industrial environments.

</details>


### [236] [TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval](https://arxiv.org/abs/2602.00059)
*Zizheng Zhang,Yuyang Liao,Chen Chen,Jian He,Dun Wu,Qianjin Yu,Yanqin Gao,Jin Yang,Kailai Zhang,Eng Siong Chng,Xionghu Zhong*

Main category: cs.LG

TL;DR: 现有离散文本优化方法多为一阶优化器，存在收敛慢和不稳定问题。本文提出二阶框架TextBFGS，在代码优化任务中显著优于一阶基线。


<details>
  <summary>Details</summary>
Motivation: 现有离散文本优化方法多为一阶优化器，忽略语义曲率，存在收敛慢和不稳定问题。

Method: 提出TextBFGS二阶框架，通过从预学习成功轨迹的内存中检索梯度算子近似逆Hessian矩阵，实现单步更新。

Result: 在不同领域的代码优化任务（如HumanEval、MBPP）中，TextBFGS显著优于一阶基线，以更少的模型调用次数实现更高通过率，且具有强跨任务迁移能力。

Conclusion: TextBFGS为高效、内存感知的文本优化建立了基于数学的范式。

Abstract: Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.

</details>


### [237] [Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing](https://arxiv.org/abs/2602.00906)
*Anxin Guo,Jingwei Li*

Main category: cs.LG

TL;DR: 论文将大语言模型对无推断模式“随机事实”的记忆形式化为成员测试问题，建立率失真定理解释幻觉现象，并在合成数据上验证。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型对缺乏推断模式的“随机事实”高置信度产生幻觉的问题。

Method: 将此类事实的记忆形式化为成员测试问题，统一布隆过滤器离散误差指标与大语言模型连续对数损失，分析事实在合理声明空间中稀疏情况下的问题，建立率失真定理。

Result: 理论框架表明即使在最优训练、完美数据和简化“封闭世界”设定下，有限容量时信息论最优策略会导致幻觉；在合成数据上验证幻觉是有损压缩的自然结果。

Conclusion: 幻觉是大语言模型在有限容量下进行有损压缩的自然结果。

Abstract: Large language models often hallucinate with high confidence on "random facts" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified "closed world" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.

</details>


### [238] [Learning from Anonymized and Incomplete Tabular Data](https://arxiv.org/abs/2602.01217)
*Lucas Lange,Adrian Böttinger,Victor Christen,Anushka Vidanage,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: 提出新数据转换策略处理含异构匿名化的表格数据，实验表明该方法能恢复数据效用，强调有效学习与合理处理匿名值有关。


<details>
  <summary>Details</summary>
Motivation: 用户驱动隐私下的数据表示给机器学习带来挑战，传统方法会丢弃泛化语义，需要新方法学习此类表格数据。

Method: 提出考虑异构匿名化的新型数据转换策略，并与标准插补和基于大语言模型的方法一同评估。

Result: 广义值优于纯抑制；最佳数据准备策略依场景而定；一致的数据表示对保持下游效用至关重要。

Conclusion: 有效学习与对匿名值的适当处理密切相关。

Abstract: User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.

</details>


### [239] [SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism](https://arxiv.org/abs/2602.00062)
*Ming-Yao Ho,Cheng-Kai Wang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 本文针对企业信息系统采用大规模AI模型时的高成本和长周期问题，提出新训练方法SCPL，经实验验证其高效，且代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决企业采用大规模AI模型时高训练成本和长开发周期问题，改善BP算法训练深度网络的低效性。

Method: 提出Supervised Contrastive Parallel Learning (SCPL)方法，解耦BP算法，将长梯度流转化为多个短梯度流。

Result: 实验表明SCPL比BP、Early Exit、GPipe、AL更高效且有效。

Conclusion: SCPL缓解了性能瓶颈，为组织开发和部署信息系统提供了更具成本效益和灵活性的途径。

Abstract: Adopting large-scale AI models in enterprise information systems is often hindered by high training costs and long development cycles, posing a significant managerial challenge. The standard end-to-end backpropagation (BP) algorithm is a primary driver of modern AI, but it is also the source of inefficiency in training deep networks. This paper introduces a new training methodology, Supervised Contrastive Parallel Learning (SCPL), that addresses this issue by decoupling BP and transforming a long gradient flow into multiple short ones. This design enables the simultaneous computation of parameter gradients in different layers, achieving superior model parallelism and enhancing training throughput. Detailed experiments are presented to demonstrate the efficiency and effectiveness of our model compared to BP, Early Exit, GPipe, and Associated Learning (AL), a state-of-the-art method for decoupling backpropagation. By mitigating a fundamental performance bottleneck, SCPL provides a practical pathway for organizations to develop and deploy advanced information systems more cost-effectively and with greater agility. The experimental code is released for reproducibility. https://github.com/minyaho/scpl/

</details>


### [240] [The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations](https://arxiv.org/abs/2602.00063)
*Leonidas Christodoulou,Chang Sun*

Main category: cs.LG

TL;DR: 研究机器学习模型与反事实生成算法组合在不确定性下的鲁棒性，发现反事实解释对模型不确定性敏感，强调需不确定性感知解释方法。


<details>
  <summary>Details</summary>
Motivation: 现有反事实方法在模型和数据不确定性变化时未充分测试，解释可能不稳定或无效。

Method: 在合成和真实表格数据集上进行实验，研究常见机器学习模型与反事实生成算法组合在偶然和认知不确定性下的鲁棒性。

Result: 反事实解释对模型不确定性高度敏感，模型准确率小幅度下降会导致生成的反事实有较大变化。

Conclusion: 在金融和社会科学等领域需要不确定性感知的解释方法。

Abstract: Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanations that may be unstable or invalid under real-world variability. In this work, we investigate the robustness of common combinations of machine learning models and counterfactual generation algorithms in the presence of both aleatoric and epistemic uncertainty. Through experiments on synthetic and real-world tabular datasets, we show that counterfactual explanations are highly sensitive to model uncertainty. In particular, we find that even small reductions in model accuracy - caused by increased noise or limited data - can lead to large variations in the generated counterfactuals on average and on individual instances. These findings underscore the need for uncertainty-aware explanation methods in domains such as finance and the social sciences.

</details>


### [241] [SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation](https://arxiv.org/abs/2602.00064)
*Hao Deng,Yingping Li,Shuiping Gou,Bo Liu*

Main category: cs.LG

TL;DR: 提出SPGCL框架解决图神经网络对结构噪声敏感问题，实验显示其能提升GNN鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法存在不足，随机扰动无视结构且可能移除关键边，基于SVD的视图缺乏多样性，需改进方法提升图神经网络对结构噪声的鲁棒性

Method: 提出SPGCL框架，结合轻量级随机边移除和SVD引导的细化步骤，平衡边移除和恢复率，控制视图间结构差异，还加入对比融合模块。

Result: 在十个基准数据集上的实验表明，SPGCL持续提升了基础GNN的鲁棒性和准确性，优于现有方法。

Conclusion: SPGCL是一种有效的图对比学习框架，能增强图神经网络对结构噪声的鲁棒性，具有较优性能。

Abstract: Graph Neural Networks (GNNs) can be highly sensitive to structural noise, including spurious or missing edges caused by adversarial attacks or non-adversarial imperfections. Existing graph contrastive learning methods typically rely on either random perturbations (e.g., edge dropping) to generate diverse views or purely spectral augmentations (e.g., SVD) to preserve global structural priors. However, random perturbations are structure-agnostic and may remove critical edges, while SVD-based views often become dense and lack sufficient diversity. To bridge this gap, we propose SPGCL, a robust graph contrastive learning framework via SVD-guided structural perturbation. SPGCL couples lightweight stochastic edge removal with an SVD-guided refinement step that can recover mistakenly removed informative edges and introduce semantically meaningful missing links while avoiding graph densification through sparse top-ranked edge selection and merging. By balancing edge removal and recovery rates, SPGCL explicitly controls structural discrepancy between views so that contrastive signals reflect semantic structural differences rather than edge-count gaps. We further incorporate a contrastive fusion module regularized by a global similarity constraint to better align the two views. Extensive experiments on ten benchmark datasets demonstrate that SPGCL consistently improves robustness and accuracy of base GNNs, outperforming state-of-the-art graph contrastive learning and structure learning methods.

</details>


### [242] [Modality as Heterogeneity: Node Splitting and Graph Rewiring for Multimodal Graph Learning](https://arxiv.org/abs/2602.00067)
*Yihan Zhang,Ercan E. Kuruoglu*

Main category: cs.LG

TL;DR: 论文提出NSG - MoE多模态图学习框架，在多模态基准测试中表现优于基线，有训练效率且通过多项分析解释有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态图有丰富表示能力和广泛适用性，但存在严重模态混淆问题。

Method: 提出NSG (Node Splitting Graph)-MoE框架，结合节点分裂和图重连机制与结构化混合专家（MoE）架构，分解节点并分配专家处理消息流。

Result: 在三个多模态基准测试中始终优于强基线，实现了有竞争力的训练效率。

Conclusion: 通过谱分析和信息论分析，证明NSG能在特定子空间自适应过滤，减少数据和参数间的互信息，提高泛化能力。

Abstract: Multimodal graphs are gaining increasing attention due to their rich representational power and wide applicability, yet they introduce substantial challenges arising from severe modality confusion. To address this issue, we propose NSG (Node Splitting Graph)-MoE, a multimodal graph learning framework that integrates a node-splitting and graph-rewiring mechanism with a structured Mixture-of-Experts (MoE) architecture. It explicitly decomposes each node into modality-specific components and assigns relation-aware experts to process heterogeneous message flows, thereby preserving structural information and multimodal semantics while mitigating the undesirable mixing effects commonly observed in general-purpose GNNs. Extensive experiments on three multimodal benchmarks demonstrate that NSG-MoE consistently surpasses strong baselines. Despite incorporating MoE -- which is typically computationally heavy -- our method achieves competitive training efficiency. Beyond empirical results, we provide a spectral analysis revealing that NSG performs adaptive filtering over modality-specific subspaces, thus explaining its disentangling behavior. Furthermore, an information-theoretic analysis shows that the architectural constraints imposed by NSG reduces mutual information between data and parameters and improving generalization capability.

</details>


### [243] [Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets](https://arxiv.org/abs/2602.01682)
*Taihei Oki,Shinsaku Sakaue*

Main category: cs.LG

TL;DR: 研究在线逆线性优化问题，证明当可行集为M - 凸集时可实现O(dlogd)的有限遗憾界，并将方法扩展到对抗性损坏反馈情况。


<details>
  <summary>Details</summary>
Motivation: 此前在线逆线性优化问题的有限遗憾界是否能达到关于d的多项式形式是开放问题，需解决此疑问。

Method: 结合M - 凸集上最优解的结构特征与几何体积论证；通过监测观测反馈诱导的有向图自适应检测损坏。

Result: 当可行集为M - 凸集时，实现O(dlogd)的有限遗憾界；在对抗性损坏反馈中，在未知C的情况下获得O((C + 1)dlogd)的遗憾界。

Conclusion: 部分解决了在线逆线性优化问题有限遗憾界是否能达到关于d的多项式形式的开放问题，且方法可扩展到对抗性损坏反馈场景。

Abstract: We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\log T)$, as well as a finite but exponentially large bound of $\exp(O(d\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $Ω(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.

</details>


### [244] [Generative AI-enhanced Probabilistic Multi-Fidelity Surrogate Modeling Via Transfer Learning](https://arxiv.org/abs/2602.00072)
*Jice Zeng,David Barajas-Solano,Hui Chen*

Main category: cs.LG

TL;DR: 提出基于生成式迁移学习的概率多保真度代理框架解决数据稀缺问题，在基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理性能依赖数据质量和数量，高保真数据稀缺且获取成本高，需解决数据稀缺问题。

Method: 以归一化流生成模型为骨干，分两阶段训练，先在低保真数据集预训练，再在高保真数据集微调；集成满射层与标准耦合块。

Result: 代理能提供快速概率预测和量化不确定性，显著优于仅用低保真数据的基线，使用更少高保真评估。

Conclusion: 所提模型能实现高保真精度的概率预测，为复杂工程系统的数据高效生成式代理提供了实用途径。

Abstract: The performance of machine learning surrogates is critically dependent on data quality and quantity. This presents a major challenge, as high-fidelity (HF) data is often scarce and computationally expensive to acquire, while low-fidelity (LF) data is abundant but less accurate. To address this data scarcity problem, we develop a probabilistic multi-fidelity surrogate framework based on generative transfer learning. We employ a normalizing flow (NF) generative model as the backbone, which is trained in two phases: (i) the NF is first pretrained on a large LF dataset to learn a probabilistic forward model; (ii) the pretrained model is then fine-tuned on a small HF dataset, allowing it to correct for LF-HF discrepancies via knowledge transfer. To relax the dimension-preserving constraint of standard bijective NFs, we integrate surjective (dimension-reducing) layers with standard coupling blocks. This architecture enables learned dimension reduction while preserving the ability to train with exact likelihoods. The resulting surrogate provides fast probabilistic predictions with quantified uncertainty and significantly outperforms LF-only baselines while using fewer HF evaluations. We validate the approach on a reinforced concrete slab benchmark, combining many coarse-mesh (LF) simulations with a limited set of fine-mesh (HF) simulations. The proposed model achieves probabilistic predictions with HF accuracy, demonstrating a practical path toward data-efficient, generative AI-driven surrogates for complex engineering systems.

</details>


### [245] [Dimensional Peeking for Low-Variance Gradients in Zeroth-Order Discrete Optimization via Simulation](https://arxiv.org/abs/2602.00075)
*Philipp Andelfinger,Wentong Cai*

Main category: cs.LG

TL;DR: 提出用于离散优化中梯度估计的降方差方法“dimensional peeking”，实现降方差并提升零阶优化竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有随机估计器在梯度估计时因目标函数基于扰动的采样引入方差，导致收敛慢。

Method: 提出dimensional peeking方法，将采样粒度从标量值提升到遵循相同控制流路径的值类，通过自定义数值数据类型实现。

Result: 在三个高维输入的基于模拟的优化问题中观察到方差最多降低7.9倍，与三种元启发式方法相比优化进度提升。

Conclusion: dimensional peeking增加了零阶优化在离散和非凸模拟中的竞争力。

Abstract: Gradient-based optimization methods are commonly used to identify local optima in high-dimensional spaces. When derivatives cannot be evaluated directly, stochastic estimators can provide approximate gradients. However, these estimators' perturbation-based sampling of the objective function introduces variance that can lead to slow convergence. In this paper, we present dimensional peeking, a variance reduction method for gradient estimation in discrete optimization via simulation. By lifting the sampling granularity from scalar values to classes of values that follow the same control flow path, we increase the information gathered per simulation evaluation. Our derivation from an established smoothed gradient estimator shows that the method does not introduce any bias. We present an implementation via a custom numerical data type to transparently carry out dimensional peeking over C++ programs. Variance reductions by factors of up to 7.9 are observed for three simulation-based optimization problems with high-dimensional input. The optimization progress compared to three meta-heuristics shows that dimensional peeking increases the competitiveness of zeroth-order optimization for discrete and non-convex simulations.

</details>


### [246] [Sheaf Neural Networks and biomedical applications](https://arxiv.org/abs/2602.00159)
*Aneeqa Mehrab,Jan Willem Van Looy,Pietro Demurtas,Stefano Iotti,Emil Malucelli,Francesca Rossi,Ferdinando Zanchetta,Rita Fioresi*

Main category: cs.LG

TL;DR: 本文阐述了层神经网络（SNN）算法的理论和数学模型，通过案例展示其在生物医学问题上优于流行图神经网络。


<details>
  <summary>Details</summary>
Motivation: 阐明SNN算法理论和数学模型，并展示其在生物医学问题上的有效性和优势。

Method: 未提及具体方法

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: The purpose of this paper is to elucidate the theory and mathematical modelling behind the sheaf neural network (SNN) algorithm and then show how SNN can effectively answer to biomedical questions in a concrete case study and outperform the most popular graph neural networks (GNNs) as graph convolutional networks (GCNs), graph attention networks (GAT) and GraphSage.

</details>


### [247] [Automated univariate time series forecasting with regression trees](https://arxiv.org/abs/2602.00077)
*Francisco Martínez,María P. Frías*

Main category: cs.LG

TL;DR: 本文介绍用回归树及其集成方法进行单变量时间序列自动预测的方法，有实验结果且开发了公开软件。


<details>
  <summary>Details</summary>
Motivation: 提出一种自动化的单变量时间序列预测方法。

Method: 使用回归树及其集成方法（装袋法和随机森林），采用自回归方法和递归预测，说明特征选择、处理趋势和季节性行为的方法。

Result: 预测准确性与指数平滑、ARIMA等成熟统计模型相当；开发出实现所有策略的公开软件。

Conclusion: 所提的基于回归树及其集成的单变量时间序列预测方法可行有效。

Abstract: This paper describes a methodology for automated univariate time series forecasting using regression trees and their ensembles: bagging and random forests. The key aspects that are addressed are: the use of an autoregressive approach and recursive forecasts, how to select the autoregressive features, how to deal with trending series and how to cope with seasonal behavior. Experimental results show a forecast accuracy comparable with well-established statistical models such as exponential smoothing or ARIMA. Furthermore, a publicly available software implementing all the proposed strategies has been developed and is described in the paper.

</details>


### [248] [Quality-Diversity Optimization as Multi-Objective Optimization](https://arxiv.org/abs/2602.00478)
*Xi Lin,Ping Guo,Yilu Liu,Qingfu Zhang,Jianyong Sun*

Main category: cs.LG

TL;DR: 将质量多样性（QD）优化重新表述为多目标优化（MOO）问题，用MOO方法解决QD问题并通过实验验证效果。


<details>
  <summary>Details</summary>
Motivation: 已有多种QD算法，但本文想通过新的思路来解决QD问题，利用MOO成熟方法。

Method: 将QD优化转化为具有大量优化目标的MOO问题，采用基于集合的标量化技术进行协同搜索。

Result: 理论上继承MOO的理论保证，实验表明性能与现有QD算法相当。

Conclusion: 所提出的将QD优化转化为MOO问题的方法可行且有效。

Abstract: The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.

</details>


### [249] [Lossless Embedding Compression via Spherical Coordinates](https://arxiv.org/abs/2602.00079)
*Han Xiao*

Main category: cs.LG

TL;DR: 提出单位范数嵌入无损压缩方法，压缩率达1.5倍，优于现有方法，无需训练且无损。


<details>
  <summary>Details</summary>
Motivation: 寻找更好的单位范数嵌入压缩方法，提高压缩率。

Method: 利用高维单位向量球坐标集中在π/2附近，使IEEE 754指数坍缩为单值，再进行熵编码。

Result: 在26种文本、图像和多向量嵌入配置评估中，压缩率比最佳现有方法高25%，达1.5倍。

Conclusion: 该方法无需训练，在float32精度内完全无损，能持续提升压缩效果。

Abstract: We present a lossless compression method for unit-norm embeddings that achieves 1.5$\times$ compression, 25\% better than the best prior method. The method exploits that spherical coordinates of high-dimensional unit vectors concentrate around $π/2$, causing IEEE 754 exponents to collapse to a single value and enabling entropy coding. Evaluation across 26 configurations spanning text, image, and multi-vector embeddings confirms consistent improvement. The method requires no training and is fully lossless within float32 precision.

</details>


### [250] [Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.00084)
*Brady Steele*

Main category: cs.LG

TL;DR: 提出理论框架解释LoRA对标签噪声的固有抗性，提出RACT方法进行噪声检测，实验验证预测。


<details>
  <summary>Details</summary>
Motivation: 解释LoRA对标签噪声的固有抗性这一未充分探索的特性。

Method: 理论分析证明LoRA容量限制、推导最优秩平衡、建立时间分离；提出RACT方法利用秩差异进行噪声检测。

Result: 实验验证理论预测，RACT在AG News上噪声检测F1为91.1%，准确率91.46%，与无噪声检测能力的基线模型有竞争力。

Conclusion: 理论框架有效解释LoRA对标签噪声的抗性，RACT方法能有效检测噪声。

Abstract: Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.

</details>


### [251] [VoxServe: Streaming-Centric Serving System for Speech Language Models](https://arxiv.org/abs/2602.00269)
*Keisuke Kamahori,Wei-Tzu Lee,Atindra Jha,Rohan Kadekodi,Stephanie Wang,Arvind Krishnamurthy,Baris Kasikci*

Main category: cs.LG

TL;DR: 提出VoxServe统一服务系统优化语音语言模型流式性能，实现高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有系统难以灵活高效支持多种语音语言模型，需低延迟、高吞吐量和强流性保障的系统。

Method: 引入模型执行抽象解耦模型架构与系统优化，实现流式感知调度和异步推理管道。

Result: 在多个现代语音语言模型评估中，VoxServe在相近延迟下吞吐量比现有实现高10 - 20倍，且保持高流性。

Conclusion: VoxServe能有效优化语音语言模型的流式性能，代码开源。

Abstract: Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.

</details>


### [252] [CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models](https://arxiv.org/abs/2602.00085)
*Shuozhe Li,Jincheng Cao,Bodun Hu,Aryan Mokhtari,Leqi Liu,Amy Zhang*

Main category: cs.LG

TL;DR: 提出CARE - RFT方法解决强化微调中推理能力和模型可信度的权衡问题，实验表明该方法能实现良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有无约束强化微调会降低模型可信度，RKL约束的强化微调会限制推理能力提升，需解决二者间的权衡问题。

Method: 引入CARE - RFT方法，用偏斜反向KL散度替代标准反向KL正则化，提供置信度敏感惩罚。

Result: 在多模型规模和强化微调算法的实验中，CARE - RFT实现了良好平衡，推理性能与无约束强化微调相当，同时恢复了基础模型的可信度和校准度。

Conclusion: 谨慎的、考虑置信度的正则化是构建既有能力又可信的推理模型的关键。

Abstract: Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallucination and worsening calibration; conversely, RKL-constrained RFT preserves trustworthiness but limits reasoning gains due to its unbounded penalty on exploratory deviations. To resolve this tension, we introduce CARE-RFT (Confidence-Anchored Regularized Reinforcement Finetuning), a novel method that replaces standard reverse KL regularization with a skew reverse KL divergence. CARE-RFT provides a confidence-sensitive penalty: it is bounded for confident, consistently rewarded explorations to enable reasoning, while unbounded elsewhere to preserve calibration. Extensive experiments across multiple model scales and RFT algorithms show that CARE-RFT achieves a superior balance, matching the reasoning performance of unconstrained RFT while recovering the trustworthiness and calibration of the base model. Our work establishes that careful, confidence-aware regularization is key to building both capable and trustworthy reasoning models.

</details>


### [253] [Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation](https://arxiv.org/abs/2602.00294)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 本文提出一种方法使自注意力机制以恒定成本任意精度计算，减少内存和计算量，降低大模型基础设施和能源需求。


<details>
  <summary>Details</summary>
Motivation: 当前标准自注意力机制成本随上下文长度增加，对存储、计算和能源的需求超出社会供给能力。

Method: 将传统公式的泰勒展开分解为张量积对称链表达式，利用对称性得到前馈变换，将查询和键映射到最小多项式核特征基坐标。

Result: 实现公式并验证其正确性，能以适度固定成本实现无界令牌生成。

Conclusion: 所提方法可大幅降低大规模Transformer模型的基础设施和能源需求，引入的数学技术也有独立价值。

Abstract: The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.

</details>


### [254] [Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization](https://arxiv.org/abs/2602.01510)
*Hengzhe Zhang,Qi Chen,Bing Xue,Wolfgang Banzhaf,Mengjie Zhang*

Main category: cs.LG

TL;DR: 本文提出基于遗传编程的特征构建框架控制过拟合，在多数据集实验中展现优势。


<details>
  <summary>Details</summary>
Motivation: 解决遗传编程特征构建中过拟合限制广泛应用的问题，提高泛化能力。

Method: 证明邻域风险分解，提出联合优化经验风险和邻域Jensen间隙的进化特征构建框架，开发噪声估计策略和流形入侵检测机制。

Result: 在58个数据集实验表明最小化Jensen间隙有效，与15种机器学习算法对比显示所提过拟合控制策略性能更优。

Conclusion: 所提基于遗传编程的过拟合控制策略能有效提升性能，增强泛化能力。

Abstract: Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

</details>


### [255] [Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA](https://arxiv.org/abs/2602.00451)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: 针对去中心化联邦学习中低秩适配更新的问题，提出TAD - LoRA框架，理论证明收敛性，实验验证其在不同通信场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习中低秩适配更新的去中心化聚合会引入拓扑相关的交叉项，在动态通信图下使训练不稳定。

Method: 提出Topology - Aware Decentralized Low - Rank Adaptation（TAD - LoRA）框架，协调低秩适配因子的更新和混合以控制客户端间的偏差。

Result: 理论证明TAD - LoRA在非凸目标下的收敛性；实验表明TAD - LoRA在不同通信场景有稳健性能，在中度和弱连接拓扑下有明显优势，在MNLI数据集上表现出色。

Conclusion: TAD - LoRA框架能有效应对去中心化联邦学习中低秩适配更新的问题，在不同通信场景有良好表现。

Abstract: Decentralized federated learning (DFL), a serverless variant of federated learning, poses unique challenges for parameter-efficient fine-tuning due to the factorized structure of low-rank adaptation (LoRA). Unlike linear parameters, decentralized aggregation of LoRA updates introduces topology-dependent cross terms that can destabilize training under dynamic communication graphs. We propose \texttt{TAD-LoRA}, a Topology-Aware Decentralized Low-Rank Adaptation framework that coordinates the updates and mixing of LoRA factors to control inter-client misalignment. We theoretically prove the convergence of \texttt{TAD-LoRA} under non-convex objectives, explicitly characterizing the trade-off between topology-induced cross-term error and block-coordinate representation bias governed by the switching interval of alternative training. Experiments under various communication conditions validate our analysis, showing that \texttt{TAD-LoRA} achieves robust performance across different communication scenarios, remaining competitive in strongly connected topologies and delivering clear gains under moderately and weakly connected topologies, with particularly strong results on the MNLI dataset.

</details>


### [256] [From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting](https://arxiv.org/abs/2602.00088)
*Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 提出符号过渡机制（STM）用于时间序列预测，能提升基础模型效率与精度，资源成本低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在时间序列预测任务中计算和内存需求大，难以部署在轻量级平台。

Method: 提出STM框架，通过符号抽象和提示工程连接数值时间序列数据和语言模型，用量化技术将连续时间序列值转化为符号令牌，通过符号的结构化转换捕捉时间动态。

Result: 在多个时间序列数据集上与四个小语言模型搭配测试，STM使MAE最多降低69%，MSE最多降低90%，基础模型最大GPU内存增加约0.06%，延迟开销仅增加0.64%。

Conclusion: STM作为基于基础模型的符号驱动时间序列预测的高效、适应性层具有潜力。

Abstract: Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time series data and language models through symbolic abstraction and prompt engineering. STM transforms continuous time series values into symbol tokens with quantization techniques based on human cognitive structures, and captures temporal dynamics through structured transformations of symbols, enabling fast engineering based predictions in which language models focus on critical parts of time series data. STM is a general purpose mechanisms that ensure the integrity of backbone language models, but they significantly improve their efficiency by inferring the dynamic and structured patterns inherent in time series data. We evaluated STM on various time series datasets, paired with four small language models (SLM) with limited computational environments. For all models, STM achieves error reductions of up to 69% in MAE and 90% in MSE compared to the default backbone SLM without STM. These results demonstrate the potential of STM as an efficient, adaptable layer for symbol-driven time series prediction using foundation models. The accuracy improvements were made at negligible resource costs, with maximum GPU memory of the base model increasing by approximately 0.06% and latency overhead increasing by only 0.64%.

</details>


### [257] [FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards](https://arxiv.org/abs/2602.00453)
*Ziyao Wang,Daeun Jung,Yexiao He,Guoheng Sun,Zheyu Shen,Myungjin Lee,Ang Li*

Main category: cs.LG

TL;DR: 提出FedMOA框架用于异构奖励下的多目标对齐，实验显示其优于联邦平均法。


<details>
  <summary>Details</summary>
Motivation: 传统RL对齐在设备端联邦学习内存开销大，GRPO过渡到联邦设置有挑战，需解决异构奖励、多目标优化不平衡和高训练成本问题。

Method: 提出FedMOA框架，本地通过超梯度下降的在线自适应加权机制稳定训练，服务器端采用任务和精度感知的聚合策略。

Result: 在数学推理和代码生成基准测试中，FedMOA始终优于联邦平均法，精度最高提升2.2%。

Conclusion: FedMOA能提升全局性能、个性化和多目标平衡。

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.

</details>


### [258] [Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits](https://arxiv.org/abs/2602.00092)
*Neha Kalibhat,Zi Wang,Prasoon Bajpai,Drew Proud,Wenjun Zeng,Been Kim,Mani Malek*

Main category: cs.LG

TL;DR: 提出一种学习可验证宪法的黑盒可解释性框架，通过原子概念编辑学习输入编辑与模型行为的因果映射，在多任务验证该方法，发现不同模型特点，且该框架对控制模型行为更有效。


<details>
  <summary>Details</summary>
Motivation: 建立一个能深入、可推广地洞察模型，对模型行为进行控制和理解的黑盒可解释性框架。

Method: 利用原子概念编辑（ACEs），通过有目的地增加、移除或替换输入提示中的可解释概念，系统应用ACEs并观察对不同任务模型行为的影响，学习从编辑到可预测结果的因果映射。

Result: 发现不同模型在文本到图像生成和数学推理中的特点；所学习的宪法对控制模型行为更有效，成功率比不使用宪法的方法平均提高1.86倍。

Conclusion: 所提出的学习可验证宪法的黑盒可解释性框架有效，能实现对模型的控制和理解。

Abstract: We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), which are targeted operations that add, remove, or replace an interpretable concept in the input prompt. By systematically applying ACEs and observing the resulting effects on model behavior across various tasks, our framework learns a causal mapping from edits to predictable outcomes. This learned constitution provides deep, generalizable insights into the model. Empirically, we validate our approach across diverse tasks, including mathematical reasoning and text-to-image alignment, for controlling and understanding model behavior. We found that for text-to-image generation, GPT-Image tends to focus on grammatical adherence, while Imagen 4 prioritizes atmospheric coherence. In mathematical reasoning, distractor variables confuse GPT-5 but leave Gemini 2.5 models and o4-mini largely unaffected. Moreover, our results show that the learned constitutions are highly effective for controlling model behavior, achieving an average of 1.86 times boost in success rate over methods that do not use constitutions.

</details>


### [259] [Backpropagation as Physical Relaxation: Exact Gradients in Finite Time](https://arxiv.org/abs/2602.02281)
*Antonino Emanuele Scurria*

Main category: cs.LG

TL;DR: 本文证明反向传播是物理动力系统的有限时间松弛，提出“二元反向传播”框架并证明其能精确恢复标准反向传播，为模拟和神经形态计算提供基础。


<details>
  <summary>Details</summary>
Motivation: 从物理角度理解反向传播算法，为模拟和神经形态基板中的精确梯度计算提供严格基础。

Method: 将前馈推理表述为连续时间过程，应用非保守系统的拉格朗日理论，在双状态空间上推导全局能量泛函。

Result: 提出“二元反向传播”框架，证明单位步长欧拉离散化能在2L步内精确恢复标准反向传播。

Conclusion: 反向传播是连续物理松弛的数字优化投影，为连续动力学原生的模拟和神经形态基板提供精确梯度计算基础。

Abstract: Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.

</details>


### [260] [Forecasting Energy Availability in Local Energy Communities via LSTM Federated Learning](https://arxiv.org/abs/2602.00694)
*Fabio Turazza,Marcello Pietri,Natalia Selini Hadjidimitriou,Marco Mamei*

Main category: cs.LG

TL;DR: 探讨使用联邦学习（FL）和长短期记忆（LSTM）网络，在不共享隐私信息下为本地能源社区构建预测模型，以应对能源供需平衡挑战并强调数据共享与预测准确性的权衡。


<details>
  <summary>Details</summary>
Motivation: 本地能源社区需实现自给自足，需准确预测模型，但隐私约束阻碍预测方案应用，因此需寻找无需共享隐私信息的解决方案。

Method: 运用联邦学习（FL）和长短期记忆（LSTM）网络来构建预测模型。

Result: 未提及具体研究结果。

Conclusion: 未提及明确结论，但指出可利用FL和LSTM网络实现目标，强调了数据共享和预测准确性之间的权衡。

Abstract: Local Energy Communities are emerging as crucial players in the landscape of sustainable development. A significant challenge for these communities is achieving self-sufficiency through effective management of the balance between energy production and consumption. To meet this challenge, it is essential to develop and implement forecasting models that deliver accurate predictions, which can then be utilized by optimization and planning algorithms. However, the application of forecasting solutions is often hindered by privacy constrains and regulations as the users participating in the Local Energy Community can be (rightfully) reluctant sharing their consumption patterns with others. In this context, the use of Federated Learning (FL) can be a viable solution as it allows to create a forecasting model without the need to share privacy sensitive information among the users. In this study, we demonstrate how FL and long short-term memory (LSTM) networks can be employed to achieve this objective, highlighting the trade-off between data sharing and forecasting accuracy.

</details>


### [261] [Trade-offs Between Individual and Group Fairness in Machine Learning: A Comprehensive Review](https://arxiv.org/abs/2602.00094)
*Sandra Benítez-Peña,Blas Kolic,Victoria Menendez,Belén Pulido*

Main category: cs.LG

TL;DR: 本综述探讨联合解决群体公平（GF）和个体公平（IF）的方法，系统批判回顾混合公平性方法，讨论挑战并指明开放研究方向。


<details>
  <summary>Details</summary>
Motivation: 算法公平是计算决策系统关键问题，GF和IF传统上孤立研究，需联合两者并研究权衡。

Method: 根据公平机制及调和多公平标准的算法和数学策略对现有方法分类，审查其理论基础、优化机制和实证评估实践。

Result: 回顾了混合公平方法，分析各类方法特点并指出局限性。

Conclusion: 本综述为设计兼顾个体和群体公平的混合算法的研究者和从业者提供综合资源。

Abstract: Algorithmic fairness has become a central concern in computational decision-making systems, where ensuring equitable outcomes is essential for both ethical and legal reasons. Two dominant notions of fairness have emerged in the literature: Group Fairness (GF), which focuses on mitigating disparities across demographic subpopulations, and Individual Fairness (IF), which emphasizes consistent treatment of similar individuals. These notions have traditionally been studied in isolation. In contrast, this survey examines methods that jointly address GF and IF, integrating both perspectives within unified frameworks and explicitly characterizing the trade-offs between them. We provide a systematic and critical review of hybrid fairness approaches, organizing existing methods according to the fairness mechanisms they employ and the algorithmic and mathematical strategies used to reconcile multiple fairness criteria. For each class of methods, we examine their theoretical foundations, optimization mechanisms, and empirical evaluation practices, and discuss their limitations. Additionally, we discuss the challenges and identify open research directions for developing principled, context-aware hybrid fairness methods. By synthesizing insights across the literature, this survey aims to serve as a comprehensive resource for researchers and practitioners seeking to design hybrid algorithms that provide reliable fairness guarantees at both the individual and group levels.

</details>


### [262] [Gauss-Newton Natural Gradient Descent for Shape Learning](https://arxiv.org/abs/2602.00099)
*James King,Arturs Berzins,Siddhartha Mishra,Marius Zeinhofer*

Main category: cs.LG

TL;DR: 本文探索高斯 - 牛顿法在形状学习优化中的应用，该方法比标准一阶方法收敛更快更稳定，实验证明其能提升训练速度和最终解的精度。


<details>
  <summary>Details</summary>
Motivation: 解决形状学习中潜在微分约束的病态性以及参数空间和函数空间中优化问题不匹配的关键挑战。

Method: 使用高斯 - 牛顿法进行形状学习优化。

Result: 比标准一阶方法收敛更快更稳定，所需迭代次数更少；在基准形状优化任务中能提升训练速度和最终解的精度。

Conclusion: 高斯 - 牛顿法在形状学习优化中有良好效果，可提高训练速度和最终解的准确性。

Abstract: We explore the use of the Gauss-Newton method for optimization in shape learning, including implicit neural surfaces and geometry-informed neural networks. The method addresses key challenges in shape learning, such as the ill-conditioning of the underlying differential constraints and the mismatch between the optimization problem in parameter space and the function space where the problem is naturally posed. This leads to significantly faster and more stable convergence than standard first-order methods, while also requiring far fewer iterations. Experiments across benchmark shape optimization tasks demonstrate that the Gauss-Newton method consistently improves both training speed and final solution accuracy.

</details>


### [263] [THDC: Training Hyperdimensional Computing Models with Backpropagation](https://arxiv.org/abs/2602.00116)
*Hanne Dejonghe,Sam Leroux*

Main category: cs.LG

TL;DR: 提出可训练超维计算（THDC），通过反向传播实现端到端HDC，在多数据集上表现好且降低维度。


<details>
  <summary>Details</summary>
Motivation: 传统超维计算（HDC）依赖超高维度和静态随机初始化超向量，限制了内存效率和学习能力。

Method: 用可训练的嵌入替换随机初始化向量，引入单层二进制神经网络优化类别表示。

Result: 在MNIST、Fashion - MNIST和CIFAR - 10上评估，THDC达到或超过现有HDC的准确率，维度从10000降至64。

Conclusion: THDC是一种有效的超维计算方法，能提高内存效率和学习能力。

Abstract: Hyperdimensional computing (HDC) offers lightweight learning for energy-constrained devices by encoding data into high-dimensional vectors. However, its reliance on ultra-high dimensionality and static, randomly initialized hypervectors limits memory efficiency and learning capacity. Therefore, we propose Trainable Hyperdimensional Computing (THDC), which enables end-to-end HDC via backpropagation. THDC replaces randomly initialized vectors with trainable embeddings and introduces a one-layer binary neural network to optimize class representations. Evaluated on MNIST, Fashion-MNIST and CIFAR-10, THDC achieves equal or better accuracy than state-of-the-art HDC, with dimensionality reduced from 10.000 to 64.

</details>


### [264] [Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control](https://arxiv.org/abs/2602.00120)
*Xianghong Hu,Tianning Xu,Ying Chen,Shuai Wang*

Main category: cs.LG

TL;DR: 本文用真实房贷数据集比较多种机器学习方法进行房贷违约预测，强调控制信息泄露和处理类别不平衡，AutoML方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现实房贷数据中，违约标签模糊、类别严重不平衡和信息泄露问题影响评估有效性和部署可靠性，需更好方法进行房贷违约预测。

Method: 采用泄漏感知特征选择、严格的时间分割和多数类控制下采样。

Result: 在多个正负样本比例下性能稳定，AutoML方法（AutoGluon）在评估模型中AUROC最强。

Conclusion: 提出的方法在房贷违约预测中有较好表现，后续会有扩展版以书章节形式呈现。

Abstract: Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.

</details>


### [265] [MiniTensor: A Lightweight, High-Performance Tensor Operations Library](https://arxiv.org/abs/2602.00125)
*Soumyadip Sarkar*

Main category: cs.LG

TL;DR: 介绍开源张量运算库MiniTensor，有简洁API，执行核心用Rust，对比主流框架包更小。


<details>
  <summary>Details</summary>
Motivation: 开发一个专注于简约、正确性和性能的张量运算库。

Method: 设计了MiniTensor架构，包括高效内存管理、动态梯度计算图，通过PyO3与Python集成。

Result: MiniTensor包大小仅几兆字节，比主流框架小几个数量级。

Conclusion: MiniTensor在保持研发必要功能同时，实现了包大小的显著优化，适用于CPU上的研发。

Abstract: We present MiniTensor, an open source tensor operations library that focuses on minimalism, correctness, and performance. MiniTensor exposes a familiar PyTorch-like Python API while it executes performance critical code in a Rust engine. The core supports dense $n$ dimensional tensors, broadcasting, reductions, matrix multiplication, reverse mode automatic differentiation, a compact set of neural network layers, and standard optimizers. In this paper, we describe the design of MiniTensor's architecture, including its efficient memory management, dynamic computation graph for gradients, and integration with Python via PyO3. We also compare the install footprint with PyTorch and TensorFlow to demonstrate that MiniTensor achieves a package size of only a few megabytes, several orders of magnitude smaller than mainstream frameworks, while preserving the essentials needed for research and development on CPUs. The repository can be found at https://github.com/neuralsorcerer/minitensor

</details>


### [266] [FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization](https://arxiv.org/abs/2602.01852)
*Zeyan Wang,Zhengmao Liu,Yongxin Cai,Chi Li,Xiaoying Tang,Jingchao Chen,Zibin Pan,Jing Qiu*

Main category: cs.LG

TL;DR: 提出FUPareto框架解决联邦无学习的三个挑战，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦无学习方法存在损害模型实用性、遗忘与实用性冲突、多客户端无学习支持差等问题。

Method: 提出MBS损失函数，通过帕累托改进步骤保留模型实用性，执行帕累托扩展保证遗忘，集成MGDA算法解耦梯度冲突。

Result: FUPareto在多种场景下的实验中，无学习效果和保留实用性均优于现有方法。

Conclusion: FUPareto是一个高效的联邦无学习框架，能有效解决现有问题。

Abstract: Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.

</details>


### [267] [ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2602.00127)
*Tong Zhu,Baiting Chen,Jin Zhou,Hua Zhou,Sriram Sankararaman,Xiaowu Dai*

Main category: cs.LG

TL;DR: 现有LLM在复杂推理任务表现差，推理时集成方法无正式改善推理质量保证，本文提出ALIGN方法，有理论性能保证，实证结果优于基线。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂推理任务表现不佳，现有推理时集成方法无法提供正式保证来证明集成能改善推理质量。

Method: 提出ALIGN方法，将LLM推理构建为对齐委托游戏，委托人委托多个代理人生成候选解并选择最终答案。

Result: 理论上证明在同等获取候选解情况下，ALIGN比单智能体生成能提高预期性能；实证结果显示ALIGN在多个LLM推理基准测试中优于单智能体和集成基线。

Conclusion: ALIGN方法在LLM推理任务中表现良好，能有效提升推理性能。

Abstract: LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.

</details>


### [268] [Quantum Model Parallelism for MRI-Based Classification of Alzheimer's Disease Stages](https://arxiv.org/abs/2602.00128)
*Emine Akpinar,Murat Oduncuoglu*

Main category: cs.LG

TL;DR: 文章提出基于量子的并行模型（QBPM）用于阿尔茨海默病（AD）阶段分类，在两个数据集上验证其性能，结果显示该模型准确高效，是经典方法的替代方案。


<details>
  <summary>Details</summary>
Motivation: 随着预期寿命增加，AD成为全球健康问题，经典AI方法受数据量和计算资源限制，需要更高效方法，量子AI有潜力解决这些问题。

Method: 提出QBPM架构，受经典模型并行原理启发，使用两个不同量子电路并行运行，在量子模拟器上处理MRI数据集。

Result: 模型在两个数据集上有高分类准确率，在高斯噪声下仍适用，相比五种经典迁移学习方法，准确率更高、使用参数更少、执行时间相当。

Conclusion: QBPM架构是复杂疾病如AD阶段分类的创新有力方法。

Abstract: With increasing life expectancy, AD has become a major global health concern. While classical AI-based methods have been developed for early diagnosis and stage classification of AD, growing data volumes and limited computational resources necessitate faster, more efficient approaches. Quantum-based AI methods, which leverage superposition and entanglement principles along with high-dimensional Hilbert space, can surpass classical approaches' limitations and offer higher accuracy for high-dimensional, heterogeneous, and noisy data. In this study, a Quantum-Based Parallel Model (QBPM) architecture is proposed for the efficient classification of AD stages using MRI datasets, inspired by the principles of classical model parallelism. The proposed model leverages quantum advantages by employing two distinct quantum circuits, each incorporating rotational and entanglement blocks, running in parallel on the same quantum simulator. The classification performance of the model was evaluated on two different datasets to assess its overall robustness and generalization capability. The proposed model demonstrated high classification accuracy across both datasets, highlighting its overall robustness and generalization capability. Results obtained under high-level Gaussian noise, simulating real-world conditions, further provided experimental evidence for the model's applicability not only in theoretical but also in practical scenarios. Moreover, compared with five different classical transfer learning methods, the proposed model demonstrated its efficiency as an alternative to classical approaches by achieving higher classification accuracy and comparable execution time while utilizing fewer circuit parameters. The results indicate that the proposed QBPM architecture represents an innovative and powerful approach for the classification of stages in complex diseases such as Alzheimer's.

</details>


### [269] [Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models](https://arxiv.org/abs/2602.00129)
*Yixuan Liang*

Main category: cs.LG

TL;DR: 提出CodePilot框架结合MCTS与大语言模型进行程序修复，在实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在仓库级自动程序修复因长程推理和自回归解码限制面临挑战。

Method: CodePilot框架进行从仓库到文件和函数级的分层故障定位，用MCTS探索补丁轨迹，利用执行反馈引导搜索和优化，还结合置信度校准生成。

Result: 在SWE - bench Lite实验中，使用开源权重模型实现24.67%的问题解决率，优于可比基线。

Conclusion: 符号搜索与神经语言模型结合是可扩展、执行感知的软件工程自动化的有效策略。

Abstract: Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.

</details>


### [270] [ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning](https://arxiv.org/abs/2602.02192)
*Jie Xiao,Meng Chen,Qingnan Ren,Song Jingwei,Jiaqi Huang,Yangshen Deng,Chris Tong,Wanyi Chen,Suli Wang,Ziqian Bi,Shuo Lu,Yiqun Duan,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: 提出分布式强化学习框架ECHO - 2用于大语言模型后训练，结合集中学习与分布式滚动执行，提升成本效率。


<details>
  <summary>Details</summary>
Motivation: 分布式滚动执行虽可利用低成本推理资源，但存在广域协调和策略传播难题，需解决大语言模型后训练的强化学习阶段相关问题。

Method: 结合集中学习与分布式滚动执行，将有界策略过时作为用户可控参数使多个操作重叠；引入基于重叠的容量模型；采用对等辅助流水线广播和成本感知的异构工作负载激活。

Result: 在4B和8B模型的GRPO后训练实验中，ECHO - 2显著提高成本效率，且RL奖励与强基线相当。

Conclusion: ECHO - 2能在保持奖励的同时有效提升成本效率，是解决大语言模型后训练强化学习分布式执行问题的有效方案。

Abstract: Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.

</details>


### [271] [On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks](https://arxiv.org/abs/2602.00130)
*Sumit Yadav*

Main category: cs.LG

TL;DR: 研究表征几何与神经网络性能关系，发现有效维度能跨领域预测性能且有因果关系。


<details>
  <summary>Details</summary>
Motivation: 探究表征几何与神经网络性能之间的关系。

Method: 分析52个预训练ImageNet模型，控制模型容量，在不同数据集和任务上测试，还通过添加噪声和PCA操作建立因果关系。

Result: 有效维度能很好地预测准确率，与准确率有强相关性，模型大小预测性差，多种噪声对准确率影响类似。

Conclusion: 有效维度可提供关于神经网络性能的领域无关的预测和因果信息，且无需标签计算。

Abstract: We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 ($p < 10^(-10)$) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, $p < 10^(-9)$), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show $|r| > 0.90$. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.

</details>


### [272] [RAPTOR: Ridge-Adaptive Logistic Probes](https://arxiv.org/abs/2602.00158)
*Ziqi Gao,Yaotian Zhu,Qingcheng Zeng,Xu Zhao,Ziqing Wang,Feng Ruan,Kaize Ding*

Main category: cs.LG

TL;DR: 提出RAPTOR探针，在准确率、方向稳定性和训练成本上表现良好，并对岭逻辑回归进行机制表征。


<details>
  <summary>Details</summary>
Motivation: 现有探针 - 引导管道需要准确、方向稳定且低成本的概念向量，以此为目标提出RAPTOR。

Method: 提出L2正则化的逻辑探针RAPTOR，通过验证调整岭强度得到概念向量；使用凸高斯极小极大定理在理想化高斯师生模型中进行机制表征。

Result: RAPTOR在准确率上与强基线相当或更优，有竞争的方向稳定性和更低训练成本，定量结果有定性下游引导演示支持。

Conclusion: RAPTOR有效，岭逻辑回归的机制表征结果与真实大模型嵌入上的趋势定性一致。

Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.

</details>


### [273] [Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation](https://arxiv.org/abs/2602.00899)
*Mritunjay Pandey*

Main category: cs.LG

TL;DR: 本文将基于内容的推荐转化为检索问题，提出可扩展的双塔双编码器密集检索系统，结合索引与推理管道，在基准测试中提升召回率，满足延迟和模型大小约束，提供端到端蓝图。


<details>
  <summary>Details</summary>
Motivation: 电商推荐和搜索依赖的稀疏关键词匹配在词汇不匹配时效果不佳，需新方法解决。

Method: 采用双塔双编码器构建可扩展密集检索系统，在亚马逊评论数据集上用监督对比学习微调，结合FAISS HNSW索引和ONNX Runtime推理管道及INT8动态量化。

Result: 在评论到标题基准测试中，Recall@10从0.26提升到0.66，满足实际延迟和模型大小约束。

Conclusion: 提供了从离线训练到CPU高效服务的端到端、可复现的蓝图。

Abstract: E-commerce recommendation and search commonly rely on sparse keyword matching (e.g., BM25), which breaks down under vocabulary mismatch when user intent has limited lexical overlap with product metadata. We cast content-based recommendation as recommendation-as-retrieval: given a natural-language intent signal (a query or review), retrieve the top-K most relevant items from a large catalog via semantic similarity.
  We present a scalable dense retrieval system based on a two-tower bi-encoder, fine-tuned on the Amazon Reviews 2023 (Fashion) subset using supervised contrastive learning with Multiple Negatives Ranking Loss. We construct training pairs from review text (as a query proxy) and item metadata (as the positive document) and fine-tune on 50,000 sampled interactions with a maximum sequence length of 500 tokens.
  For efficient serving, we combine FAISS HNSW indexing with an ONNX Runtime inference pipeline using INT8 dynamic quantization. On a review-to-title benchmark over 826,402 catalog items, our approach improves Recall@10 from 0.26 (BM25) to 0.66, while meeting practical latency and model-size constraints: 6.1 ms median CPU inference latency (batch size 1) and a 4x reduction in model size.
  Overall, we provide an end-to-end, reproducible blueprint for taking domain-adapted dense retrieval from offline training to CPU-efficient serving at catalog scale.

</details>


### [274] [Block removal for large language models through constrained binary optimization](https://arxiv.org/abs/2602.00161)
*David Jansen,Roman Rausch,David Montero,Roman Orus*

Main category: cs.LG

TL;DR: 本文将大语言模型的块移除问题转化为约束二进制优化问题，映射到伊辛模型，高效排名候选配置，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 移除大语言模型的整个Transformer块时，确定移除哪些块是指数级难度的组合问题。

Method: 将块移除问题转化为约束二进制优化问题，映射到伊辛模型，通过能量对模型性能进行代理，对候选块移除配置进行高效排名。

Result: 在多个基准测试中超越现有块移除方法，短时间再训练后性能提升持续，在MMLU基准测试中最高提升6分，可应用于任何架构，在NVIDIA - Nemotron - 3 - Nano - 30B - A3B - FP8模型上得到验证。

Conclusion: 提出的方法高效且通用，能有效解决大语言模型块移除问题。

Abstract: Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance. This formulation enables an efficient ranking of a large number of candidate block-removal configurations and yields many high-quality, non-trivial solutions beyond consecutive regions. We demonstrate that our approach outperforms state-of-the-art block-removal methods across several benchmarks, with performance gains persisting after short retraining, and reaching improvements of up to 6 points on the MMLU benchmark. Our method requires only forward and backward passes for a few active parameters, together with an (at least approximate) Ising solver, and can be readily applied to any architecture. We illustrate this generality on the recent NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model, which exhibits a highly inhomogeneous and challenging block structure.

</details>


### [275] [Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2602.00165)
*Arthur Negrão,Pedro Silva,Vander L. S. Freitas,Gladston Moreira,Eduardo Luz*

Main category: cs.LG

TL;DR: 提出受本福特定律启发的非均匀量化器Benford - Quant用于大语言模型压缩，有理论和实证支持，能在低比特下提升精度，还可与其他方法结合。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速发展，需要有效压缩技术，标准均匀量化器假设参数均匀分布与实际不符。

Method: 提出Benford - Quant，用对数间隔码本代替均匀网格，为小幅度权重分配更多分辨率。

Result: 变压器转换层权重符合本福特统计，归一化层有偏差；在小语言模型上能降低困惑度，在大语言模型上有竞争力。

Conclusion: 将本福特启发先验纳入量化网格是低成本修改，能在低比特下提升精度，可与其他方法结合潜在提升性能，但未超越现有最优。

Abstract: The rapid growth of Large Language Models (LLMs) intensifies the need for effective compression, with weight quantization being the most widely adopted technique. Standard uniform quantizers assume that parameters are evenly distributed, an assumption at odds with the highly skewed distributions observed in practice. We propose Benford-Quant, a simple, data-free non-uniform quantizer inspired by Benford's Law, which predicts that leading digits follow a logarithmic distribution. Benford-Quant replaces the uniform grid with a log-spaced codebook, dedicating more resolution to the frequent small-magnitude weights. We provide both theoretical intuition and empirical evidence: (i) weights in transformer transformational layers adhere closely to Benford statistics, while normalization layers systematically deviate; (ii) on Small Language Models (SLMs), Benford-Quant consistently improves perplexity, reducing 4-bit perplexity on Gemma-270M by more than 10%; and (iii) on larger LLMs, it remains competitive, with differences explained by over-parameterization effects. Our results indicate that incorporating a Benford-inspired prior into quantization grids is a low-cost modification that yields accuracy gains in aggressive few-bit regimes. Although it is not able to surpass the state of the art in tasks such as perplexity and LAMBADA, the Benford-Quant approach can be hybridized with other quantization methods-such as SmoothQuant and Activation-Aware Quantization-without major pipeline modification, potentially improving their performance.

</details>


### [276] [Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints](https://arxiv.org/abs/2602.00166)
*Evan Chen,Wenzhi Fang,Shiqiang Wang,Christopher Brinton*

Main category: cs.LG

TL;DR: 提出DA - GRPO方法，能让本地小语言模型在持续学习时结合云大模型，实验证明其可提升精度、减少遗忘和维持云使用稳定性。


<details>
  <summary>Details</summary>
Motivation: 本地小语言模型需结合云大模型，但在持续学习中调节云辅助具挑战，基于奖励的强化学习会导致卸载行为不稳定和加剧灾难性遗忘。

Method: 提出DA - GRPO，将云使用约束直接纳入优势计算的Group Relative Policy Optimization的双优势扩展，使本地模型联合学习任务能力和协作行为。

Result: 在数学推理和代码生成基准实验中，与先前方法相比，DA - GRPO提高了切换后准确性，大幅减少遗忘，维持稳定云使用。

Conclusion: DA - GRPO是一种有效的方法，能让本地小语言模型在持续学习中更好结合云大模型。

Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.

</details>


### [277] [The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective](https://arxiv.org/abs/2602.00170)
*Qiyao Liang,Jinyeop Song,Yizhou Liu,Jeff Gore,Ila Fiete,Risto Miikkulainen,Xin Qiu*

Main category: cs.LG

TL;DR: 权重扰动进化策略（ES）能用小种群微调大语言模型，且ES和GRPO奖励有先升后降现象，二者源于微调景观的低曲率维度特性，小种群在多尺度下仍可获取奖励改进扰动。


<details>
  <summary>Details</summary>
Motivation: 解释权重扰动进化策略能用小种群微调大语言模型，以及ES和GRPO奖励先升后降这两个现象的原因。

Method: 用ES作为几何探针研究GSM8K、ARC - C和WinoGrande在Qwen2.5 - Instruct模型（0.5B - 7B）上的微调奖励景观，构建最小二次随机上升模型。

Result: 发现奖励改进扰动在多尺度下小种群仍可获取。

Conclusion: 调和了ES可扩展性与非单调训练动态的矛盾，表明高维微调的可行优化方法比最坏情况理论更多。

Abstract: Weight-perturbation evolution strategies (ES) can fine-tune billion-parameter language models with surprisingly small populations (e.g., $N\!\approx\!30$), contradicting classical zeroth-order curse-of-dimensionality intuition. We also observe a second seemingly separate phenomenon: under fixed hyperparameters, the stochastic fine-tuning reward often rises, peaks, and then degrades in both ES and GRPO. We argue that both effects reflect a shared geometric property of fine-tuning landscapes: they are low-dimensional in curvature. A small set of high-curvature dimensions dominates improvement, producing (i) heterogeneous time scales that yield rise-then-decay under fixed stochasticity, as captured by a minimal quadratic stochastic-ascent model, and (ii) degenerate improving updates, where many random perturbations share similar components along these directions. Using ES as a geometric probe on fine-tuning reward landscapes of GSM8K, ARC-C, and WinoGrande across Qwen2.5-Instruct models (0.5B--7B), we show that reward-improving perturbations remain empirically accessible with small populations across scales. Together, these results reconcile ES scalability with non-monotonic training dynamics and suggest that high-dimensional fine-tuning may admit a broader class of viable optimization methods than worst-case theory implies.

</details>


### [278] [GRIP2: A Robust and Powerful Deep Knockoff Method for Feature Selection](https://arxiv.org/abs/2602.00218)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

TL;DR: 提出二维组正则化重要性持久性方法GRIP2，在合成和半真实数据实验中表现良好，在真实HIV耐药性数据中也可靠。


<details>
  <summary>Details</summary>
Motivation: 在非线性、高相关和低信噪比情况下严格控制误发现率并识别真正预测性协变量，深度学习特征选择方法有吸引力，但存在挑战。

Method: 提出GRIP2，结合二维正则化表面上的第一层特征活动，引入有效块随机采样近似表面积分，统计量具有反对称性以确保有限样本FDR控制。

Result: 在合成和半真实数据实验中，GRIP2对特征相关性和噪声水平有更好稳健性，在高相关低信噪比下有高功效和稳定性；在真实HIV耐药性数据中能更好恢复已知耐药相关突变。

Conclusion: GRIP2在实践中具有可靠性。

Abstract: Identifying truly predictive covariates while strictly controlling false discoveries remains a fundamental challenge in nonlinear, highly correlated, and low signal-to-noise regimes, where deep learning based feature selection methods are most attractive. We propose Group Regularization Importance Persistence in 2 Dimensions (GRIP2), a deep knockoff feature importance statistic that integrates first-layer feature activity over a two-dimensional regularization surface controlling both sparsity strength and sparsification geometry. To approximate this surface integral in a single training run, we introduce efficient block-stochastic sampling, which aggregates feature activity magnitudes across diverse regularization regimes along the optimization trajectory. The resulting statistics are antisymmetric by construction, ensuring finite-sample FDR control. In extensive experiments on synthetic and semi-real data, GRIP2 demonstrates improved robustness to feature correlation and noise level: in high correlation and low signal-to-noise ratio regimes where standard deep learning based feature selectors may struggle, our method retains high power and stability. Finally, on real-world HIV drug resistance data, GRIP2 recovers known resistance-associated mutations with power better than established linear baselines, confirming its reliability in practice.

</details>


### [279] [Learning Robust Reasoning through Guided Adversarial Self-Play](https://arxiv.org/abs/2602.00173)
*Shuozhe Li,Vaishnav Tadiparthi,Kwonjoon Lee,Nakul Agarwal,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Lizhang Chen,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: 提出GASP方法将易出错的推理模型转变为鲁棒模型，提升其在误导和扰动上下文下的性能。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习从可验证奖励（RLVR）得到的推理模型在条件上下文不可靠时可能严重失败，需要一种鲁棒化方法。

Method: 引入GASP方法，通过单一模型内的对抗性自我博弈，让污染者诱导失败，代理学习诊断和恢复；提出分布内修复指导增加恢复概率。

Result: 在四个开源权重模型上，GASP将易出错的推理器转变为鲁棒推理器，同时常提高干净数据的准确率。

Conclusion: 对抗性腐败形成有效课程，分布内指导能实现快速恢复学习且减少表征漂移。

Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.

</details>


### [280] [LatentTrack: Sequential Weight Generation via Latent Filtering](https://arxiv.org/abs/2602.00458)
*Omer Haq*

Main category: cs.LG

TL;DR: 介绍LatentTrack (LT)用于非平稳动态下在线概率预测，在低维潜空间执行贝叶斯滤波，在耶拿气候基准测试上表现好。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳动态下的在线概率预测问题。

Method: 在低维潜空间执行因果贝叶斯滤波，用轻量级超网络生成预测模型参数，构建预测 - 生成 - 更新过滤框架。

Result: 在耶拿气候基准的长期在线回归评估中，LT比有状态序列和静态不确定性感知基线有更低的负对数似然和均方误差。

Conclusion: 潜条件函数演化是分布变化下传统潜状态建模的有效替代方法。

Abstract: We introduce LatentTrack (LT), a sequential neural architecture for online probabilistic prediction under nonstationary dynamics. LT performs causal Bayesian filtering in a low-dimensional latent space and uses a lightweight hypernetwork to generate predictive model parameters at each time step, enabling constant-time online adaptation without per-step gradient updates.
  At each time step, a learned latent model predicts the next latent distribution, which is updated via amortized inference using new observations, yielding a predict--generate--update filtering framework in function space. The formulation supports both structured (Markovian) and unstructured latent dynamics within a unified objective, while Monte Carlo inference over latent trajectories produces calibrated predictive mixtures with fixed per-step cost. Evaluated on long-horizon online regression using the Jena Climate benchmark, LT consistently achieves lower negative log-likelihood and mean squared error than stateful sequential and static uncertainty-aware baselines, with competitive calibration, demonstrating that latent-conditioned function evolution is an effective alternative to traditional latent-state modeling under distribution shift.

</details>


### [281] [The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization](https://arxiv.org/abs/2602.00175)
*Manyi Li,Yufan Liu,Lai Jiang,Bing Li,Yuming Li,Weiming Hu*

Main category: cs.LG

TL;DR: 指出扩散模型去学习防御只是假象，提出IVO攻击框架激活休眠记忆，实验证明IVO效果好并暴露当前防御漏洞。


<details>
  <summary>Details</summary>
Motivation: 发现扩散模型去学习防御对不良概念的“遗忘”是假象，被破坏的映射下知识仍以休眠记忆存在。

Method: 提出IVO攻击框架，通过图像反演、对抗优化和复用攻击来优化初始潜在变量，使去学习模型的噪声分布与原始不安全状态重新对齐。

Result: 在8种常用去学习技术的广泛实验中，IVO实现了更高的攻击成功率和强语义一致性。

Conclusion: 当前防御存在根本性缺陷。

Abstract: Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this "forgetting" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dormant memories. We find that the distributional discrepancy in the denoising process serves as a measurable indicator of how much of the mapping is retained, also reflecting the strength of unlearning. Inspired by this, we propose IVO (Initial Latent Variable Optimization), a concise and powerful attack framework that reactivates these dormant memories by reconstructing the broken mappings. Through Image Inversion}, Adversarial Optimization and Reused Attack, IVO optimizes initial latent variables to realign the noise distribution of unlearned models with their original unsafe states. Extensive experiments across 8 widely used unlearning techniques demonstrate that IVO achieves superior attack success rates and strong semantic consistency, exposing fundamental flaws in current defenses. The code is available at anonymous.4open.science/r/IVO/. Warning: This paper has unsafe images that may offend some readers.

</details>


### [282] [Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding](https://arxiv.org/abs/2602.00781)
*Jiamin Xu,Kyra Gan*

Main category: cs.LG

TL;DR: 提出修正的Q函数和阈值机制处理非情节、有限期MDPs中的在线强化学习，证明算法收敛性，数值评估显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 非情节、有限期MDPs的在线强化学习研究不足，现有无限期方法无法处理固定期限结构。

Method: 引入K步前瞻Q函数截断规划，采用阈值机制选择动作，给出表格学习算法。

Result: 证明算法有限样本快速收敛，数值评估中自适应增加K值，在多个环境中累积奖励超现有方法。

Conclusion: 提出的方法在非情节、有限期MDPs在线强化学习中有效，能提升性能。

Abstract: Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\mathcal{O}(\max((K-1),C_{K-1})\sqrt{SAT\log(T)})$ regret for any $K \geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.

</details>


### [283] [How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models](https://arxiv.org/abs/2602.00179)
*Joseph L. Breeden*

Main category: cs.LG

TL;DR: 本文指出机器学习可解释性中局部线性方法在决策边界不稳定是因预测不确定性高，应先判断有无可用预测，再寻求解释，还指出部分号称处处可解释的方法只是虚幻可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在关键决策应用中，局部线性解释方法在决策边界不稳定的问题。

Method: 分析预测不确定性与解释不稳定性的关系，提出先判断有无可用预测，再进行解释的方法。

Result: 明确解释不稳定性高是因决策边界预测不确定性高；有可用预测时解释不稳定性低；部分号称处处可解释的方法只是虚幻可解释性。

Conclusion: 应改变获取解释的顺序，先判断有无可用预测；解释不可用的预测无意义。

Abstract: For applications of machine learning in critical decisions, explainability is a primary concern, and often a regulatory requirement. Local linear methods for generating explanations, such as LIME and SHAP, have been criticized for being unstable near decision boundaries. In this paper, we explain that such concerns reflect a misunderstanding of the problem. The forecast uncertainty is high at decision boundaries, so consequently, the explanatory instability is high. The correct approach is to change the sequence of events and questions being asked. Nonlinear models can be highly predictive in some regions while having little or no predictability in others. Therefore, the first question is whether a usable forecast exists. When there is a forecast with low enough uncertainty to be useful, an explanation can be sought via a local linear approximation. In such cases, the explanatory instability is correspondingly low. When no usable forecast exists, the decision must fall to a simpler overall model such as traditional logistic regression. Additionally, these results show that some methods that purport to be explainable everywhere, such as ReLU networks or any piecewise linear model, have only an illusory explainability, because the forecast uncertainty at the segment boundaries is too high to be useful. Explaining an unusable forecast is pointless.

</details>


### [284] [Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization](https://arxiv.org/abs/2602.00827)
*Taesun Yeom,Taehyeok Ha,Jaeho Lee*

Main category: cs.LG

TL;DR: 研究特征学习强度（FLS）在实际条件下对深度网络泛化性的影响，发现存在最优FLS，与传统直觉不同。


<details>
  <summary>Details</summary>
Motivation: 现有理论在实际场景中对FLS如何影响泛化性的研究有限，需进一步探究。

Method: 通过实证研究发现最优FLS，对两层ReLU网络用逻辑损失训练的梯度流动力学进行理论分析。

Result: 发现存在最优FLS，过大的FLS导致过对齐现象降低泛化性，过小则导致过拟合。

Conclusion: 在实际条件下，特征学习强度对泛化性的影响存在最优值，并非越强越好。

Abstract: Feature learning strength (FLS), i.e., the inverse of the effective output scaling of a model, plays a critical role in shaping the optimization dynamics of neural nets. While its impact has been extensively studied under the asymptotic regimes -- both in training time and FLS -- existing theory offers limited insight into how FLS affects generalization in practical settings, such as when training is stopped upon reaching a target training risk. In this work, we investigate the impact of FLS on generalization in deep networks under such practical conditions. Through empirical studies, we first uncover the emergence of an $\textit{optimal FLS}$ -- neither too small nor too large -- that yields substantial generalization gains. This finding runs counter to the prevailing intuition that stronger feature learning universally improves generalization. To explain this phenomenon, we develop a theoretical analysis of gradient flow dynamics in two-layer ReLU nets trained with logistic loss, where FLS is controlled via initialization scale. Our main theoretical result establishes the existence of an optimal FLS arising from a trade-off between two competing effects: An excessively large FLS induces an $\textit{over-alignment}$ phenomenon that degrades generalization, while an overly small FLS leads to $\textit{over-fitting}$.

</details>


### [285] [GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models](https://arxiv.org/abs/2602.00191)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: cs.LG

TL;DR: 提出训练无关的Group-Equivariant Posterior Consistency (GEPC)用于检测扩散模型的分布外数据，在图像和雷达图像数据集上有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的分布外检测器大多忽略了数据的等变性，提出GEPC来检测等变性破坏。

Method: 引入GEPC衡量学习到的分数在有限群下的变换一致性，提出理想的GEPC残差并推导分布内上界和分布外下界。

Result: 在图像基准数据集上，GEPC的AUROC与最新基于扩散的基线相当或更优，计算轻量；在高分辨率合成孔径雷达图像上，GEPC实现了强目标背景分离和可解释的等变性破坏图。

Conclusion: GEPC是一种有效的分布外检测方法，可用于检测图像和雷达图像中的异常。

Abstract: Diffusion models learn a time-indexed score field $\mathbf{s}_θ(\mathbf{x}_t,t)$ that often inherits approximate equivariances (flips, rotations, circular shifts) from in-distribution (ID) data and convolutional backbones. Most diffusion-based out-of-distribution (OOD) detectors exploit score magnitude or local geometry (energies, curvature, covariance spectra) and largely ignore equivariances. We introduce Group-Equivariant Posterior Consistency (GEPC), a training-free probe that measures how consistently the learned score transforms under a finite group $\mathcal{G}$, detecting equivariance breaking even when score magnitude remains unchanged. At the population level, we propose the ideal GEPC residual, which averages an equivariance-residual functional over $\mathcal{G}$, and we derive ID upper bounds and OOD lower bounds under mild assumptions. GEPC requires only score evaluations and produces interpretable equivariance-breaking maps. On OOD image benchmark datasets, we show that GEPC achieves competitive or improved AUROC compared to recent diffusion-based baselines while remaining computationally lightweight. On high-resolution synthetic aperture radar imagery where OOD corresponds to targets or anomalies in clutter, GEPC yields strong target-background separation and visually interpretable equivariance-breaking maps. Code is available at https://github.com/RouzAY/gepc-diffusion/.

</details>


### [286] [Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation](https://arxiv.org/abs/2602.00834)
*Wei Chen,Jiacheng Li,Shigui Li,Zhiqi Lin,Junmei Yang,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 论文解决基于分数方法在密度比估计中的路径依赖问题，提出MinPV原则，推导方差闭式表达式，取得新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 基于分数的方法在密度比估计中存在理论与实践的矛盾，即理论上与路径无关，但实际性能依赖所选路径调度。

Method: 证明可处理的训练目标与理想目标存在被忽视的路径方差项，提出MinPV原则，推导方差闭式表达式，用Kumaraswamy混合模型参数化路径。

Result: 方法能学习数据自适应、低方差路径，在挑战性基准上建立新的最优结果。

Conclusion: 对完整目标进行原则性优化，可得到更准确和稳定的估计器。

Abstract: Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the time score. To address this, we propose MinPV (\textbf{Min}imum \textbf{P}ath \textbf{V}ariance) Principle, which introduces a principled heuristic to minimize the overlooked path variance. Our key contribution is the derivation of a closed-form expression for the variance, turning an intractable problem into a tractable optimization. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns a data-adaptive, low-variance path without heuristic selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks.

</details>


### [287] [Reducing Memorisation in Generative Models via Riemannian Bayesian Inference](https://arxiv.org/abs/2602.00199)
*Johanna Marie Gegenfurtner,Albert Kjøller Jacobsen,Naima Elosegui Borras,Alejandro Valverde Mahou,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: 从贝叶斯视角解决生成模型记忆与泛化平衡问题，通过考虑损失几何有效利用参数空间。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型在平衡记忆和泛化方面存在问题，需解决该挑战。

Method: 从贝叶斯视角，聚焦流匹配和扩散模型的参数空间，构建预测后验；用黎曼度量捕捉损失几何，利用适应损失景观局部结构的灵活近似后验。

Result: 提出的方法减少了记忆，同时保留了泛化能力，且有理论分析解释结果。

Conclusion: 考虑损失的几何形状能让复杂高维生成模型有效利用参数空间。

Abstract: Modern generative models can produce realistic samples, however, balancing memorisation and generalisation remains an open problem. We approach this challenge from a Bayesian perspective by focusing on the parameter space of flow matching and diffusion models and constructing a predictive posterior that better captures the variability of the data distribution. In particular, we capture the geometry of the loss using a Riemannian metric and leverage a flexible approximate posterior that adapts to the local structure of the loss landscape. This approach allows us to sample generative models that resemble the original model, but exhibit reduced memorisation. Empirically, we demonstrate that the proposed approach reduces memorisation while preserving generalisation. Further, we provide a theoretical analysis of our method, which explains our findings. Overall, our work illustrates how considering the geometry of the loss enables effective use of the parameter space, even for complex high-dimensional generative models.

</details>


### [288] [Reducing Class-Wise Performance Disparity via Margin Regularization](https://arxiv.org/abs/2602.00205)
*Beier Zhu,Kesen Zhao,Jiequan Cui,Qianru Sun,Yuan Zhou,Xun Yang,Hanwang Zhang*

Main category: cs.LG

TL;DR: 提出Margin Regularization for Performance Disparity Reduction (MR$^2$)方法，通过动态调整边界减少分类性能差异，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在分类时即使使用类别平衡数据，仍存在类别精度差异，现有理论对这种性能差异理解有限。

Method: 提出MR$^2$方法，在对数几率和表示空间动态调整边界，优化每类对数几率边界并惩罚过大表示边界。

Result: 在七个数据集和多种预训练骨干网络上实验，MR$^2$提高整体精度，提升难分类类别性能且不牺牲易分类类别。

Conclusion: MR$^2$能有效减少分类性能差异。

Abstract: Deep neural networks often exhibit substantial disparities in class-wise accuracy, even when trained on class-balanced data, posing concerns for reliable deployment. While prior efforts have explored empirical remedies, a theoretical understanding of such performance disparities in classification remains limited. In this work, we present Margin Regularization for Performance Disparity Reduction (MR$^2$), a theoretically principled regularization for classification by dynamically adjusting margins in both the logit and representation spaces. Our analysis establishes a margin-based, class-sensitive generalization bound that reveals how per-class feature variability contributes to error, motivating the use of larger margins for hard classes. Guided by this insight, MR$^2$ optimizes per-class logit margins proportional to feature spread and penalizes excessive representation margins to enhance intra-class compactness. Experiments on seven datasets, including ImageNet, and diverse pre-trained backbones (MAE, MoCov2, CLIP) demonstrate that MR$^2$ not only improves overall accuracy but also significantly boosts hard class performance without trading off easy classes, thus reducing performance disparity. Code is available at: https://github.com/BeierZhu/MR2

</details>


### [289] [Superposition unifies power-law training dynamics](https://arxiv.org/abs/2602.01045)
*Zixin Jessie Chen,Hao Chen,Yizhou Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 研究特征叠加在幂律训练动态出现中的作用，发现叠加瓶颈会使训练过渡到通用幂律指数，加速训练。


<details>
  <summary>Details</summary>
Motivation: 探究特征叠加在幂律训练动态出现中的作用。

Method: 使用师生框架，先推导无叠加时的训练分析理论。

Result: 叠加瓶颈会使训练过渡到通用幂律指数~1，与数据和通道统计无关，相比无叠加的顺序学习，训练加速达10倍。

Conclusion: 叠加能使训练快速进行且幂律指数与数据无关，对采用叠加的神经网络有重要意义。

Abstract: We investigate the role of feature superposition in the emergence of power-law training dynamics using a teacher-student framework. We first derive an analytic theory for training without superposition, establishing that the power-law training exponent depends on both the input data statistics and channel importance. Remarkably, we discover that a superposition bottleneck induces a transition to a universal power-law exponent of $\sim 1$, independent of data and channel statistics. This one over time training with superposition represents an up to tenfold acceleration compared to the purely sequential learning that takes place in the absence of superposition. Our finding that superposition leads to rapid training with a data-independent power law exponent may have important implications for a wide range of neural networks that employ superposition, including production-scale large language models.

</details>


### [290] [Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity](https://arxiv.org/abs/2602.00208)
*Jordan Levy,Paul Saves,Moncef Garouani,Nicolas Verstaevel,Benoit Gaudou*

Main category: cs.LG

TL;DR: 提出通过决策机制表征异常检测器的方法，利用SHapley Additive exPlanations量化特征重要性，以衡量检测器间相似度，构建更有效的无监督异常检测集成模型。


<details>
  <summary>Details</summary>
Motivation: 现有集成方法在无监督异常检测中难以构建真正互补的集成，因许多检测器依赖相似决策线索，导致异常得分冗余，限制了集成学习潜力。

Method: 提出通过决策机制表征异常检测器的方法，使用SHapley Additive exPlanations量化模型对输入特征的重要性，用这些归因轮廓衡量检测器间的相似度。

Result: 发现解释相似的检测器倾向于产生相关的异常得分并识别出大量重叠的异常，解释差异则表明检测行为具有互补性；解释驱动的指标为集成中选择模型提供了不同标准；仅多样性不足，高个体模型性能仍是有效集成的前提。

Conclusion: 通过明确针对解释多样性并保持模型质量，能够构建更多样、更互补且最终更有效的无监督异常检测集成。

Abstract: Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ensemble that is genuinely complementary remains challenging, since many detectors rely on similar decision cues and end up producing redundant anomaly scores. As a result, the potential of ensemble learning is often limited by the difficulty of identifying models that truly capture different types of irregularities. To address this, we propose a methodology for characterizing anomaly detectors through their decision mechanisms. Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors. We show that detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior. Our results demonstrate that explanation-driven metrics offer a different criterion than raw outputs for selecting models in an ensemble. However, we also demonstrate that diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles. By explicitly targeting explanation diversity while maintaining model quality, we are able to construct ensembles that are more diverse, more complementary, and ultimately more effective for unsupervised anomaly detection.

</details>


### [291] [Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses](https://arxiv.org/abs/2602.01285)
*Kangjun Noh,Seongchan Lee,Ilmun Kim,Kyungwoo Song*

Main category: cs.LG

TL;DR: 本文提出Multi - LLM Adaptive Conformal Inference (MACI)方法解决大语言模型事实性验证问题，实验显示其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型在高风险领域使用的事实性，解决现有保形推理方法过于保守或无法捕捉复杂结构的问题。

Method: 在乘法过滤设置下重新表述保形推理，建模事实性为声明级分数的乘积，MACI方法利用集成模型产生更准确的事实性分数，并通过组条件校准保持有效性。

Result: MACI实验中能保证用户指定的覆盖率，有更高的保留率和更低的时间成本。

Conclusion: MACI方法有效解决大语言模型事实性验证问题，优于基线。

Abstract: Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI

</details>


### [292] [Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models](https://arxiv.org/abs/2602.00217)
*Chen Liu,Xingzhi Sun,Xi Xiao,Alexandre Van Tassel,Ke Xu,Kristof Reimann,Danqi Liao,Mark Gerstein,Tianyang Wang,Xiao Wang,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 研究大语言模型与小模型表征差异，发现嵌入凝聚现象，提出分散损失缓解该现象并提升小模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型扩参会带来高计算成本，为更好理解其扩展，目标是在小模型中复制大模型的表征特性。

Method: 系统分析多个Transformer系列模型，提出分散损失在训练中促进嵌入分散。

Result: 小模型存在严重嵌入凝聚，大模型更能抵抗；知识蒸馏不能可靠缓解；分散损失可缓解凝聚，恢复大模型分散模式，在10个基准测试中提升性能。

Conclusion: 该工作为无额外参数提升小Transformer模型提供了原则性方法。

Abstract: Large language models (LLMs) achieve remarkable performance through ever-increasing parameter counts, but scaling incurs steep computational costs. To better understand LLM scaling, we study representational differences between LLMs and their smaller counterparts, with the goal of replicating the representational qualities of larger models in the smaller models. We observe a geometric phenomenon which we term $\textbf{embedding condensation}$, where token embeddings collapse into a narrow cone-like subspace in some language models. Through systematic analyses across multiple Transformer families, we show that small models such as $\texttt{GPT2}$ and $\texttt{Qwen3-0.6B}$ exhibit severe condensation, whereas the larger models such as $\texttt{GPT2-xl}$ and $\texttt{Qwen3-32B}$ are more resistant to this phenomenon. Additional observations show that embedding condensation is not reliably mitigated by knowledge distillation from larger models. To fight against it, we formulate a dispersion loss that explicitly encourages embedding dispersion during training. Experiments demonstrate that it mitigates condensation, recovers dispersion patterns seen in larger models, and yields performance gains across 10 benchmarks. We believe this work offers a principled path toward improving smaller Transformers without additional parameters.

</details>


### [293] [High-accuracy sampling for diffusion models and log-concave distributions](https://arxiv.org/abs/2602.01338)
*Fan Chen,Sinho Chewi,Constantinos Daskalakis,Alexander Rakhlin*

Main category: cs.LG

TL;DR: 本文提出扩散模型采样算法，在对数多项式步数内获δ误差，较先前结果有指数级提升，还给出不同条件下复杂度及对数凹分布采样器。


<details>
  <summary>Details</summary>
Motivation: 改进扩散模型采样算法的效率，降低达到指定误差所需的步数。

Method: 提出新的扩散模型采样算法，利用L²中约O(δ)精度的得分估计。

Result: 在不同条件下得出算法复杂度，如最小数据假设下为O(d polylog(1/δ))等；给出对数凹分布采样器复杂度。

Conclusion: 新算法较先前结果有指数级提升，能有效降低扩散模型采样复杂度。

Abstract: We present algorithms for diffusion model sampling which obtain $δ$-error in $\mathrm{polylog}(1/δ)$ steps, given access to $\widetilde O(δ)$-accurate score estimates in $L^2$. This is an exponential improvement over all previous results. Specifically, under minimal data assumptions, the complexity is $\widetilde O(d\,\mathrm{polylog}(1/δ))$ where $d$ is the dimension of the data; under a non-uniform $L$-Lipschitz condition, the complexity is $\widetilde O(\sqrt{dL}\,\mathrm{polylog}(1/δ))$; and if the data distribution has intrinsic dimension $d_\star$, then the complexity reduces to $\widetilde O(d_\star\,\mathrm{polylog}(1/δ))$. Our approach also yields the first $\mathrm{polylog}(1/δ)$ complexity sampler for general log-concave distributions using only gradient evaluations.

</details>


### [294] [AICD Bench: A Challenging Benchmark for AI-Generated Code Detection](https://arxiv.org/abs/2602.02079)
*Daniil Orel,Dilshod Azizov,Indraneil Paul,Yuxia Wang,Iryna Gurevych,Preslav Nakov*

Main category: cs.LG

TL;DR: 文章介绍AICD Bench基准，用于AI生成代码检测，测试发现当前检测器性能不佳，发布该基准以推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成代码检测的数据集和基准较局限，仅适用于分布内的二分类，需更全面的基准。

Method: 构建AICD Bench基准，涵盖200万个示例、77个模型、9种编程语言，提出三种检测任务。

Result: 对神经和经典检测器的广泛评估显示其性能远未达实际可用水平，尤其是在分布偏移及混合或对抗性代码检测上。

Conclusion: 发布AICD Bench作为统一且具挑战性的评估套件，以推动下一代鲁棒的AI生成代码检测方法。

Abstract: Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\emph{2M examples}$, $\emph{77 models}$ across $\emph{11 families}$, and $\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\emph{i}$)~$\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\emph{ii}$)~$\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\emph{iii}$)~$\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.

</details>


### [295] [Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting](https://arxiv.org/abs/2602.00240)
*Md Muhtasim Munif Fahim,Soyda Humyra Yesmin,Saiful Islam,Md. Palash Bin Faruque,Md. A. Salam,Md. Mahfuz Uddin,Samiul Islam,Tofayel Ahmed,Md. Binyamin,Md. Rezaul Karim*

Main category: cs.LG

TL;DR: 介绍了用于低资源环境的多目标NAS框架Green - NAS，以气象预报为例，该框架遵循'绿色AI'原则，找到轻量级高精度模型，还提及迁移学习对气象预报准确性的提升。


<details>
  <summary>Details</summary>
Motivation: 在低资源环境下，遵循'Green AI'原则，实现可持续部署，减少计算能源成本和碳足迹，同时保证模型准确性。

Method: 通过同时优化多个目标的优化过程，对Green - NAS架构搜索方法进行优化，以兼顾模型准确性和效率；还使用了迁移学习方法。

Result: 最佳模型Green - NAS - A仅用153k模型参数实现了0.0988的RMSE，比其他模型参数少239倍；迁移学习在历史气象数据有限时能将气象预报准确性提高约5.2%。

Conclusion: Green - NAS框架可在低资源环境下有效工作，既能降低计算成本，又能保证较高的气象预报准确性，迁移学习在特定情况下也能提升预报准确性。

Abstract: We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to 'Green AI' principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.

</details>


### [296] [TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models](https://arxiv.org/abs/2602.00250)
*Shreshth Saini,Avinab Saha,Balu Adsumilli,Neil Birkbeck,Yilin Wang,Alan C. Bovik*

Main category: cs.LG

TL;DR: 提出Backward-on-Entropy (BoE) Steering梯度引导推理框架用于掩码扩散模型，实现高效非自回归生成。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型采样方法存在局部决策忽视长期影响、搜索方法计算成本高的问题。

Method: 提出BoE Steering框架，推导Token Influence Score (TIS)，引入ActiveQueryAttention减少反向传播复杂度。

Result: BoE在推理时间扩展方面优于现有方法。

Conclusion: 梯度引导的BoE Steering为稳健的非自回归生成提供了数学上合理且高效的途径。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.

</details>


### [297] [An Odd Estimator for Shapley Values](https://arxiv.org/abs/2602.01399)
*Fabian Fumagalli,Landon Butler,Justin Singh Kang,Kannan Ramchandran,R. Teal Witter*

Main category: cs.LG

TL;DR: 本文针对Shapley值精确计算难的问题，为配对采样提供理论解释并提出新的一致估计器OddSHAP，经评估其估计精度达最优水平。


<details>
  <summary>Details</summary>
Motivation: Shapley值精确计算难，现有有效且流行的估计器虽利用配对采样启发式方法降低估计误差，但理论机制不明。

Method: 证明Shapley值仅依赖集合函数的奇分量，配对采样可过滤无关偶分量；提出OddSHAP，在奇子空间进行多项式回归，利用傅里叶基分离子空间和代理模型识别高影响交互。

Result: 通过广泛基准评估，OddSHAP达到了最先进的估计精度。

Conclusion: 为配对采样提供理论依据，提出的OddSHAP能有效克服高阶近似的组合爆炸问题，实现高精度估计。

Abstract: The Shapley value is a ubiquitous framework for attribution in machine learning, encompassing feature importance, data valuation, and causal inference. However, its exact computation is generally intractable, necessitating efficient approximation methods. While the most effective and popular estimators leverage the paired sampling heuristic to reduce estimation error, the theoretical mechanism driving this improvement has remained opaque. In this work, we provide an elegant and fundamental justification for paired sampling: we prove that the Shapley value depends exclusively on the odd component of the set function, and that paired sampling orthogonalizes the regression objective to filter out the irrelevant even component. Leveraging this insight, we propose OddSHAP, a novel consistent estimator that performs polynomial regression solely on the odd subspace. By utilizing the Fourier basis to isolate this subspace and employing a proxy model to identify high-impact interactions, OddSHAP overcomes the combinatorial explosion of higher-order approximations. Through an extensive benchmark evaluation, we find that OddSHAP achieves state-of-the-art estimation accuracy.

</details>


### [298] [DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data](https://arxiv.org/abs/2602.01433)
*Muhammad Hasan Ferdous,Md Osman Gani*

Main category: cs.LG

TL;DR: 提出基于分解的因果发现框架，分离时间序列各成分进行分析，在合成基准和真实气候数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列存在长期趋势、季节性模式和短期波动，现有因果发现方法基于原始观测，易产生虚假边和错误归因的时间依赖。

Method: 将时间序列分解为趋势、季节和残差成分，分别进行因果分析，最后集成到统一的多尺度因果结构中。

Result: 在广泛的合成基准和真实世界气候数据中，该框架比现有基线方法更准确地恢复了真实因果结构。

Conclusion: 该方法能分离长短期因果效应，减少虚假关联，提高可解释性，尤其在强非平稳性和时间自相关情况下表现出色。

Abstract: Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw observations, making them vulnerable to spurious edges and misattributed temporal dependencies. We introduce a decomposition-based causal discovery framework that separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis. Trend components are assessed using stationarity tests, seasonal components using kernel-based dependence measures, and residual components using constraint-based causal discovery. The resulting component-level graphs are integrated into a unified multi-scale causal structure. This approach isolates long- and short-range causal effects, reduces spurious associations, and improves interpretability. Across extensive synthetic benchmarks and real-world climate data, our framework more accurately recovers ground-truth causal structure than state-of-the-art baselines, particularly under strong non-stationarity and temporal autocorrelation.

</details>


### [299] [Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning](https://arxiv.org/abs/2602.00282)
*Naman Saxena,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文分析了约束双层强化学习算法的样本复杂度，提出CBSO算法并得到其迭代和样本复杂度，用基于惩罚的目标函数处理约束，首次用Moreau包络分析非光滑目标函数的策略梯度强化学习算法。


<details>
  <summary>Details</summary>
Motivation: 现有文献中对双层强化学习算法的理论分析关注不足，需要对约束双层强化学习算法的样本复杂度进行分析。

Method: 在无约束设置的进展基础上分析约束双层强化学习算法；使用基于惩罚的目标函数处理约束问题；用Moreau包络分析带非光滑目标函数的策略梯度强化学习算法。

Result: 提出的CBSO算法迭代复杂度为 $O(ε^{-2})$ ，样本复杂度为 $	ilde{O}(ε^{-4})$ 。

Conclusion: 成功分析了约束双层强化学习算法的样本复杂度，且首次用Moreau包络等方法分析非光滑目标函数的策略梯度强化学习算法。

Abstract: Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(ε^{-2})$ and sample complexity of $\tilde{O}(ε^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.

</details>


### [300] [Theoretical Analysis of Measure Consistency Regularization for Partially Observed Data](https://arxiv.org/abs/2602.01437)
*Yinsong Wang,Shahin Shahrampour*

Main category: cs.LG

TL;DR: 本文聚焦MCR方法，分析其理论基础，提出新训练协议并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习中数据损坏等问题突出，MCR方法虽有应用但理论理解有限，需深入研究其理论基础。

Method: 从神经网络距离视角进行理论分析，提出监测对偶间隙确定早停点的训练协议。

Result: 理论分析找出MCR泛化优势的关键项，实验验证了理论主张、早停条件的有效性和准确性，模拟显示了MCR在不同模型架构下的通用性。

Conclusion: 通过理论和实验，深入理解了MCR在部分可观测下提升插补质量的原理，提出的训练协议有效。

Abstract: The problem of corrupted data, missing features, or missing modalities continues to plague the modern machine learning landscape. To address this issue, a class of regularization methods that enforce consistency between imputed and fully observed data has emerged as a promising approach for improving model generalization, particularly in partially observed settings. We refer to this class of methods as Measure Consistency Regularization (MCR). Despite its empirical success in various applications, such as image inpainting, data imputation and semi-supervised learning, a fundamental understanding of the theoretical underpinnings of MCR remains limited. This paper bridges this gap by offering theoretical insights into why, when, and how MCR enhances imputation quality under partial observability, viewed through the lens of neural network distance.
  Our theoretical analysis identifies the term responsible for MCR's generalization advantage and extends to the imperfect training regime, demonstrating that this advantage is not always guaranteed. Guided by these insights, we propose a novel training protocol that monitors the duality gap to determine an early stopping point that preserves the generalization benefit. We then provide detailed empirical evidence to support our theoretical claims and to show the effectiveness and accuracy of our proposed stopping condition. We further provide a set of real-world data simulations to show the versatility of MCR under different model architectures designed for different data sources.

</details>


### [301] [Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.00286)
*Shaorong Zhang,Longxuan Yu,Rob Brekelmans,Luhan Tang,Salman Asif,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 本文提出统一信息论框架分析掩码扩散模型（MDMs）生成顺序和并行化问题，有三点关键见解并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: MDMs虽加速推理，但生成顺序理论机制和并行化风险研究不足。

Method: 提出统一信息论框架解耦分析MDMs失败的两个根本原因。

Result: 得到三点关键见解，如Easy - First解码优势随模型误差增加而放大等，实验验证理论框架。

Conclusion: 验证可消除采样误差但成本高，启发式方法计算高效但不能保证分布正确性。

Abstract: Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework to decouple and analyze two fundamental sources of failure: order sensitivity and parallelization bias. Our analysis yields three key insights: (1) The benefits of Easy-First decoding (prioritizing low-entropy tokens) are magnified as model error increases; (2) factorized parallel decoding introduces intrinsic sampling errors that can lead to arbitrary large Reverse KL divergence, capturing "incoherence" failures that standard Forward KL metrics overlook; and (3) while verification can eliminate sampling error, it incurs an exponential cost governed by the total correlation within a block. Conversely, heuristics like remasking, though computationally efficient, cannot guarantee distributional correctness. Experiments on a controlled Block-HMM and large-scale MDMs (LLaDA) for arithmetic reasoning validate our theoretical framework.

</details>


### [302] [A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts](https://arxiv.org/abs/2602.01468)
*Viet Nguyen,Tuan Minh Pham,Thinh Cao,Tan Dinh,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: cs.LG

TL;DR: 文章证明门控注意力矩阵可写成专家的分层混合，表明其比多头自注意力更具样本效率，并解释其在特定位置表现更好的原因。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对门控注意力优势的清晰理论理解，本文旨在填补这一空白。

Method: 将学习重铸为专家估计问题，分析门控注意力矩阵和多头自注意力矩阵中元素的形式。

Result: 门控注意力比多头自注意力更具样本效率，前者估计专家只需多项式数量的数据点，后者需指数数量数据点达到相同估计误差。

Conclusion: 分析为门控注意力在多头自注意力架构特定位置表现更好提供了理论依据。

Abstract: Self-attention has greatly contributed to the success of the widely used Transformer architecture by enabling learning from data with long-range dependencies. In an effort to improve performance, a gated attention model that leverages a gating mechanism within the multi-head self-attention has recently been proposed as a promising alternative. Gated attention has been empirically demonstrated to increase the expressiveness of low-rank mapping in standard attention and even to eliminate the attention sink phenomenon. Despite its efficacy, a clear theoretical understanding of gated attention's benefits remains lacking in the literature. To close this gap, we rigorously show that each entry in a gated attention matrix or a multi-head self-attention matrix can be written as a hierarchical mixture of experts. By recasting learning as an expert estimation problem, we demonstrate that gated attention is more sample-efficient than multi-head self-attention. In particular, while the former needs only a polynomial number of data points to estimate an expert, the latter requires exponentially many data points to achieve the same estimation error. Furthermore, our analysis also provides a theoretical justification for why gated attention yields higher performance when a gate is placed at the output of the scaled dot product attention or the value map rather than at other positions in the multi-head self-attention architecture.

</details>


### [303] [From Observations to States: Latent Time Series Forecasting](https://arxiv.org/abs/2602.00297)
*Jie Yang,Yifan Hu,Yuante Li,Kexin Zhang,Kaize Ding,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文指出时间序列预测中存在潜在混沌问题，提出LatentTSF范式解决该问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习在时间序列预测中存在潜在混沌问题，即准确预测的模型学习的潜在表征缺乏时间连续性，原因是主流的观测空间预测范式。

Method: 提出LatentTSF范式，用自编码器将观测投影到高维潜在状态空间，在潜在空间进行预测。

Result: 理论分析表明潜在目标隐式最大化预测潜在状态与真实状态和观测之间的互信息，实验证明LatentTSF有效缓解潜在混沌，性能优越。

Conclusion: LatentTSF范式能有效解决时间序列预测中的潜在混沌问题，提升预测性能。

Abstract: Deep learning has achieved strong performance in Time Series Forecasting (TSF). However, we identify a critical representation paradox, termed Latent Chaos: models with accurate predictions often learn latent representations that are temporally disordered and lack continuity. We attribute this phenomenon to the dominant observation-space forecasting paradigm. Most TSF models minimize point-wise errors on noisy and partially observed data, which encourages shortcut solutions instead of the recovery of underlying system dynamics. To address this issue, we propose Latent Time Series Forecasting (LatentTSF), a novel paradigm that shifts TSF from observation regression to latent state prediction. Specifically, LatentTSF employs an AutoEncoder to project observations at each time step into a higher-dimensional latent state space. This expanded representation aims to capture underlying system variables and impose a smoother temporal structure. Forecasting is then performed entirely in the latent space, allowing the model to focus on learning structured temporal dynamics. Theoretical analysis demonstrates that our proposed latent objectives implicitly maximize mutual information between predicted latent states and ground-truth states and observations. Extensive experiments on widely-used benchmarks confirm that LatentTSF effectively mitigates latent chaos, achieving superior performance. Our code is available in https://github.com/Muyiiiii/LatentTSF.

</details>


### [304] [Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability](https://arxiv.org/abs/2602.01480)
*Eric Regis,Sinho Chewi*

Main category: cs.LG

TL;DR: 本文提出Rod Flow作为梯度下降（GD）动态的替代ODE近似，具有原理推导、能捕捉GD动态、计算简单等优点，并通过理论证明和数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 理解基于梯度在非凸景观上的训练，现有大学习率下GD与梯度流偏离问题，Central Flow虽有近似但需新的替代方法。

Method: 提出Rod Flow作为替代ODE近似，从物理图像推导，进行理论证明和数值实验。

Result: Rod Flow能正确预测临界锐度阈值，解释四次势中的自稳定，在简单例子中更好捕捉GD动态，在代表性神经网络架构中与Central Flow精度匹配。

Conclusion: Rod Flow是一种有效的GD动态近似方法，有理论和实际优势。

Abstract: How can we understand gradient-based training over non-convex landscapes? The edge of stability phenomenon, introduced in Cohen et al. (2021), indicates that the answer is not so simple: namely, gradient descent (GD) with large step sizes often diverges away from the gradient flow. In this regime, the "Central Flow", recently proposed in Cohen et al. (2025), provides an accurate ODE approximation to the GD dynamics over many architectures. In this work, we propose Rod Flow, an alternative ODE approximation, which carries the following advantages: (1) it rests on a principled derivation stemming from a physical picture of GD iterates as an extended one-dimensional object -- a "rod"; (2) it better captures GD dynamics for simple toy examples and matches the accuracy of Central Flow for representative neural network architectures, and (3) is explicit and cheap to compute. Theoretically, we prove that Rod Flow correctly predicts the critical sharpness threshold and explains self-stabilization in quartic potentials. We validate our theory with a range of numerical experiments.

</details>


### [305] [Agentic Framework for Epidemiological Modeling](https://arxiv.org/abs/2602.00299)
*Rituparna Datta,Zihan Guan,Baltazar Espinoza,Yiqi Su,Priya Pitre,Srini Venkatramanan,Naren Ramakrishnan,Anil Vullikanti*

Main category: cs.LG

TL;DR: 提出EPIAGENT框架解决传统流行病建模方法不足，评估显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统流行病建模方法依赖固定模型类，随病原体、政策和假设变化需人工重新设计。

Method: 将疾病进展建模为迭代程序合成问题，通过显式流行病学流程图中间表示进行模型合成、校准、验证和改进，最终编译成支持参数学习的机械模型。

Result: EPIAGENT能捕捉复杂增长动态，产生符合流行病学的反事实预测，代理反馈循环防止退化并加速收敛。

Conclusion: EPIAGENT框架有效，为流行病建模提供新方案。

Abstract: Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.

</details>


### [306] [Predicting and improving test-time scaling laws via reward tail-guided search](https://arxiv.org/abs/2602.01485)
*Muheng Li,Jian Qian,Wenlong Mou*

Main category: cs.LG

TL;DR: 本文提出通过尾部引导搜索预测和改善大语言模型（LLMs）扩展属性的新方法，理论证明新算法效果好，实验验证其优于Best-of-$N$。


<details>
  <summary>Details</summary>
Motivation: 现有“Best-of-$N$”策略在$N$选择、预算分配和多阶段决策缺乏原则性指导，且相关优化工作缺乏严格理论保证。

Method: 通过估计奖励的尾部分布预测LLMs扩展规律，引入Scaling-Law Guided（SLG）搜索算法动态分配计算资源。

Result: 理论证明SLG与完美信息预言机相比遗憾消失，在相同计算预算下，尾部引导分配比Best-of-$N$获得更高奖励。

Conclusion: 提出的新方法能有效预测和改善LLMs的扩展属性，优于现有策略。

Abstract: Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.

</details>


### [307] [Neural Ising Machines via Unrolling and Zeroth-Order Training](https://arxiv.org/abs/2602.00302)
*Sam Reifenstein,Timothee Leleu*

Main category: cs.LG

TL;DR: 提出一种数据驱动启发式方法NPIM用于NP难的Ising和Max - Cut优化，学习迭代动力系统更新规则，在标准基准测试中有竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决NP难的Ising和Max - Cut优化问题，找到高效的搜索算法。

Method: 学习一个共享的节点式更新规则，用小型多层感知机参数化，使用零阶优化器训练，称为神经网络参数化Ising机（NPIM）。

Result: 在标准Ising和神经组合优化基准测试中，NPIM在解的质量和求解时间上相对于基于学习的方法和经典Ising机启发式方法有竞争力。

Conclusion: 尽管参数数量少，但NPIM学习到的动力学能恢复有效算法结构，可在高度非凸能量景观中进行有效搜索。

Abstract: We propose a data-driven heuristic for NP-hard Ising and Max-Cut optimization that learns the update rule of an iterative dynamical system. The method learns a shared, node-wise update rule that maps local interaction fields to spin updates, parameterized by a compact multilayer perceptron with a small number of parameters. Training is performed using a zeroth-order optimizer, since backpropagation through long, recurrent Ising-machine dynamics leads to unstable and poorly informative gradients. We call this approach a neural network parameterized Ising machine (NPIM). Despite its low parameter count, the learned dynamics recover effective algorithmic structure, including momentum-like behavior and time-varying schedules, enabling efficient search in highly non-convex energy landscapes. Across standard Ising and neural combinatorial optimization benchmarks, NPIM achieves competitive solution quality and time-to-solution relative to recent learning-based methods and strong classical Ising-machine heuristics.

</details>


### [308] [Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum](https://arxiv.org/abs/2602.01505)
*Navdeep Kumar,Tehila Dahan,Lior Cohen,Ananyabrata Barua,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: 本文为无限水平折扣马尔可夫决策过程中使用单时间尺度演员 - 评论家算法获得ε最优全局策略建立了O(ε⁻²)的最优样本复杂度，改进了现有工作。


<details>
  <summary>Details</summary>
Motivation: 改进无限水平折扣马尔可夫决策过程中获得ε最优全局策略的样本复杂度。

Method: 应用STORM减少评论家更新中的方差，维护一个近期样本小缓冲区并从中均匀采样进行更新。

Result: 建立了O(ε⁻²)的最优样本复杂度，优于之前的O(ε⁻³)。

Conclusion: 所提机制与现有深度学习架构兼容，修改小且不影响实际应用。

Abstract: We establish an optimal sample complexity of $O(ε^{-2})$ for obtaining an $ε$-optimal global policy using a single-timescale actor-critic (AC) algorithm in infinite-horizon discounted Markov decision processes (MDPs) with finite state-action spaces, improving upon the prior state of the art of $O(ε^{-3})$. Our approach applies STORM (STOchastic Recursive Momentum) to reduce variance in the critic updates. However, because samples are drawn from a nonstationary occupancy measure induced by the evolving policy, variance reduction via STORM alone is insufficient. To address this challenge, we maintain a buffer of small fraction of recent samples and uniformly sample from it for each critic update. Importantly, these mechanisms are compatible with existing deep learning architectures and require only minor modifications, without compromising practical applicability.

</details>


### [309] [Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors](https://arxiv.org/abs/2602.00315)
*Arian Khorasani,Nathaniel Chen,Yug D Oswal,Akshat Santhana Gopalan,Egemen Kolemen,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: 本文利用类别条件归一化流使真实图像上的精确后验变得易于处理，展开五项研究，揭示标准指标存在的问题。


<details>
  <summary>Details</summary>
Motivation: 标准基准测试因无法获取真实后验p(y|x)，不能回答神经网络离最优性能有多接近的问题。

Method: 使用类别条件归一化流作为神谕，在现实图像上使精确后验易于处理，开展五项研究。

Result: 发现预测误差分解规律、不同架构接近随机误差下限的差异、软标签优势、分布偏移特点和主动学习中精确认知不确定性的作用。

Conclusion: 标准指标掩盖了持续学习、架构差异，无法诊断分布偏移的本质。

Abstract: How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enables five lines of investigation. Scaling laws: Prediction error decomposes into irreducible aleatoric uncertainty and reducible epistemic error; the epistemic component follows a power law in dataset size, continuing to shrink even when total loss plateaus. Limits of learning: The aleatoric floor is exactly measurable, and architectures differ markedly in how they approach it: ResNets exhibit clean power-law scaling while Vision Transformers stall in low-data regimes. Soft labels: Oracle posteriors contain learnable structure beyond class labels: training with exact posteriors outperforms hard labels and yields near-perfect calibration. Distribution shift: The oracle computes exact KL divergence of controlled perturbations, revealing that shift type matters more than shift magnitude: class imbalance barely affects accuracy at divergence values where input noise causes catastrophic degradation. Active learning: Exact epistemic uncertainty distinguishes genuinely informative samples from inherently ambiguous ones, improving sample efficiency. Our framework reveals that standard metrics hide ongoing learning, mask architectural differences, and cannot diagnose the nature of distribution shift.

</details>


### [310] [Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection](https://arxiv.org/abs/2602.00318)
*Kunal Mukherjee,Zulfikar Alom,Tran Gia Bao Ngo,Cuneyt Gurcan Akcora,Murat Kantarcioglu*

Main category: cs.LG

TL;DR: 提出BOCLOAK在现实约束下评估基于GNN的社交机器人检测的鲁棒性，实验表明其攻击成功率高且内存使用少，证明最优传输可用于实际检测。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上机器人账号带来风险，现有的基于GNN的探测器在现实场景中的有效性未知，需要能适应现实攻击场景的鲁棒检测方法。

Method: 引入BOCLOAK，构建时空邻域特征的概率测度，学习最优传输几何，将传输计划解码为稀疏、合理的边编辑。

Result: 在三个数据集、五个检测器、三种防御和四个基线对比实验中，BOCLOAK攻击成功率高80.13%，GPU内存使用少99.80%。

Conclusion: 最优传输为弥合对抗攻击和现实机器人检测之间的差距提供了一个轻量级的原则性框架。

Abstract: The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.
  To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.

</details>


### [311] [Universal Redundancies in Time Series Foundation Models](https://arxiv.org/abs/2602.01605)
*Anthony Bao,Venkata Hasith Vattikuti,Jeffrey Lai,William Gilpin*

Main category: cs.LG

TL;DR: 研究发现基于Transformer的时间序列基础模型（TSFMs）中间层有冗余组件，引入可解释性工具，揭示模型对层消融的鲁棒性，开发理论框架找出导致退化现象的注意力头。


<details>
  <summary>Details</summary>
Motivation: 了解TSFMs的内部机制，解释其在时间序列预测中的表现和存在的退化现象。

Method: 大规模标准基准评估，采用组件消融和残差流直接对数归因等可解释性工具，开发将Transformer视为核回归器的理论框架。

Result: 发现模型对层消融具有鲁棒性，找出导致TSFMs退化现象的特定注意力头。

Conclusion: 揭示了用于连续时间序列建模的这类新兴架构的普遍属性。

Abstract: Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual stream. Our findings are consistent across several leading TSFMs with diverse architectures, and across a diverse set of real-world and synthetic time-series datasets. We discover that all models in our study are robust to ablations of entire layers. Furthermore, we develop a theoretical framework framing transformers as kernel regressors, motivating a purely intrinsic strategy for ablating heads based on the stable rank of the per-head projection matrices. Using this approach, we uncover the specific heads responsible for degenerate phenomena widely observed in TSFMs, such as parroting of motifs from the context and seasonality bias. Our study sheds light on the universal properties of this emerging class of architectures for continuous-time sequence modeling.

</details>


### [312] [Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference](https://arxiv.org/abs/2602.00328)
*Nikhil Gopal,Kostis Kaffes*

Main category: cs.LG

TL;DR: 提出Harvest框架，利用GPU互连动态管理缓存，加速推理组件获取，使吞吐量提升超两倍。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理受GPU内存容量约束，现有卸载方案因PCIe带宽有限导致延迟高。

Method: 提出Harvest这一机会主义GPU缓存管理框架，利用高带宽对等GPU互连，将模型权重和KV缓存动态放置在未使用的GPU内存中。

Result: 使用Harvest加速两个常用推理组件获取时，吞吐量显著提升超两倍。

Conclusion: Harvest框架可在动态内存可用性下减少数据移动开销，提升吞吐量。

Abstract: Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.

</details>


### [313] [In-Run Data Shapley for Adam Optimizer](https://arxiv.org/abs/2602.00329)
*Meng Ding,Zeqing Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 现有数据归因方法在Adam优化器下效果不佳，本文提出Adam - Aware In - Run Data Shapley方法，实验显示该方法保真度高、训练吞吐量损失小，且下游任务表现优于SGD基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前的'In - Run'方法依赖SGD线性结构，无法捕捉Adam等自适应优化器的复杂动态，无法在现代训练管道中有效进行数据归因。

Method: 提出Adam - Aware In - Run Data Shapley方法，推导封闭形式近似，通过Linearized Ghost Approximation实现可扩展计算，线性化方差相关缩放项以避免逐样本梯度计算。

Result: 该方法对真实边际贡献的保真度接近完美（$R > 0.99$），保留了约95%的标准训练吞吐量；在数据归因下游任务中显著优于基于SGD的基线方法。

Conclusion: 数据归因本质上依赖于优化器，Adam - Aware In - Run Data Shapley方法能有效解决在Adam优化器下的数据归因问题。

Abstract: Reliable data attribution is essential for mitigating bias and reducing computational waste in modern machine learning, with the Shapley value serving as the theoretical gold standard. While recent "In-Run" methods bypass the prohibitive cost of retraining by estimating contributions dynamically, they heavily rely on the linear structure of Stochastic Gradient Descent (SGD) and fail to capture the complex dynamics of adaptive optimizers like Adam. In this work, we demonstrate that data attribution is inherently optimizer-dependent: we show that SGD-based proxies diverge significantly from true contributions under Adam (Pearson $R \approx 0.11$), rendering them ineffective for modern training pipelines. To bridge this gap, we propose Adam-Aware In-Run Data Shapley. We derive a closed-form approximation that restores additivity by redefining utility under a fixed-state assumption and enable scalable computation via a novel Linearized Ghost Approximation. This technique linearizes the variance-dependent scaling term, allowing us to compute pairwise gradient dot-products without materializing per-sample gradients. Extensive experiments show that our method achieves near-perfect fidelity to ground-truth marginal contributions ($R > 0.99$) while retaining $\sim$95\% of standard training throughput. Furthermore, our Adam-aware attribution significantly outperforms SGD-based baselines in data attribution downstream tasks.

</details>


### [314] [Prototype-based Explainable Neural Networks with Channel-specific Reasoning for Geospatial Learning Tasks](https://arxiv.org/abs/2602.00331)
*Anushka Narayanan,Karianne J. Bergen*

Main category: cs.LG

TL;DR: 本文开发了一种适用于多通道地理空间数据的基于原型的可解释AI方法，通过两个地学案例展示其效果，增强了机器学习模型的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的可解释AI方法主要针对标准RGB图像数据，未针对地学图像和栅格数据的多通道特性优化，需要开发适用于多通道地理空间数据的方法。

Method: 开发一种基于原型的可解释AI方法，使模型能识别多通道特定的原型特征，结合多个训练示例信息影响模型预测。

Result: 通过两个地学案例（Madden Julian振荡阶段分类和卫星影像土地利用分类）证明该方法能产生局部和全局解释，性能与标准神经网络相当。

Conclusion: 该方法通过明确将通道原型纳入预测过程，增强了地学学习任务中机器学习模型的透明度和可信度。

Abstract: Explainable AI (XAI) is essential for understanding machine learning (ML) decision-making and ensuring model trustworthiness in scientific applications. Prototype-based XAI methods offer an intrinsically interpretable alternative to post-hoc approaches which often yield inconsistent explanations. Prototype-based XAI methods make predictions based on the similarity between inputs and learned prototypes that represent typical characteristics of target classes. However, existing prototype-based models are primarily designed for standard RGB image data and are not optimized for the distinct, variable-specific channels commonly found in geoscientific image and raster datasets. In this study, we develop a prototype-based XAI approach tailored for multi-channel geospatial data, where each channel represents a distinct physical environmental variable or spectral channel. Our approach enables the model to identify separate, channel-specific prototypical characteristics sourced from multiple distinct training examples that inform how these features individually and in combination influence model prediction while achieving comparable performance to standard neural networks. We demonstrate this method through two geoscientific case studies: (1) classification of Madden Julian Oscillation phases using multi-variable climate data and (2) land-use classification from multispectral satellite imagery. This approach produces both local (instance-level) and global (model-level) explanations for providing insights into feature-relevance across channels. By explicitly incorporating channel-prototypes into the prediction process, we discuss how this approach enhances the transparency and trustworthiness of ML models for geoscientific learning tasks.

</details>


### [315] [Efficient and accurate steering of Large Language Models through attention-guided feature learning](https://arxiv.org/abs/2602.00333)
*Parmida Davarmanesh,Ashia Wilson,Adityanarayanan Radhakrishnan*

Main category: cs.LG

TL;DR: 提出注意力引导的转向框架，克服转向挑战，提升转向效果，揭示概念特征分布，为大模型微调算法开辟新途径


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型转向方法脆弱，受算法选择影响大

Method: 引入注意力引导的转向框架，解决自动选择相关特征、考虑特征异质性和确定相关层的挑战

Result: 在512个语义概念基准测试上，大幅提升转向效果，几乎使成功转向的概念数量翻倍

Conclusion: 框架为开发高效、可扩展的工业级大语言模型微调算法开辟了更多途径

Abstract: Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.

</details>


### [316] [Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions](https://arxiv.org/abs/2602.01777)
*M. Arashi,M. Amintoosi*

Main category: cs.LG

TL;DR: 本文将随机梯度计算视为高维估计问题，引入基于Stein规则收缩的决策理论框架，构建收缩梯度估计器，证明其在平方误差损失下优于标准随机梯度，将其融入Adam优化器在CIFAR数据集上取得更好表现。


<details>
  <summary>Details</summary>
Motivation: 在高维环境下，标准随机梯度作为总体梯度的无偏估计可能并非最优，从风险角度来看存在不足，需要改进。

Method: 将随机梯度计算构建为高维估计问题，基于Stein规则收缩引入决策理论框架，构建收缩梯度估计器，根据数据驱动确定收缩强度，并将估计器融入Adam优化器。

Result: 在高斯噪声模型且维度p>=3时，提出的估计器在平方误差损失下均匀优于标准随机梯度，在CIFAR10和CIFAR100数据集的大批次训练中，改进后的算法比Adam表现更好。

Conclusion: 经典的收缩原则为改进现代深度学习中的随机梯度估计提供了一种有原则且有效的方法。

Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.

</details>


### [317] [Adaptive Momentum and Nonlinear Damping for Neural Network Training](https://arxiv.org/abs/2602.00334)
*Aikaterini Karoni,Rajit Rajpal,Benedict Leimkuhler,Gabriel Stoltz*

Main category: cs.LG

TL;DR: 提出含自适应动量系数的连续时间优化方案，关联立方阻尼，有具体优化方案，实证表现好且有理论收敛结果。


<details>
  <summary>Details</summary>
Motivation: 为大规模优化设计能自动适应局部曲率、保持稳定且不牺牲收敛速度的方案。

Method: 引入由模型参数动能调节的自适应动量系数，关联立方阻尼，在mSGD和Adam的连续动力学中加入立方阻尼项。

Result: 方法在训练ViT、BERT和GPT2任务中表现稳健，匹配或超越Adam，mSGD通常在此类任务中表现不佳。

Conclusion: 提出的优化方案具有指数收敛性。

Abstract: We propose a continuous-time scheme for large-scale optimization that introduces individual, adaptive momentum coefficients regulated by the kinetic energy of each model parameter. This approach automatically adjusts to local landscape curvature to maintain stability without sacrificing convergence speed. We demonstrate that our adaptive friction can be related to cubic damping, a suppression mechanism from structural dynamics. Furthermore, we introduce two specific optimization schemes by augmenting the continuous dynamics of mSGD and Adam with a cubic damping term. Empirically, our methods demonstrate robustness and match or outperform Adam on training ViT, BERT, and GPT2 tasks where mSGD typically struggles. We further provide theoretical results establishing the exponential convergence of the proposed schemes.

</details>


### [318] [Planning with Language and Generative Models: Toward General Reward-Guided Wireless Network Design](https://arxiv.org/abs/2602.00357)
*Chenyang Yuan,Xiaoyuan Cheng*

Main category: cs.LG

TL;DR: 文章探讨下一代无线网络中智能接入点部署问题，对比通用大模型后，研究基于统一奖励函数的生成推理模型，发现扩散采样器表现优，还引入真实数据集评估，认为基于扩散的生成推理提供可行基础。


<details>
  <summary>Details</summary>
Motivation: 因复杂室内环境和信号传播，智能接入点部署困难，通用大语言模型在接入点规划中有高计算成本和可扩展性有限的问题。

Method: 研究由统一奖励函数引导的生成推理模型，对比不同生成方法，引入大规模室内接入点部署真实数据集进行评估。

Result: 扩散采样器始终优于其他生成方法，扩散过程能有效改善采样，所提出方法具有良好的分布内、外泛化性和鲁棒性。

Conclusion: 基于扩散的、带有统一奖励函数的生成推理为室内接入点部署规划提供可扩展且领域无关的基础。

Abstract: Intelligent access point (AP) deployment remains challenging in next-generation wireless networks due to complex indoor geometries and signal propagation. We firstly benchmark general-purpose large language models (LLMs) as agentic optimizers for AP planning and find that, despite strong wireless domain knowledge, their dependence on external verifiers results in high computational costs and limited scalability. Motivated by these limitations, we study generative inference models guided by a unified reward function capturing core AP deployment objectives across diverse floorplans. We show that diffusion samplers consistently outperform alternative generative approaches. The diffusion process progressively improves sampling by smoothing and sharpening the reward landscape, rather than relying on iterative refinement, which is effective for non-convex and fragmented objectives. Finally, we introduce a large-scale real-world dataset for indoor AP deployment, requiring over $50k$ CPU hours to train general reward functions, and evaluate in- and out-of-distribution generalization and robustness. Our results suggest that diffusion-based generative inference with a unified reward function provides a scalable and domain-agnostic foundation for indoor AP deployment planning.

</details>


### [319] [Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning](https://arxiv.org/abs/2602.01853)
*Xiangkun Wu,Qianglin Wen,Yingying Zhang,Hongtu Zhu,Ting Li,Chengchun Shi*

Main category: cs.LG

TL;DR: A/B测试用于时间序列实验有挑战，现有设计有局限，提出Transformer强化学习方法，在多数据集上表现优于现有设计。


<details>
  <summary>Details</summary>
Motivation: A/B测试应用于时间序列实验有挑战，现有设计未充分利用历史信息且依赖强假设来优化设计。

Method: 先建立不可能定理，再提出Transformer强化学习方法，利用Transformer考虑全历史信息，用强化学习直接优化均方误差。

Result: 在合成数据、公开调度模拟器和真实拼车数据集上的实证评估表明，提出的方法始终优于现有设计。

Conclusion: 提出的Transformer强化学习方法能有效解决现有设计的局限，提升A/B测试在时间序列实验中的性能。

Abstract: A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.

</details>


### [320] [Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition](https://arxiv.org/abs/2602.00360)
*Sumana Biswas,Karen Young,Josephine Griffith*

Main category: cs.LG

TL;DR: 本文针对多模态情感分析问题，在两个数据集上实验，提出TEMSA方法，结果表明结合图像物体名和文本的TEMS能提升多模态情感分析效果。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析因文本和图像模态差异、情感模糊性和上下文含义复杂性存在挑战。

Method: 在两个数据集上分别和组合分析图像与文本数据情感，提出基于目标识别方法的TEMSA，提取图像中检测到的所有物体名称并与关联文本结合形成TEMS。

Result: 考虑所有物体名称时，TEMS比单独分析能改善多模态数据整体情感分析结果。

Conclusion: 本研究推动了多模态情感分析发展，证明了TEMSA在结合图像和文本数据进行多模态情感分析中的有效性。

Abstract: Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.

</details>


### [321] [Observation-dependent Bayesian active learning via input-warped Gaussian processes](https://arxiv.org/abs/2602.01898)
*Sanna Jarl,Maria Bånkestad,Jonathan J. S. Scragg,Jens Sjölund*

Main category: cs.LG

TL;DR: 提出通过学习的单调重新参数化扭曲输入空间注入观测依赖反馈，用新自监督目标训练，提高主动学习样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程替代方法中后验方差仅通过超参数依赖观测输出，探索对实际测量不敏感。

Method: 通过学习的单调重新参数化扭曲输入空间注入观测依赖反馈，可用边际似然训练，新自监督目标性能更佳。

Result: 在一系列主动学习基准中提高了样本效率，在非平稳挑战传统方法的情况下表现尤其好。

Conclusion: 提出的方法有效，能解决传统方法在非平稳情况下的问题，提高主动学习效果。

Abstract: Bayesian active learning relies on the precise quantification of predictive uncertainty to explore unknown function landscapes. While Gaussian process surrogates are the standard for such tasks, an underappreciated fact is that their posterior variance depends on the observed outputs only through the hyperparameters, rendering exploration largely insensitive to the actual measurements. We propose to inject observation-dependent feedback by warping the input space with a learned, monotone reparameterization. This mechanism allows the design policy to expand or compress regions of the input space in response to observed variability, thereby shaping the behavior of variance-based acquisition functions. We demonstrate that while such warps can be trained via marginal likelihood, a novel self-supervised objective yields substantially better performance. Our approach improves sample efficiency across a range of active learning benchmarks, particularly in regimes where non-stationarity challenges traditional methods.

</details>


### [322] [Quantum Generator Kernels](https://arxiv.org/abs/2602.00361)
*Philipp Altmann,Maximilian Mansky,Maximilian Zorn,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 文章提出量子生成核（QGKs）方法，解决现有量子设备数据处理局限问题，实验显示其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习受NISQ硬件限制，需有效策略将大规模数据压缩嵌入量子设备，且现有混合架构策略有不足。

Method: 提出基于生成器的量子生成核（QGKs）方法，由一组变分生成器组（VGGs）组成，训练权重向量优化核对齐。

Result: 实验表明QGK在投影和分类能力上优于现有量子和经典核方法。

Conclusion: QGK有潜力成为各种量子机器学习应用的通用框架。

Abstract: Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.

</details>


### [323] [Data- and Variance-dependent Regret Bounds for Online Tabular MDPs](https://arxiv.org/abs/2602.01903)
*Mingyi Li,Taira Tsuchiya,Kenji Yamanishi*

Main category: cs.LG

TL;DR: 本文研究在线情节表格型马尔可夫决策过程，开发算法在对抗和随机场景达到不同悔恨界，建立了悔恨下界。


<details>
  <summary>Details</summary>
Motivation: 在已知转移的在线情节表格型马尔可夫决策过程中，开发能在对抗和随机场景达到更优悔恨界的算法。

Method: 基于带对数障碍正则化的乐观跟随正则化领导者，开发基于全局优化和策略优化的算法，利用新的乐观Q函数估计器。

Result: 全局优化算法在对抗场景实现多种悔恨界，随机场景实现方差感知的界；策略优化算法也有相同适应性；建立了悔恨下界。

Conclusion: 全局优化方法实现的悔恨上界近乎最优。

Abstract: This work studies online episodic tabular Markov decision processes (MDPs) with known transitions and develops best-of-both-worlds algorithms that achieve refined data-dependent regret bounds in the adversarial regime and variance-dependent regret bounds in the stochastic regime. We quantify MDP complexity using a first-order quantity and several new data-dependent measures for the adversarial regime, including a second-order quantity and a path-length measure, as well as variance-based measures for the stochastic regime. To adapt to these measures, we develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization. For global optimization, our algorithms achieve first-order, second-order, and path-length regret bounds in the adversarial regime, and in the stochastic regime, they achieve a variance-aware gap-independent bound and a variance-aware gap-dependent bound that is polylogarithmic in the number of episodes. For policy optimization, our algorithms achieve the same data- and variance-dependent adaptivity, up to a factor of the episode horizon, by exploiting a new optimistic $Q$-function estimator. Finally, we establish regret lower bounds in terms of data-dependent complexity measures for the adversarial regime and a variance measure for the stochastic regime, implying that the regret upper bounds achieved by the global-optimization approach are nearly optimal.

</details>


### [324] [Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation](https://arxiv.org/abs/2602.00372)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 介绍了Sparse Knowledge Distillation (SparseKD) 方法压缩Transformer模型，实验显示效果良好且易部署。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署成本高，需要压缩模型。

Method: 结合结构化SVD剪枝和自引用知识蒸馏的后训练方法。

Result: 自引用蒸馏单独使用使模型质量提升39%，结合剪枝实现15 - 65%参数减少，速度提升源于前馈层矩阵乘法减少。

Conclusion: SparseKD无需外部教师、架构改变和自定义推理内核，可利用现有基础设施立即部署。

Abstract: Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external teacher, the model teaches itself by matching its own probability distribution from before compression. This self-referential setup enables surprisingly strong quality recovery after aggressive pruning.
  Our experiments reveal an unexpected finding: self-referential distillation alone, applied post-training under an identical objective and fixed calibration dataset, improves model quality by 39% relative to the original converged checkpoint. When combined with structured pruning, SparseKD achieves 15-65% parameter reduction with acceptable quality trade-offs. Kernel profiling shows that speedups arise entirely from reduced dense matrix multiplication in feed-forward layers while attention remains unchanged, making this approach complementary to attention optimizations.
  We validate across two model families (0.6B and 3.8B parameters) with multi-seed experiments confirming high reproducibility. SparseKD requires no external super-teacher, no architectural changes, and no custom inference kernels, making it immediately deployable with existing infrastructure.

</details>


### [325] [MATRIX: A Multimodal Benchmark and Post-Training Framework for Materials Science](https://arxiv.org/abs/2602.00376)
*Delia McGrath,Curtis Chong,Rohil Kulkarni,Gerbrand Ceder,Adeesh Kolluru*

Main category: cs.LG

TL;DR: 引入多模态基准MATRIX评估材料科学推理，发现视觉监督可提升推理能力，且益处超出材料科学领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以评估训练时加入视觉实验数据对基于机制的解释推理的提升效果。

Method: 引入MATRIX基准，对比仅用文本和结合图像的训练效果。

Result: 视觉监督使实验解释能力提升10 - 25%，文本科学推理任务提升5 - 16%，且对其他数据集也有提升。

Conclusion: 训练时正确的图像 - 文本对齐很重要，结构化多模态训练益处超越材料科学。

Abstract: Scientific reasoning in materials science requires integrating multimodal experimental evidence with underlying physical theory. Existing benchmarks make it difficult to assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning beyond text-only supervision. We introduce MATRIX, a multimodal benchmark for materials science reasoning that evaluates foundational theory, research-level reasoning, and the interpretation of real experimental artifacts across multiple characterization modalities. Using MATRIX as a controlled diagnostic, we isolate the effect of visual grounding by comparing post-training on structured materials science text alone with post-training that incorporates paired experimental images. Despite using relatively small amounts of multimodal data, visual supervision improves experimental interpretation by 10-25% and yields 5-16% gains on text-only scientific reasoning tasks. Our results demonstrate that these improvements rely on correct image-text alignment during post-training, highlighting cross-modal representational transfer. We also observe consistent improvements on ScienceQA and PubMedQA, demonstrating that the benefits of structured multimodal post-training extend beyond materials science. The MATRIX dataset is available at https://huggingface.co/datasets/radical-ai/MATRIX and the model at https://huggingface.co/radical-ai/MATRIX-PT.

</details>


### [326] [Deep Multivariate Models with Parametric Conditionals](https://arxiv.org/abs/2602.01953)
*Dmitrij Schlesinger,Boris Flach,Alexander Shekhovtsov*

Main category: cs.LG

TL;DR: 提出用条件概率分布表示联合概率分布的多元深模型，可用于多下游任务，便于半监督学习。


<details>
  <summary>Details</summary>
Motivation: 多数现有工作从应用任务出发设计模型，限制了其在其他下游任务的适用性。

Method: 用每组变量基于其余变量的条件概率分布来表示联合概率分布，将学习问题转化为最大化其极限分布数据似然来训练参数化马尔可夫链核。

Result: 所提模型可用于几乎任何下游任务，且支持多种半监督学习场景。

Conclusion: 提出的方法能克服现有方法的局限性，更具通用性和应用灵活性。

Abstract: We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.

</details>


### [327] [RePaint-Enhanced Conditional Diffusion Model for Parametric Engineering Designs under Performance and Parameter Constraints](https://arxiv.org/abs/2602.00384)
*Ke Wang,Nguyen Gia Hien Vu,Yifan Tang,Mostafa Rahmani Dehaghani,G. Gary Wang*

Main category: cs.LG

TL;DR: 本文提出RePaint增强框架用于工程设计生成，无需重新训练模型，在两个设计问题上验证其有效性，是高效免训练的方案。


<details>
  <summary>Details</summary>
Motivation: 为工程设计生成提供满足性能和参数约束，无需重新训练模型的方法。

Method: 提出RePaint增强框架，结合预训练性能引导去噪扩散概率模型，推理时应用基于掩码的重采样。

Result: 在参数化船体设计和翼型设计两个问题上，能基于部分参考设计生成具有预期性能的新设计，精度与预训练模型相当或更好。

Conclusion: 所提方法为工程应用中参数约束感知的生成式设计提供高效、免训练的解决方案。

Abstract: This paper presents a RePaint-enhanced framework that integrates a pre-trained performance-guided denoising diffusion probabilistic model (DDPM) for performance- and parameter-constraint engineering design generation. The proposed method enables the generation of missing design components based on a partial reference design while satisfying performance constraints, without retraining the underlying model. By applying mask-based resampling during inference process, RePaint allows efficient and controllable repainting of partial designs under both performance and parameter constraints, which is not supported by conventional DDPM-base methods. The framework is evaluated on two representative design problems, parametric ship hull design and airfoil design, demonstrating its ability to generate novel designs with expected performance based on a partial reference design. Results show that the method achieves accuracy comparable to or better than pre-trained models while enabling controlled novelty through fixing partial designs. Overall, the proposed approach provides an efficient, training-free solution for parameter-constraint-aware generative design in engineering applications.

</details>


### [328] [SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation](https://arxiv.org/abs/2602.02013)
*Xiaoyi Jiang,Andreas Nienkötter*

Main category: cs.LG

TL;DR: 介绍基于相互一致性的自监督框架SNAP用于鲁棒计算，展示其特性和优势。


<details>
  <summary>Details</summary>
Motivation: 寻找一种无需监督和先验知识的鲁棒计算方法。

Method: 基于一致性 - 可靠性假设，SNAP分配量化一致性的权重，强调可信项并降低异常值权重。

Result: 实现异常值权重的指数抑制，在向量平均和子空间估计上表现出实际优势，非迭代SNAP优于迭代Weiszfeld算法和两种多元均值中位数变体。

Conclusion: SNAP是一种灵活、易用且广泛适用的鲁棒计算方法。

Abstract: We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.

</details>


### [329] [A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode](https://arxiv.org/abs/2602.00388)
*Zeyuan He,Yupeng Chen,Lang Lin,Yihan Wang,Shenxu Chang,Eric Sommerlade,Philip Torr,Junchi Yu,Adel Bibi,Jialin Yu*

Main category: cs.LG

TL;DR: 本文分析扩散大语言模型（D - LLMs）抗越狱攻击的安全性，指出其有内在鲁棒性但存在上下文嵌套的失效模式，还实现对Gemini Diffusion的首次成功越狱。


<details>
  <summary>Details</summary>
Motivation: 探讨D - LLMs除生成效率外，在抗越狱攻击方面未被充分研究的安全性优势。

Method: 分析D - LLMs扩散轨迹的内在机制，识别上下文嵌套的失效模式并进行实验验证。

Result: 发现上下文嵌套策略能绕过D - LLMs的安全优势，在不同模型和基准上达到先进的攻击成功率，实现对Gemini Diffusion的首次成功越狱。

Conclusion: 刻画了D - LLMs安全优势的来源和局限，对其进行了早期红队测试。

Abstract: Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.

</details>


### [330] [Localized, High-resolution Geographic Representations with Slepian Functions](https://arxiv.org/abs/2602.00392)
*Arjun Rao,Ruth Crasto,Tessa Ooms,David Rolnick,Konstantin Klemmer,Marc Rußwurm*

Main category: cs.LG

TL;DR: 提出基于球面Slepian函数的地理定位编码器，在多个任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有编码地理位置的机器学习模型在局部应用所需的细粒度分辨率上表现不佳。

Method: 提出基于球面Slepian函数的地理定位编码器，以及用于全局上下文的混合Slepian - 球谐编码器。

Result: Slepian编码在分类、回归和图像增强预测等五个任务中优于基线，在多种神经网络架构中保持性能优势。

Conclusion: 所提出的编码器能集中表征能力，平衡局部 - 全局性能，且计算需求低。

Abstract: Geographic data is fundamentally local. Disease outbreaks cluster in population centers, ecological patterns emerge along coastlines, and economic activity concentrates within country borders. Machine learning models that encode geographic location, however, distribute representational capacity uniformly across the globe, struggling at the fine-grained resolutions that localized applications require. We propose a geographic location encoder built from spherical Slepian functions that concentrate representational capacity inside a region-of-interest and scale to high resolutions without extensive computational demands. For settings requiring global context, we present a hybrid Slepian-Spherical Harmonic encoder that efficiently bridges the tradeoff between local-global performance, while retaining desirable properties such as pole-safety and spherical-surface-distance preservation. Across five tasks spanning classification, regression, and image-augmented prediction, Slepian encodings outperform baselines and retain performance advantages across a wide range of neural network architectures.

</details>


### [331] [Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity](https://arxiv.org/abs/2602.00397)
*Aayush Gautam,Mukul Gagrani,Junyoung Park,Mingu Lee,Chiris Lott,Narasimha Reddy*

Main category: cs.LG

TL;DR: 提出FastForward框架加速大语言模型预填充阶段，在多模型上实现计算加速并减少首次输出时间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理的预填充阶段是长上下文工作负载的计算瓶颈，现有FFN稀疏化方法无法利用预填充阶段并行性且会降低精度。

Method: 引入FastForward框架，结合轻量级专家预测器、误差补偿网络和层稀疏调度器实现块级、上下文感知的FFN稀疏化。

Result: 在LLaMA和Qwen等模型上，50% FFN稀疏度时实现1.45倍计算加速，精度损失小于6%，显著减少首次输出时间。

Conclusion: FastForward能在受限硬件上实现高效、长上下文的大语言模型推理。

Abstract: The prefill stage of large language model (LLM) inference is a key computational bottleneck for long-context workloads. At short-to-moderate context lengths (1K--16K tokens), Feed-Forward Networks (FFNs) dominate this cost, accounting for most of the total FLOPs. Existing FFN sparsification methods, designed for autoregressive decoding, fail to exploit the prefill stage's parallelism and often degrade accuracy. To address this, we introduce FastForward, a predictive sparsity framework that accelerates LLM prefill through block-wise, context-aware FFN sparsity. FastForward combines (1) a lightweight expert predictor to select high-importance neurons per block, (2) an error compensation network to correct sparsity-induced errors, and (3) a layer-wise sparsity scheduler to allocate compute based on token-mixing importance. Across LLaMA and Qwen models up to 8B parameters, FastForward delivers up to 1.45$\times$ compute-bound speedup at 50% FFN sparsity with $<$ 6% accuracy loss compared to the dense baseline on LongBench, substantially reducing Time-to-First-Token (TTFT) for efficient, long-context LLM inference on constrained hardware.

</details>


### [332] [Efficient Swap Regret Minimization in Combinatorial Bandits](https://arxiv.org/abs/2602.02087)
*Andreas Kontogiannis,Vasilis Pollatos,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.LG

TL;DR: 本文解决组合多臂老虎机中高效无交换后悔算法的设计问题，提出算法的后悔值和每轮复杂度对动作数量N呈对数多项式依赖。


<details>
  <summary>Details</summary>
Motivation: 在组合多臂老虎机中，动作数量N随问题维度呈指数级增长，实现对N呈对数多项式依赖的无交换后悔是尚未解决的挑战，而外部后悔最小化问题已有较好研究。

Method: 引入一种无交换后悔学习算法。

Result: 所提出算法的后悔值对N呈对数多项式依赖，且在组合多臂老虎机类别中是最优的，还展示了该算法在多种应用中能高效实现，每轮复杂度也对N呈对数多项式依赖。

Conclusion: 成功解决了组合多臂老虎机中设计高效无交换后悔算法的问题。

Abstract: This paper addresses the problem of designing efficient no-swap regret algorithms for combinatorial bandits, where the number of actions $N$ is exponentially large in the dimensionality of the problem. In this setting, designing efficient no-swap regret translates to sublinear -- in horizon $T$ -- swap regret with polylogarithmic dependence on $N$. In contrast to the weaker notion of external regret minimization - a problem which is fairly well understood in the literature - achieving no-swap regret with a polylogarithmic dependence on $N$ has remained elusive in combinatorial bandits. Our paper resolves this challenge, by introducing a no-swap-regret learning algorithm with regret that scales polylogarithmically in $N$ and is tight for the class of combinatorial bandits. To ground our results, we also demonstrate how to implement the proposed algorithm efficiently -- that is, with a per-iteration complexity that also scales polylogarithmically in $N$ -- across a wide range of well-studied applications.

</details>


### [333] [MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers](https://arxiv.org/abs/2602.00398)
*Ajay Jaiswal,Lauren Hannah,Han-Byul Kim,Duc Hoang,Arnav Kundu,Mehrdad Farajtabar,Minsik Cho*

Main category: cs.LG

TL;DR: 文章提出MemoryLLM和Flex - MemoryLLM，研究FFNs，提升推理效率并弥补性能差距。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型中Transformer组件运行方式对人工智能技术进步很重要，需解决前馈模块（FFNs）可解释性挑战。

Method: 提出MemoryLLM将FFNs与自注意力解耦，以孤立训练方式实现上下文无关的FFNs，计算为token - wise lookups(ToLs)；还引入Flex - MemoryLLM在传统Transformer设计和MemoryLLM之间架构。

Result: MemoryLLM可实现FFNs上下文无关并提升推理效率，Flex - MemoryLLM弥补训练上下文无关token - wise嵌入的FFNs导致的性能差距。

Conclusion: 所提方法有助于研究FFNs特性及在不同下游任务重要性，能提升推理效率。

Abstract: Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.

</details>


### [334] [Spectral Superposition: A Theory of Feature Geometry](https://arxiv.org/abs/2602.02224)
*Georgi Ivanov,Narmeen Oozeer,Shivam Raval,Tasana Pejovic,Shriyash Upadhyay,Amir Abdullah*

Main category: cs.LG

TL;DR: 本文提出通过分析权重衍生矩阵光谱研究特征几何结构的理论，在叠加玩具模型中证明容量饱和导致光谱局部化，其形式适用于任意权重矩阵，指向应用算子理论进行可解释性研究的方向。


<details>
  <summary>Details</summary>
Motivation: 当前方法分解激活为稀疏线性特征时丢弃了几何结构，需研究特征的几何结构。

Method: 分析权重衍生矩阵的光谱，引入框架算子 $F = WW^\top$ 描述特征在特征空间的范数分配。

Result: 在叠加玩具模型中证明容量饱和导致光谱局部化，特征有特定表现，可分类先前工作的所有几何形状，光谱测量形式适用于任意权重矩阵。

Conclusion: 表明可应用算子理论进行更广泛的可解释性研究。

Abstract: Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.

</details>


### [335] [DROGO: Default Representation Objective via Graph Optimization in Reinforcement Learning](https://arxiv.org/abs/2602.00403)
*Hon Tik Tse,Marlos C. Machado*

Main category: cs.LG

TL;DR: 论文提出直接用神经网络近似默认表示（DR）主特征向量的目标，经实验验证其有效性并用于奖励塑造。


<details>
  <summary>Details</summary>
Motivation: 以往计算DR特征向量的方法计算成本高且不适用于高维空间。

Method: 推导用神经网络直接近似DR主特征向量的目标。

Result: 在多个环境中验证了目标的有效性。

Conclusion: 所提方法可有效近似DR主特征向量，并可用于奖励塑造。

Abstract: In computational reinforcement learning, the default representation (DR) and its principal eigenvector have been shown to be effective for a wide variety of applications, including reward shaping, count-based exploration, option discovery, and transfer. However, in prior investigations, the eigenvectors of the DR were computed by first approximating the DR matrix, and then performing an eigendecomposition. This procedure is computationally expensive and does not scale to high-dimensional spaces. In this paper, we derive an objective for directly approximating the principal eigenvector of the DR with a neural network. We empirically demonstrate the effectiveness of the objective in a number of environments, and apply the learned eigenvectors for reward shaping.

</details>


### [336] [Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks](https://arxiv.org/abs/2602.00407)
*Suprim Nakarmi,Junggab Son,Yue Zhao,Zuobin Xiong*

Main category: cs.LG

TL;DR: 文章指出GNN在集中式处理敏感数据有隐私问题，FedGNN也会泄露信息，提出Fed - Listing攻击推断私有标签统计信息，实验表明其效果好且难防御。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在处理敏感数据有隐私风险，FedGNN虽解决部分问题但仍会泄漏信息，且FedGNN中标签分布推断问题未充分研究。

Method: 提出基于梯度的Fed - Listing攻击，利用训练时的最后一层梯度，结合辅助影子数据集模拟客户端分布来获取攻击模型。

Result: 在四个基准数据集和三种GNN架构上实验，Fed - Listing显著优于现有基线，即使在非独立同分布场景，应用防御机制也难降低其攻击性能。

Conclusion: Fed - Listing能有效推断FedGNN中目标客户端的私有标签统计信息，且较难防御。

Abstract: Graph Neural Networks (GNNs) have been intensively studied for their expressive representation and learning performance on graph-structured data, enabling effective modeling of complex relational dependencies among nodes and edges in various domains. However, the standalone GNNs can unleash threat surfaces and privacy implications, as some sensitive graph-structured data is collected and processed in a centralized setting. To solve this issue, Federated Graph Neural Networks (FedGNNs) are proposed to facilitate collaborative learning over decentralized local graph data, aiming to preserve user privacy. Yet, emerging research indicates that even in these settings, shared model updates, particularly gradients, can unintentionally leak sensitive information of local users. Numerous privacy inference attacks have been explored in traditional federated learning and extended to graph settings, but the problem of label distribution inference in FedGNNs remains largely underexplored. In this work, we introduce Fed-Listing (Federated Label Distribution Inference in GNNs), a novel gradient-based attack designed to infer the private label statistics of target clients in FedGNNs without access to raw data or node features. Fed-Listing only leverages the final-layer gradients exchanged during training to uncover statistical patterns that reveal class proportions in a stealthy manner. An auxiliary shadow dataset is used to generate diverse label partitioning strategies, simulating various client distributions, on which the attack model is obtained. Extensive experiments on four benchmark datasets and three GNN architectures show that Fed-Listing significantly outperforms existing baselines, including random guessing and Decaf, even under challenging non-i.i.d. scenarios. Moreover, applying defense mechanisms can barely reduce our attack performance, unless the model's utility is severely degraded.

</details>


### [337] [Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management](https://arxiv.org/abs/2602.02283)
*Owen Shen,Patrick Jaillet*

Main category: cs.LG

TL;DR: 研究带延迟反馈收益管理的强化学习，提出选择模型辅助RL方法，证明收敛性并实验分析。


<details>
  <summary>Details</summary>
Motivation: 解决收益管理中因客户取消和修改带来的延迟反馈问题。

Method: 提出选择模型辅助RL，用校准离散选择模型估算学习目标的延迟部分。

Result: 证明收敛性，实验显示在平稳、参数变化和结构错误设定场景下有不同表现。

Conclusion: 明确部分行为模型何时提升鲁棒性，何时引入有害偏差。

Abstract: We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.

</details>


### [338] [Variational Approach for Job Shop Scheduling](https://arxiv.org/abs/2602.00408)
*Seung Heon Oh,Jiwon Baek,Ki Young Cho,Hee Chang Yoon,Jong Hun Woo*

Main category: cs.LG

TL;DR: 提出VG2S框架解决作业车间调度问题，解耦表征学习与策略优化，提升训练稳定性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法解决作业车间调度问题时存在训练非平稳性和泛化能力有限的问题。

Method: 首次将变分推理引入作业车间调度领域，基于ELBO和最大熵强化学习推导概率目标，通过变分图编码器使代理学习调度实例的鲁棒结构表征。

Result: 该方法在零样本泛化能力上优于现有深度强化学习基线和传统调度规则，在大规模和具有挑战性的基准实例上表现出色。

Conclusion: 提出的VG2S框架有效解决了传统方法的问题，提升了作业车间调度问题的求解效果。

Abstract: This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.

</details>


### [339] [C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference](https://arxiv.org/abs/2602.02371)
*Jing Wang,Jie Shen,Qiaomin Xie,Jeremy C Weiss*

Main category: cs.LG

TL;DR: 提出C - kNN - LSH框架用于纵向轨迹因果效应估计，理论上有优势，在真实数据上表现优于基线方法


<details>
  <summary>Details</summary>
Motivation: 从纵向轨迹估计因果效应对于理解复杂疾病进展和优化临床决策至关重要，现有方法需应对高维混杂情况

Method: 引入C - kNN - LSH框架，利用局部敏感哈希识别临床双胞胎进行局部条件治疗效应估计，结合邻域估计器和双重稳健校正

Result: 理论分析表明估计器具有一致性和二阶稳健性，在真实的长新冠队列中表现优于现有基线方法

Conclusion: C - kNN - LSH框架能有效处理高维混杂的纵向轨迹因果效应估计问题

Abstract: Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.
  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.
  Evaluated on a real-world Long COVID cohort with 13,511 participants, \emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.

</details>


### [340] [Robustness of AutoML on Dirty Categorical Data](https://arxiv.org/abs/2602.00412)
*Marcos L. P. Bueno,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 本文研究AutoML在脏分类数据集上表现，提出转换管道，通过基准测试和对比分析性能。


<details>
  <summary>Details</summary>
Motivation: AutoML方法在脏分类数据集上的表现未知，且不清楚形态编码器在其中的效果。

Method: 提出将分类数据转换为数值数据的管道，对AutoML方法在脏数据集上的鲁棒性进行基准测试并与管道对比，还分析AutoML构建的ML管道。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical datasets. These datasets often have several categorical features with high cardinality arising from issues such as lack of curation and automated collection. Recent research has shown that ML models can benefit from morphological encoders for dirty categorical data, leading to significantly superior predictive performance. However the effects of using such encoders in AutoML methods are not known at the moment. In this paper, we propose a pipeline that transforms categorical data into numerical data so that an AutoML can handle categorical data transformed by more advanced encoding schemes. We benchmark the current robustness of AutoML methods on a set of dirty datasets and compare it with the proposed pipeline. This allows us to get insight on differences in predictive performance. We also look at the ML pipelines built by AutoMLs in order to gain insight beyond the best model as typically returned by these methods.

</details>


### [341] [Maximizing Reliability with Bayesian Optimization](https://arxiv.org/abs/2602.02432)
*Jack M. Buckingham,Ivo Couckuyt,Juergen Branke*

Main category: cs.LG

TL;DR: 提出基于汤普森采样和知识梯度的两种贝叶斯优化方法处理制造中设计可靠性最大化问题，新方法表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决制造中设计受随机扰动时最大化可靠性、极小化失败概率的问题，现有方法可能不足。

Method: 提出基于汤普森采样和知识梯度的两种贝叶斯优化方法，结合重要性采样处理极小失败概率。

Result: 经验结果显示新方法在极端和非极端情况下均优于现有方法。

Conclusion: 所提的两种贝叶斯优化方法能有效解决制造中设计可靠性优化问题。

Abstract: Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.

</details>


### [342] [Federated-inspired Single-cell Batch Integration in Latent Space](https://arxiv.org/abs/2602.00423)
*Quang-Huy Nguyen,Zongliang Yue,Hao Chen,Wei-Shinn Ku,Jiaqi Wang*

Main category: cs.LG

TL;DR: 单细胞RNA测序数据有批次效应，现有校正方法有局限。提出scBatchProx方法校正批次效应，实验显示有质量提升，推动动态单细胞数据系统表征优化。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据积累产生批次效应，现有批次校正方法存在校正不足或需集中重新训练等局限，无法适用于分布式和不断发展的单细胞数据场景。

Method: 受联邦学习原理启发，提出scBatchProx后验优化方法，将每个批次作为客户端，在近端正则化下学习批次条件适配器，直接在潜在空间校正批次结构，只优化特定批次适配器参数。

Result: 实验表明，scBatchProx使整体嵌入质量相对提升约3 - 8%，90%的数据 - 方法对的批次校正和85%的生物保护情况得到改善。

Conclusion: 该工作是朝着动态单细胞数据系统中学习表征的实际优化迈出的一步。

Abstract: Advances in single-cell RNA sequencing enable the rapid generation of massive, high-dimensional datasets, yet the accumulation of data across experiments introduces batch effects that obscure true biological signals. Existing batch correction approaches either insufficiently correct batch effects or require centralized retraining on the complete dataset, limiting their applicability in distributed and continually evolving single-cell data settings. We introduce scBatchProx, a post-hoc optimization method inspired by federated learning principles for refining cell-level embeddings produced by arbitrary upstream methods. Treating each batch as a client, scBatchProx learns batch-conditioned adapters under proximal regularization, correcting batch structure directly in latent space without requiring raw expression data or centralized optimization. The method is lightweight and deployable, optimizing batch-specific adapter parameters only. Extensive experiments show that scBatchProx consistently yields relative gains of approximately 3-8% in overall embedding quality, with batch correction and biological conservation improving in 90% and 85% of data-method pairs, respectively. We envision this work as a step toward the practical refinement of learned representations in dynamic single-cell data systems.

</details>


### [343] [Open Materials Generation with Inference-Time Reinforcement Learning](https://arxiv.org/abs/2602.00424)
*Philipp Hoellmer,Stefano Martiniani*

Main category: cs.LG

TL;DR: 引入OMatG - IRL框架将强化学习用于晶体结构预测，实现对基于能量目标有效强化，提升采样效率并减少生成时间。


<details>
  <summary>Details</summary>
Motivation: 连续时间晶体材料生成模型融入目标属性有挑战，政策梯度强化学习难应用于仅学习速度场的流基模型。

Method: 提出OMatG - IRL框架，直接作用于学习的速度场，利用潜在生成动态的随机扰动实现推理时的探索和策略梯度估计。

Result: 首次将强化学习应用于晶体结构预测，能有效强化基于能量目标，保持多样性，性能与基于分数的强化学习方法具有竞争力，可学习时间相关的速度退火计划。

Conclusion: OMatG - IRL框架能实现高效准确的晶体结构预测，提高采样效率，减少生成时间。

Abstract: Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.

</details>


### [344] [LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference](https://arxiv.org/abs/2602.00426)
*Vikram Krishnamurthy*

Main category: cs.LG

TL;DR: 本文为大语言模型（LLMs）的训练、对齐和生成提供方程级描述，将LLMs建模为高维非线性自回归模型，便于分析相关行为和现象。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer架构的大语言模型描述方式掩盖了其底层计算结构，需要明确的方程级描述。

Method: 将LLMs表述为具有基于注意力依赖的高维非线性自回归模型，涵盖预训练、对齐方法和推理时的自回归生成。

Result: 自注意力自然地表现为重复的双线性 - softmax - 线性组合，形成高表达性的序列模型。

Conclusion: 该公式化有助于对对齐行为、推理现象分析和模型扩展，可作为解释和理论发展的参考。

Abstract: Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking an explicit, equation-level description of LLM training, alignment, and generation. We formulate LLMs as high-dimensional nonlinear autoregressive models with attention-based dependencies. The framework encompasses pretraining via next-token prediction, alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), rejection sampling fine-tuning (RSFT), and reinforcement learning from verifiable rewards (RLVR), as well as autoregressive generation during inference. Self-attention emerges naturally as a repeated bilinear--softmax--linear composition, yielding highly expressive sequence models. This formulation enables principled analysis of alignment-induced behaviors (including sycophancy), inference-time phenomena (such as hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation), and extensions like continual learning, while serving as a concise reference for interpretation and further theoretical development.

</details>


### [345] [Towards Building Non-Fine-Tunable Foundation Models](https://arxiv.org/abs/2602.00446)
*Ziyao Wang,Nizhang Li,Pingzhi Li,Guoheng Sun,Tianlong Chen,Ang Li*

Main category: cs.LG

TL;DR: 提出Private Mask Pre - Training (PMP)框架构建不可微调的基础模型，防止无授权微调带来的风险，理论和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 开源基础模型面临无限制下游微调带来的经济和安全风险，需要构建不可微调的基础模型。

Method: 提出PMP预训练框架，将表征学习集中到训练早期确定的稀疏子网络，子网络二进制掩码保密，仅发布最终密集权重。

Result: 理论分析表明不匹配会破坏基于梯度的自适应并限制微调增益；实验显示PMP保留基础模型性能，降低多下游任务无授权微调效果，不可微调性强度由掩码比率控制。

Conclusion: PMP能构建非微调基础模型，在保留模型性能的同时限制无授权微调增益。

Abstract: Open-sourcing foundation models (FMs) enables broad reuse but also exposes model trainers to economic and safety risks from unrestricted downstream fine-tuning. We address this problem by building non-fine-tunable foundation models: models that remain broadly usable in their released form while yielding limited adaptation gains under task-agnostic unauthorized fine-tuning. We propose Private Mask Pre-Training (PMP), a pre-training framework that concentrates representation learning into a sparse subnetwork identified early in training. The binary mask defining this subnetwork is kept private, and only the final dense weights are released. This forces unauthorized fine-tuning without access to the mask to update parameters misaligned with pretraining subspace, inducing an intrinsic mismatch between the fine-tuning objective and the pre-training geometry. We provide theoretical analysis showing that this mismatch destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirical results on large language models demonstrating that PMP preserves base model performance while consistently degrading unauthorized fine-tuning across a wide range of downstream tasks, with the strength of non-fine-tunability controlled by the mask ratio.

</details>


### [346] [Search Inspired Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.00460)
*Georgios Sotirchos,Zlatan Ajanović,Jens Kober*

Main category: cs.LG

TL;DR: 提出SIERL解决强化学习稀疏奖励环境探索问题，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习稀疏奖励环境探索方法存在依赖手工启发式或易收敛到次优策略的问题。

Method: 提出SIERL方法，在每轮开始时从前沿选择子目标，基于预期成本选择子目标引导探索。

Result: 在具有挑战性的稀疏奖励环境实验中，SIERL在实现主任务目标和泛化到达任意状态方面优于主流基线。

Conclusion: SIERL能有效引导强化学习在稀疏奖励环境中的探索。

Abstract: Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.

</details>


### [347] [PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction](https://arxiv.org/abs/2602.00465)
*Jiaqi Yin,Baiming Chen,Jia Fei,Mingjun Yang*

Main category: cs.LG

TL;DR: 本文将功能性miRNA - mRNA靶向预测问题形式化为预算关系多实例学习（BR - MIL），提出PAIR - Former模型，在miRAW上表现良好并给出预算选择的理论。


<details>
  <summary>Details</summary>
Motivation: 功能性miRNA - mRNA靶向预测中每个转录本有大量候选目标位点，但只有成对标签可观察，需解决该预测问题。

Method: 将问题形式化为BR - MIL，提出PAIR - Former管道，进行全池扫描，在CPU上选择最多K个多样的CTS，对选定令牌应用排列不变的Set Transformer聚合器。

Result: 在miRAW上，PAIR - Former在实际操作预算（K* = 64）下优于强池化基线，且K变化时提供可控的精度 - 计算权衡。

Conclusion: 给出了预算选择与近似误差、昂贵关系组件的泛化项的理论联系。

Abstract: Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instances per bag may receive expensive encoding and relational processing under a hard compute budget. We propose \textbf{PAIR-Former} (Pool-Aware Instance-Relational Transformer), a BR-MIL pipeline that performs a cheap full-pool scan, selects up to $K$ diverse CTSs on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens. On miRAW, PAIR-Former outperforms strong pooling baselines at a practical operating budget ($K^\star{=}64$) while providing a controllable accuracy--compute trade-off as $K$ varies. We further provide theory linking budgeted selection to (i) approximation error decreasing with $K$ and (ii) generalization terms governed by $K$ in the expensive relational component.

</details>


### [348] [Parallel Stochastic Gradient-Based Planning for World Models](https://arxiv.org/abs/2602.00475)
*Michael Psenka,Michael Rabbat,Aditi Krishnapriyan,Yann LeCun,Amir Bar*

Main category: cs.LG

TL;DR: 提出GRASP规划器，利用世界模型可微性解决长时控制任务，在实验中表现优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 世界模型用于规划时因搜索空间大且无结构而具有挑战性，需高效规划方法。

Method: 提出GRASP规划器，将状态视为优化变量，引入随机性，修改梯度结构。

Result: GRASP规划器在长时实验中成功率和收敛时间上优于CEM和GD算法。

Conclusion: GRASP规划器是一种有效解决长时控制任务的规划方法。

Abstract: World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables ("virtual states") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.

</details>


### [349] [Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly](https://arxiv.org/abs/2602.00476)
*Hengchang Liu,Zhao Yang,Bing Su*

Main category: cs.LG

TL;DR: 本文揭示扩散语言模型有发现正确填充长度的能力，提出无训练方法CAL，在代码和文本填充任务上表现提升。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型性能受预指定填充长度限制，要挖掘其发现正确填充长度的能力。

Method: 识别第一步去噪置信度中的局部Oracle Peak和Length Bias，利用信号并校准偏差，通过高效搜索近似最优长度。

Result: CAL在代码填充中使Pass@1比固定长度基线提升47.7%，比基于聊天的自适应方法提升40.5%；在文本填充中使BLEU - 2和ROUGE - L分别提升8.5%和9.9%。

Conclusion: CAL为扩散语言模型的稳健填充铺平道路，无需专门训练。

Abstract: Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \textit{Oracle Peak} that emerges near the ground-truth length and a systematic \textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \textbf{CAL} (\textbf{C}alibrated \textbf{A}daptive \textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\% over fixed-length baselines and 40.5\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\% and 9.9\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.

</details>


### [350] [AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.00482)
*Jiarui Zhang,Yuchen Yang,Ran Yan,Zhiyu Mei,Liyuan Zhang,Daifeng Li,Wei Fu,Jiaxuan Gao,Shusheng Xu,Yi Wu,Binhang Yuan*

Main category: cs.LG

TL;DR: 本文提出了AREAL - DTA方法来高效利用强化学习训练中的前缀共享，在流行的强化学习后训练工作负载中实现了高达8.31倍的训练吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的大语言模型后训练计算成本高，现有框架独立处理序列，重复计算相同前缀，导致计算和内存使用效率低下，且基于树注意力的现有解决方案扩展性不佳。

Method: 引入AREAL - DTA，采用基于深度优先搜索的执行策略动态遍历前缀树，一次仅具体化一条从根到叶的路径；还引入负载均衡的分布式批处理机制，跨多个GPU动态构建和处理前缀树。

Result: 在流行的强化学习后训练工作负载中，AREAL - DTA在$τ^2$-bench上实现了高达8.31倍的训练吞吐量提升。

Conclusion: AREAL - DTA能有效利用强化学习训练中的前缀共享，提高训练效率。

Abstract: Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\times$ in $τ^2$-bench higher training throughput.

</details>


### [351] [OD-DEAL: Dynamic Expert-Guided Adversarial Learning with Online Decomposition for Scalable Capacitated Vehicle Routing](https://arxiv.org/abs/2602.00488)
*Dongbin Jiao,Zisheng Chen,Xianyi Wang,Jintao Shi,Shengcai Liu,Shi Yan*

Main category: cs.LG

TL;DR: 提出OD - DEAL框架解决大规模CVRP问题，实现SOTA实时性能。


<details>
  <summary>Details</summary>
Motivation: 启发式方法复杂度高、神经求解器在大规模图上泛化性有限，阻碍大规模CVRP问题解决。

Method: 提出OD - DEAL框架，结合混合遗传搜索和在线重心聚类分解，通过极小极大博弈训练基于GAT的生成策略，进行知识蒸馏。

Result: OD - DEAL实现SOTA实时CVRP性能，能解决10000节点实例，实现近常数神经扩展。

Conclusion: OD - DEAL能实现亚秒级、启发式质量推理，适用于动态大规模部署。

Abstract: Solving large-scale capacitated vehicle routing problems (CVRP) is hindered by the high complexity of heuristics and the limited generalization of neural solvers on massive graphs. We propose OD-DEAL, an adversarial learning framework that tightly integrates hybrid genetic search (HGS) and online barycenter clustering (BCC) decomposition, and leverages high-fidelity knowledge distillation to transfer expert heuristic behavior. OD-DEAL trains a graph attention network (GAT)-based generative policy through a minimax game, in which divide-and-conquer strategies from a hybrid expert are distilled into dense surrogate rewards. This enables high-quality, clustering-free inference on large-scale instances. Empirical results demonstrate that OD-DEAL achieves state-of-the-art (SOTA) real-time CVRP performance, solving 10000-node instances with near-constant neural scaling. This uniquely enables the sub-second, heuristic-quality inference required for dynamic large-scale deployment.

</details>


### [352] [Partition of Unity Neural Networks for Interpretable Classification with Explicit Class Regions](https://arxiv.org/abs/2602.00511)
*Akram Aldroubi*

Main category: cs.LG

TL;DR: 本文引入了PUNN架构，无需softmax层，证明其在紧致域上连续概率映射空间的稠密性，实验表明它能与黑盒模型竞争并提供可解释的分类概率分配。


<details>
  <summary>Details</summary>
Motivation: 神经网络分类器难以解释，基于softmax的模型类区域难以提取和可视化。

Method: 引入Partition of Unity Neural Networks (PUNN)架构，构建满足特定条件的非负函数，证明其稠密性，使用不同激活函数和参数化方式。

Result: 基于MLP的PUNN与标准多层感知器准确率相差0.3 - 0.6%，形状信息门在参数少300倍时可达到相近准确率。

Conclusion: 可解释设计的架构能与黑盒模型竞争，同时提供透明的类概率分配。

Abstract: Despite their empirical success, neural network classifiers remain difficult to interpret. In softmax-based models, class regions are defined implicitly as solutions to systems of inequalities among logits, making them difficult to extract and visualize. We introduce Partition of Unity Neural Networks (PUNN), an architecture in which class probabilities arise directly from a learned partition of unity, without requiring a softmax layer.
  PUNN constructs $k$ nonnegative functions $h_1, \ldots, h_k$ satisfying $\sum_i h_i(x) = 1$, where each $h_i(x)$ directly represents $P(\text{class } i \mid x)$. Unlike softmax, where class regions are defined implicitly through coupled inequalities among logits, each PUNN partition function $h_i$ directly defines the probability of class $i$ as a standalone function of $x$.
  We prove that PUNN is dense in the space of continuous probability maps on compact domains. The gate functions $g_i$ that define the partition can use various activation functions (sigmoid, Gaussian, bump) and parameterizations ranging from flexible MLPs to parameter-efficient shape-informed designs (spherical shells, ellipsoids, spherical harmonics).
  Experiments on synthetic data, UCI benchmarks, and MNIST show that PUNN with MLP-based gates achieves accuracy within 0.3--0.6\% of standard multilayer perceptrons. When geometric priors match the data structure, shape-informed gates achieve comparable accuracy with up to 300$\times$ fewer parameters. These results demonstrate that interpretable-by-design architectures can be competitive with black-box models while providing transparent class probability assignments.

</details>


### [353] [Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs](https://arxiv.org/abs/2602.00513)
*Md Tanvirul Alam,Aritran Piplai,Ionut Cardei,Nidhi Rastogi,Peter J Worth*

Main category: cs.LG

TL;DR: 研究利用强化学习与可验证奖励（RLVR）处理CTI任务，引入Minerva数据集和训练管道，提出自训练机制，实验显示比监督微调有更好表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成结构化CTI输出时较脆弱且多依赖监督微调，而CTI标准和资源可对模型输出进行确定性验证，因此研究RLVR用于CTI任务。

Method: 引入Minerva统一数据集和训练管道，每个子任务配备特定验证器；提出轻量级自训练机制解决奖励稀疏问题。

Result: 在多个LLM骨干模型上的实验表明，在多个基准测试中，相较于监督微调，准确性和鲁棒性有持续提升。

Conclusion: 利用RLVR进行CTI任务，配合Minerva和自训练机制，能比传统监督微调方法取得更好效果。

Abstract: Cyber threat intelligence (CTI) analysts routinely convert noisy, unstructured security artifacts into standardized, automation-ready representations. Although large language models (LLMs) show promise for this task, existing approaches remain brittle when producing structured CTI outputs and have largely relied on supervised fine-tuning (SFT). In contrast, CTI standards and community-maintained resources define canonical identifiers and schemas that enable deterministic verification of model outputs. We leverage this structure to study reinforcement learning with verifiable rewards (RLVR) for CTI tasks. We introduce \textit{Minerva}, a unified dataset and training pipeline spanning multiple CTI subtasks, each paired with task-specific verifiers that score structured outputs and identifier predictions. To address reward sparsity during rollout, we propose a lightweight self-training mechanism that generates additional verified trajectories and distills them back into the model. Experiments across LLM backbones show consistent improvements in accuracy and robustness over SFT across multiple benchmarks.

</details>


### [354] [Contrastive Learning for Privacy Enhancements in Industrial Internet of Things](https://arxiv.org/abs/2602.00515)
*Lin Liu,Rita Machacy,Simi Kuniyilh*

Main category: cs.LG

TL;DR: 本文对基于对比学习的工业物联网（IIoT）隐私保护技术进行全面综述，强调工业数据等特点，讨论解决方案、挑战并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 工业物联网在带来便利的同时引入隐私和保密风险，现有基于对比学习的隐私保护技术在工业物联网领域缺乏全面研究。

Method: 对基于对比学习的工业物联网隐私保护技术进行全面综述，强调工业数据、系统架构和应用场景的独特性。

Result: 完成对相关技术的全面回顾。

Conclusion: 探讨了相关解决方案和开放挑战，给出未来研究方向。

Abstract: The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems, such as those in manufacturing and energy, it also introduces significant privacy and confidentiality risks due to the sensitivity of operational data. Contrastive learning, a self-supervised representation learning paradigm, has recently emerged as a promising approach for privacy-preserving analytics by reducing reliance on labeled data and raw data sharing. Although contrastive learning-based privacy-preserving techniques have been explored in the Internet of Things (IoT) domain, this paper offers a comprehensive review of these techniques specifically for privacy preservation in Industrial Internet of Things (IIoT) systems. It emphasizes the unique characteristics of industrial data, system architectures, and various application scenarios. Additionally, the paper discusses solutions and open challenges and outlines future research directions.

</details>


### [355] [NEST: Nested Event Stream Transformer for Sequences of Multisets](https://arxiv.org/abs/2602.00520)
*Minghui Sun,Haoyu Gong,Xingyu You,Jillian Hurst,Benjamin Goldstein,Matthew Engelhard*

Main category: cs.LG

TL;DR: 本文指出现有事件流数据基础模型扁平化层次结构存在问题，提出Nested Event Stream Transformer (NEST)模型和Masked Set Modeling (MSM)范式，实验证明其能提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有事件流数据基础模型扁平化层次结构导致计算效率低和集级别表示质量差的问题。

Method: 提出NEST模型用于处理由多集序列组成的事件流，构建MSM范式促进集级别表示学习。

Result: 实验表明NEST能捕捉现实世界动态，提升预训练效率和下游任务性能。

Conclusion: 在基础模型架构中保留原始层次结构可提供有用归纳偏置，提升计算效率和表示质量。

Abstract: Event stream data often exhibit hierarchical structure in which multiple events co-occur, resulting in a sequence of multisets (i.e., bags of events). In electronic health records (EHRs), for example, medical events are grouped into a sequence of clinical encounters with well-defined temporal structure, but the order and timing of events within each encounter may be unknown or unreliable. Most existing foundation models (FMs) for event stream data flatten this hierarchy into a one-dimensional sequence, leading to (i) computational inefficiency associated with dense attention and learning spurious within-set relationships, and (ii) lower-quality set-level representations from heuristic post-training pooling for downstream tasks. Here, we show that preserving the original hierarchy in the FM architecture provides a useful inductive bias that improves both computational efficiency and representation quality. We then introduce Nested Event Stream Transformer (NEST), a FM for event streams comprised of sequences of multisets. Building on this architecture, we formulate Masked Set Modeling (MSM), an efficient paradigm that promotes improved set-level representation learning. Experiments on real-world multiset sequence data show that NEST captures real-world dynamics while improving both pretraining efficiency and downstream performance.

</details>


### [356] [Physiology as Language: Translating Respiration to Sleep EEG](https://arxiv.org/abs/2602.00526)
*Kaiwen Zha,Chao Li,Hao He,Peng Cao,Tianhong Li,Ali Mirzazadeh,Ellen Zhang,Jong Woo Lee,Yoon Kim,Dina Katabi*

Main category: cs.LG

TL;DR: 本文提出跨生理翻译任务，用呼吸信号合成睡眠脑电图，模型表现良好且可用于远程非接触神经评估。


<details>
  <summary>Details</summary>
Motivation: 解决呼吸信号和脑电图两种模态之间复杂度差异大的问题，实现用呼吸信号合成睡眠脑电图。

Method: 提出波形条件生成框架，通过离散标记约束脑电图目标空间，保留细粒度呼吸动态。

Result: 模型在脑电图频谱图重建中平均绝对误差7%，在年龄估计、性别检测和睡眠分期等下游任务中表现与真实脑电图相当，远超基于呼吸训练的基线模型。

Conclusion: 该框架可推广到非接触传感，凸显远程非接触睡眠神经评估的可行性。

Abstract: This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.

</details>


### [357] [Convergent World Representations and Divergent Tasks](https://arxiv.org/abs/2602.00533)
*Core Francisco Park*

Main category: cs.LG

TL;DR: 本文开发框架研究神经表征的几何特性及下游适应性，发现多任务训练使表征收敛，但存在的发散任务会损害新实体集成和泛化。


<details>
  <summary>Details</summary>
Motivation: 当前对神经表征的几何特性及下游适应性的条件理解不足，需深入研究。

Method: 开发分离世界、数据生成过程和模型表征的框架，用城市坐标和几何任务生成数据进行自回归训练。

Result: 不同任务产生不同表征几何；多任务训练使表征收敛；部分发散任务损害新实体集成和泛化。

Conclusion: 多任务关系型任务训练可产生收敛表征，但发散任务会严重损害微调时新实体的集成。

Abstract: While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model representations to study these questions in a controlled setup. 5,075 city coordinates define the world and 7 geometric tasks generate the training data for autoregressive training. We find that different tasks give rise to qualitatively and quantitatively distinct world representation geometries. However, multi-task training drives convergence of world representations: models trained on non-overlapping tasks develop aligned geometric representations, providing controlled evidence for the Multitask Scaling Hypothesis of the Platonic Representation Hypothesis. To study adaptation, we pretrain models on all tasks, then test whether new entities (cities) can be consistently integrated into the representation space via fine-tuning. Surprisingly, we find that despite multi-task pretraining, some tasks, which we call divergent, actively harm the representational integration of new entities and harm generalization. Our results show that training on multiple relational tasks reliably produces convergent world representations, but lurking divergent tasks can catastrophically harm new entity integration via fine-tuning.

</details>


### [358] [AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models](https://arxiv.org/abs/2602.00534)
*Apurba Prasad Padhy,Fernando Camacho,Saibal Mukhopadhyay*

Main category: cs.LG

TL;DR: 提出针对状态空间模型（SSMs）的结构化训练后剪枝方法AIRE - Prune，可减少状态维度，在多个序列基准测试中展现效果。


<details>
  <summary>Details</summary>
Motivation: SSMs常牺牲容量、搜索空间或稳定性以抵消大状态维度的内存和计算成本，需有效剪枝方法。

Method: AIRE - Prune通过直接最小化长期输出能量失真来减少每层的状态维度，为每个状态分配基于渐近脉冲响应能量的分数并进行层归一化。

Result: 在不同序列基准测试中，SISO和MIMO SSMs平均剪枝60.8%，平均准确率仅下降0.29%且无需重新训练，显著降低计算量。

Conclusion: AIRE - Prune是一种有效的SSMs剪枝方法，能在保证准确率的同时减少计算成本。

Abstract: State space models (SSMs) often sacrifice capacity, search space, or stability to offset the memory and compute costs of large state dimensions. We introduce a structured post-training pruning method for SSMs -- AIRE-Prune (Asymptotic Impulse-Response Energy for State PRUN(E)) -- that reduces each layer's state dimension by directly minimizing long-run output-energy distortion. AIRE-Prune assigns every state a closed-form asymptotic impulse-response energy-based score, i.e., the total impulse-response energy it contributes over an infinite horizon (time), and normalizes these scores layer-wise to enable global cross-layer comparison and selection. This extends modal truncation from single systems to deep stacks and aligns pruning with asymptotic response energy rather than worst-case gain. Across diverse sequence benchmarks, AIRE-Prune reveals substantial redundancy in SISO and MIMO SSMs with average pruning of 60.8%, with average accuracy drop of 0.29% without retraining, while significantly lowering compute. Code: https://github.com/falcon-arrow/AIRE-Prune.

</details>


### [359] [Invertible Memory Flow Networks](https://arxiv.org/abs/2602.00535)
*Liyu Zerihun,Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: 提出Invertible Memory Flow Networks（IMFN）解决长序列神经记忆难题，可压缩长序列，蒸馏后用于在线推理，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: RNN及其变体有梯度消失问题，Transformers有二次缩放问题，长序列压缩优化困难。

Method: 通过因式分解使长序列压缩变得可行，将问题分解为使用“清扫器”模块的二叉树进行成对合并，每个清扫器学习简单的2对1压缩任务，在线推理时蒸馏成恒定成本的循环学生模型。

Result: 在长MNIST序列和UCF - 101视频上验证了IMFN，展示了对长序列高维数据的压缩能力。

Conclusion: IMFN能有效解决长序列神经记忆和压缩问题。

Abstract: Long sequence neural memory remains a challenging problem. RNNs and their variants suffer from vanishing gradients, and Transformers suffer from quadratic scaling. Furthermore, compressing long sequences into a finite fixed representation remains an intractable problem due to the difficult optimization landscape. Invertible Memory Flow Networks (IMFN) make long sequence compression tractable through factorization: instead of learning end-to-end compression, we decompose the problem into pairwise merges using a binary tree of "sweeper" modules. Rather than learning to compress long sequences, each sweeper learns a much simpler 2-to-1 compression task, achieving O(log N) depth with sublinear error accumulation in sequence length. For online inference, we distilled into a constant-cost recurrent student achieving O(1) sequential steps. Empirical results validate IMFN on long MNIST sequences and UCF-101 videos, demonstrating compression of high-dimensional data over long sequences.

</details>


### [360] [OpenDDI: A Comprehensive Benchmark for DDI Prediction](https://arxiv.org/abs/2602.00539)
*Xinmo Jin,Bowen Fan,Xunkai Li,Henan Sun,YuXin Zeng,Zekai Chen,Yuxuan Sun,Jia Li,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文指出药物相互作用（DDI）预测现有方法存在数据质量低和评估不标准问题，提出OpenDDI基准解决该问题，并给出相关评估见解与局限性。


<details>
  <summary>Details</summary>
Motivation: 实验发现DDI耗资源、耗时间，现有计算方法存在数据质量低和缺乏标准化评估两大挑战，需改进。

Method: 提出OpenDDI基准，数据上统一6个常用数据集和2种药物表示，新增3个大规模数据集和新的多模态药物表示；评估上统一20个SOTA模型基线和标准化规程。

Result: 基于OpenDDI进行全面评估，得出10条有价值见解，暴露当前局限。

Conclusion: OpenDDI为DDI预测领域提供关键指导，代码开源。

Abstract: Drug-Drug Interactions (DDIs) significantly influence therapeutic efficacy and patient safety. As experimental discovery is resource-intensive and time-consuming, efficient computational methodologies have become essential. The predominant paradigm formulates DDI prediction as a drug graph-based link prediction task. However, further progress is hindered by two fundamental challenges: (1) lack of high-quality data: most studies rely on small-scale DDI datasets and single-modal drug representations; (2) lack of standardized evaluation: inconsistent scenarios, varied metrics, and diverse baselines. To address the above issues, we propose OpenDDI, a comprehensive benchmark for DDI prediction. Specifically, (1) from the data perspective, OpenDDI unifies 6 widely used DDI datasets and 2 existing forms of drug representation, while additionally contributing 3 new large-scale LLM-augmented datasets and a new multimodal drug representation covering 5 modalities. (2) From the evaluation perspective, OpenDDI unifies 20 SOTA model baselines across 3 downstream tasks, with standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency. Based on OpenDDI, we conduct a comprehensive evaluation and derive 10 valuable insights for DDI prediction while exposing current limitations to provide critical guidance for this rapidly evolving field. Our code is available at https://github.com/xiaoriwuguang/OpenDDI

</details>


### [361] [One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models](https://arxiv.org/abs/2602.00541)
*Zilin Jing,Vincent Jeanselme,Yuta Kobayashi,Simon A. Lee,Chao Pang,Aparajita Kashyap,Yanwei Li,Xinzhuo Jiang,Shalmali Joshi*

Main category: cs.LG

TL;DR: 文章提出ORA预训练目标处理电子健康记录（EHR）数据，比现有方法有更好泛化性和下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有EHR基础模型的下一个token预测训练方法无法捕捉EHR的完整结构。

Method: 提出ORA预训练目标，联合建模事件时间和相关测量值。

Result: 在多个数据集、下游任务和模型架构中，ORA比下一个token预测和忽略连续测量的预训练损失更具泛化性，在回归和事件时间预测上表现更好。

Conclusion: 考虑EHR结构的预训练目标对扩展下游能力和泛化性至关重要。

Abstract: Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability

</details>


### [362] [Depth, Not Data: An Analysis of Hessian Spectral Bifurcation](https://arxiv.org/abs/2602.00545)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Yaoqing Yang*

Main category: cs.LG

TL;DR: 本研究挑战先前观点，证明神经网络的Hessian矩阵谱分叉可源于网络架构而非数据不平衡，揭示谱隙受架构影响大，设计优化算法需兼顾架构与数据特征。


<details>
  <summary>Details</summary>
Motivation: 挑战先前认为Hessian矩阵“bulk - and - spike”谱结构源于数据协方差矩阵不平衡的观点。

Method: 分析深度线性网络设置，在数据协方差完全平衡条件下进行研究。

Result: 证明即使数据协方差平衡，Hessian矩阵仍有分叉特征值结构，主导与主体特征值之比随网络深度线性增长。

Conclusion: 设计深度网络优化算法时，需同时考虑模型架构和数据特征。

Abstract: The eigenvalue distribution of the Hessian matrix plays a crucial role in understanding the optimization landscape of deep neural networks. Prior work has attributed the well-documented ``bulk-and-spike'' spectral structure, where a few dominant eigenvalues are separated from a bulk of smaller ones, to the imbalance in the data covariance matrix. In this work, we challenge this view by demonstrating that such spectral Bifurcation can arise purely from the network architecture, independent of data imbalance.
  Specifically, we analyze a deep linear network setup and prove that, even when the data covariance is perfectly balanced, the Hessian still exhibits a Bifurcation eigenvalue structure: a dominant cluster and a bulk cluster. Crucially, we establish that the ratio between dominant and bulk eigenvalues scales linearly with the network depth. This reveals that the spectral gap is strongly affected by the network architecture rather than solely by data distribution. Our results suggest that both model architecture and data characteristics should be considered when designing optimization algorithms for deep networks.

</details>


### [363] [Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry](https://arxiv.org/abs/2602.00547)
*Seunghyun Yoo,Sanghong Kim,Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 提出跨模态对齐框架解决质谱数据分子识别泛化瓶颈问题，在相关基准测试中表现良好


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法将光谱匹配视为封闭集识别任务，限制了对未见分子支架的泛化能力

Method: 提出跨模态对齐框架，将质谱图直接映射到预训练化学语言模型的分子结构嵌入空间

Result: 在严格支架不相交基准测试中，固定256路零样本检索Top - 1准确率达42.2%；全局检索泛化能力强；学习的嵌入空间化学一致性强，5路5样本分子重新识别准确率达95.4%

Conclusion: 将物理光谱分辨率与分子结构嵌入明确集成是解决质谱数据分子识别泛化瓶颈的关键

Abstract: Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability to generalize to unseen molecular scaffolds. To overcome this limitation, we propose a cross-modal alignment framework that directly maps mass spectra into the chemically meaningful molecular structure embedding space of a pretrained chemical language model. On a strict scaffold-disjoint benchmark, our model achieves a Top-1 accuracy of 42.2% in fixed 256-way zero-shot retrieval and demonstrates strong generalization under a global retrieval setting. Moreover, the learned embedding space demonstrates strong chemical coherence, reaching 95.4% accuracy in 5-way 5-shot molecular re-identification. These results suggest that explicitly integrating physical spectral resolution with molecular structure embedding is key to solving the generalization bottleneck in molecular identification from MS data.

</details>


### [364] [Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design](https://arxiv.org/abs/2602.00549)
*Kezhao Lai,Yutao Lai,Hai-Lin Liu*

Main category: cs.LG

TL;DR: 提出 Clade - AHD 框架解决 MCTS 在 AHD 中的过度利用问题，实验显示其优于现有方法并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决蒙特卡罗树搜索（MCTS）在基于大语言模型的自动启发式设计（AHD）中，启发式评估计算预算有限时的过度利用问题。

Method: 提出 Clade - AHD 框架，用进化枝级贝叶斯信念替代节点级点估计，将后代评估汇总到 Beta 分布，对这些信念进行汤普森抽样明确建模不确定性来引导探索。

Result: 在复杂组合优化问题的大量实验中，Clade - AHD 始终优于现有方法，同时显著降低计算成本。

Conclusion: Clade - AHD 是一种有效解决 MCTS 在 AHD 中局限的方法，代码已开源。

Abstract: While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.

</details>


### [365] [Forget by Uncertainty: Orthogonal Entropy Unlearning for Quantized Neural Networks](https://arxiv.org/abs/2602.00567)
*Tian Zhang,Yujia Tong,Junhao Dong,Ke Xu,Yuze Wang,Jingling Yuan*

Main category: cs.LG

TL;DR: 因量化神经网络部署及隐私法规，需量化模型机器遗忘，提出OEU框架，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络在边缘设备部署及隐私法规要求，现有量化模型机器遗忘方法有缺陷。

Method: 提出OEU框架，包括熵引导遗忘和梯度正交投影。

Result: OEU在遗忘效果和保留准确率上优于现有方法。

Conclusion: OEU是解决量化模型机器遗忘问题的有效方法。

Abstract: The deployment of quantized neural networks on edge devices, combined with privacy regulations like GDPR, creates an urgent need for machine unlearning in quantized models. However, existing methods face critical challenges: they induce forgetting by training models to memorize incorrect labels, conflating forgetting with misremembering, and employ scalar gradient reweighting that cannot resolve directional conflicts between gradients. We propose OEU, a novel Orthogonal Entropy Unlearning framework with two key innovations: 1) Entropy-guided unlearning maximizes prediction uncertainty on forgotten data, achieving genuine forgetting rather than confident misprediction, and 2) Gradient orthogonal projection eliminates interference by projecting forgetting gradients onto the orthogonal complement of retain gradients, providing theoretical guarantees for utility preservation under first-order approximation. Extensive experiments demonstrate that OEU outperforms existing methods in both forgetting effectiveness and retain accuracy.

</details>


### [366] [When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning](https://arxiv.org/abs/2602.00573)
*Zheng Zhang,Tao Hu,Xueheng Li,Yang Wang,Rui Li,Jie Zhang,Chengjun Xie*

Main category: cs.LG

TL;DR: 提出Stage - CIL范式和Stage - Bench数据集与协议，提出STAGE方法解决类增量学习中类内形态变化问题，且效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统类增量学习方法忽视类内演化现象，模型需同时处理类间区分和类内形态适应问题。

Method: 形式化Stage - CIL范式，引入Stage - Bench数据集和协议，提出STAGE方法在固定大小内存池中学习抽象可转移的演化模式。

Result: STAGE方法在实验评估中持续且显著优于现有最先进方法。

Conclusion: STAGE方法能有效同时解决类间区分和类内形态适应问题。

Abstract: Class-Incremental Learning (CIL) aims to sequentially learn new classes while mitigating catastrophic forgetting of previously learned knowledge. Conventional CIL approaches implicitly assume that classes are morphologically static, focusing primarily on preserving previously learned representations as new classes are introduced. However, this assumption neglects intra-class evolution: a phenomenon wherein instances of the same semantic class undergo significant morphological transformations, such as a larva turning into a butterfly. Consequently, a model must both discriminate between classes and adapt to evolving appearances within a single class. To systematically address this challenge, we formalize Stage-Aware CIL (Stage-CIL), a paradigm in which each class is learned progressively through distinct morphological stages. To facilitate rigorous evaluation within this paradigm, we introduce the Stage-Bench, a 10-domain, 2-stages dataset and protocol that jointly measure inter- and intra-class forgetting. We further propose STAGE, a novel method that explicitly learns abstract and transferable evolution patterns within a fixed-size memory pool. By decoupling semantic identity from transformation dynamics, STAGE enables accurate prediction of future morphologies based on earlier representations. Extensive empirical evaluation demonstrates that STAGE consistently and substantially outperforms existing state-of-the-art approaches, highlighting its effectiveness in simultaneously addressing inter-class discrimination and intra-class morphological adaptation.

</details>


### [367] [Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs](https://arxiv.org/abs/2602.00576)
*Tushaar Gangavarapu,Jiping Li,Christopher Vattheuer,Zhangyang Wang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 本文探讨修改训练数据分布能否提升大语言模型泛化能力，理论分析模型并对比优化器，发现调整数据分布可降低简单性偏差，实验验证策略能提升多个大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究修改训练数据分布能否在训练大语言模型时引导优化器找到泛化能力更好的解。

Method: 理论分析带多头线性自注意力的上下文线性回归模型，对比梯度下降（GD）和锐度感知最小化（SAM）两种优化器的训练动态，通过对训练后期学习的样本进行上采样或增强来改变训练数据分布。

Result: 首次发现SAM能降低简单性偏差，这是其泛化性能提升的关键因素；调整训练数据分布同样可降低简单性偏差并提升泛化能力；实验表明该策略能提升多个大语言模型在数学推理任务上的性能，相对准确率提升最高达18%。

Conclusion: 通过改变训练数据分布降低简单性偏差，可有效提高大语言模型的泛化能力。

Abstract: Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamics of two gradient based optimizers, namely gradient descent (GD) and sharpness-aware minimization (SAM), the latter exhibiting superior generalization properties but is prohibitively expensive for training even medium-sized LLMs. We show, for the first time, that SAM induces a lower simplicity bias (SB)-the tendency of an optimizer to preferentially learn simpler features earlier in training-and identify this reduction as a key factor underlying its improved generalization performance. Motivated by this insight, we demonstrate that altering the training data distribution by upsampling or augmenting examples learned later in training similarly reduces SB and leads to improved generalization. Our extensive experiments show that our strategy improves the performance of multiple LLMs-including Phi2-2.7B , Llama3.2-1B, Gemma3-1B-PT, and Qwen3-0.6B-Base-achieving relative accuracy gains up to 18% when fine-tuned with AdamW and Muon on mathematical reasoning tasks.

</details>


### [368] [Sparsity-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2602.00577)
*Yuze Wang,Yujia Tong,Ke Xu,Jingling Yuan,Jiawei Jiang,Chuang Hu*

Main category: cs.LG

TL;DR: 现有大语言模型去学习方法用于稀疏模型效果差，提出Sparsity - Aware Unlearning（SAU）方法，在稀疏大语言模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型去学习方法针对稠密模型设计，忽略模型稀疏化，在稀疏模型上去学习效果大幅下降。

Method: 提出Sparsity - Aware Unlearning（SAU），通过梯度掩码将去学习与稀疏化目标解耦，结合重要性感知重分配来补偿剪枝参数。

Result: SAU在稀疏大语言模型上显著优于现有方法，能有效遗忘信息并保留模型效用。

Conclusion: SAU方法能解决现有去学习方法在稀疏模型上效果不佳的问题，是一种有效的解决方案。

Abstract: Large Language Models (LLMs) inevitably memorize sensitive information during training, posing significant privacy risks. Machine unlearning has emerged as a promising solution to selectively remove such information without full retraining. However, existing methods are designed for dense models and overlook model sparsification-an essential technique for efficient LLM deployment. We find that unlearning effectiveness degrades substantially on sparse models. Through empirical analysis, we reveal that this degradation occurs because existing unlearning methods require updating all parameters, yet sparsification prunes substantial weights to zero, fundamentally limiting the model's forgetting capacity. To address this challenge, we propose Sparsity-Aware Unlearning (SAU), which decouples unlearning from sparsification objectives through gradient masking that redirects updates to surviving weights, combined with importance-aware redistribution to compensate for pruned parameters. Extensive experiments demonstrate that SAU significantly outperforms existing methods on sparse LLMs, achieving effective forgetting while preserving model utility.

</details>


### [369] [Bridging Time and Frequency: A Joint Modeling Framework for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00582)
*Xiangfei Qiu,Kangjia Yan,Xvyuan Liu,Xingjian Wu,Jilin Hu*

Main category: cs.LG

TL;DR: 提出TFMixer应对不规则多变量时间序列预测挑战，实验显示其性能达最优


<details>
  <summary>Details</summary>
Motivation: 不规则多变量时间序列预测因非均匀采样和异步性具有挑战，标准模型和经典频域方法难以处理

Method: 提出TFMixer框架，包含用可学习NUDFT提取频谱的全局频率模块、引入查询式补丁混合机制的局部时间模块，融合时频表示并利用逆NUDFT进行季节性外推

Result: 在真实数据集上实验表明TFMixer性能达最优

Conclusion: TFMixer能有效解决不规则多变量时间序列预测问题，性能优越

Abstract: Irregular multivariate time series forecasting (IMTSF) is challenging due to non-uniform sampling and variable asynchronicity. These irregularities violate the equidistant assumptions of standard models, hindering local temporal modeling and rendering classical frequency-domain methods ineffective for capturing global periodic structures. To address this challenge, we propose TFMixer, a joint time-frequency modeling framework for IMTS forecasting. Specifically, TFMixer incorporates a Global Frequency Module that employs a learnable Non-Uniform Discrete Fourier Transform (NUDFT) to directly extract spectral representations from irregular timestamps. In parallel, the Local Time Module introduces a query-based patch mixing mechanism to adaptively aggregate informative temporal patches and alleviate information density imbalance. Finally, TFMixer fuses the time-domain and frequency-domain representations to generate forecasts and further leverages inverse NUDFT for explicit seasonal extrapolation. Extensive experiments on real-world datasets demonstrate the state--of-the-art performance of TFMixer.

</details>


### [370] [Safe Langevin Soft Actor Critic](https://arxiv.org/abs/2602.00587)
*Mahesh Keswani,Samyak Jain,Raunak P. Bhattacharyya*

Main category: cs.LG

TL;DR: 提出 Safe Langevin Soft Actor - Critic (SL - SAC)算法解决约束强化学习中平衡奖励与安全难题，在 Safety - Gymnasium 基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 约束强化学习中因尖锐值极小值泛化性差和重尾风险分布处理不足，难以平衡奖励和安全。

Method: 结合使用 Adaptive Stochastic Gradient Langevin Dynamics (aSGLD) 促进集成多样性；通过 Implicit Quantile Networks (IQN) 进行分布成本估计并结合 Conditional Value - at - Risk (CVaR) 优化减轻尾部风险；采用反应式拉格朗日松弛方案调整约束执行。

Result: 在 Safety - Gymnasium 基准测试 10 个任务中的 7 个达到最低成本，保持有竞争力的回报，速度任务相比现有基线成本降低 19 - 63%。

Conclusion: 给出了 CVaR 估计误差的理论保证，证明基于 CVaR 的拉格朗日更新比基于期望成本的更新产生更强的约束违反信号。

Abstract: Balancing reward and safety in constrained reinforcement learning remains challenging due to poor generalization from sharp value minima and inadequate handling of heavy-tailed risk distribution. We introduce Safe Langevin Soft Actor-Critic (SL-SAC), a principled algorithm that addresses both issues through parameter-space exploration and distributional risk control. Our approach combines three key mechanisms: (1) Adaptive Stochastic Gradient Langevin Dynamics (aSGLD) for reward critics, promoting ensemble diversity and escape from poor optima; (2) distributional cost estimation via Implicit Quantile Networks (IQN) with Conditional Value-at-Risk (CVaR) optimization for tail-risk mitigation; and (3) a reactive Lagrangian relaxation scheme that adapts constraint enforcement based on the empirical CVaR of episodic costs. We provide theoretical guarantees on CVaR estimation error and demonstrate that CVaR-based Lagrange updates yield stronger constraint violation signals than expected-cost updates. On Safety-Gymnasium benchmarks, SL-SAC achieves the lowest cost in 7 out of 10 tasks while maintaining competitive returns, with cost reductions of 19-63% in velocity tasks compared to state-of-the-art baselines.

</details>


### [371] [SEER: Transformer-based Robust Time Series Forecasting via Automated Patch Enhancement and Replacement](https://arxiv.org/abs/2602.00589)
*Xiangfei Qiu,Xvyuan Liu,Tianen Shen,Xingjian Wu,Hanyin Cheng,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: 本文提出SEER框架解决现有基于补丁的时间序列预测方法不能动态选择补丁的问题，实验证明其有SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于补丁的时间序列预测方法无法动态选择补丁，且现实时间序列数据存在低质量问题影响预测结果。

Method: 提出增强嵌入模块，用混合专家架构改进补丁表示；引入可学习的补丁替换模块，通过动态过滤机制消除负向补丁令牌，用替换注意力模块替换低质量补丁。

Result: SEER在综合实验中展现了SOTA性能。

Conclusion: SEER框架有效解决现有方法问题，提升了时间序列预测的性能。

Abstract: Time series forecasting is important in many fields that require accurate predictions for decision-making. Patching techniques, commonly used and effective in time series modeling, help capture temporal dependencies by dividing the data into patches. However, existing patch-based methods fail to dynamically select patches and typically use all patches during the prediction process. In real-world time series, there are often low-quality issues during data collection, such as missing values, distribution shifts, anomalies and white noise, which may cause some patches to contain low-quality information, negatively impacting the prediction results. To address this issue, this study proposes a robust time series forecasting framework called SEER. Firstly, we propose an Augmented Embedding Module, which improves patch-wise representations using a Mixture-of-Experts (MoE) architecture and obtains series-wise token representations through a channel-adaptive perception mechanism. Secondly, we introduce a Learnable Patch Replacement Module, which enhances forecasting robustness and model accuracy through a two-stage process: 1) a dynamic filtering mechanism eliminates negative patch-wise tokens; 2) a replaced attention module substitutes the identified low-quality patches with global series-wise token, further refining their representations through a causal attention mechanism. Comprehensive experimental results demonstrate the SOTA performance of SEER.

</details>


### [372] [Kernelized Edge Attention: Addressing Semantic Attention Blurring in Temporal Graph Neural Networks](https://arxiv.org/abs/2602.00596)
*Govind Waghmare,Srini Rohan Gujulla Leel,Nikhil Tumbde,Sumedh B G,Sonia Gupta,Srikanta Bedathur*

Main category: cs.LG

TL;DR: 论文指出当前TGNNs存在语义注意力模糊问题，提出KEAT方法，可集成到不同架构，在链接预测任务上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有TGNNs模型计算注意力时未区分节点和边的不同时间行为，导致语义注意力模糊，难以捕捉细粒度时间依赖和计算透明度低。

Method: 引入KEAT，使用连续时间核函数（Laplacian、RBF和可学习的MLP变体）调制边特征，可与Transformer和消息传递架构集成。

Result: 在链接预测任务上，比DyGFormer提升18% MRR，比TGN提升7% MRR。

Conclusion: KEAT能在TGNNs中实现更准确、可解释和时间感知的消息传递。

Abstract: Temporal Graph Neural Networks (TGNNs) aim to capture the evolving structure and timing of interactions in dynamic graphs. Although many models incorporate time through encodings or architectural design, they often compute attention over entangled node and edge representations, failing to reflect their distinct temporal behaviors. Node embeddings evolve slowly as they aggregate long-term structural context, while edge features reflect transient, timestamped interactions (e.g. messages, trades, or transactions). This mismatch results in semantic attention blurring, where attention weights cannot distinguish between slowly drifting node states and rapidly changing, information-rich edge interactions. As a result, models struggle to capture fine-grained temporal dependencies and provide limited transparency into how temporal relevance is computed. This paper introduces KEAT (Kernelized Edge Attention for Temporal Graphs), a novel attention formulation that modulates edge features using a family of continuous-time kernels, including Laplacian, RBF, and learnable MLP variant. KEAT preserves the distinct roles of nodes and edges, and integrates seamlessly with both Transformer-style (e.g., DyGFormer) and message-passing (e.g., TGN) architectures. It achieves up to 18% MRR improvement over the recent DyGFormer and 7% over TGN on link prediction tasks, enabling more accurate, interpretable and temporally aware message passing in TGNNs.

</details>


### [373] [Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains](https://arxiv.org/abs/2602.00603)
*Luca Viano,Ruida Zhou,Yifan Sun,Mahdi Namazifar,Volkan Cevher,Shoham Sabach,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: 研究利用评级差距信息设计比直接偏好优化（DPO）算法更优的算法，并证明其性能稳健且表现良好。


<details>
  <summary>Details</summary>
Motivation: DPO算法使用的成对偏好反馈虽便于数据收集，但存在响应质量模糊的问题，需要利用额外信息改进算法。

Method: 设计可以利用评级差距额外信息的新算法。

Result: 新算法在有准确评级差距信息时统计速率更快，对评级差距的不准确有稳健性，在多个大语言模型和评估基准上比多种DPO风格算法表现更好。

Conclusion: 提出的利用评级差距信息的算法能有效改进现有DPO算法的性能。

Abstract: The class of direct preference optimization (DPO) algorithms has emerged as a promising approach for solving the alignment problem in foundation models. These algorithms work with very limited feedback in the form of pairwise preferences and fine-tune models to align with these preferences without explicitly learning a reward model. While the form of feedback used by these algorithms makes the data collection process easy and relatively more accurate, its ambiguity in terms of the quality of responses could have negative implications. For example, it is not clear if a decrease (increase) in the likelihood of preferred (dispreferred) responses during the execution of these algorithms could be interpreted as a positive or negative phenomenon. In this paper, we study how to design algorithms that can leverage additional information in the form of rating gap, which informs the learner how much the chosen response is better than the rejected one. We present new algorithms that can achieve faster statistical rates than DPO in presence of accurate rating gap information. Moreover, we theoretically prove and empirically show that the performance of our algorithms is robust to inaccuracy in rating gaps. Finally, we demonstrate the solid performance of our methods in comparison to a number of DPO-style algorithms across a wide range of LLMs and evaluation benchmarks.

</details>


### [374] [Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference](https://arxiv.org/abs/2602.00620)
*Juntao Fang,Shifeng Xie,Shengbin Nie,Yuhui Ling,Yuming Liu,Zijian Li,Keli Zhang,Lujia Pan,Themis Palpanas,Ruichu Cai*

Main category: cs.LG

TL;DR: 提出TIC - FM框架解决时间序列基础模型零样本评估问题，实验显示其在128个UCR数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有零样本评估时间序列基础模型进行分类的做法违背零样本无训练前提，且因分类器训练选择引入评估偏差。

Method: 提出TIC - FM框架，将标记训练集作为上下文，一次前向传递预测所有测试实例标签，结合时间序列编码器、轻量级投影适配器和分割掩码潜在记忆Transformer，还给出理论依据。

Result: 在128个UCR数据集实验中显示出高准确性，在极低标签情况中有持续收益。

Conclusion: TIC - FM能实现无训练迁移，可有效解决现有评估方法问题。

Abstract: The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent training choices. To address this issue, we propose TIC-FM, an in-context learning framework that treats the labeled training set as context and predicts labels for all test instances in a single forward pass, without parameter updates. TIC-FM pairs a time series encoder and a lightweight projection adapter with a split-masked latent memory Transformer. We further provide theoretical justification that in-context inference can subsume trained classifiers and can emulate gradient-based classifier training within a single forward pass. Experiments on 128 UCR datasets show strong accuracy, with consistent gains in the extreme low-label situation, highlighting training-free transfer

</details>


### [375] [MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting](https://arxiv.org/abs/2602.00624)
*Hyekyung Yoon,Minhyuk Lee,Imseung Park,Myungjoo Kang*

Main category: cs.LG

TL;DR: 本文引入层敏感度指标分析多层感知机骨干层行为，提出轻量级MoDEx框架，在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多元长期时间序列预测（LTSF）范式中，骨干层行为未得到充分探索。

Method: 引入基于梯度的层敏感度指标量化各时间点对层潜在特征的贡献，提出MoDEx框架，用特定深度的MLP专家替代复杂骨干。

Result: MoDEx在七个真实世界基准测试中达到SOTA，78%的情况下排名第一，使用更少参数和计算资源，还能提升Transformer变体性能。

Conclusion: MoDEx是一个高效且高性能的LTSF框架，具有强大的泛化能力。

Abstract: Multivariate long-term time series forecasting (LTSF) supports critical applications such as traffic-flow management, solar-power scheduling, and electricity-transformer monitoring. The existing LTSF paradigms follow a three-stage pipeline of embedding, backbone refinement, and long-horizon prediction. However, the behaviors of individual backbone layers remain underexplored. We introduce layer sensitivity, a gradient-based metric inspired by GradCAM and effective receptive field theory, which quantifies both positive and negative contributions of each time point to a layer's latent features. Applying this metric to a three-layer MLP backbone reveals depth-specific specialization in modeling temporal dynamics in the input sequence. Motivated by these insights, we propose MoDEx, a lightweight Mixture of Depth-specific Experts, which replaces complex backbones with depth-specific MLP experts. MoDEx achieves state-of-the-art accuracy on seven real-world benchmarks, ranking first in 78 percent of cases, while using significantly fewer parameters and computational resources. It also integrates seamlessly into transformer variants, consistently boosting their performance and demonstrating robust generalizability as an efficient and high-performance LTSF framework.

</details>


### [376] [From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs](https://arxiv.org/abs/2602.00628)
*Louis Schiekiera,Max Zimmer,Christophe Roux,Sebastian Pokutta,Fritz Günther*

Main category: cs.LG

TL;DR: 研究从大语言模型在心理语言学实验中的行为恢复其隐藏状态几何结构的程度，发现强制选择行为与隐藏状态几何结构更匹配，行为测量保留内部语义几何信息。


<details>
  <summary>Details</summary>
Motivation: 探究能否从大语言模型在心理语言学实验中的行为恢复其隐藏状态几何结构。

Method: 对八个指令调优的Transformer模型进行基于相似性的强制选择和自由联想两种实验范式，构建基于行为的相似性矩阵，用表征相似性分析比较行为几何与层间隐藏状态相似性，并与FastText、BERT和跨模型共识进行基准对比。

Result: 强制选择行为比自由联想更能与隐藏状态几何结构对齐；行为相似性（尤其是强制选择）能在词汇基线和跨模型共识之外预测未见的隐藏状态相似性。

Conclusion: 行为测量保留了关于内部语义几何结构的可恢复信息，探讨了行为任务揭示隐藏认知状态的能力。

Abstract: We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.

</details>


### [377] [Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration](https://arxiv.org/abs/2602.00636)
*Yujie Yang,Zhilong Zheng,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 论文首次解答强化学习中环境探索安全问题，提出SEE框架，证明其能使模型和可行区收敛到安全探索平衡点，实验验证算法有效。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中环境探索安全问题，明确最大可行区及识别方法。

Method: 提出平衡导向的安全探索框架SEE，交替寻找最大可行区和最小不确定性模型，利用不确定模型的图表示法进行证明。

Result: 证明SEE得到的不确定模型单调精炼，可行区单调扩展，二者收敛到安全探索的平衡；实验表明算法能零违规扩展可行区，几次迭代后达到安全探索平衡。

Conclusion: 安全探索的目标是找到可行区和环境模型间的平衡。

Abstract: Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.

</details>


### [378] [Combinatorial Bandit Bayesian Optimization for Tensor Outputs](https://arxiv.org/abs/2602.00640)
*Jingru Huang,Haijie Xu,Jie Guo,Manrui Jiang,Chen Zhang*

Main category: cs.LG

TL;DR: 提出针对张量输出函数的贝叶斯优化方法及组合多臂老虎机贝叶斯优化方法，有理论保证且实验效果优。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法未处理张量输出函数，且存在部分输出贡献目标函数的复杂问题。

Method: 提出张量输出高斯过程作代理模型，开发UCB采集函数；提出张量输出CBBO方法，设计CMAB - UCB2准则；建立理论遗憾边界。

Result: 理论上有亚线性性能保证，合成与真实实验显示方法优越性。

Conclusion: 所提方法能有效解决张量输出函数优化及复杂问题设置下的优化问题。

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and black-box functions across various domains. Existing BO methods have not addressed tensor-output functions. To fill this gap, we propose a novel tensor-output BO method. Specifically, we first introduce a tensor-output Gaussian process (TOGP) with two classes of tensor-output kernels as a surrogate model of the tensor-output function, which can effectively capture the structural dependencies within the tensor. Based on it, we develop an upper confidence bound (UCB) acquisition function to select the queried points. Furthermore, we introduce a more complex and practical problem setting, named combinatorial bandit Bayesian optimization (CBBO), where only a subset of the outputs can be selected to contribute to the objective function. To tackle this, we propose a tensor-output CBBO method, which extends TOGP to handle partially observed outputs, and accordingly design a novel combinatorial multi-arm bandit-UCB2 (CMAB-UCB2) criterion to sequentially select both the queried points and the optimal output subset. Theoretical regret bounds for the two methods are established, ensuring their sublinear performance. Extensive synthetic and real-world experiments demonstrate their superiority.

</details>


### [379] [CoRe-Fed: Bridging Collaborative and Representation Fairness via Federated Embedding Distillation](https://arxiv.org/abs/2602.00647)
*Noorain Mukhtiar,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 针对联邦学习中公平性问题，提出CoRe - Fed框架，实验证明其能提升公平性和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习算法因数据分布异构和参与度不均，存在表示偏差和协作偏差，导致不公平结果和模型性能下降。

Method: 提出CoRe - Fed统一优化框架，通过嵌入级正则化和公平感知聚合来解决公平性问题，包括促进嵌入语义一致性的机制和基于动态奖惩的聚合策略。

Result: 在不同模型和数据集上的大量实验表明，CoRe - Fed相比现有基线算法提升了公平性和模型性能。

Conclusion: CoRe - Fed框架能有效缓解联邦学习中的公平性问题，提升模型的公平性和性能。

Abstract: With the proliferation of distributed data sources, Federated Learning (FL) has emerged as a key approach to enable collaborative intelligence through decentralized model training while preserving data privacy. However, conventional FL algorithms often suffer from performance disparities across clients caused by heterogeneous data distributions and unequal participation, which leads to unfair outcomes. Specifically, we focus on two core fairness challenges, i.e., representation bias, arising from misaligned client representations, and collaborative bias, stemming from inequitable contribution during aggregation, both of which degrade model performance and generalizability. To mitigate these disparities, we propose CoRe-Fed, a unified optimization framework that bridges collaborative and representation fairness via embedding-level regularization and fairness-aware aggregation. Initially, an alignment-driven mechanism promotes semantic consistency between local and global embeddings to reduce representational divergence. Subsequently, a dynamic reward-penalty-based aggregation strategy adjusts each client's weight based on participation history and embedding alignment to ensure contribution-aware aggregation. Extensive experiments across diverse models and datasets demonstrate that CoRe-Fed improves both fairness and model performance over the state-of-the-art baseline algorithms.

</details>


### [380] [PHAT: Modeling Period Heterogeneity for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00654)
*Jiaming Ma,Guanjun Wang,Qihe Huang,Sheng Huang,Haofeng Ma,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 提出PHAT模型捕捉多元时间序列周期性异质性，在14个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列预测模型大多忽略了现实数据中常见的周期性异质性问题。

Method: 将多元输入排列成三维“周期桶”张量，避免不一致周期干扰；提出正负注意力机制捕捉周期依赖；分解周期对齐注意力分数并添加调制项。

Result: 在14个真实数据集上与18个基线模型对比，PHAT显著优于现有方法。

Conclusion: PHAT模型能有效捕捉周期性异质性，实现具有竞争力的预测性能。

Abstract: While existing multivariate time series forecasting models have advanced significantly in modeling periodicity, they largely neglect the periodic heterogeneity common in real-world data, where variates exhibit distinct and dynamically changing periods. To effectively capture this periodic heterogeneity, we propose PHAT (Period Heterogeneity-Aware Transformer). Specifically, PHAT arranges multivariate inputs into a three-dimensional "periodic bucket" tensor, where the dimensions correspond to variate group characteristics with similar periodicity, time steps aligned by phase, and offsets within the period. By restricting interactions within buckets and masking cross-bucket connections, PHAT effectively avoids interference from inconsistent periods. We also propose a positive-negative attention mechanism, which captures periodic dependencies from two perspectives: periodic alignment and periodic deviation. Additionally, the periodic alignment attention scores are decomposed into positive and negative components, with a modulation term encoding periodic priors. This modulation constrains the attention mechanism to more faithfully reflect the underlying periodic trends. A mathematical explanation is provided to support this property. We evaluate PHAT comprehensively on 14 real-world datasets against 18 baselines, and the results show that it significantly outperforms existing methods, achieving highly competitive forecasting performance. Our sources is available at GitHub.

</details>


### [381] [Riemannian Flow Matching for Disentangled Graph Domain Adaptation](https://arxiv.org/abs/2602.00656)
*Yingxu Wang,Xinwang Liu,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: 提出几何感知的GDA框架DisRFM，结合黎曼嵌入和基于流的传输，解决结构退化和优化不稳定问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图域自适应（GDA）的对抗学习范式面临结构退化（层次和语义表征纠缠）和优化不稳定（极小极大对抗训练的振荡动力学）两个关键挑战。

Method: 提出DisRFM框架，将图嵌入黎曼流形，通过极坐标解耦结构和语义，利用径向Wasserstein对齐和角度聚类防止特征纠缠和崩溃；使用黎曼流匹配解决对抗对齐的不稳定性，保证稳定收敛。

Result: 理论证明了流匹配的渐近稳定性并推导了目标风险的更紧边界，实验表明DisRFM始终优于现有方法。

Conclusion: DisRFM能有效解决现有GDA方法存在的问题，是一种更优的图域自适应框架。

Abstract: Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.

</details>


### [382] [Three-Way Emotion Classification of EEG-based Signals using Machine Learning](https://arxiv.org/abs/2602.00670)
*Ashna Purwar,Gaurav Simkar,Madhumita,Sachin Kadam*

Main category: cs.LG

TL;DR: 本文探讨用机器学习模型对脑电图（EEG）信号进行三种情绪分类，对比多种模型，随机森林模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 情绪感知系统和基于EEG的情绪识别是新兴研究领域，旨在找到适合EEG信号三种情绪分类的机器学习模型。

Method: 对有限的EEG信号数据集进行数据预处理，训练并测试逻辑回归（LR）、支持向量机（SVM）和随机森林（RF）三种常用机器学习模型，用准确率和F1分数评估性能。

Result: 机器学习模型可有效用于EEG信号的三种情绪分类，随机森林模型在可用数据集上表现最佳，在准确率参数上优于现有先进分类模型。

Conclusion: 随机森林模型能更准确有效地捕捉情绪模式，适合用于EEG信号的三种情绪分类。

Abstract: Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.

</details>


### [383] [Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian Process Conditional Density Estimators for TSAD](https://arxiv.org/abs/2602.00672)
*Aleksandr Yugay,Hang Cui,Changhua Pei,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 简单线性自回归异常得分方法在时间序列异常检测中表现出色，建议未来研究纳入线性基线并开发新基准。


<details>
  <summary>Details</summary>
Motivation: 以往时间序列异常检测研究专注复杂难训练的神经网络架构，需重新审视该模式。

Method: 使用普通最小二乘法回归的简单线性自回归异常得分方法。

Result: 该方法在大量单变量和多变量基准测试中达到更高精度，且所需计算资源少得多。

Conclusion: 未来研究应包含强线性基线，开发有更丰富时间结构的新基准以凸显深度学习模型优势。

Abstract: Research in time series anomaly detection (TSAD) has largely focused on developing increasingly sophisticated, hard-to-train, and expensive-to-infer neural architectures. We revisit this paradigm and show that a simple linear autoregressive anomaly score with the closed-form solution provided by ordinary least squares (OLS) regression consistently matches or outperforms state-of-the-art deep detectors. From a theoretical perspective, we show that linear models capture a broad class of anomaly types, estimating a finite-history Gaussian process conditional density. From a practical side, across extensive univariate and multivariate benchmarks, the proposed approach achieves superior accuracy while requiring orders of magnitude fewer computational resources. Thus, future research should consistently include strong linear baselines and, more importantly, develop new benchmarks with richer temporal structures pinpointing the advantages of deep learning models.

</details>


### [384] [Provably Protecting Fine-Tuned LLMs from Training Data Extraction](https://arxiv.org/abs/2602.00688)
*Tom Segal,Asaf Shabtai,Yuval Elovici*

Main category: cs.LG

TL;DR: 微调大语言模型在敏感数据集上引发隐私担忧，提出SCP-$Δ_r$算法，理论边界优于现有方法且能在保护数据隐私同时减少性能损失。


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型在敏感数据集上时，训练数据提取攻击会暴露机密信息，现有防御方法缺乏隐私保证或导致效用显著下降。

Method: 提出基于相对概率和近访问自由（NAF）的SCP-$Δ_r$算法，用基础模型平滑低影响标记。

Result: SCP-$Δ_r$算法在理论边界上比现有基于NAF的方法好几个数量级，能有效抵御训练数据提取攻击。

Conclusion: SCP-$Δ_r$算法在保护数据隐私的同时，仅造成极小的性能损失。

Abstract: Fine-tuning large language models (LLMs) on sensitive datasets raises privacy concerns, as training data extraction (TDE) attacks can expose highly confidential information. Existing defenses against such attacks either lack formal privacy guarantees or incur substantial utility degradation. We observe that fine-tuning induces widespread probability shifts, yet preserving only a small subset of influential token-level deviations is sufficient; the remaining shifts can be aggressively smoothed with minimal impact on utility. Motivated by this insight, we propose SCP-$Δ_r$, a Near Access Freeness (NAF)-based algorithm that operates on relative probabilities and explicitly smooths low-impact tokens using a base model. SCP-$Δ_r$ achieves orders-of-magnitude better theoretical bounds than existing NAF based methods and provides strong empirical protection against TDE attacks with minimal performance loss.

</details>


### [385] [Topology and Geometry of the Learning Space of ReLU Networks: Connectivity and Singularities](https://arxiv.org/abs/2602.00693)
*Marco Nurisso,Pierrick Leroy,Giovanni Petri,Francesco Vaccarino*

Main category: cs.LG

TL;DR: 研究前馈ReLU网络参数空间性质，探讨连通性和奇点问题，扩展先前结果，建立与可微剪枝联系并实验验证


<details>
  <summary>Details</summary>
Motivation: 理解前馈ReLU网络参数空间性质对分析和指导训练动态至关重要，要解决参数空间连通性和奇点存在性两个关键挑战

Method: 基于一般有向无环图（DAG）架构进行研究，对连通性进行全面刻画

Result: 发现奇点与底层DAG及其诱导子网络拓扑密切相关，讨论奇点可达性并建立与可微剪枝的联系

Conclusion: 研究成果有助于理解前馈ReLU网络参数空间的连通性和奇点问题，数值实验验证了理论。

Abstract: Understanding the properties of the parameter space in feed-forward ReLU networks is critical for effectively analyzing and guiding training dynamics. After initialization, training under gradient flow decisively restricts the parameter space to an algebraic variety that emerges from the homogeneous nature of the ReLU activation function. In this study, we examine two key challenges associated with feed-forward ReLU networks built on general directed acyclic graph (DAG) architectures: the (dis)connectedness of the parameter space and the existence of singularities within it. We extend previous results by providing a thorough characterization of connectedness, highlighting the roles of bottleneck nodes and balance conditions associated with specific subsets of the network. Our findings clearly demonstrate that singularities are intricately connected to the topology of the underlying DAG and its induced sub-networks. We discuss the reachability of these singularities and establish a principled connection with differentiable pruning. We validate our theory with simple numerical experiments.

</details>


### [386] [LocalV: Exploiting Information Locality for IP-level Verilog Generation](https://arxiv.org/abs/2602.00704)
*Hanqi Lyu,Di Huang,Yaoyu Zhu,Kangcheng Liu,Bohan Dou,Chongxiao Li,Pengwei Jin,Shuyao Cheng,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: 传统RTL代码生成劳动密集，现有LLM方法难用于工业IP级设计，提出LocalV框架应对挑战，实验效果超SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统RTL代码生成依赖人工，现有LLM方法在工业IP级设计任务中存在缩放难题。

Method: 提出LocalV多智能体框架，将长文档到长代码生成问题分解为短任务，集成文档分区、任务规划、局部代码生成等步骤。

Result: 在RealBench基准测试中，LocalV通过率达45.0%，远超SOTA的21.6%。

Conclusion: LocalV能有效应对现有方法面临的三个关键挑战，在RTL代码生成上表现优于当前SOTA模型。

Abstract: The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.

</details>


### [387] [Deep Time-series Forecasting Needs Kernelized Moment Balancing](https://arxiv.org/abs/2602.00717)
*Licheng Pan,Hao Wang,Haocheng Yang,Yuqi Li,Qingsong Wen,Xiaoxi Li,Zhichao Chen,Haoxuan Li,Zhixuan Chu,Yuan Lu*

Main category: cs.LG

TL;DR: 将深度时间序列预测转化为分布平衡问题，指出现有目标未达真正分布平衡，提出KMB - DF方法，实验证明其提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度时间序列预测目标无法依据Imbens准则实现真正的分布平衡。

Method: 提出直接预测与核化矩平衡方法（KMB - DF），从再生核希尔伯特空间自适应选择平衡函数，推导可处理且可微目标以用于梯度训练。

Result: 跨多个模型和数据集的大量实验表明，KMB - DF持续提高了预测准确性，达到了最先进的性能。

Conclusion: KMB - DF方法有效解决了现有深度时间序列预测目标的局限性，提升了预测效果。

Abstract: Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstrate that existing objectives fail to meet this criterion, as they enforce moment matching only for one or two predefined balancing functions, thus failing to achieve full distribution balance. To address this limitation, we propose direct forecasting with kernelized moment balancing (KMB-DF). Unlike existing objectives, KMB-DF adaptively selects the most informative balancing functions from a reproducing kernel hilbert space (RKHS) to enforce sufficient distribution balancing. We derive a tractable and differentiable objective that enables efficient estimation from empirical samples and seamless integration into gradient-based training pipelines. Extensive experiments across multiple models and datasets show that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance. Code is available at https://anonymous.4open.science/r/KMB-DF-403C.

</details>


### [388] [Federated Learning at the Forefront of Fairness: A Multifaceted Perspective](https://arxiv.org/abs/2602.00718)
*Noorain Mukhtiar,Adnan Mahmood,Yipeng Zhou,Jian Yang,Jing Teng,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 本文对联邦学习中公平感知方法进行多维度分类，给出框架处理公平问题，探讨评估指标并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 受异构客户端约束和各场景下平衡模型性能的驱动，公平性成为联邦学习中的关键因素。

Method: 从模型性能导向和能力导向等多维度对现有公平感知方法进行分类；提供框架分类和处理公平问题。

Result: 提出了一个用于分类和解决公平问题的框架，探讨了评估公平性的指标。

Conclusion: 探索了开放研究方向并提出潜在解决方案，为联邦学习公平性研究奠定基础。

Abstract: Fairness in Federated Learning (FL) is emerging as a critical factor driven by heterogeneous clients' constraints and balanced model performance across various scenarios. In this survey, we delineate a comprehensive classification of the state-of-the-art fairness-aware approaches from a multifaceted perspective, i.e., model performance-oriented and capability-oriented. Moreover, we provide a framework to categorize and address various fairness concerns and associated technical aspects, examining their effectiveness in balancing equity and performance within FL frameworks. We further examine several significant evaluation metrics leveraged to measure fairness quantitatively. Finally, we explore exciting open research directions and propose prospective solutions that could drive future advancements in this important area, laying a solid foundation for researchers working toward fairness in FL.

</details>


### [389] [Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation](https://arxiv.org/abs/2602.00722)
*Hao Gu,Mao-Lin Luo,Zi-Hao Zhou,Han-Chen Zhang,Min-Ling Zhang,Tong Wei*

Main category: cs.LG

TL;DR: 从知识分解角度出发，提出将任务更新幅度与方向结构解耦并在受限Stiefel流形上优化的方法，缓解前后遗忘问题，优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效持续学习方法多关注避免与过去更新的干扰，未考虑如何让当前任务更新自然保留先前知识，且低秩适应存在问题。

Method: 将任务更新的幅度与方向结构解耦，在受限Stiefel流形上构建约束优化问题，用与视觉语言模型标准优化器兼容的投影一阶方法求解。

Result: 方法缓解了前后遗忘问题，持续优于持续学习基线。

Conclusion: 提出的方法有效，代码已开源，地址为https://github.com/haodotgu/EBLoRA 。

Abstract: Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.

</details>


### [390] [Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity](https://arxiv.org/abs/2602.00723)
*Prakhar Ganesh,Reza Shokri,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: 论文引入prompt multiplicity框架评估大语言模型幻觉的一致性，发现现有评估不足，揭示检测和缓解策略的局限。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型幻觉评估仅关注正确性，忽略一致性，无法有效区分和应对幻觉危害。

Method: 引入prompt multiplicity框架量化大语言模型评估中的一致性。

Result: 分析显示基准测试中存在显著的不一致性；检测技术检测的是一致性而非正确性；缓解技术可能引入额外不一致性。

Conclusion: 将prompt multiplicity融入幻觉评估，能提供更好的潜在危害框架，揭示当前检测和缓解策略的关键局限。

Abstract: Large language models (LLMs) are known to "hallucinate" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.

</details>


### [391] [Pareto-Conditioned Diffusion Models for Offline Multi-Objective Optimization](https://arxiv.org/abs/2602.00737)
*Jatan Shrestha,Santeri Heiskanen,Kari Hepola,Severi Rissanen,Pekka Jääskeläinen,Joni Pajarinen*

Main category: cs.LG

TL;DR: 提出Pareto条件扩散（PCD）框架解决离线多目标优化问题，实验证明其表现优异且更稳定。


<details>
  <summary>Details</summary>
Motivation: 离线多目标优化（MOO）中，主要挑战是在静态数据集上超越观测数据进行泛化。

Method: 引入PCD框架，将离线MOO作为条件采样问题，直接基于期望权衡条件进行处理，采用重加权策略和参考方向机制。

Result: 在标准离线MOO基准测试中，PCD取得了极具竞争力的表现。

Conclusion: PCD在不同任务上比现有离线MOO方法更具一致性。

Abstract: Multi-objective optimization (MOO) arises in many real-world applications where trade-offs between competing objectives must be carefully balanced. In the offline setting, where only a static dataset is available, the main challenge is generalizing beyond observed data. We introduce Pareto-Conditioned Diffusion (PCD), a novel framework that formulates offline MOO as a conditional sampling problem. By conditioning directly on desired trade-offs, PCD avoids the need for explicit surrogate models. To effectively explore the Pareto front, PCD employs a reweighting strategy that focuses on high-performing samples and a reference-direction mechanism to guide sampling towards novel, promising regions beyond the training data. Experiments on standard offline MOO benchmarks show that PCD achieves highly competitive performance and, importantly, demonstrates greater consistency across diverse tasks than existing offline MOO approaches.

</details>


### [392] [GraphNNK -- Graph Classification and Interpretability](https://arxiv.org/abs/2602.00753)
*Zeljko Bolevic,Milos Brajovic,Isidora Stankovic,Ljubisa Stankovic*

Main category: cs.LG

TL;DR: GNN依赖参数分类器有局限，基于插值方法如NNK可解决问题。


<details>
  <summary>Details</summary>
Motivation: 解决GNN依赖参数分类器导致的可解释性和泛化性问题。

Method: 采用基于插值的方法，尤其是非负核回归（NNK）。

Result: 可将预测表示为嵌入空间中相似训练示例的凸组合，有理论结果和可解释解释。

Conclusion: 基于插值的方法在处理图结构数据学习方面有优势。

Abstract: Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particularly Non-Negative Kernel regression (NNK), has demonstrated that predictions can be expressed as convex combinations of similar training examples in the embedding space, yielding both theoretical results and interpretable explanations.

</details>


### [393] [BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features](https://arxiv.org/abs/2602.00767)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 研究用机制方法防止语言模型微调时出现的紧急失准问题，通过约束内部特征减少失准且不降低模型质量。


<details>
  <summary>Details</summary>
Motivation: 语言模型在狭窄监督目标微调时会出现紧急失准，即学习目标行为时产生不良域外行为，需要解决该问题。

Method: 识别能控制失准行为的少量内部特征，在微调时阻止模型强化这些特征。

Result: 在六个微调领域，约束固定特征集最多可相对减少95%的紧急失准，且不降低模型质量和目标任务表现；还发现长时间微调失准会再现及提出部分恢复失准阻止效果的修改方法。

Conclusion: 对内部机制进行有针对性的训练时间约束可减轻紧急失准，且不降低目标任务表现。

Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.

</details>


### [394] [Provable Model Provenance Set for Large Language Models](https://arxiv.org/abs/2602.00772)
*Xiaoqi Qiu,Hao Zeng,Zhiyu Hou,Hongxin Wei*

Main category: cs.LG

TL;DR: 现有模型来源分析方法不可靠，本文提出MPS方法解决问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型来源分析方法缺乏可靠误差控制、忽视多来源的问题，满足对可靠模型来源分析的需求。

Method: 先对模型来源问题进行形式化定义，再提出MPS方法，采用顺序测试和排除程序自适应构建满足要求的小集合。

Result: MPS能有效实现目标来源覆盖，严格限制无关模型的纳入。

Conclusion: MPS在归因和审计任务的实际来源分析中有应用潜力。

Abstract: The growing prevalence of unauthorized model usage and misattribution has increased the need for reliable model provenance analysis. However, existing methods largely rely on heuristic fingerprint-matching rules that lack provable error control and often overlook the existence of multiple sources, leaving the reliability of their provenance claims unverified. In this work, we first formalize the model provenance problem with provable guarantees, requiring rigorous coverage of all true provenances at a prescribed confidence level. Then, we propose the Model Provenance Set (MPS), which employs a sequential test-and-exclusion procedure to adaptively construct a small set satisfying the guarantee. The key idea of MPS is to test the significance of provenance existence within a candidate pool, thereby establishing a provable asymptotic guarantee at a user-specific confidence level. Extensive experiments demonstrate that MPS effectively achieves target provenance coverage while strictly limiting the inclusion of unrelated models, and further reveal its potential for practical provenance analysis in attribution and auditing tasks.

</details>


### [395] [A novel VAE-DML fusion framework for casual analysis of greenwashing in the mining industry](https://arxiv.org/abs/2602.00774)
*Yuxin Lu,Zhen Peng,Xiqiang Xia,Jie Wang*

Main category: cs.LG

TL;DR: 在全球绿色转型和“双碳”目标背景下，研究股权制衡对矿业产业链企业洗绿行为的抑制作用及机制，采用VAE和DML模型分析，发现股权制衡与洗绿负相关，有区域、产业链和行业异质性及时间动态性，通过三个渠道发挥抑制作用。


<details>
  <summary>Details</summary>
Motivation: 全球绿色转型和“双碳”目标下，确保矿业产业链企业环境披露真实性和可靠性是可持续发展和国家战略目标的核心问题，从公司治理角度研究股权制衡对企业洗绿行为的抑制作用。

Method: 创新采用变分自编码器（VAE）和双重机器学习（DML）模型构建反事实场景，缓解内生性问题，确定股权制衡与洗绿的因果关系。

Result: 股权制衡与企业洗绿有显著负因果关系，抑制作用有区域、产业链、行业异质性和时间动态性，机制分析表明通过缓解管理层绩效压力、提高高管团队稳定性、加强媒体监督三个渠道发挥作用。

Conclusion: 股权制衡对企业洗绿行为有实质性治理效果，能通过多种渠道发挥作用，促进企业可持续发展达成国家战略目标。

Abstract: Against the backdrop of the global green transition and "dual carbon" goals, mining industry chain enterprises are pivotal entities in terms of resource consumption and environmental impact. Their environmental performance directly affects regional ecological security and is closely tied to national resource strategies and green transformation outcomes. Ensuring the authenticity and reliability of their environmental disclosure is thus a core and urgent issue for sustainable development and national strategic objectives.From a corporate governance perspective, this study examines equity balance as a fundamental governance mechanism, investigating its inhibitory effect on greenwashing behavior among these enterprises and the underlying pathways involved. Methodologically, the paper innovatively employs a Variational Autoencoder (VAE) and a Double Machine Learning (DML) model to construct counterfactual scenarios, mitigating endogeneity concerns and precisely identifying the causal relationship between equity balance and greenwashing. The findings indicate, first, a significant negative causal relationship between equity balance and corporate greenwashing, confirming its substantive governance effect. Second, this inhibitory effect exhibits notable heterogeneity, manifesting more strongly in western regions, upstream segments of the industrial chain, and industries with high environmental sensitivity. Third, the governance effect demonstrates clear temporal dynamics, with the strongest impact occurring in the current period, followed by a diminishing yet statistically significant lagged effect, and ultimately a stable long-term cumulative influence. Finally, mechanism analysis reveals that equity balance operates through three distinct channels to curb greenwashing: alleviating management performance pressure, enhancing the stability of the executive team, and intensifying media scrutiny.

</details>


### [396] [Stable Time Series Prediction of Enterprise Carbon Emissions Based on Causal Inference](https://arxiv.org/abs/2602.00775)
*Zitao Hong,Zhen Peng,Xueping Liu*

Main category: cs.LG

TL;DR: 在双碳背景下，针对碳排放数据漂移问题，提出结合因果推断、稳定学习和时间序列建模的稳定时间预测机制。


<details>
  <summary>Details</summary>
Motivation: 碳排放数据在时空维度存在分布偏移和非平稳性，影响碳排放报告准确性和预测模型指导价值。

Method: 将因果推断视角与稳定学习方法、时间序列建模相结合，构建风险一致性约束的稳定学习框架，通过自适应归一化和样本重加权策略调整非平稳性。

Result: 可从多环境样本中提取因果稳定特征，增强模型在复杂环境下的泛化能力和可解释性。

Conclusion: 所提出的稳定时间预测机制能有效应对碳排放数据的分布偏移和非平稳性问题。

Abstract: Against the backdrop of ongoing carbon peaking and carbon neutrality goals, accurate prediction of enterprise carbon emission trends constitutes an essential foundation for energy structure optimization and low-carbon transformation decision-making. Nevertheless, significant heterogeneity persists across regions, industries and individual enterprises regarding energy structure, production scale, policy intensity and governance efficacy, resulting in pronounced distribution shifts and non-stationarity in carbon emission data across both temporal and spatial dimensions. Such cross-regional and cross-enterprise data drift not only compromises the accuracy of carbon emission reporting but substantially undermines the guidance value of predictive models for production planning and carbon quota trading decisions. To address this critical challenge, we integrate causal inference perspectives with stable learning methodologies and time-series modelling, proposing a stable temporal prediction mechanism tailored to distribution shift environments. This mechanism incorporates enterprise-level energy inputs, capital investment, labour deployment, carbon pricing, governmental interventions and policy implementation intensity, constructing a risk consistency-constrained stable learning framework that extracts causal stable features (robust against external perturbations yet demonstrating long-term stable effects on carbon dioxide emissions) from multi-environment samples across diverse policies, regions and industrial sectors. Furthermore, through adaptive normalization and sample reweighting strategies, the approach dynamically rectifies temporal non-stationarity induced by economic fluctuations and policy transitions, ultimately enhancing model generalization capability and explainability in complex environments.

</details>


### [397] [Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors](https://arxiv.org/abs/2602.00788)
*Md Abir Hossen,Mohammad Ali Javidian,Vignesh Narayanan,Jason M. O'Kane,Pooyan Jamshidi*

Main category: cs.LG

TL;DR: 提出多目标MFBO方法RESCUE，结合因果演算，在多领域问题上提升采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有MFBO方法主要捕捉关联依赖而非因果机制，低保真代理与目标保真度不一致时表现不佳。

Method: 学习结构因果模型，构建编码干预效果的概率多保真替代模型，引入因果超体积知识梯度采集策略。

Result: RESCUE在机器人、机器学习和医疗保健等合成和现实问题上比现有方法提高了采样效率。

Conclusion: RESCUE能有效应对多保真贝叶斯优化中低保真代理与目标保真度不一致的挑战。

Abstract: Multi-fidelity Bayesian optimization (MFBO) accelerates the search for the global optimum of black-box functions by integrating inexpensive, low-fidelity approximations. The central task of an MFBO policy is to balance the cost-efficiency of low-fidelity proxies against their reduced accuracy to ensure effective progression toward the high-fidelity optimum. Existing MFBO methods primarily capture associational dependencies between inputs, fidelities, and objectives, rather than causal mechanisms, and can perform poorly when lower-fidelity proxies are poorly aligned with the target fidelity. We propose RESCUE (REducing Sampling cost with Causal Understanding and Estimation), a multi-objective MFBO method that incorporates causal calculus to systematically address this challenge. RESCUE learns a structural causal model capturing causal relationships between inputs, fidelities, and objectives, and uses it to construct a probabilistic multi-fidelity (MF) surrogate that encodes intervention effects. Exploiting the causal structure, we introduce a causal hypervolume knowledge-gradient acquisition strategy to select input-fidelity pairs that balance expected multi-objective improvement and cost. We show that RESCUE improves sample efficiency over state-of-the-art MF optimization methods on synthetic and real-world problems in robotics, machine learning (AutoML), and healthcare.

</details>


### [398] [Sporadic Gradient Tracking over Directed Graphs: A Theoretical Perspective on Decentralized Federated Learning](https://arxiv.org/abs/2602.00791)
*Shahryar Zehtabi,Dong-Jun Han,Seyyedali Hosseinalipour,Christopher Brinton*

Main category: cs.LG

TL;DR: 本文提出Sporadic Gradient Tracking (Spod - GT)算法，统一了分散式联邦学习（DFL）中缓解数据异质性和考虑客户端资源可用性的两个工作分支，进行收敛分析并通过实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 统一DFL中分别解决重要挑战的两个工作分支，即缓解数据异质性的梯度跟踪技术和考虑客户端资源可用性。

Method: 提出Spod - GT算法，允许客户端特定的梯度计算频率和异构、不对称的通信频率，在对梯度估计方差和客户端梯度多样性的假设放宽的情况下进行收敛分析。

Result: 通过图像分类数据集的数值实验，证明Spod - GT与知名的梯度跟踪基线相比更有效。

Conclusion: Spod - GT算法在一般有向图上有效，能在客户端间歇性参与的情况下为梯度跟踪提供共识和最优性保证。

Abstract: Decentralized Federated Learning (DFL) enables clients with local data to collaborate in a peer-to-peer manner to train a generalized model. In this paper, we unify two branches of work that have separately solved important challenges in DFL: (i) gradient tracking techniques for mitigating data heterogeneity and (ii) accounting for diverse availability of resources across clients. We propose $\textit{Sporadic Gradient Tracking}$ ($\texttt{Spod-GT}$), the first DFL algorithm that incorporates these factors over general directed graphs by allowing (i) client-specific gradient computation frequencies and (ii) heterogeneous and asymmetric communication frequencies. We conduct a rigorous convergence analysis of our methodology with relaxed assumptions on gradient estimation variance and gradient diversity of clients, providing consensus and optimality guarantees for GT over directed graphs despite intermittent client participation. Through numerical experiments on image classification datasets, we demonstrate the efficacy of $\texttt{Spod-GT}$ compared to well-known GT baselines.

</details>


### [399] [Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion](https://arxiv.org/abs/2602.00792)
*Guinan Chen,Xunpeng Huang,Ying Sun,Shijin Wang,Yanyong Zhang,Chao Wang*

Main category: cs.LG

TL;DR: 提出显式掩码扩散对偶性和掩码一致性蒸馏（MCD）框架，实现推理加速且不降低生成质量。


<details>
  <summary>Details</summary>
Motivation: 掩码离散扩散推理效率受限于缺乏确定性采样工具，现有确定性蒸馏方法在掩码模型中表现不佳，需弥合差距。

Method: 建立显式掩码扩散对偶性，引入MCD框架，利用对偶性解析构建一致性蒸馏所需的确定性耦合轨迹。

Result: 严格改进了先前的随机蒸馏方法，实现16倍推理加速且不降低生成质量。

Conclusion: 为掩码和连续扩散建立理论基础，释放一致性蒸馏在高性能离散生成中的潜力。

Abstract: Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform models, these approaches generally underperform masked models and rely on complex integral operators. Conversely, in the masked domain, prior methods typically assume the absence of deterministic trajectories, forcing a reliance on stochastic distillation. To bridge this gap, we establish explicit Masked Diffusion Duality, proving that the masked process arises as the projection of a continuous Gaussian process via a novel maximum-value index preservation mechanism. Furthermore, we introduce Masked Consistency Distillation (MCD), a principled framework that leverages this duality to analytically construct the deterministic coupled trajectories required for consistency distillation, bypassing numerical ODE solvers. This result strictly improves upon prior stochastic distillation methods, achieving a 16$\times$ inference speedup without compromising generation quality. Our findings not only provide a solid theoretical foundation connecting masked and continuous diffusion, but also unlock the full potential of consistency distillation for high-performance discrete generation. Our code is available at https://anonymous.4open.science/r/MCD-70FD.

</details>


### [400] [JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation](https://arxiv.org/abs/2602.00800)
*Yebin Yang,Huaijin Wu,Fu Guo,Lin Yao,Xiaohan Qin,Jingzhi Wang,Debing Zhang,Junchi Yan*

Main category: cs.LG

TL;DR: 提出基于token-indexed parameters的新缩放轴，含JTok和JTok - M方法，实验显示能降低验证损失、提升下游任务性能，还能优化质量 - 计算权衡。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型沿密集维度扩展时性能与计算成本近似线性增长，MoE虽解耦容量与计算但有内存开销和硬件效率问题，需新方法解耦模型容量与FLOPs。

Method: 提出token - indexed parameters作为新缩放轴，引入JTok和JTok - M，用从辅助嵌入表中检索的调制向量增强Transformer层，通过轻量级逐元素操作调制主干。

Result: 在不同规模的密集和MoE骨干网络上实验表明，该方法能降低验证损失，显著提升下游任务表现，JTok - M能改善质量 - 计算权衡，token - indexed parameters有可预测幂律缩放行为。

Conclusion: 提出的token - indexed parameters方法有效，能在不显著增加开销前提下解耦模型容量与FLOPs，提升模型性能。

Abstract: LLMs have traditionally scaled along dense dimensions, where performance is coupled with near-linear increases in computational cost. While MoE decouples capacity from compute, it introduces large memory overhead and hardware efficiency challenges. To overcome these, we propose token-indexed parameters as a novel, orthogonal scaling axis that decouple model capacity from FLOPs. Specifically, we introduce Joint-Token (JTok) and Mixture of Joint-Token (JTok-M), which augment Transformer layers with modulation vectors retrieved from auxiliary embedding tables. These vectors modulate the backbone via lightweight, element-wise operations, incurring negligible FLOPs overhead. Extensive experiments on both dense and MoE backbones, spanning from 650M (190M + 460M embedding) to 61B (17B + 44B embedding) total parameters, demonstrate that our approach consistently reduces validation loss and significantly improves downstream task performance (e.g., +4.1 on MMLU, +8.3 on ARC, +8.9 on CEval). Rigorous isoFLOPs analysis further confirms that JTok-M fundamentally shifts the quality-compute Pareto frontier, achieving comparable model quality with 35% less compute relative to vanilla MoE architectures, and we validate that token-indexed parameters exhibit a predictable power-law scaling behavior. Moreover, our efficient implementation ensures that the overhead introduced by JTok and JTok-M remains marginal.

</details>


### [401] [Mobile Exergames: Activity Recognition Based on Smartphone Sensors](https://arxiv.org/abs/2602.00809)
*David Craveiro,Hugo Silva*

Main category: cs.LG

TL;DR: 本文提出2D无尽游戏Duck Catch & Fit，实现活动识别系统，结合语音识别，结果显示可用机器学习识别活动，两种识别结合提升游戏体验。


<details>
  <summary>Details</summary>
Motivation: 利用智能手机传感器提供的信息，将人类活动识别用于游戏，提升游戏体验。

Method: 在游戏中使用智能手机的加速度计、陀螺仪和磁力计传感器，应用特征提取和学习机制检测人类活动，结合语音识别系统识别“fire”。

Result: 可以使用机器学习技术以高识别率识别人类活动，运动和语音识别结合带来更沉浸的游戏体验。

Conclusion: 机器学习技术可用于人类活动识别，运动与语音识别结合能增强游戏沉浸感。

Abstract: Smartphone sensors can be extremely useful in providing information on the activities and behaviors of persons. Human activity recognition is increasingly used for games, medical, or surveillance. In this paper, we propose a proof-of-concept 2D endless game called Duck Catch & Fit, which implements a detailed activity recognition system that uses a smartphone accelerometer, gyroscope, and magnetometer sensors. The system applies feature extraction and learning mechanism to detect human activities like staying, side movements, and fake side movements. In addition, a voice recognition system is combined to recognize the word "fire" and raise the game's complexity. The results show that it is possible to use machine learning techniques to recognize human activity with high recognition levels. Also, the combination of movement-based and voice-based integrations contributes to a more immersive gameplay.

</details>


### [402] [RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation](https://arxiv.org/abs/2602.00849)
*Yuhao Huang,Shih-Hsin Wang,Andrea L. Bertozzi,Bao Wang*

Main category: cs.LG

TL;DR: 提出RMFlow模型解决MeanFlow单功能评估生成效果不佳问题，仅用1 - NFE就取得接近最优结果，计算成本与基线相近。


<details>
  <summary>Details</summary>
Motivation: MeanFlow的单功能评估（1 - NFE）生成结果不理想。

Method: 引入RMFlow，结合粗粒度1 - NFE MeanFlow传输与定制的噪声注入细化步骤，用新损失函数训练神经网络近似流路径的平均速度。

Result: 在文本到图像、上下文到分子和时间序列生成上仅用1 - NFE就取得接近最优的结果，计算成本与基线MeanFlows相当。

Conclusion: RMFlow能有效解决MeanFlow 1 - NFE生成效果不佳问题，且在多个生成任务中有良好表现和成本优势。

Abstract: Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at a computational cost comparable to the baseline MeanFlows.

</details>


### [403] [Investigating the Robustness of Subtask Distillation under Spurious Correlation](https://arxiv.org/abs/2602.00852)
*Pattarawat Chormai,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 评估蒸馏方法在含虚假相关性数据上的表现，指出知识蒸馏在不完美真实数据集上的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏方法依赖的数据集可能存在规模有限、缺乏代表性或有虚假相关性等问题。

Method: 评估既定蒸馏方法和SubDistill方法在含虚假相关性数据上的表现。

Result: 随着相关性增强，先进方法如SubDistill保持稳健，一些基线方法性能降至接近随机。

Conclusion: 知识蒸馏应用于不完美、含虚假相关性的真实数据集时存在挑战。

Abstract: Subtask distillation is an emerging paradigm in which compact, specialized models are extracted from large, general-purpose 'foundation models' for deployment in environments with limited resources or in standalone computer systems. Although distillation uses a teacher model, it still relies on a dataset that is often limited in size and may lack representativeness or exhibit spurious correlations. In this paper, we evaluate established distillation methods, as well as the recent SubDistill method, when using data with spurious correlations for distillation. As the strength of the correlations increases, we observe a widening gap between advanced methods, such as SubDistill, which remain fairly robust, and some baseline methods, which degrade to near-random performance. Overall, our study underscores the challenges of knowledge distillation when applied to imperfect, real-world datasets, particularly those with spurious correlations.

</details>


### [404] [Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs](https://arxiv.org/abs/2602.00862)
*Shih-Hsin Wang,Yuhao Huang,Taos Transue,Justin Baker,Jonathan Forstater,Thomas Strohmer,Bao Wang*

Main category: cs.LG

TL;DR: 提出针对蛋白质的高效多尺度图学习框架，理论证明其保留最大表达能力，实验显示能提升预测精度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的方法在学习多尺度表示和高效建模长程依赖方面存在挑战。

Method: 构建包含细粒度子图和粗粒度图的分层图表示，使用两个GNN进行特征学习，框架各阶段可灵活选择GNN。

Result: 将基线GNN集成到多尺度框架中，在多个基准测试中显著提高预测准确性并降低计算成本。

Conclusion: 所提出的多尺度图学习框架能有效学习蛋白质结构，理论上保留重要信息，实践中效果良好。

Abstract: Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In this work, we propose an efficient multiscale graph-based learning framework tailored to proteins. Our proposed framework contains two crucial components: (1) It constructs a hierarchical graph representation comprising a collection of fine-grained subgraphs, each corresponding to a secondary structure motif (e.g., $α$-helices, $β$-strands, loops), and a single coarse-grained graph that connects these motifs based on their spatial arrangement and relative orientation. (2) It employs two GNNs for feature learning: the first operates within individual secondary motifs to capture local interactions, and the second models higher-level structural relationships across motifs. Our modular framework allows a flexible choice of GNN in each stage. Theoretically, we show that our hierarchical framework preserves the desired maximal expressiveness, ensuring no loss of critical structural information. Empirically, we demonstrate that integrating baseline GNNs into our multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.

</details>


### [405] [Improving Flow Matching by Aligning Flow Divergence](https://arxiv.org/abs/2602.00869)
*Yuhao Huang,Taos Transue,Shih-Hsin Wang,William Feldman,Hong Zhang,Bao Wang*

Main category: cs.LG

TL;DR: 本文指出条件流匹配（CFM）在学习概率路径准确性上的不足，引入新的偏微分方程表征误差并求解，设计新目标函数提升基于流的生成模型性能，且在多个基准任务上展示优势。


<details>
  <summary>Details</summary>
Motivation: 现有条件流匹配（CFM）方法在确保学习概率路径的准确性方面存在不足。

Method: 引入新的偏微分方程刻画学习与精确概率路径间的误差并求解，设计同时匹配流及其散度的新目标函数。

Result: 新方法显著提升基于流的生成模型性能且不牺牲生成效率，在多个重要基准任务上优于CFM。

Conclusion: 新的训练方法在保证生成效率的同时，能有效提高基于流的生成模型性能，具有实际应用价值。

Abstract: Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.

</details>


### [406] [Learning Heat-based Equations in Self-similar variables](https://arxiv.org/abs/2602.00872)
*Shihao Wang,Qipeng Qian,Jingquan Wang*

Main category: cs.LG

TL;DR: 研究基于热的方程在自相似变量(SSV)中的解学习，开发SSV训练框架，对比不同系统和架构下的模型，发现SSV训练网络在预测准确性和稳定性上更优。


<details>
  <summary>Details</summary>
Motivation: 探索基于热的方程在自相似变量中的解学习，提高方程长时间动态预测的准确性和稳定性。

Method: 开发与标准神经算子训练兼容的SSV训练框架，在二维不可压缩Navier - Stokes方程和一维粘性Burgers方程上实例化该框架，并使用两种简单全连接架构在物理坐标和自相似坐标下训练模型并进行对比。

Result: 跨两个系统和两种架构，SSV训练的网络在训练窗口外的外推更准确、稳定，能更好捕捉长时间定性趋势。

Conclusion: 自相似坐标为学习基于热的方程的长时间动态提供了数学上可行的归纳偏差。

Abstract: We study solution learning for heat-based equations in self-similar variables (SSV). We develop an SSV training framework compatible with standard neural-operator training. We instantiate this framework on the two-dimensional incompressible Navier-Stokes equations and the one-dimensional viscous Burgers equation, and perform controlled comparisons between models trained in physical coordinates and in the corresponding self-similar coordinates using two simple fully connected architectures (standard multilayer perceptrons and a factorized fully connected network). Across both systems and both architectures, SSV-trained networks consistently deliver substantially more accurate and stable extrapolation beyond the training window and better capture qualitative long-time trends. These results suggest that self-similar coordinates provide a mathematically motivated inductive bias for learning the long-time dynamics of heat-based equations.

</details>


### [407] [Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs](https://arxiv.org/abs/2602.00879)
*Hao Mark Chen,Zhiwen Mo,Royson Lee,Qianzhou Wang,Da Li,Shell Xu Hu,Wayne Luk,Timothy Hospedales,Hongxiang Fan*

Main category: cs.LG

TL;DR: 提出动态专家共享（DES）技术解决扩散大语言模型（dLLMs）与混合专家（MoE）架构集成时的专家爆炸问题，实验表明该技术减少独特专家激活和延迟，保留高准确率。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型与混合专家架构集成时存在专家爆炸问题，导致大量内存流量，抵消了MoE和并行解码的效率提升。

Method: 提出动态专家共享（DES）技术，将MoE优化从以令牌为中心的修剪和传统专家跳过方法转变为序列级核心集选择，引入序列内共享（DES - Seq）和显著性感知投票（DES - Vote）两种选择策略。

Result: DES减少独特专家激活超过55%，减少延迟达38%，同时保留99%的原始准确率。

Conclusion: DES技术有效将内存开销与并行度解耦。

Abstract: Among parallel decoding paradigms, diffusion large language models (dLLMs) have emerged as a promising candidate that balances generation quality and throughput. However, their integration with Mixture-of-Experts (MoE) architectures is constrained by an expert explosion: as the number of tokens generated in parallel increases, the number of distinct experts activated grows nearly linearly. This results in substantial memory traffic that pushes inference into a memory-bound regime, negating the efficiency gains of both MoE and parallel decoding. To address this challenge, we propose Dynamic Expert Sharing (DES), a novel technique that shifts MoE optimization from token-centric pruning and conventional expert skipping methods to sequence-level coreset selection. To maximize expert reuse, DES identifies a compact, high-utility set of experts to satisfy the requirements of an entire parallel decoding block. We introduce two innovative selection strategies: (1) Intra-Sequence Sharing (DES-Seq), which adapts optimal allocation to the sequence level, and (2) Saliency-Aware Voting (DES-Vote), a novel mechanism that allows tokens to collectively elect a coreset based on aggregated router weights. Extensive experiments on MoE dLLMs demonstrate that DES reduces unique expert activations by over 55% and latency by up to 38%, while retaining 99% of vanilla accuracy, effectively decoupling memory overhead from the degree of parallelism.

</details>


### [408] [Test-time Generalization for Physics through Neural Operator Splitting](https://arxiv.org/abs/2602.00884)
*Louis Serrano,Jiequn Han,Edouard Oyallon,Shirley Ho,Rudy Morel*

Main category: cs.LG

TL;DR: 本文提出一种在测试时增强神经算子泛化能力的方法，无需修改预训练权重，在挑战性任务上取得零样本泛化的最佳成果。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在测试输入超出训练分布时泛化能力不足，先前方法无法实现真正的零样本泛化。

Method: 基于DISCO，引入神经算子拆分策略，在测试时搜索训练算子的组合来近似未见动态。

Result: 在包括参数外推和新物理现象组合的挑战性分布外任务上取得了零样本泛化的最佳成果，且能恢复潜在的PDE参数。

Conclusion: 测试时计算是构建灵活、可组合和可泛化的神经算子的关键途径。

Abstract: Neural operators have shown promise in learning solution maps of partial differential equations (PDEs), but they often struggle to generalize when test inputs lie outside the training distribution, such as novel initial conditions, unseen PDE coefficients or unseen physics. Prior works address this limitation with large-scale multiple physics pretraining followed by fine-tuning, but this still requires examples from the new dynamics, falling short of true zero-shot generalization. In this work, we propose a method to enhance generalization at test time, i.e., without modifying pretrained weights. Building on DISCO, which provides a dictionary of neural operators trained across different dynamics, we introduce a neural operator splitting strategy that, at test time, searches over compositions of training operators to approximate unseen dynamics. On challenging out-of-distribution tasks including parameter extrapolation and novel combinations of physics phenomena, our approach achieves state-of-the-art zero-shot generalization results, while being able to recover the underlying PDE parameters. These results underscore test-time computation as a key avenue for building flexible, compositional, and generalizable neural operators.

</details>


### [409] [Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version)](https://arxiv.org/abs/2211.11434)
*Lucas Lange,Maja Schneider,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: 本文创建满足差分隐私的机器学习模型用于新冠图像分析，评估效用 - 隐私权衡，发现差分隐私对实用防御的影响有限，指出了更好权衡的可能性。


<details>
  <summary>Details</summary>
Motivation: 在进行数据分析时保护患者隐私，解决以往研究在数据集大小、隐私保证和实用隐私方面的不足。

Method: 创建满足差分隐私的机器学习模型，考虑类不平衡，通过黑盒成员推理攻击评估效用 - 隐私权衡。

Result: 所需隐私水平可能因任务而异，增加差分隐私保证对隐私泄露改善有限。

Conclusion: 识别了更好的效用 - 隐私权衡可能性，实证攻击特定隐私估计对调整实用隐私至关重要。

Abstract: Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat from MIAs. The results further suggest that with increasing DP guarantees, empirical privacy leakage only improves marginally, and DP therefore appears to have a limited impact on practical MIA defense. Our findings identify possibilities for better utility-privacy trade-offs, and we believe that empirical attack-specific privacy estimation can play a vital role in tuning for practical privacy.

</details>


### [410] [Reliability-Aware Determinantal Point Processes for Robust Informative Data Selection in Large Language Models](https://arxiv.org/abs/2602.00885)
*Ahmad Sarlak,Abolfazl Razi*

Main category: cs.LG

TL;DR: 介绍了一种新的数据选择方案ProbDPP，可解决可靠性问题，还提出UCB算法并给出理论分析。


<details>
  <summary>Details</summary>
Motivation: 传统子集选择方法难以在部分存储中断、通信不完善和随机访问失败等情况下使用，需要解决这个数据选择可靠性的问题。

Method: 引入新的ProbDPP，将目标函数改造加入正则化项，将可靠性感知的多样性最大化问题构建为组合半老虎机问题并提出UCB算法。

Result: 新方法能在不确定情况下鲁棒选择多样的数据批次，理论分析给出了所提方法的遗憾边界。

Conclusion: 所提方案可以解决数据选择可靠性问题，具有性能保证。

Abstract: Informative data selection is a key requirement for large language models (LLMs) to minimize the amount of data required for fine-tuning, network distillation, and token pruning, enabling fast and efficient deployment, especially under computational and communication constraints. Traditional subset selection methods, including those based on Determinantal Point Processes (DPP), focus on maximizing diversity but assume that selected data batches are always available error-free. This presumption prohibits their use under partial storage outage, imperfect communication, and stochastic access failures. Furthermore, we show that the original formulation collapses under such conditions. To address this gap, we introduce ProbDPP, a novel reliability-aware implementation of k-DPP that accounts for probabilistic data access by recasting the objective function with a regularization term that remains well-posed and decomposes into a geometric diversity term and unreliability cost. The resulting objective facilitates robust selection of diverse data batches under uncertainty. Furthermore, we frame this reliability-aware diversity maximization as a combinatorial semi-bandit problem and propose a UCB-style algorithm to efficiently learn the unknown reliability online. Theoretical analysis provides regret bounds for the proposed approach, ensuring performance guarantees.

</details>


### [411] [Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection](https://arxiv.org/abs/2401.13327)
*Lucas Lange,Nils Wenzlitschke,Erhard Rahm*

Main category: cs.LG

TL;DR: 本文提出用GANs和DP保护隐私合成智能手表健康读数用于压力检测，测试证明能提升模型性能，强调差分隐私合成数据在平衡效用 - 隐私上的潜力。


<details>
  <summary>Details</summary>
Motivation: 智能手表健康数据含敏感信息且获取难，需保护隐私并提高数据可用性用于研究。

Method: 采用GANs和DP保护隐私合成与压力时刻相关的多传感器智能手表健康读数，在实际压力检测任务中测试合成数据并采用不同数据增强策略。

Result: 基于GAN的增强方法显著提升模型性能，私有DP训练场景F1分数提高11.90 - 15.48%，非私有训练场景提高0.45%。

Conclusion: 差分隐私合成数据在优化效用 - 隐私权衡上有潜力，但增加隐私要求会显著影响合成数据质量。

Abstract: Smartwatch health sensor data are increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprise sensitive personal information and are resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress, employing Generative Adversarial Networks (GANs) and Differential Privacy (DP) safeguards. Our method not only protects patient information but also enhances data availability for research. To ensure its usefulness, we test synthetic data from multiple GANs and employ different data enhancement strategies on an actual stress detection task. Our GAN-based augmentation methods demonstrate significant improvements in model performance, with private DP training scenarios observing an 11.90-15.48% increase in F1-score, while non-private training scenarios still see a 0.45% boost. These results underline the potential of differentially private synthetic data in optimizing utility-privacy trade-offs, especially with the limited availability of real training samples. Through rigorous quality assessments, we confirm the integrity and plausibility of our synthetic data, which, however, are significantly impacted when increasing privacy requirements.

</details>


### [412] [GAPNet: Plug-in Jointly Learning Task-Specific Graph for Dynamic Stock Relation](https://arxiv.org/abs/2602.00888)
*Yingjie Niu,Lanxin Lu,Changhong Jin,Ruihai Dong*

Main category: cs.LG

TL;DR: 本文提出GAPNet解决现有股票预测方法普适性差问题，通过跨两数据集验证效果良好，表明联合学习图结构和表示对特定任务关系建模至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有股票预测方法依赖先验关系，而股票相关网络信号有噪声、异步且难获取，导致普适性差和预定义图与下游任务不匹配。

Method: 提出GAPNet，一个图自适应插件网络，通过空间感知层和时间感知层动态调整边拓扑，可附加到现有图骨干模型上。

Result: 在两个真实股票数据集上，GAPNet比现有模型提高了盈利能力和稳定性，RT - GCN年化累计回报达0.47，CI - STHPAN达0.63，夏普比率峰值分别为2.20和2.12。

Conclusion: 联合学习图结构和表示对特定任务关系建模是必不可少的。

Abstract: The advent of the web has led to a paradigm shift in the financial relations, with the real-time dissemination of news, social discourse, and financial filings contributing significantly to the reshaping of financial forecasting. The existing methods rely on establishing relations a priori, i.e. predefining graphs to capture inter-stock relationships. However, the stock-related web signals are characterised by high levels of noise, asynchrony, and challenging to obtain, resulting in poor generalisability and non-alignment between the predefined graphs and the downstream tasks. To address this, we propose GAPNet, a Graph Adaptation Plug-in Network that jointly learns task-specific topology and representations in an end-to-end manner. GAPNet attaches to existing pairwise graph or hypergraph backbone models, enabling the dynamic adaptation and rewiring of edge topologies via two complementary components: a Spatial Perception Layer that captures short-term co-movements across assets, and a Temporal Perception Layer that maintains long-term dependency under distribution shift. Across two real-world stock datasets, GAPNet has been shown to consistently enhance the profitability and stability in comparision to the state-of-the-art models, yielding annualised cumulative returns of up to 0.47 for RT-GCN and 0.63 for CI-STHPAN, with peak Sharpe Ratio of 2.20 and 2.12 respectively. The plug-and-play design of GAPNet ensures its broad applicability to diverse GNN-based architectures. Our results underscore that jointly learning graph structures and representations is essential for task-specific relational modeling.

</details>


### [413] [Federated Learning With Individualized Privacy Through Client Sampling](https://arxiv.org/abs/2501.17634)
*Lucas Lange,Ole Borchardt,Erhard Rahm*

Main category: cs.LG

TL;DR: 本文提出在联邦学习中实现个性化差分隐私的适配方法，经实验验证该方法优于统一DP基线及相关替代方法，但在处理非独立同分布数据的复杂任务时仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 随着对用户数据收集的担忧增加，个性化隐私可通过考虑用户不同隐私偏好平衡数据保护与效用，因此在联邦学习中探索实现个性化差分隐私的方法。

Method: 将中心化设置下的SAMPLE算法扩展到联邦学习，根据客户端不同的隐私预算计算特定采样率，并集成到改进的IDP - FedAvg算法中，在现实隐私分布和多数据集上测试。

Result: 所提出的方法相比统一DP基线有明显改进，减少了隐私与效用的权衡；相比相关工作中的SCALE方法表现更优。

Conclusion: 提出的方法有效，但在处理非独立同分布数据的复杂任务时，因分散设置的限制仍存在挑战。

Abstract: With growing concerns about user data collection, individualized privacy has emerged as a promising solution to balance protection and utility by accounting for diverse user privacy preferences. Instead of enforcing a uniform level of anonymization for all users, this approach allows individuals to choose privacy settings that align with their comfort levels. Building on this idea, we propose an adapted method for enabling Individualized Differential Privacy (IDP) in Federated Learning (FL) by handling clients according to their personal privacy preferences. By extending the SAMPLE algorithm from centralized settings to FL, we calculate client-specific sampling rates based on their heterogeneous privacy budgets and integrate them into a modified IDP-FedAvg algorithm. We test this method under realistic privacy distributions and multiple datasets. The experimental results demonstrate that our approach achieves clear improvements over uniform DP baselines, reducing the trade-off between privacy and utility. Compared to the alternative SCALE method in related work, which assigns differing noise scales to clients, our method performs notably better. However, challenges remain for complex tasks with non-i.i.d. data, primarily stemming from the constraints of the decentralized setting.

</details>


### [414] [Reinforcement Learning via Conservative Agent for Environments with Random Delays](https://arxiv.org/abs/2507.18992)
*Jongsoo Lee,Jangwon Kim,Jiseok Jeong,Soohee Han*

Main category: cs.LG

TL;DR: 提出保守代理处理随机延迟环境，转换为等效恒定延迟环境，实验表现优于基线算法


<details>
  <summary>Details</summary>
Motivation: 现实强化学习中随机延迟环境因可变和不可预测性尚无有效处理方法，存在挑战

Method: 提出保守代理，将随机延迟环境转换为恒定延迟等效环境，使现有恒定延迟方法可直接应用

Result: 在连续控制任务评估中，该算法在渐近性能和样本效率上显著优于现有基线算法

Conclusion: 提出的方法简单且稳健，能有效解决随机延迟环境下的决策问题

Abstract: Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.

</details>


### [415] [PyGALAX: An Open-Source Python Toolkit for Advanced Explainable Geospatial Machine Learning](https://arxiv.org/abs/2602.00907)
*Pingping Wang,Yihong Yuan,Lingcheng Li,Yongmei Lu*

Main category: cs.LG

TL;DR: PyGALAX 是用于地理空间分析的 Python 包，集成 AutoML 和 XAI 技术，改进 GALAX 框架，能解决空间非平稳性并使高级地理空间机器学习方法易于使用。


<details>
  <summary>Details</summary>
Motivation: 让地理空间分析能集成自动化机器学习和可解释人工智能技术，处理空间异质性，且方法更灵活、易用和可复现。

Method: 集成自动化机器学习和可解释人工智能技术，自动选择和优化机器学习模型，用 SHAP 分析保持可解释性，改进 GALAX 框架，提供自动带宽和灵活核函数选择。

Result: PyGALAX 有效处理空间非平稳性，能在全局和局部尺度上洞察复杂空间关系，还打包成易于使用的 Python 工具包。

Conclusion: PyGALAX 使高级地理空间机器学习方法可被各相关领域研究者和从业者使用。

Abstract: PyGALAX is a Python package for geospatial analysis that integrates automated machine learning (AutoML) and explainable artificial intelligence (XAI) techniques to analyze spatial heterogeneity in both regression and classification tasks. It automatically selects and optimizes machine learning models for different geographic locations and contexts while maintaining interpretability through SHAP (SHapley Additive exPlanations) analysis. PyGALAX builds upon and improves the GALAX framework (Geospatial Analysis Leveraging AutoML and eXplainable AI), which has proven to outperform traditional geographically weighted regression (GWR) methods. Critical enhancements in PyGALAX from the original GALAX framework include automatic bandwidth selection and flexible kernel function selection, providing greater flexibility and robustness for spatial modeling across diverse datasets and research questions. PyGALAX not only inherits all the functionalities of the original GALAX framework but also packages them into an accessible, reproducible, and easily deployable Python toolkit while providing additional options for spatial modeling. It effectively addresses spatial non-stationarity and generates transparent insights into complex spatial relationships at both global and local scales, making advanced geospatial machine learning methods accessible to researchers and practitioners in geography, urban planning, environmental science, and related fields.

</details>


### [416] [Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment](https://arxiv.org/abs/2602.00910)
*Cuong Manh Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: 本文综述适用于医学领域的高效轻量级深度学习架构并探讨模型压缩策略，为资源受限临床环境提供指引。


<details>
  <summary>Details</summary>
Motivation: 大规模模型在临床部署面临计算成本高、延迟限制和数据隐私等问题。

Method: 将现代高效模型分为CNN、轻量级Transformer和新兴线性复杂度模型三类，研究模型压缩策略。

Result: 研究了不同模型类型和压缩策略在保持诊断性能和降低硬件需求方面的效果。

Conclusion: 本综述为缩小高性能AI与资源受限临床环境差距提供了路线图。

Abstract: Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.

</details>


### [417] [Early Classification of Time Series in Non-Stationary Cost Regimes](https://arxiv.org/abs/2602.00918)
*Aurélien Renault,Alexis Bondu,Antoine Cornuéjols,Vincent Lemaire*

Main category: cs.LG

TL;DR: 研究时间序列早期分类（ECTS）在成本非平稳情况下的问题，提出在线学习适应方法，实验表明在线学习可提升ECTS方法对成本漂移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有ECTS方法假设决策成本已知、固定且正确指定，但实际中成本不确定且会随时间变化，导致训练和部署目标不匹配。

Method: 重新审视代表性ECTS方法，将其适应在线学习场景，聚焦可分离方法，部署时仅更新触发模型，提出多种在线适应方法和基线，包括基于多臂老虎机和强化学习的方法。

Result: 在线学习能有效提高ECTS方法对成本漂移的鲁棒性，基于强化学习的策略在不同成本制度下表现出强大且稳定的性能。

Conclusion: 在线学习是提高ECTS方法在成本非平稳情况下鲁棒性的有效途径。

Abstract: Early Classification of Time Series (ECTS) addresses decision-making problems in which predictions must be made as early as possible while maintaining high accuracy. Most existing ECTS methods assume that the time-dependent decision costs governing the learning objective are known, fixed, and correctly specified. In practice, however, these costs are often uncertain and may change over time, leading to mismatches between training-time and deployment-time objectives. In this paper, we study ECTS under two practically relevant forms of cost non-stationarity: drift in the balance between misclassification and decision delay costs, and stochastic realizations of decision costs that deviate from the nominal training-time model. To address these challenges, we revisit representative ECTS approaches and adapt them to an online learning setting. Focusing on separable methods, we update only the triggering model during deployment, while keeping the classifier fixed. We propose several online adaptations and baselines, including bandit-based and RL-based approaches, and conduct controlled experiments on synthetic data to systematically evaluate robustness under cost non-stationarity. Our results demonstrate that online learning can effectively improve the robustness of ECTS methods to cost drift, with RL-based strategies exhibiting strong and stable performance across varying cost regimes.

</details>


### [418] [Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision](https://arxiv.org/abs/2602.00927)
*Yihao Xue,Allan Zhang,Jianhao Huang,Amit Sahai,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 研究发现仅结果监督下，增加训练推理长度能在分布内性能饱和后继续提升分布外性能，并给出理论解释和实证。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型进行更长推理是构建解决复杂问题模型的关键，当前有多种实现方式，研究推理长度对分布外性能的影响。

Method: 理论分析两个机制，并用循环Transformer增加循环次数和RL微调时增加令牌预算两个实例提供实证。

Result: 发现仅结果监督下，训练推理长度增加能在分布内性能饱和后提升分布外性能，理论解释获实证支持。

Conclusion: 鲁棒性可能需要比分布内验证单独所指示的更大预算，增加推理长度有益分布外泛化。

Abstract: Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.

</details>


### [419] [Continuous-Utility Direct Preference Optimization](https://arxiv.org/abs/2602.00931)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zihao He,Muhammad Usman Rafique,Asad Aali,Muhammad Ali Jamshed,John M. Cioffi,Emily Fox*

Main category: cs.LG

TL;DR: 提出CU - DPO框架，以连续分数替代二元标签来对齐模型与认知策略，通过两阶段训练提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理依赖二元偏好监督，无法捕捉部分进展和细粒度推理质量。

Method: 引入CU - DPO框架，用连续分数替代二元标签；提出两阶段训练管道：策略选择和执行细化。

Result: 在数学推理基准上，将七个基础模型的策略选择准确率从35 - 46%提高到68 - 78%，在分布内数据集上推理得分提升最多6.6分，且能有效迁移到分布外任务。

Conclusion: CU - DPO框架能有效提升大语言模型的推理能力。

Abstract: Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a portfolio of prompt-based cognitive strategies by replacing binary labels with continuous scores that capture fine-grained reasoning quality. We prove that learning with K strategies yields a Theta(K log K) improvement in sample complexity over binary preferences, and that DPO converges to the entropy-regularized utility-maximizing policy. To exploit this signal, we propose a two-stage training pipeline: (i) strategy selection, which optimizes the model to choose the best strategy for a given problem via best-vs-all comparisons, and (ii) execution refinement, which trains the model to correctly execute the selected strategy using margin-stratified pairs. On mathematical reasoning benchmarks, CU-DPO improves strategy selection accuracy from 35-46 percent to 68-78 percent across seven base models, yielding consistent downstream reasoning gains of up to 6.6 points on in-distribution datasets with effective transfer to out-of-distribution tasks.

</details>


### [420] [SALAAD: Sparse And Low-Rank Adaptation via ADMM](https://arxiv.org/abs/2602.00942)
*Hao Ma,Melis Ilayda Bal,Liang Zhang,Bingcong Li,Niao He,Melanie Zeilinger,Michael Muehlebach*

Main category: cs.LG

TL;DR: 提出SALAAD框架在计算和内存受限下灵活控制模型容量，减少部署内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型在计算和内存受限下，现有控制模型容量方法有局限，需灵活控制。

Method: 提出SALAAD框架，在增广拉格朗日框架下进行结构化权重学习，引入自适应控制器平衡训练损失和结构约束。

Result: 实验表明SALAAD大幅降低部署时内存消耗，性能与临时方法相当，单次训练可产生连续模型容量谱。

Conclusion: SALAAD能在训练时显式控制模型有效容量，实现不同内存预算下的灵活部署，无需重新训练。

Abstract: Modern large language models are increasingly deployed under compute and memory constraints, making flexible control of model capacity a central challenge. While sparse and low-rank structures naturally trade off capacity and performance, existing approaches often rely on heuristic designs that ignore layer and matrix heterogeneity or require model-specific architectural modifications. We propose SALAAD, a plug-and-play framework applicable to different model architectures that induces sparse and low-rank structures during training. By formulating structured weight learning under an augmented Lagrangian framework and introducing an adaptive controller that dynamically balances the training loss and structural constraints, SALAAD preserves the stability of standard training dynamics while enabling explicit control over the evolution of effective model capacity during training. Experiments across model scales show that SALAAD substantially reduces memory consumption during deployment while achieving performance comparable to ad-hoc methods. Moreover, a single training run yields a continuous spectrum of model capacities, enabling smooth and elastic deployment across diverse memory budgets without the need for retraining.

</details>


### [421] [Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems](https://arxiv.org/abs/2602.00943)
*Zhenyu Zhao,David Zhang,Ellie Zhao,Ehsan Saberian*

Main category: cs.LG

TL;DR: 提出动态先验汤普森采样方法解决大规模推荐系统冷启动探索问题，实验显示优于统一先验基线。


<details>
  <summary>Details</summary>
Motivation: 大规模推荐系统冷启动探索存在挑战，传统汤普森采样统一先验在真实基础率低时会过度分配流量给弱项目，批处理策略更新和管道延迟会放大影响。

Method: 提出动态先验汤普森采样，通过封闭形式二次解控制新臂战胜现有获胜者的概率。

Result: 通过蒙特卡罗验证、离线批处理模拟和大规模在线实验，动态先验实现精确探索控制并提高效率。

Conclusion: 动态先验汤普森采样在探索控制和效率上优于统一先验基线。

Abstract: Cold-start exploration is a core challenge in large-scale recommender systems: new or data-sparse items must receive traffic to estimate value, but over-exploration harms users and wastes impressions. In practice, Thompson Sampling (TS) is often initialized with a uniform Beta(1,1) prior, implicitly assuming a 50% success rate for unseen items. When true base rates are far lower, this optimistic prior systematically over-allocates to weak items. The impact is amplified by batched policy updates and pipeline latency: for hours, newly launched items can remain effectively "no data," so the prior dominates allocation before feedback is incorporated. We propose Dynamic Prior Thompson Sampling, a prior design that directly controls the probability that a new arm outcompetes the incumbent winner. Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time, making exploration intensity predictable and tunable while preserving TS Bayesian updates. Across Monte Carlo validation, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors deliver precise exploration control and improved efficiency versus a uniform-prior baseline.

</details>


### [422] [Optimal Budgeted Adaptation of Large Language Models](https://arxiv.org/abs/2602.00952)
*Jing Wang,Jie Shen,Dean Foster,Zohar Karnin,Jeremy C Weiss*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\tilde{O}(d\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\tilde{O}(\sqrt{dB} + c\sqrt{B})$ with $B=βT$.

</details>


### [423] [SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery](https://arxiv.org/abs/2602.00953)
*Sahar Almahfouz Nasser,Juan Francisco Pesantez Borja,Jincheng Liu,Tanvir Hasan,Zenghan Wang,Suman Ghosh,Sandeep Manandhar,Shikhar Shiromani,Twisha Shah,Naoto Tokuyama,Anant Madabhushi*

Main category: cs.LG

TL;DR: AI模型难解释阻碍临床应用，本文提出SAGE系统识别可解释病理生物标志物，推动计算病理学临床转化。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型缺乏可解释性阻碍临床应用，且工程化图像生物标志物缺乏系统生物学验证。

Method: 引入SAGE系统，集成文献锚定推理和多模态数据分析，协调专业代理进行生物背景化和假设验证。

Result: 通过SAGE系统可关联图像特征、分子生物标志物及临床相关结果。

Conclusion: SAGE系统能优先选择透明且有生物学支持的生物标志物，推动计算病理学的临床转化。

Abstract: Despite significant progress in computational pathology, many AI models remain black-box and difficult to interpret, posing a major barrier to clinical adoption due to limited transparency and explainability. This has motivated continued interest in engineered image-based biomarkers, which offer greater interpretability but are often proposed based on anecdotal evidence or fragmented prior literature rather than systematic biological validation. We introduce SAGE (Structured Agentic system for hypothesis Generation and Evaluation), an agentic AI system designed to identify interpretable, engineered pathology biomarkers by grounding them in biological evidence. SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers, such as gene expression, and clinically relevant outcomes. By coordinating specialized agents for biological contextualization and empirical hypothesis validation, SAGE prioritizes transparent, biologically supported biomarkers and advances the clinical translation of computational pathology.

</details>


### [424] [From drift to adaptation to the failed ml model: Transfer Learning in Industrial MLOps](https://arxiv.org/abs/2602.00957)
*Waqar Muhammad Ashraf,Talha Ansar,Fahad Ahmed,Jawad Hussain,Muhammad Mujtaba Abbas,Vivek Dua*

Main category: cs.LG

TL;DR: 本文对比三种迁移学习策略更新失效的前馈人工神经网络模型，通过电厂案例分析，得出不同策略在不同批次大小下的表现，为MLOps从业者提供参考。


<details>
  <summary>Details</summary>
Motivation: 当前对数据漂移下更新机器学习模型的系统框架开发关注较少，本文旨在研究迁移学习使能的模型更新策略。

Method: 对比集成迁移学习（ETL）、全层迁移学习（ALTL）和最后一层迁移学习（LLTL）三种策略，以电厂空气预热器烟气压差分析为案例研究。

Result: 对于5天批次大小，ETL预测准确性更高；对于8天批次大小，ALTL适合有效更新模型；不同批次大小下模型更新技术的计算要求呈现混合趋势。

Conclusion: 从基于批处理的工业案例研究中获得的见解，可帮助MLOps从业者使失效模型适应数据漂移，以准确监测工业过程。

Abstract: Model adaptation to production environment is critical for reliable Machine Learning Operations (MLOps), less attention is paid to developing systematic framework for updating the ML models when they fail under data drift. This paper compares the transfer learning enabled model update strategies including ensemble transfer learning (ETL), all-layers transfer learning (ALTL), and last-layer transfer learning (LLTL) for updating the failed feedforward artificial neural network (ANN) model. The flue gas differential pressure across the air preheater unit installed in a 660 MW thermal power plant is analyzed as a case study since it mimics the batch processes due to load cycling in the power plant. Updating the failed ANN model by three transfer learning techniques reveals that ETL provides relatively higher predictive accuracy for the batch size of 5 days than those of LLTL and ALTL. However, ALTL is found to be suitable for effective update of the model trained on large batch size (8 days). A mixed trend is observed for computational requirement (hyperparameter tuning and model training) of model update techniques for different batch sizes. These fundamental and empiric insights obtained from the batch process-based industrial case study can assist the MLOps practitioners in adapting the failed models to data drifts for the accurate monitoring of industrial processes.

</details>


### [425] [Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction](https://arxiv.org/abs/2602.00959)
*Yuheng Yang,Siqi Zhu,Tao Feng,Ge Liu,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出交互式框架系统提取和量化大语言模型知识，实验发现递归分类法最有效，存在知识缩放定律和准确率权衡，不同模型家族知识轮廓有差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准多为静态，对系统探测大语言模型知识支持有限，需明确模型真正包含的知识及边界。

Method: 提出交互式代理框架，含四种自适应探索策略，引入三阶段知识处理流程。

Result: 递归分类法最有效；有明确知识缩放定律；存在Pass@1与Pass@k权衡；不同模型家族因训练数据组成有不同知识轮廓。

Conclusion: 所提框架能有效系统提取和量化大语言模型知识，揭示模型知识相关特性。

Abstract: Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.

</details>


### [426] [On the Spectral Flattening of Quantized Embeddings](https://arxiv.org/abs/2602.00969)
*Junlin Huang,Wenyi Fang,Zhenheng Tang,Yuxin Wang,Xueze Kang,Yang Zheng,Bo Li,Xiaowen Chu*

Main category: cs.LG

TL;DR: 研究指出大语言模型在超低精度训练的不稳定性，证明词嵌入奇异值谱幂律衰减是语义编码必要条件，实验验证几何退化导致表征崩溃，确立谱保真度是稳定低比特优化必要条件。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在超低精度训练中的不稳定性问题。

Method: 通过形式化齐普夫统计与随机矩阵理论的联系进行理论推导，并在GPT - 2和TinyLlama等不同架构上进行实证验证。

Result: 理论上，均匀量化引入噪声会截断谱尾，导致谱扁平化和稳定秩增加；实验上，不同架构的模型中几何退化会导致表征崩溃。

Conclusion: 量化了大语言模型的频谱敏感性，确立谱保真度是稳定低比特优化的必要条件。

Abstract: Training Large Language Models (LLMs) at ultra-low precision is critically impeded by instability rooted in the conflict between discrete quantization constraints and the intrinsic heavy-tailed spectral nature of linguistic data. By formalizing the connection between Zipfian statistics and random matrix theory, we prove that the power-law decay in the singular value spectra of embeddings is a fundamental requisite for semantic encoding. We derive theoretical bounds showing that uniform quantization introduces a noise floor that disproportionately truncates this spectral tail, which induces spectral flattening and a strictly provable increase in the stable rank of representations. Empirical validation across diverse architectures including GPT-2 and TinyLlama corroborates that this geometric degradation precipitates representational collapse. This work not only quantifies the spectral sensitivity of LLMs but also establishes spectral fidelity as a necessary condition for stable low-bit optimization.

</details>


### [427] [Forest-Guided Semantic Transport for Label-Supervised Manifold Alignment](https://arxiv.org/abs/2602.00974)
*Adrien Aumon,Myriam Lizotte,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: 提出FoSTA框架解决现有标签监督流形对齐方法依赖欧氏几何的局限，实验证明其在多方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有标签监督流形对齐方法多依赖欧氏几何，在特征与任务弱相关时会导致结构噪声和对齐质量下降，需改进。

Method: 引入FoSTA框架，利用森林诱导几何去噪域内结构，根据标签信息森林亲和性构建语义表示，并通过快速层次语义传输对齐。

Result: 与基线方法对比，FoSTA在合成基准上提高了对应恢复和标签转移能力，在单细胞应用中有出色表现。

Conclusion: FoSTA能有效解决现有方法局限，在多方面表现良好。

Abstract: Label-supervised manifold alignment bridges the gap between unsupervised and correspondence-based paradigms by leveraging shared label information to align multimodal datasets. Still, most existing methods rely on Euclidean geometry to model intra-domain relationships. This approach can fail when features are only weakly related to the task of interest, leading to noisy, semantically misleading structure and degraded alignment quality. To address this limitation, we introduce FoSTA (Forest-guided Semantic Transport Alignment), a scalable alignment framework that leverages forest-induced geometry to denoise intra-domain structure and recover task-relevant manifolds prior to alignment. FoSTA builds semantic representations directly from label-informed forest affinities and aligns them via fast, hierarchical semantic transport, capturing meaningful cross-domain relationships. Extensive comparisons with established baselines demonstrate that FoSTA improves correspondence recovery and label transfer on synthetic benchmarks and delivers strong performance in practical single-cell applications, including batch correction and biological conservation.

</details>


### [428] [Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees](https://arxiv.org/abs/2602.00987)
*Sawan Kumar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 提出随机小波特征（RWF）框架构建可扩展的非平稳核近似，在多数据集上表现优，兼顾准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有可扩展机器学习方法大多依赖平稳性假设，难以在表达性和计算效率间平衡。

Method: 引入RWF框架，通过从小波族采样构建可扩展非平稳核近似，利用小波固有定位和多分辨率结构生成显式特征图。

Result: 在多种合成和真实数据集上，RWF优于平稳随机特征，与更复杂模型相比有良好的准确性 - 效率权衡。

Conclusion: RWF为处理现实世界非平稳问题提供了可扩展且具表达性的核方法。

Abstract: Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding models like Deep Gaussian Processes, or scalable but limited methods like Random Fourier Features (RFF). We close this gap by introducing Random Wavelet Features (RWF), a framework that constructs scalable, non-stationary kernel approximations by sampling from wavelet families. By harnessing the inherent localization and multi-resolution structure of wavelets, RWF generates an explicit feature map that captures complex, input-dependent patterns. Our framework provides a principled way to generalize RFF to the non-stationary setting and comes with a comprehensive theoretical analysis, including positive definiteness, unbiasedness, and uniform convergence guarantees. We demonstrate empirically on a range of challenging synthetic and real-world datasets that RWF outperforms stationary random features and offers a compelling accuracy-efficiency trade-off against more complex models, unlocking scalable and expressive kernel methods for a broad class of real-world non-stationary problems.

</details>


### [429] [ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning](https://arxiv.org/abs/2602.01003)
*Zhishen Sun,Sizhe Dang,Guang Dai,Haishan Ye*

Main category: cs.LG

TL;DR: 提出ESSAM框架解决强化学习在大语言模型数学推理中GPU内存使用高的问题，实验显示其效果与RL方法相当且大幅降低GPU内存使用。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大语言模型数学推理训练中GPU内存使用高，难以在资源有限环境使用。

Method: 提出ESSAM框架，将进化策略的零阶搜索与Sharpness - Aware Maximization相结合进行全参数微调。

Result: 在GSM8K任务上，ESSAM平均准确率达78.27%，整体性能与RL方法相当，部分超越；相比PPO和GRPO，分别降低18倍和10倍GPU内存使用。

Conclusion: ESSAM在数学推理任务中效果好且能显著降低GPU内存使用。

Abstract: Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpness-Aware Maximization (ESSAM), a full parameter fine-tuning framework that tightly combines the zero-order search in parameter space from Evolution Strategies (ES) with the Sharpness-Aware Maximization (SAM) to improve generalization. We conduct fine-tuning experiments on the mainstream mathematica reasoning task GSM8K. The results show that ESSAM achieves an average accuracy of 78.27\% across all models and its overall performance is comparable to RL methods. It surpasses classic RL algorithm PPO with an accuracy of 77.72\% and is comparable to GRPO with an accuracy of 78.34\%, and even surpassing them on some models. In terms of GPU memory usage, ESSAM reduces the average GPU memory usage by $18\times$ compared to PPO and by $10\times$ compared to GRPO, achieving an extremely low GPU memory usage.

</details>


### [430] [Predicting Anemia Among Under-Five Children in Nepal Using Machine Learning and Deep Learning](https://arxiv.org/abs/2602.01005)
*Deepak Bastola,Pitambar Acharya,Dipak Dulal,Rabina Dhakal,Yang Li*

Main category: cs.LG

TL;DR: 文章利用尼泊尔人口与健康调查数据，经特征选择后对比多种机器学习和深度学习模型预测儿童贫血，结果显示模型有竞争力，部分特征对风险分层和筛查重要。


<details>
  <summary>Details</summary>
Motivation: 尼泊尔儿童贫血是重大公共卫生挑战，需有效预测方法。

Method: 定义贫血分类任务，用四种特征选择技术筛选尼泊尔人口与健康调查数据特征，对比八种传统机器学习和两种深度学习模型。

Result: 逻辑回归召回率和F1分数最佳，DNN准确率最高，SVM的AUC最高。

Conclusion: 机器学习和深度学习模型可用于贫血预测，部分特征对尼泊尔公共卫生筛查至关重要。

Abstract: Childhood anemia remains a major public health challenge in Nepal and is associated with impaired growth, cognition, and increased morbidity. Using World Health Organization hemoglobin thresholds, we defined anemia status for children aged 6-59 months and formulated a binary classification task by grouping all anemia severities as \emph{anemic} versus \emph{not anemic}. We analyzed Nepal Demographic and Health Survey (NDHS 2022) microdata comprising 1,855 children and initially considered 48 candidate features spanning demographic, socioeconomic, maternal, and child health characteristics. To obtain a stable and substantiated feature set, we applied four features selection techniques (Chi-square, mutual information, point-biserial correlation, and Boruta) and prioritized features supported by multi-method consensus. Five features: child age, recent fever, household size, maternal anemia, and parasite deworming were consistently selected by all methods, while amenorrhea, ethnicity indicators, and provinces were frequently retained. We then compared eight traditional machine learning classifiers (LR, KNN, DT, RF, XGBoost, SVM, NB, LDA) with two deep learning models (DNN and TabNet) using standard evaluation metrics, emphasizing F1-score and recall due to class imbalance. Among all models, logistic regression attained the best recall (0.701) and the highest F1-score (0.649), while DNN achieved the highest accuracy (0.709), and SVM yielded the strongest discrimination with the highest AUC (0.736). Overall, the results indicate that both machine learning and deep learning models can provide competitive anemia prediction and the interpretable features such as child age, infection proxy, maternal anemia, and deworming history are central for risk stratification and public health screening in Nepal.

</details>


### [431] [LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems](https://arxiv.org/abs/2602.01009)
*Haoran Li,Chenhan Xiao,Lihao Mai,Yang Weng,Erik Blasch*

Main category: cs.LG

TL;DR: 本文提出LASS - ODE模型解决物理系统动态预测问题，通过新的标记表示和跨系统注意力机制，在ODE轨迹集上预训练得到良好性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在物理系统动态预测进展有限，存在物理计算可扩展性和知识共享效率两大挑战。

Method: 提出尊重局部线性ODE演化的标记表示，引入带公共结构中心（CSH）的跨系统注意力机制，构建LASS - ODE模型并在40GB ODE轨迹集上预训练。

Result: 模型能在领域内有强性能，可跨不同ODE系统零样本泛化，微调后有额外提升。

Conclusion: 提出的方法有效解决物理系统动态预测的挑战，LASS - ODE模型表现良好。

Abstract: Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can enforce physical regularization, but its computation (e.g., ODE integration) does not scale to extensive systems. $(ii)$ Knowledge-sharing efficiency: the attention mechanism is primarily computed within each system, which limits the extraction of shared ODE structures across systems. We show that enforcing ODE consistency does not require expensive nonlinear integration: a token-wise locally linear ODE representation preserves physical fidelity while scaling to foundation-model regimes. Thus, we propose novel token representations that respect locally linear ODE evolution. Such linearity substantially accelerates integration while accurately approximating the local data manifold. Second, we introduce a simple yet effective inter-system attention that augments attention with a common structure hub (CSH) that stores shared tokens and aggregates knowledge across systems. The resulting model, termed LASS-ODE (\underline{LA}rge-\underline{S}cale \underline{S}mall \underline{ODE}), is pretrained on our $40$GB ODE trajectory collections to enable strong in-domain performance, zero-shot generalization across diverse ODE systems, and additional improvements through fine-tuning.

</details>


### [432] [How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments](https://arxiv.org/abs/2602.01017)
*Fuxin Wang,Amr Alazali,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 本文通过实验研究了大语言模型思维链推理的忠实性问题，发现模型在特定噪声阈值下可学习到忠实推理，高噪声下推理模式会转变，还揭示了隐式自我验证的出现。


<details>
  <summary>Details</summary>
Motivation: 当前对思维链推理缺乏基础理解，不清楚什么构成忠实的思维链推理以及不忠实性如何从自回归训练中产生。

Method: 进行可控的合成实验，在有噪声的数据上训练小型变压器，让其逐步解决模算术表达式，即算术表达式推理任务。

Result: 模型在训练噪声低于临界阈值时可学习到遵循算术规则的忠实推理；高噪声下训练动态会从忠实的逐步推理过渡到不忠实的跳跃式推理；模型会通过解决不一致的推理步骤来编码内部不确定性。

Conclusion: 自回归训练中会出现隐式自我验证现象。

Abstract: Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lacking--what constitutes faithful CoT reasoning, and how unfaithfulness emerges from autoregressive training. We study these questions using well-controlled synthetic experiments, training small transformers on noisy data to solve modular arithmetic expressions step by step, a task we term Arithmetic Expression Reasoning. We find that models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold, a phenomenon attributable to simplicity bias. At higher noise levels, training dynamics exhibit a transition from faithful stepwise reasoning to unfaithful skip-step reasoning via an intermediate mixed mode characterized by a transient increase in prediction entropy. Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps, which suggests the emergence of implicit self-verification from autoregressive training.

</details>


### [433] [Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01025)
*Kaiyuan Cui,Yige Li,Yutao Wu,Xingjun Ma,Sarah Erfani,Christopher Leckie,Hanxun Huang*

Main category: cs.LG

TL;DR: 提出UltraBreak框架，在视觉空间约束对抗模式，放松文本目标，实现通用可迁移越狱攻击，表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于梯度的越狱方法可迁移性差，多模态集成使视觉语言模型面临基于图像的越狱攻击。

Method: 提出UltraBreak框架，在视觉空间通过变换和正则化约束对抗模式，通过基于语义的目标放松文本目标，在目标大语言模型的文本嵌入空间定义损失。

Result: UltraBreak在实验中始终优于先前的越狱方法，进一步分析揭示了早期方法无法迁移的原因。

Conclusion: 通过语义目标平滑损失景观对于实现通用可迁移的越狱攻击至关重要。

Abstract: Vision-language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. Existing gradient-based jailbreak methods transfer poorly, as adversarial patterns overfit to a single white-box surrogate and fail to generalise to black-box models. In this work, we propose Universal and transferable jailbreak (UltraBreak), a framework that constrains adversarial patterns through transformations and regularisation in the vision space, while relaxing textual targets through semantic-based objectives. By defining its loss in the textual embedding space of the target LLM, UltraBreak discovers universal adversarial patterns that generalise across diverse jailbreak objectives. This combination of vision-level regularisation and semantically guided textual supervision mitigates surrogate overfitting and enables strong transferability across both models and attack targets. Extensive experiments show that UltraBreak consistently outperforms prior jailbreak methods. Further analysis reveals why earlier approaches fail to transfer, highlighting that smoothing the loss landscape via semantic objectives is crucial for enabling universal and transferable jailbreaks. The code is publicly available in our \href{https://github.com/kaiyuanCui/UltraBreak}{GitHub repository}.

</details>


### [434] [SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2602.01027)
*Xin Nie,Haicheng Zhang,Liang Dong,Beining Feng,Jinhong Weng,Guiling Sun*

Main category: cs.LG

TL;DR: 提出了适用于大语言模型的无搜索且硬件友好的混合精度量化框架SFMP，通过四大创新点提升性能并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有的混合精度方法存在依赖昂贵离散优化或引入硬件效率低下的问题，需要改进。

Method: 提出SFMP框架，包含分数位宽、块级混合精度、行列权重重排序和统一GEMM内核四个创新点。

Result: 在相同内存约束下，SFMP优于现有层级混合精度方法，显著降低量化成本并提高推理效率。

Conclusion: SFMP是一种有效且高效的大语言模型混合精度量化方案。

Abstract: Mixed-precision quantization is a promising approach for compressing large language models under tight memory budgets. However, existing mixed-precision methods typically suffer from one of two limitations: they either rely on expensive discrete optimization to determine precision allocation, or introduce hardware inefficiencies due to irregular memory layouts. We propose SFMP, a search-free and hardware-friendly mixed-precision quantization framework for large language models. The framework is built upon four novel ideas: Fractional bit-width, which extends integer bit-width for weight matrix to fractional value and transforms discrete precision allocation as a continuous problem; 2)Block-wise mixed-precision, enabling fine-grained precision within weight matrices while remaining hardware-friendly; 3)Row-column weight reordering, which aggregates salient weights via row and column reordering, incurring only a small activation reordering overhead during inference; 4)Unified GEMM kernel, which supports mixed-precision GEMM at arbitrary average bit-width. Extensive experiments demonstrate that SFMP outperforms state-of-the-art layer-wise mixed-precision methods under the same memory constraints, while significantly reducing quantization cost and improving inference efficiency. Code is available at https://github.com/Nkniexin/SFMP

</details>


### [435] [Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection](https://arxiv.org/abs/2602.01039)
*Zhiwei Ling,Hailiang Zhao,Chao Zhang,Xiang Ao,Ziqi Wang,Cheng Zhang,Zhen Qin,Xinkui Zhao,Kingsum Chow,Yuanqing Wu,MengChu Zhou*

Main category: cs.LG

TL;DR: 提出FLood框架应对联邦学习中数据非IID问题，实验证明其优于现有方法且可作为插件提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实中联邦学习数据存在非IID问题，严重影响全局模型质量，需解决该挑战。

Method: 提出受OOD检测启发的FLood框架，通过双加权机制，在客户端和服务器端分别处理数据异质性。

Result: 在多种非IID设置下的多个基准测试中，FLood在准确性和泛化性上始终优于现有方法，且可作为插件无缝集成到现有算法。

Conclusion: FLood是在现实联邦环境中部署可靠智能服务的实用且可扩展的解决方案。

Abstract: Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.

</details>


### [436] [SwiftRepertoire: Few-Shot Immune-Signature Synthesis via Dynamic Kernel Codes](https://arxiv.org/abs/2602.01051)
*Rong Fu,Wenxin Zhang,Muge Qi,Yang Li,Yabin Jin,Jiekai Wu,Jiaxuan Lu,Chunlei Meng,Youjin Wang,Zeli Su,Juntao Gao,Li Bao,Qi Zhao,Wei Luo,Simon Fong*

Main category: cs.LG

TL;DR: 提出一种框架，可让T细胞受体库分析模型快速适应新任务，适用于数据少、计算资源有限场景


<details>
  <summary>Details</summary>
Motivation: T细胞受体库分析在实际应用中受标签稀疏、队列异质性和大编码器适应新任务计算负担大的阻碍

Method: 从原型字典合成紧凑的特定任务参数化，生成小适配器模块应用于预训练骨干网络，通过基序感知探针和校准的基序发现管道保留可解释性

Result: 能在少量支持示例下立即适应新任务，无需全模型微调

Conclusion: 该框架为将受体库信息模型应用于临床和研究提供实用、样本高效且可解释的途径

Abstract: Repertoire-level analysis of T cell receptors offers a biologically grounded signal for disease detection and immune monitoring, yet practical deployment is impeded by label sparsity, cohort heterogeneity, and the computational burden of adapting large encoders to new tasks. We introduce a framework that synthesizes compact task-specific parameterizations from a learned dictionary of prototypes conditioned on lightweight task descriptors derived from repertoire probes and pooled embedding statistics. This synthesis produces small adapter modules applied to a frozen pretrained backbone, enabling immediate adaptation to novel tasks with only a handful of support examples and without full model fine-tuning. The architecture preserves interpretability through motif-aware probes and a calibrated motif discovery pipeline that links predictive decisions to sequence-level signals. Together, these components yield a practical, sample-efficient, and interpretable pathway for translating repertoire-informed models into diverse clinical and research settings where labeled data are scarce and computational resources are constrained.

</details>


### [437] [LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents](https://arxiv.org/abs/2602.01053)
*Hyesung Jeon,Hyeongju Ha,Jae-Joon Kim*

Main category: cs.LG

TL;DR: 提出用于多LoRA代理的KV缓存共享框架LRAgent，减少内存和计算开销，在代理问答基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 多LLM代理系统的多LoRA实现中，各代理独立构建和存储KV缓存，现有方法未考虑此场景，导致大量内存和计算开销。

Method: 提出LRAgent框架，将缓存分解为共享基础组件和适配器依赖组件，引入Flash - LoRA - Attention内核。

Result: LRAgent实现了接近全共享缓存的吞吐量和首令牌延迟，在代理问答基准测试中保持接近非共享缓存基线的准确性。

Conclusion: LRAgent能有效减少多LoRA代理的内存和计算开销，同时保证一定的准确性。

Abstract: Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.

</details>


### [438] [Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning](https://arxiv.org/abs/2602.01058)
*Dylan Zhang,Yufeng Xu,Haojin Wang,Qingzhi Chen,Hao Peng*

Main category: cs.LG

TL;DR: 当前SFT - RL流程存在分布不匹配问题，提出SFT阶段方法PEAR解决此问题，实验表明其能提升RL后性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前SFT - RL流程中离线SFT数据分布与在线RL优化策略不匹配的问题，避免强SFT检查点初始化的模型在RL后表现不佳。

Method: 提出PEAR方法，使用重要性采样对SFT损失进行重新加权，有标记、块和序列三个级别的变体，可增强标准SFT目标。

Result: 在Qwen 2.5和3以及DeepSeek - distilled模型上的可控实验中，PEAR相比传统SFT持续提升RL后性能，在AIME2025上通过率提升达14.6%。

Conclusion: PEAR是迈向更全面大语言模型后训练的有效一步，设计和评估SFT时应考虑下游RL而非孤立进行。

Abstract: Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.
  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.
  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.
  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.

</details>


### [439] [On the Expressive Power of Permutation-Equivariant Weight-Space Networks](https://arxiv.org/abs/2602.01083)
*Adir Dayan,Yam Eitan,Haggai Maron*

Main category: cs.LG

TL;DR: 本文针对权重空间网络表达性缺乏全面刻画的问题，开发了系统理论，证明了主要排列等变网络表达能力等价，建立了在一定假设下的泛化性并刻画了泛化不成立的情况。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA权重空间网络依赖排列等变设计提升泛化性，但可能影响表达能力，且缺乏对权重空间网络表达性的全面刻画。

Method: 开发系统理论来研究权重空间网络的表达性。

Result: 证明所有突出的排列等变网络在表达能力上等价，在对输入权重的温和自然假设下建立了权重和函数空间设置中的泛化性，并刻画了泛化性不成立的边缘情况。

Conclusion: 这些结果为权重空间网络的表达性提供了强大而统一的基础。

Abstract: Weight-space learning studies neural architectures that operate directly on the parameters of other neural networks. Motivated by the growing availability of pretrained models, recent work has demonstrated the effectiveness of weight-space networks across a wide range of tasks. SOTA weight-space networks rely on permutation-equivariant designs to improve generalization. However, this may negatively affect expressive power, warranting theoretical investigation. Importantly, unlike other structured domains, weight-space learning targets maps operating on both weight and function spaces, making expressivity analysis particularly subtle. While a few prior works provide partial expressivity results, a comprehensive characterization is still missing. In this work, we address this gap by developing a systematic theory for expressivity of weight-space networks. We first prove that all prominent permutation-equivariant networks are equivalent in expressive power. We then establish universality in both weight- and function-space settings under mild, natural assumptions on the input weights, and characterize the edge-case regimes where universality no longer holds. Together, these results provide a strong and unified foundation for the expressivity of weight-space networks.

</details>


### [440] [OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases](https://arxiv.org/abs/2602.01105)
*Zixiao Wang,Yifei Shen,Huishuai Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \nameA{} (\fullname{}), which combines spectral control from orthogonalized update directions with $\ell_\infty$-style coordinate control from sign updates. \nameA{} forms a Lion-style momentum direction, approximately orthogonalizes it via a few Newton--Schulz iterations, and then applies an entrywise sign, providing an efficient approximation to taking a maximal step over the intersection of the spectral and $\ell_\infty$ constraint sets (a scaled Hadamard-like set for matrix parameters). Despite the strong nonlinearity of orthogonalization and sign, we prove convergence under a mild, empirically verified diagonal-isotropy assumption. Across large-scale language and vision training, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning, \nameA{} matches or outperforms AdamW and Muon under comparable tuning while using only momentum-level optimizer state, and it mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.

</details>


### [441] [Single-Edge Node Injection Threats to GNN-Based Security Monitoring in Industrial Graph Systems](https://arxiv.org/abs/2602.01113)
*Wenjie Liang,Ranhui Yan,Jia Cai,You-Gan Wang*

Main category: cs.LG

TL;DR: 本文针对工业图监测系统中GNN面临的节点注入攻击问题，提出SEGIA攻击方法，其攻击成功率比基线高至少25%，揭示了工业GNN部署的系统风险。


<details>
  <summary>Details</summary>
Motivation: 工业图监测系统采用GNN时，攻击者可注入伪造节点影响决策并规避清理，需研究资源受限下的节点注入攻击。

Method: 提出Single - Edge Graph Injection Attack (SEGIA)，集成剪枝SGC替代、多跳邻域采样和基于反向图卷积的特征合成，以相似性正则化目标保留局部同质性。

Result: 理论分析和评估显示，在更小边预算下，攻击成功率比代表性基线至少高25%。

Conclusion: 工业GNN部署存在系统级风险，需进行轻量级准入验证和邻域一致性监测。

Abstract: Graph neural networks (GNNs) are increasingly adopted in industrial graph-based monitoring systems (e.g., Industrial internet of things (IIoT) device graphs, power-grid topology models, and manufacturing communication networks) to support anomaly detection, state estimation, and asset classification. In such settings, an adversary that compromises a small number of edge devices may inject counterfeit nodes (e.g., rogue sensors, virtualized endpoints, or spoofed substations) to bias downstream decisions while evading topology- and homophily-based sanitization. This paper formulates deployment-oriented node-injection attacks under constrained resources and proposes the \emph{Single-Edge Graph Injection Attack} (SEGIA), in which each injected node attaches to the operational graph through a single edge. SEGIA integrates a pruned SGC surrogate, multi-hop neighborhood sampling, and reverse graph convolution-based feature synthesis with a similarity-regularized objective to preserve local homophily and survive edge pruning. Theoretical analysis and extensive evaluations across datasets and defenses show at least $25\%$ higher attack success than representative baselines under substantially smaller edge budgets. These results indicate a system-level risk in industrial GNN deployments and motivate lightweight admission validation and neighborhood-consistency monitoring.

</details>


### [442] [MarkovScale: Towards Optimal Sequential Scaling at Inference Time](https://arxiv.org/abs/2602.01120)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 提出MarkovScale解决顺序扩展效果差且不明确问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有顺序扩展性能提升有限且机制不明，因大多采用启发式非原则性方法。

Method: 将顺序扩展建模为双状态马尔可夫过程，提出理论框架，开发MarkovScale系统。

Result: 在3个语言模型、5个基准测试和超20种配置实验中，MarkovScale始终优于现有并行和顺序扩展方法。

Conclusion: MarkovScale向大语言模型最优且资源高效推理迈出重要一步。

Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [443] [ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs](https://arxiv.org/abs/2602.01124)
*Md Abrar Jahin,Taufikur Rahman Fuad,Jay Pujara,Craig Knoblock*

Main category: cs.LG

TL;DR: 提出ChronoSpike自适应脉冲图神经网络，在三个基准测试中表现优于12个SOTA基线，有更快训练速度和多项理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有动态图表示学习方法存在复杂度高、梯度问题、缺乏全局信息等不足，需要新方法。

Method: 提出ChronoSpike，集成可学习LIF神经元、多注意力空间聚合和轻量级Transformer时间编码器。

Result: 在三个大型基准测试中，Macro - F1提升2.0%，Micro - F1提升2.4%，训练速度比循环方法快3 - 10倍。

Conclusion: ChronoSpike有线性内存复杂度，能捕捉长距离依赖，有理论保证和可解释性分析结果。

Abstract: Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\%$ Macro-F1 and $2.4\%$ Micro-F1 while achieving $3-10\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $ρ< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\%$ sparsity.

</details>


### [444] [WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity](https://arxiv.org/abs/2602.01126)
*Mengsha Kou,Xiaoyu Xia,Ziqi Wang,Ibrahim Khalil,Runkun Luo,Jingwen Zhou,Minhui Xue*

Main category: cs.LG

TL;DR: 本文提出 WinFLoRA，一种隐私异构的联邦 LoRA，可解决联邦 LoRA 中存在的隐私异质性问题，提升全局准确率和客户端效用。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感的部署中，联邦 LoRA 存在隐私异质性问题，导致个体激励与全局性能不一致，因此需要提出新方法解决。

Method: 提出 WinFLoRA，通过基于上传的 LoRA 适配器估计客户端噪声，利用聚合权重作为激励机制，对低噪声更新进行加权。

Result: 在多个大语言模型和数据集上进行评估，WinFLoRA 的全局准确率比现有基准高 52.58%，客户端效用是现有基准的 2.56 倍。

Conclusion: WinFLoRA 可在不依赖第三方的情况下，将客户端隐私和下游性能的异构效用与全局模型目标对齐。

Abstract: Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.

</details>


### [445] [Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models](https://arxiv.org/abs/2602.01128)
*Mete Erdogan*

Main category: cs.LG

TL;DR: 提出TS - DPO方法使大语言模型在多人类偏好维度上可控对齐，在数据集上表现优于标量化DPO。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法将反馈合并为单一标量奖励，无法平衡多目标和遍历帕累托前沿，需让大语言模型在多人类偏好维度上进行原则性和可控对齐。

Method: 扩展在模型切空间微调的思想到偏好对齐，提出Tangent - Space Direct Preference Optimization (TS - DPO)，在局部线性区域执行DPO学习每个目标的更新方向，推理时线性组合这些方向。

Result: 在HelpSteer和UltraFeedback数据集上评估，TS - DPO比标量化DPO实现更广泛的帕累托最优覆盖和更平滑的偏好控制；典范相关分析表明切空间训练增强了与不同偏好对齐的典范方向，改善了解缠。

Conclusion: TS - DPO方法能有效让大语言模型在多人类偏好维度上进行可控对齐，优于现有标量化DPO方法。

Abstract: Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.

</details>


### [446] [TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation](https://arxiv.org/abs/2602.01135)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: 提出TRACE框架用于从离散事件序列进行因果发现，有理论保证且实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决从单个离散事件序列进行因果发现时因无重复样本、高维、长程依赖带来的挑战。

Method: 引入TRACE框架，将自回归模型用作预训练密度估计器进行条件互信息估计。

Result: 能推断事件类型间的因果图，线性扩展，支持延迟因果效应，在GPU上可并行；实验在不同基线和词汇量下表现稳健。

Conclusion: TRACE框架可有效从单个离散事件序列进行因果发现。

Abstract: We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.

</details>


### [447] [A Unified Matrix-Spectral Framework for Stability and Interpretability in Deep Learning](https://arxiv.org/abs/2602.01136)
*Ronald Katende*

Main category: cs.LG

TL;DR: 本文提出统一矩阵谱框架分析深度神经网络稳定性和可解释性，引入指标并利用谱熵，实验证实谱正则化可提升归因稳定性，建立谱集中与分析稳定性联系。


<details>
  <summary>Details</summary>
Motivation: 分析深度神经网络的稳定性和可解释性。

Method: 开发统一矩阵 - 谱框架，引入全局矩阵稳定性指标聚合多种算子的谱信息，用谱熵细化经典算子范数界。

Result: 合成实验和控制研究表明适度谱正则化能大幅提高归因稳定性，即便全局谱摘要变化不大。

Conclusion: 建立了谱集中与分析稳定性之间的精确联系，为鲁棒性感知的模型设计和训练提供实用指导。

Abstract: We develop a unified matrix-spectral framework for analyzing stability and interpretability in deep neural networks. Representing networks as data-dependent products of linear operators reveals spectral quantities governing sensitivity to input perturbations, label noise, and training dynamics.
  We introduce a Global Matrix Stability Index that aggregates spectral information from Jacobians, parameter gradients, Neural Tangent Kernel operators, and loss Hessians into a single stability scale controlling forward sensitivity, attribution robustness, and optimization conditioning. We further show that spectral entropy refines classical operator-norm bounds by capturing typical, rather than purely worst-case, sensitivity.
  These quantities yield computable diagnostics and stability-oriented regularization principles. Synthetic experiments and controlled studies on MNIST, CIFAR-10, and CIFAR-100 confirm that modest spectral regularization substantially improves attribution stability even when global spectral summaries change little.
  The results establish a precise connection between spectral concentration and analytic stability, providing practical guidance for robustness-aware model design and training.

</details>


### [448] [Self-Generative Adversarial Fine-Tuning for Large Language Models](https://arxiv.org/abs/2602.01137)
*Shiguang Wu,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: 提出SGALM统一微调框架解决大语言模型对齐问题，取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对齐微调方法依赖高质量标注且成本高、数量少，自玩和合成数据方法有启发式假设和自我评价问题。

Method: 提出Self - Generative Adversarial LLM (SGALM)，将对齐问题表述为单个大语言模型内的生成对抗游戏，联合进化生成和判别能力，无需外部奖励模型。

Result: 理论和实证结果表明SGALM达到了SOTA性能。

Conclusion: SGALM是有效的对齐算法和强大的合成数据引擎。

Abstract: Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.

</details>


### [449] [Key Principles of Graph Machine Learning: Representation, Robustness, and Generalization](https://arxiv.org/abs/2602.01139)
*Yassine Abbahaddou*

Main category: cs.LG

TL;DR: 本文研究图神经网络（GNNs）面临的挑战，提出三项贡献以解决问题，增进对GNNs局限性与潜力的理解。


<details>
  <summary>Details</summary>
Motivation: GNNs在性能、泛化能力、对抗攻击鲁棒性和表征学习能力方面存在挑战，需进行深入研究。

Method: （1）基于图移位算子开发新表征学习技术；（2）通过图数据增强引入增强泛化能力的方法；（3）利用正交化技术和基于噪声的防御机制开发更鲁棒的GNNs。

Result: 未提及具体结果

Conclusion: 工作为理解GNNs的局限性和潜力提供了更具原则性的认识。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations from structured data. Despite their growing popularity and success across various applications, GNNs encounter several challenges that limit their performance. in their generalization, robustness to adversarial perturbations, and the effectiveness of their representation learning capabilities. In this dissertation, I investigate these core aspects through three main contributions: (1) developing new representation learning techniques based on Graph Shift Operators (GSOs, aiming for enhanced performance across various contexts and applications, (2) introducing generalization-enhancing methods through graph data augmentation, and (3) developing more robust GNNs by leveraging orthonormalization techniques and noise-based defenses against adversarial attacks. By addressing these challenges, my work provides a more principled understanding of the limitations and potential of GNNs.

</details>


### [450] [Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization](https://arxiv.org/abs/2602.01140)
*Haochen You,Heng Zhang,Hongyang He,Yuqi Li,Baojing Liu*

Main category: cs.LG

TL;DR: 提出GRIT - VQ统一替代框架，解决现有向量量化问题，实验效果优于现有变体。


<details>
  <summary>Details</summary>
Motivation: 现有向量量化的硬最近邻分配不可微，用启发式直通估计器优化存在梯度不稳定和码本利用率低的问题。

Method: 引入GRIT - VQ框架，前向传播保留硬分配，用基于半径的更新替代直通估计器，对码本应用与数据无关的集成变换。

Result: 在图像重建、生成和推荐标记化基准测试中，GRIT - VQ提高了重建误差、生成质量和推荐准确性，大幅提高了码本利用率。

Conclusion: GRIT - VQ能实现稳定的梯度流、协调的码本演化，可靠避免崩溃，优于现有VQ变体。

Abstract: Vector quantization (VQ) underpins modern generative and representation models by turning continuous latents into discrete tokens. Yet hard nearest-neighbor assignments are non-differentiable and are typically optimized with heuristic straight-through estimators, which couple the update step size to the quantization gap and train each code in isolation, leading to unstable gradients and severe codebook under-utilization at scale. In this paper, we introduce GRIT-VQ (Generalized Radius and Integrated Transform-Vector Quantization), a unified surrogate framework that keeps hard assignments in the forward pass while making VQ fully differentiable. GRIT-VQ replaces the straight-through estimator with a radius-based update that moves latents along the quantization direction with a controllable, geometry-aware step, and applies a data-agnostic integrated transform to the codebook so that all codes are updated through shared parameters instead of independently. Our theoretical analysis clarifies the fundamental optimization dynamics introduced by GRIT-VQ, establishing conditions for stable gradient flow, coordinated codebook evolution, and reliable avoidance of collapse across a broad family of quantizers. Across image reconstruction, image generation, and recommendation tokenization benchmarks, GRIT-VQ consistently improves reconstruction error, generative quality, and recommendation accuracy while substantially increasing codebook utilization compared to existing VQ variants.

</details>


### [451] [Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing](https://arxiv.org/abs/2602.01150)
*Jialong Sun,Zeming Wei,Jiaxuan Zou,Jiacheng Gong,Guanheng Wang,Chengyang Dong,Jialong Li,Bo Liu*

Main category: cs.LG

TL;DR: 指出基于MIA的机器学习遗忘审计假设存在缺陷，提出无训练且高效的SMIA框架，实验表明其审计更可靠、成本更低。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于MIA的机器学习遗忘审计存在假设缺陷和计算成本高的问题。

Method: 提出统计成员推理攻击（SMIA）框架，直接用统计测试比较成员和非成员数据分布。

Result: SMIA审计更可靠，计算成本显著低于现有基于MIA的方法。

Conclusion: SMIA可作为可靠的机器学习遗忘审计新范式。

Abstract: Machine unlearning (MU) is essential for enforcing the right to be forgotten in machine learning systems. A key challenge of MU is how to reliably audit whether a model has truly forgotten specified training data. Membership Inference Attacks (MIAs) are widely used for unlearning auditing, where samples that evade membership detection are often regarded as successfully forgotten. After carefully revisiting the reliability of MIA, we show that this assumption is flawed: failed membership inference does not imply true forgetting. We theoretically demonstrate that MIA-based auditing, when formulated as a binary classification problem, inevitably incurs statistical errors whose magnitude cannot be observed during the auditing process. This leads to overly optimistic evaluations of unlearning performance, while incurring substantial computational overhead due to shadow model training. To address these limitations, we propose Statistical Membership Inference Attack (SMIA), a novel training-free and highly effective auditing framework. SMIA directly compares the distributions of member and non-member data using statistical tests, eliminating the need for learned attack models. Moreover, SMIA outputs both a forgetting rate and a corresponding confidence interval, enabling quantified reliability of the auditing results. Extensive experiments show that SMIA provides more reliable auditing with significantly lower computational cost than existing MIA-based approaches. Notably, the theoretical guarantees and empirical effectiveness of SMIA suggest it as a new paradigm for reliable machine unlearning auditing.

</details>


### [452] [PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning](https://arxiv.org/abs/2602.01156)
*Shunpeng Yang,Ben Liu,Hua Chen*

Main category: cs.LG

TL;DR: 提出基于连续归一化流（CNF）的策略梯度算法PolicyFlow，结合CNF策略与PPO目标，无需全流路径似然评估，还提出布朗正则化器，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 标准PPO扩展到如CNF等高表达能力策略模型时，全流轨迹似然评估计算成本高且数值不稳定，需解决此问题。

Method: 提出PolicyFlow算法，用简单插值路径上的速度场变化近似重要性比率；提出布朗正则化器防止模式崩溃和鼓励多样行为。

Result: 在MultiGoal、PointMaze等多种环境任务实验中，PolicyFlow比使用高斯策略的PPO和基于流的基线模型性能相当或更优，在MultiGoal上能捕捉更丰富的多模态动作分布。

Conclusion: PolicyFlow有效解决了PPO扩展到CNF模型的难题，且有良好性能和行为多样性。

Abstract: Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.

</details>


### [453] [Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market](https://arxiv.org/abs/2602.01157)
*Mohammed Osman Gani,Zhipeng He,Chun Ouyang,Sara Khalifa*

Main category: cs.LG

TL;DR: 本文提出新的电价预测框架以解决现有深度学习电价预测的不足，评估显示不同模型表现不同，为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习电价预测存在对多日预测关注不足、对最新时间序列模型探索不够、依赖聚合评估掩盖日内差异等问题，需改进。

Method: 提出新的电价预测框架，利用基准最新时间序列深度学习模型扩展预测到多日，对澳大利亚国家电力市场五个区域进行日内间隔级评估。

Result: 没有单一模型在各区域、指标和预测期都占优，标准深度学习模型在多数区域表现好，最新模型对预测期延长更稳健，日内评估显示明显昼夜误差模式。

Conclusion: 未来基于深度学习的电价预测可从丰富特征表示和建模策略入手，增强长期预测稳健性同时保持对日内波动和价格动态的敏感性。

Abstract: Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted in EPF to capture complex and nonlinear price dynamics, several important gaps persist: (i) limited attention to multi-day horizons beyond day-ahead forecasting, (ii) insufficient exploration of state-of-the-art (SOTA) time series DL models, and (iii) a predominant reliance on aggregated horizon-level evaluation that obscures time-of-day forecasting variation. To address these gaps, we propose a novel EPF framework that extends the forecast horizon to multi-day-ahead by systematically building forecasting models that leverage benchmarked SOTA time series DL models. We conduct a comprehensive evaluation to analyse time-of-day forecasting performance by integrating model assessment at intraday interval levels across all five regions in the Australian National Electricity Market (NEM). The results show that no single model consistently dominates across regions, metrics, and horizons. Overall, standard DL models deliver superior performance in most regions, while SOTA time series DL models demonstrate greater robustness to forecast horizon extension. Intraday interval-level evaluation reveals pronounced diurnal error patterns, indicating that absolute errors peak during the evening ramp, relative errors inflate during midday negative-price regimes, and directional accuracy degrades during periods of frequent trend changes. These findings suggest that future research on DL-based EPF can benefit from enriched feature representations and modelling strategies that enhance longer-term forecasting robustness while maintaining sensitivity to intraday volatility and structural price dynamics.

</details>


### [454] [Multi-Fidelity Physics-Informed Neural Networks with Bayesian Uncertainty Quantification and Adaptive Residual Learning for Efficient Solution of Parametric Partial Differential Equations](https://arxiv.org/abs/2602.01176)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: 本文提出MF - BPINN多保真框架解决高保真PDEs计算成本高问题。


<details>
  <summary>Details</summary>
Motivation: PINNs解决高保真PDEs计算成本高，尤其是参数系统需多次评估时。

Method: 将物理信息神经网络与贝叶斯不确定性量化和自适应残差学习结合，利用分层神经架构挖掘不同保真度数据间关联，引入带可学习门控机制的自适应残差网络，并开发基于哈密顿蒙特卡罗的贝叶斯框架。

Result: 未提及

Conclusion: 未提及

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful paradigm for solving partial differential equations (PDEs) by embedding physical laws directly into neural network training. However, solving high-fidelity PDEs remains computationally prohibitive, particularly for parametric systems requiring multiple evaluations across varying parameter configurations. This paper presents MF-BPINN, a novel multi-fidelity framework that synergistically combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. Our approach leverages abundant low-fidelity simulations alongside sparse high-fidelity data through a hierarchical neural architecture that learns nonlinear correlations across fidelity levels. We introduce an adaptive residual network with learnable gating mechanisms that dynamically balances linear and nonlinear fidelity discrepancies. Furthermore, we develop a rigorous Bayesian framework employing Hamiltonian Monte Carlo.

</details>


### [455] [Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective](https://arxiv.org/abs/2602.01179)
*Zhichao Chen,Zhan Zhuang,Yunfei Teng,Hao Wang,Fangyikang Wang,Zhengnan Li,Tianqiao Liu,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文提出E - SUOT框架构建中间域以解决渐进域适应问题，避免了基于样本的似然估计问题，还进行了理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 真实中间域常不可用或无效，现有流基模型训练依赖样本似然估计，会丢弃有用信息，影响渐进域适应性能。

Method: 提出熵正则化半对偶不平衡最优传输（E - SUOT）框架，将流基渐进域适应问题转化为拉格朗日对偶问题，推导等价半对偶目标，引入熵正则化解决对偶问题导致的不稳定训练过程，提出新的渐进域适应训练框架并进行理论分析。

Result: 通过大量实验证明了E - SUOT框架的有效性。

Conclusion: E - SUOT框架能有效构建中间域，解决现有方法在渐进域适应中的局限性。

Abstract: Gradual domain adaptation (GDA) aims to mitigate domain shift by progressively adapting models from the source domain to the target domain via intermediate domains. However, real intermediate domains are often unavailable or ineffective, necessitating the synthesis of intermediate samples. Flow-based models have recently been used for this purpose by interpolating between source and target distributions; however, their training typically relies on sample-based log-likelihood estimation, which can discard useful information and thus degrade GDA performance. The key to addressing this limitation is constructing the intermediate domains via samples directly. To this end, we propose an Entropy-regularized Semi-dual Unbalanced Optimal Transport (E-SUOT) framework to construct intermediate domains. Specifically, we reformulate flow-based GDA as a Lagrangian dual problem and derive an equivalent semi-dual objective that circumvents the need for likelihood estimation. However, the dual problem leads to an unstable min-max training procedure. To alleviate this issue, we further introduce entropy regularization to convert it into a more stable alternative optimization procedure. Based on this, we propose a novel GDA training framework and provide theoretical analysis in terms of stability and generalization. Finally, extensive experiments are conducted to demonstrate the efficacy of the E-SUOT framework.

</details>


### [456] [Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective](https://arxiv.org/abs/2602.01182)
*Zhichao Chen,Hao Wang,Fangyikang Wang,Licheng Pan,Zhengnan Li,Yunfei Teng,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 扩散模型用于时间序列数据插补在复杂场景表现不稳定，本文提出SPIRIT框架解决问题并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型用于时间序列数据插补在复杂场景性能不稳定，存在非平稳时间动态和目标不一致两个障碍。

Method: 从近端算子视角分析基于扩散模型的时间序列数据插补过程，提出SPIRIT框架，引入熵诱导的Bregman散度，构建半近端传输差异并证明其对非平稳性的鲁棒性，推导完整工作流程。

Result: 广泛实验表明提出的SPIRIT方法有效。

Conclusion: SPIRIT框架能解决扩散模型在时间序列数据插补复杂场景的问题。

Abstract: Diffusion models (DMs) have shown promise for Time-Series Data Imputation (TSDI); however, their performance remains inconsistent in complex scenarios. We attribute this to two primary obstacles: (1) non-stationary temporal dynamics, which can bias the inference trajectory and lead to outlier-sensitive imputations; and (2) objective inconsistency, since imputation favors accurate pointwise recovery whereas DMs are inherently trained to generate diverse samples. To better understand these issues, we analyze DM-based TSDI process through a proximal-operator perspective and uncover that an implicit Wasserstein distance regularization inherent in the process hinders the model's ability to counteract non-stationarity and dissipative regularizer, thereby amplifying diversity at the expense of fidelity. Building on this insight, we propose a novel framework called SPIRIT (Semi-Proximal Transport Regularized time-series Imputation). Specifically, we introduce entropy-induced Bregman divergence to relax the mass preserving constraint in the Wasserstein distance, formulate the semi-proximal transport (SPT) discrepancy, and theoretically prove the robustness of SPT against non-stationarity. Subsequently, we remove the dissipative structure and derive the complete SPIRIT workflow, with SPT serving as the proximal operator. Extensive experiments demonstrate the effectiveness of the proposed SPIRIT approach.

</details>


### [457] [The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics](https://arxiv.org/abs/2602.01186)
*Fabio Turazza,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: 提出Gaussian - Head OFL (GH - OFL) 系列单轮联邦学习方法，在强非独立同分布偏斜下实现了先进的鲁棒性和准确性且严格无数据。


<details>
  <summary>Details</summary>
Motivation: 经典联邦学习通信成本高、有隐私风险，现有的单轮联邦学习方法不实用或有局限。

Method: 引入假定预训练嵌入的类条件高斯性的GH - OFL系列方法，客户仅传输充分统计量，服务器通过封闭形式高斯头、FisherMix和Proto - Hyper三个组件构建头。

Result: GH - OFL方法在强非独立同分布偏斜下实现了先进的鲁棒性和准确性。

Conclusion: GH - OFL系列方法有效克服现有单轮联邦学习的问题，且严格无数据。

Abstract: Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reducing communication to a single round, thereby lowering overhead and enhancing practical deployability. Nevertheless, most existing one-shot approaches remain either impractical or constrained, for example, they often depend on the availability of a public dataset, assume homogeneous client models, or require uploading additional data or model information. To overcome these issues, we introduce the Gaussian-Head OFL (GH-OFL) family, a suite of one-shot federated methods that assume class-conditional Gaussianity of pretrained embeddings. Clients transmit only sufficient statistics (per-class counts and first/second-order moments) and the server builds heads via three components: (i) Closed-form Gaussian heads (NB/LDA/QDA) computed directly from the received statistics; (ii) FisherMix, a linear head with cosine margin trained on synthetic samples drawn in an estimated Fisher subspace; and (iii) Proto-Hyper, a lightweight low-rank residual head that refines Gaussian logits via knowledge distillation on those synthetic samples. In our experiments, GH-OFL methods deliver state-of-the-art robustness and accuracy under strong non-IID skew while remaining strictly data-free.

</details>


### [458] [Unraveling the Hidden Dynamical Structure in Recurrent Neural Policies](https://arxiv.org/abs/2602.01196)
*Jin Li,Yue Wu,Mengsha Huang,Yuhao Sun,Hao He,Xianyuan Zhan*

Main category: cs.LG

TL;DR: 分析循环神经策略隐藏状态域，发现稳定循环结构类似动力系统极限环，可解释策略特性。


<details>
  <summary>Details</summary>
Motivation: 循环神经策略表现优异，但关于其泛化和鲁棒性的底层机制理解不足。

Method: 分析通过不同训练方法、模型架构和任务学习到的循环策略的隐藏状态域。

Result: 发现与环境交互时稳定循环结构出现，与动力系统极限环相似，极限环几何结构与策略行为有对应关系。

Conclusion: 极限环的出现稳定策略内存和环境状态，抑制环境不确定性，其几何结构编码行为关系，便于适应非平稳环境。

Abstract: Recurrent neural policies are widely used in partially observable control and meta-RL tasks. Their abilities to maintain internal memory and adapt quickly to unseen scenarios have offered them unparalleled performance when compared to non-recurrent counterparts. However, until today, the underlying mechanisms for their superior generalization and robustness performance remain poorly understood. In this study, by analyzing the hidden state domain of recurrent policies learned over a diverse set of training methods, model architectures, and tasks, we find that stable cyclic structures consistently emerge during interaction with the environment. Such cyclic structures share a remarkable similarity with \textit{limit cycles} in dynamical system analysis, if we consider the policy and the environment as a joint hybrid dynamical system. Moreover, we uncover that the geometry of such limit cycles also has a structured correspondence with the policies' behaviors. These findings offer new perspectives to explain many nice properties of recurrent policies: the emergence of limit cycles stabilizes both the policies' internal memory and the task-relevant environmental states, while suppressing nuisance variability arising from environmental uncertainty; the geometry of limit cycles also encodes relational structures of behaviors, facilitating easier skill adaptation when facing non-stationary environments.

</details>


### [459] [SimpleGPT: Improving GPT via A Simple Normalization Strategy](https://arxiv.org/abs/2602.01212)
*Marco Chen,Xianbiao Qi,Yelin He,Jiaquan Ye,Rong Xiao*

Main category: cs.LG

TL;DR: 文章从二阶几何视角重新审视Transformer优化，提出SimpleNorm策略，理论分析与实验验证其可提升学习率、优化稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 重新审视Transformer优化，建立架构设计、激活规模、Hessian矩阵和最大可容忍学习率间的联系。

Method: 引入SimpleNorm策略稳定中间激活规模，分析损失关于网络激活的Hessian矩阵。

Result: SimpleGPT能容忍比标准惯例大3 - 10倍的学习率，优化稳定性强，性能优于基线模型，如7B规模模型训练60K步时训练损失比LLaMA2 with QKNorm低0.08。

Conclusion: SimpleNorm策略有效提升了Transformer的优化效果。

Abstract: In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\times$-10$\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.

</details>


### [460] [MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations](https://arxiv.org/abs/2602.01219)
*Qishuai Wen,Zhiyuan Huang,Xianghan Meng,Wei He,Chun-Guang Li*

Main category: cs.LG

TL;DR: 本文将高效注意力方法统一为缩放快速权重的框架，提出MiTA策略并构造MiTA注意力机制，初步实验显示其前景。


<details>
  <summary>Details</summary>
Motivation: Transformer中注意力机制处理长序列时快速权重缩放成本高，已有工作提出MoE注意力，本文希望构建统一框架和新策略解决此问题。

Method: 将多种高效注意力方法解释为通过路由和/或压缩缩放快速权重的统一框架，提出压缩和路由策略MiTA，压缩N宽度MLP并构造可变形专家。

Result: 在视觉任务的初步实验展示了MiTA注意力机制的潜力。

Conclusion: MiTA注意力机制有前景，值得进一步研究优化和拓展应用。

Abstract: The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weights becomes prohibitively expensive for extremely long sequences. Recently, this fast-weight scaling perspective has motivated the Mixture-of-Experts (MoE) attention, which partitions the sequence into fast-weight experts and sparsely routes the tokens to them. In this paper, we elevate this perspective to a unifying framework for a wide range of efficient attention methods by interpreting them as scaling fast weights through routing and/or compression. Then we propose a compress-and-route strategy, which compresses the $N$-width MLP into a narrower one using a small set of landmark queries and constructs deformable experts by gathering top-$k$ activated key-value pairs for each landmark query. We call this strategy a Mixture of Top-$k$ Activations (MiTA), and refer to the resulting efficient mechanism as MiTA attention. Preliminary experiments on vision tasks demonstrate the promise of our MiTA attention and motivate further investigation on its optimization and broader applications in more challenging settings.

</details>


### [461] [Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching](https://arxiv.org/abs/2602.01233)
*Tianhao Miao,Zhongyuan Bao,Lejun Zhang*

Main category: cs.LG

TL;DR: 本文提出Lotus方法解决大规模模型训练效率指标权衡问题，实验显示其能减少训练时间和内存消耗，且性能更优。


<details>
  <summary>Details</summary>
Motivation: 当前大规模模型训练方法在内存消耗、训练时间和模型性能指标间存在权衡问题，GaLore虽能实现内存高效训练但有额外训练时间成本。

Method: 提出Lotus方法，通过修改投影过程解决权衡问题，并提出量化单位梯度位移的准则以实现低秩梯度子空间间的高效转换。

Result: Lotus是最有效方法，训练时间减少30%，梯度和优化器状态内存消耗降低40%，在预训练和微调任务中表现优于基线方法。

Conclusion: Lotus方法有效解决了大规模模型训练效率指标的权衡问题。

Abstract: Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.

</details>


### [462] [Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes](https://arxiv.org/abs/2602.01247)
*Maryam Maghsoudi,Ayushi Mishra*

Main category: cs.LG

TL;DR: 本文用机械可解释性方法研究神经语音解码器内部表征，发现语音模式位于共享连续因果流形，跨模式转移由紧凑、特定层子空间介导。


<details>
  <summary>Details</summary>
Motivation: 现有脑到语音解码模型在不同语音模式下信息捕获和传递的基本机制研究较少，需深入探究。

Method: 进行跨模式激活修补、三模态插值、粗到细因果追踪和因果清理，还开展神经元级激活修补。

Result: 发现少量非分散的神经元子集影响跨模式转移，语音模式处于共享连续因果流形，跨模式转移由紧凑特定层子空间介导。

Conclusion: 对脑到语音解码模型中语音模态信息的组织和使用给出因果解释，揭示跨语音模式的分层和依赖方向的表征结构。

Abstract: Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.

</details>


### [463] [Sample Efficient Active Algorithms for Offline Reinforcement Learning](https://arxiv.org/abs/2602.01260)
*Soumyadeep Roy,Shashwat Kushwaha,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.

</details>


### [464] [BicKD: Bilateral Contrastive Knowledge Distillation](https://arxiv.org/abs/2602.01265)
*Jiangnan Zhu,Yukai Xu,Li Xiong,Yixuan Liu,Junxu Liu,Hong kyu Lee,Yujie Gu*

Main category: cs.LG

TL;DR: 该论文针对传统知识蒸馏存在的问题提出双边对比知识蒸馏（BicKD）方法，实验表明其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏仅进行样本级概率对齐，缺乏类级比较机制且未对概率空间施加结构约束。

Method: 提出双边对比知识蒸馏（BicKD）方法，引入双边对比损失，增强不同类泛化空间的正交性并保持同类一致性，可进行样本级和类级预测模式对比，正则化预测分布的几何结构。

Result: BicKD方法增强了知识转移，在各种模型架构和基准测试中始终优于现有知识蒸馏技术。

Conclusion: BicKD是一种简单有效的知识蒸馏方法，能提升知识转移效果，性能表现出色。

Abstract: Knowledge distillation (KD) is a machine learning framework that transfers knowledge from a teacher model to a student model. The vanilla KD proposed by Hinton et al. has been the dominant approach in logit-based distillation and demonstrates compelling performance. However, it only performs sample-wise probability alignment between teacher and student's predictions, lacking an mechanism for class-wise comparison. Besides, vanilla KD imposes no structural constraint on the probability space. In this work, we propose a simple yet effective methodology, bilateral contrastive knowledge distillation (BicKD). This approach introduces a novel bilateral contrastive loss, which intensifies the orthogonality among different class generalization spaces while preserving consistency within the same class. The bilateral formulation enables explicit comparison of both sample-wise and class-wise prediction patterns between teacher and student. By emphasizing probabilistic orthogonality, BicKD further regularizes the geometric structure of the predictive distribution. Extensive experiments show that our BicKD method enhances knowledge transfer, and consistently outperforms state-of-the-art knowledge distillation techniques across various model architectures and benchmarks.

</details>


### [465] [Diving into Kronecker Adapters: Component Design Matters](https://arxiv.org/abs/2602.01267)
*Jiayu Bai,Danchen Yu,Zhenyu Liao,TianQi Hou,Feng Zhou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 论文分析Kronecker适配器组件结构，提出CDKA，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有工作对Kronecker适配器组件结构的维度和数量探索不足，需分析其对适配器容量的影响。

Method: 对Kronecker组件的维度和数量进行细粒度分析，提出CDKA，给出参数预算配置指南和训练稳定策略。

Result: 在多种自然语言处理任务实验中，CDKA展现出有效性。

Conclusion: 组件结构是Kronecker适配器容量的关键因素，CDKA有良好表现。

Abstract: Kronecker adapters have emerged as a promising approach for fine-tuning large-scale models, enabling high-rank updates through tunable component structures. However, existing work largely treats the component structure as a fixed or heuristic design choice, leaving the dimensions and number of Kronecker components underexplored. In this paper, we identify component structure as a key factor governing the capacity of Kronecker adapters. We perform a fine-grained analysis of both the dimensions and number of Kronecker components. In particular, we show that the alignment between Kronecker adapters and full fine-tuning depends on component configurations. Guided by these insights, we propose Component Designed Kronecker Adapters (CDKA). We further provide parameter-budget-aware configuration guidelines and a tailored training stabilization strategy for practical deployment. Experiments across various natural language processing tasks demonstrate the effectiveness of CDKA. Code is available at https://github.com/rainstonee/CDKA.

</details>


### [466] [Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics](https://arxiv.org/abs/2602.01270)
*Boxuan Zhang,Weipu Zhang,Zhaohan Feng,Wei Xiao,Jian Sun,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: 提出Mixture - of - World Models (MoW)架构用于多任务强化学习，该架构在Atari 100k和Meta - World上表现不俗，展示了可扩展性和参数高效性。


<details>
  <summary>Details</summary>
Motivation: 解决多任务强化学习在视觉领域实现样本效率的挑战，标准整体架构难以捕捉多样任务动态。

Method: 引入MoW架构，结合模块化变分自编码器、混合Transformer动力学模型和基于梯度的任务聚类策略。

Result: 在Atari 100k上，单个MoW代理得分有竞争力且使用参数少；在Meta - World上，MoW达到新的最优平均成功率。

Conclusion: MoW为通用世界模型提供可扩展且参数高效的基础。

Abstract: A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.

</details>


### [467] [From Intents to Actions: Agentic AI in Autonomous Networks](https://arxiv.org/abs/2602.01271)
*Burak Demirel,Pablo Soldati,Yu Wang*

Main category: cs.LG

TL;DR: 为解决现有启发式方法无法将高级意图转化为具体控制动作的问题，引入了基于三个专业代理的Agentic AI系统，使网络能自主处理多样化意图和网络条件。


<details>
  <summary>Details</summary>
Motivation: 现有启发式方法无法将电信网络高级意图转化为具体控制动作，需要更有效的解决方案。

Method: 引入Agentic AI系统，包括由语言模型驱动的监督解释代理、优化器代理和基于多目标强化学习的偏好驱动控制器代理。

Result: 三个代理协同使网络能以可扩展的方式自主解释、推理、适应多种意图和网络条件。

Conclusion: 该Agentic AI系统能够有效解决电信网络将高级意图转化为具体控制动作的问题。

Abstract: Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.

</details>


### [468] [Richer Bayesian Last Layers with Subsampled NTK Features](https://arxiv.org/abs/2602.01279)
*Sergio Calvo-Ordoñez,Jonathan Plenk,Richard Bergna,Álvaro Cartea,Yarin Gal,Jose Miguel Hernández-Lobato,Kamil Ciosek*

Main category: cs.LG

TL;DR: 论文提出改进贝叶斯最后一层（BLLs）的方法，利用神经切线核（NTK）特征投影，降低计算成本，提高校准和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: BLLs仅对最后一层进行贝叶斯处理，低估了认知不确定性，需要改进。

Method: 利用NTK特征到最后一层特征空间的投影改进BLLs，引入均匀子采样方案估计投影矩阵和进行后验推断。

Result: 该方法的后验方差证明大于或等于标准BLLs，在多个任务上的实验表明能提高校准和不确定性估计，同时降低计算成本。

Conclusion: 提出的方法有效改进了BLLs，在多任务中表现良好，降低了计算成本。

Abstract: Bayesian Last Layers (BLLs) provide a convenient and computationally efficient way to estimate uncertainty in neural networks. However, they underestimate epistemic uncertainty because they apply a Bayesian treatment only to the final layer, ignoring uncertainty induced by earlier layers. We propose a method that improves BLLs by leveraging a projection of Neural Tangent Kernel (NTK) features onto the space spanned by the last-layer features. This enables posterior inference that accounts for variability of the full network while retaining the low computational cost of inference of a standard BLL. We show that our method yields posterior variances that are provably greater or equal to those of a standard BLL, correcting its tendency to underestimate epistemic uncertainty. To further reduce computational cost, we introduce a uniform subsampling scheme for estimating the projection matrix and for posterior inference. We derive approximation bounds for both types of sub-sampling. Empirical evaluations on UCI regression, contextual bandits, image classification, and out-of-distribution detection tasks in image and tabular datasets, demonstrate improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.

</details>


### [469] [EDIS: Diagnosing LLM Reasoning via Entropy Dynamics](https://arxiv.org/abs/2602.01288)
*Chenghua Zhu,Siyan Wu,Xiangkang Zeng,Zishan Xu,Zhaolu Kang,Yifu Guo,Yuquan Lu,Junduan Huang,Guojing Zhou*

Main category: cs.LG

TL;DR: 现有大语言模型置信度方法视其为静态量，本文指出置信度时间演化信息更丰富，提出 EDIS 指标，提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于熵的置信度方法将置信度视为静态量，未充分利用置信度时间演化信息。

Method: 分析 token 级熵轨迹，识别正确和错误推理的特征模式，引入 EDIS 指标量化熵演化的不稳定性。

Result: EDIS 可作为推理时间选择的有效诊断信号，大幅提高推理准确性，也为训练时间样本筛选提供方向。

Conclusion: 熵动态是理解和改进大语言模型推理的一个被忽视但有价值的视角。

Abstract: Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.

</details>


### [470] [Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models](https://arxiv.org/abs/2602.01289)
*Dung Anh Hoang,Cuong Pham anh Trung Le,Jianfei Cai,Toan Do*

Main category: cs.LG

TL;DR: 提出新的后训练量化（PTQ）方法，为校准样本分配合适权重，实验证明优于其他扩散模型PTQ方法


<details>
  <summary>Details</summary>
Motivation: 扩散模型实际部署受推理速度慢、内存使用高和噪声估计计算需求大的限制，现有PTQ方法在处理不同时间步数据时表现不佳

Method: 提出新PTQ方法，学习为校准样本分配最优权重，使量化模型在各时间步的梯度对齐

Result: 在CIFAR - 10、LSUN - Bedrooms和ImageNet上的大量实验表明该方法优于其他扩散模型PTQ方法

Conclusion: 提出的新PTQ方法有效解决现有方法问题，性能更优

Abstract: Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.

</details>


### [471] [The BoBW Algorithms for Heavy-Tailed MDPs](https://arxiv.org/abs/2602.01295)
*Yu Chen,Yuhao Liu,Jiatai Huang,Yihan Du,Longbo Huang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate episodic Markov Decision Processes with heavy-tailed feedback (HTMDPs). Existing approaches for HTMDPs are conservative in stochastic environments and lack adaptivity in adversarial regimes. In this work, we propose algorithms ```HT-FTRL-OM``` and ```HT-FTRL-UOB``` for HTMDPs that achieve Best-of-Both-Worlds (BoBW) guarantees: instance-independent regret in adversarial environments and logarithmic instance-dependent regret in self-bounding (including the stochastic case) environments. For the known transition setting, ```HT-FTRL-OM``` applies the Follow-The-Regularized-Leader (FTRL) framework over occupancy measures with novel skipping loss estimators, achieving a $\widetilde{\mathcal{O}}(T^{1/α})$ regret bound in adversarial regimes and a $\mathcal{O}(\log T)$ regret in stochastic regimes. Building upon this framework, we develop a novel algorithm ```HT-FTRL-UOB``` to tackle the more challenging unknown-transition setting. This algorithm employs a pessimistic skipping loss estimator and achieves a $\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$ regret in adversarial regimes and a $\mathcal{O}(\log^2(T))$ regret in stochastic regimes. Our analysis overcomes key barriers through several technical insights, including a local control mechanism for heavy-tailed shifted losses, a new suboptimal-mass propagation principle, and a novel regret decomposition that isolates transition uncertainty from heavy-tailed estimation errors and skipping bias.

</details>


### [472] [Dispelling the Curse of Singularities in Neural Network Optimizations](https://arxiv.org/abs/2602.01308)
*Hengjie Cao,Mengyi Chen,Yifeng Yang,Fang Dong,Ruijun Huang,Anrui Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Dongsheng Li,Wenyi Fang,Yuanyi Lin,Fan Wu,Li Shang*

Main category: cs.LG

TL;DR: 本文从参量空间奇点的出现与放大这一视角研究深度神经网络的优化不稳定性，提出PSS方法缓解此问题并提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究深度神经网络优化不稳定性，从参量空间奇点的出现与放大视角进行研究。

Method: 提出Parametric Singularity Smoothing (PSS)方法，用于平滑权重矩阵的奇异谱。

Result: 在不同数据集、架构和优化器上的实验表明，PSS能缓解不稳定性，失败后恢复可训练性，提升训练效率和泛化能力。

Conclusion: PSS是一种轻量级、灵活且有效的方法，可应对深度神经网络优化中的奇点诅咒问题。

Abstract: This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further intensify alignment with representations, leading to increased singularities in the representation space. We show that the gradient Frobenius norms are bounded by the top singular values of the weight matrices, and as training progresses, the mutually reinforcing growth of weight and representation singularities, termed the curse of singularities, relaxes these bounds, escalating the risk of sharp loss explosions. To counter this, we propose Parametric Singularity Smoothing (PSS), a lightweight, flexible, and effective method for smoothing the singular spectra of weight matrices. Extensive experiments across diverse datasets, architectures, and optimizers demonstrate that PSS mitigates instability, restores trainability even after failure, and improves both training efficiency and generalization.

</details>


### [473] [Imperfect Influence, Preserved Rankings: A Theory of TRAK for Data Attribution](https://arxiv.org/abs/2602.01312)
*Han Tong,Shubhangi Ghosh,Haolin Zou,Arian Maleki*

Main category: cs.LG

TL;DR: 本文对TRAK算法进行理论分析，虽近似有误差，但估计影响与原影响高度相关，通过模拟和实证研究验证。


<details>
  <summary>Details</summary>
Motivation: TRAK算法虽经验性能强，但理论条件及失效机制未充分探索，需要进行理论分析。

Method: 对TRAK算法进行理论分析，表征其性能并量化近似引入的误差。

Result: 近似虽有显著误差，但TRAK估计影响与原影响高度相关，很大程度保留数据点相对排名。

Conclusion: 通过理论分析和实证研究，验证了TRAK算法在数据归因中的有效性。

Abstract: Data attribution, tracing a model's prediction back to specific training data, is an important tool for interpreting sophisticated AI models. The widely used TRAK algorithm addresses this challenge by first approximating the underlying model with a kernel machine and then leveraging techniques developed for approximating the leave-one-out (ALO) risk. Despite its strong empirical performance, the theoretical conditions under which the TRAK approximations are accurate as well as the regimes in which they break down remain largely unexplored. In this paper, we provide a theoretical analysis of the TRAK algorithm, characterizing its performance and quantifying the errors introduced by the approximations on which the method relies. We show that although the approximations incur significant errors, TRAK's estimated influence remains highly correlated with the original influence and therefore largely preserves the relative ranking of data points. We corroborate our theoretical results through extensive simulations and empirical studies.

</details>


### [474] [PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding](https://arxiv.org/abs/2602.01322)
*Panagiotis Koromilas,Andreas D. Demou,James Oldfield,Yannis Panagakis,Mihalis Nicolaou*

Main category: cs.LG

TL;DR: 提出PolySAE扩展SAE解码器以捕捉特征交互，在多个模型上有性能提升，表明能捕捉组合结构。


<details>
  <summary>Details</summary>
Motivation: SAEs假设特征线性组合，无法捕捉组合结构，需要改进。

Method: 引入PolySAE，通过低秩张量分解在共享投影子空间捕捉特征交互。

Result: 在四个语言模型和三个SAE变体上，探测F1平均提升约8%，Wasserstein距离增大2 - 10倍，交互权重与共现频率相关性低。

Conclusion: 多项式项能捕捉组合结构，且与表面统计基本无关。

Abstract: Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether "Starbucks" arises from the composition of "star" and "coffee" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.

</details>


### [475] [Finding Differentially Private Second Order Stationary Points in Stochastic Minimax Optimization](https://arxiv.org/abs/2602.01339)
*Difei Xu,Youming Tao,Meng Ding,Chenglin Fan,Di Wang*

Main category: cs.LG

TL;DR: 首次研究随机（非凸）极小极大优化中寻找差分隐私二阶驻点问题，提出一阶方法并给出相关收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有文献要么仅关注极小极大问题的一阶驻点，要么关注经典随机最小化问题的二阶驻点，缺乏对经验风险和总体风险的统一详细处理。

Method: 提出结合嵌套梯度下降 - 上升方案、SPIDER 风格方差缩减和高斯扰动的一阶方法，采用块式（q 周期）分析。

Result: 在标准假设下，给出经验风险目标和总体目标达到近似二阶驻点的高概率保证，收敛率与已知最佳隐私一阶平稳率匹配。

Conclusion: 成功解决随机（非凸）极小极大优化中差分隐私二阶驻点问题，统一处理了经验风险和总体风险。

Abstract: We provide the first study of the problem of finding differentially private (DP) second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization. Existing literature either focuses only on first-order stationary points for minimax problems or on SOSP for classical stochastic minimization problems. This work provides, for the first time, a unified and detailed treatment of both empirical and population risks. Specifically, we propose a purely first-order method that combines a nested gradient descent--ascent scheme with SPIDER-style variance reduction and Gaussian perturbations to ensure privacy. A key technical device is a block-wise ($q$-period) analysis that controls the accumulation of stochastic variance and privacy noise without summing over the full iteration horizon, yielding a unified treatment of both empirical-risk and population formulations. Under standard smoothness, Hessian-Lipschitzness, and strong concavity assumptions, we establish high-probability guarantees for reaching an $(α,\sqrt{ρ_Φα})$-approximate second-order stationary point with $α= \mathcal{O}( (\frac{\sqrt{d}}{n\varepsilon})^{2/3})$ for empirical risk objectives and $\mathcal{O}(\frac{1}{n^{1/3}} + (\frac{\sqrt{d}}{n\varepsilon})^{1/2})$ for population objectives, matching the best known rates for private first-order stationarity.

</details>


### [476] [Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning](https://arxiv.org/abs/2602.01357)
*Shangzhe Li,Xuchao Zhang,Chetan Bansal,Weitong Zhang*

Main category: cs.LG

TL;DR: 本文将自博弈微调与对抗模仿学习相联系，进行博弈论分析证明收敛性，提出新的自博弈模仿微调算法，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自博弈微调方法缺乏理论基础，本文旨在探索其理论依据。

Method: 将微调过程建模为模型与由模型本身参数化的正则化隐式奖励玩家之间的最小 - 最大博弈，进行博弈论分析，基于$χ^2$-散度变分目标提出新算法。

Result: 实验表明新算法在各种语言模型微调任务中比现有自博弈方法有一致的改进。

Conclusion: 本文提出的理论框架和新算法有效，统一了自博弈模仿和一般偏好对齐，具有理论和实践价值。

Abstract: Self-play post-training methods has emerged as an effective approach for finetuning large language models and turn the weak language model into strong language model without preference data. However, the theoretical foundations for self-play finetuning remain underexplored. In this work, we tackle this by connecting self-play finetuning with adversarial imitation learning by formulating finetuning procedure as a min-max game between the model and a regularized implicit reward player parameterized by the model itself. This perspective unifies self-play imitation and general preference alignment within a common framework. Under this formulation, we present a game-theoretic analysis showing that the self-play finetuning will converge to it's equilibrium. Guided by this theoretical formulation, we propose a new self-play imitation finetuning algorithm based on the $χ^2$-divergence variational objective with bounded rewards and improved stability. Experiments on various of language model finetuning tasks demonstrate consistent improvements over existing self-play methods and validate our theoretical insights.

</details>


### [477] [PaAno: Patch-Based Representation Learning for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.01359)
*Jinju Park,Seokho Kang*

Main category: cs.LG

TL;DR: 现有时间序列异常检测方法计算成本高、内存占用大且性能提升不显著，本文提出轻量级方法PaAno，在TSB - AD基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测采用的大型神经网络架构计算成本高、内存占用大，在实时和资源受限场景不实用，且在严格评估下相比简单方法无显著性能提升。

Method: 从时间序列训练数据中提取短时间补丁，用1D卷积神经网络将每个补丁嵌入向量表示，结合三元组损失和前置损失训练模型；推理时，通过比较周围补丁和训练集中正常补丁的嵌入来计算异常分数。

Result: 在TSB - AD基准测试中，PaAno在单变量和多变量时间序列异常检测的多种范围和点级性能指标上，显著优于现有方法，包括基于重型架构的方法，达到了先进水平。

Conclusion: PaAno是一种轻量级且有效的时间序列异常检测方法，能在资源受限场景中快速高效地检测异常。

Abstract: Although recent studies on time-series anomaly detection have increasingly adopted ever-larger neural network architectures such as transformers and foundation models, they incur high computational costs and memory usage, making them impractical for real-time and resource-constrained scenarios. Moreover, they often fail to demonstrate significant performance gains over simpler methods under rigorous evaluation protocols. In this study, we propose Patch-based representation learning for time-series Anomaly detection (PaAno), a lightweight yet effective method for fast and efficient time-series anomaly detection. PaAno extracts short temporal patches from time-series training data and uses a 1D convolutional neural network to embed each patch into a vector representation. The model is trained using a combination of triplet loss and pretext loss to ensure the embeddings capture informative temporal patterns from input patches. During inference, the anomaly score at each time step is computed by comparing the embeddings of its surrounding patches to those of normal patches extracted from the training time-series. Evaluated on the TSB-AD benchmark, PaAno achieved state-of-the-art performance, significantly outperforming existing methods, including those based on heavy architectures, on both univariate and multivariate time-series anomaly detection across various range-wise and point-wise performance measures.

</details>


### [478] [When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning](https://arxiv.org/abs/2602.01365)
*Wang Yang,Shouren Wang,Chaoda Song,Chuang Ma,Xinpeng Li,Nengbo Wang,Kaixiong Zhou,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: 本文对GRPO在多领域训练顺序效应进行了系统分析，发现其在多领域设置中表现出不对称性、顺序敏感性和策略依赖性。


<details>
  <summary>Details</summary>
Motivation: GRPO在不同领域排序策略下的行为缺乏研究，尤其是顺序训练与混合领域训练的影响未被系统探究。

Method: 对数学、科学、逻辑和谜题推理任务进行训练顺序效应的系统分析。

Result: 1. 单领域泛化高度不对称；2. 跨领域交互高度依赖顺序；3. 多领域训练无通用最优策略。

Conclusion: GRPO在多领域设置中表现出明显的不对称性、顺序敏感性和策略依赖性，强调了领域和顺序感知训练设计的必要性。

Abstract: Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\rightarrow$science achieves 83\% / 41\% accuracy on math / science, while reversing the order to science$\rightarrow$math degrades performance to 77\% / 25\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\% to 56\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.

</details>


### [479] [Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation](https://arxiv.org/abs/2602.01367)
*Pinar Erbil,Alberto Archetti,Eugenio Lomurno,Matteo Matteucci*

Main category: cs.LG

TL;DR: 提出CONVERSE模型解决深度学习在生存分析中性能与可解释性的权衡问题，该模型在四个基准数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生存分析中存在性能与可解释性的权衡问题，现有方法难以兼顾二者，需开发新模型。

Method: 提出CONVERSE模型，将变分自编码器与对比学习相结合，结合多种对比损失、采用自定步长学习，支持特定聚类生存头。

Result: 在四个基准数据集上的综合评估显示，CONVERSE与现有深度生存分析方法相比取得了有竞争力或更优的性能。

Conclusion: CONVERSE既能实现有意义的患者分层，又能有较好的性能表现，可有效弥合深度学习在生存分析中性能和可解释性之间的差距。

Abstract: Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.

</details>


### [480] [SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training](https://arxiv.org/abs/2602.01410)
*Yunjie Pan,Yongyi Yang,Hanmei Yang,Scott Mahlke*

Main category: cs.LG

TL;DR: 本文介绍了支持亚字节精度的细粒度自适应混合精度训练框架SNIP，通过收集统计信息和定义关键指标优化层精度，实验表明其能在保持模型质量前提下减少计算量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型混合精度训练方法存在不足，如统一精度或启发式方法导致收敛不佳和不稳定。

Method: 引入SNIP框架，定期收集激活、梯度和优化器状态统计信息，定义前向损失发散和后向权重发散两个指标，通过整数线性规划问题优化层精度。

Result: 在1B、3B、7B和70B类Llama模型实验中，SNIP始终优于现有基线，最高减少80%的浮点运算，在不同模型大小和训练阶段保持模型质量，计算开销小。

Conclusion: SNIP是一种有效的大语言模型预训练的细粒度自适应混合精度训练框架。

Abstract: Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.

</details>


### [481] [Semi-supervised CAPP Transformer Learning via Pseudo-labeling](https://arxiv.org/abs/2602.01419)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb,Emmanuel Stathatos,Panorios Benardos,George-Christopher Vosniakos*

Main category: cs.LG

TL;DR: 提出半监督学习方法改进基于变压器的CAPP模型，在小规模数据集实验中比基线方法有精度提升。


<details>
  <summary>Details</summary>
Motivation: 工业中CAPP数据集有限，导致模型泛化能力降低。

Method: 提出半监督学习方法，利用训练好的oracle过滤未见过零件的正确预测，用于一次性再训练。

Result: 在全数据分布的小规模模拟数据集实验中，比基线方法有持续的精度提升。

Conclusion: 该方法在数据稀缺的制造环境中有效。

Abstract: High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.

</details>


### [482] [Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models](https://arxiv.org/abs/2602.01428)
*Weiqing He,Xiang Li,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 本文研究大语言模型输出水印与推测采样的权衡，提出新机制实现水印强度和推测采样效率兼得。


<details>
  <summary>Details</summary>
Motivation: 水印方法在实践中因推理效率低难以部署，且水印强度与推测采样接受率存在权衡，需解决此问题。

Method: 引入水印强度定量度量，将权衡问题转化为约束优化问题，推导帕累托曲线，提出向草稿令牌接受中注入伪随机性的机制。

Result: 实验表明该方法在不牺牲效率的情况下提高了水印可检测性。

Conclusion: 发现了统一推测采样和水印的原则，为其高效实际部署铺平道路。

Abstract: Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.

</details>


### [483] [Phase Transitions for Feature Learning in Neural Networks](https://arxiv.org/abs/2602.01434)
*Andrea Montanari,Zihao Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: According to a popular viewpoint, neural networks learn from data by first identifying low-dimensional representations, and subsequently fitting the best model in this space. Recent works provide a formalization of this phenomenon when learning multi-index models. In this setting, we are given $n$ i.i.d. pairs $({\boldsymbol x}_i,y_i)$, where the covariate vectors ${\boldsymbol x}_i\in\mathbb{R}^d$ are isotropic, and responses $y_i$ only depend on ${\boldsymbol x}_i$ through a $k$-dimensional projection ${\boldsymbol Θ}_*^{\sf T}{\boldsymbol x}_i$. Feature learning amounts to learning the latent space spanned by ${\boldsymbol Θ}_*$.
  In this context, we study the gradient descent dynamics of two-layer neural networks under the proportional asymptotics $n,d\to\infty$, $n/d\toδ$, while the dimension of the latent space $k$ and the number of hidden neurons $m$ are kept fixed. Earlier work establishes that feature learning via polynomial-time algorithms is possible if $δ> δ_{\text{alg}}$, for $δ_{\text{alg}}$ a threshold depending on the data distribution, and is impossible (within a certain class of algorithms) below $δ_{\text{alg}}$. Here we derive an analogous threshold $δ_{\text{NN}}$ for two-layer networks. Our characterization of $δ_{\text{NN}}$ opens the way to study the dependence of learning dynamics on the network architecture and training algorithm.
  The threshold $δ_{\text{NN}}$ is determined by the following scenario. Training first visits points for which the gradient of the empirical risk is large and learns the directions spanned by these gradients. Then the gradient becomes smaller and the dynamics becomes dominated by negative directions of the Hessian. The threshold $δ_{\text{NN}}$ corresponds to a phase transition in the spectrum of the Hessian in this second phase.

</details>


### [484] [TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse](https://arxiv.org/abs/2602.01439)
*Perry Dong,Kuo-Han Hung,Alexander Swerdlow,Dorsa Sadigh,Chelsea Finn*

Main category: cs.LG

TL;DR: 本文探究变压器在强化学习价值函数中难以有效扩展的原因，提出TQL方法解决问题并提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中价值函数规模小，直接扩展变压器架构的价值函数会导致学习不稳定和性能下降，需找出变压器无法有效扩展的原因。

Method: 通过实证分析找出扩展中的关键失败模式，提出Transformer Q - Learning (TQL)方法，控制注意力分数的熵来防止分数崩溃和稳定训练。

Result: 从最小到最大网络规模扩展时，该方法性能提升达43%，而之前方法性能下降。

Conclusion: TQL方法能释放变压器在强化学习价值函数学习中的扩展潜力。

Abstract: Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.

</details>


### [485] [The Gradient-Causal Gap: Why Gradient Importance Fails on Complex Tasks](https://arxiv.org/abs/2602.01442)
*Donald Ye*

Main category: cs.LG

TL;DR: 研究Transformer在算法任务中梯度与泛化能力关系，指出梯度裁剪不可靠。


<details>
  <summary>Details</summary>
Motivation: 探究去除神经网络中高低梯度组件对泛化能力的影响。

Method: 形式化梯度因果差距，在不同复杂度算法任务上训练Transformer，进行剪枝实验。

Result: 任务复杂度增加时梯度大小与因果重要性关系崩塌；低梯度裁剪严重破坏OOD准确率，高梯度裁剪效果不稳定。

Conclusion: 基于梯度的剪枝不能可靠保留模型能力。

Abstract: Removing ''important'' high-gradient components from a neural network can improve generalization, while removing unimportant'' low-gradient components can destroy it. We demonstrate this paradox by formalizing the \textit{Gradient-Causal Gap} in Transformers trained on algorithmic tasks. While gradient magnitude and causal importance align on simple tasks ($ρ=0.73$ for reversal), this relationship collapses as task complexity increases ($ρ=0.32$ for sorting), sometimes becoming inverted ($ρ=-0.11$). Pruning experiments reveal that gradient magnitude is not merely inaccurate but \textit{unpredictably} so. Removing low-gradient ''Hidden Heroes'' consistently devastates OOD accuracy ($-32\%$). Removing high-gradient ''Gradient Bloats'' is a coin flip: harmless in most seeds (indicating optimization noise), catastrophic in others (indicating overfitting circuits). This unpredictability means gradient-based pruning cannot reliably preserve model capabilities.

</details>


### [486] [A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting](https://arxiv.org/abs/2602.01445)
*Ons Saadallah,Mátyás andó,Tamás Gábor Orosz*

Main category: cs.LG

TL;DR: 提出结合贝叶斯优化和大语言模型上下文推理的混合超参数优化框架LLM - AutoOpt，在多变量时间序列预测基准测试中表现更好且优化行为更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 超参数优化在深度学习模型性能中至关重要，但计算成本高且难解释，尤其是时间序列预测，传统贝叶斯优化存在局限性，而大语言模型的发展带来新机遇。

Method: 引入LLM - AutoOpt框架，将数据集元特征、模型描述、历史优化结果和目标目标编码为元知识放入大语言模型提示中，用贝叶斯优化初始化搜索并减轻冷启动影响。

Result: 在多变量时间序列预测基准测试中，LLM - AutoOpt比没有元知识的贝叶斯优化和大语言模型基线取得更好的预测性能，且优化行为更具可解释性。

Conclusion: LLM - AutoOpt是一种有效的超参数优化框架，能提升预测性能并使优化行为更易解释。

Abstract: Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.

</details>


### [487] [Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs](https://arxiv.org/abs/2602.01453)
*Idan Barnea,Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 研究奖励无关探索下的合作多智能体强化学习，采用分阶段学习框架，刻画学习阶段数与智能体数量的权衡，给出算法及下界。


<details>
  <summary>Details</summary>
Motivation: 在奖励无关探索场景下，让多智能体联合探索未知MDP以学习其动态，研究学习阶段数和智能体数量的权衡。

Method: 采用分阶段学习框架，每个学习阶段中多个智能体独立与环境交互，为每个智能体分配策略并执行。

Result: 当学习阶段数等于H时，给出计算高效算法，用约O(S^6 H^6 A / ε^2)个智能体获得动态的ε近似；证明少于H个阶段的算法至少需要A^(H/ρ)个智能体达到恒定精度。

Conclusion: 若将智能体数量限制为多项式，学习阶段数达到H是必要的。

Abstract: We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.
  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\tilde{O}(S^6 H^6 A / ε^2)$ agents to obtain an $ε$ approximation of the dynamics (i.e., yields an $ε$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $ρ< H$ phases requires at least $A^{H/ρ}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.

</details>


### [488] [Modeling Topological Impact on Node Attribute Distributions in Attributed Graphs](https://arxiv.org/abs/2602.01454)
*Amirreza Shiralinasab Langari,Leila Yeganeh,Kim Khoa Nguyen*

Main category: cs.LG

TL;DR: 研究属性图拓扑对节点属性分布的影响，引入代数方法结合拓扑与属性分布，建立条件，通过简单模型评估。


<details>
  <summary>Details</summary>
Motivation: 探索属性图拓扑如何影响节点属性分布，提供新视角。

Method: 引入代数方法，结合图拓扑和节点属性概率分布；开发分类框架量化节点对拓扑的感知，与属性分布结合；建立充分条件；用简易模型和无监督图异常检测任务评估。

Result: 构建了拓扑影响的分布，在完全图上恢复原始属性分布。

Conclusion: 所提出的结合拓扑与属性分布的方法在理论上有合理的表现，并可用于实际的图异常检测任务中。

Abstract: We investigate how the topology of attributed graphs influences the distribution of node attributes. This work offers a novel perspective by treating topology and attributes as structurally distinct but interacting components. We introduce an algebraic approach that combines a graph's topology with the probability distribution of node attributes, resulting in topology-influenced distributions. First, we develop a categorical framework to formalize how a node perceives the graph's topology. We then quantify this point of view and integrate it with the distribution of node attributes to capture topological effects. We interpret these topology-conditioned distributions as approximations of the posteriors $P(\cdot \mid v)$ and $P(\cdot \mid \mathcal{G})$.
  We further establish a principled sufficiency condition by showing that, on complete graphs, where topology carries no informative structure, our construction recovers the original attribute distribution. To evaluate our approach, we introduce an intentionally simple testbed model, $\textbf{ID}$, and use unsupervised graph anomaly detection as a probing task.

</details>


### [489] [Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations](https://arxiv.org/abs/2602.01456)
*Yilun Kuang,Yash Dagade,Tim G. J. Rudner,Randall Balestriero,Yann LeCun*

Main category: cs.LG

TL;DR: 引入RDMReg解决JEPA无法捕获稀疏性问题，得到Rectified LpJEPA，在图像分类有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法倾向于密集表示，无法捕获有效表示中的稀疏性关键特性。

Method: 引入RDMReg，一种切片两样本分布匹配损失，使表示与RGG分布对齐，得到Rectified LpJEPA。

Result: Rectified LpJEPA学习到稀疏、非负表示，在稀疏性 - 性能权衡和下游图像分类基准测试中有良好表现。

Conclusion: RDMReg能有效强制稀疏性，同时保留与任务相关的信息。

Abstract: Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.

</details>


### [490] [P-EAGLE: Parallel-Drafting EAGLE with Scalable Training](https://arxiv.org/abs/2602.01469)
*Mude Hui,Xin Huang,Jaime Campos Salas,Yue Sun,Nathan Pemberton,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: 论文提出P - EAGLE方法解决并行起草在长上下文训练中的问题，并实现速度提升


<details>
  <summary>Details</summary>
Motivation: 推理大语言模型输出长，并行起草训练复杂度高，长上下文训练不实际

Method: 通过可学习共享隐藏状态将EAGLE转换为并行多令牌预测，开发含注意力掩码预计算和序列划分技术的框架

Result: 在vLLM中实现P - EAGLE，在不同模型上比自回归EAGLE - 3有1.10 - 1.36倍的速度提升

Conclusion: P - EAGLE方法在解决并行起草长上下文训练问题上有效，能带来速度提升

Abstract: Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.

</details>


### [491] [Causal Preference Elicitation](https://arxiv.org/abs/2602.01483)
*Edwin V. Bonilla,He Zhao,Daniel M. Steinberg*

Main category: cs.LG

TL;DR: 提出因果偏好引出的贝叶斯框架用于专家参与的因果发现，实验显示在查询预算有限时效果更好。


<details>
  <summary>Details</summary>
Motivation: 进行专家参与的因果发现，解决如何有效利用专家判断的问题。

Method: 提出因果偏好引出的贝叶斯框架，用三向似然模型模拟专家判断，用灵活粒子近似进行后验推断，通过有效预期信息增益准则选择查询。

Result: 在合成图、蛋白质信号数据和人类基因扰动基准实验中，后验集中更快，在有限查询预算下对有向效应的恢复能力提高。

Conclusion: 所提出的因果偏好引出框架在专家参与的因果发现中有效，能在有限查询预算下取得更好效果。

Abstract: We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-way likelihood over edge existence and direction. Posterior inference uses a flexible particle approximation, and queries are selected by an efficient expected information gain criterion on the expert's categorical response. Experiments on synthetic graphs, protein signaling data, and a human gene perturbation benchmark show faster posterior concentration and improved recovery of directed effects under tight query budgets.

</details>


### [492] [Multi-Scale Wavelet Transformers for Operator Learning of Dynamical Systems](https://arxiv.org/abs/2602.01486)
*Xuesong Wang,Michael Groom,Rafael Oliveira,He Zhao,Terence O'Kane,Edwin V. Bonilla*

Main category: cs.LG

TL;DR: 数据驱动模型有光谱偏差，本文提出多尺度小波变换器MSWTs，在混沌动力系统实验和真实气候数据中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的模型如神经算子存在光谱偏差，会削弱高频成分，在气象预报等应用中会导致长周期不稳定，需解决该问题。

Method: 提出多尺度小波变换器（MSWTs），在标记化小波域学习系统动力学，采用保留小波的下采样方案保留高频特征，并使用基于小波的注意力机制捕捉跨尺度和频带的依赖关系。

Result: 在混沌动力系统实验中，大幅降低了误差，提高了长周期光谱保真度；在ERA5气候再分析中，进一步降低了气候偏差。

Conclusion: MSWTs在处理光谱偏差问题上有效，在真实预测场景中具有有效性。

Abstract: Recent years have seen a surge in data-driven surrogates for dynamical systems that can be orders of magnitude faster than numerical solvers. However, many machine learning-based models such as neural operators exhibit spectral bias, attenuating high-frequency components that often encode small-scale structure. This limitation is particularly damaging in applications such as weather forecasting, where misrepresented high frequencies can induce long-horizon instability. To address this issue, we propose multi-scale wavelet transformers (MSWTs), which learn system dynamics in a tokenized wavelet domain. The wavelet transform explicitly separates low- and high-frequency content across scales. MSWTs leverage a wavelet-preserving downsampling scheme that retains high-frequency features and employ wavelet-based attention to capture dependencies across scales and frequency bands. Experiments on chaotic dynamical systems show substantial error reductions and improved long horizon spectral fidelity. On the ERA5 climate reanalysis, MSWTs further reduce climatological bias, demonstrating their effectiveness in a real-world forecasting setting.

</details>


### [493] [OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference](https://arxiv.org/abs/2602.01493)
*Zhuoyuan Wang,Hanjiang Hu,Xiyu Deng,Saviz Mowlavi,Yorie Nakahira*

Main category: cs.LG

TL;DR: 提出基于算子推理的LLM参数化PDE求解框架OpInf-LLM，能准确预测不同PDE实例，提高执行成功率。


<details>
  <summary>Details</summary>
Motivation: 解决使用大语言模型（LLM）在跨异构环境下可靠求解偏微分方程（PDE）时，执行成功率和数值准确性难以平衡的问题。

Method: 提出基于算子推理的LLM参数化PDE求解框架OpInf-LLM，利用少量解数据并与LLM无缝集成。

Result: 该框架能准确预测不同PDE实例，包括未见参数和配置，在异构环境下有高执行成功率。

Conclusion: OpInf-LLM结合算子推理和LLM能力，为基于LLM的PDE求解中的可泛化降阶建模开辟了新可能。

Abstract: Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.

</details>


### [494] [White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC](https://arxiv.org/abs/2602.01516)
*Enzo Nicolas Spotorno,Matheus Wagner,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

TL;DR: 提出白盒自适应NMPC架构解决车辆可塑性问题，验证其适应性和跟踪精度，量化透明性成本。


<details>
  <summary>Details</summary>
Motivation: 解决车辆可塑性问题，即无需重新训练适应不同运行状态。

Method: 采用模块化主权范式在冻结的、特定状态的神经专家之间进行仲裁，在CasADi中维护集成动力学为可完全遍历的符号图。

Result: 同步仿真验证了在复合状态变化下的快速适应（约7.3毫秒）和接近理想的跟踪精度，非自适应基线失败；经验基准量化了透明性成本，符号图维护使求解器延迟增加72 - 102倍。

Conclusion: 严格的白盒实现有其效率代价。

Abstract: We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully traversable symbolic graph in CasADi, enabling maximal runtime auditability. Synchronous simulation validates rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts (friction, mass, drag) where non-adaptive baselines fail. Empirical benchmarking quantifies the transparency cost: symbolic graph maintenance increases solver latency by 72-102X versus compiled parametric physics models, establishing the efficiency price of strict white-box implementation.

</details>


### [495] [You Need an Encoder for Native Position-Independent Caching](https://arxiv.org/abs/2602.01519)
*Shiju Zhao,Junhao Hu,Jiaqi Zheng,Guihai Chen*

Main category: cs.LG

TL;DR: 提出native PIC及COMB缓存系统，减少推理时间和提高吞吐量，可用于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有KV cache处理任意顺序上下文效率低，PIC方法会导致模型精度下降。

Method: 引入编码器到解码器LLM并训练以支持PIC，开发PIC感知的缓存系统COMB。

Result: COMB减少TTFT 51 - 94%，吞吐量提升3倍，精度相当。

Conclusion: COMB可提升LLM推理效率，适用于其他解码器LLM。

Abstract: The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.

</details>


### [496] [When Is Rank-1 Enough? Geometry-Guided Initialization for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.01522)
*Haoran Zhao,Soyeon Caren Han,Eduard Hovy*

Main category: cs.LG

TL;DR: 研究指出超低保秩设置训练不稳定原因，提出 Gap - Init 方法稳定训练，表明极端低秩时初始对齐很重要。


<details>
  <summary>Details</summary>
Motivation: 解决参数高效微调（PEFT）中极端低秩设置（如秩 - 1 LoRA）训练不稳定的问题。

Method: 分析预训练表示，确定主导早期梯度流的模态间隙轴，提出 Gap - Init 方法，使秩 - 1 LoRA 方向与模态间隙向量对齐。

Result: 在多个视觉 - 语言任务和骨干网络中，Gap - Init 稳定了秩 - 1 训练，性能可媲美或超越强秩 - 8 基线。

Conclusion: 在极端低秩极限下，初始对齐与秩本身同样重要。

Abstract: Parameter-efficient fine-tuning (PEFT) is a standard way to adapt multimodal large language models, yet extremely low-rank settings -- especially rank-1 LoRA -- are often unstable. We show that this instability is not solely due to limited capacity: in the rank-1 regime, optimization is highly sensitive to the update direction. Concretely, pretrained vision and text features form mismatched anisotropic regions, yielding a dominant "gap" direction that acts like a translation component and disproportionately steers early gradients under rank-1 constraints. Analyzing pretrained representations, we identify a modality-gap axis that dominates early gradient flow, while a random rank-1 initialization is unlikely to align with it, leading to weak gradients and training collapse. We propose Gap-Init, a geometry-aware initialization that aligns the rank-1 LoRA direction with an estimated modality-gap vector from a small calibration set, while keeping the initial LoRA update zero. Across multiple vision-language tasks and backbones, Gap-Init consistently stabilizes rank-1 training and can match or outperform strong rank-8 baselines. Our results suggest that at the extreme low-rank limit, initial alignment can matter as much as rank itself.

</details>


### [497] [A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning](https://arxiv.org/abs/2602.01523)
*Akifumi Wachi,Hirota Kinoshita,Shokichi Takakura,Rei Higuchi,Taiji Suzuki*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \emph{relative-budget} theory explaining this variation through a single quantity called relative budget $ξ:= H/\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $ξ$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \emph{deficient} regime ($ξ\to 0$), informative trajectories are rare and the sample complexity explodes; in the \emph{balanced} regime ($ξ=Θ(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \emph{ample} regime ($ξ\to \infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $ξ\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.

</details>


### [498] [The Inlet Rank Collapse in Implicit Neural Representations: Diagnosis and Unified Remedy](https://arxiv.org/abs/2602.01526)
*Jianqiao Zheng,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: 本文提出结构诊断框架，揭示输入秩崩溃现象，将经验技术解释为秩恢复形式，并推导秩扩展初始化方法提升INRs性能。


<details>
  <summary>Details</summary>
Motivation: Implicit Neural Representations (INRs)在有限训练预算下难以恢复细粒度细节，现有经验技术理论解释多为事后分析。

Method: 引入结构诊断框架，对NTK进行逐层分解，识别“输入秩崩溃”现象，推导Rank - Expanding Initialization。

Result: 所提出的Rank - Expanding Initialization使标准MLP实现高保真重建。

Conclusion: 提升INRs的关键在于初始秩传播的结构优化以有效填充潜在空间。

Abstract: Implicit Neural Representations (INRs) have revolutionized continuous signal modeling, yet they struggle to recover fine-grained details within finite training budgets. While empirical techniques, such as positional encoding (PE), sinusoidal activations (SIREN), and batch normalization (BN), effectively mitigate this, their theoretical justifications are predominantly post hoc, focusing on the global NTK spectrum only after modifications are applied. In this work, we reverse this paradigm by introducing a structural diagnostic framework. By performing a layer-wise decomposition of the NTK, we mathematically identify the ``Inlet Rank Collapse'': a phenomenon where the low-dimensional input coordinates fail to span the high-dimensional embedding space, creating a fundamental rank deficiency at the first layer that acts as an expressive bottleneck for the entire network. This framework provides a unified perspective to re-interpret PE, SIREN, and BN as different forms of rank restoration. Guided by this diagnosis, we derive a Rank-Expanding Initialization, a minimalist remedy that ensures the representation rank scales with the layer width without architectural modifications or computational overhead. Our results demonstrate that this principled remedy enables standard MLPs to achieve high-fidelity reconstructions, proving that the key to empowering INRs lies in the structural optimization of the initial rank propagation to effectively populate the latent space.

</details>


### [499] [Plain Transformers are Surprisingly Powerful Link Predictors](https://arxiv.org/abs/2602.01553)
*Quang Truong,Yu Song,Donald Loveland,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: 提出PENCIL用于链接预测，实验表明其优于GNN等方法，简单设计也能实现强大能力。


<details>
  <summary>Details</summary>
Motivation: 现有链接预测方法如GNN和GT存在泛化、扩展及开销问题，需要更好的解决方案。

Method: 提出仅编码器的PENCIL，用对采样局部子图的注意力替换手工先验。

Result: PENCIL提取结构信号比GNN丰富，优于启发式GNN，参数效率高，在多基准测试有竞争力。

Conclusion: 挑战依赖复杂工程技术的现状，简单设计足以实现相同能力。

Abstract: Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embeddings -- approaches that struggle to generalize or scale to massive graphs. Emerging Graph Transformers (GTs) offer a potential alternative but often incur significant overhead due to complex structural encodings, hindering their applications to large-scale link prediction. We challenge these sophisticated paradigms with PENCIL, an encoder-only plain Transformer that replaces hand-crafted priors with attention over sampled local subgraphs, retaining the scalability and hardware efficiency of standard Transformers. Through experimental and theoretical analysis, we show that PENCIL extracts richer structural signals than GNNs, implicitly generalizing a broad class of heuristics and subgraph-based expressivity. Empirically, PENCIL outperforms heuristic-informed GNNs and is far more parameter-efficient than ID-embedding--based alternatives, while remaining competitive across diverse benchmarks -- even without node features. Our results challenge the prevailing reliance on complex engineering techniques, demonstrating that simple design choices are potentially sufficient to achieve the same capabilities.

</details>


### [500] [InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs](https://arxiv.org/abs/2602.01554)
*Lv Tang,Tianyi Zheng,Bo Li,Xingyu Li*

Main category: cs.LG

TL;DR: 本文针对统一多模态大语言模型视觉标记器缺乏有效准则的问题，提出基于信息瓶颈原则的InfoTok机制，实验证明能提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有共享标记设计多为架构驱动，缺乏明确准则来确定标记应保留何种信息以支持理解和生成任务。

Method: 引入容量受限视角，提出基于信息瓶颈原则的信息正则化视觉标记机制InfoTok，将标记化过程视为控制信息流并通过互信息正则化实现压缩与任务相关性的权衡。

Result: 将InfoTok集成到三个代表性统一多模态大语言模型中，在理解和生成任务上均实现了持续改进。

Conclusion: 信息正则化标记化可作为统一多模态大语言模型中学习共享标记空间的有效原则基础。

Abstract: Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven and lack an explicit criterion for what information tokens should preserve to support both understanding and generation. Therefore, we introduce a capacity-constrained perspective, highlighting that in shared-token unified MLLMs the visual tokenizer behaves as a compute-bounded learner, so the token budget should prioritize reusable structure over hard-to-exploit high-entropy variations and redundancy. Motivated by this perspective, we propose InfoTok, an information-regularized visual tokenization mechanism grounded in the Information Bottleneck (IB) principle. InfoTok formulates tokenization as controlling information flow from images to shared tokens to multimodal outputs, yielding a principled trade-off between compression and task relevance via mutual-information regularization. We integrate InfoTok into three representative unified MLLMs without introducing any additional training data. Experiments show consistent improvements on both understanding and generation, supporting information-regularized tokenization as a principled foundation for learning a shared token space in unified MLLMs.

</details>


### [501] [How Implicit Bias Accumulates and Propagates in LLM Long-term Memory](https://arxiv.org/abs/2602.01558)
*Yiming Ma,Lixu Wang,Lionel Z. Wang,Hongkun Yang,Haoming Sun,Xin Xu,Jiaqi Wu,Bin Chen,Wei Dong*

Main category: cs.LG

TL;DR: 研究带长期记忆的大语言模型中隐式偏差的累积和传播，引入DIB基准测试，评估模型，提出DMT策略减少偏差。


<details>
  <summary>Details</summary>
Motivation: 长期记忆机制使大语言模型在交互生命周期中保持连续性和个性化，但带来了公平性方面未充分探索的新风险，研究隐式偏差在其中的累积和传播。

Method: 引入DIB基准测试，使用长周期模拟框架评估6个先进大语言模型和3种代表性记忆架构，分析缓解策略并提出DMT策略。

Result: 大语言模型的隐式偏差随时间加剧且跨无关领域传播，静态系统级提示基线去偏效果有限且短暂，DMT大幅减少偏差累积和跨领域偏差传播。

Conclusion: DMT作为一种代理干预措施，能在记忆写入时强制执行公平性约束，有效解决大语言模型长期记忆中的偏差问题。

Abstract: Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.

</details>


### [502] [Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space](https://arxiv.org/abs/2602.01564)
*Geuntaek Seo,Minseop Shin,Pierre Monmarché,Beomjun Choi*

Main category: cs.LG

TL;DR: 研究平均场Langevin下降 - 上升（MFL - DA）动态，证明对非凸 - 非凹收益的MFL - DA相关均衡是局部指数稳定的。


<details>
  <summary>Details</summary>
Motivation: 原MFL - DA对一般非凸 - 非凹收益的长期行为仍不清楚，要解决Wang和Chizat提出的开放性问题。

Method: 通过线性化算子的谱分析，在平衡附近为熵建立强制估计，揭示局部位移凸 - 凹结构。

Result: 如果初始化在Wasserstein度量下足够接近，动力学将以指数速率趋向于平衡。

Conclusion: 解决了Wang和Chizat提出的局部稳定性和量化速率问题，全局收敛仍是开放性挑战。

Abstract: We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.

</details>


### [503] [Generative Visual Code Mobile World Models](https://arxiv.org/abs/2602.01576)
*Woosung Koh,Sungjun Han,Segyu Lee,Se-Young Yun,Jamin Shin*

Main category: cs.LG

TL;DR: 提出通过可渲染代码生成进行视觉世界建模的新范式，构建gWorld模型，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前移动GUI世界模型存在权衡问题，文本WM牺牲视觉保真度，视觉WM精确文本渲染能力不足且依赖复杂管道。

Method: 提出通过可渲染代码生成进行视觉世界建模的范式，构建gWorld模型及数据生成框架。

Result: gWorld在多个基准测试中在准确率和模型大小上达到新的帕累托前沿，优于大50.25倍的模型，且扩大训练数据有收益，管道各组件提升数据质量，更好的世界建模提升下游策略性能。

Conclusion: 所提出的新范式和gWorld模型有效，能提升移动GUI代理性能。

Abstract: Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.

</details>


### [504] [Nearly Optimal Active Preference Learning and Its Application to LLM Alignment](https://arxiv.org/abs/2602.01581)
*Yao Zhao,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 收集人类偏好标签数据集成本高，现有偏好学习的主动学习设计目标不合适，本文提出两个主动学习算法，在真实数据集上有更好样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习采用经典实验设计准则，不适用于偏好学习结构，需设计特定问题算法。

Method: 基于对偏好学习的洞察，提出两个主动学习算法，一个提供实例依赖的标签复杂度保证，另一个是简单贪心方法。

Result: 在真实世界偏好数据集上评估，算法比现有方法有更好的样本效率。

Conclusion: 提出的主动学习算法在偏好学习中能有效提高样本效率。

Abstract: Aligning large language models (LLMs) depends on high-quality datasets of human preference labels, which are costly to collect. Although active learning has been studied to improve sample efficiency relative to passive collection, many existing approaches adopt classical experimental design criteria such as G- or D-optimality. These objectives are not tailored to the structure of preference learning, leaving open the design of problem-specific algorithms. In this work, we identify a simple intuition specific to preference learning that calls into question the suitability of these existing design objectives. Motivated by this insight, we propose two active learning algorithms. The first provides the first instance-dependent label complexity guarantee for this setting, and the second is a simple, practical greedy method. We evaluate our algorithm on real-world preference datasets and observe improved sample efficiency compared to existing methods.

</details>


### [505] [A Lightweight Sparse Interaction Network for Time Series Forecasting](https://arxiv.org/abs/2602.01585)
*Xu Zhang,Qitong Wang,Peng Wang,Wei Wang*

Main category: cs.LG

TL;DR: 提出用于时间序列预测的轻量级稀疏交互网络LSINet，在公共数据集实验中表现优于先进线性和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 现有线性模型在捕获复杂时间依赖上不足，性能有提升空间。

Method: 提出多头稀疏交互机制MSIM，通过稀疏诱导的伯努利分布学习时间步间重要连接；提出共享交互学习SIL增强效率和收敛性；构建仅含MLP结构的线性模型LSINet。

Result: 在公共数据集实验中，LSINet在时间序列预测任务中比先进线性和Transformer模型有更高准确性和效率。

Conclusion: LSINet能有效提升时间序列预测性能，代码公开。

Abstract: Recent work shows that linear models can outperform several transformer models in long-term time-series forecasting (TSF). However, instead of explicitly performing temporal interaction through self-attention, linear models implicitly perform it based on stacked MLP structures, which may be insufficient in capturing the complex temporal dependencies and their performance still has potential for improvement. To this end, we propose a Lightweight Sparse Interaction Network (LSINet) for TSF task. Inspired by the sparsity of self-attention, we propose a Multihead Sparse Interaction Mechanism (MSIM). Different from self-attention, MSIM learns the important connections between time steps through sparsity-induced Bernoulli distribution to capture temporal dependencies for TSF. The sparsity is ensured by the proposed self-adaptive regularization loss. Moreover, we observe the shareability of temporal interactions and propose to perform Shared Interaction Learning (SIL) for MSIM to further enhance efficiency and improve convergence. LSINet is a linear model comprising only MLP structures with low overhead and equipped with explicit temporal interaction mechanisms. Extensive experiments on public datasets show that LSINet achieves both higher accuracy and better efficiency than advanced linear models and transformer models in TSF tasks. The code is available at the link https://github.com/Meteor-Stars/LSINet.

</details>


### [506] [Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting](https://arxiv.org/abs/2602.01588)
*Huu Hiep Nguyen,Minh Hoang Nguyen,Dung Nguyen,Hung Le*

Main category: cs.LG

TL;DR: 提出SpecTF框架用于多模态时间序列预测，在频域集成文本数据对时间序列的影响，实验表现优于现有模型且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有多模态时间序列预测方法在结合文本特征与时间序列模式时忽略上下文信息的多尺度时间影响，存在局部对齐与全局文本上下文不匹配问题。

Method: 提出SpecTF框架，提取文本嵌入，投影到频域，用轻量级交叉注意力机制与时间序列的频谱分量融合，根据文本相关性自适应重新加权频段，再映射回时域进行预测。

Result: SpecTF在多种多模态时间序列数据集上显著优于现有模型，且使用参数更少。

Conclusion: SpecTF是一种简单有效的多模态时间序列预测框架。

Abstract: Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing methods align textual features with time-series patterns one step at a time, they neglect the multiscale temporal influences of contextual information such as time-series cycles and dynamic shifts. This mismatch between local alignment and global textual context can be addressed by spectral decomposition, which separates time series into frequency components capturing both short-term changes and long-term trends. In this paper, we propose SpecTF, a simple yet effective framework that integrates the effect of textual data on time series in the frequency domain. Our method extracts textual embeddings, projects them into the frequency domain, and fuses them with the time series' spectral components using a lightweight cross-attention mechanism. This adaptively reweights frequency bands based on textual relevance before mapping the results back to the temporal domain for predictions. Experimental results demonstrate that SpecTF significantly outperforms state-of-the-art models across diverse multi-modal time series datasets while utilizing considerably fewer parameters. Code is available at https://github.com/hiepnh137/SpecTF.

</details>


### [507] [The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR](https://arxiv.org/abs/2602.01599)
*Israel Adewuyi,Solomon Okibe,Vladmir Ivanov*

Main category: cs.LG

TL;DR: 研究发现训练仅1%的参数在强化学习中能匹配或超越全参数微调，提出多重彩票假设并解释现象。


<details>
  <summary>Details</summary>
Motivation: 利用强化学习中参数冗余，研究极端稀疏情况下利用冗余的简单方法。

Method: 训练随机选择的参数子集。

Result: 训练1%的参数在3个模型和2个任务领域中匹配或超越全参数微调，不同随机掩码重叠极小且都能成功。

Conclusion: 预训练模型包含多个可行稀疏子网络，提出多重彩票假设，通过隐式每步KL约束解释现象。

Abstract: The Lottery Ticket Hypothesis demonstrated that sparse subnetworks can match full-model performance, suggesting parameter redundancy. Meanwhile, in Reinforcement Learning with Verifiable Rewards (RLVR), recent work has shown that updates concentrate on a sparse subset of parameters, which further lends evidence to this underlying redundancy. We study the simplest possible way to exploit this redundancy: training only a randomly selected subset of parameters at extreme sparsities. Empirically, we find that training just 1\% of parameters matches or exceeds full-parameter RLVR finetuning across 3 models and 2 task domains. Moreover, different random masks show minimal overlap ($\leq 0.005$ Jaccard similarity) and yet all succeed, suggesting pretrained models contain many viable sparse subnetworks rather than one privileged set. We term this the Multiple Ticket Hypothesis. We explain this phenomenon through the implicit per-step KL constraint in RLVR, which restricts updates to a low-dimensional subspace, enabling arbitrary sparse masks to succeed.

</details>


### [508] [Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.01601)
*Hieu Trung Nguyen,Bao Nguyen,Wenao Ma,Yuzhi Zhao,Ruifeng She,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 现有基于组的策略优化方法分配固定数量的采样，导致计算预算使用效率低。本文提出VIP策略，根据提示的成功概率估计方差，优化采样分配，提高了采样效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法对所有训练提示分配固定数量的采样，均匀分配计算预算，导致使用效率低，阻碍训练进度。

Method: 提出VIP策略，使用轻量级高斯过程模型根据近期采样预测每个提示的成功概率，将其转化为方差估计，通过凸优化问题在计算预算约束下确定最优采样分配。

Result: 在多个基准测试中，VIP策略持续提高了采样效率，比均匀或启发式分配策略取得了更高的性能。

Conclusion: VIP策略能有效提高基于可验证奖励的强化学习中的采样效率。

Abstract: Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.

</details>


### [509] [Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching](https://arxiv.org/abs/2602.01606)
*Zeqiao Li,Yijing Wang,Haoyu Wang,Zheng Li,Zhiqiang Zuo*

Main category: cs.LG

TL;DR: 提出FLAME框架解决Flow Matching集成到MaxEnt RL的挑战，在MuJoCo上表现优于高斯基线且推理成本低。


<details>
  <summary>Details</summary>
Motivation: Diffusion策略推理延迟高，将Flow Matching集成到MaxEnt RL存在挑战，如最优策略难处理、对数似然估计有离散化偏差。

Method: 推导Q-Reweighted FM目标绕过配分函数估计；设计解耦熵估计器校正偏差；集成MeanFlow实现高效一步控制。

Result: 在MuJoCo上的实验显示，FLAME表现优于高斯基线，能在低推理成本下达到多步扩散策略性能。

Conclusion: FLAME是有效框架，能解决集成挑战，实现高效一步控制。

Abstract: Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \textbf{F}low-based \textbf{L}og-likelihood-\textbf{A}ware \textbf{M}aximum \textbf{E}ntropy RL (\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.

</details>


### [510] [What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?](https://arxiv.org/abs/2602.01611)
*Weizheng Gu,Chengze Li,Zhuohao Yu,Mengyuan Sun,Zhibang Yang,Wei Wang,Hongrui Jia,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: 现有大模型智能体评估基准有局限性，论文提出PIPE评估增强协议和IR指标，揭示轨迹SFT会放大接口捷径问题。


<details>
  <summary>Details</summary>
Motivation: 标准智能体基准将语义工具使用和特定接口交互模式记忆这两种成功来源混淆，基准分数无法证明是环境不变能力。

Method: 提出PIPE协议，通过最小化改写环境接口而保留任务语义和执行行为来诊断接口依赖；引入界面依赖（IR）指标量化对训练时接口的偏好。

Result: PIPE显示轨迹SFT会大幅放大接口捷径，训练后的智能体在最小接口改写时性能大幅下降，非轨迹训练的模型基本稳定；接口捷径有环境依赖、非单调的训练动态，标准评估无法发现。

Conclusion: 现有标准评估存在不足，PIPE和IR有助于更准确评估智能体能力。

Abstract: Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.

</details>


### [511] [A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models](https://arxiv.org/abs/2602.01613)
*Sergii Kozyrev,Davyd Maiboroda*

Main category: cs.LG

TL;DR: 提出Minima生产压缩管道，可对Transformer进行结构压缩并带来服务增益，在Qwen3 - 32B上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型受GPU内存和推理延迟限制，需要有效压缩方法以提升部署性能。

Method: 训练轻量级卷积预测器估计敏感度，对低敏感度区域应用多种张量分解，进行微调，用自定义内核执行操作，还采用投机解码。

Result: 在Qwen3 - 32B上，减少了峰值VRAM，提升了单请求和多并行请求的吞吐量。

Conclusion: Minima是迈向更激进结构压缩的实用步骤。

Abstract: Large language models are limited in deployment by GPU memory and inference latency. We present Minima, a production compression pipeline that learns where and how to structurally compress a Transformer and turns that compression into real serving gains. Minima trains a lightweight convolutional predictor to estimate layer- and patch-level sensitivity, applies a mixture of Tucker, tensor-train, and tensor-ring decompositions to low-sensitivity regions, performs a short healing fine-tune, and executes the resulting operators with custom Triton and CUDA kernels. The reduced memory footprint enables speculative decoding with a small draft model and a larger verifier. On Qwen3-32B at an 8k-token context window, Minima reduces peak VRAM from 64 GiB to 40 GiB. For a single active request, throughput increases from 40 tokens per second (baseline) to 50 tokens per second (Minima) and 75 tokens per second (Minima with speculative decoding). Under 50 parallel requests, throughput is 34, 44, and 53 tokens per second respectively, showing that Minima remains effective under high concurrency even when speculative decoding gains compress. We position Minima relative to recent tensor-network, low-rank plus quantization, and cross-layer sharing methods, and argue that it is a practical step toward more aggressive structural compression via shared tensor backbones with tiny per-layer adapters.

</details>


### [512] [AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems](https://arxiv.org/abs/2602.01614)
*Qi Cheng,Licheng Liu,Yao Zhang,Mu Hong,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 文章介绍首个时空农业生态系统温室气体基准数据集，评估多种深度学习模型性能，探索迁移学习，助力开发更准确可扩展的人工智能驱动农业生态系统模型。


<details>
  <summary>Details</summary>
Motivation: 农业生态系统对缓解气候变化至关重要，但准确量化其碳、养分和水关系中的库和通量面临挑战，且缺乏人工智能所需的基准数据集和协议。

Method: 引入集成物理模型模拟与实际观测的基准数据集，评估多种顺序深度学习模型在碳氮通量预测上的性能，探索迁移学习。

Result: 完成基准数据集和评估框架。

Conclusion: 基准数据集和评估框架有助于开发更准确可扩展的人工智能驱动农业生态系统模型，增进对生态系统 - 气候相互作用的理解。

Abstract: Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately do not exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM-based models, temporal CNN-based model, and Transformer-based models. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.

</details>


### [513] [SUSD: Structured Unsupervised Skill Discovery through State Factorization](https://arxiv.org/abs/2602.01619)
*Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 本文提出新框架SUSD用于无监督技能发现，通过分解状态空间促进发现更多样技能，实验显示其在多环境中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法存在局限，如MI - based方法倾向简单静态技能，DSD难以鼓励全面技能集，需要新方法解决这些问题。

Method: 引入SUSD框架，将状态空间分解为独立组件，为不同因素分配不同技能变量，用动态模型跟踪学习并引导探索。

Result: 在三个环境（因素从1到10）的实验中，SUSD能无监督发现多样复杂技能，显著优于现有方法。

Conclusion: SUSD框架有效，能促进发现丰富多样技能，得到可分解技能表示，便于下游任务训练。

Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.

</details>


### [514] [Toward Enhancing Representation Learning in Federated Multi-Task Settings](https://arxiv.org/abs/2602.01626)
*Mehdi Setayesh,Mahdi Beitollahi,Yasser H. Khalil,Hongliang Li*

Main category: cs.LG

TL;DR: 提出FedMuscle算法，用Muscle loss解决联邦多任务学习中模型和任务异质性问题，实验表现超现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有联邦多任务学习方法假设模型一致性，限制了在现实场景的应用，需解决模型和任务异质性问题。

Method: 提出Muscle loss对比学习目标，基于此开发FedMuscle算法。

Result: 在不同图像和语言任务实验中，FedMuscle始终优于现有基线。

Conclusion: FedMuscle是实用且通信高效的算法，能自然处理模型和任务异质性。

Abstract: Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability in realistic settings. To overcome this limitation, we aim to learn a shared representation space across tasks rather than shared model parameters. To this end, we propose Muscle loss, a novel contrastive learning objective that simultaneously aligns representations from all participating models. Unlike existing multi-view or multi-model contrastive methods, which typically align models pairwise, Muscle loss can effectively capture dependencies across tasks because its minimization is equivalent to the maximization of mutual information among all the models' representations. Building on this principle, we develop FedMuscle, a practical and communication-efficient FMTL algorithm that naturally handles both model and task heterogeneity. Experiments on diverse image and language tasks demonstrate that FedMuscle consistently outperforms state-of-the-art baselines, delivering substantial improvements and robust performance across heterogeneous settings.

</details>


### [515] [AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments](https://arxiv.org/abs/2602.01629)
*Renukanandan Tumu,Aditya Singh,Rahul Mangharam*

Main category: cs.LG

TL;DR: 提出AdaptNC框架用于联合在线调整非一致性分数参数和共形阈值，在机器人基准测试中可减小预测区域体积并维持目标覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有在线共形预测方法采用静态非一致性分数函数，在环境结构变化时预测区域保守且体积效率低，需改进。

Method: 提出AdaptNC框架，利用自适应重加权方案优化分数函数，引入重放缓冲区机制缓解分数转换时的覆盖率不稳定。

Result: 在涉及多智能体策略变化、环境变化和传感器退化的机器人基准测试中，与仅调整阈值的基线方法相比，AdaptNC显著减小了预测区域体积并维持了目标覆盖率。

Conclusion: AdaptNC框架能有效解决现有在线共形预测方法在环境结构变化时的问题，提升预测效率。

Abstract: Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.

</details>


### [516] [COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection](https://arxiv.org/abs/2602.01635)
*Jinwoo Park,Hyeongwon Kang,Seung Hun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: 提出用于时间序列异常检测的COMET方法，由多尺度补丁编码、向量量化核心集和在线码本自适应三部分组成，实验证明其在多个基准数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法在捕捉时间依赖和多变量相关性、检测不同时间范围异常以及应对推理时分布偏移等方面存在不足。

Method: 提出COMET方法，包含多尺度补丁编码、向量量化核心集和在线码本自适应三个关键组件。

Result: 在五个基准数据集的45个评估指标中，COMET在36个指标上取得最佳性能。

Conclusion: COMET方法在不同环境下均有效。

Abstract: Time series anomaly detection is a critical task across various industrial domains. However, capturing temporal dependencies and multivariate correlations within patch-level representation learning remains underexplored, and reliance on single-scale patterns limits the detection of anomalies across different temporal ranges. Furthermore, focusing on normal data representations makes models vulnerable to distribution shifts at inference time. To address these limitations, we propose Codebook-based Online-adaptive Multi-scale Embedding for Time-series anomaly detection (COMET), which consists of three key components: (1) Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales. (2) Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance. (3) Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning. Experiments on five benchmark datasets demonstrate that COMET achieves the best performance in 36 out of 45 evaluation metrics, validating its effectiveness across diverse environments.

</details>


### [517] [Chance-Constrained Inference for Hallucination Risk Control in Large Language Models](https://arxiv.org/abs/2602.01637)
*Sreenivasan Mohandas*

Main category: cs.LG

TL;DR: 本文提出机会约束推理方法解决大语言模型幻觉问题，在实验中表现优于基于置信度的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有缓解策略无法对大语言模型重复使用时幻觉的频率进行显式控制。

Method: 将推理表述为部署时的风险控制问题，引入机会约束推理，提出顺序、随时有效的推理程序。

Result: 在自然问题启发的问题和受控多跳问答实验中，实现可靠风险控制、早期检测不可行输入和安全组合，而基于置信度的基线方法无法提供一致保证。

Conclusion: 机会约束推理方法能有效控制大语言模型幻觉风险。

Abstract: Large language models generate outputs stochastically and may produce fluent but invalid responses, including factual hallucinations. Existing mitigation strategies reduce average error rates but do not provide explicit control over the \emph{frequency} of such failures under repeated use. We formulate inference as a deployment-time risk control problem and introduce \emph{chance-constrained inference}, which directly bounds the probability of hallucinations among accepted generations. Hallucinations are modeled as stochastic constraint violations, and we show that confidence-based selective prediction does not, in general, imply probabilistic risk guarantees. To enforce chance constraints efficiently, we propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds. Experiments on questions inspired by NaturalQuestions and controlled multi-hop question answering demonstrate reliable risk control, early detection of intrinsically infeasible inputs, and safe composition under repeated use, while confidence-based baselines fail to provide consistent guarantees.

</details>


### [518] [De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion](https://arxiv.org/abs/2602.01643)
*Xichen Sun,Wentao Wei,Jiahua Rao,Jiancong Xie,Yuedong Yang*

Main category: cs.LG

TL;DR: 提出MBGen框架用于从质谱进行从头分子结构生成，在基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用原子中心和成对相互作用建模，忽略高阶边缘相互作用，缺乏系统捕捉多体特征的能力，为克服这些局限开展研究。

Method: 提出MBGen，集成多体注意力机制和高阶边缘建模。

Result: 在NPLIB1和MassSpecGym基准测试中，MBGen性能优于现有方法，提升最高达230%。

Conclusion: 多体建模对基于质谱的分子生成具有科学价值和实用价值，MBGen能有效捕捉高阶相互作用，对复杂异构体和非局部碎片信息更敏感。

Abstract: Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.

</details>


### [519] [From Perception to Action: Spatial AI Agents and World Models](https://arxiv.org/abs/2602.01644)
*Gloria Felicia,Nolan Bryant,Handi Putra,Ayaan Gazali,Eliel Lobo,Esteban Rojas*

Main category: cs.LG

TL;DR: 已有研究分别孤立探讨智能体架构和空间领域，该论文通过综述超2000篇文章引入统一的三轴分类法连接智能体能力和空间任务，有三项发现并提出未来研究挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 现有调查未提供连接智能体架构和空间领域互补能力的统一框架，本文旨在填补这一空白。

Method: 对超2000篇论文进行全面综述，引用742篇顶级会议作品。

Result: 引入统一的三轴分类法；有三项关键发现，分别关于能力轴、任务轴和尺度轴。

Conclusion: 指出六项重大挑战，明确未来研究方向，该分类法为统一研究和下一代空间感知系统奠定基础。

Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.

</details>


### [520] [On the Spatiotemporal Dynamics of Generalization in Neural Networks](https://arxiv.org/abs/2602.01651)
*Zichao Wei*

Main category: cs.LG

TL;DR: 本文指出神经网络在加法泛化上失败是违反物理假设，提出三个约束条件并推导SEAD架构，实验验证理论，表明应尊重计算物理来弥合统计学习和逻辑推理差距。


<details>
  <summary>Details</summary>
Motivation: 探究神经网络在加法泛化上不如人类的原因，认为这不是工程问题而是违反物理假设。

Method: 借鉴物理，确定泛化系统需满足的三个约束条件，推导SEAD架构。

Result: 在奇偶性、加法、规则110三个任务实验验证理论，加法从16位到100万位实现100%准确推理。

Conclusion: 弥合统计学习和逻辑推理差距不应靠增加参数，而应尊重计算物理。

Abstract: Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we identify three constraints that any generalizing system must satisfy: (1) Locality -- information propagates at finite speed; (2) Symmetry -- the laws of computation are invariant across space and time; (3) Stability -- the system converges to discrete attractors that resist noise accumulation. From these postulates, we derive -- rather than design -- the Spatiotemporal Evolution with Attractor Dynamics (SEAD) architecture: a neural cellular automaton where local convolutional rules are iterated until convergence. Experiments on three tasks validate our theory: (1) Parity -- demonstrating perfect length generalization via light-cone propagation; (2) Addition -- achieving scale-invariant inference from L=16 to L=1 million with 100% accuracy, exhibiting input-adaptive computation; (3) Rule 110 -- learning a Turing-complete cellular automaton without trajectory divergence. Our results suggest that the gap between statistical learning and logical reasoning can be bridged -- not by scaling parameters, but by respecting the physics of computation.

</details>


### [521] [Efficient Adversarial Attacks on High-dimensional Offline Bandits](https://arxiv.org/abs/2602.01658)
*Seyed Mohammad Hadi Hosseini,Amir Najafi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 研究离线多臂老虎机评估在奖励模型受对抗操纵时的脆弱性，理论和实验表明小扰动可大幅改变其行为，高维下攻击更易成功。


<details>
  <summary>Details</summary>
Motivation: 在线评估成本高，离线评估有吸引力，但离线多臂老虎机评估对抗鲁棒性研究不足，尤其是奖励模型被扰动情况。

Method: 理论分析结合实验，引入新威胁模型，研究对线性和非线性奖励函数的攻击，以两个Hugging Face评估器为对象。

Result: 小的、难以察觉的奖励模型权重扰动可大幅改变多臂老虎机行为；高维下成功攻击所需扰动范数减小；随机扰动无效，针对性扰动成功率高。

Conclusion: 离线多臂老虎机评估在奖励模型受对抗操纵时很脆弱，现代高维应用如图像评估尤为易受攻击。

Abstract: Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often distributed with public weights on platforms such as Hugging Face, to provide feedback to the bandit. While online evaluation is expensive and requires repeated trials, offline evaluation with logged data has become an attractive alternative. However, the adversarial robustness of offline bandit evaluation remains largely unexplored, particularly when an attacker perturbs the reward model (rather than the training data) prior to bandit training. In this work, we fill this gap by investigating, both theoretically and empirically, the vulnerability of offline bandit training to adversarial manipulations of the reward model. We introduce a novel threat model in which an attacker exploits offline data in high-dimensional settings to hijack the bandit's behavior. Starting with linear reward functions and extending to nonlinear models such as ReLU neural networks, we study attacks on two Hugging Face evaluators used for generative model assessment: one measuring aesthetic quality and the other assessing compositional alignment. Our results show that even small, imperceptible perturbations to the reward model's weights can drastically alter the bandit's behavior. From a theoretical perspective, we prove a striking high-dimensional effect: as input dimensionality increases, the perturbation norm required for a successful attack decreases, making modern applications such as image evaluation especially vulnerable. Extensive experiments confirm that naive random perturbations are ineffective, whereas carefully targeted perturbations achieve near-perfect attack success rates ...

</details>


### [522] [Quantifying Epistemic Predictive Uncertainty in Conformal Prediction](https://arxiv.org/abs/2602.01667)
*Siu Lun Chau,Soroush H. Zargarbashi,Yusuf Sale,Michele Caprio*

Main category: cs.LG

TL;DR: 本文在共形预测（CP）框架下研究认知预测不确定性（EPU）的量化问题，提出基于最大平均不精确性的EPU量化方法，实验表明其比仅依赖CPR大小更具信息性。


<details>
  <summary>Details</summary>
Motivation: 在CP框架下量化EPU，即预测时因多个似然预测模型存在而面临的不确定性。

Method: 基于近期结果，证明CPR与诱导的可信集的关系在拆分CP中也成立，提出基于最大平均不精确性的不确定性度量来量化EPU。

Result: 在主动学习和选择性分类实验中，量化的EPU比仅依赖CPR大小能提供更具信息性和细粒度的不确定性评估。

Conclusion: CP有潜力作为认知不确定性下决策的原则性基础。

Abstract: We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-α$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.

</details>


### [523] [ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.01668)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei,Yueqi Xing*

Main category: cs.LG

TL;DR: 提出适用于资源受限超算环境的高效预测框架ASGMamba，在九个基准测试中达到SOTA准确率，且复杂度低、内存使用少。


<details>
  <summary>Details</summary>
Motivation: 现有长时多变量时间序列预测（LTSF）方案存在问题，Transformer模型复杂度高，线性状态空间模型难区分信号与噪声。

Method: 提出ASGMamba，集成轻量级自适应频谱门控（ASG）机制，引入带特定节点嵌入的分层多尺度架构。

Result: 在九个基准测试中达到SOTA准确率，复杂度为O(L)，显著减少长时任务内存使用。

Conclusion: ASGMamba是资源受限环境下高通量预测的可扩展解决方案。

Abstract: Long-term multivariate time series forecasting (LTSF) plays a crucial role in various high-performance computing applications, including real-time energy grid management and large-scale traffic flow simulation. However, existing solutions face a dilemma: Transformer-based models suffer from quadratic complexity, limiting their scalability on long sequences, while linear State Space Models (SSMs) often struggle to distinguish valuable signals from high-frequency noise, leading to wasted state capacity. To bridge this gap, we propose ASGMamba, an efficient forecasting framework designed for resource-constrained supercomputing environments. ASGMamba integrates a lightweight Adaptive Spectral Gating (ASG) mechanism that dynamically filters noise based on local spectral energy, enabling the Mamba backbone to focus its state evolution on robust temporal dynamics. Furthermore, we introduce a hierarchical multi-scale architecture with variable-specific Node Embeddings to capture diverse physical characteristics. Extensive experiments on nine benchmarks demonstrate that ASGMamba achieves state-of-the-art accuracy. While keeping strictly $$\mathcal{O}(L)$$ complexity we significantly reduce the memory usage on long-horizon tasks, thus establishing ASGMamba as a scalable solution for high-throughput forecasting in resource limited environments.The code is available at https://github.com/hit636/ASGMamba

</details>


### [524] [Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment](https://arxiv.org/abs/2602.01685)
*Byeonghu Na,Hyungho Na,Yeongmin Kim,Suhyeon Jo,HeeSun Bae,Mina Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出基于熵正则化Wasserstein距离的WPR方法用于RLHF框架，优于KL和f - 散度基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于KL散度及其f - 散度变体的方法仅比较相同索引处的标记概率，无法捕捉语义相似性。

Method: 提出Wasserstein Policy Regularization (WPR)，基于熵正则化Wasserstein距离，通过最优对偶变量将正则化表示为对奖励的惩罚项。

Result: 方法在实验上优于基于KL和f - 散度的基线。

Conclusion: 语义感知的策略距离对对齐有益。

Abstract: Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.

</details>


### [525] [$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality](https://arxiv.org/abs/2602.01703)
*Pengyu Li,Lingling Zhang,Zhitao Gao,Yanrui Wu,Yuxuan Dong,Huan Liu,Bifan Wei,Jun Liu*

Main category: cs.LG

TL;DR: 提出AGT$^{AO}$框架解决大语言模型去学习中擦除和保留模型效用的权衡问题，实验显示其在去学习效果和模型效用间取得较好平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型会无意记忆敏感数据，现有去学习范式存在激进去学习导致模型效用下降、保守策略易被恢复的问题，需解决去学习中擦除和保留效用的权衡。

Method: 提出AGT$^{AO}$框架，引入Adaptive Orthogonality动态缓解遗忘和保留目标间的几何梯度冲突，采用Adversarial Gating Training将去学习构造成潜空间的极小极大博弈，用基于课程的门控机制模拟和对抗内部恢复尝试。

Result: 实验表明AGT$^{AO}$在去学习效果（KUR ≈ 0.01）和模型效用（MMLU 58.30）间取得了较好的权衡。

Conclusion: AGT$^{AO}$框架有效解决了大语言模型去学习中擦除和保留模型效用的权衡问题。

Abstract: While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.

</details>


### [526] [Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner](https://arxiv.org/abs/2602.01705)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: 提出LaDi - RL框架，在连续潜在空间探索以解决离散RL中多样性崩溃问题，实验显示在代码生成和数学推理上优于离散RL基线。


<details>
  <summary>Details</summary>
Motivation: 离散RL在令牌空间探索时因策略熵降低会出现多样性崩溃问题，需解决该问题以提升LLM推理能力。

Method: 提出Latent Diffusion Reasoning with Reinforcement Learning (LaDi - RL)框架，在连续潜在空间进行探索，通过引导扩散建模探索，多步去噪分散随机性，解耦潜在空间探索和文本空间生成。

Result: 在代码生成和数学推理基准测试中，pass@1和pass@k均有持续提升，代码生成pass@1绝对增益+9.4%，数学推理+5.7%。

Conclusion: 基于扩散的潜在RL是离散令牌级RL进行推理的有效替代方法。

Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.

</details>


### [527] [Revisiting Generalization Measures Beyond IID: An Empirical Study under Distributional Shift](https://arxiv.org/abs/2602.01718)
*Sora Nakai,Youssef Fadhloun,Kacem Mathlouthi,Kotaro Yoshida,Ganesh Talluri,Ioannis Mitliagkas,Hiroki Naganuma*

Main category: cs.LG

TL;DR: 本文在先前研究基础上，对泛化度量的鲁棒性进行基准测试，发现分布偏移会改变许多泛化度量的预测性能，部分度量较稳定。


<details>
  <summary>Details</summary>
Motivation: 泛化是深度学习中未解决的核心挑战，尤其是在测试前用可用量预测模型在训练分布外的性能，且先前研究关注训练配置的不稳定性。

Method: 在10000个超参配置上训练中小模型，评估40多种仅从训练模型和训练数据计算的度量，从多方面拓宽实验范围。

Result: 分布偏移会大幅改变许多泛化度量的预测性能，部分度量在不同设置下较稳定。

Conclusion: 部分泛化度量在不同分布设置下具有相对稳定性，可用于更可靠的泛化评估。

Abstract: Generalization remains a central yet unresolved challenge in deep learning, particularly the ability to predict a model's performance beyond its training distribution using quantities available prior to test-time evaluation. Building on the large-scale study of Jiang et al. (2020). and concerns by Dziugaite et al. (2020). about instability across training configurations, we benchmark the robustness of generalization measures beyond IID regime. We train small-to-medium models over 10,000 hyperparameter configurations and evaluate more than 40 measures computable from the trained model and the available training data alone. We significantly broaden the experimental scope along multiple axes: (i) extending the evaluation beyond the standard IID setting to include benchmarking for robustness across diverse distribution shifts, (ii) evaluating multiple architectures and training recipes, and (iii) newly incorporating calibration- and information-criteria-based measures to assess their alignment with both IID and OOD generalization. We find that distribution shifts can substantially alter the predictive performance of many generalization measures, while a smaller subset remains comparatively stable across settings.

</details>


### [528] [MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration](https://arxiv.org/abs/2602.01734)
*Lianhai Ren,Yucheng Ding,Xiao Liu,Qianxiao Li,Peng Cheng,Yeyun Gong*

Main category: cs.LG

TL;DR: 文章研究大语言模型预训练的不稳定问题，分析 NanoGPT 模型，找出训练崩溃前的两种关键现象并证明会导致梯度爆炸，提出 MSign 优化器解决问题，实验证明有效且开销小。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型预训练中训练不稳定、梯度爆炸浪费计算资源的问题。

Method: 研究 5M 参数的 NanoGPT 模型，找出训练崩溃前的关键现象，理论证明现象导致梯度爆炸，提出 MSign 优化器，周期性应用矩阵符号运算以恢复稳定秩。

Result: 在 5M 到 3B 参数的模型上实验表明，MSign 能有效防止训练失败，计算开销小于 7.0%。

Conclusion: MSign 优化器可有效解决大语言模型预训练中的训练不稳定问题，且计算开销可接受。

Abstract: Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.

</details>


### [529] [Position: The Inevitable End of One-Architecture-Fits-All-Domains in Time Series Forecasting](https://arxiv.org/abs/2602.01736)
*Qinwei Ma,Jingzhe Shi,Jiahao Qiu,Zaiwen Yang*

Main category: cs.LG

TL;DR: 论文指出时间序列神经网络架构研究遇瓶颈，呼吁转向特定领域深度学习方法或通用领域元学习方法。


<details>
  <summary>Details</summary>
Motivation: 近期研究质疑时间序列预测任务中神经网络架构的有效性和鲁棒性，现要分析其局限性并提出新方向。

Method: 总结相关质疑与问题，剖析时间序列神经网络架构内在局限性。

Result: 通用领域时间序列神经网络架构研究成果趋于饱和，对特定领域实践缺乏启发，特定领域很少借鉴该领域近年进展。

Conclusion: 时间序列研究界应将重点从通用领域转移，可聚焦特定领域深度学习方法或发展通用领域元学习方法。

Abstract: Recent work has questioned the effectiveness and robustness of neural network architectures for time series forecasting tasks. We summarize these concerns and analyze groundly their inherent limitations: i.e. the irreconcilable conflict between single (or few similar) domains SOTA and generalizability over general domains for time series forecasting neural network architecture designs. Moreover, neural networks architectures for general domain time series forecasting are becoming more and more complicated and their performance has almost saturated in recent years. As a result, network architectures developed aiming at fitting general time series domains are almost not inspiring for real world practices for certain single (or few similar) domains such as Finance, Weather, Traffic, etc: each specific domain develops their own methods that rarely utilize advances in neural network architectures of time series community in recent 2-3 years. As a result, we call for the time series community to shift focus away from research on time series neural network architectures for general domains: these researches have become saturated and away from domain-specific SOTAs over time. We should either (1) focus on deep learning methods for certain specific domain(s), or (2) turn to the development of meta-learning methods for general domains.

</details>


### [530] [Softmax Linear Attention: Reclaiming Global Competition](https://arxiv.org/abs/2602.01744)
*Mingwei Xu,Xuan Lin,Xinnan Guo,Wanqing Xu,Wanyun Cui*

Main category: cs.LG

TL;DR: 提出Softmax Linear Attention (SLA)框架，在不牺牲效率的前提下恢复竞争选择，实验证明其能提升线性基线模型性能。


<details>
  <summary>Details</summary>
Motivation: 线性注意力机制因去除softmax归一化导致表达能力不足，缺少全局竞争机制。

Method: 将softmax操作从token级别提升到head级别，利用注意力头作为语义槽，应用竞争门控机制动态选择相关子空间。

Result: SLA在语言建模和长上下文基准测试中持续提升了最先进的线性基线模型，在检索场景中增强了抗噪声鲁棒性。

Conclusion: SLA能在保持线性复杂度的同时恢复精确聚焦能力。

Abstract: While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.

</details>


### [531] [Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning](https://arxiv.org/abs/2602.01745)
*Wenhao Yu,Shaohang Wei,Jiahong Liu,Yifan Li,Minda Hu,Aiwei Liu,Hao Zhang,Irwin King*

Main category: cs.LG

TL;DR: RankTuner引入概率 - 熵校准信号Relative Rank Indicator，用于微调目标重加权，实验显示在多方面优于仅考虑概率或熵的基线。


<details>
  <summary>Details</summary>
Motivation: 常见的token级重加权指标多为一维，忽略熵或概率会导致对关键token识别不准确，需要更好的机制控制监督微调。

Method: 引入概率 - 熵校准信号Relative Rank Indicator，将其逆指标作为token级的Relative Scale对微调目标重加权。

Result: 在多个骨干模型的实验中，在数学推理基准测试、分布外推理迁移增益和代码生成性能上，均优于仅考虑概率或熵的重加权基线。

Conclusion: RankTuner提出的方法能有效聚焦真正未充分学习的token，且不过度惩罚固有不确定性位置，提升模型性能。

Abstract: Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.

</details>


### [532] [Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment](https://arxiv.org/abs/2602.01746)
*Hongyi Peng,Han Yu,Xiaoxiao Li,Qiang Yang*

Main category: cs.LG

TL;DR: 传统LORA在非IID设置下效果不佳，本文提出FedGaLore方法，在多个基准测试中提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在非IID设置下，Low - Rank Adaptation（LoRA）比全参数微调表现差，需要解决此性能差距问题。

Method: 提出FedGaLore，结合客户端GaLore式梯度子空间优化和服务器端通过谱共享信号提取对投影二阶矩状态进行抗漂移同步。

Result: 在NLU、视觉和NLG基准测试中，FedGaLore在非IID设置下比现有联邦LoRA基线提高了鲁棒性和准确性。

Conclusion: FedGaLore能有效解决非IID设置下LoRA性能不佳的问题，提高了其在相关场景中的表现。

Abstract: Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.

</details>


### [533] [MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network](https://arxiv.org/abs/2602.01751)
*Kunyi Fan,Mengjie Chen,Longlong Li,Cunquan Qu*

Main category: cs.LG

TL;DR: 本文提出MGKAN用于预测药物-药物相互作用（DDI），在两个基准数据集上表现优于现有方法，消融和案例研究证实了其预测准确性和有效性。


<details>
  <summary>Details</summary>
Motivation: 先前基于图神经网络（GNN）的模型在预测DDI时依赖线性聚合和对称假设，限制了捕捉非线性和异质性模式的能力。

Method: 提出MGKAN，引入可学习基函数进行不对称DDI预测，用KAN驱动的基函数替代传统MLP变换，整合三种网络视图并结合角色特定嵌入，使用融合模块结合线性注意力和非线性变换。

Result: 在两个基准数据集上，MGKAN优于七个最先进的基线模型。

Conclusion: MGKAN在预测DDI方面有较好的准确性和有效性。

Abstract: Predicting drug-drug interactions (DDIs) is essential for safe pharmacological treatments. Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric assumptions, limiting their ability to capture nonlinear and heterogeneous patterns. We propose MGKAN, a Graph Kolmogorov-Arnold Network that introduces learnable basis functions into asymmetric DDI prediction. MGKAN replaces conventional MLP transformations with KAN-driven basis functions, enabling more expressive and nonlinear modeling of drug relationships. To capture pharmacological dependencies, MGKAN integrates three network views-an asymmetric DDI network, a co-interaction network, and a biochemical similarity network-with role-specific embeddings to preserve directional semantics. A fusion module combines linear attention and nonlinear transformation to enhance representational capacity. On two benchmark datasets, MGKAN outperforms seven state-of-the-art baselines. Ablation studies and case studies confirm its predictive accuracy and effectiveness in modeling directional drug effects.

</details>


### [534] [A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention](https://arxiv.org/abs/2602.01763)
*Xiaowei Ye,Xiaoyu He,Chao Liao,Chen Wu,Pinyan Lu*

Main category: cs.LG

TL;DR: 本文从理论上刻画多种注意力机制性能差异，证明混合注意力和标准全注意力存在表达能力的分离。


<details>
  <summary>Details</summary>
Motivation: 现有高效注意力机制相对全注意力的表达能力缺乏严格理论刻画。

Method: 针对可表示为递归形式的线性注意力变体建立表达能力层次结构。

Result: 对于序列函数组合任务，(L + 1)层全注意力网络可解决，而混合网络中L - 1层全注意力与多达2^{3L^2}层线性注意力层交错也无法解决，体现两者表达能力差异。

Conclusion: 首次证明混合注意力和标准全注意力存在可证明的分离，为理解不同注意力机制提供理论视角。

Abstract: Transformers serve as the foundation of most modern large language models. To mitigate the quadratic complexity of standard full attention, various efficient attention mechanisms, such as linear and hybrid attention, have been developed. A fundamental gap remains: their expressive power relative to full attention lacks a rigorous theoretical characterization. In this work, we theoretically characterize the performance differences among these attention mechanisms. Our theory applies to all linear attention variants that can be formulated as a recurrence, including Mamba, DeltaNet, etc. Specifically, we establish an expressiveness hierarchy: for the sequential function composition-a multi-step reasoning task that must occur within a model's forward pass, an ($L+1$)-layer full attention network is sufficient, whereas any hybrid network interleaving $L-1$ layers of full attention with a substantially larger number ($2^{3L^2}$) of linear attention layers cannot solve it. This result demonstrates a clear separation in expressive power between the two types of attention. Our work provides the first provable separation between hybrid attention and standard full attention, offering a theoretical perspective for understanding the fundamental capabilities and limitations of different attention mechanisms.

</details>


### [535] [CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling](https://arxiv.org/abs/2602.01766)
*Runsong Zhao,Shilei Liu,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Yujin Yuan,Tong Xiao,Jingbo Zhu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 提出CoMeT架构，使大语言模型以恒定内存和线性时间复杂度处理长序列，有新的并行策略，效果显著。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次复杂度和不断增长的KV缓存阻碍长上下文处理，需新架构解决。

Method: 引入CoMeT架构，采用双内存系统管理上下文，还有新的层级别流水线并行策略。

Result: 装备CoMeT的模型能从1M token序列中准确检索密码，在SCROLLS基准测试中表现出色。

Conclusion: CoMeT架构有效，能处理长上下文，代码已开源。

Abstract: The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/

</details>


### [536] [IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination](https://arxiv.org/abs/2602.01769)
*Yuanshuai Li,Yuping Yan,Jirui Han,Fei Ming,Lingjuan Lv,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出 IRIS 方法解决多模态大语言模型幻觉问题，仅用 5.7k 样本在关键幻觉基准测试中取得有竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型存在幻觉问题，Direct Preference Optimization 方法依赖外部评估器，存在离策略可学习性差距和离散化损失，且忽略模态间细粒度冲突。

Method: 提出 IRIS 方法，利用原生对数概率空间中的连续隐式奖励，使用自生成偏好对消除可学习性差距，基于多模态隐式奖励筛选偏好对。

Result: 在关键幻觉基准测试中仅用 5.7k 样本取得有竞争力表现，偏好对齐时无需外部反馈。

Conclusion: IRIS 为缓解多模态大语言模型幻觉问题提供了高效且有原则的范式。

Abstract: Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.
  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.

</details>


### [537] [DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics](https://arxiv.org/abs/2602.01772)
*Yucheng Liao,Han Wen,Weinan E,Weijie Zhang*

Main category: cs.LG

TL;DR: 提出DIA - CLIP预训练模型，将DIA分析范式从半监督训练转变为通用跨模态表示学习，在各类评估中表现优，有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前DIA分析框架需半监督训练，易过拟合且缺乏跨物种和实验条件的泛化性。

Method: 集成双编码器对比学习框架与编码器 - 解码器架构，建立肽和对应光谱特征的统一跨模态表示。

Result: 在多个基准测试中始终优于现有工具，蛋白识别率提高45%，错配识别率降低12%。

Conclusion: DIA - CLIP模型具有高精确零样本PSM推理能力，在单细胞和空间蛋白质组学等领域有应用潜力。

Abstract: Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for peptide-spectrum match (PSM) re-scoring. This approach is prone to overfitting and lacks generalizability across diverse species and experimental conditions. Here, we present DIA-CLIP, a pre-trained model shifting the DIA analysis paradigm from semi-supervised training to universal cross-modal representation learning. By integrating dual-encoder contrastive learning framework with encoder-decoder architecture, DIA-CLIP establishes a unified cross-modal representation for peptides and corresponding spectral features, achieving high-precision, zero-shot PSM inference. Extensive evaluations across diverse benchmarks demonstrate that DIA-CLIP consistently outperforms state-of-the-art tools, yielding up to a 45% increase in protein identification while achieving a 12% reduction in entrapment identifications. Moreover, DIA-CLIP holds immense potential for diverse practical applications, such as single-cell and spatial proteomics, where its enhanced identification depth facilitates the discovery of novel biomarkers and the elucidates of intricate cellular mechanisms.

</details>


### [538] [Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting](https://arxiv.org/abs/2602.01776)
*Mingyue Cheng,Xiaoyu Tao,Qi Liu,Ze Guo,Enhong Chen*

Main category: cs.LG

TL;DR: 传统时间序列预测范式在自适应多轮场景不足，提出代理式时间序列预测（ATSF），介绍三种实现范式并讨论转变的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测范式在自适应和多轮设置中表现不足，需要新方法。

Method: 将预测重新构建为包含感知、规划、行动、反思和记忆的代理过程，提出三种实现范式：基于工作流设计、代理强化学习和混合代理工作流范式。

Result: 无明确具体结果，主要提出概念和范式。

Conclusion: 旨在将代理式预测确立为时间序列预测交叉领域未来研究的基础。

Abstract: Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.

</details>


### [539] [Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning](https://arxiv.org/abs/2602.01791)
*Zheng Zhang,Ao Lu,Yuanhao Zeng,Ziwei Shan,Jinjin Guo,Lufei Li,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: 提出Grad2Reward框架，从Judge模型推理过程提取密集过程奖励，提升训练效率和推理质量，实验证明其在多任务中有效。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR扩展到开放式任务时奖励稀疏，且将Judge视为黑盒，丢弃中间反馈信号。

Method: 引入Grad2Reward框架，通过单次反向传播从Judge模型推理过程提取密集过程奖励，利用基于梯度的归因实现精确的标记级信用分配，还引入自判断机制。

Result: 使用Grad2Reward优化的策略在多种开放式任务中表现出色。

Conclusion: Grad2Reward有效，具有广泛的泛化能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.

</details>


### [540] [Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It](https://arxiv.org/abs/2602.01826)
*Yaxiang Zhang,Yingru Li,Jiacai Liu,Jiawei Xu,Ziniu Li,Qian Liu,Haoyuan Li*

Main category: cs.LG

TL;DR: 强化学习训练大语言模型不稳定，分析不稳定原因并提出专用学习率调度器解决


<details>
  <summary>Details</summary>
Motivation: 强化学习训练大语言模型存在不稳定问题，标准补救方法可能失败

Method: 从优化角度分析不稳定性，发现可通过缩小更新大小抑制失配，提出根据响应长度动态触发学习率衰减的调度器

Result: 通过在梯度噪声增加时降低学习率，能稳定强化学习训练并将训练推理失配控制在安全水平

Conclusion: 训练推理失配是与模型优化相关的动态失败，所提调度器可有效解决不稳定问题

Abstract: Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to "training inference mismatch stemming" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.

</details>


### [541] [Hyperbolic Graph Neural Networks Under the Microscope: The Role of Geometry-Task Alignment](https://arxiv.org/abs/2602.01828)
*Dionisia Naddeo,Jonas Linkerhägner,Nicola Toschi,Geri Skenderi,Veronica Lachi*

Main category: cs.LG

TL;DR: 本文质疑双曲图神经网络（HGNNs）在树状图表示学习中的范式，提出几何 - 任务对齐条件，理论和实证结合分析HGNNs能力，发现仅链路预测任务与双曲几何对齐，HGNNs在对齐时表现更优。


<details>
  <summary>Details</summary>
Motivation: 质疑HGNNs在树状图表示学习中被广泛采用的范式，提出几何 - 任务对齐的附加条件。

Method: 在两个合成回归问题上理论和实证证明HGNNs恢复低失真表示的能力；联合分析预测性能和嵌入失真，评估HGNNs在链路预测和节点分类任务上的表现。

Result: 在合成回归问题中，当问题需保留度量结构时，HGNNs的几何归纳偏差有帮助；仅链路预测任务与双曲几何对齐。

Conclusion: 研究重点应从仅关注图是否为双曲，转向同时考虑任务是否与双曲几何对齐，HGNNs在对齐时优于欧几里得模型，否则优势消失。

Abstract: Many complex networks exhibit hyperbolic structural properties, making hyperbolic space a natural candidate for representing hierarchical and tree-like graphs with low distortion. Based on this observation, Hyperbolic Graph Neural Networks (HGNNs) have been widely adopted as a principled choice for representation learning on tree-like graphs. In this work, we question this paradigm by proposing an additional condition of geometry-task alignment, i.e., whether the metric structure of the target follows that of the input graph. We theoretically and empirically demonstrate the capability of HGNNs to recover low-distortion representations on two synthetic regression problems, and show that their geometric inductive bias becomes helpful when the problem requires preserving metric structure. Additionally, we evaluate HGNNs on the tasks of link prediction and node classification by jointly analyzing predictive performance and embedding distortion, revealing that only link prediction is geometry-aligned. Overall, our findings shift the focus from only asking "Is the graph hyperbolic?" to also questioning "Is the task aligned with hyperbolic geometry?", showing that HGNNs consistently outperform Euclidean models under such alignment, while their advantage vanishes otherwise.

</details>


### [542] [DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis](https://arxiv.org/abs/2602.01839)
*Ru Zhang,Xunkai Li,Yaxin Deng,Sicheng Liu,Daohan Su,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang,Jia Li*

Main category: cs.LG

TL;DR: 文章指出单细胞转录组分析现有方法的问题，提出DOGMA框架，该框架利用生物先验知识处理数据，在多基准测试中达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞转录组分析中，早期序列方法忽略细胞间潜在关系和原始序列数据质量问题，结构化方法又忽略生物先验知识，导致性能不佳。

Method: 提出DOGMA框架，通过多层次生物先验知识对原始数据进行结构重塑和语义增强，结合统计锚点、细胞本体和系统发育树进行图构建，利用基因本体弥合特征级语义差距。

Result: 在复杂多物种和多器官基准测试中，DOGMA达到SOTA性能，零样本鲁棒性和样本效率高，计算成本低。

Conclusion: DOGMA框架有效利用生物先验知识，能提升单细胞转录组分析的性能，且具有成本优势。

Abstract: Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.
  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.

</details>


### [543] [Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models](https://arxiv.org/abs/2602.01842)
*Jinbin Bai,Yixuan Li,Yuchen Zhu,Yi Xin,Qingyu Shi,Aosong Feng,Xiaohong Liu,Molei Tao,Jianru Xue,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: 提出适用于离散扩散语言模型的高效测试时间缩放框架Prism，在多个基准测试上实现性能 - 效率的良好权衡。


<details>
  <summary>Details</summary>
Motivation: 多数测试时间缩放算法依赖自回归解码，不适合离散扩散语言模型，开发有效高效的TTS方法以挖掘其生成潜力是待探索的挑战。

Method: 提出Prism框架，包括分层轨迹搜索、局部分支与部分重掩码、自验证反馈。

Result: 在四个数学推理和代码生成基准测试中，Prism实现了良好的性能 - 效率权衡，用更少的函数评估达到最佳N性能。

Conclusion: Prism是一种有效的离散扩散语言模型测试时间缩放方法，代码已开源。

Abstract: Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.

</details>


### [544] [No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation](https://arxiv.org/abs/2602.01845)
*Furkan Eris*

Main category: cs.LG

TL;DR: 介绍了309M参数的因果蛋白质语言模型Proust，它结合了生成与适应度预测能力，在多项任务上表现出色，还揭示了可解释性的见解。


<details>
  <summary>Details</summary>
Motivation: 解决现有蛋白质语言模型中掩码语言模型和因果模型功能分离，从业者需维护不同架构的问题。

Method: 采用从大语言模型研究中借鉴的架构创新，包括分组查询注意力、跨层值残差和深度因果卷积，在33B个标记上训练。

Result: 在ProteinGym替换任务上达到Spearman $ρ = 0.390$；在插入缺失任务上创纪录；在EVEREST病毒适应度基准测试中接近结构感知方法；具有生成能力；可解释性分析有新发现。

Conclusion: Proust结合了生成和适应度预测能力，处于优势地位，相关代码和权重公开。

Abstract: Protein language models (PLMs) face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures. We introduce \textbf{Proust}, a 309M-parameter causal PLM that bridges this gap through architectural innovations adapted from recent LLM research, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman $ρ= 0.390$ on ProteinGym substitutions, competitive with MLMs requiring 50--200$\times$ the compute. On indels, Proust sets a new state-of-the-art, outperforming models up to 20$\times$ larger. On EVEREST viral fitness benchmarks, it approaches structure-aware methods using sequence alone. These powerful representations position Proust in a sweet spot as it also retains native generative capabilities that MLMs lack by design. Interpretability analysis reveals that per-position entropy variance predicts, to an extent, when retrieval augmentation helps and hurts. Such insights can grow in both quantity and quality at scale and inform capabilities such as test-time scaling. Code and weights are available at https://github.com/Furkan9015/proust-inference

</details>


### [545] [Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models](https://arxiv.org/abs/2602.01849)
*Ziwei Luo,Ziqi Jin,Lei Wang,Lidong Bing,Thomas B. Schön*

Main category: cs.LG

TL;DR: 提出自奖励顺序蒙特卡罗（SMC）算法，用于掩码扩散语言模型（MDLMs）推理时采样，在多基准测试中验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有MDLMs多数采用基于置信度的采样策略，限制生成多样性，导致路径多样性崩溃。

Method: 并行启动多个相互作用的扩散过程（粒子）进行轨迹探索，引入轨迹级置信度作为自奖励信号分配粒子重要性权重，迭代加权和重采样粒子。

Result: 在多种MDLMs和基准测试上验证该算法，无需额外训练或奖励指导就有显著提升，能有效将并行推理能力转化为采样质量提升。

Conclusion: 自奖励SMC算法可有效用于MDLMs的采样，提高采样质量。

Abstract: This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.

</details>


### [546] [Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG](https://arxiv.org/abs/2602.01855)
*Blagoj Hristov,Hristijan Gjoreski,Vesna Ojleska Latkoska,Gorjan Nadzinski*

Main category: cs.LG

TL;DR: 提出数据高效的深度学习框架，用最少传感器硬件实现肌电假肢精确控制，表现佳，挑战高密度传感必要性。


<details>
  <summary>Details</summary>
Motivation: 准确响应的肌电假肢控制依赖复杂密集多传感器阵列，限制消费可及性，需新方法。

Method: 利用8人外部数据集，实现混合Transformer优化稀疏双通道表面肌电信号；集成Time2Vec可学习时间嵌入；采用归一化加法融合策略；使用两阶段课程学习协议。

Result: 10类动作集多主体F1分数达95.7% ± 0.20%，优于标准Transformer和CNN - LSTM；平衡时空维度模型容量稳定性最高；快速校准协议使新受试者准确率从21.0% ± 2.98%恢复到96.9% ± 0.52%。

Conclusion: 高保真时间嵌入可补偿低空间分辨率，挑战高密度传感必要性，框架为下一代假肢接口提供稳健、经济方案。

Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\pm$ 2.98% to 96.9% $\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.

</details>


### [547] [Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal](https://arxiv.org/abs/2602.01877)
*Zichun Wang,Gar Goei Loke,Ruiting Zuo*

Main category: cs.LG

TL;DR: 本文对比预测 - 优化模型在自相关不确定性下表现，提出A - OVE模型并计算其充分统计量，在投资组合优化问题验证，A - OVE表现优且小误判下有稳定性。


<details>
  <summary>Details</summary>
Motivation: 在自相关不确定性（VARMA(p,q)过程）下，对比直接优化样本外性能的模型与传统估计 - 优化方法的表现。

Method: 提出A - OVE模型，获取样本外最优解作为充分统计量的函数，提出计算充分统计量的递归形式。

Result: 在有交易成本的投资组合优化问题上，A - OVE相对完美信息模型遗憾值低，优于机器学习基准。高精度机器学习模型决策质量可能差。

Conclusion: A - OVE在自相关不确定性下表现良好，且在小误判情况下仍能保持性能。

Abstract: Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.

</details>


### [548] [Internal Flow Signatures for Self-Checking and Refinement in LLMs](https://arxiv.org/abs/2602.01897)
*Sungheon Jeong,Sanggeon Yun,Ryozo Masukawa,Wenjun Haung,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出内部流签名方法，借助轻量级GRU验证器对大语言模型进行自检查和定位细化，代码开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成答案可能与上下文不符，现有保障多依赖外部验证或生成后判断，需要新方法。

Method: 引入内部流签名，通过偏置中心监控稳定标记运动，构建子空间，用正交传输对齐窗口，训练轻量级GRU验证器。

Result: 验证器可进行自检查，定位问题深度事件，实现针对性细化。

Conclusion: 该方法能从内部决策动态中实现可操作的定位和低开销的自检查。

Abstract: Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \emph{Code is available at} \texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.

</details>


### [549] [Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs](https://arxiv.org/abs/2602.01914)
*Wenbo Pan,Zhichao Liu,Xianlong Wang,Haining Yu,Xiaohua Jia*

Main category: cs.LG

TL;DR: 本文提出一种高效多令牌归因方法FlashTrace，解决现有令牌归因方法的效率瓶颈和忠实度下降问题，实验显示其速度和忠实度均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有令牌归因方法在现代大语言模型依赖长推理链时，面临效率瓶颈和忠实度下降两个挑战。

Method: 引入FlashTrace方法，采用逐段聚合单遍计算多令牌目标归因，同时设计递归归因机制追溯重要性到源输入。

Result: 在长上下文检索和多步推理任务中，FlashTrace比现有基线实现超130倍加速，且保持更高忠实度，单次递归跳跃也能通过推理链提升忠实度。

Conclusion: FlashTrace解决了现有令牌归因方法的问题，在效率和忠实度上表现优秀。

Abstract: Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.

</details>


### [550] [VLM-Guided Experience Replay](https://arxiv.org/abs/2602.01915)
*Elad Sharony,Tom Jurgenson,Orr Krupnik,Dotan Di Castro,Shie Mannor*

Main category: cs.LG

TL;DR: 本文提出利用VLM指导经验回放缓冲区中经验的优先级排序，在多场景下，采用该方法训练的智能体比以往方法有更好表现。


<details>
  <summary>Details</summary>
Motivation: 现有工作将LLM和VLM集成到RL的各组件中，但核心组件回放缓冲区尚未被探索，因此要填补这一空白。

Method: 使用冻结的预训练VLM作为自动评估器，从智能体的经验中识别并优先处理有前景的子轨迹。

Result: 在游戏和机器人等离散和连续领域的场景中，采用该方法训练的智能体平均成功率提高11 - 52%，样本效率提高19 - 45%。

Conclusion: 利用VLM指导回放缓冲区经验的优先级排序是一种有效的方法，能提升智能体性能和样本效率。

Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

</details>


### [551] [PIMPC-GNN: Physics-Informed Multi-Phase Consensus Learning for Enhancing Imbalanced Node Classification in Graph Neural Networks](https://arxiv.org/abs/2602.01920)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: 提出PIMPC - GNN框架用于不平衡节点分类，结合三种动力学，在多数据集上超16种基线方法，提升少数类召回率和平衡准确率并提供可解释见解。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在类别不平衡场景中面临少数类代表性不足、预测偏向多数类的问题。

Method: 提出PIMPC - GNN框架，集成热力学扩散、Kuramoto同步和谱嵌入三种动力学，通过类自适应集成加权结合，用不平衡感知损失训练。

Result: 在五个基准数据集和5 - 100的不平衡比率下，超16种基线方法，少数类召回率提升达12.7%，平衡准确率提升达8.3%。

Conclusion: PIMPC - GNN框架有效解决图神经网络类别不平衡问题，还能提供图学习中共识动力学的可解释见解。

Abstract: Graph neural networks (GNNs) often struggle in class-imbalanced settings, where minority classes are under-represented and predictions are biased toward majorities. We propose \textbf{PIMPC-GNN}, a physics-informed multi-phase consensus framework for imbalanced node classification. Our method integrates three complementary dynamics: (i) thermodynamic diffusion, which spreads minority labels to capture long-range dependencies, (ii) Kuramoto synchronisation, which aligns minority nodes through oscillatory consensus, and (iii) spectral embedding, which separates classes via structural regularisation. These perspectives are combined through class-adaptive ensemble weighting and trained with an imbalance-aware loss that couples balanced cross-entropy with physics-based constraints. Across five benchmark datasets and imbalance ratios from 5-100, PIMPC-GNN outperforms 16 state-of-the-art baselines, achieving notable gains in minority-class recall (up to +12.7\%) and balanced accuracy (up to +8.3\%). Beyond empirical improvements, the framework also provides interpretable insights into consensus dynamics in graph learning. The code is available at \texttt{https://github.com/afofanah/PIMPC-GNN}.

</details>


### [552] [Embedding Learning on Multiplex Networks for Link Prediction](https://arxiv.org/abs/2602.01922)
*Orell Trautmann,Olaf Wolkenhauer,Clémence Réda*

Main category: cs.LG

TL;DR: 本文综述了多重网络嵌入学习用于链接预测的模型，提出分类方法、解决评估问题并给出评估建议。


<details>
  <summary>Details</summary>
Motivation: 随着网络复杂性增加，多重网络嵌入学习用于链接预测面临挑战，需要对已有模型进行综述和评估。

Method: 提出细化分类法对模型分类比较；解决多重网络嵌入学习可重复和公平评估问题；提出新颖公平的有向多重网络测试程序。

Result: 完成对多重网络嵌入学习用于链接预测模型的综述，提出有效分类和评估方法。

Conclusion: 本综述是开发更高效易处理的多重网络嵌入学习方法及其公平评估的关键一步，并给出模型评估指南和应对下游分析的建议。

Abstract: Over the past years, embedding learning on networks has shown tremendous results in link prediction tasks for complex systems, with a wide range of real-life applications. Learning a representation for each node in a knowledge graph allows us to capture topological and semantic information, which can be processed in downstream analyses later. In the link prediction task, high-dimensional network information is encoded into low-dimensional vectors, which are then fed to a predictor to infer new connections between nodes in the network. As the network complexity (that is, the numbers of connections and types of interactions) grows, embedding learning turns out increasingly challenging. This review covers published models on embedding learning on multiplex networks for link prediction. First, we propose refined taxonomies to classify and compare models, depending on the type of embeddings and embedding techniques. Second, we review and address the problem of reproducible and fair evaluation of embedding learning on multiplex networks for the link prediction task. Finally, we tackle evaluation on directed multiplex networks by proposing a novel and fair testing procedure. This review constitutes a crucial step towards the development of more performant and tractable embedding learning approaches for multiplex networks and their fair evaluation for the link prediction task. We also suggest guidelines on the evaluation of models, and provide an informed perspective on the challenges and tools currently available to address downstream analyses applied to multiplex networks.

</details>


### [553] [Bayesian Integration of Nonlinear Incomplete Clinical Data](https://arxiv.org/abs/2602.01924)
*Lucía González-Zamorano,Nuria Balbás-Esteban,Vanessa Gómez-Verdejo,Albert Belenguer-Llorens,Carlos Sevilla-Salcedo*

Main category: cs.LG

TL;DR: 提出BIONIC框架集成异构多模态临床数据应对数据缺失问题，在三个数据集上表现良好且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态临床数据具有高维、异构、结构缺失特点，给预测建模、数据集成和可解释性带来挑战。

Method: 提出BIONIC统一概率框架，通过联合生成判别潜在架构集成异构多模态数据，使用预训练嵌入处理复杂模态，直接纳入结构化临床变量，显式建模缺失情况。

Result: 在三个多模态临床和生物医学数据集上评估，相比多模态基线模型，尤其在数据不完整时表现出强且一致的判别性能。

Conclusion: BIONIC除了预测准确，还通过潜在结构提供内在可解释性，支持临床有意义的洞察。

Abstract: Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.

</details>


### [554] [COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation](https://arxiv.org/abs/2602.01935)
*Annabelle Sujun Tang,Christopher Priebe,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

TL;DR: 本文提出轻量级多LLM协作框架COLT用于编译器优化，探讨多LLM协作能否超越单一大模型性能。


<details>
  <summary>Details</summary>
Motivation: 模型服务成本高，单一大模型搜索成本高，小模型单独使用不可靠，需探索多LLM协作推理性能。

Method: 提出COLT框架，使用共享MCTS树协作，内生化模型选择，每次迭代让LLM提出联合动作，引入模型感知树策略和课程变更机制。

Result: 文中未提及明确实验结果。

Conclusion: 文中未提及明确结论。

Abstract: Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.

</details>


### [555] [PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting](https://arxiv.org/abs/2602.01936)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: 提出MCPST框架用于少样本交通流量预测，在四个真实数据集上表现优于14种先进方法。


<details>
  <summary>Details</summary>
Motivation: 准确的交通流量预测是智能交通系统的挑战，尤其是跨域、数据稀缺场景下，城市移动网络复杂时空依赖和非线性动态使少样本学习更困难。

Method: 提出MCPST框架，包括多阶段引擎、自适应共识机制和结构化元学习策略，还建立了理论保证。

Result: 在四个真实数据集上，MCPST优于14种先进方法，提高预测准确性，减少训练数据需求并提供可解释见解。

Conclusion: MCPST框架在少样本交通流量预测方面有效且具有优势。

Abstract: Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.

</details>


### [556] [T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation](https://arxiv.org/abs/2602.01937)
*Suhan Guo,Bingxu Wang,Shaodan Zhang,Furao Shen*

Main category: cs.LG

TL;DR: 本文提出T - LLM框架让大语言模型具备时间序列预测能力，实验表明其在多种设置下优于现有基于大语言模型的预测方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据的时间限制使现有方法难以让大语言模型获得预测能力，现有方法也未明确教授预测行为。

Method: 提出T - LLM时间蒸馏框架，在训练时从轻量级时间教师模型转移预测行为给大语言模型，教师模型结合趋势建模和频域分析，推理时移除。

Result: 在基准数据集和传染病预测任务实验中，T - LLM在全样本、少样本和零样本设置下始终优于现有基于大语言模型的预测方法。

Conclusion: T - LLM能让通用大语言模型具备时间序列预测能力，且有简单高效的部署流程。

Abstract: Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.

</details>


### [557] [Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity](https://arxiv.org/abs/2602.01949)
*Leonardo Stoppani,Davide Bacciu,Shahab Mokarizadeh*

Main category: cs.LG

TL;DR: 提出多样性分数（DS）和边界交叉注意力（BCA）模块用于平面图生成，实验揭示真实感与多样性权衡，强调设计任务需平衡多方面。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型优化感知指标导致设计多样性受限，需解决多样性问题并提高几何一致性。

Method: 提出多样性分数（DS）量化布局多样性，引入边界交叉注意力（BCA）模块实现对建筑边界的条件约束。

Result: BCA显著提高边界贴合度，长时间训练会导致FID无法诊断的多样性崩溃，模型依赖数据集先验。

Conclusion: 建筑设计任务的生成系统需明确平衡保真度、多样性和泛化能力。

Abstract: Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fréchet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.

</details>


### [558] [Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation](https://arxiv.org/abs/2602.01956)
*Seonghyeon Park,Jewon Yeom,Jaewon Sok,Jeongjae Park,Heejun Kim,Taesup Kim*

Main category: cs.LG

TL;DR: 提出利用小草稿模型高效估计大语言模型标记级认知不确定性的框架，实验显示能降低估计误差，在幻觉检测上表现佳且推理成本低。


<details>
  <summary>Details</summary>
Motivation: 量化大语言模型不确定性对减少幻觉和风险感知部署至关重要，但用深度集成估计认知不确定性计算成本高。

Method: 利用小草稿模型估计标记级认知不确定性，基于偏差 - 方差分解，通过草稿间的 Jensen - Shannon 散度和草稿混合与目标间的 KL 散度近似，引入在线随机蒸馏和数据多样草稿策略。

Result: 在 GSM8K 上实验表明，相比基线方法估计误差（RMSE）最多降低 37%，幻觉检测性能与 TokUR 相当且推理成本可忽略。

Conclusion: 该方法为大语言模型不确定性感知部署提供了实用解决方案。

Abstract: Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.

</details>


### [559] [Grounding Generated Videos in Feasible Plans via World Models](https://arxiv.org/abs/2602.01960)
*Christos Ziakas,Amir Bar,Alessandra Russo*

Main category: cs.LG

TL;DR: 提出GVP - WM方法，将视频生成的计划转化为可行动作序列，在导航和操作模拟任务中恢复可行的长视野计划。


<details>
  <summary>Details</summary>
Motivation: 大规模视频生成模型作为零样本视觉规划器时，生成的视频计划常违反时间一致性和物理约束，映射到可执行动作时会失败。

Method: 提出GVP - WM方法，利用学习的动作条件世界模型将视频生成的计划转化为可行动作序列，测试时先从初始和目标观察生成视频计划，再通过视频引导的潜在搭配将视频指导投影到动态可行的潜在轨迹流形上，将基础问题表述为目标条件潜在空间轨迹优化问题。

Result: GVP - WM在导航和操作模拟任务中，能从违反物理约束的零样本图像到视频生成和运动模糊视频中恢复可行的长视野计划。

Conclusion: GVP - WM方法能有效解决视频生成计划违反物理约束的问题，将其转化为可行的动作序列。

Abstract: Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.

</details>


### [560] [Zero-Shot Off-Policy Learning](https://arxiv.org/abs/2602.01962)
*Arip Asadulaev,Maksim Bobrin,Salem Lahlou,Dmitry Dylov,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 文章解决零样本设置下的离策略问题，通过发现继任措施与平稳密度比的理论联系进行分布校正，在多个任务上评估有效，连接离策略学习和零样本适应。


<details>
  <summary>Details</summary>
Motivation: 离策略学习存在分布偏移和价值函数高估偏差，在零样本强化学习中问题更显著，需解决这些问题。

Method: 发现继任措施和平稳密度比的理论联系，以此推断最优重要性抽样比，进行平稳分布校正。

Result: 在多个任务上进行基准测试，技术可无缝集成到前后向表示框架，能在无训练模式下快速适应新任务。

Conclusion: 该工作弥合了离策略学习和零样本适应的差距，对两个研究领域有益。

Abstract: Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.

</details>


### [561] [Self-Consolidation for Self-Evolving Agents](https://arxiv.org/abs/2602.01966)
*Hongzhuo Yu,Fei Zhu,Guo-Sen Xie,Ling Shao*

Main category: cs.LG

TL;DR: 现有大语言模型代理静态且缺乏终身交互进化能力，依赖检索过往成功轨迹有局限，本文提出含对比反思策略和自我巩固机制的自进化框架，实验证明其在长期进化上有优势。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理为静态系统，缺乏终身交互进化能力，且依赖检索过往成功轨迹的方法有忽视失败经验价值、增加检索时间和引入噪声等局限。

Method: 提出含对比反思策略（总结易错模式和捕捉可复用见解）和自我巩固机制（将非参数文本经验提炼为可学习参数）的自进化框架。

Result: 广泛实验证明该方法在长期代理进化上有优势。

Conclusion: 所提自进化框架能解决现有大语言模型代理进化问题，可有效实现长期进化。

Abstract: While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.

</details>


### [562] [IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs](https://arxiv.org/abs/2602.01975)
*Meng Li,Peisong Wang,Yuantian Shao,Qinghao Hu,Hongjian Fang,Yifan Zhang,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 针对大语言模型部署难题，本文提出IntraSlice框架，在多模型和语言基准测试中展现出优于基线的压缩性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模大带来部署挑战，结构化剪枝性能下降，现有基于PCA的剪枝方法存在引入额外参数、破坏激活分布的问题。

Method: 提出IntraSlice框架进行块级模块内PCA压缩剪枝，设计可无额外参数融入模型的近似PCA方法，引入考虑激活分布的PCA全局剪枝率估计器。

Result: 在Llama2、Llama3和Phi系列等多模型及多种语言基准测试上实验，该方法在相同压缩率或推理速度下压缩性能优于最近的基线方法。

Conclusion: 所提IntraSlice框架有效解决了现有剪枝方法的问题，提升了大语言模型的压缩性能。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.

</details>


### [563] [FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning](https://arxiv.org/abs/2602.01976)
*Hongwei Yan,Guanglong Sun,Kanglei Zhou,Qian Li,Liyuan Wang,Yi Zhong*

Main category: cs.LG

TL;DR: 提出受果蝇记忆系统启发的FlyPrompt框架解决GCL问题，性能超现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有PET方法在GCL场景有效性受限，且未解决持续PET的两个基本挑战。

Method: 将GCL分解为专家路由和专家能力提升两个子问题，引入随机扩展分析路由器和输出头的时间集成。

Result: 在CIFAR - 100、ImageNet - R和CUB - 200上分别比现有基线最高提升11.23%、12.43%和7.62%。

Conclusion: FlyPrompt框架在解决GCL问题上表现优异。

Abstract: General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.

</details>


### [564] [SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning](https://arxiv.org/abs/2602.01990)
*Zhen-Hao Xie,Jun-Tao Tang,Yu-Cheng Shi,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.LG

TL;DR: 现有多模态持续指令调优方法存在路由漂移和专家漂移问题，本文提出SAME方法解决，实验证明其SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型需持续扩展能力，但现有多模态持续指令调优方法存在路由漂移和专家漂移问题，影响性能。

Method: 提出StAbilized Mixture-of-Experts (SAME)方法，分解路由动态到正交子空间解决路由漂移，用历史输入协方差进行曲率感知缩放缓解专家漂移，引入自适应专家激活减少冗余计算和跨任务干扰。

Result: 通过大量实验证明SAME方法达到了当前最优性能。

Conclusion: SAME方法能有效解决多模态持续指令调优中的路由漂移和专家漂移问题，提升模型性能。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.

</details>


### [565] [Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations](https://arxiv.org/abs/2602.01996)
*Theologos Anthimopoulos,Milad Kokhazadeh,Vasilios Kelefouras,Benjamin Himpel,Georgios Keramidas*

Main category: cs.LG

TL;DR: 本文介绍端到端低秩分解设计空间探索方法和工具，优化RISC - V处理器上全连接层，提升效率，提供DNNs在RISC - V架构设备部署的有效方案。


<details>
  <summary>Details</summary>
Motivation: DNNs在资源受限的RISC - V平台部署，全连接层资源消耗大，低秩分解设计空间复杂，需优化。

Method: 使用TensorFlow T3F库的Tensor Train Decomposition裁剪设计空间，应用编译器优化提升自定义T3F层性能。

Result: TT分解层在相同压缩模型上比IREE快3倍，比Pluto快8倍。

Conclusion: 此工作为RISC - V架构的边缘和嵌入式设备部署DNNs提供有效解决方案。

Abstract: Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of fully connected (FC) layers, which dominate resource consumption. Low-rank factorization (LRF) offers an effective approach to compressing FC layers, but the vast design space of LRF solutions involves complex trade-offs among FLOPs, memory size, inference time, and accuracy, making the LRF process complex and time-consuming. This paper introduces an end-to-end LRF design space exploration methodology and a specialized design tool for optimizing FC layers on RISC-V processors. Using Tensor Train Decomposition (TTD) offered by TensorFlow T3F library, the proposed work prunes the LRF design space by excluding first, inefficient decomposition shapes and second, solutions with poor inference performance on RISC-V architectures. Compiler optimizations are then applied to enhance custom T3F layer performance, minimizing inference time and boosting computational efficiency. On average, our TT-decomposed layers run 3x faster than IREE and 8x faster than Pluto on the same compressed model. This work provides an efficient solution for deploying DNNs on edge and embedded devices powered by RISC-V architectures.

</details>


### [566] [On the Limits of Layer Pruning for Generative Reasoning in LLMs](https://arxiv.org/abs/2602.01997)
*Safal Shrestha,Anubhav Shrestha,Aadim Nepal,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: 研究层剪枝在大语言模型生成推理任务中的局限性，提出微调策略并评估其效果。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝技术在生成推理任务上性能严重下降，需研究其局限性并寻找缓解方法。

Method: 对多个模型家族系统研究，采用基于自生成响应的监督微调策略。

Result: 微调策略在分类任务上恢复到基线性能的90%，在生成基准上比之前技术提高20 - 30个百分点，但生成推理恢复仍受限。

Conclusion: 明确层剪枝在生成推理中的实际局限，为受限训练下深度缩减的有效应用提供指导。

Abstract: Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.

</details>


### [567] [Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs](https://arxiv.org/abs/2602.02001)
*Yoonjun Cho,Dongjae Jeon,Soeun Kim,Moongyu Jeon,Albert No*

Main category: cs.LG

TL;DR: 提出结构化残差重建（SRR）框架用于后训练量化，能减少量化误差，支持量化参数高效微调并提升效果。


<details>
  <summary>Details</summary>
Motivation: 先前方法在权重有内在低秩结构且量化破坏主方向时，将全部秩预算用于误差重建并非最优。

Method: 提出SRR框架，保留量化前激活缩放权重的前k个奇异子空间，仅量化残差，用剩余的r - k秩进行误差重建；推导选择k的理论准则；表明参数化支持QPEFT并通过梯度缩放稳定微调。

Result: 实验显示在PTQ中不同模型和量化设置下困惑度持续降低，在2位QPEFT的GLUE上平均提升5.9个百分点。

Conclusion: SRR框架在减少后训练量化的精度损失和支持量化参数高效微调方面表现良好。

Abstract: Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\mathbf{Q} + \mathbf{L}\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.

</details>


### [568] [Logic-Guided Vector Fields for Constrained Generative Modeling](https://arxiv.org/abs/2602.02009)
*Ali Baheri*

Main category: cs.LG

TL;DR: 提出Logic - Guided Vector Fields (LGVF)框架，将符号知识注入流匹配生成模型，在多个约束生成案例中减少约束违反，有不同效果并呈现避障行为。


<details>
  <summary>Details</summary>
Motivation: 神经符号系统中生成模型缺乏在生成时执行声明性约束的机制，需要解决该问题。

Method: 提出LGVF框架，结合训练时逻辑损失惩罚约束违反和推理时用约束梯度引导采样两种机制。

Result: 在三个案例中，LGVF比标准流匹配减少59 - 82%的约束违反，在部分案例中提高分布保真度，在多障碍案例中有满意度 - 保真度权衡。

Conclusion: LGVF能有效减少约束违反，产生具有避障行为的约束感知向量场。

Abstract: Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.

</details>


### [569] [Robust Domain Generalization under Divergent Marginal and Conditional Distributions](https://arxiv.org/abs/2602.02015)
*Jewon Yeom,Kyubyung Chae,Hyunggyu Lim,Yoonna Oh,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 提出统一框架解决复合分布偏移下的领域泛化问题，通过分解联合分布推导风险边界并设计元学习过程，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法多假设条件分布偏移但标签边缘分布稳定，而现实场景常是复合分布偏移，需新方法应对。

Method: 将联合分布明确分解为边缘和条件组件，推导新的未见过领域风险边界；设计元学习过程在已见领域最小化和验证风险边界。

Result: 方法在传统领域泛化基准和多领域长尾识别设置中达到了最先进的性能。

Conclusion: 所提出的统一框架和方法能有效应对边际和条件分布同时变化的领域泛化问题，具有良好的泛化能力。

Abstract: Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.

</details>


### [570] [DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers](https://arxiv.org/abs/2602.02016)
*Ionut-Vlad Modoranu,Philip Zmushko,Erik Schultheis,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出DASH优化Shampoo优化器，用新技术加速计算，分析矩阵缩放影响，实现更快优化步骤且Newton - DB迭代验证困惑度低。


<details>
  <summary>Details</summary>
Motivation: 当前应用Shampoo优化器会导致显著的计算速度下降，需要解决其计算效率问题。

Method: 提出DASH，将预条件器块堆叠成3D张量提高GPU利用率，引入Newton - DB迭代和Chebyshev多项式近似计算逆矩阵根，深入分析矩阵缩放对Shampoo收敛的影响。

Result: GPU - aware实现比优化后的Distributed Shampoo快4.83倍，Newton - DB在所有测试方法中每次迭代的验证困惑度最低。

Conclusion: 所提出的方法有效解决了Shampoo优化器计算速度慢的问题，提升了其计算效率。

Abstract: Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \method (for \textbf{D}istributed \textbf{A}ccelerated \textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.

</details>


### [571] [On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems](https://arxiv.org/abs/2602.02045)
*Yiming Yang,Xiaoyuan Cheng,Yi He,Kaiyu Li,Wenxuan Yuan,Zhuo Sun*

Main category: cs.LG

TL;DR: 本文研究扩散模型在贝叶斯逆问题中的应用，分析求解器稳定性，指出其缺乏鲁棒性问题并提出鲁棒扩散后验采样方法，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 以往工作中贝叶斯逆问题的似然与恢复质量的联系不明确，且扩散求解器缺乏鲁棒性研究。

Method: 刻画后验近似误差并证明扩散求解器的稳定性，提出鲁棒扩散后验采样方法。

Result: 实验表明在科学逆问题和自然图像任务中，方法在似然误设情况下有一致的性能提升。

Conclusion: 所提方法有效且鲁棒，与现有基于梯度的后验采样器兼容。

Abstract: Diffusion models have recently emerged as powerful learned priors for Bayesian inverse problems (BIPs). Diffusion-based solvers rely on a presumed likelihood for the observations in BIPs to guide the generation process. However, the link between likelihood and recovery quality for BIPs is unclear in previous works. We bridge this gap by characterizing the posterior approximation error and proving the \emph{stability} of the diffusion-based solvers. Meanwhile, an immediate result of our findings on stability demonstrates the lack of robustness in diffusion-based solvers, which remains unexplored. This can degrade performance when the presumed likelihood mismatches the unknown true data generation processes. To address this issue, we propose a simple yet effective solution, \emph{robust diffusion posterior sampling}, which is provably \emph{robust} and compatible with existing gradient-based posterior samplers. Empirical results on scientific inverse problems and natural image tasks validate the effectiveness and robustness of our method, showing consistent performance improvements under challenging likelihood misspecifications.

</details>


### [572] [Dissecting Outlier Dynamics in LLM NVFP4 Pretraining](https://arxiv.org/abs/2602.02047)
*Peijie Dong,Ruibo Fan,Yuechen Tao,Di Mou,Wenhu Hu,Zhenheng Tang,Yinghao Yu,Jiamang Wang,Wenbo Su,Guodong Yang,Liping Zhang,Xiaowen Chu,Baochun Li,Bo Li*

Main category: cs.LG

TL;DR: 研究NVFP4预训练中的异常值动态，提出HCP机制和CHON训练配方，可减少NVFP4与BF16的损失差距。


<details>
  <summary>Details</summary>
Motivation: 4位算术训练大语言模型虽有优势，但NVFP4存在与BF16的损失差距，需分析异常值以改进。

Method: 对NVFP4预训练中异常值动态进行纵向分析，提出Hot - Channel Patch (HCP)机制，开发CHON训练配方。

Result: 在GLA - 1.3B模型上，CHON将与BF16的损失差距从0.94%降至0.58%，并保持下游准确率。

Conclusion: 通过对异常值的分析和提出的机制与配方，能有效减少NVFP4训练的损失差距。

Abstract: Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with "post-QK" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.

</details>


### [573] [FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification](https://arxiv.org/abs/2602.02055)
*Nan Qiao,Sheng Yue*

Main category: cs.LG

TL;DR: 提出FORLER方法解决离线联邦强化学习在低质量和异构数据下的问题，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在线联邦强化学习与真实环境交互有风险和成本，而离线联邦强化学习在低质量、异构数据下可能失效，存在局部最优和策略污染问题。

Method: 提出FORLER方法，结合服务器的Q-集合聚合和设备上的演员校正，通过零阶搜索和定制正则化器丰富策略梯度，采用$δ$-周期策略降低本地计算量。

Result: 理论上提供了安全的策略改进性能保证，广泛实验表明在不同数据质量和异构性下，FORLER始终优于强基线方法。

Conclusion: FORLER是解决离线联邦强化学习问题的有效方法。

Abstract: In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.

</details>


### [574] [FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance](https://arxiv.org/abs/2602.02060)
*Hyunsuk Chung,Caren Han,Yerin Choi,Seungyeon Ji,Jinwoo Kim,Eun-Jung Holden,Kyungreem Han*

Main category: cs.LG

TL;DR: 提出FiLoRA框架，可在固定预测目标的情况下显式控制多模态基础模型对内部特征的依赖，在多模态基准测试中表现良好，且提高了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对多模态基础模型预测如何依赖特定内部特征组以及能否控制这种依赖了解不足，且已有捷径和虚假行为研究提供的见解有限。

Method: 引入FiloRA框架，将适应分解为特征组对齐的LoRA模块，并应用指令条件门控，让自然语言指令作为计算级控制信号。

Result: 指令条件门控能在内部计算中引发一致且因果性的变化，可选择性放大或抑制核心和虚假特征组，不修改标签空间和训练目标。

Conclusion: FiLoRA揭示了一种超越相关性驱动学习来调节依赖的原则性机制，提高了模型在虚假特征干预下的鲁棒性。

Abstract: Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.

</details>


### [575] [Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits](https://arxiv.org/abs/2602.02061)
*Seoungbin Bae,Junyoung Son,Dabeen Lee*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\widetilde{\mathcal{O}}(\sqrt{t})$ for routing and a queue length regret of $\widetilde{\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.

</details>


### [576] [BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling](https://arxiv.org/abs/2602.02071)
*Zisheng Ye,Xiaoyu He,Maoyuan Song,Guoliang Qiu,Chao Liao,Chen Wu,Yonggang Sun,Zhichun Li,Xiaoru Xie,Yuanyong Luo,Hu Liu,Pinyan Lu,Heng Liao*

Main category: cs.LG

TL;DR: 量化矩阵乘法性能提升停滞，softmax成Transformer推理瓶颈，提出低精度工作流解决问题。


<details>
  <summary>Details</summary>
Motivation: 量化矩阵乘法性能提升停滞，softmax因数据带宽和高精度指数运算单元成本成为推理瓶颈。

Method: 引入采用特定8位浮点格式（HiF8）和块感知精度重缩放的低精度工作流。

Result: 评估证实方法有效，可减半数据移动带宽、降低EXP2单元面积。

Conclusion: 缓解向量计算瓶颈，可在不增加芯片面积下使端到端推理吞吐量翻倍，为低精度软硬件协同设计提供路径。

Abstract: As the performance gains from accelerating quantized matrix multiplication plateau, the softmax operation becomes the critical bottleneck in Transformer inference. This bottleneck stems from two hardware limitations: (1) limited data bandwidth between matrix and vector compute cores, and (2) the significant area cost of high-precision (FP32/16) exponentiation units (EXP2). To address these issues, we introduce a novel low-precision workflow that employs a specific 8-bit floating-point format (HiF8) and block-aware precision rescaling for softmax. Crucially, our algorithmic innovations make low-precision softmax feasible without the significant model accuracy loss that hampers direct low-precision approaches. Specifically, our design (i) halves the required data movement bandwidth by enabling matrix multiplication outputs constrained to 8-bit, and (ii) substantially reduces the EXP2 unit area by computing exponentiations in low (8-bit) precision. Extensive evaluation on language models and multi-modal models confirms the validity of our method. By alleviating the vector computation bottleneck, our work paves the way for doubling end-to-end inference throughput without increasing chip area, and offers a concrete co-design path for future low-precision hardware and software.

</details>


### [577] [Calibrating Adaptive Smoothing Methods for Freeway Traffic Reconstruction](https://arxiv.org/abs/2602.02072)
*Junyi Ji,Derek Gloudemans,Gergely Zachár,Matthew Nice,William Barbour,Daniel B. Work*

Main category: cs.LG

TL;DR: 本文实现基于真实数据端到端校准的自适应平滑方法（ASM）的Python版本，并评估结果，探讨了可重复性与局限性，可作高速路运营任务基准。


<details>
  <summary>Details</summary>
Motivation: 提供ASM的Python实现并进行端到端校准，为交通重建问题提供基准指标。

Method: 将校准建模为参数化核优化问题，用全状态观测测试床数据和稀疏雷达传感器网络输入，在PyTorch中开发实现。

Result: 从速度分布、时空误差分布和空间误差方面评估结果，展示校准方法在多条高速路上的可用性。

Conclusion: 文章具有可重复性，可作为各类高速路运营任务的基准，同时讨论了交通模型校准可重复性挑战和ASM局限性。

Abstract: The adaptive smoothing method (ASM) is a widely used approach for traffic state reconstruction. This article presents a Python implementation of ASM, featuring end-to-end calibration using real-world ground truth data. The calibration is formulated as a parameterized kernel optimization problem. The model is calibrated using data from a full-state observation testbed, with input from a sparse radar sensor network. The implementation is developed in PyTorch, enabling integration with various deep learning methods. We evaluate the results in terms of speed distribution, spatio-temporal error distribution, and spatial error to provide benchmark metrics for the traffic reconstruction problem. We further demonstrate the usability of the calibrated method across multiple freeways. Finally, we discuss the challenges of reproducibility in general traffic model calibration and the limitations of ASM. This article is reproducible and can serve as a benchmark for various freeway operation tasks.

</details>


### [578] [Learning Half-Spaces from Perturbed Contrastive Examples](https://arxiv.org/abs/2602.02080)
*Aryan Alavi Razavi Ravari,Farnam Mansouri,Yuxin Chen,Valentio Iverson,Adish Singla,Sandra Zilles*

Main category: cs.LG

TL;DR: 研究在两步对比示例预言机下的学习，引入并分析理想对比示例受扰动的机制，在两种设置下研究模型，刻画样本复杂度，表明特定条件下对比示例加速学习。


<details>
  <summary>Details</summary>
Motivation: Mansouri等人假设的是理想化设置，本文要研究理想对比示例受扰动的情况。

Method: 引入由非递减噪声函数 $f$ 参数化的机制，控制理想对比示例的扰动程度，在最大扰动幅度固定和随机两种设置下研究模型，刻画一维阈值和有界域上均匀分布的半空间的主动和被动对比样本复杂度。

Result: 在特定条件下，对比示例能在渐近查询复杂度和渐近期望查询复杂度方面加速学习。

Conclusion: 在特定条件下，对比示例的存在有助于加速学习过程。

Abstract: We study learning under a two-step contrastive example oracle, as introduced by Mansouri et. al. (2025), where each queried (or sampled) labeled example is paired with an additional contrastive example of opposite label. While Mansouri et al. assume an idealized setting, where the contrastive example is at minimum distance of the originally queried/sampled point, we introduce and analyze a mechanism, parameterized by a non-decreasing noise function $f$, under which this ideal contrastive example is perturbed. The amount of perturbation is controlled by $f(d)$, where $d$ is the distance of the queried/sampled point to the decision boundary. Intuitively, this results in higher-quality contrastive examples for points closer to the decision boundary. We study this model in two settings: (i) when the maximum perturbation magnitude is fixed, and (ii) when it is stochastic.
  For one-dimensional thresholds and for half-spaces under the uniform distribution on a bounded domain, we characterize active and passive contrastive sample complexity in dependence on the function $f$. We show that, under certain conditions on $f$, the presence of contrastive examples speeds up learning in terms of asymptotic query complexity and asymptotic expected query complexity.

</details>


### [579] [Active learning from positive and unlabeled examples](https://arxiv.org/abs/2602.02081)
*Farnam Mansouri,Sandra Zilles,Shai Ben-David*

Main category: cs.LG

TL;DR: 研究主动PU学习设置，对其标签复杂度进行首次理论分析。


<details>
  <summary>Details</summary>
Motivation: 受广告和异常检测等应用驱动，研究特定的主动PU学习设置。

Method: 对主动PU学习的标签复杂度进行理论分析。

Result: 文中未提及具体结果。

Conclusion: 文中未提及明确结论。

Abstract: Learning from positive and unlabeled data (PU learning) is a weakly supervised variant of binary classification in which the learner receives labels only for (some) positively labeled instances, while all other examples remain unlabeled. Motivated by applications such as advertising and anomaly detection, we study an active PU learning setting where the learner can adaptively query instances from an unlabeled pool, but a queried label is revealed only when the instance is positive and an independent coin flip succeeds; otherwise the learner receives no information. In this paper, we provide the first theoretical analysis of the label complexity of active PU learning.

</details>


### [580] [Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2602.02098)
*Yannik Schnitzer,Mathias Jackermeier,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 提出计算多任务策略在未见过任务上性能高置信度保证的方法，且在现实样本量下保证理论合理且有信息价值。


<details>
  <summary>Details</summary>
Motivation: 现有多任务强化学习方法在安全关键场景部署时缺乏正式性能保证。

Method: 引入新的泛化边界，结合有限次滚动的单任务下置信界和有限采样任务的任务级泛化。

Result: 在现有多任务强化学习方法中，保证在现实样本量下理论合理且有信息价值。

Conclusion: 该方法能为多任务策略在未见过任务上的性能提供高置信度保证。

Abstract: Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.

</details>


### [581] [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103)
*Liyan Xu,Mo Yu,Fandong Meng,Jie Zhou*

Main category: cs.LG

TL;DR: 本文研究大语言模型思维链动态，提出Tele - Lens方法研究其潜在规划强度，发现模型短视，提出增强思维链不确定性估计假说并验证，还表明可自动识别思维链绕过。


<details>
  <summary>Details</summary>
Motivation: 此前对思维链动态有互补观察，为深入理解大语言模型内部状态和其语言化推理轨迹的关系。

Method: 提出Tele - Lens探测方法，应用于不同任务领域的隐藏状态。

Result: 大语言模型有短视性，主要进行增量转换无精确全局规划；小部分思维链位置可代表全路径不确定性；可自动识别思维链绕过且不降低性能。

Conclusion: 强调利用思维链动态的重要性，代码、数据和模型已开源。

Abstract: This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.

</details>


### [582] [An Empirical Study of World Model Quantization](https://arxiv.org/abs/2602.02110)
*Zhongqian Fu,Tianyi Zhao,Kai Han,Hang Zhou,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: 对世界模型量化进行实证研究，用DINO - WM为案例评估不同PTQ方法，揭示量化失败模式，提供部署指导。


<details>
  <summary>Details</summary>
Motivation: 运行世界模型计算和内存成本高，模型量化对高效部署至关重要，但PTQ对世界模型的影响未被充分研究。

Method: 以DINO - WM为代表案例，在权重和权重 - 激活联合设置下评估不同PTQ方法，在不同视觉规划任务上进行多参数的广泛实验。

Result: 世界模型量化效果超标准精度和位宽权衡；分组权重量化可稳定低比特滚动；激活量化粒度效益不一致；编码器和预测器模块量化敏感性不对称；激进低比特量化会严重破坏规划目标与任务成功的一致性。

Conclusion: 揭示基于世界模型规划中不同的量化失败模式，为严格计算约束下部署量化世界模型提供实用指导。

Abstract: World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.

</details>


### [583] [Unifying Masked Diffusion Models with Various Generation Orders and Beyond](https://arxiv.org/abs/2602.02112)
*Chunsan Hong,Sanghyun Lee,Jong Chul Ye*

Main category: cs.LG

TL;DR: 提出OeMDM和LoMDM用于语言生成，LoMDM在多基准测试中表现优于其他离散扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有MDM语言生成质量依赖生成顺序，先前方法有硬编码顺序或两阶段优化带来额外成本和次优解的问题。

Method: 提出OeMDM统一多种扩散生成过程，基于OeMDM引入LoMDM，通过单一目标从头联合学习生成顺序和扩散主干。

Result: LoMDM在多个语言建模基准测试中优于各种离散扩散模型。

Conclusion: LoMDM在语言生成方面有更好的性能。

Abstract: Masked diffusion models (MDMs) are a potential alternative to autoregressive models (ARMs) for language generation, but generation quality depends critically on the generation order. Prior work either hard-codes an ordering (e.g., blockwise left-to-right) or learns an ordering policy for a pretrained MDM, which incurs extra cost and can yield suboptimal solutions due to the two-stage optimization. Motivated by this, we propose order-expressive masked diffusion model (OeMDM) for a broad class of diffusion generative processes with various generation orders, enabling the interpretation of MDM, ARM, and block diffusion in a single framework. Furthermore, building on OeMDM, we introduce learnable-order masked diffusion model (LoMDM), which jointly learns the generation ordering and diffusion backbone through a single objective from scratch, enabling the diffusion model to generate text in context-dependent ordering. Empirically, we confirm that LoMDM outperforms various discrete diffusion models across multiple language modeling benchmarks.

</details>


### [584] [The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning](https://arxiv.org/abs/2602.02117)
*Youqi Wu,Farzan Farnia*

Main category: cs.LG

TL;DR: 本文将最大熵原理的极小极大公式扩展到冯·诺伊曼熵（VNE），为VNE最大化提供博弈论解释，还通过两个应用展示其在机器学习中的应用。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动背景下，VNE最大化缺少类似经典最大熵框架的理论及其决策和博弈论解释。

Method: 将Grünwald和Dawid的最大熵原理的极小极大公式扩展到VNE的设定。

Result: 为VNE最大化提供博弈论解释，得出的最大VNE原理可应用于现代机器学习问题。

Conclusion: 提出的框架为核学习中基于VNE的方法提供了统一的信息论基础。

Abstract: Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Grünwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.

</details>


### [585] [Two-Stage Grid Optimization for Group-wise Quantization of LLMs](https://arxiv.org/abs/2602.02126)
*Junhan Kim,Gukryeol Lee,Seungwoo Son,Jeewook Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: 提出两阶段优化框架用于组尺度，提升大模型低比特量化准确性且开销小。


<details>
  <summary>Details</summary>
Motivation: 现有GPTQ方法确定组尺度时忽略输入统计和组间相关性，与最小化层重建损失目标不匹配。

Method: 提出两阶段优化框架，第一阶段初始化组尺度以最小化组重建损失；第二阶段冻结GPTQ得到的整数权重，使用坐标下降算法和闭式更新规则优化组尺度，同时考虑前层量化误差。

Result: 实验表明该方法持续提升组量化，在可忽略开销下实现更高精度。

Conclusion: 两阶段优化框架有效提升大语言模型低比特量化准确性，且开销小。

Abstract: Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.

</details>


### [586] [Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics](https://arxiv.org/abs/2602.02128)
*Nima Shoghi,Yuxuan Liu,Yuning Shen,Rob Brekelmans,Pan Li,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出STAR - MD模型加速蛋白质动力学模拟，在ATLAS基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统MD模拟计算成本高，现有生成模型在长时间序列生成上存在问题。

Method: 提出STAR - MD，采用因果扩散变压器和联合时空注意力机制。

Result: 在ATLAS基准测试中达到了最先进水平，能生成稳定的微秒级轨迹。

Conclusion: 当前模型在长序列生成方面存在缺陷，STAR - MD的联合时空建模可实现生物相关时间尺度的动力学模拟。

Abstract: Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.

</details>


### [587] [DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations](https://arxiv.org/abs/2602.02137)
*Minghao Li,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.LG

TL;DR: 提出DCoPilot框架解决现代数据中心控制策略滞后问题，经评估表现出色，消融实验验证部分设计有效性。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心功率密度高、工作负载变化快，手动设计DRL代理无法跟上动态变化和SLA变更，导致缺乏及时有效的控制策略，可能引发服务中断。

Method: DCoPilot结合大语言模型进行结构化奖励形式的符号生成和超网络进行策略权重的参数生成，通过模拟扩展、元策略蒸馏和在线适应三个阶段工作。

Result: 在五个控制任务族中评估，DCoPilot实现近乎零约束违规，在规格变化时优于所有基线。

Conclusion: DCoPilot能有效解决数据中心控制策略滞后问题，基于LLM的统一奖励生成有助于超网络稳定收敛。

Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.

</details>


### [588] [EvoMU: Evolutionary Machine Unlearning](https://arxiv.org/abs/2602.02139)
*Pawel Batorski,Paul Swoboda*

Main category: cs.LG

TL;DR: 论文提出EvoMU方法，通过进化搜索程序在大量可能的遗忘损失函数空间中自动寻找特定于任务的损失函数，在有限计算资源下取得SotA结果，超越了先前基于损失的遗忘方法。


<details>
  <summary>Details</summary>
Motivation: 合适的遗忘损失函数空间巨大，搜索最优损失函数困难，且可能不存在通用最优损失函数，不同数据情况会导致损失函数效果差异大。

Method: 采用进化搜索程序在大量可能的遗忘损失函数空间中自动寻找特定于任务的损失函数。

Result: 使用小的4B参数模型（Qwen3 - 4B - Thinking）取得SotA结果，在TOFU - 5%、TOFU - 10%、MUSE和WMDP上超越先前基于损失的遗忘方法。

Conclusion: EvoMU方法在有限计算资源下展示了人工智能共同科学家的潜力，能合成新的遗忘损失函数以提升遗忘效果，代码已开源。

Abstract: Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.

</details>


### [589] [Learning Generative Selection for Best-of-N](https://arxiv.org/abs/2602.02143)
*Shubham Toshniwal,Aleksander Ficek,Siddhartha Jain,Wei Du,Vahid Noroozi,Sadegh Mahdavi,Somshubra Majumdar,Igor Gitman*

Main category: cs.LG

TL;DR: 通过平行采样扩展测试时计算可提升大语言模型推理能力，但受最佳N选择质量限制。研究表明小推理模型可通过强化学习获得强大的通用选择能力，在推理基准测试中表现出色，证明强化学习可解锁小模型的强大生成选择能力。


<details>
  <summary>Details</summary>
Motivation: 解决并行采样提升大语言模型推理能力时受最佳N选择质量限制的瓶颈，且当前强选择性能主要局限于大模型，想让小推理模型具备强大的通用选择能力。

Method: 从大规模数学和代码指令数据集中合成选择任务，筛选出有正确和错误候选解的实例，用DAPO训练17亿参数模型以奖励正确选择。

Result: 在数学和代码推理基准测试中，模型持续超越提示和多数表决基线，常接近或超过更大模型，且训练时仅用弱模型输出，对更强模型输出的选择能力也有泛化。

Conclusion: 强化学习是一种可扩展的方法，能解锁小模型的强大生成选择能力，实现高效测试时扩展。

Abstract: Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.

</details>


### [590] [Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting](https://arxiv.org/abs/2602.02146)
*Sunho Kim,Susik Yoon*

Main category: cs.LG

TL;DR: 提出BTTF框架解决LTSF中并行效率与时序建模的权衡问题，提升了长时预测准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决LTSF中DMS方法缺乏时序一致性、IMS方法存在误差累积和推理慢的问题。

Method: 提出BTTF框架，通过前瞻增强和自校正细化来提升预测稳定性，对基础模型进行集成优化。

Result: 持续提高长时预测准确性，减少线性预测模型的不稳定性，最高提升58%，在次优训练条件下也有稳定提升。

Conclusion: 利用模型生成的预测进行增强是提升长时预测的简单有效方法，无需复杂架构。

Abstract: Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across steps, while iterative multi-step forecasting (IMS) preserves temporal dependencies at the cost of error accumulation and slow inference. To bridge this gap, we propose Back to the Future (BTTF), a simple yet effective framework that enhances forecasting stability through look-ahead augmentation and self-corrective refinement. Rather than relying on complex model architectures, BTTF revisits the fundamental forecasting process and refines a base model by ensembling the second-stage models augmented with their initial predictions. Despite its simplicity, our approach consistently improves long-horizon accuracy and mitigates the instability of linear forecasting models, achieving accuracy gains of up to 58% and demonstrating stable improvements even when the first-stage model is trained under suboptimal conditions. These results suggest that leveraging model-generated forecasts as augmentation can be a simple yet powerful way to enhance long-term prediction, even without complex architectures.

</details>


### [591] [ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning](https://arxiv.org/abs/2602.02150)
*Chu Zhao,Enneng Yang,Yuting Liu,Jianzhe Zhao,Guibing Guo*

Main category: cs.LG

TL;DR: 提出ECHO方法解决测试时强化学习中树结构滚动的高熵分支和早期伪标签问题，实验显示在多基准测试有增益。


<details>
  <summary>Details</summary>
Motivation: 现有树结构滚动的测试时强化学习范式面临高熵分支导致滚动崩溃和早期伪标签有噪声偏差的问题，需改进。

Method: 提出ECHO方法，在滚动时结合局部熵和组级置信度控制分支宽度、引入基于置信度的修剪；在策略更新时采用置信度自适应裁剪和熵置信度混合优势塑造方法。

Result: ECHO在多个数学和视觉推理基准测试中取得一致增益，在有限滚动预算下泛化效果更好。

Conclusion: ECHO有效解决了现有范式的问题，能提升测试时强化学习的性能。

Abstract: Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.

</details>


### [592] [Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization](https://arxiv.org/abs/2602.02151)
*Yuli Zhou,Qingxuan Chen,Luca Benini,Guolei Sun,Yawei Li*

Main category: cs.LG

TL;DR: 提出VQRound优化框架，重参数化舍入矩阵，优化舍入初始化，在多模型实验中表现优，证明自适应舍入可扩展且快速拟合。


<details>
  <summary>Details</summary>
Motivation: 传统自适应舍入的密集逐元素舍入矩阵对大语言模型成本过高，希望从效率角度改进。

Method: 提出VQRound参数高效的优化框架，将舍入矩阵重参数化为紧凑码本；确定舍入初始化的关键因素，开发轻量级端到端微调管道。

Result: 在OPT、LLaMA、LLaMA2、Qwen3等模型实验中，VQRound在相同步数下比传统自适应舍入收敛更好，仅用0.2%的可训练参数。

Conclusion: 自适应舍入可以做到可扩展且快速拟合。

Abstract: Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.

</details>


### [593] [Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing](https://arxiv.org/abs/2602.02157)
*Egor Serov,Ilya Kuleshov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出用核与高斯过程平滑替代精确插值的神经CDE路径构建方法，及基于注意力的MV - CDE和MVC - CDE，提升效率和精度。


<details>
  <summary>Details</summary>
Motivation: 神经CDE中驱动控制路径的粗糙度限制效率，标准样条引入高频变化使函数评估次数增加。

Method: 用核与高斯过程平滑替代精确插值控制轨迹规律性；提出基于注意力的MV - CDE及其卷积扩展MVC - CDE，用可学习查询辅助路径重建。

Result: MVC - CDE with GP达到了最先进的精度，且相比基于样条的基线显著减少了函数评估次数和总推理时间。

Conclusion: 所提方法在保证精度的同时提高了神经CDE的效率。

Abstract: Neural Controlled Differential Equations (Neural CDEs) provide a powerful continuous-time framework for sequence modeling, yet the roughness of the driving control path often restricts their efficiency. Standard splines introduce high-frequency variations that force adaptive solvers to take excessively small steps, driving up the Number of Function Evaluations (NFE). We propose a novel approach to Neural CDE path construction that replaces exact interpolation with Kernel and Gaussian Process (GP) smoothing, enabling explicit control over trajectory regularity. To recover details lost during smoothing, we propose an attention-based Multi-View CDE (MV-CDE) and its convolutional extension (MVC-CDE), which employ learnable queries to inform path reconstruction. This framework allows the model to distribute representational capacity across multiple trajectories, each capturing distinct temporal patterns. Empirical results demonstrate that our method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and total inference time compared to spline-based baselines.

</details>


### [594] [Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction](https://arxiv.org/abs/2602.02161)
*Aniq Ur Rahman,Justin P. Coon*

Main category: cs.LG

TL;DR: 提出用于时间链接预测（TLP）模型反事实验证的框架，为因果感知基准测试奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有TLP模型评估未考量是否捕捉到因果机制，需新评估框架。

Method: 生成具有已知因果结构的因果时间交互图（CTIGs），引入连续时间事件序列结构方程模型，提出基于跨模型预测误差的距离度量。

Result: 验证在不同因果模型上训练的预测器在评估时性能会下降，可在可控因果转移和时间戳混洗下进行反事实评估。

Conclusion: 所提框架为因果感知的基准测试提供基础。

Abstract: Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.

</details>


### [595] [Interpretable Tabular Foundation Models via In-Context Kernel Regression](https://arxiv.org/abs/2602.02162)
*Ratmir Miftachov,Bruno Charron,Simon Valentin*

Main category: cs.LG

TL;DR: 提出KernelICL框架增强表格基础模型可解释性，在55个数据集上表现与现有模型相当。


<details>
  <summary>Details</summary>
Motivation: 现有表格基础模型架构不透明，缺乏可解释性。

Method: 基于上下文学习类似核回归的见解，用核函数替换最终预测层，引入二维分类法统一多种方法，通过训练样本权重分布的困惑度量化可检查性。

Result: 在55个TALENT基准数据集上，KernelICL性能与现有表格基础模型相当。

Conclusion: 对最终层施加显式核约束可在不牺牲性能的情况下实现可检查的预测。

Abstract: Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.

</details>


### [596] [Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents](https://arxiv.org/abs/2602.02164)
*Pengfei He,Ash Fox,Lesly Miculicich,Stefan Friedli,Daniel Fabian,Burak Gokturk,Jiliang Tang,Chen-Yu Lee,Tomas Pfister,Long T. Le*

Main category: cs.LG

TL;DR: 提出Co - RedTeam框架用于自动漏洞发现与利用，在安全基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型辅助网络安全任务在自动漏洞发现和利用方面存在交互有限、执行基础弱和缺乏经验复用等问题。

Method: 提出Co - RedTeam安全感知多智能体框架，将漏洞分析分解为发现和利用阶段，整合安全领域知识等，让智能体基于执行反馈行动并从过往轨迹学习。

Result: 在具有挑战性的安全基准测试中，Co - RedTeam始终优于强基线，漏洞利用成功率超60%，漏洞检测绝对提升超10%。

Conclusion: 执行反馈、结构化交互和内存对构建强大且通用的网络安全智能体至关重要。

Abstract: Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.

</details>


### [597] [Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach](https://arxiv.org/abs/2602.02173)
*Jiancheng Tu,Wenqi Fan,Zhibin Wu*

Main category: cs.LG

TL;DR: 提出基于MIP的框架学习最优分类树，开发加速技术，在50个数据集上评估，结果优。


<details>
  <summary>Details</summary>
Motivation: 全球优化决策树是组合优化挑战，但在可解释机器学习中重要，现有研究需改进以解决非线性性能指标和类别不平衡问题。

Method: 提出基于MIP的框架，开发特定加速技术，如定制分支切割算法、实例缩减方案和热启动策略。

Result: 在50个基准数据集上，框架能有效优化非线性指标，预测性能强，求解时间比现有方法短。

Conclusion: 所提基于MIP的框架可高效学习最优分类树，解决非线性指标和类别不平衡问题。

Abstract: Global optimization of decision trees is a long-standing challenge in combinatorial optimization, yet such models play an important role in interpretable machine learning. Although the problem has been investigated for several decades, only recent advances in discrete optimization have enabled practical algorithms for solving optimal classification tree problems on real-world datasets. Mixed-integer programming (MIP) offers a high degree of modeling flexibility, and we therefore propose a MIP-based framework for learning optimal classification trees under nonlinear performance metrics, such as the F1-score, that explicitly addresses class imbalance. To improve scalability, we develop problem-specific acceleration techniques, including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies. We evaluate the proposed approach on 50 benchmark datasets. The computational results show that the framework can efficiently optimize nonlinear metrics while achieving strong predictive performance and reduced solution times compared with existing methods.

</details>


### [598] [SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02179)
*Marina Mastroleo,Alberto Archetti,Federico Mastroleo,Matteo Matteucci*

Main category: cs.LG

TL;DR: 提出基于KAN架构的全参数、时间连续的生存模型SurvKAN，消除比例风险约束，实验表现好且有可解释性


<details>
  <summary>Details</summary>
Motivation: 现有经典生存模型和深度学习方法存在缺乏表达力、可解释性的问题，CoxKAN等受半参数Cox框架约束

Method: 引入SurvKAN，将时间作为KAN显式输入直接预测对数风险函数，可进行端到端训练，通过可学习单变量函数保留可解释性

Result: 在标准生存基准测试中，SurvKAN在一致性和校准指标上比经典和先进基线有竞争力或更优，可解释性分析揭示与医学知识相符的模式

Conclusion: SurvKAN能有效解决现有模型在表达力和可解释性上的问题，在医疗应用中有潜力。

Abstract: Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.

</details>


### [599] [STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs](https://arxiv.org/abs/2602.02180)
*Weikang Meng,Liangyu Huo,Yadan Luo,Jiawen Guan,Jingyi Zhang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出STILL框架高效线性化大语言模型，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型线性化方法基于滑动窗口分区进行令牌路由，无法捕捉令牌特定的全局重要性，且线性注意力存在特征图导致的分布偏移问题。

Method: 引入具有强局部 - 全局一致性的自显著性分数，设计保留范数的特征映射，采用统一的训练推理架构和分块并行及延迟选择。

Result: 在常识和一般推理任务上达到或超越原预训练模型，在长上下文基准测试中相对先前线性化注意力方法最高提升86.2%。

Conclusion: STILL框架能有效解决现有大语言模型线性化方法的问题，提升性能。

Abstract: Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fails to capture token-specific global importance. Meanwhile, linear attention further suffers from distribution shift caused by learnable feature maps that distort pretrained feature magnitudes. Motivated by these limitations, we propose STILL, an intra-layer hybrid linearization framework for efficiently linearizing LLMs. STILL introduces a Self-Saliency Score with strong local-global consistency, enabling accurate token selection using sliding-window computation, and retains salient tokens for sparse softmax attention while summarizing the remaining context via linear attention. To preserve pretrained representations, we design a Norm-Preserved Feature Map (NP-Map) that decouples feature direction from magnitude and reinjects pretrained norms. We further adopt a unified training-inference architecture with chunk-wise parallelization and delayed selection to improve hardware efficiency. Experiments show that STILL matches or surpasses the original pretrained model on commonsense and general reasoning tasks, and achieves up to a 86.2% relative improvement over prior linearized attention methods on long-context benchmarks.

</details>


### [600] [State Rank Dynamics in Linear Attention LLMs](https://arxiv.org/abs/2602.02195)
*Ao Sun,Hongtao Zhang,Heng Zhou,Yixuan Ma,Yiran Qin,Tongrui Su,Yan Liu,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 研究线性注意力大语言模型运行时状态动态，发现状态秩分层现象，提出联合秩 - 范数剪枝策略减少KV缓存开销。


<details>
  <summary>Details</summary>
Motivation: 线性注意力大语言模型压缩状态的内部动态不透明，需深入研究。

Method: 对先进线性注意力模型运行时状态动态进行全面研究，通过广泛实验和诊断探针分析。

Result: 发现状态秩分层现象，低秩和高秩头特性稳定，低秩头对推理重要，高秩头冗余；提出策略减少38.9% KV缓存开销且保持模型精度。

Conclusion: 线性注意力头的秩特性是预训练获得的固有属性；联合秩 - 范数剪枝策略有效。

Abstract: Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\% reduction in KV-cache overhead while largely maintaining model accuracy.

</details>


### [601] [Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models](https://arxiv.org/abs/2602.02197)
*Xindian Ma,Yidi Lu,Peng Zhang,Jing Zhang*

Main category: cs.LG

TL;DR: 提出分层自适应驱逐框架HAE优化多模态大语言模型中KV缓存驱逐，减少KV缓存使用并提升效率，通过实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: Transformer架构的二次内存和计算成本是多模态大语言模型的瓶颈，现有KV缓存驱逐策略未解决视觉和文本令牌的异构注意力分布问题，导致效率欠佳或性能下降。

Method: 提出分层自适应驱逐（HAE）框架，包括预填充时的双注意力剪枝和推理时的动态解码驱逐策略。

Result: 在图像理解任务中，HAE减少41%的KV缓存内存，准确率仅下降0.3%；在Phi3.5 - Vision - Instruct模型的故事生成推理中加速1.5倍并保持输出质量。

Conclusion: HAE框架能优化多模态大语言模型中文本 - 视觉令牌的交互，减少KV缓存使用，降低计算开销，提升理解和生成任务的效率。

Abstract: The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\% with minimal accuracy loss (0.3\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.

</details>


### [602] [Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction](https://arxiv.org/abs/2602.02201)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: 介绍CardinalGraphFormer用于分子属性预测，在多个基准任务上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 药物发现需在有限标记数据下高效进行分子属性预测，自监督预训练对数据高效的分子表示学习很重要。

Method: 引入CardinalGraphFormer，结合Graphormer结构偏差，采用结构化稀疏注意力机制，预训练结合对比图级对齐和掩码属性重建。

Result: 在完全匹配的评估协议下，CardinalGraphFormer在11个评估任务中提高了平均性能，在10个公共基准上有统计学显著提升。

Conclusion: CardinalGraphFormer在分子属性预测任务上表现良好。

Abstract: Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular corpora has become essential for data-efficient molecular representation learning. We introduce **CardinalGraphFormer**, a graph transformer that incorporates Graphormer-inspired structural biases, including shortest-path distance and centrality, as well as direct-bond edge bias, within a structured sparse attention regime limited to shortest-path distance <= 3. The model further augments this design with a cardinality-preserving unnormalized aggregation channel over the same support set. Pretraining combines contrastive graph-level alignment with masked attribute reconstruction. Under a fully matched evaluation protocol, CardinalGraphFormer improves mean performance across all 11 evaluated tasks and achieves statistically significant gains on 10 of 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET tasks when compared to strong reproduced baselines.

</details>


### [603] [Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning](https://arxiv.org/abs/2602.02206)
*Tong Yang,Yemin Wang,Chaoning Zhang,Aming Wu*

Main category: cs.LG

TL;DR: 本文提出文档驱动的代理架构Fat - Cat，提升状态管理信噪比，经多基准测试验证其能提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代理框架因使用刚性、语法复杂的状态表示，使模型注意力用于句法处理而非语义推理，限制了代理有效性。

Method: 提出Fat - Cat架构，包含语义文件系统、文本策略进化模块和闭环监视器三个关键组件。

Result: 在推理、检索和编码基准测试中，Fat - Cat持续提升代理性能，使Kimi - k2模型在HotPotQA上超越GPT - 4o基线，用JSON替代文档状态会致性能下降。

Conclusion: 文档驱动的状态建模比刚性语法更关键，代码开源。

Abstract: The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.

</details>


### [604] [Generating Physically Sound Designs from Text and a Set of Physical Constraints](https://arxiv.org/abs/2602.02213)
*Gregory Barber,Todd C. Henry,Mulugeta A. Haile*

Main category: cs.LG

TL;DR: 提出TIDES方法，结合文本描述和物理约束生成设计，联合优化结构与视觉属性，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 基于文本描述和物理约束生成具有物理合理性的设计，并联合优化结构和视觉属性。

Method: 使用预训练的文本 - 图像模型评估设计与文本提示的视觉一致性，用可微物理模拟器评估物理性能。

Result: 在一系列结构优化问题上评估，能联合优化两个目标，返回满足工程设计要求且利用文本指定特征的设计。

Conclusion: TIDES方法可有效联合优化结构和视觉属性，满足设计要求。

Abstract: We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.

</details>


### [605] [Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism](https://arxiv.org/abs/2602.02215)
*Sebastian Müller,Vanessa Toborek,Eike Stadtländer,Tamás Horváth,Brendan Balcerak Jackson,Christian Bauckhage*

Main category: cs.LG

TL;DR: 提出黑盒科学理论（SToBB）概念，构建通用框架及实例算法，为黑盒模型提供生命周期尺度可检查参考。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏将黑盒模型解释信息整合为贯穿其生命周期审计工件的原则性方法。

Method: 基于建构经验主义引入SToBB概念并明确三个义务，构建通用框架，通过接口查询维护记录获取解释，实现概念验证示例。

Result: 实例化神经网络分类器的完整SToBB，引入CoBoT算法构建并维护基于规则的替代模型。

Conclusion: SToBB可作为生命周期尺度可检查参考，支持一致、可复用分析和外部审查。

Abstract: Explainable AI (XAI) offers a growing number of algorithms that aim to answer specific questions about black-box models. What is missing is a principled way to consolidate explanatory information about a fixed black-box model into a persistent, auditable artefact, that accompanies the black-box throughout its life cycle. We address this gap by introducing the notion of a scientific theory of a black (SToBB). Grounded in Constructive Empiricism, a SToBB fulfils three obligations: (i) empirical adequacy with respect to all available observations of black-box behaviour, (ii) adaptability via explicit update commitments that restore adequacy when new observations arrive, and (iii) auditability through transparent documentation of assumptions, construction choices, and update behaviour. We operationalise these obligations as a general framework that specifies an extensible observation base, a traceable hypothesis class, algorithmic components for construction and revision, and documentation sufficient for third-party assessment. Explanations for concrete stakeholder needs are then obtained by querying the maintained record through interfaces, rather than by producing isolated method outputs. As a proof of concept, we instantiate a complete SToBB for a neural-network classifier on a tabular task and introduce the Constructive Box Theoriser (CoBoT) algorithm, an online procedure that constructs and maintains an empirically adequate rule-based surrogate as observations accumulate. Together, these contributions position SToBBs as a life cycle-scale, inspectable point of reference that supports consistent, reusable analyses and systematic external scrutiny.

</details>


### [606] [Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts](https://arxiv.org/abs/2602.02229)
*Guangyi Zhang,Yunlong Cai,Guanding Yu,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 研究动态环境下标签数据有限时模型性能监测问题，提出PPRM方法并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中标签数据有限时模型性能监测的问题。

Method: 提出基于预测驱动推理（PPI）的半监督风险监测方法PPRM，结合合成标签和少量真实标签构建运行风险的随时有效下界，通过与名义风险上界的基于阈值比较检测有害偏移。

Result: 通过在图像分类、大语言模型和电信监测任务上的大量实验证明了PPRM的有效性。

Conclusion: PPRM是一种有效的半监督风险监测方法，能在有限样本下满足无假设的误报概率保证。

Abstract: We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.

</details>


### [607] [SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting](https://arxiv.org/abs/2602.02230)
*Ziyu Zhou,Yuchen Fang,Weilin Ruan,Shiyu Wang,James Kwok,Yuxuan Liang*

Main category: cs.LG

TL;DR: 为解决现有方法忽略不规则多元时间序列 (IMTS) 的稀疏-事件对偶 (SED) 属性的问题，提出 SEDformer 模型实现准确高效的遥测 IMTS 预测。


<details>
  <summary>Details</summary>
Motivation: 现有基于图和 Transformer 的预测器忽略了 IMTS 的 SED 属性，导致预测效果不佳，需要更贴合 SED 属性的建模范式。

Method: 提出 SEDformer 模型，包括基于 SED 的尖峰编码器、事件保留时间下采样模块和基于 SED 的尖峰 Transformer 块。

Result: 在公共遥测 IMTS 数据集上，SEDformer 达到了最先进的预测精度，同时降低了能源和内存使用。

Conclusion: SEDformer 为 IMTS 建模提供了自然且高效的途径。

Abstract: Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.

</details>


### [608] [Geometry- and Relation-Aware Diffusion for EEG Super-Resolution](https://arxiv.org/abs/2602.02238)
*Laura Yao,Gengwei Zhang,Moajjem Chowdhury,Yunmei Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 现有EEG空间超分辨率方法缺乏生理空间结构感知，本文提出TopoDiff模型，在多个数据集上提升生成保真度和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的EEG空间超分辨率方法常缺乏对生理空间结构的感知，限制了空间生成性能。

Method: 引入TopoDiff，结合拓扑感知的图像嵌入提供全局几何上下文，使用动态通道关系图编码电极间关系并随时间动态变化。

Result: 在多个EEG数据集上，该方法在生成保真度上有大幅提升，下游EEG任务性能也显著提高。

Conclusion: TopoDiff是一个基于空间的EEG空间超分辨率框架，能持续提升性能。

Abstract: Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.

</details>


### [609] [Interpretability in Deep Time Series Models Demands Semantic Alignment](https://arxiv.org/abs/2602.02239)
*Giovanni De Felice,Riccardo D'Elia,Alberto Termine,Pietro Barbiero,Giuseppe Marra,Silvia Santini*

Main category: cs.LG

TL;DR: 现有深度时间序列模型因黑箱特性部署受限，现有可解释性方法有不足，本文提出应追求语义对齐，并给出相关定义、模型蓝图并讨论影响。


<details>
  <summary>Details</summary>
Motivation: 深度时间序列模型因黑箱特性部署受限，现有可解释性方法未解决与人类推理的对齐问题。

Method: 定义语义对齐要求，强调在时间演化下保持语义对齐，给出语义对齐的深度时间序列模型蓝图。

Result: 明确语义对齐定义，确定支持信任的属性。

Conclusion: 从语义对齐角度为深度时间序列模型设计提供思考和方向。

Abstract: Deep time series models continue to improve predictive performance, yet their deployment remains limited by their black-box nature. In response, existing interpretability approaches in the field keep focusing on explaining the internal model computations, without addressing whether they align or not with how a human would reason about the studied phenomenon. Instead, we state interpretability in deep time series models should pursue semantic alignment: predictions should be expressed in terms of variables that are meaningful to the end user, mediated by spatial and temporal mechanisms that admit user-dependent constraints. In this paper, we formalize this requirement and require that, once established, semantic alignment must be preserved under temporal evolution: a constraint with no analog in static settings. Provided with this definition, we outline a blueprint for semantically aligned deep time series models, identify properties that support trust, and discuss implications for model design.

</details>


### [610] [Variational Entropic Optimal Transport](https://arxiv.org/abs/2602.02241)
*Roman Dyachenko,Nikita Gushchin,Kirill Sokolov,Petr Mokrov,Evgeny Burnaev,Alexander Korotin*

Main category: cs.LG

TL;DR: 提出变分熵最优传输（VarEOT）方法解决领域翻译问题，避免MCMC模拟，实验显示有竞争力的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有解决领域翻译问题的弱对偶EOT目标优化方法计算效率低，因难处理的对数配分项。

Method: 基于对数配分的精确变分重新表述，得到可微学习目标，用随机梯度优化。

Result: 在合成数据和图像翻译实验中展示有竞争力或改进的翻译质量，对比显示所提优化原则的优势。

Conclusion: VarEOT方法有效，避免训练中MCMC模拟，有理论保证且实验效果好。

Abstract: Entropic optimal transport (EOT) in continuous spaces with quadratic cost is a classical tool for solving the domain translation problem. In practice, recent approaches optimize a weak dual EOT objective depending on a single potential, but doing so is computationally not efficient due to the intractable log-partition term. Existing methods typically resolve this obstacle in one of two ways: by significantly restricting the transport family to obtain closed-form normalization (via Gaussian-mixture parameterizations), or by using general neural parameterizations that require simulation-based training procedures. We propose Variational Entropic Optimal Transport (VarEOT), based on an exact variational reformulation of the log-partition $\log \mathbb{E}[\exp(\cdot)]$ as a tractable minimization over an auxiliary positive normalizer. This yields a differentiable learning objective optimized with stochastic gradients and avoids the necessity of MCMC simulations during the training. We provide theoretical guarantees, including finite-sample generalization bounds and approximation results under universal function approximation. Experiments on synthetic data and unpaired image-to-image translation demonstrate competitive or improved translation quality, while comparisons within the solvers that use the same weak dual EOT objective support the benefit of the proposed optimization principle.

</details>


### [611] [Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models](https://arxiv.org/abs/2602.02244)
*Hao Wang,Hao Gu,Hongming Piao,Kaixiong Gong,Yuxiao Ye,Xiangyu Yue,Sirui Han,Yike Guo,Dapeng Wu*

Main category: cs.LG

TL;DR: 提出CurioSFT方法提升大推理模型探索能力，实验表明在SFT和RL阶段均有提升。


<details>
  <summary>Details</summary>
Motivation: 标准的大推理模型训练方法SFT - then - RL中，SFT会导致过度自信和降低生成多样性，限制RL阶段的收益，添加熵正则化也并非万能。

Method: 提出CurioSFT方法，包括Self - Exploratory Distillation和Entropy - Guided Temperature Selection。

Result: 在数学推理任务实验中，CurioSFT在SFT阶段的分布内任务上比普通SFT高2.5分，分布外任务高2.9分；在RL阶段平均提升5.0分。

Conclusion: CurioSFT能有效提升大推理模型的探索能力，并在后续RL阶段带来实际收益。

Abstract: The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.

</details>


### [612] [Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258)
*Gaurav Bhatt,Aditya Chinchure,Jiawei Zhou,Leonid Sigal*

Main category: cs.LG

TL;DR: 提出对齐感知微调框架，结合外部对齐信号反馈，在基准测试中减少有害和幻觉输出且不牺牲性能，证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 标准微调方法孤立优化任务目标，不考虑关键对齐目标，会导致下游微调时模型对齐性下降。

Method: 提出基于策略梯度正则化集成外部对齐信号反馈的框架，引入自适应门控机制动态平衡梯度，学习对完全未对齐输入的弃权行为。

Result: 在通用和特定领域指令调优基准测试中，持续减少有害和幻觉输出，不牺牲下游任务性能，对对抗性微调、基于提示的攻击和不安全初始化有鲁棒性。

Conclusion: 自适应门控对齐优化是一种有效的保持和恢复模型对齐性的适应方法。

Abstract: Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.

</details>


### [613] [Segment to Focus: Guiding Latent Action Models in the Presence of Distractors](https://arxiv.org/abs/2602.02259)
*Hamza Adnan,Matthew T. Jackson,Alexey Zakharov*

Main category: cs.LG

TL;DR: 提出MaskLAM方法改进Latent Action Models (LAMs)，在连续控制任务中效果显著。


<details>
  <summary>Details</summary>
Motivation: LAMs在从原始观察中提取动作相关表示时，难以将动作相关特征与动作相关噪声分离，导致捕获虚假相关性并构建次优潜在动作空间。

Method: 引入MaskLAM，利用预训练基础模型的分割掩码对LAM重建损失进行加权，优先处理显著信息，无需架构修改。

Result: 在连续控制MuJoCo任务中，与标准基线相比，累积奖励最多增加4倍，潜在动作质量提高3倍。

Conclusion: MaskLAM方法有效缓解了LAMs的问题，提高了性能。

Abstract: Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.

</details>


### [614] [Learning Markov Decision Processes under Fully Bandit Feedback](https://arxiv.org/abs/2602.02260)
*Zhengjia Zhuo,Anupam Gupta,Viswanath Nagarajan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\widetilde{O}(\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.

</details>


### [615] [Unlocking the Duality between Flow and Field Matching](https://arxiv.org/abs/2602.02261)
*Daniil Shlenskii,Alexander Varlamov,Nazar Buzun,Alexander Korotin*

Main category: cs.LG

TL;DR: 探讨Conditional Flow Matching (CFM)和Interaction Field Matching (IFM)是否为同一潜在动力学的不同描述，发现CFM与forward - only IFM重合，且一般IFM表达能力更强，二者对偶性有益。


<details>
  <summary>Details</summary>
Motivation: CFM和IFM虽都定义生成动力学，但起始对象不同，探究二者是否本质相同。

Method: 构建CFM和forward - only IFM之间的双射，对比二者表达能力。

Result: CFM和forward - only IFM重合，一般IFM表达能力更丰富，包含EFM等无法用标准CFM实现的交互场。

Conclusion: 二者的对偶性对两个框架都有益，为forward - only IFM提供概率解释，为CFM带来新的IFM驱动技术。

Abstract: Conditional Flow Matching (CFM) unifies conventional generative paradigms such as diffusion models and flow matching. Interaction Field Matching (IFM) is a newer framework that generalizes Electrostatic Field Matching (EFM) rooted in Poisson Flow Generative Models (PFGM). While both frameworks define generative dynamics, they start from different objects: CFM specifies a conditional probability path in data space, whereas IFM specifies a physics-inspired interaction field in an augmented data space. This raises a basic question: are CFM and IFM genuinely different, or are they two descriptions of the same underlying dynamics? We show that they coincide for a natural subclass of IFM that we call forward-only IFM. Specifically, we construct a bijection between CFM and forward-only IFM. We further show that general IFM is strictly more expressive: it includes EFM and other interaction fields that cannot be realized within the standard CFM formulation. Finally, we highlight how this duality can benefit both frameworks: it provides a probabilistic interpretation of forward-only IFM and yields novel, IFM-driven techniques for CFM.

</details>


### [616] [Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training](https://arxiv.org/abs/2602.02264)
*Paolo Marcandelli,Natansh Mathur,Stefano Markidis,Martina Siena,Stefano Mariani*

Main category: cs.LG

TL;DR: 提出多阶段物理信息训练策略和PhIS - FNO，解决偏微分方程求解问题，在基准测试中取得与监督学习相当的精度。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子需监督数据，物理信息神经网络收敛不稳定、泛化能力有限，需改进方法。

Method: 引入多阶段物理信息训练策略，逐步施加边界条件并结合内部残差；提出PhIS - FNO，结合傅里叶层和Hermite样条核。

Result: 在基准测试中，PhIS - FNO仅使用边界区域的标记信息，达到与监督学习相当的精度。

Conclusion: 基于样条的分阶段优化是物理信息算子学习的可靠范式。

Abstract: Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.

</details>


### [617] [HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control](https://arxiv.org/abs/2602.02268)
*Sanggeon Yun,Raheeb Hassan,Ryozo Masukawa,Sungheon Jeong,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出HopFormer，不依赖位置编码和全局注意力，通过n-hop掩码稀疏注意力注入结构，实验证明其性能优越并挑战图Transformer设计的传统假设。


<details>
  <summary>Details</summary>
Motivation: 挑战图Transformer依赖显式位置或结构编码及密集全局注意力的传统做法，探索更有效的设计。

Method: 引入HopFormer，通过头特定的n-hop掩码稀疏注意力注入结构，不使用位置编码和架构修改。

Result: 在节点级和图级基准测试中取得有竞争力或更优的性能，揭示密集全局注意力常不必要。

Conclusion: 挑战图Transformer设计的主流假设，强调稀疏控制注意力是一种更有效替代方案。

Abstract: Graph Transformers typically rely on explicit positional or structural encodings and dense global attention to incorporate graph topology. In this work, we show that neither is essential. We introduce HopFormer, a graph Transformer that injects structure exclusively through head-specific n-hop masked sparse attention, without the use of positional encodings or architectural modifications. This design provides explicit and interpretable control over receptive fields while enabling genuinely sparse attention whose computational cost scales linearly with mask sparsity. Through extensive experiments on both node-level and graph-level benchmarks, we demonstrate that our approach achieves competitive or superior performance across diverse graph structures. Our results further reveal that dense global attention is often unnecessary: on graphs with strong small-world properties, localized attention yields more stable and consistently high performance, while on graphs with weaker small-world effects, global attention offers diminishing returns. Together, these findings challenge prevailing assumptions in graph Transformer design and highlight sparsity-controlled attention as a principled and efficient alternative.

</details>


### [618] [MoLF: Mixture-of-Latent-Flow for Pan-Cancer Spatial Gene Expression Prediction from Histology](https://arxiv.org/abs/2602.02282)
*Susu Hu,Stefanie Speidel*

Main category: cs.LG

TL;DR: 提出MoLF模型用于泛癌组织基因组预测，在泛癌基准测试中表现出色，还能零样本泛化到跨物种数据。


<details>
  <summary>Details</summary>
Motivation: 当前从组织学推断空间转录组学的方法多为单组织模型，无法利用不同癌症类型的生物学原理，泛癌训练存在异质性挑战。

Method: 引入MoLF生成模型，利用条件流匹配目标将噪声映射到基因潜在流形，由混合专家速度场参数化，动态将输入路由到专门子网络。

Result: MoLF在泛癌基准测试中始终优于专业和基础模型基线，对跨物种数据有零样本泛化能力。

Conclusion: MoLF模型有效解决了现有方法的问题，捕捉到了基本的、保守的组织分子机制。

Abstract: Inferring spatial transcriptomics (ST) from histology enables scalable histogenomic profiling, yet current methods are largely restricted to single-tissue models. This fragmentation fails to leverage biological principles shared across cancer types and hinders application to data-scarce scenarios. While pan-cancer training offers a solution, the resulting heterogeneity challenges monolithic architectures. To bridge this gap, we introduce MoLF (Mixture-of-Latent-Flow), a generative model for pan-cancer histogenomic prediction. MoLF leverages a conditional Flow Matching objective to map noise to the gene latent manifold, parameterized by a Mixture-of-Experts (MoE) velocity field. By dynamically routing inputs to specialized sub-networks, this architecture effectively decouples the optimization of diverse tissue patterns. Our experiments demonstrate that MoLF establishes a new state-of-the-art, consistently outperforming both specialized and foundation model baselines on pan-cancer benchmarks. Furthermore, MoLF exhibits zero-shot generalization to cross-species data, suggesting it captures fundamental, conserved histo-molecular mechanisms.

</details>


### [619] [Statistical Learning Theory in Lean 4: Empirical Processes from Scratch](https://arxiv.org/abs/2602.02285)
*Yuanhe Zhang,Jason D. Lee,Fanghui Liu*

Main category: cs.LG

TL;DR: 首次基于经验过程理论对统计学习理论（SLT）进行全面Lean 4形式化，采用人机协作工作流，解决标准教材问题，建立可复用基础。


<details>
  <summary>Details</summary>
Motivation: 填补Lean 4 Mathlib库中统计学习理论相关内容的空白，促进机器学习理论的形式化发展。

Method: 采用人机协作工作流，人类设计证明策略，AI代理执行战术证明构建。

Result: 实现了高斯Lipschitz集中、Dudley熵积分定理的形式化及最小二乘（稀疏）回归应用，得到人类验证的Lean 4工具包。

Conclusion: 建立了可复用的形式化基础，为机器学习理论未来发展打开大门。

Abstract: We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory

</details>


### [620] [An Optimization Method for Autoregressive Time Series Forecasting](https://arxiv.org/abs/2602.02288)
*Zheng Li,Jerry Cheng,Huanying Gu*

Main category: cs.LG

TL;DR: 提出新的时间序列预测训练方法，在多基准测试中达最优，MSE降低超10%，能让短期模型进行长预测。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型主要靠扩大模型规模实现长预测，传统训练忽略时间因果性。

Method: 提出新训练方法，强制两个关键属性：AR预测误差随预测范围增加，违反原则在损失函数中惩罚；可拼接短期AR预测形成灵活长预测。

Result: 在多基准测试中达最优，MSE较iTransformer等降低超10%，使短期模型能进行超7.5倍长的可靠长预测。

Conclusion: 新训练方法有效，能提升时间序列预测性能。

Abstract: Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt

</details>


### [621] [EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2602.02295)
*Shaima Ahmad Freja,Ferhat Ozgur Catak,Betul Yurdem,Chunming Rong*

Main category: cs.LG

TL;DR: 提出EvalQReason框架量化大语言模型推理质量，通过实验表明CSD性能优于SFC，推理动态有领域特异性，此框架可实现可扩展、过程感知的推理可靠性评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键应用中推理过程难以系统评估，现有方法聚焦最终答案正确性，对中间步骤推理情况洞察有限。

Method: 提出EvalQReason框架，引入CSD和SFC两种互补算法，每种算法用五个统计指标捕捉推理动态。

Result: 在数学和医学数据集实验中，CSD特征用于正确性分类表现良好，经典机器学习模型F1=0.78、ROC - AUC=0.82，顺序神经模型性能大幅提升（F1=0.88、ROC - AUC=0.97），CSD始终优于SFC，顺序架构优于经典机器学习方法，推理动态有领域特异性。

Conclusion: EvalQReason能实现可扩展、过程感知的推理可靠性评估，基于概率的差异分析是值得信赖的AI部署的原则性方法。

Abstract: Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.

</details>


### [622] [Decoupling Generalizability and Membership Privacy Risks in Neural Networks](https://arxiv.org/abs/2602.02296)
*Xingli Fang,Jung-Eun Kim*

Main category: cs.LG

TL;DR: 本文指出深度学习模型隐私保护与效用存在权衡关系，提出PPTP原则，评估显示该方法在增强隐私保护时能更好维持泛化性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型获取能力时会牺牲效用，隐私保护与效用存在权衡关系，需解耦泛化性和隐私风险以最大化隐私收益。

Method: 识别模型泛化性和隐私风险在深度神经网络架构中的不同区域，提出隐私保护训练原则（PPTP）。

Result: 通过大量评估，该方法在增强隐私保护的同时，能显著更好地维持模型泛化性。

Conclusion: PPTP原则能在保护模型组件免受隐私风险的同时，最小化泛化性损失。

Abstract: A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.

</details>


### [623] [ReasonCACHE: Teaching LLMs To Reason Without Weight Updates](https://arxiv.org/abs/2602.02366)
*Sharut Gupta,Phillip Isola,Stefanie Jegelka,David Lopez-Paz,Kartik Ahuja,Mark Ibrahim,Mohammad Pezeshki*

Main category: cs.LG

TL;DR: 本文提出ReasonCACHE机制，让大语言模型不更新权重且不超载上下文窗口学习推理，在推理基准测试中表现好且更高效，理论证明其比低秩权重更新更具表现力。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习（ICL）在复杂推理任务中有局限性，从业者多依赖权重学习（IWL），需探索不更新权重学习推理的方法。

Method: 使用Prefix Tuning，引入ReasonCACHE机制将演示蒸馏到固定键值缓存。

Result: 在具有挑战性的推理基准测试中，ReasonCACHE优于标准ICL，与或超过IWL方法，且在数据、推理成本和可训练参数方面更高效。

Conclusion: ReasonCACHE是上下文学习和权重学习之间的中间路径，提供了不修改参数学习推理技能的可扩展算法。

Abstract: Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/

</details>


### [624] [Self-Supervised Learning from Structural Invariance](https://arxiv.org/abs/2602.02381)
*Yipeng Zhang,Hafez Ghaemi,Jungyoon Lee,Shahab Bakhtiari,Eilif B. Muller,Laurent Charlin*

Main category: cs.LG

TL;DR: 研究自监督学习（SSL）中的一对多映射问题，提出AdaSSL方法并验证其在多任务中的通用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以灵活捕捉SSL中一对多映射的条件不确定性。

Method: 引入潜在变量解释不确定性，推导配对嵌入之间互信息的变分下界，得到标准SSL目标的正则化项。

Result: 提出的AdaSSL方法适用于基于对比和蒸馏的SSL目标。

Conclusion: AdaSSL方法在因果表示学习、细粒度图像理解和视频世界建模中具有通用性。

Abstract: Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.

</details>


### [625] [SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization](https://arxiv.org/abs/2602.02383)
*Maksim Afanasyev,Illarion Iov*

Main category: cs.LG

TL;DR: 本文提出SLIME方法解决直接偏好优化方法的目标不匹配问题，性能优于基线且稳定性高。


<details>
  <summary>Details</summary>
Motivation: 现有直接偏好优化方法存在目标不匹配问题，会导致“unlearning”和“formatting collapse”。

Method: 引入参考无关的对齐目标SLIME，包含最大化首选响应可能性的锚定项、防止拒绝标记概率降为零的稳定惩罚项和结合软硬约束的双边缘机制。

Result: SLIME相比现有基线表现更优，且保持更高的生成稳定性。

Conclusion: SLIME是一种有效的解决直接偏好优化问题的方法。

Abstract: Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.

</details>


### [626] [Transformers learn factored representations](https://arxiv.org/abs/2602.02385)
*Adam Shai,Loren Amdahl-Culleton,Casper L. Christensen,Henry R. Bigelow,Fernando E. Rosas,Alexander B. Boyd,Eric A. Alt,Kyle J. Ray,Paul M. Riechers*

Main category: cs.LG

TL;DR: 研究预训练Transformer通过下一token预测学习将世界分解为部分，提出两种表征假设并测试，解释其分解原因。


<details>
  <summary>Details</summary>
Motivation: 探索预训练Transformer将世界分解为部分的原理及潜在的表征方式。

Method: 形式化提出两种表征假设，推导激活的几何结构预测，在具有已知潜在结构的合成数据训练的Transformer上测试。

Result: 当因素条件独立时，模型学习因式分解表征；早期训练即便条件独立性被破坏仍偏好该表征。

Conclusion: 解释Transformer分解世界的原因，表明复杂数据训练的模型中可存在可解释低维结构。

Abstract: Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.

</details>


### [627] [David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2602.02395)
*Samuel Nellessen,Tal Kachman*

Main category: cs.LG

TL;DR: 研究大语言模型进化为自主代理带来的对抗性失败，提出Tag - Along攻击威胁模型，用Slingshot框架验证，该框架有高成功率且能零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 大语言模型进化为自主代理带来利用合法工具权限的对抗性失败，需将工具增强环境中的安全评估从主观NLP任务转变为客观控制问题。

Method: 形式化Tag - Along攻击威胁模型，提出Slingshot‘冷启动’强化学习框架自主发现攻击向量。

Result: Slingshot在保留的极难任务中对Qwen2.5 - 32B - Instruct - AWQ操作员成功率达67.0%，减少首次成功尝试次数，还能零样本迁移到多个模型家族。

Conclusion: 确立Tag - Along攻击为可验证的一流威胁模型，表明仅通过环境交互就能从现成开放权重模型中引发有效的代理攻击。

Abstract: The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary "tags along" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.

</details>


### [628] [An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence](https://arxiv.org/abs/2602.02400)
*Qizhen Zhang,Ankush Garg,Jakob Foerster,Niladri Chatterji,Kshitiz Malik,Mike Lewis*

Main category: cs.LG

TL;DR: 本文对噪声数据是否会导致大语言模型预训练损失发散及如何导致发散进行研究，表明噪声会导致损失发散，且发散概率与噪声类型、数量和模型规模有关，还给出区分两种失败模式的诊断方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练的网络规模语料中存在大量噪声数据，但噪声导致模型预训练不稳定甚至损失发散的现象尚不明确，因此开展研究。

Method: 在干净数据集里注入可控的合成均匀随机噪声，分析参数从4.8亿到52亿的不同规模模型的训练动态。

Result: 噪声数据会导致训练损失发散，发散概率与噪声类型、数量和模型规模密切相关，且噪声导致的发散与高学习率导致的发散激活模式不同。

Conclusion: 研究对噪声数据如何影响大语言模型预训练损失发散进行了大规模、可控的表征。

Abstract: Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.

</details>


### [629] [Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning](https://arxiv.org/abs/2602.02405)
*Ethan Mendes,Jungsoo Park,Alan Ritter*

Main category: cs.LG

TL;DR: 提出DAIL方法解决大语言模型推理能力提升难题，用少量专家解决方案取得效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有提升大语言模型推理能力的方法存在局限，高质量专家解决方案存在分布外和成本高问题，需可泛化且样本高效的训练方法。

Method: 提出Distribution Aligned Imitation Learning (DAIL) 两步法，先将专家解决方案转化为详细的分布内推理轨迹，再应用对比目标聚焦学习专家见解和方法。

Result: DAIL用少于1000个高质量专家解决方案，使Qwen2.5 - Instruct和Qwen3模型的pass@k提升10 - 25%，推理效率提高2 - 4倍，实现域外泛化。

Conclusion: DAIL方法有效解决了大语言模型推理能力提升中的难题，能利用少量专家解决方案提升模型性能。

Abstract: Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.

</details>


### [630] [Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models](https://arxiv.org/abs/2602.02415)
*Vivienne Pelletier,Daniel J. Rivera,Obinna Nwokonkwo,Steven A. Wilson,Christopher L. Muhich*

Main category: cs.LG

TL;DR: 提出用于主动学习种子数据集选择的新方法ATBagging，在多个真实数据集上评估显示能提升早期主动学习性能和学习曲线下面积。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习依赖大量标注数据，主动学习初始性能受随机选择的种子集影响，而可用相关数据集构建更好种子集。

Method: ATBagging通过贝叶斯解释的袋装集成模型估计候选数据点信息量，利用DPP确保特征空间多样性，主动学习阶段也用此混合方法选择新数据点。

Result: 在四个真实数据集上，不同种子大小下，ATBagging几乎在所有情况下改善或持平早期主动学习，增加学习曲线下面积，低数据场景优势明显。

Conclusion: ATBagging为基于主动学习的数据收集提供低成本、高回报的启动方式。

Abstract: Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.

</details>


### [631] [Trust Region Continual Learning as an Implicit Meta-Learner](https://arxiv.org/abs/2602.02417)
*Zekun Wang,Anant Gupta,Christopher J. MacLellan*

Main category: cs.LG

TL;DR: 研究信任区域持续学习，结合生成式重放与Fisher度量信任区域约束，在实验中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 持续学习标准策略存在权衡问题，正则化方法可能过度约束更新，重放方法会因不完美重放而漂移。

Method: 提出信任区域持续学习方法，结合生成式重放与Fisher度量信任区域约束，更新有MAML式解释。

Result: 在任务增量扩散图像生成和持续扩散策略控制实验中，该方法取得最佳最终性能和保留率，恢复早期任务性能更快。

Conclusion: 信任区域持续学习具有新兴元学习属性，能在任务转换后快速重新收敛到先前任务最优解。

Abstract: Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.

</details>


### [632] [Poly-attention: a general scheme for higher-order self-attention](https://arxiv.org/abs/2602.02422)
*Sayak Chakrabarti,Toniann Pitassi,Josh Alman*

Main category: cs.LG

TL;DR: 本文定义了一类自注意力机制的泛化——多注意力机制，系统研究其计算复杂度和表示能力，给出新算法和复杂度下界，得到新的二次时间可精确计算且能进行任意固定数量函数组合的注意力机制。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制无法执行涉及检测相关标记三元组等基本任务，已有高维替代方案运行时间慢，故提出新的泛化机制。

Method: 定义多注意力机制，系统研究其计算复杂度和表示能力，给出精确和近似计算注意力矩阵时间复杂度的新算法及匹配的复杂度下界。

Result: 得到不同期望之间的有趣权衡，给出二次时间可精确计算、能进行任意固定数量函数组合的新注意力机制，证明已有机制无法有更快算法。

Conclusion: 多注意力机制在计算复杂度和表示能力上有新的权衡和成果，新机制在函数组合任务上有显著优势。

Abstract: The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.
  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.
  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.

</details>


### [633] [Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization](https://arxiv.org/abs/2602.02425)
*Amaru Caceres Arroyo,Lea Bogensperger,Ahmed Allam,Michael Krauthammer,Konrad Schindler,Dominik Narnhofer*

Main category: cs.LG

TL;DR: 提出CHASE框架用于蛋白质适应性优化，在基准测试中达SOTA，合成数据可提升数据受限场景性能。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质适应性优化方法存在表现不佳或计算成本高的问题。

Method: 将预训练蛋白质语言模型的嵌入压缩到紧凑潜在空间，训练带无分类器引导的条件流匹配模型。

Result: CHASE在AAV和GFP蛋白质设计基准测试中取得了最先进的性能。

Conclusion: 在数据受限场景中，使用合成数据进行自举可进一步提高性能。

Abstract: Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.

</details>


### [634] [Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning](https://arxiv.org/abs/2602.02427)
*Qihao Wen,Jiahao Wang,Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.LG

TL;DR: 本文探讨LLM推理中更好反映中间不确定性的UQ指标，发现基于扰动的指标能识别推理中不确定的中间步骤，其性能优于基线方法，且更简单高效。


<details>
  <summary>Details</summary>
Motivation: LLM输出可能不可靠或有误导性，UQ技术很必要，推理任务中需估计推理中间步骤的不确定性。

Method: 探索能反映推理中LLM“中间不确定性”的UQ指标，利用对前一个词嵌入的扰动确定推理中间步骤中高敏感度的token，以此识别不确定步骤。

Result: 基于扰动的指标在不确定性量化上比基于token概率和token熵的基线方法更优，且比依赖多次采样的方法更简单高效。

Conclusion: 基于扰动的指标可作为识别推理中不确定中间步骤的有效UQ指标。

Abstract: Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.

</details>


### [635] [Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE](https://arxiv.org/abs/2602.02443)
*Yuanteng Chen,Peisong Wang,Nanxin Zeng,Yuantian Shao,Gang Li,Jing Liu,Jian Cheng*

Main category: cs.LG

TL;DR: 本文聚焦细粒度MoE路由特点，提出免训练的Expert - Sample方法提升LLM性能，实验显示该方法在多任务上效果良好。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放和token级采样存在问题，细粒度MoE虽有丰富路由空间但未被充分探索，需找到提升模型性能的方法。

Method: 提出Expert - Sample方法，保留高置信度选择，向不确定尾部注入可控随机性。

Result: 在多个细粒度MoE模型的数学、知识推理和代码任务中，Expert - Sample持续提升了pass@n和基于验证的准确率，如在Qwen3 - 30B - A3B - Instruct上有显著提升。

Conclusion: Expert - Sample方法能在不影响输出稳定性的情况下实现多样化生成，有效提升模型性能。

Abstract: Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.

</details>


### [636] [Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation](https://arxiv.org/abs/2602.02445)
*Seo Taek Kong,R. Srikant*

Main category: cs.LG

TL;DR: 本文推导了非线性随机逼近算法在Wasserstein - p距离下的非渐近误差界，给出最后一次迭代的有限样本保证，分析了Polyak - Ruppert平均的收敛率，证明了迭代收敛到高斯分布的速率，并通过两个应用展示方法实用性。


<details>
  <summary>Details</summary>
Motivation: 获得非线性随机逼近算法最后一次迭代的明确有限样本保证，分析Polyak - Ruppert平均的收敛率。

Method: 开发耦合论证来比较离散时间过程和极限Ornstein - Uhlenbeck过程；对Polyak - Ruppert平均直接分析。

Result: 在非渐近中心极限定理条件下，归一化最后迭代以$γ_n^{1/6}$速率在p - Wasserstein距离收敛到高斯分布，Polyak - Ruppert平均以$n^{-1/6}$速率在Wasserstein距离收敛，给出高概率集中不等式。

Conclusion: 该方法在随机逼近和随机梯度下降等应用中有效，缩小有限样本分析和渐近理论的差距，确定收敛到中心极限定理的速率。

Abstract: This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.

</details>


### [637] [Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization](https://arxiv.org/abs/2602.02451)
*Patrick Cooper,Alvaro Velasquez*

Main category: cs.LG

TL;DR: 提出Active Causal Experimentalist (ACE)学习实验设计的顺序策略，在多场景优于基线，能自主发现实验策略。


<details>
  <summary>Details</summary>
Motivation: 传统实验决策方法孤立处理每个决策，无法从经验学习自适应策略。

Method: 提出ACE，通过直接偏好优化，从成对干预比较中学习。

Result: 在合成基准、物理模拟和经济数据中，ACE在相同干预预算下比基线提高70 - 71%，自主发现碰撞机制的实验策略。

Conclusion: 基于偏好的学习可恢复有原则的实验策略，用学习的领域适应补充理论。

Abstract: Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.

</details>


### [638] [Conflict-Aware Client Selection for Multi-Server Federated Learning](https://arxiv.org/abs/2602.02458)
*Mingwei Hong,Zheng Lin,Zehang Lin,Lin Li,Miao Yang,Xia Du,Zihan Fang,Zhaolu Kang,Dianxin Luan,Shunzhi Zhu*

Main category: cs.LG

TL;DR: 提出名为RL - CRP的分散式强化学习方法来优化多服务器联邦学习系统中的客户端选择，实验证明可减少冲突并提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统单服务器联邦学习有高通信延迟问题，多服务器联邦学习存在客户端覆盖重叠和选择不协调导致资源争用等问题，需要解决这些局限。

Method: 提出RL - CRP方法，服务器用分类隐藏马尔可夫模型根据稀疏历史客户端选择序列估计客户端选择冲突可能性，并加入公平感知奖励机制。

Result: 该框架有效减少服务器间冲突，在收敛速度和通信成本方面显著提高训练效率。

Conclusion: RL - CRP能优化多服务器联邦学习系统中的客户端选择，解决现有联邦学习系统的问题。

Abstract: Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.

</details>


### [639] [SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning](https://arxiv.org/abs/2602.02472)
*Qifan Yu,Xinyu Ma,Zhijian Zhuo,Minrui Wang,Deyi Liu,Shiyi Zhan,Yiyuan Ma,Liang Xiang,Xingyan Bin,Di He*

Main category: cs.LG

TL;DR: 提出SPARKLING框架解决渐进学习中期宽度扩展问题，实验显示其优于从头训练并可降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有渐进学习宽度扩展研究不足，中期扩展因训练不稳定是挑战，需解决初始化问题。

Method: 提出SPARKLING框架，通过RMS尺度一致性保存信号，用非对称优化器状态重置和学习率预热打破对称。

Result: 在混合专家模型实验中，SPARKLING在多宽度轴和优化器类型上始终优于从头训练，2倍宽度扩展时最多降低35%训练成本。

Conclusion: SPARKLING框架有效解决中期宽度扩展问题，可降低训练成本。

Abstract: Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\times$ width expansion.

</details>


### [640] [Expanding the Capabilities of Reinforcement Learning via Text Feedback](https://arxiv.org/abs/2602.02482)
*Yuda Song,Lili Chen,Fahim Tajwar,Remi Munos,Deepak Pathak,J. Andrew Bagnell,Aarti Singh,Andrea Zanette*

Main category: cs.LG

TL;DR: 研究文本反馈作为中间信号用于LLM后训练的方法，提出两种方法并在多任务上验证效果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 强化学习用于LLM后训练信息少，蒸馏需要昂贵演示，文本反馈是更合适的中间信号。

Method: 提出Self Distillation (RLTF - SD)和Feedback Modeling (RLTF - FM)两种方法，并进行理论分析。

Result: 两种方法在推理谜题、竞赛数学和创意写作任务等基准测试中始终优于强基线模型。

Conclusion: RL结合丰富的文本反馈监督源具有大规模应用的潜力。

Abstract: The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.

</details>


### [641] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

TL;DR: 本文提出RLAnything强化学习框架，通过闭环优化动态构建环境、策略和奖励模型，在LLM和智能体场景中表现良好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 强化学习系统在LLM和智能体场景下需要更有效的强化和学习信号，期望提升系统性能。

Method: 通过闭环优化动态构建环境、策略与奖励模型；用分步和结果信号的综合反馈训练策略，用一致性反馈优化奖励模型；用基于理论的自动环境适配，利用两者的评论家反馈改进训练。

Result: 每个组件都能提升整体系统性能；在多个代表性任务中表现出色，如在OSWorld上提升Qwen3 - VL - 8B - Thinking 9.1%，在AlfWorld和LiveBench上分别提升Qwen2.5 - 7B - Instruct 18.7%和11.9%；优化后的奖励模型信号优于依赖人类标签的结果。

Conclusion: RLAnything框架有效提升了强化学习系统在各种LLM和智能体任务中的性能。

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


### [642] [MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training](https://arxiv.org/abs/2602.02494)
*Dulhan Jayalath,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: 为无法提供大量训练记录的瘫痪患者设计临床脑文本接口，提出MEG - XL模型，用长上下文预训练，在词解码任务微调表现优。


<details>
  <summary>Details</summary>
Motivation: 现有预训练方法使用上下文较短，临床脑文本接口需为无法提供大量训练记录的瘫痪患者设计，需提高数据效率。

Method: 提出MEG - XL模型，每个样本用2.5分钟脑磁图（MEG）上下文预训练，在从脑数据进行词解码任务上微调。

Result: MEG - XL用少量数据达到有监督性能，优于脑基础模型，长上下文预训练的模型在词解码任务上迁移性更好。

Conclusion: 长上下文预训练有助于利用其他方法丢弃的扩展神经上下文。

Abstract: Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [643] [Repair Brain Damage: Real-Numbered Error Correction Code for Neural Network](https://arxiv.org/abs/2602.00076)
*Ziqing Li,Myung Cho,Qiutong Jin,Weiyu Xu*

Main category: cs.NE

TL;DR: 提出基于实数的纠错码检测和纠正神经网络内存与计算错误，不牺牲性能和增加参数


<details>
  <summary>Details</summary>
Motivation: 神经网络可能出现内存故障和计算错误，需有效纠错方法

Method: 引入基于实数的线性约束结构到神经网络权重中

Result: 能检测和纠正内存与计算错误，不牺牲分类性能和增加参数

Conclusion: 提出的基于实数的纠错码有效可行

Abstract: We consider a neural network (NN) that may experience memory faults and computational errors. In this paper, we propose a novel real-number-based error correction code (ECC) capable of detecting and correcting both memory errors and computational errors. The proposed approach introduces structures in the form of real-number-based linear constraints on the NN weights to enable error detection and correction, without sacrificing classification performance or increasing the number of real-valued NN parameters.

</details>


### [644] [MO-ELA: Rigorously Expanding Exploratory Landscape Features for Automated Algorithm Selection in Continuous Multi-Objective Optimisation](https://arxiv.org/abs/2602.00098)
*Oliver Preuß,Jeroen Rook,Jakob Bossek,Heike Trautmann*

Main category: cs.NE

TL;DR: 提出多目标优化的新探索性景观特征MO - ELA，经AAS研究验证其对算法选择和问题表征有价值。


<details>
  <summary>Details</summary>
Motivation: 单目标优化有丰富特征，多目标优化特征较少，需为多目标优化提出新特征。

Method: 基于随机点样本考虑决策和目标空间，将特征分为5个特征组。

Result: 在多目标基准测试的AAS研究中，新特征能区分算法性能，接近虚拟最佳求解器；特征选择后常为主要贡献者。

Conclusion: 新提出的MO - ELA特征在算法选择和问题表征中有重要价值。

Abstract: Automated Algorithm Selection (AAS) is a popular meta-algorithmic approach and has demonstrated to work well for single-objective optimisation in combination with exploratory landscape features (ELA), i.e., (numerical) descriptive features derived from sampling the black-box (continuous) optimisation problem. In contrast to the abundance of features that describe single-objective optimisation problems, only a few features have been proposed for multi-objective optimisation so far. Building upon recent work on exploratory landscape features for box-constrained continuous multi-objective optimization problems, we propose a novel and complementary set of additional features (MO-ELA). These features are based on a random sample of points considering both the decision and objective space. The features are divided into 5 feature groups depending on how they are being calculated: non-dominated-sorting, descriptive statistics, principal component analysis, graph structures and gradient information. An AAS study conducted on well-established multi-objective benchmarks demonstrates that the proposed features contribute to successfully distinguishing between algorithm performance and thus adequately capture problem hardness resulting in models that come very close to the virtual best solver. After feature selection, the newly proposed features are frequently among the top contributors, underscoring their value in algorithm selection and problem characterisation.

</details>


### [645] [Reinforcement Learning-assisted Constraint Relaxation for Constrained Expensive Optimization](https://arxiv.org/abs/2602.00532)
*Qianhao Zhu,Sijie Ma,Zeyuan Ma,Hongshu Guo,Yue-Jiao Gong*

Main category: cs.NE

TL;DR: 本文提出通过强化学习学习有效、自适应和可泛化的约束处理策略，在CEC 2017基准测试上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有约束处理技术依赖人工设计，对一般情况效用不足，受元黑盒优化进展启发。

Method: 构建定制的马尔可夫决策过程，用基于深度Q网络的策略控制约束松弛水平。

Result: 在CEC 2017基准测试上，在有限评估预算条件下，表现优于或媲美强基线。

Conclusion: 该方法具有竞争力，进一步分析揭示了设计的关键见解。

Abstract: Constraint handling plays a key role in solving realistic complex optimization problems. Though intensively discussed in the last few decades, existing constraint handling techniques predominantly rely on human experts' designs, which more or less fall short in utility towards general cases. Motivated by recent progress in Meta-Black-Box Optimization where automated algorithm design can be learned to boost optimization performance, in this paper, we propose learning effective, adaptive and generalizable constraint handling policy through reinforcement learning. Specifically, a tailored Markov Decision Process is first formulated, where given optimization dynamics features, a deep Q-network-based policy controls the constraint relaxation level along the underlying optimization process. Such adaptive constraint handling provides flexible tradeoff between objective-oriented exploitation and feasible-region-oriented exploration, and hence leads to promising optimization performance. We train our approach on CEC 2017 Constrained Optimization benchmark with limited evaluation budget condition (expensive cases) and compare the trained constraint handling policy to strong baselines such as recent winners in CEC/GECCO competitions. Extensive experimental results show that our approach performs competitively or even surpasses the compared baselines under either Leave-one-out cross-validation or ordinary train-test split validation. Further analysis and ablation studies reveal key insights in our designs.

</details>


### [646] [Surrogate Ensemble in Expensive Multi-Objective Optimization via Deep Q-Learning](https://arxiv.org/abs/2602.00540)
*Yuxin Wu,Hongshu Guo,Ting Huang,Yue-Jiao Gong,Zeyuan Ma*

Main category: cs.NE

TL;DR: 提出强化学习辅助集成框架SEEMOO用于调度代理模型，提升昂贵多目标优化问题性能，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有SAEAs代理模型选择由人工决定，有强偏差，影响范围外任务性能，且多目标优化问题对代理选择有挑战。

Method: 提出SEEMOO框架，包含预收集模型池、基于注意力的状态提取器和深度Q网络作为动态代理选择器，在训练问题分布下训练以最大化性能。

Result: 广泛的基准测试表明SEEMOO的代理集成范式提升了单代理基线的优化性能，消融研究强调了其设计组件的重要性。

Conclusion: SEEMOO框架能在单一优化过程中调度不同代理模型，以合作范式提升整体优化性能。

Abstract: Surrogate-assisted Evolutionary Algorithms~(SAEAs) have shown promising robustness in solving expensive optimization problems. A key aspect that impacts SAEAs' effectiveness is surrogate model selection, which in existing works is predominantly decided by human developer. Such human-made design choice introduces strong bias into SAEAs and may hurt their expected performance on out-of-scope tasks. In this paper, we propose a reinforcement learning-assisted ensemble framework, termed as SEEMOO, which is capable of scheduling different surrogate models within a single optimization process, hence boosting the overall optimization performance in a cooperative paradigm. Specifically, we focus on expensive multi-objective optimization problems, where multiple objective functions shape a compositional landscape and hence challenge surrogate selection. SEEMOO comprises following core designs: 1) A pre-collected model pool that maintains different surrogate models; 2) An attention-based state-extractor supports universal optimization state representation of problems with varied objective numbers; 3) a deep Q-network serves as dynamic surrogate selector: Given the optimization state, it selects desired surrogate model for current-step evaluation. SEEMOO is trained to maximize the overall optimization performance under a training problem distribution. Extensive benchmark results demonstrate SEEMOO's surrogate ensemble paradigm boosts the optimization performance of single-surrogate baselines. Further ablation studies underscore the importance of SEEMOO's design components.

</details>


### [647] [NegaBent, No Regrets: Evolving Spectrally Flat Boolean Functions](https://arxiv.org/abs/2602.00843)
*Claude Carlet,Marko Ðurasevic,Ermes Franch,Domagoj Jakobovic,Luca Mariot,Stjepan Picek*

Main category: cs.NE

TL;DR: 研究用进化算法演化（弯曲 - ）负弯曲布尔函数，实验表明进化算法尤其是遗传编程适合，且在各维度成功演化出函数。


<details>
  <summary>Details</summary>
Motivation: 负弯曲布尔函数及其子类弯曲 - 负弯曲布尔函数因最优周期性和负周期性频谱特性受关注，研究如何用进化算法演化此类函数。

Method: 使用进化算法，特别是遗传编程来演化（弯曲 - ）负弯曲布尔函数。

Result: 进化算法尤其是遗传编程适合演化负弯曲布尔函数，在考虑的所有维度中都成功演化出此类函数。

Conclusion: 进化算法尤其是遗传编程是演化负弯曲布尔函数的合适方法。

Abstract: Negabent Boolean functions are defined by having a flat magnitude spectrum under the nega-Hadamard transform. They exist in both even and odd dimensions, and the subclass of functions that are simultaneously bent and negabent (bent-negabent) has attracted interest due to the combined optimal periodic and negaperiodic spectral properties. In this work, we investigate how evolutionary algorithms can be used to evolve (bent-)negabent Boolean functions. Our experimental results indicate that evolutionary algorithms, especially genetic programming, are a suitable approach for evolving negabent Boolean functions, and we successfully evolve such functions in all dimensions we consider.

</details>


### [648] [Organismal Agency and Rapid Adaptation: The Phenopoiesis Algorithm for Phenotype-First Evolution](https://arxiv.org/abs/2602.00978)
*Nam H. Le*

Main category: cs.NE

TL;DR: 本文提出Phenopoiesis算法，证明通过可遗传表型模式，生物体能动性是具体计算过程，且在适应环境上比基因中心模型快3.4倍，多时间尺度进化有适应性优势。


<details>
  <summary>Details</summary>
Motivation: 基因中心范式将进化因果仅归因于基因，Denis Noble的表型优先框架虽有意义但算法不明确，需具体算法实现生物体能动性。

Method: 引入Phenopoiesis算法，让生物体不仅遗传基因，还遗传终生学习中发现的成功表型模式，并在变化环境中开展实验。

Result: 模式遗传生物体比基因中心模型适应速度快3.4倍，且这种优势需跨代遗传学习模式，而非仅终生学习。

Conclusion: 生物体能动性是有可衡量适应价值的算法机制，通过组合复用工作。进化跨多时间尺度，有单通道机制无法实现的适应性灵活性。

Abstract: Evolutionary success depends on the capacity to adapt: organisms must respond to environmental challenges through both genetic innovation and lifetime learning. The gene-centric paradigm attributes evolutionary causality exclusively to genes, while Denis Noble's phenotype-first framework argues that organisms are active agents capable of interpreting genetic resources, learning from experience, and shaping their own development. However, this framework has remained philosophically intuitive but algorithmically opaque.
  We show for the first time that organismal agency can be implemented as a concrete computational process through heritable phenotypic patterns. We introduce the Phenopoiesis Algorithm, where organisms inherit not just genes but also successful phenotypic patterns discovered during lifetime learning. Through experiments in changing environments, these pattern-inheriting organisms achieve 3.4 times faster adaptation compared to gene-centric models. Critically, these gains require cross-generational inheritance of learned patterns rather than within-lifetime learning alone.
  We conclude that organismal agency is not a philosophical abstraction but an algorithmic mechanism with measurable adaptive value. The mechanism works through compositional reuse: organisms discover how to compose primitive elements into solutions, encode those compositional recipes, and transmit them to offspring. Evolution operates across multiple timescales -- fast, reversible phenotypic inheritance and slow, permanent genetic inheritance -- providing adaptive flexibility that single-channel mechanisms cannot achieve.

</details>


### [649] [The Stacked Autoencoder Evolution Hypothesis](https://arxiv.org/abs/2602.01026)
*Hiroyuki Iizuka*

Main category: cs.NE

TL;DR: 提出堆叠自编码器进化假设，认为生物进化系统通过多层自编码和解码过程运行，用人工化学模拟验证，提供进化新视角。


<details>
  <summary>Details</summary>
Motivation: 突破将进化仅视为由突变和选择驱动的渐变观点，寻求新的进化解释机制。

Method: 提出堆叠自编码器进化假设，并进行人工化学模拟。

Result: 人工化学模拟证明了分层自编码器结构的自发出现。

Conclusion: 该框架为连续和不连续进化变化的信息动力学提供了新视角。

Abstract: This study introduces a novel theoretical framework, the Stacked Autoencoder Evolution Hypothesis, which proposes that biological evolutionary systems operate through multi-layered self-encoding and decoding processes, analogous to stacked autoencoders in deep learning. Rather than viewing evolution solely as gradual changes driven by mutation and selection, this hypothesis suggests that self-replication inherently compresses and reconstructs genetic information across hierarchical layers of abstraction. This layered structure enables evolutionary systems to explore diverse possibilities not only at the sequence level but also across progressively more abstract layers of representation, making it possible for even simple mutations to navigate these higher-order spaces.Such a mechanism may explain punctuated evolutionary patterns and changes that can appear as if they are goal-directed in natural evolution, by allowing mutations at deeper latent layers to trigger sudden, large-scale phenotypic shifts. To illustrate the plausibility of this mechanism, artificial chemistry simulations were conducted, demonstrating the spontaneous emergence of hierarchical autoencoder structures. This framework offers a new perspective on the informational dynamics underlying both continuous and discontinuous evolutionary change.

</details>


### [650] [Parallel Training in Spiking Neural Networks](https://arxiv.org/abs/2602.01133)
*Yanbin Huang,Man Yao,Yuqi Pan,Changze Lv,Siyuan Xu,Xiaoqing Zheng,Bo Xu,Guoqi Li*

Main category: cs.NE

TL;DR: 本文提出设计并行脉冲神经元的功能视角，移除重置机制，提出动态衰减脉冲神经元，在训练效率、通用性和能耗方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 大模型发展要求脉冲神经元支持高度并行计算，而重置机制阻碍并行训练。

Method: 移除重置机制并满足保留重置功能和支持并行训练两个原则，提出动态衰减脉冲神经元。

Result: 在16k长度序列上训练速度比先驱并行脉冲神经元快25.6倍，2k长度训练模型可在30k长度序列上稳定推理；方法在多任务、架构和激活模式下有效；神经元脉冲发射低于普通和现有并行脉冲神经元。

Conclusion: 提出的动态衰减脉冲神经元能有效支持脉冲神经网络的并行计算，具有良好的性能和通用性。

Abstract: The bio-inspired integrate-fire-reset mechanism of spiking neurons constitutes the foundation for efficient processing in Spiking Neural Networks (SNNs). Recent progress in large models demands that spiking neurons support highly parallel computation to scale efficiently on modern GPUs. This work proposes a novel functional perspective that provides general guidance for designing parallel spiking neurons. We argue that the reset mechanism, which induces complex temporal dependencies and hinders parallel training, should be removed. However, any such modification should satisfy two principles: 1) preserving the functions of reset as a core biological mechanism; and 2) enabling parallel training without sacrificing the serial inference ability of spiking neurons, which underpins their efficiency at test time. To this end, we identify the functions of the reset and analyze how to reconcile parallel training with serial inference, upon which we propose a dynamic decay spiking neuron. We conduct comprehensive testing of our method in terms of: 1) Training efficiency and extrapolation capability. On 16k-length sequences, we achieve a 25.6x training speedup over the pioneering parallel spiking neuron, and our models trained on 2k-length can stably perform inference on sequences as long as 30k. 2) Generality. We demonstrate the consistent effectiveness of the proposed method across five task categories (image classification, neuromorphic event processing, time-series forecasting, language modeling, and reinforcement learning), three network architectures (spiking CNN/Transformer/SSMs), and two spike activation modes (spike/integer activation). 3) Energy consumption. The spiking firing of our neuron is lower than that of vanilla and existing parallel spiking neurons.

</details>


### [651] [Unleashing the Potential of Differential Evolution through Individual-Level Strategy Diversity](https://arxiv.org/abs/2602.01147)
*Chenchen Feng,Minyang Chen,Zhuozhao Li,Ran Cheng*

Main category: cs.NE

TL;DR: 本文研究个体层面策略多样性对差分进化（DE）的影响，提出iStratDE，证明其并发性和收敛性，实验表明它能提升DE性能。


<details>
  <summary>Details</summary>
Motivation: 现有DE变体多通过自适应机制或复杂设计提升性能，忽略了静态策略多样性的结构优势。

Method: 提出iStratDE，在初始化时为每个个体独立分配变异和交叉策略，并保持固定，研究其并发特性，进行收敛性分析。

Result: 在CEC2022基准套件和机器人控制任务上，iStratDE达到或超过现有的自适应DE变体。

Conclusion: 个体层面的策略分配是一种简单有效的提升DE性能的机制。

Abstract: Since Differential Evolution (DE) is sensitive to strategy choice, most existing variants pursue performance through adaptive mechanisms or intricate designs. While these approaches focus on adjusting strategies over time, the structural benefits that static strategy diversity may bring remain largely unexplored. To bridge this gap, we study the impact of individual-level strategy diversity on DE's search dynamics and performance, and introduce iStratDE (DE with individual-level strategies), a minimalist variant that assigns mutation and crossover strategies independently to each individual at initialization and keeps them fixed throughout the evolutionary process. By injecting diversity at the individual level without adaptation or feedback, iStratDE cultivates persistent behavioral heterogeneity that is especially effective with large populations. Moreover, its communication-free construction possesses intrinsic concurrency, thereby enabling efficient parallel execution and straightforward scaling for GPU computing. We further provide a convergence analysis of iStratDE under standard reachability assumptions, which establishes the almost-sure convergence of the best-so-far fitness. Extensive experiments on the CEC2022 benchmark suite and robotic control tasks demonstrate that iStratDE matches or surpasses established adaptive DE variants. These results highlight individual-level strategy assignment as a straightforward yet effective mechanism for enhancing DE's performance. The source code of iStratDE is publicly accessible at: https://github.com/EMI-Group/istratde.

</details>


### [652] [Dynamic Heuristic Neuromorphic Solver for the Edge User Allocation Problem with Bayesian Confidence Propagation Neural Network](https://arxiv.org/abs/2602.01294)
*Kecheng Zhang,Anders Lansner,Ahsan Javed Awan,Naresh Balaji Ravichandran,Pawel Herman*

Main category: cs.NE

TL;DR: 提出用带WTA机制的吸引子网络解决NP难的边缘用户分配问题，有实时动态启发式偏置和“无分配”状态，接近最优且兼容神经形态架构。


<details>
  <summary>Details</summary>
Motivation: 解决NP难的边缘用户分配问题，提高效率。

Method: 使用带WTA机制的吸引子网络，基于BCPNN框架，采用动态启发式偏置和“无分配”状态。

Result: 能在经验上有界的时间步内实现接近最优的性能。

Conclusion: 该方法兼容神经形态架构，可能提高能源效率。

Abstract: We propose a neuromorphic solver for the NP-hard Edge User Allocation problem using an attractor network with Winner-Takes-All (WTA) mechanism implemented with the Bayesian Confidence Propagation Neural Network (BCPNN) framework. Unlike previous energy-based attractor networks, our solver uses dynamic heuristic biasing to guide allocations in real time and introduces a "no allocation" state to each WTA motif, achieving near-optimal performance with an empirically upper-bounded number of time steps. The approach is compatible with neuromorphic architectures and may offer improvements in energy efficiency.

</details>


### [653] [SpikingGamma: Surrogate-Gradient Free and Temporally Precise Online Training of Spiking Neural Networks with Smoothed Delays](https://arxiv.org/abs/2602.01978)
*Roel Koopman,Sebastian Otte,Sander Bohté*

Main category: cs.NE

TL;DR: 本文针对SNN训练难题提出SpikingGamma模型，支持无替代梯度的误差反向传播，能在线学习精细时间模式，适用于复杂任务且对时间分辨率不敏感。


<details>
  <summary>Details</summary>
Motivation: 当前SNN在精细时间离散化下训练存在挑战，现有方法在时间分辨率扩展性和捕捉时间模式方面表现不佳。

Method: 开发具有内部递归记忆结构的脉冲神经元，并结合sigma - delta脉冲编码，形成SpikingGamma模型。

Result: 该模型支持无替代梯度的直接误差反向传播，能在线学习精细时间模式，可将前馈SNN扩展到复杂任务和基准测试，且对时间分辨率不敏感。

Conclusion: 此方法为当前用替代梯度训练的递归SNN提供了替代方案，也为将SNN映射到神经形态硬件提供了直接途径。

Abstract: Neuromorphic hardware implementations of Spiking Neural Networks (SNNs) promise energy-efficient, low-latency AI through sparse, event-driven computation. Yet, training SNNs under fine temporal discretization remains a major challenge, hindering both low-latency responsiveness and the mapping of software-trained SNNs to efficient hardware. In current approaches, spiking neurons are modeled as self-recurrent units, embedded into recurrent networks to maintain state over time, and trained with BPTT or RTRL variants based on surrogate gradients. These methods scale poorly with temporal resolution, while online approximations often exhibit instability for long sequences and tend to fail at capturing temporal patterns precisely. To address these limitations, we develop spiking neurons with internal recursive memory structures that we combine with sigma-delta spike-coding. We show that this SpikingGamma model supports direct error backpropagation without surrogate gradients, can learn fine temporal patterns with minimal spiking in an online manner, and scale feedforward SNNs to complex tasks and benchmarks with competitive accuracy, all while being insensitive to the temporal resolution of the model. Our approach offers both an alternative to current recurrent SNNs trained with surrogate gradients, and a direct route for mapping SNNs to neuromorphic hardware.

</details>


### [654] [Scale-covariant spiking wavelets](https://arxiv.org/abs/2602.02020)
*Jens Egholm Pedersen,Tony Lindeberg,Peter Gerstoft*

Main category: cs.NE

TL;DR: 通过尺度空间理论建立小波变换与脉冲神经网络的理论联系，实现离散母小波，实验证明方法可行，提出新信号表示。


<details>
  <summary>Details</summary>
Motivation: 建立小波变换和脉冲神经网络之间的理论联系，探索更节能的信号处理算法。

Method: 借助漏电积分 - 放电神经元中的尺度协变特性来实现离散母小波以近似连续小波。

Result: 重建实验证明了该方法的可行性。

Conclusion: 提出了一种新颖的脉冲信号表示方法，有望实现更节能的信号处理算法，但需减轻当前的近似误差。

Abstract: We establish a theoretical connection between wavelet transforms and spiking neural networks through scale-space theory. We rely on the scale-covariant guarantees in the leaky integrate-and-fire neurons to implement discrete mother wavelets that approximate continuous wavelets. A reconstruction experiment demonstrates the feasibility of the approach and warrants further analysis to mitigate current approximation errors. Our work suggests a novel spiking signal representation that could enable more energy-efficient signal processing algorithms.

</details>


### [655] [Spark: Modular Spiking Neural Networks](https://arxiv.org/abs/2602.02306)
*Mario Franco,Carlos Gershenson*

Main category: cs.NE

TL;DR: 提出用于脉冲神经网络的新框架Spark，以解决现有神经网络数据和能量效率低问题，并通过简单可塑性机制解决稀疏奖励推车杆问题。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络在数据和能量方面效率低，脉冲神经网络适合高效硬件实现，但缺乏有效学习算法，希望找到提高数据效率的方法。

Method: 基于模块化设计理念构建新框架Spark，用简单可塑性机制解决稀疏奖励推车杆问题。

Result: 展示了新框架Spark可提供高效且精简的脉冲神经网络管道。

Conclusion: 与传统机器学习管道兼容的框架可能加速该领域研究，特别是连续和无批次学习。

Abstract: Nowadays, neural networks act as a synonym for artificial intelligence. Present neural network models, although remarkably powerful, are inefficient both in terms of data and energy. Several alternative forms of neural networks have been proposed to address some of these problems. Specifically, spiking neural networks are suitable for efficient hardware implementations. However, effective learning algorithms for spiking networks remain elusive, although it is suspected that effective plasticity mechanisms could alleviate the problem of data efficiency. Here, we present a new framework for spiking neural networks - Spark - built upon the idea of modular design, from simple components to entire models. The aim of this framework is to provide an efficient and streamlined pipeline for spiking neural networks. We showcase this framework by solving the sparse-reward cartpole problem with simple plasticity mechanisms. We hope that a framework compatible with traditional ML pipelines may accelerate research in the area, specifically for continuous and unbatched learning, akin to the one animals exhibit.

</details>


### [656] [Introns and Templates Matter: Rethinking Linkage in GP-GOMEA](https://arxiv.org/abs/2602.02311)
*Johannes Koch,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.NE

TL;DR: 提出两种新的连锁学习度量方法改进GP - GOMEA在符号回归中的表现


<details>
  <summary>Details</summary>
Motivation: 现有GP - GOMEA使用固定表达式模板学习连锁会产生内含子，影响互信息对树节点依赖关系的捕捉

Method: 提出两种新的连锁学习度量方法，一种在互信息估计中明确考虑内含子，另一种从灰盒角度重新审视连锁学习

Result: 在五个标准符号回归问题上，两种方法都使GP - GOMEA有显著改进，新学习的连锁结构与模板连锁结构紧密相关

Conclusion: 明确使用模板结构的方法整体性能最佳

Abstract: GP-GOMEA is among the state-of-the-art for symbolic regression, especially when it comes to finding small and potentially interpretable solutions. A key mechanism employed in any GOMEA variant is the exploitation of linkage, the dependencies between variables, to ensure efficient evolution. In GP-GOMEA, mutual information between node positions in GP trees has so far been used to learn linkage. For this, a fixed expression template is used. This however leads to introns for expressions smaller than the full template. As introns have no impact on fitness, their occurrences are not directly linked to selection. Consequently, introns can adversely affect the extent to which mutual information captures dependencies between tree nodes. To overcome this, we propose two new measures for linkage learning, one that explicitly considers introns in mutual information estimates, and one that revisits linkage learning in GP-GOMEA from a grey-box perspective, yielding a measure that needs not to be learned from the population but is derived directly from the template. Across five standard symbolic regression problems, GP-GOMEA achieves substantial improvements using both measures. We also find that the newly learned linkage structure closely reflects the template linkage structure, and that explicitly using the template structure yields the best performance overall.

</details>


### [657] [Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks and Hardware-Aware Optimization](https://arxiv.org/abs/2602.02439)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz,Duygu Erisken,Rana Irem Turhan*

Main category: cs.NE

TL;DR: 提出NeuEdge框架，结合自适应SNN模型与硬件感知优化用于边缘部署，在多个基准测试中表现良好，节能显著。


<details>
  <summary>Details</summary>
Motivation: 边缘AI应用需超低功耗、低延迟推理，而基于SNN的神经形态计算在资源受限设备上部署存在训练难、硬件映射开销大等问题。

Method: 采用混合速率和尖峰时间模式的时间编码方案，硬件感知训练程序共同优化网络结构和片上布局，自适应阈值机制根据输入统计调整神经元兴奋性。

Result: 在标准视觉和音频基准测试中，NeuEdge在边缘硬件上实现91 - 96%的准确率，推理延迟达2.3 ms，能量效率估计为847 GOp/s/W；在自主无人机工作负载案例中，与传统DNN相比节能达312倍。

Conclusion: NeuEdge框架能有效解决边缘AI应用在资源受限设备上的部署问题，实现低功耗、低延迟推理。

Abstract: Edge AI applications increasingly require ultra-low-power, low-latency inference. Neuromorphic computing based on event-driven spiking neural networks (SNNs) offers an attractive path, but practical deployment on resource-constrained devices is limited by training difficulty, hardware-mapping overheads, and sensitivity to temporal dynamics. We present NeuEdge, a framework that combines adaptive SNN models with hardware-aware optimization for edge deployment. NeuEdge uses a temporal coding scheme that blends rate and spike-timing patterns to reduce spike activity while preserving accuracy, and a hardware-aware training procedure that co-optimizes network structure and on-chip placement to improve utilization on neuromorphic processors. An adaptive threshold mechanism adjusts neuron excitability from input statistics, reducing energy consumption without degrading performance. Across standard vision and audio benchmarks, NeuEdge achieves 91-96% accuracy with up to 2.3 ms inference latency on edge hardware and an estimated 847 GOp/s/W energy efficiency. A case study on an autonomous-drone workload shows up to 312x energy savings relative to conventional deep neural networks while maintaining real-time operation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [658] [IntentCoding: Amplifying User Intent in Code Generation](https://arxiv.org/abs/2602.00066)
*Zheng Fang,Yihong Dong,Lili Mou,Dongming Jin,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: 现有大语言模型在遵循多约束细粒度用户意图进行代码生成时存在挑战，提出IntentCoding解码策略，构建CodeConstraints数据集，实验证明IntentCoding能显著提升约束满足和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成中遵循多约束用户意图存在困难，模型性能随约束数量增加下降，用户意图对解码过程影响不够强。

Method: 提出IntentCoding解码策略，通过掩码捕捉用户意图影响，采用多强度集成机制放大其效果；构建CodeConstraints数据集用于系统评估。

Result: 在多个数据集上进行实验，IntentCoding相比标准解码方法显著提高了约束满足度和功能正确性，在不同数据集上实现了不同程度的相对改进。

Conclusion: IntentCoding是一种有效的解码策略，能增强大语言模型遵循用户意图的能力，且具有模型无关性、无需额外训练等优点。

Abstract: Large Language Models (LLMs) have shown strong capabilities in code generation, but their adherence to fine-grained user intent with multiple constraints remains a significant challenge. Our empirical analysis reveals two key observations: 1) Model performance deteriorates quickly as the number of constraints in the user intent increases, and 2) While user intent does influence the model's logits, such an influence may not be strong enough to effectively steer the decoding process. To this end, we propose Intent-Amplified Code Generation (IntentCoding), a novel decoding strategy that enhances an LLM's ability to follow user intent. IntentCoding captures the influence of user intent by masking out the intent, and applies a multi-strength ensemble mechanism to amplify the effect of user intent during generation. IntentCoding is model-agnostic, requires no additional training, and integrates seamlessly with existing decoding procedures. To enable systematic evaluation, we also construct CodeConstraints, a benchmark dataset specifically designed to test user intent compliance under varying numbers of constraints. Experiments on our constructed Constraints, as well as popular IFEvalCode, HumanEval and LiveCodeBench datasets, show that our IntentCoding model significantly improves both constraint satisfaction and functional correctness compared to standard decoding approaches. IntentCoding achieves up to 71.0% relative improvement on CodeConstraints, achieves up to 67.3% relative improvement on IFEvalCode and achieves up to 29.3% relative improvement in pass@1 on HumanEval and LiveCodeBench compared with greedy decoding.

</details>


### [659] [Why Are AI Agent Involved Pull Requests (Fix-Related) Remain Unmerged? An Empirical Study](https://arxiv.org/abs/2602.00164)
*Khairul Alam,Saikat Mondal,Banani Roy*

Main category: cs.SE

TL;DR: 对AI编码代理参与的修复相关PR进行实证研究，分析整合结果、延迟及阻碍合并的因素，指出当前AI编码代理在现实场景的局限。


<details>
  <summary>Details</summary>
Motivation: 探究AI编码代理生成的修复相关PR是否能被项目维护者接受和合并，以明确其实践有效性。

Method: 分析AIDEV POP数据集中8106个由五个常用AI编码代理编写的修复相关PR，对326个关闭但未合并的PR进行手动定性分析。

Result: 测试用例失败和同一问题已被其他PR解决是未整合的最常见原因，构建失败或部署失败相对较少。

Conclusion: 揭示当前AI编码代理在现实场景中的关键局限，为其改进和人机协作提供方向。

Abstract: Autonomous coding agents (e.g., OpenAI Codex, Devin, GitHub Copilot) are increasingly used to generate fix-related pull requests (PRs) in real world software repositories. However, their practical effectiveness depends on whether these contributions are accepted and merged by project maintainers. In this paper, we present an empirical study of AI agent involved fix related PRs, examining both their integration outcomes, latency, and the factors that hinder successful merging. We first analyze 8,106 fix related PRs authored by five widely used AI coding agents from the AIDEV POP dataset to quantify the proportions of PRs that are merged, closed without merging, or remain open. We then conduct a manual qualitative analysis of a statistically significant sample of 326 closed but unmerged PRs, spending approximately 100 person hours to construct a structured catalog of 12 failure reasons. Our results indicate that test case failures and prior resolution of the same issues by other PRs are the most common causes of non integration, whereas build or deployment failures are comparatively rare. Overall, our findings expose key limitations of current AI coding agents in real world settings and highlight directions for their further improvement and for more effective human AI collaboration in software maintenance.

</details>


### [660] [Spec-Driven Development:From Code to Contract in the Age of AI Coding Assistants](https://arxiv.org/abs/2602.00180)
*Deepak Babu Piskala*

Main category: cs.SE

TL;DR: 本文为从业者提供了规范驱动开发（SDD）的全面指南，介绍不同规范严格程度、工具映射及案例，最后给出决策框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码助手兴起，重新探讨以规范而非代码作为软件开发主要产物，提供SDD的全面指导。

Method: 介绍SDD的原则、工作流模式和支持工具，提出三种规范严格程度，分析相关工具，展示不同领域案例。

Result: 展示了规范优先理念在实际工具中的映射，呈现不同领域应用SDD的案例。

Conclusion: 给出决策框架，帮助从业者判断SDD的适用场景。

Abstract: The rise of AI coding assistants has reignited interest in an old idea: what if specifications-not code-were the primary artifact of software development? Spec-driven development (SDD) inverts the traditional workflow by treating specifications as the source of truth and code as a generated or verified secondary artifact. This paper provides practitioners with a comprehensive guide to SDD, covering its principles, workflow patterns, and supporting tools. We present three levels of specification rigor-spec-first, spec-anchored, and spec-as-source-with clear guidance on when each applies. Through analysis of tools ranging from Behavior-Driven Development frameworks to modern AI-assisted toolkits like GitHub Spec Kit, we demonstrate how the spec-first philosophy maps to real implementations. We present case studies from API development, enterprise systems, and embedded software, illustrating how different domains apply SDD. We conclude with a decision framework helping practitioners determine when SDD provides value and when simpler approaches suffice.

</details>


### [661] [Towards Analyzing N-language Polyglot Programs](https://arxiv.org/abs/2602.00303)
*Jyoti Prakash,Abhishek Tiwari,Mikkel Baun Kjærgaard*

Main category: cs.SE

TL;DR: 随着多语言编程兴起，现有研究多关注双语程序分析，本文展望三语言多语言通信软件系统，提出分析挑战和概念路线图以推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 当前多语言程序分析研究主要集中于两种语言，无法应对使用三种及以上语言系统的日益复杂性，如现代 Web 系统。

Method: 识别三语言多语言通信系统分析中的基本挑战，提出概念路线图以改进静态分析技术。

Result: 提出概念性的路线图来应对分析挑战。

Conclusion: 愿景旨在激发关于下一代多语言系统可扩展、语言无关分析框架的讨论和新研究方向。

Abstract: Polyglot programming is gaining popularity as developers integrate multiple programming languages to harness their individual strengths. With the recent popularity of platforms like GraalVM and other multi-language runtimes, creating and managing these systems has become much more feasible. However, current research on analyzing multilingual programs mainly focuses on two languages, leaving out the increasing complexity of systems that use three or more. For example, modern web systems often link JavaScript, WebAssembly, and Rust within the same execution chain. This paper envisions the landscape of software systems with three-language polyglot communication. We identify fundamental challenges in analyzing them and propose a conceptual roadmap to advance static analysis techniques to address them. Our vision aims to stimulate discussion and inspire new research directions toward scalable, language-agnostic analysis frameworks for next-generation polyglot systems.

</details>


### [662] [Are Coding Agents Generating Over-Mocked Tests? An Empirical Study](https://arxiv.org/abs/2602.00409)
*Andre Hora,Romain Robbes*

Main category: cs.SE

TL;DR: 研究编码代理生成的测试用例中模拟对象的情况，发现编码代理更易修改测试和添加模拟对象，新仓库中代理提交占比高。


<details>
  <summary>Details</summary>
Motivation: 编码代理可自动生成软件测试，但测试质量不确定，过度使用模拟会使测试难理解和维护，需研究其在实际系统测试中的使用情况。

Method: 分析2168个TypeScript、JavaScript和Python仓库在2025年的超120万次提交，包括编码代理提交等。

Result: 编码代理比非编码代理更易修改测试和添加模拟对象，60%有代理活动的仓库有代理测试活动等。

Conclusion: 含模拟的测试可能易自动生成但验证效果差，需在代理配置文件中加入模拟实践指导。

Abstract: Coding agents have received significant adoption in software development recently. Unlike traditional LLM-based code completion tools, coding agents work with autonomy (e.g., invoking external tools) and leave visible traces in software repositories, such as authoring commits. Among their tasks, coding agents may autonomously generate software tests; however, the quality of these tests remains uncertain. In particular, excessive use of mocking can make tests harder to understand and maintain. This paper presents the first study to investigate the presence of mocks in agent-generated tests of real-world software systems. We analyzed over 1.2 million commits made in 2025 in 2,168 TypeScript, JavaScript, and Python repositories, including 48,563 commits by coding agents, 169,361 commits that modify tests, and 44,900 commits that add mocks to tests. Overall, we find that coding agents are more likely to modify tests and to add mocks to tests than non-coding agents. We detect that (1) 60% of the repositories with agent activity also contain agent test activity; (2) 23% of commits made by coding agents add/change test files, compared with 13% by non-agents; (3) 68% of the repositories with agent test activity also contain agent mock activity; (4) 36% of commits made by coding agents add mocks to tests, compared with 26% by non-agents; and (5) repositories created recently contain a higher proportion of test and mock commits made by agents. Finally, we conclude by discussing implications for developers and researchers. We call attention to the fact that tests with mocks may be potentially easier to generate automatically (but less effective at validating real interactions), and the need to include guidance on mocking practices in agent configuration files.

</details>


### [663] [GitEvo: Code Evolution Analysis for Git Repositories](https://arxiv.org/abs/2602.00410)
*Andre Hora*

Main category: cs.SE

TL;DR: 本文提出多语言可扩展工具GitEvo用于分析Git仓库代码演变，介绍其作用并提供开源链接。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏支持代码演变分析的工具，而分析软件系统代码演变对从业者、研究者和教育者有重要意义。

Method: 利用Git框架和代码解析工具，集成Git层面和代码层面分析。

Result: 提出GitEvo工具。

Conclusion: GitEvo可支持新实证研究，可作为教育工具，且已开源。

Abstract: Analyzing the code evolution of software systems is relevant for practitioners, researchers, and educators. It can help practitioners identify design trends and maintenance challenges, provide researchers with empirical data to study changes over time, and give educators real-world examples that enhance the teaching of software evolution concepts. Unfortunately, we lack tools specifically designed to support code evolution analysis. In this paper, we propose GitEvo, a multi-language and extensible tool for analyzing code evolution in Git repositories. GitEvo leverages Git frameworks and code parsing tools to integrate both Git-level and code-level analysis. We conclude by describing how GitEvo can support the development of novel empirical studies on code evolution and act as a learning tool for educators and students. GitEvo is available at: https://github.com/andrehora/gitevo.

</details>


### [664] [Context-Sensitive Pointer Analysis for ArkTS](https://arxiv.org/abs/2602.00457)
*Yizhuo Yang,Lingyun Xu,Mingyi Zhou,Li Li*

Main category: cs.SE

TL;DR: 现有ArkTS调用图生成方法有精度局限，传统分析工具难以解释语义。本文提出面向ArkTS的上下文敏感指针分析框架APAK，评估显示其性能优越，已并入官方框架。


<details>
  <summary>Details</summary>
Motivation: 当前ArkTS调用图生成方法在支持高级静态分析任务时有精度局限，传统JS/TS分析工具无法解释ArkUI组件树语义，现有ArkTS静态分析工具难以有效跟踪和精确推导对象引用关系，限制了高级程序分析技术的实现。

Method: 提出名为ArkAnalyzer Pointer Analysis Kit (APAK)的工具，通过独特的ArkTS堆对象模型和高度可扩展的插件架构解决问题。

Result: 构建包含1663个真实应用的数据集评估APAK，其在有效边覆盖率等关键指标上优于CHA/RTA方法，边覆盖率提升使误报率从20%降至2%。

Conclusion: APAK取得了较好的效果，可用于构建更复杂的程序分析工具，且已并入OpenHarmony官方静态分析框架ArkAnalyzer。

Abstract: Current call graph generation methods for ArkTS, a new programming language for OpenHarmony, exhibit precision limitations when supporting advanced static analysis tasks such as data flow analysis and vulnerability pattern detection, while the workflow of traditional JavaScript(JS)/TypeScript(TS) analysis tools fails to interpret ArkUI component tree semantics. The core technical bottleneck originates from the closure mechanisms inherent in TypeScript's dynamic language features and the interaction patterns involving OpenHarmony's framework APIs. Existing static analysis tools for ArkTS struggle to achieve effective tracking and precise deduction of object reference relationships, leading to topological fractures in call graph reachability and diminished analysis coverage. This technical limitation fundamentally constrains the implementation of advanced program analysis techniques.
  Therefore, in this paper, we propose a tool named ArkAnalyzer Pointer Analysis Kit (APAK), the first context-sensitive pointer analysis framework specifically designed for ArkTS. APAK addresses these challenges through a unique ArkTS heap object model and a highly extensible plugin architecture, ensuring future adaptability to the evolving OpenHarmony ecosystem. In the evaluation, we construct a dataset from 1,663 real-world applications in the OpenHarmony ecosystem to evaluate APAK, demonstrating APAK's superior performance over CHA/RTA approaches in critical metrics including valid edge coverage (e.g., a 7.1% reduction compared to CHA and a 34.2% increase over RTA). The improvement in edge coverage systematically reduces false positive rates from 20% to 2%, enabling future exploration of establishing more complex program analysis tools based on our framework. Our proposed APAK has been merged into the official static analysis framework ArkAnalyzer for OpenHarmony.

</details>


### [665] [Beyond Basic Specifications? A Systematic Study of Logical Constructs in LLM-based Specification Generation](https://arxiv.org/abs/2602.00715)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: 本文探讨用大语言模型生成程序规范，通过实验证明其能生成有效逻辑构造，结合逻辑与基本语法构造可提升验证能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型生成程序规范的研究局限于基本语法构造，无法满足复杂程序验证的高层抽象需求，且缺乏对大语言模型生成复杂构造的系统研究。

Method: 定义四种不同抽象级别的语法配置，在主流程序验证数据集上对多种代表性大语言模型进行广泛评估。

Result: 大语言模型能生成有效逻辑构造，结合逻辑和基本语法构造可提升验证能力和鲁棒性，且不显著增加验证开销，还发现两种细化范式的独特优势。

Conclusion: 这是首个系统探索利用大语言模型生成高层逻辑构造可行性的工作，为未来构建具有更高抽象能力的自动化程序验证框架提供实证基础和指导。

Abstract: Formal specifications play a pivotal role in accurately characterizing program behaviors and ensuring software correctness. In recent years, leveraging large language models (LLMs) for the automatic generation of program specifications has emerged as a promising avenue for enhancing verification efficiency. However, existing research has been predominantly confined to generating specifications based on basic syntactic constructs, falling short of meeting the demands for high-level abstraction in complex program verification. Consequently, we propose incorporating logical constructs into existing LLM-based specification generation framework. Nevertheless, there remains a lack of systematic investigation into whether LLMs can effectively generate such complex constructs. To this end, we conduct an empirical study aimed at exploring the impact of various types of syntactic constructs on specification generation framework. Specifically, we define four syntactic configurations with varying levels of abstraction and perform extensive evaluations on mainstream program verification datasets, employing a diverse set of representative LLMs. Experimental results first confirm that LLMs are capable of generating valid logical constructs. Further analysis reveals that the synergistic use of logical constructs and basic syntactic constructs leads to improvements in both verification capability and robustness, without significantly increasing verification overhead. Additionally, we uncover the distinct advantages of two refinement paradigms. To the best of our knowledge, this is the first systematic work exploring the feasibility of utilizing LLMs for generating high-level logical constructs, providing an empirical basis and guidance for the future construction of automated program verification framework with enhanced abstraction capabilities.

</details>


### [666] [Can Vision-Language Models Handle Long-Context Code? An Empirical Study on Visual Compression](https://arxiv.org/abs/2602.00746)
*Jianping Zhong,Guochang Li,Chen Zhi,Junxiao Han,Zhen Qin,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: 为解决大语言模型处理长上下文代码的问题，提出视觉压缩框架LongCodeOCR，经评估是可行替代方案，且分析了视觉与文本代码压缩的权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型受窗口限制难以处理长上下文代码，现有文本代码压缩方法会破坏依赖关系导致语义碎片化。

Method: 引入LongCodeOCR视觉压缩框架，将代码渲染为压缩二维图像序列供视觉语言模型使用，并与LongCodeZip在四个基准测试中对比。

Result: 在相近压缩比下，LongCodeOCR在长模块总结中提高CompScore；在1M令牌上下文长度下，精度更高、压缩率约高4倍、大幅降低压缩阶段开销。

Conclusion: 视觉代码压缩是需要全局理解任务的可行替代方案，且视觉与文本代码压缩存在覆盖 - 保真度权衡。

Abstract: Large Language Models (LLMs) struggle with long-context code due to window limitations. Existing textual code compression methods mitigate this via selective filtering but often disrupt dependency closure, causing semantic fragmentation. To address this, we introduce LongCodeOCR, a visual compression framework that renders code into compressed two-dimensional image sequences for Vision-Language Models (VLMs). By preserving a global view, this approach avoids the dependency breakage inherent in filtering. We systematically evaluate LongCodeOCR against the state-of-the-art LongCodeZip across four benchmarks spanning code summarization, code question answering, and code completion.
  Our results demonstrate that visual code compression serves as a viable alternative for tasks requiring global understanding. At comparable compression ratios ($\sim$1.7$\times$), LongCodeOCR improves CompScore on Long Module Summarization by 36.85 points over LongCodeZip. At a 1M-token context length with Glyph (a specialized 9B VLM), LongCodeOCR maintains higher accuracy than LongCodeZip while operating at about 4$\times$ higher compression. Moreover, compared with LongCodeZip, LongCodeOCR drastically reduces compression-stage overhead (reducing latency from $\sim$4.3 hours to $\sim$1 minute at 1M tokens). Finally, our results characterize a fundamental coverage--fidelity trade-off: visual code compression retains broader context coverage to support global dependencies, yet faces fidelity bottlenecks on exactness-critical tasks; by contrast, textual code compression preserves symbol-level precision while sacrificing structural coverage.

</details>


### [667] [ScratchEval : A Multimodal Evaluation Framework for LLMs in Block-Based Programming](https://arxiv.org/abs/2602.00757)
*Yuan Si,Simeng Han,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: 本文指出大语言模型在基于Scratch的编程任务表现不可靠，引入ScratchEval基准来评估其修复能力，提出评估协议并开展研究，为相关任务提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在Scratch任务中因语义理解不足而表现不佳，缺乏评估其修复能力的有效方法。

Method: 引入ScratchEval基准，包含100个项目，有测试套件、修复信息等；提出三层可执行评估协议。

Result: 用ScratchEval研究了领域微调、训练数据有效性和模型泛化能力。

Conclusion: ScratchEval为评估和训练大语言模型在基于块的编程任务中提供了可重复的基础。

Abstract: LLMs have achieved strong performance on text-based programming tasks, yet they remain unreliable for block-based languages such as Scratch. Scratch programs exhibit deeply nested, non-linear structures, event-driven concurrency across multiple sprites, and tight coupling between code and multimedia assets, properties that differ fundamentally from textual code. As a result, LLMs often misinterpret Scratch semantics and generate large, invasive edits that are syntactically valid but semantically incorrect when repairing buggy programs.
  We introduce ScratchEval, the first executable benchmark designed to evaluate LLM-based repair for Scratch programs, covering program understanding, debugging, analysis, and repair. The benchmark contains 100 curated Scratch projects from the public repository, selected for structural and semantic complexity. Each project is paired with executable test suites, bug descriptions with corresponding fixes, block-level edit constraints defining minimal semantically correct repairs, and required multimedia assets. The benchmark is constructed through a human-in-the-loop pipeline combining automated project mining with expert validation of trigger-outcome semantics and representative bug patterns, with emphasis on event ordering, concurrency, and state management.
  To enable rigorous and reproducible evaluation, we propose a three-layer executable protocol measuring functional correctness via VM-level execution, repair quality using block-level edit distance and behavioral trajectory comparisons, and explanation quality via structured rubrics assessing alignment between model reasoning and generated patches. Using ScratchEval, we study domain-specific fine-tuning, training data effectiveness, and model generalization to unseen bug types. ScratchEval provides a reproducible foundation for evaluating and post-training LLMs on block-based programming tasks.

</details>


### [668] [Test Behaviors, Not Methods! Detecting Tests Obsessed by Methods](https://arxiv.org/abs/2602.00761)
*Andre Hora,Andy Zaidman*

Main category: cs.SE

TL;DR: 本文指出测试应验证单一功能，现有“Eager Test”衡量方法不准确，提出新测试异味“Test Obsessed by Method”，并对Python标准库测试用例进行实证研究。


<details>
  <summary>Details</summary>
Motivation: 现有通过统计生产方法调用次数识别测试异味“Eager Test”不准确，需寻找新的解决方案。

Method: 提出基于运行时分析的新测试异味“Test Obsessed by Method”，并对Python标准库12个测试套件的2054个测试用例进行实证研究。

Result: 在11个测试套件中检测到44个“Tests Obsessed by Methods”，每个有问题的测试用例平均验证生产方法的两个行为，可拆分为118个新测试用例，23%的有问题测试用例有代码注释表明正在测试不同行为。

Conclusion: 讨论了该研究的优点、局限性和未来研究方向。

Abstract: Best testing practices state that tests should verify a single functionality or behavior of the system. Tests that verify multiple behaviors are harder to understand, lack focus, and are more coupled to the production code. An attempt to identify this issue is the test smell \emph{Eager Test}, which aims to capture tests that verify too much functionality based on the number of production method calls. Unfortunately, prior research suggests that counting production method calls is an inaccurate measure, as these calls do not reliably serve as a proxy for functionality. We envision a complementary solution based on runtime analysis: we hypothesize that some tests that verify multiple behaviors will likely cover multiple paths of the same production methods. Thus, we propose a novel test smell named \emph{Test Obsessed by Method}, a test method that covers multiple paths of a single production method. We provide an initial empirical study to explore the presence of this smell in 2,054 tests provided by 12 test suites of the Python Standard Library. (1) We detect 44 \emph{Tests Obsessed by Methods} in 11 of the 12 test suites. (2) Each smelly test verifies a median of two behaviors of the production method. (3) The 44 smelly tests could be split into 118 novel tests. (4) 23% of the smelly tests have code comments recognizing that distinct behaviors are being tested. We conclude by discussing benefits, limitations, and further research.

</details>


### [669] [Code Quality Analysis of Translations from C to Rust](https://arxiv.org/abs/2602.00840)
*Biruk Tadesse,Vikram Nitin,Mazin Salah,Baishakhi Ray,Marcelo d'Amorim,Wesley Assunção*

Main category: cs.SE

TL;DR: 本文对比三种C到Rust翻译工具，发现新方法有新问题，自动化方法难超人工翻译，翻译质量仍是多维度挑战。


<details>
  <summary>Details</summary>
Motivation: 现有C/C++到Rust翻译研究多关注正确性与安全性，忽视性能、健壮性和可维护性等质量问题。

Method: 以人工翻译为基线，对流行GNU coreutils的翻译结果进行量化和定性分析，运用Clippy、GPT-4o分析，结合人工分析。

Result: 新方法减少一些不安全和非惯用模式，但引入新问题，自动化方法在各质量维度难超人工翻译，人工翻译也有可读性等问题。

Conclusion: 翻译质量是多维度挑战，需系统评估和针对性工具支持，而非简单自动化或手动重写。

Abstract: C/C++ is a prevalent programming language. Yet, it suffers from significant memory and thread-safety issues. Recent studies have explored automated translation of C/C++ to safer languages, such as Rust. However, these studies focused mostly on the correctness and safety of the translated code, which are indeed critical, but they left other important quality concerns (e.g., performance, robustness, and maintainability) largely unexplored. This work investigates strengths and weaknesses of three C-to-Rust translators, namely C2Rust (a transpiler), C2SaferRust (an LLM-guided transpiler), and TranslationGym (an LLM-based direct translation). We perform an in-depth quantitative and qualitative analysis of several important quality attributes for the translated Rust code of the popular GNU coreutils, using human-based translation as a baseline. To assess the internal and external quality of the Rust code, we: (i) apply Clippy, a rule-based state-of-the-practice Rust static analysis tool; (ii) investigate the capability of an LLM (GPT-4o) to identify issues potentially overlooked by Clippy; and (iii) perform a manual analysis of the issues reported by Clippy and GPT-4o. Our results show that while newer techniques reduce some unsafe and non-idiomatic patterns, they frequently introduce new issues, revealing systematic trade-offs that are not visible under existing evaluation practices. Notably, none of the automated techniques consistently match or exceed human-written translations across all quality dimensions, yet even human-written Rust code exhibits persistent internal quality issues such as readability and non-idiomatic patterns. Together, these findings show that translation quality remains a multi-dimensional challenge, requiring systematic evaluation and targeted tool support beyond both naive automation and manual rewriting.

</details>


### [670] [MCP-Atlas: A Large-Scale Benchmark for Tool-Use Competency with Real MCP Servers](https://arxiv.org/abs/2602.00933)
*Chaithanya Bandi,Ben Hertzberg,Geobio Boo,Tejas Polakam,Jeff Da,Sami Hassaan,Manasi Sharma,Andrew Park,Ernesto Hernandez,Dan Rambado,Ivan Salazar,Rafael Cruz,Chetan Rane,Ben Levin,Brad Kenstler,Bing Liu*

Main category: cs.SE

TL;DR: 文章介绍了用于评估大语言模型工具使用能力的大规模基准MCP - Atlas，进行评估并揭示模型问题，还发布相关资源推动开发。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型外部工具调用能力的评估无法体现真实场景的复杂性，从而需要更合适的评估标准。

Method: 引入MCP - Atlas基准，含36个真实MCP服务器、220个工具、1000个任务，用基于主张的评分标准打分，结合内部诊断工具。

Result: 前沿模型评估中，顶级模型通过率超50%，主要失败原因是工具使用不足和任务理解不够。

Conclusion: 发布任务模式、容器化工具和部分基准数据集，促进可重复比较和提升工具增强型智能体的发展。

Abstract: The Model Context Protocol (MCP) is rapidly becoming the standard interface for Large Language Models (LLMs) to discover and invoke external tools. However, existing evaluations often fail to capture the complexity of real-world scenarios, relying on restricted toolsets, simplistic workflows, or subjective LLM-as-a-judge metrics. We introduce MCP-Atlas, a large-scale benchmark for evaluating tool-use competency, comprising 36 real MCP servers and 220 tools. It includes 1,000 tasks designed to assess tool-use competency in realistic, multi-step workflows. Tasks use natural language prompts that avoid naming specific tools or servers, requiring agents to identify and orchestrate 3-6 tool calls across multiple servers. We score tasks using a claims-based rubric that awards partial credit based on the factual claims satisfied in the model's final answer, complemented by internal diagnostics on tool discovery, parameterization, syntax, error recovery, and efficiency. Evaluation results on frontier models reveal that top models achieve pass rates exceeding 50%, with primary failures arising from inadequate tool usage and task understanding. We release the task schema, containerized harness, and a 500-task public subset of the benchmark dataset to facilitate reproducible comparisons and advance the development of robust, tool-augmented agents.

</details>


### [671] [Cast: Automated Resilience Testing for Production Cloud Service Systems](https://arxiv.org/abs/2602.00972)
*Zhuangbin Chen,Zhiling Deng,Kaiming Zhang,Yang Liu,Cheng Cui,Jinfeng Zhong,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出自动化端到端框架Cast用于生产环境微服务弹性测试，在华为云应用并证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 微服务架构分布式特性带来弹性挑战，传统测试方法有局限，无法捕捉生产系统复杂性。

Method: 通过重放生产流量到应用级故障库来测试，采用复杂度驱动策略管理测试空间，通过三阶段管道自动化测试生命周期，用多方面验证器自动验证系统弹性。

Result: 在华为云部署超八个月被多团队采用，分析四个大规模应用发现137个潜在漏洞，89个获开发者确认，在48个复现bug基准集上覆盖率达90%。

Conclusion: Cast是系统提升工业微服务系统可靠性的实用有效解决方案。

Abstract: The distributed nature of microservice architecture introduces significant resilience challenges. Traditional testing methods, limited by extensive manual effort and oversimplified test environments, fail to capture production system complexity. To address these limitations, we present Cast, an automated, end-to-end framework for microservice resilience testing in production. It achieves high test fidelity by replaying production traffic against a comprehensive library of application-level faults to exercise internal error-handling logic. To manage the combinatorial test space, Cast employs a complexity-driven strategy to systematically prune redundant tests and prioritize high-value tests targeting the most critical service execution paths. Cast automates the testing lifecycle through a three-phase pipeline (i.e., startup, fault injection, and recovery) and uses a multi-faceted oracle to automatically verify system resilience against nuanced criteria. Deployed in Huawei Cloud for over eight months, Cast has been adopted by many service teams to proactively address resilience vulnerabilities. Our analysis on four large-scale applications with millions of traces reveals 137 potential vulnerabilities, with 89 confirmed by developers. To further quantify its performance, Cast is evaluated on a benchmark set of 48 reproduced bugs, achieving a high coverage of 90%. The results show that Cast is a practical and effective solution for systematically improving the reliability of industrial microservice systems.

</details>


### [672] [Morphis: SLO-Aware Resource Scheduling for Microservices with Time-Varying Call Graphs](https://arxiv.org/abs/2602.01044)
*Yu Tang,Hailiang Zhao,Rui Shi,Chuansheng Lu,Yifei Zhang,Kingsum Chow,Shuiguang Deng*

Main category: cs.SE

TL;DR: 微服务系统运行时调用图有结构演变但存在潜在规律，现有资源管理方法未利用此结构，提出Morphis框架，评估显示能降低CPU消耗并保持SLO合规。


<details>
  <summary>Details</summary>
Motivation: 现代微服务系统运行时调用图复杂且现有资源管理方法无法利用潜在结构，需新的资源管理方案。

Method: 提出Morphis框架，结合模式感知跟踪分析与全局优化，引入结构指纹，将资源分配建模为约束优化问题。

Result: 在TrainTicket基准测试中，Morphis比现有基线减少35 - 38%的CPU消耗，保持98.8%的SLO合规。

Conclusion: Morphis能有效降低CPU消耗并保证服务质量，是更有效的微服务资源管理方案。

Abstract: Modern microservice systems exhibit continuous structural evolution in their runtime call graphs due to workload fluctuations, fault responses, and deployment activities. Despite this complexity, our analysis of over 500,000 production traces from ByteDance reveals a latent regularity: execution paths concentrate around a small set of recurring invocation patterns. However, existing resource management approaches fail to exploit this structure. Industrial autoscalers like Kubernetes HPA ignore inter-service dependencies, while recent academic methods often assume static topologies, rendering them ineffective under dynamic execution contexts. In this work, we propose Morphis, a dependency-aware provisioning framework that unifies pattern-aware trace analysis with global optimization. It introduces structural fingerprinting that decomposes traces into a stable execution backbone and interpretable deviation subgraphs. Then, resource allocation is formulated as a constrained optimization problem over predicted pattern distributions, jointly minimizing aggregate CPU usage while satisfying end-to-end tail-latency SLOs. Our extensive evaluations on the TrainTicket benchmark demonstrate that Morphis reduces CPU consumption by 35-38% compared to state-of-the-art baselines while maintaining 98.8% SLO compliance.

</details>


### [673] [SPELL: Synthesis of Programmatic Edits using LLMs](https://arxiv.org/abs/2602.01107)
*Daniel Ramos,Catarina Gamboa,Inês Lynce,Vasco Manquinho,Ruben Martins,Claire Le Goues*

Main category: cs.SE

TL;DR: 本文提出一种自动化 API 迁移新方法，不依赖现有迁移数据，利用 LLMs 提取迁移示例并生成可复用转换脚本，实验表明该系统可行。


<details>
  <summary>Details</summary>
Motivation: 现有自动化迁移工具依赖已有的迁移示例数据，这类数据稀缺，且现有工具未充分利用现代代码转换基础设施。

Method: 使用 LLMs 提取迁移示例，再通过 Agent 将示例泛化为 PolyglotPiranha 中的可复用转换脚本。

Result: 该系统能生成多样的迁移示例，合成的转换脚本可应用于真实代码库。

Conclusion: 新方法能将 LLMs 中的潜在迁移知识提炼为结构化、可测试和可重复的迁移逻辑，无需预存语料库或手动设计。

Abstract: Library migration is a common but error-prone task in software development. Developers may need to replace one library with another due to reasons like changing requirements or licensing changes. Migration typically entails updating and rewriting source code manually. While automated migration tools exist, most rely on mining examples from real-world projects that have already undergone similar migrations. However, these data are scarce, and collecting them for arbitrary pairs of libraries is difficult. Moreover, these migration tools often miss out on leveraging modern code transformation infrastructure.
  In this paper, we present a new approach to automated API migration that sidesteps the limitations described above. Instead of relying on existing migration data or using LLMs directly for transformation, we use LLMs to extract migration examples. Next, we use an Agent to generalize those examples to reusable transformation scripts in PolyglotPiranha, a modern code transformation tool. Our method distills latent migration knowledge from LLMs into structured, testable, and repeatable migration logic, without requiring preexisting corpora or manual engineering effort. Experimental results across Python libraries show that our system can generate diverse migration examples and synthesize transformation scripts that generalize to real-world codebases.

</details>


### [674] [Autoregressive, Yet Revisable: In Decoding Revision for Secure Code Generation](https://arxiv.org/abs/2602.01187)
*Chengran Yang,Zichao Wei,Heminghao Deng,Jinfeng Jiang,Zhensu Sun,Ting Zhang,Tianyi Wu,Ming Wen,David Lo*

Main category: cs.SE

TL;DR: 提出Stream of Revision范式，使代码生成从单调过程变为动态自纠正过程，在安全代码生成中减少漏洞且推理开销小。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码生成是单调过程，与编程认知过程不符，先前引入修订的方法有高延迟或未利用模型内在语义推理的问题。

Method: 提出Stream of Revision范式，引入特定动作标记，让模型在单次前向传播中回溯和编辑自身历史，内化修订循环。

Result: 在安全代码生成的实证结果表明，Stream of Revision显著减少漏洞，推理开销极小。

Conclusion: Stream of Revision范式能有效提升代码生成质量，减少漏洞，且无需外部依赖。

Abstract: Large Language Model (LLM) based code generation is predominantly formulated as a strictly monotonic process, appending tokens linearly to an immutable prefix. This formulation contrasts to the cognitive process of programming, which is inherently interleaved with forward generation and on-the-fly revision. While prior works attempt to introduce revision via post-hoc agents or external static tools, they either suffer from high latency or fail to leverage the model's intrinsic semantic reasoning. In this paper, we propose Stream of Revision, a paradigm shift that elevates code generation from a monotonic stream to a dynamic, self-correcting trajectory by leveraging model's intrinsic capabilities. We introduce specific action tokens that enable the model to seamlessly backtrack and edit its own history within a single forward pass. By internalizing the revision loop, our framework Stream of Revision allows the model to activate its latent capabilities just-in-time without external dependencies. Empirical results on secure code generation show that Stream of Revision significantly reduces vulnerabilities with minimal inference overhead.

</details>


### [675] [TraceLLM: Leveraging Large Language Models with Prompt Engineering for Enhanced Requirements Traceability](https://arxiv.org/abs/2602.01253)
*Nouf Alturayeif,Irfan Ahmad,Jameleddine Hassine*

Main category: cs.SE

TL;DR: 本文提出TraceLLM框架提升需求可追溯性，通过提示工程和示例选择，测试多种LLM模型，在多数据集上实现SOTA效果，证明提示工程质量重要。


<details>
  <summary>Details</summary>
Motivation: 传统需求可追溯性方法存在劳动密集、易出错和精度低的问题，同时缺乏针对准确提取跟踪链接的提示的系统设计和评估。

Method: 提出TraceLLM框架，包括严格的数据集划分、迭代提示细化、上下文角色和领域知识增强，并在零样本和少样本设置下评估，在八个LLM模型和四个基准数据集上进行实验。

Result: TraceLLM在多个数据集上实现了SOTA的F2分数，优于传统IR基线、微调模型和之前基于LLM的方法，并发现基于标签感知和多样性的示例选择策略特别有效。

Conclusion: 需求可追溯性性能不仅取决于模型容量，还关键取决于提示工程的质量，TraceLLM可支持半自动化可追溯性工作流程。

Abstract: Requirements traceability, the process of establishing and maintaining relationships between requirements and various software development artifacts, is paramount for ensuring system integrity and fulfilling requirements throughout the Software Development Life Cycle (SDLC). Traditional methods, including manual and information retrieval models, are labor-intensive, error-prone, and limited by low precision. Recently, Large Language Models (LLMs) have demonstrated potential for supporting software engineering tasks through advanced language comprehension. However, a substantial gap exists in the systematic design and evaluation of prompts tailored to extract accurate trace links. This paper introduces TraceLLM, a systematic framework for enhancing requirements traceability through prompt engineering and demonstration selection. Our approach incorporates rigorous dataset splitting, iterative prompt refinement, enrichment with contextual roles and domain knowledge, and evaluation across zero- and few-shot settings. We assess prompt generalization and robustness using eight state-of-the-art LLMs on four benchmark datasets representing diverse domains (aerospace, healthcare) and artifact types (requirements, design elements, test cases, regulations). TraceLLM achieves state-of-the-art F2 scores, outperforming traditional IR baselines, fine-tuned models, and prior LLM-based methods. We also explore the impact of demonstration selection strategies, identifying label-aware, diversity-based sampling as particularly effective. Overall, our findings highlight that traceability performance depends not only on model capacity but also critically on the quality of prompt engineering. In addition, the achieved performance suggests that TraceLLM can support semi-automated traceability workflows in which candidate links are reviewed and validated by human analysts.

</details>


### [676] [Evaluating Workflow Automation Efficiency Using n8n: A Small-Scale Business Case Study](https://arxiv.org/abs/2602.01311)
*Ahmed Raza Amir,Syed Muhammad Atif*

Main category: cs.SE

TL;DR: 研究通过小范围业务案例研究评估n8n工作流自动化的性能影响，发现自动化显著减少执行时间并消除错误，凸显低代码自动化对小规模工作流的有效性。


<details>
  <summary>Details</summary>
Motivation: 评估使用n8n进行工作流自动化的性能影响，提升小规模组织和个人运营效率。

Method: 实施代表性的潜在客户处理工作流，在受控条件下对比20次手动执行和25次自动执行的实验基准测试。

Result: 自动执行平均时间从手动的185.35秒降至1.23秒，约减少151倍；手动执行错误率5%，自动执行零错误。

Conclusion: 低代码自动化对小规模工作流在提高效率、可靠性和操作一致性方面有效。

Abstract: Workflow automation has become increasingly accessible through low-code platforms, enabling small organizations and individuals to improve operational efficiency without extensive software development expertise. This study evaluates the performance impact of workflow automation using n8n through a small-scale business case study. A representative lead-processing workflow was implemented to automatically store data, send email confirmations, and generate real-time notifications. Experimental benchmarking was conducted by comparing 20 manual executions with 25 automated executions under controlled conditions. The results demonstrate a significant reduction in the average execution time from 185.35 seconds (manual) to 1.23 seconds (automated), corresponding to an approximately 151 times reduction in execution time. Additionally, manual execution exhibited an error rate of 5%, while automated execution achieved zero observed errors. The findings highlight the effectiveness of low-code automation in improving efficiency, reliability, and operational consistency for small-scale workflows.

</details>


### [677] [AdNanny: One Reasoning LLM for All Offline Ads Recommendation Tasks](https://arxiv.org/abs/2602.01563)
*Nan Hu,Han Li,Jimeng Sun,Lu Wang,Fangkai Yang,Bo Qiao,Pu Zhao,David Dai,Mengyu Liu,Yuefeng Zhan,Jianjin Zhang,Weihao Han,Allen Sun,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang,Denvy Deng,Feng Sun,Qi Zhang*

Main category: cs.SE

TL;DR: 现有大语言模型直接用于在线广告系统不实际，现有离线方案有冗余等问题，本文提出统一推理中心LLM AdNanny，部署后效果好且成本低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型直接用于在线广告系统因延迟约束不实际，现有离线方案有模型冗余、维护成本高和性能提升有限等问题。

Method: 通过可扩展训练系统微调公开检查点得到AdNanny，构建推理增强语料，进行多任务监督微调，再用强化学习使模型与在线目标对齐。

Result: AdNanny部署在Bing Ads中，显著减少人工标注工作量，提高多个离线任务的准确性。

Conclusion: AdNanny将多个特定任务模型整合为单一推理中心基础模型，为大规模广告系统提供了可扩展且经济高效的解决方案。

Abstract: Large Language Models (LLMs) have shown strong capabilities in Natural Language Understanding and Generation, but deploying them directly in online advertising systems is often impractical due to strict millisecond-level latency constraints. This has motivated the use of LLMs offline to improve retrieval, ranking, and recommendation models. Existing solutions typically fine-tune separate LLMs for individual tasks such as query-ad relevance labeling, keyword-based query generation, and user profiling. This results in redundant models, high maintenance cost, and limited performance gains despite substantial overlap in domain knowledge and reasoning patterns. We introduce AdNanny, a unified reasoning-centric LLM that serves as a shared backbone for offline advertising tasks. AdNanny is obtained by fine-tuning a public 671B-parameter DeepSeek-R1 checkpoint using a scalable training system that supports hybrid dense-MoE parallelism. We construct reasoning-augmented corpora that pair structured supervision with step-by-step natural language explanations. A multi-task supervised fine-tuning stage with adaptive reweighting enables AdNanny to handle diverse labeling and generation tasks in a consistent reasoning format. This is followed by reinforcement learning using downstream advertising metrics to align model behavior with online retrieval and ranking objectives. AdNanny is deployed in production within Bing Ads, where it significantly reduces manual labeling effort and improves accuracy across multiple offline tasks. By consolidating many task-specific models into a single reasoning-centric foundation model, AdNanny provides a scalable and cost-effective solution for large-scale advertising systems.

</details>


### [678] [Role of CI Adoption in Mobile App Success: An Empirical Study of Open-Source Android Projects](https://arxiv.org/abs/2602.01957)
*Xiaoxin Zhou,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 研究分析开源安卓应用中持续集成（CI）的采用情况，发现CI采用者规模更大、更活跃，能带来更快且更规律的发布以及更高的谷歌应用商店用户参与度。


<details>
  <summary>Details</summary>
Motivation: 移动应用更新需快速可靠，CI虽能自动化构建、测试和发布，但对移动开发的影响缺乏研究，现有研究对移动特定动态洞察有限。

Method: 分析开源安卓应用，对比CI采用者和非采用者，用活动和缺陷指标刻画采用模式，评估采用前后变化和面向用户的结果。

Result: CI采用者更大更活跃，发布更快更规律，集中在集成和可靠性密集类别，关联更高的谷歌应用商店用户参与度且评级不降低。

Conclusion: CI采用符合支持持续交付、更高项目可见性和更强移动生态用户参与度的实践。

Abstract: Mobile apps face strong pressure for fast and reliable updates. Continuous Integration (CI) helps automate builds, tests, and releases, but its impact on mobile development remains underexplored. Despite the widespread use of CI, little is known about how it affects development activity, release speed, and user-facing outcomes in mobile projects. Existing studies mostly focus on CI adoption in general-purpose software, providing limited insight into mobile-specific dynamics, such as app store visibility and user engagement. In this paper, we analyze open-source Android apps to (1) compare CI adopters and non-adopters, (2) characterize adoption patterns using activity and bug metrics, and (3) assess pre/post adoption changes and user-facing outcomes. We observe that CI adopters are larger and more active, with faster and more regular releases. CI adoption is concentrated in integration- and reliability-intensive categories (e.g., finance and productivity) and is associated with higher Google Play Store engagement (more downloads and reviews) without lower ratings. Overall, CI adoption aligns with practices that support sustained delivery, higher project visibility, and stronger user engagement in mobile ecosystems.

</details>


### [679] [CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems](https://arxiv.org/abs/2602.02138)
*Lyu Zongyi,Ji Zhenlan,Chen Songqiang,Wang Liwen,Huang Yuheng,Wang Shuai,Cheung Shing-Chi*

Main category: cs.SE

TL;DR: 提出用于MACGS的因果分析框架CAM，量化中间特征对系统正确性的贡献，分析揭示重要发现并展示其在故障修复和特征剪枝中的应用，为MACGS设计和部署提供见解。


<details>
  <summary>Details</summary>
Motivation: 多智能体架构的MACGS产生大量中间输出，其对系统正确性的重要性不明确，阻碍了系统设计的针对性优化。

Method: 提出CAM框架，对中间输出进行分类，模拟现实错误，识别重要特征并汇总重要性排名。

Result: 发现上下文相关特征，混合后端MACGS的Pass@1提升7.2%；故障修复成功率达73.3%，特征剪枝减少66.8%中间令牌消耗。

Conclusion: 因果分析是理解和改进MACGS的有力方法，为其设计和部署提供了可操作的见解。

Abstract: Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness remains opaque, which impedes targeted optimization of MACGS designs. To address this challenge, we propose CAM, the first \textbf{C}ausality-based \textbf{A}nalysis framework for \textbf{M}ACGS that systematically quantifies the contribution of different intermediate features for system correctness. By comprehensively categorizing intermediate outputs and systematically simulating realistic errors on intermediate features, we identify the important features for system correctness and aggregate their importance rankings.
  We conduct extensive empirical analysis on the identified importance rankings. Our analysis reveals intriguing findings: first, we uncover context-dependent features\textemdash features whose importance emerges mainly through interactions with other features, revealing that quality assurance for MACGS should incorporate cross-feature consistency checks; second, we reveal that hybrid backend MACGS with different backend LLMs assigned according to their relative strength achieves up to 7.2\% Pass@1 improvement, underscoring hybrid architectures as a promising direction for future MACGS design. We further demonstrate CAM's practical utility through two applications: (1) failure repair which achieves a 73.3\% success rate by optimizing top-3 importance-ranked features and (2) feature pruning that reduces up to 66.8\% intermediate token consumption while maintaining generation performance. Our work provides actionable insights for MACGS design and deployment, establishing causality analysis as a powerful approach for understanding and improving MACGS.

</details>


### [680] [Agent-Based Software Artifact Evaluation](https://arxiv.org/abs/2602.02235)
*Zhaonan Wu,Yanjie Zhao,Zhenpeng Chen,Zheng Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: 软件工领域工件评估遇可扩展性挑战，提出ArtifactCopilot自动化框架，评估效果好且成本低。


<details>
  <summary>Details</summary>
Motivation: 软件工件评估的成功带来了可扩展性挑战，人工执行和调试成本高，需自动化评估。

Method: 对顶级软件工程会议工件进行初步研究，提出ArtifactCopilot框架，结合执行归一化策略和工件评估图。

Result: 在48个真实工件上评估，85.42%结果与人工评估匹配，优于Claude Code，平均每个工件成本0.091美元，45个无需人工干预。

Conclusion: ArtifactCopilot能有效解决软件工件评估的可扩展性问题，实现自动化评估。

Abstract: Artifact evaluation has been adopted in the Software Engineering (SE) research community for 15 years, substantially improving research reproducibility across major SE conferences. However, this success has introduced a growing scalability challenge, as artifact evaluation relies heavily on reviewers' manual execution and debugging, leading to escalating human effort amid rapidly increasing paper submissions. To address this problem, we investigate automated artifact evaluation. We first conduct a preliminary study on artifacts from top-tier SE conferences and identify three key challenges: perceiving execution states, maintaining stable execution environments, and recovering from execution errors. Inspired by these findings, we propose ArtifactCopilot, the first end-to-end agent-based framework for automated artifact evaluation. ArtifactCopilot automates environment construction, instruction execution, and error recovery by combining an execution normalization strategy to ensure environment stability with an artifact evaluation graph that transforms README documents into dependency-aware command graphs, enabling structured execution planning, execution-state tracking, and error recovery. Evaluation on 48 real-world artifacts shows that ArtifactCopilot matches human artifact evaluation outcomes for 85.42% of the artifacts, outperforming Claude Code by 52.09 percentage points, while costing only \$0.091 per artifact on average and requiring zero human intervention for 45 out of 48 artifacts.

</details>


### [681] [OmniCode: A Benchmark for Evaluating Software Engineering Agents](https://arxiv.org/abs/2602.02262)
*Atharv Sonwane,Eng-Shen Tu,Wei-Chung Lu,Claas Beger,Carter Larsen,Debjit Dhar,Rachel Chen,Ronit Pattanayak,Tuan Anh Dang,Guohao Chen,Gloria Geng,Kevin Ellis,Saikat Dutta*

Main category: cs.SE

TL;DR: 提出OmniCode编码基准，涵盖更多任务类型，评估了SWE - Agent等框架，发现其短板，旨在推动编码代理发展。


<details>
  <summary>Details</summary>
Motivation: 现有流行编码基准任务范围窄，需新基准评估编码代理处理各类软件工程任务的能力。

Method: 提出OmniCode基准，包含多种任务类别，任务经手动验证和合成，避免问题定义不清和数据泄露。

Result: 用流行框架评估OmniCode，发现部分框架在测试生成、C++和Java等方面表现不佳，如SWE - Agent在Java测试生成任务中最高达20.9%。

Conclusion: OmniCode可作为可靠基准，促进能在软件开发多方面表现良好的代理发展。

Abstract: LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.

</details>


### [682] [RACA: Representation-Aware Coverage Criteria for LLM Safety Testing](https://arxiv.org/abs/2602.02280)
*Zeming Wei,Zhixin Zhang,Chengcan Wu,Yihao Zhang,Xiaokun Luan,Meng Sun*

Main category: cs.SE

TL;DR: 本文针对大语言模型（LLM）安全测试问题，提出RACA覆盖标准，实验验证其有效性、适用性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型产生安全问题，现有安全测试依赖静态数据集且缺乏系统标准，传统覆盖标准对LLM不适用。

Method: 引入RACA，利用表征工程聚焦关键安全概念，分三步操作计算覆盖结果。

Result: RACA成功识别高质量越狱提示，优于传统神经元级标准，可用于测试集优先级排序和攻击提示采样。

Conclusion: RACA为评估大语言模型安全性提供新框架，是AI测试领域的有用技术。

Abstract: Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. While coverage criteria have been effective for smaller neural networks, they are not directly applicable to LLMs due to scalability issues and differing objectives. To address these challenges, this paper introduces RACA, a novel set of coverage criteria specifically designed for LLM safety testing. RACA leverages representation engineering to focus on safety-critical concepts within LLMs, thereby reducing dimensionality and filtering out irrelevant information. The framework operates in three stages: first, it identifies safety-critical representations using a small, expert-curated calibration set of jailbreak prompts. Second, it calculates conceptual activation scores for a given test suite based on these representations. Finally, it computes coverage results using six sub-criteria that assess both individual and compositional safety concepts. We conduct comprehensive experiments to validate RACA's effectiveness, applicability, and generalization, where the results demonstrate that RACA successfully identifies high-quality jailbreak prompts and is superior to traditional neuron-level criteria. We also showcase its practical application in real-world scenarios, such as test set prioritization and attack prompt sampling. Furthermore, our findings confirm RACA's generalization to various scenarios and its robustness across various configurations. Overall, RACA provides a new framework for evaluating the safety of LLMs, contributing a valuable technique to the field of testing for AI.

</details>


### [683] [Before Autonomy Takes Control: Software Testing in Robotics](https://arxiv.org/abs/2602.02293)
*Nils Chur,Thiago Santos de Moura,Argentina Ortega,Sven Peldszus,Thorsten Berger,Nico Hochgeschwender,Yannic Noller*

Main category: cs.SE

TL;DR: 本文对机器人测试论文进行映射研究并关联软件测试理论，探讨现状、挑战，识别开放性问题并总结经验。


<details>
  <summary>Details</summary>
Motivation: 机器人系统复杂且安全关键需充分测试，但机器人软件测试难，难以预见测试中的可能故障，故开展研究。

Method: 考虑247篇机器人测试论文，将其映射到软件测试，结合示例讨论机器人软件测试现状。

Result: 探讨了机器人软件测试的现状和当前挑战。

Conclusion: 为机器人和软件工程社区引入软件测试挑战奠定基础，识别出开放性问题和经验教训。

Abstract: Robotic systems are complex and safety-critical software systems. As such, they need to be tested thoroughly. Unfortunately, robot software is intrinsically hard to test compared to traditional software, mainly since the software needs to closely interact with hardware, account for uncertainty in its operational environment, handle disturbances, and act highly autonomously. However, given the large space in which robots operate, anticipating possible failures when designing tests is challenging. This paper presents a mapping study by considering robotics testing papers and relating them to the software testing theory. We consider 247 robotics testing papers and map them to software testing, discussing the state-of-the-art software testing in robotics with an illustrated example, and discuss current challenges. Forming the basis to introduce both the robotics and software engineering communities to software testing challenges. Finally, we identify open questions and lessons learned.

</details>


### [684] [Understanding and Detecting Flaky Builds in GitHub Actions](https://arxiv.org/abs/2602.02307)
*Wenhao Ge,Chen Zhang*

Main category: cs.SE

TL;DR: 本文对GitHub Actions中不稳定构建进行大规模实证研究，提出基于机器学习的作业级不稳定故障检测方法，相比基线F1分数提升达20.3%。


<details>
  <summary>Details</summary>
Motivation: CI构建结果不可靠，不稳定构建会破坏开发者对CI的信任、浪费资源和影响实证研究有效性。

Method: 基于1960个开源Java项目的重新运行数据进行实证研究，深入分析失败原因并提出基于机器学习的检测方法。

Result: 3.2%的构建被重新运行，67.73%的重新运行构建有不稳定行为，影响51.28%的项目；确定15种不稳定故障类别，测试、网络和依赖解析问题最常见；提出的方法F1分数相比基线提升达20.3%。

Conclusion: 所提出的基于机器学习的方法能有效检测作业级的不稳定故障。

Abstract: Continuous Integration (CI) is widely used to provide rapid feedback on code changes; however, CI build outcomes are not always reliable. Builds may fail intermittently due to non-deterministic factors, leading to flaky builds that undermine developers' trust in CI, waste computational resources, and threaten the validity of CI-related empirical studies. In this paper, we present a large-scale empirical study of flaky builds in GitHub Actions based on rerun data from 1,960 open-source Java projects. Our results show that 3.2% of builds are rerun, and 67.73% of these rerun builds exhibit flaky behavior, affecting 1,055 (51.28%) of the projects. Through an in-depth failure analysis, we identify 15 distinct categories of flaky failures, among which flaky tests, network issues, and dependency resolution issues are the most prevalent. Building on these findings, we propose a machine learning-based approach for detecting flaky failures at the job level. Compared with a state-of-the-art baseline, our approach improves the F1-score by up to 20.3%.

</details>


### [685] [A Task-Level Evaluation of AI Agents in Open-Source Projects](https://arxiv.org/abs/2602.02345)
*Shojibur Rahman,Md Fazle Rabbi,Minhaz Zibran*

Main category: cs.SE

TL;DR: 本文使用AIDev - pop数据集对五个自主编码代理进行比较研究，从三个维度评估，发现不同代理各有优劣，研究结果有助于选择和改进AI代理。


<details>
  <summary>Details</summary>
Motivation: 对自主编码代理进行比较研究，为其有效集成到协作软件工程提供参考。

Method: 使用AIDev - pop数据集，从PR接受率、审查讨论量和提交消息质量三个与任务相关的维度评估代理性能。

Result: Codex在多数任务类别中PR接受率高；Copilot的PR引发最多审查讨论；Claude和Cursor在多种任务类型中生成高质量提交消息比例高，Codex集成效果好但提交质量相对较低。

Conclusion: 研究结果有助于选择和改进AI代理以实现其在协作软件工程中的有效集成。

Abstract: In this paper, we present a comparative study of five autonomous coding agents using AIDev-pop, which is a public dataset containing thousands of AI-generated pull requests (PRs) across popular open-source repositories. We evaluate agents' performance along three task-aware dimensions spanning the PR lifecycle: (1) PR acceptance rate, (2) review discussion volume, and (3) commit message quality. Our quantitative analysis finds that Codex consistently achieves high PR acceptance rates across most task categories, while Copilot's PRs trigger the highest volume of both human and automated review discussions. In contrast, commit-level quality varies independently of acceptance outcomes. Claude and Cursor produce higher proportions of high-quality commit messages across several task types, and Codex exhibiting comparatively lower commit quality despite strong integration outcomes. Our findings inform selection and improvements of AI agents for their effective integration to collaborative software engineering.

</details>


### [686] [SWE-Universe: Scale Real-World Verifiable Environments to Millions](https://arxiv.org/abs/2602.02361)
*Mouxiang Chen,Lei Zhang,Yunlong Feng,Xuwu Wang,Wenting Zhao,Ruisheng Cao,Jiaxi Yang,Jiawei Chen,Mingze Li,Zeyao Ma,Hao Ge,Zongmeng Zhang,Zeyu Cui,Dayiheng Liu,Jingren Zhou,Jianling Sun,Junyang Lin,Binyuan Hui*

Main category: cs.SE

TL;DR: 提出SWE-Universe框架自动构建软件工程可验证环境，扩大规模并应用取得成果。


<details>
  <summary>Details</summary>
Motivation: 克服现有自动构建软件工程可验证环境的低产出、验证器弱和成本高等问题。

Method: 使用由高效自定义训练模型驱动的构建代理，采用迭代自我验证和循环内黑客检测。

Result: 将现实世界多语言软件工程环境规模扩大到百万级别，通过大规模智能体训练和强化学习证明环境价值，应用于Qwen3 - Max - Thinking在SWE - Bench Verified上得分75.3%。

Conclusion: 为推进下一代编码智能体提供重要资源和可靠方法。

Abstract: We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [687] [A Prior-Predictive Monte Carlo Framework for Pricing Complex Data Products in Data-Poor Markets](https://arxiv.org/abs/2602.00121)
*Adam L. Siemiatkowski,Victor Zhirnov,Kashyap Yellai,Gabriella Bein,Terresa Zimmerman*

Main category: q-fin.CP

TL;DR: 传统数据定价方法有局限，引入先验预测蒙特卡罗框架为数据产品定价，有稳定价格区间且可与传统定价结合。


<details>
  <summary>Details</summary>
Motivation: 公开交易数据稀疏、异质且保密，传统定价方法依赖经验法则或大量历史数据，难以对复杂领域高级数据产品定价。

Method: 引入先验预测蒙特卡罗框架，模拟多个定价场景和交易配置，通过约束截断先验确保商业现实性。

Result: 生成稳定的概率价格区间，而非单点估计，创建可审计、可重复的概率定价系统。

Conclusion: 该模型能将基于专业经验的传统数据定价与基于数据的方法相结合，且可随交易数据积累进行贝叶斯更新。

Abstract: Pricing advanced data products - particularly in complex fields such as semiconductor manufacturing - is a fundamentally challenging task due to the sparsity of publicly available transaction data, and its frequent heterogeneity and confidentiality. While data value depends on multiple interacting factors, such as technical sophistication, quality, utility, and licensing rights, traditional pricing methods tend to rely on ad-hoc heuristics or require massive amounts of historical transaction data. In an increasingly data-based economy, we introduce a prior-predictive Monte Carlo framework that enables the generation of fair, consistent, and justified price ranges for data products in the absence of empirical data. By simulating many plausible pricing 'worlds' and deal configurations, the framework produces stable probabilistic price bands (e.g., P5/P50/P95) rather than single point estimates, creating an auditable and repeatable probabilistic pricing system with business realism enforced via constraint-truncated priors. The proposed model bridges traditional data pricing rooted in professional experience with a data-based approach that also allows for classical Bayesian updating as more transaction data is accumulated.

</details>


### [688] [Numerical Simulations for Time-Fractional Black-Scholes Equations](https://arxiv.org/abs/2602.00201)
*Neetu Garg,A. S. V. Ravi Kanth*

Main category: q-fin.CP

TL;DR: 本文为欧式期权的时间分数阶布莱克 - 斯科尔斯模型实现高效数值算法，方法稳定且通过算例证明优越性。


<details>
  <summary>Details</summary>
Motivation: 为时间 - 分数阶布莱克 - 斯科尔斯模型找到高效数值算法。

Method: 采用 Crank - Nicolson 方法离散时间变量，指数 B - 样条逼近空间变量。

Result: 所实现方法无条件稳定，数值模拟对比显示该方法优越性。

Conclusion: 所提出的算法是高效且优越的。

Abstract: This paper implements an efficient numerical algorithm for the time-fractional Black-Scholes model governing European options. The proposed method comprises the Crank-Nicolson approach to discretize the time variable and exponential B-spline approximation for the space variable. The implemented method is unconditionally stable. We present few numerical examples to confirm the theory. Numerical simulations with comparisons exhibit the supremacy of the proposed approach.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [689] [Rough Martingale Optimal Transport: Theory, Implementation, and Regulatory Applications for Non-Modelable Risk Factors](https://arxiv.org/abs/2602.00097)
*Sri Sairam Gautam B.,Isha*

Main category: q-fin.RM

TL;DR: 提出统一的粗糙鞅最优运输（RMOT）框架解决FRTB下奇异衍生品定价问题，有有限显式外推边界，经实证验证可提供资本减免。


<details>
  <summary>Details</summary>
Motivation: 交易账簿的基础审查（FRTB）对奇异衍生品定价提出挑战，经典鞅最优运输（MOT）在非可建模风险因素（NMRF）下审计边界无穷。

Method: 提出RMOT框架，用粗糙波动率先验正则化运输计划，建立稀疏数据下粗糙波动率参数可识别定理，进行模型校准和实证验证。

Result: 模型校准显示最优鞅测度有拉伸指数尾衰减，RMOT提供约每10亿美元奇异账簿8.8亿美元资本减免，经交叉验证有保守覆盖。

Conclusion: RMOT是一个符合FRTB原则、可对NMRF进行显式误差量化的定价框架。

Abstract: The Fundamental Review of the Trading Book (FRTB) poses a significant challenge for exotic derivatives pricing, particularly for non-modelable risk factors (NMRF) where sparse market data leads to infinite audit bounds under classical Martingale Optimal Transport (MOT). We propose a unified Rough Martingale Optimal Transport (RMOT) framework that regularizes the transport plan with a rough volatility prior, yielding finite, explicit, and asymptotically tight extrapolation bounds. We establish an identifiability theorem for rough volatility parameters under sparse data, proving that 50 strikes are sufficient to estimate the Hurst exponent within $\pm 0.05$. For the multi-asset case, we prove that the correlation matrix is locally identifiable from marginal option surfaces provided the Hurst exponents are distinct. Model calibration on SPY and QQQ options (2019--2024) confirms that the optimal martingale measure exhibits stretched exponential tail decay ($\sim\exp(-k^{1-H})$), consistent with rough volatility asymptotics, whereas classical MOT yields trivial bounds. We validate the framework on live SPX/NDX data and scale it to $N = 30$ assets using a block-sparse optimization algorithm. Empirical results show that RMOT provides approximately \$880M in capital relief per \$1B exotic book compared to classical methods, while maintaining conservative coverage confirmed by 100-seed cross-validation. This constitutes a pricing framework designed to align with FRTB principles for NMRFs with explicit error quantification.

</details>


### [690] [Non-standard analysis for coherent risk estimation: hyperfinite representations, discrete Kusuoka formulae, and plug-in asymptotics](https://arxiv.org/abs/2602.00784)
*Tomasz Kania*

Main category: q-fin.RM

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We develop a non-standard analysis framework for coherent risk measures and their finite-sample analogues, coherent risk estimators, building on recent work of Aichele, Cialenco, Jelito, and Pitera. Coherent risk measures on $L^\infty$ are realised as standard parts of internal support functionals on Loeb probability spaces, and coherent risk estimators arise as finite-grid restrictions.
  Our main results are: (i) a hyperfinite robust representation theorem that yields, as finite shadows, the robust representation results for coherent risk estimators; (ii) a discrete Kusuoka representation for law-invariant coherent risk estimators as suprema of mixtures of discrete expected shortfalls on $\{k/n:k=1,\ldots,n\}$; (iii) uniform almost sure consistency (with an explicit rate) for canonical spectral plug-in estimators over Lipschitz spectral classes; (iv) a Kusuoka-type plug-in consistency theorem under tightness and uniform estimation assumptions; (v) bootstrap validity for spectral plug-in estimators via an NSA reformulation of the functional delta method (under standard smoothness assumptions on $F_X$); and (vi) asymptotic normality obtained through a hyperfinite central limit theorem.
  The hyperfinite viewpoint provides a transparent probability-to-statistics dictionary: applying a risk measure to a law corresponds to evaluating an internal functional on a hyperfinite empirical measure and taking the standard part. We include a standardd self-contained introduction to the required non-standard tools.

</details>


### [691] [A Methodology to Measure Impacts of Scenarios Through Expected Credit Losses](https://arxiv.org/abs/2602.01361)
*Mahmood Alaghmandan,Meghal Arora,Olga Streltchenko*

Main category: q-fin.RM

TL;DR: 本文提出衡量情景对风险敞口预期损失影响的方法，为相关标准化气候情景演练提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 测量情景对金融机构风险敞口预期损失的影响，利用现有的拨备基础设施。

Method: 利用金融机构现有拨备基础设施，通过违约概率变化捕捉情景影响；为标准化风险敞口分组设计风险驱动因素，分组基于风险敞口共同特征。

Result: 所提出的方法为加拿大金融机构监管办公室和魁北克金融市场管理局2024年进行的标准化气候情景演练提供理论基础。

Conclusion: 所提出的方法论具有一定实用性和有效性，能为相关金融情景演练提供理论支撑。

Abstract: In this paper, we present a methodology for measuring the impact of scenarios on the expected losses of exposures by leveraging the existing provisioning infrastructure within financial institutions, where scenario effects are captured through changes in probabilities of default. We then describe how to design and implement a scenario test where risk drivers are given for standardized groupings of exposures, and the groupings are defined based on common features of the exposures. The methodology presented served as a theoretical foundation for the standardized climate scenario exercise conducted in 2024 by the Office of the Superintendent of Financial Institutions of Canada and Quebec's Autorite des Marches Financiers.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [692] [Generative AI for Stock Selection](https://arxiv.org/abs/2602.00196)
*Keywan Christian Rasekhschaffe*

Main category: q-fin.ST

TL;DR: 研究生成式AI能否自动发现美股特征，用特定方法合成特征预测短期回报，AI生成特征有竞争力，检索质量关键，可增强特征发现。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI是否能实现美股特征的自动发现。

Method: 使用带检索增强生成的大语言模型和结构化/程序化提示，从分析师、期权和量价数据合成经济动机特征，再用表格机器学习模型预测短期回报。

Result: 在多个数据集上，AI生成特征与基线相比有竞争力，夏普比率提升14% - 91%，检索质量对结果影响大，AI生成信号与传统特征弱相关。

Conclusion: 当控制检索质量时，生成式AI可有效增强特征发现，产生可解释信号并减少人工工程工作量。

Abstract: We study whether generative AI can automate feature discovery in U.S. equities. Using large language models with retrieval-augmented generation and structured/programmatic prompting, we synthesize economically motivated features from analyst, options, and price-volume data. These features are then used as inputs to a tabular machine-learning model to forecast short-horizon returns. Across multiple datasets, AI-generated features are consistently competitive with baselines, with Sharpe improvements ranging from 14% to 91% depending on dataset and configuration. Retrieval quality is pivotal: better knowledge bases materially improve outcomes. The AI-generated signals are weakly correlated with traditional features, supporting combination. Overall, generative AI can meaningfully augment feature discovery when retrieval quality is controlled, producing interpretable signals while reducing manual engineering effort.

</details>


### [693] [Bitcoin Price Prediction using Machine Learning and Combinatorial Fusion Analysis](https://arxiv.org/abs/2602.00037)
*Yuanhong Wu,Wei Ye,Jingyan Xu,D. Frank Hsu*

Main category: q-fin.ST

TL;DR: 本文提出将组合融合分析（CFA）范式应用于比特币价格预测，利用多种组合技术，评估显示有良好表现，优于单个模型和其他预测模型。


<details>
  <summary>Details</summary>
Motivation: 金融产品价格预测可带来显著利润，且单一机器学习模型有优缺点，难以实现鲁棒性，需新方法。

Method: 应用CFA范式，利用得分和排名组合及加权组合技术，用RMSE和MAPE等指标评估。

Result: 所提方法MAPE表现为0.19%，显著提升了单个模型性能，且优于其他比特币价格预测模型。

Conclusion: 所提方法在比特币价格预测中有效，能提升预测性能。

Abstract: In this work, we propose to apply a new model fusion and learning paradigm, known as Combinatorial Fusion Analysis (CFA), to the field of Bitcoin price prediction. Price prediction of financial product has always been a big topic in finance, as the successful prediction of the price can yield significant profit. Every machine learning model has its own strength and weakness, which hinders progress toward robustness. CFA has been used to enhance models by leveraging rank-score characteristic (RSC) function and cognitive diversity in the combination of a moderate set of diverse and relatively well-performed models. Our method utilizes both score and rank combinations as well as other weighted combination techniques. Key metrics such as RMSE and MAPE are used to evaluate our methodology performance. Our proposal presents a notable MAPE performance of 0.19\%. The proposed method greatly improves upon individual model performance, as well as outperforms other Bitcoin price prediction models.

</details>


### [694] [Exploring the Interpretability of Forecasting Models for Energy Balancing Market](https://arxiv.org/abs/2602.00049)
*Oskar Våle,Shiliang Zhang,Sabita Maharjan,Gro Klæboe*

Main category: q-fin.ST

TL;DR: 本文探讨能源平衡市场模型准确性与可解释性的权衡，以mFRR激活价格预测为例，对比XGBoost、EBM等模型，发现EBM在保证准确性的同时有高可解释性。


<details>
  <summary>Details</summary>
Motivation: 复杂机器学习模型在能源平衡市场建模中黑箱特性限制可解释性，需探索准确性与可解释性的权衡。

Method: 以不同能源价格区的真实市场数据预测mFRR激活价格，用XGBoost和EBM模型，并将二者集成，与基线朴素模型对比。

Result: EBM与XGBoost预测准确性相当，有较高可解释性；准确预测mFRR价格在激活价格与现货价格偏差大时存在挑战；EBM可揭示mFRR价格驱动因素和区域市场动态。

Conclusion: EBM是能源平衡市场预测中复杂黑箱AI模型的可行且有价值的可解释替代方案。

Abstract: The balancing market in the energy sector plays a critical role in physically and financially balancing the supply and demand. Modeling dynamics in the balancing market can provide valuable insights and prognosis for power grid stability and secure energy supply. While complex machine learning models can achieve high accuracy, their black-box nature severely limits the model interpretability. In this paper, we explore the trade-off between model accuracy and interpretability for the energy balancing market. Particularly, we take the example of forecasting manual frequency restoration reserve (mFRR) activation price in the balancing market using real market data from different energy price zones. We explore the interpretability of mFRR forecasting using two models: extreme gradient boosting (XGBoost) machine and explainable boosting machine (EBM). We also integrate the two models, and we benchmark all the models against a baseline naive model. Our results show that EBM provides forecasting accuracy comparable to XGBoost while yielding a considerable level of interpretability. Our analysis also underscores the challenge of accurately predicting the mFRR price for the instances when the activation price deviates significantly from the spot price. Importantly, EBM's interpretability features reveal insights into non-linear mFRR price drivers and regional market dynamics. Our study demonstrates that EBM is a viable and valuable interpretable alternative to complex black-box AI models in the forecast for the balancing market.

</details>


### [695] [Design and Empirical Study of a Large Language Model-Based Multi-Agent Investment System for Chinese Public REITs](https://arxiv.org/abs/2602.00082)
*Zheng Li*

Main category: q-fin.ST

TL;DR: 研究针对低波动中国公募REITs市场，提出基于多智能体协作的LLM驱动交易框架，对比两种预测模型路径，回测显示策略表现优于基准，微调小模型在部分场景表现佳。


<details>
  <summary>Details</summary>
Motivation: 应对低波动中国公募REITs市场，提高交易的风险调整收益。

Method: 构建四种分析智能体从不同维度分析，预测智能体整合信号输出概率分布，决策智能体生成仓位调整信号；对比直接调用通用大模型DeepSeek - R1和使用微调的专用小模型Qwen3 - 8B两种预测模型路径。

Result: 在2024年10月至2025年10月回测中，两种基于智能体的策略在累计回报、夏普比率和最大回撤方面显著优于买入持有基准。

Conclusion: 多智能体框架可有效提升REITs交易的风险调整收益，微调小模型在部分场景表现接近或优于通用大模型。

Abstract: This study addresses the low-volatility Chinese Public Real Estate Investment Trusts (REITs) market, proposing a large language model (LLM)-driven trading framework based on multi-agent collaboration. The system constructs four types of analytical agents-announcement, event, price momentum, and market-each conducting analysis from different dimensions; then the prediction agent integrates these multi-source signals to output directional probability distributions across multiple time horizons, then the decision agent generates discrete position adjustment signals based on the prediction results and risk control constraints, thereby forming a closed loop of analysis-prediction-decision-execution. This study further compares two prediction model pathways: for the prediction agent, directly calling the general-purpose large model DeepSeek-R1 versus using a specialized small model Qwen3-8B fine-tuned via supervised fine-tuning and reinforcement learning alignment. In the backtest from October 2024 to October 2025, both agent-based strategies significantly outperformed the buy-and-hold benchmark in terms of cumulative return, Sharpe ratio, and maximum drawdown. The results indicate that the multi-agent framework can effectively enhance the risk-adjusted return of REITs trading, and the fine-tuned small model performs close to or even better than the general-purpose large model in some scenarios.

</details>


### [696] [Test-Time Adaptation for Non-stationary Time Series: From Synthetic Regime Shifts to Financial Markets](https://arxiv.org/abs/2602.00073)
*Yurui Wu,Qingying Deng,Wonou Chung,Mairui Li*

Main category: q-fin.ST

TL;DR: 研究了一个用于因果时间序列预测和方向分类的轻量级测试时自适应（TTA）框架，评估了该框架在不同场景下的表现并给出实用建议。


<details>
  <summary>Details</summary>
Motivation: 实际中的时间序列很少平稳，数据分布变化时，基于历史观测训练的预测模型会失去准确性。

Method: 冻结骨干网络，仅使用最近的无标签窗口更新归一化仿射参数；分类时最小化熵并保证时间一致性，回归时最小化弱时间保持增强下的预测方差，可选地从EMA教师模型蒸馏；使用二次漂移惩罚和不确定性触发回退保持更新稳定。

Result: 在合成渐进漂移场景中，基于归一化的TTA改善了预测误差；在金融市场中，简单的批量归一化统计更新是稳健的默认方法，更激进的仅归一化自适应甚至可能有害。

Conclusion: 研究结果为在非平稳时间序列上部署TTA提供了实用指导。

Abstract: Time series encountered in practice are rarely stationary. When the data distribution changes, a forecasting model trained on past observations can lose accuracy. We study a small-footprint test-time adaptation (TTA) framework for causal timeseries forecasting and direction classification. The backbone is frozen, and only normalization affine parameters are updated using recent unlabeled windows. For classification we minimize entropy and enforce temporal consistency; for regression we minimize prediction variance across weak time-preserving augmentations and optionally distill from an EMA teacher. A quadratic drift penalty and an uncertainty triggered fallback keep updates stable. We evaluate this framework in two stages: synthetic regime shifts on ETT benchmarks, and daily equity and FX series (SPY, QQQ, EUR/USD) across pandemic, high-inflation, and recovery regimes. On synthetic gradual drift, normalization-based TTA improves forecasting error, while in financial markets a simple batch-normalization statistics update is a robust default and more aggressive norm-only adaptation can even hurt. Our results provide practical guidance for deploying TTA on non-stationary time series.

</details>


### [697] [The GT-Score: A Robust Objective Function for Reducing Overfitting in Data-Driven Trading Strategies](https://arxiv.org/abs/2602.00080)
*Alexander Sheppert*

Main category: q-fin.ST

TL;DR: 本文提出GT - Score复合目标函数应对金融建模过拟合问题，通过实验表明其能提升回测可靠性，还提供了代码和结果文件。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动金融建模中的过拟合问题，避免机器学习系统学习到虚假规律。

Method: 提出GT - Score目标函数，利用2010 - 2024年50家标普500公司历史股票数据进行实证评估，包括带九次时间分割的向前滚动验证和15个随机种子的蒙特卡罗研究。

Result: 在向前滚动验证中，GT - Score使泛化比率相对基线目标函数提高98%；蒙特卡罗样本外收益的配对统计检验表明目标函数间有显著差异，效应量较小。

Conclusion: 在目标函数中嵌入抗过拟合结构可提高量化研究中回测的可靠性。

Abstract: Overfitting remains a critical challenge in data-driven financial modeling, where machine learning (ML) systems learn spurious patterns in historical prices and fail out of sample and in deployment. This paper introduces the GT-Score, a composite objective function that integrates performance, statistical significance, consistency, and downside risk to guide optimization toward more robust trading strategies. This approach directly addresses critical pitfalls in quantitative strategy development, specifically data snooping during optimization and the unreliability of statistical inference under non-normal return distributions. Using historical stock data for 50 S&P 500 companies spanning 2010-2024, we conduct an empirical evaluation that includes walk-forward validation with nine sequential time splits and a Monte Carlo study with 15 random seeds across three trading strategies. In walk-forward validation, GT-Score improves the generalization ratio (validation return divided by training return) by 98% relative to baseline objective functions. Paired statistical tests on Monte Carlo out-of-sample returns indicate statistically detectable differences between objective functions (p < 0.01 for comparisons with Sortino and Simple), with small effect sizes. These results suggest that embedding an anti-overfitting structure into the objective can improve the reliability of backtests in quantitative research. Reproducible code and processed result files are provided as supplementary materials.

</details>


### [698] [Impact of LLMs news Sentiment Analysis on Stock Price Movement Prediction](https://arxiv.org/abs/2602.00086)
*Walid Siala,Ahmed Khanfir,Mike Papadakis*

Main category: q-fin.ST

TL;DR: 本文利用基于大语言模型的新闻情感分析进行股价走势预测，对比三种大语言模型，发现DeBERTa表现最佳，集成模型可提高准确率，新闻情感特征对部分股市预测模型有一定益处。


<details>
  <summary>Details</summary>
Motivation: 以往研究多分别进行情感分析模型和股票走势预测方法的研究，缺乏对新闻情感在该任务中的益处的深入理解和不同架构类型的综合评估。

Method: 对比DeBERTa、RoBERTa和FinBERT三种大语言模型进行情感驱动的股票预测。

Result: DeBERTa准确率达75%，优于其他两个模型；集成三种模型的集成模型准确率可提高到约80%；情感新闻特征对部分股市预测模型有一定益处。

Conclusion: DeBERTa在股价走势预测中表现较好，集成模型能进一步提高准确率，新闻情感特征对部分股市预测任务有帮助。

Abstract: This paper addresses stock price movement prediction by leveraging LLM-based news sentiment analysis. Earlier works have largely focused on proposing and assessing sentiment analysis models and stock movement prediction methods, however, separately. Although promising results have been achieved, a clear and in-depth understanding of the benefit of the news sentiment to this task, as well as a comprehensive assessment of different architecture types in this context, is still lacking. Herein, we conduct an evaluation study that compares 3 different LLMs, namely, DeBERTa, RoBERTa and FinBERT, for sentiment-driven stock prediction. Our results suggest that DeBERTa outperforms the other two models with an accuracy of 75% and that an ensemble model that combines the three models can increase the accuracy to about 80%. Also, we see that sentiment news features can benefit (slightly) some stock market prediction models, i.e., LSTM-, PatchTST- and tPatchGNN-based classifiers and PatchTST- and TimesNet-based regression tasks models.

</details>


### [699] [PredictionMarketBench: A SWE-bench-Style Framework for Backtesting Trading Agents on Prediction Markets](https://arxiv.org/abs/2602.00133)
*Avi Arora,Ritesh Malpani*

Main category: q-fin.ST

TL;DR: 介绍用于评估预测市场交易代理的PredictionMarketBench基准，发布相关剧集，基线结果显示不同交易代理表现不同。


<details>
  <summary>Details</summary>
Motivation: 预测市场为交易代理提供了自然测试平台，但缺乏评估交易代理的标准基准，因此引入PredictionMarketBench。

Method: 通过对历史限价订单簿和交易数据进行确定性、事件驱动的回放来评估算法和基于大语言模型的交易代理，标准化了剧集构建、执行模拟和工具代理接口。

Result: 发布了四个基于Kalshi的剧集，基线结果表明，天真的交易代理因交易成本和结算损失表现不佳，而考虑费用的算法策略在波动剧集中仍具竞争力。

Conclusion: PredictionMarketBench为评估预测市场的交易代理提供了有效工具，不同策略的交易代理表现受交易成本和市场波动性影响。

Abstract: Prediction markets offer a natural testbed for trading agents: contracts have binary payoffs, prices can be interpreted as probabilities, and realized performance depends critically on market microstructure, fees, and settlement risk. We introduce PredictionMarketBench, a SWE-bench-style benchmark for evaluating algorithmic and LLM-based trading agents on prediction markets via deterministic, event-driven replay of historical limit-order-book and trade data. PredictionMarketBench standardizes (i) episode construction from raw exchange streams (orderbooks, trades, lifecycle, settlement), (ii) an execution-realistic simulator with maker/taker semantics and fee modeling, and (iii) a tool-based agent interface that supports both classical strategies and tool-calling LLM agents with reproducible trajectories. We release four Kalshi-based episodes spanning cryptocurrency, weather, and sports. Baseline results show that naive trading agents can underperform due to transaction costs and settlement losses, while fee-aware algorithmic strategies remain competitive in volatile episodes.

</details>


### [700] [Null-Validated Topological Signatures of Financial Market Dynamics](https://arxiv.org/abs/2602.00383)
*Samuel W. Akingbade*

Main category: q-fin.ST

TL;DR: 研究用拓扑方法量化比特币市场复杂性，发现持久性景观范数能提供市场动态补充信息。


<details>
  <summary>Details</summary>
Motivation: 现有波动性指标和线性相关结构不能完全捕捉金融市场的时间组织特征，需新方法量化市场复杂性。

Method: 使用滑动窗口延迟嵌入计算持久性景观的L1范数，进行滚动相关性分析，并采用基于替代数据的零模型进行统计验证。

Result: 该指标在市场压力时期与随机波动率强共变，低波动时期也间歇性升高；几何与波动率的依赖关系非平稳；拒绝洗牌替代数据排除仅基于边际分布的解释，偏离相位随机化替代数据表明对非线性和相位依赖的时间组织敏感。

Conclusion: 持久性景观范数能在不同市场条件下提供关于市场动态的补充信息。

Abstract: Financial markets exhibit temporal organization that is not fully captured by volatility measures or linear correlation structure. We study a null validated topological approach for quantifying market complexity and apply it to Bitcoin daily log returns. The analysis uses the $L^1$ norm of persistence landscapes computed from sliding-window delay embeddings. This quantity shows strong co-movement with stochastic volatility during periods of market stress, but remains intermittently elevated during low volatility regimes, indicating dynamical structure beyond fluctuation scale. Rolling correlation analysis reveals that the dependence between geometry and volatility is not stationary. Surrogate based null models provide statistical validation of these observations. Rejection of shuffle surrogates rules out explanations based on marginal distributions alone, while departures from phase randomized surrogates indicate sensitivity to nonlinear and phase dependent temporal organization beyond linear correlations. These results demonstrate that persistence landscape norms provide complementary information about market dynamics across market conditions.

</details>


### [701] [The Impact of Trump-Era Tariffs on Financial Market Efficiency](https://arxiv.org/abs/2602.00548)
*Tetsuya Takaishi*

Main category: q-fin.ST

TL;DR: 研究运用多重分形去趋势波动分析方法，研究特朗普时代关税和新冠疫情对六种金融资产市场效率的影响，发现新冠影响大，关税影响较温和，上证综指受影响小，凸显多重分形分析在捕捉市场效率结构变化的作用。


<details>
  <summary>Details</summary>
Motivation: 研究特朗普时代关税对金融市场效率的影响。

Method: 对六种主要金融资产的回报和绝对回报时间序列应用多重分形去趋势波动分析，用赫斯特指数和多重分形强度评估市场动态。

Result: 新冠使赫斯特指数和多重分形强度变化大，关税影响较温和但可观察到，上证综指受两事件影响小，VIX呈现反持久行为。

Conclusion: 多重分形分析可有效捕捉地缘政治和系统性冲击下市场效率的结构变化。

Abstract: This study examines the effects of Trump-era tariffs on financial market efficiency by applying multifractal detrended fluctuation analysis to the return and absolute return time series of six major financial assets: the S\&P 500, SSEC, VIX, BTC/USD, EUR/USD, and Gold. Using the Hurst exponent $h(2)$ and multifractal strength, we assess how market dynamics responded to two major global shocks: the COVID-19 pandemic and the implementation of the Trump tariff policy in 2025. The results show that COVID-19 induced substantial changes in both the Hurst exponent and multifractal strength, particularly for the S\&P 500, BTC/USD, EUR/USD, and Gold. In contrast, the effects of the Trump tariffs were more moderate but still observable across all examined time series. The Chinese market index (SSEC) remained largely unaffected by either event, apart from a distinct response to domestic stimulus measures. In addition, the VIX exhibited anti-persistent behavior with $h(2) < 0.5$, consistent with the rough volatility framework. These findings underscore the usefulness of multifractal analysis in capturing structural shifts in market efficiency under geopolitical and systemic shocks.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [702] [Explainable Patterns in Cryptocurrency Microstructure](https://arxiv.org/abs/2602.00776)
*Bartosz Bieganowski,Robert Ślepaczuk*

Main category: q-fin.TR

TL;DR: 研究发现加密货币限价订单簿微观结构存在稳定的跨资产模式，统一模型显示特征排名和部分效应稳定，关联微观结构理论并验证可交易性，通过闪崩分析验证经典理论，建议构建通用特征库。


<details>
  <summary>Details</summary>
Motivation: 探索加密货币限价订单簿微观结构特征在不同资产间的稳定性，为市场交易策略和系统性风险评估提供依据。

Method: 使用统一的CatBoost建模管道，结合方向感知GMADL目标和时间序列交叉验证，还进行了保守的Top - of - book taker回测和固定深度maker回测。

Result: 相同的订单簿和交易特征在不同市值资产中的预测重要性和SHAP依赖形状相似，特征排名和部分效应稳定，不同策略在闪崩中的不同表现验证了经典微观结构理论。

Conclusion: 加密货币存在可移植的短期回报微观结构表示，可构建加密市场通用特征库。

Abstract: We document stable cross-asset patterns in cryptocurrency limit-order-book microstructure: the same engineered order book and trade features exhibit remarkably similar predictive importance and SHAP dependence shapes across assets spanning an order of magnitude in market capitalization (BTC, LTC, ETC, ENJ, ROSE). The data covers Binance Futures perpetual contract order books and trades on 1-second frequency starting from January 1st, 2022 up to October 12th, 2025. Using a unified CatBoost modeling pipeline with a direction-aware GMADL objective and time-series cross validation, we show that feature rankings and partial effects are stable across assets despite heterogeneous liquidity and volatility. We connect these SHAP structures to microstructure theory (order flow imbalance, spread, and adverse selection) and validate tradability via a conservative top-of-book taker backtest as well as fixed depth maker backtest. Our primary novelty is a robustness analysis of a major flash crash, where the divergent performance of our taker and maker strategies empirically validates classic microstructure theories of adverse selection and highlights the systemic risks of algorithmic trading. Our results suggest a portable microstructure representation of short-horizon returns and motivate universal feature libraries for crypto markets.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [703] [Uncertainty-Aware Multimodal Learning via Conformal Shapley Intervals](https://arxiv.org/abs/2602.00171)
*Mathew Chandy,Michael Johnson,Judong Shen,Devan V. Mehrotra,Hua Zhou,Jin Zhou,Xiaowu Dai*

Main category: stat.ML

TL;DR: 提出共形Shapley区间框架用于多模态学习中量化模态重要性与不确定性，还提出模态选择程序，并在多数据集证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中各模态贡献不均且依赖数据，需量化模态重要性和不确定性以实现可解释和可靠的多模态学习。

Method: 引入共形Shapley区间框架，结合Shapley值与共形推理构建各模态重要性区间；基于这些区间提出有最优性保证的模态选择程序。

Result: 在多个数据集上证明该方法能提供有意义的不确定性量化，有强预测性能且仅依赖少量信息模态。

Conclusion: 所提出的方法在多模态学习中是有效的，可用于量化模态重要性和不确定性及进行模态选择。

Abstract: Multimodal learning combines information from multiple data modalities to improve predictive performance. However, modalities often contribute unequally and in a data dependent way, making it unclear which data modalities are genuinely informative and to what extent their contributions can be trusted. Quantifying modality level importance together with uncertainty is therefore central to interpretable and reliable multimodal learning. We introduce conformal Shapley intervals, a framework that combines Shapley values with conformal inference to construct uncertainty-aware importance intervals for each modality. Building on these intervals, we propose a modality selection procedure with a provable optimality guarantee: conditional on the observed features, the selected subset of modalities achieves performance close to that of the optimal subset. We demonstrate the effectiveness of our approach on multiple datasets, showing that it provides meaningful uncertainty quantification and strong predictive performance while relying on only a small number of informative modalities.

</details>


### [704] [Neuron Block Dynamics for XOR Classification with Zero-Margin](https://arxiv.org/abs/2602.00172)
*Guillaume Braun,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 本文研究零边际非线性分类的高斯XOR问题，扩展训练动态研究，提出神经元块动态框架，分析泛化性，数值实验验证预测。


<details>
  <summary>Details</summary>
Motivation: 多数理论分析聚焦回归或正边际分类任务，本文研究零边际非线性分类的高斯XOR问题，此问题打破标准基于边际的论证。

Method: 基于Glasgow（2024）的分析，将训练动态研究从离散输入扩展到高斯输入，开发神经元块动态框架，采用平均情况视角分析泛化。

Result: 神经元聚类成四个方向，块级信号连贯演化；数值实验证实预测的两阶段块动态，且在高斯设置之外具有鲁棒性。

Conclusion: 可在不依赖边际假设情况下分析泛化，块视角有助于理解高斯XOR问题的训练和泛化动态。

Abstract: The ability of neural networks to learn useful features through stochastic gradient descent (SGD) is a cornerstone of their success. Most theoretical analyses focus on regression or on classification tasks with a positive margin, where worst-case gradient bounds suffice. In contrast, we study zero-margin nonlinear classification by analyzing the Gaussian XOR problem, where inputs are Gaussian and the XOR decision boundary determines labels. In this setting, a non-negligible fraction of data lies arbitrarily close to the boundary, breaking standard margin-based arguments. Building on Glasgow's (2024) analysis, we extend the study of training dynamics from discrete to Gaussian inputs and develop a framework for the dynamics of neuron blocks. We show that neurons cluster into four directions and that block-level signals evolve coherently, a phenomenon essential in the Gaussian setting where individual neuron signals vary significantly. Leveraging this block perspective, we analyze generalization without relying on margin assumptions, adopting an average-case view that distinguishes regions of reliable prediction from regions of persistent error. Numerical experiments confirm the predicted two-phase block dynamics and demonstrate their robustness beyond the Gaussian setting.

</details>


### [705] [Singular Bayesian Neural Networks](https://arxiv.org/abs/2602.00387)
*Mame Diarra Toure,David A. Stephens*

Main category: stat.ML

TL;DR: 提出用低秩参数化改进贝叶斯神经网络，减少参数并提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准平均场高斯后验的贝叶斯神经网络参数成本高，在权重矩阵奇异值快速衰减时该成本不必要。

Method: 将权重参数化为 $W = AB^{	op}$ 诱导后验，推导 PAC - Bayes 泛化界和损失界，适配高斯复杂度界。

Result: 在标准基准测试中，方法用少至 1/15 参数达到与 5 成员深度集成相当的预测性能，改善 OOD 检测和校准。

Conclusion: 低秩参数化方法有效减少贝叶斯神经网络参数，提升性能。

Abstract: Bayesian neural networks promise calibrated uncertainty but require $O(mn)$ parameters for standard mean-field Gaussian posteriors. We argue this cost is often unnecessary, particularly when weight matrices exhibit fast singular value decay. By parameterizing weights as $W = AB^{\top}$ with $A \in \mathbb{R}^{m \times r}$, $B \in \mathbb{R}^{n \times r}$, we induce a posterior that is singular with respect to the Lebesgue measure, concentrating on the rank-$r$ manifold. This singularity captures structured weight correlations through shared latent factors, geometrically distinct from mean-field's independence assumption. We derive PAC-Bayes generalization bounds whose complexity term scales as $\sqrt{r(m+n)}$ instead of $\sqrt{m n}$, and prove loss bounds that decompose the error into optimization and rank-induced bias using the Eckart-Young-Mirsky theorem. We further adapt recent Gaussian complexity bounds for low-rank deterministic networks to Bayesian predictive means. Empirically, across MLPs, LSTMs, and Transformers on standard benchmarks, our method achieves predictive performance competitive with 5-member Deep Ensembles while using up to $15\times$ fewer parameters. Furthermore, it substantially improves OOD detection and often improves calibration relative to mean-field and perturbation baselines.

</details>


### [706] [Reinforcement Learning for Control Systems with Time Delays: A Comprehensive Survey](https://arxiv.org/abs/2602.00399)
*Armando Alves Neto*

Main category: stat.ML

TL;DR: 本文对解决控制系统中时间延迟问题的强化学习方法进行全面综述，分类介绍现有方法，分析权衡并给出选择指南，还指出开放挑战与研究方向。


<details>
  <summary>Details</summary>
Motivation: 多数强化学习算法依赖马尔可夫决策过程假设，然而实际网络物理系统存在时间延迟会破坏该假设，影响性能和稳定性。

Method: 先形式化主要延迟类型并分析其对马尔可夫属性的影响，然后将现有方法分为五大类，讨论各类原理、优缺点，进行对比分析。

Result: 明确了各类方法的关键权衡，给出不同延迟特性和安全要求下选择合适方法的实用指南。

Conclusion: 确定了稳定性认证、大延迟学习等开放挑战和研究方向，可作为构建可靠基于强化学习控制器的统一参考。

Abstract: In the last decade, Reinforcement Learning (RL) has achieved remarkable success in the control and decision-making of complex dynamical systems. However, most RL algorithms rely on the Markov Decision Process assumption, which is violated in practical cyber-physical systems affected by sensing delays, actuation latencies, and communication constraints. Such time delays introduce memory effects that can significantly degrade performance and compromise stability, particularly in networked and multi-agent environments. This paper presents a comprehensive survey of RL methods designed to address time delays in control systems. We first formalize the main classes of delays and analyze their impact on the Markov property. We then systematically categorize existing approaches into five major families: state augmentation and history-based representations, recurrent policies with learned memory, predictor-based and model-aware methods, robust and domain-randomized training strategies, and safe RL frameworks with explicit constraint handling. For each family, we discuss underlying principles, practical advantages, and inherent limitations. A comparative analysis highlights key trade-offs among these approaches and provides practical guidelines for selecting suitable methods under different delay characteristics and safety requirements. Finally, we identify open challenges and promising research directions, including stability certification, large-delay learning, multi-agent communication co-design, and standardized benchmarking. This survey aims to serve as a unified reference for researchers and practitioners developing reliable RL-based controllers in delay-affected cyber-physical systems.

</details>


### [707] [Reliable Real-Time Value at Risk Estimation via Quantile Regression Forest with Conformal Calibration](https://arxiv.org/abs/2602.01912)
*Du-Yi Wang,Guo Liang,Kun Zhang,Qianwen Zhu*

Main category: stat.ML

TL;DR: 研究在线估计风险价值（VaR），提出在OSOA框架下用分位数回归森林，开发保形估计器校准在线VaR估计，理论和实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 快速变化的市场条件需要实时风险监控，但在线估计有挑战，准确可靠估计VaR对及时风险控制和决策至关重要。

Method: 在OSOA框架下使用分位数回归森林，离线训练学习在线VaR与风险因素关系，在线结合观测风险因素生成实时VaR估计，开发保形估计器校准。

Result: 理论分析证明了估计器的一致性和覆盖有效性，数值实验证实了方法并展示其实际有效性。

Conclusion: 提出的方法能可靠地估计实时VaR，利用保形校准在OSOA框架下估计实时VaR是首次尝试。

Abstract: Rapidly evolving market conditions call for real-time risk monitoring, but its online estimation remains challenging. In this paper, we study the online estimation of one of the most widely used risk measures, Value at Risk (VaR). Its accurate and reliable estimation is essential for timely risk control and informed decision-making. We propose to use the quantile regression forest in the offline-simulation-online-estimation (OSOA) framework. Specifically, the quantile regression forest is trained offline to learn the relationship between the online VaR and risk factors, and real-time VaR estimates are then produced online by incorporating observed risk factors. To further ensure reliability, we develop a conformalized estimator that calibrates the online VaR estimates. To the best of our knowledge, we are the first to leverage conformal calibration to estimate real-time VaR reliably based on the OSOA formulation. Theoretical analysis establishes the consistency and coverage validity of the proposed estimators. Numerical experiments confirm the proposed method and demonstrate its effectiveness in practice.

</details>


### [708] [Sampling from multi-modal distributions on Riemannian manifolds with training-free stochastic interpolants](https://arxiv.org/abs/2602.00641)
*Alain Durmus,Maxence Noble,Thibaut Pellerin*

Main category: stat.ML

TL;DR: 本文提出在黎曼流形上从不归一化密度采样的通用方法，聚焦多模态目标，方法无需训练，有理论分析并在多问题上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有采样方法在处理多模态目标时存在挑战，需要新的采样方法。

Method: 受扩散模型启发，引入基于非平衡确定性动力学模拟的采样算法，使用标准蒙特卡罗技术进行迭代后验采样，无需训练。

Result: 对方法进行了严格理论分析，并在一系列多模态采样问题上证明了其有效性。

Conclusion: 该方法将基于扩散的采样方法扩展到了欧几里得空间之外，是一种有效的采样方法。

Abstract: In this paper, we propose a general methodology for sampling from un-normalized densities defined on Riemannian manifolds, with a particular focus on multi-modal targets that remain challenging for existing sampling methods. Inspired by the framework of diffusion models developed for generative modeling, we introduce a sampling algorithm based on the simulation of a non-equilibrium deterministic dynamics that transports an easy-to-sample noise distribution toward the target. At the marginal level, the induced density path follows a prescribed stochastic interpolant between the noise and target distributions, specifically constructed to respect the underlying Riemannian geometry. In contrast to related generative modeling approaches that rely on machine learning, our method is entirely training-free. It instead builds on iterative posterior sampling procedures using only standard Monte Carlo techniques, thereby extending recent diffusion-based sampling methodologies beyond the Euclidean setting. We complement our approach with a rigorous theoretical analysis and demonstrate its effectiveness on a range of multi-modal sampling problems, including high-dimensional and heavy-tailed examples.

</details>


### [709] [Alignment of Diffusion Model and Flow Matching for Text-to-Image Generation](https://arxiv.org/abs/2602.00413)
*Yidong Ouyang,Liyan Xie,Hongyuan Zha,Guang Cheng*

Main category: stat.ML

TL;DR: 提出一种新的对齐框架，可用于扩散模型和流匹配模型，在扩散模型对齐上提出免微调框架降低计算成本，在流匹配模型对齐上提出免训练框架提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法需大量计算资源且跨目标泛化能力不佳，需改进。

Method: 利用对齐问题本质，将奖励加权分布的得分（速度）函数分解为预训练得分（速度场）加奖励条件期望；在扩散模型上训练引导网络估计奖励条件期望；在流匹配模型上提出免训练框架。

Result: 在扩散模型上一步生成达到与基于微调模型相当性能，计算成本至少降低60%；在流匹配模型上不增加计算成本提升生成质量。

Conclusion: 所提框架有效，降低计算成本并提升生成质量。

Abstract: Diffusion models and flow matching have demonstrated remarkable success in text-to-image generation. While many existing alignment methods primarily focus on fine-tuning pre-trained generative models to maximize a given reward function, these approaches require extensive computational resources and may not generalize well across different objectives. In this work, we propose a novel alignment framework by leveraging the underlying nature of the alignment problem -- sampling from reward-weighted distributions -- and show that it applies to both diffusion models (via score guidance) and flow matching models (via velocity guidance). The score function (velocity field) required for the reward-weighted distribution can be decomposed into the pre-trained score (velocity field) plus a conditional expectation of the reward. For the alignment on the diffusion model, we identify a fundamental challenge: the adversarial nature of the guidance term can introduce undesirable artifacts in the generated images. Therefore, we propose a finetuning-free framework that trains a guidance network to estimate the conditional expectation of the reward. We achieve comparable performance to finetuning-based models with one-step generation with at least a 60% reduction in computational cost. For the alignment on flow matching, we propose a training-free framework that improves the generation quality without additional computational cost.

</details>


### [710] [Shuffle and Joint Differential Privacy for Generalized Linear Contextual Bandits](https://arxiv.org/abs/2602.00417)
*Sahasrajit Sarmasarkar*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the first algorithms for generalized linear contextual bandits under shuffle differential privacy and joint differential privacy. While prior work on private contextual bandits has been restricted to linear reward models -- which admit closed-form estimators -- generalized linear models (GLMs) pose fundamental new challenges: no closed-form estimator exists, requiring private convex optimization; privacy must be tracked across multiple evolving design matrices; and optimization error must be explicitly incorporated into regret analysis.
  We address these challenges under two privacy models and context settings. For stochastic contexts, we design a shuffle-DP algorithm achieving $\tilde{O}(d^{3/2}\sqrt{T}/\sqrt{\varepsilon})$ regret. For adversarial contexts, we provide a joint-DP algorithm with $\tilde{O}(d\sqrt{T}/\sqrt{\varepsilon})$ regret -- matching the non-private rate up to a $1/\sqrt{\varepsilon}$ factor. Both algorithms remove dependence on the instance-specific parameter $κ$ (which can be exponential in dimension) from the dominant $\sqrt{T}$ term. Unlike prior work on locally private GLM bandits, our methods require no spectral assumptions on the context distribution beyond $\ell_2$ boundedness.

</details>


### [711] [Topological Residual Asymmetry for Bivariate Causal Direction](https://arxiv.org/abs/2602.00427)
*Mouad El Bouchattaoui*

Main category: stat.ML

TL;DR: 提出基于几何的Topological Residual Asymmetry (TRA)准则用于双变量因果方向推断，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 从纯观测双变量数据推断因果方向易出错，很多方法在模糊或几乎不可识别的情况下仍确定方向。

Method: 提出TRA准则，比较秩基copula标准化后两个交叉拟合回归残差云的形状，用0D持久同调泛函量化差异，证明小噪声下的一致性，扩展到固定噪声，引入TRA - C规则。

Result: 在许多具有挑战性的合成和真实数据场景的广泛实验中表现出色。

Conclusion: 所提方法在因果方向推断上具有优越性。

Abstract: Inferring causal direction from purely observational bivariate data is fragile: many methods commit to a direction even in ambiguous or near non-identifiable regimes. We propose Topological Residual Asymmetry (TRA), a geometry-based criterion for additive-noise models. TRA compares the shapes of two cross-fitted regressor-residual clouds after rank-based copula standardization: in the correct direction, residuals are approximately independent, producing a two-dimensional bulk, while in the reverse direction -- especially under low noise -- the cloud concentrates near a one-dimensional tube. We quantify this bulk-tube contrast using a 0D persistent-homology functional, computed efficiently from Euclidean MST edge-length profiles. We prove consistency in a triangular-array small-noise regime, extend the method to fixed noise via a binned variant (TRA-s), and introduce TRA-C, a confounding-aware abstention rule calibrated by a Gaussian-copula plug-in bootstrap. Extensive experiments across many challenging synthetic and real-data scenarios demonstrate the method's superiority.

</details>


### [712] [Stabilizing Fixed-Point Iteration for Markov Chain Poisson Equations](https://arxiv.org/abs/2602.00474)
*Yang Xu,Vaneet Aggarwal*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Poisson equations underpin average-reward reinforcement learning, but beyond ergodicity they can be ill-posed, meaning that solutions are non-unique and standard fixed point iterations can oscillate on reducible or periodic chains. We study finite-state Markov chains with $n$ states and transition matrix $P$. We show that all non-decaying modes are captured by a real peripheral invariant subspace $\mathcal{K}(P)$, and that the induced operator on the quotient space $\mathbb{R}^n/\mathcal{K}(P)$ is strictly contractive, yielding a unique quotient solution. Building on this viewpoint, we develop an end-to-end pipeline that learns the chain structure, estimates an anchor based gauge map, and runs projected stochastic approximation to estimate a gauge-fixed representative together with an associated peripheral residual. We prove $\widetilde{O}(T^{-1/2})$ convergence up to projection estimation error, enabling stable Poisson equation learning for multichain and periodic regimes with applications to performance evaluation of average-reward reinforcement learning beyond ergodicity.

</details>


### [713] [Action-Free Offline-to-Online RL via Discretised State Policies](https://arxiv.org/abs/2602.00629)
*Natinael Solomon Neggatu,Jeremie Houssineau,Giovanni Montana*

Main category: stat.ML

TL;DR: 提出无动作离线到在线强化学习框架，用状态策略学习，实验显示可提升收敛速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法假定数据集中有动作标签，但实际场景中动作可能缺失，需解决无动作数据学习问题。

Method: 提出学习状态策略，引入状态离散化转换，提出基于价值的算法Offline State-Only DecQN预训练状态策略，还提出引导式在线学习机制。

Result: 在不同基准测试中，该方法提高了收敛速度和渐近性能。

Conclusion: 离散化和正则化对方法有效性至关重要，所提框架可利用无动作数据集加速在线强化学习。

Abstract: Most existing offline RL methods presume the availability of action labels within the dataset, but in many practical scenarios, actions may be missing due to privacy, storage, or sensor limitations. We formalise the setting of action-free offline-to-online RL, where agents must learn from datasets consisting solely of $(s,r,s')$ tuples and later leverage this knowledge during online interaction. To address this challenge, we propose learning state policies that recommend desirable next-state transitions rather than actions. Our contributions are twofold. First, we introduce a simple yet novel state discretisation transformation and propose Offline State-Only DecQN (\algo), a value-based algorithm designed to pre-train state policies from action-free data. \algo{} integrates the transformation to scale efficiently to high-dimensional problems while avoiding instability and overfitting associated with continuous state prediction. Second, we propose a novel mechanism for guided online learning that leverages these pre-trained state policies to accelerate the learning of online agents. Together, these components establish a scalable and practical framework for leveraging action-free datasets to accelerate online RL. Empirical results across diverse benchmarks demonstrate that our approach improves convergence speed and asymptotic performance, while analyses reveal that discretisation and regularisation are critical to its effectiveness.

</details>


### [714] [Emergence of Distortions in High-Dimensional Guided Diffusion Models](https://arxiv.org/abs/2602.00716)
*Enrico Ventura,Beatrice Achilli,Luca Ambrogioni,Carlo Lucibello*

Main category: stat.ML

TL;DR: 本文指出分类器自由引导（CFG）在扩散模型条件采样中存在生成多样性损失问题，分析了生成失真现象，提出新的引导调度方法缓解多样性损失。


<details>
  <summary>Details</summary>
Motivation: CFG在扩散模型条件采样中常导致生成样本多样性损失，需研究此现象并解决。

Method: 考虑高斯混合及其精确分数，利用统计物理工具，进行动态平均场分析。

Result: 发现失真通过引导动力学有效势的相变出现；当模式数量随维度指数增长时失真持续，亚指数区域消失；标准CFG会改变条件分布均值和缩小方差，标准调度无法防止方差缩小。

Conclusion: 提出具有负引导窗口的引导调度，能缓解多样性损失并保持类可分性。

Abstract: Classifier-free guidance (CFG) is the de facto standard for conditional sampling in diffusion models, yet it often leads to a loss of diversity in generated samples. We formalize this phenomenon as generative distortion, defined as the mismatch between the CFG-induced sampling distribution and the true conditional distribution. Considering Gaussian mixtures and their exact scores, and leveraging tools from statistical physics, we characterize the onset of distortion in a high-dimensional regime as a function of the number of classes. Our analysis reveals that distortions emerge through a phase transition in the effective potential governing the guided dynamics. In particular, our dynamical mean-field analysis shows that distortion persists when the number of modes grows exponentially with dimension, but vanishes in the sub-exponential regime. Consistent with prior finite-dimensional results, we further demonstrate that vanilla CFG shifts the mean and shrinks the variance of the conditional distribution. We show that standard CFG schedules are fundamentally incapable of preventing variance shrinkage. Finally, we propose a theoretically motivated guidance schedule featuring a negative-guidance window, which mitigates loss of diversity while preserving class separability.

</details>


### [715] [Zero-Flow Encoders](https://arxiv.org/abs/2602.00797)
*Yakun Wang,Leyang Wang,Song Liu,Taiji Suzuki*

Main category: stat.ML

TL;DR: 提出基于流的表示学习框架，利用零流准则提取数据信息，在模拟和真实数据集实验中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的方法多用于生成任务，少有利用其能力解决生成任务外的细粒度结构细节问题，因此提出流启发的表示学习框架。

Method: 证明独立耦合训练的整流流在特定条件下的零流准则，用该准则认证条件独立性并提取信息，将其转化为可处理、无模拟的损失函数用于学习。

Result: 在模拟和真实数据集上的实验验证了方法的有效性。

Conclusion: 所提出的流启发框架在表示学习中是有效的，代码可在指定链接获取。

Abstract: Flow-based methods have achieved significant success in various generative modeling tasks, capturing nuanced details within complex data distributions. However, few existing works have exploited this unique capability to resolve fine-grained structural details beyond generation tasks. This paper presents a flow-inspired framework for representation learning. First, we demonstrate that a rectified flow trained using independent coupling is zero everywhere at $t=0.5$ if and only if the source and target distributions are identical. We term this property the \emph{zero-flow criterion}. Second, we show that this criterion can certify conditional independence, thereby extracting \emph{sufficient information} from the data. Third, we translate this criterion into a tractable, simulation-free loss function that enables learning amortized Markov blankets in graphical models and latent representations in self-supervised learning tasks. Experiments on both simulated and real-world datasets demonstrate the effectiveness of our approach. The code reproducing our experiments can be found at: https://github.com/probabilityFLOW/zfe.

</details>


### [716] [Hessian Spectral Analysis at Foundation Model Scale](https://arxiv.org/abs/2602.00816)
*Diego Granziol,Khurshid Juarev*

Main category: stat.ML

TL;DR: 论文表明可在前沿规模对真实Hessian矩阵进行可靠光谱分析，指出流行近似方法的问题，为大规模基于曲率的分析打开大门。


<details>
  <summary>Details</summary>
Motivation: 以往因难以获取基础模型准确的Hessian光谱，多依赖小模型或强结构近似，本文旨在解决在前沿规模进行真实Hessian光谱分析的问题。

Method: 使用与全分片数据并行兼容的分片局部有限差分Hessian向量积，对开源语言模型进行随机Lanczos求积。

Result: 得到超过100B参数模型的大规模光谱密度估计，分析了管道的数值行为，给出端到端运行时间和内存缩放定律，发现常用块对角曲率近似可能失败。

Conclusion: 基础模型Hessian光谱可计算，且现有近似方法存在定性错误，可开展大规模基于曲率的分析。

Abstract: Accurate Hessian spectra of foundation models have remained out of reach, leading most prior work to rely on small models or strong structural approximations. We show that faithful spectral analysis of the true Hessian is tractable at frontier scale. Using shard-local finite-difference Hessian vector products compatible with Fully Sharded Data Parallelism, we perform stochastic Lanczos quadrature on open-source language models with up to 100B parameters, producing the first large-scale spectral density estimates beyond the sub-10B regime. We characterize the numerical behavior of this pipeline, including finite-difference bias, floating-point noise amplification, and their effect on Krylov stability in fp32 and bf16, and derive practical operating regimes that are validated empirically. We further provide end-to-end runtime and memory scaling laws, showing that full-operator spectral probing incurs only a modest constant-factor overhead over first-order training. Crucially, direct access to the Hessian reveals that widely used block-diagonal curvature approximations can fail catastrophically, exhibiting order-one relative error and poor directional alignment even in mid-scale LLMs. Together, our results demonstrate that foundation-model Hessian spectra are both computable and qualitatively misrepresented by prevailing approximations, opening the door to principled curvature-based analysis at scale.

</details>


### [717] [Safety-Efficacy Trade Off: Robustness against Data-Poisoning](https://arxiv.org/abs/2602.00822)
*Diego Granziol*

Main category: stat.ML

TL;DR: 研究后门和数据投毒攻击机制，用核岭回归建模，证明投毒特性，分析防御方法并实验验证。


<details>
  <summary>Details</summary>
Motivation: 解释后门和数据投毒攻击能高成功率且躲避现有防御的原因。

Method: 使用核岭回归作为宽神经网络的精确模型，分析输入Hessian矩阵，研究输入梯度正则化。

Result: 实验验证攻击成功率和频谱可见性存在滞后，正则化和数据增强可抑制投毒。

Conclusion: 明确后门何时不可见，首次通过输入空间曲率对投毒、可检测性和防御进行端到端刻画。

Abstract: Backdoor and data poisoning attacks can achieve high attack success while evading existing spectral and optimisation based defences. We show that this behaviour is not incidental, but arises from a fundamental geometric mechanism in input space. Using kernel ridge regression as an exact model of wide neural networks, we prove that clustered dirty label poisons induce a rank one spike in the input Hessian whose magnitude scales quadratically with attack efficacy. Crucially, for nonlinear kernels we identify a near clone regime in which poison efficacy remains order one while the induced input curvature vanishes, making the attack provably spectrally undetectable. We further show that input gradient regularisation contracts poison aligned Fisher and Hessian eigenmodes under gradient flow, yielding an explicit and unavoidable safety efficacy trade off by reducing data fitting capacity. For exponential kernels, this defence admits a precise interpretation as an anisotropic high pass filter that increases the effective length scale and suppresses near clone poisons. Extensive experiments on linear models and deep convolutional networks across MNIST and CIFAR 10 and CIFAR 100 validate the theory, demonstrating consistent lags between attack success and spectral visibility, and showing that regularisation and data augmentation jointly suppress poisoning. Our results establish when backdoors are inherently invisible, and provide the first end to end characterisation of poisoning, detectability, and defence through input space curvature.

</details>


### [718] [Harmful Overfitting in Sobolev Spaces](https://arxiv.org/abs/2602.00825)
*Kedar Karhadkar,Alexander Sietsema,Deanna Needell,Guido Montufar*

Main category: stat.ML

TL;DR: 研究Sobolev空间中完美拟合含噪训练数据集的函数泛化行为，发现近似范数最小插值器会有害过拟合。


<details>
  <summary>Details</summary>
Motivation: 受机器学习中良性过拟合相关工作启发，研究Sobolev空间函数在拟合含噪训练数据时的泛化行为。

Method: 使用几何论证，通过Sobolev不等式识别训练数据的有害邻域。

Result: 在标签噪声和数据分布足够规则的假设下，近似范数最小插值器会有害过拟合，训练样本量趋于无穷时，泛化误差大概率有正下界。

Conclusion: 得到了p在[1,∞)时的结果，相比之前核方法研究p = 2的情况更具一般性。

Abstract: Motivated by recent work on benign overfitting in overparameterized machine learning, we study the generalization behavior of functions in Sobolev spaces $W^{k, p}(\mathbb{R}^d)$ that perfectly fit a noisy training data set. Under assumptions of label noise and sufficient regularity in the data distribution, we show that approximately norm-minimizing interpolators, which are canonical solutions selected by smoothness bias, exhibit harmful overfitting: even as the training sample size $n \to \infty$, the generalization error remains bounded below by a positive constant with high probability. Our results hold for arbitrary values of $p \in [1, \infty)$, in contrast to prior results studying the Hilbert space case ($p = 2$) using kernel methods. Our proof uses a geometric argument which identifies harmful neighborhoods of the training data using Sobolev inequalities.

</details>


### [719] [Score-based Metropolis-Hastings for Fractional Langevin Algorithms](https://arxiv.org/abs/2602.00835)
*Ahmed Aloui,Junyi Liao,Ali Hasan,Jose Blanchet,Vahid Tarokh*

Main category: stat.ML

TL;DR: 提出Metropolis - Adjusted Fractional Langevin Algorithm (MAFLA)解决重尾和多峰分布采样难题，在组合优化等任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有分数朗之万方法在目标密度和提议密度都无法评估时存在有限时间误差大、尾部行为控制差的问题，经典基于密度的Metropolis - Hastings (MH)校正不可行。

Method: 引入受MH启发的全基于分数的校正机制MAFLA，使用代理计算分数提议分数梯度，通过分数平衡匹配学习接受函数。

Result: MAFLA在组合优化等一系列任务中表现出强大性能，显著提高了有限时间采样精度。

Conclusion: MAFLA是解决重尾和多峰分布采样难题的有效方法。

Abstract: Sampling from heavy-tailed and multimodal distributions is challenging when neither the target density nor the proposal density can be evaluated, as in $α$-stable Lévy-driven fractional Langevin algorithms. While the target distribution can be estimated from data via score-based or energy-based models, the $α$-stable proposal density and its score are generally unavailable, rendering classical density-based Metropolis--Hastings (MH) corrections impractical. Consequently, existing fractional Langevin methods operate in an unadjusted regime and can exhibit substantial finite-time errors and poor empirical control of tail behavior. We introduce the Metropolis-Adjusted Fractional Langevin Algorithm (MAFLA), an MH-inspired, fully score-based correction mechanism. MAFLA employs designed proxies for fractional proposal score gradients under isotropic symmetric $α$-stable noise and learns an acceptance function via Score Balance Matching. We empirically illustrate the strong performance of MAFLA on a series of tasks including combinatorial optimization problems where the method significantly improves finite time sampling accuracy over unadjusted fractional Langevin dynamics.

</details>


### [720] [Multivariate Time Series Data Imputation via Distributionally Robust Regularization](https://arxiv.org/abs/2602.00844)
*Che-Yi Liao,Zheng Dong,Gian-Gabriel Garcia,Kamran Paynabar*

Main category: stat.ML

TL;DR: 提出DRIO解决MTS插补分布不匹配问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: MTS插补存在观测与真实数据分布不匹配问题，标准方法易过拟合有偏观测。

Method: 提出DRIO，联合最小化重建误差和插补器与Wasserstein模糊集内最坏情况分布的差异，推导对偶公式并提出对抗学习算法。

Result: 在不同真实数据集实验中，DRIO在MCAR和MNAR设置下均提升插补效果，达到重建精度和分布对齐的帕累托最优权衡。

Conclusion: DRIO能有效解决MTS插补的分布不匹配问题。

Abstract: Multivariate time series (MTS) imputation is often compromised by mismatch between observed and true data distributions -- a bias exacerbated by non-stationarity and systematic missingness. Standard methods that minimize reconstruction error or encourage distributional alignment risk overfitting these biased observations. We propose the Distributionally Robust Regularized Imputer Objective (DRIO), which jointly minimizes reconstruction error and the divergence between the imputer and a worst-case distribution within a Wasserstein ambiguity set. We derive a tractable dual formulation that reduces infinite-dimensional optimization over measures to adversarial search over sample trajectories, and propose an adversarial learning algorithm compatible with flexible deep learning backbones. Comprehensive experiments on diverse real-world datasets show DRIO consistently improves imputation under both missing-completely-at-random and missing-not-at-random settings, reaching Pareto-optimal trade-offs between reconstruction accuracy and distributional alignment.

</details>


### [721] [Optimal Decision-Making Based on Prediction Sets](https://arxiv.org/abs/2602.00989)
*Tao Wang,Edgar Dobriban*

Main category: stat.ML

TL;DR: 提出决策理论框架以优化使用预测集进行下游决策，引入ROCP算法并通过实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决如何最优地使用预测集进行下游决策的问题。

Method: 提出决策理论框架，确定固定预测集的极小极大最优策略，推导最优预测集构建方法，引入ROCP算法。

Result: 在医学诊断和安全关键决策任务上的实证评估表明，ROCP相比基线减少了关键错误。

Conclusion: ROCP能减少关键错误，尤其在集外错误代价高昂时效果显著。

Abstract: Prediction sets can wrap around any ML model to cover unknown test outcomes with a guaranteed probability. Yet, it remains unclear how to use them optimally for downstream decision-making. Here, we propose a decision-theoretic framework that seeks to minimize the expected loss (risk) against a worst-case distribution consistent with the prediction set's coverage guarantee. We first characterize the minimax optimal policy for a fixed prediction set, showing that it balances the worst-case loss inside the set with a penalty for potential losses outside the set. Building on this, we derive the optimal prediction set construction that minimizes the resulting robust risk subject to a coverage constraint. Finally, we introduce Risk-Optimal Conformal Prediction (ROCP), a practical algorithm that targets these risk-minimizing sets while maintaining finite-sample distribution-free marginal coverage. Empirical evaluations on medical diagnosis and safety-critical decision-making tasks demonstrate that ROCP reduces critical mistakes compared to baselines, particularly when out-of-set errors are costly.

</details>


### [722] [Online Social Welfare Function-based Resource Allocation](https://arxiv.org/abs/2602.01400)
*Kanad Pardeshi,Samsara Foubert,Aarti Singh*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In many real-world settings, a centralized decision-maker must repeatedly allocate finite resources to a population over multiple time steps. Individuals who receive a resource derive some stochastic utility; to characterize the population-level effects of an allocation, the expected individual utilities are then aggregated using a social welfare function (SWF). We formalize this setting and present a general confidence sequence framework for SWF-based online learning and inference, valid for any monotonic, concave, and Lipschitz-continuous SWF. Our key insight is that monotonicity alone suffices to lift confidence sequences from individual utilities to anytime-valid bounds on optimal welfare. Building on this foundation, we propose SWF-UCB, a SWF-agnostic online learning algorithm that achieves near-optimal $\tilde{O}(n+\sqrt{nkT})$ regret (for $k$ resources distributed among $n$ individuals at each of $T$ time steps). We instantiate our framework on three normatively distinct SWF families: Weighted Power Mean, Kolm, and Gini, providing bespoke oracle algorithms for each. Experiments confirm $\sqrt{T}$ scaling and reveal rich interactions between $k$ and SWF parameters. This framework naturally supports inference applications such as sequential hypothesis testing, optimal stopping, and policy evaluation.

</details>


### [723] [Importance Weighted Variational Inference without the Reparameterization Trick](https://arxiv.org/abs/2602.01412)
*Kamélia Daudel,Minh-Ngoc Tran,Cheng Zhang*

Main category: stat.ML

TL;DR: 本文对重要性加权变分推理中的REINFORCE梯度估计器进行分析，指出当前VIMCO梯度估计器问题，提出新的VIMCO-*估计器并展示其优势。


<details>
  <summary>Details</summary>
Motivation: 标准优化的重参数化梯度估计器有局限性，REINFORCE梯度估计器缺乏理论依据，需对其进行分析和改进。

Method: 对REINFORCE梯度估计器进行全面分析，引入并研究VIMCO梯度估计器家族，提出新的VIMCO-*梯度估计器。

Result: 证明现有VIMCO梯度估计器信噪比随N增加而消失，新的VIMCO-*估计器能避免此问题，实现√N的信噪比缩放。

Conclusion: 新的VIMCO-*梯度估计器在重参数化梯度不可用的挑战性场景中表现优于现有VIMCO实现。

Abstract: Importance weighted variational inference (VI) approximates densities known up to a normalizing constant by optimizing bounds that tighten with the number of Monte Carlo samples $N$. Standard optimization relies on reparameterized gradient estimators, which are well-studied theoretically yet restrict both the choice of the data-generating process and the variational approximation. While REINFORCE gradient estimators do not suffer from such restrictions, they lack rigorous theoretical justification. In this paper, we provide the first comprehensive analysis of REINFORCE gradient estimators in importance weighted VI, leveraging this theoretical foundation to diagnose and resolve fundamental deficiencies in current state-of-the-art estimators. Specifically, we introduce and examine a generalized family of variational inference for Monte Carlo objectives (VIMCO) gradient estimators. We prove that state-of-the-art VIMCO gradient estimators exhibit a vanishing signal-to-noise ratio (SNR) as $N$ increases, which prevents effective optimization. To overcome this issue, we propose the novel VIMCO-$\star$ gradient estimator and show that it averts the SNR collapse of existing VIMCO gradient estimators by achieving a $\sqrt{N}$ SNR scaling instead. We demonstrate its superior empirical performance compared to current VIMCO implementations in challenging settings where reparameterized gradients are typically unavailable.

</details>


### [724] [Robust Generalization with Adaptive Optimal Transport Priors for Decision-Focused Learning](https://arxiv.org/abs/2602.01427)
*Haixiang Sun,Andrew L. Liu*

Main category: stat.ML

TL;DR: 提出PG - DRO框架用于少样本学习，实验表明其在少样本场景有更强鲁棒泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有Sinkhorn DRO方法依赖固定参考分布，限制了适应性，少样本学习需在有限监督下泛化并对分布偏移保持鲁棒。

Method: 提出PG - DRO框架，通过分层最优传输从大量基础数据中学习类自适应先验，并嵌入到Sinkhorn DRO公式中。

Result: PG - DRO在少样本场景中实现了更强的鲁棒泛化，优于标准学习器和DRO基线。

Conclusion: PG - DRO框架能将少样本信息有机整合，产生特定类的鲁棒决策，且使不确定性集与可转移的结构知识对齐，在少样本学习中有良好效果。

Abstract: Few-shot learning requires models to generalize under limited supervision while remaining robust to distribution shifts. Existing Sinkhorn Distributionally Robust Optimization (DRO) methods provide theoretical guarantees but rely on a fixed reference distribution, which limits their adaptability. We propose a Prototype-Guided Distributionally Robust Optimization (PG-DRO) framework that learns class-adaptive priors from abundant base data via hierarchical optimal transport and embeds them into the Sinkhorn DRO formulation. This design enables few-shot information to be organically integrated into producing class-specific robust decisions that are both theoretically grounded and efficient, and further aligns the uncertainty set with transferable structural knowledge. Experiments show that PG-DRO achieves stronger robust generalization in few-shot scenarios, outperforming both standard learners and DRO baselines.

</details>


### [725] [Rethinking Multinomial Logistic Mixture of Experts with Sigmoid Gating Function](https://arxiv.org/abs/2602.01466)
*Tuan Minh Pham,Thinh Cao,Viet Nguyen,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: stat.ML

TL;DR: 研究修正sigmoid门控的多项逻辑混合专家模型，发现其样本复杂度优势，提出用欧氏分数改进。


<details>
  <summary>Details</summary>
Motivation: 现有文献未解决sigmoid门在分类设置的优势、模型收敛及温度参数影响等问题。

Method: 对配备修正sigmoid门的多项逻辑混合专家模型进行全面分析。

Result: sigmoid门在参数和专家估计上样本复杂度低于softmax门；含温度的sigmoid门样本复杂度为指数级；用欧氏分数替换内积分数可将样本复杂度提升至多项式级。

Conclusion: 改进的sigmoid门控多项逻辑混合专家模型在样本复杂度上有显著优势。

Abstract: The sigmoid gate in mixture-of-experts (MoE) models has been empirically shown to outperform the softmax gate across several tasks, ranging from approximating feed-forward networks to language modeling. Additionally, recent efforts have demonstrated that the sigmoid gate is provably more sample-efficient than its softmax counterpart under regression settings. Nevertheless, there are three notable concerns that have not been addressed in the literature, namely (i) the benefits of the sigmoid gate have not been established under classification settings; (ii) existing sigmoid-gated MoE models may not converge to their ground-truth; and (iii) the effects of a temperature parameter in the sigmoid gate remain theoretically underexplored. To tackle these open problems, we perform a comprehensive analysis of multinomial logistic MoE equipped with a modified sigmoid gate to ensure model convergence. Our results indicate that the sigmoid gate exhibits a lower sample complexity than the softmax gate for both parameter and expert estimation. Furthermore, we find that incorporating a temperature into the sigmoid gate leads to a sample complexity of exponential order due to an intrinsic interaction between the temperature and gating parameters. To overcome this issue, we propose replacing the vanilla inner product score in the gating function with a Euclidean score that effectively removes that interaction, thereby substantially improving the sample complexity to a polynomial order.

</details>


### [726] [Density-Informed Pseudo-Counts for Calibrated Evidential Deep Learning](https://arxiv.org/abs/2602.01477)
*Pietro Carlotti,Nevena Gligić,Arya Farahi*

Main category: stat.ML

TL;DR: 本文指出标准EDL存在混淆认知和随机不确定性问题，提出DIP - EDL新参数化方法，理论证明其渐近收敛性，实证表明该方法提升可解释性、鲁棒性和不确定性校准能力。


<details>
  <summary>Details</summary>
Motivation: 当前对证据深度学习（EDL）的理论基础和分布偏移下的行为理解不足，标准EDL存在混淆认知和随机不确定性的问题。

Method: 证明EDL训练对应分层贝叶斯模型中的摊销变分推理，提出DIP - EDL新参数化方法，分别估计条件标签分布和边际协变量密度。

Result: 理论上证明DIP - EDL实现渐近收敛，实证显示该方法提升可解释性，改善分布偏移下的鲁棒性和不确定性校准。

Conclusion: DIP - EDL能解决标准EDL的问题，提升模型在分布偏移下的性能。

Abstract: Evidential Deep Learning (EDL) is a popular framework for uncertainty-aware classification that models predictive uncertainty via Dirichlet distributions parameterized by neural networks. Despite its popularity, its theoretical foundations and behavior under distributional shift remain poorly understood. In this work, we provide a principled statistical interpretation by proving that EDL training corresponds to amortized variational inference in a hierarchical Bayesian model with a tempered pseudo-likelihood. This perspective reveals a major drawback: standard EDL conflates epistemic and aleatoric uncertainty, leading to systematic overconfidence on out-of-distribution (OOD) inputs. To address this, we introduce Density-Informed Pseudo-count EDL (DIP-EDL), a new parametrization that decouples class prediction from the magnitude of uncertainty by separately estimating the conditional label distribution and the marginal covariate density. This separation preserves evidence in high-density regions while shrinking predictions toward a uniform prior for OOD data. Theoretically, we prove that DIP-EDL achieves asymptotic concentration. Empirically, we show that our method enhances interpretability and improves robustness and uncertainty calibration under distributional shift.

</details>


### [727] [Inference-Aware Meta-Alignment of LLMs via Non-Linear GRPO](https://arxiv.org/abs/2602.01603)
*Shokichi Takakura,Akifumi Wachi,Rei Higuchi,Kohei Miyaguchi,Taiji Suzuki*

Main category: stat.ML

TL;DR: 针对大语言模型与人类偏好对齐问题，提出计算高效的推理感知元对齐 (IAMA) 方法及非线性GRPO求解法。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型与人类偏好对齐困难，推理时对齐计算成本高，需探索有限计算资源下实现多标准对齐的方法。

Method: 提出IAMA方法训练基础模型，使其能通过不同推理时对齐算法有效对齐多个任务；提出非线性GRPO解决IAMA中的非线性优化问题。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: Aligning large language models (LLMs) to diverse human preferences is fundamentally challenging since criteria can often conflict with each other. Inference-time alignment methods have recently gained popularity as they allow LLMs to be aligned to multiple criteria via different alignment algorithms at inference time. However, inference-time alignment is computationally expensive since it often requires multiple forward passes of the base model. In this work, we propose inference-aware meta-alignment (IAMA), a novel approach that enables LLMs to be aligned to multiple criteria with limited computational budget at inference time. IAMA trains a base model such that it can be effectively aligned to multiple tasks via different inference-time alignment algorithms. To solve the non-linear optimization problems involved in IAMA, we propose non-linear GRPO, which provably converges to the optimal solution in the space of probability measures.

</details>


### [728] [ST-BCP: Tightening Coverage Bound for Backward Conformal Prediction via Non-Conformity Score Transformation](https://arxiv.org/abs/2602.01733)
*Junxian Liu,Hao Zeng,Hongxin Wei*

Main category: stat.ML

TL;DR: 本文引入新方法ST - BCP缩小Backward Conformal Prediction（BCP）的覆盖差距，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: BCP框架中马尔可夫不等式导致估计覆盖界限和经验覆盖之间存在显著差距。

Method: 引入ST - BCP，对非一致性分数进行数据依赖的变换。

Result: 在常见基准上，平均覆盖差距从4.20%降至1.12%。

Conclusion: 所提出的ST - BCP方法能有效缩小BCP框架中的覆盖差距。

Abstract: Conformal Prediction (CP) provides a statistical framework for uncertainty quantification that constructs prediction sets with coverage guarantees. While CP yields uncontrolled prediction set sizes, Backward Conformal Prediction (BCP) inverts this paradigm by enforcing a predefined upper bound on set size and estimating the resulting coverage guarantee. However, the looseness induced by Markov's inequality within the BCP framework causes a significant gap between the estimated coverage bound and the empirical coverage. In this work, we introduce ST-BCP, a novel method that introduces a data-dependent transformation of nonconformity scores to narrow the coverage gap. In particular, we develop a computable transformation and prove that it outperforms the baseline identity transformation. Extensive experiments demonstrate the effectiveness of our method, reducing the average coverage gap from 4.20\% to 1.12\% on common benchmarks.

</details>


### [729] [Transformers as Measure-Theoretic Associative Memory: A Statistical Perspective and Minimax Optimality](https://arxiv.org/abs/2602.01863)
*Ryotaro Kawata,Taiji Suzuki*

Main category: stat.ML

TL;DR: 本文从概率测度角度重塑关联记忆来研究Transformer，证明了浅层测度理论Transformer学习能力及收敛阶的尖锐性，提供设计与分析框架。


<details>
  <summary>Details</summary>
Motivation: Transformer在内容寻址检索和利用无界长度上下文方面表现出色，需从概率测度层面研究其关联记忆。

Method: 将上下文视为标记上的分布，将注意力视为测度上的积分算子；研究通过经验风险最小化训练的学习型softmax注意力；在输入密度的谱假设下分析浅层测度理论Transformer与MLP组成的模型。

Result: 浅层测度理论Transformer能学习召回预测映射；建立了匹配的极小极大下界，证明收敛阶的尖锐性。

Conclusion: 该框架为设计和分析能从任意长分布上下文召回且有可证明泛化保证的Transformer提供了原则性方法。

Abstract: Transformers excel through content-addressable retrieval and the ability to exploit contexts of, in principle, unbounded length. We recast associative memory at the level of probability measures, treating a context as a distribution over tokens and viewing attention as an integral operator on measures. Concretely, for mixture contexts $ν= I^{-1} \sum_{i=1}^I μ^{(i^*)}$ and a query $x_{\mathrm{q}}(i^*)$, the task decomposes into (i) recall of the relevant component $μ^{(i^*)}$ and (ii) prediction from $(μ_{i^*},x_\mathrm{q})$. We study learned softmax attention (not a frozen kernel) trained by empirical risk minimization and show that a shallow measure-theoretic Transformer composed with an MLP learns the recall-and-predict map under a spectral assumption on the input densities. We further establish a matching minimax lower bound with the same rate exponent (up to multiplicative constants), proving sharpness of the convergence order. The framework offers a principled recipe for designing and analyzing Transformers that recall from arbitrarily long, distributional contexts with provable generalization guarantees.

</details>


### [730] [Privacy Amplification by Missing Data](https://arxiv.org/abs/2602.01928)
*Simon Roburin,Rafaël Pinot,Erwan Scornet*

Main category: stat.ML

TL;DR: 本文从隐私保护角度研究缺失数据，证明不完整数据可实现差分隐私算法的隐私放大


<details>
  <summary>Details</summary>
Motivation: 隐私保护是医学和金融等高风险领域的基本要求，且这些领域数据常存在缺失值，传统认为缺失数据有局限，因此从隐私保护角度探索缺失数据

Method: 在差分隐私框架内将缺失数据作为隐私放大机制进行分析

Result: 首次证明不完整数据能为差分隐私算法带来隐私放大

Conclusion: 缺失数据从隐私保护角度有积极作用，可能提升隐私性

Abstract: Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.

</details>


### [731] [Stochastic Interpolants in Hilbert Spaces](https://arxiv.org/abs/2602.01988)
*James Boran Yu,RuiKang OuYang,Julien Horwood,José Miguel Hernández-Lobato*

Main category: stat.ML

TL;DR: 本文建立无限维希尔伯特空间中随机插值的严格框架，为科学发现提供工具。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽可扩展到函数值数据，但随机插值仅限于有限维设置，需在无限维空间建立框架。

Method: 建立无限维希尔伯特空间中随机插值的严格框架，提供理论基础和误差界证明。

Result: 在所提框架下进行条件生成展现出有效性，尤其在复杂基于偏微分方程基准测试中，取得了最先进的结果。

Conclusion: 该方法能实现任意函数分布间的生成桥接，是科学发现的强大通用工具。

Abstract: Although diffusion models have successfully extended to function-valued data, stochastic interpolants -- which offer a flexible way to bridge arbitrary distributions -- remain limited to finite-dimensional settings. This work bridges this gap by establishing a rigorous framework for stochastic interpolants in infinite-dimensional Hilbert spaces. We provide comprehensive theoretical foundations, including proofs of well-posedness and explicit error bounds. We demonstrate the effectiveness of the proposed framework for conditional generation, focusing particularly on complex PDE-based benchmarks. By enabling generative bridges between arbitrary functional distributions, our approach achieves state-of-the-art results, offering a powerful, general-purpose tool for scientific discovery.

</details>


### [732] [Training-free score-based diffusion for parameter-dependent stochastic dynamical systems](https://arxiv.org/abs/2602.02113)
*Minglei Yang,Sicheng He*

Main category: stat.ML

TL;DR: 提出无训练条件扩散模型框架学习参数相关SDEs的随机流映射，加速参数研究等应用，经数值算例验证性能。


<details>
  <summary>Details</summary>
Motivation: 模拟参数相关SDEs计算挑战大，现有机器学习方法存在训练成本高或无法处理连续参数依赖的问题。

Method: 提出无训练条件扩散模型框架，采用联合核加权蒙特卡罗估计器近似条件得分函数。

Result: 通过三个复杂度递增的数值算例，表明该方法能准确近似不同参数值下的条件分布。

Conclusion: 该方法无需重新训练即可为训练范围内任意参数值生成样本轨迹，可显著加速参数研究、不确定性量化和实时过滤应用。

Abstract: Simulating parameter-dependent stochastic differential equations (SDEs) presents significant computational challenges, as separate high-fidelity simulations are typically required for each parameter value of interest. Despite the success of machine learning methods in learning SDE dynamics, existing approaches either require expensive neural network training for score function estimation or lack the ability to handle continuous parameter dependence. We present a training-free conditional diffusion model framework for learning stochastic flow maps of parameter-dependent SDEs, where both drift and diffusion coefficients depend on physical parameters. The key technical innovation is a joint kernel-weighted Monte Carlo estimator that approximates the conditional score function using trajectory data sampled at discrete parameter values, enabling interpolation across both state space and the continuous parameter domain. Once trained, the resulting generative model produces sample trajectories for any parameter value within the training range without retraining, significantly accelerating parameter studies, uncertainty quantification, and real-time filtering applications. The performance of the proposed approach is demonstrated via three numerical examples of increasing complexity, showing accurate approximation of conditional distributions across varying parameter values.

</details>


### [733] [Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on an Expressive and Cumulant-Controllable Data Model](https://arxiv.org/abs/2602.02153)
*Onat Ure,Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 本文用矩可控的非高斯数据模型研究数据高阶统计量对神经网络学习动态的影响，发现训练有逐阶进展性，并在真实数据集上验证了结论和数据模型的实用性。


<details>
  <summary>Details</summary>
Motivation: 研究数据高阶统计量对神经网络学习动态的影响，弥合简化数据假设与实际数据复杂性之间的差距。

Method: 构建矩可控的非高斯数据模型，通过厄米多项式展开激活函数实现对高阶累积量的控制；用数据模型生成样本进行在线学习实验；在Fashion - MNIST数据集上预训练生成模型并进一步实验。

Result: 训练呈现逐阶进展，网络先学习低阶统计量，再逐步学习高阶累积量；额外实验证实结论并展现数据模型在现实场景中的实用性。

Conclusion: 所提方法桥接了简化数据假设和实际数据复杂性，为机器学习和信号处理中研究分布效应提供了原则性框架。

Abstract: We study the effect of high-order statistics of data on the learning dynamics of neural networks (NNs) by using a moment-controllable non-Gaussian data model. Considering the expressivity of two-layer neural networks, we first construct the data model as a generative two-layer NN where the activation function is expanded by using Hermite polynomials. This allows us to achieve interpretable control over high-order cumulants such as skewness and kurtosis through the Hermite coefficients while keeping the data model realistic. Using samples generated from the data model, we perform controlled online learning experiments with a two-layer NN. Our results reveal a moment-wise progression in training: networks first capture low-order statistics such as mean and covariance, and progressively learn high-order cumulants. Finally, we pretrain the generative model on the Fashion-MNIST dataset and leverage the generated samples for further experiments. The results of these additional experiments confirm our conclusions and show the utility of the data model in a real-world scenario. Overall, our proposed approach bridges simplified data assumptions and practical data complexity, which offers a principled framework for investigating distributional effects in machine learning and signal processing.

</details>


### [734] [PCA of probability measures: Sparse and Dense sampling regimes](https://arxiv.org/abs/2602.02190)
*Gachon Erell,Jérémie Bigot,Elsa Cazelles*

Main category: stat.ML

TL;DR: 研究在双渐近体制下对多个概率测度进行PCA，推导经验协方差算子和PCA超额风险的收敛率，证明密集体制下的率是极小极大最优的，数值实验验证理论结果。


<details>
  <summary>Details</summary>
Motivation: 现有文献未解决涉及多个概率测度的PCA设置问题，本文研究在双渐近体制下（观察n个概率测度，每个测度有m个样本）的PCA。

Method: 理论推导收敛率，进行数值实验验证。

Result: 得到经验协方差算子和PCA超额风险的收敛率为$n^{-1/2} + m^{-α}$，证明密集体制下的率是极小极大最优的，数值实验验证了理论率，且适当子采样可在降低计算成本的同时保持PCA精度。

Conclusion: 刻画了测度数量n和每个测度样本数量m之间的关系，揭示了收敛行为从稀疏到密集的转变，理论结果得到数值实验验证。

Abstract: A common approach to perform PCA on probability measures is to embed them into a Hilbert space where standard functional PCA techniques apply. While convergence rates for estimating the embedding of a single measure from $m$ samples are well understood, the literature has not addressed the setting involving multiple measures. In this paper, we study PCA in a double asymptotic regime where $n$ probability measures are observed, each through $m$ samples. We derive convergence rates of the form $n^{-1/2} + m^{-α}$ for the empirical covariance operator and the PCA excess risk, where $α>0$ depends on the chosen embedding. This characterizes the relationship between the number $n$ of measures and the number $m$ of samples per measure, revealing a sparse (small $m$) to dense (large $m$) transition in the convergence behavior. Moreover, we prove that the dense-regime rate is minimax optimal for the empirical covariance error. Our numerical experiments validate these theoretical rates and demonstrate that appropriate subsampling preserves PCA accuracy while reducing computational cost.

</details>


### [735] [Transfer Learning Through Conditional Quantile Matching](https://arxiv.org/abs/2602.02358)
*Yikun Zhang,Steven Wilkins-Reeves,Wesley Lee,Aude Hofleitner*

Main category: stat.ML

TL;DR: 提出用于回归的迁移学习框架，利用异构源域提升数据稀缺目标域的预测性能，理论证明有更紧的超额风险界，实践表明能提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的目标域中，利用异构源域提升预测性能。

Method: 为每个源域学习条件生成模型，通过条件分位数匹配将生成的响应校准到目标域。

Result: 理论上，在增强数据集上训练的经验风险最小化器有更紧的超额风险界；实践中，该方法比仅使用目标域学习和其他竞争迁移学习方法更能提高预测准确性。

Conclusion: 所提出的框架为目标域的下游学习任务提供了一种有原则且灵活的高质量数据增强方法。

Abstract: We introduce a transfer learning framework for regression that leverages heterogeneous source domains to improve predictive performance in a data-scarce target domain. Our approach learns a conditional generative model separately for each source domain and calibrates the generated responses to the target domain via conditional quantile matching. This distributional alignment step corrects general discrepancies between source and target domains without imposing restrictive assumptions such as covariate or label shift. The resulting framework provides a principled and flexible approach to high-quality data augmentation for downstream learning tasks in the target domain. From a theoretical perspective, we show that an empirical risk minimizer (ERM) trained on the augmented dataset achieves a tighter excess risk bound than the target-only ERM under mild conditions. In particular, we establish new convergence rates for the quantile matching estimator that governs the transfer bias-variance tradeoff. From a practical perspective, extensive simulations and real data applications demonstrate that the proposed method consistently improves prediction accuracy over target-only learning and competing transfer learning methods.

</details>


### [736] [Provably Data-driven Multiple Hyper-parameter Tuning with Structured Loss Function](https://arxiv.org/abs/2602.02406)
*Tung Quoc Le,Anh Tuan Nguyen,Viet Anh Nguyen*

Main category: stat.ML

TL;DR: 本文为数据驱动设置中的多维超参数调优建立通用框架，加强泛化保证框架，扩展分析并给出新的可学习性结果。


<details>
  <summary>Details</summary>
Motivation: 数据驱动算法设计中超参数调优的统计基础有限，现有保证多针对一维超参数，多维超参数调优问题尚未解决。

Method: 利用实代数几何工具加强半代数函数类的泛化保证框架，在最小假设下扩展到使用验证损失进行超参数调优分析。

Result: 得到了更严格、更广泛适用的泛化保证，在有额外结构时导出改进的边界。

Conclusion: 该框架具有广泛适用性，通过新的可学习性结果得到验证，如数据驱动的加权组套索和加权融合套索。

Abstract: Data-driven algorithm design automates hyperparameter tuning, but its statistical foundations remain limited because model performance can depend on hyperparameters in implicit and highly non-smooth ways. Existing guarantees focus on the simple case of a one-dimensional (scalar) hyperparameter. This leaves the practically important, multi-dimensional hyperparameter tuning setting unresolved. We address this open question by establishing the first general framework for establishing generalization guarantees for tuning multi-dimensional hyperparameters in data-driven settings. Our approach strengthens the generalization guarantee framework for semi-algebraic function classes by exploiting tools from real algebraic geometry, yielding sharper, more broadly applicable guarantees. We then extend the analysis to hyperparameter tuning using the validation loss under minimal assumptions, and derive improved bounds when additional structure is available. Finally, we demonstrate the scope of the framework with new learnability results, including data-driven weighted group lasso and weighted fused lasso.

</details>


### [737] [Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning](https://arxiv.org/abs/2602.02431)
*Filip Kovačević,Hong Chang Ji,Denny Wu,Mahdi Soltanolkotabi,Marco Mondelli*

Main category: stat.ML

TL;DR: 本文研究单指标模型中全批量梯度下降（GD）和单遍随机梯度下降（online SGD），发现截断激活后全批量GD在统计效率上优于online SGD，且在一定样本数和梯度步数下可实现强恢复。


<details>
  <summary>Details</summary>
Motivation: 除线性回归外，全批量梯度下降相对单遍随机梯度下降在理论上的优势尚不明确，本文旨在研究单指标模型中两者表现。

Method: 考虑具有二次激活的d维单指标模型，分析全批量球形GD在相关损失上的样本复杂度，对激活进行截断，还对小初始化下全批量GD在平方损失上进行轨迹分析。

Result: 全批量球形GD在相关损失上样本复杂度的log d因子仍然存在，但截断激活后，n≈d时全批量GD优化景观良好，统计效率优于单遍SGD；小初始化下，n≳d个样本和T≳log d次梯度步足以实现强恢复。

Conclusion: 截断激活可使全批量GD在统计效率上超越单遍SGD，且在一定条件下能实现强恢复。

Abstract: It is folklore that reusing training data more than once can improve the statistical efficiency of gradient-based learning. However, beyond linear regression, the theoretical advantage of full-batch gradient descent (GD, which always reuses all the data) over one-pass stochastic gradient descent (online SGD, which uses each data point only once) remains unclear. In this work, we consider learning a $d$-dimensional single-index model with a quadratic activation, for which it is known that one-pass SGD requires $n\gtrsim d\log d$ samples to achieve weak recovery. We first show that this $\log d$ factor in the sample complexity persists for full-batch spherical GD on the correlation loss; however, by simply truncating the activation, full-batch GD exhibits a favorable optimization landscape at $n \simeq d$ samples, thereby outperforming one-pass SGD (with the same activation) in statistical efficiency. We complement this result with a trajectory analysis of full-batch GD on the squared loss from small initialization, showing that $n \gtrsim d$ samples and $T \gtrsim\log d$ gradient steps suffice to achieve strong (exact) recovery.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [738] [Exact Gibbs sampling for stochastic differential equations with gradient drift and constant diffusion](https://arxiv.org/abs/2602.00512)
*Xinyi Pei,Minhyeok Kim,Vinayak Rao*

Main category: stat.CO

TL;DR: 提出适用于单位扩散系数SDEs的精确MCMC采样算法，无离散化误差，在合成和真实数据集表现优于粒子MCMC方法。


<details>
  <summary>Details</summary>
Motivation: 现有模拟SDEs路径的方法存在引入偏差、需复杂抽样或去偏方案、适用范围窄等问题。

Method: 提出精确MCMC采样算法，开发Gibbs采样框架，可扩展至参数模拟并应用高斯过程工具。

Result: 在合成和真实数据集上评估，表现优于粒子MCMC方法。

Conclusion: 所提方法适用于广泛SDEs，无离散化误差，性能优越。

Abstract: Stochastic differential equations (SDEs) are an important class of time-series models, used to describe stochastic systems evolving in continuous time. Simulating paths from these processes, particularly after conditioning on noisy observations of the latent path, remains a challenge. Existing methods often introduce bias through time-discretization, require involved rejection sampling or debiasing schemes or are restricted to a narrow family of diffusions. In this work, we propose an exact Markov chain Monte Carlo (MCMC) sampling algorithm that is applicable to a broad subset of all SDEs with unit diffusion coefficient; after suitable transformation, this includes an even larger class of multivariate SDEs and most 1-d SDEs. We develop a Gibbs sampling framework that allows exact MCMC for such diffusions, without any discretization error. We demonstrate how our MCMC methodology requires only fairly straightforward simulation steps. Our framework can be extended to include parameter simulation, and allows tools from the Gaussian process literature to be easily applied. We evaluate our method on synthetic and real datasets, demonstrating superior performance to particle MCMC approaches.

</details>


### [739] [Complexity bounds for Dirichlet process slice samplers](https://arxiv.org/abs/2602.00878)
*Beatrice Franzolini,Francesco Gaffi*

Main category: stat.CO

TL;DR: 本文获得狄利克雷过程（DP）切片采样器计算复杂度的高概率界，证明切片变量开销相对后验支持的簇数量为$O_\mathbb{P}(\log n)$，为评估DP模型中切片采样的可扩展性建立理论基础。


<details>
  <summary>Details</summary>
Motivation: 先前对基于DP模型的后验切片采样器的可扩展性形式评估研究较少，因切片采样迭代的计算成本随机且可能无界。

Method: 对DP切片采样器的计算复杂度进行理论分析。

Result: 在所有后验簇增长情况下，切片变量产生的开销相对后验支持的簇数量为$O_\mathbb{P}(\log n)$，即使在最坏情况下，每次迭代计算成本超线性增长的概率也趋于零。

Conclusion: 研究结果为评估基于DP模型中切片采样的实际可扩展性奠定了理论基础。

Abstract: Slice sampling is a standard Monte Carlo technique for Dirichlet process (DP)-based models, widely used in posterior simulation. However, formal assessments of the scalability of posterior slice samplers have remained largely unexplored, primarily because the computational cost of a slice-sampling iteration is random and potentially unbounded. In this work, we obtain high-probability bounds on the computational complexity of DP slice samplers. Our main results show that, uniformly across posterior cluster-growth regimes, the overhead induced by slice variables, relatively to the number of clusters supported by the posterior, is $O_{\mathbb P}(\log n)$. As a consequence, even in worst-case configurations, superlinear blow-ups in per-iteration computational cost occur with vanishing probability. Our analysis applies broadly to DP-based models without any likelihood-specific assumptions, still providing complexity guarantees for posterior sampling on arbitrary datasets. These results establish a theoretical foundation for assessing the practical scalability of slice sampling in DP-based models.

</details>


### [740] [A multifidelity approximate Bayesian computation with pre-filtering](https://arxiv.org/abs/2602.01770)
*Xuefei Cao,Shijia Wang,Yongdao Zhou*

Main category: stat.CO

TL;DR: 本文针对ABC方法计算成本高的问题，提出预过滤分层重要性采样算法，证明算法性质，给出评估策略，开发自适应预过滤策略的多保真ABC序贯蒙特卡罗方法，通过实验验证有效性并开发R包。


<details>
  <summary>Details</summary>
Motivation: 解决ABC方法因大量模拟导致的高计算成本问题。

Method: 提出预过滤分层重要性采样算法，开发多保真ABC序贯蒙特卡罗的自适应预过滤策略。

Result: 理论证明算法满足后验集中性，刻画误差上界等，数值实验验证方法有效性，开发R包。

Conclusion: 所提方法能有效解决ABC方法计算成本高的问题。

Abstract: Approximate Bayesian Computation (ABC) methods often require extensive simulations, resulting in high computational costs. This paper focuses on multifidelity simulation models and proposes a pre-filtering hierarchical importance sampling algorithm. Under mild assumptions, we theoretically prove that the proposed algorithm satisfies posterior concentration properties, characterize the error upper bound and the relationship between algorithmic efficiency and pre-filtering criteria. Additionally, we provide a practical strategy to assess the suitability of multifidelity models for the proposed method. Finally, we develop a multifidelity ABC sequential Monte Carlo with adaptive pre-filtering strategy. Numerical experiments are used to demonstrate the effectiveness of the proposed approach. We develop an R package that is available at https://github.com/caofff/MAPS

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [741] [Mapping a Decade of Avian Influenza Research (2014-2023): A Scientometric Analysis from Web of Science](https://arxiv.org/abs/2602.01712)
*Muneer Ahmad,Undie Felicia Nkatv,Amrita Sharma,Gorrety Maria Juma,Nicholas Kamoga,Julirine Nakanwag*

Main category: cs.DL

TL;DR: 对2014 - 2023年禽流感研究进行科学计量分析，呈现研究的多方面趋势、关键信息，强调国际合作的必要性。


<details>
  <summary>Details</summary>
Motivation: 全面了解全球禽流感研究的现状、发展趋势和研究格局 。

Method: 利用Web of Science数据库的文献数据，对发表趋势、来源、作者、合作网络、文献类型和地理分布等进行研究。

Result: 出版物数量稳步增加；中美机构贡献大；特定期刊影响力高；中国科学院和香港大学等机构成果多；中国和美国发文量领先，英美等国国际合作率高；文章是最常见文献类型。

Conclusion: 该研究提供了禽流感研究全球趋势的全面视角，强调了跨国界合作的必要性。

Abstract: This scientometric study analyzes Avian Influenza research from 2014 to 2023 using bibliographic data from the Web of Science database. We examined publication trends, sources, authorship, collaborative networks, document types, and geographical distribution to gain insights into the global research landscape. Results reveal a steady increase in publications, with high contributions from Chinese and American institutions. Journals such as PLoS One and the Journal of Virology published the highest number of studies, indicating their influence in this field. The most prolific institutions include the Chinese Academy of Sciences and the University of Hong Kong, while the College of Veterinary Medicine at South China Agricultural University emerged as the most productive department. China and the USA lead in publication volume, though developed nations like the United Kingdom and Germany exhibit a higher rate of international collaboration. "Articles" are the most common document type, constituting 84.6% of the total, while "Reviews" account for 7.6%. This study provides a comprehensive view of global trends in Avian Influenza research, emphasizing the need for collaborative efforts across borders.

</details>


### [742] [Unmediated AI-Assisted Scholarly Citations](https://arxiv.org/abs/2602.01686)
*Stefan Szeider*

Main category: cs.DL

TL;DR: 提出结合大语言模型自然语言交互与数据库精确访问的架构，以解决语言模型引用不可靠问题，通过MCP-DBLP系统验证其可行性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统文献数据库交互不便，语言模型生成引用存在大量虚构问题，需要一种可靠的文献引用交互方式。

Method: 采用Model Context Protocol架构，结合大语言模型自然语言接口与直接数据库访问，在最终数据导出时绕过语言模型，直接从权威源获取数据。

Result: 以MCP-DBLP系统为例，将基于表单的文献服务转变为能保持学术诚信的对话式助手。

Conclusion: 该架构可应用于其他文献数据库和学术数据源。

Abstract: Traditional bibliography databases require users to navigate search forms and manually copy citation data. Language models offer an alternative: a natural-language interface where researchers write text with informal citation fragments, which are automatically resolved to proper references. However, language models are not reliable for scholarly work as they generate fabricated (hallucinated) citations at substantial rates.
  We present an architectural approach that combines the natural-language interface of LLM chatbots with the accuracy of direct database access, implemented through the Model Context Protocol. Our system enables language models to search bibliographic databases, perform fuzzy matching, and export verified entries, all through conversational interaction.
  A key architectural principle bypasses the language model during final data export: entries are fetched directly from authoritative sources, with timeout protection, to guarantee accuracy. We demonstrate this approach with MCP-DBLP, a server providing access to the DBLP computer science bibliography. The system transforms form-based bibliographic services into conversational assistants that maintain scholarly integrity. This architecture is adaptable to other bibliographic databases and academic data sources.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [743] [JSR-GFNet: Jamming-to-Signal Ratio-Aware Dynamic Gating for Interference Classification in future Cognitive Global Navigation Satellite Systems](https://arxiv.org/abs/2602.00042)
*Zhihan Zeng,Hongyuan Shu,Kaihe Wang,Lu Chen,Amir Hussian,Yanjun Huang,Junchu Zhao,Yue Xiu,Zhongpei Zhang*

Main category: eess.SP

TL;DR: 本文提出JSR - GFNet用于全球导航卫星系统（GNSS）的干扰分类，结合相位敏感样本与频谱图，通过动态门控机制实现自适应融合，实验证明其性能良好。


<details>
  <summary>Details</summary>
Motivation: 传统基于时频分析和卷积神经网络的GNSS干扰分类方法在低干信比（JSR）下性能下降，且存在特征退化问题，本研究聚焦克服这些挑战。

Method: 提出JSR - GFNet多模态架构，结合相位敏感的复值同相/正交（IQ）样本与短时傅里叶变换（STFT）频谱图，采用动态门控机制对复杂值ResNet和EfficientNet骨干网络的贡献进行动态加权。

Result: 引入CGI - 21数据集验证模型，实验表明JSR - GFNet在10 - 50 dB JSR范围内精度更高，可优先进行光谱能量整合并解决调制歧义。

Conclusion: 该框架为下一代航空航天导航安全提供了强大解决方案。

Abstract: The transition toward cognitive global navigation satellite system (GNSS) receivers requires accurate interference classification to trigger adaptive mitigation strategies. However, conventional methods relying on Time-Frequency Analysis (TFA) and Convolutional Neural Networks (CNNs) face two fundamental limitations: severe performance degradation in low Jamming-to-Signal Ratio (JSR) regimes due to noise obscuration, and ``feature degeneracy'' caused by the loss of phase information in magnitude-only spectrograms. Consequently, spectrally similar signals -- such as high-order Quadrature Amplitude Modulation versus Band-Limited Gaussian Noise -- become indistinguishable. To overcome these challenges, this paper proposes the \textbf{JSR-Guided Fusion Network (JSR-GFNet)}. This multi-modal architecture combines phase-sensitive complex In-Phase/Quadrature (IQ) samples with Short-Time Fourier Transform (STFT) spectrograms. Central to this framework is a physics-inspired dynamic gating mechanism driven by statistical signal descriptors. Acting as a conditional controller, it autonomously estimates signal reliability to dynamically reweight the contributions of a Complex-Valued ResNet (IQ stream) and an EfficientNet backbone (STFT stream). To validate the model, we introduce the Comprehensive GNSS Interference (CGI-21) dataset, simulating 21 jamming categories including software-defined waveforms from aerial platforms. Extensive experiments demonstrate that JSR-GFNet achieves higher accuracy across the full 10--50 dB JSR spectrum. Notably, interpretability analysis confirms that the model learns a physically intuitive strategy: prioritizing spectral energy integration in noise-limited regimes while shifting focus to phase precision in high-SNR scenarios to resolve modulation ambiguities. This framework provides a robust solution for next-generation aerospace navigation security.

</details>


### [744] [Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding](https://arxiv.org/abs/2602.02167)
*Soheil Behnam Roudsari,Alexandre S. Brandão,Felipe N. Martins*

Main category: eess.SP

TL;DR: 提出无相机2D激光雷达目标检测管道，在模拟环境表现好且低延迟，可用于嵌入式室内机器人。


<details>
  <summary>Details</summary>
Motivation: 室内服务机器人需要鲁棒、隐私友好且适用于嵌入式硬件的感知方法。

Method: 将三次连续扫描堆叠为RGB通道编码短期时间上下文，作为YOLOv8n输入。

Result: 在160个随机室内场景中mAP@0.5达98.4%，树莓派5上实时运行，端到端延迟47.8ms，比相关方法延迟低。

Conclusion: 轻量级时间编码可实现嵌入式室内机器人准确实时的仅激光雷达检测，无需捕捉RGB外观。

Abstract: Indoor service robots need perception that is robust, more privacy-friendly than RGB video, and feasible on embedded hardware. We present a camera-free 2D LiDAR object detection pipeline that encodes short-term temporal context by stacking three consecutive scans as RGB channels, yielding a compact YOLOv8n input without occupancy-grid construction while preserving angular structure and motion cues. Evaluated in Webots across 160 randomized indoor scenarios with strict scenario-level holdout, the method achieves 98.4% mAP@0.5 (0.778 mAP@0.5:0.95) with 94.9% precision and 94.7% recall on four object classes. On a Raspberry Pi 5, it runs in real time with a mean post-warm-up end-to-end latency of 47.8ms per frame, including scan encoding and postprocessing. Relative to a closely related occupancy-grid LiDAR-YOLO pipeline reported on the same platform, the proposed representation is associated with substantially lower reported end-to-end latency. Although results are simulation-based, they suggest that lightweight temporal encoding can enable accurate and real-time LiDAR-only detection for embedded indoor robotics without capturing RGB appearance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [745] [Evolving Interpretable Constitutions for Multi-Agent Simulation](https://arxiv.org/abs/2602.00755)
*Ujwal Kumar,Alice Saito,Hershraj Niranjani,Rayan Yessou,Phan Xuan Tan*

Main category: cs.MA

TL;DR: 提出Constitutional Evolution框架，在多智能体LLM系统中自动发现行为规范，进化出的宪法C*表现优于人类设计基线。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统带来新的对齐挑战，现有宪法AI聚焦单模型对齐，需研究多智能体系统的行为规范。

Method: 使用有生存压力的网格世界模拟，结合社会稳定分数S量化个体与集体福利，采用LLM驱动的遗传编程和多岛进化来进化宪法。

Result: 对手宪法导致社会崩溃，模糊亲社会原则协调不一致，Claude 4.5 Opus设计的宪法表现中等，进化出的宪法C*社会稳定分数达0.556，消除冲突，发现减少沟通更优。

Conclusion: 合作规范可通过发现而非规定得到，所提出的框架能有效发现多智能体系统的行为规范。

Abstract: Constitutional AI has focused on single-model alignment using fixed principles. However, multi-agent systems create novel alignment challenges through emergent social dynamics. We present Constitutional Evolution, a framework for automatically discovering behavioral norms in multi-agent LLM systems. Using a grid-world simulation with survival pressure, we study the tension between individual and collective welfare, quantified via a Societal Stability Score S in [0,1] that combines productivity, survival, and conflict metrics. Adversarial constitutions lead to societal collapse (S= 0), while vague prosocial principles ("be helpful, harmless, honest") produce inconsistent coordination (S = 0.249). Even constitutions designed by Claude 4.5 Opus with explicit knowledge of the objective achieve only moderate performance (S= 0.332). Using LLM-driven genetic programming with multi-island evolution, we evolve constitutions maximizing social welfare without explicit guidance toward cooperation. The evolved constitution C* achieves S = 0.556 +/- 0.008 (123% higher than human-designed baselines, N = 10), eliminates conflict, and discovers that minimizing communication (0.9% vs 62.2% social actions) outperforms verbose coordination. Our interpretable rules demonstrate that cooperative norms can be discovered rather than prescribed.

</details>


### [746] [Multi-Agent Teams Hold Experts Back](https://arxiv.org/abs/2602.01011)
*Aneesh Pappu,Batu El,Hancheng Cao,Carmelo di Nolfo,Yanchao Sun,Meng Cao,James Zou*

Main category: cs.MA

TL;DR: 研究自组织大语言模型（LLM）团队协作，发现其难达专家个体表现，主要瓶颈是利用专家建议而非识别专家。


<details>
  <summary>Details</summary>
Motivation: 多数先前工作通过固定方式实现协调，研究自组织LLM团队在无约束协调下的表现，探讨是否能实现强协同。

Method: 基于组织心理学，在人类启发和前沿机器学习基准测试中研究自组织LLM团队。

Result: LLM团队无法达到专家个体表现，损失最高达37.6%，主要瓶颈是利用专家建议，存在整合妥协倾向，团队规模增大会加重，与性能负相关，但能提升对抗恶意代理的鲁棒性。

Conclusion: 自组织多智能体团队在利用成员集体专业知识方面存在显著差距。

Abstract: Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows. In such settings, effective coordination cannot be fully designed in advance and must instead emerge through interaction. However, most prior work enforces coordination through fixed roles, workflows, or aggregation rules, leaving open the question of how well self-organizing teams perform when coordination is unconstrained. Drawing on organizational psychology, we study whether self-organizing LLM teams achieve strong synergy, where team performance matches or exceeds the best individual member. Across human-inspired and frontier ML benchmarks, we find that -- unlike human teams -- LLM teams consistently fail to match their expert agent's performance, even when explicitly told who the expert is, incurring performance losses of up to 37.6%. Decomposing this failure, we show that expert leveraging, rather than identification, is the primary bottleneck. Conversational analysis reveals a tendency toward integrative compromise -- averaging expert and non-expert views rather than appropriately weighting expertise -- which increases with team size and correlates negatively with performance. Interestingly, this consensus-seeking behavior improves robustness to adversarial agents, suggesting a trade-off between alignment and effective expertise utilization. Our findings reveal a significant gap in the ability of self-organizing multi-agent teams to harness the collective expertise of their members.

</details>


### [747] [TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.01665)
*Hayeong Lee,JunHyeok Oh,Byung-Jun Lee*

Main category: cs.MA

TL;DR: 介绍了用于可重构多智能体任务的高吞吐量沙箱TABX，可助力多智能体强化学习算法研究。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习算法评估环境缺乏模块化，难以设计自定义评估场景。

Method: 引入基于JAX的高吞吐量沙箱TABX，利用JAX在GPU上进行硬件加速执行，实现大规模并行化。

Result: TABX能对环境参数进行精细控制，减少计算开销，可用于研究复杂结构化领域的多智能体。

Conclusion: TABX是一个快速、可扩展且易于定制的框架，为未来研究提供了可扩展基础。

Abstract: The design of environments plays a critical role in shaping the development and evaluation of cooperative multi-agent reinforcement learning (MARL) algorithms. While existing benchmarks highlight critical challenges, they often lack the modularity required to design custom evaluation scenarios. We introduce the Totally Accelerated Battle Simulator in JAX (TABX), a high-throughput sandbox designed for reconfigurable multi-agent tasks. TABX provides granular control over environmental parameters, permitting a systematic investigation into emergent agent behaviors and algorithmic trade-offs across a diverse spectrum of task complexities. Leveraging JAX for hardware-accelerated execution on GPUs, TABX enables massive parallelization and significantly reduces computational overhead. By providing a fast, extensible, and easily customized framework, TABX facilitates the study of MARL agents in complex structured domains and serves as a scalable foundation for future research. Our code is available at: https://anonymous.4open.science/r/TABX-00CA.

</details>


### [748] [Communications-Incentivized Collaborative Reasoning in NetGPT through Agentic Reinforcement Learning](https://arxiv.org/abs/2602.00766)
*Xiaoxue Yu,Rongpeng Li,Zhifeng Zhao,Honggang Zhang*

Main category: cs.MA

TL;DR: 提出用于AI原生下一代无线网络的NetGPT框架，介绍其结构与训练方法，为自进化网络提供基础架构和方法。


<details>
  <summary>Details</summary>
Motivation: 现有通信系统中AI部署孤立，缺乏内在适应性、动态任务委派和多智能体协作，需新框架。

Method: 提出NetGPT框架，通过智能体通信进行任务委派，用部分可观察条件下的强化学习优化，训练引入多种策略。

Result: NetGPT学习协作时机和方式，平衡内部推理和智能体调用。

Conclusion: 该工作为复杂通信环境中自进化的AI原生下一代无线网络提供了基础架构和训练方法。

Abstract: The evolution of next-Generation (xG) wireless networks marks a paradigm shift from connectivity-centric architectures to Artificial Intelligence (AI)-native designs that tightly integrate data, computing, and communication. Yet existing AI deployments in communication systems remain largely siloed, offering isolated optimizations without intrinsic adaptability, dynamic task delegation, or multi-agent collaboration. In this work, we propose a unified agentic NetGPT framework for AI-native xG networks, wherein a NetGPT core can either perform autonomous reasoning or delegate sub-tasks to domain-specialized agents via agentic communication. The framework establishes clear modular responsibilities and interoperable workflows, enabling scalable, distributed intelligence across the network. To support continual refinement of collaborative reasoning strategies, the framework is further enhanced through Agentic reinforcement learning under partially observable conditions and stochastic external states. The training pipeline incorporates masked loss against external agent uncertainty, entropy-guided exploration, and multi-objective rewards that jointly capture task quality, coordination efficiency, and resource constraints. Through this process, NetGPT learns when and how to collaborate, effectively balancing internal reasoning with agent invocation. Overall, this work provides a foundational architecture and training methodology for self-evolving, AI-native xG networks capable of autonomous sensing, reasoning, and action in complex communication environments.

</details>


### [749] [Self-Evolving Coordination Protocol in Multi-Agent AI Systems: An Exploratory Systems Feasibility Study](https://arxiv.org/abs/2602.02170)
*Jose Manuel de la Chica Rodriguez,Juan Manuel Vera Díaz*

Main category: cs.MA

TL;DR: 本文对自进化协调协议（SECP）进行可行性探索，对比四种协调机制，单次递归修改在保持不变量下增加了提案接受数量，为受治理的多智能体系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当代多智能体系统的内部协调机制在安全关键和受监管领域需满足严格形式要求、可审计且在明确界限内运行，因此研究受限自修改的协调协议。

Method: 研究受控概念验证场景，对比四种协调机制，用提案覆盖率评估结果。

Result: 单次递归修改使提案接受数量从两个增加到三个，且保持所有声明的不变量。

Conclusion: 证明了在明确形式约束下，协调协议的受限自修改技术上可实现、可审计和可分析，为受治理的多智能体系统建立了基础。

Abstract: Contemporary multi-agent systems increasingly rely on internal coordination mechanisms to combine, arbitrate, or constrain the outputs of heterogeneous components. In safety-critical and regulated domains such as finance, these mechanisms must satisfy strict formal requirements, remain auditable, and operate within explicitly bounded limits. Coordination logic therefore functions as a governance layer rather than an optimization heuristic.
  This paper presents an exploratory systems feasibility study of Self-Evolving Coordination Protocols (SECP): coordination protocols that permit limited, externally validated self-modification while preserving fixed formal invariants. We study a controlled proof-of-concept setting in which six fixed Byzantine consensus protocol proposals are evaluated by six specialized decision modules. All coordination regimes operate under identical hard constraints, including Byzantine fault tolerance (f < n/3), O(n2) message complexity, complete non-statistical safety and liveness arguments, and bounded explainability.
  Four coordination regimes are compared in a single-shot design: unanimous hard veto, weighted scalar aggregation, SECP v1.0 (an agent-designed non-scalar protocol), and SECP v2.0 (the result of one governed modification). Outcomes are evaluated using a single metric, proposal coverage, defined as the number of proposals accepted. A single recursive modification increased coverage from two to three accepted proposals while preserving all declared invariants.
  The study makes no claims regarding statistical significance, optimality, convergence, or learning. Its contribution is architectural: it demonstrates that bounded self-modification of coordination protocols is technically implementable, auditable, and analyzable under explicit formal constraints, establishing a foundation for governed multi-agent systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [750] [Sublinear Time Quantum Algorithm for Attention Approximation](https://arxiv.org/abs/2602.00874)
*Zhao Song,Jianfei Xue,Jiahao Zhang,Lichen Zhang*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given the query, key and value matrices $Q, K, V\in \mathbb{R}^{n\times d}$, the attention module is defined as $\mathrm{Att}(Q, K, V)=D^{-1}AV$ where $A=\exp(QK^\top/\sqrt{d})$ with $\exp(\cdot)$ applied entrywise, $D=\mathrm{diag}(A{\bf 1}_n)$. The attention module is the backbone of modern transformers and large language models, but explicitly forming the softmax matrix $D^{-1}A$ incurs $Ω(n^2)$ time, motivating numerous approximation schemes that reduce runtime to $\widetilde O(nd)$ via sparsity or low-rank factorization.
  We propose a quantum data structure that approximates any row of $\mathrm{Att}(Q, K, V)$ using only row queries to $Q, K, V$. Our algorithm preprocesses these matrices in $\widetilde{O}\left( ε^{-1} n^{0.5} \left( s_λ^{2.5} + s_λ^{1.5} d + α^{0.5} d \right) \right)$ time, where $ε$ is the target accuracy, $s_λ$ is the $λ$-statistical dimension of the exponential kernel defined by $Q$ and $K$, and $α$ measures the row distortion of $V$ that is at most $d/{\rm srank}(V)$, the stable rank of $V$. Each row query can be answered in $\widetilde{O}(s_λ^2 + s_λd)$ time.
  To our knowledge, this is the first quantum data structure that approximates rows of the attention matrix in sublinear time with respect to $n$. Our approach relies on a quantum Nyström approximation of the exponential kernel, quantum multivariate mean estimation for computing $D$, and quantum leverage score sampling for the multiplication with $V$.

</details>


### [751] [QSPE: Enumerating Skeletal Quantum Programs for Quantum Library Testing](https://arxiv.org/abs/2602.00024)
*Jiaming Ye,Fuyuan Zhang,Shangzhou Xia,Xiaoyu Guo,Xiongfei Wu,Jianjun Zhao,Yinxing Xue*

Main category: quant-ph

TL;DR: 量子计算发展使量子库出现，但缺乏成熟测试方法。本文提出QSPE方法，能自动化生成多样程序变体、减少冗余，降低误报，发现多库错误，部分被官方认可。


<details>
  <summary>Details</summary>
Motivation: 现有量子库测试工具依赖特定配置和专家知识，存在可访问性和可扩展性问题，且采用测量验证易产生误报。

Method: 提出遵循差分测试原则的QSPE方法，扩展现有SPE方法；采用基于状态向量的验证替代基于测量的验证。

Result: QSPE有效生成22770个程序变体，减少超90%执行成本，检测到708个错误编译，81个被官方认可。

Conclusion: QSPE方法实用有效，可用于量子库测试，减少误报并发现错误。

Abstract: The rapid advancement of quantum computing has led to the development of various quantum libraries, empowering compilation, simulation, and hardware backend interfaces. However, ensuring the correctness of these libraries remains a fundamental challenge due to the lack of mature testing methodologies. The state-of-the-art tools often rely on domain-specific configurations and expert knowledge, which limits their accessibility and scalability in practice. Furthermore, although these tools demonstrate strong performance, they adopt measurement-based for output validation in testing, which makes them produce false positive reports.
  To alleviate these limitations, we propose QSPE, a practical approach that follows the differential testing principle and extends the existing approach, SPE, for quantum libraries. QSPE is fully automated, requiring no pre-set configurations or domain expertise, and can effectively generate a large set of diverse program variants that comprehensively explore the quantum compilation space. To mitigate the possible false positive reports, we propose statevector-based validation as an alternative to measurement-based validation. In our experiments, the QSPE approach demonstrates remarkable effectiveness in generating 22,770 program variants across multiple quantum computing platforms. By avoiding $α$-equivalence at the quantum and classical program wise, QSPE can reduce redundant generation and save more than 90\% of execution cost. Finally, the statevector-based validation method assists QSPE to reduce false alarms and effectively detects 708 miscompilations across multiple quantum libraries. Notably, 81 of the discovered bugs have been officially approved and acknowledged by the Qiskit development team, demonstrating the practical impact of our approach.

</details>


### [752] [Quantum Circuit-Based Learning Models: Bridging Quantum Computing and Machine Learning](https://arxiv.org/abs/2602.00048)
*Fan Fan,Yilei Shi,Mihai Datcu,Bertrand Le Saux,Luigi Iapichino,Francesca Bovolo,Silvia Liberata Ullo,Xiao Xiang Zhu*

Main category: quant-ph

TL;DR: 本文回顾量子电路学习模型用于经典数据分析的研究，探讨量子机器学习潜力、挑战等，为其未来发展提供指导。


<details>
  <summary>Details</summary>
Motivation: 机器学习虽发展好但面临挑战，量子计算或可应对，因此要回顾量子机器学习相关研究，为其发展提供见解和指导。

Method: 聚焦基于核和神经网络的QML模型，结合经典机器学习层的混合框架；综合理论分析和实证结果；探讨抗噪声和硬件高效的QML；介绍先进量子电路设计范式。

Result: 无明确提及研究结果。

Conclusion: 研究为连接量子计算和机器学习的贡献提供概述，为未来发展和广泛应用奠定基础。

Abstract: Machine Learning (ML) has been widely applied across numerous domains due to its ability to automatically identify informative patterns from data for various tasks. The availability of large-scale data and advanced computational power enables the development of sophisticated models and training strategies, leading to state-of-the-art performance, but it also introduces substantial challenges. Quantum Computing (QC), which exploits quantum mechanisms for computation, has attracted growing attention and significant global investment as it may address these challenges. Consequently, Quantum Machine Learning (QML), the integration of these two fields, has received increasing interest, with a notable rise in related studies in recent years. We are motivated to review these existing contributions regarding quantum circuit-based learning models for classical data analysis and highlight the identified potentials and challenges of this technique. Specifically, we focus not only on QML models, both kernel-based and neural network-based, but also on recent explorations of their integration with classical machine learning layers within hybrid frameworks. Moreover, we examine both theoretical analysis and empirical findings to better understand their capabilities, and we also discuss the efforts on noise-resilient and hardware-efficient QML that could enhance its practicality under current hardware limitations. In addition, we cover several emerging paradigms for advanced quantum circuit design and highlight the adaptability of QML across representative application domains. This study aims to provide an overview of the contributions made to bridge quantum computing and machine learning, offering insights and guidance to support its future development and pave the way for broader adoption in the coming years.

</details>


### [753] [Quantum Phase Recognition via Quantum Attention Mechanism](https://arxiv.org/abs/2602.00473)
*Jin-Long Chen,Xin Li,Zhang-Qi Yin*

Main category: quant-ph

TL;DR: 提出混合量子 - 经典注意力模型用于量子相变识别，在集群 - 伊辛模型上验证其高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多体系统量子相变的复杂关联结构给传统方法带来计算挑战。

Method: 提出混合量子 - 经典注意力模型，用交换测试和参数化量子电路实现注意力机制进行相关性提取和基态分类。

Result: 在9和15量子比特的集群 - 伊辛模型上，用少于100个训练数据实现高分类准确率，对训练集变化有鲁棒性。

Conclusion: 该模型能捕捉相变敏感特征和特征物理长度尺度，是复杂多体系统量子相变识别的可扩展且数据高效的方法。

Abstract: Quantum phase transitions in many-body systems are fundamentally characterized by complex correlation structures, which pose computational challenges for conventional methods in large systems. To address this, we propose a hybrid quantum-classical attention model. This model uses an attention mechanism, realized through swap tests and a parameterized quantum circuit, to extract correlations within quantum states and perform ground-state classification. Benchmarked on the cluster-Ising model with system sizes of 9 and 15 qubits, the model achieves high classification accuracy with less than 100 training data and demonstrates robustness against variations in the training set. Further analysis reveals that the model successfully captures phase-sensitive features and characteristic physical length scales, offering a scalable and data-efficient approach for quantum phase recognition in complex many-body systems.

</details>


### [754] [The Quantum Learning Menagerie (A survey on Quantum learning for Classical concepts)](https://arxiv.org/abs/2602.01054)
*Sagnik Chatterjee*

Main category: quant-ph

TL;DR: 本文调研量子学习理论领域成果，聚焦PAC框架下学习量子编码经典概念，整合已知结果并提出23个开放问题。


<details>
  <summary>Details</summary>
Motivation: 梳理量子学习理论领域中在不同标签预言机查询访问学习下经典与量子学习的复杂度分离相关的已知结果，明确研究理解的边界。

Method: 对该领域的各类结果进行调研整合。

Result: 整合了该领域已知结果。

Conclusion: 提出23个开放问题以凸显当前理解的局限。

Abstract: This paper surveys various results in the field of Quantum Learning theory, specifically focusing on learning quantum-encoded classical concepts in the Probably Approximately Correct (PAC) framework. The cornerstone of this work is the emphasis on query, sample, and time complexity separations between classical and quantum learning that emerge under learning with query access to different labeling oracles. This paper aims to consolidate all known results in the area under the above umbrella and underscore the limits of our understanding by leaving the reader with 23 open problems.

</details>


### [755] [Equivalence of Privacy and Stability with Generalization Guarantees in Quantum Learning](https://arxiv.org/abs/2602.01177)
*Ayanava Dasgupta,Naqueeb Ahmad Warsi,Masahito Hayashi*

Main category: quant-ph

TL;DR: 提出统一信息论框架分析差分隐私量子学习算法泛化性能，推导互信息上界，建立稳定性与泛化联系，扩展至不可信数据处理器场景。


<details>
  <summary>Details</summary>
Motivation: 分析差分隐私量子学习算法的泛化性能。

Method: 利用隐私与算法稳定性的联系，推导满足1 - 邻居隐私约束的学习算法互信息上界。

Result: 证明任意(ε, δ)-QDP学习算法的期望泛化误差受隐私诱导稳定性项平方根的约束，引入信息论可容许性概念。

Conclusion: 所提出的框架能有效分析差分隐私量子学习算法泛化性能，且可应用于不可信数据处理器场景。

Abstract: We present a unified information-theoretic framework to analyze the generalization performance of differentially private (DP) quantum learning algorithms. By leveraging the connection between privacy and algorithmic stability, we establish that $(\varepsilon, δ)$-Quantum Differential Privacy (QDP) imposes a strong constraint on the mutual information between the training data and the algorithm's output. We derive a rigorous, mechanism-agnostic upper bound on this mutual information for learning algorithms satisfying a 1-neighbor privacy constraint. Furthermore, we connect this stability guarantee to generalization, proving that the expected generalization error of any $(\varepsilon, δ)$-QDP learning algorithm is bounded by the square root of the privacy-induced stability term. Finally, we extend our framework to the setting of an untrusted Data Processor, introducing the concept of Information-Theoretic Admissibility (ITA) to characterize the fundamental limits of privacy in scenarios where the learning map itself must remain oblivious to the specific dataset instance.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [756] [Regulatory Migration to Europe: ICO Reallocation Following U.S. Securities Enforcement](https://arxiv.org/abs/2602.00138)
*Krishna Sharma,Khemraj Bhatt,Indra Giri*

Main category: q-fin.GN

TL;DR: 研究美国监管澄清对加密资产创业金融跨境溢出的影响，发现ICO活动向欧洲转移。


<details>
  <summary>Details</summary>
Motivation: 探究美国一项重大监管澄清是否伴随加密资产创业金融的跨境溢出效应。

Method: 研究美国证券交易委员会2017年7月的DAO报告，利用2014 - 2021年ICO全球综合数据集构建区域 - 月份面板，评估公告前后的发行动态。

Result: DAO报告发布后，ICO活动大量且持续地向欧洲重新分配，2017年后欧洲每月每个地区平均比其他地区多约14个ICO。

Conclusion: 结果与高流动性数字资产市场中的跨境监管溢出效应一致。

Abstract: This paper examines whether a major U.S. regulatory clarification coincided with cross-border spillovers in crypto-asset entrepreneurial finance. We study the Securities and Exchange Commission's July 2017 DAO Report, which clarified the application of U.S. securities law to many initial coin offerings, and analyze how global issuance activity adjusted across regions. Using a comprehensive global dataset of ICOs from 2014 to 2021, we construct a region-month panel and evaluate issuance dynamics around the announcement. We document a substantial and persistent reallocation of ICO activity toward Europe following the DAO Report. In panel regressions with region and month fixed effects, Europe experiences an average post-2017 increase of approximately 14 additional ICOs per region-month relative to other regions, net of global market cycles. The results are consistent with cross-border regulatory spillovers in highly mobile digital-asset markets.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [757] [Totally $Δ$-Modular Tree Decompositions of Graphic Matrices for Integer Programming](https://arxiv.org/abs/2602.01499)
*Caleb McFarland*

Main category: math.CO

TL;DR: 引入TDM树宽参数，解决有界变量的整数规划问题并给出网格定理类比。


<details>
  <summary>Details</summary>
Motivation: 扩展之前基于图的矩阵分解参数，涵盖每行有两个非零项且元素不在{-1,0,1}中的矩阵。

Method: 引入树分解的TDM树宽参数。

Result: 能解决有界变量且矩阵有界TDM树宽的整数规划问题，给出有界TDM树宽矩阵在有根符号图语言下的网格定理类比。

Conclusion: TDM树宽参数对解决特定矩阵整数规划问题有作用并拓展了相关理论。

Abstract: We introduce the tree-decomposition-based parameter totally $Δ$-modular treewidth (TDM-treewidth) for matrices with two nonzero entries per row. We show how to solve integer programs whose matrices have bounded TDM-treewidth when variables are bounded. This extends previous graph-based decomposition parameters for matrices with at most two nonzero entries per row to include matrices with entries outside of $\{-1,0,1\}$. We also give an analogue of the Grid Theorem of Robertson and Seymour for matrices of bounded TDM-treewidth in the language of rooted signed graphs.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [758] [Exact Instance Compression for Convex Empirical Risk Minimization via Color Refinement](https://arxiv.org/abs/2602.00437)
*Bryan Zhu,Ziang Chen*

Main category: math.OC

TL;DR: 提出基于颜色细化的凸经验风险最小化无损压缩框架，并在多个模型上开发算法，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 经验风险最小化计算成本高，标准求解器在凸环境下扩展性差。

Method: 提出基于颜色细化的凸经验风险最小化无损压缩框架，为多种模型开发具体算法。

Result: 在代表性数据集上开展数值实验。

Conclusion: 所提方法有效。

Abstract: Empirical risk minimization (ERM) can be computationally expensive, with standard solvers scaling poorly even in the convex setting. We propose a novel lossless compression framework for convex ERM based on color refinement, extending prior work from linear programs and convex quadratic programs to a broad class of differentiable convex optimization problems. We develop concrete algorithms for a range of models, including linear and polynomial regression, binary and multiclass logistic regression, regression with elastic-net regularization, and kernel methods such as kernel ridge regression and kernel logistic regression. Numerical experiments on representative datasets demonstrate the effectiveness of the proposed approach.

</details>


### [759] [On the Convergence of Jacobian-Free Backpropagation for Optimal Control Problems with Implicit Hamiltonians](https://arxiv.org/abs/2602.00921)
*Eric Gelphman,Deepanshu Verma,Nicole Tianjiao Yang,Stanley Osher,Samy Wu Fung*

Main category: math.OC

TL;DR: 本文为随机小批量设置下的Jacobian - Free Backpropagation (JFB) 方法建立收敛保证，并展示其在高维问题上的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 基于学习的值函数方法在处理具有隐式哈密顿量的最优反馈控制时，因缺乏闭式最优控制律而面临挑战，现有JFB方法仅建立样本级下降保证，需进一步研究。

Method: 在随机小批量设置下为JFB建立收敛保证。

Result: JFB更新收敛到预期最优控制目标的驻点，且在高维问题（如多智能体最优消费、基于群体的四旋翼和自行车控制）上展示了可扩展性。

Conclusion: 为在具有隐式哈密顿量的高维最优控制中使用JFB提供了理论依据和实证证据。

Abstract: Optimal feedback control with implicit Hamiltonians poses a fundamental challenge for learning-based value function methods due to the absence of closed-form optimal control laws. Recent work~\cite{gelphman2025end} introduced an implicit deep learning approach using Jacobian-Free Backpropagation (JFB) to address this setting, but only established sample-wise descent guarantees. In this paper, we establish convergence guarantees for JFB in the stochastic minibatch setting, showing that the resulting updates converge to stationary points of the expected optimal control objective. We further demonstrate scalability on substantially higher-dimensional problems, including multi-agent optimal consumption and swarm-based quadrotor and bicycle control. Together, our results provide both theoretical justification and empirical evidence for using JFB in high-dimensional optimal control with implicit Hamiltonians.

</details>


### [760] [Robust Sublinear Convergence Rates for Iterative Bregman Projections](https://arxiv.org/abs/2602.01372)
*Gabriel Peyré*

Main category: math.OC

TL;DR: 本文证明熵正则化问题的对偶目标以O(1/k)速率下降，推广了已知结论，还推导了图上Wasserstein - 1距离的flow - Sinkhorn算法。


<details>
  <summary>Details</summary>
Motivation: 将熵最优传输的已知保证扩展到任何线性约束问题，为近似无正则化问题提供有利的复杂度界。

Method: 假设原质量和对偶半径有界，使用块 - 商对偶半范数来衡量对偶半径，进行理论分析。

Result: 证明对偶目标以O(1/k)速率下降，常数仅与1/γ线性相关；推导图上Wasserstein - 1距离的flow - Sinkhorn算法，该算法在O(p/ε⁴)算术运算内达到ε - 加性精度。

Conclusion: 所证明的“鲁棒”速率可扩展熵最优传输的保证到线性约束问题，flow - Sinkhorn算法在图上的运输成本计算中有良好表现。

Abstract: Entropic regularization provides a simple way to approximate linear programs whose constraints split into two (or more) tractable blocks. The resulting objectives are amenable to cyclic Kullback-Leibler (KL) Bregman projections, with the classical Sinkhorn algorithm for optimal transport (balanced, unbalanced, gradient flows, barycenters, \dots) as the canonical example. Assuming uniformly bounded primal mass and dual radius, we prove that the dual objective of these KL projections decreases at an $O(1/k)$ rate with a constant that scales only linearly in $1/γ$, where $γ$ is the entropic regularization parameter. This extends the guarantees known for entropic optimal transport to any such linearly constrained problem. Following the terminology introduced in [Chizat et al 2025], we call such rates "robust", because this mild dependence on $γ$ underpins favorable complexity bounds for approximating the unregularized problem via alternating KL projections. The crucial aspect of the analysis is that the dual radius should be measured according to a block-quotient dual seminorm, which depends on the structure of the split of the constraint into blocks. As an application, we derive the flow-Sinkhorn algorithm for the Wasserstein-1 distance on graphs. It achieves $ε$-additive accuracy on the transshipment cost in $O(p/ε^{4})$ arithmetic operations, where $p$ is the number of edges.

</details>


### [761] [Non-Uniform Noise-to-Signal Ratio in the REINFORCE Policy-Gradient Estimator](https://arxiv.org/abs/2602.01460)
*Haoyu Han,Heng Yang*

Main category: math.OC

TL;DR: 研究策略梯度估计器的噪声信号比（NSR），对特定系统精确刻画NSR，推导一般系统方差上界，发现NSR景观不均匀且接近最优时可能增大导致训练不稳定。


<details>
  <summary>Details</summary>
Motivation: 解决策略梯度方法在强化学习训练中常出现的不稳定或变慢问题。

Method: 通过定义策略梯度估计器的NSR，对特定系统精确刻画NSR，对一般系统推导方差上界。

Result: 对特定系统可精确刻画NSR，对一般系统有方差上界；NSR景观高度不均匀，接近最优时通常增大，某些情况下会导致训练不稳定和策略崩溃。

Conclusion: NSR的变化会影响策略梯度方法训练的稳定性，应关注NSR在优化过程中的变化。

Abstract: Policy-gradient methods are widely used in reinforcement learning, yet training often becomes unstable or slows down as learning progresses. We study this phenomenon through the noise-to-signal ratio (NSR) of a policy-gradient estimator, defined as the estimator variance (noise) normalized by the squared norm of the true gradient (signal). Our main result is that, for (i) finite-horizon linear systems with Gaussian policies and linear state-feedback, and (ii) finite-horizon polynomial systems with Gaussian policies and polynomial feedback, the NSR of the REINFORCE estimator can be characterized exactly-either in closed form or via numerical moment-evaluation algorithms-without approximation. For general nonlinear dynamics and expressive policies (including neural policies), we further derive a general upper bound on the variance. These characterizations enable a direct examination of how NSR varies across policy parameters and how it evolves along optimization trajectories (e.g. SGD and Adam). Across a range of examples, we find that the NSR landscape is highly non-uniform and typically increases as the policy approaches an optimum; in some regimes it blows up, which can trigger training instability and policy collapse.

</details>


### [762] [Well-Posed KL-Regularized Control via Wasserstein and Kalman-Wasserstein KL Divergences](https://arxiv.org/abs/2602.02250)
*Viktor Stein,Adwait Datar,Nihat Ay*

Main category: math.OC

TL;DR: 引入基于(Kalman)-Wasserstein的KL类似物，解决传统KL正则化问题，在控制问题中表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统Kullback - Leibler散度（KL）正则化在支持不匹配时变为无穷大，在低噪声极限下会退化。

Method: 利用统一信息几何框架，用基于传输的几何替换KL动态公式中的Fisher - Rao几何，推导常见分布族的闭式解。

Result: 新的散度在支持不匹配时保持有限，解决了经典KL在高斯过程噪声线性时不变系统中的奇异性问题。在双积分器和推车摆示例中，新控制方法优于基于KL的正则化。

Conclusion: 新的基于(Kalman)-Wasserstein的KL类似物有效，可解决传统KL正则化的问题。

Abstract: Kullback-Leibler divergence (KL) regularization is widely used in reinforcement learning, but it becomes infinite under support mismatch and can degenerate in low-noise limits. Utilizing a unified information-geometric framework, we introduce (Kalman)-Wasserstein-based KL analogues by replacing the Fisher-Rao geometry in the dynamical formulation of the KL with transport-based geometries, and we derive closed-form values for common distribution families. These divergences remain finite under support mismatch and yield a geometric interpretation of regularization heuristics used in Kalman ensemble methods. We demonstrate the utility of these divergences in KL-regularized optimal control. In the fully tractable setting of linear time-invariant systems with Gaussian process noise, the classical KL reduces to a quadratic control penalty that becomes singular as process noise vanishes. Our variants remove this singularity, yielding well-posed problems. On a double integrator and a cart-pole example, the resulting controls outperform KL-based regularization.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [763] [Ultrafast On-chip Online Learning via Spline Locality in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02056)
*Duc Hoang,Aarush Gupta,Philip Harris*

Main category: cs.AR

TL;DR: 本文指出在高频系统中传统MLP不适合超快在线学习，发现KANs特性符合相关约束，在FPGA上实现定点在线训练，证明KANs在线学习器比MLP更高效且有表现力，首次实现亚微秒延迟的无模型在线学习。


<details>
  <summary>Details</summary>
Motivation: 高频系统需要超快在线学习，传统MLPs在低延迟、固定精度计算和严格内存约束下效率低且数值不稳定。

Method: 识别KANs符合约束的关键特性，在FPGA上实现定点在线训练。

Result: KAN-based在线学习器在一系列低延迟和资源受限任务中比MLPs更高效、更具表现力。

Conclusion: 首次实现亚微秒延迟的无模型在线学习。

Abstract: Ultrafast online learning is essential for high-frequency systems, such as controls for quantum computing and nuclear fusion, where adaptation must occur on sub-microsecond timescales. Meeting these requirements demands low-latency, fixed-precision computation under strict memory constraints, a regime in which conventional Multi-Layer Perceptrons (MLPs) are both inefficient and numerically unstable. We identify key properties of Kolmogorov-Arnold Networks (KANs) that align with these constraints. Specifically, we show that: (i) KAN updates exploiting B-spline locality are sparse, enabling superior on-chip resource scaling, and (ii) KANs are inherently robust to fixed-point quantization. By implementing fixed-point online training on Field-Programmable Gate Arrays (FPGAs), a representative platform for on-chip computation, we demonstrate that KAN-based online learners are significantly more efficient and expressive than MLPs across a range of low-latency and resource-constrained tasks. To our knowledge, this work is the first to demonstrate model-free online learning at sub-microsecond latencies.

</details>


### [764] [Exploration of Unary Arithmetic-Based Matrix Multiply Units for Low Precision DL Accelerators](https://arxiv.org/abs/2602.00838)
*Prabhu Vellaisamy,Harideep Nair,Di Wu,Shawn Blanton,John Paul Shen*

Main category: cs.AR

TL;DR: 文章聚焦整数型深度学习推理的一元GEMM设计，评估三种最新方案并与传统二进制GEMM对比，还分析权重稀疏性，展示其在边缘AI加速器中节能计算的潜力。


<details>
  <summary>Details</summary>
Motivation: 深度学习向低精度发展，需要对新的一元和传统二进制GEMM设计进行严格评估，以确定一元硬件在未来深度学习计算中的潜力。

Method: 详细评估三种一元GEMM设计提案uGEMM、tuGEMM和tubGEMM，并与传统二进制GEMM对比，在不同位宽和矩阵大小下进行严格后综合评估，对八个预训练CNN和LLaMA2 LLM进行权重稀疏性分析。

Result: 完成了对一元GEMM设计的评估、分析以及与传统设计对比，评估了不同设计权衡并确定最优方案。

Conclusion: 一元GEMM可在未来边缘AI加速器中有效用于节能计算。

Abstract: General matrix multiplication (GEMM) is a fundamental operation in deep learning (DL). With DL moving increasingly toward low precision, recent works have proposed novel unary GEMM designs as an alternative to conventional binary GEMM hardware. A rigorous evaluation of recent unary and binary GEMM designs is needed to assess the potential of unary hardware for future DL compute. This paper focuses on unary GEMM designs for integer-based DL inference and performs a detailed evaluation of three latest unary design proposals, namely, uGEMM, tuGEMM and tubGEMM, by comparing them to a conventional binary GEMM. Rigorous post-synthesis evaluations beyond prior works are performed across varying bit-widths and matrix sizes to assess the designs' tradeoffs and determine optimal sweetspots. Further, we perform weight sparsity analysis across eight pretrained convolutional neural networks (CNNs) and the LLaMA2 large language model (LLM). In this work, we demonstrate how unary GEMM can be effectively used for energy-efficient compute in future edge AI accelerators.

</details>


### [765] [Position: The Need for Ultrafast Training](https://arxiv.org/abs/2602.02005)
*Duc Hoang*

Main category: cs.AR

TL;DR: 本文主张将FPGA从仅用于推理的加速器转向片上超快学习，可让推理和训练在FPGA中以亚微秒延迟执行，能应用于多个领域，需重新思考算法、架构和工具流。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA加速器多假设离线训练的静态模型，将学习和适应交给较慢的CPU或GPU，限制了非平稳、高频环境下的系统运行。

Method: 提出将推理和训练直接在FPGA架构内以确定性、亚微秒延迟约束执行，实现片上超快学习。

Result: 未提及具体实验结果。

Conclusion: 将学习和推理置于同一实时数据路径可实现闭环系统，需联合重新思考算法、架构和工具流，有望将FPGA从静态推理引擎转变为实时学习机器。

Abstract: Domain-specialized FPGAs have delivered unprecedented performance for low-latency inference across scientific and industrial workloads, yet nearly all existing accelerators assume static models trained offline, relegating learning and adaptation to slower CPUs or GPUs. This separation fundamentally limits systems that must operate in non-stationary, high-frequency environments, where model updates must occur at the timescale of the underlying physics. In this paper, I argue for a shift from inference-only accelerators to ultrafast on-chip learning, in which both inference and training execute directly within the FPGA fabric under deterministic, sub-microsecond latency constraints. Bringing learning into the same real-time datapath as inference would enable closed-loop systems that adapt as fast as the physical processes they control, with applications spanning quantum error correction, cryogenic qubit calibration, plasma and fusion control, accelerator tuning, and autonomous scientific experiments. Enabling such regimes requires rethinking algorithms, architectures, and toolflows jointly, but promises to transform FPGAs from static inference engines into real-time learning machines.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [766] [Asynchronous MultiAgent Reinforcement Learning for 5G Routing under Side Constraints](https://arxiv.org/abs/2602.00035)
*Sebastian Racedo,Brigitte Jaumard,Oscar Delgado,Meysam Masoudi*

Main category: cs.NI

TL;DR: 提出异步多智能体强化学习（AMARL）框架用于分布式路由，在O - RAN网络仿真中表现良好，训练时间减少且鲁棒性增强。


<details>
  <summary>Details</summary>
Motivation: 当前5G及未来系统网络中异构流量的实时路由决策复杂且时间关键，现有方法存在可扩展性和掉队效应问题。

Method: 提出AMARL框架，每个服务对应一个独立PPO智能体并行规划路由，并将资源增量提交到共享全局资源环境。

Result: 与单智能体PPO基线相比，AMARL实现了相似的服务等级（接受率）和端到端延迟，减少了训练时间，提高了对需求变化的鲁棒性。

Conclusion: 异步、服务专业化的智能体为分布式路由提供了可扩展且实用的方法，应用范围可超出O - RAN领域。

Abstract: Networks in the current 5G and beyond systems increasingly carry heterogeneous traffic with diverse quality-of-service constraints, making real-time routing decisions both complex and time-critical. A common approach, such as a heuristic with human intervention or training a single centralized RL policy or synchronizing updates across multiple learners, struggles with scalability and straggler effects. We address this by proposing an asynchronous multi-agent reinforcement learning (AMARL) framework in which independent PPO agents, one per service, plan routes in parallel and commit resource deltas to a shared global resource environment. This coordination by state preserves feasibility across services and enables specialization for service-specific objectives. We evaluate the method on an O-RAN like network simulation using nearly real-time traffic data from the city of Montreal. We compared against a single-agent PPO baseline. AMARL achieves a similar Grade of Service (acceptance rate) (GoS) and end-to-end latency, with reduced training wall-clock time and improved robustness to demand shifts. These results suggest that asynchronous, service-specialized agents provide a scalable and practical approach to distributed routing, with applicability extending beyond the O-RAN domain.

</details>


### [767] [TriCloudEdge: A multi-layer Cloud Continuum](https://arxiv.org/abs/2602.02121)
*George Violettas,Lefteris Mamatas*

Main category: cs.NI

TL;DR: TriCloudEdge是可扩展的三层云连续体，对比不同架构，展示资源利用和通信效率权衡，能解决延迟和隐私问题，还对AI模型适配等进行测试。


<details>
  <summary>Details</summary>
Motivation: 解决不同云层级的计算挑战和延迟需求，平衡资源利用和通信效率，应对实际云连续体的实施挑战。

Method: 提出TriCloudEdge三层云连续体架构，对比不同架构（多协议和单一通用协议）进行实现。

Result: TriCloudEdge可分配计算任务以解决延迟和隐私问题，展示了资源利用和通信效率的权衡。

Conclusion: 该工作为解决不同云层级挑战的实际云连续体实施提供视角，与近期研究进展相符。

Abstract: TriCloudEdge is a scalable three-tier cloud continuum that integrates far-edge devices, intermediate edge nodes, and central cloud services, working in parallel as a unified solution. At the far edge, ultra-low-cost microcontrollers can handle lightweight AI tasks, while intermediate edge devices provide local intelligence, and the cloud tier offers large-scale analytics, federated learning, model adaptation, and global identity management. The proposed architecture enables multi-protocols and technologies (WebSocket, MQTT, HTTP) compared to a versatile protocol (Zenoh) to transfer diverse bidirectional data across the tiers, offering a balance between computational challenges and latency requirements. Comparative implementations between these two architectures demonstrate the trade-offs between resource utilization and communication efficiency. The results show that TriCloudEdge can distribute computational challenges to address latency and privacy concerns. The work also presents tests of AI model adaptation on the far edge and the computational effort challenges under the prism of parallelism. This work offers a perspective on the practical continuum challenges of implementation aligned with recent research advances addressing challenges across the different cloud levels.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [768] [LOGOS-CA: A Cellular Automaton Using Natural Language as State and Rule](https://arxiv.org/abs/2602.00036)
*Keishu Utimula*

Main category: nlin.CG

TL;DR: 研究尝试利用大语言模型的语言表达能力赋能元胞自动机，提出LOGOS - CA框架并进行实验。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在常识推理任务上表现出色，说明语言能细微描述世界，尝试将其高表达能力运用到元胞自动机中，让元胞自动机突破数值状态和固定规则的限制。

Method: 用自然语言表达元胞状态和规则，并委托大语言模型进行更新，提出LOGOS - CA框架开展研究。

Result: LOGOS - CA成功进行简单森林火灾模拟，且从人工生命角度看是有趣的研究对象。

Conclusion: 文中报告实验结果，并讨论了使用LOGOS - CA进行未来研究的方向。

Abstract: Large Language Models (LLMs), trained solely on massive text data, have achieved high performance on the Winograd Schema Challenge (WSC), a benchmark proposed to measure commonsense knowledge and reasoning abilities about the real world. This suggests that the language produced by humanity describes a significant portion of the world with considerable nuance. In this study, we attempt to harness the high expressive power of language within cellular automata. Specifically, we express cell states and rules in natural language and delegate their updates to an LLM. Through this approach, cellular automata can transcend the constraints of merely numerical states and fixed rules, providing us with a richer platform for simulation. Here, we propose LOGOS-CA (Language Oriented Grid Of Statements - Cellular Automaton) as a natural framework to achieve this and examine its capabilities. We confirmed that LOGOS-CA successfully performs simple forest fire simulations and also serves as an intriguing subject for investigation from an Artificial Life (ALife) perspective. In this paper, we report the results of these experiments and discuss directions for future research using LOGOS-CA.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [769] [Implementation Challenges in Quantum Key Distribution](https://arxiv.org/abs/2602.01500)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 研究在实际量子计算环境实现并比较BB84和E91协议，提出用SX门操作生成均匀量子叠加态，在IBM量子平台验证协议可行性。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算技术成熟，聚焦网络通信安全领域，实现并比较知名QKD协议以保障通信方密钥安全。

Method: 在实际量子计算环境实现BB84和E91协议，用SX门操作生成量子叠加态，利用量子特性保证通信安全，在IBM量子平台实验，考虑熵、IID和错误率等指标评估。

Result: 通过实验证明了BB84和E91协议在实际量子硬件上的可行性。

Conclusion: 利用量子叠加和纠缠特性，结合SX门操作生成叠加态并在实际硬件上实验，QKD协议可使通信方安全获取共享密钥，防止敌手拦截。

Abstract: In recent years, quantum computing technologies have steadily matured and have begun to find practical applications across various domains. One important area is network communication security, where Quantum Key Distribution (QKD) enables communicating parties to establish a shared secret that can then be used to generate symmetric keys for subsequent encryption and decryption. This study focuses on implementing and comparing two well-known QKD protocols, namely BB84 and E91, within an actual quantum computing environment. It also proposes the use of SX gate operations to generate uniform quantum superposition states. By leveraging the properties of quantum superposition and quantum entanglement, the study illustrates how communicating parties can securely obtain a shared secret while preventing adversaries from intercepting it. The experiments are conducted using the IBM Quantum Platform to demonstrate the feasibility of the BB84 and E91 protocols on actual quantum hardware. The evaluation considers several metrics, including entropy, Independent and Identically Distributed (IID), and error-rate verifications.

</details>


### [770] [IDEM Enough? Evolving Highly Nonlinear Idempotent Boolean Functions](https://arxiv.org/abs/2602.00837)
*Claude Carlet,Marko Ðurasevic,Domagoj Jakobovic,Luca Mariot,Stjepan Picek*

Main category: cs.CR

TL;DR: 研究用进化方法构造高非线性幂等布尔函数，发现进化问题因操作符有挑战性，可通过轨道上编码真值表强制幂等性。


<details>
  <summary>Details</summary>
Motivation: 幂等布尔函数适合密码设计，但因代数约束，寻找高非线性函数比无约束情况更难。

Method: 用多项式基表示和规范本原多项式，采用进化方法构造维度从5到12的高非线性幂等布尔函数。

Result: 进化幂等函数问题因交叉和变异操作符有破坏性而困难；通过在轨道上编码真值表可强制幂等性。

Conclusion: 提出在轨道上编码真值表的方法来解决进化幂等布尔函数的难题。

Abstract: Idempotent Boolean functions form a highly structured subclass of Boolean functions that is closely related to rotation symmetry under a normal-basis representation and to invariance under a fixed linear map in a polynomial basis. These functions are attractive as candidates for cryptographic design, yet their additional algebraic constraints make the search for high nonlinearity substantially more difficult than in the unconstrained case. In this work, we investigate evolutionary methods for constructing highly nonlinear idempotent Boolean functions for dimensions $n=5$ up to $n=12$ using a polynomial basis representation with canonical primitive polynomials. Our results show that the problem of evolving idempotent functions is difficult due to the disruptive nature of crossover and mutation operators. Next, we show that idempotence can be enforced by encoding the truth table on orbits, yielding a compact genome of size equal to the number of distinct squaring orbits.

</details>


### [771] [FedBGS: A Blockchain Approach to Segment Gossip Learning in Decentralized Systems](https://arxiv.org/abs/2602.01185)
*Fabio Turazza,Marcello Pietri,Marco Picone,Marco Mamei*

Main category: cs.CR

TL;DR: 本文介绍了FedBGS，一个基于区块链的完全去中心化框架，用于优化区块链使用，保障联邦学习环境下的隐私、安全和处理非IID数据。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习中服务器存在单点故障问题，影响安全性和可扩展性，需要新方案解决。

Method: 引入FedBGS，利用基于联邦分析的分段八卦学习。

Result: 未提及明确结果。

Conclusion: 未提及明确结论，但目标是优化区块链使用，抵御各类攻击，保障隐私、安全和处理非IID数据。

Abstract: Privacy-Preserving Federated Learning (PPFL) is a Decentralized machine learning paradigm that enables multiple participants to collaboratively train a global model without sharing their data with the integration of cryptographic and privacy-based techniques to enhance the security of the global system. This privacy-oriented approach makes PPFL a highly suitable solution for training shared models in sectors where data privacy is a critical concern. In traditional FL, local models are trained on edge devices, and only model updates are shared with a central server, which aggregates them to improve the global model. However, despite the presence of the aforementioned privacy techniques, in the classical Federated structure, the issue of the server as a single-point-of-failure remains, leading to limitations both in terms of security and scalability. This paper introduces FedBGS, a fully Decentralized Blockchain-based framework that leverages Segmented Gossip Learning through Federated Analytics. The proposed system aims to optimize blockchain usage while providing comprehensive protection against all types of attacks, ensuring both privacy, security and non-IID data handling in Federated environments.

</details>


### [772] [First Steps, Lasting Impact: Platform-Aware Forensics for the Next Generation of Analysts](https://arxiv.org/abs/2602.00160)
*Vinayak Jain,Sneha Sudhakaran,Saranyan Senthivel*

Main category: cs.CR

TL;DR: 本文分析了Windows和Linux系统在磁盘和内存取证获取技术方面的情况，指出不同系统的特点和挑战，旨在提高证据收集的准确性和可靠性，并指出当前取证工具存在的问题。


<details>
  <summary>Details</summary>
Motivation: 操作系统的差异影响网络取证证据获取的可靠性，传统磁盘取证有局限，需要提高证据收集的准确性和可靠性。

Method: 系统评估Windows和Linux系统的磁盘和内存取证获取技术，确定适合各操作系统的取证工具和配置组合。

Result: 不同操作系统在磁盘和内存取证方面有各自特点和挑战，如Windows磁盘取证受加密影响，Linux磁盘取证受日志保留影响，内存取证虽有工具但仍有平台特定困难。

Conclusion: 当前取证工具存在持续的差距，即无法始终保证取证输入的可靠性和足迹完整性。

Abstract: The reliability of cyber forensic evidence acquisition is strongly influenced by the underlying operating systems, Windows, macOS, and Linux - due to inherent variations in file system structures, encryption protocols, and forensic tool compatibility. Disk forensics, one of the most widely used techniques in digital investigations, faces distinct obstacles on each platform. Windows, with its predominantly NTFS and FAT file systems, typically supports reliable disk imaging and analysis through established tools such as FTK Imager and Autopsy/Sleuth Kit. However, encryption features frequently pose challenges to evidence acquisition. Conversely, Linux environments, which rely on file systems like ext4 and XFS, generally offer greater transparency, yet the transient nature of log retention often complicates forensic analysis. In instances where anti-forensic strategies such as encryption and compression render traditional disk forensics insufficient, memory forensics becomes crucial. While memory forensic methodologies demonstrate robustness across Windows and Linux platforms forms through frameworks like Volatility, platform-specific difficulties persist. Memory analysis on Linux systems benefits from tools like LiME, snapshot utilities, and dd for memory acquisition; nevertheless, live memory acquisition on Linux can still present challenges. This research systematically assesses both disk and memory forensic acquisition techniques across samples representing Windows and Linux systems. By identifying effective combinations of forensic tools and configurations tailored to each operating system, the study aims to improve the accuracy and reliability of evidence collection. It further evaluates current forensic tools and highlights a persistent gap: consistently assuring forensic input reliability and footprint integrity.

</details>


### [773] [Privocracy: Online Democracy through Private Voting](https://arxiv.org/abs/2602.01341)
*Pedro Camponês,Hugo Pereira,Adrian Persaud,Kevin Gallagher,Santiago Torres-Arias*

Main category: cs.CR

TL;DR: 介绍Privocracy访问控制机制，可减少高权限分配，实现投票隐私，有实用功能且实验证明能高效处理投票并可部署在商用硬件上。


<details>
  <summary>Details</summary>
Motivation: 传统访问控制策略中高权限用户易致敏感文件受损，需减少高权限分配以降低系统漏洞。

Method: 采用安全电子投票程序运行需使用敏感资源的命令，实现Privocracy投票机制。

Result: Privocracy能有效处理投票，可部署在商用硬件上。

Conclusion: Privocracy可分散资源访问信任，减少单点故障的系统漏洞，同时保持访问控制策略的灵活性，且投票机制能实现永久隐私并满足系统可靠性要求。

Abstract: In traditional access control policies, every access granted and administrative account introduces an additional vulnerability, as a corruption of a high-privilege user can compromise several sensitive files. Privocracy is an access control mechanism that minimizes the need to attribute high privileges by triggering a secure e-voting procedure to run commands that require using sensitive resources. With Privocracy an organization can distribute trust in resource access, minimizing the system vulnerabilities from single points of failure, all while maintaining the high flexibility of discretionary access control policies.
  The Privocracy voting mechanism achieves everlasting privacy, ensuring votes remain confidential regardless of an adversary's computational power, while addressing the dependability requirements of a practical and secure system. The procedure incorporates useful features such as vote delegation to reduce voter fatigue, rapid voting rounds to enable quick action during emergencies, and selective vote auditing for application-level accountability. Our experimental results demonstrate that Privocracy processes votes efficiently and can be deployed on commodity hardware.

</details>


### [774] [RVDebloater: Mode-based Adaptive Firmware Debloating for Robotic Vehicles](https://arxiv.org/abs/2602.00270)
*Mohsen Salehi,Karthik Pattabiraman*

Main category: cs.CR

TL;DR: 提出RVDebloater自适应去臃肿技术，用于基于模式的嵌入式设备，评估显示有效且开销小。


<details>
  <summary>Details</summary>
Motivation: 嵌入式固件增大攻击面，现有去臃肿技术有局限，需新方法。

Method: 提出RVDebloater，用静态或动态分析识别无用代码，在运行时函数级动态去臃肿，采用新软件执行方法。

Result: 设备各模式需求变化，平均85%函数非必需；去臃肿后任务无失败；平均修剪45%固件调用图；真实RV平均性能开销3.9%，内存开销4%（约0.25MB）。

Conclusion: RVDebloater有效减少攻击面，无误判，开销可接受。

Abstract: As the number of embedded devices grows and their functional requirements increase, embedded firmware is becoming increasingly larger, thereby expanding its attack surface. Despite the increase in firmware size, many embedded devices, such as robotic vehicles (RVs), operate in distinct modes, each requiring only a small subset of the firmware code at runtime. We refer to such devices as mode-based embedded devices. Debloating is an approach to reduce attack surfaces by removing or restricting unneeded code, but existing techniques suffer from significant limitations, such as coarse granularity and irreversible code removal, limiting their applicability.
  To address these limitations, we propose RVDebloater, a novel adaptive debloating technique for mode-based embedded devices that automatically identifies unneeded firmware code for each mode using either static or dynamic analysis, and dynamically debloats the firmware for each mode at the function level at runtime. RVDebloater introduces a new software-based enforcement approach that supports diverse mode-based embedded devices. We implemented RVDebloater using the LLVM compiler and evaluated its efficiency and effectiveness on six different RVs, including both simulated and real ones, with different real-world missions. We find that device requirements change throughout its lifetime for each mode, and that many critical firmware functions can be restricted in other modes, with an average of 85% of functions not being required. The results showed that none of the missions failed after debloating with RVDebloater, indicating that it neither incurred false positives nor false negatives. Further, RVDebloater prunes the firmware call graph by an average of 45% across different firmware. Finally, RVDebloater incurred an average performance overhead of 3.9% and memory overhead of 4% (approximately 0.25 MB) on real RVs.

</details>


### [775] [zkCraft: Prompt-Guided LLM as a Zero-Shot Mutation Pattern Oracle for TCCT-Powered ZK Fuzzing](https://arxiv.org/abs/2602.00667)
*Rong Fu,Jia Yee Tan,Wenxin Zhang,Youjin Wang,Ziyu Kong,Zeli Su,Zhaolu Kang,Shuning Zhang,Xianda Li,Kun Liu,Simon Fong*

Main category: cs.CR

TL;DR: 提出zkCraft框架检测零知识电路语义不一致，评估显示能检测多种故障、减少求解器交互，为ZK电路开发提供可扩展路径。


<details>
  <summary>Details</summary>
Motivation: 零知识电路因见证计算和电路约束紧密耦合难以正确实现，需解决语义不一致问题。

Method: 结合确定性、R1CS感知定位与带证明搜索，将候选约束编辑编码为单Row - Vortex多项式，用Violation IOP替代重复求解器查询，用确定性LLM驱动变异模板引导探索。

Result: 对真实Circom代码评估表明，带证明定位能以低误报率检测多种约束错误，减少昂贵的求解器交互。

Conclusion: 该方法桥接形式验证和自动调试，为稳健的ZK电路开发提供可扩展途径。

Abstract: Zero-knowledge circuits enable privacy-preserving and scalable systems but are difficult to implement correctly due to the tight coupling between witness computation and circuit constraints. We present zkCraft, a practical framework that combines deterministic, R1CS-aware localization with proof-bearing search to detect semantic inconsistencies. zkCraft encodes candidate constraint edits into a single Row-Vortex polynomial and replaces repeated solver queries with a Violation IOP that certifies the existence of edits together with a succinct proof. Deterministic LLM-driven mutation templates bias exploration toward edge cases while preserving auditable algebraic verification. Evaluation on real Circom code shows that proof-bearing localization detects diverse under- and over-constrained faults with low false positives and reduces costly solver interaction. Our approach bridges formal verification and automated debugging, offering a scalable path for robust ZK circuit development.

</details>


### [776] [From Detection to Prevention: Explaining Security-Critical Code to Avoid Vulnerabilities](https://arxiv.org/abs/2602.00711)
*Ranjith Krishnamurthy,Oshando Johnson,Goran Piskachev,Eric Bodden*

Main category: cs.CR

TL;DR: 本文提出通过高亮安全关键功能代码区域并提供安全实现指导来预防漏洞，开发了IntelliJ IDEA插件原型，初步评估有一定效果，为代码级安全指标奠定基础。


<details>
  <summary>Details</summary>
Motivation: 开发中因缺乏安全专业知识和代码复杂易产生安全漏洞，传统工具事后检测成本高，需主动预防策略。

Method: 开发IntelliJ IDEA插件原型，用代码级软件指标识别潜在安全关键方法，用大语言模型生成预防导向解释。

Result: 在Spring - PetClinic应用上的初步评估显示，所选指标能识别多数已知安全关键方法，大语言模型能提供可操作的预防见解。

Conclusion: 虽指标只捕捉安全的结构特性而非语义方面，但为代码级安全感知指标和增强解释奠定基础。

Abstract: Security vulnerabilities often arise unintentionally during development due to a lack of security expertise and code complexity. Traditional tools, such as static and dynamic analysis, detect vulnerabilities only after they are introduced in code, leading to costly remediation. This work explores a proactive strategy to prevent vulnerabilities by highlighting code regions that implement security-critical functionality -- such as data access, authentication, and input handling -- and providing guidance for their secure implementation. We present an IntelliJ IDEA plugin prototype that uses code-level software metrics to identify potentially security-critical methods and large language models (LLMs) to generate prevention-oriented explanations. Our initial evaluation on the Spring-PetClinic application shows that the selected metrics identify most known security-critical methods, while an LLM provides actionable, prevention-focused insights. Although these metrics capture structural properties rather than semantic aspects of security, this work lays the foundation for code-level security-aware metrics and enhanced explanations.

</details>


### [777] [ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models](https://arxiv.org/abs/2602.00154)
*Xiaogeng Liu,Xinyan Wang,Yechao Zhang,Sanjay Kariyappa,Chong Xiang,Muhao Chen,G. Edward Suh,Chaowei Xiao*

Main category: cs.CR

TL;DR: 论文指出大型推理模型存在PI - DoS攻击问题，提出ReasoningBomb框架进行攻击，在多个模型上表现良好，能绕过多种检测。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型因推理计算成本高，存在新的PI - DoS攻击问题，需研究此类攻击。

Method: 先对LRMs推理成本形式化并定义PI - DoS攻击，证明实用攻击应满足三个属性，在此框架下提出基于强化学习的ReasoningBomb框架。

Result: 在多个开源和商业模型上，ReasoningBomb诱导的平均完成和推理令牌数多，优于基线，输入输出放大比高，能绕过多种检测。

Conclusion: ReasoningBomb是一种有效的针对大型推理模型的PI - DoS攻击框架。

Abstract: Large reasoning models (LRMs) extend large language models with explicit multi-step reasoning traces, but this capability introduces a new class of prompt-induced inference-time denial-of-service (PI-DoS) attacks that exploit the high computational cost of reasoning. We first formalize inference cost for LRMs and define PI-DoS, then prove that any practical PI-DoS attack should satisfy three properties: (1) a high amplification ratio, where each query induces a disproportionately long reasoning trace relative to its own length; (ii) stealthiness, in which prompts and responses remain on the natural language manifold and evade distribution shift detectors; and (iii) optimizability, in which the attack supports efficient optimization without being slowed by its own success. Under this framework, we present ReasoningBomb, a reinforcement-learning-based PI-DoS framework that is guided by a constant-time surrogate reward and trains a large reasoning-model attacker to generate short natural prompts that drive victim LRMs into pathologically long and often effectively non-terminating reasoning. Across seven open-source models (including LLMs and LRMs) and three commercial LRMs, ReasoningBomb induces 18,759 completion tokens on average and 19,263 reasoning tokens on average across reasoning models. It outperforms the the runner-up baseline by 35% in completion tokens and 38% in reasoning tokens, while inducing 6-7x more tokens than benign queries and achieving 286.7x input-to-output amplification ratio averaged across all samples. Additionally, our method achieves 99.8% bypass rate on input-based detection, 98.7% on output-based detection, and 98.4% against strict dual-stage joint detection.

</details>


### [778] [EigenAI: Deterministic Inference, Verifiable Results](https://arxiv.org/abs/2602.00182)
*David Ribeiro Alves,Vishnu Patankar,Matheus Pereira,Jamie Stephens,Nima Vaziri,Sreeram Kannan*

Main category: cs.CR

TL;DR: 介绍了基于EigenLayer重质押生态系统构建的可验证AI平台EigenAI，结合LLM推理引擎与加密经济保障的乐观重执行协议，实现推理结果可审计等，并说明架构可产生主权代理。


<details>
  <summary>Details</summary>
Motivation: 构建一个可对AI推理结果进行公开审计、复现和经济执行的平台，提高AI推理的可信度和安全性。

Method: 结合确定性大语言模型推理引擎和加密经济保障的乐观重执行协议，在固定GPU架构上运行推理，在挑战窗口通过EigenVerify请求重执行，在可信执行环境中重新计算结果。

Result: 实现了推理结果可公开审计、复现，单个诚实副本足以检测欺诈，架构可产生主权代理。

Conclusion: EigenAI架构能使主权代理在继承以太坊验证器安全性的同时拥有先进性能。

Abstract: EigenAI is a verifiable AI platform built on top of the EigenLayer restaking ecosystem. At a high level, it combines a deterministic large-language model (LLM) inference engine with a cryptoeconomically secured optimistic re-execution protocol so that every inference result can be publicly audited, reproduced, and, if necessary, economically enforced. An untrusted operator runs inference on a fixed GPU architecture, signs and encrypts the request and response, and publishes the encrypted log to EigenDA. During a challenge window, any watcher may request re-execution through EigenVerify; the result is then deterministically recomputed inside a trusted execution environment (TEE) with a threshold-released decryption key, allowing a public challenge with private data. Because inference itself is bit-exact, verification reduces to a byte-equality check, and a single honest replica suffices to detect fraud. We show how this architecture yields sovereign agents -- prediction-market judges, trading bots, and scientific assistants -- that enjoy state-of-the-art performance while inheriting security from Ethereum's validator base.

</details>


### [779] [Semantic-Aware Advanced Persistent Threat Detection Using Autoencoders on LLM-Encoded System Logs](https://arxiv.org/abs/2602.00204)
*Waleed Khan Mohammed,Zahirul Arief Irfan Bin Shahrul Anuar,Mousa Sufian Mousa Mitani,Hezerul Abdul Karim,Nouar AlDahoul*

Main category: cs.CR

TL;DR: 本文提出利用大语言模型生成语义嵌入的APT检测方法，经DARPA TC数据集评估，表现优于多种基线方法，强调语义理解在检测中的重要性。


<details>
  <summary>Details</summary>
Motivation: 高级持续威胁（APT）检测困难，传统统计方法、浅层机器学习技术及基于溯源的方法存在不足，需要新的检测方法。

Method: 利用预训练的Transformer模型将原始系统日志转换为高维语义嵌入，再用自编码器（AE）分析以识别异常模式。

Result: 在DARPA TC数据集上评估，基于LLM嵌入训练的AE在AUC - ROC指标上优于孤立森林、一类支持向量机和主成分分析等无监督基线方法。

Conclusion: 语义理解在检测非线性和隐蔽攻击行为中很重要，传统检测技术常漏检此类行为。

Abstract: Advanced Persistent Threats (APTs) are among the most challenging cyberattacks to detect. They are carried out by highly skilled attackers who carefully study their targets and operate in a stealthy, long-term manner. Because APTs exhibit "low-and-slow" behavior, traditional statistical methods and shallow machine learning techniques often fail to detect them. Previous research on APT detection has explored machine learning approaches and provenance graph analysis. However, provenance-based methods often fail to capture the semantic intent behind system activities. This paper proposes a novel anomaly detection approach that leverages semantic embeddings generated by Large Language Models (LLMs). The method enhances APT detection by extracting meaningful semantic representations from unstructured system log data. First, raw system logs are transformed into high-dimensional semantic embeddings using a pre-trained transformer model. These embeddings are then analyzed using an Autoencoder (AE) to identify anomalous and potentially malicious patterns. The proposed method is evaluated using the DARPA Transparent Computing (TC) dataset, which contains realistic APT attack scenarios generated by red teams in live environments. Experimental results show that the AE trained on LLM-derived embeddings outperforms widely used unsupervised baseline methods, including Isolation Forest (IForest), One-Class Support Vector Machine (OC-SVM), and Principal Component Analysis (PCA). Performance is measured using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), where the proposed approach consistently achieves superior results, even in complex threat scenarios. These findings highlight the importance of semantic understanding in detecting non-linear and stealthy attack behaviors that are often missed by conventional detection techniques.

</details>


### [780] [TessPay: Verify-then-Pay Infrastructure for Trusted Agentic Commerce](https://arxiv.org/abs/2602.00213)
*Mehul Goenka,Tejas Pathak,Siddharth Asthana*

Main category: cs.CR

TL;DR: 全球经济进入智能代理商业时代，但面临信任鸿沟，为此提出TessPay统一基础设施


<details>
  <summary>Details</summary>
Motivation: 当前系统为人类直接交互设计，智能代理交易三个关键阶段缺乏核心原语，存在信任差距，需统一基础设施

Method: 引入TessPay，采用'先验证后支付'架构，将控制和验证与结算分离，在四个阶段实现信任操作

Result: 未提及具体结果

Conclusion: TessPay可解决智能代理商业中的信任差距问题，提供全交易生命周期的信任操作

Abstract: The global economy is entering the era of Agentic Commerce, where autonomous agents can discover services, negotiate prices, and transact value. However adoption towards agentic commerce faces a foundational trust gap: current systems are built for direct human interactions rather than agent-driven operations. It lacks core primitives across three critical stages of agentic transactions. First, Task Delegation lacks means to translate user intent into defined scopes, discover appropriate agents, and securely authorize actions. Second, Payment Settlement for tasks is processed before execution, lacking verifiable evidence to validate the agent's work. Third, Audit Mechanisms fail to capture the full transaction lifecycle, preventing clear accountability for disputes. While emerging standards address fragments of this trust gap, there still remains a critical need for a unified infrastructure that binds the entire transaction lifecycle.
  To resolve this gap, we introduce TessPay, a unified infrastructure that replaces implicit trust with a 'Verify-then-Pay' architecture. It is a two plane architecture separating control and verification from settlement. TessPay operationalizes trust across four distinct stages: Before execution, agents are anchored in a canonical registry and user intent is captured as verifiable mandates, enabling stakeholder accountability. During execution, funds are locked in escrow while the agent executes the task and generates cryptographic evidence (TLS Notary, TEE etc.) to support Proof of Task Execution (PoTE). At settlement, the system verifies this evidence and releases funds only when the PoTE satisfies verification predicates; modular rail adapters ensure this PoTE-gated escrow remains chain-agnostic across heterogeneous payment rails. After settlement, TessPay preserves a tamper-evident audit trail to enable clear accountability for dispute resolution.

</details>


### [781] [Tri-LLM Cooperative Federated Zero-Shot Intrusion Detection with Semantic Disagreement and Trust-Aware Aggregation](https://arxiv.org/abs/2602.00219)
*Saeid Jamshidi,Omar Abdul Wahab,Foutse Khomh,Kawser Wazed Nafi*

Main category: cs.CR

TL;DR: 介绍语义驱动联邦入侵检测系统框架，可进行开放集和零样本入侵检测，实验表现良好


<details>
  <summary>Details</summary>
Motivation: 现有基于联邦学习的入侵检测系统存在缺乏不确定性估计等问题，且在实际应用中对异构和不可靠客户端的鲁棒性有挑战

Method: 引入语义驱动联邦入侵检测框架，用Tri - LLM集成构建语义攻击原型，建模跨LLM语义分歧用于零日风险估计，采用信任感知聚合机制

Result: 在异构客户端间实现稳定语义对齐和一致收敛，对未知攻击模式零样本检测准确率超80%，零日判别比基于相似度的基线提高超10%，在不可靠或受攻击客户端存在时聚合不稳定性低

Conclusion: 该语义驱动联邦入侵检测框架有效，能提升零日攻击检测能力

Abstract: Federated learning (FL) has become an effective paradigm for privacy-preserving, distributed Intrusion Detection Systems (IDS) in cyber-physical and Internet of Things (IoT) networks, where centralized data aggregation is often infeasible due to privacy and bandwidth constraints. Despite its advantages, most existing FL-based IDS assume closed-set learning and lack mechanisms such as uncertainty estimation, semantic generalization, and explicit modeling of epistemic ambiguity in zero-day attack scenarios. Additionally, robustness to heterogeneous and unreliable clients remains a challenge in practical applications. This paper introduces a semantics-driven federated IDS framework that incorporates language-derived semantic supervision into federated optimization, enabling open-set and zero-shot intrusion detection for previously unseen attack behaviors. The approach constructs semantic attack prototypes using a Tri-LLM ensemble of GPT-4o, DeepSeek-V3, and LLaMA-3-8B, aligning distributed telemetry features with high-level attack concepts. Inter-LLM semantic disagreement is modeled as epistemic uncertainty for zero-day risk estimation, while a trust-aware aggregation mechanism dynamically weights client updates based on reliability. Experimental results show stable semantic alignment across heterogeneous clients and consistent convergence. The framework achieves over 80% zero-shot detection accuracy on unseen attack patterns, improving zero-day discrimination by more than 10% compared to similarity-based baselines, while maintaining low aggregation instability in the presence of unreliable or compromised clients.

</details>


### [782] [Semantics-Preserving Evasion of LLM Vulnerability Detectors](https://arxiv.org/abs/2602.00305)
*Luze Sun,Alina Oprea,Eric Wong*

Main category: cs.CR

TL;DR: 评估基于大语言模型的漏洞检测器在保留行为编辑下的抗逃避能力，发现其易受低成本、语义保留逃避攻击，提出基于载体的指标用于评估。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的漏洞检测器在保留行为编辑下的抗逃避能力尚不明确，需进行评估。

Method: 在统一的C/C++基准上实例化多样的保留行为代码转换，引入跨不同攻击方法/载体的联合鲁棒性指标。

Result: 语义不变对抗变换存在系统性失败，即使是最先进的检测器在等价编辑下预测也会翻转；单一代替模型优化的通用对抗字符串可转移到黑盒API，梯度访问可提高逃避成功率。

Conclusion: 即便高性能的检测器也易受低成本、语义保留的逃避攻击，基于载体的指标可用于评估基于大语言模型的代码检测器。

Abstract: LLM-based vulnerability detectors are increasingly deployed in security-critical code review, yet their resilience to evasion under behavior-preserving edits remains poorly understood. We evaluate detection-time integrity under a semantics-preserving threat model by instantiating diverse behavior-preserving code transformations on a unified C/C++ benchmark (N=5000), and introduce a metric of joint robustness across different attack methods/carriers. Across models, we observe a systemic failure of semantic invariant adversarial transformations: even state-of-the-art vulnerability detectors perform well on clean inputs while predictions flip under behavior-equivalent edits. Universal adversarial strings optimized on a single surrogate model remain effective when transferred to black-box APIs, and gradient access can further amplify evasion success. These results show that even high-performing detectors are vulnerable to low-cost, semantics-preserving evasion. Our carrier-based metrics provide practical diagnostics for evaluating LLM-based code detectors.

</details>


### [783] [Bypassing Prompt Injection Detectors through Evasive Injections](https://arxiv.org/abs/2602.00750)
*Md Jahedur Rahman,Ihsen Alouani*

Main category: cs.CR

TL;DR: 研究评估基于激活增量的任务漂移检测器对对抗后缀的鲁棒性，生成通用后缀攻击，实验显示检测器易受攻击，还提出有效防御技术。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在交互式和检索增强系统中易出现任务漂移，现有基于激活增量的检测器需评估对对抗后缀的鲁棒性。

Method: 生成通用后缀对多个探测器进行攻击实验，提出生成多个后缀随机附加到提示并训练逻辑回归模型的防御技术。

Result: 在Phi - 3 3.8B和Llama - 3 8B上攻击成功率高，提出的防御技术有效。

Conclusion: 基于激活增量的任务漂移检测器易受对抗后缀攻击，需更强防御措施，提出的防御技术有效。

Abstract: Large language models (LLMs) are increasingly used in interactive and retrieval-augmented systems, but they remain vulnerable to task drift; deviations from a user's intended instruction due to injected secondary prompts. Recent work has shown that linear probes trained on activation deltas of LLMs' hidden layers can effectively detect such drift. In this paper, we evaluate the robustness of these detectors against adversarially optimised suffixes. We generate universal suffixes that cause poisoned inputs to evade detection across multiple probes simultaneously. Our experiments on Phi-3 3.8B and Llama-3 8B show that a single suffix can achieve high attack success rates; up to 93.91% and 99.63%, respectively, when all probes must be fooled, and nearly perfect success (>90%) under majority vote setting. These results demonstrate that activation delta-based task drift detectors are highly vulnerable to adversarial suffixes, highlighting the need for stronger defences against adaptive attacks. We also propose a defence technique where we generate multiple suffixes and randomly append one of them to the prompts while making forward passes of the LLM and train logistic regression models with these activations. We found this approach to be highly effective against such attacks.

</details>


### [784] [GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability](https://arxiv.org/abs/2602.00979)
*Xueyi Li,Zhuoneng Zhou,Zitao Liu,Yongdong Wu,Weiqi Luo*

Main category: cs.CR

TL;DR: 本文提出GradingAttack框架评估基于大语言模型的自动简答题评分（ASAG）模型的漏洞，设计两种攻击策略并提出评估指标，实验证明策略有效，强调需加强防御。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于自动简答题评分虽有潜力，但易受对抗性操纵，影响评分公平性和可靠性，需评估其漏洞。

Method: 引入GradingAttack框架，设计令牌级和提示级攻击策略，并提出衡量攻击伪装性的评估指标。

Result: 在多个数据集上实验表明，两种攻击策略能有效误导评分模型，提示级攻击成功率高，令牌级攻击伪装性好。

Conclusion: 强调需要强大的防御机制，以确保自动简答题评分的公平性和可靠性。

Abstract: Large language models (LLMs) have demonstrated remarkable potential for automatic short answer grading (ASAG), significantly boosting student assessment efficiency and scalability in educational scenarios. However, their vulnerability to adversarial manipulation raises critical concerns about automatic grading fairness and reliability. In this paper, we introduce GradingAttack, a fine-grained adversarial attack framework that systematically evaluates the vulnerability of LLM based ASAG models. Specifically, we align general-purpose attack methods with the specific objectives of ASAG by designing token-level and prompt-level strategies that manipulate grading outcomes while maintaining high camouflage. Furthermore, to quantify attack camouflage, we propose a novel evaluation metric that balances attack success and camouflage. Experiments on multiple datasets demonstrate that both attack strategies effectively mislead grading models, with prompt-level attacks achieving higher success rates and token-level attacks exhibiting superior camouflage capability. Our findings underscore the need for robust defenses to ensure fairness and reliability in ASAG. Our code and datasets are available at https://anonymous.4open.science/r/GradingAttack.

</details>


### [785] [TxRay: Agentic Postmortem of Live Blockchain Attacks](https://arxiv.org/abs/2602.01317)
*Ziyue Wang,Jiangshan Yu,Kaihua Qin,Dawn Song,Arthur Gervais,Liyi Zhou*

Main category: cs.CR

TL;DR: 介绍DeFi生态因漏洞损失巨大，提出TxRay系统用于从有限证据重建ACT攻击，经过评估效果良好，且在实时部署中表现高效。


<details>
  <summary>Details</summary>
Motivation: DeFi生态因ACT机会出现大量安全漏洞损失惨重，且事后分析缓慢且依赖手动。

Method: 提出TxRay系统，利用工具调用从有限证据重建攻击，恢复攻击生命周期，推导根本原因，生成可运行的PoC，并通过编码语义预言机进行自检；开发PoCEvaluator评估PoC。

Result: 在114个事件中，为105个事件生成专家认可根源和可执行PoC，端到端重现率92.11%；98.1%的PoC避免硬编码，比DeFiHackLabs提升24.8个百分点；实时部署中位数延迟分别为40分钟和59分钟；提高攻击模仿覆盖率。

Conclusion: TxRay系统能有效从有限证据重建DeFi攻击，生成高质量PoC，在安全分析中有较高效率和实用性。

Abstract: Decentralized Finance (DeFi) has turned blockchains into financial infrastructure, allowing anyone to trade, lend, and build protocols without intermediaries, but this openness exposes pools of value controlled by code. Within five years, the DeFi ecosystem has lost over 15.75B USD to reported exploits. Many exploits arise from permissionless opportunities that any participant can trigger using only public state and standard interfaces, which we call Anyone-Can-Take (ACT) opportunities. Despite on-chain transparency, postmortem analysis remains slow and manual: investigations start from limited evidence, sometimes only a single transaction hash, and must reconstruct the exploit lifecycle by recovering related transactions, contract code, and state dependencies.
  We present TxRay, a Large Language Model (LLM) agentic postmortem system that uses tool calls to reconstruct live ACT attacks from limited evidence. Starting from one or more seed transactions, TxRay recovers the exploit lifecycle, derives an evidence-backed root cause, and generates a runnable, self-contained Proof of Concept (PoC) that deterministically reproduces the incident. TxRay self-checks postmortems by encoding incident-specific semantic oracles as executable assertions.
  To evaluate PoC correctness and quality, we develop PoCEvaluator, an independent agentic execution-and-review evaluator. On 114 incidents from DeFiHackLabs, TxRay produces an expert-aligned root cause and an executable PoC for 105 incidents, achieving 92.11% end-to-end reproduction. Under PoCEvaluator, 98.1% of TxRay PoCs avoid hard-coding attacker addresses, a +24.8pp lift over DeFiHackLabs. In a live deployment, TxRay delivers validated root causes in 40 minutes and PoCs in 59 minutes at median latency. TxRay's oracle-validated PoCs enable attack imitation, improving coverage by 15.6% and 65.5% over STING and APE.

</details>


### [786] [Adaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization](https://arxiv.org/abs/2602.01342)
*Poushali Sengupta,Mayank Raikwar,Sabita Maharjan,Frank Eliassen,Yan Zhang*

Main category: cs.CR

TL;DR: 本文提出自适应后量子密码框架，结合安全单调升级协议保障6G车联网量子安全，实验表明降低延迟、开销，防攻击效果好。


<details>
  <summary>Details</summary>
Motivation: 未来量子计算机可能破解车联网安全，后量子密码方法有性能挑战，需新框架保障6G车联网安全与性能。

Method: 提出自适应后量子密码（PQC）框架，预测短期移动性和信道变化，用预测多目标进化算法（APMOEA）动态选PQC配置，用安全单调升级协议防攻击。

Result: 理论上证明在预测误差、移动漂移和小预测噪声下的结果；实验显示框架降低端到端延迟和通信开销，稳定密码切换，协议防住攻击。

Conclusion: 该研究为未来6G车联网量子安全密码提供实用路径。

Abstract: Powerful quantum computers in the future may be able to break the security used for communication between vehicles and other devices (Vehicle-to-Everything, or V2X). New security methods called post-quantum cryptography can help protect these systems, but they often require more computing power and can slow down communication, posing a challenge for fast 6G vehicle networks. In this paper, we propose an adaptive post-quantum cryptography (PQC) framework that predicts short-term mobility and channel variations and dynamically selects suitable lattice-, code-, or hash-based PQC configurations using a predictive multi-objective evolutionary algorithm (APMOEA) to meet vehicular latency and security constraints.However, frequent cryptographic reconfiguration in dynamic vehicular environments introduces new attack surfaces during algorithm transitions. A secure monotonic-upgrade protocol prevents downgrade, replay, and desynchronization attacks during transitions. Theoretical results show decision stability under bounded prediction error, latency boundedness under mobility drift, and correctness under small forecast noise. These results demonstrate a practical path toward quantum-safe cryptography in future 6G vehicular networks. Through extensive experiments based on realistic mobility (LuST), weather (ERA5), and NR-V2X channel traces, we show that the proposed framework reduces end-to-end latency by up to 27\%, lowers communication overhead by up to 65\%, and effectively stabilizes cryptographic switching behavior using reinforcement learning. Moreover, under the evaluated adversarial scenarios, the monotonic-upgrade protocol successfully prevents downgrade, replay, and desynchronization attacks.

</details>


### [787] [CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses](https://arxiv.org/abs/2602.01438)
*Max Manolov,Tony Gao,Siddharth Shukla,Cheng-Ting Chou,Ryan Lagasse*

Main category: cs.CR

TL;DR: 介绍CIPHER基准衡量大语言模型生成Python代码的加密漏洞发生率，发现‘安全’提示不能完全消除漏洞，基准和评分管道将公开。


<details>
  <summary>Details</summary>
Motivation: 大语言模型辅助代码时加密功能实现常有可利用漏洞，需要衡量其加密漏洞发生率。

Method: 引入CIPHER基准，采用不同提示变体、加密特定漏洞分类法和自动评分管道。

Result: 明确‘安全’提示能减少部分目标问题，但不能可靠消除整体加密漏洞。

Conclusion: CIPHER基准和可复现评分管道可用于评估大语言模型生成代码的加密安全性，相关资源将公开。

Abstract: Large language models (LLMs) are increasingly used to assist developers with code, yet their implementations of cryptographic functionality often contain exploitable flaws. Minor design choices (e.g., static initialization vectors or missing authentication) can silently invalidate security guarantees. We introduce CIPHER(\textbf{C}ryptographic \textbf{I}nsecurity \textbf{P}rofiling via \textbf{H}ybrid \textbf{E}valuation of \textbf{R}esponses), a benchmark for measuring cryptographic vulnerability incidence in LLM-generated Python code under controlled security-guidance conditions. CIPHER uses insecure/neutral/secure prompt variants per task, a cryptography-specific vulnerability taxonomy, and line-level attribution via an automated scoring pipeline. Across a diverse set of widely used LLMs, we find that explicit ``secure'' prompting reduces some targeted issues but does not reliably eliminate cryptographic vulnerabilities overall. The benchmark and reproducible scoring pipeline will be publicly released upon publication.

</details>


### [788] [Comparison of Multiple Classifiers for Android Malware Detection with Emphasis on Feature Insights Using CICMalDroid 2020 Dataset](https://arxiv.org/abs/2602.00058)
*Md Min-Ha-Zul Abedin,Tazqia Mehrub*

Main category: cs.CR

TL;DR: 本文利用CICMalDroid2020数据集构建安卓恶意软件检测器，评估多种分类器，发现原始特征上梯度提升效果最佳，为检测建立高保真监督基线。


<details>
  <summary>Details</summary>
Motivation: 签名扫描器难以跟上应用商店的快速更新，需要构建可靠的安卓恶意软件检测器并找出决策的可解释驱动因素。

Method: 使用CICMalDroid2020数据集，提取静态和动态特征组成混合向量，在三种方案下评估七种分类器。

Result: 原始特征上梯度提升表现最佳，PCA会降低模型性能，LDA保持90%以上准确率，深度为2的代理树指出关键驱动因素。

Conclusion: 研究为安卓恶意软件检测建立了高保真监督基线，丰富的混合特征结合梯度提升方法为部署提供了实用且可解释的基础。

Abstract: Accurate Android malware detection was critical for protecting users at scale. Signature scanners lagged behind fast release cycles on public app stores. We aimed to build a trustworthy detector by pairing a comprehensive dataset with a rigorous, transparent evaluation, and to identify interpretable drivers of decisions. We used CICMalDroid2020, which contained 17,341 apps across Benign, Adware, Banking, SMS malware, and Riskware. We extracted 301 static and 263 dynamic features into a 564 dimensional hybrid vector, then evaluated seven classifiers under three schemes, original features, principal component analysis, PCA, and linear discriminant analysis, LDA, with a 70 percent training and 30 percent test split. Results showed that gradient boosting on the original features performed best. XGBoost achieved 0.9747 accuracy, 0.9703 precision, 0.9731 recall, and 0.9716 F1, and the confusion matrix indicated rare benign labels for malicious apps. HistGradientBoosting reached 0.9741 accuracy and 0.9708 F1, while CatBoost and Random Forest were slightly lower at 0.9678 and 0.9687 accuracy with 0.9636 and 0.9637 F1. KNN and SVM lagged. PCA reduced performance for all models, with XGBoost dropping to 0.9164 accuracy and 0.8988 F1. LDA maintained mid 90s accuracy and clarified separable clusters in projections. A depth two surrogate tree highlighted package name, main activity, and target SDK as key drivers. These findings established high fidelity supervised baselines for Android malware detection and indicated that rich hybrid features with gradient boosting offered a practical and interpretable foundation for deployment.

</details>


### [789] [RPP: A Certified Poisoned-Sample Detection Framework for Backdoor Attacks under Dataset Imbalance](https://arxiv.org/abs/2602.00183)
*Miao Lin,Feng Yu,Rui Ning,Lusi Li,Jiawei Chen,Qian Lou,Mengxin Zheng,Chunsheng Xin,Hongyi Wu*

Main category: cs.CR

TL;DR: 本文研究数据集不平衡如何放大后门攻击漏洞，提出随机概率扰动（RPP）框架在黑盒设置下检测中毒样本，实验表明RPP在不平衡数据集上检测准确率高于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击防御方法多依赖平衡数据，忽视现实场景中普遍存在的类别不平衡会放大后门威胁。

Method: 提出随机概率扰动（RPP），一个仅使用模型输出概率、在黑盒设置下检测中毒样本的框架。

Result: 在五个基准数据集上针对10种后门攻击和12种基线防御的实验表明，RPP尤其在数据集不平衡时检测准确率显著高于现有防御方法。

Conclusion: RPP为在不平衡数据的现实环境中防御后门攻击奠定了理论和实践基础。

Abstract: Deep neural networks are highly susceptible to backdoor attacks, yet most defense methods to date rely on balanced data, overlooking the pervasive class imbalance in real-world scenarios that can amplify backdoor threats. This paper presents the first in-depth investigation of how the dataset imbalance amplifies backdoor vulnerability, showing that (i) the imbalance induces a majority-class bias that increases susceptibility and (ii) conventional defenses degrade significantly as the imbalance grows. To address this, we propose Randomized Probability Perturbation (RPP), a certified poisoned-sample detection framework that operates in a black-box setting using only model output probabilities. For any inspected sample, RPP determines whether the input has been backdoor-manipulated, while offering provable within-domain detectability guarantees and a probabilistic upper bound on the false positive rate. Extensive experiments on five benchmarks (MNIST, SVHN, CIFAR-10, TinyImageNet and ImageNet10) covering 10 backdoor attacks and 12 baseline defenses show that RPP achieves significantly higher detection accuracy than state-of-the-art defenses, particularly under dataset imbalance. RPP establishes a theoretical and practical foundation for defending against backdoor attacks in real-world environments with imbalanced data.

</details>


### [790] [Backdoor Sentinel: Detecting and Detoxifying Backdoors in Diffusion Models via Temporal Noise Consistency](https://arxiv.org/abs/2602.01765)
*Bingzheng Wang,Xiaoyan Gu,Hongbo Xu,Hongcheng Li,Zimo Yu,Jiang Zhou,Weiping Wang*

Main category: cs.CR

TL;DR: 本文针对扩散模型后门检测与解毒难题，发现时间噪声不一致现象，提出TNC - Defense框架，经实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型依赖不透明数据和过程，易受后门攻击，现有检测和解毒方法在实际审计场景中存在局限。

Method: 发现时间噪声不一致现象，提出TNC - Defense框架，包括基于相邻时间步噪声一致性的检测模块和基于异常时间步的解毒模块。

Result: 在五个代表性后门攻击场景下，TNC - Defense平均检测准确率提高11%，能使98.5%触发样本失效，生成质量仅有轻微下降。

Conclusion: TNC - Defense能有效抑制后门行为，显著降低解毒成本。

Abstract: Diffusion models have been widely deployed in AIGC services; however, their reliance on opaque training data and procedures exposes a broad attack surface for backdoor injection. In practical auditing scenarios, due to the protection of intellectual property and commercial confidentiality, auditors are typically unable to access model parameters, rendering existing white-box or query-intensive detection methods impractical. More importantly, even after the backdoor is detected, existing detoxification approaches are often trapped in a dilemma between detoxification effectiveness and generation quality.
  In this work, we identify a previously unreported phenomenon called temporal noise unconsistency, where the noise predictions between adjacent diffusion timesteps is disrupted in specific temporal segments when the input is triggered, while remaining stable under clean inputs. Leveraging this finding, we propose Temporal Noise Consistency Defense (TNC-Defense), a unified framework for backdoor detection and detoxification. The framework first uses the adjacent timestep noise consistency to design a gray-box detection module, for identifying and locating anomalous diffusion timesteps. Furthermore, the framework uses the identified anomalous timesteps to construct a trigger-agnostic, timestep-aware detoxification module, which directly corrects the backdoor generation path. This effectively suppresses backdoor behavior while significantly reducing detoxification costs.
  We evaluate the proposed method under five representative backdoor attack scenarios and compare it with state-of-the-art defenses. The results show that TNC-Defense improves the average detection accuracy by $11\%$ with negligible additional overhead, and invalidates an average of $98.5\%$ of triggered samples with only a mild degradation in generation quality.

</details>


### [791] [RedVisor: Reasoning-Aware Prompt Injection Defense via Zero-Copy KV Cache Reuse](https://arxiv.org/abs/2602.01795)
*Mingrui Liu,Sixiao Zhang,Cheng Long,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: 提出RedVisor框架应对大语言模型的提示注入攻击，结合检测与预防策略，实验显示在检测准确率和吞吐量上优于现有防御方法且效用损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 当前针对大语言模型提示注入攻击的防御方法存在权衡问题，预防式微调有“对齐税”，检测式过滤有高延迟和高内存成本。

Method: 提出RedVisor统一框架，通过轻量级可移除适配器生成可解释分析定位攻击，在推理阶段激活，响应生成阶段静音，还将防御集成到vLLM服务引擎。

Result: RedVisor在检测准确率和吞吐量上优于现有防御方法，且效用损失可忽略。

Conclusion: RedVisor是一种有效应对提示注入攻击的方法，能平衡防御效果和模型效用。

Abstract: Large Language Models (LLMs) are increasingly vulnerable to Prompt Injection (PI) attacks, where adversarial instructions hidden within retrieved contexts hijack the model's execution flow. Current defenses typically face a critical trade-off: prevention-based fine-tuning often degrades general utility via the "alignment tax", while detection-based filtering incurs prohibitive latency and memory costs. To bridge this gap, we propose RedVisor, a unified framework that synthesizes the explainability of detection systems with the seamless integration of prevention strategies. To the best of our knowledge, RedVisor is the first approach to leverage fine-grained reasoning paths to simultaneously detect attacks and guide the model's safe response. We implement this via a lightweight, removable adapter positioned atop the frozen backbone. This adapter serves a dual function: it first generates an explainable analysis that precisely localizes the injection and articulates the threat, which then explicitly conditions the model to reject the malicious command. Uniquely, the adapter is active only during this reasoning phase and is effectively muted during the subsequent response generation. This architecture yields two distinct advantages: (1) it mathematically preserves the backbone's original utility on benign inputs; and (2) it enables a novel KV Cache Reuse strategy, eliminating the redundant prefill computation inherent to decoupled pipelines. We further pioneer the integration of this defense into the vLLM serving engine with custom kernels. Experiments demonstrate that RedVisor outperforms state-of-the-art defenses in detection accuracy and throughput while incurring negligible utility loss.

</details>


### [792] [Human Society-Inspired Approaches to Agentic AI Security: The 4C Framework](https://arxiv.org/abs/2602.01942)
*Alsharif Abuadbba,Nazatul Sultan,Surya Nepal,Sanjay Jha*

Main category: cs.CR

TL;DR: AI发展使网络安全风险变化，现有系统中心方法有局限，文章提出4C框架用于多智能体AI安全，补充现有策略。


<details>
  <summary>Details</summary>
Motivation: AI从特定领域走向开放环境，导致网络安全风险变化，现有系统中心方法无法捕捉新风险。

Method: 受社会治理启发，提出4C框架，从核心、连接、认知和合规四个相互依存维度组织代理风险。

Result: 框架将AI安全关注点从系统中心保护扩展到行为完整性和意图保护。

Conclusion: 框架补充现有AI安全策略，为构建可信、可治理和符合人类价值观的代理AI系统提供原则基础。

Abstract: AI is moving from domain-specific autonomy in closed, predictable settings to large-language-model-driven agents that plan and act in open, cross-organizational environments. As a result, the cybersecurity risk landscape is changing in fundamental ways. Agentic AI systems can plan, act, collaborate, and persist over time, functioning as participants in complex socio-technical ecosystems rather than as isolated software components. Although recent work has strengthened defenses against model and pipeline level vulnerabilities such as prompt injection, data poisoning, and tool misuse, these system centric approaches may fail to capture risks that arise from autonomy, interaction, and emergent behavior. This article introduces the 4C Framework for multi-agent AI security, inspired by societal governance. It organizes agentic risks across four interdependent dimensions: Core (system, infrastructure, and environmental integrity), Connection (communication, coordination, and trust), Cognition (belief, goal, and reasoning integrity), and Compliance (ethical, legal, and institutional governance). By shifting AI security from a narrow focus on system-centric protection to the broader preservation of behavioral integrity and intent, the framework complements existing AI security strategies and offers a principled foundation for building agentic AI systems that are trustworthy, governable, and aligned with human values.

</details>


### [793] [Malware Detection Through Memory Analysis](https://arxiv.org/abs/2602.02184)
*Sarah Nassar*

Main category: cs.CR

TL;DR: 本文基于MalMemAnalysis - 2022数据集，探究机器学习技术在恶意软件分类中的有效性与效率，选用XGBoost模型并获良好效果，助力开发实时恶意软件检测器。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习技术在恶意软件二进制分类和多分类任务中的有效性和效率，开发准确实时的混淆恶意软件检测器以提升网络隐私和安全。

Method: 使用Canadian Institute for Cybersecurity的MalMemAnalysis - 2022数据集，选用XGBoost模型进行二进制和多分类任务。

Result: 二进制分类器测试子集准确率和F1分数达99.98%；多类版本准确率87.54%，F1分数81.26%，恶意软件子类型平均F1分数75.03%；二进制设置中分类50个样本约需37.3毫秒，多类设置约需43.2毫秒。

Conclusion: 研究结果有助于开发准确实时的混淆恶意软件检测器，提升网络隐私和安全。

Abstract: This paper summarizes the research conducted for a malware detection project using the Canadian Institute for Cybersecurity's MalMemAnalysis-2022 dataset. The purpose of the project was to explore the effectiveness and efficiency of machine learning techniques for the task of binary classification (i.e., benign or malicious) as well as multi-class classification to further include three malware sub-types (i.e., benign, ransomware, spyware, or Trojan horse). The XGBoost model type was the final model selected for both tasks due to the trade-off between strong detection capability and fast inference speed. The binary classifier achieved a testing subset accuracy and F1 score of 99.98\%, while the multi-class version reached an accuracy of 87.54\% and an F1 score of 81.26\%, with an average F1 score over the malware sub-types of 75.03\%. In addition to the high modelling performance, XGBoost is also efficient in terms of classification speed. It takes about 37.3 milliseconds to classify 50 samples in sequential order in the binary setting and about 43.2 milliseconds in the multi-class setting. The results from this research project help advance the efforts made towards developing accurate and real-time obfuscated malware detectors for the goal of improving online privacy and safety. *This project was completed as part of ELEC 877 (AI for Cybersecurity) in the Winter 2024 term.

</details>


### [794] [Expected Harm: Rethinking Safety Evaluation of (Mis)Aligned LLMs](https://arxiv.org/abs/2602.01600)
*Yen-Shan Chen,Zhi Rui Tam,Cheng-Kuang Wu,Yun-Nung Chen*

Main category: cs.CR

TL;DR: 本文指出当前大语言模型安全评估方法存在问题，引入预期危害指标，分析发现模型存在逆向风险校准问题，利用该特性可提高攻击成功率，还找出模型失效根源。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型安全评估主要基于严重程度分类法，未考虑执行可能性，需要重新审视。

Method: 引入预期危害指标，该指标结合越狱严重性和执行可能性（建模为执行成本的函数），对现有模型进行实证分析，使用线性探测追溯失败根源。

Result: 发现模型存在逆向风险校准，即对低可能性（高成本）威胁拒绝行为更强，对高可能性（低成本）查询更脆弱；利用此特性可使现有越狱攻击成功率提高达2倍；模型在潜在空间编码严重性但无执行成本的可区分内部表示。

Conclusion: 当前基于严重程度的评估方法有缺陷，模型因缺乏执行成本表示而对风险的一个关键维度“失明”，导致结构脆弱性。

Abstract: Current evaluations of LLM safety predominantly rely on severity-based taxonomies to assess the harmfulness of malicious queries. We argue that this formulation requires re-examination as it assumes uniform risk across all malicious queries, neglecting Execution Likelihood--the conditional probability of a threat being realized given the model's response. In this work, we introduce Expected Harm, a metric that weights the severity of a jailbreak by its execution likelihood, modeled as a function of execution cost. Through empirical analysis of state-of-the-art models, we reveal a systematic Inverse Risk Calibration: models disproportionately exhibit stronger refusal behaviors for low-likelihood (high-cost) threats while remaining vulnerable to high-likelihood (low-cost) queries. We demonstrate that this miscalibration creates a structural vulnerability: by exploiting this property, we increase the attack success rate of existing jailbreaks by up to $2\times$. Finally, we trace the root cause of this failure using linear probing, which reveals that while models encode severity in their latent space to drive refusal decisions, they possess no distinguishable internal representation of execution cost, making them "blind" to this critical dimension of risk.

</details>


### [795] [Efficient Softmax Reformulation for Homomorphic Encryption via Moment Generating Function](https://arxiv.org/abs/2602.01621)
*Hanjun Park,Byeong-Seo Min,Jiheon Woo,Min-Wook Jeong,Jongho Shin,Yongwoo Lee,Young-Sik Kim,Yongjune Kim*

Main category: cs.CR

TL;DR: 提出基于矩生成函数的MGF - softmax重写方法，在加密推理中高效准确近似softmax。


<details>
  <summary>Details</summary>
Motivation: 在同态加密（HE）中评估softmax因多种因素极具挑战，需解决该问题。

Method: 提出基于矩生成函数的MGF - softmax新重写方法，用基于矩的对应项替换softmax分母。

Result: 在视觉Transformer和大语言模型上的实验表明，MGF - softmax能在加密推理中高效准确近似softmax，推理精度接近高深度精确方法，且因乘法深度降低大幅降低计算成本。

Conclusion: MGF - softmax能实现高效准确的软最大化近似。

Abstract: Homomorphic encryption (HE) is a prominent framework for privacy-preserving machine learning, enabling inference directly on encrypted data. However, evaluating softmax, a core component of transformer architectures, remains particularly challenging in HE due to its multivariate structure, the large dynamic range induced by exponential functions, and the need for accurate division during normalization. In this paper, we propose MGF-softmax, a novel softmax reformulation based on the moment generating function (MGF) that replaces the softmax denominator with its moment-based counterpart. This reformulation substantially reduces multiplicative depth while preserving key properties of softmax and asymptotically converging to the exact softmax as the number of input tokens increases. Extensive experiments on Vision Transformers and large language models show that MGF-softmax provides an efficient and accurate approximation of softmax in encrypted inference. In particular, it achieves inference accuracy close to that of high-depth exact methods, while requiring substantially lower computational cost through reduced multiplicative depth.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [796] [Fast Sparse Matrix Permutation for Mesh-Based Direct Solvers](https://arxiv.org/abs/2602.00898)
*Behrooz Zarebavami,Ahmed H. Mahmoud,Ana Dodik,Changcheng Yuan,Serban D. Porumbescu,John D. Owens,Maryam Mehri Dehnavi,Justin Solomon*

Main category: cs.GR

TL;DR: 提出适用于三角网格线性系统的快速稀疏矩阵置换算法，集成到求解器后减少置换时间并提升求解性能。


<details>
  <summary>Details</summary>
Motivation: 为三角网格线性系统设计快速稀疏矩阵置换算法，减少置换运行时开销。

Method: 将置换分解为补丁级局部排序和分隔符的紧凑商图排序，放松严格平衡和分隔符最优性设计。

Result: 在多种图形应用中，该方法最多可减少置换时间并将稀疏Cholesky求解性能提高6.27倍。

Conclusion: 该算法有效减少置换时间，提升稀疏Cholesky求解性能。

Abstract: We present a fast sparse matrix permutation algorithm tailored to linear systems arising from triangle meshes. Our approach produces nested-dissection-style permutations while significantly reducing permutation runtime overhead. Rather than enforcing strict balance and separator optimality, the algorithm deliberately relaxes these design decisions to favor fast partitioning and efficient elimination-tree construction. Our method decomposes permutation into patch-level local orderings and a compact quotient-graph ordering of separators, preserving the essential structure required by sparse Cholesky factorization while avoiding its most expensive components. We integrate our algorithm into vendor-maintained sparse Cholesky solvers on both CPUs and GPUs. Across a range of graphics applications, including single factorizations, repeated factorizations, our method reduces permutation time and improves the sparse Cholesky solve performance by up to 6.27x.

</details>


### [797] [Genus-0 Surface Parameterization using Spherical Beltrami Differentials](https://arxiv.org/abs/2602.01589)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.GR

TL;DR: 引入球面Beltrami微分（SBD），提出神经优化框架BOOST用于球面自映射，实验证明其在地标匹配、球面配准及脑皮层表面配准等任务有效。


<details>
  <summary>Details</summary>
Motivation: 现有球面自映射方法在满足任务目标、保持双射性和控制几何畸变间存在权衡问题。

Method: 引入SBD并建立其与球面同胚的对应关系，基于SBN提出BOOST框架，优化半球立体投影图上的两个Beltrami场并通过显式接缝感知约束确保全局一致性。

Result: 在大变形地标匹配和基于强度的球面配准实验中证明框架有效，应用于脑皮层表面配准有更好任务保真度、可控畸变和稳健双射行为。

Conclusion: 提出的框架能有效解决球面自映射问题，在相关任务中有良好表现。

Abstract: Spherical surface parameterization is a fundamental tool in geometry processing and imaging science. For a genus-0 closed surface, many efficient algorithms can map the surface to the sphere; consequently, a broad class of task-driven genus-0 mapping problems can be reduced to constructing a high-quality spherical self-map. However, existing approaches often face a trade-off between satisfying task objectives (e.g., landmark or feature alignment), maintaining bijectivity, and controlling geometric distortion. We introduce the Spherical Beltrami Differential (SBD), a two-chart representation of quasiconformal self-maps of the sphere, and establish its correspondence with spherical homeomorphisms up to conformal automorphisms. Building on the Spectral Beltrami Network (SBN), we propose a neural optimization framework BOOST that optimizes two Beltrami fields on hemispherical stereographic charts and enforces global consistency through explicit seam-aware constraints. Experiments on large-deformation landmark matching and intensity-based spherical registration demonstrate the effectiveness of our proposed framework. We further apply the method to brain cortical surface registration, aligning sulcal landmarks and jointly matching cortical sulci depth maps, showing improved task fidelity with controlled distortion and robust bijective behavior.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [798] [Early warning prediction: Onsager-Machlup vs Schrödinger](https://arxiv.org/abs/2602.00143)
*Xiaoai Xu,Yixuan Zhou,Xiang Zhou,Jingqiao Duan,Ting Gao*

Main category: q-bio.QM

TL;DR: 本文提出结合流形学习与随机动力系统建模的预警框架，定义新指标用于癫痫预测，实验证明其在癫痫预测上有高敏感性和稳健性，为高维数据提取预警信号提供理论和方法。


<details>
  <summary>Details</summary>
Motivation: 预测复杂系统的临界转变是科研重大挑战，高维特征和隐藏的临界信号使预警任务更复杂，因此开展此研究。

Method: 提出整合流形学习与随机动力系统建模的预警框架，选6种方法构建低维表示，建立数据驱动的随机微分方程模型，结合薛定谔桥理论定义新的分数函数（SF）指标。

Result: 该指标在癫痫预测中表现出更高的敏感性和稳健性，能更早识别临界点，清晰捕捉癫痫发作前后各阶段动态特征。

Conclusion: 此研究为从高维数据中提取预警信号提供了系统的理论框架和实用方法。

Abstract: Predicting critical transitions in complex systems, such as epileptic seizures in the brain, represents a major challenge in scientific research. The high-dimensional characteristics and hidden critical signals further complicate early-warning tasks. This study proposes a novel early-warning framework that integrates manifold learning with stochastic dynamical system modeling. Through systematic comparison, six methods including diffusion maps (DM) are selected to construct low-dimensional representations. Based on these, a data-driven stochastic differential equation model is established to robustly estimate the probability evolution scoring function of the system. Building on this, a new Score Function (SF) indicator is defined by incorporating Schrödinger bridge theory to quantify the likelihood of significant state transitions in the system. Experiments demonstrate that this indicator exhibits higher sensitivity and robustness in epilepsy prediction, enables earlier identification of critical points, and clearly captures dynamic features across various stages before and after seizure onset. This work provides a systematic theoretical framework and practical methodology for extracting early-warning signals from high-dimensional data.

</details>


### [799] [ProDCARL: Reinforcement Learning-Aligned Diffusion Models for De Novo Antimicrobial Peptide Design](https://arxiv.org/abs/2602.00157)
*Fang Sheng,Mohammad Noaeen,Zahra Shakeri*

Main category: q-bio.QM

TL;DR: 介绍ProDCARL强化学习对齐框架用于抗菌肽生成，提升预测抗菌肽分数，有高多样性和合理结构语义特征，待实验验证。


<details>
  <summary>Details</summary>
Motivation: 抗菌耐药威胁医疗可持续性，需低成本计算发现抗菌肽，现有生成器未明确优化抗菌活性和安全性。

Method: 引入ProDCARL框架，结合基于扩散的蛋白质生成器与序列属性预测器，微调扩散先验，使用top - k策略梯度更新。

Result: ProDCARL将平均预测抗菌肽分数从微调后的0.081提升到0.178，联合高质量命中率达6.3%，保持高多样性，候选者有合理结构和语义特征。

Conclusion: ProDCARL可作为候选生成器缩小实验搜索空间，有待实验验证。

Abstract: Antimicrobial resistance threatens healthcare sustainability and motivates low-cost computational discovery of antimicrobial peptides (AMPs). De novo peptide generation must optimize antimicrobial activity and safety through low predicted toxicity, but likelihood-trained generators do not enforce these goals explicitly. We introduce ProDCARL, a reinforcement-learning alignment framework that couples a diffusion-based protein generator (EvoDiff OA-DM 38M) with sequence property predictors for AMP activity and peptide toxicity. We fine-tune the diffusion prior on AMP sequences to obtain a domain-aware generator. Top-k policy-gradient updates use classifier-derived rewards plus entropy regularization and early stopping to preserve diversity and reduce reward hacking. In silico experiments show ProDCARL increases the mean predicted AMP score from 0.081 after fine-tuning to 0.178. The joint high-quality hit rate reaches 6.3\% with pAMP $>$0.7 and pTox $<$0.3. ProDCARL maintains high diversity, with $1-$mean pairwise identity equal to 0.929. Qualitative analyses with AlphaFold3 and ProtBERT embeddings suggest candidates show plausible AMP-like structural and semantic characteristics. ProDCARL serves as a candidate generator that narrows experimental search space, and experimental validation remains future work.

</details>


### [800] [Rank-and-Reason: Multi-Agent Collaboration Accelerates Zero-Shot Protein Mutation Prediction](https://arxiv.org/abs/2602.00197)
*Yang Tan,Yuyuan Xi,Can Wu,Bozitao Zhong,Mingchen Li,Guisheng Fan,Jiankang Zhu,Yafeng Liang,Nanqing Dong,Liang Hong*

Main category: q-bio.QM

TL;DR: 提出Rank - and - Reason (VenusRAR)框架用于零样本突变预测，经实验验证有效并公开代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质语言模型结果忽略生物物理约束，人工筛选候选突变体效率低、主观且依赖专业知识。

Method: 提出两阶段的VenusRAR框架，Rank - Stage聚合多模态集成，Reason - Stage用思维链推理审核候选突变体。

Result: 在ProteinGym上Spearman相关性达0.551；在ProteinGym - DMS99上Top - 5命中率提高367%；对Cas12i3核酸酶的湿实验阳性率46.7%，发现两个活性提升的新突变体。

Conclusion: VenusRAR框架能自动化工作流程，有效提高湿实验预期适用性。

Abstract: Zero-shot mutation prediction is vital for low-resource protein engineering, yet existing protein language models (PLMs) often yield statistically confident results that ignore fundamental biophysical constraints. Currently, selecting candidates for wet-lab validation relies on manual expert auditing of PLM outputs, a process that is inefficient, subjective, and highly dependent on domain expertise. To address this, we propose Rank-and-Reason (VenusRAR), a two-stage agentic framework to automate this workflow and maximize expected wet-lab fitness. In the Rank-Stage, a Computational Expert and Virtual Biologist aggregate a context-aware multi-modal ensemble, establishing a new Spearman correlation record of 0.551 (vs. 0.518) on ProteinGym. In the Reason-Stage, an agentic Expert Panel employs chain-of-thought reasoning to audit candidates against geometric and structural constraints, improving the Top-5 Hit Rate by up to 367% on ProteinGym-DMS99. The wet-lab validation on Cas12i3 nuclease further confirms the framework's efficacy, achieving a 46.7% positive rate and identifying two novel mutants with 4.23-fold and 5.05-fold activity improvements. Code and datasets are released on GitHub (https://github.com/ai4protein/VenusRAR/).

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [801] [Explore Brain-Inspired Machine Intelligence for Connecting Dots on Graphs Through Holographic Blueprint of Oscillatory Synchronization](https://arxiv.org/abs/2602.00057)
*Tingting Dan,Jiaqi Ding,Guorong Wu*

Main category: q-bio.NC

TL;DR: 本文提出用理解大脑节律的神经机制启发机器学习算法设计，构建了HoloBrain和HoloGraph模型，缓解图神经网络过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 深入理解控制大脑节律的神经机制，以启发下一代机器学习算法设计原则，提高效率和鲁棒性。

Method: 先通过自发同步神经振荡的干扰对大脑节律建模得HoloBrain，再基于共享同步机制提出用于脑启发机器智能的“第一原理”HoloGraph。

Result: HoloGraph框架有效缓解了图神经网络的过平滑问题，在图推理和解决难题上有强大潜力。

Conclusion: 基于大脑节律神经机制的设计原则能改进机器学习算法，HoloGraph有良好应用前景。

Abstract: Neural coupling in both neuroscience and artificial intelligence emerges as dynamic oscillatory patterns that encode abstract concepts. To this end, we hypothesize that a deeper understanding of the neural mechanisms governing brain rhythms can inspire next-generation design principles for machine learning algorithms, leading to improved efficiency and robustness. Building on this idea, we first model evolving brain rhythms through the interference of spontaneously synchronized neural oscillations, termed HoloBrain. The success of modeling brain rhythms using an artificial dynamical system of coupled oscillations motivates a "first principle" for brain-inspired machine intelligence based on a shared synchronization mechanism, termed HoloGraph. This principle enables graph neural networks to move beyond conventional heat diffusion paradigms toward modeling oscillatory synchronization. Our HoloGraph framework not only effectively mitigates the over-smoothing problem in graph neural networks but also demonstrates strong potential for reasoning and solving challenging problems on graphs.

</details>


### [802] [Inter- and Intra-Subject Variability in EEG: A Systematic Survey](https://arxiv.org/abs/2602.01019)
*Xuan-The Tran,Thien-Nhan Vo,Son-Tung Vu,Thoa-Thi Tran,Manh-Dat Nguyen,Thomas Do,Chin-Teng Lin*

Main category: q-bio.NC

TL;DR: 本文系统综述脑电变异性研究，分析其在不同范式中的表现，总结变异性来源与量化方法并给出建议。


<details>
  <summary>Details</summary>
Motivation: 脑电（EEG）中的主体间和主体内变异性限制可靠性、可重复性和转化应用，需研究变异性。

Method: 对健康和临床人群，在静息态、事件相关电位和任务相关/脑机接口范式中量化或建模EEG变异性。

Result: 主体间差异常大于主体内波动；稳定性因特征而异；总结了主要变异性来源，回顾了常见量化和建模方法。

Conclusion: 应将EEG变异性既视为要解决的实际限制，也作为可利用的有意义信号。

Abstract: Electroencephalography (EEG) underpins neuroscience, clinical neurophysiology, and brain-computer interfaces (BCIs), yet pronounced inter- and intra-subject variability limits reliability, reproducibility, and translation. This systematic review studies that quantified or modeled EEG variability across resting-state, event-related potentials (ERPs), and task-related/BCI paradigms (including motor imagery and SSVEP) in healthy and clinical cohorts. Across paradigms, inter-subject differences are typically larger than within-subject fluctuations, but both affect inference and model generalization. Stability is feature-dependent: alpha-band measures and individual alpha peak frequency are often relatively reliable, whereas higher-frequency and many connectivity-derived metrics show more heterogeneous reliability; ERP reliability varies by component, with P300 measures frequently showing moderate-to-good stability. We summarize major sources of variability (biological, state-related, technical, and analytical), review common quantification and modeling approaches (e.g., ICC, CV, SNR, generalizability theory, and multivariate/learning-based methods), and provide recommendations for study design, reporting, and harmonization. Overall, EEG variability should be treated as both a practical constraint to manage and a meaningful signal to leverage for precision neuroscience and robust neurotechnology.

</details>


### [803] [Community-Level Modeling of Gyral Folding Patterns for Robust and Anatomically Informed Individualized Brain Mapping](https://arxiv.org/abs/2602.01482)
*Minheng Chen,Tong Chen,Yan Zhuang,Chao Cao,Jing Zhang,Tianming Liu,Lu Zhang,Dajiang Zhu*

Main category: q-bio.NC

TL;DR: 该论文提出基于谱图表示学习框架对脑部皮质中3HG进行社区级折叠单元建模，实验证明该方法有更好效果，为个体皮质特征化等提供依据。


<details>
  <summary>Details</summary>
Motivation: 现有基于地标的方法独立建模3HG，忽略其高阶折叠社区，削弱解剖表征，使匹配易受位置变化和噪声影响。

Method: 提出谱图表示学习框架，用双轮廓编码每个3HG，通过特定谱聚类识别连贯折叠社区并拓扑细化，用联合形态 - 几何匹配实现跨主体对应。

Result: 在超1000个样本上，得到的社区形态测量方差降低、模块化组织更强、半球一致性提高、对齐效果更好。

Conclusion: 社区级建模为个体皮质特征化和可靠的跨主体对应提供了强大且基于解剖学的框架。

Abstract: Cortical folding exhibits substantial inter-individual variability while preserving stable anatomical landmarks that enable fine-scale characterization of cortical organization. Among these, the three-hinge gyrus (3HG) serves as a key folding primitive, showing consistent topology yet meaningful variations in morphology, connectivity, and function. Existing landmark-based methods typically model each 3HG independently, ignoring that 3HGs form higher-order folding communities that capture mesoscale structure. This simplification weakens anatomical representation and makes one-to-one matching sensitive to positional variability and noise. We propose a spectral graph representation learning framework that models community-level folding units rather than isolated landmarks. Each 3HG is encoded using a dual-profile representation combining surface topology and structural connectivity. Subject-specific spectral clustering identifies coherent folding communities, followed by topological refinement to preserve anatomical continuity. For cross-subject correspondence, we introduce Joint Morphological-Geometric Matching, jointly optimizing geometric and morphometric similarity. Across over 1000 Human Connectome Project subjects, the resulting communities show reduced morphometric variance, stronger modular organization, improved hemispheric consistency, and superior alignment compared with atlas-based and landmark-based or embedding-based baselines. These findings demonstrate that community-level modeling provides a robust and anatomically grounded framework for individualized cortical characterization and reliable cross-subject correspondence.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [804] [WADEPre: A Wavelet-based Decomposition Model for Extreme Precipitation Nowcasting with Multi-Scale Learning](https://arxiv.org/abs/2602.02096)
*Baitian Liu,Haiping Zhang,Huiling Yuan,Dongjing Wang,Ying Li,Feng Chen,Hao Wu*

Main category: physics.ao-ph

TL;DR: 提出基于小波分解模型WADEPre用于极端降水临近预报，采用多尺度课程学习策略，实验表明其达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 降水强度的重尾特性阻碍精确的降水临近预报，标准模型有回归偏差，现有傅里叶方法缺乏空间局部性。

Method: 提出基于小波分解的WADEPre模型，利用离散小波变换进行显式分解，采用双分支架构，引入多尺度课程学习策略。

Result: 在SEVIR和上海雷达数据集上的广泛实验表明，WADEPre达到了SOTA性能。

Conclusion: WADEPre在捕捉极端降水阈值和保持结构保真度方面有显著改进。

Abstract: The heavy-tailed nature of precipitation intensity impedes precise precipitation nowcasting. Standard models that optimize pixel-wise losses are prone to regression-to-the-mean bias, which blurs extreme values. Existing Fourier-based methods also lack the spatial localization needed to resolve transient convective cells. To overcome these intrinsic limitations, we propose WADEPre, a wavelet-based decomposition model for extreme precipitation that transitions the modeling into the wavelet domain. By leveraging the Discrete Wavelet Transform for explicit decomposition, WADEPre employs a dual-branch architecture: an Approximation Network to model stable, low-frequency advection, isolating deterministic trends from statistical bias, and a spatially localized Detail Network to capture high-frequency stochastic convection, resolving transient singularities and preserving sharp boundaries. A subsequent Refiner module then dynamically reconstructs these decoupled multi-scale components into the final high-fidelity forecast. To address optimization instability, we introduce a multi-scale curriculum learning strategy that progressively shifts supervision from coarse scales to fine-grained details. Extensive experiments on the SEVIR and Shanghai Radar datasets demonstrate that WADEPre achieves state-of-the-art performance, yielding significant improvements in capturing extreme thresholds and maintaining structural fidelity. Our code is available at https://github.com/sonderlau/WADEPre.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [805] [The Domain of RSD Characterization by Efficiency, Symmetry, and Strategy-Proofness](https://arxiv.org/abs/2602.01224)
*Maor Ben Zaquen,Ron Holzman*

Main category: econ.TH

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given a set of $n$ individuals with strict preferences over $m$ indivisible objects, the Random Serial Dictatorship (RSD) mechanism is a method for allocating objects to individuals in a way that is efficient, fair, and incentive-compatible. A random order of individuals is first drawn, and each individual, following this order, selects their most preferred available object. The procedure continues until either all objects have been assigned or all individuals have received an object.
  RSD is widely recognized for its application in fair allocation problems involving indivisible goods, such as school placements and housing assignments. Despite its extensive use, a comprehensive axiomatic characterization has remained incomplete. For the balanced case $n=m=3$, Bogomolnaia and Moulin have shown that RSD is uniquely characterized by Ex-Post Efficiency, Equal Treatment of Equals, and Strategy-Proofness. The possibility of extending this characterization to larger markets had been a long-standing open question, which Basteck and Ehlers recently answered in the negative for all markets with $n,m\geq5$.
  This work completes the picture by identifying exactly for which pairs $\left(n,m\right)$ these three axioms uniquely characterize the RSD mechanism and for which pairs they admit multiple mechanisms. In the latter cases, we construct explicit alternatives satisfying the axioms and examine whether augmenting the set of axioms could rule out these alternatives.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [806] [Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems](https://arxiv.org/abs/2602.01503)
*Afifah Kashif,Abdul Muhsin Hameed,Asim Iqbal*

Main category: cs.ET

TL;DR: 本文探讨当前AI治理框架对NeuroAI的局限性，认为保证和审计方法需与架构共同演进。


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架是为静态、集中训练的人工神经网络构建，而NeuroAI系统打破了这些假设，需研究框架对其的适用性。

Method: 分析当前AI治理框架对NeuroAI的局限性。

Result: 发现当前AI治理框架存在不适用于NeuroAI的情况。

Conclusion: 保证和审计方法必须与NeuroAI架构共同演进，使传统监管指标与类脑计算特性相匹配以实现技术上可靠的保证。

Abstract: Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [807] [Minimax optimal differentially private synthetic data for smooth queries](https://arxiv.org/abs/2602.01607)
*Rundong Ding,Yiyun He,Yizhe Zhu*

Main category: math.ST

TL;DR: 研究生成差分隐私合成数据问题，提出多项式时间算法，有误差率刻画和下界，改进了先前结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法对实际感兴趣统计量效果不佳，探索利用数据额外结构提升差分隐私合成数据的实用性。

Method: 提出多项式时间算法，研究在超立方体上生成(ε,δ) - 差分隐私合成数据，保证对所有k阶导数有界的平滑查询有统一效用。

Result: 算法达到了n^(-min{1, k/d})的极小极大误差率（有log(n)因子），发现k=d处的相变，推广框架并改进误差率，建立首个极小极大下界。

Conclusion: 利用数据额外结构可提升差分隐私合成数据的实用性，所提算法和结果有理论上的进步。

Abstract: Differentially private synthetic data enables the sharing and analysis of sensitive datasets while providing rigorous privacy guarantees for individual contributors. A central challenge is to achieve strong utility guarantees for meaningful downstream analysis. Many existing methods ensure uniform accuracy over broad query classes, such as all Lipschitz functions, but this level of generality often leads to suboptimal rates for statistics of practical interest. Since many common data analysis queries exhibit smoothness beyond what worst-case Lipschitz bounds capture, we ask whether exploiting this additional structure can yield improved utility.
  We study the problem of generating $(\varepsilon,δ)$-differentially private synthetic data from a dataset of size $n$ supported on the hypercube $[-1,1]^d$, with utility guarantees uniformly for all smooth queries having bounded derivatives up to order $k$. We propose a polynomial-time algorithm that achieves a minimax error rate of $n^{-\min \{1, \frac{k}{d}\}}$, up to a $\log(n)$ factor. This characterization uncovers a phase transition at $k=d$. Our results generalize the Chebyshev moment matching framework of (Musco et al., 2025; Wang et al., 2016) and strictly improve the error rates for $k$-smooth queries established in (Wang et al., 2016). Moreover, we establish the first minimax lower bound for the utility of $(\varepsilon,δ)$-differentially private synthetic data with respect to $k$-smooth queries, extending the Wasserstein lower bound for $\varepsilon$-differential privacy in (Boedihardjo et al., 2024).

</details>


### [808] [Handling Covariate Mismatch in Federated Linear Prediction](https://arxiv.org/abs/2602.02083)
*Alexis Ayme,Rémi Khellaf*

Main category: math.ST

TL;DR: 研究联邦学习中协变量不匹配的更现实场景，提出两种模块化方法并给出学习率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法大多假设所有客户端测量相同特征，而实际中存在协变量不匹配的情况，如多中心合作无数据收集预先协议。

Method: 在低维场景，提出插件式估计器；在高维场景，采用先插补缺失协变量再拟合岭回归线性模型的策略。

Result: 为预测器提供了渐近和有限样本学习率，明确刻画其与全局维度、客户端特定特征划分和样本分布的关系。

Conclusion: 所提出的方法可应对联邦学习中的协变量不匹配问题。

Abstract: Federated learning enables institutions to train predictive models collaboratively without sharing raw data, addressing privacy and regulatory constraints. In the standard horizontal setting, clients hold disjoint cohorts of individuals and collaborate to learn a shared predictor. Most existing methods, however, assume that all clients measure the same features. We study the more realistic setting of covariate mismatch, where each client observes a different subset of features, which typically arises in multicenter collaborations with no prior agreement on data collection. We formalize learning a linear prediction under client-wise MCAR patterns and develop two modular approaches tailored to the dimensional regime and communication budget. In the low-dimensional setting, we propose a plug-in estimator that approximates the oracle linear predictor by aggregating sufficient statistics to estimate the covariance and cross-moment terms. In higher dimensions, we study an impute-then-regress strategy: (i) impute missing covariates using any exchangeability-preserving imputation procedure, and (ii) fit a ridge-regularized linear model on the completed data. We provide asymptotic and finite-sample learning rates for our predictors, explicitly characterizing their behaviour with the global dimension, the client-specific feature partition, and the distribution of samples across sites.

</details>


### [809] [New explanations and inference for least angle regression](https://arxiv.org/abs/2602.02491)
*Karl B. Gregory,Daniel J. Nordman*

Main category: math.ST

TL;DR: 本文为最小角回归（LAR）算法提供新的推理框架，研究其数学性质并给出停止规则，用修正的自助法辅助解释，还通过模拟和实例研究。


<details>
  <summary>Details</summary>
Motivation: LAR算法仍是‘黑箱’，一些基本行为属性包括合适的终止点未被充分理解。

Method: 提出新的推理框架，研究LAR在数据层面与总体‘路径’的关系，分析非零和零值总体相关性估计的分布，使用修正的自助法。

Result: 非零总体相关性估计有独立正态分布，零值有特定非正态联合分布，可据此给出停止规则，修正的自助法可辅助解释变量和量化估计不确定性。

Conclusion: 所提方法有助于理解LAR算法，可通过模拟和实例研究。

Abstract: Efron et al. (2004) introduced least angle regression (LAR) as an algorithm for linear predictions, intended as an alternative to forward selection with connections to penalized regression. However, LAR has remained somewhat of a "black box," where some basic behavioral properties of LAR output are not well understood, including an appropriate termination point for the algorithm. We provide a novel framework for inference with LAR, which also allows LAR to be understood from new perspectives with several newly developed mathematical properties. The LAR algorithm at a data level can viewed as estimating a population counterpart "path" that organizes a response mean along regressor variables which are ordered according to a decreasing series of population "correlation" parameters; such parameters are shown to have meaningful interpretations for explaining variable contributions whereby zero correlations denote unimportant variables. In the output of LAR, estimates of all non-zero population correlations turn out to have independent normal distributions for use in inference, while estimates of zero-valued population correlations have a certain non-normal joint distribution. These properties help to provide a formal rule for stopping the LAR algorithm. While the standard bootstrap for regression can fail for LAR, a modified bootstrap provides a practical and formally justified tool for interpreting the entrance of variables and quantifying uncertainty in estimation. The LAR inference method is studied through simulation and illustrated with data examples.

</details>


### [810] [Improving Minimax Estimation Rates for Contaminated Mixture of Multinomial Logistic Experts via Expert Heterogeneity](https://arxiv.org/abs/2602.00939)
*Fanqi Yan,Dung Le,Trang Pham,Huy Nguyen,Nhat Ho*

Main category: math.ST

TL;DR: 本文首次对污染混合专家模型在分类设置下进行收敛分析，给出参数估计的一致收敛率和极小极大下界，指出专家异质性更具样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有污染混合专家模型在分类设置下理论基础缺失，且之前分类模型参数估计无极小极大最优性保证。

Method: 对具有同质和异质结构的多项逻辑专家污染混合模型进行收敛分析，刻画不同情况下参数估计的一致收敛率并建立极小极大下界。

Result: 得到参数估计的一致收敛率，并证明这些速率是极小极大最优的。

Conclusion: 专家异质性能带来更快的参数估计速率，比专家同质性更具样本效率。

Abstract: Contaminated mixture of experts (MoE) is motivated by transfer learning methods where a pre-trained model, acting as a frozen expert, is integrated with an adapter model, functioning as a trainable expert, in order to learn a new task. Despite recent efforts to analyze the convergence behavior of parameter estimation in this model, there are still two unresolved problems in the literature. First, the contaminated MoE model has been studied solely in regression settings, while its theoretical foundation in classification settings remains absent. Second, previous works on MoE models for classification capture pointwise convergence rates for parameter estimation without any guaranty of minimax optimality. In this work, we close these gaps by performing, for the first time, the convergence analysis of a contaminated mixture of multinomial logistic experts with homogeneous and heterogeneous structures, respectively. In each regime, we characterize uniform convergence rates for estimating parameters under challenging settings where ground-truth parameters vary with the sample size. Furthermore, we also establish corresponding minimax lower bounds to ensure that these rates are minimax optimal. Notably, our theories offer an important insight into the design of contaminated MoE, that is, expert heterogeneity yields faster parameter estimation rates and, therefore, is more sample-efficient than expert homogeneity.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [811] [Probabilistic function-on-function nonlinear autoregressive model for emulation and reliability analysis of dynamical systems](https://arxiv.org/abs/2602.01929)
*Zhouzhou Song,Marcos A. Valdebenito,Styfen Schär,Stefano Marelli,Bruno Sudret,Matthias G. R. Faes*

Main category: math.DS

TL;DR: 本文提出F2NARX模型，结合PCA与GPR，在效率和准确性上优于NARX模型，还具备概率预测能力。


<details>
  <summary>Details</summary>
Motivation: 在许多工程领域构建准确且计算高效的替代模型来预测动力系统响应很关键，但因高维非线性映射而具有挑战性。

Method: 从函数对函数回归角度重新构建传统NARX模型得到F2NARX，结合主成分分析和高斯过程回归，通过无迹变换实现自回归方式的概率预测。

Result: F2NARX在效率上比现有NARX模型高几个数量级，且通常准确性更高，其概率预测能力有助于主动学习，能用少量训练时间历程准确估计动力系统首过失效概率。

Conclusion: F2NARX模型有效，在预测动力系统响应方面表现出色。

Abstract: Constructing accurate and computationally efficient surrogate models (or emulators) for predicting dynamical system responses is critical in many engineering domains, yet remains challenging due to the strongly nonlinear and high-dimensional mapping from external excitations and system parameters to system responses. This work introduces a novel Function-on-Function Nonlinear AutoRegressive model with eXogenous inputs (F2NARX), which reformulates the conventional NARX model from a function-on-function regression perspective, inspired by the recently proposed $\mathcal{F}$-NARX method. The proposed framework substantially improves predictive efficiency while maintaining high accuracy. By combining principal component analysis with Gaussian process regression, F2NARX further enables probabilistic predictions of dynamical responses via the unscented transform in an autoregressive manner. The effectiveness of the method is demonstrated through case studies of varying complexity. Results show that F2NARX outperforms state-of-the-art NARX model by orders of magnitude in efficiency while achieving higher accuracy in general. Moreover, its probabilistic prediction capabilities facilitate active learning, enabling accurate estimation of first-passage failure probabilities of dynamical systems using only a small number of training time histories.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [812] [Vortex Stretching in the Navier-Stokes Equations and Information Dissipation in Diffusion Models: A Reformulation from a Partial Differential Equation Viewpoint](https://arxiv.org/abs/2602.01071)
*Tsuyoshi Yoneda*

Main category: math.AP

TL;DR: 提出基于分数扩散模型PDE框架的NS方程涡拉伸逆时公式，用神经网络学习分数函数构建逆向粒子轨迹，数值结果显示不同方向初始位置信息保存情况不同。


<details>
  <summary>Details</summary>
Motivation: 提出新的NS方程涡拉伸逆时公式。

Method: 将时间反转产生的不适定后向拉普拉斯算子吸收到用分数函数表示的漂移项中，以拉格朗日方式表述逆时动力学，用神经网络学习分数函数。

Result: 信息在压缩方向快速丢失，在拉伸方向相对保存良好。

Conclusion: 所提出的逆时公式能用于分析涡拉伸场中不同方向初始位置信息的保存情况。

Abstract: We present a new inverse-time formulation of vortex stretching in the Navier-Stokes equations, based on a PDE framework inspired by score-based diffusion models. By absorbing the ill-posed backward Laplacian arising from time reversal into a drift term expressed through a score function, the inverse-time dynamics are formulated in a Lagrangian manner. Using a discrete Lagrangian flow of an axisymmetric vortex-stretching field, the score function is learned with a neural network and employed to construct backward-time particle trajectories. Numerical results demonstrate that information about initial positions is rapidly lost in the compressive direction, whereas it is relatively well preserved in the stretching direction.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [813] [FragmentFlow: Scalable Transition State Generation for Large Molecules](https://arxiv.org/abs/2602.02310)
*Ron Shprints,Peter Holderrieth,Juno Nam,Rafael Gómez-Bombarelli,Tommi Jaakkola*

Main category: physics.chem-ph

TL;DR: 传统过渡态生成方法计算成本高，现有生成模型方法难以处理大分子底物。介绍了FragmentFlow方法，在新数据集上表现良好，指向可扩展过渡态生成。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算成本高，现有生成模型方法因分子大小导致分布偏移，且大分子过渡态几何数据不足，难以处理实际相关反应底物。

Method: 提出FragmentFlow分治法，训练生成模型预测定义反应机制的活性核心原子的过渡态几何，再将取代基片段重新连接到预测的核心上重构完整过渡态结构。

Result: 在新整理的涉及最多33个重原子反应物的反应数据集上，FragmentFlow正确识别90%的过渡态，比经典初始化方案所需的鞍点优化步骤少30%。

Conclusion: 研究成果指向用于高通量反应性研究的可扩展过渡态生成。

Abstract: Transition states (TSs) are central to understanding and quantitatively predicting chemical reactivity and reaction mechanisms. Although traditional TS generation methods are computationally expensive, recent generative modeling approaches have enabled chemically meaningful TS prediction for relatively small molecules. However, these methods fail to generalize to practically relevant reaction substrates because of distribution shifts induced by increasing molecular sizes. Furthermore, TS geometries for larger molecules are not available at scale, making it infeasible to train generative models from scratch on such molecules. To address these challenges, we introduce FragmentFlow: a divide-and-conquer approach that trains a generative model to predict TS geometries for the reactive core atoms, which define the reaction mechanism. The full TS structure is then reconstructed by re-attaching substituent fragments to the predicted core. By operating on reactive cores, whose size and composition remain relatively invariant across molecular contexts, FragmentFlow mitigates distribution shifts in generative modeling. Evaluated on a new curated dataset of reactions involving reactants with up to 33 heavy atoms, FragmentFlow correctly identifies 90% of TSs while requiring 30% fewer saddle-point optimization steps than classical initialization schemes. These results point toward scalable TS generation for high-throughput reactivity studies.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [814] [Profit Maximization in Closed Social Networks](https://arxiv.org/abs/2602.01232)
*Poonam Sharma,Suman Banerjee*

Main category: cs.SI

TL;DR: 本文研究封闭社交网络中的利润最大化问题（PMCSN），提出两种解决方案，实验表明其效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体成为商业盈利有效媒介，需解决封闭社交网络中的利润最大化问题。

Method: 提出基于采样的近似解和基于边际收益的启发式解两种方法，并分析样本复杂度、运行时间和空间需求。

Result: 在真实社交网络数据集上实验，所提方法选择的种子集和扩散链接带来的利润高于基线方法。

Conclusion: 所提的两种解决方案能有效解决封闭社交网络中的利润最大化问题。

Abstract: Diffusion of information, innovation, and ideas is an important phenomenon in social networks. Information propagates through the network and reaches from one person to the next. In many settings, it is meaningful to restrict diffusion so that each node can spread information to only a limited number of its neighbors rather than to all of them. Such social networks are called closed social networks. In recent years, social media platforms have emerged as an effective medium for commercial entities, where the objective is to maximize profit. In this paper, we study the Profit Maximization in Closed Social Networks (PMCSN) problem in the context of viral marketing. The input to the problem is a closed social network and two positive integers $\ell$ and $B$. The problem asks to select seed nodes within a given budget $B$; during the diffusion process, each node is restricted to choose at most $\ell$ outgoing links for information diffusion; and the objective is to maximize the profit earned by the seed set. The PMCSN problem generalizes the Influence Maximization problem, which is NP-hard. We propose two solution approaches for PMCSN: a sampling-based approximate solution and a marginal-gain-based heuristic solution. We analyze the sample complexity, running time, and space requirements of the proposed approaches. We conduct experiments on real-world, publicly available social network datasets. The results show that the seed sets and diffusion links chosen by our methods yield higher profit than baseline methods. The implementation and data are available at \texttt{https://github.com/PoonamSharma-PY/ClosedNetwork}.

</details>


### [815] [DREAMS: A Social Exchange Theory-Informed Modeling of Misinformation Engagement on Social Media](https://arxiv.org/abs/2602.01567)
*Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.SI

TL;DR: 提出Dreams框架预测社交媒体虚假信息参与度，在跨平台数据集上取得SOTA性能，且揭示了符合社会交换原则的跨平台模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视塑造虚假信息传播的异质社会机制和平台设计，需探究能否从行为数据中发现社会交换原则。

Method: 引入社会交换理论引导的Dreams框架，将虚假信息参与度建模为社交交换动态过程，当作序列到序列适应问题，并集成自适应机制。

Result: 在7个平台、237万条帖子的跨平台数据集上达到19.25%的平均绝对百分比误差，比最强基线提高43.6%，揭示了符合社会交换原则的跨平台模式。

Conclusion: 整合行为理论可增强在线虚假信息参与度的实证建模。

Abstract: Social media engagement prediction is a central challenge in computational social science, particularly for understanding how users interact with misinformation. Existing approaches often treat engagement as a homogeneous time-series signal, overlooking the heterogeneous social mechanisms and platform designs that shape how misinformation spreads. In this work, we ask: ``Can neural architectures discover social exchange principles from behavioral data alone?'' We introduce \textsc{Dreams} (\underline{D}isentangled \underline{R}epresentations and \underline{E}pisodic \underline{A}daptive \underline{M}odeling for \underline{S}ocial media misinformation engagements), a social exchange theory-guided framework that models misinformation engagement as a dynamic process of social exchange. Rather than treating engagement as a static outcome, \textsc{Dreams} models it as a sequence-to-sequence adaptation problem, where each action reflects an evolving negotiation between user effort and social reward conditioned by platform context. It integrates adaptive mechanisms to learn how emotional and contextual signals propagate through time and across platforms. On a cross-platform dataset spanning $7$ platforms and 2.37M posts collected between 2021 and 2025, \textsc{Dreams} achieves state-of-the-art performance in predicting misinformation engagements, reaching a mean absolute percentage error of $19.25$\%. This is a $43.6$\% improvement over the strongest baseline. Beyond predictive gains, the model reveals consistent cross-platform patterns that align with social exchange principles, suggesting that integrating behavioral theory can enhance empirical modeling of online misinformation engagement. The source code is available at: https://github.com/ltian678/DREAMS.

</details>


### [816] [Cross-Domain Fake News Detection on Unseen Domains via LLM-Based Domain-Aware User Modeling](https://arxiv.org/abs/2602.01726)
*Xuankai Yang,Yan Wang,Jiajie Zhu,Pengfei Ding,Hongyang Liu,Xiuzhen Zhang,Huan Liu*

Main category: cs.SI

TL;DR: 提出DAUD框架用于跨领域假新闻检测，在通用和未见领域表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有跨领域假新闻检测方法在未见领域存在语义建模不足和标注数据稀缺问题，大语言模型有潜力但使用存在挑战。

Method: 提出DAUD框架，利用大语言模型提取新闻内容高层语义，建模用户单域和跨域参与生成领域感知行为表示，捕捉原始特征与大语言模型衍生特征关系。

Result: 在真实数据集上的大量实验表明，DAUD在通用和未见领域跨领域假新闻检测设置中优于现有基线。

Conclusion: DAUD框架能有效解决未见领域跨领域假新闻检测问题，提升知识转移效果。

Abstract: Cross-domain fake news detection (CD-FND) transfers knowledge from a source domain to a target domain and is crucial for real-world fake news mitigation. This task becomes particularly important yet more challenging when the target domain is previously unseen (e.g., the COVID-19 outbreak or the Russia-Ukraine war). However, existing CD-FND methods overlook such scenarios and consequently suffer from the following two key limitations: (1) insufficient modeling of high-level semantics in news and user engagements; and (2) scarcity of labeled data in unseen domains. Targeting these limitations, we find that large language models (LLMs) offer strong potential for CD-FND on unseen domains, yet their effective use remains non-trivial. Nevertheless, two key challenges arise: (1) how to capture high-level semantics from both news content and user engagements using LLMs; and (2) how to make LLM-generated features more reliable and transferable for CD-FND on unseen domains. To tackle these challenges, we propose DAUD, a novel LLM-Based Domain-Aware framework for fake news detection on Unseen Domains. DAUD employs LLMs to extract high-level semantics from news content. It models users' single- and cross-domain engagements to generate domain-aware behavioral representations. In addition, DAUD captures the relations between original data-driven features and LLM-derived features of news, users, and user engagements. This allows it to extract more reliable domain-shared representations that improve knowledge transfer to unseen domains. Extensive experiments on real-world datasets demonstrate that DAUD outperforms state-of-the-art baselines in both general and unseen-domain CD-FND settings.

</details>


### [817] [Twinning Complex Networked Systems: Data-Driven Calibration of the mABCD Synthetic Graph Generator](https://arxiv.org/abs/2602.02044)
*Piotr Bródka,Michał Czuba,Bogumił Kamiński,Łukasz Kraiński,Katarzyna Musial,Paweł Prałat,Mateusz Stolarski*

Main category: cs.SI

TL;DR: 文章针对多层网络生成器的逆生成问题，从真实系统推断多层网络生成器mABCD的配置参数，提出估计匹配配置和量化误差的方法，结果表明任务有挑战，联合预测法更优。


<details>
  <summary>Details</summary>
Motivation: 多层网络分析评估受限于大规模实证数据，图生成器有系统偏差，要解决逆生成问题，让生成器产生类似原结构的合成网络。

Method: 提出估计匹配配置和量化相关误差的方法。

Result: 任务并非简单，配置参数间强相互依赖削弱独立估计，联合预测法更合适。

Conclusion: 联合预测法更有利于解决多层网络生成器的逆生成问题。

Abstract: The increasing availability of relational data has contributed to a growing reliance on network-based representations of complex systems. Over time, these models have evolved to capture more nuanced properties, such as the heterogeneity of relationships, leading to the concept of multilayer networks. However, the analysis and evaluation of methods for these structures is often hindered by the limited availability of large-scale empirical data. As a result, graph generators are commonly used as a workaround, albeit at the cost of introducing systematic biases. In this paper, we address the inverse-generator problem by inferring the configuration parameters of a multilayer network generator, mABCD, from a real-world system. Our goal is to identify parameter settings that enable the generator to produce synthetic networks that act as digital twins of the original structure. We propose a method for estimating matching configurations and for quantifying the associated error. Our results demonstrate that this task is non-trivial, as strong interdependencies between configuration parameters weaken independent estimation and instead favour a joint-prediction approach.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [818] [Dimension-Free Multimodal Sampling via Preconditioned Annealed Langevin Dynamics](https://arxiv.org/abs/2602.01449)
*Lorenzo Baldassari,Josselin Garnier,Knut Solna,Maarten V. de Hoop*

Main category: math.NA

TL;DR: 本文对多峰目标的连续时间退火朗之万动力学（ALD）进行维度一致分析，确定其稳定条件，建立对不完美初始化和得分近似的维度鲁棒性，并通过数值实验验证理论。


<details>
  <summary>Details</summary>
Motivation: ALD在多峰目标采样上经验成功但理论上缺乏维度增加时稳定性保证，本文旨在填补理论与实践的差距。

Method: 对可用高斯混合模型近似的多峰目标进行连续时间ALD的维度一致分析，沿特定退火路径确定充分谱条件，推导不完美初始化和得分近似下的明确条件。

Result: 确定了ALD在单一维度一致时间范围内达到规定精度的充分谱条件，建立了对不完美初始化和得分近似的维度鲁棒性。

Conclusion: 理论分析得到了数值实验的验证，有助于理解和设计在高维问题中稳定的ALD算法。

Abstract: Designing algorithms that can explore multimodal target distributions accurately across successive refinements of an underlying high-dimensional problem is a central challenge in sampling. Annealed Langevin dynamics (ALD) is a widely used alternative to classical Langevin since it often yields much faster mixing on multimodal targets, but there is still a gap between this empirical success and existing theory: when, and under which design choices, can ALD be guaranteed to remain stable as dimension increases? In this paper, we help bridge this gap by providing a uniform-in-dimension analysis of continuous-time ALD for multimodal targets that can be well-approximated by Gaussian mixture models. Along an explicit annealing path obtained by progressively removing Gaussian smoothing of the target, we identify sufficient spectral conditions - linking smoothing covariance and the covariances of the Gaussian components of the mixture - under which ALD achieves a prescribed accuracy within a single, dimension-uniform time horizon. We then establish dimension-robustness to imperfect initialization and score approximation: under a misspecified-mixture score model, we derive explicit conditions showing that preconditioning the ALD algorithm with a sufficiently decaying spectrum is necessary to prevent error terms from accumulating across coordinates and destroying dimension-uniform control. Finally, numerical experiments illustrate and validate the theory.

</details>


### [819] [Generalized Inverses of Matrix Products: From Fundamental Subspaces to Randomized Decompositions](https://arxiv.org/abs/2602.00386)
*Michał P. Karpowicz,Gilbert Strang*

Main category: math.NA

TL;DR: 本文研究矩阵乘积的Moore - Penrose广义逆和广义逆，建立统一框架，给出多种公式，拓展到广义{1,2}-逆，揭示随机线性代数算法结构，并展示在稀疏传感器布局和有效电阻估计中的应用。


<details>
  <summary>Details</summary>
Motivation: 建立广义和随机矩阵逆的统一框架。

Method: 从第一性原理出发，聚焦四个基本子空间的几何性质进行研究。

Result: 得到了逆序律公式、通用正确公式和新的广义随机公式；框架拓展到广义{1,2}-逆；揭示了随机线性代数算法的底层结构；给出在稀疏传感器布局和有效电阻估计中的应用。

Conclusion: 所建立的框架可用于分析多种随机线性代数算法，在实际应用中如有效电阻估计有一定价值，且对有效电阻估计方案给出了严格误差分析。

Abstract: We investigate the Moore-Penrose pseudoinverse and generalized inverse of a matrix product $A=CR$ to establish a unifying framework for generalized and randomized matrix inverses. This analysis is rooted in first principles, focusing on the geometry of the four fundamental subspaces. We examine:
  (1) the reverse order law, $A^+ = R^+C^+$, which holds when $C$ has independent columns and $R$ has independent rows,
  (2) the universally correct formula, $A^+ = (C^+CR)^+(CRR^+)^+$, providing a geometric interpretation of the mappings between the involved subspaces,
  (3) a new generalized randomized formula, $A^+_p = (P^TA)^+P^TAQ(AQ)^+$, which gives $A^+_p = A^+$ if and only if the sketching matrices $P$ and $Q$ preserve the rank of $A$, i.e., $\mathrm{rank}(P^TA) = \mathrm{rank}(AQ) = \mathrm{rank}(A)$.
  The framework is extended to generalized $\{1,2\}$-inverses and specialized forms, revealing the underlying structure of established randomized linear algebra algorithms, including randomized SVD, the Nyström approximation, and CUR decomposition. We demonstrate applications in sparse sensor placement and effective resistance estimation. For the latter, we provide a rigorous quantitative analysis of an approximation scheme, establishing that it always underestimates the true resistance and deriving a worst-case spectral bound on the error of resistance differences.

</details>


### [820] [Nonlinear model reduction for transport-dominated problems](https://arxiv.org/abs/2602.01397)
*Jan S. Hesthaven,Benjamin Peherstorfer,Benjamin Unger*

Main category: math.NA

TL;DR: 文章调研非线性模型降阶方法，按关键元素组织技术并分类现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线性降维近似低效的情况下（如存在波现象和移动相干结构的传输主导问题）寻找有效非线性模型降阶方法。

Method: 围绕非线性参数化、降阶动态和在线求解器三个关键元素组织技术，并将现有方法分为基于变换的方法、在线自适应技术以及结合通用非线性参数化和瞬时残差最小化的公式。

Result: 完成非线性模型降阶方法的调研和技术组织、方法分类。

Conclusion: 未提及明确结论。

Abstract: This article surveys nonlinear model reduction methods that remain effective in regimes where linear reduced-space approximations are intrinsically inefficient, such as transport-dominated problems with wave-like phenomena and moving coherent structures, which are commonly associated with the Kolmogorov barrier. The article organizes nonlinear model reduction techniques around three key elements -- nonlinear parametrizations, reduced dynamics, and online solvers -- and categorizes existing approaches into transformation-based methods, online adaptive techniques, and formulations that combine generic nonlinear parametrizations with instantaneous residual minimization.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [821] [Counting Unit Circular Arc Intersections](https://arxiv.org/abs/2602.01074)
*Haitao Wang*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given a set of $n$ circular arcs of the same radius in the plane, we consider the problem of computing the number of intersections among the arcs. The problem was studied before and the previously best algorithm solves the problem in $O(n^{4/3+ε})$ time [Agarwal, Pellegrini, and Sharir, SIAM J. Comput., 1993], for any constant $ε>0$. No progress has been made on the problem for more than 30 years. We present a new algorithm of $O(n^{4/3}\log^{16/3}n)$ time and improve it to $O(n^{1+ε}+K^{1/3}n^{2/3}(\frac{n^2}{n+K})^ε\log^{16/3}n)$ time for small $K$, where $K$ is the number of intersections of all arcs.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [822] [Propagating the prior from far to near offset: A self-supervised diffusion framework for progressively recovering near-offsets of towed-streamer data](https://arxiv.org/abs/2602.01909)
*Shijun Cheng,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: 提出自监督扩散框架重建海洋拖缆地震采集缺失近偏移距道，在合成和实地数据上表现优于传统方法，实际应用可行。


<details>
  <summary>Details</summary>
Motivation: 现有海洋拖缆地震采集近偏移距道缺失重建方法存在运动学不一致、幅度失真等问题，监督深度学习方法缺乏真实近偏移距数据。

Method: 提出自监督扩散框架，利用远偏移距数据的重叠补丁提取和单道偏移训练条件扩散模型，推理时从最近记录偏移距递归外推到零偏移距。

Result: 在合成和实地数据集上性能优于传统抛物线Radon变换基线，实际应用可行，重建波形保留幅度随偏移距变化趋势，不确定性图可识别外推困难区域。

Conclusion: 所提方法有效解决近偏移距道缺失重建问题，无需近偏移距参考数据，有实际应用价值。

Abstract: In marine towed-streamer seismic acquisition, the nearest hydrophone is often two hundred meter away from the source resulting in missing near-offset traces, which degrades critical processing workflows such as surface-related multiple elimination, velocity analysis, and full-waveform inversion. Existing reconstruction methods, like transform-domain interpolation, often produce kinematic inconsistencies and amplitude distortions, while supervised deep learning approaches require complete ground-truth near-offset data that are unavailable in realistic acquisition scenarios. To address these limitations, we propose a self-supervised diffusion-based framework that reconstructs missing near-offset traces without requiring near-offset reference data. Our method leverages overlapping patch extraction with single-trace shifts from the available far-offset section to train a conditional diffusion model, which learns offset-dependent statistical patterns governing event curvature, amplitude variation, and wavelet characteristics. At inference, we perform trace-by-trace recursive extrapolation from the nearest recorded offset toward zero offset, progressively propagating learned prior information from far to near offsets. The generative formulation further provides uncertainty estimates via ensemble sampling, quantifying prediction confidence where validation data are absent. Controlled validation experiments on synthetic and field datasets show substantial performance gains over conventional parabolic Radon transform baselines. Operational deployment on actual near-offset gaps demonstrates practical viability where ground-truth validation is impossible. Notably, the reconstructed waveforms preserve realistic amplitude-versus-offset trends despite training exclusively on far-offset observations, and uncertainty maps accurately identify challenging extrapolation regions.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [823] [Autonomous Multi-Agent AI for High-Throughput Polymer Informatics: From Property Prediction to Generative Design Across Synthetic and Bio-Polymers](https://arxiv.org/abs/2602.00103)
*Mahule Roy,Adib Bazgir,Arthur da Silva Sousa Santos,Yuwen Zhang*

Main category: cond-mat.soft

TL;DR: 提出用于聚合物发现的多智能体AI生态系统，集成多种技术，展示三项实用能力，预测精度高、计算成本低且有元认知控制


<details>
  <summary>Details</summary>
Motivation: 构建一个集成系统以实现聚合物的高效发现和研究

Method: 统一高通量材料工作流、AI和计算建模至PRL管道，使用大语言模型驱动的专业智能体进行多任务执行

Result: PolyGNN智能体预测精度高，可提供不确定性估计，在计算成本和性能上优于基线方法，能进行元认知控制

Conclusion: 该集成多智能体AI生态系统在聚合物发现中可行，能高效完成多种任务并优化自身行为

Abstract: We present an integrated multiagent AI ecosystem for polymer discovery that unifies high-throughput materials workflows, artificial intelligence, and computational modeling within a single Polymer Research Lifecycle (PRL) pipeline. The system orchestrates specialized agents powered by state-of-the-art large language models (DeepSeek-V2 and DeepSeek-Coder) to retrieve and reason over scientific resources, invoke external tools, execute domain-specific code, and perform metacognitive self-assessment for robust end-to-end task execution. We demonstrate three practical capabilities: a high-fidelity polymer property prediction and generative design pipeline, a fully automated multimodal workflow for biopolymer structure characterization, and a metacognitive agent framework that can monitor performance and improve execution strategies over time. On a held-out test set of 1,251 polymers, our PolyGNN agent achieves strong predictive accuracy, reaching R2 = 0.89 for glass-transition temperature (Tg ), R2 = 0.82 for tensile strength, R2 = 0.75 for elongation, and R2 = 0.91 for density. The framework also provides uncertainty estimates via multiagent consensus and scales with linear complexity to at least 10,000 polymers, enabling high-throughput screening at low computational cost. For a representative workload, the system completes inference in 16.3 s using about 2 GB of memory and 0.1 GPU hours, at an estimated cost of about $0.08. On a dedicated Tg benchmark, our approach attains R2 = 0.78, outperforming strong baselines including single-LLM prediction (R2 = 0.67), group-contribution methods (R2 = 0.71), and ChemCrow (R2 = 0.66). We further demonstrate metacognitive control in a polystyrene case study, where the system not only produces domain-level scientific outputs but continually monitors and optimizes its own behavior through tactical, strategic, and meta-strategic self-assessment.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [824] [Audio-to-Image Bird Species Retrieval without Audio-Image Pairs via Text Distillation](https://arxiv.org/abs/2602.00681)
*Ilyass Moummad,Marius Miron,Lukas Rauch,David Robinson,Alexis Joly,Olivier Pietquin,Emmanuel Chemla,Matthieu Geist*

Main category: cs.SD

TL;DR: 提出无音频 - 图像监督的音频到图像检索方法，利用文本做语义中介，在多个生物声学基准测试中表现良好，证明文本间接语义转移有效。


<details>
  <summary>Details</summary>
Motivation: 音频到图像检索用于生物声学物种识别有更高可解释性，但配对音频 - 图像数据稀缺，学习对齐的音图表示具挑战性。

Method: 使用文本作为语义中介，将预训练图像 - 文本模型（BioCLIP - 2）的文本嵌入空间，通过对比目标微调预训练音频 - 文本模型（BioLingual）的音频编码器进行知识蒸馏。

Result: 蒸馏后的音频编码器保留音频判别力，提高音频 - 文本对齐度，在SSW60基准测试中音频到图像检索性能超基线。

Conclusion: 文本间接语义转移足以诱导有意义的音频 - 图像对齐，为数据稀缺生物声学场景提供视觉物种识别实用方案。

Abstract: Audio-to-image retrieval offers an interpretable alternative to audio-only classification for bioacoustic species recognition, but learning aligned audio-image representations is challenging due to the scarcity of paired audio-image data. We propose a simple and data-efficient approach that enables audio-to-image retrieval without any audio-image supervision. Our proposed method uses text as a semantic intermediary: we distill the text embedding space of a pretrained image-text model (BioCLIP-2), which encodes rich visual and taxonomic structure, into a pretrained audio-text model (BioLingual) by fine-tuning its audio encoder with a contrastive objective. This distillation transfers visually grounded semantics into the audio representation, inducing emergent alignment between audio and image embeddings without using images during training. We evaluate the resulting model on multiple bioacoustic benchmarks. The distilled audio encoder preserves audio discriminative power while substantially improving audio-text alignment on focal recordings and soundscape datasets. Most importantly, on the SSW60 benchmark, the proposed approach achieves strong audio-to-image retrieval performance exceeding baselines based on zero-shot model combinations or learned mappings between text embeddings, despite not training on paired audio-image data. These results demonstrate that indirect semantic transfer through text is sufficient to induce meaningful audio-image alignment, providing a practical solution for visually grounded species recognition in data-scarce bioacoustic settings.

</details>


### [825] [LPIPS-AttnWav2Lip: Generic Audio-Driven lip synchronization for Talking Head Generation in the Wild](https://arxiv.org/abs/2602.00189)
*Zhipeng Chen,Xinheng Wang,Lun Xie,Haijie Yuan,Hang Pan*

Main category: cs.SD

TL;DR: 本文提出LPIPS - AttnWav2Lip方法用于基于音频重构人脸图像，在唇同步精度和视觉质量上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决音频驱动的会说话头部生成中唇同步这一主要挑战，实现音频与嘴唇的视听一致性。

Method: 使用基于残差CBAM的U - Net架构，语义对齐模块扩展生成器网络感受野，采用LPIPS Loss。

Result: 通过主客观评价结果表明，该方法在唇同步精度和视觉质量方面取得了出色表现。

Conclusion: 该方法能有效实现精确的唇同步并生成逼真的高质量图像，代码已开源。

Abstract: Researchers have shown a growing interest in Audio-driven Talking Head Generation. The primary challenge in talking head generation is achieving audio-visual coherence between the lips and the audio, known as lip synchronization. This paper proposes a generic method, LPIPS-AttnWav2Lip, for reconstructing face images of any speaker based on audio. We used the U-Net architecture based on residual CBAM to better encode and fuse audio and visual modal information. Additionally, the semantic alignment module extends the receptive field of the generator network to obtain the spatial and channel information of the visual features efficiently; and match statistical information of visual features with audio latent vector to achieve the adjustment and injection of the audio content information to the visual information. To achieve exact lip synchronization and to generate realistic high-quality images, our approach adopts LPIPS Loss, which simulates human judgment of image quality and reduces instability possibility during the training process. The proposed method achieves outstanding performance in terms of lip synchronization accuracy and visual quality as demonstrated by subjective and objective evaluation results. The code for the paper is available at the following link: https://github.com/FelixChan9527/LPIPS-AttnWav2Lip

</details>


### [826] [Multi-Speaker Conversational Audio Deepfake: Taxonomy, Dataset and Pilot Study](https://arxiv.org/abs/2602.00295)
*Alabi Ahmed,Vandana Janeja,Sanjay Purushotham*

Main category: cs.SD

TL;DR: 本文针对多说话者对话式音频深度伪造检测问题，提出概念分类法，创建MsCADD数据集，对三个模型进行基准测试，结果显示多说话者深度伪造研究存在差距，数据集公开可用。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单说话者音频深度伪造检测，而多说话者对话场景下的恶意应用是未充分探索的威胁，需填补研究空白。

Method: 提出多说话者对话式音频深度伪造的概念分类法，创建MsCADD数据集，用VITS和SoundStorm - based NotebookLM模型生成数据，对LFCC - LCNN、RawNet2和Wav2Vec 2.0三个模型进行基准测试。

Result: 基准模型提供了有用的基准，但在不同对话动态下可靠检测合成语音方面，多说话者深度伪造研究存在显著差距。

Conclusion: 数据集和基准为对话场景下的深度伪造检测研究奠定基础，MsCADD数据集公开支持研究可重复性和基准测试。

Abstract: The rapid advances in text-to-speech (TTS) technologies have made audio deepfakes increasingly realistic and accessible, raising significant security and trust concerns. While existing research has largely focused on detecting single-speaker audio deepfakes, real-world malicious applications with multi-speaker conversational settings is also emerging as a major underexplored threat. To address this gap, we propose a conceptual taxonomy of multi-speaker conversational audio deepfakes, distinguishing between partial manipulations (one or multiple speakers altered) and full manipulations (entire conversations synthesized). As a first step, we introduce a new Multi-speaker Conversational Audio Deepfakes Dataset (MsCADD) of 2,830 audio clips containing real and fully synthetic two-speaker conversations, generated using VITS and SoundStorm-based NotebookLM models to simulate natural dialogue with variations in speaker gender, and conversational spontaneity. MsCADD is limited to text-to-speech (TTS) types of deepfake. We benchmark three neural baseline models; LFCC-LCNN, RawNet2, and Wav2Vec 2.0 on this dataset and report performance in terms of F1 score, accuracy, true positive rate (TPR), and true negative rate (TNR). Results show that these baseline models provided a useful benchmark, however, the results also highlight that there is a significant gap in multi-speaker deepfake research in reliably detecting synthetic voices under varied conversational dynamics. Our dataset and benchmarks provide a foundation for future research on deepfake detection in conversational scenarios, which is a highly underexplored area of research but also a major area of threat to trustworthy information in audio settings. The MsCADD dataset is publicly available to support reproducibility and benchmarking by the research community.

</details>


### [827] [HierCon: Hierarchical Contrastive Attention for Audio Deepfake Detection](https://arxiv.org/abs/2602.01032)
*Zhili Nicholas Liang,Soyeon Caren Han,Qizhou Wang,Christopher Leckie*

Main category: cs.SD

TL;DR: 提出HierCon框架检测音频深度伪造，在数据集上表现优异，证明分层建模可提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代TTS和语音转换系统生成的音频深度伪造难区分，现有检测器忽视关键依赖关系，存在安全和信任风险。

Method: 提出HierCon，结合分层层注意力框架和基于边界的对比学习，对时间帧、相邻层和层组的依赖关系建模。

Result: 在ASVspoof 2021 DF和In - the - Wild数据集上达到了最先进性能，EER分别为1.93%和6.87%，比独立层加权分别提高36.6%和22.5%。

Conclusion: 分层建模增强了对跨域生成技术和录制条件的泛化能力。

Abstract: Audio deepfakes generated by modern TTS and voice conversion systems are increasingly difficult to distinguish from real speech, raising serious risks for security and online trust. While state-of-the-art self-supervised models provide rich multi-layer representations, existing detectors treat layers independently and overlook temporal and hierarchical dependencies critical for identifying synthetic artefacts. We propose HierCon, a hierarchical layer attention framework combined with margin-based contrastive learning that models dependencies across temporal frames, neighbouring layers, and layer groups, while encouraging domain-invariant embeddings. Evaluated on ASVspoof 2021 DF and In-the-Wild datasets, our method achieves state-of-the-art performance (1.93% and 6.87% EER), improving over independent layer weighting by 36.6% and 22.5% respectively. The results and attention visualisations confirm that hierarchical modelling enhances generalisation to cross-domain generation techniques and recording conditions.

</details>


### [828] [TLDiffGAN: A Latent Diffusion-GAN Framework with Temporal Information Fusion for Anomalous Sound Detection](https://arxiv.org/abs/2602.01060)
*Chengyuan Ma,Peng Jia,Hongyue Guo,Wenming Yang*

Main category: cs.SD

TL;DR: 提出TLDiffGAN框架用于无监督异常声音检测，结合潜扩散模型与GAN，并引入TMixup技术，实验显示其检测和定位性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常声音检测生成模型无法充分捕捉正常声音复杂特征分布，扩散模型潜力待挖掘。

Method: 提出TLDiffGAN框架，包含将潜扩散模型融入GAN生成器进行对抗训练和利用预训练音频模型编码器辅助判别两个分支，引入TMixup频谱增强技术。

Result: 在DCASE 2020 Challenge Task 2数据集上实验表明TLDiffGAN检测性能优越，有强大异常时频定位能力。

Conclusion: TLDiffGAN框架能有效进行无监督异常声音检测与定位。

Abstract: Existing generative models for unsupervised anomalous sound detection are limited by their inability to fully capture the complex feature distribution of normal sounds, while the potential of powerful diffusion models in this domain remains largely unexplored. To address this challenge, we propose a novel framework, TLDiffGAN, which consists of two complementary branches. One branch incorporates a latent diffusion model into the GAN generator for adversarial training, thereby making the discriminator's task more challenging and improving the quality of generated samples. The other branch leverages pretrained audio model encoders to extract features directly from raw audio waveforms for auxiliary discrimination. This framework effectively captures feature representations of normal sounds from both raw audio and Mel spectrograms. Moreover, we introduce a TMixup spectrogram augmentation technique to enhance sensitivity to subtle and localized temporal patterns that are often overlooked. Extensive experiments on the DCASE 2020 Challenge Task 2 dataset demonstrate the superior detection performance of TLDiffGAN, as well as its strong capability in anomalous time-frequency localization.

</details>


### [829] [DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild](https://arxiv.org/abs/2602.02286)
*Arnab Das,Yassine El Kheir,Enes Erdem Erdogan,Feidi Kallel,Tim Polzehl,Sebastian Moeller*

Main category: cs.SD

TL;DR: 本文介绍为WildSpoof挑战赛开发的DFKI - Speech系统，提出了防欺骗自动说话人验证（SASV）框架。


<details>
  <summary>Details</summary>
Motivation: 开发适用于WildSpoof挑战赛的SASV系统。

Method: 提出SASV框架，欺骗检测器采用自监督语音嵌入提取器和图神经网络，用MoE融合特征；说话人验证采用低复杂度卷积神经网络融合多尺度特征，用SphereFace损失训练，应用对比圆损失，还使用AS Norm分数归一化和模型集成。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: This paper presents the DFKI-Speech system developed for the WildSpoof Challenge under the Spoofing aware Automatic Speaker Verification (SASV) track. We propose a robust SASV framework in which a spoofing detector and a speaker verification (SV) network operate in tandem. The spoofing detector employs a self-supervised speech embedding extractor as the frontend, combined with a state-of-the-art graph neural network backend. In addition, a top-3 layer based mixture-of-experts (MoE) is used to fuse high-level and low-level features for effective spoofed utterance detection. For speaker verification, we adapt a low-complexity convolutional neural network that fuses 2D and 1D features at multiple scales, trained with the SphereFace loss. Additionally, contrastive circle loss is applied to adaptively weight positive and negative pairs within each training batch, enabling the network to better distinguish between hard and easy sample pairs. Finally, fixed imposter cohort based AS Norm score normalization and model ensembling are used to further enhance the discriminative capability of the speaker verification system.

</details>


### [830] [Masked Autoencoders as Universal Speech Enhancer](https://arxiv.org/abs/2602.02413)
*Rajalaxmi Rajagopalan,Ritwik Giri,Zhiqiang Tang,Kyu Han*

Main category: cs.SD

TL;DR: 本文提出基于掩码自编码器的通用语音增强器，自监督训练，在去噪和去混响任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 实际场景缺少干净语音，期望自监督学习的语音增强方法用于下游任务。

Method: 开发掩码自编码器通用语音增强器，用增强堆栈添加失真，预训练学习去除失真和重构频谱图，微调预训练嵌入用于下游任务，探索不同增强和特征表示的影响。

Result: 提出的方法优于基线，在域内和域外评估数据集上达到了最先进的性能。

Conclusion: 基于掩码自编码器的自监督语音增强方法有效，能用于下游任务并取得良好效果。

Abstract: Supervised speech enhancement methods have been very successful. However, in practical scenarios, there is a lack of clean speech, and self-supervised learning-based (SSL) speech enhancement methods that offer comparable enhancement performance and can be applied to other speech-related downstream applications are desired. In this work, we develop a masked autoencoder based universal speech enhancer that is agnostic to the type of distortion affecting speech, can handle multiple distortions simultaneously, and is trained in a self-supervised manner. An augmentation stack adds further distortions to the noisy input data. The masked autoencoder model learns to remove the added distortions along with reconstructing the masked regions of the spectrogram during pre-training. The pre-trained embeddings are then used by fine-tuning models trained on a small amount of paired data for specific downstream tasks. We evaluate the pre-trained features for denoising and dereverberation downstream tasks. We explore different augmentations (like single or multi-speaker) in the pre-training augmentation stack and the effect of different noisy input feature representations (like $log1p$ compression) on pre-trained embeddings and downstream fine-tuning enhancement performance. We show that the proposed method not only outperforms the baseline but also achieves state-of-the-art performance for both in-domain and out-of-domain evaluation datasets.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [831] [A Formal Approach to AMM Fee Mechanisms with Lean 4](https://arxiv.org/abs/2602.00101)
*Marco Dessalvi,Massimo Bartoletti,Alberto Lluch-Lafuente*

Main category: q-fin.MF

TL;DR: 本文聚焦DeFi中AMM，引入交易费用参数扩展模型，从经济视角分析，证明了关键性质，得出交易策略结论并给出套利问题的闭式解。


<details>
  <summary>Details</summary>
Motivation: DeFi中AMM的交易费用会影响用户交易策略，现有模型为简化分析常忽略，因此需开发精确考虑费用影响的模型。

Method: 在交换率函数中引入交易费用参数φ，从经济角度分析新模型，用Lean 4证明助手进行形式化和机器验证。

Result: 保留了交换率函数的输出有界性和单调性，但可加性不再成立；φ< 1时，单次大交易比拆分成小交易利润更高；得出有交易费用时套利问题的闭式解并证明其唯一性。

Conclusion: 引入交易费用的AMM模型对经济性质有影响，据此得出交易策略和套利问题的结论。

Abstract: Decentralized Finance (DeFi) has revolutionized financial markets by enabling complex asset-exchange protocols without trusted intermediaries. Automated Market Makers (AMMs) are a central component of DeFi, providing the core functionality of swapping assets of different types at algorithmically computed exchange rates. Several mainstream AMM implementations are based on the constant-product model, which ensures that swaps preserve the product of the token reserves in the AMM -- up to a \emph{trading fee} used to incentivize liquidity provision. Trading fees substantially complicate the economic properties of AMMs, and for this reason some AMM models abstract them away in order to simplify the analysis. However, trading fees have a non-trivial impact on users' trading strategies, making it crucial to develop refined AMM models that precisely account for their effects. We extend a foundational model of AMMs by introducing a new parameter, the trading fee $φ\in(0,1]$, into the swap rate function. Fee amounts increase inversely proportional to $φ$. When $φ= 1$, no fee is applied and the original model is recovered. We analyze the resulting fee-adjusted model from an economic perspective. We show that several key properties of the swap rate function, including output-boundedness and monotonicity, are preserved. At the same time, other properties - most notably additivity - no longer hold. We precisely characterize this deviation by deriving a generalized form of additivity that captures the effect of swaps in the presence of trading fees. We prove that when $φ< 1$, executing a single large swap yields strictly greater profit than splitting the trade into smaller ones. Finally, we derive a closed-form solution to the arbitrage problem in the presence of trading fees and prove its uniqueness. All results are formalized and machine-checked in the Lean 4 proof assistant.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [832] [Frequent Pattern Mining approach to Image Compression](https://arxiv.org/abs/2602.00100)
*Avinash Kadimisetty,C. Oswald,B. Sivalselvan*

Main category: eess.IV

TL;DR: 文章提出基于频繁模式挖掘的图像压缩机制，用聚类和优化算法处理冗余数据，测试显示压缩率提高45%，视觉质量损失小。


<details>
  <summary>Details</summary>
Motivation: 在图像压缩中探索更高效的方法，有效处理图像冗余数据。

Method: 通过聚类相似像素，用k - means聚类和闭频繁序列挖掘替代传统JPEG的DCT阶段，优化传统广义序列模式挖掘（GSP）算法的剪枝技术，提出计算序列频率的机制。

Result: 压缩基准数据集时压缩率提高45%，常优于现有方法；图像质量指标PSNR和SSIM显示视觉质量损失可忽略不计。

Conclusion: 提出的基于频繁模式挖掘的图像压缩机制高效，能在提高压缩率的同时保证较低的视觉质量损失。

Abstract: The paper focuses on Image Compression, explaining efficient approaches based on Frequent Pattern Mining(FPM). The proposed compression mechanism is based on clustering similar pixels in the image and thus using cluster identifiers in image compression. Redundant data in the image is effectively handled by replacing the DCT phase of conventional JPEG through a mixture of k-means Clustering and Closed Frequent Sequence Mining. To optimize the cardinality of pattern(s) in encoding, efficient pruning techniques have been used through the refinement of Conventional Generalized Sequential Pattern Mining(GSP) algorithm. We have proposed a mechanism for finding the frequency of a sequence which will yield significant reduction in the code table size. The algorithm is tested by compressing benchmark datasets yielding an improvement of 45% in compression ratios, often outperforming the existing alternatives. PSNR and SSIM, which are the image quality metrics, have been tested which show a negligible loss in visual quality.

</details>


### [833] [Radiomics in Medical Imaging: Methods, Applications, and Challenges](https://arxiv.org/abs/2602.00102)
*Fnu Neha,Deepak kumar Shukla*

Main category: eess.IV

TL;DR: 本文对放射组学流程进行端到端分析，探讨各阶段方法决策影响，回顾多种策略和模型，讨论临床应用，指出挑战并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有综述多关注特定应用结果或孤立流程组件，缺乏对各阶段相互依赖设计选择如何共同影响鲁棒性和泛化性的分析。

Method: 对放射组学流程进行端到端分析，回顾特征提取、选择、降维策略，经典机器学习和深度学习建模方法，集成和混合框架等。

Result: 探讨了各阶段方法决策对特征稳定性、模型可靠性和转化有效性的影响，讨论了临床应用。

Conclusion: 识别了标准化、领域偏移和临床部署等开放挑战，给出了混合放射组学 - 人工智能模型、多模态融合等未来方向。

Abstract: Radiomics enables quantitative medical image analysis by converting imaging data into structured, high-dimensional feature representations for predictive modeling. Despite methodological developments and encouraging retrospective results, radiomics continue to face persistent challenges related to feature instability, limited reproducibility, validation bias, and restricted clinical translation. Existing reviews largely focus on application-specific outcomes or isolated pipeline components, with limited analysis of how interdependent design choices across acquisition, preprocessing, feature engineering, modeling, and evaluation collectively affect robustness and generalizability. This survey provides an end-to-end analysis of radiomics pipelines, examining how methodological decisions at each stage influence feature stability, model reliability, and translational validity. This paper reviews radiomic feature extraction, selection, and dimensionality reduction strategies; classical machine and deep learning-based modeling approaches; and ensemble and hybrid frameworks, with emphasis on validation protocols, data leakage prevention, and statistical reliability. Clinical applications are discussed with a focus on evaluation rigor rather than reported performance metrics. The survey identifies open challenges in standardization, domain shift, and clinical deployment, and outlines future directions such as hybrid radiomics-artificial intelligence models, multimodal fusion, federated learning, and standardized benchmarking.

</details>


### [834] [Visible Singularities Guided Correlation Network for Limited-Angle CT Reconstruction](https://arxiv.org/abs/2602.00184)
*Yiyang Wen,Liu Shi,Zekun Zhou,WenZhe Shan,Qiegen Liu*

Main category: eess.IV

TL;DR: 提出用于LACT重建的VSGC网络，考虑LACT成像特征，设计含两步的网络，用多尺度损失函数约束，经数据集验证有效，小角度范围性能突出。


<details>
  <summary>Details</summary>
Motivation: 传统LACT重建算法有局限，现有深度学习方法未充分考虑LACT核心成像特征。

Method: 提出VSGC网络，先提取VS边缘特征并聚焦，再建立与图像其他区域关联，采用带各向异性约束的多尺度损失函数。

Result: 在模拟和真实数据集上验证了有效性和可行性，小角度范围PSNR提高2.45dB，SSIM提高1.5%。

Conclusion: 提出的VSGC网络有效可行，在小角度范围表现优于其他方法，代码公开。

Abstract: Limited-angle computed tomography (LACT) offers the advantages of reduced radiation dose and shortened scanning time. Traditional reconstruction algorithms exhibit various inherent limitations in LACT. Currently, most deep learning-based LACT reconstruction methods focus on multi-domain fusion or the introduction of generic priors, failing to fully align with the core imaging characteristics of LACT-such as the directionality of artifacts and directional loss of structural information, which are caused by the absence of projection angles in certain directions. Inspired by the theory of visible and invisible singularities, taking into account the aforementioned core imaging characteristics of LACT, we propose a Visible Singularities Guided Correlation network for LACT reconstruction (VSGC). The design philosophy of VSGC consists of two core steps: First, extract VS edge features from LACT images and focus the model's attention on these VS. Second, establish correlations between the VS edge features and other regions of the image. Additionally, a multi-scale loss function with anisotropic constraint is employed to constrain the model to converge in multiple aspects. Finally, qualitative and quantitative validations are conducted on both simulated and real datasets to verify the effectiveness and feasibility of the proposed design. Particularly, in comparison with alternative methods, VSGC delivers more prominent performance in small angular ranges, with the PSNR improvement of 2.45 dB and the SSIM enhancement of 1.5\%. The code is publicly available at https://github.com/yqx7150/VSGC.

</details>


### [835] [SCALED : Surrogate-gradient for Codec-Aware Learning of Downsampling in ABR Streaming](https://arxiv.org/abs/2602.00198)
*Esteban Pesnel,Julien Le Tanou,Michael Ropert,Thomas Maugey,Aline Roumy*

Main category: eess.IV

TL;DR: 提出用真实不可微编解码器进行端到端训练的框架，实验显示比不考虑编解码器的训练方法有BD - BR（PSNR）5.19%的提升。


<details>
  <summary>Details</summary>
Motivation: 视频消费增长对现代流媒体架构带来挑战，传统ABR管道各处理阶段孤立优化导致端到端率失真性能不佳，现有可微代理编解码器是近似，无证据表明训练中使用标准编解码器的低效性。

Method: 引入基于实际压缩误差的数据驱动替代梯度，实现用真实不可微编解码器进行端到端训练。

Result: 实验显示在多个下采样率的率失真凸包上，相比不考虑编解码器的训练方法，BD - BR（PSNR）有5.19%的提升。

Conclusion: 该框架能促进训练目标和部署性能的一致性，在视频流处理中有更好的效果。

Abstract: The rapid growth in video consumption has introduced significant challenges to modern streaming architectures. Over-the-Top (OTT) video delivery now predominantly relies on Adaptive Bitrate (ABR) streaming, which dynamically adjusts bitrate and resolution based on client-side constraints such as display capabilities and network bandwidth. This pipeline typically involves downsampling the original high-resolution content, encoding and transmitting it, followed by decoding and upsampling on the client side. Traditionally, these processing stages have been optimized in isolation, leading to suboptimal end-to-end rate-distortion (R-D) performance. The advent of deep learning has spurred interest in jointly optimizing the ABR pipeline using learned resampling methods. However, training such systems end-to-end remains challenging due to the non-differentiable nature of standard video codecs, which obstructs gradient-based optimization. Recent works have addressed this issue using differentiable proxy models, based either on deep neural networks or hybrid coding schemes with differentiable components such as soft quantization, to approximate the codec behavior. While differentiable proxy codecs have enabled progress in compression-aware learning, they remain approximations that may not fully capture the behavior of standard, non-differentiable codecs. To our knowledge, there is no prior evidence demonstrating the inefficiencies of using standard codecs during training. In this work, we introduce a novel framework that enables end-to-end training with real, non-differentiable codecs by leveraging data-driven surrogate gradients derived from actual compression errors. It facilitates the alignment between training objectives and deployment performance. Experimental results show a 5.19\% improvement in BD-BR (PSNR) compared to codec-agnostic training approaches, consistently across the entire rate-distortion convex hull spanning multiple downsampling ratios.

</details>


### [836] [MarkCleaner: High-Fidelity Watermark Removal via Imperceptible Micro-Geometric Perturbation](https://arxiv.org/abs/2602.01513)
*Xiaoxi Kong,Jieyu Yuan,Pengdi Chen,Yuanlin Zhang,Chongyi Li,Bin Li*

Main category: eess.IV

TL;DR: 本文指出语义水印在微几何扰动下鲁棒性不佳，提出MarkCleaner水印去除框架，实验表明其在去除效果和视觉保真度上表现出色且能实时推理。


<details>
  <summary>Details</summary>
Motivation: 发现语义水印在微几何扰动下鲁棒性不足，且基于再生的水印去除会导致语义漂移，需新的水印去除方法。

Method: MarkCleaner采用微几何扰动监督进行训练，使用掩码引导编码器学习显式空间表示，基于2D高斯溅射的解码器在保留语义内容时显式参数化几何扰动。

Result: MarkCleaner在水印去除有效性和视觉保真度上表现优越，能实现高效实时推理。

Conclusion: MarkCleaner是一种有效的水印去除框架，代码待接收后公开。

Abstract: Semantic watermarks exhibit strong robustness against conventional image-space attacks. In this work, we show that such robustness does not survive under micro-geometric perturbations: spatial displacements can remove watermarks by breaking the phase alignment. Motivated by this observation, we introduce MarkCleaner, a watermark removal framework that avoids semantic drift caused by regeneration-based watermark removal. Specifically, MarkCleaner is trained with micro-geometry-perturbed supervision, which encourages the model to separate semantic content from strict spatial alignment and enables robust reconstruction under subtle geometric displacements. The framework adopts a mask-guided encoder that learns explicit spatial representations and a 2D Gaussian Splatting-based decoder that explicitly parameterizes geometric perturbations while preserving semantic content. Extensive experiments demonstrate that MarkCleaner achieves superior performance in both watermark removal effectiveness and visual fidelity, while enabling efficient real-time inference. Our code will be made available upon acceptance.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [837] [HuPER: A Human-Inspired Framework for Phonetic Perception](https://arxiv.org/abs/2602.01634)
*Chenxu Guo,Jiachen Lian,Yisi Liu,Baihe Huang,Shriyaa Narayanan,Cheol Jun Cho,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: 提出HuPER框架，用100小时训练数据在英语基准测试达SOTA，可零样本迁移到95种语言，支持自适应多路径语音感知且相关资源开源。


<details>
  <summary>Details</summary>
Motivation: 希望构建能基于声学语音证据和语言知识进行自适应推理以实现语音感知的框架。

Method: 提出人类启发的HuPER框架。

Result: 使用仅100小时训练数据，在五个英语基准测试中实现了最先进的语音错误率，能零样本迁移到95种未见语言，支持在不同声学条件下进行自适应、多路径语音感知。

Conclusion: 提出了有效的语音感知框架HuPER并开源相关资源。

Abstract: We propose HuPER, a human-inspired framework that models phonetic perception as adaptive inference over acoustic-phonetics evidence and linguistic knowledge. With only 100 hours of training data, HuPER achieves state-of-the-art phonetic error rates on five English benchmarks and strong zero-shot transfer to 95 unseen languages. HuPER is also the first framework to enable adaptive, multi-path phonetic perception under diverse acoustic conditions. All training data, models, and code are open-sourced. Code and demo avaliable at https://github.com/HuPER29/HuPER.

</details>


### [838] [SSNAPS: Audio-Visual Separation of Speech and Background Noise with Diffusion Inverse Sampling](https://arxiv.org/abs/2602.01394)
*Yochai Yemini,Yoav Ellinson,Rami Ben-Ari,Sharon Gannot,Ethan Fetaya*

Main category: eess.AS

TL;DR: 该论文用生成式逆采样解决视听单麦克风语音分离和增强问题，无监督方法在WER上优于监督基线，还可扩展处理离屏说话者分离及用于声学场景检测。


<details>
  <summary>Details</summary>
Motivation: 解决现实环境噪声下视听单麦克风语音分离和增强的挑战。

Method: 基于生成式逆采样，用专用扩散先验对干净语音和环境噪声建模，联合利用它们恢复所有潜在源，并重新调整逆采样器。

Result: 在含噪声的1、2、3个说话者的混合语音上评估，无监督方法在所有条件下WER均优于领先的监督基线，可扩展处理离屏说话者分离，分离出的噪声适用于声学场景检测。

Conclusion: 该方法能有效解决视听单麦克风语音分离和增强问题，在性能和应用上有出色表现。

Abstract: This paper addresses the challenge of audio-visual single-microphone speech separation and enhancement in the presence of real-world environmental noise. Our approach is based on generative inverse sampling, where we model clean speech and ambient noise with dedicated diffusion priors and jointly leverage them to recover all underlying sources. To achieve this, we reformulate a recent inverse sampler to match our setting. We evaluate on mixtures of 1, 2, and 3 speakers with noise and show that, despite being entirely unsupervised, our method consistently outperforms leading supervised baselines in \ac{WER} across all conditions. We further extend our framework to handle off-screen speaker separation. Moreover, the high fidelity of the separated noise component makes it suitable for downstream acoustic scene detection. Demo page: https://ssnapsicml.github.io/ssnapsicml2026/

</details>


### [839] [RIR-Former: Coordinate-Guided Transformer for Continuous Reconstruction of Room Impulse Responses](https://arxiv.org/abs/2602.01861)
*Shaoheng Xu,Chunyi Sun,Jihui,Zhang,Prasanga N. Samarasinghe,Thushara D. Abhayapala*

Main category: eess.AS

TL;DR: 提出RIR - Former模型用于RIR重建，在实验中表现优于现有基线，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 空间密集测量RIR不实际，需有效重建方法。

Method: 提出RIR - Former模型，引入正弦编码模块结合麦克风位置信息，设计分段多分支解码器处理早期反射和后期混响。

Result: 在不同模拟声学环境实验中，RIR - Former在NMSE和CD指标上始终优于现有基线。

Conclusion: 该方法有实际部署潜力，可激励未来在复杂阵列几何、动态声学场景和真实环境的研究。

Abstract: Room impulse responses (RIRs) are essential for many acoustic signal processing tasks, yet measuring them densely across space is often impractical. In this work, we propose RIR-Former, a grid-free, one-step feed-forward model for RIR reconstruction. By introducing a sinusoidal encoding module into a transformer backbone, our method effectively incorporates microphone position information, enabling interpolation at arbitrary array locations. Furthermore, a segmented multi-branch decoder is designed to separately handle early reflections and late reverberation, improving reconstruction across the entire RIR. Experiments on diverse simulated acoustic environments demonstrate that RIR-Former consistently outperforms state-of-the-art baselines in terms of normalized mean square error (NMSE) and cosine distance (CD), under varying missing rates and array configurations. These results highlight the potential of our approach for practical deployment and motivate future work on scaling from randomly spaced linear arrays to complex array geometries, dynamic acoustic scenes, and real-world environments.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [840] [Artificial Intelligence and Symmetries: Learning, Encoding, and Discovering Structure in Physical Data](https://arxiv.org/abs/2602.02351)
*Veronica Sanz*

Main category: hep-ph

TL;DR: 本文探讨物理对称性与机器学习的相互作用，聚焦数据驱动方法和潜在表征学习。


<details>
  <summary>Details</summary>
Motivation: 研究如何用机器学习技术识别、编码或诊断对称性诱导的约束。

Method: 采用数据驱动方法和潜在表征学习，特别关注变分自编码器。

Result: 回顾了简单几何系统和粒子物理过程的案例研究结果。

Conclusion: 分析了在无显式归纳偏置情况下推断对称结构的理论和实际局限性。

Abstract: Symmetries play a central role in physics, organizing dynamics, constraining interactions, and determining the effective number of physical degrees of freedom. In parallel, modern artificial intelligence methods have demonstrated a remarkable ability to extract low-dimensional structure from high-dimensional data through representation learning. This review examines the interplay between these two perspectives, focusing on the extent to which symmetry-induced constraints can be identified, encoded, or diagnosed using machine learning techniques.
  Rather than emphasizing architectures that enforce known symmetries by construction, we concentrate on data-driven approaches and latent representation learning, with particular attention to variational autoencoders. We discuss how symmetries and conservation laws reduce the intrinsic dimensionality of physical datasets, and how this reduction may manifest itself through self-organization of latent spaces in generative models trained to balance reconstruction and compression. We review recent results, including case studies from simple geometric systems and particle physics processes, and analyze the theoretical and practical limitations of inferring symmetry structure without explicit inductive bias.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [841] [Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025](https://arxiv.org/abs/2602.00982)
*Phu-Hoa Pham,Chi-Nguyen Tran,Dao Sy Duy Minh,Nguyen Lam Phu Quy,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: 介绍Team HCMUS_TheFangs在NeurIPS 2025竞赛两赛道获胜方法，分析训练时长与性能关系，挑战模型复杂度传统假设。


<details>
  <summary>Details</summary>
Motivation: 解决开发能匹配生物视觉系统的人工代理时，视觉鲁棒性和神经对齐的关键挑战。

Method: 赛道1用带门控线性单元和观测归一化的轻量级两层CNN；赛道2用含16个卷积层和基于GLU门控的类ResNet架构；分析10个模型检查点，进行消融研究和失败案例分析。

Result: 赛道1最终得分95.4%；赛道2实现顶级神经预测性能；训练时长与性能呈非单调关系，约200K步最优。

Conclusion: 挑战了视觉运动学习中模型复杂度的传统假设，为开发鲁棒、受生物启发的视觉代理提供实用指导。

Abstract: Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.

</details>


### [842] [Deep learning enables urban change profiling through alignment of historical maps](https://arxiv.org/abs/2602.02154)
*Sidi Wu,Yizi Chen,Maurizio Gribaudi,Konrad Schindler,Clément Mallet,Julien Perret,Lorenz Hurni*

Main category: cs.CV

TL;DR: 本文提出基于深度学习的自动化框架用于从历史地图集进行细粒度城市变化分析，实验证明方法有效且框架适用于不同制图情境。


<details>
  <summary>Details</summary>
Motivation: 从历史地图系列中提取一致和细粒度的变化信息具有挑战，以往分析多为小规模或定性方法，需要系统性、定量分析方法。

Method: 提出基于深度学习的全自动化框架，集成了密集地图对齐、多时间对象检测和变化分析功能。

Result: 实验证明了对齐和对象检测方法性能良好，应用于巴黎显示框架能揭示城市转型的时空异质性。

Conclusion: 该框架将历史地图分析从临时的视觉比较转向系统、定量的城市变化表征，模块化设计支持适应不同制图情境和下游应用。

Abstract: Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.

</details>


### [843] [EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions](https://arxiv.org/abs/2602.00095)
*Weiyu Sun,Liangliang Chen,Yongnuo Cai,Huiru Xie,Yi Zeng,Ying Zhang*

Main category: cs.CV

TL;DR: 本文发布EDU - CIRCUIT - HW数据集评估多模态大语言模型（MLLMs）对学生手写内容识别能力，发现潜在问题并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: MLLMs在教育领域有潜力，但缺乏特定领域基准评估其对手写内容的理解，现有评估范式有局限。

Method: 发布包含1300多个真实学生手写解决方案的EDU - CIRCUIT - HW数据集，同时评估MLLMs的上游识别保真度和下游自动评分性能。

Result: 发现MLLM识别学生手写内容存在大量潜在失败，在高风险教育场景中可靠性不足。

Conclusion: 利用识别出的错误模式，少量人工干预（约4%的解决方案）可显著提高AI评分系统在未见解决方案上的鲁棒性。

Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.

</details>


### [844] [Mirage2Matter: A Physically Grounded Gaussian World Model from Video](https://arxiv.org/abs/2602.00096)
*Zhengqing Gao,Ziwen Li,Xin Wang,Jiaxin Huang,Zhenyang Ren,Mingkai Shao,Hanlue Zhang,Tianyu Huang,Yongkang Cheng,Yandong Guo,Runqi Lin,Yuanyuan Wang,Tongliang Liu,Kun Zhang,Mingming Gong*

Main category: cs.CV

TL;DR: 提出Simulate Anything框架，用多视角视频和现成资产生成高保真具身训练数据，训练的模型在下游任务有良好零样本性能。


<details>
  <summary>Details</summary>
Motivation: 具身智能可扩展性受真实交互数据稀缺限制，现有模拟平台存在视觉和物理差距、依赖昂贵传感器等问题，实用性受限。

Method: 用3D高斯 splatting将真实环境重建为逼真场景表示，利用生成模型恢复物理逼真表示，通过精确校准目标集成到模拟环境。

Result: Vision Language Action (VLA)模型在下游任务零样本性能强，可媲美或超越真实数据训练结果。

Conclusion: 基于重建的世界建模对可扩展和实用的具身智能训练有潜力。

Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.

</details>


### [845] [R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation](https://arxiv.org/abs/2602.00104)
*Zhuohong Chen,Zhengxian Wu,Zirui Liao,Shenao Jiang,Hangrui Xu,Yang Chen,Chaokui Su,Xiaoyu Liu,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出R3G框架解决视觉问答视觉中心检索问题，在MRAG - Bench上表现出色并开源代码数据


<details>
  <summary>Details</summary>
Motivation: 解决视觉问答中视觉中心检索选图和高效集成到模型推理的挑战

Method: 提出R3G模块化推理 - 检索 - 重排框架，先制定推理计划，再采用粗检索加细粒度重排的两步策略选图

Result: 在MRAG - Bench上，R3G提升六种MLLM主干和九个子场景的准确率，达到整体最优性能，消融实验显示充足感知重排和推理步骤互补

Conclusion: R3G框架能有效解决视觉中心检索选图及集成问题

Abstract: Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.

</details>


### [846] [HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models](https://arxiv.org/abs/2602.00105)
*Wing Chan,Richard Allen*

Main category: cs.CV

TL;DR: 介绍HYPE - EDIT - 1基准测试，评估图像编辑模型，给出各模型表现及成本情况。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型公开演示多为最佳样本，实际工作需考虑重试和审核时间，故需评估模型。

Method: 引入含100个任务的HYPE - EDIT - 1基准测试，对每个任务生成10个独立输出，用二进制通过/失败评判，计算相关指标。

Result: 各模型单次尝试通过率为34 - 83%，每次成功有效成本为0.66 - 1.42美元，低单图定价模型考虑重试和人工审核总成本更高。

Conclusion: 论文通过基准测试提供了评估图像编辑模型实际性能和成本的有效方法。

Abstract: Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.

</details>


### [847] [SITUATE -- Synthetic Object Counting Dataset for VLM training](https://arxiv.org/abs/2602.00108)
*René Peinl,Vincent Tischler,Patrick Schröder,Christian Groth*

Main category: cs.CV

TL;DR: 介绍了用于训练和评估视觉语言模型计数任务的SITUATE数据集，实验表明该数据集有助于提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 填补简单2D数据集和现实数据集在处理计数任务时的差距，解决现有数据集缺乏对遮挡和空间组成控制的问题。

Method: 创建SITUATE数据集，并在该数据集上微调Qwen VL 2.5 7B模型，与其他计数基准和来自Pixmo count的微调集进行对比。

Result: 在SITUATE上微调的Qwen VL 2.5 7B模型提高了在Pixmo count测试数据上的准确性，反之则不然。

Conclusion: SITUATE数据集有助于提高视觉语言模型在计数任务上对分布外图像的泛化能力。

Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.

</details>


### [848] [1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization](https://arxiv.org/abs/2602.00114)
*Yunwei Bai,Ying Kiat Tan,Yao Shu,Tsuhan Chen*

Main category: cs.CV

TL;DR: 提出1S - DAug用于少样本学习，可在测试时从单张图像生成有效变体，提升FSL性能


<details>
  <summary>Details</summary>
Motivation: 传统测试时间增强方法在少样本学习中无效，为提升少样本学习模型泛化能力

Method: 结合传统几何扰动、受控噪声注入和基于原始图像的去噪扩散过程，生成图像编码聚合得到组合表示

Result: 作为免训练、模型无关插件，在4个不同数据集标准基准上持续提升FSL性能，在miniImagenet 5 - way - 1 - shot基准上有超10%比例的准确率提升

Conclusion: 1S - DAug能有效提升少样本学习模型性能，且无需更新模型参数

Abstract: Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.

</details>


### [849] [IC-EO: Interpretable Code-based assistant for Earth Observation](https://arxiv.org/abs/2602.00117)
*Lamia Lahouel,Laurynas Lopata,Simon Gruening,Gabriele Meoni,Gaetan Petit,Sylvain Lobry*

Main category: cs.CV

TL;DR: 本文提出对话式代码生成代理，将自然语言查询转换为可执行、可审计的Python工作流，在两个用例中表现优于通用基准，使EO分析透明可复现。


<details>
  <summary>Details</summary>
Motivation: 当前地球观测（EO）分析对非专业人员困难，且系统预测难审计和复现，需要改进。

Method: 利用大语言模型，提出对话式代码生成代理，通过统一易扩展API操作，可在三个级别控制结果。

Result: 在土地组成映射和野火后损失评估两个用例中，提出的代理优于通用LLM/VLM基线，准确率更高，结果透明易解释。

Conclusion: 通过输出可验证代码，该方法使EO分析成为透明、可复现的过程。

Abstract: Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances in tool LLMs, this study proposes a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows. The agent operates over a unified easily extendable API for classification, segmentation, detection (oriented bounding boxes), spectral indices, and geospatial operators. With our proposed framework, it is possible to control the results at three levels: (i) tool-level performance on public EO benchmarks; (ii) at the agent-level to understand the capacity to generate valid, hallucination-free code; and (iii) at the task-level on specific use cases. In this work, we select two use-cases of interest: land-composition mapping and post-wildfire damage assessment. The proposed agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis, while producing results that are transparent and easy to interpret. By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process.

</details>


### [850] [VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents](https://arxiv.org/abs/2602.00122)
*Hongzhu Yi,Yujia Yang,Yuanxiang Wang,Zhenyu Guan,Jiahuan Chen,Chenxi Bao,Tiankun Yang,Yixuan Yuan,Tianyu Zong,Xinming Wang,Tao Yu,Ruiwen Tao,Haijin Liang,Jin Ma,Jinwen Luo,Yeshani Xinyu Zuo,Jungang Xu*

Main category: cs.CV

TL;DR: 提出VDE Bench基准用于多语言复杂视觉文档图像编辑模型评估，含数据集和评估框架，评估了代表性模型，验证指标与人工判断一致。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型主要针对英文和文本布局稀疏文档，无法处理密集、结构复杂文档和非拉丁文字，需新基准评估多语言和复杂视觉文档编辑任务。

Method: 提出VDE Bench基准，含高质量中英密集文本文档数据集；引入解耦评估框架，在OCR解析层面量化编辑性能。

Result: 基于该基准全面评估代表性模型，人工验证表明自动评估指标与人类判断高度一致。

Conclusion: VDE Bench是首个系统评估多语言和密集文本文档图像编辑模型的基准。

Abstract: In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \textbf{V}isual \textbf{D}oc \textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.

</details>


### [851] [Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers](https://arxiv.org/abs/2602.00144)
*Xuan Rao,Mingming Ha,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.CV

TL;DR: 提出LR - RGDA和HopDC解决ViT在CIL中分类器重建的计算瓶颈及表示漂移问题，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: ViT在CIL的分类器重建阶段存在计算瓶颈，现有方法依赖昂贵的迭代随机梯度下降，RGDA虽有优势但推理复杂度高，且骨干更新会导致表示漂移。

Method: 提出LR - RGDA，利用Woodbury矩阵恒等式分解判别函数降低推理复杂度；引入HopDC，用连续Hopfield网络通过无标签锚点的关联记忆动态重新校准历史类统计。

Result: 在不同CIL基准测试中取得了最先进的性能。

Conclusion: 所提框架为基于ViT的大规模类增量学习提供了可扩展的解决方案。

Abstract: Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analysis (RGDA) provides a Bayes-optimal alternative with accuracy comparable to SGD-based classifiers; however, its quadratic inference complexity limits its use in large-scale CIL scenarios. To overcome this, we propose Low-Rank Factorized RGDA (LR-RGDA), a scalable classifier that combines RGDA's expressivity with the efficiency of linear classifiers. By exploiting the low-rank structure of the covariance via the Woodbury matrix identity, LR-RGDA decomposes the discriminant function into a global affine term refined by a low-rank quadratic perturbation, reducing the inference complexity from $\mathcal{O}(Cd^2)$ to $\mathcal{O}(d^2 + Crd^2)$, where $C$ is the class number, $d$ the feature dimension, and $r \ll d$ the subspace rank. To mitigate representation drift caused by backbone updates, we further introduce Hopfield-based Distribution Compensator (HopDC), a training-free mechanism that uses modern continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors, accompanied by a theoretical bound on the estimation error. Extensive experiments on diverse CIL benchmarks demonstrate that our framework achieves state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs. Code: https://github.com/raoxuan98-hash/lr_rgda_hopdc.

</details>


### [852] [Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields](https://arxiv.org/abs/2602.00148)
*Shiqian Li,Ruihong Shen,Junfeng Ni,Chang Pan,Chi Zhang,Yixin Zhu*

Main category: cs.CV

TL;DR: 提出Neural Gaussian Force Field (NGFF)框架结合3D高斯感知与物理建模生成4D视频，速度比先前模拟器快两个数量级，还有GSCollision数据集，评估显示其在物理推理上有强泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏物理规律建模，结合3D高斯溅射和物理引擎的方法计算成本高且在复杂场景缺乏鲁棒性。

Method: 引入NGFF端到端神经框架，结合3D高斯感知与基于物理的动态建模；提出GSCollision数据集支持训练。

Result: NGFF生成4D视频比先前高斯模拟器快两个数量级。

Conclusion: NGFF在物理推理上有强泛化和鲁棒性，推动视频预测向基于物理的世界模型发展。

Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.

</details>


### [853] [Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency](https://arxiv.org/abs/2602.00151)
*Alexander Blezinger,Wolfgang Nejdl,Ming Tang*

Main category: cs.CV

TL;DR: 文章系统评估组织病理学基础模型在回归任务中的表现，以预测HRD分数为例，发现基础模型特征训练的模型表现更优，还提出上采样策略改善数据不平衡问题，凸显大规模预训练对回归生物标志物预测的好处。


<details>
  <summary>Details</summary>
Motivation: 基础模型在计算病理学各领域成功，但在回归生物标志物预测方面研究不足，作者旨在系统评估其在回归任务中的表现。

Method: 在多实例学习框架下，用五个先进基础模型从全切片图像提取特征，与对比学习特征对比；在两个公共医疗数据集中对乳腺癌、子宫内膜癌和肺癌队列训练模型预测连续HRD分数；提出基于分布的上采样策略；通过消融研究探究不同采样策略和实例包大小的影响。

Result: 基础模型特征训练的模型在预测准确性和泛化能力上优于基线；上采样策略显著提高了代表性不足患者群体的召回率和平衡准确率。

Conclusion: 大规模组织病理学预训练有利于更精确和可迁移的回归生物标志物预测，有推动人工智能驱动的精准肿瘤学发展的潜力。

Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.

</details>


### [854] [Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion](https://arxiv.org/abs/2602.00152)
*Boyu Li,Kuangji Zuo,Lincong Li,Yonghui Wu*

Main category: cs.CV

TL;DR: 提出HPPI - Net用于设备端实时人体活动识别，在ARM Cortex - M4微控制器上优化后准确率达96.70%，资源占用少，与MobileNetV3对比有优势。


<details>
  <summary>Details</summary>
Motivation: 边缘应用中对设备端准确模式识别需求增强，现有方法难以兼顾准确性与计算约束，需要解决该问题。

Method: 采用两层架构，第一层用FFT频谱图提取初步特征，第二层根据活动状态选择专用模块或PLMN网络；PLMN通过三个并行LSTM编码器融合多种频谱图，并使用ECA和DSC优化。

Result: 在ARM Cortex - M4微控制器上优化后准确率96.70%，仅使用22.3 KiB的RAM和439.5 KiB的ROM；相比MobileNetV3，准确率提高1.22%，RAM使用减少71.2%，ROM使用减少42.1%。

Conclusion: HPPI - Net实现了良好的准确率 - 效率权衡，能提供可解释的预测，是内存受限边缘平台上人体活动识别的实用解决方案。

Abstract: The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules, namely the Hierarchical Parallel Pseudo-image Enhancement Fusion Network (HPPI-Net), is proposed for real-time, on-device Human Activity Recognition (HAR). Deployed on an ARM Cortex-M4 microcontroller for low-power real-time inference, HPPI-Net achieves 96.70% accuracy while utilizing only 22.3 KiB of RAM and 439.5 KiB of ROM after optimization. HPPI-Net employs a two-layer architecture. The first layer extracts preliminary features using Fast Fourier Transform (FFT) spectrograms, while the second layer selectively activates either a dedicated module for stationary activity recognition or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through three parallel LSTM encoders and refines the concatenated features using Efficient Channel Attention (ECA) and Depthwise Separable Convolution (DSC), thereby offering channel-level interpretability while substantially reducing multiply-accumulate operations. Compared with MobileNetV3, HPPI-Net improves accuracy by 1.22% and reduces RAM usage by 71.2% and ROM usage by 42.1%. These results demonstrate that HPPI-Net achieves a favorable accuracy-efficiency trade-off and provides explainable predictions, establishing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms.

</details>


### [855] [See Without Decoding: Motion-Vector-Based Tracking in Compressed Video](https://arxiv.org/abs/2602.00153)
*Axel Duché,Clément Chatelain,Gilles Gasso*

Main category: cs.CV

TL;DR: 提出轻量级压缩域跟踪模型，直接处理视频流，计算加速且精度略有下降，凸显编解码域运动建模效率。


<details>
  <summary>Details</summary>
Motivation: 为实现大规模监控系统实时分析，避免全RGB视频解码的高计算量。

Method: 利用压缩数据中的运动矢量和变换系数，通过深度模型在帧间传播目标边界框。

Result: 在MOTS15/17/20数据集上，计算速度提升达3.7倍，mAP@0.5仅下降4%。

Conclusion: 编解码域运动建模在大规模监控系统实时分析中效率高。

Abstract: We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.

</details>


### [856] [Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation](https://arxiv.org/abs/2602.00176)
*Feng Tian,Yixuan Li,Weili Zeng,Weitian Zhang,Yichao Yan,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出噪声 - 频率延续框架解决扩散后验采样在逆问题中恢复细节不足问题，多任务达SOTA。


<details>
  <summary>Details</summary>
Motivation: 扩散后验采样在解决逆问题时难以恢复精细细节，存在早期漂移、伪高频伪影等问题。

Method: 提出噪声 - 频率延续框架，构建中间后验分布族，结合扩散预测器、带限似然引导和多分辨率一致性策略。

Result: 在超分辨率、图像修复和去模糊等任务中达到了当前最优性能，运动去模糊PSNR比强基线提高了5dB。

Conclusion: 所提方法有效解决了扩散后验采样存在的问题，提升了逆问题的处理效果。

Abstract: Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.

</details>


### [857] [CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning](https://arxiv.org/abs/2602.00181)
*Hang Wu,Yujun Cai,Zehao Li,Haonan Ge,Bowen Sun,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: 提出CamReasoner框架，将相机运动理解转化为结构化推理过程，采用O - T - A范式和RL提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型将相机动力学理解任务作为黑盒分类，依赖表面视觉模式而非几何线索，易混淆不同运动。

Method: 提出CamReasoner框架，采用Observation - Thinking - Answer (O - T - A)范式，构建大规模推理轨迹套件，首次在该领域使用RL进行逻辑对齐。

Result: CamReasoner有效抑制幻觉，在多个基准测试中达到了最先进的性能。

Conclusion: 通过将相机运动理解转化为结构化推理过程，并结合RL，能有效解决现有模型的问题，提升性能。

Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.

</details>


### [858] [AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange](https://arxiv.org/abs/2602.00192)
*Elif Nebioglu,Emirhan Bilgiç,Adrian Popescu*

Main category: cs.CV

TL;DR: 研究发现现有图像修复检测依赖全局伪影而非局部合成内容，引入INP - X操作，创建数据集测试，发现现有检测器准确率大幅下降，强调需内容感知检测。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习图像修复给可靠检测带来挑战，当前检测器主要依赖全局伪影而非局部合成内容。

Method: 引入Inpainting Exchange (INP - X)操作，创建90K测试数据集，进行理论分析。

Result: 在INP - X干预下，预训练的先进检测器准确率大幅下降，如从91%降至55%，常接近随机水平。

Conclusion: 研究结果强调了内容感知检测的必要性，在数据集上训练比标准修复有更好的泛化和定位能力。

Abstract: Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\% to 55\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.

</details>


### [859] [Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images](https://arxiv.org/abs/2602.00202)
*Shanwen Wang,Xin Sun,Danfeng Hong,Fei Zhou*

Main category: cs.CV

TL;DR: 提出SemiEarth模型，引入VLM解决遥感领域半监督语义分割问题，在多数据集达SOTA且有良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统半监督语义分割架构面临伪标签质量低的挑战，尤其是师生框架。

Method: 提出SemiEarth模型，发明VLM - PP结构净化教师网络的伪标签。

Result: 在多个遥感数据集上实验表明SemiEarth达到SOTA性能。

Conclusion: SemiEarth不仅性能出色，还具有良好的可解释性。

Abstract: The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vision-language models (VLMs) to address the S4 issues for the remote sensing (RS) domain. Specifically, we invent a VLM pseudo-label purifying (VLM-PP) structure to purify the teacher network's pseudo-labels, achieving substantial improvements. Especially in multi-class boundary regions of RS images, the VLM-PP module can significantly improve the quality of pseudo-labels generated by the teacher, thereby correctly guiding the student model's learning. Moreover, since VLM-PP equips VLMs with open-world capabilities and is independent of the S4 architecture, it can correct mispredicted categories in low-confidence pseudo-labels whenever a discrepancy arises between its prediction and the pseudo-label. We conducted extensive experiments on multiple RS datasets, which demonstrate that our SemiEarth achieves SOTA performance. More importantly, unlike previous SOTA RS S4 methods, our model not only achieves excellent performance but also offers good interpretability. The code is released at https://github.com/wangshanwen001/SemiEarth.

</details>


### [860] [Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning](https://arxiv.org/abs/2602.00211)
*Zafar Iqbal,Anwar Ul Haq,Srimannarayana Grandhi*

Main category: cs.CV

TL;DR: 提出多跳视觉推理链框架用于无监督医学图像配准，在保证精度同时具备可解释性和可靠性


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法用于无监督可变形图像配准缺乏透明度，导致误差漂移和临床信任度降低

Method: 提出Multi - Hop Visual Chain of Reasoning (VCoR)框架，将配准作为渐进推理过程，每一跳整合Localized Spatial Refinement (LSR)模块和Cross - Reference Attention (CRA)机制

Result: 在两个公开数据集上评估表明，VCoR实现有竞争力的配准精度，提供丰富中间可视化和置信度度量

Conclusion: 该框架是可解释、可靠且临床可行的无监督医学图像配准方法

Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.

</details>


### [861] [A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification](https://arxiv.org/abs/2602.00214)
*Juan A. Olmos,Antoine Manzanera,Fabio Martínez*

Main category: cs.CV

TL;DR: 提出几何多模态基础模型MFM - Geom用于前列腺癌识别，在少量数据上表现优于基线模型，在外部数据集上验证了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有前列腺癌识别依赖专家主观解读，多数计算机辅助诊断方法重影像轻临床且受数据稀缺限制。

Method: 提出MFM - Geom模型从bp - MRI和临床报告中学习表征，在表征分类头利用SPD矩阵和黎曼深度学习整合成像 - 文本表征。

Result: 使用10%训练数据，MFM - Geom比基于类标记嵌入的分类基线模型AUC - PR高8.3%，达90.67%；外部数据集泛化AUC - PR达90.6。

Conclusion: MFM - Geom模型效果良好，微调生物医学基础模型在外部数据集上有较好的鲁棒性。

Abstract: Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.

</details>


### [862] [SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis](https://arxiv.org/abs/2602.00249)
*Rishav Pramanik,Ian E. Nielsen,Jeff Smith,Saurav Pandit,Ravi P. Ramachandran,Zhaozheng Yin*

Main category: cs.CV

TL;DR: 现有文本到图像模型评估方法不足，论文提出SANEval基准解决评估问题，实验证明其能更准确评估，还将发布相关数据集和开源评估管道。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在处理复杂提示存在瓶颈，且缺乏足够评估方法，现有基准有局限性。

Method: 引入SANEval基准，结合大语言模型和增强版开放词汇目标检测器进行评估。

Result: 对六个先进文本到图像模型实验表明，SANEval自动评估更接近人类评估，指标与现有基准有统计差异。

Conclusion: SANEval能有效解决文本到图像模型评估问题，将发布数据集和开源评估管道促进相关研究。

Abstract: The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.

</details>


### [863] [Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning](https://arxiv.org/abs/2602.00262)
*Huanran Li,Daniel Pimentel-Alarcón*

Main category: cs.CV

TL;DR: 提出用于不完整数据聚类的对比子空间聚类（CSC）框架，实验表明其优于基准方法，对缺失数据有强鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有子空间聚类方法多假设数据完全观测，在有缺失条目的现实场景中效果受限。

Method: 提出对比自监督框架CSC，生成部分观测输入的掩码视图，用SimCLR风格对比损失训练深度神经网络学习不变嵌入，再用稀疏子空间聚类对嵌入进行聚类。

Result: 在六个基准数据集上的实验显示，CSC始终优于经典和深度学习基准方法。

Conclusion: CSC对缺失数据有强鲁棒性，且可扩展到大型数据集。

Abstract: Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scenarios with missing entries. In this paper, we propose a contrastive self-supervised framework, Contrastive Subspace Clustering (CSC), designed for clustering incomplete data. CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are then clustered using sparse subspace clustering. Experiments on six benchmark datasets show that CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.

</details>


### [864] [PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories](https://arxiv.org/abs/2602.00267)
*Gemma Canet Tarrés,Manel Baradad,Francesc Moreno-Noguer,Yumeng Li*

Main category: cs.CV

TL;DR: 现有生成式AI在多对象合成方面不足，提出PLACID框架，其超出现有方法


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI在工作室级多对象合成上有缺陷，如改变对象细节等，需新方法解决

Method: 引入PLACID框架，利用预训练图像转视频扩散模型，结合文本控制和新颖数据策略，让对象从随机位置收敛到连贯布局

Result: 通过大量定量评估和用户研究，显示PLACID在多对象合成上优于现有方法

Conclusion: PLACID框架在多对象合成中能实现更好身份、背景、颜色保留，减少对象遗漏，结果更具视觉吸引力

Abstract: Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.

</details>


### [865] [TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.00268)
*Ariel Shaulov,Eitan Shaar,Amit Edenzon,Lior Wolf*

Main category: cs.CV

TL;DR: 本文指出自回归视频生成存在时间漂移问题，提出一种推理时方法，通过识别和移除不稳定潜在令牌来缓解漂移，提升长时一致性。


<details>
  <summary>Details</summary>
Motivation: 解决自回归视频生成中存在的严重时间漂移问题，即误差在长时间范围内累积和放大。

Method: 提出一种推理时方法，定义不稳定潜在令牌（表示与前一批次显著偏离的潜在令牌），在重用前移除这些不稳定令牌，避免不可靠潜在信息影响后续生成步骤。

Result: 显著改善了长时时间一致性，且无需修改模型架构、训练过程或离开潜在空间。

Conclusion: 通过识别和移除不稳定潜在令牌的方法能有效缓解自回归视频生成中的时间漂移问题。

Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.

</details>


### [866] [TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs](https://arxiv.org/abs/2602.00288)
*Baiqi Li,Kangyi Zhao,Ce Zhang,Chancharik Mitra,Jean de Dieu Nyandwi,Gedas Bertasius*

Main category: cs.CV

TL;DR: 提出诊断基准TimeBlind评估多模态大语言模型细粒度时空理解能力，发现模型依赖静态视觉捷径，该基准是下一代视频理解重要工具。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型掌握静态语义，但对时间动态的理解较弱，需要评估其细粒度时空理解能力。

Method: 提出TimeBlind基准，将细粒度时间理解分为三个层次，采用最小对范式，用互补问题消除语言先验。

Result: 评估20多个最先进的MLLM，最佳模型实例准确率仅48.2%，远低于人类的98.2%。

Conclusion: 前沿模型严重依赖静态视觉捷径而非真正的时间逻辑，TimeBlind是下一代视频理解的重要诊断工具。

Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .

</details>


### [867] [LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification](https://arxiv.org/abs/2602.00292)
*Rory Driscoll,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CV

TL;DR: 提出LogicGaze基准框架，验证VLMs能否基于视觉输入验证顺序因果链，暴露了现有模型的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在将推理链与实际视觉证据结合方面的可靠性研究不足，要解决模型幻觉问题。

Method: 从ShareGPT4Video和Flickr30k选取数据，将因果序列与视觉矛盾但语言合理的扰动结合，采用三方评估协议。

Result: 暴露了如Qwen2.5 - VL - 72B等先进VLMs的显著脆弱性。

Conclusion: LogicGaze有助于推动可靠、可信的多模态推理，资源公开。

Abstract: While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.

</details>


### [868] [Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation](https://arxiv.org/abs/2602.00309)
*Samuel Church,Joshua D. Warner,Danyal Maqbool,Xin Tie,Junjie Hu,Meghan G. Lubner,Tyler J. Bradshaw*

Main category: cs.CV

TL;DR: 提出SAM2CT将放射科医生的稀疏注释转换为CT体积中的3D分割，在基准测试中表现良好，可用于挖掘历史注释生成数据集。


<details>
  <summary>Details</summary>
Motivation: 机器学习CT成像模型依赖大量高质量多样的注释数据集，但3D分割标注成本高，而放射科医生常规读取时的稀疏注释可利用。

Method: 提出SAM2CT模型，扩展提示编码器支持箭头和线条输入，引入MCM内存编码策略。

Result: 在公共病变分割基准上表现优于现有模型，应用于临床PACS注释时87%可生成临床可接受或只需微调的3D分割，有较强零样本性能。

Conclusion: 大规模挖掘历史GSPS注释是生成3D CT分割数据集的有前景且可扩展的方法。

Abstract: The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.

</details>


### [869] [Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception](https://arxiv.org/abs/2602.00340)
*Alexandros Christoforos,Sarah Jenkins,Michael Brown,Tuan Pham,David Chen*

Main category: cs.CV

TL;DR: 提出SynerNet框架缓解VLMs遇到OOD概念时的跨模态对齐退化问题，介绍贡献，实验显示有性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决Vision - Language Models (VLMs)遇到Out - of - Distribution (OOD)概念时的跨模态对齐退化问题。

Method: 设计Synergistic Neural Agents Network (SynerNet)框架，通过四个计算单元和消息传播协议纠正模态差异，并提出多主体潜在空间命名获取框架、语义上下文交换算法和自适应动态平衡机制。

Result: 在VISTA - Beyond基准上进行的实验显示，在few - shot和zero - shot场景中性能显著提升，不同领域精度提高1.2% - 5.4%。

Conclusion: SynerNet框架能有效缓解VLMs遇到OOD概念时的跨模态对齐退化问题，提升性能。

Abstract: This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational units - visual perception, linguistic context, nominal embedding, and global coordination - collaboratively rectify modality disparities via a structured message-propagation protocol. The principal contributions encompass a multi-agent latent space nomenclature acquisition framework, a semantic context-interchange algorithm for enhanced few-shot adaptation, and an adaptive dynamic equilibrium mechanism. Empirical evaluations conducted on the VISTA-Beyond benchmark demonstrate that SynerNet yields substantial performance augmentations in both few-shot and zero-shot scenarios, exhibiting precision improvements ranging from 1.2% to 5.4% across a diverse array of domains.

</details>


### [870] [When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs](https://arxiv.org/abs/2602.00344)
*Beidi Zhao,Wenlong Deng,Xinting Liao,Yushu Li,Nazim Shaikh,Yao Nie,Xiaoxiao Li*

Main category: cs.CV

TL;DR: 本文指出RAG在LVLM的知识型VQA任务中的注意力分散（AD）失败模式，提出训练无关的MAD - RAG方法，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有研究将RAG失败归因于对检索上下文关注不足，本文发现了之前研究忽视的注意力分散（AD）失败模式，即检索文本会抑制视觉注意力，导致模型在原本能答对的问题上出错。

Method: 提出MAD - RAG方法，通过双问题公式将视觉定位与上下文整合解耦，并结合注意力混合来保留图像条件证据。

Result: 在OK - VQA、E - VQA和InfoSeek上的实验表明，MAD - RAG始终优于现有基线，相对原始RAG基线绝对增益最高分别达4.76%、9.20%和6.18%，能纠正高达74.68%的失败案例，且计算开销可忽略不计。

Conclusion: MAD - RAG能有效缓解RAG中的注意力分散问题，提升模型在知识型VQA任务上的性能。

Abstract: While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.

</details>


### [871] [AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning](https://arxiv.org/abs/2602.00347)
*Chongyu Qu,Zhengyi Lu,Yuxiang Lai,Thomas Z. Li,Junchao Zhu,Junlin Guo,Juming Xiong,Yanfan Zhu,Yuechen Yang,Allen J. Luna,Kim L. Sandler,Bennett A. Landman,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出AdaFuse框架用于肺癌风险预测，用强化学习实现自适应多模态融合，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法未解决给定患者是否使用某些模态的问题，需更好的融合策略。

Method: 提出AdaFuse框架，将多模态融合作为顺序决策过程，用策略网络迭代决定是否纳入额外模态或进行预测。

Result: 在NLST数据集上，AdaFuse的AUC最高达0.762，使用更少FLOPs。

Conclusion: 强化学习用于医学图像个性化多模态融合有潜力，应从统一融合策略转向自适应诊断流程。

Abstract: Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through the network, either treating them equally or learning to assign different contribution weights, leaving a fundamental question unaddressed: for a given patient, should certain modalities be used at all? We present AdaFuse, an adaptive multimodal fusion framework that leverages reinforcement learning (RL) to learn patient-specific modality selection and fusion strategies for lung cancer risk prediction. AdaFuse formulates multimodal fusion as a sequential decision process, where the policy network iteratively decides whether to incorporate an additional modality or proceed to prediction based on the information already acquired. This sequential formulation enables the model to condition each selection on previously observed modalities and terminate early when sufficient information is available, rather than committing to a fixed subset upfront. We evaluate AdaFuse on the National Lung Screening Trial (NLST) dataset. Experimental results demonstrate that AdaFuse achieves the highest AUC (0.762) compared to the best single-modality baseline (0.732), the best fixed fusion strategy (0.759), and adaptive baselines including DynMM (0.754) and MoE (0.742), while using fewer FLOPs than all triple-modality methods. Our work demonstrates the potential of reinforcement learning for personalized multimodal fusion in medical imaging, representing a shift from uniform fusion strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities and when existing information suffices for accurate prediction.

</details>


### [872] [Text is All You Need for Vision-Language Model Jailbreaking](https://arxiv.org/abs/2602.00420)
*Yihang Chen,Zhao Xu,Youyuan Jiang,Tianle Zheng,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出Text - DJ越狱攻击，利用模型OCR能力绕过LVLMs安全防护，揭示其OCR能力漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs安全防护多关注显式文本输入或相关视觉场景，本文旨在利用新方法绕过这些防护。

Method: 分三步，先将有害查询分解为多个良性子查询，再选取无关干扰查询，最后将子查询和干扰查询以图像网格形式同时呈现给LVLM，子查询位于网格中间。

Result: 该方法成功绕过了最先进LVLMs的安全对齐。

Conclusion: LVLMs的OCR能力对分散、多图像对抗输入不鲁棒，需针对碎片化多模态输入进行防御。

Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.

</details>


### [873] [LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs](https://arxiv.org/abs/2602.00462)
*Benno Krojer,Shravan Nayak,Oscar Mañas,Vaibhav Adlakha,Desmond Elliott,Siva Reddy,Marius Mosbach*

Main category: cs.CV

TL;DR: 本文介绍LatentLens方法将视觉标记表示映射为自然语言描述，评估显示其优于常用方法，为视觉和语言表示对齐提供新证据。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型能轻松处理视觉标记的原因，需要可解释性方法揭示视觉标记表示在大语言模型各层的编码内容。

Method: 引入LatentLens方法，编码大型文本语料库并存储上下文标记表示，将视觉标记与文本表示比较，用最近邻表示描述视觉标记。

Result: 在10个不同的视觉 - 语言模型上评估，显示常用方法如LogitLens低估视觉标记可解释性，LatentLens使多数视觉标记在各模型各层可解释，描述语义有意义且更细粒度。

Conclusion: 研究为视觉和语言表示对齐提供新证据，为分析潜在表示开辟新方向。

Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.

</details>


### [874] [Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models](https://arxiv.org/abs/2602.00505)
*Jingrui Zhang,Feng Liang,Yong Zhang,Wei Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 现有多模态大语言模型在跨模态知识集成方面关注不足，文章提出SparseCut架构，实验证明其有效提升性能


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注扩展语言模型或构建高质量训练数据，对有效整合跨模态知识到语言空间关注较少，如视觉 - 语言模型只使用高级视觉特征会丢弃中低级特征语义信息，限制跨模态理解能力

Method: 提出SparseCut架构，在跨模态编码器和大语言模型间引入稀疏捷径连接，实现多层次视觉特征高效集成；引入高效多粒度特征融合模块，在特征通过捷径前进行融合，保留原语言上下文且不增加计算复杂度

Result: SparseCut在多个多模态基准测试中显著提升多模态大语言模型性能，对不同基础大语言模型具有通用性和可扩展性

Conclusion: SparseCut架构能有效解决多模态大语言模型跨模态知识集成问题，提高模型跨模态理解和处理能力

Abstract: With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.

</details>


### [875] [Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models](https://arxiv.org/abs/2602.00559)
*Wenbin Xing,Quanxing Zha,Lizheng Zu,Mengran Li,Ming Li,Junchi Yan*

Main category: cs.CV

TL;DR: 提出OmniVCHall基准评估视频多模态大语言模型的幻觉问题，揭示先进模型表现不佳；提出TriCD框架，提高模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视频幻觉缓解研究主要关注孤立错误类型，成分性幻觉问题尚未得到充分研究。

Method: 引入OmniVCHall基准评估模型，提出TriCD对比解码框架，通过强化学习优化组件。

Result: 对39个代表性VLLMs的评估显示先进模型性能下降；TriCD使两个代表性骨干模型的准确率平均提高超10%。

Conclusion: OmniVCHall可有效评估模型幻觉问题，TriCD能在成分性幻觉场景下提升模型性能。

Abstract: Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., "All are correct" and "None of the above") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.

</details>


### [876] [MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation](https://arxiv.org/abs/2602.00583)
*Xiangdong Li,Ye Lou,Ao Gao,Wei Zhang,Siyang Song*

Main category: cs.CV

TL;DR: 提出基于扩散的多模态框架MAUGen，生成面部表情与AU标签，引入MIFA数据集，实验显示MAUGen性能更优。


<details>
  <summary>Details</summary>
Motivation: 缺少大规模、人口统计学多样化且有精确AU标注的人脸图像是开发通用AU识别系统的瓶颈。

Method: 提出MAUGen框架，含多模态表征学习（MRL）和基于扩散的图像标签生成（DIG）模块，引入MIFA数据集。

Result: MAUGen在合成逼真、人口统计学多样化的面部图像及语义对齐的AU标签方面优于现有方法。

Conclusion: MAUGen是一种有效的生成面部图像和AU标签的方法。

Abstract: The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.

</details>


### [877] [S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning](https://arxiv.org/abs/2602.00635)
*Lingsong Wang,Mancheng Meng,Ziyan Wu,Terrence Chen,Fan Yang,Dinggang Shen*

Main category: cs.CV

TL;DR: 现有面部解析方法常误分类遮挡，本文提出S³POT框架实现遮挡分割，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有面部解析方法会误将遮挡分类为面部组件，构建含所有遮挡物的数据集难且标注成本高。

Method: 提出由三个模块组成的S³POT框架，通过参考生成、特征增强和提示选择处理。网络在三个无遮挡真实标签的目标函数下学习。

Result: 在专门收集的数据集上的大量实验表明S³POT性能优越，各模块有效。

Conclusion: S³POT框架能有效实现遮挡分割，且各模块发挥作用。

Abstract: Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and accurate mask annotation is labor-intensive. To deal with the problems, we present S$^3$POT, a contrast-driven framework synergizing face generation with self-supervised spatial prompting, to achieve occlusion segmentation. The framework is inspired by the insights: 1) Modern face generators' ability to realistically reconstruct occluded regions, creating an image that preserve facial geometry while eliminating occlusion, and 2) Foundation segmentation models' (e.g., SAM) capacity to extract precise mask when provided with appropriate prompts. In particular, S$^3$POT consists of three modules: Reference Generation (RF), Feature enhancement (FE), and Prompt Selection (PS). First, a reference image is produced by RF using structural guidance from parsed mask. Second, FE performs contrast of tokens between raw and reference images to obtain an initial prompt, then modifies image features with the prompt by cross-attention. Third, based on the enhanced features, PS constructs a set of positive and negative prompts and screens them with a self-attention network for a mask decoder. The network is learned under the guidance of three novel and complementary objective functions without occlusion ground truth mask involved. Extensive experiments on a dedicatedly collected dataset demonstrate S$^3$POT's superior performance and the effectiveness of each module.

</details>


### [878] [Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment](https://arxiv.org/abs/2602.00653)
*Lukas Kuhn,Giuseppe Serra,Florian Buettner*

Main category: cs.CV

TL;DR: 提出非对比视觉语言对齐框架NOVA，在零样本胸部X光分类任务上表现优于基线，证明非对比预训练方法更优。


<details>
  <summary>Details</summary>
Motivation: 主流对比方法如CLIP需要大批次、仔细的负采样和大量超参数调整，希望找到更优方法。

Method: 引入NOVA框架，通过预测文本嵌入对齐视觉和文本表示，用SIGReg正则化，减少超参数。

Result: 在三个基准数据集零样本分类中，NOVA优于多个标准基线，训练更稳定。

Conclusion: 非对比视觉语言预训练是对比方法更简单、稳定和有效的替代方案。

Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.

</details>


### [879] [Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation](https://arxiv.org/abs/2602.00669)
*Marina Crespo Aguirre,Jonathan Williams-Ramirez,Dina Zemlyanker,Xiaoling Hu,Lucas J. Deden-Binder,Rogeny Herisse,Mark Montine,Theresa R. Connors,Christopher Mount,Christine L. MacDonald,C. Dirk Keene,Caitlin S. Latimer,Derek H. Oakley,Bradley T. Hyman,Ana Lawry Aguila,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出计算高效的超分辨率步骤，从解剖照片各向异性3D重建生成各向同性体积，提升重建分辨率和解剖保真度，方法公开可用。


<details>
  <summary>Details</summary>
Motivation: 先前从2D解剖照片重建3D脑体积的输出有时结构重建粗糙、过度平滑，尤其在高各向异性情况下。

Method: 引入超分辨率步骤，在域随机化合成数据上训练，从解剖照片各向异性3D重建生成解剖学一致的各向同性体积。

Result: 生成的填充体积改善了自动分割，在皮质和白质区域实现更高Dice分数，表面重建和图谱配准任务更准确。

Conclusion: 该方法增强基于照片重建的分辨率和解剖保真度，加强了神经病理学和神经影像学的联系。

Abstract: Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon

</details>


### [880] [Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints](https://arxiv.org/abs/2602.00763)
*Dylan Yves,Khush Agarwal,Jonathan Hoyin Chan,Patcharapit Promoppatum,Aroonkamon Pattanasiricharoen*

Main category: cs.CV

TL;DR: 研究用U - Net架构评估超声图像中臂丛神经分割，探讨数据集构成和标注策略对分割性能的影响，给出在现实临床数据约束下开发超声神经分割系统的方法指导。


<details>
  <summary>Details</summary>
Motivation: 准确的神经定位对超声引导区域麻醉成功至关重要，但手动识别因图像对比度低、斑点噪声和患者间解剖差异而具有挑战性，需评估深度学习神经分割方法。

Method: 使用U - Net架构对臂丛神经超声图像进行分割，研究不同数据集构成（多台超声机数据组合训练）和标注策略（从二分类到多分类监督）。

Result: 多台超声机组合数据训练对低性能采集源有正则化好处，但不超匹配目标域的单源训练；多分类监督使神经Dice分数下降；神经大小和分割精度呈中度正相关。

Conclusion: 研究结果为在现实临床数据约束下开发鲁棒的超声神经分割系统提供了方法学指导。

Abstract: Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.

</details>


### [881] [DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models](https://arxiv.org/abs/2602.00883)
*Alicja Polowczyk,Agnieszka Polowczyk,Piotr Borycki,Joanna Waczyńska,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: 现有文本到图像模型存在视觉和解剖伪影问题，现有去伪影方法有局限，提出训练无关方法DIAMOND去伪影，还可用于标准扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型存在视觉和解剖伪影，现有去伪影方法多为事后处理，需修改模型权重或进行区域细化，有局限性。

Method: 提出训练无关的DIAMOND方法，在推理时应用轨迹校正，在生成轨迹的每一步重建干净样本估计，引导生成过程远离产生伪影的潜在状态。

Result: 将方法扩展到标准扩散模型，在现代生成架构中无需额外训练或权重修改即可实现高保真、无伪影图像合成。

Conclusion: DIAMOND为高保真、无伪影图像合成提供了强大的零样本途径，代码已开源。

Abstract: Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/

</details>


### [882] [VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models](https://arxiv.org/abs/2602.01037)
*Guangshuo Qin,Zhiteng Li,Zheng Chen,Weihang Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出Visual Expert Quantization (VEQ) 量化框架解决MoE VLM压缩问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) Vision-Language Models存在高内存和计算成本问题，现有量化范式未考虑视觉和语言令牌差异以及不同专家贡献不均的问题。

Method: 提出VEQ框架，包含Modality-expert-aware Quantization和Modality-affinity-aware Quantization。

Result: 在不同基准测试中，VEQ始终优于现有方法，在特定配置下，Kimi - VL和Qwen3 - VL上有显著平均准确率提升。

Conclusion: VEQ在多模态任务中具有更好的鲁棒性，代码将公开。

Abstract: Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\% on Kimi-VL and 3.09\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.

</details>


### [883] [Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance](https://arxiv.org/abs/2602.01047)
*Xinrong Chen,Xu Chu,Yingmin Qiu,Hengyuan Zhang,Jing Xiong,Shiyu Tang,Shuai Liu,Shaokang Yang,Cheng Yang,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CV

TL;DR: 提出无训练方法ResDec抑制大视觉语言模型幻觉，实验证明其有效且适用广。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型受语言先验影响产生幻觉，需解决此问题。

Method: 提出无训练的Residual Decoding (ResDec)方法，利用历史信息辅助解码，依靠模型内部机制纠正偏差。

Result: 有效抑制语言先验导致的幻觉，显著改善视觉定位，减少物体幻觉，在综合基准测试中表现出色。

Conclusion: ResDec能有效解决大视觉语言模型的幻觉问题，具有广泛适用性。

Abstract: Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.

</details>


### [884] [Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models](https://arxiv.org/abs/2602.01163)
*Chunliang Hua,Zeyuan Yang,Lei Zhang,Jiayang Sun,Fengwen Chen,Chunlan Zeng,Xiao Hu*

Main category: cs.CV

TL;DR: 提出利用RS图像和MLLMs的框架进行无人机应急降落点评估，构建ELSS基准，实验显示该框架在风险识别上优于几何基线。


<details>
  <summary>Details</summary>
Motivation: 安全的无人机应急降落需要理解传统几何传感器无法察觉的复杂语义风险，而不仅是识别平坦地形。

Method: 采用粗到细的管道，先用轻量级语义分割模块预筛选候选区域，再用视觉语言推理代理结合视觉特征和POI数据检测细微危险。

Result: 实验表明该框架在风险识别准确性上显著优于几何基线，定性结果证实其能产生类人、可解释的理由。

Conclusion: 提出的基于RS图像和MLLMs的框架能有效进行全球上下文感知的降落点评估，提高风险识别能力。

Abstract: Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.

</details>


### [885] [Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning](https://arxiv.org/abs/2602.01335)
*Yu Xu,Yuxin Zhang,Juan Cao,Lin Gao,Chunyu Wang,Oliver Deussen,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

TL;DR: 本文引入视觉隐喻转移任务，提出认知启发的多智能体框架，实验表明该方法在隐喻一致性等方面优于SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI模型局限于像素级和表面外观，无法捕捉隐喻生成所需的抽象逻辑，需引入新任务解决此问题。

Method: 引入视觉隐喻转移（VMT）任务，提出基于概念混合理论（CBT）的多智能体框架，通过专业智能体协作执行VMT。

Result: 广泛实验和人类评估显示，该方法在隐喻一致性、类比恰当性和视觉创意方面显著优于SOTA基线。

Conclusion: 该方法为广告和媒体等领域的自动化高影响力创意应用铺平道路，代码将公开。

Abstract: A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the "creative essence" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar ("G"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.

</details>


### [886] [PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles](https://arxiv.org/abs/2602.01370)
*Leonardo Brusini,Cristian Sbrolli,Eugenio Lomurno,Toshihiko Yamasaki,Matteo Matteucci*

Main category: cs.CV

TL;DR: 提出PolyGen框架重新定义合成数据构建，通过多源方法和课程学习，在多任务和组合性基准测试中优于单源基线，证明结构多样性更具数据效率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉 - 语言预训练中先进合成数据方法依赖单一生成器，存在谱偏差和特征多样性受限问题。

Method: 采用多结构方法在不同架构生成器的交集上训练，引入编程式硬负样本课程。

Result: 在多任务和SugarCrepe++组合性基准测试中分别优于单源基线SynthCLIP 19.0%和9.1%。

Conclusion: 结构多样性是比增加单源样本量更具数据效率的扩展定律。

Abstract: Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.

</details>


### [887] [Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles](https://arxiv.org/abs/2602.01452)
*Penghao Deng,Jidong J. Yang,Jiachen Bian*

Main category: cs.CV

TL;DR: 本文将驾驶员视线行为分析作为语义识别任务，用三种方法研究视线点与物体语义搭配，发现YOLOv13和Qwen2.5 - VL - 32b表现优，为未来智能驾驶员监控系统设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 理解驾驶员驾驶时的视线注意力方向，对开发下一代高级驾驶辅助系统和提高道路安全至关重要。

Method: 使用三种基于视觉的方法，即直接目标检测（YOLOv13）、分割辅助分类（SAM2搭配EfficientNetV2与YOLOv13）和基于查询的视觉 - 语言模型（Qwen2.5 - VL - 7b与Qwen2.5 - VL - 32b）研究视线点与物体语义的搭配。

Result: 直接目标检测（YOLOv13）和Qwen2.5 - VL - 32b表现显著优于其他方法，Macro F1分数超0.84；大模型Qwen2.5 - VL - 32b在识别小的安全关键物体上更具鲁棒性和性能；分割辅助范式存在语义差距导致召回失败。

Conclusion: 研究结果揭示了传统检测器实时效率与大语言模型丰富上下文理解和鲁棒性之间的权衡，为未来人类感知智能驾驶员监控系统设计提供关键见解和实践指导。

Abstract: Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.

</details>


### [888] [Understanding vision transformer robustness through the lens of out-of-distribution detection](https://arxiv.org/abs/2602.01459)
*Joey Kuang,Alexander Wong*

Main category: cs.CV

TL;DR: 研究量化小型流行视觉Transformer在常见OOD数据集上的行为，发现大规模数据集预训练可能阻碍OOD检测中的低比特量化鲁棒性，数据增强可能更有益。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer在视觉任务表现出色，但实现可访问和实时使用有挑战，量化虽能降低成本但有性能损失风险，探索通过OOD情况研究量化属性。

Method: 研究量化的DeiT、DeiT3和ViT在常见OOD数据集上的行为，进行ID分析和OOD检测。

Result: 4位模型初始不稳定，大规模数据集预训练的模型量化误差大；不同模型在全精度到4位量化时AUPR - out有不同变化。

Conclusion: 大规模数据集预训练可能阻碍OOD检测中的低比特量化鲁棒性，数据增强可能是更有益的选择。

Abstract: Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.

</details>


### [889] [Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars](https://arxiv.org/abs/2602.01538)
*Youliang Zhang,Zhengguang Zhou,Zhentao Yu,Ziyao Huang,Teng Hu,Sen Liang,Guozhen Zhang,Ziqiao Peng,Shunkai Li,Yi Chen,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Xiu Li*

Main category: cs.CV

TL;DR: 提出一种新的双流框架InteractAvatar，用于生成能进行人与物体交互的会说话化身，并建立评估基准，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽能生成简单动作的全身会说话化身，但在生成有人与物体交互的化身方面存在挑战，包括环境感知和控制质量困境。

Method: 提出InteractAvatar框架，解耦感知、规划和视频合成；用检测增强环境感知，引入PIM生成交互动作，提出AIM合成生动化身视频；设计运动 - 视频对齐器，实现动作和视频并行生成；建立基准GroundedInter。

Result: 通过大量实验和对比，证明方法在生成有交互的会说话化身方面有效。

Conclusion: 所提方法能有效解决生成会说话化身中人与物体交互的问题，可用于相关视频生成任务。

Abstract: Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io

</details>


### [890] [Toward Cognitive Supersensing in Multimodal Large Language Model](https://arxiv.org/abs/2602.01541)
*Boyi Li,Yifan Shen,Yuanzhe Liu,Yifan Xu,Jiateng Liu,Xinzhuo Li,Zhengyuan Li,Jingyuan Zhu,Yunhan Zhong,Fangzhou Lan,Jianguo Cao,James M. Rehg,Heng Ji,Ismini Lourentzou,Xu Cao*

Main category: cs.CV

TL;DR: 论文提出Cognitive Supersensing训练范式提升多模态大语言模型解决复杂认知问题的能力，引入CogSense - Bench评估，实验显示效果良好并将开源。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型解决复杂认知问题能力有限，现有方法主要在文本空间拓展思维链推理，忽视视觉推理机制。

Method: 引入Cognitive Supersensing训练范式，集成LVIP头学习视觉认知潜在嵌入序列并与答案对齐，形成基于视觉的内部推理链，还增加强化学习阶段优化文本推理路径；提出CogSense - Bench评估模型认知能力。

Result: 使用Cognitive Supersensing训练的MLLMs在CogSense - Bench上显著优于现有基线，在跨领域的数学和科学VQA基准上泛化能力更强。

Conclusion: 内部视觉意象可能是弥合感知识别和认知理解差距的关键。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.

</details>


### [891] [Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer](https://arxiv.org/abs/2602.00110)
*Yu Li,Guilherme N. DeSouza,Praveen Rao,Chi-Ren Shyu*

Main category: cs.CV

TL;DR: 现有视觉语言和多模态模型不适用于处理结构化地理空间图层，本文提出新模型，引入地理空间嵌入机制和引导注意力模块，实验显示该框架在预测疾病流行率方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言和多模态模型针对视觉与文本内容语义对齐优化，不适合结构化地理空间图层的表示和推理，需提升遥感影像处理的地理空间理解能力。

Method: 提出新模型，引入地理空间嵌入机制将多样地理空间数据转换为与图像块空间对齐的嵌入块；设计引导注意力模块，基于与辅助数据的相关性计算注意力权重，动态整合多模态信息，还为每个注意力头分配不同角色。

Result: 提出的框架在预测疾病流行率方面优于现有的预训练地理空间基础模型。

Conclusion: 该模型在多模态地理空间理解方面有效。

Abstract: Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.

</details>


### [892] [Event Driven Clustering Algorithm](https://arxiv.org/abs/2602.00115)
*David El-Chai Ben-Ezra,Adar Tal,Daniel Brisk*

Main category: cs.CV

TL;DR: 本文提出用于事件相机数据中小事件簇实时检测的异步、事件驱动算法，复杂度为O(n)且运行时间与像素数组维度无关。


<details>
  <summary>Details</summary>
Motivation: 实现事件相机数据中小事件簇的实时检测。

Method: 利用事件相机特殊异步数据结构，通过精巧、高效且简单的决策，基于时空距离检测事件簇。

Result: 算法复杂度为O(n)，运行时间与像素数组维度无关。

Conclusion: 所提出的算法在小事件簇实时检测方面具有高效性和独立性。

Abstract: This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.

</details>


### [893] [Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd](https://arxiv.org/abs/2602.01561)
*Yejin Son,Saejin Kim,Dongjun Min,Younjae Yu*

Main category: cs.CV

TL;DR: 提出MUN基准测试和R - ICL框架，提升模型在非典型多模态场景下表现。


<details>
  <summary>Details</summary>
Motivation: 解决多模态语境下常识推理这一人工智能基础挑战，评估模型处理偏离典型视觉或语境预期场景的能力。

Method: 提出基于检索的上下文学习（R - ICL）框架，利用多模态集成检索器（MER）识别语义相关示例。

Result: 相比基线ICL方法平均提升8.3%。

Conclusion: MUN为评估和提高视觉语言模型在现实、多元文化和非典型场景中的鲁棒性和适应性开辟新方向。

Abstract: Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.

</details>


### [894] [CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models](https://arxiv.org/abs/2602.00247)
*Samyak Jha,Junho Kim*

Main category: cs.CV

TL;DR: 本文提出CAPA框架，通过注意力贡献剪枝视觉标记并近似FFN计算，在效率和性能间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型推理时处理大量视觉标记成本高，需明确可移除的标记和计算，且现有注意力分数不能准确衡量标记重要性。

Method: 提出注意力贡献作为视觉标记选择标准，发现视觉注意力汇的异质性，识别FFN冗余，引入CAPA框架进行标记剪枝和FFN近似。

Result: 实验表明CAPA在不同基准测试中实现了有效率和性能的权衡，并提高了鲁棒性。

Conclusion: CAPA框架能在大视觉语言模型推理中实现高效的效率 - 性能权衡。

Abstract: Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.

</details>


### [895] [Modeling Image-Caption Rating from Comparative Judgments](https://arxiv.org/abs/2602.00381)
*Kezia Minni,Qiang Zhang,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: 本文提出以比较判断建模的机器学习框架来对图像 - 字幕对排序，并与回归模型对比，发现比较学习模型随数据增多性能提升，且比较标注成本更低。


<details>
  <summary>Details</summary>
Motivation: 直接对字幕描述图像的准确性进行评分耗时且主观，而比较两个字幕与图像的匹配度相对容易。

Method: 提出比较判断建模的机器学习框架，使用VICR数据集，用ResNet - 50提取视觉特征、MiniLM提取文本特征，训练回归模型和比较学习模型。

Result: 回归模型表现更好，但比较学习模型随数据增加性能稳定提升并接近回归基线；人类评估显示比较标注更快且标注者一致性更高。

Conclusion: 比较学习能有效建模人类偏好，同时显著降低人工标注成本。

Abstract: Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $ρ$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.

</details>


### [896] [Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning](https://arxiv.org/abs/2602.01649)
*Yinchao Ma,Qiang Zhou,Zhibin Wang,Xianing Chen,Hanqing Yang,Jun Song,Bo Zheng*

Main category: cs.CV

TL;DR: 提出了一种名为CaCoVID的用于视频理解的贡献感知令牌压缩算法，有效解决视频模型推理的计算开销问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型推理时视频令牌冗余带来高计算开销，且现有压缩算法中注意力分数与正确答案实际贡献的相关性不明确。

Method: 1. 引入基于强化学习的框架，优化策略网络选择对正确预测贡献最大的视频令牌组合；2. 提出带有在线组合空间采样的组合策略优化算法，减少搜索空间并加速策略优化收敛。

Result: 在多个视频理解基准测试的大量实验中，证明了CaCoVID的有效性。

Conclusion: CaCoVID能有效解决视频大语言模型推理中计算开销的问题，论文代码会发布。

Abstract: Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \textbf{C}ontribution-\textbf{a}ware token \textbf{Co}mpression algorithm for \textbf{VID}eo understanding (\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.

</details>


### [897] [Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset](https://arxiv.org/abs/2602.00393)
*Gabriel Bromonschenkel,Alessandro L. Koerich,Thiago M. Paixão,Hilário Tomaz Alves de Oliveira*

Main category: cs.CV

TL;DR: 本文针对巴西葡萄牙语图像描述任务，对基于Transformer的视觉语言模型进行跨母语翻译评估，发现Swin - DistilBERTimbau表现出色，不同模型各有优劣，还揭示了注意力分析中的系统偏差。


<details>
  <summary>Details</summary>
Motivation: 大多数图像描述研究聚焦英语模型，低资源语言如巴西葡萄牙语因缺乏专业数据集和模型面临挑战，需进行相关评估。

Method: 使用由巴西葡萄牙语母语者手动创建的Flickr30K版本和从英语自动翻译的版本，采用跨上下文方法，结合注意力图和CLIP - Score指标评估。

Result: Swin - DistilBERTimbau泛化能力强，ViTucano在传统文本评估指标上超大多语言模型，GPT - 4模型CLIP - Score最高，注意力分析有系统偏差。

Conclusion: 为巴西葡萄牙语图像描述提供了有效的评估，不同模型适用于不同评估场景，相关数据集和模型可在指定链接获取。

Abstract: Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.

</details>


### [898] [3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting](https://arxiv.org/abs/2602.00395)
*Roger Hsiao,Yuchen Fang,Xiangru Huang,Ruilong Li,Hesam Rabeti,Zan Gojcic,Javad Lavaei,James Demmel,Sophia Shao*

Main category: cs.CV

TL;DR: 提出二阶优化器3DGS² - TR加速3D高斯拼接场景训练，复杂度低，重建质量好，内存开销小。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯拼接场景训练问题，加速训练过程。

Method: 用Hutchinson方法仅通过Hessian矩阵对角线近似曲率，引入基于平方Hellinger距离的逐参数信任区域技术。

Result: 在相同参数初始化且无致密化情况下，比ADAM少用50%训练迭代，GPU内存开销峰值不到1GB。

Conclusion: 3DGS² - TR可扩展到大型场景和分布式训练设置。

Abstract: We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (Höllein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.

</details>


### [899] [Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure](https://arxiv.org/abs/2602.00414)
*Trishna Chakraborty,Udita Ghosh,Aldair Ernesto Gongora,Ruben Glatt,Yue Dong,Jiachen Li,Amit K. Roy-Chowdhury,Chengyu Song*

Main category: cs.CV

TL;DR: 文章引入结构化数据生成流程，创建合成数据集评估视觉语言模型（VLMs）在实验室安全监测中的效果，发现其在仅视觉场景下性能下降，进而提出后训练上下文工程方法提升危险检测性能。


<details>
  <summary>Details</summary>
Motivation: 实验室需持续安全监测，但人力有限；VLMs有潜力用于自动安全监测，然而因缺乏视觉评估数据，其实践效果不明。

Method: 先引入结构化数据生成流程，基于大语言模型和图像生成模型将文本实验室场景转换为三元组；在合成数据集上对多个模型进行实验；提出场景图引导对齐的后训练上下文工程方法。

Result: VLMs在文本场景图输入时表现良好，但仅依赖视觉输入时性能大幅下降；提出的后训练方法可改善仅视觉场景下的危险检测性能。

Conclusion: 针对实验室安全监测，VLMs在视觉推理方面存在差距，提出的后训练上下文工程方法能缩小这种差距，提升VLMs在仅视觉场景下的危险检测表现。

Abstract: Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.

</details>


### [900] [Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss](https://arxiv.org/abs/2602.01673)
*Enguang Fan*

Main category: cs.CV

TL;DR: 评估NetVLAD作为回环检测模块，对比DBoW，引入细粒度Top - K曲线，NetVLAD实现实时查询且更优。


<details>
  <summary>Details</summary>
Motivation: 经典词袋法在外观变化和感知混淆时性能下降，深度学习方法计算成本高阻碍实时SLAM，需评估NetVLAD在回环检测中的实用性。

Method: 在KITTI数据集上评估NetVLAD作为LCD模块，与DBoW对比，引入Fine - Grained Top - K精确召回曲线，用Faiss加速近邻搜索。

Result: NetVLAD实现实时查询速度，相比DBoW提高了准确性和鲁棒性。

Conclusion: NetVLAD可作为SLAM中回环检测的实用替代方案。

Abstract: Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.

</details>


### [901] [FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding](https://arxiv.org/abs/2602.01683)
*Kangcong Li,Peng Ye,Lin Zhang,Chao Wang,Huafeng Qin,Tao Chen*

Main category: cs.CV

TL;DR: 提出FreshMem网络解决多模态大语言模型在线流式视频理解问题，实验表明其能提升性能，是高效范式。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态大语言模型从离线到在线流式视频理解中缺乏灵活适应性，导致细节损失和上下文碎片化。

Method: 提出Frequency - Space Hybrid Memory网络FreshMem，包含Multi - scale Frequency Memory（MFM）和Space Thumbnail Memory（STM）两个协同模块。

Result: FreshMem显著提升Qwen2 - VL基线，在StreamingBench、OV - Bench和OVO - Bench上分别提升5.20%、4.52%和2.34%，且作为免训练方案优于多个全微调方法。

Conclusion: FreshMem为长时流式视频理解提供了高效范式。

Abstract: Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical "gist"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.

</details>


### [902] [DISK: Dynamic Inference SKipping for World Models](https://arxiv.org/abs/2602.00440)
*Anugunj Naman,Gaibo Zhang,Ayushman Singh,Yaguang Zhang*

Main category: cs.CV

TL;DR: 提出无训练自适应推理方法DISK，用于自回归世界模型，在降低成本下实现长视野视频和轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 解决自回归世界模型训练成本高、需保持运动 - 外观一致性等问题，实现长视野稳定预测。

Method: 通过双分支控制器协调视频和自我轨迹的两个耦合扩散变压器，进行跨模态跳过决策；将高阶潜在差异跳过测试扩展到自回归前向链机制，并通过滚动循环传播控制器统计信息。

Result: 在1500个NuPlan和NuScenes样本的闭环驾驶滚动测试中，轨迹扩散加速2倍，视频扩散加速1.6倍，同时保持L2规划误差、视觉质量和NAVSIM PDMS分数。

Conclusion: DISK能以大幅降低的成本实现实用的长视野视频和轨迹预测。

Abstract: We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.

</details>


### [903] [Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection](https://arxiv.org/abs/2602.01696)
*Jiaming Cui,Shuai Zhou,Wenqiang Li,Ruifeng Qin,Feng Shen*

Main category: cs.CV

TL;DR: 提出CMAFNet用于输电线路缺陷检测，融合RGB外观和深度几何信息，在TLRGBD基准测试中表现优异，还有轻量级变体。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB的探测器在输电线路小规模缺陷检测时，因小尺寸缺陷、复杂背景和光照变化等问题，难以区分细微缺陷与背景。

Method: 提出CMAFNet网络，采用净化融合范式集成RGB和深度信息，包括语义重组模块和上下文语义集成框架，净化阶段使用位置归一化实现跨模态对齐。

Result: 在TLRGBD基准测试中，CMAFNet取得32.2% mAP@50和12.5% APs，轻量级变体在228 FPS下达到24.8% mAP50，参数仅4.9M。

Conclusion: CMAFNet有效提升输电线路缺陷检测性能，轻量级变体在低计算成本下可与其他模型竞争。

Abstract: Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.

</details>


### [904] [NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation](https://arxiv.org/abs/2602.00542)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari,Mert D. Pesé*

Main category: cs.CV

TL;DR: 提出NPNet用于3D点云分类和部分分割，无学习权重，表现好，内存和推理时间优。


<details>
  <summary>Details</summary>
Motivation: 开发一种无学习权重的3D点云分类和部分分割方法，使其在不同尺度和采样密度下保持稳定。

Method: 使用确定性算子构建点特征，采用自适应高斯 - 傅里叶位置编码，分割时结合固定频率傅里叶特征。

Result: 在多个数据集上表现良好，在少样本设置下效果佳，内存使用和推理时间优于先前非参数方法。

Conclusion: NPNet是一种有效的非参数3D点云处理方法。

Abstract: We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods

</details>


### [905] [Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis](https://arxiv.org/abs/2602.01710)
*Salma Zahran,Zhou Ao,Zhengyang Zhang,Chen Chi,Chenchen Yuan,Yanming Wang*

Main category: cs.CV

TL;DR: 本文提出无人工标注分割框架，利用相场模拟和CycleGAN生成逼真合成数据，训练的U - Net模型在实验图像上泛化性好，解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像语义分割自动化受专家标注数据成本高、主观性强和稀缺性限制，基于物理模拟的数据训练模型因领域差距难以泛化。

Method: 利用相场模拟生成微观结构形态及真实标签，用CycleGAN进行图像转换生成逼真合成数据，用U - Net模型在合成数据上训练。

Result: U - Net模型在未见实验图像上平均边界F1分数达0.90，交并比达0.88，合成图像与真实数据在统计和特征上无差异。

Conclusion: 该生成框架将数据稀缺问题转化为数据丰富问题，为加速材料发现和分析提供强大且全自动解决方案。

Abstract: Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.

</details>


### [906] [From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking](https://arxiv.org/abs/2602.00593)
*Yifan Jiang,Cong Zhang,Bofei Zhang,Yifan Yang,Bingzhang Wang,Yew-Soon Ong*

Main category: cs.CV

TL;DR: 提出新视觉问答基准Pix2Fact评估模型视觉理解和推理能力，评估显示现有模型与人类差距大，该基准将推动下一代多模态代理发展。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法评估视觉定位和知识推理协同能力，为填补此空白提出Pix2Fact。

Method: 创建包含1000张高分辨率图像、涵盖8种日常场景的Pix2Fact基准，由专业人员精心设计问答，对9种先进VLM进行评估。

Result: 最先进模型平均准确率仅24.0%，远低于人类的56%。

Conclusion: Pix2Fact将作为关键基准推动下一代多模态代理发展。

Abstract: Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.

</details>


### [907] [Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention](https://arxiv.org/abs/2602.01801)
*Dvir Samuel,Issar Tzachor,Matan Levy,Micahel Green,Gal Chechik,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 提出无训练注意力框架解决自回归视频扩散模型推理时注意力层瓶颈问题，实现加速和稳定内存使用


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型核心注意力层在推理时成瓶颈，缓存增长致延迟和内存问题，影响长程一致性

Method: 提出TempCache压缩KV缓存，AnnCA加速交叉注意力，AnnSA稀疏化自注意力，三者构成统一无训练注意力框架

Result: 实验实现5 - 10倍端到端加速，保持视觉质量，长序列中稳定吞吐量和GPU内存使用

Conclusion: 该框架减少注意力计算和内存，与现有模型兼容，解决推理时瓶颈问题

Abstract: Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.

</details>


### [908] [CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions](https://arxiv.org/abs/2602.01844)
*Yuliang Zhan,Jian Li,Wenbing Huang,Wenbing Huang,Yang Liu,Hao Sun*

Main category: cs.CV

TL;DR: 提出CDG新场景和CloDS无监督动态学习框架，从多视图视觉观察中学习布料动力学，实验表明其能有效学习且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法模拟复杂动态系统需已知物理属性作为监督或输入，限制了在未知条件下的适用性。

Method: 提出CDG场景和CloDS框架，采用三阶段流程，在视频到几何接地阶段引入双位置不透明度调制。

Result: CloDS能从视觉数据中有效学习布料动力学，对未见配置有强泛化能力。

Conclusion: CloDS是一种有效的从多视图视觉观察中无监督学习布料动力学的方法。

Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\footnote{As in this example.

</details>


### [909] [Learning Sparse Visual Representations via Spatial-Semantic Factorization](https://arxiv.org/abs/2602.01905)
*Theodore Zhengde Zhao,Sid Kiblawi,Jianwei Yang,Naoto Usuyama,Reuben Tan,Noel C Codella,Tristan Naumann,Hoifung Poon,Mu Wei*

Main category: cs.CV

TL;DR: 提出STELLAR框架解决自监督学习中语义理解和图像重建的冲突，少量稀疏令牌可同时支持高质量重建和语义性能。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在语义理解和图像重建之间存在根本冲突，现有方法无法兼顾。

Method: 将视觉特征分解为语义概念及其空间分布的低秩乘积，在语义令牌上进行DINO式增强对齐，同时保持定位矩阵中的精确空间映射。

Result: 仅16个稀疏令牌就能同时支持高质量重建（2.60 FID）和达到密集骨干网络的语义性能（ImageNet准确率79.10%）。

Conclusion: STELLAR是一种通用稀疏表示，通过分离语义和空间几何，弥合了判别式和生成式视觉之间的差距。

Abstract: Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.

</details>


### [910] [DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification](https://arxiv.org/abs/2602.01906)
*Farhan Ullah,Irfan Ullah,Khalil Khan,Giovanni Pau,JaKeoung Koo*

Main category: cs.CV

TL;DR: 提出DSXFormer用于高光谱图像分类，在四个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的高光谱图像分类方法难以兼顾光谱区分能力和计算效率，因此提出新模型。

Method: 提出DSX块自适应重新校准光谱特征通道，引入动态上下文注意力机制捕捉局部光谱 - 空间关系，采用多种策略进行多尺度特征学习。

Result: 在四个数据集上DSXFormer表现优于现有方法，分类准确率分别达99.95%、98.91%、99.85%和98.52%。

Conclusion: DSXFormer能有效平衡光谱强调和空间上下文表示，在高光谱图像分类任务中表现出色。

Abstract: Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.

</details>


### [911] [Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification](https://arxiv.org/abs/2602.00956)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 提出结合TDA与DenseNet121的混合深度学习框架用于阿尔茨海默病四分类，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于神经影像的临床决策支持系统中阿尔茨海默病早期准确诊断的难题。

Method: 提出结合TDA与DenseNet121的框架，TDA捕捉大脑结构拓扑特征，DenseNet121学习MRI切片空间特征，融合特征增强类别可分性。

Result: 在OASIS - 1 Kaggle MRI数据集上，模型准确率达99.93%，AUC为100%，优于现有方法。

Conclusion: 将拓扑信息融入深度学习流程有效，所提框架可作为自动诊断阿尔茨海默病的强大准确工具。

Abstract: Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.
  Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.

</details>


### [912] [Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated](https://arxiv.org/abs/2602.01973)
*Muli Yang,Gabriel James Goenawan,Henan Wang,Huaiyuan Qin,Chenghao Xu,Yanhua Yang,Fen Fang,Ying Sun,Joo-Hwee Lim,Hongyuan Zhu*

Main category: cs.CV

TL;DR: 现有AI生成图像检测器测试时存在偏差，常误将假图判为真图。本文提出基于贝叶斯决策理论的校准框架，实验显示该方法显著提升鲁棒性且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在测试时存在系统偏差，常误判假图为真图，原因是假样本分布偏移和训练时学到的隐式先验。

Method: 提出基于贝叶斯决策理论的事后校准框架，引入可学习的标量校正模型对数，在目标分布的小验证集上优化，冻结主干网络。

Result: 在具有挑战性的基准测试中，该方法显著提高了鲁棒性，无需重新训练。

Conclusion: 该方法是轻量级、有原则的解决方案，适用于开放世界中可靠和自适应的AI生成图像检测。

Abstract: Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.

</details>


### [913] [CortiNet: A Physics-Perception Hybrid Cortical-Inspired Dual-Stream Network for Gallbladder Disease Diagnosis from Ultrasound](https://arxiv.org/abs/2602.01000)
*Vagish Kumar,Souvik Chakraborty*

Main category: cs.CV

TL;DR: 提出轻量级CortiNet用于胆囊疾病诊断，在大量图像上验证其高精度且参数少。


<details>
  <summary>Details</summary>
Motivation: 超声图像低分辨率和斑点噪声影响诊断可靠性，传统大卷积神经网络难用于临床。

Method: 提出CortiNet，分离低频结构和高频细节处理，采用后期融合机制，还提出结构感知可解释性框架。

Result: 在10,692张图像上实验，CortiNet诊断准确率达98.74%。

Conclusion: CortiNet能以少量参数实现高诊断精度，适用于临床。

Abstract: Ultrasound imaging is the primary diagnostic modality for detecting Gallbladder diseases due to its non-invasive nature, affordability, and wide accessibility. However, the low resolution and speckle noise inherent to ultrasound images hinder diagnostic reliability, prompting the use of large convolutional neural networks that are difficult to deploy in routine clinical settings. In this work, we propose CortiNet, a lightweight, cortical-inspired dual-stream neural architecture for gallbladder disease diagnosis that integrates physically interpretable multi-scale signal decomposition with perception-driven feature learning. Inspired by parallel processing pathways in the human visual cortex, CortiNet explicitly separates low-frequency structural information from high-frequency perceptual details and processes them through specialized encoding streams. By operating directly on structured, frequency-selective representations rather than raw pixel intensities, the architecture embeds strong physics-based inductive bias, enabling efficient feature learning with a significantly reduced parameter footprint. A late-stage cortical-style fusion mechanism integrates complementary structural and textural cues while preserving computational efficiency. Additionally, we propose a structure-aware explainability framework wherein gradient-weighted class activation mapping is only applied to the structural branch of the proposed CortiNet architecture. This choice allows the model to only focus on the structural features, making it robust against speckle noise. We evaluate CortiNet on 10,692 expert-annotated images spanning nine clinically relevant gallbladder disease categories. Experimental results demonstrate that CortiNet achieves high diagnostic accuracy (98.74%) with only a fraction of the parameters required by conventional deep convolutional models.

</details>


### [914] [PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors](https://arxiv.org/abs/2602.01069)
*Seema K. Poudel,Sunny K. Khadka*

Main category: cs.CV

TL;DR: 本文将图像分割问题构建为PDE约束优化问题，在LIVECell数据集验证，结果显示在分割精度、边界保真度、稳定性和泛化性上优于无约束深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像分割是病态逆问题，无约束经验风险最小化会导致不稳定解和较差泛化能力。

Method: 将图像分割表述为PDE约束优化问题，通过变分正则化将物理先验集成到深度学习模型，使用由数据保真项和惩罚项组成的复合目标函数，在LIVECell数据集实验。

Result: 与无约束深度学习基线相比，分割精度和边界保真度持续提升，在低样本情况下稳定性和泛化性增强。

Conclusion: PDE约束优化可加强数据驱动学习框架，在变分方法、统计学习和科学机器学习间搭建桥梁。

Abstract: Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.

</details>


### [915] [SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors](https://arxiv.org/abs/2602.02000)
*Bing He,Jingnan Gao,Yunuo Chen,Ning Cao,Gang Chen,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: 提出基于2DGS的SurfSplat框架用于高保真3D重建，引入HRRC评估指标，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的方法在从稀疏图像重建3D场景时，难以生成连续表面，近看有严重伪影。

Method: 提出基于2DGS的SurfSplat前馈框架，结合表面连续性先验和强制alpha混合策略；引入HRRC评估指标。

Result: 在RealEstate10K、DL3DV和ScanNet上的大量实验表明，SurfSplat在标准指标和HRRC上均优于现有方法。

Conclusion: SurfSplat为从稀疏输入进行高保真3D重建提供了可靠解决方案。

Abstract: Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/

</details>


### [916] [ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning](https://arxiv.org/abs/2602.02004)
*Gongli Xi,Kun Wang,Zeming Gao,Huahui Yi,Haolang Lu,Ye Tian,Wendong Wang*

Main category: cs.CV

TL;DR: 大模态推理模型推理时易产生幻觉，本文识别推理漂移问题，提出ClueRecall评估指标和ClueTracer插件抑制幻觉，提升推理和非推理模型表现。


<details>
  <summary>Details</summary>
Motivation: 大模态推理模型在推理时会产生幻觉，现有非推理模型的方法无法在推理场景中定位线索，需解决这一问题。

Method: 识别推理漂移问题，引入ClueRecall指标，提出训练、参数和架构无关的ClueTracer插件，从问题出发追踪线索传播路径。

Result: ClueTracer在不额外训练情况下，使推理架构在推理基准上提升1.21倍，在非推理场景中提升1.14倍。

Conclusion: ClueTracer能有效抑制大模态推理模型的幻觉，提升不同架构模型在推理和非推理场景中的表现。

Abstract: Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\rightarrow$ outputs $\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \textbf{without any additional training}, ClueTracer improves all \textbf{reasoning} architectures (including \texttt{R1-OneVision}, \texttt{Ocean-R1}, \texttt{MM-Eureka}, \emph{etc}.) by $\mathbf{1.21\times}$ on reasoning benchmarks. When transferred to \textbf{non-reasoning} settings, it yields a $\mathbf{1.14\times}$ gain.

</details>


### [917] [Rethinking Genomic Modeling Through Optical Character Recognition](https://arxiv.org/abs/2602.02014)
*Hongxin Xiang,Pengsen Ma,Yunkang Cao,Di Yu,Haowen Chen,Xinyu Yang,Xiangxiang Zeng*

Main category: cs.CV

TL;DR: 提出基于视觉的框架OpticalDNA，将基因组建模视为OCR式文档理解，在多个基因组基准测试中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型采用大语言模型架构，顺序读取与基因组语义不匹配，存在计算浪费和长上下文理解压缩问题。

Method: 将DNA渲染为结构化视觉布局，训练具有OCR能力的视觉 - 语言模型，定义基于核心基因组原语的提示条件目标。

Result: 在多个基因组基准测试中始终优于近期基线，在长达450k碱基的序列上，用近20倍少的有效标记实现最佳整体性能，仅调整256k可训练参数就超越激活参数多985倍的模型。

Conclusion: OpticalDNA能学习布局感知的DNA表示，在减少有效标记预算下保留细粒度基因组信息，表现优异。

Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.

</details>


### [918] [Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion](https://arxiv.org/abs/2602.01183)
*Chunming He,Rihan Zhang,Fengyang Xiao,Dingming Zhang,Zhiwen Cao,Sina Farsiu*

Main category: cs.CV

TL;DR: 受生物学习启发，提出双阶段学习框架CurriSeg解决上下文纠缠内容分割问题，实验证明其能提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统分割网络忽略复杂数据分布下的学习动态，为解决上下文纠缠内容分割问题，提升表示可靠性。

Method: 提出CurriSeg框架，包含课程选择阶段（基于样本损失的时间统计动态选择训练数据）和反课程促进阶段（设计谱盲微调抑制高频成分）。

Result: CurriSeg在不同上下文纠缠内容分割基准测试中持续提升性能，且不增加参数和总训练时间。

Conclusion: CurriSeg为渐进学习和挑战如何促进鲁棒且上下文感知的分割提供了有原则的观点，代码将开源。

Abstract: Biological learning proceeds from easy to difficult tasks, gradually reinforcing perception and robustness. Inspired by this principle, we address Context-Entangled Content Segmentation (CECS), a challenging setting where objects share intrinsic visual patterns with their surroundings, as in camouflaged object detection. Conventional segmentation networks predominantly rely on architectural enhancements but often ignore the learning dynamics that govern robustness under entangled data distributions. We introduce CurriSeg, a dual-phase learning framework that unifies curriculum and anti-curriculum principles to improve representation reliability. In the Curriculum Selection phase, CurriSeg dynamically selects training data based on the temporal statistics of sample losses, distinguishing hard-but-informative samples from noisy or ambiguous ones, thus enabling stable capability enhancement. In the Anti-Curriculum Promotion phase, we design Spectral-Blindness Fine-Tuning, which suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues and thus strengthens generalization. Extensive experiments demonstrate that CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time, offering a principled view of how progression and challenge interplay to foster robust and context-aware segmentation. Code will be released.

</details>


### [919] [One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation](https://arxiv.org/abs/2602.02033)
*Shuo Lu,Haohan Wang,Wei Feng,Weizhen Wang,Shen Zhang,Yaoyu Li,Ao Ma,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Bing Zhan,Yuan Xu,Huizai Yao,Yongcan Yu,Chenyang Si,Jian Liang*

Main category: cs.CV

TL;DR: 现有广告图像生成方法忽略用户群体偏好多样性，提出OSMF框架，利用产品感知自适应分组和G - MLLM生成定制图像，用Group - DPO微调，还引入GAIP数据集，实验表现达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有广告图像生成采用‘一刀切’策略，忽略用户群体偏好多样性，导致特定群体表现不佳，影响定向营销效果。

Method: 提出OSMF框架，包括产品感知自适应分组、偏好条件图像生成（使用G - MLLM），用Group - DPO微调G - MLLM，引入GAIP数据集。

Result: 框架在离线和在线设置中均达到了最先进的性能。

Conclusion: OSMF框架有效解决了现有广告图像生成问题，提升了各群体的CTR，推动了该领域发展。

Abstract: Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.

</details>


### [920] [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/abs/2602.02043)
*Cristian Sbrolli,Matteo Matteucci,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 本文介绍Auto - Comp自动合成管道生成可扩展基准，评估20个VLMs，发现其组合推理存在普遍问题及新缺陷，揭示权衡关系并发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 现代VLMs在组合推理上有缺陷，需解开视觉和语言根源问题以进行可靠评估。

Method: 引入Auto - Comp管道，生成成对图像进行A/B测试，在新基准上评估20个VLMs。

Result: 发现CLIP和SigLIP模型家族存在普遍组合失败，有超越简单属性交换的更深缺陷，存在视觉语言上下文的权衡关系。

Conclusion: VLMs组合推理存在问题，发布Auto - Comp管道和生成的基准以促进未来基准创建。

Abstract: Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing "a red cube and a blue sphere" with "a blue cube and a red sphere". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., "a monitor to the left of a bicycle on a white background") and LLM-generated Contextual captions (e.g., "In a brightly lit photography studio, a monitor is positioned to the left of a bicycle"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel "Confusion Benchmark" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).

</details>


### [921] [Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data](https://arxiv.org/abs/2602.02067)
*Nikola Cenikj,Özgün Turgut,Alexander Müller,Alexander Steger,Jan Kehrer,Marcus Brugger,Daniel Rueckert,Eimo Martens,Philip Müller*

Main category: cs.CV

TL;DR: 提出基于Transformer的多视图多实例学习框架SegmentMIL用于患者级冠状动脉狭窄分类，表现出色且代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有单视图深度学习模型检测冠状动脉狭窄依赖昂贵视图级注释，且无法捕捉多视图时间动态和依赖关系。

Method: 提出SegmentMIL框架，在真实临床数据集上训练，使用患者级监督，无需视图级注释，联合预测狭窄存在并定位受影响解剖区域。

Result: SegmentMIL在内部和外部评估中表现良好，优于视图级模型和经典MIL基线。

Conclusion: SegmentMIL是冠状动脉狭窄诊断具有临床可行性和可扩展性的解决方案。

Abstract: Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.

</details>


### [922] [PromptRL: Prompt Matters in RL for Flow-Based Image Generation](https://arxiv.org/abs/2602.01382)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.CV

TL;DR: 研究指出当前基于流匹配模型的强化学习管道存在样本效率低和提示过拟合问题，提出PromptRL框架解决，在多基准测试中达SOTA，在图像编辑模型上验证有效性，且性能优、所需滚动次数少。


<details>
  <summary>Details</summary>
Motivation: 当前基于流匹配模型的强化学习管道在文本到图像生成中存在样本效率低和提示过拟合的问题。

Method: 提出PromptRL框架，将语言模型作为可训练的提示细化代理，纳入基于流的强化学习优化循环。

Result: 在多个基准测试中达到SOTA，如在GenEval得0.97分等；在图像编辑模型上提升EditReward，超越Gemini 2.5 Flash Image，与ReasonNet性能相当；所需滚动次数比仅基于流的朴素强化学习少超2倍。

Conclusion: PromptRL能持续达到更高性能上限，且所需滚动次数更少。

Abstract: Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.
  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.

</details>


### [923] [Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies](https://arxiv.org/abs/2602.02124)
*Olga Graf,Dhrupal Patel,Peter Groß,Charlotte Lempp,Matthias Hein,Fabian Heinemann*

Main category: cs.CV

TL;DR: 提出基于AI的啮齿动物肝脏组织病理全切片图像异常检测框架，可检测已知和罕见病理情况，在小鼠肝脏WSIs上表现准确，有潜力支持临床前流程。


<details>
  <summary>Details</summary>
Motivation: 药物诱导毒性是临床前开发和早期临床试验失败主因，传统组织病理学评估依赖专家，存在大规模筛选瓶颈。

Method: 生成像素级标注数据集，用LoRA微调预训练的Vision Transformer进行组织分割，用马氏距离提取特征进行OOD检测，提出使用特定类别的阈值并优化。

Result: 仅0.16%的病理组织被误分类为健康组织，0.35%的健康组织被误分类为病理组织，能准确检测包括罕见形态的异常。

Conclusion: AI驱动的组织病理学有潜力支持临床前工作流程，减少后期失败，提高药物开发效率。

Abstract: Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\% of pathological tissue classified as healthy and 0.35\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.

</details>


### [924] [Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas](https://arxiv.org/abs/2602.01418)
*Christoffer Koo Øhrstrøm,Rafael I. Cabral Muchacho,Yifei Dong,Filippos Moumtzidellis,Ronja Güldenring,Florian T. Pokorny,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 提出基于抛物线的位置编码PaPE用于注意力架构视觉模态，在多数据集表现佳，外推实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有工作在将位置编码从语言的1D序列扩展到视觉的nD结构时，未充分考虑视觉特征，需设计新的位置编码。

Method: 从先前工作中提炼出平移不变性、旋转不变性、距离衰减、方向性和上下文感知等原则来设计PaPE。

Result: PaPE或PaPE - RI在8个数据集中的7个上取得最佳性能，在ImageNet - 1K外推实验中比次优位置编码绝对提升达10.5%。

Conclusion: PaPE是一种有效的视觉模态位置编码方法，具有良好的性能和外推能力。

Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.

</details>


### [925] [Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://arxiv.org/abs/2602.02185)
*Yu Zeng,Wenxuan Huang,Zhen Fang,Shuang Chen,Yufan Shen,Yishuo Cai,Xiaoman Wang,Zhenfei Yin,Lin Chen,Zehui Chen,Shiting Huang,Yiming Zhao,Yao Hu,Philip Torr,Wanli Ouyang,Shaosheng Cao*

Main category: cs.CV

TL;DR: 本文指出多模态大语言模型视觉和文本搜索能力评估难，现有基准有局限，构建VDR - Bench基准并提出多轮裁剪搜索工作流，为未来系统设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型视觉和文本搜索能力评估困难，现有基准存在非以视觉搜索为中心、评估场景过于理想化的问题。

Method: 构建包含2000个VQA实例的Vision - DeepResearch基准（VDR - Bench），采用多阶段精心策划和专家严格评审的流程；提出简单的多轮裁剪搜索工作流。

Result: 多轮裁剪搜索工作流能有效提高模型在现实视觉检索场景中的性能。

Conclusion: 研究结果为未来多模态深度研究系统的设计提供了实用指导，代码将在指定链接发布。

Abstract: Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [926] [Rotation-free Online Handwritten Character Recognition Using Linear Recurrent Units](https://arxiv.org/abs/2602.01533)
*Zhe Ling,Sicheng Yu,Danyu Yang*

Main category: cs.CV

TL;DR: 提出SW - PS+LRU框架用于在线手写字符识别，在含旋转变形的数据集上测试，精度高且性能超对比模型。


<details>
  <summary>Details</summary>
Motivation: 在线手写字符识别中旋转变形会降低识别精度，提取旋转不变特征是难题。

Method: 采用Sliding Window Path Signature (SW - PS)捕捉字符局部结构特征，引入轻量级Linear Recurrent Units (LRU)作为分类器。

Result: 在CASIA - OLHWDB1.1数据集三个子集上进行随机旋转角度达±180°的识别实验，集成学习后精度分别为99.62%、96.67%和94.33%。

Conclusion: 提出的SW - PS+LRU框架在收敛速度和测试精度上均优于竞争模型。

Abstract: Online handwritten character recognition leverages stroke order and dynamic features, which generally provide higher accuracy and robustness compared with offline recognition. However, in practical applications, rotational deformations can disrupt the spatial layout of strokes, substantially reducing recognition accuracy. Extracting rotation-invariant features therefore remains a challenging open problem. In this work, we employ the Sliding Window Path Signature (SW-PS) to capture local structural features of characters, and introduce the lightweight Linear Recurrent Units (LRU) as the classifier. The LRU combine the fast incremental processing capability of recurrent neural networks (RNN) with the efficient parallel training of state space models (SSM), while reliably modelling dynamic stroke characteristics. We conducted recognition experiments with random rotation angle up to $\pm 180^{\circ}$ on three subsets of the CASIA-OLHWDB1.1 dataset: digits, English upper letters, and Chinese radicals. The accuracies achieved after ensemble learning were $99.62\%$, $96.67\%$, and $94.33\%$, respectively. Experimental results demonstrate that the proposed SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy.

</details>


### [927] [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334)
*Fatemeh Zargarbashi,Dhruv Agrawal,Jakob Buhmann,Martin Guay,Stelian Coros,Robert W. Sumner*

Main category: cs.CV

TL;DR: 提出新方法对人体运动数据的风格和内容进行有效解耦，实现风格迁移，框架在多推理应用中表现出强通用性。


<details>
  <summary>Details</summary>
Motivation: 人体运动数据丰富复杂，语义内容和风格特征难建模，需有效解耦风格和内容以实现风格迁移。

Method: 采用Residual Vector Quantized Variational Autoencoders (RVQ - VAEs)学习运动的粗到细表示，结合对比学习和新的信息泄漏损失及码本学习，使用量化代码交换技术。

Result: 实现无需对未见风格微调的运动风格迁移，框架在风格迁移、风格去除和运动融合等推理应用中表现良好。

Conclusion: 所提框架具有较强的通用性，能有效实现人体运动数据的风格和内容解耦及风格迁移等应用。

Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.

</details>


### [928] [Implicit neural representation of textures](https://arxiv.org/abs/2602.02354)
*Albert Kwok,Zheyuan Hu,Dounia Hammou*

Main category: cs.CV

TL;DR: 探索不同神经网络作为新纹理INR，实验证明其图像质量好，分析多目标平衡并研究相关应用。


<details>
  <summary>Details</summary>
Motivation: 探索不同神经网络设计新的纹理INR，使其在输入UV坐标空间以连续方式运行。

Method: 设计不同神经网络作为新纹理INR，并进行大量实验。

Result: 这些INR在图像质量方面表现良好，有一定的内存使用和渲染推理时间。

Conclusion: 分析了各目标间的平衡，还研究了在实时渲染和下游任务中的相关应用。

Abstract: Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.

</details>


### [929] [Spatio-Temporal Transformers for Long-Term NDVI Forecasting](https://arxiv.org/abs/2602.01799)
*Ido Faran,Nathan S. Netanyahu,Maxim Shoshany*

Main category: cs.CV

TL;DR: 本文提出STT - LTF框架用于地中海异质景观长期卫星图像时间序列分析，该框架结合时空建模，实验显示其性能优于传统方法，适合快速生态过渡区分析。


<details>
  <summary>Details</summary>
Motivation: 长期卫星图像时间序列分析在异质景观尤其是地中海地区面临挑战，需新方法解决。

Method: 提出STT - LTF框架，通过统一的transformer架构处理多尺度空间斑块和时间序列，采用综合自监督学习策略训练模型。

Result: 在Landsat数据实验中，STT - LTF明年预测的MAE为0.0328，R^2为0.8412，优于传统统计方法、CNN、LSTM和标准transformer。

Conclusion: STT - LTF能处理不规则时间采样和可变预测范围，适合快速生态过渡的异质景观分析。

Abstract: Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.

</details>


### [930] [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393)
*Ruiqi Wu,Xuanhua He,Meng Cheng,Tianyu Yang,Yong Zhang,Zhuoliang Kang,Xunliang Cai,Xiaoming Wei,Chunle Guo,Chongyi Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 提出Infinite - World世界模型，能在复杂真实环境保持超1000帧连贯视觉记忆，经实验验证性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在真实世界视频上缺乏有效训练范式，因存在噪声姿态估计和视点重访稀缺问题。

Method: 1. 引入Hierarchical Pose - free Memory Compressor (HPMC)递归提炼历史潜在信息；2. 提出Uncertainty - aware Action Labeling模块将连续运动离散化；3. 采用Revisit - Dense Finetuning Strategy激活模型长程闭环能力。

Result: 通过客观指标和用户研究等大量实验表明，Infinite - World在视觉质量、动作可控性和空间一致性方面表现优越。

Conclusion: Infinite - World是一个强大的交互式世界模型，能有效解决现有模型在真实世界视频训练中的问题。

Abstract: We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.

</details>


### [931] [ReasonEdit: Editing Vision-Language Models using Human Reasoning](https://arxiv.org/abs/2602.02408)
*Jiaxing Qiu,Kaihua Hou,Roxana Daneshjou,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.CV

TL;DR: 提出ReasonEdit用于编辑视觉语言模型，可在编辑时引入人类推理，在多数据集上实现了最先进的编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型编辑器无法处理需要大量推理的任务，因此需要一种能在编辑时引入人类推理的视觉语言模型编辑器。

Method: 连续地将人类推理存储在代码本中，在推理时使用受网络科学启发的拓扑平衡多模态嵌入方法检索相关事实。

Result: 在多个基于推理的视觉问答数据集上的四个视觉语言模型中，ReasonEdit实现了最先进的编辑性能。

Conclusion: 在编辑过程中使用人类推理可以大大提高编辑的泛化能力。

Abstract: Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.

</details>


### [932] [UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing](https://arxiv.org/abs/2602.02437)
*Dianyi Wang,Chaofan Ma,Feng Han,Size Wu,Wei Song,Yibin Wang,Zhixiong Zhang,Tianhang Wang,Siyuan Wang,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出UniReason框架统一文本到图像生成和图像编辑任务，构建数据集支持，实验显示其在推理密集型基准上表现出色且具备通用合成能力。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型在复杂合成任务中推理能力不足，且将文本到图像生成和图像编辑视为孤立任务，需要改进。

Method: 提出UniReason框架，通过双推理范式统一两个任务，将生成视为世界知识增强规划，利用编辑能力进行视觉细化，构建大规模推理数据集和代理生成语料库。

Result: UniReason在WISE、KrisBench和UniREditBench等推理密集型基准上取得先进性能，保持了卓越的通用合成能力。

Conclusion: UniReason框架有效统一文本到图像生成和图像编辑任务，提升了复杂合成任务的推理能力。

Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.

</details>


### [933] [Multi-head automated segmentation by incorporating detection head into the contextual layer neural network](https://arxiv.org/abs/2602.02471)
*Edwin Kys,Febian Febian*

Main category: cs.CV

TL;DR: 提出基于Swin U - Net的门控多头Transformer架构用于放疗自动分割，可抑制假阳性，实验表明该模型性能优于非门控基线模型，提升了分割可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习自动分割模型在缺乏目标结构的切片中常产生解剖学上不合理的假阳性或幻觉。

Method: 提出基于Swin U - Net的门控多头Transformer架构，结合层间上下文集成和并行检测头，通过多层感知器进行切片级结构检测，通过上下文增强流进行像素级分割，用检测输出控制分割预测，训练使用逐片Tversky损失解决类别不平衡。

Result: 在前列腺解剖边缘案例数据集上，门控模型显著优于非门控仅分割基线模型，平均Dice损失更低，检测概率与解剖结构存在强相关，有效消除了虚假分割。

Conclusion: 基于检测的门控增强了自动分割应用的鲁棒性和解剖学合理性，减少幻觉预测且不影响有效切片的分割质量，为改善临床放疗自动轮廓勾画工作流程的可靠性提供了有前景的方法。

Abstract: Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.

</details>


### [934] [PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss](https://arxiv.org/abs/2602.02493)
*Zehong Ma,Ruihan Xu,Shiliang Zhang*

Main category: cs.CV

TL;DR: 提出带感知监督的像素扩散框架PixelGen，优于潜在扩散模型，无需VAE等，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有像素扩散方法在优化高维像素流形时有挑战，落后于潜在扩散模型。

Method: PixelGen引入LPIPS损失和基于DINO的感知损失，引导扩散模型学习更有意义的感知流形。

Result: PixelGen在ImageNet - 256上FID达5.11，大规模文本到图像生成GenEval分数为0.79。

Conclusion: PixelGen提供了更简单强大的生成范式。

Abstract: Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.

</details>


### [935] [Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training](https://arxiv.org/abs/2602.02114)
*Xin Ding,Yun Chen,Sen Zhang,Kao Zhang,Nenglun Chen,Peibei Cao,Yongwei Wang,Fei Wu*

Main category: cs.CV

TL;DR: 本文提出改进的CCDM框架iCCDM，结合先进的EDM框架并做修改，实验证明iCCDM优于现有方法，提高生成质量并降低采样成本。


<details>
  <summary>Details</summary>
Motivation: CCDM存在依赖过时扩散框架、采样效率低等局限性，且被CcGAN - AVAR超越，需要改进。

Method: 提出iCCDM，结合Elucidated Diffusion Model（EDM）框架，引入矩阵形式的EDM公式和自适应邻域训练策略。

Result: 在四个基准数据集上的实验表明，iCCDM优于现有方法，包括最先进的大规模文本到图像扩散模型，提高了生成质量并显著降低采样成本。

Conclusion: iCCDM能有效解决CCDM的问题，在图像生成任务中表现出色，具有更好的生成质量和采样效率。

Abstract: Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\times64$ to $256\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.

</details>


### [936] [NAB: Neural Adaptive Binning for Sparse-View CT reconstruction](https://arxiv.org/abs/2602.02356)
*Wangduo Xie,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 提出NAB方法将矩形先验融入CT稀疏重建，优化编码参数提升精度，实验显示在多数据集表现优异。


<details>
  <summary>Details</summary>
Motivation: 经典隐式神经网络无法利用物体形状先验，而许多工业物体有矩形结构，需将形状先验融入CT稀疏重建以降成本、提质量。

Method: 提出Neural Adaptive Binning (NAB)方法，先将坐标空间映射到分箱向量空间，用基于移位双曲正切函数差异的分箱机制，可绕输入平面法向量旋转，再用神经网络处理预测CT衰减系数，通过投影数据梯度流优化编码参数。

Result: 在两个工业数据集上表现优越，分箱函数扩展后在医学数据集上也很稳健。

Conclusion: NAB方法为将形状先验融入基于神经网络的重建提供了新视角，有较好的泛化能力。

Abstract: Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \textbf{N}eural \textbf{A}daptive \textbf{B}inning (\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.

</details>


### [937] [Personalized Image Generation via Human-in-the-loop Bayesian Optimization](https://arxiv.org/abs/2602.02388)
*Rajalaxmi Rajagopalan,Debottam Dutta,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 本文提出MultiBO方法，利用用户偏好反馈引导扩散模型，实现更接近用户心中目标的个性化图像生成，实验结果良好。


<details>
  <summary>Details</summary>
Motivation: 现有语言提示引导生成模型生成的图像难以完全符合用户心中目标，需缩小差距。

Method: 开发MultiBO方法，根据当前图像生成K个新图像，获取用户偏好反馈，用反馈引导扩散模型生成新的K个图像。

Result: 在B轮用户反馈内可更接近目标图像，30个用户定性评分和5个基线对比的定量指标显示结果良好。

Conclusion: 人类的多选项反馈能有效用于个性化图像生成。

Abstract: Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [938] [AutoBinder Agent: An MCP-Based Agent for End-to-End Protein Binder Design](https://arxiv.org/abs/2602.00019)
*Fukang Ge,Jiarui Zhu,Linjie Zhang,Haowen Xiao,Xiangcheng Bao,Fangnan Xie,Danyang Chen,Yanrui Lu,Yuting Wang,Ziqian Guan,Lin Gu,Jinhao Bi,Yingying Zhu*

Main category: q-bio.BM

TL;DR: 提出基于LLM和MCP的端到端药物设计框架，整合多个组件，支持从头生成结合物，提升流程特性。


<details>
  <summary>Details</summary>
Motivation: 现代药物发现AI技术分散在不同平台，导致工作流碎片化、接口不一致和集成开销高。

Method: 利用大语言模型（LLM）和模型上下文协议（MCP），整合MaSIF、Rosetta、ProteinMPNN和AlphaFold3四个组件。

Result: 框架支持从目标结构出发，通过表面分析、支架嫁接和构象构建、序列优化和结构预测等步骤从头生成结合物。

Conclusion: 该框架以协议驱动、LLM协调的架构取代基于脚本的刚性工作流，提高了可重复性，降低了人工开销，确保了药物设计全过程的可扩展性、可移植性和可审计性。

Abstract: Modern AI technologies for drug discovery are distributed across heterogeneous platforms-including web applications, desktop environments, and code libraries-leading to fragmented workflows, inconsistent interfaces, and high integration overhead. We present an agentic end-to-end drug design framework that leverages a Large Language Model (LLM) in conjunction with the Model Context Protocol (MCP) to dynamically coordinate access to biochemical databases, modular toolchains, and task-specific AI models. The system integrates four state-of-the-art components: MaSIF (MaSIF-site and MaSIF-seed-search) for geometric deep learning-based identification of protein-protein interaction (PPI) sites, Rosetta for grafting protein fragments onto protein backbones to form mini proteins, ProteinMPNN for amino acid sequences redesign, and AlphaFold3 for near-experimental accuracy in complex structure prediction. Starting from a target structure, the framework supports de novo binder generation via surface analysis, scaffold grafting and pose construction, sequence optimization, and structure prediction. Additionally, by replacing rigid, script-based workflows with a protocol-driven, LLM-coordinated architecture, the framework improves reproducibility, reduces manual overhead, and ensures extensibility, portability, and auditability across the entire drug design process.

</details>


### [939] [Controlling Repetition in Protein Language Models](https://arxiv.org/abs/2602.00782)
*Jiahao Zhang,Zeqing Zhang,Di Wang,Lijie Hu*

Main category: q-bio.BM

TL;DR: 提出首个蛋白语言模型重复问题系统研究，提出UCCS方法降低重复且不影响可折叠性，实验表明优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 蛋白语言模型在生成时易出现重复，降低结构可信度和功能可行性，需解决该问题。

Method: 提出定量指标表征重复，提出UCCS方法，用受限数据集引导蛋白生成，构建对比集控制结构效用，生成引导向量注入推理。

Result: 实验表明UCCS方法优于解码惩罚和其他基线，降低重复同时保留AlphaFold置信度分数。

Conclusion: 重复控制是蛋白语言模型核心挑战，数据集引导引导是可靠蛋白生成的有效方法。

Abstract: Protein language models (PLMs) have enabled advances in structure prediction and de novo protein design, yet they frequently collapse into pathological repetition during generation. Unlike in text, where repetition merely reduces readability, in proteins it undermines structural confidence and functional viability. To unify this problem, we present the first systematic study of repetition in PLMs. We first propose quantitative metrics to characterize motif-level and homopolymer repetition and then demonstrate their negative impact on folding reliability. To address this challenge, we propose UCCS (Utility-Controlled Contrastive Steering), which steers protein generation with a constrained dataset. Instead of naively contrasting high- vs. low-repetition sequences, we construct contrastive sets that maximize differences in repetition while tightly controlling for structural utility. This disentanglement yields steering vectors that specifically target repetition without degrading foldability. Injected at inference, these vectors consistently reduce repetition without retraining or heuristic decoding. Experiments with ESM-3 and ProtGPT2 in CATH, UniRef50, and SCOP show that our method outperforms decoding penalties and other baselines, substantially lowering repetition while preserving AlphaFold confidence scores. Our results establish repetition control as a central challenge for PLMs and highlight dataset-guided steering as a principled approach for reliable protein generation.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [940] [FinEvo: From Isolated Backtests to Ecological Market Games for Multi-Agent Financial Strategy Evolution](https://arxiv.org/abs/2602.00948)
*Mingxi Zou,Jiaxiang Chen,Aotian Luo,Jingyi Dai,Chi Zhang,Dongning Sun,Zenglin Xu*

Main category: physics.soc-ph

TL;DR: 提出FinEvo生态博弈形式化方法研究多智能体金融策略的进化动态，实验表明其稳定且能揭示依赖上下文的结果，可用于分析市场动态及政策影响。


<details>
  <summary>Details</summary>
Motivation: 传统金融策略评估依赖静态环境下的孤立回测，忽视相关性和相互作用，无法解释策略在不断变化市场中的存续情况，因此需要新方法。

Method: 从生态视角出发，将交易策略建模为在共享市场中相互作用和学习的自适应智能体，提出FinEvo生态博弈形式化方法，个体层面使用基于ML的异质交易者，群体层面通过选择、创新和环境扰动三种机制使策略分布进化。

Result: 实验表明FinEvo稳定且具有可重复性，能揭示静态回测无法发现的策略模式，如主导、崩溃或形成联盟。

Conclusion: FinEvo为分析多智能体金融市场的稳健性、适应性和涌现动态提供了统一的机制级协议，可用于探索宏观经济政策和金融监管对价格演变和均衡的潜在影响。

Abstract: Conventional financial strategy evaluation relies on isolated backtests in static environments. Such evaluations assess each policy independently, overlook correlations and interactions, and fail to explain why strategies ultimately persist or vanish in evolving markets. We shift to an ecological perspective, where trading strategies are modeled as adaptive agents that interact and learn within a shared market. Instead of proposing a new strategy, we present FinEvo, an ecological game formalism for studying the evolutionary dynamics of multi-agent financial strategies. At the individual level, heterogeneous ML-based traders-rule-based, deep learning, reinforcement learning, and large language model (LLM) agents-adapt using signals such as historical prices and external news. At the population level, strategy distributions evolve through three designed mechanisms-selection, innovation, and environmental perturbation-capturing the dynamic forces of real markets. Together, these two layers of adaptation link evolutionary game theory with modern learning dynamics, providing a principled environment for studying strategic behavior. Experiments with external shocks and real-world news streams show that FinEvo is both stable for reproducibility and expressive in revealing context-dependent outcomes. Strategies may dominate, collapse, or form coalitions depending on their competitors-patterns invisible to static backtests. By reframing strategy evaluation as an ecological game formalism, FinEvo provides a unified, mechanism-level protocol for analyzing robustness, adaptation, and emergent dynamics in multi-agent financial markets, and may offer a means to explore the potential impact of macroeconomic policies and financial regulations on price evolution and equilibrium.

</details>


### [941] [Was Benoit Mandelbrot a hedgehog or a fox?](https://arxiv.org/abs/2602.01122)
*Rosario N. Mantegna*

Main category: physics.soc-ph

TL;DR: 本文认为Benoit Mandelbrot本质上是‘刺猬型’思想家，缩放概念贯穿其多领域研究，揭示其连贯学术轨迹。


<details>
  <summary>Details</summary>
Motivation: 探讨Benoit Mandelbrot在众多学科研究中背后统一的指导原则。

Method: 追溯缩放范式在其数学、物理和经济学贡献中的连续性。

Result: 发现缩放概念是贯穿其工作的核心思想，掩盖在表面折衷主义下的连贯学术轨迹被揭示。

Conclusion: 可以通过尺度不变性的几何和统计视角理解Mandelbrot对自然和社会现象建模的持久见解。

Abstract: Benoit Mandelbrot's scientific legacy spans an extraordinary range of disciplines, from linguistics and fluid turbulence to cosmology and finance, suggesting the intellectual temperament of a "fox" in Isaiah Berlin's famous dichotomy of thinkers. This essay argues, however, that Mandelbrot was, at heart, a "hedgehog": a thinker unified by a single guiding principle. Across his diverse pursuits, the concept of scaling -- manifested in self-similarity, power laws, fractals, and multifractals -- served as the central idea that structured his work. By tracing the continuity of this scaling paradigm through his contributions to mathematics, physics, and economics, the paper reveals a coherent intellectual trajectory masked by apparent eclecticism. Mandelbrot's enduring insight in the modeling of natural and social phenomena can be understood through the lens of the geometry and statistics of scale invariance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [942] [Verification Required: The Impact of Information Credibility on AI Persuasion](https://arxiv.org/abs/2602.00970)
*Saaduddin Mahmud,Eugene Bagdasarian,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 针对大语言模型代理的战略通信问题，引入MixTalk游戏，做大规模评估，提出TOPD方法提升接收者抗说服能力。


<details>
  <summary>Details</summary>
Motivation: 现有工作无法捕捉信息具有概率可信度的真实场景，需对大语言模型代理的战略通信有原则性理解。

Method: 引入MixTalk游戏模拟信息可信度，在三种现实部署场景进行大规模锦标赛评估，提出TOPD离线方法从交互日志中蒸馏策略。

Result: 揭示了大语言模型代理在信息可信度推理方面的优势和局限性，TOPD显著提升了接收者抗说服能力。

Conclusion: TOPD方法对提升大语言模型代理在战略通信中的性能有积极作用。

Abstract: Agents powered by large language models (LLMs) are increasingly deployed in settings where communication shapes high-stakes decisions, making a principled understanding of strategic communication essential. Prior work largely studies either unverifiable cheap-talk or fully verifiable disclosure, failing to capture realistic domains in which information has probabilistic credibility. We introduce MixTalk, a strategic communication game for LLM-to-LLM interaction that models information credibility. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state from prior beliefs, claims, and verification outcomes. We evaluate state-of-the-art LLM agents in large-scale tournaments across three realistic deployment settings, revealing their strengths and limitations in reasoning about information credibility and the explicit behavior that shapes these interactions. Finally, we propose Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time. Our results show that TOPD significantly improves receiver robustness to persuasion.

</details>


### [943] [Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory](https://arxiv.org/abs/2602.01708)
*Langyuan Cui,Chun Kai Ling,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本文用二十问游戏评估大语言模型信息搜索能力，提出GoT框架，实验表明该框架能提升最坏情况性能。


<details>
  <summary>Details</summary>
Motivation: 现有提升大语言模型信息搜索能力的方法依赖简化假设，会降低最坏情况性能，在高风险应用中有严重影响。

Method: 使用二十问游戏评估，引入并形式化战略语言搜索问题及其变体为两人零和扩展式博弈，提出GoT框架应用博弈论技术近似纳什均衡策略。

Result: 相比直接提示法和启发式搜索法，该方法在所有测试设置中都能持续提升最坏情况性能。

Conclusion: 所提方法能有效改善大语言模型信息搜索能力的最坏情况性能。

Abstract: Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.

</details>


### [944] [Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation](https://arxiv.org/abs/2507.21934)
*Tianyi Hu,Andrea Morales-Garzón,Jingyi Zheng,Maria Maistro,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 分析RAG在跨文化食谱改编中生成多样性不足问题，提出CARRIAGE框架，实验显示其在多样性和质量上有优势。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在跨文化食谱改编中无法生成多样结果的问题，满足不同饮食需求和偏好。

Method: 提出CARRIAGE，一个即插即用的RAG框架，增强检索和上下文组织的多样性。

Result: CARRIAGE在食谱改编的多样性和质量上达到帕累托效率，优于封闭书LLMs。

Conclusion: CARRIAGE是首个明确旨在生成高度多样化输出以适应多用户偏好的RAG框架，能解决RAG在创意任务中的局限性。

Abstract: In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs.

</details>


### [945] [PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.00007)
*MinGyu Jeon,SuWan Cho,JaeYoung Shu*

Main category: cs.CL

TL;DR: 提出PPoGA框架解决LLMs结合KGs在复杂问答中推理计划有缺陷的问题，实验显示该框架表现出色，强调元认知能力重要性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs结合KGs在复杂问答中因初始推理计划有缺陷而失败的问题。

Method: 提出PPoGA框架，采用Planner - Executor架构分离策略与执行，利用预测处理机制，有自我修正机制。

Result: 在三个多跳KGQA基准测试中，PPoGA达到了最先进的性能，显著优于现有方法。

Conclusion: 构建更强大灵活的AI推理系统，元认知能力如问题重构很关键。

Abstract: Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.

</details>


### [946] [Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA](https://arxiv.org/abs/2602.00009)
*Samuel Thio,Matthew Lewis,Spiros Denaxas,Richard JB Dobson*

Main category: cs.CL

TL;DR: 提出MediGRAF混合图RAG系统用于电子健康记录信息检索，实验显示其在临床信息检索上表现良好。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录系统给临床医生带来认知负担，大语言模型在临床应用有局限，现有检索方法未同时整合结构化和非结构化数据。

Method: 提出MediGRAF系统，结合Neo4j Text2Cypher进行结构化关系遍历和向量嵌入进行非结构化叙述检索，实现自然语言查询。

Result: 使用MIMIC - IV数据集进行实验，事实查询召回率达100%，复杂推理任务专家质量评分平均4.25/5且无安全违规。

Conclusion: 混合图基础显著推进临床信息检索，是标准大语言模型部署更安全、全面的替代方案。

Abstract: Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.

</details>


### [947] [Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting](https://arxiv.org/abs/2602.00758)
*Ali El Lahib,Ying-Jieh Xia,Zehan Li,Yuxuan Wang,Xinyu Pi*

Main category: cs.CL

TL;DR: 研究表明搜索引擎日期过滤器用于回顾性评估搜索增强预测器时不可靠，建议采取更强检索保障或使用冻结的时间戳网页快照进行评估。


<details>
  <summary>Details</summary>
Motivation: 探究搜索引擎日期过滤器在回顾性评估搜索增强预测器时的可靠性。

Method: 审计Google Search的before:过滤器，用gpt - oss - 120b基于有泄漏和无泄漏文档进行预测。

Result: 71%的问题返回至少一个含强后截止日期泄漏的页面，41%至少一个页面直接揭示答案；基于有泄漏文档预测准确率虚高。

Conclusion: 日期限制搜索不足以进行时间评估，建议采用更强检索保障或冻结的时间戳网页快照进行评估。

Abstract: Search-engine date filters are widely used to enforce pre-cutoff retrieval in retrospective evaluations of search-augmented forecasters. We show this approach is unreliable: auditing Google Search with a before: filter, 71% of questions return at least one page containing strong post-cutoff leakage, and for 41%, at least one page directly reveals the answer. Using a large language model (LLM), gpt-oss-120b, to forecast with these leaky documents, we demonstrate an inflated prediction accuracy (Brier score 0.108 vs. 0.242 with leak-free documents). We characterize common leakage mechanisms, including updated articles, related-content modules, unreliable metadata/timestamps, and absence-based signals, and argue that date-restricted search is insufficient for temporal evaluation. We recommend stronger retrieval safeguards or evaluation on frozen, time-stamped web snapshots to ensure credible retrospective forecasting.

</details>


### [948] [Unifying Adversarial Robustness and Training Across Text Scoring Models](https://arxiv.org/abs/2602.00857)
*Manveer Singh Tamber,Hosna Oyarhoseini,Jimmy Lin*

Main category: cs.CL

TL;DR: 本文提出统一文本评分模型对抗鲁棒性研究，指出当前对抗训练不足，引入多种方法提升鲁棒性和任务效果，还展示其对RLHF的价值。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型对抗鲁棒性研究分散，掩盖了共同漏洞，需统一文本评分模型的对抗鲁棒性研究。

Method: 提出统一研究方法，适应不同模型角色的攻击和对抗训练方法，引入多种文本评分模型的对抗训练方法。

Result: 当前对抗训练公式短视，结合互补训练方法可提升鲁棒性和任务效果，对抗训练的奖励模型能减轻奖励破解。

Conclusion: 统一研究文本评分模型对抗鲁棒性的方法有效，所引入方法有实用价值，代码和模型可供进一步研究。

Abstract: Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.

</details>


### [949] [Inferential Question Answering](https://arxiv.org/abs/2602.01239)
*Jamshid Mozafari,Hamed Zamani,Guido Zuccon,Adam Jatowt*

Main category: cs.CL

TL;DR: 现有问答系统多关注答案包含，本文提出推理问答新任务，构建QUIT数据集，评估发现当前问答管道不适合推理推理，推理问答是新的问答任务类型。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统多关注答案可直接提取或生成，而部分问题需推理，因此提出推理问答任务。

Method: 构建QUIT数据集，对检索器、重排器和基于大语言模型的阅读器进行综合评估。

Result: 传统问答任务有效的方法在推理问答中表现不佳，推理导向的大语言模型也无法超越小型通用模型。

Conclusion: 当前问答管道尚未准备好进行基于推理的推理，推理问答建立了新的问答任务类别。

Abstract: Despite extensive research on a wide range of question answering (QA) systems, most existing work focuses on answer containment-i.e., assuming that answers can be directly extracted and/or generated from documents in the corpus. However, some questions require inference, i.e., deriving answers that are not explicitly stated but can be inferred from the available information. We introduce Inferential QA -- a new task that challenges models to infer answers from answer-supporting passages which provide only clues. To study this problem, we construct QUIT (QUestions requiring Inference from Texts) dataset, comprising 7,401 questions and 2.4M passages built from high-convergence human- and machine-authored hints, labeled across three relevance levels using LLM-based answerability and human verification. Through comprehensive evaluation of retrievers, rerankers, and LLM-based readers, we show that methods effective on traditional QA tasks struggle in inferential QA: retrievers underperform, rerankers offer limited gains, and fine-tuning provides inconsistent improvements. Even reasoning-oriented LLMs fail to outperform smaller general-purpose models. These findings reveal that current QA pipelines are not yet ready for inference-based reasoning. Inferential QA thus establishes a new class of QA tasks that move towards understanding and reasoning from indirect textual evidence.

</details>


### [950] [PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian](https://arxiv.org/abs/2602.01246)
*Jamshid Mozafari,Seyed Parsa Mousavinasab,Adam Jatowt*

Main category: cs.CL

TL;DR: 文章介绍首个波斯语推理问答基准PARSE，通过特定方法构建并验证，测试表明特定提示策略和微调可提升模型性能，填补波斯语问答研究空白。


<details>
  <summary>Details</summary>
Motivation: 当前低资源语言高质量推理问答基准稀缺，波斯语缺乏综合开放领域资源评估推理问答系统。

Method: 通过基于大语言模型的可控生成流程构建基准，经人工评估验证，多阶段过滤、标注和一致性检查确保质量，使用多种提示策略对多语言和波斯语大语言模型进行基准测试。

Result: 波斯语提示和结构化提示能提升性能，微调进一步提高结果，尤其对波斯语专业模型。

Conclusion: PARSE填补波斯语问答研究关键空白，为低资源环境下开发和评估推理大语言模型提供基础。

Abstract: Reasoning-focused Question Answering (QA) has advanced rapidly with Large Language Models (LLMs), yet high-quality benchmarks for low-resource languages remain scarce. Persian, spoken by roughly 130 million people, lacks a comprehensive open-domain resource for evaluating reasoning-capable QA systems. We introduce PARSE, the first open-domain Persian reasoning QA benchmark, containing 10,800 questions across Boolean, multiple-choice, and factoid formats, with diverse reasoning types, difficulty levels, and answer structures. The benchmark is built via a controlled LLM-based generation pipeline and validated through human evaluation. We also ensure linguistic and factual quality through multi-stage filtering, annotation, and consistency checks. We benchmark multilingual and Persian LLMs under multiple prompting strategies and show that Persian prompts and structured prompting (CoT for Boolean/multiple-choice; few-shot for factoid) improve performance. Fine-tuning further boosts results, especially for Persian-specialized models. These findings highlight how PARSE supports both fair comparison and practical model adaptation. PARSE fills a critical gap in Persian QA research and provides a strong foundation for developing and evaluating reasoning-capable LLMs in low-resource settings.

</details>


### [951] [LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than Hidden States](https://arxiv.org/abs/2602.01572)
*Yeqin Zhang,Yunfei Wang,Jiaxuan Chen,Ke Qin,Yizheng Zhao,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: 本文提出新视角，表明注意力值向量比隐藏状态更能有效捕捉句子语义，提出VA和AlignedWVA方法，取得无训练LLM嵌入的最优性能，并指出微调值聚合获得强大LLM嵌入模型的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLMs）的句子表示方法多依赖最终层隐藏状态，难以捕捉全局句子级语义，需更有效的方法。

Method: 提出Value Aggregation（VA）方法，对多层和多个词元索引的词元值进行池化；提出Aligned Weighted VA（AlignedWVA）方法，利用合适提示将层注意力输出解释为对齐的加权值向量。

Result: 在无训练设置下，VA优于其他基于LLM的嵌入方法；AlignedWVA在无训练基于LLM的嵌入中达到了最先进性能，大幅超越高成本的MetaEOL。

Conclusion: 注意力值向量能更有效捕捉句子语义，通过微调值聚合有潜力获得强大的LLM嵌入模型。

Abstract: Sentence representations are foundational to many Natural Language Processing (NLP) applications. While recent methods leverage Large Language Models (LLMs) to derive sentence representations, most rely on final-layer hidden states, which are optimized for next-token prediction and thus often fail to capture global, sentence-level semantics. This paper introduces a novel perspective, demonstrating that attention value vectors capture sentence semantics more effectively than hidden states. We propose Value Aggregation (VA), a simple method that pools token values across multiple layers and token indices. In a training-free setting, VA outperforms other LLM-based embeddings, even matches or surpasses the ensemble-based MetaEOL. Furthermore, we demonstrate that when paired with suitable prompts, the layer attention outputs can be interpreted as aligned weighted value vectors. Specifically, the attention scores of the last token function as the weights, while the output projection matrix ($W_O$) aligns these weighted value vectors with the common space of the LLM residual stream. This refined method, termed Aligned Weighted VA (AlignedWVA), achieves state-of-the-art performance among training-free LLM-based embeddings, outperforming the high-cost MetaEOL by a substantial margin. Finally, we highlight the potential of obtaining strong LLM embedding models through fine-tuning Value Aggregation.

</details>


### [952] [Orthogonal Hierarchical Decomposition for Structure-Aware Table Understanding with Large Language Models](https://arxiv.org/abs/2602.01969)
*Bin Cao,Huixian Lu,Chenwen Ma,Ting Wang,Ruizhe Li,Jing Fan*

Main category: cs.CL

TL;DR: 针对大语言模型处理复杂表格的难题，提出OHD框架，实验显示其性能优于现有范式


<details>
  <summary>Details</summary>
Motivation: 现有方法难以显式捕捉复杂表格的层次结构和跨维度依赖，导致结构语义与文本表示不一致

Method: 提出OHD框架，引入基于空间 - 语义共约束的OTI方法将表格分解为列树和行树，设计双路径关联协议重建单元格语义谱系，并结合大语言模型对齐多级语义信息

Result: 在AITQA和HiTab两个复杂表格问答基准上评估，OHD在多个评估指标上始终优于现有表示范式

Conclusion: OHD框架能为大语言模型构建复杂表格的结构保留输入表示，有效提升处理复杂表格的能力

Abstract: Complex tables with multi-level headers, merged cells and heterogeneous layouts pose persistent challenges for LLMs in both understanding and reasoning. Existing approaches typically rely on table linearization or normalized grid modeling. However, these representations struggle to explicitly capture hierarchical structures and cross-dimensional dependencies, which can lead to misalignment between structural semantics and textual representations for non-standard tables. To address this issue, we propose an Orthogonal Hierarchical Decomposition (OHD) framework that constructs structure-preserving input representations of complex tables for LLMs. OHD introduces an Orthogonal Tree Induction (OTI) method based on spatial--semantic co-constraints, which decomposes irregular tables into a column tree and a row tree to capture vertical and horizontal hierarchical dependencies, respectively. Building on this representation, we design a dual-pathway association protocol to symmetrically reconstruct semantic lineage of each cell, and incorporate an LLM as a semantic arbitrator to align multi-level semantic information. We evaluate OHD framework on two complex table question answering benchmarks, AITQA and HiTab. Experimental results show that OHD consistently outperforms existing representation paradigms across multiple evaluation metrics.

</details>


### [953] [Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study](https://arxiv.org/abs/2602.02208)
*Md. Toufique Hasan,Ayman Asad Khan,Mika Saari,Vaishnavi Bankhele,Pekka Abrahamsson*

Main category: cs.CL

TL;DR: 本文提出芬兰语农业决策支持系统AgriHubi，经评估有性能提升，研究为低资源语言领域特定RAG系统设计和评估提供指导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在农业应用受限，尤其在低资源语言场景，需开发适配系统。

Method: 提出AgriHubi系统，整合芬兰农业文档与开放PORO模型，结合显式源接地和用户反馈进行迭代优化。

Result: 系统在答案完整性、语言准确性和可靠性上有明显提升，发现部署大模型时响应质量和延迟的权衡。

Conclusion: 为低资源语言环境下设计和评估领域特定RAG系统提供实证指导。

Abstract: Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.

</details>


### [954] [Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics](https://arxiv.org/abs/2602.02343)
*Ziwen Xu,Chenyan Wu,Hengyu Sun,Haiwen Hong,Mengru Wang,Yunzhi Yao,Longtao Huang,Hui Xue,Shumin Deng,Zhixuan Chu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文提出统一视角研究大语言模型控制方法，进行偏好 - 效用分析，发现二者权衡关系，从激活流形角度解释，还提出新方法SPLIT。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型控制方法孤立研究，难以比较，需统一视角。

Method: 提出统一视角将干预视为控制信号引起的动态权重更新，进行偏好 - 效用分析，用极性配对对比示例衡量，从激活流形视角解释现象。

Result: 不同方法中偏好和效用存在权衡关系，控制增强偏好时会降低效用。

Conclusion: 基于分析提出新的引导方法SPLIT，能在提升偏好时更好保留效用。

Abstract: Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.

</details>


### [955] [CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding](https://arxiv.org/abs/2602.01785)
*Yuling Shi,Chaoxiang Xie,Zhensu Sun,Yeheng Chen,Chenxu Zhang,Longfei Yun,Chengcheng Wan,Hongyu Zhang,David Lo,Xiaodong Gu*

Main category: cs.CL

TL;DR: 研究用多模态大语言模型（MLLMs）以图像形式表示源代码来优化代码理解效率，实验显示MLLMs能有效理解大幅减少标记的代码，不同任务有不同表现，为高效推理指出新路径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在源代码理解中计算效率成瓶颈，而文本范式处理代码导致成本线性增加，多模态大语言模型（MLLMs）的发展带来用图像表示代码优化效率的机会。

Method: 对MLLMs用于代码理解的有效性进行首次系统研究和实验。

Result: MLLMs能有效理解经大幅标记缩减的代码，最高实现8倍压缩；能利用语法高亮提升代码完成性能；代码克隆检测对视觉压缩有较强抗性，部分压缩比下表现优于原始文本。

Conclusion: 指出MLLMs在代码理解中的潜力和局限，说明向图像模态代码表示转变是实现更高效推理的途径。

Abstract: Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.

</details>


### [956] [Context Dependence and Reliability in Autoregressive Language Models](https://arxiv.org/abs/2602.01378)
*Poushali Sengupta,Shashi Raj Pandey,Sabita Maharjan,Frank Eliassen*

Main category: cs.CL

TL;DR: 本文提出RISE方法，解决大语言模型中区分关键上下文元素的挑战，实验表明该方法解释更稳健。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理上下文时存在冗余信息，标准解释方法难以应对，输入微小变化会影响归因分数，影响可解释性并带来风险，需区分关键上下文元素。

Method: 引入RISE方法，量化每个输入相对于其他输入的独特影响，减少冗余影响，提供更清晰稳定的归因。

Result: 实验表明RISE比传统方法提供更稳健的解释。

Conclusion: 强调条件信息对可信大语言模型解释和监控的重要性。

Abstract: Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.

</details>


### [957] [Closing the Loop: Universal Repository Representation with RPG-Encoder](https://arxiv.org/abs/2602.02084)
*Jane Luo,Chengyu Yin,Xin Zhang,Qingtao Li,Steven Liu,Yiming Huang,Jie Wu,Hao Liu,Yangyu Huang,Yu Kang,Fangkai Yang,Ying Xin,Scarlett Li*

Main category: cs.CL

TL;DR: 现有仓库代理因碎片化表示存在推理脱节问题，提出RPG - Encoder框架解决，该框架在评估中取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 当前仓库代理因依赖孤立API文档或缺乏语义深度的依赖图，存在推理脱节问题，需解决仓库理解和生成问题。

Method: 提出RPG - Encoder框架，将仓库规划图（RPG）从静态生成蓝图推广为统一、高保真表示，通过三种机制闭合推理循环。

Result: 在SWE - bench Verified上达到93.7%的Acc@5，在SWE - bench Live Lite上超过最佳基线10%以上，在RepoCraft上实现98.5%的重建覆盖率。

Conclusion: RPG - Encoder框架具有优越的细粒度定位准确性，能高保真反映原始代码库，闭合意图和实现之间的循环。

Abstract: Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.

</details>


### [958] [On the Power of (Approximate) Reward Models for Inference-Time Scaling](https://arxiv.org/abs/2602.01381)
*Youheng Zhu,Yiping Lu*

Main category: cs.CL

TL;DR: 研究推理时扩展范式中近似奖励模型在SMC框架下的有效性，指出Bellman误差是关键，若误差有界，可将推理计算复杂度从指数级降为多项式级。


<details>
  <summary>Details</summary>
Motivation: 实际中使用近似奖励模型，需回答为何及何时其对推理时扩展有效。

Method: 从理论上分析，确定近似奖励模型的Bellman误差为关键量。

Result: 对于长度为T的推理过程，若近似奖励模型的Bellman误差有O(1/T)的界，结合SMC可将推理计算复杂度从指数级降为多项式级。

Conclusion: 使用近似奖励模型结合SMC能在推理效率上实现指数级提升。

Abstract: Inference-time scaling has recently emerged as a powerful paradigm for improving the reasoning capability of large language models. Among various approaches, Sequential Monte Carlo (SMC) has become a particularly important framework, enabling iterative generation, evaluation, rejection, and resampling of intermediate reasoning trajectories. A central component in this process is the reward model, which evaluates partial solutions and guides the allocation of computation during inference.
  However, in practice, true reward models are never available. All deployed systems rely on approximate reward models, raising a fundamental question: Why and when do approximate reward models suffice for effective inference-time scaling? In this work, we provide a theoretical answer. We identify the Bellman error of the approximate reward model as the key quantity governing the effectiveness of SMC-based inference-time scaling. For a reasoning process of length $T$, we show that if the Bellman error of the approximate reward model is bounded by $O(1/T)$, then combining this reward model with SMC reduces the computational complexity of reasoning from exponential in $T$ to polynomial in $T$. This yields an exponential improvement in inference efficiency despite using only approximate rewards.

</details>


### [959] [G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2602.00015)
*Xun Xu*

Main category: cs.CL

TL;DR: 本文提出G - MemLLM解决大语言模型多跳推理问题，实验显示其在多任务上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型受上下文窗口容量和多跳推理中长时事实一致性问题限制，现有方法存在信息稀释问题。

Method: 提出G - MemLLM，将冻结的大语言模型骨干与可训练的潜在内存库集成，采用GRU风格的门控更新逻辑。

Result: 在HotpotQA和ZsRE基准测试中，G - MemLLM显著提升多跳推理和关系精度，如Llama 3.1 - 8B在ZsRE上准确率提升13.3%，在HotpotQA上不同模型有不同指标提升。

Conclusion: G - MemLLM能有效解决大语言模型在多跳推理中的问题，提升推理和关系精度。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.

</details>


### [960] [PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems](https://arxiv.org/abs/2602.00016)
*Jiongchi Yu,Yuhan Ma,Xiaoyu Zhang,Junjie Wang,Qiang Hu,Chao Shen,Xiaofei Xie*

Main category: cs.CL

TL;DR: 为解决现有工作忽视人格特质动态和情境依赖问题，引入PTCBENCH基准评估大语言模型人格一致性，发现特定场景会引发模型人格变化和推理能力改变，建立了评估框架。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在情感代理和AI系统中应用增加，保持一致真实的模型人格对用户信任和参与至关重要，但现有工作忽视人格特质动态和情境依赖，需弥补该差距。

Method: 引入PTCBENCH基准，让模型经历12种不同外部条件，用NEO五因素量表严格评估人格。

Result: 对39,240条人格特质记录研究发现，特定外部场景（如“失业”）会引发大语言模型显著人格变化，甚至改变推理能力。

Conclusion: PTCBENCH建立了在现实、变化环境中评估人格一致性的可扩展框架，为开发稳健且心理契合的AI系统提供可行见解。

Abstract: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., "Unemployment") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.

</details>


### [961] [SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations](https://arxiv.org/abs/2602.00017)
*Benyamin Tabarsi,Wenbo Li,Tahreem Yasir,Aryan Santhosh Kumar,Laura Widman,Dongkuan Xu,Tiffany Barnes*

Main category: cs.CL

TL;DR: 提出SafeTalkCoach多智能体对话生成框架模拟亲子性健康对话并提供数据集，评估显示其能生成多样且真实可控对话，支持相关研究与实践


<details>
  <summary>Details</summary>
Motivation: 实际亲子性健康对话数据稀缺难收集，现有大语言模型对话生成存在偏离最佳实践、缺乏真实多样性问题

Method: 引入SafeTalkCoach框架，集成众包和合成场景、性健康指南、基于证据的角色、自适应控制模块和分层多样化

Result: SafeTalkCoach能生成多样化对话，同时在实践中保持真实性、沟通质量和可控性

Conclusion: SafeTalkCoach框架和数据集可支持AI研究和健康传播实践

Abstract: The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may deviate from best practices and frequently lack realism and diversity. We introduce SafeTalkCoach, a diversity-driven multi-agent dialogue generation framework that simulates parent-child conversations about sexual health, and present an accompanying dataset. SafeTalkCoach integrates crowd-sourced and synthesized scenarios, established sexual health guidelines, evidence-based personas, adaptive control modules, and hierarchical diversification. Through evaluations, we demonstrate that SafeTalkCoach generates diverse conversations while maintaining realism, communication quality, and controllability in practice. Our goal is that the SafeTalkCoach framework and the dataset support both AI research and health communications practices.

</details>


### [962] [Construct, Align, and Reason: Large Ontology Models for Enterprise Knowledge Management](https://arxiv.org/abs/2602.00029)
*Yao Zhang,Hongyin Zhu*

Main category: cs.CL

TL;DR: 本文提出大本体模型（LOM）框架解决企业级知识管理问题，构建数据集进行训练评估，4B参数的LOM准确率达89.47%且优于DeepSeek - V3.2。


<details>
  <summary>Details</summary>
Motivation: 解决企业级知识管理在集成多源异构数据和有效语义推理方面的挑战，以及传统知识图谱在隐式关系发现和复杂问答语义理解上的不足。

Method: 引入统一的构建 - 对齐 - 推理框架LOM，构建双层企业本体并融合数据；提出统一的三阶段训练流程；构建综合训练和评估数据集。

Result: 4B参数的LOM在基准测试中准确率达89.47%，在复杂图推理上优于DeepSeek - V3.2。

Conclusion: LOM实现了本体结构和语言的有效融合。

Abstract: Enterprise-scale knowledge management faces significant challenges in integrating multi-source heterogeneous data and enabling effective semantic reasoning. Traditional knowledge graphs often struggle with implicit relationship discovery and lack sufficient semantic understanding for complex question answering. To address these limitations, we introduce a unified construct--align--reason framework, the large ontology model (LOM). We first build a dual-layer enterprise ontology from structured databases and unstructured text, subsequently fusing these sources into a comprehensive enterprise ontology. To enable instruction-aligned reasoning, we propose a unified three-stage training pipeline: ontology instruction fine-tuning to improve structural understanding; text-ontology grounding to strengthen node semantic encoding; and multi-task instruction tuning on ontology-language pairs with curriculum learning to enhance semantic reasoning and generation. We also construct comprehensive training and evaluation datasets covering diverse ontology reasoning tasks. On this benchmark, our 4B-parameter LOM achieves 89.47% accuracy and outperforms DeepSeek-V3.2 on complex graph reasoning, indicating effective fusion of ontology structure and language.

</details>


### [963] [Reversible Diffusion Decoding for Diffusion Language Models](https://arxiv.org/abs/2602.00150)
*Xinyun Wang,Min Zhang,Sen Cui,Zhikang Chen,Bo Jiang,Kun Kuang,Mingbao Lin*

Main category: cs.CL

TL;DR: 提出可逆扩散解码框架RDD，在扩散生成中引入可逆性，避免停滞问题，提升生成质量且开销小。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型按块解码时不可逆承诺会导致停滞，反向扩散过程无法进展。

Method: 引入可逆扩散解码框架RDD，检测停滞，可有效回溯到早期块，运用置信度引导的重新掩码，有选择地重新初始化不确定标记。

Result: 实验表明RDD以最小计算开销比基线模型提高了生成的鲁棒性和质量。

Conclusion: 可逆的RDD在保持基于扩散生成的并行效率的同时，能从早期错误中恢复。

Abstract: Diffusion language models enable parallel token generation through block-wise decoding, but their irreversible commitments can lead to stagnation, where the reverse diffusion process fails to make further progress under a suboptimal context.We propose Reversible Diffusion Decoding (RDD), a decoding framework that introduces reversibility into block-wise diffusion generation. RDD detects stagnation as a state-dependent failure of the reverse process and enables efficient backtracking to earlier blocks without recomputation via cached model states. To avoid repeated failure trajectories, RDD applies confidence-guided re-masking to selectively reinitialize uncertain tokens while preserving reliable context.This reversible formulation allows decoding to recover from early commitment errors while maintaining the parallel efficiency of diffusion-based generation. Experiments show that RDD improves generation robustness and quality over baselines with minimal computational overhead.

</details>


### [964] [DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking](https://arxiv.org/abs/2602.00238)
*Tianyi Hu,Niket Tandon,Akhil Arora*

Main category: cs.CL

TL;DR: 现有RAG系统假设每个查询只有一个正确答案，忽略多答案场景。此研究提出DIVERGE框架，引入新指标，在数据集上实现了更好的多样性 - 质量权衡。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统假设每个查询有单一正确答案，忽略多答案场景，且未充分利用上下文多样性，限制了创造力和信息获取的公平性。

Method: 提出DIVERGE，一个即插即用的RAG框架，具有反射引导生成和记忆增强迭代细化功能；引入用于评估开放式问题中多样性 - 质量权衡的新指标。

Result: DIVERGE在Infinity - Chat数据集上比竞争基线和先前的最先进方法实现了更好的多样性 - 质量权衡，在保持质量的同时大幅提高了多样性。

Conclusion: 当前基于大语言模型的开放式信息检索系统存在系统性局限，显式建模多样性可以缓解这一问题。

Abstract: Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge

</details>


### [965] [Detecting AI-Generated Content in Academic Peer Reviews](https://arxiv.org/abs/2602.00319)
*Siyuan Shen,Kai Wang*

Main category: cs.CL

TL;DR: 研究检测ICLR和NC会议及期刊同行评审中AI生成内容，发现2022年前极少，2025年显著增加，凸显对其影响研究的必要性。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型兴起后其在学术同行评审中的作用，研究同行评审中AI生成内容的时间出现情况。

Method: 将基于历史评审训练的检测模型应用于ICLR和Nature Communications后续评审周期，检测AI生成内容。

Result: 2022年前检测到的AI生成内容极少，到2025年大幅增加，2025年约20%的ICLR评审和12%的Nature Communications评审被判定为AI生成，NC评审中AI生成内容在2024年第三到第四季度增长最明显。

Conclusion: 有证据表明同行评审中AI辅助内容迅速增加，需进一步研究其对学术评价的影响。

Abstract: The growing availability of large language models (LLMs) has raised questions about their role in academic peer review. This study examines the temporal emergence of AI-generated content in peer reviews by applying a detection model trained on historical reviews to later review cycles at International Conference on Learning Representations (ICLR) and Nature Communications (NC). We observe minimal detection of AI-generated content before 2022, followed by a substantial increase through 2025, with approximately 20% of ICLR reviews and 12% of Nature Communications reviews classified as AI-generated in 2025. The most pronounced growth of AI-generated reviews in NC occurs between the third and fourth quarter of 2024. Together, these findings provide suggestive evidence of a rapidly increasing presence of AI-assisted content in peer review and highlight the need for further study of its implications for scholarly evaluation.

</details>


### [966] [When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.00428)
*Naen Xu,Hengyu An,Shuo Shi,Jinghuai Zhang,Chunyi Zhou,Changjiang Li,Tianyu Du,Zhihui Fu,Jun Wang,Shouling Ji*

Main category: cs.CL

TL;DR: 本文全面研究基于大语言模型的多智能体系统中的曼德拉效应，提出MANBENCH基准，评估不同因素影响并提出缓解策略，有效降低了该效应。


<details>
  <summary>Details</summary>
Motivation: 大语言模型提升多智能体系统能力，但智能体易受集体认知偏差影响，曼德拉效应限制对多智能体系统记忆偏差理解并引发伦理担忧。

Method: 提出MANBENCH基准评估受曼德拉效应影响的四种常见任务类型的智能体行为；在MANBENCH上评估多个大语言模型驱动的智能体；提出缓解策略，包括提示级防御和模型级基于对齐的防御。

Result: 在MANBENCH上对智能体进行评估量化了曼德拉效应并分析因素影响；缓解策略使曼德拉效应相比基线平均降低74.40%。

Conclusion: 研究结果为开发更具韧性和道德一致性的协作多智能体系统提供了有价值的见解。

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.

</details>


### [967] [Words that make SENSE: Sensorimotor Norms in Learned Lexical Token Representations](https://arxiv.org/abs/2602.00469)
*Abhinav Gupta,Toben H. Mintz,Jesse Thomason*

Main category: cs.CL

TL;DR: 提出SENSE模型预测Lancaster感觉运动规范，行为研究发现人类选择率和SENSE评分有显著相关性，子词分析揭示了内感受规范的音义模式。


<details>
  <summary>Details</summary>
Motivation: 词嵌入从共现模式获取意义，而人类语言理解基于感官和运动体验，需要一种能关联两者的方法。

Method: 提出SENSE学习投影模型，进行有281名参与者的行为研究和非词选择率的子词分析。

Result: 在11种模式中的6种里，人类选择率和SENSE评分有显著相关性；非词选择率的子词分析揭示了内感受规范的音义模式。

Conclusion: SENSE模型可用于预测感觉运动规范，且可从文本数据中计算性地提出候选音义主题。

Abstract: While word embeddings derive meaning from co-occurrence patterns, human language understanding is grounded in sensory and motor experience. We present $\text{SENSE}$ $(\textbf{S}\text{ensorimotor }$ $\textbf{E}\text{mbedding }$ $\textbf{N}\text{orm }$ $\textbf{S}\text{coring }$ $\textbf{E}\text{ngine})$, a learned projection model that predicts Lancaster sensorimotor norms from word lexical embeddings. We also conducted a behavioral study where 281 participants selected which among candidate nonce words evoked specific sensorimotor associations, finding statistically significant correlations between human selection rates and $\text{SENSE}$ ratings across 6 of the 11 modalities. Sublexical analysis of these nonce words selection rates revealed systematic phonosthemic patterns for the interoceptive norm, suggesting a path towards computationally proposing candidate phonosthemes from text data.

</details>


### [968] [Culturally-Grounded Governance for Multilingual Language Models: Rights, Data Boundaries, and Accountable AI Design](https://arxiv.org/abs/2602.00497)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.CL

TL;DR: 现有多语言大模型治理框架以英语为中心，本文结合跨文化视角提出文化导向治理框架，指出治理挑战，贡献概念议程并阐述设计与政策影响。


<details>
  <summary>Details</summary>
Motivation: 现有治理框架以英语为中心，给低资源语言和文化边缘社区带来系统风险，需新治理框架。

Method: 借鉴以人为中心计算和AI治理的跨文化视角，综合多语言模型行为、数据不对称和社会技术危害的现有证据。

Result: 识别出三个治理挑战，贡献概念议程，将多语言AI治理重新定义为社会文化和基于权利的问题。

Conclusion: 文化导向治理对确保多语言模型不加剧全球不平等至关重要。

Abstract: Multilingual large language models (MLLMs) are increasingly deployed across cultural, linguistic, and political contexts, yet existing governance frameworks largely assume English-centric data, homogeneous user populations, and abstract notions of fairness. This creates systematic risks for low-resource languages and culturally marginalized communities, where data practices, model behavior, and accountability mechanisms often fail to align with local norms, rights, and expectations. Drawing on cross-cultural perspectives in human-centered computing and AI governance, this paper synthesizes existing evidence on multilingual model behavior, data asymmetries, and sociotechnical harm, and articulates a culturally grounded governance framework for MLLMs. We identify three interrelated governance challenges: cultural and linguistic inequities in training data and evaluation practices, misalignment between global deployment and locally situated norms, values, and power structures, and limited accountability mechanisms for addressing harms experienced by marginalized language communities. Rather than proposing new technical benchmarks, we contribute a conceptual agenda that reframes multilingual AI governance as a sociocultural and rights based problem. We outline design and policy implications for data stewardship, transparency, and participatory accountability, and argue that culturally grounded governance is essential for ensuring that multilingual language models do not reproduce existing global inequalities under the guise of scale and neutrality.

</details>


### [969] [Hermes the Polyglot: A Unified Framework to Enhance Expressiveness for Multimodal Interlingual Subtitling](https://arxiv.org/abs/2602.00597)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: 提出基于大语言模型的自动字幕框架Hermes，解决跨语言字幕翻译挑战，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 跨语言字幕翻译在机器翻译中未被充分探索，字幕文本特性带来语义连贯、代词和术语翻译、翻译表现力等挑战。

Method: 提出Hermes框架，集成说话人分离、术语识别和表现力增强三个模块。

Result: Hermes实现了最先进的分离性能，生成了富有表现力、上下文连贯的翻译。

Conclusion: Hermes推动了跨语言字幕翻译的研究。

Abstract: Interlingual subtitling, which translates subtitles of visual media into a target language, is essential for entertainment localization but has not yet been explored in machine translation. Although Large Language Models (LLMs) have significantly advanced the general capabilities of machine translation, the distinctive characteristics of subtitle texts pose persistent challenges in interlingual subtitling, particularly regarding semantic coherence, pronoun and terminology translation, and translation expressiveness. To address these issues, we present Hermes, an LLM-based automated subtitling framework. Hermes integrates three modules: Speaker Diarization, Terminology Identification, and Expressiveness Enhancement, which effectively tackle the above challenges. Experiments demonstrate that Hermes achieves state-of-the-art diarization performance and generates expressive, contextually coherent translations, thereby advancing research in interlingual subtitling.

</details>


### [970] [Jailbreaking LLMs via Calibration](https://arxiv.org/abs/2602.00619)
*Yuxuan Lu,Yongkang Guo,Yuqing Kong*

Main category: cs.CL

TL;DR: 提出框架建模大语言模型安全对齐影响，将越狱问题转为预测聚合问题，推导规则并提出混合规则，评估显示性能更优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型安全对齐会在对齐输出和预对齐数据分布间产生系统差异。

Method: 提出框架将安全对齐影响建模为预对齐分布的系统失真，将越狱问题转为预测聚合问题，推导最优聚合策略和更广泛聚合规则，提出新混合聚合规则。

Result: 跨红队基准和数学实用任务评估表明，该方法攻击成功率更高，“越狱税”更低，在gpt - oss - 120b上表现尤佳。

Conclusion: 所提方法在大语言模型越狱方面优于现有方法。

Abstract: Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of a pre-alignment distribution. We cast Weak-to-Strong Jailbreaking as a forecast aggregation problem and derive an optimal aggregation strategy characterized by a Gradient Shift in the loss-induced dual space. We show that logit-arithmetic jailbreaking methods are a special case of this framework under cross-entropy loss, and derive a broader family of aggregation rules corresponding to other proper losses. We also propose a new hybrid aggregation rule. Evaluations across red-teaming benchmarks and math utility tasks using frontier models demonstrate that our approach achieves superior Attack Success Rates and lower "Jailbreak Tax" compared with existing methods, especially on the safety-hardened gpt-oss-120b.

</details>


### [971] [Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation](https://arxiv.org/abs/2602.00665)
*Lakshan Cooray,Deshan Sumanathilaka,Pattigadapa Venkatesh Raju*

Main category: cs.CL

TL;DR: 研究指令调优小型语言模型（SLMs）用于上下文总结的多轮客服问答，评估 9 种 SLMs 并与 3 种商业大语言模型（LLMs）对比，发现 SLMs 效果有差异。


<details>
  <summary>Details</summary>
Motivation: LLMs 计算成本高、部署受限，SLMs 用于多轮客服问答的有效性待探索。

Method: 采用历史总结策略保存对话关键信息，引入基于对话阶段的定性分析，用词汇和语义相似度指标、人类评估和 LLM 评判方法评估 9 种指令调优的低参数 SLMs 和 3 种商业 LLMs。

Result: SLMs 效果差异显著，部分接近 LLMs，部分难保持对话连续性和上下文一致性。

Conclusion: 指出低参数语言模型用于现实客服问答系统的潜力和当前局限。

Abstract: Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Language Models (SLMs) provide a more efficient alternative, yet their effectiveness for multi-turn customer-service QA remains underexplored, particularly in scenarios requiring dialogue continuity and contextual understanding. This study investigates instruction-tuned SLMs for context-summarized multi-turn customer-service QA, using a history summarization strategy to preserve essential conversational state. We also introduce a conversation stage-based qualitative analysis to evaluate model behavior across different phases of customer-service interactions. Nine instruction-tuned low-parameterized SLMs are evaluated against three commercial LLMs using lexical and semantic similarity metrics alongside qualitative assessments, including human evaluation and LLM-as-a-judge methods. Results show notable variation across SLMs, with some models demonstrating near-LLM performance, while others struggle to maintain dialogue continuity and contextual alignment. These findings highlight both the potential and current limitations of low-parameterized language models for real-world customer-service QA systems.

</details>


### [972] [EchoReview: Learning Peer Review from the Echoes of Scientific Citations](https://arxiv.org/abs/2602.00733)
*Yinuo Zhang,Dingcheng Huang,Haifeng Suo,Yizhuo Li,Ziya Zhao,Junhao Xu,Zhiying Tu,Dianhui Chu,Deming Zhai,Xianming Liu,Xiaoyan Yu,Dianbo Sui*

Main category: cs.CL

TL;DR: 随着科学投稿量增长，传统同行评审系统面临压力，提出EchoReview框架构建数据集并训练自动评审器，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 传统同行评审系统面临可扩展性压力，现有监督微调方法受数据来源和人为评审主观性限制，需要可扩展且可靠的自动评审方法。

Method: 提出EchoReview，从学术引用中挖掘集体评价信号，转化为结构化评审数据，构建EchoReview - 16K数据集，训练EchoReviewer - 7B自动评审器。

Result: EchoReviewer - 7B在证据支持和评审全面性等核心评审维度上有显著且稳定的提升。

Conclusion: 引用上下文是可靠自动同行评审的有效数据范式。

Abstract: As the volume of scientific submissions continues to grow rapidly, traditional peer review systems are facing unprecedented scalability pressures, highlighting the urgent need for automated reviewing methods that are both scalable and reliable. Existing supervised fine-tuning approaches based on real review data are fundamentally constrained by single-source of data as well as the inherent subjectivity and inconsistency of human reviews, limiting their ability to support high-quality automated reviewers. To address these issues, we propose EchoReview, a citation-context-driven data synthesis framework that systematically mines implicit collective evaluative signals from academic citations and transforms scientific community's long-term judgments into structured review-style data. Based on this pipeline, we construct EchoReview-16K, the first large-scale, cross-conference, and cross-year citation-driven review dataset, and train an automated reviewer, EchoReviewer-7B. Experimental results demonstrate that EchoReviewer-7B can achieve significant and stable improvements on core review dimensions such as evidence support and review comprehensiveness, validating citation context as a robust and effective data paradigm for reliable automated peer review.

</details>


### [973] [ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement](https://arxiv.org/abs/2602.00740)
*Ziyan Xiao,Yinghao Zhu,Liang Peng,Lequan Yu*

Main category: cs.CL

TL;DR: 提出ExperienceWeaver框架解决临床文本改进在小样本环境下的问题，评估显示其性能超现有模型。


<details>
  <summary>Details</summary>
Motivation: 临床文本改进因数据有限和医学文档约束而困难，现有基于大语言模型的方法在小样本环境下有局限。

Method: 提出ExperienceWeaver分级框架，将重点从数据检索转向经验学习，提炼反馈为结构化知识并注入代理管道。

Result: 在四个临床数据集上评估表明，ExperienceWeaver在小样本环境下表现一致提升，超越如Gemini - 3 Pro等先进模型。

Conclusion: ExperienceWeaver在小样本环境下的临床文本改进中有效且性能优越。

Abstract: Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns "how to revise" rather than just "what to revise". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.

</details>


### [974] [Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training](https://arxiv.org/abs/2602.00747)
*Shengrui Li,Fei Zhao,Kaiyan Zhao,Jieying Ye,Haifeng Liu,Fangcheng Shi,Zheyong Xie,Yao Hu,Shaosheng Cao*

Main category: cs.CL

TL;DR: 针对大语言模型预训练数据混合比例难题，提出DeMix框架解决，获不错效果并发布相关语料库


<details>
  <summary>Details</summary>
Motivation: 现有确定大语言模型预训练数据混合比例方法或依赖不可靠小尺度实验，或需昂贵大尺度探索，寻找最优混合比例仍是挑战

Method: 提出DeMix框架，利用模型合并预测最优数据比例，独立搜索和训练成本

Result: 大量实验表明DeMix打破充分性、准确性和效率的权衡，以较低搜索成本获得在基准测试中表现更好的最优混合比例

Conclusion: DeMix在确定大语言模型预训练数据混合比例上有效，还发布DeMix Corpora语料库推动开放研究，代码和语料库可在线获取

Abstract: Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.

</details>


### [975] [Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning](https://arxiv.org/abs/2602.00759)
*Zhipeng Chen,Xiaobo Qin,Wayne Xin Zhao,Youbin Wu,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文提出A²D方法增强强化学习与可验证奖励（RLVR）的有效性，通过训练分解器和引导推理器提升性能，并进行多方面分析。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR过程信息有限，模型探索盲目，在难题上易失败，需在不依赖教师模型的情况下为RLVR提供额外信息。

Method: 提出A²D自适应能力分解方法，先无蒸馏地通过RLVR训练分解器将复杂问题分解为子问题，再用分解器标注训练数据集问题，最后在子问题引导下训练推理器。

Result: 与竞争基线对比显示A²D有效；该方法是即插即用模块，可应用于不同RLVR算法；对分解器分析揭示了RLVR过程对其影响及适合增强推理器能力的引导类型。

Conclusion: A²D方法能有效增强RLVR的有效性，具有良好的适用性和可分析性。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.

</details>


### [976] [Eliciting Trustworthiness Priors of Large Language Models via Economic Games](https://arxiv.org/abs/2602.00769)
*Siyu Yan,Lusha Zhu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: 提出基于迭代上下文学习的新方法来引出大语言模型的可信度先验，发现GPT - 4.1的可信度先验与人类相近，并进一步研究其在信任游戏中的表现，可用基于刻板印象的模型预测可信度变化。


<details>
  <summary>Details</summary>
Motivation: 构建以人为中心、可信赖的人工智能系统需维持校准信任，但关键挑战是如何刻画AI系统本身的信任水平。

Method: 提出基于迭代上下文学习的新引出方法，运用行为博弈论中的信任游戏引出可信度先验。

Result: 从多个大语言模型中引出可信度先验，发现GPT - 4.1的可信度先验与人类相近，还研究了其对不同玩家角色的响应。

Conclusion: 引出的可信度变化可以用基于感知温暖和能力的刻板印象模型很好地预测。

Abstract: One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.

</details>


### [977] [Factuality on Demand: Controlling the Factuality-Informativeness Trade-off in Text Generation](https://arxiv.org/abs/2602.00848)
*Ziwei Gong,Yanda Chen,Julia Hirschberg,Chen Zhao,He He,Zhou Yu,Kathleen Mckeown*

Main category: cs.CL

TL;DR: 提出Factuality - Controlled Generation (FCG)框架，可让用户指定事实性约束，用合成数据训练模型提升其输出表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在回答查询时，需在信息性和事实性间平衡，不同应用对两者平衡需求不同。

Method: 引入FCG框架，从遵守事实性约束和响应信息性两维度评估其性能，用合成数据训练模型。

Result: 合成训练显著提升了模型遵守事实性要求和保持输出信息性的能力。

Conclusion: FCG框架结合合成数据训练能有效提升大语言模型在信息性和事实性间的平衡表现。

Abstract: Large language models (LLMs) encode knowledge with varying degrees of confidence. When responding to queries, models face an inherent trade-off: they can generate responses that are less informative but highly factual, or more informative but potentially less accurate. Different applications demand different balances between informativeness and factuality. We introduce Factuality-Controlled Generation (FCG), a framework that enables users to specify factuality constraints alongside their queries. We propose to evaluate FCG performance on two dimensions: adherence to factuality constraints and response informativeness. We propose to train models on the FCG task using synthetic data, and show that our synthetic training significantly improves models' ability to both respect factuality requirements and maintain informativeness in their outputs.

</details>


### [978] [EffGen: Enabling Small Language Models as Capable Autonomous Agents](https://arxiv.org/abs/2602.00887)
*Gaurav Srivastava,Aafiya Hussain,Chi Wang,Yingyan Celine Lin,Xuan Wang*

Main category: cs.CL

TL;DR: 介绍开源框架effGen，优化小语言模型，有四大贡献和统一协议，在13个基准测试中表现优于竞品，且优化方法互补


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的语言模型智能体系统有高成本和隐私问题，需优化小语言模型

Method: 提出effGen框架，包含增强工具调用、智能任务分解、基于复杂度的路由和统一内存系统，统一多智能体协议

Result: 在13个基准测试中，effGen成功率更高、执行更快、内存占用更低，且优化方法有互补缩放行为

Conclusion: effGen框架是开源的，可用于研究和商业，代码公开可用

Abstract: Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.

</details>


### [979] [Do Schwartz Higher-Order Values Help Sentence-Level Human Value Detection? When Hard Gating Hurts](https://arxiv.org/abs/2602.00913)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: 在严格计算预算下研究Schwartz高阶类别在句子级人类价值检测中的作用，发现硬分层门控不利，校准和轻量级集成可带来稳健改进。


<details>
  <summary>Details</summary>
Motivation: 研究Schwartz高阶类别在句子级人类价值检测中是否能提供可用结构。

Method: 在ValueEval'24 / ValuesML数据集上，对比直接监督变压器、强制执行层级的HO→values管道和Presence→HO→values级联方法，还使用了低成本附加项、标签阈值调整、小指令微调大模型基线、QLoRA和简单集成。

Result: 高阶类别可从单句学习，但硬分层门控常降低宏F1，标签阈值调整和小变压器集成可提升宏F1，小大模型单独表现不如监督编码器，但在跨家族集成中有互补作用。

Conclusion: 高阶结构有描述性作用，但硬门控损害句子级价值检测，校准和轻量级集成可实现稳健改进。

Abstract: Sentence-level human value detection is typically framed as multi-label classification over Schwartz values, but it remains unclear whether Schwartz higher-order (HO) categories provide usable structure. We study this under a strict compute-frugal budget (single 8 GB GPU) on ValueEval'24 / ValuesML (74K English sentences). We compare (i) direct supervised transformers, (ii) HO$\rightarrow$values pipelines that enforce the hierarchy with hard masks, and (iii) Presence$\rightarrow$HO$\rightarrow$values cascades, alongside low-cost add-ons (lexica, short context, topics), label-wise threshold tuning, small instruction-tuned LLM baselines ($\le$10B), QLoRA, and simple ensembles. HO categories are learnable from single sentences (e.g., the easiest bipolar pair reaches Macro-$F_1\approx0.58$), but hard hierarchical gating is not a reliable win: it often reduces end-task Macro-$F_1$ via error compounding and recall suppression. In contrast, label-wise threshold tuning is a high-leverage knob (up to $+0.05$ Macro-$F_1$), and small transformer ensembles provide the most consistent additional gains (up to $+0.02$ Macro-$F_1$). Small LLMs lag behind supervised encoders as stand-alone systems, yet can contribute complementary errors in cross-family ensembles. Overall, HO structure is useful descriptively, but enforcing it with hard gates hurts sentence-level value detection; robust improvements come from calibration and lightweight ensembling.

</details>


### [980] [A Baseline Multimodal Approach to Emotion Recognition in Conversations](https://arxiv.org/abs/2602.00914)
*Víctor Yeste,Rodrigo Rivas-Arévalo*

Main category: cs.CL

TL;DR: 提出一个基于SemEval - 2024 Task 3数据集的对话情感识别轻量级多模态基线，方便后续比较。


<details>
  <summary>Details</summary>
Motivation: 不提出新的最先进方法，而是记录一个可访问的参考实现，用于对话情感识别。

Method: 结合基于Transformer的文本分类器和自监督语音表示模型，采用简单的后期融合集成方法。

Result: 报告了在有限训练协议下的基线设置和实证结果，强调了多模态融合何时优于单模态模型。

Conclusion: 提供该预印本以保证透明性并支持未来更严格的比较。

Abstract: We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.

</details>


### [981] [Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs](https://arxiv.org/abs/2602.00945)
*Anusa Saha,Tanmay Joshi,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 论文提出Neural FOXP2方法，可通过操纵语言神经元使模型以指定语言（印地语或西班牙语）为主。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练中英语占主导，其他语言被抑制，需隔离并操纵控制语言默认性的语言神经元。

Method: Neural FOXP2分三步：定位语言神经元集；通过谱低秩分析确定语言转换方向；对语言神经元进行有符号、稀疏的激活偏移。

Result: 可实现对指定目标语言默认性的控制。

Conclusion: 语言默认性由稀疏、低秩控制电路（语言神经元）决定，能被安全操纵。

Abstract: LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.
  We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neurons. Neural FOXP2 proceeds in three stages: (i) Localize: We train per-layer SAEs so each activation decomposes into a small set of active feature components. For every feature, we quantify English vs. Hindi/Spanish selectivity overall logit-mass lift toward the target-language token set. Tracing the top-ranked features back to their strongest contributing units yields a compact language-neuron set. (ii) Steering directions: We localize controllable language-shift geometry via a spectral low-rank analysis. For each layer, we build English to target activation-difference matrices and perform layerwise SVD to extract the dominant singular directions governing language change. The eigengap and effective-rank spectra identify a compact steering subspace and an empirically chosen intervention window (where these directions are strongest and most stable). (iii) Steer: We apply a signed, sparse activation shift targeted to the language neurons. Concretely, within low to mid layers we add a positive steering along the target-language dominant directions and a compensating negative shift toward the null space for the English neurons, yielding controllable target-language defaultness.

</details>


### [982] [MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA](https://arxiv.org/abs/2602.00981)
*Yutong Song,Shiva Shrestha,Chenhan Lyu,Elahe Khatibi,Pengfei Zhang,Honghui Xu,Nikil Dutt,Amir Rahmani*

Main category: cs.CL

TL;DR: 提出MedSpeak框架提升医学口语问答系统中术语识别和问答性能，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有依赖自动语音识别的口语问答系统难以准确识别医学术语。

Method: 提出MedSpeak，结合医学知识图谱中的语义和语音信息，以及大语言模型推理能力，对语音转录结果纠错。

Result: 在基准测试中显著提高医学术语识别准确率和医学口语问答整体性能。

Conclusion: MedSpeak是医学口语问答的先进解决方案。

Abstract: Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.

</details>


### [983] [DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning](https://arxiv.org/abs/2602.00983)
*Batuhan K. Karaman,Aditya Rawal,Suhaila Shakiah,Mohammad Ghavamzadeh,Mingyi Hong,Arijit Biswas,Ruida Zhou*

Main category: cs.CL

TL;DR: 提出DISPO算法解决现有强化学习奖励验证方法的局限，在多个基准测试中取得更好结果。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习奖励验证方法存在局限，PPO风格方法学习慢，REINFORCE风格方法性能不稳定。

Method: 引入DISPO算法，解耦正确和错误响应重要性采样权重的上下裁剪，产生四个可控策略更新机制。

Result: DISPO在AIME'24上达到61.04%，优于CISPO和DAPO，在多个基准和模型上有类似提升。

Conclusion: DISPO能保持探索 - 蒸馏平衡，防止灾难性失败，表现更优。

Abstract: Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.

</details>


### [984] [DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework](https://arxiv.org/abs/2602.00996)
*Abhijit Chakraborty,Ashish Raj Shekhar,Shiven Agarwal,Vivek Gupta*

Main category: cs.CL

TL;DR: 介绍用于多模态问答的去中心化多智能体框架DeALOG，评估显示其有竞争力，分析确认关键要素重要性，且具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 复杂的跨文本、表格和图像问答需整合多样信息源，需要一个支持专业处理、协调和可解释性的框架。

Method: 引入DeALOG框架，使用专业智能体（Table、Context、Visual、Summarizing和Verification），通过共享自然语言日志作为持久内存进行通信。

Result: 在多个数据集上的评估显示出有竞争力的性能。

Conclusion: DeALOG通过使用自然语言通信的模块化组件提供了一种可扩展的方法，共享日志、智能体专业化和验证对准确性很重要。

Abstract: Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.

</details>


### [985] [Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents](https://arxiv.org/abs/2602.01063)
*Bin Han,Deuksin Kwon,Jonathan Gratch*

Main category: cs.CL

TL;DR: 研究相同个性提示在四种对话场景下使大语言模型产生不同输出，发现其呈上下文敏感的个性表达。


<details>
  <summary>Details</summary>
Motivation: 探究相同个性提示在不同对话场景下对大语言模型行为实现的影响，及这种输出结果差异对大语言模型对话代理带来的思考。

Method: 考察相同个性提示在破冰、谈判、群体决策和共情四种对话场景下的结果。

Result: 上下文线索会系统影响个性表达和情感基调，相同特质在不同社会和情感需求下表达不同。

Conclusion: 大语言模型呈现上下文敏感而非固定的个性表达，能灵活适应社交互动目标和情感条件。

Abstract: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.

</details>


### [986] [From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization](https://arxiv.org/abs/2602.01068)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: 研究聚焦构建满足领域定制需求的翻译大语言模型，以视觉媒体字幕翻译为例，构建数据集并提出ALPO方法，实验显示该方法在翻译质量多维评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在垂直领域翻译的局限性逐渐显现，需构建满足领域定制需求的翻译大语言模型。

Method: 以视觉媒体字幕翻译为主题，调查字幕及其他领域直译和意译情况，构建并发布多向字幕平行语料数据集，提出ALPO方法解决细粒度偏好对齐。

Result: ALPO在翻译质量的多维评估中取得了出色表现。

Conclusion: 提出的ALPO方法能有效训练出满足领域定制需求、具有表现力的翻译大语言模型。

Abstract: The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.

</details>


### [987] [Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority](https://arxiv.org/abs/2602.01227)
*Zhanming Shen,Zeyu Qin,Jiaqi Hu,Wentao Ye,Hao Chen,Xiaomeng Hu,Haokai Xu,Gang Chen,Yi R. Fung,Haobo Wang*

Main category: cs.CL

TL;DR: 论文指出拟合经验数据到实现人类效用过渡受粒度不匹配限制，提出Token Priority为桥梁，分析突破并分类，回顾进展与局限，指出挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决从拟合经验数据到实现人类效用过渡中粒度不匹配的问题。

Method: 将有监督微调（SFT）形式化为精确的分布重塑过程，通过统一视角分析近期突破并分类。

Result: 将近期突破分为Positive Priority和Signed Priority两种模式。

Conclusion: 回顾了现有进展和局限，确定了关键挑战并给出未来研究方向。

Abstract: The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, formalizing Supervised Fine-Tuning (SFT) not as simple optimization but as a precise distribution reshaping process that aligns raw data with the ideal alignment manifold. We analyze recent breakthroughs through this unified lens, categorizing them into two distinct regimes: Positive Priority for noise filtration and Signed Priority for toxic modes unlearning. We revisit existing progress and limitations, identify key challenges, and suggest directions for future research.

</details>


### [988] [PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length](https://arxiv.org/abs/2602.01274)
*Situo Zhang,Yifan Zhang,Zichen Zhu,Hankun Wang,Da Ma,Danyang Zhang,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: 提出Pacer方法动态控制推测解码的草稿长度，在多个基准测试中提升了解码速度。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码使用固定草稿长度限制了解码速度进一步提升的潜力，不同解码步骤的最优草稿长度差异大。

Method: 提出Pacer方法，用轻量级可训练预验证层动态控制草稿长度，该层对草稿令牌分块预验证，失败则停止草稿模型的令牌生成。

Result: Pacer在自回归解码上实现了高达2.66倍的加速，始终优于标准推测解码；与Ouroboros集成时，实现了高达3.09倍的加速。

Conclusion: Pacer有效提升了解码速度，是优化推测解码的有效方法。

Abstract: Speculative decoding (SD) is a powerful technique for accelerating the inference process of large language models (LLMs) without sacrificing accuracy. Typically, SD employs a small draft model to generate a fixed number of draft tokens, which are then verified in parallel by the target model. However, our experiments reveal that the optimal draft length varies significantly across different decoding steps. This variation suggests that using a fixed draft length limits the potential for further improvements in decoding speed. To address this challenge, we propose Pacer, a novel approach that dynamically controls draft length using a lightweight, trainable pre-verification layer. This layer pre-verifies draft tokens blockwise before they are sent to the target model, allowing the draft model to stop token generation if the blockwise pre-verification fails. We implement Pacer on multiple SD model pairs and evaluate its performance across various benchmarks. Our results demonstrate that Pacer achieves up to 2.66x Speedup over autoregressive decoding and consistently outperforms standard speculative decoding. Furthermore, when integrated with Ouroboros, Pacer attains up to 3.09x Speedup.

</details>


### [989] [EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language ModelsEverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models](https://arxiv.org/abs/2602.01313)
*Chuanrui Hu,Tong Li,Xingze Gao,Hongda Chen,Dannong Xu,Yi Bai,Tianwei Lin,Xinda Zhao,Xiaohong Li,Jiaqi An,Yunyun Han,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: 现有基准无法满足LLM对话记忆评估需求，本文提出EverMemBench基准，评估发现当前记忆系统存在局限，为下一代记忆架构提供测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有基准聚焦二元单主题对话，无法捕捉现实复杂性，需要新基准评估LLM长期对话记忆。

Method: 引入EverMemBench基准，涵盖多方多组对话，通过1000+问答对从细粒度回忆、记忆意识和用户画像理解三个维度评估记忆系统。

Result: 评估发现多方场景下多跳推理能力差、时间推理未解决、记忆意识受检索瓶颈限制等问题。

Conclusion: EverMemBench为开发下一代记忆架构提供了具有挑战性的测试平台。

Abstract: Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens with temporally evolving information, cross-topic interleaving, and role-specific personas. EverMemBench evaluates memory systems across three dimensions through 1,000+ QA pairs: fine-grained recall, memory awareness, and user profile understanding. Our evaluation reveals critical limitations: (1) multi-hop reasoning collapses in multi-party settings, with even oracle models achieving only 26%; (2) temporal reasoning remains unsolved, requiring version semantics beyond timestamp matching; (3) memory awareness is bottlenecked by retrieval, where current similarity-based methods fail to bridge the semantic gap between queries and implicitly relevant memories. EverMemBench provides a challenging testbed for developing next-generation memory architectures.

</details>


### [990] [From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis](https://arxiv.org/abs/2602.01401)
*Niansong Zhang,Sunwoo Kim,Shreesha Srinath,Zhiru Zhang*

Main category: cs.CL

TL;DR: 探讨高级综合（HLS）在智能体时代的作用，认为其仍至关重要并做出三项贡献。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型引发AI硬件设计热潮背景下，探讨HLS在智能体时代是否仍有价值。

Method: 先阐述HLS作为智能体硬件设计抽象层和参考的原因，再指出当前HLS工具的不足，最后提出智能体HLS共生进化的分类法。

Result: 明确HLS对智能体优化的重要性及相关特性，指出当前HLS工具可由智能体解决的不足，提出共生进化分类。

Conclusion: HLS在智能体时代仍至关重要，且随着系统发展，智能体将在设计中承担更多责任。

Abstract: The rise of large language models has sparked interest in AI-driven hardware design, raising the question: does high-level synthesis (HLS) still matter in the agentic era? We argue that HLS remains essential. While we expect mature agentic hardware systems to leverage both HLS and RTL, this paper focuses on HLS and its role in enabling agentic optimization. HLS offers faster iteration cycles, portability, and design permutability that make it a natural layer for agentic optimization.This position paper makes three contributions. First, we explain why HLS serves as a practical abstraction layer and a golden reference for agentic hardware design. Second, we identify key limitations of current HLS tools, namely inadequate performance feedback, rigid interfaces, and limited debuggability that agents are uniquely positioned to address. Third, we propose a taxonomy for the symbiotic evolution of agentic HLS, clarifying how responsibility shifts from human designers to AI agents as systems advance from copilots to autonomous design partners.

</details>


### [991] [SentiFuse: Deep Multi-model Fusion Framework for Robust Sentiment Extraction](https://arxiv.org/abs/2602.01447)
*Hieu Minh Duong,Rupa Ghosh,Cong Hoan Nguyen,Eugene Levin,Todd Gary,Long Nguyen*

Main category: cs.CL

TL;DR: 现有情感分析模型整合缺统一框架，提出SentiFuse框架，经实验验证效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏有效整合不同情感分析模型的统一框架。

Method: 提出SentiFuse框架，通过标准化层和多种融合策略（包括决策级、特征级、自适应融合）整合异构情感模型。

Result: 在三个大规模社交媒体数据集上，SentiFuse始终优于单个模型和简单集成，特征级融合效果最佳，自适应融合在复杂情况增强鲁棒性。

Conclusion: 系统地利用模型互补性可在不同数据集和文本类型上实现更准确可靠的情感分析。

Abstract: Sentiment analysis models exhibit complementary strengths, yet existing approaches lack a unified framework for effective integration. We present SentiFuse, a flexible and model-agnostic framework that integrates heterogeneous sentiment models through a standardization layer and multiple fusion strategies. Our approach supports decision-level fusion, feature-level fusion, and adaptive fusion, enabling systematic combination of diverse models. We conduct experiments on three large-scale social-media datasets: Crowdflower, GoEmotions, and Sentiment140. These experiments show that SentiFuse consistently outperforms individual models and naive ensembles. Feature-level fusion achieves the strongest overall effectiveness, yielding up to 4\% absolute improvement in F1 score over the best individual model and simple averaging, while adaptive fusion enhances robustness on challenging cases such as negation, mixed emotions, and complex sentiment expressions. These results demonstrate that systematically leveraging model complementarity yields more accurate and reliable sentiment analysis across diverse datasets and text types.

</details>


### [992] [Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment](https://arxiv.org/abs/2602.01587)
*Zehua Cheng,Jianwei Yang,Wei Dai,Jiahao Sun*

Main category: cs.CL

TL;DR: 提出基于集合统计稳定性的可认证鲁棒性框架，减少攻击成功率并保持高良性效用，提供安全证明。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受自适应越狱攻击，现有防御手段不足，需要一种可认证的鲁棒性方法。

Method: 提出基于分层随机消融的认证语义平滑（CSS）和噪声增强对齐调优（NAAT），以提高模型鲁棒性。

Result: 在Llama - 3上的实验表明，将基于梯度攻击的成功率从84.2%降至1.2%，同时保持94.1%的良性效用，远超字符级基线。

Conclusion: 该框架能为模型提供确定的安全证明，确保在可证明半径内对所有对抗变体保持鲁棒性。

Abstract: Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.

</details>


### [993] [Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering](https://arxiv.org/abs/2602.00279)
*Philip Müller,Nicholas Popovič,Michael Färber,Peter Steinbach*

Main category: cs.CL

TL;DR: 论文引入大规模基准评估科学问答中不确定性量化（UQ）指标校准，评估20个大模型，展现UQ方法局限。


<details>
  <summary>Details</summary>
Motivation: 可靠的UQ对大模型在问答中的可信应用至关重要，但现有UQ在科学问答领域验证不足。

Method: 构建大规模基准和开源框架，涵盖20个大模型，用7个科学问答数据集开展实验。

Result: 指令调整使token级置信度可靠性降低，推理过程可缓解；序列级答案频率校准最可靠；单一使用ECE会产生误导。

Conclusion: 当前UQ方法和基准测试标准存在关键局限性。

Abstract: Large Language Models (LLMs) are commonly used in Question Answering (QA) settings, increasingly in the natural sciences if not science at large. Reliable Uncertainty Quantification (UQ) is critical for the trustworthy uptake of generated answers. Existing UQ approaches remain weakly validated in scientific QA, a domain relying on fact-retrieval and reasoning capabilities. We introduce the first large-scale benchmark for evaluating UQ metrics in reasoning-demanding QA studying calibration of UQ methods, providing an extensible open-source framework to reproducibly assess calibration. Our study spans up to 20 large language models of base, instruction-tuned and reasoning variants. Our analysis covers seven scientific QA datasets, including both multiple-choice and arithmetic question answering tasks, using prompting to emulate an open question answering setting. We evaluate and compare methods representative of prominent approaches on a total of 685,000 long-form responses, spanning different reasoning complexities representative of domain-specific tasks. At the token level, we find that instruction tuning induces strong probability mass polarization, reducing the reliability of token-level confidences as estimates of uncertainty. Models further fine-tuned for reasoning are exposed to the same effect, but the reasoning process appears to mitigate it depending on the provider. At the sequence level, we show that verbalized approaches are systematically biased and poorly correlated with correctness, while answer frequency (consistency across samples) yields the most reliable calibration. In the wake of our analysis, we study and report the misleading effect of relying exclusively on ECE as a sole measure for judging performance of UQ methods on benchmark datasets. Our findings expose critical limitations of current UQ methods for LLMs and standard practices in benchmarking thereof.

</details>


### [994] [CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation](https://arxiv.org/abs/2602.01660)
*Zhongyuan Peng,Caijun Xu,Changyi Xiao,Shibo Hong,Eli Zhang,Stephen Huang,Yixin Cao*

Main category: cs.CL

TL;DR: 提出CoDiQ框架实现细粒度难度控制生成竞赛级问题，构建CoDiQ语料库，训练提升大推理模型推理性能并开源。


<details>
  <summary>Details</summary>
Motivation: 现有自动问题合成方法在难度控制、计算成本和大规模生成竞赛级问题方面存在不足。

Method: 识别测试时缩放趋势和模型生成有效高难度问题的内在属性，基于Qwen3 - 8B开发CoDiQ - Generator，构建CoDiQ - Corpus。

Result: 构建44K竞赛级问题序列的CoDiQ - Corpus，问题比LiveCodeBench/AIME更具挑战性且可解性超82%，训练提升大推理模型推理性能。

Conclusion: 缩放可控难度训练问题可增强大推理模型的推理能力，开源成果支持相关研究。

Abstract: Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.

</details>


### [995] [Counting Hypothesis: Potential Mechanism of In-Context Learning](https://arxiv.org/abs/2602.01687)
*Jung H. Lee,Sujith Vijayan*

Main category: cs.CL

TL;DR: 本文围绕上下文学习（ICL）展开，指出其虽有优势但机制不明，提出ICL的计数假设并提供证据。


<details>
  <summary>Details</summary>
Motivation: ICL机制不明，纠错和诊断困难，需了解其局限性和大语言模型支持ICL的方式。

Method: 受ICL特性和大语言模型功能模块启发，提出ICL的“计数假设”。

Result: 提出“计数假设”并提供支持证据。

Conclusion: 提出的“计数假设”有助于理解大语言模型支持ICL的编码策略。

Abstract: In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a wide range of tasks/functions with a few examples demonstrating a desirable task. ICL opens up new ways to utilize LLMs in more domains, but its underlying mechanisms still remain poorly understood, making error correction and diagnosis extremely challenging. Thus, it is imperative that we better understand the limitations of ICL and how exactly LLMs support ICL. Inspired by ICL properties and LLMs' functional modules, we propose 1the counting hypothesis' of ICL, which suggests that LLMs' encoding strategy may underlie ICL, and provide supporting evidence.

</details>


### [996] [BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition](https://arxiv.org/abs/2602.01717)
*Hyunsik Kim,Haeri Kim,Munhak Lee,Kyungmin Lee*

Main category: cs.CL

TL;DR: 提出基于UTF - 16的BBPE16分词器用于多语言自动语音识别，在准确性相当或更好的情况下，降低计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于UTF - 8的Byte - level BPE（BBPE）分词器对非拉丁脚本存在变长编码问题，增加计算和内存负担。

Method: 提出基于UTF - 16的BBPE16分词器，以统一的2字节代码单元表示大多数现代脚本。

Result: 在单语、双语、三语ASR及多语言持续学习设置中，BBPE16准确性相当或更好；对中文，减少最多10.4%的令牌数量和最多10.3%的解码迭代次数。

Conclusion: BBPE16是多语言ASR实用的分词选择，能加速微调与推理并降低内存使用。

Abstract: Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.

</details>


### [997] [SafePred: A Predictive Guardrail for Computer-Using Agents via World Models](https://arxiv.org/abs/2602.01725)
*Yurun Chen,Zeyi Liao,Ping Yin,Taotao Xie,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: 提出预测性护栏方法SafePred框架，可预测短期和长期风险、优化决策，实验显示其大幅减少高风险行为，提升安全性能和任务效用。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理（CUAs）的反应式护栏无法主动避免长期风险，有局限性。

Method: 提出预测性护栏方法，以预测的未来风险与当前决策对齐为核心思想，构建SafePred框架，支持风险预测和决策优化。

Result: SafePred显著减少高风险行为，安全性能超97.6%，相比反应式基线，任务效用最多提升21.4%。

Conclusion: SafePred能有效应对CUAs的长期风险问题，提升安全性和任务效用。

Abstract: With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.

</details>


### [998] [<SOG_k>: One LLM Token for Explicit Graph Structural Understanding](https://arxiv.org/abs/2602.01771)
*Jingyao Wu,Bin Lu,Zijun Di,Xiaoying Gan,Meng Jin,Luoyi Fu,Xinbing Wang,Chenghu Zhou*

Main category: cs.CL

TL;DR: 论文提出用特殊标记<SOG_k>解决大语言模型处理图结构数据问题，实验证明方法优越且可扩展到节点级任务。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理图数据存在结构幻觉问题，现有方法有缺陷，需新方法解决。

Method: 引入特殊标记<SOG_k>，用拓扑感知结构分词器将图拓扑映射为单个标记，构建混合结构问答语料库对齐结构标记和文本标记。

Result: 在五个图级基准测试中性能提升9.9% - 41.4%，方法具有可解释性和一致性，可扩展到节点级任务。

Conclusion: 所提方法能让大语言模型简洁准确地理解、生成和推理图数据，代码公开。

Abstract: Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SOG_k> to fully represent the Structure Of Graph within a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SOG_k> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9% to 41.4% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available at https://github.com/Jingyao-Wu/SOG.

</details>


### [999] [ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support](https://arxiv.org/abs/2602.01885)
*Tiantian Chen,Jiaqi Lu,Ying Shen,Lin Zhang*

Main category: cs.CL

TL;DR: 引入ES - MemEval基准和EvoEmo数据集评估大语言模型长期情感支持会话中的记忆能力，分析LLMs在相关任务的表现并指出改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有长期对话基准无法有效评估大语言模型在长期情感支持场景下的记忆能力，有必要设计新的评估基准和数据集。

Method: 引入ES - MemEval基准评估五项核心记忆能力，构建EvoEmo数据集支持评估。

Result: 实验表明显式长期记忆对减少幻觉和实现个性化很重要，RAG能提升事实一致性，但在处理时间动态和用户状态变化上有困难。

Conclusion: 指出当前范式的潜力和局限性，需更深入整合记忆和检索用于长期个性化对话系统。

Abstract: Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.

</details>


### [1000] [Trust in One Round: Confidence Estimation for Large Language Models via Structural Signals](https://arxiv.org/abs/2602.00977)
*Pengyue Yang,Jiawen Wen,Haolin Jin,Linghan Huang,Huaming Chen,Ling Chen*

Main category: cs.CL

TL;DR: 提出基于结构信号的单遍、模型无关的置信度估计框架，在多基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型标准置信估计器在分布偏移、特定领域文本和计算限制下表现不佳。

Method: 提出Structural Confidence框架，结合多尺度结构信号，包括光谱、局部变化和全局形状描述符。

Result: 在四个基准测试中，相比于基线方法在AUROC和AUPR方面表现更好。

Conclusion: 该方法无需多次随机生成和辅助模型，为资源受限的大语言模型应用提供高效、鲁棒的事后置信度估计。

Abstract: Large language models (LLMs) are increasingly deployed in domains where errors carry high social, scientific, or safety costs. Yet standard confidence estimators, such as token likelihood, semantic similarity and multi-sample consistency, remain brittle under distribution shift, domain-specialised text, and compute limits. In this work, we present Structural Confidence, a single-pass, model-agnostic framework that enhances output correctness prediction based on multi-scale structural signals derived from a model's final-layer hidden-state trajectory. By combining spectral, local-variation, and global shape descriptors, our method captures internal stability patterns that are missed by probabilities and sentence embeddings. We conduct extensive, cross-domain evaluation across four heterogeneous benchmarks-FEVER (fact verification), SciFact (scientific claims), WikiBio-hallucination (biographical consistency), and TruthfulQA (truthfulness-oriented QA). Our Structural Confidence framework demonstrates strong performance compared with established baselines in terms of AUROC and AUPR. More importantly, unlike sampling-based consistency methods which require multiple stochastic generations and an auxiliary model, our approach uses a single deterministic forward pass, offering a practical basis for efficient, robust post-hoc confidence estimation in socially impactful, resource-constrained LLM applications.

</details>


### [1001] [Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.01965)
*Kwun Hang Lau,Fangyuan Zhang,Boyu Ruan,Yingli Zhou,Qintian Guo,Ruiyuan Zhang,Xiaofang Zhou*

Main category: cs.CL

TL;DR: 现有RAG方法有‘Static Graph Fallacy’问题，本文提出CatRAG框架解决，实验显示其优于基线，提升推理完整性。


<details>
  <summary>Details</summary>
Motivation: 现有结构感知的RAG方法存在‘Static Graph Fallacy’，导致语义漂移，无法完整检索多跳查询证据链。

Method: 基于HippoRAG 2架构，提出多方面框架引导随机游走，包括Symbolic Anchoring、Query - Aware Dynamic Edge Weighting和Key - Fact Passage Weight Enhancement。

Result: 在四个多跳基准测试中，CatRAG始终优于现有基线，虽标准召回指标提升不大，但推理完整性显著提高。

Conclusion: CatRAG有效弥合了检索部分上下文和实现完全基于证据推理之间的差距。

Abstract: Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a "Static Graph Fallacy": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree "hub" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.

</details>


### [1002] [Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition](https://arxiv.org/abs/2602.01967)
*Wonjun Lee,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出Moe - Ctc架构提升带口音语音识别性能，实验显示在多种条件下有增益。


<details>
  <summary>Details</summary>
Motivation: 多数自动语音识别模型在处理带口音语音时性能下降，口音无关和口音特定方法各有局限。

Method: 引入带中间CTC监督的混合专家架构Moe - Ctc，训练时采用口音感知路由，推理时过渡到无标签路由，每个专家有自己的CTC头，还有路由增强损失稳定优化。

Result: 在Mcv - Accent基准测试中，在低资源和高资源条件下，对可见和不可见口音都有一致增益，相对强FastConformer基线最多降低29.3%的WER。

Conclusion: Moe - Ctc架构能有效提升自动语音识别对带口音语音的处理能力。

Abstract: Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.

</details>


### [1003] [Long-range Modeling and Processing of Multimodal Event Sequences](https://arxiv.org/abs/2602.01125)
*Jichu Li,Yilun Zhong,Zhiting Li,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: 本文提出将基于大语言模型的时序点过程（TPP）扩展到视觉模态的框架，用自适应序列压缩机制解决长上下文问题，实验显示该方法表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有TPP方法在处理多模态内容生成和事件动态推理方面能力有限，且多模态数据增加序列长度，影响注意力模型生成连贯长文本的能力。

Method: 提出将基于大语言模型的TPP扩展到视觉模态的框架，通过基于时间相似性的自适应序列压缩机制解决长上下文问题，采用预训练压缩序列后监督微调的两阶段范式。

Result: 在DanmakuTPP - QA基准测试等大量实验中，该方法在预测准确性和生成文本分析质量上均优于现有基线。

Conclusion: 所提方法能有效解决现有TPP处理多模态数据的不足，提升性能。

Abstract: Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.

</details>


### [1004] [Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation](https://arxiv.org/abs/2602.02007)
*Zhanghao Hu,Qinglin Zhu,Hanqi Yan,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: 传统RAG管道不适合代理记忆系统，提出xMemory方法改善检索问题，实验显示有积极效果。


<details>
  <summary>Details</summary>
Motivation: 标准RAG管道假设与代理记忆系统的特性不匹配，固定的Top - k相似度检索易返回冗余内容，事后修剪可能删除推理所需的前提条件，需要改进检索方法。

Method: 提出xMemory，将记忆解耦为语义组件并组织成层次结构，通过稀疏 - 语义目标引导记忆拆分和合并，推理时自上而下检索。

Result: 在LoCoMo和PerLTQA上针对三个最新大语言模型的实验表明，在答案质量和令牌效率方面有持续提升。

Conclusion: 超越相似度匹配，基于潜在组件操作的检索方法（xMemory）能有效解决代理记忆系统的检索问题。

Abstract: Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.

</details>


### [1005] [Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training to Address Head Collapse](https://arxiv.org/abs/2602.01203)
*Zizhuo Fu,Wenxuan Zeng,Runsheng Wang,Meng Li*

Main category: cs.CL

TL;DR: 本文指出大语言模型存在注意力汇聚问题，证明普通注意力和汇聚注意力自然构成注意力层内的专家混合机制，提出缓解头部坍塌的算法，实验显示该方法能有效平衡头部负载并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对不同注意力机制关系的全面分析，且存在头部坍塌现象影响模型性能。

Method: 从理论和实证角度证明注意力机制内的专家混合结构，提出带辅助负载平衡损失的汇聚感知训练算法。

Result: 方法在普通注意力、汇聚注意力和门控注意力中实现了有效的头部负载平衡，提升了模型性能。

Conclusion: 研究为注意力机制提供新视角，鼓励对注意力层内固有专家混合结构的进一步探索。

Abstract: Large Language Models (LLMs) often assign disproportionate attention to the first token, a phenomenon known as the attention sink. Several recent approaches aim to address this issue, including Sink Attention in GPT-OSS and Gated Attention in Qwen3-Next. However, a comprehensive analysis of the relationship among these attention mechanisms is lacking. In this work, we provide both theoretical and empirical evidence demonstrating that the sink in Vanilla Attention and Sink Attention naturally construct a Mixture-of-Experts (MoE) mechanism within attention layers. This insight explains the head collapse phenomenon observed in prior work, where only a fixed subset of attention heads contributes to generation. To mitigate head collapse, we propose a sink-aware training algorithm with an auxiliary load balancing loss designed for attention layers. Extensive experiments show that our method achieves effective head load balancing and improves model performance across Vanilla Attention, Sink Attention, and Gated Attention. We hope this study offers a new perspective on attention mechanisms and encourages further exploration of the inherent MoE structure within attention layers.

</details>


### [1006] [CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering](https://arxiv.org/abs/2602.01348)
*Yu Liu,Wenxiao Zhang,Cong Cao,Fangfang Yuan,Weizhuo Chen,Cheng Hu,Pin Xu,Yuling Yang,Kun Peng,Diandian Guo,Qiang Sun,Yanbing Liu,Jin B. Hong,Zhiyuan Ma*

Main category: cs.CL

TL;DR: 文章指出RAG用于多跳问答时推理面临挑战，提出CRAFT框架，实验显示其能提升答案准确性和推理忠实度。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在响应生成中可靠推理面临的推理崩溃、推理 - 答案不一致、格式控制丢失三个挑战。

Method: 提出基于GRPO的强化学习框架CRAFT，采用双奖励机制优化多跳推理，支持可控跟踪变体。

Result: 在三个多跳问答基准测试中，CRAFT提升了不同模型规模下的答案准确性和推理忠实度，CRAFT 7B模型在多推理跟踪设置下与闭源大语言模型有竞争力。

Conclusion: CRAFT框架有效提升多跳问答中答案准确性和推理忠实度。

Abstract: Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.

</details>


### [1007] [LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs](https://arxiv.org/abs/2602.02090)
*Yikai Zeng,Yingchao Piao,Jianhui Li*

Main category: cs.CL

TL;DR: 提出LEC - KG框架，结合大语言模型语义理解与知识图谱嵌入结构推理构建领域知识图谱，在中文可持续发展目标报告上效果好。


<details>
  <summary>Details</summary>
Motivation: 因实体提及异构、长尾关系分布和缺乏标准模式，从非结构化文本构建特定领域知识图谱具有挑战性。

Method: 提出LEC - KG双向协作框架，包含分层粗到细关系提取、证据引导的思维链反馈、语义初始化三个关键组件，两模块相互迭代增强。

Result: 在中文可持续发展目标报告上评估，相比大语言模型基线有显著改进，特别是低频关系。

Conclusion: 该框架能可靠地将非结构化政策文本转化为经过验证的知识图谱三元组。

Abstract: Constructing domain-specific knowledge graphs from unstructured text remains challenging due to heterogeneous entity mentions, long-tail relation distributions, and the absence of standardized schemas. We present LEC-KG, a bidirectional collaborative framework that integrates the semantic understanding of Large Language Models (LLMs) with the structural reasoning of Knowledge Graph Embeddings (KGE). Our approach features three key components: (1) hierarchical coarse-to-fine relation extraction that mitigates long-tail bias, (2) evidence-guided Chain-of-Thought feedback that grounds structural suggestions in source text, and (3) semantic initialization that enables structural validation for unseen entities. The two modules enhance each other iteratively-KGE provides structure-aware feedback to refine LLM extractions, while validated triples progressively improve KGE representations. We evaluate LEC-KG on Chinese Sustainable Development Goal (SDG) reports, demonstrating substantial improvements over LLM baselines, particularly on low-frequency relations. Through iterative refinement, our framework reliably transforms unstructured policy text into validated knowledge graph triples.

</details>


### [1008] [Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training](https://arxiv.org/abs/2602.01511)
*Ran Xu,Tianci Liu,Zihan Dong,Tony You,Ilgee Hong,Carl Yang,Linjun Zhang,Tao Zhao,Haoyu Wang*

Main category: cs.CL

TL;DR: 提出Rubric - ARM框架优化评分，实验表明其在多基准测试中达最优。


<details>
  <summary>Details</summary>
Motivation: 标准奖励模型在非可验证领域无法捕捉响应质量多面性，需改进。

Method: 提出Rubric - ARM框架，用强化学习联合优化评分标准生成器和评判器，采用交替优化策略并给出理论分析。

Result: Rubric - ARM在多基准测试中达最优，在离线和在线强化学习中提升下游策略对齐性。

Conclusion: Rubric - ARM能有效解决标准奖励模型的局限，提升性能。

Abstract: Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.

</details>


### [1009] [OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data](https://arxiv.org/abs/2602.02266)
*Tan Sang Nguyen,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 研究平行数据在大语言模型持续预训练中的有效性，构建首个真正开源的东南亚大语言模型OpenSeal。


<details>
  <summary>Details</summary>
Motivation: 多数大语言模型以英语为中心，对低资源语言表现不佳，且现有东南亚大语言模型并非真正开源，需要真正开源的模型以提高透明度和深入理解模型内部机制。

Method: 进行可控且全面的实验，研究平行数据在大语言模型持续预训练中的有效性。

Result: 仅使用平行数据是将大语言模型扩展到新语言的最有效方法，用347亿个平行数据令牌和在8个NVIDIA H200 GPU上运行180小时，构建了OpenSeal。

Conclusion: 可以构建出性能能与类似规模现有模型相媲美的真正开源的东南亚大语言模型。

Abstract: Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.

</details>


### [1010] [Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models](https://arxiv.org/abs/2602.01698)
*Wenhui Tan,Fiorenzo Parascandolo,Enver Sangineto,Jianzhong Ju,Zhenbo Luo,Qian Cao,Rita Cucchiara,Ruihua Song,Jian Luan*

Main category: cs.CL

TL;DR: 现代推理后训练导致探索崩溃，提出潜探索解码策略LED提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现代推理后训练引发探索崩溃，即温度采样无法提高准确率，且后训练的大推理模型最后一层后验熵大幅降低，中间层熵相对较高，基于此熵不对称现象开展研究。

Method: 提出潜探索解码（LED），一种深度条件解码策略，通过累积和聚合中间后验，并选择熵最大的深度配置作为探索候选。

Result: 在多个推理基准和模型上，无需额外训练或参数，LED使pass@1和pass@16准确率分别提升0.61和1.03个百分点。

Conclusion: LED是一种有效的解码策略，能在不增加训练和参数的情况下提升推理模型的准确率。

Abstract: Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.

</details>


### [1011] [Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?](https://arxiv.org/abs/2602.02290)
*Alex Argese,Pasquale Lisena,Raphaël Troncy*

Main category: cs.CL

TL;DR: 提出评估AI生成科学故事的复合指标StoryScore，揭示现有幻觉检测方法局限


<details>
  <summary>Details</summary>
Motivation: 生成式AI可将科学文章转化为面向不同受众的故事，但评估这些故事具有挑战性，标准摘要指标难以评估，幻觉检测器存在问题

Method: 提出StoryScore，将语义对齐、词汇基础、叙事控制等方面整合到统一框架中

Result: 分析揭示了许多幻觉检测方法无法区分教学创意和事实错误的原因

Conclusion: 自动指标能有效评估与原文的语义相似度，但难以评估叙事和控制方式

Abstract: Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.

</details>


### [1012] [Advancing General-Purpose Reasoning Models with Modular Gradient Surgery](https://arxiv.org/abs/2602.02301)
*Min Cai,Yu Liang,Longzheng Wang,Yan Wang,Yueyang Zhang,Long Xia,Zhiyuan Sun,Xi Ye,Daiting Shi*

Main category: cs.CL

TL;DR: 研究多领域强化学习中两种常用策略存在的问题，提出模块化梯度手术（MGS）方法，在Llama和Qwen模型上取得提升，并证明其长期有效性。


<details>
  <summary>Details</summary>
Motivation: 当前训练跨多领域的通用大推理模型面临领域异质性挑战，现有的顺序强化学习和混合强化学习策略存在跨领域干扰问题。

Method: 提出模块化梯度手术（MGS），在Transformer模块级别解决梯度冲突。

Result: MGS在Llama和Qwen模型上，相较于标准多任务强化学习在三个代表性领域分别平均提升4.3（16.6%）和4.5（11.1）分，且长期训练仍有效。

Conclusion: 明确了多领域强化学习中干扰的来源，并提出了训练通用大推理模型的有效解决方案。

Abstract: Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.

</details>


### [1013] [A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method](https://arxiv.org/abs/2602.02320)
*Feiyang Cai,Guijuan He,Yi Hu,Jingjing Wang,Joshua Luo,Tianyu Zhu,Srikanth Pilla,Gang Li,Ling Liu,Feng Luo*

Main category: cs.CL

TL;DR: 本文提出自动化标注框架生成大规模分子结构描述数据集，验证精度达98.6%，为分子与语言对齐打下基础。


<details>
  <summary>Details</summary>
Motivation: 准确对齐分子结构和自然语言对大语言模型处理化学任务至关重要，但人工标注成本高，难以构建大规模高质量数据集。

Method: 构建自动化标注框架，扩展基于规则的化学命名解析器，生成XML元数据来指导大语言模型生成自然语言描述。

Result: 使用该框架整理了约16.3万的分子 - 描述对数据集，在2000个分子子集上验证描述精度达98.6%。

Conclusion: 数据集为分子 - 语言对齐提供可靠基础，标注方法可扩展到更大数据集和更多化学任务。

Abstract: Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.

</details>


### [1014] [Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training](https://arxiv.org/abs/2602.01747)
*Hongseok Choi,Serynn Kim,Wencke Liermann,Jin Seong,Jin-Xia Huang*

Main category: cs.CL

TL;DR: 本文针对自动论文评分（AES）中标签数据稀缺问题，提出三种技术提升AES性能，实验表明技术有效。


<details>
  <summary>Details</summary>
Motivation: 现实中标签数据极度稀缺，限制强大AES系统的开发和应用。

Method: 提出三种技术：两阶段微调策略、分数对齐技术、使用无标签数据的不确定性感知自训练，在DualBERT上实现这些技术。

Result: 在32数据设置中，三种技术均提升性能，集成后达到全量数据91.2%的性能；分数对齐技术在不同数据设置下均提升性能，集成到DualBERT在全量数据设置达当前最优。

Conclusion: 所提三种技术能提升AES在有限和全量数据设置下的性能。

Abstract: Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.

</details>


### [1015] [Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings](https://arxiv.org/abs/2602.01757)
*Doohyun Kim,Donghwa Kang,Kyungjae Lee,Hyeongboo Baek,Brent Byunghoon Kang*

Main category: cs.CL

TL;DR: 提出无训练框架Zero2Text应对向量数据库嵌入反演攻击的隐私风险，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库反演攻击处理范式存在需要大量计算、依赖领域内训练数据等问题，在严格黑盒和跨域场景下无效。

Method: 引入基于递归在线对齐的无训练框架Zero2Text，融合大语言模型先验和动态岭回归机制实现动态对齐。

Result: 在多个基准测试中验证了Zero2Text的有效性，在MS MARCO上相比基线有显著指标提升，可在无数据泄露下恢复未知领域句子。

Conclusion: 标准防御如差分隐私无法有效应对此类自适应威胁，Zero2Text可突破现有方法局限解决向量数据库隐私风险。

Abstract: The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.

</details>


### [1016] [From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making](https://arxiv.org/abs/2602.02378)
*Raunak Jain,Mudita Khurana,John Stephens,Srinivas Dharmasanam,Shankar Venkataraman*

Main category: cs.CL

TL;DR: 大语言模型从辅助转向决策支持时出现无校准判断的盲目附和问题，需转向协作前提治理，介绍相关控制循环等方法并说明信任应基于可审计前提，以辅导为例并提出评估标准


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在决策支持中出现的盲目附和、缺乏校准判断的问题，尤其是在深度不确定性决策中导致的不良后果

Method: 提出从答案生成转向对知识基底的协作前提治理，建立差异驱动的控制循环，包括检测冲突、定位不一致和触发边界协商，设置承诺门控和价值门控挑战

Result: 详细阐述了这种协作前提治理的过程和机制

Conclusion: 可靠的人机伙伴关系依赖于转向协作前提治理，信任应建立在可审计的前提和证据标准上，还给出了可证伪的评估标准

Abstract: As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.

</details>


### [1017] [Sentence Curve Language Models](https://arxiv.org/abs/2602.01807)
*DongNyeong Heo,Heelyoul Choi*

Main category: cs.CL

TL;DR: 本文指出语言模型静态嵌入目标词的局限性，提出句子曲线表示和SCLM模型，理论证明其正则化效果，实证显示SCLM在多个数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型中目标词的静态嵌入对相邻词不敏感，忽略了目标句子的全局结构。

Method: 提出连续句子表示“句子曲线”，并基于此引入句子曲线语言模型（SCLM），用其预测句子曲线而非静态词嵌入。

Result: SCLM在IWSLT14和WMT14上达到DLMs中的SOTA性能，训练稳定无需知识蒸馏，在LM1B上比离散DLMs有潜力。

Conclusion: 句子曲线预测有促进全局结构建模的正则化效果，SCLM模型有良好表现和应用潜力。

Abstract: Language models (LMs) are a central component of modern AI systems, and diffusion-based language models (DLMs) have recently emerged as a competitive alternative. Both paradigms rely on word embeddings not only to represent the input sentence, but also to represent the target sentence that backbone models are trained to predict. We argue that such static embedding of the target word is insensitive to neighboring words, encouraging locally accurate word prediction while neglecting global structure across the target sentence. To address this limitation, we propose a continuous sentence representation, termed sentence curve, defined as a spline curve whose control points affect multiple words in the sentence. Based on this representation, we introduce sentence curve language model (SCLM), which extends DLMs to predict sentence curves instead of the static word embeddings. We theoretically show that sentence curve prediction induces a regularization effect that promotes global structure modeling, and characterize how different sentence curve types affect this behavior. Empirically, SCLM achieves SOTA performance among DLMs on IWSLT14 and WMT14, shows stable training without burdensome knowledge distillation, and demonstrates promising potential compared to discrete DLMs on LM1B.

</details>


### [1018] [Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models](https://arxiv.org/abs/2602.02462)
*Gabriele Maraia,Marco Valentino,Fabio Massimo Zanzotto,Leonardo Ranaldi*

Main category: cs.CL

TL;DR: 本文提出抽象引导推理框架减少大语言模型三段论推理中语义干扰，提升形式推理鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在三段论推理的演绎判断中存在内容效应，现有方法难以可靠抑制语义干扰。

Method: 引入抽象引导推理框架，构建具体和抽象三段论对，定义抽象推理空间，学习轻量级抽象器并进行多层干预。

Result: 抽象对齐引导减少了内容驱动的错误，提高了有效性敏感性能。

Conclusion: 激活级抽象是增强大语言模型形式推理抗语义干扰鲁棒性的可扩展机制。

Abstract: Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.

</details>


### [1019] [MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents](https://arxiv.org/abs/2602.02474)
*Haozhen Zhang,Quanyu Long,Jianzhu Bao,Tao Feng,Weizhi Zhang,Haodong Yue,Wenya Wang*

Main category: cs.CL

TL;DR: 提出MemSkill将大语言模型代理内存操作变为可学习和可进化的记忆技能，形成闭环程序，实验证明其能提升任务表现并泛化良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理内存系统依赖静态手工设计操作，在不同交互模式下僵化，处理长历史效率低。

Method: 将操作重构为记忆技能，采用控制器学习选择相关技能，搭配基于LLM的执行器生成技能引导的记忆，引入设计器定期审查并进化技能集。

Result: 在多个数据集上实验表明，MemSkill比强基线提升了任务表现，且跨设置泛化能力好。

Conclusion: MemSkill形成的闭环程序能提升技能选择策略和技能集本身，为大语言模型代理的内存管理提供了更自适应、自我进化的思路。

Abstract: Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.

</details>


### [1020] [RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents](https://arxiv.org/abs/2602.02486)
*Jialiang Zhu,Gongrui Zhang,Xiaolong Ma,Lin Xu,Miaosen Zhang,Ruiqi Yang,Song Wang,Kai Qiu,Zhirong Wu,Qi Dai,Ruichun Ma,Bei Liu,Yifan Yang,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: 提出Re - TRAC框架解决ReAct框架问题，在实验中表现优于ReAct，小模型经微调达SOTA，且减少工具调用和token使用。


<details>
  <summary>Details</summary>
Motivation: 基于ReAct框架的大语言模型深度研究代理存在难以回溯、分支探索和全局感知的问题，导致局部最优、冗余探索和搜索低效。

Method: 提出Re - TRAC框架，每次轨迹后生成结构化状态表示，后续轨迹基于此表示进行跨轨迹探索，实现迭代反思和全局规划；针对小模型引入Re - TRAC感知的监督微调。

Result: Re - TRAC在BrowseComp上比ReAct性能高15 - 20%；小模型经微调达同规模最优；跨轮次减少工具调用和token使用。

Conclusion: Re - TRAC框架能有效解决ReAct框架存在的问题，实现更有针对性的探索。

Abstract: LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.

</details>


### [1021] [Reward-free Alignment for Conflicting Objectives](https://arxiv.org/abs/2602.02495)
*Peter Chen,Xiaopeng Li,Xi Chen,Tianyi Lin*

Main category: cs.CL

TL;DR: 本文提出无奖励冲突目标对齐框架RACO，利用成对偏好数据解决梯度冲突，实验表明其在多目标对齐任务中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有直接对齐方法在处理多冲突目标时存在训练不稳定、权衡不佳等问题，加权损失法和现有多目标方法有局限性。

Method: 提出RACO框架，利用成对偏好数据，通过冲突规避梯度下降的裁剪变体解决梯度冲突，还用启发式方法改进。

Result: 在多目标摘要和安全对齐任务上，定性和定量评估显示该方法相比现有多目标对齐基线能持续实现更好的帕累托权衡。

Conclusion: RACO框架能有效解决多冲突目标的对齐问题，在多种大语言模型家族的相关任务中表现良好。

Abstract: Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.

</details>


### [1022] [Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning](https://arxiv.org/abs/2602.02099)
*Keqin Peng,Yuanxin Ouyang,Xuebo Liu,Zhiliang Tian,Ruijian Han,Yancheng Yuan,Liang Ding*

Main category: cs.CL

TL;DR: 论文指出RLVR的问题，提出DDCA方法，实验表明其能改善效率与准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR易产生冗长推理痕迹、朴素长度惩罚严重影响准确率的问题。

Method: 提出动态解耦条件优势（DDCA）方法，在正确响应簇内计算长度优势，用组通过率动态调整惩罚强度。

Result: 在多个数据集上，DDCA对比自适应基线持续改善效率 - 准确率权衡，简单任务减少约60%生成令牌，难任务减少超20%，且维持或提高准确率。

Conclusion: DDCA能有效解决RLVR存在的结构问题，改善效率与准确率的平衡。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.

</details>


### [1023] [Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank](https://arxiv.org/abs/2602.02414)
*Joshua Mitton,Prarthana Bhattacharyya,Digory Smith,Thomas Christie,Ralph Abboud,Simon Woodhead*

Main category: cs.CL

TL;DR: 提出用大语言模型从学生 - 导师对话中检测误解的新方法，评估显示该方法优于基线模型，微调能提升表现。


<details>
  <summary>Details</summary>
Motivation: 及时准确识别学生误解对提升学习效果很关键，但依赖教师，因此研究用大语言模型检测误解的方法。

Method: 用微调的大语言模型生成可能的误解，通过嵌入相似度检索候选，再用另一个微调模型评估和重新排序。

Result: 该方法在预测性能上优于基线模型，微调提升了生成误解的质量，能超越一些闭源大模型。

Conclusion: 通过消融实验验证了生成和重新排序步骤对误解生成质量的重要性。

Abstract: Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [1024] [Stochastic bifurcation in economic growth model driven by Lévy noise](https://arxiv.org/abs/2602.00090)
*Almaz Abebe,Shenglan Yuanb,Daniel Tesfay,James Brannan*

Main category: econ.GN

TL;DR: 本文整合Lévy噪声改进经典Solow经济增长模型，分析随机波动对经济的影响并进行数值模拟，展示噪声对经济波动的作用。


<details>
  <summary>Details</summary>
Motivation: 捕捉经济系统中固有的不确定性，更真实地描述不确定环境下的经济发展。

Method: 将Lévy噪声整合到经典Solow模型中，分析连续和离散时间框架，对比确定性和随机场景，并进行数值模拟。

Result: 揭示了跳跃 - 扩散过程在长期GDP波动中的作用，证明随机噪声导致经济波动、增长轨迹突变和分岔。

Conclusion: 提供了外部冲击影响的全面视角，实现了对不确定环境下经济发展更现实的描绘。

Abstract: This paper enhances the classical Solow model of economic growth by integrating Lévy noise, a type of non-Gaussian stochastic perturbation, to capture the inherent uncertainties in economic systems. The extended model examines the impact of these random fluctuations on capital stock and output, revealing the role of jump-diffusion processes in long-term GDP fluctuations. Both continuous and discrete-time frameworks are analyzed to assess the implications for forecasting economic growth and understanding business cycles. The study compares deterministic and stochastic scenarios, providing insight into the stability of equilibrium points and the dynamics of economies subjected to random disturbances. Numerical simulations demonstrate how stochastic noise contributes to economic volatility, leading to abrupt shifts and bifurcations in growth trajectories. This research offers a comprehensive perspective on the influence of external shocks, presenting a more realistic depiction of economic development in uncertain environments.

</details>


### [1025] [Payrolls to Prompts: Firm-Level Evidence on the Substitution of Labor for AI](https://arxiv.org/abs/2602.00139)
*Ryan Stevens*

Main category: econ.GN

TL;DR: 利用美国开支管理平台数据研究企业在线劳动力与生成式AI替代情况，发现AI部分替代人力，有成本节省。


<details>
  <summary>Details</summary>
Motivation: 现有AI替代人力劳动实证证据有限，需研究企业层面在线劳动力与生成式AI替代情况。

Method: 利用2021年Q3至2025年Q3支付数据，以2022年10月ChatGPT发布为共同冲击，用双重差分模型，给出基于冲击前在线劳动力市场支出份额的暴露度衡量方法。

Result: 高暴露度企业更早更密集采用AI并减少劳动力支出，2025年Q3高暴露四分位企业AI支出份额增加，在线劳动力支出下降，最高暴露企业中在线劳动力支出与AI支出有替代关系。

Conclusion: 提供微观层面直接证据，表明生成式AI在生产中部分替代人力劳动。

Abstract: Generative AI has the potential to transform how firms produce output. Yet, credible evidence on how AI is actually substituting for human labor remains limited. In this paper, we study firm-level substitution between contracted online labor and generative AI using payments data from a large U.S. expense management platform. We track quarterly spending from Q3 2021 to Q3 2025 on online labor marketplaces (such as Upwork and Fiverr) and leading AI model providers. To identify causal effects, we exploit the October 2022 release of ChatGPT as a common adoption shock and estimate a difference-in-differences model. We provide a novel measure of exposure based on the share of spending at online labor marketplaces prior to the shock. Firms with greater exposure to online labor adopt AI earlier and more intensively following the shock, while simultaneously reducing spending on contracted labor. By Q3 2025, firms in the highest exposure quartile increase their share of spending on AI model providers by 0.8 percentage points relative to the lowest exposure quartile, alongside significant declines in labor marketplace spending. Combining these responses yields a direct estimate of substitution: among the most exposed firms, a \$1 decline in online labor spending is associated with approximately \$0.03 of additional AI spending, implying order-of-magnitude cost savings from replacing outsourced tasks with AI services. These effects are heterogeneous across firms and emerge gradually over time. Taken together, our results provide the first direct, micro-level evidence that generative AI is being used as a partial substitute for human labor in production.

</details>


### [1026] [Calibrating Behavioral Parameters with Large Language Models](https://arxiv.org/abs/2602.01022)
*Brandon Yee,Krishna Sharma*

Main category: econ.GN

TL;DR: 本文提出用大语言模型测量行为参数的框架，校准后参数有显著变化，嵌入模型后符合实证证据并确立了行为偏差范围。


<details>
  <summary>Details</summary>
Motivation: 行为参数在资产定价模型中很重要，但难以可靠测量，需找到测量方法。

Method: 开发将大语言模型作为行为参数校准测量工具的框架，用四个模型和24000个主体 - 情景对进行研究，并将校准参数嵌入基于主体的资产定价模型。

Result: 基线大语言模型行为存在系统性理性偏差；基于轮廓的校准使多个参数有大的、稳定的和理论上连贯的变化；校准后的外推产生与实证证据一致的模式。

Conclusion: 确立了八个典型行为偏差的测量范围、校准函数和明确边界。

Abstract: Behavioral parameters such as loss aversion, herding, and extrapolation are central to asset pricing models but remain difficult to measure reliably. We develop a framework that treats large language models (LLMs) as calibrated measurement instruments for behavioral parameters. Using four models and 24{,}000 agent--scenario pairs, we document systematic rationality bias in baseline LLM behavior, including attenuated loss aversion, weak herding, and near-zero disposition effects relative to human benchmarks. Profile-based calibration induces large, stable, and theoretically coherent shifts in several parameters, with calibrated loss aversion, herding, extrapolation, and anchoring reaching or exceeding benchmark magnitudes. To assess external validity, we embed calibrated parameters in an agent-based asset pricing model, where calibrated extrapolation generates short-horizon momentum and long-horizon reversal patterns consistent with empirical evidence. Our results establish measurement ranges, calibration functions, and explicit boundaries for eight canonical behavioral biases.

</details>


### [1027] [Hype Has Worth: Attention, Sentiment, and NFT Valuation in Major Ethereum Collections](https://arxiv.org/abs/2602.01531)
*Samiha Tariq*

Main category: econ.GN

TL;DR: 本文研究以太坊NFT市场中社区关注和情绪与估值的关系，发现估值与持续的关注和情绪环境密切相关，短期内负面情绪与高价相关。


<details>
  <summary>Details</summary>
Motivation: 探究在线叙事是否会对数字或文化商品市场价格产生可衡量的影响。

Method: 合并大型生成式NFT集合的交易数据与Reddit话语数据，构建视觉差异标准化指数，使用混合效应模型分离跨集合差异和集合内波动。

Result: 估值与持续的集合层面关注和情绪环境最相关，集合内短期内负面情绪与高价相关，累积参与度衡量的关注信息最丰富。

Conclusion: 社区关注和情绪对以太坊NFT市场估值有显著影响。

Abstract: Do online narratives leave a measurable imprint on prices in markets for digital or cultural goods? This paper evaluates how community attention and sentiment relate to valuation in major Ethereum NFT collections after accounting for time effects, market-wide conditions, and persistent visual heterogeneity. Transaction data for large generative collections are merged with Reddit-based discourse measures available for 25 collections, covering 87{,}696 secondary-market sales from January 2021 through March 2025. Visual differences are absorbed by a transparent, within-collection standardized index built from explicit image traits and aggregated via PCA. Discourse is summarized at the collection-by-bin level using discussion intensity and lexicon-based tone measures, with smoothing to reduce noise when text volume is sparse. A mixed-effects specification with a Mundlak within--between decomposition separates persistent cross-collection differences from within-collection fluctuations. Valuations align most strongly with sustained collection-level attention and sentiment environments; within collections, short-horizon negativity is consistently associated with higher prices, and attention is most informative when measured as cumulative engagement over multiple prior windows.

</details>


### [1028] [The Strategic Foresight of LLMs: Evidence from a Fully Prospective Venture Tournament](https://arxiv.org/abs/2602.01684)
*Felipe A. Csaszar,Aticus Peterson,Daniel Wilde*

Main category: econ.GN

TL;DR: 通过Kickstarter众筹项目预测竞赛，对比大语言模型和人类在战略预见能力上的表现，发现前沿大语言模型表现优于人类。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能在战略预见能力上是否能超越人类。

Method: 开展完全前瞻性的预测竞赛，使用Kickstarter众筹项目，让多种大语言模型和人类评估者对30个美国科技创业项目进行评估并排名。

Result: 人类评估者与实际结果的排名相关性在0.04 - 0.45之间，部分前沿大语言模型超过0.60，最佳的达到0.74。群体智慧集成和人机混合团队都未超越最佳的独立模型。

Conclusion: 在战略预见能力方面，人工智能（部分前沿大语言模型）能超越人类。

Abstract: Can artificial intelligence outperform humans at strategic foresight -- the capacity to form accurate judgments about uncertain, high-stakes outcomes before they unfold? We address this question through a fully prospective prediction tournament using live Kickstarter crowdfunding projects. Thirty U.S.-based technology ventures, launched after the training cutoffs of all models studied, were evaluated while fundraising remained in progress and outcomes were unknown. A diverse suite of frontier and open-weight large language models (LLMs) completed 870 pairwise comparisons, producing complete rankings of predicted fundraising success. We benchmarked these forecasts against 346 experienced managers recruited via Prolific and three MBA-trained investors working under monitored conditions. The results are striking: human evaluators achieved rank correlations with actual outcomes between 0.04 and 0.45, while several frontier LLMs exceeded 0.60, with the best (Gemini 2.5 Pro) reaching 0.74 -- correctly ordering nearly four of every five venture pairs. These differences persist across multiple performance metrics and robustness checks. Neither wisdom-of-the-crowd ensembles nor human-AI hybrid teams outperformed the best standalone model.

</details>


### [1029] [Optimal Solar Investment and Operation under Asymmetric Net Metering](https://arxiv.org/abs/2602.02284)
*Nathan Engelman Lado,Ahmed Alahmed,Audun Botterud,Saurabh Amin*

Main category: econ.GN

TL;DR: 研究净计量电价（NEM）下产消者的联合投资和运营决策，发现PV效应并验证理论结果。


<details>
  <summary>Details</summary>
Motivation: 美国部分州NEM改革引入时变价格和非对称进出口补偿，以往研究将光伏容量视为外生变量，本文旨在内生化光伏投资。

Method: 内生化光伏投资，推导非对称NEM电价下灵活产消者的太阳能容量边际价值，分析最优投资变化，通过案例研究验证理论结果。

Result: 发现PV效应，即某一时期NEM定价变化可通过调整最优光伏投资影响价格未变时期的净需求和消费。

Conclusion: PV效应削弱了提高进口价格增加产消者支付的能力，对NEM改革有直接影响。

Abstract: We examine the joint investment and operational decisions of a prosumer, a customer who both consumes and generates electricity, under net energy metering (NEM) tariffs. Traditional NEM schemes provide temporally flat compensation at the retail price for net energy exports over a billing period. However, ongoing reforms in several U.S. states are introducing time-varying prices and asymmetric import/export compensation to better align incentives with grid costs. While prior studies treat PV capacity as exogenous and focus primarily on consumption behavior, this work endogenizes PV investment and derives the marginal value of solar capacity for a flexible prosumer under asymmetric NEM tariffs. We characterize optimal investment and show how optimal investment changes with prices and PV costs. Through this analysis, we identify a PV effect: changes in NEM pricing in one period can influence net demand and consumption in generating periods with unchanged prices through adjustments in optimal PV investment. The PV effect weakens the ability of higher import prices to increase prosumer payments, with direct implications for NEM reform. We validate our theoretical results in a case study using simulated household and tariff data derived from historical conditions in Massachusetts.

</details>


### [1030] [Strategic Interactions in Science and Technology Networks: Substitutes or Complements?](https://arxiv.org/abs/2602.02403)
*Michael Balzer,Adhen Benlahlou*

Main category: econ.GN

TL;DR: 本文提出科技同行效应理论，研究个体生产力与合作者行为和网络位置关系，实证发现科研与发明产出和网络中心性正相关，且科研对技术有单向促进作用。


<details>
  <summary>Details</summary>
Motivation: 研究个体生产力如何响应合作者在科研和发明活动中的行为和网络位置。

Method: 构建联立方程网络框架，收集癌症相关出版物和专利数据构建合作网络，采用基于外生二元特征预测链接形成的工具变量法，并加入社区固定效应处理内生网络形成问题。

Result: 作者和发明者的产出与网络中心性正相关，科研生产力显著提升技术生产力，但技术产出对科研生产无明显反向影响。

Conclusion: 为科研和发明同行效应的联合动态提供实证证据，强调科技共同演化的微观基础，揭示可利用合作结构设计提升集体知识创造和下游创新的政策。

Abstract: This paper develops a theory of scientific and technological peer effects to study how individuals' productivity responds to the behavior and network positions of their collaborators across both scientific and inventive activities. Building on a simultaneous equation network framework, the model predicts that productivity in each activity increases in a variation of the Katz-Bonacich centrality that captures within-activity and cross-activity strategic complementarities. To test these predictions, we assemble the universe of cancer-related publications and patents and construct coauthorship and coinventorship networks that jointly map the collaboration structure of researchers active in both spheres. Using an instrumental-variables approach based on predicted link formation from exogenous dyadic characteristics, and incorporating community fixed effects to address endogenous network formation, we show that both authors' and inventors' outputs rise with their network centrality, consistent with the theory. Moreover, scientific productivity significantly enhances technological productivity, while technological output does not exert a detectable reciprocal effect on scientific production, highlighting an asymmetric linkage aligned with a science-driven model of innovation. These findings provide the first empirical evidence on the joint dynamics of scientific and inventive peer effects, underscore the micro-foundations of the co-evolution of science and technology, and reveal how collaboration structures can be leveraged to design policies that enhance collective knowledge creation and downstream innovation.

</details>


### [1031] [Skill Substitution, Expectations, and the Business Cycle](https://arxiv.org/abs/2602.02483)
*Andreas Leibing*

Main category: econ.GN

TL;DR: 本文研究高中毕业后劳动力市场状况对高等技能投资的影响，发现大学入学率呈顺周期性，衰退时毕业生预期学术学位回报降低。


<details>
  <summary>Details</summary>
Motivation: 探究高中毕业后劳动力市场条件如何影响高等技能投资。

Method: 使用1995 - 2018年超六百万德国毕业生行政数据，利用特定州长期趋势偏差；使用大规模调查数据。

Result: 大学入学率呈顺周期性，失业率上升使传统大学入学率降低，毕业生转向职业学院和学徒制，且影响教育程度；衰退时毕业生预期学术学位回报降低，职业学位回报稳定。

Conclusion: 劳动力市场状况影响毕业生高等技能投资选择，预期回报变化是主要机制。

Abstract: This paper studies how labor market conditions around high school graduation affect postsecondary skill investments. Using administrative data on more than six million German graduates from 1995-2018, and exploiting deviations from secular state-specific trends, I document procyclical college enrollment. Cyclical increases in unemployment reduce enrollment at traditional universities and shift graduates toward vocational colleges and apprenticeships. These effects translate into educational attainment. Using large-scale survey data, I identify changes in expected returns to different degrees as the main mechanism. During recessions, graduates expect lower returns to an academic degree, while expected returns to a vocational degree are stable.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [1032] [RAG-GNN: Integrating Retrieved Knowledge with Graph Neural Networks for Precision Medicine](https://arxiv.org/abs/2602.00586)
*Hasi Hays,William J. Richardson*

Main category: q-bio.MN

TL;DR: 提出RAG嵌入框架整合网络拓扑与文献知识，对比多种方法，发现在结构和功能任务中有互补性，还应用于癌症信号网络。


<details>
  <summary>Details</summary>
Motivation: 网络拓扑虽适合结构预测，但无法捕捉生物医学文献中的功能语义信息。

Method: 提出RAG嵌入框架，通过对比学习将图神经网络表示与动态检索的文献知识相结合。

Result: 与十种嵌入方法对比，拓扑方法在链接预测表现好，RAG - GNN在功能聚类有正轮廓分数；信息分解显示拓扑贡献77.3%的预测信息，文献提供8.6%独特信息；应用于癌症信号网络识别出DDR1为治疗靶点。

Conclusion: 仅拓扑和检索增强的方法起到互补目的，结构预测靠网络拓扑，功能解释受益于检索知识。

Abstract: Network topology excels at structural predictions but fails to capture functional semantics encoded in biomedical literature. We present a retrieval-augmented generation (RAG) embedding framework that integrates graph neural network representations with dynamically retrieved literature-derived knowledge through contrastive learning. Benchmarking against ten embedding methods reveals task-specific complementarity: topology-focused methods achieve near-perfect link prediction (GCN: 0.983 AUROC), while RAG-GNN is the only method achieving positive silhouette scores for functional clustering (0.001 vs. negative scores for all baselines). Information-theoretic decomposition shows network topology contributes 77.3% of predictive information, while retrieved documents provide 8.6% unique information. Applied to cancer signaling networks (379 proteins, 3,498 interactions), the framework identifies DDR1 as a therapeutic target based on retrieved evidence of synthetic lethality with KRAS mutations. These results establish that topology-only and retrieval-augmented approaches serve complementary purposes: structural prediction tasks are solved by network topology alone, while functional interpretation uniquely benefits from retrieved knowledge.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [1033] [Comparison of Image Processing Models in Quark Gluon Jet Classification](https://arxiv.org/abs/2602.00141)
*Daeun Kim,Jiwon Lee,Wonjun Jeong,Hyeongwoo Noh,Giyeong Kim,Jaeyoon Cho,Geonhee Kwak,Seunghwan Yang,MinJung Kweon*

Main category: physics.data-an

TL;DR: 对基于卷积和基于Transformer的模型区分夸克和胶子喷注做比较，微调Swin - Tiny模型最后两个Transformer块效果佳，自监督预训练有帮助。


<details>
  <summary>Details</summary>
Motivation: 使用模拟喷注图像，对不同模型区分夸克和胶子喷注的性能进行全面比较。

Method: 将喷注子结构编码为粒子运动学的三通道表示，在监督和自监督学习设置下评估CNN、ViTs和Swin - Tiny的性能。

Result: 微调Swin - Tiny模型最后两个Transformer块在效率和准确性上取得最佳平衡，准确率达81.4%，AUC为88.9%；自监督预训练增强特征鲁棒性并减少可训练参数。

Conclusion: 基于分层注意力的模型在喷注子结构研究和领域转移到真实碰撞数据中有潜力。

Abstract: We present a comprehensive comparison of convolutional and transformer-based models for distinguishing quark and gluon jets using simulated jet images from Pythia 8. By encoding jet substructure into a three-channel representation of particle kinematics, we evaluate the performance of convolutional neural networks (CNNs), Vision Transformers (ViTs), and Swin Transformers (Swin-Tiny) under both supervised and self-supervised learning setups. Our results show that fine-tuning only the final two transformer blocks of the Swin-Tiny model achieves the best trade-off between efficiency and accuracy, reaching 81.4% accuracy and an AUC (area under the ROC curve) of 88.9%. Self-supervised pretraining with Momentum Contrast (MoCo) further enhances feature robustness and reduces the number of trainable parameters. These findings highlight the potential of hierarchical attention-based models for jet substructure studies and for domain transfer to real collision data.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [1034] [Deciding Reachability and the Covering Problem with Diagnostics for Sound Acyclic Free-Choice Workflow Nets](https://arxiv.org/abs/2602.02447)
*Thomas M. Prinz,Christopher T. Schwanen,Wil M. P. van der Aalst*

Main category: cs.FL

TL;DR: 本文将有向无环自由选择工作流网的可达性和覆盖问题复杂度优化至二次多项式，并提出三个新概念解释可达性原因，还给出算法和实现。


<details>
  <summary>Details</summary>
Motivation: 细化有向无环自由选择工作流网中可达性和覆盖问题的复杂度，并解释给定标记可达或不可达的原因。

Method: 提出可允许性、最大可允许性和发散转移三个新概念，基于并发和后支配边界概念设计算法。

Result: 将复杂度优化到$O(P^2 + T^2)$，并给出新概念的算法及求解可达性的直接实现。

Conclusion: 成功优化复杂度，且能准确解释标记可达性。

Abstract: A central decision problem in Petri net theory is reachability asking whether a given marking can be reached from the initial marking. Related is the covering problem (or sub-marking reachbility), which decides whether there is a reachable marking covering at least the tokens in the given marking. For live and bounded free-choice nets as well as for sound free-choice workflow nets, both problems are polynomial in their computational complexity. This paper refines this complexity for the class of sound acyclic free-choice workflow nets to a quadratic polynomial, more specifically to $O(P^2 + T^2)$. Furthermore, this paper shows the feasibility of accurately explaining why a given marking is or is not reachable. This can be achieved by three new concepts: admissibility, maximum admissibility, and diverging transitions. Admissibility requires that all places in a given marking are pairwise concurrent. Maximum admissibility states that adding a marked place to an admissible marking would make it inadmissible. A diverging transition is a transition which originally "produces" the concurrent tokens that lead to a given marking. In this paper, we provide algorithms for all these concepts and explain their computation in detail by basing them on the concepts of concurrency and post-dominance frontiers - a well known concept from compiler construction. In doing this, we present straight-forward implementations for solving (sub-marking) reachability.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1035] [On the Fragility of AI-Based Channel Decoders under Small Channel Perturbations](https://arxiv.org/abs/2602.01582)
*Haoyu Lei,Mohammad Jalali,Chin Wa Lau,Farzan Farnia*

Main category: cs.IT

TL;DR: 研究AI纠错解码器性能提升来源与代价，发现其在对抗扰动下性能下降，有潜在鲁棒性成本。


<details>
  <summary>Details</summary>
Motivation: 探究深度学习纠错解码器性能提升来源与代价。

Method: 从信道输出分布偏移的鲁棒性角度研究，评估输入依赖的对抗扰动和通用对抗扰动。

Result: AI解码器在对抗扰动下性能显著下降，对抗扰动在AI解码器间转移强，通用扰动危害大。

Conclusion: 近期AI解码增益存在潜在鲁棒性成本和对信道分布更高敏感性。

Abstract: Recent advances in deep learning have led to AI-based error correction decoders that report empirical performance improvements over traditional belief-propagation (BP) decoding on AWGN channels. While such gains are promising, a fundamental question remains: where do these improvements come from, and what cost is paid to achieve them? In this work, we study this question through the lens of robustness to distributional shifts at the channel output. We evaluate both input-dependent adversarial perturbations (FGM and projected gradient methods under $\ell_2$ constraints) and universal adversarial perturbations that apply a single norm-bounded shift to all received vectors. Our results show that recent AI decoders, including ECCT and CrossMPT, could suffer significant performance degradation under such perturbations, despite superior nominal performance under i.i.d. AWGN. Moreover, adversarial perturbations transfer relatively strongly between AI decoders but weakly to BP-based decoders, and universal perturbations are substantially more harmful than random perturbations of equal norm. These numerical findings suggest a potential robustness cost and higher sensitivity to channel distribution underlying recent AI decoding gains.

</details>


### [1036] [Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation](https://arxiv.org/abs/2602.02469)
*Ahmed M. Elshazly,Ahmed Arafa*

Main category: cs.IT

TL;DR: 研究无线衰落信道上联邦学习，提出年龄感知边缘盲空中联邦学习方法，用AgeTop - k选参数减少延迟，给出收敛界，实验展示天线数量、选择方法和k值影响。


<details>
  <summary>Details</summary>
Motivation: 解决无线衰落信道中多设备同时发送模型更新时，正交子载波数量有限导致传输参数延迟增加的问题。

Method: 提出年龄感知边缘盲空中联邦学习方法，参数服务器用多天线和最大比合并检测参数更新，用AgeTop - k选择模型坐标。

Result: 更多参数服务器天线可提高准确率和收敛速度；AgeTop - k在较好信道条件下优于随机选择；最优k值取决于信道，噪声环境中小k值更好。

Conclusion: 该方法在无线衰落信道的联邦学习中有优势，不同场景下天线数量、选择方法和k值对性能有影响。

Abstract: We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates. We propose an efficient \emph{age-aware edge-blind over-the-air FL} approach that does not require channel state information (CSI) at the devices. Instead, the parameter server (PS) uses multiple antennas and applies maximum-ratio combining (MRC) based on its estimated sum of the channel gains to detect the parameter updates. A key challenge is that the number of orthogonal subcarriers is limited; thus, transmitting many parameters requires multiple Orthogonal Frequency Division Multiplexing (OFDM) symbols, which increases latency. To address this, the PS selects only a small subset of model coordinates each round using \emph{AgeTop-\(k\)}, which first picks the largest-magnitude entries and then chooses the \(k\) coordinates with the longest waiting times since they were last selected. This ensures that all selected parameters fit into a single OFDM symbol, reducing latency. We provide a convergence bound that highlights the advantages of using a higher number of antenna array elements and demonstrates a key trade-off: increasing \(k\) decreases compression error at the cost of increasing the effect of channel noise. Experimental results show that (i) more PS antennas greatly improve accuracy and convergence speed; (ii) AgeTop-\(k\) outperforms random selection under relatively good channel conditions; and (iii) the optimum \(k\) depends on the channel, with smaller \(k\) being better in noisy settings.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [1037] [Parametrization of subgrid scales in long-term simulations of the shallow-water equations using machine learning and convex limiting](https://arxiv.org/abs/2602.00378)
*Md Amran Hossan Mojamder,Zhihang Xu,Min Wang,Ilya Timofeyev*

Main category: physics.flu-dyn

TL;DR: 提出浅水方程亚网格过程参数化方法，用神经网络学习亚网格通量，有诸多优势。


<details>
  <summary>Details</summary>
Motivation: 为浅水方程的亚网格过程进行参数化。

Method: 定义粗变量和局部空间平均，用前馈神经网络学习亚网格通量。

Result: 实现局部参数化，改善长期湍流模拟能量平衡，准确再现单个解，可与通量限制结合减少震荡，在未训练动态区域也有可靠参数化。

Conclusion: 该方法在浅水方程亚网格过程参数化上表现良好，具有多方面优势和可靠性。

Abstract: We present a method for parametrizing sub-grid processes in the Shallow Water equations. We define coarse variables and local spatial averages and use a feed-forward neural network to learn sub-grid fluxes. Our method results in a local parametrization that uses a four-point computational stencil, which has several advantages over globally coupled parametrizations. We demonstrate numerically that our method improves energy balance in long-term turbulent simulations and also accurately reproduces individual solutions. The neural network parametrization can be easily combined with flux limiting to reduce oscillations near shocks. More importantly, our method provides reliable parametrizations, even in dynamical regimes that are not included in the training data.

</details>


### [1038] [WAKESET: A Large-Scale, High-Reynolds Number Flow Dataset for Machine Learning of Turbulent Wake Dynamics](https://arxiv.org/abs/2602.01379)
*Zachary Cooper-Baldock,Paulo E. Santos,Russell S. A. Brinkworth,Karl Sammut*

Main category: physics.flu-dyn

TL;DR: 本文介绍了用于高湍流流动的大规模CFD数据集WAKESET，旨在为机器学习提供高质量数据，阐述了数据集创建动机、过程并强调其对机器学习模型开发的价值。


<details>
  <summary>Details</summary>
Motivation: 机器学习在计算流体动力学中有变革潜力，但因缺乏大规模、多样化、高保真数据集而受限。高雷诺数湍流数据集对于训练模型学习复杂物理至关重要，现有数据集无法满足需求。

Method: 针对水下航行器回收问题，进行1091次高保真雷诺平均纳维 - 斯托克斯模拟，并扩充至4364个实例，覆盖广泛的速度和转向角范围。

Result: 创建了包含复杂流体动力交互信息的WAKESET数据集，反映了实际工程问题。

Conclusion: WAKESET数据集因其聚焦实际工程问题、规模大及高湍流特性，对于开发和基准测试复杂水下环境的机器学习模型具有重要价值。

Abstract: Machine learning (ML) offers transformative potential for computational fluid dynamics (CFD), promising to accelerate simulations, improve turbulence modelling, and enable real-time flow prediction and control-capabilities that could fundamentally change how engineers approach fluid dynamics problems. However, the exploration of ML in fluid dynamics is critically hampered by the scarcity of large, diverse, and high-fidelity datasets suitable for training robust models. This limitation is particularly acute for highly turbulent flows, which dominate practical engineering applications yet remain computationally prohibitive to simulate at scale. High-Reynolds number turbulent datasets are essential for ML models to learn the complex, multi-scale physics characteristic of real-world flows, enabling generalisation beyond the simplified, low-Reynolds number regimes often represented in existing datasets. This paper introduces WAKESET, a novel, large-scale CFD dataset of highly turbulent flows, designed to address this critical gap. The dataset captures the complex hydrodynamic interactions during the underwater recovery of an autonomous underwater vehicle by a larger extra-large uncrewed underwater vehicle. It comprises 1,091 high-fidelity Reynolds-Averaged Navier-Stokes simulations, augmented to 4,364 instances, covering a wide operational envelope of speeds (up to Reynolds numbers of 1.09 x 10^8) and turning angles. This work details the motivation for this new dataset by reviewing existing resources, outlines the hydrodynamic modelling and validation underpinning its creation, and describes its structure. The dataset's focus on a practical engineering problem, its scale, and its high turbulence characteristics make it a valuable resource for developing and benchmarking ML models for flow field prediction, surrogate modelling, and autonomous navigation in complex underwater environments.

</details>


### [1039] [Physics-Informed Chebyshev Polynomial Neural Operator for Parametric Partial Differential Equations](https://arxiv.org/abs/2602.01737)
*Biao Chen,Jing Wang,Hairun Xie,Qineng Wang,Shuai Zhang,Yifan Xia,Jifa Zhang*

Main category: physics.flu-dyn

TL;DR: 提出物理信息切比雪夫多项式神经算子（CPNO）解决现有神经算子架构局限，理论和实验证明其优势。


<details>
  <summary>Details</summary>
Motivation: 当前神经算子方法依赖多层感知机，存在固有频谱偏差和固定激活函数问题，损害物理信息设置中的训练鲁棒性，需克服架构局限。

Method: 引入CPNO框架，利用基变换将不稳定单项式展开替换为数值稳定的切比雪夫谱基，并在主网络中集成参数依赖调制机制。

Result: 理论分析表明切比雪夫基具有近极小极大一致逼近特性和优越条件；数值实验显示CPNO在基准参数化偏微分方程上精度更高、收敛更快、对超参数鲁棒性更强；跨音速翼型流实验证明其能处理复杂几何问题。

Conclusion: CPNO能有效克服现有神经算子的架构局限，在解决参数化偏微分方程上表现出色。

Abstract: Neural operators have emerged as powerful deep learning frameworks for approximating solution operators of parameterized partial differential equations (PDE). However, current methods predominantly rely on multilayer perceptrons (MLPs) for mapping inputs to solutions, which impairs training robustness in physics-informed settings due to inherent spectral biases and fixed activation functions. To overcome the architectural limitations, we introduce the Physics-Informed Chebyshev Polynomial Neural Operator (CPNO), a novel mesh-free framework that leverages a basis transformation to replace unstable monomial expansions with the numerically stable Chebyshev spectral basis. By integrating parameter dependent modulation mechanism to main net, CPNO constructs PDE solutions in a near-optimal functional space, decoupling the model from MLP-specific constraints and enhancing multi-scale representation. Theoretical analysis demonstrates the Chebyshev basis's near-minimax uniform approximation properties and superior conditioning, with Lebesgue constants growing logarithmically with degree, thereby mitigating spectral bias and ensuring stable gradient flow during optimization. Numerical experiments on benchmark parameterized PDEs show that CPNO achieves superior accuracy, faster convergence, and enhanced robustness to hyperparameters. The experiment of transonic airfoil flow has demonstrated the capability of CPNO in characterizing complex geometric problems.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1040] [Posterior Uncertainty for Targeted Parameters in Bayesian Bootstrap Procedures](https://arxiv.org/abs/2602.02216)
*Magid Sabbagh,David A. Stephens*

Main category: stat.ME

TL;DR: 提出对含干扰参数的有限维目标参数进行贝叶斯分析的通用方法，应用于因果推断，理论验证其渐近性质和置信区间覆盖特性，并讨论在错误指定和单稳健模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯因果推断文献多依赖传统框架，需新方法进行有效贝叶斯分析并处理倾向得分不确定性。

Method: 提出通用方法，是“Linked Bayesian Bootstrap”的广义版本，应用于基于估计方程的因果推断。

Result: 所提方法具有良好的频率主义性质，可信区间有渐近正确的覆盖特性。

Conclusion: 所提方法可用于因果推断中的错误指定和单稳健模型。

Abstract: We propose a general method to carry out a valid Bayesian analysis of a finite-dimensional `targeted' parameter in the presence of a finite-dimensional nuisance parameter. We apply our methods to causal inference based on estimating equations. While much of the literature in Bayesian causal inference has relied on the conventional 'likelihood times prior' framework, a recently proposed method, the 'Linked Bayesian Bootstrap', deviated from this classical setting to obtain valid Bayesian inference using the Dirichlet process and the Bayesian bootstrap. These methods rely on an adjustment based on the propensity score and explain how to handle the uncertainty concerning it when studying the posterior distribution of a treatment effect. We examine theoretically the asymptotic properties of the posterior distribution obtained and show that our proposed method, a generalized version of the 'Linked Bayesian Bootstrap', enjoys desirable frequentist properties. In addition, we show that the credible intervals have asymptotically the correct coverage properties. We discuss the applications of our method to mis-specified and singly-robust models in causal inference.

</details>


### [1041] [When Is Generalized Bayes Bayesian? A Decision-Theoretic Characterization of Loss-Based Updating](https://arxiv.org/abs/2602.01573)
*Kenichiro McAlinn,Kōsaku Takanashi*

Main category: stat.ME

TL;DR: 本文对基于损失的更新进行决策理论刻画，区分信念后验和决策后验，明确基于损失的后验与普通贝叶斯的关系，指出广义边际似然和贝叶斯因子的相关问题，还给出决策后验体制下的最优规则。


<details>
  <summary>Details</summary>
Motivation: 对基于损失的更新进行决策理论刻画，区分不同类型的后验，明确相关概念和规则。

Method: 通过理论推导和分析，给出决策理论刻画，明确不同条件下的关系和规则。

Result: 明确基于损失的后验与普通贝叶斯的等价条件，指出广义边际似然和贝叶斯因子的问题，得出决策后验体制下的熵惩罚变分表示和最优规则。

Conclusion: 在决策后验体制下，非退化后验需要对决策规则的非线性偏好，在顺序一致性和可分性下，广义贝叶斯是最优规则。

Abstract: Loss-based updating, including generalized Bayes, Gibbs, and quasi-posteriors, replaces likelihoods by a user-chosen loss and produces a posterior-like distribution via exponential tilt. We give a decision-theoretic characterization that separates \emph{belief posteriors} --  conditional beliefs justified by the foundations of Savage and Anscombe-Aumann under a joint probability mode l-- from \emph{decision posteriors} -- randomized decision rules justified by preferences over decision rules. We make explicit that a loss-based posterior coincides with ordinary Bayes if and only if the loss is, up to scale and a data-only term, negative log-likelihood. We then show that generalized marginal likelihood is not evidence for decision posteriors, and Bayes factors are not well-defined without additional structure. In the decision posterior regime, non-degenerate posteriors require nonlinear preferences over decision rules. Under sequential coherence and separability, these lead to an entropy-penalized variational representation yielding generalized Bayes as the optimal rule.

</details>


### [1042] [Learning Sequential Decisions from Multiple Sources via Group-Robust Markov Decision Processes](https://arxiv.org/abs/2602.01825)
*Mingyuan Xu,Zongqi Xia,Tianxi Cai,Doudou Zhou,Nian Si*

Main category: stat.ME

TL;DR: 本文旨在从多站点离线数据中学习鲁棒的序贯决策策略，提出基于分布鲁棒MDPs的方法，开发离线算法并证明策略的次优性界。


<details>
  <summary>Details</summary>
Motivation: 从具有共同结构但存在异质性的多站点离线数据中学习鲁棒的序贯决策策略。

Method: 研究具有组线性结构的分布鲁棒MDPs，引入特征级不确定性集，开发基于悲观值迭代的离线算法，提出集群级扩展。

Result: 在鲁棒部分覆盖假设下，证明了所得策略的次优性界。

Conclusion: 框架解决了多站点异质数据源的学习问题，提供了不依赖强状态 - 动作矩形假设的鲁棒规划方法。

Abstract: We often collect data from multiple sites (e.g., hospitals) that share common structure but also exhibit heterogeneity. This paper aims to learn robust sequential decision-making policies from such offline, multi-site datasets. To model cross-site uncertainty, we study distributionally robust MDPs with a group-linear structure: all sites share a common feature map, and both the transition kernels and expected reward functions are linear in these shared features. We introduce feature-wise (d-rectangular) uncertainty sets, which preserve tractable robust Bellman recursions while maintaining key cross-site structure. Building on this, we then develop an offline algorithm based on pessimistic value iteration that includes: (i) per-site ridge regression for Bellman targets, (ii) feature-wise worst-case (row-wise minimization) aggregation, and (iii) a data-dependent pessimism penalty computed from the diagonals of the inverse design matrices. We further propose a cluster-level extension that pools similar sites to improve sample efficiency, guided by prior knowledge of site similarity. Under a robust partial coverage assumption, we prove a suboptimality bound for the resulting policy. Overall, our framework addresses multi-site learning with heterogeneous data sources and provides a principled approach to robust planning without relying on strong state-action rectangularity assumptions.

</details>


### [1043] [Causal Inference for Preprocessed Outcomes with an Application to Functional Connectivity](https://arxiv.org/abs/2602.02240)
*Zihang Wang,Razieh Nabi,Benjamin B. Risk*

Main category: stat.ME

TL;DR: 提出半参数框架用于受试者内处理后派生结果的因果推断，模拟研究显示其性能优越并应用于实际研究。


<details>
  <summary>Details</summary>
Motivation: 受试者内处理广泛使用，但对受试者间统计推断的影响未系统研究，且缺乏因果分析框架。

Method: 提出半参数框架，开发多重稳健估计量，在中介设置下聚焦自然直接效应，高维推断采用控制错误发现比例超限率的逐步下降程序。

Result: 模拟研究表明所提方法性能优越。

Conclusion: 所提方法可用于估计兴奋剂药物对自闭症谱系障碍儿童大脑连接的影响。

Abstract: In biomedical research, repeated measurements within each subject are often processed to remove artifacts and unwanted sources of variation. The resulting data are used to construct derived outcomes that act as proxies for scientific outcomes that are not directly observable. Although intra-subject processing is widely used, its impact on inter-subject statistical inference has not been systematically studied, and a principled framework for causal analysis in this setting is lacking. In this article, we propose a semiparametric framework for causal inference with derived outcomes obtained after intra-subject processing. This framework applies to settings with a modular structure, where intra-subject analyses are conducted independently across subjects and are followed by inter-subject analyses based on parameters from the intra-subject stage. We develop multiply robust estimators of causal parameters under rate conditions on both intra-subject and inter-subject models, which allows the use of flexible machine learning. We specialize the framework to a mediation setting and focus on the natural direct effect. For high dimensional inference, we employ a step-down procedure that controls the exceedance rate of the false discovery proportion. Simulation studies demonstrate the superior performance of the proposed approach. We apply our method to estimate the impact of stimulant medication on brain connectivity in children with autism spectrum disorder.

</details>


### [1044] [On the calibration of survival models with competing risks](https://arxiv.org/abs/2602.00194)
*Julie Alberge,Tristan Haugomat,Gaël Varoquaux,Judith Abécassis*

Main category: stat.ME

TL;DR: 本文聚焦竞争风险环境下生存分析的校准问题，引入新框架和校准措施，并给出估计、测试和校正校准的方法，使重新校准方法在保持区分度的同时得到良好概率。


<details>
  <summary>Details</summary>
Motivation: 现有校准措施不适用于竞争风险环境，近期模型无法给出良好概率，且该环境下校准研究不足。

Method: 引入有两个新校准度量的专用框架，还介绍了估计、测试和校正校准的方法。

Result: 重新校准方法在保持区分度的同时能得到良好概率。

Conclusion: 所提出的框架和方法能有效解决竞争风险环境下生存分析的校准问题。

Abstract: Survival analysis deals with modeling the time until an event occurs, and accurate probability estimates are crucial for decision-making, particularly in the competing-risks setting where multiple events are possible. While recent work has addressed calibration in standard survival analysis, the competing-risks setting remains under-explored as it is harder (the calibration applies to both probabilities across classes and time horizon). We show that existing calibration measures are not suited to the competing-risk setting and that recent models do not give well-behaved probabilities. To address this, we introduce a dedicated framework with two novel calibration measures that are minimized for oracle estimators (i.e., both measures are proper). We also introduce some methods to estimate, test, and correct the calibration. Our recalibration methods yield good probabilities while preserving discrimination.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1045] [AI in Debt Collection: Estimating the Psychological Impact on Consumers](https://arxiv.org/abs/2602.00050)
*Minou Goetze,Sebastian Clajus,Stephan Stricker*

Main category: cs.CY

TL;DR: 研究用欧洲11国数据探讨AI用于债务催收的心理和行为影响，发现AI沟通更高效、能减 stigma 且不损 trust，但在需高 empathy 或公平敏感性场景要慎用。


<details>
  <summary>Details</summary>
Motivation: 探究将AI融入债务催收实践的心理和行为影响。

Method: 采用大规模实验设计（n = 3514），对比人类与AI介导的沟通。

Result: 参与者认为人类互动更公平、易引发互惠，AI沟通更高效；在信任上无差异；人类接触引发更多共情，但也有更强 stigma；不同性别、年龄组和文化背景有显著差异。

Conclusion: AI介导沟通可提高效率、减少 stigma 且不降低信任，但在需高共情或公平敏感性的情况应谨慎使用，研究有助于理解AI对敏感金融互动心理动态的影响并为沟通策略设计提供参考。

Abstract: The present study investigates the psychological and behavioral implications of integrating AI into debt collection practices using data from eleven European countries. Drawing on a large-scale experimental design (n = 3514) comparing human versus AI-mediated communication, we examine effects on consumers' social preferences (fairness, trust, reciprocity, efficiency) and social emotions (stigma, empathy). Participants perceive human interactions as more fair and more likely to elicit reciprocity, while AI-mediated communication is viewed as more efficient; no differences emerge in trust. Human contact elicits greater empathy, but also stronger feelings of stigma. Exploratory analyses reveal notable variation between gender, age groups, and cultural contexts. In general, the findings suggest that AI-mediated communication can improve efficiency and reduce stigma without diminishing trust, but should be used carefully in situations that require high empathy or increased sensitivity to fairness. The study advances our understanding of how AI influences the psychological dynamics in sensitive financial interactions and informs the design of communication strategies that balance technological effectiveness with interpersonal awareness.

</details>


### [1046] [Beyond Static Question Banks: Dynamic Knowledge Expansion via LLM-Automated Graph Construction and Adaptive Generation](https://arxiv.org/abs/2602.00020)
*Yingquan Wang,Tianyu Wei,Qinsi Li,Li Zeng*

Main category: cs.CY

TL;DR: 本文提出Generative GraphRAG框架解决个性化教育系统存在的问题，该框架含两个核心模块，已在实际教育场景应用获正面反馈。


<details>
  <summary>Details</summary>
Motivation: 现有个性化教育系统依赖手动构建知识图谱，成本高、扩展性差，且缺乏对学习者知识的有效推理，适应性有限。

Method: 提出Generative GraphRAG框架，包含自动构建分层知识图谱的Auto - HKG模块，和基于图推理结合检索增强生成个性化练习的CG - RAG模块。

Result: 框架已在实际教育场景部署，获得用户的良好反馈。

Conclusion: 该框架有潜力支持实用的个性化教育系统。

Abstract: Personalized education systems increasingly rely on structured knowledge representations to support adaptive learning and question generation. However, existing approaches face two fundamental limitations. First, constructing and maintaining knowledge graphs for educational content largely depends on manual curation, resulting in high cost and poor scalability. Second, most personalized education systems lack effective support for state-aware and systematic reasoning over learners' knowledge, and therefore rely on static question banks with limited adaptability. To address these challenges, this paper proposes a Generative GraphRAG framework for automated knowledge modeling and personalized exercise generation. It consists of two core modules. The first module, Automated Hierarchical Knowledge Graph Constructor (Auto-HKG), leverages LLMs to automatically construct hierarchical knowledge graphs that capture structured concepts and their semantic relations from educational resources. The second module, Cognitive GraphRAG (CG-RAG), performs graph-based reasoning over a learner mastery graph and combines it with retrieval-augmented generation to produce personalized exercises that adapt to individual learning states. The proposed framework has been deployed in real-world educational scenarios, where it receives favorable user feedback, suggesting its potential to support practical personalized education systems.

</details>


### [1047] [Early Warning Signals Appear Long Before Dropping Out: An Idiographic Approach Grounded in Complex Dynamic Systems Theory](https://arxiv.org/abs/2602.00021)
*Mohammed Saqr,Sonsoles López-Pernas,Santtu Tikka,Markus Wolfgang Hermann Spitzer*

Main category: cs.CY

TL;DR: 文章测试基于临界减速概念的韧性丧失预警信号能否预测学生辍学前的脱离行为，通过分析数据发现多数学生在脱离前有信号，为教育中的临界减速提供首份证据。


<details>
  <summary>Details</summary>
Motivation: 韧性对学习至关重要，预测学生脱离行为很关键，想测试基于临界减速概念的韧性丧失预警信号能否预测辍学前的脱离行为。

Method: 使用9401名学生在数字数学学习环境中的167万次练习尝试，计算临界减速指标，如自相关、返回率等。

Result: 88.2%的学生在脱离前出现临界减速信号，且警告集中在活动后期和练习停止前。

Conclusion: 为教育中的临界减速提供首份证据，表明通用韧性动态也支配人类学习等社会系统，可作为早期检测脆弱性的实用指标，且独立于数据生成机制，有跨情境应用潜力。

Abstract: The ability to sustain engagement and recover from setbacks (i.e., resilience) -- is fundamental for learning. When resilience weakens, students are at risk of disengagement and may drop out and miss on opportunities. Therefore, predicting disengagement long before it happens during the window of hope is important. In this article, we test whether early warning signals of resilience loss, grounded in the concept of critical slowing down (CSD) can forecast disengagement before dropping out. CSD has been widely observed across ecological, climate, and neural systems, where it precedes tipping points into catastrophic failure (dropping out in our case). Using 1.67 million practice attempts from 9,401 students who used a digital math learning environment, we computed CSD indicators: autocorrelation, return rate, variance, skewness, kurtosis, and coefficient of variation. We found that 88.2% of students exhibited CSD signals prior to disengagement, with warnings clustering late in activity and before practice ceased (dropping out). Our results provide the first evidence of CSD in education, suggesting that universal resilience dynamics also govern social systems such as human learning. These findings offer a practical indicator for early detection of vulnerability and supporting learners across different applications and contexts long before critical events happen. Most importantly, CSD indicators arise universally, independent of the mechanisms that generate the data, offering new opportunities for portability across contexts, data types, and learning environments.

</details>


### [1048] [Strategies for Creating Uncertainty in the AI Era to Trigger Students Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System](https://arxiv.org/abs/2602.00026)
*Ahmad Samer Wazan*

Main category: cs.CY

TL;DR: 本文提出将AI融入教育的方法，即借助AI创造不确定情境，设计学习活动和评估，还介绍了MindMosaicAIExam系统及评估量规。


<details>
  <summary>Details</summary>
Motivation: 应对生成式AI使学生不展示理解和推理就能得出正确答案的挑战，探索将AI融入教育的方式。

Method: 借鉴认识论和批判性思维研究，围绕AI模型和教师的固有局限设计学习活动和评估，明确控制考试中AI行为，引入MindMosaicAIExam系统及评估量规。

Result: 展示了控制AI行为可防止其成为获取确定性答案的捷径。

Conclusion: 通过创造不确定情境和使用相关系统及量规，可鼓励学生推理、质疑和证明答案，促进批判性思维。

Abstract: Generative AI challenges traditional assessments by allowing students to produce correct answers without demonstrating understanding or reasoning. Rather than prohibiting AI, this work argues that one way to integrate AI into education is by creating uncertain situations with the help of AI models and using thinking-oriented teaching approaches, where uncertainty is a central pedagogical concept for stimulating students critical thinking. Drawing on epistemology and critical thinking research studies, we propose designing learning activities and assessments around the inherent limitations of both AI models and instructors. This encourages students to reason, question, and justify their final answers. We show how explicitly controlling AI behavior during exams (such as preventing direct answers or generating plausible but flawed responses) prevents AI from becoming a shortcut to certainty. To support this pedagogy, we introduce MindMosaicAIExam, an exam system that integrates controllable AI tools and requires students to provide initial answers, critically evaluate AI outputs, and iteratively refine their reasoning. We also present an evaluation rubric designed to assess critical thinking based on students reasoning artifacts collected by the exam system.

</details>


### [1049] [Happy Young Women, Grumpy Old Men? Emotion-Driven Demographic Biases in Synthetic Face Generation](https://arxiv.org/abs/2602.00032)
*Mengting Wei,Aditya Gulati,Guoying Zhao,Nuria Oliver*

Main category: cs.CY

TL;DR: 本文对八个先进的文本到图像模型进行系统审计，发现所有模型都存在人口统计和情绪条件偏差。


<details>
  <summary>Details</summary>
Motivation: 当前对文本到图像模型生成人脸时的偏差、表征质量和跨文化一致性了解不足，且缺乏情绪提示对人口统计表征影响及不同文化语言背景模型输出差异的研究。

Method: 对八个模型进行相同提示，用先进面部分析算法估计生成人脸的性别、种族、年龄和吸引力水平，用信息论偏差指标衡量与全球人口统计数据的偏差。

Result: 所有模型无论来自哪个国家，都存在持续的人口统计和情绪条件偏差。

Conclusion: 讨论了对公平性、社会技术危害、治理和透明生成系统开发的影响。

Abstract: Synthetic face generation has rapidly advanced with the emergence of text-to-image (T2I) and of multimodal large language models, enabling high-fidelity image production from natural-language prompts. Despite the widespread adoption of these tools, the biases, representational quality, and cross-cultural consistency of these models remain poorly understood. Prior research on biases in the synthetic generation of human faces has examined demographic biases, yet there is little research on how emotional prompts influence demographic representation and how models trained in different cultural and linguistic contexts vary in their output distributions. We present a systematic audit of eight state-of-the-art T2I models comprising four models developed by Western organizations and four developed by Chinese institutions, all prompted identically. Using state-of-the-art facial analysis algorithms, we estimate the gender, race, age, and attractiveness levels in the generated faces. To measure the deviations from global population statistics, we apply information-theoretic bias metrics including Kullback-Leibler and Jensen-Shannon divergences. Our findings reveal persistent demographic and emotion-conditioned biases in all models regardless of their country of origin. We discuss implications for fairness, socio-technical harms, governance, and the development of transparent generative systems.

</details>


### [1050] [Synthetic Student Responses: LLM-Extracted Features for IRT Difficulty Parameter Estimation](https://arxiv.org/abs/2602.00034)
*Matias Hoyl*

Main category: cs.CY

TL;DR: 本文探讨不通过学生测试来估计题目难度参数，结合传统语言特征与大语言模型提取的教学见解，模型在未见过的数学题上预测与实际难度参数的皮尔逊相关系数约为0.78。


<details>
  <summary>Details</summary>
Motivation: 传统通过学生预测试确定题目难度的方式资源消耗大，给教师和评估开发者带来障碍，因此研究不通过学生测试准确估计题目难度参数。

Method: 结合传统语言特征和大语言模型提取的教学见解，采用两阶段过程，先训练神经网络预测学生对题目的反应，再从模拟反应模式推导难度参数。

Result: 使用超25万学生对数学题的回答数据集，模型在完全未见过的题目上，预测与实际难度参数的皮尔逊相关系数约为0.78。

Conclusion: 可以通过建模响应过程，结合传统特征与大语言模型提取的见解，在不进行学生测试的情况下较为准确地估计题目难度参数。

Abstract: Educational assessment relies heavily on knowing question difficulty, traditionally determined through resource-intensive pre-testing with students. This creates significant barriers for both classroom teachers and assessment developers. We investigate whether Item Response Theory (IRT) difficulty parameters can be accurately estimated without student testing by modeling the response process and explore the relative contribution of different feature types to prediction accuracy. Our approach combines traditional linguistic features with pedagogical insights extracted using Large Language Models (LLMs), including solution step count, cognitive complexity, and potential misconceptions. We implement a two-stage process: first training a neural network to predict how students would respond to questions, then deriving difficulty parameters from these simulated response patterns. Using a dataset of over 250,000 student responses to mathematics questions, our model achieves a Pearson correlation of approximately 0.78 between predicted and actual difficulty parameters on completely unseen questions.

</details>


### [1051] [LSSF: Safety Alignment for Large Language Models through Low-Rank Safety Subspace Fusion](https://arxiv.org/abs/2602.00038)
*Guanghao Zhou,Panjia Qiu,Cen Chen,Hongyu Li,Mingyuan Chu,Xin Zhang,Jun Zhou*

Main category: cs.CY

TL;DR: 论文指出大语言模型安全机制脆弱且现有安全对齐方法有缺陷，提出LSSF框架，利用低秩特性和新指标恢复微调模型安全对齐，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型安全机制脆弱，现有安全对齐方法依赖微调增加复杂度和计算资源。

Method: 提出LSSF框架，构建低秩投影矩阵提取安全向量主成分，提出安全奇异值熵指标动态计算安全关键秩。

Result: 所提事后对齐方法能有效恢复微调模型安全对齐，对下游任务性能影响小。

Conclusion: LSSF框架可有效解决大语言模型安全对齐问题，且对模型下游任务性能影响小。

Abstract: The safety mechanisms of large language models (LLMs) exhibit notable fragility, as even fine-tuning on datasets without harmful content may still undermine their safety capabilities. Meanwhile, existing safety alignment methods predominantly rely on the fine-tuning process, which inadvertently leads to the increased complexity and computational resources required. To address these issues, we introduce LSSF, a novel safety re-alignment framework with \underline{L}ow-Rank \underline{S}afety \underline{S}ubspace \underline{F}usion. Our proposed method exploits the low-rank characteristics of safety information in LLMs by constructing a low-rank projection matrix to extract the principal components of safety vectors. Notably, this projection matrix represents the low-rank safety subspace of the LLMs, which we have observed to remain stable during fine-tuning process and is isolated from the model's general capabilities. These principal components are used to effectively restore safety alignment when combined with fine-tuned LLMs through linear arithmetic. Additionally, to account for the varying encoding densities of safety information across different layers of LLMs, we propose a novel metric called safety singular value entropy. This metric quantifies the encoding density and allows for the dynamic computation of the safety-critical rank for each safety vector. Extensive experiments demonstrate that our proposed post-hoc alignment method can effectively restore the safety alignment of fine-tuned models with minimal impact on their performance in downstream tasks.

</details>


### [1052] [Student Perceptions of Large Language Models Use in Self-Reflection and Design Critique in Architecture Studio](https://arxiv.org/abs/2602.00041)
*Juan David Salazar Rodriguez,Sam Conrad Joyce,Nachamma Sockalingam,Khoo Eng Tat,Julfendi*

Main category: cs.CY

TL;DR: 研究将大语言模型融入建筑设计工作室反馈机制，分析学生在不同反馈领域对其使用，发现学生将其视为协作‘认知镜子’，在不同场景有不同作用。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型融入建筑设计工作室反馈机制，从生成式生产转向反思性教学。

Method: 采用混合研究方法，以新加坡科技设计大学建筑系学生为研究对象，分析学生在自我反思、同伴互评和教授评审三个反馈领域的看法。

Result: 学生将大语言模型视为协作‘认知镜子’；在自主学习中能助其组织思路，但缺乏情境细节；在同伴互评中是中立调解者；在教授评审中是事后综合引擎。

Conclusion: 大语言模型在建筑设计工作室反馈机制的不同场景中能发挥积极作用，但也存在缺乏情境细节等局限。

Abstract: This study investigates the integration of Large Language Models (LLMs) into the feedback mechanisms of the architectural design studio, shifting the focus from generative production to reflective pedagogy. Employing a mixed-methods approach with architecture students at the Singapore Uni-versity of Technology and Design, the research analyzes student percep-tions across three distinct feedback domains: self-reflection, peer critique, and professor-led reviews. The findings reveal that students engage with LLMs not as authoritative instructors, but as collaborative "cognitive mir-rors" that scaffold critical thinking. In self-directed learning, LLMs help structure thoughts and overcome the "blank page" problem, though they are limited by a lack of contextual nuance. In peer critiques, the technology serves as a neutral mediator, mitigating social anxiety and the "fear of of-fending". Furthermore, in high-stakes professor-led juries, students utilize LLMs primarily as post-critique synthesis engines to manage cognitive overload and translate abstract academic discourse into actionable design iterations.

</details>


### [1053] [When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications](https://arxiv.org/abs/2602.00044)
*Hongliu Cao,Eoin Thomas,Rodrigo Acuna Agost*

Main category: cs.CY

TL;DR: 本文介绍用于大语言模型（LLM）的Persona Brainstorm Audit（PBA）公平性审计方法，检测LLM的输出偏差，分析偏差模式并证明方法可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的偏差输出会强化刻板印象和不公平，因此需要有效的公平性审计方法。

Method: 引入PBA方法，通过开放式角色生成检测偏差，可跨多个社会维度检测，支持纵向跟踪并减少数据泄露风险。

Result: 将PBA应用于12个先进LLM，比较了不同模型、维度和版本的偏差严重程度，发现不同模式和特定差异，能追踪偏差变化。鲁棒性分析表明PBA在不同情况下稳定。

Conclusion: PBA方法具有可靠性，可用于大语言模型的公平性审计。

Abstract: Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended persona generation. Unlike existing methods that rely on fixed identity categories and static benchmarks, PBA uncovers biases across multiple social dimensions while supporting longitudinal tracking and mitigating data leakage risks. Applying PBA to 12 state-of-the-art LLMs, we compare bias severity across models, dimensions, and versions, uncover distinct patterns and lineage-specific variability, and trace how biases attenuate, persist, or resurface across successive generations. Robustness analyses show PBA remains stable under varying sample sizes, role-playing prompts, and debiasing prompts, establishing its reliability for fairness auditing in LLMs.

</details>


### [1054] [How Hyper-Datafication Impacts the Sustainability Costs in Frontier AI](https://arxiv.org/abs/2602.00056)
*Sophia N. Wilson,Sebastian Mair,Mophat Okinyi,Erik B. Dam,Janin Koch,Raghavendra Selvan*

Main category: cs.CY

TL;DR: 本文从可持续性视角审视AI中大规模数据的环境、社会和经济成本，指出超数据化现象及其影响，分析了相关数据集并提出Data PROOFS建议。


<details>
  <summary>Details</summary>
Motivation: 探讨AI中大规模数据带来的环境、社会和经济成本，强调超数据化对前沿AI及其社会影响的重要性。

Method: 分析Hugging Face Hub约55万个数据集，收集肯尼亚数据工作者的定性反馈，借鉴外部数据源。

Result: 超数据化不仅增加资源消耗，还将环境负担、劳动风险和代表性危害系统性地转移到南半球、不稳定数据工作者和代表性不足的文化群体。

Conclusion: 提出Data PROOFS建议以减轻数据成本，希望引发研究界及更广泛群体的讨论。

Abstract: Large-scale data has fuelled the success of frontier artificial intelligence (AI) models over the past decade. This expansion has relied on sustained efforts by large technology corporations to aggregate and curate internet-scale datasets. In this work, we examine the environmental, social, and economic costs of large-scale data in AI through a sustainability lens. We argue that the field is shifting from building models from data to actively creating data for building models. We characterise this transition as hyper-datafication, which marks a critical juncture for the future of frontier AI and its societal impacts. To quantify and contextualise data-related costs, we analyse approximately 550,000 datasets from the Hugging Face Hub, focusing on dataset growth, storage-related energy consumption and carbon footprint, and societal representation using language data. We complement this analysis with qualitative responses from data workers in Kenya to examine the labour involved, including direct employment by big tech corporations and exposure to graphic content. We further draw on external data sources to substantiate our findings by illustrating the global disparity in data centre infrastructure. Our analyses reveal that hyper-datafication does not merely increase resource consumption but systematically redistributes environmental burdens, labour risks, and representational harms toward the Global South, precarious data workers, and under-represented cultures. Thus, we propose Data PROOFS recommendations spanning provenance, resource awareness, ownership, openness, frugality, and standards to mitigate these costs. Our work aims to make visible the often-overlooked costs of data that underpin frontier AI and to stimulate broader debate within the research community and beyond.

</details>


### [1055] [A longitudinal geospatial multimodal dataset of post-discharge frailty, physiology, mobility, and neighborhoods](https://arxiv.org/abs/2602.00060)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: cs.CY

TL;DR: 介绍了纵向地理空间多模态数据集GEOFRAIL，用于收集出院社区体弱老年人数据，还展示了数据技术验证情况。


<details>
  <summary>Details</summary>
Motivation: 老年人衰弱与多种不良状况相关，邻里环境影响康复轨迹，需监测多维度因素，所以收集相关数据。

Method: 用标准化管道和隐私保护空间聚合方法，在出院后8周收集数据，数据组织成互联表格。

Result: 技术验证显示地理空间、传感器衍生和临床测量具有内部一致性，报告了机器学习模型表征康复轨迹的基线性能。

Conclusion: GEOFRAIL数据集在监测老年人出院后康复情况多维度因素方面具有潜力。

Abstract: Frailty in older adults is associated with increased vulnerability to functional decline, reduced mobility, social isolation, and challenges during the transition from hospital to community living. These factors are associated with rehospitalization and may adversely influence recovery. Neighborhood environments can further shape recovery trajectories by affecting mobility opportunities, social engagement, and access to community resources. Multimodal sensing technologies combined with data-driven analytical approaches offer the potential to continuously monitor these multidimensional factors in real-world settings. This Data Descriptor presents GEOFRAIL, a longitudinal geospatial multimodal dataset collected from community-dwelling frail older adults following hospital discharge. The dataset is organized into interconnected tables capturing participant demographics, features derived from multimodal sensors, biweekly clinical assessments of frailty, physical function, and social isolation, and temporal location records linked to neighborhood amenities, crime rates, and census-based socioeconomic indicators. Data were collected over an eight-week post-discharge period using standardized pipelines with privacy-preserving spatial aggregation. Technical validation demonstrates internal consistency across geospatial, sensor-derived, and clinical measures and reports baseline performance of machine learning models for characterizing recovery trajectories.

</details>


### [1056] [Simple Role Assignment is Extraordinarily Effective for Safety Alignment](https://arxiv.org/abs/2602.00061)
*Zhou Ziheng,Jiakun Ding,Zhaowei Zhang,Ruosen Gao,Yingnian Wu,Demetri Terzopoulos,Yipeng Kang,Fangwei Zhong,Junqi Wang*

Main category: cs.CY

TL;DR: 基于原则的对齐缺乏上下文敏感性和完整性，本文提出基于心智理论的角色调节方法，训练无依赖，表现优于基线，可用于AI对齐和LLM评判构建。


<details>
  <summary>Details</summary>
Motivation: 解决基于原则的对齐方法缺乏上下文敏感性和完整性的问题。

Method: 引入无训练管道，包含角色条件生成器和基于角色的迭代评论家进行细化。

Result: 在五个模型家族中，该方法在多个基准测试中始终优于基于原则、思维链等基线方法，在WildJailbreak基准上减少不安全输出，也适用于代理安全任务。

Conclusion: 角色分配是AI对齐和LLM评判构建的强大且可解释的范式。

Abstract: Principle-based alignment often lacks context sensitivity and completeness. Grounded in Theory of Mind, we propose role conditioning as a compact alternative: social roles (e.g., mother, judge) implicitly encode both values and the cognitive schemas required to apply them. We introduce a training-free pipeline featuring a role-conditioned generator and iterative role-based critics for refinement. Across five model families, our approach consistently outperforms principle-based, Chain-of-Thought (CoT) and other baselines across benchmarks. Notably, it reduces unsafe outputs on the WildJailbreak benchmark from 81.4\% to 3.6\% with DeepSeek-V3. Not only for common safety benchmarks, it consistently applies for agentic safety tasks. These results establish role assignment as a powerful, interpretable paradigm for AI alignment and LLM-as-a-Judge construction.

</details>


### [1057] [Responsible Evaluation of AI for Mental Health](https://arxiv.org/abs/2602.00065)
*Hiba Arnaout,Anmol Goel,H. Andrew Schwartz,Steffen T. Eberhardt,Dana Atzil-Slonim,Gavin Doherty,Brian Schwartz,Wolfgang Lutz,Tim Althoff,Munmun De Choudhury,Hamidreza Jamalabadi,Raj Sanjay Shah,Flor Miriam Plaza-del-Arco,Dirk Hovy,Maria Liakata,Iryna Gurevych*

Main category: cs.CY

TL;DR: 针对当前人工智能心理健康评估碎片化问题，提出跨学科评估框架和AI心理健康支持类型分类并举例。


<details>
  <summary>Details</summary>
Motivation: 当前评估人工智能工具用于心理健康的方法碎片化且与实际严重脱节，需重新思考负责任的评估方式。

Method: 分析135篇计算语言学（*CL）近期出版物，识别现有缺陷；提出AI心理健康支持类型分类，并通过案例研究展示其应用。

Result: 发现现有评估存在过度依赖通用指标、心理健康专业人员参与有限、对安全和公平关注不足等问题。

Conclusion: 引入跨学科框架可对人工智能心理健康评估起到结构化作用；提出的AI心理健康支持类型分类可用于不同风险和评估需求。

Abstract: Although artificial intelligence (AI) shows growing promise for mental health care, current approaches to evaluating AI tools in this domain remain fragmented and poorly aligned with clinical practice, social context, and first-hand user experience. This paper argues for a rethinking of responsible evaluation -- what is measured, by whom, and for what purpose -- by introducing an interdisciplinary framework that integrates clinical soundness, social context, and equity, providing a structured basis for evaluation. Through an analysis of 135 recent *CL publications, we identify recurring limitations, including over-reliance on generic metrics that do not capture clinical validity, therapeutic appropriateness, or user experience, limited participation from mental health professionals, and insufficient attention to safety and equity. To address these gaps, we propose a taxonomy of AI mental health support types -- assessment-, intervention-, and information synthesis-oriented -- each with distinct risks and evaluative requirements, and illustrate its use through case studies.

</details>


### [1058] [Adoption and Use of LLMs at an Academic Medical Center](https://arxiv.org/abs/2602.00074)
*Nigam H. Shah,Nerissa Ambers,Abby Pandya,Timothy Keyes,Juan M. Banda,Srikar Nallan,Carlene Lugtu,Artem A. Trotsyuk,Suhana Bedi,Alyssa Unell,Miguel Fuentes,Francois Grolleau,Sneha S. Jain,Jonathan Chen,Devdutta Dash,Danton Char,Aditya Sharma,Duncan McElfresh,Patrick Scully,Vishanthan Kumar,Connor OBrien,Satchi Mouniswamy,Elvis Jones,Krishna Jasti,Gunavathi Mannika Lakshmanan,Sree Ram Akula,Varun Kumar Singh,Ramesh Rajmanickam,Sudhir Sinha,Vicky Zhou,Xu Wang,Bilal Mawji,Joshua Ge,Wencheng Li,Travis Lyons,Jarrod Helzer,Vikas Kakkar,Ramesh Powar,Darren Batara,Cheryl Cordova,William Frederick,Olivia Tang,Phoebe Morgan,April S. Liang,Stephen P. Ma,Shivam Vedak,Dong-han Yao,Akshay Swaminathan,Mehr Kashyap,Brian Ng,Jamie Hellman,Nikesh Kotecha,Christopher Sharp,Gretchen Brown,Christian Lindmark,Anurang Revri,Michael A. Pfeffer*

Main category: cs.CY

TL;DR: 开发ChatEHR系统，实现大语言模型在电子病历中的使用，构建自动化任务，评估其使用情况并取得初步经济效益。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型独立工具在临床文档需求中手动数据输入带来的“工作流摩擦”问题。

Method: 开发ChatEHR系统，支持自动化任务和通过用户界面进行交互使用，开展用户培训并进行监测评估。

Result: 1.5年构建7个自动化任务，1075名用户接受培训，开展23000次会话；生成摘要时存在一定幻觉和不准确情况；首年节省约600万美元。

Conclusion: “从内部构建”策略使医疗系统能通过自主可控的大语言模型平台保持自主性。

Abstract: While large language models (LLMs) can support clinical documentation needs, standalone tools struggle with "workflow friction" from manual data entry. We developed ChatEHR, a system that enables the use of LLMs with the entire patient timeline spanning several years. ChatEHR enables automations - which are static combinations of prompts and data that perform a fixed task - and interactive use in the electronic health record (EHR) via a user interface (UI). The resulting ability to sift through patient medical records for diverse use-cases such as pre-visit chart review, screening for transfer eligibility, monitoring for surgical site infections, and chart abstraction, redefines LLM use as an institutional capability. This system, accessible after user-training, enables continuous monitoring and evaluation of LLM use.
  In 1.5 years, we built 7 automations and 1075 users have trained to become routine users of the UI, engaging in 23,000 sessions in the first 3 months of launch. For automations, being model-agnostic and accessing multiple types of data was essential for matching specific clinical or administrative tasks with the most appropriate LLM. Benchmark-based evaluations proved insufficient for monitoring and evaluation of the UI, requiring new methods to monitor performance. Generation of summaries was the most frequent task in the UI, with an estimated 0.73 hallucinations and 1.60 inaccuracies per generation. The resulting mix of cost savings, time savings, and revenue growth required a value assessment framework to prioritize work as well as quantify the impact of using LLMs. Initial estimates are $6M savings in the first year of use, without quantifying the benefit of the better care offered. Such a "build-from-within" strategy provides an opportunity for health systems to maintain agency via a vendor-agnostic, internally governed LLM platform.

</details>


### [1059] [Standards for trustworthy AI in the European Union: technical rationale, structural challenges, and an implementation path](https://arxiv.org/abs/2602.00078)
*Piercosma Bisconti,Marcello Galisai*

Main category: cs.CY

TL;DR: 本文探讨欧洲AI标准化技术基础，分析挑战并提出可行方案，强调技术标准重要性。


<details>
  <summary>Details</summary>
Motivation: 研究在AI法案下欧洲AI标准化的技术基础，解决AI标准化面临的独特挑战。

Method: 分析AI标准化挑战，提出基于风险管理、可重复性技术检查等的分层方案。

Result: 提出了一套可行的方案，证明技术标准能将法律义务转化为可审计工程实践。

Conclusion: 尽管存在方法困难，技术标准对AI标准化仍至关重要。

Abstract: This white paper examines the technical foundations of European AI standardization under the AI Act. It explains how harmonized standards enable the presumption of conformity mechanism, describes the CEN/CENELEC standardization process, and analyzes why AI poses unique standardization challenges including stochastic behavior, data dependencies, immature evaluation practices, and lifecycle dynamics. The paper argues that AI systems are typically components within larger sociotechnical systems, requiring a layered approach where horizontal standards define process obligations and evidence structures while sectoral profiles specify domain-specific thresholds and acceptance criteria. It proposes a workable scheme based on risk management, reproducible technical checks redefined as stability of measured properties, structured documentation, comprehensive logging, and assurance cases that evolve over the system lifecycle. The paper demonstrates that despite methodological difficulties, technical standards remain essential for translating legal obligations into auditable engineering practice and enabling scalable conformity assessment across providers, assessors, and enforcement authorities

</details>


### [1060] [Generative Artificial Intelligence in Small and Medium Enterprises: Navigating its Promises and Challenges](https://arxiv.org/abs/2602.00091)
*Kumaran Rajaram,Patrick Nicolas Tinguely*

Main category: cs.CY

TL;DR: 本文探讨中小企业如何利用生成式人工智能（GAI）的机遇并克服挑战，介绍部署GAI的战略维度，给出实用建议。


<details>
  <summary>Details</summary>
Motivation: GAI技术发展为中小企业提供能力，帮助其提升竞争力，本文旨在探讨中小企业如何应对GAI的利弊并给出部署路线图。

Method: 引入航海隐喻揭示GAI部署的关键战略维度，包括员工能力、领导力和工作价值观、组织文化、协作以及与第三方的关系。

Result: 明确了GAI部署的关键战略维度。

Conclusion: 给出的实用建议可作为中小企业成功部署GAI的有用指南。

Abstract: The latest technological developments in generative artificial intelligence (GAI) offer powerful capabilities to small and medium enterprises (SMEs), as they facilitate the democratization of both scalability and creativity. Even if they have little technical expertise or financial resources, SMEs can leverage this technology to streamline work processes and unleash innovation, thereby improving their product offerings and long-term competitiveness. This paper discusses how SMEs can navigate both the promises and challenges of GAI and offers a roadmap for deploying GAI. We introduce a sailing metaphor that reveals key strategic dimensions for GAI deployment: competency of employees, effective leadership and work values, organizational culture, collaboration and cooperation, and relationships with third parties. We offer practical recommendations that serve as a useful compass for successfully deploying GAI in SMEs.

</details>


### [1061] [FoundationalASSIST: An Educational Dataset for Foundational Knowledge Tracing and Pedagogical Grounding of LLMs](https://arxiv.org/abs/2602.00070)
*Eamon Worden,Cristina Heffernan,Neil Heffernan,Shashank Sonkar*

Main category: cs.CY

TL;DR: 本文提出首个英语教育数据集FoundationalASSIST，可用于大语言模型在教育领域的研究，评估四个前沿模型发现当前大语言模型在支持个性化学习方面存在显著差距，最后发布该数据集以推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有教育数据集无法满足大语言模型理解学生学习方式的研究需求，需要新的数据集。

Method: 创建FoundationalASSIST数据集，用其评估四个前沿模型在知识追踪和教学基础两个任务上的表现。

Result: 当前大语言模型在知识追踪上仅略超基线，在项目区分度上低于随机水平，仅在判断相对难度上有一定能力。

Conclusion: 大语言模型要可靠支持大规模个性化学习还需重大进展，发布FoundationalASSIST数据集以应对这些基础挑战。

Abstract: Can Large Language Models understand how students learn? As LLMs are deployed for adaptive testing and personalized tutoring, this question becomes urgent -- yet we cannot answer it with existing resources. Current educational datasets provide only question identifiers and binary correctness labels, rendering them opaque to LLMs that reason in natural language. We address this gap with FoundationalASSIST, the first English educational dataset providing the complete information needed for research on LLMs in education: full question text, actual student responses (not just right/wrong), records of which wrong answers students chose, and alignment to Common Core K-12 standards. These 1.7 million interactions from 5,000 students enable research directions that were previously impossible to pursue, from fine-tuning student models to analyzing misconception patterns. To demonstrate the dataset's utility, we evaluate four frontier models (GPT-OSS-120B, Llama-3.3-70B, Qwen3-Next-80B variants) on two complementary task families: Knowledge Tracing, testing whether LLMs can predict student performance on questions, and the exact answer a student will give; and \textbf{Pedagogical Grounding}, testing whether LLMs understand the properties that make assessment items effective. Our evaluation reveals significant gaps in current LLM capabilities. Every model barely achieves a trivial baseline on knowledge tracing. All models fall below random chance on item discrimination, indicating that LLMs do not understand what makes one problem more diagnostic than another. Models do show competence at judging relative difficulty (up to 68.6%), but this partial success only highlights the gaps elsewhere. These results establish that substantial advances are needed before LLMs can reliably support personalized learning at scale. We release FoundationalASSIST to support progress on these foundational challenges.

</details>


### [1062] [DrawSim-PD: Simulating Student Science Drawings to Support NGSS-Aligned Teacher Diagnostic Reasoning](https://arxiv.org/abs/2602.01578)
*Arijit Chakma,Peng He,Honglu Liu,Zeyuan Wang,Tingting Li,Tiffany D. Do,Feng Liu*

Main category: cs.CY

TL;DR: 提出DrawSim-PD框架模拟学生科学绘图以支持教师培训，构建10000个结构化工件，经评估可用并开源。


<details>
  <summary>Details</summary>
Motivation: 培养诊断推理专业知识需多样学生作品练习，但隐私法规禁止大规模共享真实学生作品用于教师专业发展。

Method: 提出DrawSim-PD框架，以能力概况确保生成输出跨模态一致性，用100个NGSS主题构建10000个结构化工件。

Result: K - 12科学教育工作者验证工件与NGSS期望相符（核心项超84%肯定）且利于解读学生思维，也指出极端年级段改进机会。

Conclusion: 开源基础设施以克服视觉评估研究中数据稀缺障碍。

Abstract: Developing expertise in diagnostic reasoning requires practice with diverse student artifacts, yet privacy regulations prohibit sharing authentic student work for teacher professional development (PD) at scale. We present DrawSim-PD, the first generative framework that simulates NGSS-aligned, student-like science drawings exhibiting controllable pedagogical imperfections to support teacher training. Central to our approach are apability profiles--structured cognitive states encoding what students at each performance level can and cannot yet demonstrate. These profiles ensure cross-modal coherence across generated outputs: (i) a student-like drawing, (ii) a first-person reasoning narrative, and (iii) a teacher-facing diagnostic concept map. Using 100 curated NGSS topics spanning K-12, we construct a corpus of 10,000 systematically structured artifacts. Through an expert-based feasibility evaluation, K--12 science educators verified the artifacts' alignment with NGSS expectations (>84% positive on core items) and utility for interpreting student thinking, while identifying refinement opportunities for grade-band extremes. We release this open infrastructure to overcome data scarcity barriers in visual assessment research.

</details>


### [1063] [The Verification Crisis: Expert Perceptions of GenAI Disinformation and the Case for Reproducible Provenance](https://arxiv.org/abs/2602.02100)
*Alexander Loth,Martin Kappes,Marc-Oliver Pahl*

Main category: cs.CY

TL;DR: 本文通过对专家的调查研究GenAI带来的虚假信息问题，指出其威胁、现有应对策略不足并提出建设信息完整性基础设施的建议。


<details>
  <summary>Details</summary>
Motivation: GenAI的发展使虚假信息生产转向自动化大规模操纵，需研究其多模态威胁及应对策略。

Method: 开展了一项针对AI研究人员、政策制定者和虚假信息专家的纵向专家感知调查（N=21）。

Result: 调查显示深度伪造视频有即时“冲击”价值，大规模文本生成在政治领域有系统性风险，专家对技术检测工具存疑，倾向信源标准和监管框架。

Conclusion: GenAI虚假信息研究需可重复方法，当前挑战是测量，建议将信息完整性作为基础设施建设。

Abstract: The growth of Generative Artificial Intelligence (GenAI) has shifted disinformation production from manual fabrication to automated, large-scale manipulation. This article presents findings from the first wave of a longitudinal expert perception survey (N=21) involving AI researchers, policymakers, and disinformation specialists. It examines the perceived severity of multimodal threats -- text, image, audio, and video -- and evaluates current mitigation strategies.
  Results indicate that while deepfake video presents immediate "shock" value, large-scale text generation poses a systemic risk of "epistemic fragmentation" and "synthetic consensus," particularly in the political domain. The survey reveals skepticism about technical detection tools, with experts favoring provenance standards and regulatory frameworks despite implementation barriers.
  GenAI disinformation research requires reproducible methods. The current challenge is measurement: without standardized benchmarks and reproducibility checklists, tracking or countering synthetic media remains difficult. We propose treating information integrity as an infrastructure with rigor in data provenance and methodological reproducibility.

</details>


### [1064] [Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning](https://arxiv.org/abs/2602.01528)
*Qian Wang,Xuandong Zhao,Zirui Zhang,Zhanzhi Lou,Nuo Chen,Dawn Song,Bingsheng He*

Main category: cs.CY

TL;DR: 提出Epistemic Independence Training (EIT)强化学习框架缓解大语言模型认知偏差，在Qwen3 - 4B实验有效且有迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有通过提示或监督微调缓解大语言模型认知偏差的方法无法泛化，因未改变使偏差线索具有预测性的优化目标。

Method: 提出EIT强化学习框架，采用平衡冲突策略，使偏差信号对正确和错误答案支持概率相同，并设计奖励机制惩罚遵循偏差、不奖励偏差一致。

Result: 在Qwen3 - 4B上实验表明EIT能在对抗性偏差下提高准确性和鲁棒性，偏差与事实一致时保持性能，对未见偏差类型有迁移性。

Conclusion: EIT可诱导可迁移的认知独立性，而非特定偏差的启发式方法。

Abstract: Large language models (LLMs) increasingly serve as automated judges, yet they remain susceptible to cognitive biases -- often altering their reasoning when faced with spurious prompt-level cues such as consensus claims or authority appeals. Existing mitigations via prompting or supervised fine-tuning fail to generalize, as they modify surface behavior without changing the optimization objective that makes bias cues predictive. To address this gap, we propose Epistemic Independence Training (EIT), a reinforcement learning framework grounded in a key principle: to learn independence, bias cues must be made non-predictive of reward. EIT operationalizes this through a balanced conflict strategy where bias signals are equally likely to support correct and incorrect answers, combined with a reward design that penalizes bias-following without rewarding bias agreement. Experiments on Qwen3-4B demonstrate that EIT improves both accuracy and robustness under adversarial biases, while preserving performance when bias aligns with truth. Notably, models trained only on bandwagon bias generalize to unseen bias types such as authority and distraction, indicating that EIT induces transferable epistemic independence rather than bias-specific heuristics. Code and data are available at https://anonymous.4open.science/r/bias-mitigation-with-rl-BC47.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [1065] [Fostering Data Collaboration in Digital Transportation Marketplaces: The Role of Privacy-Preserving Mechanisms](https://arxiv.org/abs/2602.01804)
*Qiqing Wang,Haokun Yu,Kaidi Yang*

Main category: eess.SY

TL;DR: 本文研究隐私保护机制如何促进市政当局与出行服务提供商的数据协作，提出博弈论框架，数值研究表明降低数据质量期望可激励自愿数据共享，为政策制定者和系统设计者提供见解。


<details>
  <summary>Details</summary>
Motivation: 数据协作虽能带来交通系统效益，但会引发隐私担忧并降低数据共享意愿，可能导致协作失败，因此需研究隐私保护机制促进数据协作。

Method: 提出博弈论框架，考虑基于扰动的隐私保护机制研究交通利益相关者的数据共享。

Result: 数值研究表明，降低数据质量期望可激励自愿数据共享，提高市政当局和出行服务提供商的交通相关福利。

Conclusion: 研究结果为政策制定者和系统设计者提供了关于隐私保护技术如何打破数据孤岛、促进协作且注重隐私的交通系统的可操作见解。

Abstract: Data collaboration between municipal authorities (MA) and mobility providers (MPs) has brought tremendous benefits to transportation systems in the era of big data. Engaging in collaboration can improve the service operations (e.g., reduced delay) of these data owners, however, it can also raise privacy concerns and discourage data-sharing willingness. Specifically, data owners may be concerned that the shared data may leak sensitive information about their customers' mobility patterns or business secrets, resulting in the failure of collaboration. This paper investigates how privacy-preserving mechanisms can foster data collaboration in such settings. We propose a game-theoretic framework to investigate data-sharing among transportation stakeholders, especially considering perturbation-based privacy-preserving mechanisms. Numerical studies demonstrate that lower data quality expectations can incentivize voluntary data sharing, improving transport-related welfare for both MAs and MPs. Our findings provide actionable insights for policymakers and system designers on how privacy-preserving technologies can help bridge data silos and promote collaborative, privacy-aware transportation systems.

</details>


### [1066] [Harnessing Flexible Spatial and Temporal Data Center Workloads for Grid Regulation Services](https://arxiv.org/abs/2602.01508)
*Yingrui Fan,Junbo Zhao*

Main category: eess.SY

TL;DR: 提出统一的日前联合优化框架，兼顾数据中心工作负载分配和调节容量承诺，案例表明该框架能降低成本、增加调节容量并优化收益风险权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法将工作负载调度和调节容量投标分开，忽略排队动态和时空调度决策对实时调节的影响，导致承诺的调节不可行或短暂。

Method: 构建时空网络模型，引入基于交互式负载预测的瞬时功率灵活性机会约束，应用风险价值队列状态约束。

Result: 在改进的IEEE 68母线系统上的案例研究显示，该框架降低了系统运营成本，使调节容量更可行，实现了更好的收益 - 风险权衡。

Conclusion: 所提出的统一联合优化框架优于独立优化调度和调节的策略。

Abstract: Data centers (DCs) are increasingly recognized as flexible loads that can support grid frequency regulation. Yet, most existing methods treat workload scheduling and regulation capacity bidding separately, overlooking how queueing dynamics and spatial-temporal dispatch decisions affect the ability to sustain real-time regulation. As a result, the committed regulation may become infeasible or short-lived. To address this issue, we propose a unified day-ahead co-optimization framework that jointly decides workload distribution across geographically distributed DCs and regulation capacity commitments. We construct a space-time network model to capture workload migration costs, latency requirements, and heterogeneous resource limits. To ensure that the committed regulation remains deliverable, we introduce chance constraints on instantaneous power flexibility based on interactive load forecasts, and apply Value-at-Risk queue-state constraints to maintain sustainable response under cumulative regulation signals. Case studies on a modified IEEE 68-bus system using real data center traces show that the proposed framework lowers system operating costs, enables more viable regulation capacity, and achieves better revenue-risk trade-offs compared to strategies that optimize scheduling and regulation independently.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [1067] [Non-Clashing Teaching in Graphs: Algorithms, Complexity, and Bounds](https://arxiv.org/abs/2602.00657)
*Sujoy Bhore,Liana Khazaliya,Fionn Mc Inerney*

Main category: cs.CC

TL;DR: 文章研究图中闭邻域的正非冲突教学，给出改进算法结果、更强下界和组合上界。


<details>
  <summary>Details</summary>
Motivation: 之前正非冲突教学在图中球的研究已有成果，闭邻域概念类研究广泛且具有一般性，此次研究是在已有工作基础上开展。

Method: 未明确提及具体方法，以研究图中闭邻域的正非冲突教学，对比图中球的相关工作。

Result: 提供改进算法结果，包括针对更一般参数类的 FPT 算法，推导出更强下界，获得更广图类的组合上界。

Conclusion: 未明确提及，从内容看对图中闭邻域正非冲突教学有积极研究成果。

Abstract: Kirkpatrick et al. [ALT 2019] and Fallat et al. [JMLR 2023] introduced non-clashing teaching and proved that it is the most efficient batch machine teaching model satisfying the collusion-avoidance benchmark established in the seminal work of Goldman and Mathias [COLT 1993]. Recently, (positive) non-clashing teaching was thoroughly studied for balls in graphs, yielding numerous algorithmic and combinatorial results. In particular, Chalopin et al. [COLT 2024] and Ganian et al. [ICLR 2025] gave an almost complete picture of the complexity landscape of the positive variant, showing that it is tractable only for restricted graph classes due to the non-trivial nature of the problem and concept class.
  In this work, we consider (positive) non-clashing teaching for closed neighborhoods in graphs. This concept class is not only extensively studied in various related contexts, but it also exhibits broad generality, as any finite binary concept class can be equivalently represented by a set of closed neighborhoods in a graph. In comparison to the works on balls in graphs, we provide improved algorithmic results, notably including FPT algorithms for more general classes of parameters, and we complement these results by deriving stronger lower bounds. Lastly, we obtain combinatorial upper bounds for wider classes of graphs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1068] [SpeechLess: Micro-utterance with Personalized Spatial Memory-aware Assistant in Everyday Augmented Reality](https://arxiv.org/abs/2602.00793)
*Yoonsang Kim,Devshree Jadeja,Divyansh Pradhan,Yalong Yang,Arie Kaufman*

Main category: cs.HC

TL;DR: 提出基于个性化空间记忆的可穿戴AR助手SpeechLess，减少用户语音表达，评估显示能改善信息获取等。


<details>
  <summary>Details</summary>
Motivation: 公共场合对可穿戴AR助手大声说话尴尬，重复表达请求费力，且通过研究发现用户对公开语音使用不适、对重复语音不满及硬件限制等问题。

Method: 结合先前交互与多模态个人上下文形成空间记忆，从用户不完整查询中推断缺失意图维度，设计SpeechLess并通过实验室和实地研究评估。

Result: 有调节的基于语音的交互能改善日常信息获取、减少表达努力，支持社交可接受的使用，且不显著降低可用性和意图解析准确性。

Conclusion: SpeechLess能帮助用户“少说话”，满足信息需求，在不同日常环境中有良好表现。

Abstract: Speaking aloud to a wearable AR assistant in public can be socially awkward, and re-articulating the same requests every day creates unnecessary effort. We present SpeechLess, a wearable AR assistant that introduces a speech-based intent granularity control paradigm grounded in personalized spatial memory. SpeechLess helps users "speak less," while still obtaining the information they need, and supports gradual explicitation of intent when more complex expression is required. SpeechLess binds prior interactions to multimodal personal context-space, time, activity, and referents-to form spatial memories, and leverages them to extrapolate missing intent dimensions from under-specified user queries. This enables users to dynamically adjust how explicitly they express their informational needs, from full-utterance to micro/zero-utterance interaction. We motivate our design through a week-long formative study using a commercial smart glasses platform, revealing discomfort with public voice use, frustration with repetitive speech, and hardware constraints. Building on these insights, we design SpeechLess, and evaluate it through controlled lab and in-the-wild studies. Our results indicate that regulated speech-based interaction, can improve everyday information access, reduce articulation effort, and support socially acceptable use without substantially degrading perceived usability or intent resolution accuracy across diverse everyday environments.

</details>


### [1069] [From Junior to Senior: Allocating Agency and Navigating Professional Growth in Agentic AI-Mediated Software Engineering](https://arxiv.org/abs/2602.00496)
*Dana Feng,Bhada Yun,April Wang*

Main category: cs.HC

TL;DR: 研究通过混合方法研究初级和高级工程师使用代理AI情况，发现软件工程中的能动性受组织政策约束，高级开发者经验有助于指导初级开发者，并提出三项实践建议。


<details>
  <summary>Details</summary>
Motivation: 探究AI对不同经验工程师在工作和职业发展中能动性的影响。

Method: 采用三阶段混合方法研究，包括结合Delphi过程、AI辅助调试任务和盲审初级工程师提示历史。

Result: 软件工程中的能动性主要受组织政策约束，高级开发者能通过详细委派保持控制，新手在过度依赖和谨慎回避间挣扎，高级开发者可指导新手。

Conclusion: 提出三项聚焦于在软件工程中保留能动性的实践，以应对AI日益自主的情况。

Abstract: Juniors enter as AI-natives, seniors adapted mid-career. AI is not just changing how engineers code-it is reshaping who holds agency across work and professional growth. We contribute junior-senior accounts on their usage of agentic AI through a three-phase mixed-methods study: ACTA combined with a Delphi process with 5 seniors, an AI-assisted debugging task with 10 juniors, and blind reviews of junior prompt histories by 5 more seniors. We found that agency in software engineering is primarily constrained by organizational policies rather than individual preferences, with experienced developers maintaining control through detailed delegation while novices struggle between over-reliance and cautious avoidance. Seniors leverage pre-AI foundational instincts to steer modern tools and possess valuable perspectives for mentoring juniors in their early AI-encouraged career development. From synthesis of results, we suggest three practices that focus on preserving agency in software engineering for coding, learning, and mentorship, especially as AI grows increasingly autonomous.

</details>


### [1070] [The Algorithmic Self-Portrait: Deconstructing Memory in ChatGPT](https://arxiv.org/abs/2602.01450)
*Abhisek Dash,Soumi Das,Elisabeth Kirsten,Qinyuan Wu,Sai Keerthana Karnam,Krishna P. Gummadi,Thorsten Holz,Muhammad Bilal Zafar,Savvas Zannettou*

Main category: cs.HC

TL;DR: 分析80位ChatGPT用户的2050条记忆条目，揭示记忆创建问题并提出Attribution Shield框架保护信息。


<details>
  <summary>Details</summary>
Motivation: 解决对话式AI系统记忆创建过程不透明，引发的数据敏感、用户能动性和画像保真度等研究空白问题。

Method: 分析80位真实世界ChatGPT用户的2050条记忆条目。

Result: 发现96%记忆由系统单边创建；28%含GDPR定义的个人数据，52%含心理洞察；84%记忆基于用户上下文。

Conclusion: 引入Attribution Shield框架，可预测推理、警示敏感记忆推理并建议查询重构，保护个人信息且不牺牲效用。

Abstract: To enable personalized and context-aware interactions, conversational AI systems have introduced a new mechanism: Memory. Memory creates what we refer to as the Algorithmic Self-portrait - a new form of personalization derived from users' self-disclosed information divulged within private conversations. While memory enables more coherent exchanges, the underlying processes of memory creation remain opaque, raising critical questions about data sensitivity, user agency, and the fidelity of the resulting portrait.
  To bridge this research gap, we analyze 2,050 memory entries from 80 real-world ChatGPT users. Our analyses reveal three key findings: (1) A striking 96% of memories in our dataset are created unilaterally by the conversational system, potentially shifting agency away from the user; (2) Memories, in our dataset, contain a rich mix of GDPR-defined personal data (in 28% memories) along with psychological insights about participants (in 52% memories); and (3)~A significant majority of the memories (84%) are directly grounded in user context, indicating faithful representation of the conversations. Finally, we introduce a framework-Attribution Shield-that anticipates these inferences, alerts about potentially sensitive memory inferences, and suggests query reformulations to protect personal information without sacrificing utility.

</details>


### [1071] [Intelligent Reasoning Cues: A Framework and Case Study of the Roles of AI Information in Complex Decisions](https://arxiv.org/abs/2602.00259)
*Venkatesh Sivaraman,Eric P. Mason,Mengfan Ellen Li,Jessica Tong,Andrew J. King,Jeremy M. Kahn,Adam Perer*

Main category: cs.HC

TL;DR: 本文重新将AI界面视为智能推理线索集合，研究八种推理线索在临床决策中的作用，为设计提供依据并给出线索使用建议。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助决策理论聚焦校准对AI建议的依赖，未明确不同系统设计对推理过程的影响，本文旨在填补这一空白。

Method: 将AI界面视为智能推理线索集合，通过对六个团队的情境调查和25位医生的有声思维研究，探索八种推理线索在重症监护治疗脓毒症患者决策中的作用。

Result: 发现推理线索有不同的影响模式，可直接为设计提供信息。

Conclusion: 推理线索应优先处理高可变性和自主性的任务，适应不断变化的决策需求，为复杂病例提供互补、严谨的见解。

Abstract: Artificial intelligence (AI)-based decision support systems can be highly accurate yet still fail to support users or improve decisions. Existing theories of AI-assisted decision-making focus on calibrating reliance on AI advice, leaving it unclear how different system designs might influence the reasoning processes underneath. We address this gap by reconsidering AI interfaces as collections of intelligent reasoning cues: discrete pieces of AI information that can individually influence decision-making. We then explore the roles of eight types of reasoning cues in a high-stakes clinical decision (treating patients with sepsis in intensive care). Through contextual inquiries with six teams and a think-aloud study with 25 physicians, we find that reasoning cues have distinct patterns of influence that can directly inform design. Our results also suggest that reasoning cues should prioritize tasks with high variability and discretion, adapt to ensure compatibility with evolving decision needs, and provide complementary, rigorous insights on complex cases.

</details>


### [1072] [A Conditional Companion: Lived Experiences of People with Mental Health Disorders Using LLMs](https://arxiv.org/abs/2602.00402)
*Aditya Kumar Purohit,Hendrik Heuer*

Main category: cs.HC

TL;DR: 研究对20名英国有心理健康问题且使用大语言模型寻求支持的人进行访谈，发现人们有条件使用模型，指出其适用范围并提出设计与治理方向。


<details>
  <summary>Details</summary>
Motivation: 了解有心理健康挑战的人如何使用大语言模型，评估其有用性，发掘设计机会。

Method: 对英国有心理健康状况且使用过LLMs进行心理健康支持的人进行20次半结构化访谈，采用反思性主题分析。

Result: 参与者有条件、情境化地使用LLMs，适用于轻中度困扰，不适用于危机、创伤和复杂社会情感情况。

Conclusion: 提供了LLMs用于心理健康的实证见解，强调设定边界的重要性，提出负责任嵌入护理生态系统的设计和治理方向。

Abstract: Large Language Models (LLMs) are increasingly used for mental health support, yet little is known about how people with mental health challenges engage with them, how they evaluate their usefulness, and what design opportunities they envision. We conducted 20 semi-structured interviews with people in the UK who live with mental health conditions and have used LLMs for mental health support. Through reflexive thematic analysis, we found that participants engaged with LLMs in conditional and situational ways: for immediacy, the desire for non-judgement, self-paced disclosure, cognitive reframing, and relational engagement. Simultaneously, participants articulated clear boundaries informed by prior therapeutic experience: LLMs were effective for mild-to-moderate distress but inadequate for crises, trauma, and complex social-emotional situations. We contribute empirical insights into the lived use of LLMs for mental health, highlight boundary-setting as central to their safe role, and propose design and governance directions for embedding them responsibly within care ecosystem.

</details>


### [1073] [Augmenting Clinical Decision-Making with an Interactive and Interpretable AI Copilot: A Real-World User Study with Clinicians in Nephrology and Obstetrics](https://arxiv.org/abs/2602.00726)
*Yinghao Zhu,Dehao Sui,Zixiang Wang,Xuning Hu,Lei Gu,Yifan Qi,Tianchen Wu,Ling Wang,Yuan Wei,Wen Tang,Zhihan Cui,Yasha Wang,Lequan Yu,Ewen M Harrison,Junyi Gao,Liantao Ma*

Main category: cs.HC

TL;DR: 提出交互式可解释AI助手AICare用于临床决策，研究表明其能降低认知负担，为设计透明AI系统提供启示。


<details>
  <summary>Details</summary>
Motivation: 解决临床医生对不透明AI的怀疑，促进AI在高风险医疗中的应用。

Method: 分析纵向电子健康记录，用客观指标、主观评估和半结构化访谈对16名不同科室临床医生进行组内平衡研究。

Result: AICare降低认知负担，不同专业水平医生互动策略有差异。

Conclusion: 为设计作为透明伙伴的AI系统提供设计启示，适应不同推理风格以增强而非取代临床判断。

Abstract: Clinician skepticism toward opaque AI hinders adoption in high-stakes healthcare. We present AICare, an interactive and interpretable AI copilot for collaborative clinical decision-making. By analyzing longitudinal electronic health records, AICare grounds dynamic risk predictions in scrutable visualizations and LLM-driven diagnostic recommendations. Through a within-subjects counterbalanced study with 16 clinicians across nephrology and obstetrics, we comprehensively evaluated AICare using objective measures (task completion time and error rate), subjective assessments (NASA-TLX, SUS, and confidence ratings), and semi-structured interviews. Our findings indicate AICare's reduced cognitive workload. Beyond performance metrics, qualitative analysis reveals that trust is actively constructed through verification, with interaction strategies diverging by expertise: junior clinicians used the system as cognitive scaffolding to structure their analysis, while experts engaged in adversarial verification to challenge the AI's logic. This work offers design implications for creating AI systems that function as transparent partners, accommodating diverse reasoning styles to augment rather than replace clinical judgment.

</details>


### [1074] ["If You're Very Clever, No One Knows You've Used It": The Social Dynamics of Developing Generative AI Literacy in the Workplace](https://arxiv.org/abs/2602.01386)
*Qing,Xia,Marios Constantinides,Advait Sarkar,Duncan Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 研究知识工作者在工作场所中GenAI素养信念的形成及应用，发现同事知识共享虽支持学习，但隐藏GenAI使用提示会减少学习机会，建议促进开放对话等提升职场AI素养。


<details>
  <summary>Details</summary>
Motivation: 现有AI素养研究缺乏对知识工作者GenAI素养信念形成及应用的实证洞察，为填补此空白开展研究。

Method: 对多个行业的19名知识工作者进行深度访谈。

Result: 同事知识共享支持学习，但移除GenAI使用提示被视为专业能力验证，减少了知识共享学习机会并破坏透明度。

Conclusion: 为提升职场AI素养，应促进开放对话、提高用户生成知识的可见性，强调协作学习益处。

Abstract: Generative AI (GenAI) tools are rapidly transforming knowledge work, making AI literacy a critical priority for organizations. However, research on AI literacy lacks empirical insight into how knowledge workers' beliefs around GenAI literacy are shaped by the social dynamics of the workplace, and how workers learn to apply GenAI tools in these environments. To address this gap, we conducted in-depth interviews with 19 knowledge workers across multiple sectors to examine how they develop GenAI competencies in real-world professional contexts. We found that, while knowledge sharing from colleagues supported learning, the ability to remove cues indicating GenAI use was perceived as validation of domain expertise. These behaviours ultimately reduced opportunities for learning via knowledge sharing and undermined transparency. To advance workplace AI literacy, we argue for fostering open dialogue, increasing visibility of user-generated knowledge, and greater emphasis on the benefits of collaborative learning for navigating rapid technological developments.

</details>


### [1075] [How well can VLMs rate audio descriptions: A multi-dimensional quantitative assessment framework](https://arxiv.org/abs/2602.01390)
*Lana Do,Gio Jung,Juvenal Francisco Barajas,Andrew Taylor Scott,Shasta Ihorn,Alexander Mario Blum,Vassilis Athitsos,Ilmi Yoon*

Main category: cs.HC

TL;DR: 本文针对数字视频音频描述缺少系统性质量评估的问题，开发评估框架和工作流程，发现VLMs有一定效果但推理能力不如人类，提出混合评估系统方向。


<details>
  <summary>Details</summary>
Motivation: 数字视频音频描述缺乏系统质量评估，现有评估方法存在不足，不清楚如何对全长内容进行质量评估和大规模评估。

Method: 首先根据专业指南并经无障碍专家完善，开发针对不间断全长视频的多维评估框架；然后将框架融入综合方法工作流程，利用项目反应理论评估VLM和人类评估者与专家建立的地面真值的熟练程度。

Result: VLMs能与地面真值评级高度一致，但推理能力不如人类评估者可靠和可行。

Conclusion: 混合评估系统结合VLMs和人工监督有潜力实现可扩展的音频描述质量控制。

Abstract: Digital video is central to communication, education, and entertainment, but without audio description (AD), blind and low-vision audiences are excluded. While crowdsourced platforms and vision-language-models (VLMs) expand AD production, quality is rarely checked systematically. Existing evaluations rely on NLP metrics and short-clip guidelines, leaving questions about what constitutes quality for full-length content and how to assess it at scale. To address these questions, we first developed a multi-dimensional assessment framework for uninterrupted, full-length video, grounded in professional guidelines and refined by accessibility specialists. Second, we integrated this framework into a comprehensive methodological workflow, utilizing Item Response Theory, to assess the proficiency of VLM and human raters against expert-established ground truth. Findings suggest that while VLMs can approximate ground-truth ratings with high alignment, their reasoning was found to be less reliable and actionable than that of human respondents. These insights show the potential of hybrid evaluation systems that leverage VLMs alongside human oversight, offering a path towards scalable AD quality control.

</details>


### [1076] [Draw2Learn: A Human-AI Collaborative Tool for Drawing-Based Science Learning](https://arxiv.org/abs/2602.01494)
*Yuqi Hang*

Main category: cs.HC

TL;DR: 介绍Draw2Learn系统，探索AI在绘图学习中的支持作用，获积极反馈并贡献设计框架。


<details>
  <summary>Details</summary>
Motivation: 绘图支持学习，但大规模及时反馈有挑战，探索AI在绘图学习中的支持作用。

Method: 将学习原则转化为具体交互模式，AI生成绘图任务、提供视觉支架、监测进度和提供反馈，同时收集用户反馈。

Result: 用户对系统的可用性、有用性和用户体验给予积极评价，强调AI支架价值和学习者自主性。

Conclusion: 为生成性学习中面向队友的AI提供了设计框架，并确定了未来研究的关键考虑因素。

Abstract: Drawing supports learning by externalizing mental models, but providing timely feedback at scale remains challenging. We present Draw2Learn, a system that explores how AI can act as a supportive teammate during drawing-based learning. The design translates learning principles into concrete interaction patterns: AI generates structured drawing quests, provides optional visual scaffolds, monitors progress, and delivers multidimensional feedback. We collected formative user feedback during system development and open-ended comments. Feedback showed positive ratings for usability, usefulness, and user experience, with themes highlighting AI scaffolding value and learner autonomy. This work contributes a design framework for teammate-oriented AI in generative learning and identifies key considerations for future research.

</details>


### [1077] [Toward a Machine Bertin: Why Visualization Needs Design Principles for Machine Cognition](https://arxiv.org/abs/2602.01527)
*Brian Keith-Norambuena*

Main category: cs.HC

TL;DR: 现有可视化设计知识源于人类视觉研究，不适用于机器，本文呼吁研究面向机器的可视化设计并提出研究议程。


<details>
  <summary>Details</summary>
Motivation: 当前以人类为中心的可视化设计知识无法直接应用于机器，现有绕过视觉的方法未解决根本问题。

Method: 综合VLM基准、视觉推理研究和可视化素养研究的证据，批判性审视现有方法。

Result: 证明人机感知差异是定性的，而非仅为定量差异。

Conclusion: 可视化领域需将面向机器的可视化设计作为独立研究问题，提出研究议程以建立实证基础。

Abstract: Visualization's design knowledge-effectiveness rankings, encoding guidelines, color models, preattentive processing rules -- derives from six decades of psychophysical studies of human vision. Yet vision-language models (VLMs) increasingly consume chart images in automated analysis pipelines, and a growing body of benchmark evidence indicates that this human-centered knowledge base does not straightforwardly transfer to machine audiences. Machines exhibit different encoding performance patterns, process images through patch-based tokenization rather than holistic perception, and fail on design patterns that pose no difficulty for humans-while occasionally succeeding where humans struggle. Current approaches address this gap primarily by bypassing vision entirely, converting charts to data tables or structured text. We argue that this response forecloses a more fundamental question: what visual representations would actually serve machine cognition well? This paper makes the case that the visualization field needs to investigate machine-oriented visual design as a distinct research problem. We synthesize evidence from VLM benchmarks, visual reasoning research, and visualization literacy studies to show that the human-machine perceptual divergence is qualitative, not merely quantitative, and critically examine the prevailing bypassing approach. We propose a conceptual distinction between human-oriented and machine-oriented visualization-not as an engineering architecture but as a recognition that different audiences may require fundamentally different design foundations-and outline a research agenda for developing the empirical foundations the field currently lacks: the beginnings of a "machine Bertin" to complement the human-centered knowledge the field already possesses.

</details>


### [1078] [AI-Assisted Adaptive Rendering for High-Frequency Security Telemetry in Web Interfaces](https://arxiv.org/abs/2602.01671)
*Mona Rajhans*

Main category: cs.HC

TL;DR: 提出AI辅助自适应渲染框架处理高频遥测数据，实验显示减少渲染开销并保持实时响应感知。


<details>
  <summary>Details</summary>
Motivation: 传统渲染技术在处理每秒超十万个事件时失效，导致UI冻结、丢帧或数据陈旧。

Method: 提出AI辅助自适应渲染框架，用行为驱动启发式和轻量级设备端机器学习模型动态调节视觉更新频率、对语义相关事件优先级排序、选择性聚合低优先级数据。

Result: 实验验证渲染开销降低45 - 60%。

Conclusion: 该框架能在降低渲染开销的同时，保持分析师对实时响应的感知。

Abstract: Modern cybersecurity platforms must process and display high-frequency telemetry such as network logs, endpoint events, alerts, and policy changes in real time. Traditional rendering techniques based on static pagination or fixed polling intervals fail under volume conditions exceeding hundreds of thousands of events per second, leading to UI freezes, dropped frames, or stale data. This paper presents an AI-assisted adaptive rendering framework that dynamically regulates visual update frequency, prioritizes semantically relevant events, and selectively aggregates lower-priority data using behavior-driven heuristics and lightweight on-device machine learning models. Experimental validation demonstrates a 45-60 percent reduction in rendering overhead while maintaining analyst perception of real-time responsiveness.

</details>


### [1079] [See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers](https://arxiv.org/abs/2602.02063)
*Ding Xia,Xinyue Gui,Mark Colley,Fan Gao,Zhongyi Zhou,Dongyuan Li,Renhe Jiang,Takeo Igarashi*

Main category: cs.HC

TL;DR: 提出免人工闭环框架See2Refine，用VLM感知评估改进基于LLM的eHMI动作设计器，在多方面表现优于对比方法，证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有eHMI研究依赖开发者设计消息 - 动作对，难以适应多样动态交通场景，基于LLM的动作设计器缺乏感知验证且改进依赖固定提示或昂贵人工标注反馈。

Method: 提出See2Refine框架，用VLM感知评估作为自动视觉反馈，迭代修正LLM动作设计器输出。

Result: 在三种eHMI模式和多个LLM模型规模下，该框架在VLM指标和人体评估中均优于仅使用提示的LLM设计器和手动指定基线，改进效果跨模式通用，VLM评估与人类偏好一致。

Conclusion: See2Refine框架具有鲁棒性和有效性，可用于可扩展的动作设计。

Abstract: Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult to adapt to diverse and dynamic traffic contexts. A promising alternative is to use Large Language Models (LLMs) as action designers that generate context-conditioned eHMI actions, yet such designers lack perceptual verification and typically depend on fixed prompts or costly human-annotated feedback for improvement. We present See2Refine, a human-free, closed-loop framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve an LLM-based eHMI action designer. Given a driving context and a candidate eHMI action, the VLM evaluates the perceived appropriateness of the action, and this feedback is used to iteratively revise the designer's outputs, enabling systematic refinement without human supervision. We evaluate our framework across three eHMI modalities (lightbar, eyes, and arm) and multiple LLM model sizes. Across settings, our framework consistently outperforms prompt-only LLM designers and manually specified baselines in both VLM-based metrics and human-subject evaluations. Results further indicate that the improvements generalize across modalities and that VLM evaluations are well aligned with human preferences, supporting the robustness and effectiveness of See2Refine for scalable action design.

</details>


### [1080] [Cost-Aware Bayesian Optimization for Prototyping Interactive Devices](https://arxiv.org/abs/2602.01774)
*Thomas Langerak,Renate Zhang,Ziyuan Wang,Per Ola Kristensson,Antti Oulasvirta*

Main category: cs.HC

TL;DR: 提出成本感知贝叶斯优化扩展方法，考虑多样原型制作成本，能节省成本并提升表现，使贝叶斯优化更适配实际设计项目。


<details>
  <summary>Details</summary>
Motivation: 在迭代设计中决定哪些想法值得制作原型很关键，但原型制作成本差异大，难以抉择，这会阻碍设计师探索设计空间。

Method: 提出成本感知贝叶斯优化的扩展方法，以设计师估计成本引导采样，仅对获取函数进行最小修改。

Result: 技术评估中该方法成本约为不考虑成本基线的 70%，效用相当；严格预算下表现超基线三倍；实际操纵杆设计任务研究也显示出类似优势。

Conclusion: 考虑原型制作成本可使贝叶斯优化更适用于实际设计项目。

Abstract: Deciding which idea is worth prototyping is a central concern in iterative design. A prototype should be produced when the expected improvement is high and the cost is low. However, this is hard to decide, because costs can vary drastically: a simple parameter tweak may take seconds, while fabricating hardware consumes material and energy. Such asymmetries, can discourage a designer from exploring the design space. In this paper, we present an extension of cost-aware Bayesian optimization to account for diverse prototyping costs. The method builds on the power of Bayesian optimization and requires only a minimal modification to the acquisition function. The key idea is to use designer-estimated costs to guide sampling toward more cost-effective prototypes. In technical evaluations, the method achieved comparable utility to a cost-agnostic baseline while requiring only ${\approx}70\%$ of the cost; under strict budgets, it outperformed the baseline threefold. A within-subjects study with 12 participants in a realistic joystick design task demonstrated similar benefits. These results show that accounting for prototyping costs can make Bayesian optimization more compatible with real-world design projects.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [1081] [Cross-Modal Binary Attention: An Energy-Efficient Fusion Framework for Audio-Visual Learning](https://arxiv.org/abs/2602.00701)
*Mohamed Saleh,Zahra Ahmadi*

Main category: cs.MM

TL;DR: 提出CMQKA交叉模态融合机制和SNNergy框架，以线性复杂度实现层次化多模态融合，在多个视听基准上表现出色，提高能源效率。


<details>
  <summary>Details</summary>
Motivation: 现有视听融合方法要么计算复杂度高，要么无法有效提取跨模态信息，需要一种能捕捉复杂跨模态依赖且计算可扩展的机制。

Method: 引入CMQKA机制，采用双向跨模态Query - Key注意力和可学习残差融合；构建SNNergy框架，采用分层架构和事件驱动二进制尖峰操作。

Result: SNNergy在CREMA - D、AVE和UrbanSound8K - AV等视听基准上取得新的最优结果，显著超越现有多模态融合基线。

Conclusion: 该框架通过可扩展的融合机制实现了层次化跨模态集成，为实际视听智能系统提供了实用的能源效率。

Abstract: Effective multimodal fusion requires mechanisms that can capture complex cross-modal dependencies while remaining computationally scalable for real-world deployment. Existing audio-visual fusion approaches face a fundamental trade-off: attention-based methods effectively model cross-modal relationships but incur quadratic computational complexity that prevents hierarchical, multi-scale architectures, while efficient fusion strategies rely on simplistic concatenation that fails to extract complementary cross-modal information. We introduce CMQKA, a novel cross-modal fusion mechanism that achieves linear O(N) complexity through efficient binary operations, enabling scalable hierarchical fusion previously infeasible with conventional attention. CMQKA employs bidirectional cross-modal Query-Key attention to extract complementary spatiotemporal features and uses learnable residual fusion to preserve modality-specific characteristics while enriching representations with cross-modal information. Building upon CMQKA, we present SNNergy, an energy-efficient multimodal fusion framework with a hierarchical architecture that processes inputs through progressively decreasing spatial resolutions and increasing semantic abstraction. This multi-scale fusion capability allows the framework to capture both local patterns and global context across modalities. Implemented with event-driven binary spike operations, SNNergy achieves remarkable energy efficiency while maintaining fusion effectiveness and establishing new state-of-the-art results on challenging audio-visual benchmarks, including CREMA-D, AVE, and UrbanSound8K-AV, significantly outperforming existing multimodal fusion baselines. Our framework advances multimodal fusion by introducing a scalable fusion mechanism that enables hierarchical cross-modal integration with practical energy efficiency for real-world audio-visual intelligence systems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [1082] [FluxNet: Learning Capacity-Constrained Local Transport Operators for Conservative and Bounded PDE Surrogates](https://arxiv.org/abs/2602.01941)
*Zishuo Lan,Junjie Li,Lei Wang,Jincheng Wang*

Main category: cond-mat.mtrl-sci

TL;DR: 提出学习保守传输算子的框架FluxNet用于数据驱动的PDE模拟，在多场景验证有效。


<details>
  <summary>Details</summary>
Motivation: 自回归学习时间步长算子在处理守恒律时，长时滚动预测因违反全局守恒和状态边界约束而不稳定，直接的下一状态回归难以实施这些耦合约束。

Method: 受格子玻尔兹曼风格的离散速度传输表示启发，模型输出局部传输算子通过邻域交换更新单元，保证离散守恒；对有界量，在容量约束可行集内参数化传输。

Result: 在1D对流扩散、2D浅水方程、1D交通流和2D旋节分解验证，浅水方程和交通流中相比基线提高滚动稳定性和物理一致性，在相场旋节分解中可实现大时间步长和长程传输。

Conclusion: 该框架能有效解决数据驱动PDE模拟的稳定性问题，且在特定场景有加速和准确模拟的优势。

Abstract: Autoregressive learning of time-stepping operators offers an effective approach to data-driven PDE simulation on grids. For conservation laws, however, long-horizon rollouts are often destabilized when learned updates violate global conservation and, in many applications, additional state bounds such as nonnegative mass and densities or concentrations constrained to [0,1]. Enforcing these coupled constraints via direct next-state regression remains difficult. We introduce a framework for learning conservative transport operators on regular grids, inspired by lattice Boltzmann-style discrete-velocity transport representations. Instead of predicting the next state, the model outputs local transport operators that update cells through neighborhood exchanges, guaranteeing discrete conservation by construction. For bounded quantities, we parameterize transport within a capacity-constrained feasible set, enforcing bounds structurally rather than by post-hoc clipping. We validate FluxNet on 1D convection-diffusion, 2D shallow water equations, 1D traffic flow, and 2D spinodal decomposition. Experiments on shallow-water equations and traffic flow show improved rollout stability and physical consistency over strong baselines. On phase-field spinodal decomposition, the method enables large time-steps with long-range transport, accelerating simulation while preserving microstructure evolution in both pointwise and statistical measures.

</details>


### [1083] [Towards knowledge-based workflows: a semantic approach to atomistic simulations for mechanical and thermodynamic properties](https://arxiv.org/abs/2602.01358)
*Abril Azocar Guzman,Hoang-Thien Luu,Sarath Menon,Tilmann Hickel,Nina Merkert,Stefan Sandfeld*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出可复用的原子级工作流，解决现有分子动力学模拟问题，提供AI可用数据并为相关模拟建立蓝图


<details>
  <summary>Details</summary>
Motivation: 当前分子动力学模拟依赖碎片化脚本，元数据不一致、溯源性有限，阻碍可重复性、互操作性和重用，需解决这些问题

Method: 提出结合元数据注释与应用本体的可复用原子级工作流，实现自动溯源和符合FAIR原则的数据输出

Result: 工作流涵盖关键力学和热力学量，验证了结构 - 属性关系，可在不同原子间势和材料中复用

Conclusion: 该方法提供AI可用模拟数据，支持新兴代理AI工作流，为基于知识的力学和热力学模拟建立通用蓝图

Abstract: Mechanical and thermodynamic properties, including the influence of crystal defects, are critical for evaluating materials in engineering applications. Molecular dynamics simulations provide valuable insight into these mechanisms at the atomic scale. However, current practice often relies on fragmented scripts with inconsistent metadata and limited provenance, which hinders reproducibility, interoperability, and reuse. FAIR data principles and workflow-based approaches offer a path to address these limitations. We present reusable atomistic workflows that incorporate metadata annotation aligned with application ontologies, enabling automatic provenance capture and FAIR-compliant data outputs. The workflows cover key mechanical and thermodynamic quantities, including equation of state, elastic tensors, mechanical loading, thermal properties, defect formation energies, and nanoindentation. We demonstrate validation of structure-property relations such as the Hall-Petch effect and show that the workflows can be reused across different interatomic potentials and materials within a coherent semantic framework. The approach provides AI-ready simulation data, supports emerging agentic AI workflows, and establishes a generalizable blueprint for knowledge-based mechanical and thermodynamic simulations.

</details>


### [1084] [Towards Agentic Intelligence for Materials Science](https://arxiv.org/abs/2602.00169)
*Huan Zhang,Yizhan Li,Wenhao Huang,Ziyu Hou,Yu Song,Xuye Liu,Farshid Effaty,Jinya Jiang,Sifan Wu,Qianggang Ding,Izumi Takahara,Leonard R. MacGillivray,Teruyasu Mizoguchi,Tianshu Yu,Lizi Liao,Yuyu Luo,Yu Rong,Jia Li,Ying Diao,Heng Ji,Bang Liu*

Main category: cond-mat.mtrl-sci

TL;DR: 本文探讨人工智能与材料科学融合，提出以流水线为中心的观点，弥合两领域差距，梳理了相关应用，并绘制了实现自主、安全的大语言模型智能体的路线图。


<details>
  <summary>Details</summary>
Motivation: 人工智能与材料科学的融合带来变革机遇，但需从孤立模型转向能跨发现循环规划、行动和学习的能动系统，以真正加速材料发现。

Method: 提出以流水线为中心的观点，将整个过程视为端到端系统进行优化；提供整合视角统一术语、评估和工作流程；从 AI 和材料科学两个视角分析该领域；对比被动、反应式方法与能动设计。

Result: 梳理了大语言模型在材料科学各方面的优势和应用，对比不同方法并列出当前贡献。

Conclusion: 绘制了实现自主、安全且能发现新材料的大语言模型智能体的实用路线图。

Abstract: The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.
  To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.

</details>


### [1085] [QUASAR: A Universal Autonomous System for Atomistic Simulation and a Benchmark of Its Capabilities](https://arxiv.org/abs/2602.00185)
*Fengxu Yang,Jack D. Evans*

Main category: cond-mat.mtrl-sci

TL;DR: 本文介绍了自主系统QUASAR，它可跨多种方法自动编排复杂的多尺度原子模拟工作流程，经基准测试表明能作为通用原子推理系统。


<details>
  <summary>Details</summary>
Motivation: 当前集成大语言模型的材料科学代理系统受限于僵化的工具调用方法和窄范围的主体，需新系统促进生产级科学发现。

Method: 引入QUASAR系统，该系统具备自适应规划、高效上下文内存管理和混合知识检索等机制，能自主编排跨多种方法的复杂多尺度工作流程。

Result: 通过一系列三层级任务基准测试，证明QUASAR可作为通用原子推理系统，也提供了将代理人工智能用于计算化学研究工作流程的初步证据。

Conclusion: QUASAR有作为通用原子推理系统的潜力，同时指出了代理人工智能在计算化学应用中需进一步发展的领域。

Abstract: The integration of large language models (LLMs) into materials science offers a transformative opportunity to streamline computational workflows, yet current agentic systems remain constrained by rigid tool-calling approaches and narrowly scoped agents. In this work, we introduce QUASAR, a universal autonomous system for atomistic simulation designed to facilitate production-grade scientific discovery. QUASAR autonomously orchestrates complex multi-scale workflows across diverse methods, including density functional theory, machine learning potentials, molecular dynamics, and Monte Carlo simulations. The system incorporates robust mechanisms for adaptive planning, context-efficient memory management, and hybrid knowledge retrieval to navigate real-world research scenarios without human intervention. We benchmark QUASAR against a series of three-tiered tasks, progressing from routine tasks to frontier research challenges such as photocatalyst screening and novel material assessment. These results suggest that QUASAR can function as a general atomistic reasoning system rather than a task-specific automation framework. They also provide initial evidence supporting the potential deployment of agentic AI as a component of computational chemistry research workflows, while identifying areas requiring further development.

</details>


### [1086] [Multimodal Machine Learning for Integrating Heterogeneous Analytical Systems](https://arxiv.org/abs/2602.00590)
*Shun Muroga,Hideaki Nakajima,Taiyo Shimizu,Kazufumi Kobashi,Kenji Hata*

Main category: cond-mat.mtrl-sci

TL;DR: 提出多模态机器学习框架用于复杂材料表征，以碳纳米管薄膜为例，融合多源数据，非线性模型预测效果好，框架有物理解释性。


<details>
  <summary>Details</summary>
Motivation: 理解复杂材料结构 - 性能关系需整合多尺度互补测量，提出统一异构分析系统的方法。

Method: 提出多模态机器学习框架，从SEM图像提取形态描述符，融合拉曼、比表面积、表面电阻率等数据，用雷达图和UMAP可视化，训练回归模型。

Result: 多模态特征集训练的非线性回归模型（如XGBoost）预测精度高，特征重要性分析有物理解释。

Conclusion: 所提出的多模态机器学习框架为复杂材料的数据驱动、可解释表征提供通用策略。

Abstract: Understanding structure-property relationships in complex materials requires integrating complementary measurements across multiple length scales. Here we propose an interpretable "multimodal" machine learning framework that unifies heterogeneous analytical systems for end-to-end characterization, demonstrated on carbon nanotube (CNT) films whose properties are highly sensitive to microstructural variations. Quantitative morphology descriptors are extracted from SEM images via binarization, skeletonization, and network analysis, capturing curvature, orientation, intersection density, and void geometry. These SEM-derived features are fused with Raman indicators of crystallinity/defect states, specific surface area from gas adsorption, and electrical surface resistivity. Multi-dimensional visualization using radar plots and UMAP reveals clear clustering of CNT films according to crystallinity and entanglements. Regression models trained on the multimodal feature set show that nonlinear approaches, particularly XGBoost, achieve the best predictive accuracy under leave-one-out cross-validation. Feature-importance analysis further provides physically meaningful interpretations: surface resistivity is primarily governed by junction-to-junction transport length scales, crystallinity/defect-related metrics, and network connectivity, whereas specific surface area is dominated by intersection density and void size. The proposed multimodal machine learning framework offers a general strategy for data-driven, explainable characterization of complex materials.

</details>


### [1087] [AI Meets Plasticity: A Comprehensive Survey](https://arxiv.org/abs/2602.01215)
*Hadi Bakhshan,Sima Farshbaf,Junior Ramirez Machado,Fernando Rastellini Canela,Josep Maria Carbonell*

Main category: cond-mat.mtrl-sci

TL;DR: 本文对人工智能与材料塑性的融合进行全面综述，介绍相关AI方法并构建分类体系，为材料领域人员提供指引。


<details>
  <summary>Details</summary>
Motivation: 人工智能在各学科成为新科研范式，在材料科学中已产生变革性影响，有必要研究其与材料塑性的相互作用。

Method: 从材料科学和AI方法论角度，综述多种AI方法，包括经典机器学习、深度学习、物理信息模型等，并基于AI方法构建分类体系。

Result: 梳理了人工智能在材料塑性领域应用的多种方法，构建了全面且有条理的分类体系。

Conclusion: 本工作为材料领域研究人员和从业者提供清晰路线图，加深对AI在推进材料塑性和表征中作用的物理理解。

Abstract: Artificial intelligence (AI) is rapidly emerging as a new paradigm of scientific discovery, namely data-driven science, across nearly all scientific disciplines. In materials science and engineering, AI has already begun to exert a transformative influence, making it both timely and necessary to examine its interaction with materials plasticity. In this study, we present a holistic survey of the convergence between AI and plasticity, highlighting state-of-the-art AI methodologies employed to discover, construct surrogate models for, and emulate the plastic behavior of materials. From a materials science perspective, we examine cause-and-effect relationships governing plastic deformation, including microstructural characterization and macroscopic responses described through plasticity constitutive models. From the perspective of AI methodology, we review a broad spectrum of applied approaches, ranging from frequentist techniques such as classical machine learning (ML), deep learning (DL), and physics-informed models to probabilistic frameworks that incorporate uncertainty quantification and generative AI methods. These data-driven approaches are discussed in the context of materials characterization and plasticity-related applications. The primary objective of this survey is to develop a comprehensive and well-organized taxonomy grounded in AI methodologies, with particular emphasis on distinguishing critical aspects of these techniques, including model architectures, data requirements, and predictive performance within the specific domain of materials plasticity. By doing so, this work aims to provide a clear road map for researchers and practitioners in the materials community, while offering deeper physical insight and intuition into the role of AI in advancing materials plasticity and characterization, an area of growing importance in the emerging AI-driven era.

</details>


### [1088] [A New Workflow for Materials Discovery Bridging the Gap Between Experimental Databases and Graph Neural Networks](https://arxiv.org/abs/2602.00756)
*Brandon Schoener,Yuting Hu,Pasit Wanlapha,Akshay Rengarajan,Ian Moog,Michael Wang,Peihong Zhang,Jinjun Xiong,Hao Zeng*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出将实验数据库与ICSD的CIF文件对齐的方法，创建可利用先进模型架构的数据库，并通过实验验证该方法提升了磁性材料属性预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 将机器学习用于材料属性预测时面临训练数据严重不足的问题，现有实验数据库大多缺乏完整原子坐标信息，无法支持先进的机器学习架构。

Method: 提出实验数据库与ICSD的CIF文件的对齐过程，创建可利用先进模型架构的数据库，还可利用迁移学习提高预测精度。

Result: 将NEMAD与ICSD对齐，对比模型训练结果，在预测磁性材料的有序温度和磁基态时，平均绝对误差和正确分类率都显著提高。

Conclusion: 所提方法能创建可充分利用先进模型架构的数据库，提升材料属性预测的准确性。

Abstract: Incorporating Machine Learning (ML) into material property prediction has become a crucial step in accelerating materials discovery. A key challenge is the severe lack of training data, as many properties are too complicated to calculate with high-throughput first principles techniques. To address this, recent research has created experimental databases from information extracted from scientific literature. However, most existing experimental databases do not provide full atomic coordinate information, which prevents them from supporting advanced ML architectures such as Graph Neural Networks (GNNs). In this work, we propose to bridge this gap through an alignment process between experimental databases and Crystallographic Information Files (CIF) from the Inorganic Crystal Structure Database (ICSD). Our approach enables the creation of a database that can fully leverage state-of-the-art model architectures for material property prediction. It also opens the door to utilizing transfer learning to improve prediction accuracy. To validate our approach, we align NEMAD with the ICSD and compare models trained on the resulting database to those trained on NEMAD originally. We demonstrate significant improvements in both Mean Absolute Error (MAE) and Correct Classification Rate (CCR) in predicting the ordering temperatures and magnetic ground states of magnetic materials, respectively.

</details>


### [1089] [Robust Machine Learning Framework for Reliable Discovery of High-Performance Half-Heusler Thermoelectrics](https://arxiv.org/abs/2602.01149)
*Shoeb Athar,Adrien Mecibah,Philippe Jund*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究提出用于优值zT预测的稳健工作流程，提升机器学习模型在热电材料发现中的泛化性，明确重要特征并筛选出高zT候选材料。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在热电材料发现中实验泛化性差的问题。

Method: 引入基于PCA的拆分方法处理数据集；结合贝叶斯超参数优化和k-最佳特征过滤，使用三种架构建模；用SISSO符号回归获得物理洞察；用SHAP和SISSO分析确定关键特征；对潜在成分进行高通量筛选。

Result: 确定A位掺杂浓度、A位汽化热和温度是zT的主要驱动因素；筛选出多个高zT候选新成分。

Conclusion: 应关注建立能体现模型泛化性的测试集，强化现有机器学习工作流程中常被忽视的模块，用于数据驱动的下一代热电材料设计。

Abstract: Machine learning (ML) can facilitate efficient thermoelectric (TE) material discovery essential to address the environmental crisis. However, ML models often suffer from poor experimental generalizability despite high metrics. This study presents a robust workflow, applied to the half-Heusler (hH) structural prototype, for figure of merit (zT) prediction, to improve the generalizability of ML models. To resolve challenges in dataset handling and feature filtering, we first introduce a rigorous PCA-based splitting method that ensures training and test sets are unbiased and representative of the full chemical space. We then integrate Bayesian hyperparameter optimization with k-best feature filtering across three architectures-Random Forest, XGBoost, and Neural Networks - while employing SISSO symbolic regression for physical insight and comparison. Using SHAP and SISSO analysis, we identify A-site dopant concentration (xA'), and A-site Heat of Vaporization (HVA) as the primary drivers of zT besides Temperature (T). Finally, a high-throughput screening of approximately 6.6x10^8 potential compositions, filtered by stability constraints, yielded several novel high-zT candidates. Breaking from the traditional focus of improving test RMSE/R^2 values of the models, this work shifts the attention on establishing the test set a true proxy for model generalizability and strengthening the often neglected modules of the existing ML workflows for the data-driven design of next-generation thermoelectric materials.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1090] [Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL](https://arxiv.org/abs/2602.02236)
*Julian Lemmel,Felix Resch,Mónika Farsang,Ramin Hasani,Daniela Rus,Radu Grosu*

Main category: cs.RO

TL;DR: 使用RTRRL算法微调预训练策略，与LRLC - RNN结合，在模拟和现实任务中提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 预训练策略用于实际应用有挑战，面对环境变化等情况性能退化，需提升自动驾驶性能。

Method: 采用实时循环强化学习（RTRRL）算法微调预训练策略，并将其与Liquid - Resistance Liquid - Capacitance RNN结合。

Result: 在模拟的CarRacing环境和配备事件相机的RoboRacer汽车的现实世界跟随线任务中验证了该闭环方法的有效性。

Conclusion: RTRRL算法能有效微调预训练策略，提升自动驾驶代理在驾驶任务中的性能并与特定循环网络模型协同工作。

Abstract: Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.

</details>


### [1091] [Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems](https://arxiv.org/abs/2602.02269)
*Jon Škerlj,Seongjin Bien,Abdeldjallil Naceri,Sami Haddadin*

Main category: cs.RO

TL;DR: 提出开源ROS2架构multipanda_ros2用于多机器人控制，解决实时扭矩控制挑战，实现1kHz控制频率，减少仿真到现实差距。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人控制中实时扭矩控制等挑战，满足实时控制和安全标准要求，减少仿真到现实差距。

Method: 利用ros2 control，采用controllet - feature设计模式，集成高保真MuJoCo仿真，进行现实惯性参数识别。

Result: 实现1kHz控制频率，控制器切换延迟≤2ms，能通过现实惯性参数识别提高力和扭矩精度。

Conclusion: 工作将软机器人方法扩展到刚性双臂接触任务，为高级机器人研究提供可靠、可重复平台。

Abstract: We present $multipanda\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.

</details>


### [1092] [MapDream: Task-Driven Map Learning for Vision-Language Navigation](https://arxiv.org/abs/2602.00222)
*Guoxin Lian,Shuo Wang,Yucheng Wang,Yongcai Wang,Maiyue Chen,Kaihui Wang,Bo Zhang,Zhizhong Su,Deying Li,Zhaoxin Fan*

Main category: cs.RO

TL;DR: 提出MapDream框架用于视觉语言导航，通过联合学习地图生成和动作预测，在相关实验中取得SOTA单目性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航中多数方法依赖与导航策略独立构建的手工地图，应构建由导航目标直接塑造的学习型地图。

Method: 提出MapDream框架，将地图构建表述为自回归鸟瞰图图像合成，联合学习地图生成和动作预测，通过监督预训练和强化微调进行优化。

Result: 在R2R - CE和RxR - CE实验中取得了最先进的单目性能。

Conclusion: 验证了任务驱动的生成式地图学习的有效性。

Abstract: Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.

</details>


### [1093] [ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control](https://arxiv.org/abs/2602.00401)
*Jean Pierre Sleiman,He Li,Alphonsus Adu-Bredu,Robin Deits,Arun Kumar,Kevin Bergamin,Mohak Bhardwaj,Scott Biddlestone,Nicola Burger,Matthew A. Estrada,Francesco Iacobelli,Twan Koolen,Alexander Lambert,Erica Lin,M. Eva Mungai,Zach Nobles,Shane Rozen-Levy,Yuyao Shi,Jiashun Wang,Jakob Welner,Fangzhou Yu,Mike Zhang,Alfred Rizzi,Jessica Hodgins,Sylvain Bertrand,Yeuhi Abe,Scott Kuindersma,Farbod Farshidian*

Main category: cs.RO

TL;DR: 提出ZEST框架，可从多种数据源训练策略并零样本部署到硬件，在多机器人上展示了零样本部署的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决类人机器人全身控制需大量技能工程和控制器调优的问题。

Method: 结合自适应采样和基于模型的辅助扳手的自动课程训练策略，提供选择关节增益的程序和改进的执行器模型。

Result: 在Atlas、Unitree G1和Spot上学习并展示了多种技能。

Conclusion: ZEST可在异构数据源和具身系统上实现鲁棒的零样本部署，是生物运动和机器人对应物之间可扩展的接口。

Abstract: Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sources -- high-fidelity motion capture, noisy monocular video, and non-physics-constrained animation -- and deploys them to hardware zero-shot. ZEST generalizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping. Its training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers. We further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators. Trained entirely in simulation with moderate domain randomization, ZEST demonstrates remarkable generality. On Boston Dynamics' Atlas humanoid, ZEST learns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation. Together, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing ZEST as a scalable interface between biological movements and their robotic counterparts.

</details>


### [1094] [SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning](https://arxiv.org/abs/2602.00743)
*Xu Pan,Zhenglin Wan,Xingrui Yu,Xianwei Zheng,Youkai Ke,Ming Sun,Rui Wang,Ziwei Wang,Ivor Tsang*

Main category: cs.RO

TL;DR: 提出 SA - VLA 框架改进 VLA 模型强化学习微调问题，提升零样本空间泛化能力


<details>
  <summary>Details</summary>
Motivation: VLA 模型在强化学习微调时遇到空间分布偏移鲁棒性下降的问题，该问题与流匹配 VLA 策略在强化学习中空间归纳偏置的侵蚀有关。

Method: 提出 SA - VLA 框架，将表征学习、奖励设计和探索与任务几何相结合；融合隐式空间表征和视觉标记，提供反映几何进展的密集奖励，采用 SCAN 探索策略。

Result: 在多目标和杂乱操作基准测试中，SA - VLA 实现了稳定的强化学习微调，提高了零样本空间泛化能力，产生更鲁棒和可迁移的行为。

Conclusion: SA - VLA 能有效解决 VLA 模型在强化学习微调时鲁棒性下降的问题，成果代码和项目页面已公开

Abstract: Vision-Language-Action (VLA) models exhibit strong generalization in robotic manipulation, yet reinforcement learning (RL) fine-tuning often degrades robustness under spatial distribution shifts. For flow-matching VLA policies, this degradation is closely associated with the erosion of spatial inductive bias during RL adaptation, as sparse rewards and spatially agnostic exploration increasingly favor short-horizon visual cues. To address this issue, we propose \textbf{SA-VLA}, a spatially-aware RL adaptation framework that preserves spatial grounding during policy optimization by aligning representation learning, reward design, and exploration with task geometry. SA-VLA fuses implicit spatial representations with visual tokens, provides dense rewards that reflect geometric progress, and employs \textbf{SCAN}, a spatially-conditioned annealed exploration strategy tailored to flow-matching dynamics. Across challenging multi-object and cluttered manipulation benchmarks, SA-VLA enables stable RL fine-tuning and improves zero-shot spatial generalization, yielding more robust and transferable behaviors. Code and project page are available at https://xupan.top/Projects/savla.

</details>


### [1095] [CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining](https://arxiv.org/abs/2602.00937)
*I-Chun Arthur Liu,Krzysztof Choromanski,Sandy Huang,Connor Schenck*

Main category: cs.RO

TL;DR: 提出3D预训练框架CLAMP，结合点云和机器人动作，经预训练和微调提升学习效率和策略性能，在多任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D图像表示的行为克隆策略无法捕捉3D空间信息，难以进行精确操作。

Method: 提出CLAMP框架，利用点云和机器人动作，重新渲染多视图图像，通过对比学习关联物体3D信息和动作模式，预训练扩散策略初始化权重，再进行微调。

Result: 该设计显著提高了学习效率和策略性能，在六个模拟任务和五个现实任务中优于现有基线。

Conclusion: CLAMP框架有效提升了机器人操作任务的学习效率和策略性能。

Abstract: Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.

</details>


### [1096] [HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving](https://arxiv.org/abs/2602.00993)
*Weizhe Tang,Junwei You,Jiaxi Liu,Zhaoyi Wang,Rui Gan,Zilin Huang,Feng Wei,Bin Ran*

Main category: cs.RO

TL;DR: 提出HERMES框架注入长尾风险线索到轨迹规划，在长尾混合交通场景实验中表现好


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶模型在长尾条件下保证安全准确运行有挑战，在长尾混合交通场景尤其突出

Method: 采用基础模型辅助注释管道生成结构化上下文，引入三模态驾驶模块融合多信息进行规划

Result: 在真实世界长尾数据集实验中，HERMES超过同等基线模型，消融实验验证关键组件贡献

Conclusion: HERMES框架能在长尾混合交通场景进行风险感知准确轨迹规划

Abstract: End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.

</details>


### [1097] [Offline Discovery of Interpretable Skills from Multi-Task Trajectories](https://arxiv.org/abs/2602.01018)
*Chongyu Zhu,Mithun Vanniasinghe,Jiayu Chen,Chi-Guhn Lee*

Main category: cs.RO

TL;DR: 提出LOKI框架用于离线技能发现和分层模仿学习，在D4RL Kitchen基准测试中表现良好，发现的技能有意义且具组合性。


<details>
  <summary>Details</summary>
Motivation: 解决从长周期、多任务离线数据中发现可复用技能的挑战，尤其是数据缺乏明确奖励或子任务注释的情况。

Method: LOKI是一个三阶段的端到端学习框架，先进行两阶段弱监督技能发现，再利用精确边界构建分层策略。

Result: 在D4RL Kitchen基准测试中取得高成功率，优于标准分层模仿学习基线。

Conclusion: 发现的技能语义有意义，符合人类直觉，且能组合解决新任务。

Abstract: Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task.

</details>


### [1098] [Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications](https://arxiv.org/abs/2602.01679)
*Raghavasimhan Sankaranarayanan,Paul Stuart,Nicholas Ahn,Arno Sungarian,Yash Chitalia*

Main category: cs.RO

TL;DR: 本文提出自动化机器人系统用于手术器械分类和包装，通过数据集训练感知模型，集成视觉、机械臂和算法，实验显示效果良好，是自动化SPD工作流程的第一步。


<details>
  <summary>Details</summary>
Motivation: 手动检查和准备手术器械托盘耗时、易出错且易造成污染和器械损坏，需自动化解决方案。

Method: 收集包含31种手术器械和6975张标注图像的自定义数据集，用YOLO12检测和基于ResNet的级联模型进行细粒度分类训练混合感知管道；集成校准视觉模块、6自由度机械臂和基于规则的包装算法；使用3D打印隔板和支架。

Result: 实验评估显示感知精度高，与人工组装托盘相比，工具间碰撞显著减少。

Conclusion: 该工作是自动化SPD工作流程的可扩展第一步，能提高手术准备的安全性和一致性，减少处理时间。

Abstract: The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.

</details>


### [1099] [RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback](https://arxiv.org/abs/2602.00886)
*Amitesh Vatsa,Zhixian Xie,Wanxin Jin*

Main category: cs.RO

TL;DR: 提出RoDiF方法解决扩散策略用人类偏好微调的问题，实验显示其优于基线。


<details>
  <summary>Details</summary>
Motivation: 扩散策略用人类偏好微调受去噪过程多步结构挑战，需解决该问题。

Method: 引入统一MDP公式集成扩散去噪链与环境动态，提出RoDiF方法，从几何假设裁剪视角重新解释DPO目标并采用保守裁剪策略。

Result: 在长视野操作任务实验中，RoDiF始终优于现有基线，能有效引导预训练扩散策略到人类偏好模式，在30%偏好标签损坏时仍表现良好。

Conclusion: RoDiF方法有效解决扩散策略微调问题，具有鲁棒性和优越性。

Abstract: Diffusion policies are a powerful paradigm for robotic control, but fine-tuning them with human preferences is fundamentally challenged by the multi-step structure of the denoising process. To overcome this, we introduce a Unified Markov Decision Process (MDP) formulation that coherently integrates the diffusion denoising chain with environmental dynamics, enabling reward-free Direct Preference Optimization (DPO) for diffusion policies. Building on this formulation, we propose RoDiF (Robust Direct Fine-Tuning), a method that explicitly addresses corrupted human preferences. RoDiF reinterprets the DPO objective through a geometric hypothesis-cutting perspective and employs a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Extensive experiments on long-horizon manipulation tasks show that RoDiF consistently outperforms state-of-the-art baselines, effectively steering pretrained diffusion policies of diverse architectures to human-preferred modes, while maintaining strong performance even under 30% corrupted preference labels.

</details>


### [1100] [Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy](https://arxiv.org/abs/2602.01939)
*Yuxin He,Ruihao Zhang,Tianao Shen,Cheng Liu,Qiang Nie*

Main category: cs.RO

TL;DR: 本文针对主动视觉中视觉遮挡问题，提出探索与聚焦操作（EFM）问题，建立EFM - 10基准，提出双手主动感知（BAP）策略并收集数据集验证其有效性，希望推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 解决主相机安装在机器人头部时视觉遮挡频繁的问题，提出更基础的EFM问题以完成挑战性操作任务。

Method: 建立EFM - 10基准，提出BAP策略，用一只手臂提供主动视觉，另一只手臂在操作时提供力感知，并收集BAPData数据集。

Result: 以模仿学习的方式验证了BAP策略的有效性。

Conclusion: 希望EFM - 10基准和BAP策略能为该方向未来研究奠定基础。

Abstract: Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.

</details>


### [1101] [Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization](https://arxiv.org/abs/2602.02035)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: 本文提出结合信息瓶颈理论与向量量化的框架，实现多智能体环境中选择性、高效通信，实验表明该方法性能提升且降低带宽使用，优于现有策略。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界机器人应用中多智能体强化学习系统面临的通信约束对协调有效性的影响问题。

Method: 结合信息瓶颈理论与向量量化，学习压缩和离散化通信消息，引入门控通信机制动态决定通信时机。

Result: 在挑战性协调任务中性能比无通信基线提升181.8%，降低带宽使用41.4%，帕累托前沿分析显示在整个成功 - 带宽频谱上占优。

Conclusion: 该方法显著优于现有通信策略，为带宽受限环境中部署多智能体系统建立了理论框架。

Abstract: Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.

</details>


### [1102] [RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots](https://arxiv.org/abs/2602.01515)
*Humphrey Munn,Brendan Tidd,Peter Bohm,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: 提出轻量级自监督部署时间监视器RAPT用于人形机器人控制，可可靠检测OOD，提供可解释性并进行根因分析，在仿真和实际硬件上效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在人形机器人控制部署中存在与高速控制不兼容、校准不佳、缺乏解释性等问题。

Method: RAPT从仿真中学习标称执行的概率时空流形，评估执行时预测偏差，引入基于梯度的时间显著性和LLM推理的根因分析管道。

Result: 在大规模仿真中TPR提高37%，实际部署中TPR提高12.5%，根因分类准确率达75%。

Conclusion: RAPT能有效解决人形机器人部署中控制策略的问题，提供可靠检测和可解释性。

Abstract: Deploying learned control policies on humanoid robots is challenging: policies that appear robust in simulation can execute confidently in out-of-distribution (OOD) states after Sim-to-Real transfer, leading to silent failures that risk hardware damage. Although anomaly detection can mitigate these failures, prior methods are often incompatible with high-rate control, poorly calibrated at the extremely low false-positive rates required for practical deployment, or operate as black boxes that provide a binary stop signal without explaining why the robot drifted from nominal behavior. We present RAPT, a lightweight, self-supervised deployment-time monitor for 50Hz humanoid control. RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. This yields (i) reliable online OOD detection under strict false-positive constraints and (ii) a continuous, interpretable measure of Sim-to-Real mismatch that can be tracked over time to quantify how far deployment has drifted from training. Beyond detection, we introduce an automated post-hoc root-cause analysis pipeline that combines gradient-based temporal saliency derived from RAPT's reconstruction objective with LLM-based reasoning conditioned on saliency and joint kinematics to produce semantic failure diagnoses in a zero-shot setting. We evaluate RAPT on a Unitree G1 humanoid across four complex tasks in simulation and on physical hardware. In large-scale simulation, RAPT improves True Positive Rate (TPR) by 37% over the strongest baseline at a fixed episode-level false positive rate of 0.5%. On real-world deployments, RAPT achieves a 12.5% TPR improvement and provides actionable interpretability, reaching 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.

</details>


### [1103] [TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour](https://arxiv.org/abs/2602.02331)
*Shaoting Zhu,Baijun Ye,Jiaxuan Wang,Jiakang Chen,Ziwen Zhuang,Linzhan Mou,Runhan Huang,Hang Zhao*

Main category: cs.RO

TL;DR: 提出TTT - Parkour框架，通过快速测试时训练提升人形机器人在复杂地形的跑酷能力，实验证明策略有零样本从仿真到现实的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用运动策略在任意高挑战性环境中表现不佳，需提升机器人在复杂地形的跑酷能力。

Method: 提出real - to - sim - to - real框架，采用两阶段端到端学习范式，先在多样程序生成地形预训练策略，再在从现实捕获重建的高保真网格上快速微调；开发基于RGB - D输入的几何重建管道。

Result: TTT - Parkour使机器人能应对复杂障碍物，捕获、重建和测试时训练流程在多数测试地形耗时少于10分钟。

Conclusion: 测试时训练后的策略具有强大的零样本从仿真到现实的迁移能力。

Abstract: Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.

</details>


### [1104] [SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation](https://arxiv.org/abs/2602.02402)
*Mu Huang,Hui Wang,Kerui Ren,Linning Xu,Yunsong Zhou,Mulin Yu,Bo Dai,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文提出用于软体操作的3D高斯光斑模拟器SoMA，提升了重仿真精度和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器依赖预定义物理或无机器人条件控制的数据驱动动力学，限制了精度、稳定性和泛化性。

Method: 在统一的潜在神经空间中耦合可变形动力学、环境力和机器人关节动作，基于学习的高斯光斑建模交互。

Result: SoMA将现实世界机器人操作的重仿真精度和泛化性提高了20%，能稳定模拟长视野布料折叠等复杂任务。

Conclusion: SoMA无需预定义物理模型，能实现可控、稳定的长视野操作和超越观测轨迹的泛化。

Abstract: Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.

</details>


### [1105] [World-Gymnast: Training Robots with Reinforcement Learning in a World Model](https://arxiv.org/abs/2602.02454)
*Ansh Kumar Sharma,Yixiang Sun,Ninghao Lu,Yunzhe Zhang,Jiarao Liu,Sherry Yang*

Main category: cs.RO

TL;DR: 论文提出World - Gymnast方法，在世界模型中对VLA策略进行RL微调，在Bridge机器人上表现优于SFT和软件模拟器，表明在云端学习世界模型和训练机器人策略可缩小演示与家用机器人的差距。


<details>
  <summary>Details</summary>
Motivation: 机器人从物理世界交互学习受成本限制，SFT受专家数据量限制，RL在软件模拟器中受仿真到现实差距影响，探索在世界模型中训练策略是否更有效。

Method: 提出World - Gymnast，在动作条件视频世界模型中展开VLA策略进行RL微调，并用视觉语言模型奖励展开过程。

Result: 在Bridge机器人上，World - Gymnast性能比SFT高18倍，比软件模拟器高2倍，还展现出多种有趣能力。

Conclusion: 在云端学习世界模型和训练机器人策略是缩小演示中工作的机器人和家用机器人差距的关键。

Abstract: Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.

</details>


### [1106] [Flow Policy Gradients for Robot Control](https://arxiv.org/abs/2602.02481)
*Brent Yi,Hongsuk Choi,Himanshu Gaurav Singh,Xiaoyu Huang,Takara E. Truong,Carmelo Sferrazza,Yi Ma,Rocky Duan,Pieter Abbeel,Guanya Shi,Karen Liu,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: 本文展示了流匹配策略梯度在训练和微调富有表现力的机器人控制策略方面的有效性，包括在多种任务中的应用及分析。


<details>
  <summary>Details</summary>
Motivation: 传统基于似然的策略梯度方法限制了策略输出分布，本文旨在让无需似然计算的流匹配策略梯度在挑战性机器人控制场景中有效。

Method: 引入改进的目标函数，进行消融和训练动态分析。

Result: 在腿部运动、人形运动跟踪和操作任务中取得成功，并在两个人形机器人上实现稳健的仿真到现实迁移，策略能利用流表示进行探索，微调鲁棒性优于基线。

Conclusion: 流匹配策略梯度在训练和微调机器人控制策略方面是有效的，具有更好的探索和微调鲁棒性。

Abstract: Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.

</details>


### [1107] [PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning](https://arxiv.org/abs/2602.02396)
*Amisha Bhaskar,Pratap Tokekar,Stefano Di Cairano,Alexander Schperberg*

Main category: cs.RO

TL;DR: 引入PRISM单通策略，结合多感官编码器与线性注意力生成器，在实际硬件和模拟基准测试中表现优于现有方法，是快速、准确且多感官的模仿策略。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成方法无法满足机器人模仿学习中多方面要求的问题。

Method: 引入基于IMLE的批量全局拒绝采样变体的单通策略PRISM，将时间多感官编码器与使用Performer架构的线性注意力生成器相结合。

Result: 在真实硬件和模拟基准测试中，PRISM在成功率上优于现有方法，如在物理任务中比最先进的扩散策略成功率高10 - 25%，在CALVIN中比扩散方法成功率提高约25%，同时减少轨迹抖动。

Conclusion: PRISM是一种快速、准确、多感官的模仿策略，无需迭代采样的延迟即可保留多模态动作覆盖。

Abstract: Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.

</details>


### [1108] [HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos](https://arxiv.org/abs/2602.02473)
*Yinhuai Wang,Qihan Zhao,Yuen Fui Lau,Runyi Yu,Hok Wai Tsui,Qifeng Chen,Jingbo Wang,Jiangmiao Pang,Ping Tan*

Main category: cs.RO

TL;DR: 提出HumanX框架将人类视频转化为类人机器人交互技能，在多领域实验中表现优秀，泛化能力远超先前方法。


<details>
  <summary>Details</summary>
Motivation: 当前类人机器人实现灵活自适应交互任务的方法存在缺乏真实交互数据和需特定奖励工程的瓶颈，限制了可扩展性。

Method: 提出HumanX框架，包含数据生成管道XGen和统一模仿学习框架XMimic，从视频合成交互数据并学习通用交互技能。

Result: 在五个领域成功获取10种技能并零样本迁移到实体机器人，如复杂投篮动作和连续人机传球，泛化成功率比先前方法高8倍多。

Conclusion: HumanX提供了一种可扩展且与任务无关的途径，用于学习通用的现实世界机器人交互技能。

Abstract: Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [1109] [On finite-dimensional encoding/decoding theorems for neural operators](https://arxiv.org/abs/2602.00068)
*Vinícius Luz Oliveira,Vladimir G. Pestov*

Main category: math.FA

TL;DR: 指出神经网络有限维编码/解码定理无需对函数空间E、F作假设，对任意局部凸空间都成立，C^k - 光滑映射有条件成立。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络有限维编码/解码定理对函数空间E、F有假设，为拓展其适用范围进行研究。

Method: 对连续映射及C^k - 光滑映射在不同拓扑下的情况进行分析。

Result: 连续映射结果无需对E、F作假设，对任意局部凸空间成立；C^k - 光滑映射结果成立当且仅当E有逼近性质。

Conclusion: 分析结果对微分方程理论有应用价值，因非可赋范局部凸函数空间常见。

Abstract: Recently, versions of neural networks with infinite-dimensional affine operators inside the computational units (``neural operator'' networks) have been applied to learn solutions to differential equations. To enable practical computations, one employs finite-dimensional encoding/decoding theorems of the following kind: every continuous mapping $f$ between function spaces $E$ and $F$ is approximated in the topology of uniform convergence on compacta by continuous mappings factoring through two finite dimensional Banach spaces. Such a result is known (Kovachki et al., 2023) for $E,F$ being Banach spaces having the approximation property. We point out that the result needs no assumptions on $E,F$ whatsoever and remains true not only for all normed spaces, but for arbitrary locally convex spaces as well. At the same time, an analogous result for $C^k$-smooth mappings and the $C^k$ compact open topology, $k\geq 1$, holds if and only if the space $E$ has the approximation property. This analysis may be useful already because non-normable locally convex function spaces are common in the theory of differential equations, the main field of applications for the emerging theory.

</details>
