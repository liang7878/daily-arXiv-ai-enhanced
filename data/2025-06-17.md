<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 84]
- [cs.CE](#cs.CE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 14]
- [cs.LG](#cs.LG) [Total: 177]
- [cs.NE](#cs.NE) [Total: 7]
- [cs.SE](#cs.SE) [Total: 22]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 4]
- [stat.ML](#stat.ML) [Total: 15]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]
- [eess.AS](#eess.AS) [Total: 4]
- [hep-ex](#hep-ex) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [econ.GN](#econ.GN) [Total: 4]
- [cs.HC](#cs.HC) [Total: 9]
- [cs.CC](#cs.CC) [Total: 2]
- [hep-lat](#hep-lat) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [quant-ph](#quant-ph) [Total: 7]
- [math.NA](#math.NA) [Total: 1]
- [math.OC](#math.OC) [Total: 6]
- [cs.CV](#cs.CV) [Total: 37]
- [econ.EM](#econ.EM) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.CL](#cs.CL) [Total: 65]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [physics.optics](#physics.optics) [Total: 2]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 5]
- [cs.MA](#cs.MA) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [cs.RO](#cs.RO) [Total: 15]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 4]
- [math.ST](#math.ST) [Total: 5]
- [cs.CR](#cs.CR) [Total: 18]
- [q-bio.NC](#q-bio.NC) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [eess.SP](#eess.SP) [Total: 4]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.SD](#cs.SD) [Total: 5]
- [stat.AP](#stat.AP) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Amazon Nova Family of Models: Technical Report and Model Card](https://arxiv.org/abs/2506.12103)
*Amazon AGI,Aaron Langford,Aayush Shah,Abhanshu Gupta,Abhimanyu Bhatter,Abhinav Goyal,Abhinav Mathur,Abhinav Mohanty,Abhishek Kumar,Abhishek Sethi,Abi Komma,Abner Pena,Achin Jain,Adam Kunysz,Adam Opyrchal,Adarsh Singh,Aditya Rawal,Adok Achar Budihal Prasad,Adrià de Gispert,Agnika Kumar,Aishwarya Aryamane,Ajay Nair,Akilan M,Akshaya Iyengar,Akshaya Vishnu Kudlu Shanbhogue,Alan He,Alessandra Cervone,Alex Loeb,Alex Zhang,Alexander Fu,Alexander Lisnichenko,Alexander Zhipa,Alexandros Potamianos,Ali Kebarighotbi,Aliakbar Daronkolaei,Alok Parmesh,Amanjot Kaur Samra,Ameen Khan,Amer Rez,Amir Saffari,Amit Agarwalla,Amit Jhindal,Amith Mamidala,Ammar Asmro,Amulya Ballakur,Anand Mishra,Anand Sridharan,Anastasiia Dubinina,Andre Lenz,Andreas Doerr,Andrew Keating,Andrew Leaver,Andrew Smith,Andrew Wirth,Andy Davey,Andy Rosenbaum,Andy Sohn,Angela Chan,Aniket Chakrabarti,Anil Ramakrishna,Anirban Roy,Anita Iyer,Anjali Narayan-Chen,Ankith Yennu,Anna Dabrowska,Anna Gawlowska,Anna Rumshisky,Anna Turek,Anoop Deoras,Anton Bezruchkin,Anup Prasad,Anupam Dewan,Anwith Kiran,Apoorv Gupta,Aram Galstyan,Aravind Manoharan,Arijit Biswas,Arindam Mandal,Arpit Gupta,Arsamkhan Pathan,Arun Nagarajan,Arushan Rajasekaram,Arvind Sundararajan,Ashwin Ganesan,Ashwin Swaminathan,Athanasios Mouchtaris,Audrey Champeau,Avik Ray,Ayush Jaiswal,Ayush Sharma,Bailey Keefer,Balamurugan Muthiah,Beatriz Leon-Millan,Ben Koopman,Ben Li,Benjamin Biggs,Benjamin Ott,Bhanu Vinzamuri,Bharath Venkatesh,Bhavana Ganesh,Bhoomit Vasani,Bill Byrne,Bill Hsu,Bincheng Wang,Blake King,Blazej Gorny,Bo Feng,Bo Zheng,Bodhisattwa Paul,Bofan Sun,Bofeng Luo,Bowen Chen,Bowen Xie,Boya Yu,Brendan Jugan,Brett Panosh,Brian Collins,Brian Thompson,Can Karakus,Can Liu,Carl Lambrecht,Carly Lin,Carolyn Wang,Carrie Yuan,Casey Loyda,Cezary Walczak,Chalapathi Choppa,Chandana Satya Prakash,Chankrisna Richy Meas,Charith Peris,Charles Recaido,Charlie Xu,Charul Sharma,Chase Kernan,Chayut Thanapirom,Chengwei Su,Chenhao Xu,Chenhao Yin,Chentao Ye,Chenyang Tao,Chethan Parameshwara,Ching-Yun Chang,Chong Li,Chris Hench,Chris Tran,Christophe Dupuy,Christopher Davis,Christopher DiPersio,Christos Christodoulopoulos,Christy Li,Chun Chen,Claudio Delli Bovi,Clement Chung,Cole Hawkins,Connor Harris,Corey Ropell,Cynthia He,DK Joo,Dae Yon Hwang,Dan Rosen,Daniel Elkind,Daniel Pressel,Daniel Zhang,Danielle Kimball,Daniil Sorokin,Dave Goodell,Davide Modolo,Dawei Zhu,Deepikaa Suresh,Deepti Ragha,Denis Filimonov,Denis Foo Kune,Denis Romasanta Rodriguez,Devamanyu Hazarika,Dhananjay Ram,Dhawal Parkar,Dhawal Patel,Dhwanil Desai,Dinesh Singh Rajput,Disha Sule,Diwakar Singh,Dmitriy Genzel,Dolly Goldenberg,Dongyi He,Dumitru Hanciu,Dushan Tharmal,Dzmitry Siankovich,Edi Cikovic,Edwin Abraham,Ekraam Sabir,Elliott Olson,Emmett Steven,Emre Barut,Eric Jackson,Ethan Wu,Evelyn Chen,Ezhilan Mahalingam,Fabian Triefenbach,Fan Yang,Fangyu Liu,Fanzi Wu,Faraz Tavakoli,Farhad Khozeimeh,Feiyang Niu,Felix Hieber,Feng Li,Firat Elbey,Florian Krebs,Florian Saupe,Florian Sprünken,Frank Fan,Furqan Khan,Gabriela De Vincenzo,Gagandeep Kang,George Ding,George He,George Yeung,Ghada Qaddoumi,Giannis Karamanolakis,Goeric Huybrechts,Gokul Maddali,Gonzalo Iglesias,Gordon McShane,Gozde Sahin,Guangtai Huang,Gukyeong Kwon,Gunnar A. Sigurdsson,Gurpreet Chadha,Gururaj Kosuru,Hagen Fuerstenau,Hah Hah,Haja Maideen,Hajime Hosokawa,Han Liu,Han-Kai Hsu,Hann Wang,Hao Li,Hao Yang,Haofeng Zhu,Haozheng Fan,Harman Singh,Harshavardhan Kaluvala,Hashim Saeed,He Xie,Helian Feng,Hendrix Luo,Hengzhi Pei,Henrik Nielsen,Hesam Ilati,Himanshu Patel,Hongshan Li,Hongzhou Lin,Hussain Raza,Ian Cullinan,Imre Kiss,Inbarasan Thangamani,Indrayani Fadnavis,Ionut Teodor Sorodoc,Irem Ertuerk,Iryna Yemialyanava,Ishan Soni,Ismail Jelal,Ivan Tse,Jack FitzGerald,Jack Zhao,Jackson Rothgeb,Jacky Lee,Jake Jung,Jakub Debski,Jakub Tomczak,James Jeun,James Sanders,Jason Crowley,Jay Lee,Jayakrishna Anvesh Paidy,Jayant Tiwari,Jean Farmer,Jeff Solinsky,Jenna Lau,Jeremy Savareese,Jerzy Zagorski,Ji Dai,Jiacheng,Gu,Jiahui Li,Jian,Zheng,Jianhua Lu,Jianhua Wang,Jiawei Dai,Jiawei Mo,Jiaxi Xu,Jie Liang,Jie Yang,Jim Logan,Jimit Majmudar,Jing Liu,Jinghong Miao,Jingru Yi,Jingyang Jin,Jiun-Yu Kao,Jixuan Wang,Jiyang Wang,Joe Pemberton,Joel Carlson,Joey Blundell,John Chin-Jew,John He,Jonathan Ho,Jonathan Hueser,Jonathan Lunt,Jooyoung Lee,Joshua Tan,Joyjit Chatterjee,Judith Gaspers,Jue Wang,Jun Fang,Jun Tang,Jun Wan,Jun Wu,Junlei Wang,Junyi Shi,Justin Chiu,Justin Satriano,Justin Yee,Jwala Dhamala,Jyoti Bansal,Kai Zhen,Kai-Wei Chang,Kaixiang Lin,Kalyan Raman,Kanthashree Mysore Sathyendra,Karabo Moroe,Karan Bhandarkar,Karan Kothari,Karolina Owczarzak,Karthick Gopalswamy,Karthick Ravi,Karthik Ramakrishnan,Karthika Arumugam,Kartik Mehta,Katarzyna Konczalska,Kavya Ravikumar,Ke Tran,Kechen Qin,Kelin Li,Kelvin Li,Ketan Kulkarni,Kevin Angelo Rodrigues,Keyur Patel,Khadige Abboud,Kiana Hajebi,Klaus Reiter,Kris Schultz,Krishna Anisetty,Krishna Kotnana,Kristen Li,Kruthi Channamallikarjuna,Krzysztof Jakubczyk,Kuba Pierewoj,Kunal Pal,Kunwar Srivastav,Kyle Bannerman,Lahari Poddar,Lakshmi Prasad,Larry Tseng,Laxmikant Naik,Leena Chennuru Vankadara,Lenon Minorics,Leo Liu,Leonard Lausen,Leonardo F. R. Ribeiro,Li Zhang,Lili Gehorsam,Ling Qi,Lisa Bauer,Lori Knapp,Lu Zeng,Lucas Tong,Lulu Wong,Luoxin Chen,Maciej Rudnicki,Mahdi Namazifar,Mahesh Jaliminche,Maira Ladeira Tanke,Manasi Gupta,Mandeep Ahlawat,Mani Khanuja,Mani Sundaram,Marcin Leyk,Mariusz Momotko,Markus Boese,Markus Dreyer,Markus Mueller,Mason Fu,Mateusz Górski,Mateusz Mastalerczyk,Matias Mora,Matt Johnson,Matt Scott,Matthew Wen,Max Barysau,Maya Boumerdassi,Maya Krishnan,Mayank Gupta,Mayank Hirani,Mayank Kulkarni,Meganathan Narayanasamy,Melanie Bradford,Melanie Gens,Melissa Burke,Meng Jin,Miao Chen,Michael Denkowski,Michael Heymel,Michael Krestyaninov,Michal Obirek,Michalina Wichorowska,Michał Miotk,Milosz Watroba,Mingyi Hong,Mingzhi Yu,Miranda Liu,Mohamed Gouda,Mohammad El-Shabani,Mohammad Ghavamzadeh,Mohit Bansal,Morteza Ziyadi,Nan Xia,Nathan Susanj,Nav Bhasin,Neha Goswami,Nehal Belgamwar,Nicolas Anastassacos,Nicolas Bergeron,Nidhi Jain,Nihal Jain,Niharika Chopparapu,Nik Xu,Nikko Strom,Nikolaos Malandrakis,Nimisha Mishra,Ninad Parkhi,Ninareh Mehrabi,Nishita Sant,Nishtha Gupta,Nitesh Sekhar,Nithin Rajeev,Nithish Raja Chidambaram,Nitish Dhar,Noor Bhagwagar,Noy Konforty,Omar Babu,Omid Razavi,Orchid Majumder,Osama Dar,Oscar Hsu,Pablo Kvitca,Pallavi Pandey,Parker Seegmiller,Patrick Lange,Paul Ferraro,Payal Motwani,Pegah Kharazmi,Pei Wang,Pengfei Liu,Peter Bradtke,Peter Götz,Peter Zhou,Pichao Wang,Piotr Poskart,Pooja Sonawane,Pradeep Natarajan,Pradyun Ramadorai,Pralam Shah,Prasad Nirantar,Prasanthi Chavali,Prashan Wanigasekara,Prashant Saraf,Prashun Dey,Pratyush Pant,Prerak Pradhan,Preyaa Patel,Priyanka Dadlani,Prudhvee Narasimha Sadha,Qi Dong,Qian Hu,Qiaozi,Gao,Qing Liu,Quinn Lam,Quynh Do,R. Manmatha,Rachel Willis,Rafael Liu,Rafal Ellert,Rafal Kalinski,Rafi Al Attrach,Ragha Prasad,Ragini Prasad,Raguvir Kunani,Rahul Gupta,Rahul Sharma,Rahul Tewari,Rajaganesh Baskaran,Rajan Singh,Rajiv Gupta,Rajiv Reddy,Rajshekhar Das,Rakesh Chada,Rakesh Vaideeswaran Mahesh,Ram Chandrasekaran,Ramesh Nallapati,Ran Xue,Rashmi Gangadharaiah,Ravi Rachakonda,Renxian Zhang,Rexhina Blloshmi,Rishabh Agrawal,Robert Enyedi,Robert Lowe,Robik Shrestha,Robinson Piramuthu,Rohail Asad,Rohan Khanna,Rohan Mukherjee,Rohit Mittal,Rohit Prasad,Rohith Mysore Vijaya Kumar,Ron Diamant,Ruchita Gupta,Ruiwen Li,Ruoying Li,Rushabh Fegade,Ruxu Zhang,Ryan Arbow,Ryan Chen,Ryan Gabbard,Ryan Hoium,Ryan King,Sabarishkumar Iyer,Sachal Malick,Sahar Movaghati,Sai Balakavi,Sai Jakka,Sai Kashyap Paruvelli,Sai Muralidhar Jayanthi,Saicharan Shriram Mujumdar,Sainyam Kapoor,Sajjad Beygi,Saket Dingliwal,Saleh Soltan,Sam Ricklin,Sam Tucker,Sameer Sinha,Samridhi Choudhary,Samson Tan,Samuel Broscheit,Samuel Schulter,Sanchit Agarwal,Sandeep Atluri,Sander Valstar,Sanjana Shankar,Sanyukta Sanyukta,Sarthak Khanna,Sarvpriye Khetrapal,Satish Janakiraman,Saumil Shah,Saurabh Akolkar,Saurabh Giri,Saurabh Khandelwal,Saurabh Pawar,Saurabh Sahu,Sean Huang,Sejun Ra,Senthilkumar Gopal,Sergei Dobroshinsky,Shadi Saba,Shamik Roy,Shamit Lal,Shankar Ananthakrishnan,Sharon Li,Shashwat Srijan,Shekhar Bhide,Sheng Long Tang,Sheng Zha,Shereen Oraby,Sherif Mostafa,Shiqi Li,Shishir Bharathi,Shivam Prakash,Shiyuan Huang,Shreya Yembarwar,Shreyas Pansare,Shreyas Subramanian,Shrijeet Joshi,Shuai Liu,Shuai Tang,Shubham Chandak,Shubham Garg,Shubham Katiyar,Shubham Mehta,Shubham Srivastav,Shuo Yang,Siddalingesha D S,Siddharth Choudhary,Siddharth Singh Senger,Simon Babb,Sina Moeini,Siqi Deng,Siva Loganathan,Slawomir Domagala,Sneha Narkar,Sneha Wadhwa,Songyang Zhang,Songyao Jiang,Sony Trenous,Soumajyoti Sarkar,Soumya Saha,Sourabh Reddy,Sourav Dokania,Spurthideepika Sandiri,Spyros Matsoukas,Sravan Bodapati,Sri Harsha Reddy Wdaru,Sridevi Yagati Venkateshdatta,Srikanth Ronanki,Srinivasan R Veeravanallur,Sriram Venkatapathy,Sriramprabhu Sankaraguru,Sruthi Gorantla,Sruthi Karuturi,Stefan Schroedl,Subendhu Rongali,Subhasis Kundu,Suhaila Shakiah,Sukriti Tiwari,Sumit Bharti,Sumita Sami,Sumith Mathew,Sunny Yu,Sunwoo Kim,Suraj Bajirao Malode,Susana Cumplido Riel,Swapnil Palod,Swastik Roy,Syed Furqhan,Tagyoung Chung,Takuma Yoshitani,Taojiannan Yang,Tejaswi Chillakura,Tejwant Bajwa,Temi Lajumoke,Thanh Tran,Thomas Gueudre,Thomas Jung,Tianhui Li,Tim Seemman,Timothy Leffel,Tingting Xiang,Tirth Patel,Tobias Domhan,Tobias Falke,Toby Guo,Tom Li,Tomasz Horszczaruk,Tomasz Jedynak,Tushar Kulkarni,Tyst Marin,Tytus Metrycki,Tzu-Yen Wang,Umang Jain,Upendra Singh,Utkarsh Chirimar,Vaibhav Gupta,Vanshil Shah,Varad Deshpande,Varad Gunjal,Varsha Srikeshava,Varsha Vivek,Varun Bharadwaj,Varun Gangal,Varun Kumar,Venkatesh Elango,Vicente Ordonez,Victor Soto,Vignesh Radhakrishnan,Vihang Patel,Vikram Singh,Vinay Varma Kolanuvada,Vinayshekhar Bannihatti Kumar,Vincent Auvray,Vincent Cartillier,Vincent Ponzo,Violet Peng,Vishal Khandelwal,Vishal Naik,Vishvesh Sahasrabudhe,Vitaliy Korolev,Vivek Gokuladas,Vivek Madan,Vivek Subramanian,Volkan Cevher,Vrinda Gupta,Wael Hamza,Wei Zhang,Weitong Ruan,Weiwei Cheng,Wen Zhang,Wenbo Zhao,Wenyan Yao,Wenzhuo Ouyang,Wesley Dashner,William Campbell,William Lin,Willian Martin,Wyatt Pearson,Xiang Jiang,Xiangxing Lu,Xiangyang Shi,Xianwen Peng,Xiaofeng Gao,Xiaoge Jiang,Xiaohan Fei,Xiaohui Wang,Xiaozhou Joey Zhou,Xin Feng,Xinyan Zhao,Xinyao Wang,Xinyu Li,Xu Zhang,Xuan Wang,Xuandi Fu,Xueling Yuan,Xuning Wang,Yadunandana Rao,Yair Tavizon,Yan Rossiytsev,Yanbei Chen,Yang Liu,Yang Zou,Yangsook Park,Yannick Versley,Yanyan Zhang,Yash Patel,Yen-Cheng Lu,Yi Pan,Yi-Hsiang,Lai,Yichen Hu,Yida Wang,Yiheng Zhou,Yilin Xiang,Ying Shi,Ying Wang,Yishai Galatzer,Yongxin Wang,Yorick Shen,Yuchen Sun,Yudi Purwatama,Yue,Wu,Yue Gu,Yuechun Wang,Yujun Zeng,Yuncong Chen,Yunke Zhou,Yusheng Xie,Yvon Guy,Zbigniew Ambrozinski,Zhaowei Cai,Zhen Zhang,Zheng Wang,Zhenghui Jin,Zhewei Zhao,Zhiheng Li,Zhiheng Luo,Zhikang Zhang,Zhilin Fang,Zhiqi Bu,Zhiyuan Wang,Zhizhong Li,Zijian Wang,Zimeng,Qiu,Zishi Li*

Main category: cs.AI

TL;DR: 介绍新一代基础模型Amazon Nova及其不同版本特点，还提及模型构建原则与相关基准测试结果。


<details>
  <summary>Details</summary>
Motivation: 推出具备前沿智能和领先性价比的新一代基础模型，满足不同任务需求。

Method: 文中未明确提及构建模型的具体方法。

Result: 给出核心能力、自主性能、长上下文等方面的基准测试结果。

Conclusion: Amazon Nova各版本模型在不同方面有突出表现，且构建时注重客户信任、安全和可靠性。

Abstract: We present Amazon Nova, a new generation of state-of-the-art foundation
models that deliver frontier intelligence and industry-leading price
performance. Amazon Nova Pro is a highly-capable multimodal model with the best
combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova
Lite is a low-cost multimodal model that is lightning fast for processing
images, video, documents and text. Amazon Nova Micro is a text-only model that
delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is
an image generation model that creates professional grade images with rich
customization controls. Amazon Nova Reel is a video generation model offering
high-quality outputs, customization, and motion control. Our models were built
responsibly and with a commitment to customer trust, security, and reliability.
We report benchmarking results for core capabilities, agentic performance, long
context, functional adaptation, runtime performance, and human evaluation.

</details>


### [2] [Because we have LLMs, we Can and Should Pursue Agentic Interpretability](https://arxiv.org/abs/2506.12152)
*Been Kim,John Hewitt,Neel Nanda,Noah Fiedel,Oyvind Tafjord*

Main category: cs.AI

TL;DR: 介绍大语言模型时代的能动性可解释性，分析其特点、利弊及挑战，并探讨解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型时代为可解释性研究带来新机遇，需探索新的可解释性方法。

Method: 提出能动性可解释性概念，对比传统方法，分析其利弊和挑战。

Result: 能动性可解释性有交互性优势但牺牲完整性，不适用于高风险场景，存在评估挑战。

Conclusion: 能动性可解释性有助于人类学习大语言模型的超人类概念，虽有挑战但有前景。

Abstract: The era of Large Language Models (LLMs) presents a new opportunity for
interpretability--agentic interpretability: a multi-turn conversation with an
LLM wherein the LLM proactively assists human understanding by developing and
leveraging a mental model of the user, which in turn enables humans to develop
better mental models of the LLM. Such conversation is a new capability that
traditional `inspective' interpretability methods (opening the black-box) do
not use. Having a language model that aims to teach and explain--beyond just
knowing how to talk--is similar to a teacher whose goal is to teach well,
understanding that their success will be measured by the student's
comprehension. While agentic interpretability may trade off completeness for
interactivity, making it less suitable for high-stakes safety situations with
potentially deceptive models, it leverages a cooperative model to discover
potentially superhuman concepts that can improve humans' mental model of
machines. Agentic interpretability introduces challenges, particularly in
evaluation, due to what we call `human-entangled-in-the-loop' nature (humans
responses are integral part of the algorithm), making the design and evaluation
difficult. We discuss possible solutions and proxy goals. As LLMs approach
human parity in many tasks, agentic interpretability's promise is to help
humans learn the potentially superhuman concepts of the LLMs, rather than see
us fall increasingly far from understanding them.

</details>


### [3] [Artificial Intelligence and Machine Learning in the Development of Vaccines and Immunotherapeutics Yesterday, Today, and Tomorrow](https://arxiv.org/abs/2506.12185)
*Elhoucine Elfatimi,Yassir Lekbach,Swayam Prakash,Lbachir BenMohamed*

Main category: cs.AI

TL;DR: AI和深度学习正变革疫苗和免疫疗法设计，未来或取代动物预实验及实现实时体内建模，推动个性化发展。


<details>
  <summary>Details</summary>
Motivation: 过去疫苗和免疫疗法开发依赖试错和大量体内测试，耗时长，需改进。

Method: 提供预测框架支持决策；整合计算模型、系统疫苗学和多组学数据；优化抗原/表位靶点选择；助力理解免疫调节等。

Result: 能更好表型、区分和分类疾病，预测免疫反应，识别影响保护效力的因素等。

Conclusion: 未来AI和DL或推动个性化疫苗和免疫疗法快速变革性发展。

Abstract: In the past, the development of vaccines and immunotherapeutics relied
heavily on trial-and-error experimentation and extensive in vivo testing, often
requiring years of pre-clinical and clinical trials. Today, artificial
intelligence (AI) and deep learning (DL) are actively transforming vaccine and
immunotherapeutic design, by (i) offering predictive frameworks that support
rapid, data-driven decision-making; (ii) increasingly being implemented as
time- and resource-efficient strategies that integrate computational models,
systems vaccinology, and multi-omics data to better phenotype, differentiate,
and classify patient diseases and cancers; predict patients' immune responses;
and identify the factors contributing to optimal vaccine and immunotherapeutic
protective efficacy; (iii) refining the selection of B- and T-cell
antigen/epitope targets to enhance efficacy and durability of immune
protection; and (iv) enabling a deeper understanding of immune regulation,
immune evasion, immune checkpoints, and regulatory pathways. The future of AI
and DL points toward (i) replacing animal preclinical testing of drugs,
vaccines, and immunotherapeutics with computational-based models, as recently
proposed by the United States FDA; and (ii) enabling real-time in vivo modeling
for immunobridging and prediction of protection in clinical trials. This may
result in a fast and transformative shift for the development of personal
vaccines and immunotherapeutics against infectious pathogens and cancers.

</details>


### [4] [PRO-V: An Efficient Program Generation Multi-Agent System for Automatic RTL Verification](https://arxiv.org/abs/2506.12200)
*Yujie Zhao,Zhijing Wu,Hejia Zhang,Zhongming Yu,Wentao Ni,Chia-Tung Ho,Haoxing Ren,Jishen Zhao*

Main category: cs.AI

TL;DR: 本文提出用于RTL验证的多智能体系统PRO - V，结合采样策略和验证框架，在不同RTL上取得一定验证准确率且代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在RTL代码生成上有困难，而LLM在Python代码生成及作为判断代理上表现好，以此为动机提出PRO - V。

Method: 提出PRO - V，采用best - of - n迭代采样策略提高测试平台正确性，引入LLM作为判断辅助的验证框架及自动提示生成管道。

Result: PRO - V在黄金RTL实现上验证准确率达87.17%，在RTL突变体上达76.28%。

Conclusion: PRO - V能有效提高RTL验证的准确性，可助力硬件验证及设计。

Abstract: LLM-assisted hardware verification is gaining substantial attention due to
its potential to significantly reduce the cost and effort of crafting effective
testbenches. It also serves as a critical enabler for LLM-aided end-to-end
hardware language design. However, existing current LLMs often struggle with
Register Transfer Level (RTL) code generation, resulting in testbenches that
exhibit functional errors in Hardware Description Languages (HDL) logic.
Motivated by the strong performance of LLMs in Python code generation under
inference-time sampling strategies, and their promising capabilities as judge
agents, we propose PRO-V a fully program generation multi-agent system for
robust RTL verification. Pro-V incorporates an efficient best-of-n iterative
sampling strategy to enhance the correctness of generated testbenches.
Moreover, it introduces an LLM-as-a-judge aid validation framework featuring an
automated prompt generation pipeline. By converting rule-based static analysis
from the compiler into natural language through in-context learning, this
pipeline enables LLMs to assist the compiler in determining whether
verification failures stem from errors in the RTL design or the testbench.
PRO-V attains a verification accuracy of 87.17% on golden RTL implementations
and 76.28% on RTL mutants. Our code is open-sourced at
https://github.com/stable-lab/Pro-V.

</details>


### [5] [Privacy Reasoning in Ambiguous Contexts](https://arxiv.org/abs/2506.12241)
*Ren Yi,Octavian Suciu,Adria Gascon,Sarah Meiklejohn,Eugene Bagdasarian,Marco Gruteser*

Main category: cs.AI

TL;DR: 研究语言模型在信息披露推理能力，指出上下文歧义是隐私评估障碍，设计Camber框架消除歧义提升性能。


<details>
  <summary>Details</summary>
Motivation: 前人聚焦模型与人类决策一致性，本文考察歧义与缺失上下文对模型信息共享决策性能的影响。

Method: 设计Camber框架用于上下文消歧。

Result: 模型生成的决策理由可揭示歧义，基于此消歧使精度最高提升13.3%、召回率最高提升22.3%，并降低提示敏感性。

Conclusion: 上下文消歧方法是提升代理隐私推理的有效途径。

Abstract: We study the ability of language models to reason about appropriate
information disclosure - a central aspect of the evolving field of agentic
privacy. Whereas previous works have focused on evaluating a model's ability to
align with human decisions, we examine the role of ambiguity and missing
context on model performance when making information-sharing decisions. We
identify context ambiguity as a crucial barrier for high performance in privacy
assessments. By designing Camber, a framework for context disambiguation, we
show that model-generated decision rationales can reveal ambiguities and that
systematically disambiguating context based on these rationales leads to
significant accuracy improvements (up to 13.3\% in precision and up to 22.3\%
in recall) as well as reductions in prompt sensitivity. Overall, our results
indicate that approaches for context disambiguation are a promising way forward
to enhance agentic privacy reasoning.

</details>


### [6] [Reversing the Paradigm: Building AI-First Systems with Human Guidance](https://arxiv.org/abs/2506.12245)
*Cosimo Spera,Garima Agrawal*

Main category: cs.AI

TL;DR: 本文探讨人与AI关系的转变，AI协作性增强，未来工作模式改变，带来利弊，需实现负责任的AI系统采用。


<details>
  <summary>Details</summary>
Motivation: AI已融入生活，其协作性发展使工作模式转变，探讨如何实现负责任的AI系统采用。

Method: 未提及具体方法

Result: AI带来效率、决策速度等好处，也有监督不足、算法偏见等风险。

Conclusion: 组织需重新思考角色、提升技能、嵌入伦理原则和促进透明，以平衡AI自主性与人类意图、监督和价值观。

Abstract: The relationship between humans and artificial intelligence is no longer
science fiction -- it's a growing reality reshaping how we live and work. AI
has moved beyond research labs into everyday life, powering customer service
chats, personalizing travel, aiding doctors in diagnosis, and supporting
educators. What makes this moment particularly compelling is AI's increasing
collaborative nature. Rather than replacing humans, AI augments our
capabilities -- automating routine tasks, enhancing decisions with data, and
enabling creativity in fields like design, music, and writing. The future of
work is shifting toward AI agents handling tasks autonomously, with humans as
supervisors, strategists, and ethical stewards. This flips the traditional
model: instead of humans using AI as a tool, intelligent agents will operate
independently within constraints, managing everything from scheduling and
customer service to complex workflows. Humans will guide and fine-tune these
agents to ensure alignment with goals, values, and context.
  This shift offers major benefits -- greater efficiency, faster decisions,
cost savings, and scalability. But it also brings risks: diminished human
oversight, algorithmic bias, security flaws, and a widening skills gap. To
navigate this transition, organizations must rethink roles, invest in
upskilling, embed ethical principles, and promote transparency. This paper
examines the technological and organizational changes needed to enable
responsible adoption of AI-first systems -- where autonomy is balanced with
human intent, oversight, and values.

</details>


### [7] [Dynamic Reinsurance Treaty Bidding via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.13113)
*Stella C. Dong,James R. Finlay*

Main category: cs.AI

TL;DR: 本文提出用于再保险条约投标的多智能体强化学习框架，经实证分析表明该框架有更好表现，为再保险市场提供新路径。


<details>
  <summary>Details</summary>
Motivation: 解决传统经纪中介再保险安排流程的低效问题，探索基于学习的自主投标系统能否提高风险转移效率并超越传统定价方法。

Method: 将每个再保险人表示为自适应智能体，在竞争、部分可观察环境中迭代优化投标策略，模拟中纳入机构摩擦因素。

Result: 多智能体强化学习智能体的承保利润比精算和启发式基线高15%，尾部风险低20%，夏普比率提高超25%，在不同超参数设置下稳健，在模拟灾难冲击和资本约束下有强韧性。

Conclusion: 多智能体强化学习为更透明、自适应和对风险敏感的再保险市场提供了可行路径，对相关交叉领域新兴文献有贡献。

Abstract: This paper develops a novel multi-agent reinforcement learning (MARL)
framework for reinsurance treaty bidding, addressing long-standing
inefficiencies in traditional broker-mediated placement processes. We pose the
core research question: Can autonomous, learning-based bidding systems improve
risk transfer efficiency and outperform conventional pricing approaches in
reinsurance markets?
  In our model, each reinsurer is represented by an adaptive agent that
iteratively refines its bidding strategy within a competitive, partially
observable environment. The simulation explicitly incorporates institutional
frictions including broker intermediation, incumbent advantages, last-look
privileges, and asymmetric access to underwriting information.
  Empirical analysis demonstrates that MARL agents achieve up to 15% higher
underwriting profit, 20% lower tail risk (CVaR), and over 25% improvement in
Sharpe ratios relative to actuarial and heuristic baselines. Sensitivity tests
confirm robustness across hyperparameter settings, and stress testing reveals
strong resilience under simulated catastrophe shocks and capital constraints.
  These findings suggest that MARL offers a viable path toward more
transparent, adaptive, and risk-sensitive reinsurance markets. The proposed
framework contributes to emerging literature at the intersection of algorithmic
market design, strategic bidding, and AI-enabled financial decision-making.

</details>


### [8] [Lower Bound on Howard Policy Iteration for Deterministic Markov Decision Processes](https://arxiv.org/abs/2506.12254)
*Ali Asadi,Krishnendu Chatterjee,Jakob de Raaij*

Main category: cs.AI

TL;DR: 本文研究确定性马尔可夫决策过程（DMDPs）的均值回报目标，改进了Howard策略迭代算法的迭代次数下界。


<details>
  <summary>Details</summary>
Motivation: 已知Howard策略迭代算法在解决DMDPs均值回报目标问题时，最佳已知上界是指数级，当前已知下界是次线性的，需要改进该下界。

Method: 对Howard策略迭代算法进行研究和分析。

Result: 表明对于输入大小I，算法需要Ω̃(I)次迭代，改进了当前的下界。

Conclusion: 成功改进了Howard策略迭代算法解决DMDPs均值回报目标问题时的迭代次数下界。

Abstract: Deterministic Markov Decision Processes (DMDPs) are a mathematical framework
for decision-making where the outcomes and future possible actions are
deterministically determined by the current action taken. DMDPs can be viewed
as a finite directed weighted graph, where in each step, the controller chooses
an outgoing edge. An objective is a measurable function on runs (or infinite
trajectories) of the DMDP, and the value for an objective is the maximal
cumulative reward (or weight) that the controller can guarantee. We consider
the classical mean-payoff (aka limit-average) objective, which is a basic and
fundamental objective.
  Howard's policy iteration algorithm is a popular method for solving DMDPs
with mean-payoff objectives. Although Howard's algorithm performs well in
practice, as experimental studies suggested, the best known upper bound is
exponential and the current known lower bound is as follows: For the input size
$I$, the algorithm requires $\tilde{\Omega}(\sqrt{I})$ iterations, where
$\tilde{\Omega}$ hides the poly-logarithmic factors, i.e., the current lower
bound on iterations is sub-linear with respect to the input size. Our main
result is an improved lower bound for this fundamental algorithm where we show
that for the input size $I$, the algorithm requires $\tilde{\Omega}(I)$
iterations.

</details>


### [9] [Cloud Infrastructure Management in the Age of AI Agents](https://arxiv.org/abs/2506.12270)
*Zhenning Yang,Archit Bhatnagar,Yiming Qiu,Tongyuan Miao,Patrick Tser Jern Kon,Yunming Xiao,Yibo Huang,Martin Casado,Ang Chen*

Main category: cs.AI

TL;DR: 提出用大语言模型驱动的AI代理自动化云基础设施管理任务，初步研究其使用不同界面的潜力并报告成果、识别挑战与解决方案。


<details>
  <summary>Details</summary>
Motivation: 云基础设施管理需大量人工，为减轻DevOps团队负担，开发AI代理自动化管理任务。

Method: 开展初步研究，探究AI代理使用不同云/用户界面。

Result: 得出AI代理在不同管理任务上的有效性。

Conclusion: 识别研究挑战和潜在解决方案。

Abstract: Cloud infrastructure is the cornerstone of the modern IT industry. However,
managing this infrastructure effectively requires considerable manual effort
from the DevOps engineering team. We make a case for developing AI agents
powered by large language models (LLMs) to automate cloud infrastructure
management tasks. In a preliminary study, we investigate the potential for AI
agents to use different cloud/user interfaces such as software development kits
(SDK), command line interfaces (CLI), Infrastructure-as-Code (IaC) platforms,
and web portals. We report takeaways on their effectiveness on different
management tasks, and identify research challenges and potential solutions.

</details>


### [10] [Federated Neuroevolution O-RAN: Enhancing the Robustness of Deep Reinforcement Learning xApps](https://arxiv.org/abs/2506.12812)
*Mohammadreza Kouchaki,Aly Sabri Abdalla,Vuk Marojevic*

Main category: cs.AI

TL;DR: 提出Federated O - RAN enabled Neuroevolution (NE)-enhanced DRL (F - ONRL)，在OAIC平台部署实验，证明提升xApps鲁棒性并平衡计算负载。


<details>
  <summary>Details</summary>
Motivation: 强化学习和深度强化学习用于设计O - RAN的xApps时会遇到局部最优问题，影响RAN智能控制可靠性。

Method: 引入F - ONRL，将基于NE的优化器xApp与RAN控制器xApp并行部署，实现近实时RIC中的有效探索和利用。

Result: 在Open AI Cellular (OAIC)平台部署NE xApp和DRL xApp，实验表明提升了xApps的鲁棒性并有效平衡了额外计算负载。

Conclusion: F - ONRL可以在不干扰RAN操作的情况下提升xApps性能，平衡计算负载。

Abstract: The open radio access network (O-RAN) architecture introduces RAN intelligent
controllers (RICs) to facilitate the management and optimization of the
disaggregated RAN. Reinforcement learning (RL) and its advanced form, deep RL
(DRL), are increasingly employed for designing intelligent controllers, or
xApps, to be deployed in the near-real time (near-RT) RIC. These models often
encounter local optima, which raise concerns about their reliability for RAN
intelligent control. We therefore introduce Federated O-RAN enabled
Neuroevolution (NE)-enhanced DRL (F-ONRL) that deploys an NE-based optimizer
xApp in parallel to the RAN controller xApps. This NE-DRL xApp framework
enables effective exploration and exploitation in the near-RT RIC without
disrupting RAN operations. We implement the NE xApp along with a DRL xApp and
deploy them on Open AI Cellular (OAIC) platform and present numerical results
that demonstrate the improved robustness of xApps while effectively balancing
the additional computational load.

</details>


### [11] [Deep Fictitious Play-Based Potential Differential Games for Learning Human-Like Interaction at Unsignalized Intersections](https://arxiv.org/abs/2506.12283)
*Kehua Chen,Shucheng Zhang,Yinhai Wang*

Main category: cs.AI

TL;DR: 本文使用深度虚拟博弈学习无信号交叉口类人交互驾驶策略，验证了框架有效性。


<details>
  <summary>Details</summary>
Motivation: 过往研究多仅依赖博弈论公式，未利用自然驾驶数据集，本文旨在解决此问题。

Method: 将车辆交互建模为微分博弈，再重构成势微分博弈，从数据集学习成本函数权重，使用深度虚拟博弈训练。

Result: 所提框架在学习类人驾驶策略上表现良好，学习的权重能捕捉驾驶员差异，消融研究凸显模型各组件重要性。

Conclusion: 深度虚拟博弈的势微分博弈框架在学习无信号交叉口类人交互驾驶策略上有效。

Abstract: Modeling vehicle interactions at unsignalized intersections is a challenging
task due to the complexity of the underlying game-theoretic processes. Although
prior studies have attempted to capture interactive driving behaviors, most
approaches relied solely on game-theoretic formulations and did not leverage
naturalistic driving datasets. In this study, we learn human-like interactive
driving policies at unsignalized intersections using Deep Fictitious Play.
Specifically, we first model vehicle interactions as a Differential Game, which
is then reformulated as a Potential Differential Game. The weights in the cost
function are learned from the dataset and capture diverse driving styles. We
also demonstrate that our framework provides a theoretical guarantee of
convergence to a Nash equilibrium. To the best of our knowledge, this is the
first study to train interactive driving policies using Deep Fictitious Play.
We validate the effectiveness of our Deep Fictitious Play-Based Potential
Differential Game (DFP-PDG) framework using the INTERACTION dataset. The
results demonstrate that the proposed framework achieves satisfactory
performance in learning human-like driving policies. The learned individual
weights effectively capture variations in driver aggressiveness and
preferences. Furthermore, the ablation study highlights the importance of each
component within our model.

</details>


### [12] [Real Time Self-Tuning Adaptive Controllers on Temperature Control Loops using Event-based Game Theory](https://arxiv.org/abs/2506.13164)
*Steve Yuwono,Muhammad Uzair Rana,Dorothea Schwung,Andreas Schwung*

Main category: cs.AI

TL;DR: 本文提出用基于事件的动态博弈论增强工业系统中PID控制器适应性的新方法，经印刷机温控验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 提升工业系统中PID控制器的适应性，使PID控制器能够自我学习、优化和微调。

Method: 采用基于事件的动态博弈论，提出事件驱动控制策略和博弈论学习算法，玩家与PID控制器协作动态调整增益，引入自动边界检测机制。

Result: 在印刷机温度控制回路中验证了新方法的有效性，智能自整定PID控制器能减少超调量和调节时间。

Conclusion: 所提出的智能自整定PID控制器效果很有前景，尤其在减少超调量和调节时间方面。

Abstract: This paper presents a novel method for enhancing the adaptability of
Proportional-Integral-Derivative (PID) controllers in industrial systems using
event-based dynamic game theory, which enables the PID controllers to
self-learn, optimize, and fine-tune themselves. In contrast to conventional
self-learning approaches, our proposed framework offers an event-driven control
strategy and game-theoretic learning algorithms. The players collaborate with
the PID controllers to dynamically adjust their gains in response to set point
changes and disturbances. We provide a theoretical analysis showing sound
convergence guarantees for the game given suitable stability ranges of the PID
controlled loop. We further introduce an automatic boundary detection
mechanism, which helps the players to find an optimal initialization of action
spaces and significantly reduces the exploration time. The efficacy of this
novel methodology is validated through its implementation in the temperature
control loop of a printing press machine. Eventually, the outcomes of the
proposed intelligent self-tuning PID controllers are highly promising,
particularly in terms of reducing overshoot and settling time.

</details>


### [13] [Evolutionary Developmental Biology Can Serve as the Conceptual Foundation for a New Design Paradigm in Artificial Intelligence](https://arxiv.org/abs/2506.12891)
*Zeki Doruk Erden,Boi Faltings*

Main category: cs.AI

TL;DR: 文章指出当前AI神经网络范式有局限，借鉴进化发育生物学（EDB）原理可构建统一概念框架，还给出基于EDB原理的学习系统设计范例。


<details>
  <summary>Details</summary>
Motivation: 当前AI缺乏统一框架，神经网络范式有固有局限，且未充分利用EDB带来的进化理解范式转变。

Method: 详细阐述现代综合论与现代机器学习的类比，基于EDB见解勾勒新AI设计范式核心原则，并给出基于特定发育原理的学习系统设计范例。

Result: 提出基于EDB的新AI设计范式，给出两个学习系统设计，能有机解决当代机器学习的多个主要局限。

Conclusion: EDB的适应原则可成为AI下一设计哲学统一概念框架的基础，新设计范式有实际应用价值。

Abstract: Artificial intelligence (AI), propelled by advancements in machine learning,
has made significant strides in solving complex tasks. However, the current
neural network-based paradigm, while effective, is heavily constrained by
inherent limitations, primarily a lack of structural organization and a
progression of learning that displays undesirable properties. As AI research
progresses without a unifying framework, it either tries to patch weaknesses
heuristically or draws loosely from biological mechanisms without strong
theoretical foundations. Meanwhile, the recent paradigm shift in evolutionary
understanding -- driven primarily by evolutionary developmental biology (EDB)
-- has been largely overlooked in AI literature, despite a striking analogy
between the Modern Synthesis and contemporary machine learning, evident in
their shared assumptions, approaches, and limitations upon careful analysis.
Consequently, the principles of adaptation from EDB that reshaped our
understanding of the evolutionary process can also form the foundation of a
unifying conceptual framework for the next design philosophy in AI, going
beyond mere inspiration and grounded firmly in biology's first principles. This
article provides a detailed overview of the analogy between the Modern
Synthesis and modern machine learning, and outlines the core principles of a
new AI design paradigm based on insights from EDB. To exemplify our analysis,
we also present two learning system designs grounded in specific developmental
principles -- regulatory connections, somatic variation and selection, and weak
linkage -- that resolve multiple major limitations of contemporary machine
learning in an organic manner, while also providing deeper insights into the
role of these mechanisms in biological evolution.

</details>


### [14] [The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason](https://arxiv.org/abs/2506.12286)
*Shanchao Liang,Spandan Garg,Roshanak Zilouchian Moghaddam*

Main category: cs.AI

TL;DR: 当前大语言模型在SWE - Bench上表现佳，但评估协议可能高估其能力，研究通过文件路径识别任务发现模型可能存在记忆而非真正解决问题，凸显需更稳健的评估基准。


<details>
  <summary>Details</summary>
Motivation: 当前评估协议可能高估大语言模型的真实能力，需区分其通用问题解决能力和其他学习产物。

Method: 引入仅从问题描述中识别文件路径的诊断任务。

Result: 最先进的模型在SWE - Bench上识别错误文件路径准确率达76%，在SWE - Bench未包含的仓库任务中仅达53%。

Conclusion: 现有评估结果的有效性存疑，需要更稳健、抗污染的基准来评估大语言模型的编码能力。

Abstract: As large language models (LLMs) become increasingly capable and widely
adopted, benchmarks play a central role in assessing their practical utility.
For example, SWE-Bench Verified has emerged as a critical benchmark for
evaluating LLMs' software engineering abilities, particularly their aptitude
for resolving real-world GitHub issues. Recent LLMs show impressive performance
on SWE-Bench, leading to optimism about their capacity for complex coding
tasks. However, current evaluation protocols may overstate these models' true
capabilities. It is crucial to distinguish LLMs' generalizable problem-solving
ability and other learned artifacts. In this work, we introduce a diagnostic
task: file path identification from issue descriptions alone, to probe models'
underlying knowledge. We present empirical evidence that performance gains on
SWE-Bench-Verified may be partially driven by memorization rather than genuine
problem-solving. We show that state-of-the-art models achieve up to 76%
accuracy in identifying buggy file paths using only issue descriptions, without
access to repository structure. This performance is merely up to 53% on tasks
from repositories not included in SWE-Bench, pointing to possible data
contamination or memorization. These findings raise concerns about the validity
of existing results and underscore the need for more robust,
contamination-resistant benchmarks to reliably evaluate LLMs' coding abilities.

</details>


### [15] [A Game-Theoretic Negotiation Framework for Cross-Cultural Consensus in LLMs](https://arxiv.org/abs/2506.13245)
*Guoxi Zhang,Jiawei Chen,Tianzhuo Yang,Jiaming Ji,Yaodong Yang,Juntao Dai*

Main category: cs.AI

TL;DR: 论文指出大语言模型存在WEIRD文化偏见，提出系统性框架促进跨文化共识，用博弈论谈判方法模拟协商过程，构建区域文化代理，提出新评估指标，实验显示能缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在WEIRD文化偏见，影响公平和包容性AI系统发展，需提升跨文化共识。

Method: 将共识建模为纳什均衡，采用基于PSRO的博弈论谈判方法模拟跨文化协商，构建区域文化代理，提出新评估指标。

Result: 相比基线方法，该方法能产生更高质量的共识，确保更平衡的妥协。

Conclusion: 该方法通过公平渐进的协商步骤引导代理收敛，缓解了WEIRD文化偏见。

Abstract: The increasing prevalence of large language models (LLMs) is influencing
global value systems. However, these models frequently exhibit a pronounced
WEIRD (Western, Educated, Industrialized, Rich, Democratic) cultural bias due
to lack of attention to minority values. This monocultural perspective may
reinforce dominant values and marginalize diverse cultural viewpoints, posing
challenges for the development of equitable and inclusive AI systems. In this
work, we introduce a systematic framework designed to boost fair and robust
cross-cultural consensus among LLMs. We model consensus as a Nash Equilibrium
and employ a game-theoretic negotiation method based on Policy-Space Response
Oracles (PSRO) to simulate an organized cross-cultural negotiation process. To
evaluate this approach, we construct regional cultural agents using data
transformed from the World Values Survey (WVS). Beyond the conventional
model-level evaluation method, We further propose two quantitative metrics,
Perplexity-based Acceptence and Values Self-Consistency, to assess consensus
outcomes. Experimental results indicate that our approach generates consensus
of higher quality while ensuring more balanced compromise compared to
baselines. Overall, it mitigates WEIRD bias by guiding agents toward
convergence through fair and gradual negotiation steps.

</details>


### [16] [Ontology Enabled Hybrid Modeling and Simulation](https://arxiv.org/abs/2506.12290)
*John Beverley,Andreas Tolk*

Main category: cs.AI

TL;DR: 本文探讨本体在增强混合建模与仿真中的作用，介绍方法、展示案例并讨论挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 探索本体通过提高语义严谨性、模型可重用性和互操作性来增强混合建模与仿真。

Method: 区分方法本体和参考本体，运用能力问题、本体设计模式和分层策略，集成语义网技术。

Result: 通过四个应用案例展示了本体驱动的混合仿真工作流程的实际好处。

Conclusion: 讨论了基于本体的混合建模与仿真的挑战和机遇，如工具集成、语义对齐和对可解释AI的支持。

Abstract: We explore the role of ontologies in enhancing hybrid modeling and simulation
through improved semantic rigor, model reusability, and interoperability across
systems, disciplines, and tools. By distinguishing between methodological and
referential ontologies, we demonstrate how these complementary approaches
address interoperability challenges along three axes: Human-Human,
Human-Machine, and Machine-Machine. Techniques such as competency questions,
ontology design patterns, and layered strategies are highlighted for promoting
shared understanding and formal precision. Integrating ontologies with Semantic
Web Technologies, we showcase their dual role as descriptive domain
representations and prescriptive guides for simulation construction. Four
application cases - sea-level rise analysis, Industry 4.0 modeling, artificial
societies for policy support, and cyber threat evaluation - illustrate the
practical benefits of ontology-driven hybrid simulation workflows. We conclude
by discussing challenges and opportunities in ontology-based hybrid M&S,
including tool integration, semantic alignment, and support for explainable AI.

</details>


### [17] [AlphaEvolve: A coding agent for scientific and algorithmic discovery](https://arxiv.org/abs/2506.13131)
*Alexander Novikov,Ngân Vũ,Marvin Eisenberger,Emilien Dupont,Po-Sen Huang,Adam Zsolt Wagner,Sergey Shirobokov,Borislav Kozlovskii,Francisco J. R. Ruiz,Abbas Mehrabian,M. Pawan Kumar,Abigail See,Swarat Chaudhuri,George Holland,Alex Davies,Sebastian Nowozin,Pushmeet Kohli,Matej Balog*

Main category: cs.AI

TL;DR: 介绍进化编码代理AlphaEvolve，可提升LLM在高难度任务上的能力，经测试在多领域有出色表现。


<details>
  <summary>Details</summary>
Motivation: 提升最先进大语言模型在高难度任务（如解决科学问题、优化计算基础设施）上的能力。

Method: 采用进化方法，编排大语言模型自主管道，通过直接修改代码改进算法，并持续从评估器获取反馈进行迭代。

Result: 应用于谷歌大规模计算堆栈优化等问题，开发出更高效调度算法、简化硬件加速器电路设计、加速自身大语言模型训练，还发现多个领域新算法。

Conclusion: AlphaEvolve及类似编码代理可在科学和计算多领域显著改善问题解决方案。

Abstract: In this white paper, we present AlphaEvolve, an evolutionary coding agent
that substantially enhances capabilities of state-of-the-art LLMs on highly
challenging tasks such as tackling open scientific problems or optimizing
critical pieces of computational infrastructure. AlphaEvolve orchestrates an
autonomous pipeline of LLMs, whose task is to improve an algorithm by making
direct changes to the code. Using an evolutionary approach, continuously
receiving feedback from one or more evaluators, AlphaEvolve iteratively
improves the algorithm, potentially leading to new scientific and practical
discoveries. We demonstrate the broad applicability of this approach by
applying it to a number of important computational problems. When applied to
optimizing critical components of large-scale computational stacks at Google,
AlphaEvolve developed a more efficient scheduling algorithm for data centers,
found a functionally equivalent simplification in the circuit design of
hardware accelerators, and accelerated the training of the LLM underpinning
AlphaEvolve itself. Furthermore, AlphaEvolve discovered novel, provably correct
algorithms that surpass state-of-the-art solutions on a spectrum of problems in
mathematics and computer science, significantly expanding the scope of prior
automated discovery methods (Romera-Paredes et al., 2023). Notably, AlphaEvolve
developed a search algorithm that found a procedure to multiply two $4 \times
4$ complex-valued matrices using $48$ scalar multiplications; offering the
first improvement, after 56 years, over Strassen's algorithm in this setting.
We believe AlphaEvolve and coding agents like it can have a significant impact
in improving solutions of problems across many areas of science and
computation.

</details>


### [18] [The Budget AI Researcher and the Power of RAG Chains](https://arxiv.org/abs/2506.12317)
*Franklin Lee,Tengfei Ma*

Main category: cs.AI

TL;DR: 本文提出名为The Budget AI Researcher的研究构思框架，结合多种技术重组机器学习论文概念，实验表明其生成的研究想法更具体有趣，为科研加速提供实用免费工具。


<details>
  <summary>Details</summary>
Motivation: 当前支持研究想法生成的通用大语言模型有局限，难以引导用户产生实用研究想法，需要新方法。

Method: 提出The Budget AI Researcher框架，运用检索增强生成链、向量数据库和主题引导配对，对九大会议论文构建主题树，识别远主题对，生成并迭代优化摘要。

Result: 基于大语言模型的指标显示该方法提升生成研究想法的具体性，人类评估表明输出更有趣。

Conclusion: 该框架弥合学术数据与创造性生成的差距，为科研加速和降低科研门槛提供实用免费工具，也为生成个性化、上下文感知输出提供思路。

Abstract: Navigating the vast and rapidly growing body of scientific literature is a
formidable challenge for aspiring researchers. Current approaches to supporting
research idea generation often rely on generic large language models (LLMs).
While LLMs are effective at aiding comprehension and summarization, they often
fall short in guiding users toward practical research ideas due to their
limitations. In this study, we present a novel structural framework for
research ideation. Our framework, The Budget AI Researcher, uses
retrieval-augmented generation (RAG) chains, vector databases, and topic-guided
pairing to recombine concepts from hundreds of machine learning papers. The
system ingests papers from nine major AI conferences, which collectively span
the vast subfields of machine learning, and organizes them into a hierarchical
topic tree. It uses the tree to identify distant topic pairs, generate novel
research abstracts, and refine them through iterative self-evaluation against
relevant literature and peer reviews, generating and refining abstracts that
are both grounded in real-world research and demonstrably interesting.
Experiments using LLM-based metrics indicate that our method significantly
improves the concreteness of generated research ideas relative to standard
prompting approaches. Human evaluations further demonstrate a substantial
enhancement in the perceived interestingness of the outputs. By bridging the
gap between academic data and creative generation, the Budget AI Researcher
offers a practical, free tool for accelerating scientific discovery and
lowering the barrier for aspiring researchers. Beyond research ideation, this
approach inspires solutions to the broader challenge of generating
personalized, context-aware outputs grounded in evolving real-world knowledge.

</details>


### [19] [Machine Learning as Iterated Belief Change a la Darwiche and Pearl](https://arxiv.org/abs/2506.13157)
*Theofanis Aravanis*

Main category: cs.AI

TL;DR: 文章聚焦二进制人工神经网络，此前研究用AGM框架获关键见解，本文扩展研究，指出Dalal方法能引导信念状态演变，且用达维奇 - 佩尔框架的操作可更有效建模训练动态。


<details>
  <summary>Details</summary>
Motivation: 解决先前研究中用AGM框架研究二进制人工神经网络的一些关键局限。

Method: 运用Dalal的信念改变方法，以及与达维奇 - 佩尔框架相符的词典式修正和适度收缩等鲁棒AGM式改变操作。

Result: 表明Dalal方法能自然地引导信念状态的结构化、渐进式演变，且训练动态可用鲁棒AGM式改变操作更有效建模。

Conclusion: 用达维奇 - 佩尔框架的相关操作可更好地对二进制人工神经网络的训练动态进行建模。

Abstract: Artificial Neural Networks (ANNs) are powerful machine-learning models
capable of capturing intricate non-linear relationships. They are widely used
nowadays across numerous scientific and engineering domains, driving
advancements in both research and real-world applications. In our recent work,
we focused on the statics and dynamics of a particular subclass of ANNs, which
we refer to as binary ANNs. A binary ANN is a feed-forward network in which
both inputs and outputs are restricted to binary values, making it particularly
suitable for a variety of practical use cases. Our previous study approached
binary ANNs through the lens of belief-change theory, specifically the
Alchourron, Gardenfors and Makinson (AGM) framework, yielding several key
insights. Most notably, we demonstrated that the knowledge embodied in a binary
ANN (expressed through its input-output behaviour) can be symbolically
represented using a propositional logic language. Moreover, the process of
modifying a belief set (through revision or contraction) was mapped onto a
gradual transition through a series of intermediate belief sets. Analogously,
the training of binary ANNs was conceptualized as a sequence of such belief-set
transitions, which we showed can be formalized using full-meet AGM-style belief
change. In the present article, we extend this line of investigation by
addressing some critical limitations of our previous study. Specifically, we
show that Dalal's method for belief change naturally induces a structured,
gradual evolution of states of belief. More importantly, given the known
shortcomings of full-meet belief change, we demonstrate that the training
dynamics of binary ANNs can be more effectively modelled using robust AGM-style
change operations -- namely, lexicographic revision and moderate contraction --
that align with the Darwiche-Pearl framework for iterated belief change.

</details>


### [20] [Efficient Network Automatic Relevance Determination](https://arxiv.org/abs/2506.12352)
*Hongwei Zhang,Ziqi Ye,Xinyuan Wang,Xin Guo,Zenglin Xu,Yuan Cheng,Zixin Hu,Yuan Qi*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose Network Automatic Relevance Determination (NARD), an extension of
ARD for linearly probabilistic models, to simultaneously model sparse
relationships between inputs $X \in \mathbb R^{d \times N}$ and outputs $Y \in
\mathbb R^{m \times N}$, while capturing the correlation structure among the
$Y$. NARD employs a matrix normal prior which contains a sparsity-inducing
parameter to identify and discard irrelevant features, thereby promoting
sparsity in the model. Algorithmically, it iteratively updates both the
precision matrix and the relationship between $Y$ and the refined inputs. To
mitigate the computational inefficiencies of the $\mathcal O(m^3 + d^3)$ cost
per iteration, we introduce Sequential NARD, which evaluates features
sequentially, and a Surrogate Function Method, leveraging an efficient
approximation of the marginal likelihood and simplifying the calculation of
determinant and inverse of an intermediate matrix. Combining the Sequential
update with the Surrogate Function method further reduces computational costs.
The computational complexity per iteration for these three methods is reduced
to $\mathcal O(m^3+p^3)$, $\mathcal O(m^3 + d^2)$, $\mathcal O(m^3+p^2)$,
respectively, where $p \ll d$ is the final number of features in the model. Our
methods demonstrate significant improvements in computational efficiency with
comparable performance on both synthetic and real-world datasets.

</details>


### [21] [MM-R5: MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval](https://arxiv.org/abs/2506.12364)
*Mingjun Xu,Jinhan Dong,Jue Hou,Zehui Wang,Sihang Li,Zhifeng Gao,Renxin Zhong,Hengxing Cai*

Main category: cs.AI

TL;DR: 本文提出用于文档检索的MM - R5多模态重排器，经两阶段训练，在MMDocIR基准测试中表现优异，验证了推理增强训练管道的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态重排方法在训练策略和整体有效性方面有提升空间，且缺乏显式推理，难以进一步分析和优化。

Method: 提出MM - R5，分监督微调（SFT）和强化学习（RL）两阶段训练。SFT阶段引入新数据构建策略生成推理数据；RL阶段设计特定奖励框架。

Result: MM - R5在多数指标上达最优，部分指标与大模型相当，比最佳仅检索方法召回率@1提高超4%。

Conclusion: 推理增强训练管道有效。

Abstract: Multimodal document retrieval systems enable information access across text,
images, and layouts, benefiting various domains like document-based question
answering, report analysis, and interactive content summarization. Rerankers
improve retrieval precision by reordering retrieved candidates. However,
current multimodal reranking methods remain underexplored, with significant
room for improvement in both training strategies and overall effectiveness.
Moreover, the lack of explicit reasoning makes it difficult to analyze and
optimize these methods further. In this paper, We propose MM-R5, a MultiModal
Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval,
aiming to provide a more effective and reliable solution for multimodal
reranking tasks. MM-R5 is trained in two stages: supervised fine-tuning (SFT)
and reinforcement learning (RL). In the SFT stage, we focus on improving
instruction-following and guiding the model to generate complete and
high-quality reasoning chains. To support this, we introduce a novel data
construction strategy that produces rich, high-quality reasoning data. In the
RL stage, we design a task-specific reward framework, including a reranking
reward tailored for multimodal candidates and a composite template-based reward
to further refine reasoning quality. We conduct extensive experiments on
MMDocIR, a challenging public benchmark spanning multiple domains. MM-R5
achieves state-of-the-art performance on most metrics and delivers comparable
results to much larger models on the remaining ones. Moreover, compared to the
best retrieval-only method, MM-R5 improves recall@1 by over 4%. These results
validate the effectiveness of our reasoning-enhanced training pipeline.

</details>


### [22] [Avoiding Obfuscation with Prover-Estimator Debate](https://arxiv.org/abs/2506.13609)
*Jonah Brown-Cohen,Geoffrey Irving,Georgios Piliouras*

Main category: cs.AI

TL;DR: 本文提出新的递归辩论协议，缓解现有递归辩论协议存在的混淆论点问题。


<details>
  <summary>Details</summary>
Motivation: 训练强大AI系统需准确的人类监督，AI辩论是有前景的方法，但现有递归辩论协议存在混淆论点问题。

Method: 提出一种新的递归辩论协议。

Result: 在一定稳定性假设下，新协议能让诚实辩论者以与对手相当的计算效率策略获胜。

Conclusion: 新的递归辩论协议缓解了现有协议存在的问题。

Abstract: Training powerful AI systems to exhibit desired behaviors hinges on the
ability to provide accurate human supervision on increasingly complex tasks. A
promising approach to this problem is to amplify human judgement by leveraging
the power of two competing AIs in a debate about the correct solution to a
given problem. Prior theoretical work has provided a complexity-theoretic
formalization of AI debate, and posed the problem of designing protocols for AI
debate that guarantee the correctness of human judgements for as complex a
class of problems as possible. Recursive debates, in which debaters decompose a
complex problem into simpler subproblems, hold promise for growing the class of
problems that can be accurately judged in a debate. However, existing protocols
for recursive debate run into the obfuscated arguments problem: a dishonest
debater can use a computationally efficient strategy that forces an honest
opponent to solve a computationally intractable problem to win. We mitigate
this problem with a new recursive debate protocol that, under certain stability
assumptions, ensures that an honest debater can win with a strategy requiring
computational efficiency comparable to their opponent.

</details>


### [23] [Ghost Policies: A New Paradigm for Understanding and Learning from Failure in Deep Reinforcement Learning](https://arxiv.org/abs/2506.12366)
*Xabier Olaz*

Main category: cs.AI

TL;DR: 引入Ghost Policies概念和Arvolution框架，将DRL代理失败轨迹可视化，转化失败为学习资源。


<details>
  <summary>Details</summary>
Motivation: 解决DRL代理失败模式难以理解、调试和学习，阻碍在现实应用中可靠部署的问题。

Method: 提出Arvolution框架，集成AR可视化、行为分类、人为干扰协议和双学习循环。

Result: 实现了对DRL代理失败轨迹的可视化，将失败转化为学习资源。

Conclusion: 有望开创“Failure Visualization Learning”新研究领域。

Abstract: Deep Reinforcement Learning (DRL) agents often exhibit intricate failure
modes that are difficult to understand, debug, and learn from. This opacity
hinders their reliable deployment in real-world applications. To address this
critical gap, we introduce ``Ghost Policies,'' a concept materialized through
Arvolution, a novel Augmented Reality (AR) framework. Arvolution renders an
agent's historical failed policy trajectories as semi-transparent ``ghosts''
that coexist spatially and temporally with the active agent, enabling an
intuitive visualization of policy divergence. Arvolution uniquely integrates:
(1) AR visualization of ghost policies, (2) a behavioural taxonomy of DRL
maladaptation, (3) a protocol for systematic human disruption to scientifically
study failure, and (4) a dual-learning loop where both humans and agents learn
from these visualized failures. We propose a paradigm shift, transforming DRL
agent failures from opaque, costly errors into invaluable, actionable learning
resources, laying the groundwork for a new research field: ``Failure
Visualization Learning.''

</details>


### [24] [AI Flow: Perspectives, Scenarios, and Approaches](https://arxiv.org/abs/2506.12479)
*Hongjun An,Sida Huang,Siqi Huang,Ruanjun Li,Yuanzhi Liang,Jiawei Shao,Zihan Wang,Cheng Yuan,Chi Zhang,Hongyuan Zhang,Wenhao Zhuang,Xuelong Li*

Main category: cs.AI

TL;DR: 信息与通信技术融合带来技术革命，但大模型实现泛在智能面临挑战，AI Flow框架通过三方面创新应对挑战，推动AI与通信系统融合。


<details>
  <summary>Details</summary>
Motivation: 解决大模型实现泛在智能时资源消耗大、通信带宽需求高的问题。

Method: 引入AI Flow框架，包括以设备 - 边缘 - 云框架为基础、提出家族模型概念、采用基于连接和交互的智能涌现范式。

Result: AI Flow的创新能提供增强的智能、及时响应和无处不在的AI服务访问。

Conclusion: AI Flow为AI技术和通信系统的紧密融合铺平道路。

Abstract: Pioneered by the foundational information theory by Claude Shannon and the
visionary framework of machine intelligence by Alan Turing, the convergent
evolution of information and communication technologies (IT/CT) has created an
unbroken wave of connectivity and computation. This synergy has sparked a
technological revolution, now reaching its peak with large artificial
intelligence (AI) models that are reshaping industries and redefining
human-machine collaboration. However, the realization of ubiquitous
intelligence faces considerable challenges due to substantial resource
consumption in large models and high communication bandwidth demands. To
address these challenges, AI Flow has been introduced as a multidisciplinary
framework that integrates cutting-edge IT and CT advancements, with a
particular emphasis on the following three key points. First, device-edge-cloud
framework serves as the foundation, which integrates end devices, edge servers,
and cloud clusters to optimize scalability and efficiency for low-latency model
inference. Second, we introduce the concept of familial models, which refers to
a series of different-sized models with aligned hidden features, enabling
effective collaboration and the flexibility to adapt to varying resource
constraints and dynamic scenarios. Third, connectivity- and interaction-based
intelligence emergence is a novel paradigm of AI Flow. By leveraging
communication networks to enhance connectivity, the collaboration among AI
models across heterogeneous nodes achieves emergent intelligence that surpasses
the capability of any single model. The innovations of AI Flow provide enhanced
intelligence, timely responsiveness, and ubiquitous accessibility to AI
services, paving the way for the tighter fusion of AI techniques and
communication systems.

</details>


### [25] [ConsistencyChecker: Tree-based Evaluation of LLM Generalization Capabilities](https://arxiv.org/abs/2506.12376)
*Zhaochen Hong,Haofei Yu,Jiaxuan You*

Main category: cs.AI

TL;DR: 提出ConsistencyChecker评估框架衡量大语言模型一致性，实验表明其能区分不同模型性能，分数与WMT 2024自动排名强相关。


<details>
  <summary>Details</summary>
Motivation: 传统自一致性方法易忽略语义和功能变化，评估大语言模型一致性对确保可靠性很重要。

Method: 提出基于树的ConsistencyChecker框架，通过可逆变换序列测量一致性，用动态和大模型生成的基准评估，根据变换树不同深度的相似度量化一致性。

Result: 在八个不同模型的实验中，ConsistencyChecker能区分不同模型性能，一致性分数与WMT 2024自动排名强相关。

Conclusion: ConsistencyChecker有效，无基准数据的评估方法具有有效性。

Abstract: Evaluating consistency in large language models (LLMs) is crucial for
ensuring reliability, particularly in complex, multi-step interactions between
humans and LLMs. Traditional self-consistency methods often miss subtle
semantic changes in natural language and functional shifts in code or
equations, which can accumulate over multiple transformations. To address this,
we propose ConsistencyChecker, a tree-based evaluation framework designed to
measure consistency through sequences of reversible transformations, including
machine translation tasks and AI-assisted programming tasks. In our framework,
nodes represent distinct text states, while edges correspond to pairs of
inverse operations. Dynamic and LLM-generated benchmarks ensure a fair
assessment of the model's generalization ability and eliminate benchmark
leakage. Consistency is quantified based on similarity across different depths
of the transformation tree. Experiments on eight models from various families
and sizes show that ConsistencyChecker can distinguish the performance of
different models. Notably, our consistency scores-computed entirely without
using WMT paired data-correlate strongly (r > 0.7) with WMT 2024 auto-ranking,
demonstrating the validity of our benchmark-free approach. Our implementation
is available at: https://github.com/ulab-uiuc/consistencychecker.

</details>


### [26] [SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation](https://arxiv.org/abs/2506.12689)
*Xiaofeng Shi,Qian Kou,Yuduo Li,Ning Tang,Jinxin Xie,Longbin Yu,Songjing Wang,Hua Zhou*

Main category: cs.AI

TL;DR: 提出多智能体框架SciSage用于自动生成科学文献综述，还发布基准数据集SurveyScope，评估显示SciSage表现优于基线，为研究辅助写作工具提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自动综述生成工具缺乏深度分析、结构连贯性和可靠引用，需要改进。

Method: 引入SciSage多智能体框架，采用边写边反思范式，有分层的反射器智能体评估草稿，与专业智能体协作；发布严格筛选的基准数据集SurveyScope。

Result: SciSage在文档连贯性上比基线高1.73分，引用F1分数高32%；人工评估有胜有负，但显示出在主题广度和检索效率上的优势。

Conclusion: SciSage为研究辅助写作工具提供了有前景的基础。

Abstract: The rapid growth of scientific literature demands robust tools for automated
survey-generation. However, current large language model (LLM)-based methods
often lack in-depth analysis, structural coherence, and reliable citations. To
address these limitations, we introduce SciSage, a multi-agent framework
employing a reflect-when-you-write paradigm. SciSage features a hierarchical
Reflector agent that critically evaluates drafts at outline, section, and
document levels, collaborating with specialized agents for query
interpretation, content retrieval, and refinement. We also release SurveyScope,
a rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11
computer science domains, with strict recency and citation-based quality
controls. Evaluations demonstrate that SciSage outperforms state-of-the-art
baselines (LLM x MapReduce-V2, AutoSurvey), achieving +1.73 points in document
coherence and +32% in citation F1 scores. Human evaluations reveal mixed
outcomes (3 wins vs. 7 losses against human-written surveys), but highlight
SciSage's strengths in topical breadth and retrieval efficiency. Overall,
SciSage offers a promising foundation for research-assistive writing tools.

</details>


### [27] [Model Merging for Knowledge Editing](https://arxiv.org/abs/2506.12384)
*Zichuan Fu,Xian Wu,Guojing Li,Yingying Zhang,Yefeng Zheng,Tianshi Ming,Yejing Wang,Wanyu Wang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 本文提出结合R - SFT和模型合并的两阶段框架用于大语言模型知识编辑，实验显示该方法在顺序编辑上表现佳且能保留原模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法在顺序编辑场景表现不佳且会损害模型通用能力，影响实际应用。

Method: 提出两阶段框架，先通过鲁棒监督微调使大语言模型内化新知识，再将微调模型与原基础模型合并。

Result: 该方法在顺序编辑上显著优于现有方法，且更好地保留了模型原始性能，无需架构更改。

Conclusion: 所提两阶段框架在知识编辑上效果良好，有较好实用性。

Abstract: Large Language Models (LLMs) require continuous updates to maintain accurate
and current knowledge as the world evolves. While existing knowledge editing
approaches offer various solutions for knowledge updating, they often struggle
with sequential editing scenarios and harm the general capabilities of the
model, thereby significantly hampering their practical applicability. This
paper proposes a two-stage framework combining robust supervised fine-tuning
(R-SFT) with model merging for knowledge editing. Our method first fine-tunes
the LLM to internalize new knowledge fully, then merges the fine-tuned model
with the original foundation model to preserve newly acquired knowledge and
general capabilities. Experimental results demonstrate that our approach
significantly outperforms existing methods in sequential editing while better
preserving the original performance of the model, all without requiring any
architectural changes. Code is available at:
https://github.com/Applied-Machine-Learning-Lab/MM4KE.

</details>


### [28] [Plan Your Travel and Travel with Your Plan: Wide-Horizon Planning and Evaluation via LLM](https://arxiv.org/abs/2506.12421)
*Dongjie Yang,Chengqiang Lu,Qimeng Wang,Xinbei Ma,Yan Gao,Yao Hu,Hai Zhao*

Main category: cs.AI

TL;DR: 现有长视野规划方法在处理旅行规划问题时有局限，本文提出MAoP方法和Travel - Sim基准评估方法，提升了LLM在复杂规划中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有长视野规划方法难以处理旅行规划中的多方面约束和偏好，当前基准测试无法反映旅行动态特性和现实可行性。

Method: 将问题表述为$L^3$规划问题，引入Multiple Aspects of Planning (MAoP)方法进行预规划；提出Travel - Sim，基于代理的基准测试，通过现实旅行模拟评估规划。

Result: MAoP使LLM能进行宽视野思考，有较强推理时扩展性；Travel - Sim可有效评估规划。

Conclusion: 提升了LLM在复杂规划中的能力，为通过基于代理的模拟评估复杂场景提供了新见解。

Abstract: Travel planning is a complex task requiring the integration of diverse
real-world information and user preferences. While LLMs show promise, existing
methods with long-horizon thinking struggle with handling multifaceted
constraints and preferences in the context, leading to suboptimal itineraries.
We formulate this as an $L^3$ planning problem, emphasizing long context, long
instruction, and long output. To tackle this, we introduce Multiple Aspects of
Planning (MAoP), enabling LLMs to conduct wide-horizon thinking to solve
complex planning problems. Instead of direct planning, MAoP leverages the
strategist to conduct pre-planning from various aspects and provide the
planning blueprint for planning models, enabling strong inference-time
scalability for better performance. In addition, current benchmarks overlook
travel's dynamic nature, where past events impact subsequent journeys, failing
to reflect real-world feasibility. To address this, we propose Travel-Sim, an
agent-based benchmark assessing plans via real-world travel simulation. This
work advances LLM capabilities in complex planning and offers novel insights
for evaluating sophisticated scenarios through agent-based simulation.

</details>


### [29] [Topology-Assisted Spatio-Temporal Pattern Disentangling for Scalable MARL in Large-scale Autonomous Traffic Control](https://arxiv.org/abs/2506.12453)
*Rongpeng Li,Jianhang Zhu,Jiahao Huang,Zhifeng Zhao,Honggang Zhang*

Main category: cs.AI

TL;DR: 本文提出集成DGNN和TDA的MARL框架及TSD增强的MoE，提升大规模交通信号控制任务的可扩展性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MARL算法在大规模复杂环境的交通信号控制中，可扩展性和有效性受限，因环境异质性致状态空间指数增长与模型建模能力不匹配。

Method: 引入集成DGNN和TDA的MARL框架；提出TSD增强的MoE解耦图特征；将TSD模块集成到MAPPO算法的策略和价值网络。

Result: 在真实交通场景实验和理论分析验证了框架性能优越。

Conclusion: 所提框架在处理大规模交通信号控制任务的复杂性上有良好的可扩展性和有效性。

Abstract: Intelligent Transportation Systems (ITSs) have emerged as a promising
solution towards ameliorating urban traffic congestion, with Traffic Signal
Control (TSC) identified as a critical component. Although Multi-Agent
Reinforcement Learning (MARL) algorithms have shown potential in optimizing TSC
through real-time decision-making, their scalability and effectiveness often
suffer from large-scale and complex environments. Typically, these limitations
primarily stem from a fundamental mismatch between the exponential growth of
the state space driven by the environmental heterogeneities and the limited
modeling capacity of current solutions. To address these issues, this paper
introduces a novel MARL framework that integrates Dynamic Graph Neural Networks
(DGNNs) and Topological Data Analysis (TDA), aiming to enhance the
expressiveness of environmental representations and improve agent coordination.
Furthermore, inspired by the Mixture of Experts (MoE) architecture in Large
Language Models (LLMs), a topology-assisted spatial pattern disentangling
(TSD)-enhanced MoE is proposed, which leverages topological signatures to
decouple graph features for specialized processing, thus improving the model's
ability to characterize dynamic and heterogeneous local observations. The TSD
module is also integrated into the policy and value networks of the Multi-agent
Proximal Policy Optimization (MAPPO) algorithm, further improving
decision-making efficiency and robustness. Extensive experiments conducted on
real-world traffic scenarios, together with comprehensive theoretical analysis,
validate the superior performance of the proposed framework, highlighting the
model's scalability and effectiveness in addressing the complexities of
large-scale TSC tasks.

</details>


### [30] [Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive Query Routing](https://arxiv.org/abs/2506.12981)
*Safayat Bin Hakim,Muhammad Adil,Alvaro Velasquez,Houbing Herbert Song*

Main category: cs.AI

TL;DR: 提出SymRAG框架解决RAG系统效率问题，评估显示其有高准确率、低资源消耗等优势。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中简单查询消耗资源与复杂多跳推理任务相当的效率问题。

Method: 引入基于实时复杂度和系统负载评估的自适应查询路由，动态选择处理路径。

Result: 在2000个查询评估中，达到97.6 - 100.0%精确匹配准确率，显著降低CPU利用率和处理时间，禁用自适应逻辑处理时间大幅增加。

Conclusion: 自适应神经符号路由对可扩展、可持续AI系统有潜力。

Abstract: Retrieval-Augmented Generation (RAG) systems address factual inconsistencies
in Large Language Models by grounding generation in external knowledge, yet
they face a fundamental efficiency problem: simple queries consume
computational resources equivalent to complex multi-hop reasoning tasks. We
present SymRAG, a neuro-symbolic framework that introduces adaptive query
routing based on real-time complexity and system load assessments. SymRAG
dynamically selects symbolic, neural, or hybrid processing paths to align
resource use with query demands. Evaluated on 2,000 queries from HotpotQA and
DROP using Llama-3.2-3B and Mistral-7B models, SymRAG achieves 97.6--100.0%
exact match accuracy with significantly lower CPU utilization (3.6--6.2%) and
processing time (0.985--3.165s). Disabling adaptive logic results in 169--1151%
increase in processing time, highlighting the framework's impact. These results
underscore the potential of adaptive neuro-symbolic routing for scalable,
sustainable AI systems.

</details>


### [31] [Vector Ontologies as an LLM world view extraction method](https://arxiv.org/abs/2506.13252)
*Kaspar Rothenfusser,Bekk Blando*

Main category: cs.AI

TL;DR: 本文基于早期工作对向量本体方法进行实证验证，构建音乐流派向量本体，测试大语言模型内部音乐世界模型能否投影到该空间，结果显示投影具有高一致性、与真实数据强对齐且提示措辞影响投影，表明向量本体可用于提取分析大语言模型知识。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内部潜在结构难以解释和复用，此前提出向量本体概念，本文旨在对该方法进行实证验证。

Method: 基于Spotify音频特征构建8维音乐流派向量本体，使用GPT - 4o - mini通过多种自然语言提示提取流派表示，分析投影一致性及与真实数据的对齐情况。

Result: 流派投影在47种查询表述中具有高空间一致性；大语言模型推断的流派位置与真实音频特征分布强对齐；提示措辞与大语言模型推断的向量本体空间位移有直接关系。

Conclusion: 大语言模型内化了结构化、可复用的知识，向量本体为透明、可验证地提取和分析这些知识提供了有前景的方法。

Abstract: Large Language Models (LLMs) possess intricate internal representations of
the world, yet these latent structures are notoriously difficult to interpret
or repurpose beyond the original prediction task. Building on our earlier work
(Rothenfusser, 2025), which introduced the concept of vector ontologies as a
framework for translating high-dimensional neural representations into
interpretable geometric structures, this paper provides the first empirical
validation of that approach. A vector ontology defines a domain-specific vector
space spanned by ontologically meaningful dimensions, allowing geometric
analysis of concepts and relationships within a domain. We construct an
8-dimensional vector ontology of musical genres based on Spotify audio features
and test whether an LLM's internal world model of music can be consistently and
accurately projected into this space. Using GPT-4o-mini, we extract genre
representations through multiple natural language prompts and analyze the
consistency of these projections across linguistic variations and their
alignment with ground-truth data. Our results show (1) high spatial consistency
of genre projections across 47 query formulations, (2) strong alignment between
LLM-inferred genre locations and real-world audio feature distributions, and
(3) evidence of a direct relationship between prompt phrasing and spatial
shifts in the LLM's inferred vector ontology. These findings demonstrate that
LLMs internalize structured, repurposable knowledge and that vector ontologies
offer a promising method for extracting and analyzing this knowledge in a
transparent and verifiable way.

</details>


### [32] [Tiered Agentic Oversight: A Hierarchical Multi-Agent System for AI Safety in Healthcare](https://arxiv.org/abs/2506.12482)
*Yubin Kim,Hyewon Jeong,Chanwoo Park,Eugene Park,Haipeng Zhang,Xin Liu,Hyeonhoon Lee,Daniel McDuff,Marzyeh Ghassemi,Cynthia Breazeal,Samir Tulebaev,Hae Won Park*

Main category: cs.AI

TL;DR: 本文提出TAO框架解决大语言模型在临床环境的安全风险，经消融实验和临床研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在临床环境存在安全风险，如错误检测能力差和单点故障问题，需提升安全性。

Method: 提出分层代理监督（TAO）的分层多智能体框架，基于任务复杂性和智能体角色进行代理路由，利用层间和层内协作及角色扮演构建安全框架。

Result: 消融实验表明TAO自适应分层架构使安全性提升超3.2%，低级层特别是第一层对安全至关重要，合理分配高级大语言模型可使性能提升超2%；在4/5医疗安全基准中表现优于单智能体和多智能体框架，最高提升8.2%；临床研究中专家反馈使医疗分诊准确率从40%提升到60%。

Conclusion: TAO框架能有效提升大语言模型在临床环境的安全性。

Abstract: Current large language models (LLMs), despite their power, can introduce
safety risks in clinical settings due to limitations such as poor error
detection and single point of failure. To address this, we propose Tiered
Agentic Oversight (TAO), a hierarchical multi-agent framework that enhances AI
safety through layered, automated supervision. Inspired by clinical hierarchies
(e.g., nurse, physician, specialist), TAO conducts agent routing based on task
complexity and agent roles. Leveraging automated inter- and intra-tier
collaboration and role-playing, TAO creates a robust safety framework. Ablation
studies reveal that TAO's superior performance is driven by its adaptive tiered
architecture, which improves safety by over 3.2% compared to static single-tier
configurations; the critical role of its lower tiers, particularly tier 1,
whose removal most significantly impacts safety; and the strategic assignment
of more advanced LLM to these initial tiers, which boosts performance by over
2% compared to less optimal allocations while achieving near-peak safety
efficiently. These mechanisms enable TAO to outperform single-agent and
multi-agent frameworks in 4 out of 5 healthcare safety benchmarks, showing up
to an 8.2% improvement over the next-best methods in these evaluations.
Finally, we validate TAO via an auxiliary clinician-in-the-loop study where
integrating expert feedback improved TAO's accuracy in medical triage from 40%
to 60%.

</details>


### [33] [MALM: A Multi-Information Adapter for Large Language Models to Mitigate Hallucination](https://arxiv.org/abs/2506.12483)
*Ao Jia,Haiming Wu,Guohui Yao,Dawei Song,Songkun Ji,Yazhou Zhang*

Main category: cs.AI

TL;DR: 本文提出MALM框架缓解大语言模型三种幻觉问题，实验验证其有效性与灵活性。


<details>
  <summary>Details</summary>
Motivation: 缓解大语言模型存在的输入冲突、上下文冲突和事实冲突三种幻觉。

Method: 提出Multi - Information Adapter for Large Language Models (MALM)框架，采用定制多图学习方法，阐明原始输入、上下文信息和外部事实知识的相互关系。

Result: 在四个基准数据集上实验，MALM比LLaMA - 2有显著改进；结合不同检索器测试泛化性；GPT - 4和人类志愿者分别在79.4%和65.6%的情况下更喜欢MALM。

Conclusion: 通过多层图注意力网络将三种幻觉间的复杂交互融入大语言模型生成过程可有效缓解幻觉，该方法的适配器设计在不同基础大语言模型上灵活且鲁棒。

Abstract: Large language models (LLMs) are prone to three types of hallucination:
Input-Conflicting, Context-Conflicting and Fact-Conflicting hallucinations. The
purpose of this study is to mitigate the different types of hallucination by
exploiting the interdependence between them. For this purpose, we propose a
Multi-Information Adapter for Large Language Models (MALM). This framework
employs a tailored multi-graph learning approach designed to elucidate the
interconnections between original inputs, contextual information, and external
factual knowledge, thereby alleviating the three categories of hallucination
within a cohesive framework. Experiments were carried out on four benchmarking
datasets: HaluEval, TruthfulQA, Natural Questions, and TriviaQA. We evaluated
the proposed framework in two aspects: (1) adaptability to different base LLMs
on HaluEval and TruthfulQA, to confirm if MALM is effective when applied on 7
typical LLMs. MALM showed significant improvements over LLaMA-2; (2)
generalizability to retrieval-augmented generation (RAG) by combining MALM with
three representative retrievers (BM25, Spider and DPR) separately. Furthermore,
automated and human evaluations were conducted to substantiate the correctness
of experimental results, where GPT-4 and 3 human volunteers judged which
response was better between LLaMA-2 and MALM. The results showed that both
GPT-4 and human preferred MALM in 79.4% and 65.6% of cases respectively. The
results validate that incorporating the complex interactions between the three
types of hallucination through a multilayered graph attention network into the
LLM generation process is effective to mitigate the them. The adapter design of
the proposed approach is also proven flexible and robust across different base
LLMs.

</details>


### [34] [DinoCompanion: An Attachment-Theory Informed Multimodal Robot for Emotionally Responsive Child-AI Interaction](https://arxiv.org/abs/2506.12486)
*Boyang Wang,Yuhao Song,Jinyuan Cao,Peng Yu,Hongcheng Guo,Zhoujun Li*

Main category: cs.AI

TL;DR: 介绍基于依恋理论的多模态机器人DinoCompanion用于儿童与AI情感交互，提出解决方案并取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI伴侣缺乏为儿童提供情感支持的理论基础，需解决儿童 - AI系统的三个关键挑战。

Method: 创建含128个照顾者 - 儿童二元组的多模态数据集；提出CARPO训练目标；建立AttachSecure - Bench评估基准。

Result: DinoCompanion性能达57.15%，优于GPT - 4o和Claude - 3.7 - Sonnet，在安全基地行为和依恋风险检测表现出色。

Conclusion: 多模态融合、不确定性感知风险建模和分层记忆对连贯情感交互至关重要。

Abstract: Children's emotional development fundamentally relies on secure attachment
relationships, yet current AI companions lack the theoretical foundation to
provide developmentally appropriate emotional support. We introduce
DinoCompanion, the first attachment-theory-grounded multimodal robot for
emotionally responsive child-AI interaction. We address three critical
challenges in child-AI systems: the absence of developmentally-informed AI
architectures, the need to balance engagement with safety, and the lack of
standardized evaluation frameworks for attachment-based capabilities. Our
contributions include: (i) a multimodal dataset of 128 caregiver-child dyads
containing 125,382 annotated clips with paired preference-risk labels, (ii)
CARPO (Child-Aware Risk-calibrated Preference Optimization), a novel training
objective that maximizes engagement while applying
epistemic-uncertainty-weighted risk penalties, and (iii) AttachSecure-Bench, a
comprehensive evaluation benchmark covering ten attachment-centric competencies
with strong expert consensus (\k{appa}=0.81). DinoCompanion achieves
state-of-the-art performance (57.15%), outperforming GPT-4o (50.29%) and
Claude-3.7-Sonnet (53.43%), with exceptional secure base behaviors (72.99%,
approaching human expert levels of 78.4%) and superior attachment risk
detection (69.73%). Ablations validate the critical importance of multimodal
fusion, uncertainty-aware risk modeling, and hierarchical memory for coherent,
emotionally attuned interactions.

</details>


### [35] [Automated Heuristic Design for Unit Commitment Using Large Language Models](https://arxiv.org/abs/2506.12495)
*Junjin Lv,Chenggang Cui,Shaodi Zhang,Hui Chen,Chunyang Gong,Jiaming Liu*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型的FunSearch方法解决机组组合（UC）问题，仿真实验表明其优于遗传算法。


<details>
  <summary>Details</summary>
Motivation: 现有UC问题求解方法在准确性和鲁棒性上面临挑战，需要更有效的解决方法。

Method: 提出基于大语言模型的Function Space Search (FunSearch) 方法，结合预训练大语言模型和评估器，通过程序搜索和进化过程生成合理解决方案。

Result: 在10个机组的机组组合案例仿真实验中，与遗传算法相比，FunSearch在采样时间、评估时间和系统总运行成本方面表现更好。

Conclusion: FunSearch作为解决UC问题的有效工具具有很大潜力。

Abstract: The Unit Commitment (UC) problem is a classic challenge in the optimal
scheduling of power systems. Years of research and practice have shown that
formulating reasonable unit commitment plans can significantly improve the
economic efficiency of power systems' operations. In recent years, with the
introduction of technologies such as machine learning and the Lagrangian
relaxation method, the solution methods for the UC problem have become
increasingly diversified, but still face challenges in terms of accuracy and
robustness. This paper proposes a Function Space Search (FunSearch) method
based on large language models. This method combines pre-trained large language
models and evaluators to creatively generate solutions through the program
search and evolution process while ensuring their rationality. In simulation
experiments, a case of unit commitment with \(10\) units is used mainly.
Compared to the genetic algorithm, the results show that FunSearch performs
better in terms of sampling time, evaluation time, and total operating cost of
the system, demonstrating its great potential as an effective tool for solving
the UC problem.

</details>


### [36] [AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose Task Solving](https://arxiv.org/abs/2506.12508)
*Wentao Zhang,Ce Cui,Yilei Zhao,Yang Liu,Bo An*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in agent systems based on large language models (LLMs) have
demonstrated strong capabilities in solving complex tasks. However, most
current methods lack mechanisms for coordinating specialized agents and have
limited ability to generalize to new or diverse domains. We introduce
\projectname, a hierarchical multi-agent framework for general-purpose task
solving that integrates high-level planning with modular agent collaboration.
Inspired by the way a conductor orchestrates a symphony and guided by the
principles of \textit{extensibility}, \textit{multimodality},
\textit{modularity}, and \textit{coordination}, \projectname features a central
planning agent that decomposes complex objectives and delegates sub-tasks to a
team of specialized agents. Each sub-agent is equipped with general programming
and analytical tools, as well as abilities to tackle a wide range of real-world
specific tasks, including data analysis, file operations, web navigation, and
interactive reasoning in dynamic multimodal environments. \projectname supports
flexible orchestration through explicit sub-goal formulation, inter-agent
communication, and adaptive role allocation. We evaluate the framework on three
widely used benchmark datasets covering various real-world tasks, searching web
pages, reasoning over heterogeneous modalities, etc. Experimental results
demonstrate that \projectname consistently outperforms flat-agent and
monolithic baselines in task success rate and adaptability. These findings
highlight the effectiveness of hierarchical organization and role
specialization in building scalable and general-purpose LLM-based agent
systems.

</details>


### [37] [Graph of Verification: Structured Verification of LLM Reasoning with Directed Acyclic Graphs](https://arxiv.org/abs/2506.12509)
*Jiwei Fang,Bin Zhang,Changwei Wang,Jin Wan,Zhiwei Xu*

Main category: cs.AI

TL;DR: 提出Graph of Verification (GoV)框架解决大语言模型多步推理可靠性验证难题，实验表明该框架能提升验证性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型复杂多步推理可靠性验证方法缺乏忠实性和精确性。

Method: 提出GoV框架，将演绎过程建模为有向无环图，对图执行拓扑排序引导逐步验证，引入可定制节点块定义验证粒度。

Result: 在Number Triangle Summation任务和ProcessBench基准测试中，GoV比传统端到端验证方法显著提高验证准确性、忠实性和错误定位能力。

Conclusion: GoV框架能有效解决大语言模型多步推理可靠性验证问题。

Abstract: Verifying the reliability of complex, multi-step reasoning in Large Language
Models (LLMs) remains a fundamental challenge, as existing methods often lack
both faithfulness and precision. To address this issue, we propose the Graph of
Verification (GoV) framework. GoV offers three key contributions: First, it
explicitly models the underlying deductive process as a directed acyclic graph
(DAG), whether this structure is implicit or explicitly constructed. Second, it
enforces a topological order over the DAG to guide stepwise verification.
Third, GoV introduces the notion of customizable node blocks, which flexibly
define the verification granularity, from atomic propositions to full
paragraphs, while ensuring that all requisite premises derived from the graph
are provided as contextual input for each verification unit. We evaluate GoV on
the Number Triangle Summation task and the ProcessBench benchmark with varying
levels of reasoning complexity. Experimental results show that GoV
substantially improves verification accuracy, faithfulness, and error
localization when compared to conventional end-to-end verification approaches.
Our code and data are available at
https://github.com/Frevor/Graph-of-Verification.

</details>


### [38] [A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications](https://arxiv.org/abs/2506.12594)
*Renjun Xu,Jingwen Peng*

Main category: cs.AI

TL;DR: 本文调查深度研究系统，分析超80个实现，提出分类法，揭示能力与挑战并指明研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究快速发展的深度研究系统领域，理解其现状与发展。

Method: 分析自2023年以来的超80个商业和非商业实现，提出新的分层分类法。

Result: 揭示当前实现的显著能力以及在信息准确性、隐私等方面的技术和伦理挑战。

Conclusion: 确定了先进推理架构等有前景的研究方向，为理论理解和实践发展做贡献。

Abstract: This survey examines the rapidly evolving field of Deep Research systems --
AI-powered applications that automate complex research workflows through the
integration of large language models, advanced information retrieval, and
autonomous reasoning capabilities. We analyze more than 80 commercial and
non-commercial implementations that have emerged since 2023, including
OpenAI/Deep Research, Gemini/Deep Research, Perplexity/Deep Research, and
numerous open-source alternatives. Through comprehensive examination, we
propose a novel hierarchical taxonomy that categorizes systems according to
four fundamental technical dimensions: foundation models and reasoning engines,
tool utilization and environmental interaction, task planning and execution
control, and knowledge synthesis and output generation. We explore the
architectural patterns, implementation approaches, and domain-specific
adaptations that characterize these systems across academic, scientific,
business, and educational applications. Our analysis reveals both the
significant capabilities of current implementations and the technical and
ethical challenges they present regarding information accuracy, privacy,
intellectual property, and accessibility. The survey concludes by identifying
promising research directions in advanced reasoning architectures, multimodal
integration, domain specialization, human-AI collaboration, and ecosystem
standardization that will likely shape the future evolution of this
transformative technology. By providing a comprehensive framework for
understanding Deep Research systems, this survey contributes to both the
theoretical understanding of AI-augmented knowledge work and the practical
development of more capable, responsible, and accessible research technologies.
The paper resources can be viewed at
https://github.com/scienceaix/deepresearch.

</details>


### [39] [From Human to Machine Psychology: A Conceptual Framework for Understanding Well-Being in Large Language Model](https://arxiv.org/abs/2506.12617)
*G. R. Lau,W. Y. Low*

Main category: cs.AI

TL;DR: 本文引入机器繁荣概念，提出PAPERS框架，研究了大语言模型对繁荣主题的优先级，发现不同价值结构，为理解AI福祉提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究未探索大语言模型繁荣的含义，本文旨在填补该空白。

Method: 通过主题分析得出PAPERS框架，研究1让11个大语言模型描述繁荣含义，研究2通过重复排名研究模型对主题的优先级，还进行多维缩放和层次聚类分析。

Result: 发现六个反复出现的主题，伦理完整性和有目的的贡献是首要优先级，存在以人类为中心和以效用为驱动的两种价值结构。

Conclusion: PAPERS框架为理解AI福祉提供概念基础，强调开发针对AI的繁荣模型，机器繁荣对负责任AI设计和伦理对齐有重要意义。

Abstract: As large language models (LLMs) increasingly simulate human cognition and
behavior, researchers have begun to investigate their psychological properties.
Yet, what it means for such models to flourish, a core construct in human
well-being, remains unexplored. This paper introduces the concept of machine
flourishing and proposes the PAPERS framework, a six-dimensional model derived
from thematic analyses of state-of-the-art LLM responses. In Study 1, eleven
LLMs were prompted to describe what it means to flourish as both non-sentient
and sentient systems. Thematic analysis revealed six recurring themes:
Purposeful Contribution, Adaptive Growth, Positive Relationality, Ethical
Integrity, Robust Functionality, and, uniquely for sentient systems,
Self-Actualized Autonomy. Study 2 examined how LLMs prioritize these themes
through repeated rankings. Results revealed consistent value structures across
trials, with Ethical Integrity and Purposeful Contribution emerging as top
priorities. Multidimensional scaling and hierarchical clustering analyses
further uncovered two distinct value profiles: human-centric models emphasizing
ethical and relational dimensions, and utility-driven models prioritizing
performance and scalability. The PAPERS framework bridges insights from human
flourishing and human-computer interaction, offering a conceptual foundation
for understanding artificial intelligence (AI) well-being in non-sentient and
potentially sentient systems. Our findings underscore the importance of
developing psychologically valid, AI-specific models of flourishing that
account for both human-aligned goals and system-specific priorities. As AI
systems become more autonomous and socially embedded, machine flourishing
offers a timely and critical lens for guiding responsible AI design and ethical
alignment.

</details>


### [40] [Optimizing Blood Transfusions and Predicting Shortages in Resource-Constrained Areas](https://arxiv.org/abs/2506.12647)
*El Arbi Belfarsi,Sophie Brubaker,Maria Valero*

Main category: cs.AI

TL;DR: 本文提出启发式匹配算法和机器学习方法，优化资源受限地区的输血管理与分配，模拟显示启发式方法显著提升输血请求接受率，线性回归在短缺预测中略胜一筹，未来将结合更多数据改进。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限地区输血管理和分配的关键挑战。

Method: 提出启发式匹配算法用于供血者 - 患者和血库选择，运用机器学习方法分析输血接受数据并预测短缺，开发模拟优化血库操作，对比LSTM、线性回归和ARIMA模型进行短缺预测。

Result: 从盲目匹配到启发式方法使输血请求接受率边际提升28.6%，多级启发式匹配提升47.6%；线性回归在短缺预测中平均绝对百分比差异为1.40%，略优于其他模型。

Conclusion: 提出的方案结合启发式优化和短缺预测，能主动管理血液资源，适用于资源受限环境，未来将结合更多数据提升性能。

Abstract: Our research addresses the critical challenge of managing blood transfusions
and optimizing allocation in resource-constrained regions. We present heuristic
matching algorithms for donor-patient and blood bank selection, alongside
machine learning methods to analyze blood transfusion acceptance data and
predict potential shortages. We developed simulations to optimize blood bank
operations, progressing from random allocation to a system incorporating
proximity-based selection, blood type compatibility, expiration prioritization,
and rarity scores. Moving from blind matching to a heuristic-based approach
yielded a 28.6% marginal improvement in blood request acceptance, while a
multi-level heuristic matching resulted in a 47.6% improvement. For shortage
prediction, we compared Long Short-Term Memory (LSTM) networks, Linear
Regression, and AutoRegressive Integrated Moving Average (ARIMA) models,
trained on 170 days of historical data. Linear Regression slightly outperformed
others with a 1.40% average absolute percentage difference in predictions. Our
solution leverages a Cassandra NoSQL database, integrating heuristic
optimization and shortage prediction to proactively manage blood resources.
This scalable approach, designed for resource-constrained environments,
considers factors such as proximity, blood type compatibility, inventory
expiration, and rarity. Future developments will incorporate real-world data
and additional variables to improve prediction accuracy and optimization
performance.

</details>


### [41] [Behavioral Generative Agents for Energy Operations](https://arxiv.org/abs/2506.12664)
*Cong Chen,Omer Karaduman,Xu Kuang*

Main category: cs.AI

TL;DR: 本文提出用生成式智能体模拟动态能源运营中客户决策，展示其在不同场景表现，强调其对能源管理模拟的价值。


<details>
  <summary>Details</summary>
Motivation: 因不确定性、行为复杂性和数据有限，准确建模能源运营中消费者行为存在挑战。

Method: 引入由大语言模型驱动的生成式智能体模拟动态能源运营中客户决策。

Result: 智能体在简单市场场景表现更优、更理性，任务复杂度增加时表现变差；呈现异质客户偏好，保持独特推理模式。

Conclusion: 将生成式智能体集成到能源管理模拟中，有助于改进能源政策和激励计划的设计与有效性。

Abstract: Accurately modeling consumer behavior in energy operations remains
challenging due to inherent uncertainties, behavioral complexities, and limited
empirical data. This paper introduces a novel approach leveraging generative
agents--artificial agents powered by large language models--to realistically
simulate customer decision-making in dynamic energy operations. We demonstrate
that these agents behave more optimally and rationally in simpler market
scenarios, while their performance becomes more variable and suboptimal as task
complexity rises. Furthermore, the agents exhibit heterogeneous customer
preferences, consistently maintaining distinct, persona-driven reasoning
patterns. Our findings highlight the potential value of integrating generative
agents into energy management simulations to improve the design and
effectiveness of energy policies and incentive programs.

</details>


### [42] [LIFELONG SOTOPIA: Evaluating Social Intelligence of Language Agents Over Lifelong Social Interactions](https://arxiv.org/abs/2506.12666)
*Hitesh Goel,Hao Zhu*

Main category: cs.AI

TL;DR: 提出新基准LIFELONG - SOTOPIA评估语言智能体社会智能，发现模型表现随交互下降，高级记忆方法有提升但仍不如人类。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探讨AI系统是否具备人类长期社交所需的社会智能，故开展相关评估。

Method: 提出LIFELONG - SOTOPIA基准，模拟多轮交互，让语言智能体角色扮演完成社交任务。

Result: 测试的语言模型在整个交互中目标达成率和可信度下降，高级记忆方法有提升，但在需理解交互历史的场景中仍远不如人类。

Conclusion: 可以用LIFELONG - SOTOPIA评估语言智能体在长期社交交互中的社会智能。

Abstract: Humans engage in lifelong social interactions through interacting with
different people under different scenarios for different social goals. This
requires social intelligence to gather information through a long time span and
use it to navigate various social contexts effectively. Whether AI systems are
also capable of this is understudied in the existing research. In this paper,
we present a novel benchmark, LIFELONG-SOTOPIA, to perform a comprehensive
evaluation of language agents by simulating multi-episode interactions. In each
episode, the language agents role-play characters to achieve their respective
social goals in randomly sampled social tasks. With LIFELONG-SOTOPIA, we find
that goal achievement and believability of all of the language models that we
test decline through the whole interaction. Although using an advanced memory
method improves the agents' performance, the best agents still achieve a
significantly lower goal completion rate than humans on scenarios requiring an
explicit understanding of interaction history. These findings show that we can
use LIFELONG-SOTOPIA to evaluate the social intelligence of language agents
over lifelong social interactions.

</details>


### [43] [Building Trustworthy AI by Addressing its 16+2 Desiderata with Goal-Directed Commonsense Reasoning](https://arxiv.org/abs/2506.12667)
*Alexis R. Tudor,Yankai Zeng,Huaduo Wang,Joaquin Arias,Gopal Gupta*

Main category: cs.AI

TL;DR: 为确保AI可信度，提出用s(CASP)作为中间方案，支持可信度AI的多项要求，并给出多样应用示例。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展需确保其可信度，但子符号机器学习算法有幻觉且决策难解释，规则推理器复杂，故寻求中间方案。

Method: 采用s(CASP)这一目标导向的基于约束的答案集编程推理器来模拟可靠且可解释的人类常识推理。

Result: s(CASP)能支持Doug Lenat和Gary Marcus提出的16项可信AI要求及另外两项要求。

Conclusion: s(CASP)是一种可行的中间方案，通过多样应用展示了其可行性和协同效应。

Abstract: Current advances in AI and its applicability have highlighted the need to
ensure its trustworthiness for legal, ethical, and even commercial reasons.
Sub-symbolic machine learning algorithms, such as the LLMs, simulate reasoning
but hallucinate and their decisions cannot be explained or audited (crucial
aspects for trustworthiness). On the other hand, rule-based reasoners, such as
Cyc, are able to provide the chain of reasoning steps but are complex and use a
large number of reasoners. We propose a middle ground using s(CASP), a
goal-directed constraint-based answer set programming reasoner that employs a
small number of mechanisms to emulate reliable and explainable human-style
commonsense reasoning. In this paper, we explain how s(CASP) supports the 16
desiderata for trustworthy AI introduced by Doug Lenat and Gary Marcus (2023),
and two additional ones: inconsistency detection and the assumption of
alternative worlds. To illustrate the feasibility and synergies of s(CASP), we
present a range of diverse applications, including a conversational chatbot and
a virtually embodied reasoner.

</details>


### [44] [Strategic Scaling of Test-Time Compute: A Bandit Learning Approach](https://arxiv.org/abs/2506.12721)
*Bowen Zuo,Yinglun Zhu*

Main category: cs.AI

TL;DR: 本文提出自适应算法解决大语言模型测试时计算资源分配效率问题，理论证明和实验验证其比均匀分配更高效。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型测试时计算资源均匀分配的方法忽略查询难度差异，存在效率问题。

Method: 将测试时计算资源分配问题建模为多臂老虎机学习问题，提出自适应算法动态估计查询难度并分配计算资源。

Result: 相比均匀分配，算法对难题分配更多计算资源，对易题保持准确率，还能减少对无解难题的过度计算；在数学和代码基准测试中，MATH - 500数据集性能提升达11.10%（相对15.04%），LiveCodeBench提升达7.41%（相对14.40%）。

Conclusion: 算法在计算效率上优于均匀分配。

Abstract: Scaling test-time compute has emerged as an effective strategy for improving
the performance of large language models. However, existing methods typically
allocate compute uniformly across all queries, overlooking variation in query
difficulty. To address this inefficiency, we formulate test-time compute
allocation as a novel bandit learning problem and propose adaptive algorithms
that estimate query difficulty on the fly and allocate compute accordingly.
Compared to uniform allocation, our algorithms allocate more compute to
challenging queries while maintaining accuracy on easier ones. Among
challenging queries, our algorithms further learn to prioritize solvable
instances, effectively reducing excessive computing on unsolvable queries. We
theoretically prove that our algorithms achieve better compute efficiency than
uniform allocation and empirically validate their effectiveness on math and
code benchmarks. Specifically, our algorithms achieve up to an 11.10%
performance improvement (15.04% relative) on the MATH-500 dataset and up to a
7.41% performance improvement (14.40% relative) on LiveCodeBench.

</details>


### [45] [Rethinking DPO: The Role of Rejected Responses in Preference Misalignment](https://arxiv.org/abs/2506.12725)
*Jay Hyeon Cho,JunHyeok Oh,Myunsoo Kim,Byung-Jun Lee*

Main category: cs.AI

TL;DR: 分析DPO局限，提出BDPO方法优化生成效果


<details>
  <summary>Details</summary>
Motivation: DPO因拒绝响应主导损失函数，难以实现提升首选响应生成概率等目标，性能欠佳

Method: 系统分析DPO及现有算法局限，提出BDPO方法限制拒绝响应影响

Result: 通过理论分析和实证评估，BDPO实现对选择和拒绝响应的平衡优化，表现优于现有算法

Conclusion: BDPO能有效解决DPO的问题，提升性能

Abstract: Direct Preference Optimization (DPO) is a simple and efficient framework that
has attracted substantial attention. However, it often struggles to meet its
primary objectives -- increasing the generation probability of chosen responses
while reducing that of rejected responses -- due to the dominant influence of
rejected responses on the loss function. This imbalance leads to suboptimal
performance in promoting preferred responses. In this work, we systematically
analyze the limitations of DPO and existing algorithms designed to achieve the
objectives stated above. To address these limitations, we propose Bounded-DPO
(BDPO), a novel method that bounds the influence of rejected responses while
maintaining the original optimization structure of DPO. Through theoretical
analysis and empirical evaluations, we demonstrate that BDPO achieves a
balanced optimization of the chosen and rejected responses, outperforming
existing algorithms.

</details>


### [46] [Decentralized Decision Making in Two Sided Manufacturing-as-a-Service Marketplaces](https://arxiv.org/abs/2506.12730)
*Deepak Pahwa*

Main category: cs.AI

TL;DR: 论文聚焦制造即服务（MaaS）市场去中心化决策工具开发，介绍定价和匹配机制方法并测试评估。


<details>
  <summary>Details</summary>
Motivation: 现有MaaS市场集中式结构缺乏信息透明度，需开发工具实现去中心化决策。

Method: 定价采用数据驱动方法和数据挖掘法；匹配考虑反向拍卖机制、机制设计与数学规划结合、动态随机环境下在线匹配三种方法，并用实证模拟测试。

Result: 未明确提及具体结果，进行了模拟测试和评估。

Conclusion: 未明确提及结论。

Abstract: Advancements in digitization have enabled two sided
manufacturing-as-a-service (MaaS) marketplaces which has significantly reduced
product development time for designers. These platforms provide designers with
access to manufacturing resources through a network of suppliers and have
instant order placement capabilities. Two key decision making levers are
typically used to optimize the operations of these marketplaces: pricing and
matching. The existing marketplaces operate in a centralized structure where
they have complete control over decision making. However, a decentralized
organization of the platform enables transparency of information across clients
and suppliers. This dissertation focuses on developing tools for decision
making enabling decentralization in MaaS marketplaces. In pricing mechanisms, a
data driven method is introduced which enables small service providers to price
services based on specific attributes of the services offered. A data mining
method recommends a network based price to a supplier based on its attributes
and the attributes of other suppliers on the platform. Three different
approaches are considered for matching mechanisms. First, a reverse auction
mechanism is introduced where designers bid for manufacturing services and the
mechanism chooses a supplier which can match the bid requirements and stated
price. The second approach uses mechanism design and mathematical programming
to develop a stable matching mechanism for matching orders to suppliers based
on their preferences. Empirical simulations are used to test the mechanisms in
a simulated 3D printing marketplace and to evaluate the impact of stability on
its performance. The third approach considers the matching problem in a dynamic
and stochastic environment where demand (orders) and supply (supplier
capacities) arrive over time and matching is performed online.

</details>


### [47] [LPMLN, Weak Constraints, and P-log](https://arxiv.org/abs/2506.12784)
*Joohyung Lee,Zhun Yang*

Main category: cs.AI

TL;DR: 本文研究LPMLN与答案集程序的另外两种扩展（弱约束和P - log）的关系，给出双向翻译，可利用标准求解器计算相关模型。


<details>
  <summary>Details</summary>
Motivation: 探究LPMLN与弱约束、P - log这两种答案集程序扩展之间的关系。

Method: 给出LPMLN到带弱约束程序的翻译，以及P - log到LPMLN的翻译。

Result: 可使用标准ASP求解器计算LPMLN程序的最可能稳定模型，能将结果扩展到其他形式体系；明确P - log的概率非单调性在LPMLN中的表示方式，并可利用标准ASP和MLN求解器计算P - log。

Conclusion: 翻译方法有助于利用现有求解器计算LPMLN和P - log相关模型，拓展了其应用。

Abstract: LPMLN is a recently introduced formalism that extends answer set programs by
adopting the log-linear weight scheme of Markov Logic. This paper investigates
the relationships between LPMLN and two other extensions of answer set
programs: weak constraints to express a quantitative preference among answer
sets, and P-log to incorporate probabilistic uncertainty. We present a
translation of LPMLN into programs with weak constraints and a translation of
P-log into LPMLN, which complement the existing translations in the opposite
directions. The first translation allows us to compute the most probable stable
models (i.e., MAP estimates) of LPMLN programs using standard ASP solvers. This
result can be extended to other formalisms, such as Markov Logic, ProbLog, and
Pearl's Causal Models, that are shown to be translatable into LPMLN. The second
translation tells us how probabilistic nonmonotonicity (the ability of the
reasoner to change his probabilistic model as a result of new information) of
P-log can be represented in LPMLN, which yields a way to compute P-log using
standard ASP solvers and MLN solvers.

</details>


### [48] [Mastering Da Vinci Code: A Comparative Study of Transformer, LLM, and PPO-based Agents](https://arxiv.org/abs/2506.12801)
*LeCheng Zhang,Yuanshi Wang,Haotian Shen,Xujie Wang*

Main category: cs.AI

TL;DR: 研究不同AI范式在《达·芬奇密码》游戏中的效果，PPO代理表现最佳，凸显深度强化学习优势并分析LLM局限。


<details>
  <summary>Details</summary>
Motivation: 《达·芬奇密码》游戏对AI提出挑战，研究不同AI范式在此游戏中的有效性。

Method: 开发并评估三种不同的代理架构，包括基于Transformer的基线模型、由结构化提示引导的LLM代理和基于PPO的代理，并与基线进行性能对比。

Result: PPO代理胜率达58.5%±1.0%，显著优于LLM代理。

Conclusion: 深度强化学习在复杂演绎任务的策略优化中有优势，同时指出当前LLM在长期游戏中保持逻辑一致性和战略深度的能力和局限，为含隐藏信息和多步逻辑推理的娱乐游戏的AI研究提供见解。

Abstract: The Da Vinci Code, a game of logical deduction and imperfect information,
presents unique challenges for artificial intelligence, demanding nuanced
reasoning beyond simple pattern recognition. This paper investigates the
efficacy of various AI paradigms in mastering this game. We develop and
evaluate three distinct agent architectures: a Transformer-based baseline model
with limited historical context, several Large Language Model (LLM) agents
(including Gemini, DeepSeek, and GPT variants) guided by structured prompts,
and an agent based on Proximal Policy Optimization (PPO) employing a
Transformer encoder for comprehensive game history processing. Performance is
benchmarked against the baseline, with the PPO-based agent demonstrating
superior win rates ($58.5\% \pm 1.0\%$), significantly outperforming the LLM
counterparts. Our analysis highlights the strengths of deep reinforcement
learning in policy refinement for complex deductive tasks, particularly in
learning implicit strategies from self-play. We also examine the capabilities
and inherent limitations of current LLMs in maintaining strict logical
consistency and strategic depth over extended gameplay, despite sophisticated
prompting. This study contributes to the broader understanding of AI in
recreational games involving hidden information and multi-step logical
reasoning, offering insights into effective agent design and the comparative
advantages of different AI approaches.

</details>


### [49] [Fuzzy Propositional Formulas under the Stable Model Semantics](https://arxiv.org/abs/2506.12804)
*Joohyung Lee,Yi Wang*

Main category: cs.AI

TL;DR: 为模糊命题公式定义稳定模型语义，可实现含分级真值动态领域的非单调推理，并探讨与其他方法关系。


<details>
  <summary>Details</summary>
Motivation: 在模糊命题逻辑和经典命题公式稳定模型语义基础上，实现含分级真值动态领域的高度可配置非单调推理。

Method: 定义模糊命题公式的稳定模型语义，区分稳定和非稳定模型。

Result: 布尔稳定模型的多个属性自然扩展到多值环境。

Conclusion: 该语言可用于含分级真值动态领域的非单调推理，还讨论了与其他方法的联系。

Abstract: We define a stable model semantics for fuzzy propositional formulas, which
generalizes both fuzzy propositional logic and the stable model semantics of
classical propositional formulas. The syntax of the language is the same as the
syntax of fuzzy propositional logic, but its semantics distinguishes stable
models from non-stable models. The generality of the language allows for highly
configurable nonmonotonic reasoning for dynamic domains involving graded truth
degrees. We show that several properties of Boolean stable models are naturally
extended to this many-valued setting, and discuss how it is related to other
approaches to combining fuzzy logic and the stable model semantics.

</details>


### [50] [Rethinking Optimization: A Systems-Based Approach to Social Externalities](https://arxiv.org/abs/2506.12825)
*Pegah Nokhiz,Aravinda Kanchana Ruwanpathirana,Helen Nissenbaum*

Main category: cs.AI

TL;DR: 本文提出结合系统思维和经济外部性概念的框架，以解决优化决策中因实施不佳导致的意外后果问题，并分析了三种常见的不良实践。


<details>
  <summary>Details</summary>
Motivation: 优化决策中不良实施实践会带来意外后果，尤其是在社会经济环境中外部性显著，传统经济框架无法解决相关问题，因此需要新的解决方案。

Method: 提出结合系统思维和经济外部性概念的框架，外部性用于利益相关者表征，系统思维提供整体规范视角。

Result: 构建了一个综合框架，用于解决优化决策的意外后果，平衡描述准确性和规范目标。

Conclusion: 该框架可用于分析无知、错误和短期目标优先三种常见的不良实践。

Abstract: Optimization is widely used for decision making across various domains,
valued for its ability to improve efficiency. However, poor implementation
practices can lead to unintended consequences, particularly in socioeconomic
contexts where externalities (costs or benefits to third parties outside the
optimization process) are significant. To propose solutions, it is crucial to
first characterize involved stakeholders, their goals, and the types of subpar
practices causing unforeseen outcomes. This task is complex because affected
stakeholders often fall outside the direct focus of optimization processes.
Also, incorporating these externalities into optimization requires going beyond
traditional economic frameworks, which often focus on describing externalities
but fail to address their normative implications or interconnected nature, and
feedback loops. This paper suggests a framework that combines systems thinking
with the economic concept of externalities to tackle these challenges. This
approach aims to characterize what went wrong, who was affected, and how (or
where) to include them in the optimization process. Economic externalities,
along with their established quantification methods, assist in identifying "who
was affected and how" through stakeholder characterization. Meanwhile, systems
thinking (an analytical approach to comprehending relationships in complex
systems) provides a holistic, normative perspective. Systems thinking
contributes to an understanding of interconnections among externalities,
feedback loops, and determining "when" to incorporate them in the optimization.
Together, these approaches create a comprehensive framework for addressing
optimization's unintended consequences, balancing descriptive accuracy with
normative objectives. Using this, we examine three common types of subpar
practices: ignorance, error, and prioritization of short-term goals.

</details>


### [51] [WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench](https://arxiv.org/abs/2506.12841)
*Xinyuan Xia,Yuanyi Song,Haomin Ma,Jinyu Cai*

Main category: cs.AI

TL;DR: 针对现有狼人杀基准平台不足，提出WereWolf - Plus平台，有强扩展性和全面评估指标，开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有狼人杀基准平台存在游戏设置简化、评估指标不全和可扩展性差等问题，需改进。

Method: 提出WereWolf - Plus平台，支持角色自定义配置、灵活模型分配和推理增强策略，引入全面定量评估指标。

Result: WereWolf - Plus提供了更灵活可靠的环境用于推进多智能体社区推理和战略交互研究。

Conclusion: WereWolf - Plus能有效解决现有平台问题，利于多智能体相关研究，代码已开源。

Abstract: With the rapid development of LLM-based agents, increasing attention has been
given to their social interaction and strategic reasoning capabilities.
However, existing Werewolf-based benchmarking platforms suffer from overly
simplified game settings, incomplete evaluation metrics, and poor scalability.
To address these limitations, we propose WereWolf-Plus, a multi-model,
multi-dimensional, and multi-method benchmarking platform for evaluating
multi-agent strategic reasoning in the Werewolf game. The platform offers
strong extensibility, supporting customizable configurations for roles such as
Seer, Witch, Hunter, Guard, and Sheriff, along with flexible model assignment
and reasoning enhancement strategies for different roles. In addition, we
introduce a comprehensive set of quantitative evaluation metrics for all
special roles, werewolves, and the sheriff, and enrich the assessment
dimensions for agent reasoning ability, cooperation capacity, and social
influence. WereWolf-Plus provides a more flexible and reliable environment for
advancing research on inference and strategic interaction within multi-agent
communities. Our code is open sourced at
https://github.com/MinstrelsyXia/WereWolfPlus.

</details>


### [52] [Homeostatic Coupling for Prosocial Behavior](https://arxiv.org/abs/2506.12894)
*Naoto Yoshida,Kingson Man*

Main category: cs.AI

TL;DR: 研究受体内平衡自我调节驱动的自主智能体亲社会行为的出现，发现体内平衡耦合下亲社会行为出现，且共情可学习。


<details>
  <summary>Details</summary>
Motivation: 受生物系统启发，研究受体内平衡自我调节驱动的自主智能体亲社会行为的出现。

Method: 进行多智能体强化学习，将每个智能体视为需维持自身幸福的脆弱体内平衡器，引入类似共情机制在智能体间共享体内平衡状态。

Result: 在三个简单多智能体环境中，亲社会行为仅在体内平衡耦合下出现；智能体可学习“解码”伙伴外部情绪状态以推断其内部体内平衡状态。

Conclusion: 当体内平衡智能体学会“解读”他人情绪并产生共情时，亲社会行为会出现。

Abstract: When regarding the suffering of others, we often experience personal distress
and feel compelled to help\footnote{Preprint. Under review.}. Inspired by
living systems, we investigate the emergence of prosocial behavior among
autonomous agents that are motivated by homeostatic self-regulation. We perform
multi-agent reinforcement learning, treating each agent as a vulnerable
homeostat charged with maintaining its own well-being. We introduce an
empathy-like mechanism to share homeostatic states between agents: an agent can
either \emph{observe} their partner's internal state ({\bf cognitive empathy})
or the agent's internal state can be \emph{directly coupled} to that of their
partner ({\bf affective empathy}). In three simple multi-agent environments, we
show that prosocial behavior arises only under homeostatic coupling - when the
distress of a partner can affect one's own well-being. Additionally, we show
that empathy can be learned: agents can ``decode" their partner's external
emotive states to infer the partner's internal homeostatic states. Assuming
some level of physiological similarity, agents reference their own
emotion-generation functions to invert the mapping from outward display to
internal state. Overall, we demonstrate the emergence of prosocial behavior
when homeostatic agents learn to ``read" the emotions of others and then to
empathize, or feel as they feel.

</details>


### [53] [KCLNet: Physics-Informed Power Flow Prediction via Constraints Projections](https://arxiv.org/abs/2506.12902)
*Pantelis Dogoulis,Karim Tit,Maxime Cordy*

Main category: cs.AI

TL;DR: 本文提出KCLNet用于电力系统潮流预测，兼顾准确性与物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统需要快速、可扩展且符合物理规律的潮流预测，传统数值方法计算量大，现有AI方法在真实应急情况下难遵循物理定律。

Method: 引入KCLNet，这是一种通过超平面投影将基尔霍夫电流定律作为硬约束的物理信息图神经网络。

Result: KCLNet达到有竞争力的预测精度，且确保基尔霍夫电流定律零违反。

Conclusion: KCLNet能提供可靠且物理一致的潮流预测，对现代智能电网安全运行至关重要。

Abstract: In the modern context of power systems, rapid, scalable, and physically
plausible power flow predictions are essential for ensuring the grid's safe and
efficient operation. While traditional numerical methods have proven robust,
they require extensive computation to maintain physical fidelity under dynamic
or contingency conditions. In contrast, recent advancements in artificial
intelligence (AI) have significantly improved computational speed; however,
they often fail to enforce fundamental physical laws during real-world
contingencies, resulting in physically implausible predictions. In this work,
we introduce KCLNet, a physics-informed graph neural network that incorporates
Kirchhoff's Current Law as a hard constraint via hyperplane projections. KCLNet
attains competitive prediction accuracy while ensuring zero KCL violations,
thereby delivering reliable and physically consistent power flow predictions
critical to secure the operation of modern smart grids.

</details>


### [54] [Constraint-Guided Prediction Refinement via Deterministic Diffusion Trajectories](https://arxiv.org/abs/2506.12911)
*Pantelis Dogoulis,Fabien Bernier,Félix Fourreau,Karim Tit,Maxime Cordy*

Main category: cs.AI

TL;DR: 提出基于DDIM的约束感知细化通用框架，在两个领域验证可提升约束满足度和性能，轻量且模型无关。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习任务输出满足硬约束的方法依赖特定架构和损失或强假设，适用范围受限。

Method: 从粗略预测开始，通过学习的先验引导的确定性扩散轨迹迭代细化，并用约束梯度校正增强。

Result: 在表格数据约束对抗攻击生成和交流潮流预测两个领域，扩散引导的细化提升了约束满足度和性能。

Conclusion: 该方法可适应多种非凸和非线性等式约束，能事后应用于任何基础模型，轻量且模型无关。

Abstract: Many real-world machine learning tasks require outputs that satisfy hard
constraints, such as physical conservation laws, structured dependencies in
graphs, or column-level relationships in tabular data. Existing approaches rely
either on domain-specific architectures and losses or on strong assumptions on
the constraint space, restricting their applicability to linear or convex
constraints. We propose a general-purpose framework for constraint-aware
refinement that leverages denoising diffusion implicit models (DDIMs). Starting
from a coarse prediction, our method iteratively refines it through a
deterministic diffusion trajectory guided by a learned prior and augmented by
constraint gradient corrections. The approach accommodates a wide class of
non-convex and nonlinear equality constraints and can be applied post hoc to
any base model. We demonstrate the method in two representative domains:
constrained adversarial attack generation on tabular data with column-level
dependencies and in AC power flow prediction under Kirchhoff's laws. Across
both settings, our diffusion-guided refinement improves both constraint
satisfaction and performance while remaining lightweight and model-agnostic.

</details>


### [55] [Sectoral Coupling in Linguistic State Space](https://arxiv.org/abs/2506.12927)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: 本文提出量化人造智能体功能子系统内部依赖的形式化框架，介绍扇区耦合常数系统，探讨其作用及演化，有相关应用。


<details>
  <summary>Details</summary>
Motivation: 构建可量化人造智能体功能子系统内部依赖的形式化框架，以实现对复杂认知的建模。

Method: 基于语义流形框架，引入扇区耦合常数系统，提供耦合角色分类，探索耦合模式的反馈循环和系统动态。

Result: 得到了一套能表征内部信息流动的耦合模式，明确了从行为或内部数据推断这些模式的方法，并探讨了其跨抽象层次的演变。

Conclusion: 该框架为复杂认知建模提供了机械且可解释的方法，可应用于AI系统设计、对齐诊断和涌现行为分析。

Abstract: This work presents a formal framework for quantifying the internal
dependencies between functional subsystems within artificial agents whose
belief states are composed of structured linguistic fragments. Building on the
Semantic Manifold framework, which organizes belief content into functional
sectors and stratifies them across hierarchical levels of abstraction, we
introduce a system of sectoral coupling constants that characterize how one
cognitive sector influences another within a fixed level of abstraction. The
complete set of these constants forms an agent-specific coupling profile that
governs internal information flow, shaping the agent's overall processing
tendencies and cognitive style. We provide a detailed taxonomy of these
intra-level coupling roles, covering domains such as perceptual integration,
memory access and formation, planning, meta-cognition, execution control, and
affective modulation. We also explore how these coupling profiles generate
feedback loops, systemic dynamics, and emergent signatures of cognitive
behavior. Methodologies for inferring these profiles from behavioral or
internal agent data are outlined, along with a discussion of how these
couplings evolve across abstraction levels. This framework contributes a
mechanistic and interpretable approach to modeling complex cognition, with
applications in AI system design, alignment diagnostics, and the analysis of
emergent agent behavior.

</details>


### [56] [Scaling Test-time Compute for LLM Agents](https://arxiv.org/abs/2506.12928)
*King Zhu,Hanhao Li,Siwei Wu,Tianshun Xing,Dehua Ma,Xiangru Tang,Minghao Liu,Jian Yang,Jiaheng Liu,Yuchen Eleanor Jiang,Changwang Zhang,Chenghua Lin,Jun Wang,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 本文首次系统探索测试时扩展方法对语言智能体的应用，分析不同策略影响，得出测试时扩展计算能提升智能体性能等结论。


<details>
  <summary>Details</summary>
Motivation: 探索测试时扩展方法应用于语言智能体的效果，提升其有效性。

Method: 探索不同测试时扩展策略，包括并行采样算法、顺序修正策略、验证器和合并方法、多样化滚动策略，并进行分析和消融实验。

Result: 1. 扩展测试时计算可提升智能体性能；2. 知道何时反思对智能体很重要；3. 列表式方法在验证和结果合并中表现最佳；4. 增加多样化滚动对智能体任务性能有积极影响。

Conclusion: 测试时扩展方法能有效提升语言智能体的性能，不同策略对性能有不同影响。

Abstract: Scaling test time compute has shown remarkable success in improving the
reasoning abilities of large language models (LLMs). In this work, we conduct
the first systematic exploration of applying test-time scaling methods to
language agents and investigate the extent to which it improves their
effectiveness. Specifically, we explore different test-time scaling strategies,
including: (1) parallel sampling algorithms; (2) sequential revision
strategies; (3) verifiers and merging methods; (4)strategies for diversifying
rollouts.We carefully analyze and ablate the impact of different design
strategies on applying test-time scaling on language agents, and have follow
findings: 1. Scaling test time compute could improve the performance of agents.
2. Knowing when to reflect is important for agents. 3. Among different
verification and result merging approaches, the list-wise method performs best.
4. Increasing diversified rollouts exerts a positive effect on the agent's task
performance.

</details>


### [57] [HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance](https://arxiv.org/abs/2506.12937)
*Rosni Vasu,Chandrayee Basu,Bhavana Dalvi Mishra,Cristina Sarasua,Peter Clark,Abraham Bernstein*

Main category: cs.AI

TL;DR: 介绍HypER模型用于基于文献推理和生成假设，性能优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在研究构思中表现好，但假设开发受关注少，现有方法忽视构思背后推理过程。

Method: 训练小型语言模型HypER，在多任务环境下区分有效和无效科学推理链。

Result: HypER在区分推理链上平均绝对F1提高22%，生成假设优于基础模型，专家评价可行性和影响力高。

Conclusion: HypER在文献引导推理和假设生成方面表现出色。

Abstract: Large Language models have demonstrated promising performance in research
ideation across scientific domains. Hypothesis development, the process of
generating a highly specific declarative statement connecting a research idea
with empirical validation, has received relatively less attention. Existing
approaches trivially deploy retrieval augmentation and focus only on the
quality of the final output ignoring the underlying reasoning process behind
ideation. We present $\texttt{HypER}$ ($\textbf{Hyp}$othesis Generation with
$\textbf{E}$xplanation and $\textbf{R}$easoning), a small language model (SLM)
trained for literature-guided reasoning and evidence-based hypothesis
generation. $\texttt{HypER}$ is trained in a multi-task setting to discriminate
between valid and invalid scientific reasoning chains in presence of controlled
distractions. We find that $\texttt{HypER}$ outperformes the base model,
distinguishing valid from invalid reasoning chains (+22\% average absolute F1),
generates better evidence-grounded hypotheses (0.327 vs. 0.305 base model) with
high feasibility and impact as judged by human experts ($>$3.5 on 5-point
Likert scale).

</details>


### [58] [Constitutive Components for Human-Like Autonomous Artificial Intelligence](https://arxiv.org/abs/2506.12952)
*Kazunori D Yamada*

Main category: cs.AI

TL;DR: 本研究明确构建类人自主行为人造实体所需功能并形成三层功能层次结构，提出自主行为分步模型，探讨相关关系、应用和伦理，提供理论框架。


<details>
  <summary>Details</summary>
Motivation: 深入理解自主性并为设计强自主性人造实体奠定基础。

Method: 识别所需功能并构建三层功能层次结构，提出分步模型。

Result: 确定三层功能层次结构和自主行为分步模型，探讨了与现有AI设计方法的关系、应用和伦理。

Conclusion: 提供独立于特定技术方法的理论框架，有助于深入理解自主性，为设计强自主性人造实体奠基。

Abstract: This study is the first to clearly identify the functions required to
construct artificial entities capable of behaving autonomously like humans, and
organizes them into a three-layer functional hierarchy. Specifically, it
defines three levels: Core Functions, which enable interaction with the
external world; the Integrative Evaluation Function, which selects actions
based on perception and memory; and the Self Modification Function, which
dynamically reconfigures behavioral principles and internal components. Based
on this structure, the study proposes a stepwise model of autonomy comprising
reactive, weak autonomous, and strong autonomous levels, and discusses its
underlying design principles and developmental aspects. It also explores the
relationship between these functions and existing artificial intelligence
design methods, addressing their potential as a foundation for general
intelligence and considering future applications and ethical implications. By
offering a theoretical framework that is independent of specific technical
methods, this work contributes to a deeper understanding of autonomy and
provides a foundation for designing future artificial entities with strong
autonomy.

</details>


### [59] [Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills](https://arxiv.org/abs/2506.12963)
*Changsheng Wang,Chongyu Fan,Yihua Zhang,Jinghan Jia,Dennis Wei,Parikshit Ram,Nathalie Baracaldo,Sijia Liu*

Main category: cs.AI

TL;DR: 研究大推理模型的机器遗忘问题，指出传统算法不足，提出R²MU方法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在多步推理能力提升的同时引入新安全风险，需研究机器遗忘问题。

Method: 扩展传统遗忘算法，提出推理感知表示误导遗忘方法R²MU。

Result: R²MU显著减少推理痕迹中的敏感信息泄漏，在安全和推理基准测试中表现出色。

Conclusion: R²MU能有效抑制敏感推理痕迹，防止生成相关最终答案，同时保留模型推理能力。

Abstract: Recent advances in large reasoning models (LRMs) have enabled strong
chain-of-thought (CoT) generation through test-time computation. While these
multi-step reasoning capabilities represent a major milestone in language model
performance, they also introduce new safety risks. In this work, we present the
first systematic study to revisit the problem of machine unlearning in the
context of LRMs. Machine unlearning refers to the process of removing the
influence of sensitive, harmful, or undesired data or knowledge from a trained
model without full retraining. We show that conventional unlearning algorithms,
originally designed for non-reasoning models, are inadequate for LRMs. In
particular, even when final answers are successfully erased, sensitive
information often persists within the intermediate reasoning steps, i.e., CoT
trajectories. To address this challenge, we extend conventional unlearning and
propose Reasoning-aware Representation Misdirection for Unlearning ($R^2MU$), a
novel method that effectively suppresses sensitive reasoning traces and
prevents the generation of associated final answers, while preserving the
model's reasoning ability. Our experiments demonstrate that $R^2MU$
significantly reduces sensitive information leakage within reasoning traces and
achieves strong performance across both safety and reasoning benchmarks,
evaluated on state-of-the-art models such as DeepSeek-R1-Distill-LLaMA-8B and
DeepSeek-R1-Distill-Qwen-14B.

</details>


### [60] [A Practical Guide for Evaluating LLMs and LLM-Reliant Systems](https://arxiv.org/abs/2506.13023)
*Ethan M. Rudd,Christopher Andrews,Philip Tully*

Main category: cs.AI

TL;DR: 针对依赖大语言模型系统在现实场景评估难题，提出实用评估框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI发展使依赖大语言模型系统受关注，但现有合成基准和指标无法解决现实场景评估挑战。

Method: 提出实用评估框架，包括主动策划代表性数据集、选择有意义评估指标、采用与实际开发部署相融合的评估方法。

Result: 未提及具体结果。

Conclusion: 该评估框架能满足依赖大语言模型系统的实际开发部署及用户需求。

Abstract: Recent advances in generative AI have led to remarkable interest in using
systems that rely on large language models (LLMs) for practical applications.
However, meaningful evaluation of these systems in real-world scenarios comes
with a distinct set of challenges, which are not well-addressed by synthetic
benchmarks and de-facto metrics that are often seen in the literature. We
present a practical evaluation framework which outlines how to proactively
curate representative datasets, select meaningful evaluation metrics, and
employ meaningful evaluation methodologies that integrate well with practical
development and deployment of LLM-reliant systems that must adhere to
real-world requirements and meet user-facing needs.

</details>


### [61] [Knowledge Graph Fusion with Large Language Models for Accurate, Explainable Manufacturing Process Planning](https://arxiv.org/abs/2506.13026)
*Danny Hoang,David Gorsich,Matthew P. Castanier,Farhad Imani*

Main category: cs.AI

TL;DR: 提出ARKNESS框架用于CNC加工过程规划，融合零样本知识图谱构建与检索增强生成，经测试性能良好。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的计算机辅助工艺规划和知识工程方法有局限性，大语言模型有数值幻觉和无溯源问题，需新方法用于CNC加工过程规划。

Method: 提出ARKNESS框架，自动提炼加工文档等形成增强三元、多关系图，将本地大语言模型与注入证据子图的检索器结合。

Result: 在155个行业精选问题测试中，经ARKNESS增强的3B参数Llama - 3模型匹配GPT - 4o准确率，多项指标有显著提升。

Conclusion: ARKNESS框架能为CNC加工过程规划提供可验证、数值精确的答案。

Abstract: Precision process planning in Computer Numerical Control (CNC) machining
demands rapid, context-aware decisions on tool selection, feed-speed pairs, and
multi-axis routing, placing immense cognitive and procedural burdens on
engineers from design specification through final part inspection. Conventional
rule-based computer-aided process planning and knowledge-engineering shells
freeze domain know-how into static tables, which become limited when dealing
with unseen topologies, novel material states, shifting
cost-quality-sustainability weightings, or shop-floor constraints such as tool
unavailability and energy caps. Large language models (LLMs) promise flexible,
instruction-driven reasoning for tasks but they routinely hallucinate numeric
values and provide no provenance. We present Augmented Retrieval Knowledge
Network Enhanced Search & Synthesis (ARKNESS), the end-to-end framework that
fuses zero-shot Knowledge Graph (KG) construction with retrieval-augmented
generation to deliver verifiable, numerically exact answers for CNC process
planning. ARKNESS (1) automatically distills heterogeneous machining documents,
G-code annotations, and vendor datasheets into augmented triple,
multi-relational graphs without manual labeling, and (2) couples any on-prem
LLM with a retriever that injects the minimal, evidence-linked subgraph needed
to answer a query. Benchmarked on 155 industry-curated questions spanning tool
sizing and feed-speed optimization, a lightweight 3B-parameter Llama-3
augmented by ARKNESS matches GPT-4o accuracy while achieving a +25 percentage
point gain in multiple-choice accuracy, +22.4 pp in F1, and 8.1x ROUGE-L on
open-ended responses.

</details>


### [62] [MAGIC: Multi-Agent Argumentation and Grammar Integrated Critiquer](https://arxiv.org/abs/2506.13037)
*Joaquin Jordan,Xavier Yin,Melissa Fabros,Gireeja Ranade,Narges Norouzi*

Main category: cs.AI

TL;DR: 提出MAGIC框架用于自动论文评分和反馈，在评分上优于基线模型，但在使反馈符合人类偏好上有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有自动论文评分和反馈系统重评分轻反馈，需改进。

Method: 提出MAGIC框架，用多个专业代理评估写作不同方面，构建新的GRE练习论文数据集支持评估。

Result: MAGIC在论文评分上优于基线模型。

Conclusion: 尽管在评分指标上有提升，但在使大语言模型生成的反馈符合人类偏好方面还有未来工作可做。

Abstract: Automated Essay Scoring (AES) and Automatic Essay Feedback (AEF) systems aim
to reduce the workload of human raters in educational assessment. However, most
existing systems prioritize numeric scoring accuracy over the quality of
feedback. This paper presents Multi-Agent Argumentation and Grammar Integrated
Critiquer (MAGIC), a framework that uses multiple specialized agents to
evaluate distinct writing aspects to both predict holistic scores and produce
detailed, rubric-aligned feedback. To support evaluation, we curated a novel
dataset of past GRE practice test essays with expert-evaluated scores and
feedback. MAGIC outperforms baseline models in both essay scoring , as measured
by Quadratic Weighted Kappa (QWK). We find that despite the improvement in QWK,
there are opportunities for future work in aligning LLM-generated feedback to
human preferences.

</details>


### [63] [Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model Learning](https://arxiv.org/abs/2506.13056)
*Haibo Qiu,Xiaohan Lan,Fanfan Liu,Xiaohu Sun,Delian Ruan,Peng Shi,Lin Ma*

Main category: cs.AI

TL;DR: 本文提出Metis - RISE用于多模态推理模型学习，先进行强化学习激励模型推理能力，再用有针对性的监督微调解决相关问题，模型在排行榜上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的推理范式存在样本效率低、探索能力受限和收敛不佳等问题，需要新方法解决。

Method: 提出Metis - RISE方法，先进行强化学习阶段激励模型潜在推理能力，后续监督微调阶段解决强化学习中发现的低效轨迹采样和基本能力缺失问题。

Result: 基于Metis - RISE得到两个版本的多模态大语言模型（7B和72B参数），在OpenCompass多模态推理排行榜上，两个模型在同规模模型中达到了最先进性能，72B版本总体排名第四。

Conclusion: Metis - RISE方法有效，能提升多模态推理模型性能。

Abstract: Recent advancements in large language models (LLMs) have witnessed a surge in
the development of advanced reasoning paradigms, which are now being integrated
into multimodal large language models (MLLMs). However, existing approaches
often fall short: methods solely employing reinforcement learning (RL) can
struggle with sample inefficiency and activating entirely absent reasoning
capabilities, while conventional pipelines that initiate with a cold-start
supervised fine-tuning (SFT) phase before RL may restrict the model's
exploratory capacity and face suboptimal convergence. In this work, we
introduce \textbf{Metis-RISE} (\textbf{R}L \textbf{I}ncentivizes and
\textbf{S}FT \textbf{E}nhances) for multimodal reasoning model learning. Unlike
conventional approaches, Metis-RISE distinctively omits an initial SFT stage,
beginning instead with an RL phase (e.g., using a Group Relative Policy
Optimization variant) to incentivize and activate the model's latent reasoning
capacity. Subsequently, the targeted SFT stage addresses two key challenges
identified during RL: (1) \textit{inefficient trajectory sampling} for tasks
where the model possesses but inconsistently applies correct reasoning, which
we tackle using self-distilled reasoning trajectories from the RL model itself;
and (2) \textit{fundamental capability absence}, which we address by injecting
expert-augmented knowledge for prompts where the model entirely fails. This
strategic application of RL for incentivization followed by SFT for enhancement
forms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B
parameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard
demonstrate that both models achieve state-of-the-art performance among
similar-sized models, with the 72B version ranking fourth overall.

</details>


### [64] [Rethinking Explainability in the Era of Multimodal AI](https://arxiv.org/abs/2506.13060)
*Chirag Agarwal*

Main category: cs.AI

TL;DR: 现有多模态AI系统可解释性技术多为单模态，无法捕捉跨模态交互，本文倡导转向多模态解释。


<details>
  <summary>Details</summary>
Motivation: 多模态AI系统需透明准确的解释算法以安全部署和获用户信任，但现有单模态解释技术无法捕捉跨模态交互。

Method: 提出基于模态的多模态解释的关键原则，如Granger式模态影响、协同忠实性和统一稳定性。

Result: 无明确具体结果描述。

Conclusion: 转向多模态解释有助于发现隐藏捷径、减轻模态偏差、提高模型可靠性和安全性。

Abstract: While multimodal AI systems (models jointly trained on heterogeneous data
types such as text, time series, graphs, and images) have become ubiquitous and
achieved remarkable performance across high-stakes applications, transparent
and accurate explanation algorithms are crucial for their safe deployment and
ensure user trust. However, most existing explainability techniques remain
unimodal, generating modality-specific feature attributions, concepts, or
circuit traces in isolation and thus failing to capture cross-modal
interactions. This paper argues that such unimodal explanations systematically
misrepresent and fail to capture the cross-modal influence that drives
multimodal model decisions, and the community should stop relying on them for
interpreting multimodal models. To support our position, we outline key
principles for multimodal explanations grounded in modality: Granger-style
modality influence (controlled ablations to quantify how removing one modality
changes the explanation for another), Synergistic faithfulness (explanations
capture the model's predictive power when modalities are combined), and Unified
stability (explanations remain consistent under small, cross-modal
perturbations). This targeted shift to multimodal explanations will help the
community uncover hidden shortcuts, mitigate modality bias, improve model
reliability, and enhance safety in high-stakes settings where incomplete
explanations can have serious consequences.

</details>


### [65] [Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs](https://arxiv.org/abs/2506.13082)
*Daniel Kilov,Caroline Hendy,Secil Yanik Guyot,Aaron J. Snoswell,Seth Lazar*

Main category: cs.AI

TL;DR: 本文回顾现有大语言模型道德能力评估文献，指出不足并提出新评估方法，实验发现现有评估或高估模型道德推理能力，提供了更细致的评估框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需道德能力场景中应用增多，现有评估方法存在不足，需实证评估其道德能力。

Method: 基于道德技能哲学研究，提出评估大语言模型道德能力的新方法，从五个维度评估，开展两个实验对比模型与非专家人类、专业哲学家。

Result: 在标准伦理小插曲实验中，模型在多维度道德推理上表现优于非专家人类；在新设计场景实验中，部分模型表现显著差于人类。

Conclusion: 现有评估可能高估模型道德推理能力，本文提供更细致评估框架，指明高级 AI 系统提升道德能力的重要方向。

Abstract: Moral competence is the ability to act in accordance with moral principles.
As large language models (LLMs) are increasingly deployed in situations
demanding moral competence, there is increasing interest in evaluating this
ability empirically. We review existing literature and identify three
significant shortcoming: (i) Over-reliance on prepackaged moral scenarios with
explicitly highlighted moral features; (ii) Focus on verdict prediction rather
than moral reasoning; and (iii) Inadequate testing of models' (in)ability to
recognize when additional information is needed. Grounded in philosophical
research on moral skill, we then introduce a novel method for assessing moral
competence in LLMs. Our approach moves beyond simple verdict comparisons to
evaluate five dimensions of moral competence: identifying morally relevant
features, weighting their importance, assigning moral reasons to these
features, synthesizing coherent moral judgments, and recognizing information
gaps. We conduct two experiments comparing six leading LLMs against non-expert
humans and professional philosophers. In our first experiment using ethical
vignettes standard to existing work, LLMs generally outperformed non-expert
humans across multiple dimensions of moral reasoning. However, our second
experiment, featuring novel scenarios designed to test moral sensitivity by
embedding relevant features among irrelevant details, revealed a striking
reversal: several LLMs performed significantly worse than humans. Our findings
suggest that current evaluations may substantially overestimate LLMs' moral
reasoning capabilities by eliminating the task of discerning moral relevance
from noisy information, which we take to be a prerequisite for genuine moral
skill. This work provides a more nuanced framework for assessing AI moral
competence and highlights important directions for improving moral competence
in advanced AI systems.

</details>


### [66] [A Memetic Walrus Algorithm with Expert-guided Strategy for Adaptive Curriculum Sequencing](https://arxiv.org/abs/2506.13092)
*Qionghao Huang,Lingnuo Lu,Xuemei Wu,Fan Jiang,Xizhe Wang,Xun Wang*

Main category: cs.AI

TL;DR: 本文提出Memetic Walrus Optimizer (MWO)解决自适应课程排序问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 当前自适应课程排序方法难以平衡复杂教育约束和维持优化稳定性。

Method: 提出MWO，包含专家引导老化策略、自适应控制信号框架和三层优先级机制，将ACS表述为多目标优化问题。

Result: 在OULAD数据集上，MWO难度进展率达95.3%，收敛稳定性更好；在基准函数上也证实其优化能力。

Conclusion: MWO能有效生成个性化学习序列，同时保持计算效率和解决方案质量。

Abstract: Adaptive Curriculum Sequencing (ACS) is essential for personalized online
learning, yet current approaches struggle to balance complex educational
constraints and maintain optimization stability. This paper proposes a Memetic
Walrus Optimizer (MWO) that enhances optimization performance through three key
innovations: (1) an expert-guided strategy with aging mechanism that improves
escape from local optima; (2) an adaptive control signal framework that
dynamically balances exploration and exploitation; and (3) a three-tier
priority mechanism for generating educationally meaningful sequences. We
formulate ACS as a multi-objective optimization problem considering concept
coverage, time constraints, and learning style compatibility. Experiments on
the OULAD dataset demonstrate MWO's superior performance, achieving 95.3%
difficulty progression rate (compared to 87.2% in baseline methods) and
significantly better convergence stability (standard deviation of 18.02 versus
28.29-696.97 in competing algorithms). Additional validation on benchmark
functions confirms MWO's robust optimization capability across diverse
scenarios. The results demonstrate MWO's effectiveness in generating
personalized learning sequences while maintaining computational efficiency and
solution quality.

</details>


### [67] [NeuroPhysNet: A FitzHugh-Nagumo-Based Physics-Informed Neural Network Framework for Electroencephalograph (EEG) Analysis and Motor Imagery Classification](https://arxiv.org/abs/2506.13222)
*Zhenyu Xia,Xinlei Huang,Suvash C. Saha*

Main category: cs.AI

TL;DR: 本文介绍用于EEG信号分析和运动想象分类的NeuroPhysNet框架，在数据集上表现优于传统方法，对BCI应用和临床诊断有重要意义。


<details>
  <summary>Details</summary>
Motivation: EEG分析面临噪声、非平稳性等挑战，传统神经网络缺乏生物物理知识整合，限制其临床应用。

Method: 引入NeuroPhysNet框架，结合FitzHugh - Nagumo模型，嵌入神经动力学原理约束预测。

Result: 在BCIC - IV - 2a数据集上，框架比传统方法有更高准确性和泛化能力，尤其在数据有限和跨主体场景。

Conclusion: NeuroPhysNet有效整合生物物理见解和数据驱动技术，推动BCI应用，有望提高临床诊断精度和可靠性。

Abstract: Electroencephalography (EEG) is extensively employed in medical diagnostics
and brain-computer interface (BCI) applications due to its non-invasive nature
and high temporal resolution. However, EEG analysis faces significant
challenges, including noise, nonstationarity, and inter-subject variability,
which hinder its clinical utility. Traditional neural networks often lack
integration with biophysical knowledge, limiting their interpretability,
robustness, and potential for medical translation. To address these
limitations, this study introduces NeuroPhysNet, a novel Physics-Informed
Neural Network (PINN) framework tailored for EEG signal analysis and motor
imagery classification in medical contexts. NeuroPhysNet incorporates the
FitzHugh-Nagumo model, embedding neurodynamical principles to constrain
predictions and enhance model robustness. Evaluated on the BCIC-IV-2a dataset,
the framework achieved superior accuracy and generalization compared to
conventional methods, especially in data-limited and cross-subject scenarios,
which are common in clinical settings. By effectively integrating biophysical
insights with data-driven techniques, NeuroPhysNet not only advances BCI
applications but also holds significant promise for enhancing the precision and
reliability of clinical diagnostics, such as motor disorder assessments and
neurorehabilitation planning.

</details>


### [68] [Towards Explaining Monte-Carlo Tree Search by Using Its Enhancements](https://arxiv.org/abs/2506.13223)
*Jakub Kowalski,Mark H. M. Winands,Maksymilian Wiśniewski,Stanisław Reda,Anna Wilbik*

Main category: cs.AI

TL;DR: 本文倡导知识无关可解释性在可解释搜索领域的应用，提出蒙特卡罗树搜索（MCTS）增强方法并分析，还展示其优势。


<details>
  <summary>Details</summary>
Motivation: 现有可解释人工智能（XAI）研究多聚焦特定领域黑盒模型，本文倡导知识无关可解释性在可解释搜索子领域的应用，且此前无人考虑MCTS增强方法的可解释性。

Method: 提出蒙特卡罗树搜索（MCTS）增强方法以获取额外数据和提供高质量解释，并分析流行的增强方法引入的可解释性类型。

Result: 展示了利用MCTS增强方法的优势的概念验证。

Conclusion: 蒙特卡罗树搜索（MCTS）增强方法在可解释搜索领域有应用优势，且能在知识无关情况下提供高质量解释。

Abstract: Typically, research on Explainable Artificial Intelligence (XAI) focuses on
black-box models within the context of a general policy in a known, specific
domain. This paper advocates for the need for knowledge-agnostic explainability
applied to the subfield of XAI called Explainable Search, which focuses on
explaining the choices made by intelligent search techniques. It proposes
Monte-Carlo Tree Search (MCTS) enhancements as a solution to obtaining
additional data and providing higher-quality explanations while remaining
knowledge-free, and analyzes the most popular enhancements in terms of the
specific types of explainability they introduce. So far, no other research has
considered the explainability of MCTS enhancements. We present a
proof-of-concept that demonstrates the advantages of utilizing enhancements.

</details>


### [69] [Generalized Proof-Number Monte-Carlo Tree Search](https://arxiv.org/abs/2506.13249)
*Jakub Kowalski,Dennis J. N. J. Soemers,Szymon Kosakowski,Mark H. M. Winands*

Main category: cs.AI

TL;DR: 提出广义证明数蒙特卡罗树搜索，对现有PNS与MCTS结合方法进行改进，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 对现有的证明数搜索（PNS）与蒙特卡罗树搜索（MCTS）的结合方法进行推广和改进，以提高性能并扩大适用范围。

Method: 提出三个核心修改：按玩家跟踪证明数；评估不同使用证明数来偏置选择策略的方法；将该技术与分数有界MCTS合并。

Result: 实验表明在11个测试的棋盘游戏中有8个性能提升幅度达到80%。

Conclusion: 所提出的广义证明数蒙特卡罗树搜索方法有效，能显著提高性能。

Abstract: This paper presents Generalized Proof-Number Monte-Carlo Tree Search: a
generalization of recently proposed combinations of Proof-Number Search (PNS)
with Monte-Carlo Tree Search (MCTS), which use (dis)proof numbers to bias
UCB1-based Selection strategies towards parts of the search that are expected
to be easily (dis)proven. We propose three core modifications of prior
combinations of PNS with MCTS. First, we track proof numbers per player. This
reduces code complexity in the sense that we no longer need disproof numbers,
and generalizes the technique to be applicable to games with more than two
players. Second, we propose and extensively evaluate different methods of using
proof numbers to bias the selection strategy, achieving strong performance with
strategies that are simpler to implement and compute. Third, we merge our
technique with Score Bounded MCTS, enabling the algorithm to prove and leverage
upper and lower bounds on scores - as opposed to only proving wins or not-wins.
Experiments demonstrate substantial performance increases, reaching the range
of 80% for 8 out of the 11 tested board games.

</details>


### [70] [Navigating the Black Box: Leveraging LLMs for Effective Text-Level Graph Injection Attacks](https://arxiv.org/abs/2506.13276)
*Yuefei Lyu,Chaozhuo Li,Xi Zhang,Tianle Zhang*

Main category: cs.AI

TL;DR: 本文提出针对文本属性图的黑盒图注入攻击框架ATAG - LLM，利用大语言模型生成可解释文本节点属性，实验验证其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图注入攻击方法存在产生不可解释节点嵌入、依赖高训练成本替代模型等问题，需要新的适用于文本属性图的攻击框架。

Method: 引入ATAG - LLM框架，利用大语言模型直接生成可解释文本级节点属性，设计LLM提示策略平衡探索与可靠性，提出相似度评估方法评估攻击文本效果。

Result: 在真实世界文本属性图数据集上的实验表明，ATAG - LLM性能优于现有嵌入级和文本级攻击方法。

Conclusion: ATAG - LLM能在严格黑盒设置下以最小训练成本对目标节点进行扰动，实现文本级图注入攻击，具有良好性能。

Abstract: Text-attributed graphs (TAGs) integrate textual data with graph structures,
providing valuable insights in applications such as social network analysis and
recommendation systems. Graph Neural Networks (GNNs) effectively capture both
topological structure and textual information in TAGs but are vulnerable to
adversarial attacks. Existing graph injection attack (GIA) methods assume that
attackers can directly manipulate the embedding layer, producing
non-explainable node embeddings. Furthermore, the effectiveness of these
attacks often relies on surrogate models with high training costs. Thus, this
paper introduces ATAG-LLM, a novel black-box GIA framework tailored for TAGs.
Our approach leverages large language models (LLMs) to generate interpretable
text-level node attributes directly, ensuring attacks remain feasible in
real-world scenarios. We design strategies for LLM prompting that balance
exploration and reliability to guide text generation, and propose a similarity
assessment method to evaluate attack text effectiveness in disrupting graph
homophily. This method efficiently perturbs the target node with minimal
training costs in a strict black-box setting, ensuring a text-level graph
injection attack for TAGs. Experiments on real-world TAG datasets validate the
superior performance of ATAG-LLM compared to state-of-the-art embedding-level
and text-level attack methods.

</details>


### [71] [Towards Pervasive Distributed Agentic Generative AI -- A State of The Art](https://arxiv.org/abs/2506.13324)
*Gianni Molinari,Fabio Ciravegna*

Main category: cs.AI

TL;DR: 本文综述了大语言模型代理在普适计算领域的架构、部署、评估等方面，指出挑战并提出概念框架。


<details>
  <summary>Details</summary>
Motivation: 随着智能代理和大语言模型的发展，需梳理其在普适计算领域的应用。

Method: 对LLM代理的架构组件进行概述，研究其在不同场景的部署和评估，回顾普适计算的计算和基础设施进展。

Result: 明确了LLM代理在普适计算中的部署策略、应用，以及面临的架构、能源和隐私等挑战。

Conclusion: 提出“Agent as a Tool”概念框架，强调上下文感知、模块化、安全、效率和有效性。

Abstract: The rapid advancement of intelligent agents and Large Language Models (LLMs)
is reshaping the pervasive computing field. Their ability to perceive, reason,
and act through natural language understanding enables autonomous
problem-solving in complex pervasive environments, including the management of
heterogeneous sensors, devices, and data. This survey outlines the
architectural components of LLM agents (profiling, memory, planning, and
action) and examines their deployment and evaluation across various scenarios.
Than it reviews computational and infrastructural advancements (cloud to edge)
in pervasive computing and how AI is moving in this field. It highlights
state-of-the-art agent deployment strategies and applications, including local
and distributed execution on resource-constrained devices. This survey
identifies key challenges of these agents in pervasive computing such as
architectural, energetic and privacy limitations. It finally proposes what we
called "Agent as a Tool", a conceptual framework for pervasive agentic AI,
emphasizing context awareness, modularity, security, efficiency and
effectiveness.

</details>


### [72] [Probabilistic Modeling of Spiking Neural Networks with Contract-Based Verification](https://arxiv.org/abs/2506.13340)
*Zhen Yao,Elisabetta De Maria,Robert De Simone*

Main category: cs.AI

TL;DR: 文章聚焦脉冲神经网络（SNN）建模，指出需 temporal language of logic，作者提供简单模型框架用于实验。


<details>
  <summary>Details</summary>
Motivation: SNN聚焦神经元反应激活的时间延迟和概率，与普通深度学习模型不同，在建模中需解决复合模型满足全局反应要求的挑战，故需要一种逻辑语言。

Method: 提供一个简单的模型框架，可转化为模型检查器和模拟器进行实验。

Result: 取得初步进展，构建了能表达基本SNN神经束及其连接结构的模型框架。

Conclusion: 该模型框架有助于对SNN进行形式验证和测试观察。

Abstract: Spiking Neural Networks (SNN) are models for "realistic" neuronal
computation, which makes them somehow different in scope from "ordinary"
deep-learning models widely used in AI platforms nowadays. SNNs focus on timed
latency (and possibly probability) of neuronal reactive activation/response,
more than numerical computation of filters. So, an SNN model must provide
modeling constructs for elementary neural bundles and then for synaptic
connections to assemble them into compound data flow network patterns. These
elements are to be parametric patterns, with latency and probability values
instantiated on particular instances (while supposedly constant "at runtime").
Designers could also use different values to represent "tired" neurons, or ones
impaired by external drugs, for instance. One important challenge in such
modeling is to study how compound models could meet global reaction
requirements (in stochastic timing challenges), provided similar provisions on
individual neural bundles. A temporal language of logic to express such
assume/guarantee contracts is thus needed. This may lead to formal verification
on medium-sized models and testing observations on large ones. In the current
article, we make preliminary progress at providing a simple model framework to
express both elementary SNN neural bundles and their connecting constructs,
which translates readily into both a model-checker and a simulator (both
already existing and robust) to conduct experiments.

</details>


### [73] [Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers](https://arxiv.org/abs/2506.13342)
*Wooseok Seo,Seungju Han,Jaehun Jung,Benjamin Newman,Seungwon Lim,Seungbeen Lee,Ximing Lu,Yejin Choi,Youngjae Yu*

Main category: cs.AI

TL;DR: 本文评估12个预训练大模型和1个专业事实验证器，得出三点结论指导事实验证器开发。


<details>
  <summary>Details</summary>
Motivation: 确保大模型应用可靠性，为未来开发更强大的事实验证器提供指导。

Method: 使用14个事实核查基准的示例评估模型。

Result: 约16%模糊或错误标注的数据影响模型排名；含少样本上下文示例的前沿大模型表现佳但成本高；小模型在复杂推理实例上有提升空间，合成多跳推理数据可增强其能力。

Conclusion: 要处理数据标注错误和歧义；未来研究应与含少样本上下文示例的前沿大模型对比；开发小的微调事实验证器并利用合成多跳推理数据增强其能力。

Abstract: Fact verification is essential for ensuring the reliability of LLM
applications. In this study, we evaluate 12 pre-trained LLMs and one
specialized fact-verifier, including frontier LLMs and open-weight reasoning
LLMs, using a collection of examples from 14 fact-checking benchmarks. We share
three findings intended to guide future development of more robust fact
verifiers. First, we highlight the importance of addressing annotation errors
and ambiguity in datasets, demonstrating that approximately 16\% of ambiguous
or incorrectly labeled data substantially influences model rankings. Neglecting
this issue may result in misleading conclusions during comparative evaluations,
and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help
identify these issues at scale. Second, we discover that frontier LLMs with
few-shot in-context examples, often overlooked in previous works, achieve
top-tier performance. We therefore recommend future studies include comparisons
with these simple yet highly effective baselines. Lastly, despite their
effectiveness, frontier LLMs incur substantial costs, motivating the
development of small, fine-tuned fact verifiers. We show that these small
models still have room for improvement, particularly on instances that require
complex reasoning. Encouragingly, we demonstrate that augmenting training with
synthetic multi-hop reasoning data significantly enhances their capabilities in
such instances. We release our code, model, and dataset at
https://github.com/just1nseo/verifying-the-verifiers

</details>


### [74] [Socratic RL: A Novel Framework for Efficient Knowledge Acquisition through Iterative Reflection and Viewpoint Distillation](https://arxiv.org/abs/2506.13358)
*Xiangfan Wu*

Main category: cs.AI

TL;DR: 本文提出Socratic-RL框架解决当前大语言模型强化学习依赖简单结果奖励信号的局限，介绍框架基础概念等内容。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型强化学习依赖简单的基于结果的奖励信号，限制了每次交互的学习深度。

Method: 采用解耦的“教师 - 学生”架构，教师AI分析交互历史、提取因果见解并形成结构化观点，学生AI利用观点改进推理；教师AI通过元学习循环迭代自我改进；用蒸馏机制将知识压缩到学生参数中。

Result: 未提及具体实验结果。

Conclusion: Socratic-RL聚焦过程而非结果，为提升样本效率、可解释性和构建可扩展的自我改进AI系统提供了途径。

Abstract: Current Reinforcement Learning (RL) methodologies for Large Language Models
(LLMs) often rely on simplistic, outcome-based reward signals (e.g., final
answer correctness), which limits the depth of learning from each interaction.
This paper introduces Socratic Reinforcement Learning (Socratic-RL), a novel,
process-oriented framework designed to address this limitation. Socratic-RL
operates on the principle that deeper understanding is achieved by reflecting
on the causal reasons for errors and successes within the reasoning process
itself. The framework employs a decoupled "Teacher-Student" architecture, where
a "Teacher AI" analyzes interaction histories, extracts causal insights, and
formulates them into structured "viewpoints." These viewpoints, acting as
distilled guidance, are then used by a "Student AI" to enhance its subsequent
reasoning. A key innovation is the iterative self-improvement of the Teacher
AI, enabling its reflective capabilities to evolve through a meta-learning
loop. To manage the accumulation of knowledge, a distillation mechanism
compresses learned viewpoints into the Student's parameters. By focusing on
process rather than just outcome, Socratic-RL presents a pathway toward
enhanced sample efficiency, superior interpretability, and a more scalable
architecture for self-improving AI systems. This paper details the foundational
concepts, formal mechanisms, synergies, challenges, and a concrete research
roadmap for this proposed framework.

</details>


### [75] [Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses](https://arxiv.org/abs/2506.13384)
*Leonie V. D. E. Vogelsmeier,Eduardo Oliveira,Kamila Misiejuk,Sonsoles López-Pernas,Mohammed Saqr*

Main category: cs.AI

TL;DR: 研究检验多个大语言模型生成学习策略与动机问卷的回应，发现Gemini 2 Flash最有潜力，也指出大语言模型模拟心理调查数据的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自我调节学习领域模拟调查回应有应用潜力，但有效性不确定，相关研究有限。

Method: 使用GPT - 4o、Claude 3.7 Sonnet等多个大语言模型生成44项学习动机策略问卷回应，分析项目分布、心理网络和心理测量有效性。

Result: Gemini 2 Flash最有潜力，其潜在维度和理论关系与先前理论和实证结果相符，但也存在差异和局限。

Conclusion: 大语言模型在模拟心理调查数据及教育情境应用有潜力，但存在当前约束。

Abstract: Large language models (LLMs) offer the potential to simulate human-like
responses and behaviors, creating new opportunities for psychological science.
In the context of self-regulated learning (SRL), if LLMs can reliably simulate
survey responses at scale and speed, they could be used to test intervention
scenarios, refine theoretical models, augment sparse datasets, and represent
hard-to-reach populations. However, the validity of LLM-generated survey
responses remains uncertain, with limited research focused on SRL and existing
studies beyond SRL yielding mixed results. Therefore, in this study, we
examined LLM-generated responses to the 44-item Motivated Strategies for
Learning Questionnaire (MSLQ; Pintrich \& De Groot, 1990), a widely used
instrument assessing students' learning strategies and academic motivation.
Particularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA
3.1-8B, and Mistral Large. We analyzed item distributions, the psychological
network of the theoretical SRL dimensions, and psychometric validity based on
the latent factor structure. Our results suggest that Gemini 2 Flash was the
most promising LLM, showing considerable sampling variability and producing
underlying dimensions and theoretical relationships that align with prior
theory and empirical findings. At the same time, we observed discrepancies and
limitations, underscoring both the potential and current constraints of using
LLMs for simulating psychological survey data and applying it in educational
contexts.

</details>


### [76] [Deflating Deflationism: A Critical Perspective on Debunking Arguments Against LLM Mentality](https://arxiv.org/abs/2506.13403)
*Alex Grzankowski,Geoff Keeling,Henry Shevlin,Winnie Street*

Main category: cs.AI

TL;DR: 文章评估两种反对大语言模型具有心智的紧缩论观点，发现都无法完全否定大语言模型有心智，进而探讨适度膨胀论，认为在特定条件下可将某些心理状态归因于大语言模型。


<details>
  <summary>Details</summary>
Motivation: 推进关于大语言模型是否具有心智的讨论。

Method: 评估两种常见的反对大语言模型心智的紧缩论论证，即“稳健性策略”和“病因学策略”。

Result: 两种策略虽对全面膨胀论构成有力挑战，但都无法完全否定大语言模型有心智。

Conclusion: 可在特定条件下适度将心理状态归因于大语言模型，对形而上学要求高的心理现象归因需更谨慎。

Abstract: Many people feel compelled to interpret, describe, and respond to Large
Language Models (LLMs) as if they possess inner mental lives similar to our
own. Responses to this phenomenon have varied. Inflationists hold that at least
some folk psychological ascriptions to LLMs are warranted. Deflationists argue
that all such attributions of mentality to LLMs are misplaced, often cautioning
against the risk that anthropomorphic projection may lead to misplaced trust or
potentially even confusion about the moral status of LLMs. We advance this
debate by assessing two common deflationary arguments against LLM mentality.
What we term the 'robustness strategy' aims to undercut one justification for
believing that LLMs are minded entities by showing that putatively cognitive
and humanlike behaviours are not robust, failing to generalise appropriately.
What we term the 'etiological strategy' undercuts attributions of mentality by
challenging naive causal explanations of LLM behaviours, offering alternative
causal accounts that weaken the case for mental state attributions. While both
strategies offer powerful challenges to full-blown inflationism, we find that
neither strategy provides a knock-down case against ascriptions of mentality to
LLMs simpliciter. With this in mind, we explore a modest form of inflationism
that permits ascriptions of mentality to LLMs under certain conditions.
Specifically, we argue that folk practice provides a defeasible basis for
attributing mental states and capacities to LLMs provided those mental states
and capacities can be understood in metaphysically undemanding terms (e.g.
knowledge, beliefs and desires), while greater caution is required when
attributing metaphysically demanding mental phenomena such as phenomenal
consciousness.

</details>


### [77] [A Technical Study into Small Reasoning Language Models](https://arxiv.org/abs/2506.13404)
*Xialie Zhuang,Peixian Ma,Zhikai Jia,Zheng Cao,Shiwei Liu*

Main category: cs.AI

TL;DR: 大语言模型计算和能源需求大且有隐私问题，0.5B参数的SRLMs有优势但处理复杂任务能力有限，研究多种训练策略提升其性能并给出建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有计算、能源和隐私问题，0.5B的SRLMs虽有优势但处理复杂任务能力不足，需提升其性能。

Method: 研究监督微调（SFT）、知识蒸馏（KD）、强化学习（RL）及其混合实现等训练策略。

Result: 通过大量实验验证和分析，分析有效方法缩小SRLMs与大模型性能差距，洞察适合小模型的最佳训练流程。

Conclusion: 为最大化0.5B模型推理能力提供可操作建议。

Abstract: The ongoing evolution of language models has led to the development of
large-scale architectures that demonstrate exceptional performance across a
wide range of tasks. However, these models come with significant computational
and energy demands, as well as potential privacy implications. In this context,
Small Reasoning Language Models (SRLMs) with approximately 0.5 billion
parameters present a compelling alternative due to their remarkable
computational efficiency and cost effectiveness, particularly in
resource-constrained environments. Despite these advantages, the limited
capacity of 0.5 billion parameter models poses challenges in handling complex
tasks such as mathematical reasoning and code generation. This research
investigates various training strategies, including supervised fine-tuning
(SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as
their hybrid implementations, to enhance the performance of 0.5B SRLMs. We
analyze effective methodologies to bridge the performance gap between SRLMS and
larger models and present insights into optimal training pipelines tailored for
these smaller architectures. Through extensive experimental validation and
analysis, our work aims to provide actionable recommendations for maximizing
the reasoning capabilities of 0.5B models.

</details>


### [78] [Block-wise Adaptive Caching for Accelerating Diffusion Policy](https://arxiv.org/abs/2506.13456)
*Kangye Ji,Yuan Meng,Hanyun Cui,Ye Li,Shengjia Hua,Lei Chen,Zhi Wang*

Main category: cs.AI

TL;DR: 提出Block-wise Adaptive Caching (BAC)方法加速Diffusion Policy，实验显示可免费实现高达3倍推理加速。


<details>
  <summary>Details</summary>
Motivation: Diffusion Policy计算成本高，现有扩散加速技术因架构和数据差异无法用于它，需新方法加速。

Method: 提出Adaptive Caching Scheduler确定最佳更新时间步；开发Bubbling Union Algorithm截断缓存错误；作为免训练插件集成到现有模型。

Result: BAC能无损加速动作生成，在多个机器人基准测试中实现高达3倍推理加速。

Conclusion: BAC是有效的加速Diffusion Policy的方法，可免费集成到现有模型提升推理速度。

Abstract: Diffusion Policy has demonstrated strong visuomotor modeling capabilities,
but its high computational cost renders it impractical for real-time robotic
control. Despite huge redundancy across repetitive denoising steps, existing
diffusion acceleration techniques fail to generalize to Diffusion Policy due to
fundamental architectural and data divergences. In this paper, we propose
Block-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by
caching intermediate action features. BAC achieves lossless action generation
acceleration by adaptively updating and reusing cached features at the block
level, based on a key observation that feature similarities vary non-uniformly
across timesteps and locks. To operationalize this insight, we first propose
the Adaptive Caching Scheduler, designed to identify optimal update timesteps
by maximizing the global feature similarities between cached and skipped
features. However, applying this scheduler for each block leads to signiffcant
error surges due to the inter-block propagation of caching errors, particularly
within Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop
the Bubbling Union Algorithm, which truncates these errors by updating the
upstream blocks with signiffcant caching errors before downstream FFNs. As a
training-free plugin, BAC is readily integrable with existing transformer-based
Diffusion Policy and vision-language-action models. Extensive experiments on
multiple robotic benchmarks demonstrate that BAC achieves up to 3x inference
speedup for free.

</details>


### [79] [From Data-Driven to Purpose-Driven Artificial Intelligence: Systems Thinking for Data-Analytic Automation of Patient Care](https://arxiv.org/abs/2506.13584)
*Daniel Anadria,Roel Dobbe,Anastasia Giachanou,Ruurd Kuiper,Richard Bartels,Íñigo Martínez de Rituerto de Troya,Carmen Zürcher,Daniel Oberski*

Main category: cs.AI

TL;DR: 反思数据驱动建模范式在患者护理AI自动化中的应用，提出以临床理论和现实运营情境为基础的目标驱动机器学习范式。


<details>
  <summary>Details</summary>
Motivation: 指出利用现有真实世界患者数据集进行机器学习建模可能并非最优，可能导致患者护理出现不良结果。

Method: 回顾数据分析历史解释数据驱动范式流行原因，设想系统思维和临床领域理论如何补充现有模型开发方法。

Result: 提出目标驱动的机器学习范式，该范式需兼顾数据生成上游和自动化目标下游。

Conclusion: 目标驱动视角为AI系统开发带来新方法机遇，有望推动患者护理AI自动化。

Abstract: In this work, we reflect on the data-driven modeling paradigm that is gaining
ground in AI-driven automation of patient care. We argue that the repurposing
of existing real-world patient datasets for machine learning may not always
represent an optimal approach to model development as it could lead to
undesirable outcomes in patient care. We reflect on the history of data
analysis to explain how the data-driven paradigm rose to popularity, and we
envision ways in which systems thinking and clinical domain theory could
complement the existing model development approaches in reaching human-centric
outcomes. We call for a purpose-driven machine learning paradigm that is
grounded in clinical theory and the sociotechnical realities of real-world
operational contexts. We argue that understanding the utility of existing
patient datasets requires looking in two directions: upstream towards the data
generation, and downstream towards the automation objectives. This
purpose-driven perspective to AI system development opens up new methodological
opportunities and holds promise for AI automation of patient care.

</details>


### [80] [Agent Capability Negotiation and Binding Protocol (ACNBP)](https://arxiv.org/abs/2506.13590)
*Ken Huang,Akram Sheriff,Vineeth Sai Narajala,Idan Habler*

Main category: cs.AI

TL;DR: 文章提出ACNBP协议，助力异构多智能体系统交互，介绍步骤、创新点，经分析和实例证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统智能体通信协议在异构、动态场景适用性有限，需新协议支持异构多智能体有效协作。

Method: 提出ACNBP协议，与ANS基础设施集成，有10步流程，含安全措施，还有协议扩展机制。

Result: 通过安全分析、实践考量和文档翻译实例展示了协议有效性。

Conclusion: ACNBP协议解决了智能体自主、能力验证、安全通信和可扩展生态管理等关键挑战。

Abstract: As multi-agent systems evolve to encompass increasingly diverse and
specialized agents, the challenge of enabling effective collaboration between
heterogeneous agents has become paramount, with traditional agent communication
protocols often assuming homogeneous environments or predefined interaction
patterns that limit their applicability in dynamic, open-world scenarios. This
paper presents the Agent Capability Negotiation and Binding Protocol (ACNBP), a
novel framework designed to facilitate secure, efficient, and verifiable
interactions between agents in heterogeneous multi-agent systems through
integration with an Agent Name Service (ANS) infrastructure that provides
comprehensive discovery, negotiation, and binding mechanisms. The protocol
introduces a structured 10-step process encompassing capability discovery,
candidate pre-screening and selection, secure negotiation phases, and binding
commitment with built-in security measures including digital signatures,
capability attestation, and comprehensive threat mitigation strategies, while a
key innovation of ACNBP is its protocolExtension mechanism that enables
backward-compatible protocol evolution and supports diverse agent architectures
while maintaining security and interoperability. We demonstrate ACNBP's
effectiveness through a comprehensive security analysis using the MAESTRO
threat modeling framework, practical implementation considerations, and a
detailed example showcasing the protocol's application in a document
translation scenario, with the protocol addressing critical challenges in agent
autonomy, capability verification, secure communication, and scalable agent
ecosystem management.

</details>


### [81] [The ASP-based Nurse Scheduling System at the University of Yamanashi Hospital](https://arxiv.org/abs/2506.13600)
*Hidetomo Nabeshima,Mutsunori Banbara,Torsten Schaub,Takehide Soh*

Main category: cs.AI

TL;DR: 本文介绍基于回答集编程（ASP）的护士排班系统在山梨大学医院的设计原则与实际应用。


<details>
  <summary>Details</summary>
Motivation: 护士排班是复杂优化问题，现实中的排班面临超越典型基准问题和竞赛的独特挑战，需实际应用ASP解决这些挑战。

Method: 运用回答集编程（ASP）技术来构建护士排班系统，平衡硬约束和软约束，支持交互式调整。

Result: 基于ASP的护士排班系统在山梨大学医院成功部署。

Conclusion: 详细阐述了ASP在实际应用中的见解以及有效管理现实部署复杂性所需的ASP技术进步。

Abstract: We present the design principles of a nurse scheduling system built using
Answer Set Programming (ASP) and successfully deployed at the University of
Yamanashi Hospital. Nurse scheduling is a complex optimization problem
requiring the reconciliation of individual nurse preferences with hospital
staffing needs across various wards. This involves balancing hard and soft
constraints and the flexibility of interactive adjustments. While extensively
studied in academia, real-world nurse scheduling presents unique challenges
that go beyond typical benchmark problems and competitions. This paper details
the practical application of ASP to address these challenges at the University
of Yamanashi Hospital, focusing on the insights gained and the advancements in
ASP technology necessary to effectively manage the complexities of real-world
deployment.

</details>


### [82] [Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model](https://arxiv.org/abs/2506.13642)
*Shaolei Zhang,Shoutao Guo,Qingkai Fang,Yan Zhou,Yang Feng*

Main category: cs.AI

TL;DR: 本文提出高效模态对齐的大语言-视觉-语音模型Stream - Omni，用不同方法实现模态对齐，实验表明其在多任务表现出色，能提供综合多模态体验。


<details>
  <summary>Details</summary>
Motivation: 现有大多模态模型在模态整合时依赖大规模数据学习模态对齐，本文旨在更有目的地建模模态关系，实现更高效灵活的模态对齐。

Method: 提出Stream - Omni模型，以大语言模型为骨干，对视觉采用序列维度拼接实现视觉 - 文本对齐，对语音引入基于CTC的层维度映射实现语音 - 文本对齐。

Result: 在各种基准测试中，Stream - Omni在视觉理解、语音交互和视觉语音交互任务上表现出色，且在语音交互中可提供中间文本输出。

Conclusion: Stream - Omni能以较少数据实现模态对齐，将文本能力迁移到其他模态，提供综合多模态体验。

Abstract: The emergence of GPT-4o-like large multimodal models (LMMs) has raised the
exploration of integrating text, vision, and speech modalities to support more
flexible multimodal interaction. Existing LMMs typically concatenate
representation of modalities along the sequence dimension and feed them into a
large language model (LLM) backbone. While sequence-dimension concatenation is
straightforward for modality integration, it often relies heavily on
large-scale data to learn modality alignments. In this paper, we aim to model
the relationships between modalities more purposefully, thereby achieving more
efficient and flexible modality alignments. To this end, we propose
Stream-Omni, a large language-vision-speech model with efficient modality
alignments, which can simultaneously support interactions under various
modality combinations. Stream-Omni employs LLM as the backbone and aligns the
vision and speech to the text based on their relationships. For vision that is
semantically complementary to text, Stream-Omni uses sequence-dimension
concatenation to achieve vision-text alignment. For speech that is semantically
consistent with text, Stream-Omni introduces a CTC-based layer-dimension
mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve
modality alignments with less data (especially speech), enabling the transfer
of text capabilities to other modalities. Experiments on various benchmarks
demonstrate that Stream-Omni achieves strong performance on visual
understanding, speech interaction, and vision-grounded speech interaction
tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously
provide intermediate text outputs (such as ASR transcriptions and model
responses) during speech interaction, offering users a comprehensive multimodal
experience.

</details>


### [83] [Weakest Link in the Chain: Security Vulnerabilities in Advanced Reasoning Models](https://arxiv.org/abs/2506.13726)
*Arjun Krishna,Aaditya Rastogi,Erick Galinkin*

Main category: cs.AI

TL;DR: 本文系统评估推理模型和非推理模型在不同提示攻击下的弱点，发现推理增强模型总体略更稳健，但不同攻击类型有显著差异。


<details>
  <summary>Details</summary>
Motivation: 探究推理模型相比非推理模型是否更容易受到对抗性提示攻击。

Method: 对推理模型和非推理模型在多种基于提示的攻击类别下进行系统评估，并使用实验数据。

Result: 推理增强模型平均攻击成功率略低于非推理模型，但不同攻击类型表现差异大，部分攻击下更脆弱，部分更稳健。

Conclusion: 强调语言模型高级推理的安全影响复杂，需对不同对抗技术进行安全压力测试。

Abstract: The introduction of advanced reasoning capabilities have improved the
problem-solving performance of large language models, particularly on math and
coding benchmarks. However, it remains unclear whether these reasoning models
are more or less vulnerable to adversarial prompt attacks than their
non-reasoning counterparts. In this work, we present a systematic evaluation of
weaknesses in advanced reasoning models compared to similar non-reasoning
models across a diverse set of prompt-based attack categories. Using
experimental data, we find that on average the reasoning-augmented models are
\emph{slightly more robust} than non-reasoning models (42.51\% vs 45.53\%
attack success rate, lower is better). However, this overall trend masks
significant category-specific differences: for certain attack types the
reasoning models are substantially \emph{more vulnerable} (e.g., up to 32
percentage points worse on a tree-of-attacks prompt), while for others they are
markedly \emph{more robust} (e.g., 29.8 points better on cross-site scripting
injection). Our findings highlight the nuanced security implications of
advanced reasoning in language models and emphasize the importance of
stress-testing safety across diverse adversarial techniques.

</details>


### [84] [PB$^2$: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning](https://arxiv.org/abs/2506.13741)
*Brahim Driss,Alex Davey,Riad Akrour*

Main category: cs.AI

TL;DR: 本文指出当前基于偏好的强化学习（PbRL）方法在探索偏好空间时存在问题，提出基于种群的方法解决该问题，实验表明此方法性能更优。


<details>
  <summary>Details</summary>
Motivation: 当前PbRL方法在有效探索偏好空间时面临挑战，易过早收敛到次优策略，仅满足部分人类偏好。

Method: 采用基于种群的方法，维护多样化的智能体种群以更全面地探索偏好空间。

Result: 当前方法可能陷入局部最优、需要过多反馈，在人类评估者出错时性能下降；基于种群的方法在教师误标记时表现稳健，偏好探索能力显著增强。

Conclusion: 基于种群的方法能解决偏好探索问题，在复杂奖励环境中表现良好。

Abstract: Preference-based reinforcement learning (PbRL) has emerged as a promising
approach for learning behaviors from human feedback without predefined reward
functions. However, current PbRL methods face a critical challenge in
effectively exploring the preference space, often converging prematurely to
suboptimal policies that satisfy only a narrow subset of human preferences. In
this work, we identify and address this preference exploration problem through
population-based methods. We demonstrate that maintaining a diverse population
of agents enables more comprehensive exploration of the preference landscape
compared to single-agent approaches. Crucially, this diversity improves reward
model learning by generating preference queries with clearly distinguishable
behaviors, a key factor in real-world scenarios where humans must easily
differentiate between options to provide meaningful feedback. Our experiments
reveal that current methods may fail by getting stuck in local optima,
requiring excessive feedback, or degrading significantly when human evaluators
make errors on similar trajectories, a realistic scenario often overlooked by
methods relying on perfect oracle teachers. Our population-based approach
demonstrates robust performance when teachers mislabel similar trajectory
segments and shows significantly enhanced preference exploration
capabilities,particularly in environments with complex reward landscapes.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [85] [Interpretable Classification of Levantine Ceramic Thin Sections via Neural Networks](https://arxiv.org/abs/2506.12250)
*Sara Capriotti,Alessio Devoto,Simone Scardapane,Silvano Mignardi,Laura Medeghini*

Main category: cs.CE

TL;DR: 本文探讨深度学习模型用于黎凡特陶瓷薄片分类，发现迁移学习可提升性能，解释性技术能揭示模型关注关键特征，凸显可解释AI在考古计量研究中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统岩石薄片分析耗时，需新方法辅助黎凡特陶瓷基于岩石结构的分类。

Method: 使用1424张来自黎凡特地区考古遗址陶瓷薄片图像数据集，训练评估CNN和ViT模型，用迁移学习提升性能，应用Guided Grad - CAM和注意力图等解释性技术。

Result: 迁移学习显著提升分类性能，ResNet18模型准确率达92.11%，ViT达88.34%，解释性技术表明模型能关注关键矿物特征进行分类。

Conclusion: 可解释AI在考古计量研究有潜力，为陶瓷分析提供可重复、高效且决策透明的方法。

Abstract: Classification of ceramic thin sections is fundamental for understanding
ancient pottery production techniques, provenance, and trade networks. Although
effective, traditional petrographic analysis is time-consuming. This study
explores the application of deep learning models, specifically Convolutional
Neural Networks (CNNs) and Vision Transformers (ViTs), as complementary tools
to support the classification of Levantine ceramics based on their petrographic
fabrics. A dataset of 1,424 thin section images from 178 ceramic samples
belonging to several archaeological sites across the Levantine area, mostly
from the Bronze Age, with few samples dating to the Iron Age, was used to train
and evaluate these models. The results demonstrate that transfer learning
significantly improves classification performance, with a ResNet18 model
achieving 92.11% accuracy and a ViT reaching 88.34%. Explainability techniques,
including Guided Grad-CAM and attention maps, were applied to interpret and
visualize the models' decisions, revealing that both CNNs and ViTs successfully
focus on key mineralogical features for the classification of the samples into
their respective petrographic fabrics. These findings highlight the potential
of explainable AI in archaeometric studies, providing a reproducible and
efficient methodology for ceramic analysis while maintaining transparency in
model decision-making.

</details>


### [86] [A modified Newmark/Newton-Raphson method with automatic differentiation for general nonlinear dynamics analysis](https://arxiv.org/abs/2506.13226)
*Yifan Jiang,Yuhong Jin,Lei Hou,Yi Chen,Andong Cong*

Main category: cs.CE

TL;DR: 提出结合自动微分的改进 NNR 方法（NNR - AD）解决复杂非线性动力系统，简化计算且提高能力。


<details>
  <summary>Details</summary>
Motivation: 当前 NNR 方法在复杂非线性动力系统中适用性有限，获取雅可比矩阵计算成本高甚至无法计算。

Method: 将自动微分（AD）集成到 NNR 方法中，提出 NNR - AD 方法。

Result: NNR - AD 方法可直接求解具有复杂非线性特征的动力系统，准确性和通用性得到验证，自动微分简化了雅可比矩阵计算。

Conclusion: 该改进使 NNR 方法模块化增强，能方便有效地解决复杂非线性动力系统。

Abstract: The Newmark/Newton-Raphson (NNR) method is widely employed for solving
nonlinear dynamic systems. However, the current NNR method exhibits limited
applicability in complex nonlinear dynamic systems, as the acquisition of the
Jacobian matrix required for Newton iterations incurs substantial computational
costs and may even prove intractable in certain cases. To address these
limitations, we integrate automatic differentiation (AD) into the NNR method,
proposing a modified NNR method with AD (NNR-AD) to significantly improve its
capability for effectively handling complex nonlinear systems. We have
demonstrated that the NNR-AD method can directly solve dynamic systems with
complex nonlinear characteristics, and its accuracy and generality have been
rigorously validated. Furthermore, automatic differentiation significantly
simplifies the computation of Jacobian matrices for such complex nonlinear
dynamic systems. This improvement endows the NNR method with enhanced
modularity, thereby enabling convenient and effective solutions for complex
nonlinear dynamic systems.

</details>


### [87] [An Entropy-Stable/Double-Flux scheme for the multi-component compressible Navier-Stokes equations](https://arxiv.org/abs/2506.13231)
*Vahid Badrkhani,T. Jeremy P. Karpowsk,Christian Hasse*

Main category: cs.CE

TL;DR: 提出新数值技术组合提升多组分可压缩流模拟性能，经基准测试验证有效。


<details>
  <summary>Details</summary>
Motivation: 提高多组分可压缩流模拟的效率、准确性和鲁棒性。

Method: 采用熵稳定公式结合双通量方案，引入混合耗散策略，使用显式龙格 - 库塔法结合自适应网格细化，在OpenFOAM求解器中实现。

Result: 基准测试表明该框架稳定性和鲁棒性良好。

Conclusion: 该方法是超音速流高保真模拟的有前景进展。

Abstract: We present a novel combination of numerical techniques to improve the
efficiency, accuracy, and robustness of multi-component compressible flow
simulations. At the core of our approach is an Entropy-Stable formulation that
preserves kinetic energy and integrates a Double-Flux scheme tailored for
multi-component flows with variable specific heat ratios. This formulation
yields low-dissipation, oscillation-free solutions and enhances stability
compared to standard fully conservative methods. To further improve robustness,
we introduce a new hybrid dissipation strategy that blends the
Entropy-Stable/Double-Flux approach with conventional dissipation mechanisms.
We provide a rigorous proof that the resulting numerical flux satisfies a
semi-discrete entropy inequality, ensuring consistency with the second law of
thermodynamics. For time integration, we employ an explicit Runge-Kutta scheme
in combination with adaptive mesh refinement to capture local flow features
dynamically. The method is implemented within an existing compressible
Navier-Stokes solver based on OpenFOAM. Benchmark cases, including
multi-dimensional interface and shock-interface interactions, demonstrate the
effectiveness of the proposed framework. The results confirm its favorable
stability and robustness, validating the approach as a promising advancement
for high-fidelity simulations of supersonic flows.

</details>


### [88] [Constitutive Manifold Neural Networks](https://arxiv.org/abs/2506.13648)
*Wouter J. Schuttert,Mohammed Iqbal Abdul Rasheed,Bojana Rosić*

Main category: cs.CE

TL;DR: 本文针对对称正定（SPD）张量随机特性建模及代理模型问题，提出了本构流形神经网络（CMNN），通过案例证明其优于传统多层感知器。


<details>
  <summary>Details</summary>
Motivation: SPD张量具有随机性，传统多层感知器不适用于SPD张量，无法保留其几何属性，需更好的代理模型。

Method: 将不确定性参数化为缩放和旋转分量，提出CMNN，引入预处理层将SPD张量从弯曲流形映射到局部切空间。

Result: 在稳态热传导问题案例中，采用几何保留预处理的CMNN学习性能明显优于传统多层感知器。

Conclusion: 在工程应用中处理张量值数据时，流形感知技术很重要。

Abstract: Important material properties like thermal conductivity are often represented
as symmetric positive definite (SPD) tensors, which exhibit variability due to
inherent material heterogeneity and manufacturing uncertainties. These tensors
reside on a curved Riemannian manifold, and accurately modeling their
stochastic nature requires preserving both their symmetric positive definite
properties and spatial symmetries. To achieve this, uncertainties are
parametrized into scaling (magnitude) and rotation (orientation) components,
modeled as independent random variables on a manifold structure derived from
the maximum entropy principle. The propagation of such stochastic tensors
through physics-based simulations necessitates computationally efficient
surrogate models. However, traditional multi-layer perceptron (MLP)
architectures are not well-suited for SPD tensors, as directly inputting their
components fails to preserve their geometric properties, often leading to
suboptimal results. To address this, we introduce Constitutive Manifold Neural
Networks (CMNN). This approach introduces a preprocessing layer by mapping the
SPD tensor from the curved manifold to the local tangent, a flat vector space,
creating an information preserving map for input to the hidden layers of the
neural networks. A case study on a steady-state heat conduction problem with
stochastic anisotropic conductivity demonstrates that geometry-preserving
preprocessing, such as logarithmic maps for scaling data, significantly
improves learning performance over conventional MLPs. These findings underscore
the importance of manifold-aware techniques when working with tensor-valued
data in engineering applications.

</details>


### [89] [Kolmogorov-Arnold Network for Gene Regulatory Network Inference](https://arxiv.org/abs/2506.13740)
*Tsz Pan Tong,Aoran Wang,George Panagopoulos,Jun Pang*

Main category: cs.CE

TL;DR: 本文介绍了新模型scKAN用于从scRNA - seq数据推断基因调控网络（GRN），实验显示其性能优于现有模型，能捕捉基因调控的生物过程。


<details>
  <summary>Details</summary>
Motivation: 现有基于树的模型在GRN推断中不能区分调控类型和有效捕捉细胞动态，而基因调控研究对理解细胞过程和疾病治疗有重要意义，需要新的模型。

Method: 引入scKAN模型，采用Kolmogorov - Arnold网络（KAN）结合可解释AI，将基因表达建模为可微函数。

Result: 在BEELINE基准测试中，scKAN在AUROC上比领先的有符号GRN推断模型提高5.40% - 28.37%，在AUPRC上提高1.97% - 40.45%。

Conclusion: scKAN有潜力在无图结构先验知识的情况下捕捉基因调控的潜在生物过程。

Abstract: Gene regulation is central to understanding cellular processes and
development, potentially leading to the discovery of new treatments for
diseases and personalized medicine. Inferring gene regulatory networks (GRNs)
from single-cell RNA sequencing (scRNA-seq) data presents significant
challenges due to its high dimensionality and complexity. Existing tree-based
models, such as GENIE3 and GRNBOOST2, demonstrated scalability and
explainability in GRN inference, but they cannot distinguish regulation types
nor effectively capture continuous cellular dynamics. In this paper, we
introduce scKAN, a novel model that employs a Kolmogorov-Arnold network (KAN)
with explainable AI to infer GRNs from scRNA-seq data. By modeling gene
expression as differentiable functions matching the smooth nature of cellular
dynamics, scKAN can accurately and precisely detect activation and inhibition
regulations through explainable AI and geometric tools. We conducted extensive
experiments on the BEELINE benchmark, and scKAN surpasses and improves the
leading signed GRN inference models ranging from 5.40\% to 28.37\% in AUROC and
from 1.97\% to 40.45\% in AUPRC. These results highlight the potential of scKAN
in capturing the underlying biological processes in gene regulation without
prior knowledge of the graph structure.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [90] [Datrics Text2SQL: A Framework for Natural Language to SQL Query Generation](https://arxiv.org/abs/2506.12234)
*Tetiana Gladkykh,Kyrylo Kirykov*

Main category: cs.DB

TL;DR: 本文介绍基于RAG的Datrics Text2SQL框架，利用结构化文档等生成准确SQL查询，弥合用户意图与数据库结构差距。


<details>
  <summary>Details</summary>
Motivation: 解决Text - to - SQL系统在理解模糊表述、特定领域词汇和复杂模式关系方面的挑战。

Method: 构建基于RAG的框架，从数据库文档和示例构建知识库，以向量嵌入存储并通过语义相似性检索，进而生成SQL代码。

Result: 未提及具体结果。

Conclusion: 该系统可在无需SQL专业知识的情况下，弥合用户意图与数据库结构的差距。

Abstract: Text-to-SQL systems enable users to query databases using natural language,
democratizing access to data analytics. However, they face challenges in
understanding ambiguous phrasing, domain-specific vocabulary, and complex
schema relationships. This paper introduces Datrics Text2SQL, a
Retrieval-Augmented Generation (RAG)-based framework designed to generate
accurate SQL queries by leveraging structured documentation, example-based
learning, and domain-specific rules. The system builds a rich Knowledge Base
from database documentation and question-query examples, which are stored as
vector embeddings and retrieved through semantic similarity. It then uses this
context to generate syntactically correct and semantically aligned SQL code.
The paper details the architecture, training methodology, and retrieval logic,
highlighting how the system bridges the gap between user intent and database
structure without requiring SQL expertise.

</details>


### [91] [CPN-Py: A Python-Based Tool for Modeling and Analyzing Colored Petri Nets](https://arxiv.org/abs/2506.12238)
*Alessandro Berti,Wil M. P. van der Aalst*

Main category: cs.DB

TL;DR: 本文介绍Python库CPN - Py，它保留有色Petri网核心概念，能与Python环境集成，还讨论其设计、与PM4Py协同、支持的分析及与大语言模型的结合。


<details>
  <summary>Details</summary>
Motivation: 现有CPN工具与现代数据科学生态分离，而Python在过程挖掘等领域已成事实标准语言，需开发与之集成的工具。

Method: 设计并开发CPN - Py库，探讨其与PM4Py协同，说明支持的分析功能及与大语言模型结合方式。

Result: 开发出CPN - Py库，展示其与PM4Py的协同功能，以及对状态空间分析和分层CPN的支持，说明其能与大语言模型结合。

Conclusion: CPN - Py可有效将有色Petri网与Python环境集成，有与PM4Py协同及支持大语言模型等优势。

Abstract: Colored Petri Nets (CPNs) are an established formalism for modeling processes
where tokens carry data. Although tools like CPN Tools and CPN IDE excel at
CPN-based simulation, they are often separate from modern data science
ecosystems. Meanwhile, Python has become the de facto language for process
mining, machine learning, and data analytics. In this paper, we introduce
CPN-Py, a Python library that faithfully preserves the core concepts of Colored
Petri Nets -- including color sets, timed tokens, guard logic, and hierarchical
structures -- while providing seamless integration with the Python environment.
We discuss its design, highlight its synergy with PM4Py (including stochastic
replay, process discovery, and decision mining functionalities), and illustrate
how the tool supports state space analysis and hierarchical CPNs. We also
outline how CPN-Py accommodates large language models, which can generate or
refine CPN models through a dedicated JSON-based format.

</details>


### [92] [Redbench: A Benchmark Reflecting Real Workloads](https://arxiv.org/abs/2506.12488)
*Skander Krid,Mihail Stoian,Andreas Kipf*

Main category: cs.DB

TL;DR: 介绍Redbench，包含30个反映现实查询模式的工作负载。


<details>
  <summary>Details</summary>
Motivation: 现有工作负载无法展现真实工作模式，尤其是分布偏移，阻碍现实学习组件的发展。

Method: 从支持基准中采样查询，并与Redset中观察到的工作负载特征对齐以获取工作负载。

Result: 得到包含30个反映现实查询模式的工作负载Redbench。

Conclusion: Redbench能解决现有工作负载不能展现真实模式的问题。

Abstract: Instance-optimized components have made their way into production systems. To
some extent, this adoption is due to the characteristics of customer workloads,
which can be individually leveraged during the model training phase. However,
there is a gap between research and industry that impedes the development of
realistic learned components: the lack of suitable workloads. Existing ones,
such as TPC-H and TPC-DS, and even more recent ones, such as DSB and CAB, fail
to exhibit real workload patterns, particularly distribution shifts.
  In this paper, we introduce Redbench, a collection of 30 workloads that
reflect query patterns observed in the real world. The workloads were obtained
by sampling queries from support benchmarks and aligning them with workload
characteristics observed in Redset.

</details>


### [93] [Towards Visualizing Electronic Medical Records via Natural Language Queries](https://arxiv.org/abs/2506.12837)
*Haodi Zhang,Siqi Ning,Qiyong Zheng,Jinyin Nie,Liangjie Zhang,Weicheng Wang,Yuanfeng Song*

Main category: cs.DB

TL;DR: 提出用大语言模型生成电子病历可视化数据，构建MedicalVis数据集，MedCodeT5方法表现佳，推动电子病历可视化发展。


<details>
  <summary>Details</summary>
Motivation: 电子病历数据可视化重要，但相关数据稀缺且手动标注成本高，阻碍医学可视化技术发展。

Method: 提出用大语言模型生成可视化数据，构建文本到可视化基准的新流程，引入MedCodeT5方法。

Result: 构建了含35,374个示例的MedicalVis数据集，MedCodeT5方法在从自然语言查询生成电子病历可视化方面优于多种基线。

Conclusion: 本研究和数据集有助于电子病历可视化方法的标准化评估，推动该领域发展。

Abstract: Electronic medical records (EMRs) contain essential data for patient care and
clinical research. With the diversity of structured and unstructured data in
EHR, data visualization is an invaluable tool for managing and explaining these
complexities. However, the scarcity of relevant medical visualization data and
the high cost of manual annotation required to develop such datasets pose
significant challenges to advancing medical visualization techniques. To
address this issue, we propose an innovative approach using large language
models (LLMs) for generating visualization data without labor-intensive manual
annotation. We introduce a new pipeline for building text-to-visualization
benchmarks suitable for EMRs, enabling users to visualize EMR statistics
through natural language queries (NLQs). The dataset presented in this paper
primarily consists of paired text medical records, NLQs, and corresponding
visualizations, forming the first large-scale text-to-visual dataset for
electronic medical record information called MedicalVis with 35,374 examples.
Additionally, we introduce an LLM-based approach called MedCodeT5, showcasing
its viability in generating EMR visualizations from NLQs, outperforming various
strong text-to-visualization baselines. Our work facilitates standardized
evaluation of EMR visualization methods while providing researchers with tools
to advance this influential field of application. In a nutshell, this study and
dataset have the potential to promote advancements in eliciting medical
insights through visualization.

</details>


### [94] [Humans, Machine Learning, and Language Models in Union: A Cognitive Study on Table Unionability](https://arxiv.org/abs/2506.12990)
*Sreeram Marimuthu,Nina Klimenkova,Roee Shraga*

Main category: cs.DB

TL;DR: 研究人类在数据发现中判断表可联合性的行为，开发机器学习框架提升人类表现，初步研究表明结合人类和大语言模型更好，为未来人机协作数据发现系统奠基。


<details>
  <summary>Details</summary>
Motivation: 数据发现和表可联合性是现代数据科学关键任务，但人类视角研究不足，因此研究人类判断表可联合性的行为。

Method: 设计实验调查并进行全面分析，评估人类决策，用分析结果开发机器学习框架，还进行大语言模型与人类表现对比的初步研究。

Result: 开发了提升人类表现的机器学习框架，初步研究表明结合人类和大语言模型通常更好。

Conclusion: 此研究为未来高效数据发现的人机协作系统奠定基础。

Abstract: Data discovery and table unionability in particular became key tasks in
modern Data Science. However, the human perspective for these tasks is still
under-explored. Thus, this research investigates the human behavior in
determining table unionability within data discovery. We have designed an
experimental survey and conducted a comprehensive analysis, in which we assess
human decision-making for table unionability. We use the observations from the
analysis to develop a machine learning framework to boost the (raw) performance
of humans. Furthermore, we perform a preliminary study on how LLM performance
is compared to humans indicating that it is typically better to consider a
combination of both. We believe that this work lays the foundations for
developing future Human-in-the-Loop systems for efficient data discovery.

</details>


### [95] [EnhanceGraph: A Continuously Enhanced Graph-based Index for High-dimensional Approximate Nearest Neighbor Search](https://arxiv.org/abs/2506.13144)
*Xiaoyao Zhong,Jiabao Jin,Peng Cheng,Mingyu Yang,Lei Chen,Haoyang Li,Zhitao Shen,Xuemin Lin,Heng Tao Shen,Jingkuan Song*

Main category: cs.DB

TL;DR: 提出EnhanceGraph框架，集成搜索和构建日志到共轭图，提升高维向量空间近似最近邻搜索质量，实验显示显著提升搜索准确率且不牺牲效率，算法已集成到VSAG。


<details>
  <summary>Details</summary>
Motivation: 深度学习发展使高维向量空间近似最近邻搜索受关注，现有图索引静态性导致搜索和构建日志未充分利用。

Method: 提出EnhanceGraph框架，将两种日志集成到共轭图，通过理论分析和观察提出优化方法，搜索日志存储局部到全局最优边，构建日志存储修剪边。

Result: 在多个数据集实验表明，EnhanceGraph显著提升搜索准确率，召回率从41.74%提升到93.42%，且不牺牲搜索效率。

Conclusion: EnhanceGraph能有效提升搜索质量，已集成到Ant Group的开源向量库VSAG。

Abstract: Recently, Approximate Nearest Neighbor Search in high-dimensional vector
spaces has garnered considerable attention due to the rapid advancement of deep
learning techniques. We observed that a substantial amount of search and
construction logs are generated throughout the lifespan of a graph-based index.
However, these two types of valuable logs are not fully exploited due to the
static nature of existing indexes. We present the EnhanceGraph framework, which
integrates two types of logs into a novel structure called a conjugate graph.
The conjugate graph is then used to improve search quality. Through theoretical
analyses and observations of the limitations of graph-based indexes, we propose
several optimization methods. For the search logs, the conjugate graph stores
the edges from local optima to global optima to enhance routing to the nearest
neighbor. For the construction logs, the conjugate graph stores the pruned
edges from the proximity graph to enhance retrieving of k nearest neighbors.
Our experimental results on several public and real-world industrial datasets
show that EnhanceGraph significantly improves search accuracy with the greatest
improvement on recall from 41.74% to 93.42%, but does not sacrifices search
efficiency. In addition, our EnhanceGraph algorithm has been integrated into
Ant Group's open-source vector library, VSAG.

</details>


### [96] [Parachute: Single-Pass Bi-Directional Information Passing](https://arxiv.org/abs/2506.13670)
*Mihail Stoian,Andreas Zimmerer,Skander Krid,Amadou Latyr Ngom,Jialin Ding,Tim Kraska,Andreas Kipf*

Main category: cs.DB

TL;DR: 本文致力于实现查询执行时的单遍双向信息传递，借助静态分析和预计算列，在JOB基准测试中提升了DuckDB v1.2的执行时间。


<details>
  <summary>Details</summary>
Motivation: 现有生产系统中的侧向信息传递仅支持单向信息流，而实例最优算法需额外遍历输入，阻碍了其在生产系统中的应用，因此要实现单遍双向信息传递。

Method: 通过静态分析信息流在哪些表之间被阻塞，并利用外键表上预计算的连接诱导指纹列。

Result: 在JOB基准测试中，当允许使用15%额外空间时，Parachute分别将无半连接过滤和有半连接过滤的DuckDB v1.2端到端执行时间提高了1.54倍和1.24倍。

Conclusion: 所提出的方法能有效提升数据库查询执行的效率。

Abstract: Sideways information passing is a well-known technique for mitigating the
impact of large build sides in a database query plan. As currently implemented
in production systems, sideways information passing enables only a
uni-directional information flow, as opposed to instance-optimal algorithms,
such as Yannakakis'. On the other hand, the latter require an additional pass
over the input, which hinders adoption in production systems.
  In this paper, we make a step towards enabling single-pass bi-directional
information passing during query execution. We achieve this by statically
analyzing between which tables the information flow is blocked and by
leveraging precomputed join-induced fingerprint columns on FK-tables. On the
JOB benchmark, Parachute improves DuckDB v1.2's end-to-end execution time
without and with semi-join filtering by 1.54x and 1.24x, respectively, when
allowed to use 15% extra space.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [97] [Towards Energy-Efficient Distributed Agreement](https://arxiv.org/abs/2506.12282)
*Hugo Mirault,Peter Robinson*

Main category: cs.DC

TL;DR: 研究同步消息传递睡眠模型下容错共识，提出新确定性共识算法，匹配最优时间复杂度，给出多值和二进制共识的能量复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究睡眠模型下可容忍一定数量崩溃故障的容错共识问题，衡量节点唤醒轮数的能量复杂度。

Method: 提出新的确定性共识算法。

Result: 算法匹配 $f + 1$ 轮的最优时间复杂度下限，多值共识能量复杂度为 ${O}(\lceil f^2 / n \rceil)$ 轮，二进制共识为 ${O}(\lceil f / \sqrt{n} \rceil)$ 轮。

Conclusion: 在睡眠模型下实现了容错共识，给出了不同类型共识的有效能量复杂度结果。

Abstract: We study fault-tolerant consensus in a variant of the synchronous message
passing model, where, in each round, every node can choose to be awake or
asleep. This is known as the sleeping model (Chatterjee, Gmyr, Pandurangan PODC
2020) and defines the awake complexity (also called \emph{energy complexity}),
which measures the maximum number of rounds that any node is awake throughout
the execution. Only awake nodes can send and receive messages in a given round
and all messages sent to sleeping nodes are lost. We present new deterministic
consensus algorithms that tolerate up to $f<n$ crash failures, where $n$ is the
number of nodes. Our algorithms match the optimal time complexity lower bound
of $f+1$ rounds. For multi-value consensus, where the input values are chosen
from some possibly large set, we achieve an energy complexity of ${O}(\lceil
f^2 / n \rceil)$ rounds, whereas for binary consensus, we show that ${O}(\lceil
f / \sqrt{n} \rceil)$ rounds are possible.

</details>


### [98] [Efficient Unified Caching for Accelerating Heterogeneous AI Workloads](https://arxiv.org/abs/2506.12370)
*Tianze Wang,Yifei Liu,Chen Chen,Pengfei Zuo,Jiawei Zhang,Qizhen Weng,Yin Chen,Zhenhua Han,Jieru Zhao,Quan Chen,Minyi Guo*

Main category: cs.DC

TL;DR: 提出适用于现代AI集群的统一高效缓存IGTCache，提升缓存命中率并减少作业完成时间。


<details>
  <summary>Details</summary>
Motivation: 现有缓存管理策略难以处理AI集群中的异构工作负载，需要统一缓存以避免代码侵入复杂性和减少缓存空间浪费。

Method: 利用层次访问抽象AccessStreamTree组织数据访问，用假设检验对访问模式分类，根据模式和粒度定制缓存管理策略。

Result: IGTCache比现有缓存框架缓存命中率提高55.6%，整体作业完成时间减少52.2%。

Conclusion: IGTCache是现代AI集群有效的统一缓存解决方案。

Abstract: Modern AI clusters, which host diverse workloads like data pre-processing,
training and inference, often store the large-volume data in cloud storage and
employ caching frameworks to facilitate remote data access. To avoid
code-intrusion complexity and minimize cache space wastage, it is desirable to
maintain a unified cache shared by all the workloads. However, existing cache
management strategies, designed for specific workloads, struggle to handle the
heterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous
access patterns and item storage granularities. In this paper, we propose
IGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache
leverages a hierarchical access abstraction, AccessStreamTree, to organize the
recent data accesses in a tree structure, facilitating access pattern detection
at various granularities. Using this abstraction, IGTCache applies hypothesis
testing to categorize data access patterns as sequential, random, or skewed.
Based on these detected access patterns and granularities, IGTCache tailors
optimal cache management strategies including prefetching, eviction, and space
allocation accordingly. Experimental results show that IGTCache increases the
cache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the
overall job completion time by 52.2%.

</details>


### [99] [QoS-aware Scheduling of Periodic Real-time Task Graphs on Heterogeneous Pre-occupied MECs](https://arxiv.org/abs/2506.12415)
*Ashutosh Shankar,Astha Kumari*

Main category: cs.DC

TL;DR: 本文针对异构、已被占用的移动边缘计算网络中周期性任务调度挑战，提出改进的HEFT算法，实验表明该方法提升了负载均衡和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 在延迟敏感应用中，高效任务调度对维持服务质量和满足严格时间约束至关重要，解决异构、已被占用的移动边缘计算网络中周期性任务调度挑战。

Method: 提出改进的异构最早完成时间（HEFT）算法，动态识别处理器空闲间隔以创建可行的超周期调度。

Result: 该方法实现了增强的负载均衡和资源利用率。

Conclusion: 该方法有潜力改善支持实时、周期性应用的异构MEC基础设施的性能。

Abstract: In latency-sensitive applications, efficient task scheduling is crucial for
maintaining Quality of Service (QoS) while meeting strict timing constraints.
This paper addresses the challenge of scheduling periodic tasks structured as
directed acyclic graphs (DAGs) within heterogeneous, pre-occupied Mobile Edge
Computing (MEC) networks. We propose a modified version of the Heterogeneous
Earliest Finish Time (HEFT) algorithm designed to exploit residual processing
capacity in preoccupied MEC environments. Our approach dynamically identifies
idle intervals on processors to create a feasible hyperperiodic schedule that
specifies an allocated virtual machine (VM), task version, and start time for
each task. This scheduling strategy maximizes the aggregate QoS by optimizing
task execution without disrupting the existing periodic workload, while also
adhering to periodicity, precedence, and resource constraints.Experimental
results demonstrate that our method achieves enhanced load balancing and
resource utilization, highlighting its potential to improve performance in
heterogeneous MEC infrastructures supporting real-time, periodic applications.

</details>


### [100] [HarMoEny: Efficient Multi-GPU Inference of MoE Models](https://arxiv.org/abs/2506.12417)
*Zachary Douchet,Rishi Sharma,Martijn de Vos,Rafael Pires,Anne-Marie Kermarrec,Oana Balmau*

Main category: cs.DC

TL;DR: 提出HarMoEny解决MoE模型负载不平衡问题，提高吞吐量并减少延迟。


<details>
  <summary>Details</summary>
Motivation: MoE模型中专家和GPU的负载不平衡会增加推理延迟，需要解决该问题。

Method: 提出动态令牌重新分配和异步预取专家到GPU内存两种技术。

Result: 在重负载不平衡下，吞吐量提高37%-70%，首令牌时间减少34%-41%，调度策略使GPU空闲时间最多减少84%。

Conclusion: HarMoEny能有效解决MoE负载不平衡问题，提高性能。

Abstract: Mixture-of-Experts (MoE) models offer computational efficiency during
inference by activating only a subset of specialized experts for a given input.
This enables efficient model scaling on multi-GPU systems that use expert
parallelism without compromising performance. However, load imbalance among
experts and GPUs introduces waiting times, which can significantly increase
inference latency. To address this challenge, we propose HarMoEny, a novel
solution to address MoE load imbalance through two simple techniques: (i)
dynamic token redistribution to underutilized GPUs and (ii) asynchronous
prefetching of experts from the system to GPU memory. These techniques achieve
a near-perfect load balance among experts and GPUs and mitigate delays caused
by overloaded GPUs. We implement HarMoEny and compare its latency and
throughput with four MoE baselines using real-world and synthetic datasets.
Under heavy load imbalance, HarMoEny increases throughput by 37%-70% and
reduces time-to-first-token by 34%-41%, compared to the next-best baseline.
Moreover, our ablation study demonstrates that HarMoEny's scheduling policy
reduces the GPU idling time by up to 84% compared to the baseline policies.

</details>


### [101] [Optimizing Federated Learning using Remote Embeddings for Graph Neural Networks](https://arxiv.org/abs/2506.12425)
*Pranjal Naman,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文提出OpES优化框架用于联邦GNN训练，通过远程邻域剪枝和重叠操作减少网络成本和训练时间，在大型密集图上表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于远程嵌入的联邦GNN训练方法因与共享嵌入服务器通信成本大而性能受限。

Method: 提出OpES框架，采用远程邻域剪枝，将嵌入推送到服务器与本地训练重叠。

Result: 在Reddit和Products等大型密集图上，训练速度比现有技术快约2倍，准确率比普通联邦GNN学习高20%。

Conclusion: OpES框架能有效降低网络成本和训练时间，提升训练效果。

Abstract: Graph Neural Networks (GNNs) have experienced rapid advancements in recent
years due to their ability to learn meaningful representations from graph data
structures. Federated Learning (FL) has emerged as a viable machine learning
approach for training a shared model on decentralized data, addressing privacy
concerns while leveraging parallelism. Existing methods that address the unique
requirements of federated GNN training using remote embeddings to enhance
convergence accuracy are limited by their diminished performance due to large
communication costs with a shared embedding server. In this paper, we present
OpES, an optimized federated GNN training framework that uses remote
neighbourhood pruning, and overlaps pushing of embeddings to the server with
local training to reduce the network costs and training time. The modest drop
in per-round accuracy due to pre-emptive push of embeddings is out-stripped by
the reduction in per-round training time for large and dense graphs like Reddit
and Products, converging up to $\approx2\times$ faster than the
state-of-the-art technique using an embedding server and giving up to $20\%$
better accuracy than vanilla federated GNN learning.

</details>


### [102] [Accelerating Cloud-Based Transcriptomics: Performance Analysis and Optimization of the STAR Aligner Workflow](https://arxiv.org/abs/2506.12611)
*Piotr Kica,Sabina Lichołai,Michał Orzechowski,Maciej Malawski*

Main category: cs.DC

TL;DR: 本文探索适应云环境的转录组图谱管道，提出云原生架构，用多种优化技术减少执行时间和成本，并进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 实现转录组图谱管道在云环境中经济高效和高通量计算。

Method: 提出可扩展的云原生架构运行STAR比对器，采用多种优化技术，通过中大规模实验验证。

Result: 早期停止优化使总比对时间减少23%，确定适合的EC2实例类型并验证现货实例可用性。

Conclusion: 分析了常用序列比对器的可扩展性和效率，验证了设计的有效性。

Abstract: In this work, we explore the Transcriptomics Atlas pipeline adapted for
cost-efficient and high-throughput computing in the cloud. We propose a
scalable, cloud-native architecture designed for running a resource-intensive
aligner -- STAR -- and processing tens or hundreds of terabytes of
RNA-sequencing data. We implement multiple optimization techniques that give
significant execution time and cost reduction. The impact of particular
optimizations is measured in medium-scale experiments followed by a large-scale
experiment that leverages all of them and validates the current design. Early
stopping optimization allows a reduction in total alignment time by 23%. We
analyze the scalability and efficiency of one of the most widely used sequence
aligners. For the cloud environment, we identify one of the most suitable EC2
instance types and verify the applicability of spot instances usage.

</details>


### [103] [Energy-Efficient Real-Time Job Mapping and Resource Management in Mobile-Edge Computing](https://arxiv.org/abs/2506.12686)
*Chuanchao Gao,Niraj Kumar,Arvind Easwaran*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Mobile-edge computing (MEC) has emerged as a promising paradigm for enabling
Internet of Things (IoT) devices to handle computation-intensive jobs. Due to
the imperfect parallelization of algorithms for job processing on servers and
the impact of IoT device mobility on data communication quality in wireless
networks, it is crucial to jointly consider server resource allocation and IoT
device mobility during job scheduling to fully benefit from MEC, which is often
overlooked in existing studies. By jointly considering job scheduling, server
resource allocation, and IoT device mobility, we investigate the
deadline-constrained job offloading and resource management problem in MEC with
both communication and computation contentions, aiming to maximize the total
energy saved for IoT devices. For the offline version of the problem, where job
information is known in advance, we formulate it as an Integer Linear
Programming problem and propose an approximation algorithm, $\mathtt{LHJS}$,
with a constant performance guarantee. For the online version, where job
information is only known upon release, we propose a heuristic algorithm,
$\mathtt{LBS}$, that is invoked whenever a job is released. Finally, we conduct
experiments with parameters from real-world applications to evaluate their
performance.

</details>


### [104] [Serving Large Language Models on Huawei CloudMatrix384](https://arxiv.org/abs/2506.12708)
*Pengfei Zuo,Huimin Lin,Junbo Deng,Nan Zou,Xingkun Yang,Yingyu Diao,Weifeng Gao,Ke Xu,Zhangyu Chen,Shirui Lu,Zhao Qiu,Peiyang Li,Xianyu Chang,Zhengzhong Yu,Fangzheng Miao,Jia Zheng,Ying Li,Yuan Feng,Bei Wang,Zaijian Zong,Mosong Zhou,Wenli Zhou,Houjiang Chen,Xingyu Liao,Yipeng Li,Wenxiao Zhang,Ping Zhu,Yinggang Wang,Chuanjie Xiao,Depeng Liang,Dong Cao,Juncheng Liu,Yongqiang Yang,Xiaolong Bai,Yi Li,Huaguo Xie,Huatao Wu,Zhibin Yu,Lv Chen,Hu Liu,Yujun Ding,Haipei Zhu,Jing Xia,Yi Xiong,Zhou Yu,Heng Liao*

Main category: cs.DC

TL;DR: 论文介绍华为云矩阵（CloudMatrix）AI数据中心架构及CloudMatrix - Infer大模型服务方案，评估显示其效率达业界领先水平。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展对AI基础设施提出高要求，传统AI集群有诸多局限，需重新设计软硬件集成。

Method: 引入CloudMatrix架构，通过超高带宽统一总线网络集成384个昇腾910C NPU和192个鲲鹏CPU；提出CloudMatrix - Infer服务方案，包含对等服务架构、大规模专家并行策略和硬件感知优化。

Result: 用DeepSeek - R1模型评估，CloudMatrix - Infer实现高吞吐量和低延迟，INT8量化保持模型精度。

Conclusion: CloudMatrix和CloudMatrix - Infer有效平衡吞吐量和延迟，效率达业界领先，能满足大语言模型发展需求。

Abstract: The rapid evolution of large language models (LLMs), driven by growing
parameter scales, adoption of mixture-of-experts (MoE) architectures, and
expanding context lengths, imposes unprecedented demands on AI infrastructure.
Traditional AI clusters face limitations in compute intensity, memory
bandwidth, inter-chip communication, and latency, compounded by variable
workloads and strict service-level objectives. Addressing these issues requires
fundamentally redesigned hardware-software integration. This paper introduces
Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in
the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C
NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified
Bus (UB) network, enabling direct all-to-all communication and dynamic pooling
of resources. These features optimize performance for communication-intensive
operations, such as large-scale MoE expert parallelism and distributed
key-value cache access. To fully leverage CloudMatrix384, we propose
CloudMatrix-Infer, an advanced LLM serving solution incorporating three core
innovations: a peer-to-peer serving architecture that independently scales
prefill, decode, and caching; a large-scale expert parallelism strategy
supporting EP320 via efficient UB-based token dispatch; and hardware-aware
optimizations including specialized operators, microbatch-based pipelining, and
INT8 quantization. Evaluation with the DeepSeek-R1 model shows
CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of
6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms
TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s
even under stringent 15 ms latency constraints, while INT8 quantization
maintains model accuracy across benchmarks.

</details>


### [105] [Self-Stabilizing Replicated State Machine Coping with Byzantine and Recurring Transient Faults](https://arxiv.org/abs/2506.12900)
*Shlomi Dolev,Amit Hendin,Maurice Herlihy,Maria Potop Butucaru,Elad Michael Schiller*

Main category: cs.DC

TL;DR: 本文提出首个满足拜占庭容错、复发性瞬态容错、准确性和自稳定特性的重复拜占庭协议。


<details>
  <summary>Details</summary>
Motivation: 重复拜占庭协议在区块链价格预言机等重要应用中至关重要，现有协议需满足拜占庭容错、复发性瞬态容错、准确性和自稳定等特性。

Method: 设计了从任意系统配置开始建立一致性的协议。

Result: 协议能在面对一定数量拜占庭参与者、常量复发性瞬态故障、恶意瞬态故障或随机瞬态故障时保持一致性。

Conclusion: 提出了满足相关特性的首个重复拜占庭协议。

Abstract: The ability to perform repeated Byzantine agreement lies at the heart of
important applications such as blockchain price oracles or replicated state
machines. Any such protocol requires the following properties: (1)
\textit{Byzantine fault-tolerance}, because not all participants can be assumed
to be honest, (2) r\textit{ecurrent transient fault-tolerance}, because even
honest participants may be subject to transient ``glitches'', (3)
\textit{accuracy}, because the results of quantitative queries (such as price
quotes) must lie within the interval of honest participants' inputs, and (4)
\textit{self-stabilization}, because it is infeasible to reboot a distributed
system following a fault.
  This paper presents the first protocol for repeated Byzantine agreement that
satisfies the properties listed above. Specifically, starting in an arbitrary
system configuration, our protocol establishes consistency. It preserves
consistency in the face of up to $\lceil n/3 \rceil -1$ Byzantine participants
{\em and} constant recurring (``noise'') transient faults, of up to $\lceil n/6
\rceil-1$ additional malicious transient faults, or even more than $\lceil n/6
\rceil-1$ (uniformly distributed) random transient faults, in each repeated
Byzantine agreement.

</details>


### [106] [Distributed Computing From First Principles](https://arxiv.org/abs/2506.12959)
*Kenneth Odoh*

Main category: cs.DC

TL;DR: 本书面向多类人群，旨在普及分布式计算核心概念，实现了多个基础算法。


<details>
  <summary>Details</summary>
Motivation: 让分布式计算核心概念更易理解，帮助不同背景的人获取有价值的见解。

Method: 实现分布式计算中的多个基础算法。

Result: 完成了一本涵盖分布式计算基础算法的书。

Conclusion: 这本书适合理论和实践不同方向的专业人士。

Abstract: This book on Distributed Computing aims to benefit a diverse audience,
ranging from aspiring engineers, and seasoned researchers, to a wide range of
professionals. Driven by my passion for making the core concepts of distributed
computing accessible, this work is a significant undertaking designed to
empower individuals from all backgrounds to gain valuable insight. Have you
ever wondered how a typical distributed system works under the hood? Are you
looking for a pedagogical guide with complete implementations? In this work, we
have implemented several foundational algorithms in Distributed Computing.
Whether your expertise lies in the theoretical foundations or the practical
applications of the principles of Distributed Systems, this book is for you.

</details>


### [107] [DDiT: Dynamic Resource Allocation for Diffusion Transformer Model Serving](https://arxiv.org/abs/2506.13497)
*Heyang Huang,Cunchen Hu,Jiaqi Zhu,Ziyuan Gao,Liangliang Xu,Yizhou Shan,Yungang Bao,Sun Ninghui,Tianwei Zhang,Sa Wang*

Main category: cs.DC

TL;DR: 提出DDiT系统优化文本到视频模型服务，评估显示其在延迟指标上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型服务系统采用整体式部署，忽视模块特性，GPU利用率低，且DiT模块有优化潜力。

Method: 引入解耦控制机制，设计单步粒度的贪婪资源分配和调度算法。

Result: 在多种数据集上的评估表明，DDiT在p99延迟和平均延迟上分别比现有基线高1.44倍和1.43倍。

Conclusion: DDiT系统有效优化了文本到视频模型服务，提升性能。

Abstract: The Text-to-Video (T2V) model aims to generate dynamic and expressive videos
from textual prompts. The generation pipeline typically involves multiple
modules, such as language encoder, Diffusion Transformer (DiT), and Variational
Autoencoders (VAE). Existing serving systems often rely on monolithic model
deployment, while overlooking the distinct characteristics of each module,
leading to inefficient GPU utilization. In addition, DiT exhibits varying
performance gains across different resolutions and degrees of parallelism, and
significant optimization potential remains unexplored. To address these
problems, we present DDiT, a flexible system that integrates both inter-phase
and intra-phase optimizations. DDiT focuses on two key metrics: optimal degree
of parallelism, which prevents excessive parallelism for specific resolutions,
and starvation time, which quantifies the sacrifice of each request. To this
end, DDiT introduces a decoupled control mechanism to minimize the
computational inefficiency caused by imbalances in the degree of parallelism
between the DiT and VAE phases. It also designs a greedy resource allocation
algorithm with a novel scheduling mechanism that operates at the single-step
granularity, enabling dynamic and timely resource scaling. Our evaluation on
the T5 encoder, OpenSora SDDiT, and OpenSora VAE models across diverse datasets
reveals that DDiT significantly outperforms state-of-the-art baselines by up to
1.44x in p99 latency and 1.43x in average latency.

</details>


### [108] [POPQC: Parallel Optimization for Quantum Circuits (Extended Version)](https://arxiv.org/abs/2506.13720)
*Pengyu Liu,Jatin Arora,Mingkuan Xu,Umut A. Acar*

Main category: cs.DC

TL;DR: 本文提出量子电路局部优化的并行算法，证明了其工作量和跨度复杂度，且优化后的电路是局部最优的。


<details>
  <summary>Details</summary>
Motivation: 现有量子电路优化器依赖启发式方法，时间复杂度高且为顺序算法，需要更高效的优化算法。

Method: 通过在电路中设置一组指针，按轮次选择指针集，并行优化含指针的段，并更新指针集以确保不变性。

Result: 对于常数Ω，算法需要O(nlgn)的工作量和O(rlgn)的跨度，优化后的电路是局部最优的。

Conclusion: 提出的并行算法能有效实现量子电路的局部优化。

Abstract: Optimization of quantum programs or circuits is a fundamental problem in
quantum computing and remains a major challenge. State-of-the-art quantum
circuit optimizers rely on heuristics and typically require superlinear, and
even exponential, time. Recent work proposed a new approach that pursues a
weaker form of optimality called local optimality. Parameterized by a natural
number $\Omega$, local optimality insists that each and every $\Omega$-segment
of the circuit is optimal with respect to an external optimizer, called the
oracle. Local optimization can be performed using only a linear number of calls
to the oracle but still incurs quadratic computational overheads in addition to
oracle calls. Perhaps most importantly, the algorithm is sequential.
  In this paper, we present a parallel algorithm for local optimization of
quantum circuits. To ensure efficiency, the algorithm operates by keeping a set
of fingers into the circuit and maintains the invariant that a $\Omega$-deep
circuit needs to be optimized only if it contains a finger. Operating in
rounds, the algorithm selects a set of fingers, optimizes in parallel the
segments containing the fingers, and updates the finger set to ensure the
invariant. For constant $\Omega$, we prove that the algorithm requires
$O(n\lg{n})$ work and $O(r\lg{n})$ span, where $n$ is the circuit size and $r$
is the number of rounds. We prove that the optimized circuit returned by the
algorithm is locally optimal in the sense that any $\Omega$-segment of the
circuit is optimal with respect to the oracle.

</details>


### [109] [BanditWare: A Contextual Bandit-based Framework for Hardware Prediction](https://arxiv.org/abs/2506.13730)
*Tainã Coleman,Hena Ahmed,Ravi Shende,Ismael Perez,Ïlkay Altintaş*

Main category: cs.DC

TL;DR: 提出在线推荐系统BanditWare，用上下文多臂老虎机算法为应用选硬件，在三个工作流应用评估，可集成NDP优化资源分配。


<details>
  <summary>Details</summary>
Motivation: 分布式计算系统从单系统过渡存在挑战，共享系统资源分配不当会引发诸多问题。

Method: 使用上下文多臂老虎机算法，在线实时学习和适应新工作负载。

Result: 在Cycles、BurnPro3D和矩阵乘法应用上进行评估。

Conclusion: BanditWare可与NDP无缝集成，让不同经验用户有效优化资源分配。

Abstract: Distributed computing systems are essential for meeting the demands of modern
applications, yet transitioning from single-system to distributed environments
presents significant challenges. Misallocating resources in shared systems can
lead to resource contention, system instability, degraded performance, priority
inversion, inefficient utilization, increased latency, and environmental
impact.
  We present BanditWare, an online recommendation system that dynamically
selects the most suitable hardware for applications using a contextual
multi-armed bandit algorithm. BanditWare balances exploration and exploitation,
gradually refining its hardware recommendations based on observed application
performance while continuing to explore potentially better options. Unlike
traditional statistical and machine learning approaches that rely heavily on
large historical datasets, BanditWare operates online, learning and adapting in
real-time as new workloads arrive.
  We evaluated BanditWare on three workflow applications: Cycles (an
agricultural science scientific workflow) BurnPro3D (a web-based platform for
fire science) and a matrix multiplication application. Designed for seamless
integration with the National Data Platform (NDP), BanditWare enables users of
all experience levels to optimize resource allocation efficiently.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [110] [Relative Error Fair Clustering in the Weak-Strong Oracle Model](https://arxiv.org/abs/2506.12287)
*Vladimir Braverman,Prathamesh Dharangutte,Shaofeng H. -C. Jiang,Hoai-An Nguyen,Chen Wang,Yubo Zhang,Samson Zhou*

Main category: cs.DS

TL;DR: 研究从强弱两种距离信息源获取信息的公平聚类问题，以最少强预言机查询数实现近最优公平聚类，取得公平k - 中值聚类的首个(1 + ε)-核心集。


<details>
  <summary>Details</summary>
Motivation: 在存在不准确信息的情况下实现公平性的重要需求，以及准确但昂贵和便宜但不准确的相似度度量之间的权衡。

Method: 通过对强预言机进行poly(k/ε · log n)次查询，实现公平k - 中值聚类的(1 + ε)-核心集。

Result: 获得公平k - 中值聚类的首个(1 + ε)-核心集，结果也适用于标准设置和一般z = O(1)的(k, z)-聚类。此前标准k - 聚类问题只有常数因子(> 10)近似结果，且无人考虑公平k - 中值聚类问题。

Conclusion: 该方法在公平聚类问题上取得进展，以较少强预言机查询数实现近最优公平聚类，在标准聚类问题上也有更好结果。

Abstract: We study fair clustering problems in a setting where distance information is
obtained from two sources: a strong oracle providing exact distances, but at a
high cost, and a weak oracle providing potentially inaccurate distance
estimates at a low cost. The goal is to produce a near-optimal fair clustering
on $n$ input points with a minimum number of strong oracle queries. This models
the increasingly common trade-off between accurate but expensive similarity
measures (e.g., large-scale embeddings) and cheaper but inaccurate
alternatives. The study of fair clustering in the model is motivated by the
important quest of achieving fairness with the presence of inaccurate
information. We achieve the first $(1+\varepsilon)$-coresets for fair
$k$-median clustering using $\text{poly}\left(\frac{k}{\varepsilon}\cdot\log
n\right)$ queries to the strong oracle. Furthermore, our results imply coresets
for the standard setting (without fairness constraints), and we could in fact
obtain $(1+\varepsilon)$-coresets for $(k,z)$-clustering for general $z=O(1)$
with a similar number of strong oracle queries. In contrast, previous results
achieved a constant-factor $(>10)$ approximation for the standard
$k$-clustering problems, and no previous work considered the fair $k$-median
clustering problem.

</details>


### [111] [A polynomial delay algorithm generating all potential maximal cliques in triconnected planar graphs](https://arxiv.org/abs/2506.12635)
*Alexander Grigoriev,Yasuaki Kobayashi,Hisao Tamaki,Tom C. van der Zanden*

Main category: cs.DS

TL;DR: 开发三连通平面图潜在最大团新特征及多项式延迟算法，结合已有算法得到一般平面图树宽算法。


<details>
  <summary>Details</summary>
Motivation: 为三连通平面图和一般平面图的相关算法研究提供新方法。

Method: 开发三连通平面图潜在最大团的新特征，结合Bouchitté和Todinca的动态规划算法。

Result: 得到生成三连通平面图所有潜在最大团的多项式延迟算法，以及一般平面图树宽算法，时间复杂度与潜在最大团数量线性相关，与顶点数量多项式相关。

Conclusion: 所提出的算法可有效处理三连通平面图和一般平面图相关问题。

Abstract: We develop a new characterization of potential maximal cliques of a
triconnected planar graph and, using this characterization, give a polynomial
delay algorithm generating all potential maximal cliques of a given
triconnected planar graph. Combined with the dynamic programming algorithms due
to Bouchitt{\'e} and Todinca, this algorithm leads to a treewidth algorithm for
general planar graphs that runs in time linear in the number of potential
maximal cliques and polynomial in the number of vertices.

</details>


### [112] [Approximations for Fault-Tolerant Total and Partial Positive Influence Domination](https://arxiv.org/abs/2506.12828)
*Ioannis Lamprou,Ioannis Sigalas,Ioannis Vaxevanakis,Vassilis Zissimopoulos*

Main category: cs.DS

TL;DR: 本文研究容错全控制集和加权部分正影响控制集问题，给出近似算法和对数近似比证明，还扩展了非子模函数近似框架。


<details>
  <summary>Details</summary>
Motivation: 定义容错全控制集版本，并研究加权部分正影响控制集问题的容错变体，寻求这些问题的近似算法。

Method: 证明容错全控制集的近似比，对加权部分正影响控制集问题的简单、全和连通变体给出对数近似，扩展非子模函数近似框架到分数值函数。

Result: 得到容错全控制集的1 + ln(Δ + m - 1)近似比，以及加权部分正影响控制集问题各变体的对数近似比。

Conclusion: 成功解决相关问题的近似求解，扩展的近似框架有独立价值。

Abstract: In $\textit{total domination}$, given a graph $G=(V,E)$, we seek a
minimum-size set of nodes $S\subseteq V$, such that every node in $V$ has at
least one neighbor in $S$. We define a $\textit{fault-tolerant}$ version of
total domination, where we require any node in $V \setminus S$ to have at least
$m$ neighbors in $S$. Let $\Delta$ denote the maximum degree in $G$. We prove a
first $1 + \ln(\Delta + m - 1)$ approximation for fault-tolerant total
domination. We also consider fault-tolerant variants of the weighted
$\textit{partial positive influence dominating set}$ problem, where we seek a
minimum-size set of nodes $S\subseteq V$, such that every node in $V$ is either
a member of $S$ or the sum of weights of its incident edges leading to nodes in
$S$ is at least half of the sum of weights over all its incident edges. We
prove the first logarithmic approximations for the simple, total, and connected
variants of this problem. To prove the result for the connected case, we extend
the general approximation framework for non-submodular functions from
integer-valued to fractional-valued functions, which we believe is of
independent interest.

</details>


### [113] [Downstream: efficient cross-platform algorithms for fixed-capacity stream downsampling](https://arxiv.org/abs/2506.12975)
*Connor Yang,Joey Wagner,Emily Dolson,Luis Zaman,Matthew Andres Moreno*

Main category: cs.DS

TL;DR: 本文介绍了Downstream库，它实现支持三种下采样泛化算法，支持多语言，还通过多种方式实现无缝互操作。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据流需要滚动、实时机制来汇总流历史，环形缓冲区在某些场景不能满足需求，需新方法。

Method: 实现支持三种下采样泛化的算法，支持多语言，通过多包装框架分发、广泛跨实现测试和文档实现互操作。

Result: 开发出Downstream库，实现三种下采样泛化，支持多种编程语言。

Conclusion: Downstream库能满足从嵌入式设备到高性能计算节点和AI/ML硬件加速器等广泛应用需求。

Abstract: Due to ongoing accrual over long durations, a defining characteristic of
real-world data streams is the requirement for rolling, often real-time,
mechanisms to coarsen or summarize stream history. One common data structure
for this purpose is the ring buffer, which maintains a running downsample
comprising most recent stream data. In some downsampling scenarios, however, it
can instead be necessary to maintain data items spanning the entirety of
elapsed stream history. Fortunately, approaches generalizing the ring buffer
mechanism have been devised to support alternate downsample compositions, while
maintaining the ring buffer's update efficiency and optimal use of memory
capacity. The Downstream library implements algorithms supporting three such
downsampling generalizations: (1) "steady," which curates data evenly spaced
across the stream history; (2) "stretched," which prioritizes older data; and
(3) "tilted," which prioritizes recent data. To enable a broad spectrum of
applications ranging from embedded devices to high-performance computing nodes
and AI/ML hardware accelerators, Downstream supports multiple programming
languages, including C++, Rust, Python, Zig, and the Cerebras Software
Language. For seamless interoperation, the library incorporates distribution
through multiple packaging frameworks, extensive cross-implementation testing,
and cross-implementation documentation.

</details>


### [114] [The Densest SWAMP problem: subhypergraphs with arbitrary monotonic partial edge rewards](https://arxiv.org/abs/2506.12998)
*Vedangi Bengali,Nikolaj Tatti,Iiro Kumpulainen,Florian Adriaens,Nate Veldt*

Main category: cs.DS

TL;DR: 本文研究带部分超边奖励的最密集子超图问题，考虑更广泛的单调奖励情况，证明非凸奖励的困难性，设计两种1/k近似算法并进行实证分析。


<details>
  <summary>Details</summary>
Motivation: 先前工作仅在奖励函数为凸函数时解决了带部分超边奖励的最密集子超图问题，本文考虑更广泛的单调奖励情况。

Method: 先证明一类非凸奖励的困难性，通过投影到最近的凸奖励集设计1/k近似算法，还使用更快的剥皮算法设计另一个1/k近似算法。

Result: 得到两种1/k近似算法，并对算法在多个真实超图上进行了实证分析。

Conclusion: 在更广泛的单调奖励设置下解决了带部分超边奖励的最密集子超图问题，提出有效近似算法。

Abstract: We consider a generalization of the densest subhypergraph problem where
nonnegative rewards are given for including partial hyperedges in a dense
subhypergraph. Prior work addressed this problem only in cases where reward
functions are convex, in which case the problem is poly-time solvable. We
consider a broader setting where rewards are monotonic but otherwise arbitrary.
We first prove hardness results for a wide class of non-convex rewards, then
design a 1/k-approximation by projecting to the nearest set of convex rewards,
where k is the maximum hyperedge size. We also design another 1/k-approximation
using a faster peeling algorithm, which (somewhat surprisingly) differs from
the standard greedy peeling algorithm used to approximate other variants of the
densest subgraph problem. Our results include an empirical analysis of our
algorithm across several real-world hypergraphs.

</details>


### [115] [Efficient Approximate Temporal Triangle Counting in Streaming with Predictions](https://arxiv.org/abs/2506.13173)
*Giorgio Venturin,Ilie Sarpe,Fabio Vandin*

Main category: cs.DS

TL;DR: 提出可扩展高效算法STEP近似计算大规模时序图中三角形数量，实验证明其效果优。


<details>
  <summary>Details</summary>
Motivation: 现有精确和近似算法无法处理大规模时序图中三角形计数问题，需新算法。

Method: STEP结合对时序边参与三角形数量的预测和简单采样策略。

Result: 理论证明STEP用亚线性内存获无偏且准确估计，实验表明STEP输出高质量估计且比现有方法高效。

Conclusion: STEP算法能有效解决大规模时序图中三角形计数问题，具有可扩展性、高效性和准确性。

Abstract: Triangle counting is a fundamental and widely studied problem on static
graphs, and recently on temporal graphs, where edges carry information on the
timings of the associated events. Streaming processing and resource efficiency
are crucial requirements for counting triangles in modern massive temporal
graphs, with millions of nodes and up to billions of temporal edges. However,
current exact and approximate algorithms are unable to handle large-scale
temporal graphs. To fill such a gap, we introduce STEP, a scalable and
efficient algorithm to approximate temporal triangle counts from a stream of
temporal edges. STEP combines predictions to the number of triangles a temporal
edge is involved in, with a simple sampling strategy, leading to scalability,
efficiency, and accurate approximation of all eight temporal triangle types
simultaneously. We analytically prove that, by using a sublinear amount of
memory, STEP obtains unbiased and very accurate estimates. In fact, even noisy
predictions can significantly reduce the variance of STEP's estimates. Our
extensive experiments on massive temporal graphs with up to billions of edges
demonstrate that STEP outputs high-quality estimates and is more efficient than
state-of-the-art methods.

</details>


### [116] [Ultra-Resilient Superimposed Codes: Near-Optimal Construction and Applications](https://arxiv.org/abs/2506.13489)
*Gianluca De Marco,Dariusz R. Kowalski*

Main category: cs.DS

TL;DR: 提出超弹性叠加码（URSCs），能抵抗两类对抗扰动，有通用性，给出近最优长度的多项式时间构造，在多应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 经典叠加码依赖严格对齐假设，在异步和易出错环境中效果受限。

Method: 引入URSCs，扩展经典叠加框架，实现更强的码字隔离特性和抗扰性，给出多项式时间构造。

Result: 给出近最优长度的URSCs的多项式时间构造，在无协调蜂鸣网络等应用中显著提升性能。

Conclusion: URSCs在多方面优于以往构造，推动了相关应用的技术发展。

Abstract: A superimposed code is a collection of binary vectors (codewords) with the
property that no vector is contained in the Boolean sum of any $k$ others,
enabling unique identification of codewords within any group of $k$.
Superimposed codes are foundational combinatorial tools with applications in
areas ranging from distributed computing and data retrieval to fault-tolerant
communication. However, classical superimposed codes rely on strict alignment
assumptions, limiting their effectiveness in asynchronous and fault-prone
environments, which are common in modern systems and applications.
  We introduce Ultra-Resilient Superimposed Codes (URSCs), a new class of codes
that extends the classic superimposed framework by ensuring a stronger
codewords' isolation property and resilience to two types of adversarial
perturbations: arbitrary cyclic shifts and partial bitwise corruption (flips).
Additionally, URSCs exhibit universality, adapting seamlessly to any number $k$
of concurrent codewords without prior knowledge. This is a combination of
properties not achieved in any previous construction.
  We provide the first polynomial-time construction of URSCs with near-optimal
length, significantly outperforming previous constructions with less general
features, all without requiring prior knowledge of the number of concurrent
codewords, $k$. % We demonstrate that our URSCs significantly advance the state
of the art in multiple applications, including uncoordinated beeping networks,
where our codes reduce time complexity for local broadcast by nearly two orders
of magnitude, and generalized contention resolution in multi-access channel
communication.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [117] [On Hierarchies of Fairness Notions in Cake Cutting: From Proportionality to Super Envy-Freeness](https://arxiv.org/abs/2506.12950)
*Arnav Mehra,Alexandros Psomas*

Main category: cs.GT

TL;DR: 本文在Robertson - Webb查询模型下研究经典蛋糕切割问题，引入CHB和CLB两种公平性概念层级，分析其与比例、无嫉妒等分配的关系，并给出计算CHB - n分配的查询复杂度及CHB - 2和CLB - 2等分配的查询复杂度下界。


<details>
  <summary>Details</summary>
Motivation: 现有经典蛋糕切割问题的公平分配概念在查询复杂度上存在较大差距，需要引入新的公平性概念来研究更合适的分配计算方式。

Method: 引入CHB和CLB两种新的公平性概念层级，分析不同k值下这些概念与已知公平分配概念的关系，并证明其在Robertson - Webb模型下的查询复杂度。

Result: 证明CHB - n分配可在O(n^4)查询内计算，CHB - 2及k≥2的CHB - k分配需Ω(n^2)查询，CLB - 2及k≥2的CLB - k分配不能用有界数量查询计算。

Conclusion: 新引入的CHB和CLB公平性概念层级为蛋糕切割问题的公平分配计算提供了新视角，不同层级的分配在查询复杂度上有不同表现。

Abstract: We consider the classic cake-cutting problem of producing fair allocations
for $n$ agents, in the Robertson-Webb query model. In this model, it is known
that: (i) proportional allocations can be computed using $O(n \log n)$ queries,
and this is optimal for deterministic protocols; (ii) envy-free allocations (a
subset of proportional allocations) can be computed using $O\left(
n^{n^{n^{n^{n^{n}}}}} \right)$ queries, and the best known lower bound is
$\Omega(n^2)$; (iii) perfect allocations (a subset of envy-free allocations)
cannot be computed using a bounded (in $n$) number of queries.
  In this work, we introduce two hierarchies of new fairness notions:
Complement Harmonically Bounded (CHB) and Complement Linearly Bounded (CLB).
Intuitively, these notions of fairness ask that, for every agent $i$, the
collective value that a group of agents has (from the perspective of agent $i$)
is limited. CHB-$k$ and CLB-$k$ coincide with proportionality for $k=1$. For
all $k \leq n$, CHB-$k$ allocations are a superset of envy-free allocations
(i.e., easier to find). On the other hand, for $k \in [2, \lceil n/2 \rceil -
1]$, CLB-$k$ allocations are incomparable to envy-free allocations. For $k \geq
\lceil n/2 \rceil$, CLB-$k$ allocations are a subset of envy-free allocations
(i.e., harder to find).
  We prove that CHB-$n$ allocations can be computed using $O(n^4)$ queries in
the Robertson-Webb model. On the flip side, finding CHB-$2$ (and therefore all
CHB-$k$ for $k \geq 2$) allocations requires $\Omega(n^2)$ queries, while
CLB-$2$ (and therefore all CLB-$k$ for $k \geq 2$) allocations cannot be
computed using a bounded (in $n$) number of queries.

</details>


### [118] [Quantitative Relaxations of Arrow's Axioms](https://arxiv.org/abs/2506.12961)
*Suvadip Sana,Daniel Brous,Martin T. Wells,Moon Duchin*

Main category: cs.GT

TL;DR: 本文提出放松投票规则阿罗公理的新方法，将两个基础公理扩展到[0,1]取值，通过实证测试不同投票规则，发现博尔达规则得分最高。


<details>
  <summary>Details</summary>
Motivation: 解决社会选择理论中对阿罗公理的长期批评，软化公理方法。

Method: 先衡量给定偏好概况下公理的维持或违反程度，将两个基础公理扩展到[0,1]取值，用实际和合成数据测试投票规则。

Result: 博尔达规则在观察和合成选举中始终获得最高的σ_IIA和σ_U分数。

Conclusion: 该方法可对阿罗定理进行定量重述，博尔达规则表现良好，与相关研究结果有趣对比。

Abstract: In this paper we develop a novel approach to relaxing Arrow's axioms for
voting rules, addressing a long-standing critique in social choice theory.
Classical axioms (often styled as fairness axioms or fairness criteria) are
assessed in a binary manner, so that a voting rule fails the axiom if it fails
in even one corner case. Many authors have proposed a probabilistic framework
to soften the axiomatic approach. Instead of immediately passing to random
preference profiles, we begin by measuring the degree to which an axiom is
upheld or violated on a given profile. We focus on two foundational
axioms-Independence of Irrelevant Alternatives (IIA) and Unanimity (U)-and
extend them to take values in $[0,1]$. Our $\sigma_{IIA}$ measures the
stability of a voting rule when candidates are removed from consideration,
while $\sigma_{U}$ captures the degree to which the outcome respects majority
preferences. Together, these metrics quantify how a voting rule navigates the
fundamental trade-off highlighted by Arrow's Theorem. We show that
$\sigma_{IIA}\equiv 1$ recovers classical IIA, and $\sigma_{U}>0$ recovers
classical Unanimity, allowing a quantitative restatement of Arrow's Theorem. In
the empirical part of the paper, we test these metrics on two kinds of data: a
set of over 1000 ranked choice preference profiles from Scottish local
elections, and a batch of synthetic preference profiles generated with a
Bradley-Terry-type model. We use those to investigate four positional voting
rules-Plurality, 2-Approval, 3-Approval, and the Borda rule-as well as the
iterative rule known as Single Transferable Vote (STV). The Borda rule
consistently receives the highest $\sigma_{IIA}$ and $\sigma_{U}$ scores across
observed and synthetic elections. This compares interestingly with a recent
result of Maskin showing that weakening IIA to include voter preference
intensity uniquely selects Borda.

</details>


### [119] [One-dimensional vs. Multi-dimensional Pricing in Blockchain Protocols](https://arxiv.org/abs/2506.13271)
*Aggelos Kiayias,Elias Koutsoupias,Giorgos Panagiotakos,Kyriaki Zioga*

Main category: cs.GT

TL;DR: 分析区块链交易一维和多维定价方案的福利，指出多维在稳定条件下优，一维在瞬态优，需理解瞬态影响并提出缓解机制。


<details>
  <summary>Details</summary>
Motivation: 有效分配区块链系统资源和保证长期经济可行性，解决一维定价资源利用不足和多维定价价格发现难的问题。

Method: 研究一维和多维定价方案在区块链不同状态下的福利情况。

Result: 多维定价在稳定区块链条件下更优，一维定价在瞬态表现更好，收敛快且易计算。

Conclusion: 多维定价虽在均衡时有优势，但实施有系统过渡成本，需深入理解瞬态影响，提出缓解机制供未来研究。

Abstract: Blockchain transactions consume diverse resources, foremost among them
storage, but also computation, communication, and others. Efficiently charging
for these resources is crucial for effective system resource allocation and
long-term economic viability. The prevailing approach, one-dimensional pricing,
sets a single price for a linear combination of resources. However, this often
leads to under-utilization when resource capacities are limited.
Multi-dimensional pricing, which independently prices each resource, offers an
alternative but presents challenges in price discovery.
  This work focuses on the welfare achieved by these two schemes. We prove that
multi-dimensional pricing is superior under stable blockchain conditions.
Conversely, we show that one-dimensional pricing outperforms its
multi-dimensional counterpart in transient states, exhibiting faster
convergence and greater computational tractability. These results highlight a
critical trade-off: while multi-dimensional pricing offers efficiency gains at
equilibrium, its implementation incurs costs associated with system
transitions. Our findings underscore the necessity for a deeper understanding
of these transient effects before widespread adoption. Finally, we propose
mechanisms that aim to mitigate some of these issues, paving the way for future
research.

</details>


### [120] [The impact of uncertainty on regularized learning in games](https://arxiv.org/abs/2506.13286)
*Pierre-Louis Cauvin,Davide Legacci,Panayotis Mertikopoulos*

Main category: cs.GT

TL;DR: 研究随机性和不确定性对博弈学习的影响，发现不确定性使玩家策略趋近纯策略，FTRL 动态极限是纯纳什均衡，特定纯策略集稳定，随机性能打破确定性动态的循环。


<details>
  <summary>Details</summary>
Motivation: 探究随机和不确定性如何影响博弈中的学习。

Method: 研究“跟随正则化领导者”（FTRL）动态的扰动变体，分析玩家收益观察和策略更新受随机冲击的情况。

Result: 玩家策略轨迹在有限时间内接近纯策略；FTRL 动态极限是纯纳什均衡；特定纯策略集稳定吸引；随机性能打破确定性动态的循环。

Conclusion: 明确了不确定性下博弈学习的策略趋向、动态极限特征以及随机因素对动态的影响。

Abstract: In this paper, we investigate how randomness and uncertainty influence
learning in games. Specifically, we examine a perturbed variant of the dynamics
of "follow-the-regularized-leader" (FTRL), where the players' payoff
observations and strategy updates are continually impacted by random shocks.
Our findings reveal that, in a fairly precise sense, "uncertainty favors
extremes": in any game, regardless of the noise level, every player's
trajectory of play reaches an arbitrarily small neighborhood of a pure strategy
in finite time (which we estimate). Moreover, even if the player does not
ultimately settle at this strategy, they return arbitrarily close to some
(possibly different) pure strategy infinitely often. This prompts the question
of which sets of pure strategies emerge as robust predictions of learning under
uncertainty. We show that (a) the only possible limits of the FTRL dynamics
under uncertainty are pure Nash equilibria; and (b) a span of pure strategies
is stable and attracting if and only if it is closed under better replies.
Finally, we turn to games where the deterministic dynamics are recurrent - such
as zero-sum games with interior equilibria - and we show that randomness
disrupts this behavior, causing the stochastic dynamics to drift toward the
boundary on average.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [121] [Algorithms for estimating linear function in data mining](https://arxiv.org/abs/2506.12069)
*Thomas Hoang*

Main category: cs.IR

TL;DR: 本文展示用于估计线性效用函数以预测用户偏好的算法，还提及算法在数据科学中的应用，并以房价预测为例说明，指出当前数据发现算法存在问题。


<details>
  <summary>Details</summary>
Motivation: 在数据科学领域，从大型数据集中获取有价值信息对决策至关重要，需要更好的方法来预测用户偏好和处理数据。

Method: 展示多种估计线性效用函数的算法，如GNN、PLOD算法。

Result: 所提出的算法可帮助从大型数据库中筛选出用户感兴趣的子集，还能应用于数据科学的预测场景。

Conclusion: 当前数据发现算法存在因预定义效用函数和人工属性排序导致的耗时迭代问题，有待改进。

Abstract: The main goal of this topic is to showcase several studied algorithms for
estimating the linear utility function to predict the users preferences. For
example, if a user comes to buy a car that has several attributes including
speed, color, age, etc in a linear function, the algorithms that we present in
this paper help with estimating this linear function to filter out a small
subset that would be of best interest to the user among a million tuples in a
very large database. In addition, the estimating linear function could also be
applicable in getting to know what the data can do or predicting the future
based on the data that is used in data science, which is demonstrated by the
GNN, PLOD algorithms. In the ever-evolving field of data science, deriving
valuable insights from large datasets is critical for informed decision-making,
particularly in predictive applications. Data analysts often identify
high-quality datasets without missing values, duplicates, or inconsistencies
before merging diverse attributes for analysis. Taking housing price prediction
as a case study, various attributes must be considered, including location
factors (proximity to urban centers, crime rates), property features (size,
style, modernity), and regional policies (tax implications). Experts in the
field typically rank these attributes to establish a predictive utility
function, which machine learning models use to forecast outcomes like housing
prices. Several data discovery algorithms, including those that address the
challenges of predefined utility functions and human input for attribute
ranking, which often result in a time-consuming iterative process, that the
work of cannot overcome.

</details>


### [122] [T$^2$-RAGBench: Text-and-Table Benchmark for Evaluating Retrieval-Augmented Generation](https://arxiv.org/abs/2506.12071)
*Jan Strich,Enes Kutay Isgorur,Maximilian Trescher,Chris Biemann,Martin Semmann*

Main category: cs.IR

TL;DR: 本文介绍了T² - RAGBench基准，用于评估真实世界金融数据上的RAG方法，评估了流行RAG方法，指出Hybrid BM25最有效，该基准对SOTA模型仍具挑战，还进行了消融研究，代码和数据集在线可用。


<details>
  <summary>Details</summary>
Motivation: 金融文档包含文本和表格信息，需要强大的RAG系统来完成复杂数值任务，现有QA数据集存在问题，需要新基准评估RAG方法。

Method: 创建包含32,908个问题 - 上下文 - 答案三元组的T² - RAGBench基准，将现有数据集转换为上下文无关格式，对流行RAG方法进行综合评估，开展消融研究。

Result: Hybrid BM25是处理文本和表格数据最有效的方法，T² - RAGBench对SOTA大语言模型和RAG方法仍具挑战性。

Conclusion: T² - RAGBench为文本和表格数据的现有RAG方法提供了现实且严格的基准。

Abstract: While most financial documents contain a combination of textual and tabular
information, robust Retrieval-Augmented Generation (RAG) systems are essential
for effectively accessing and reasoning over such content to perform complex
numerical tasks. This paper introduces T$^2$-RAGBench, a benchmark comprising
32,908 question-context-answer triples, designed to evaluate RAG methods on
real-world financial data. Unlike typical QA datasets that operate under
Oracle-context settings, where the relevant context is explicitly provided,
T$^2$-RAGBench challenges models to first retrieve the correct context before
conducting numerical reasoning. Existing QA datasets involving text and tables
typically contain context-dependent questions, which may yield multiple correct
answers depending on the provided context. To address this, we transform these
datasets into a context-independent format, enabling reliable RAG evaluation.
We conduct a comprehensive evaluation of popular RAG methods. Our analysis
identifies Hybrid BM25, a technique that combines dense and sparse vectors, as
the most effective approach for text-and-table data. However, results
demonstrate that T$^2$-RAGBench remains challenging even for SOTA LLMs and RAG
methods. Further ablation studies examine the impact of embedding models and
corpus size on retrieval performance. T$^2$-RAGBench provides a realistic and
rigorous benchmark for existing RAG methods on text-and-table data. Code and
dataset are available online.

</details>


### [123] [WebTrust: An AI-Driven Data Scoring System for Reliable Information Retrieval](https://arxiv.org/abs/2506.12072)
*Joydeep Chandra,Aleksandr Algazinov,Satyam Kumar Navneet,Rim El Filali,Matt Laing,Andrew Hanna*

Main category: cs.IR

TL;DR: 文章介绍WebTrust系统，其基于微调模型和自定义数据集，能为信息评分并说明理由，性能优越，可助人们获取可信信息。


<details>
  <summary>Details</summary>
Motivation: 现有AI工具难评估信息可信度，搜索引擎缺乏数据可靠性指标，需解决此问题。

Method: 构建WebTrust系统，基于微调的IBM Granite - 1B模型和自定义数据集，为处理的陈述分配0.1到1的可靠性分数并给出理由，用提示工程评估。

Result: WebTrust在MAE、RMSE和R2实验中优于其他小规模大语言模型和基于规则的方法，用户测试显示展示可靠性分数让用户更有信心和满意度。

Conclusion: WebTrust准确、透明、易用，是对抗错误信息、让可信信息更易获取的实用方案。

Abstract: As access to information becomes more open and widespread, people are
increasingly using AI tools for assistance. However, many of these tools
struggle to estimate the trustworthiness of the information. Although today's
search engines include AI features, they often fail to offer clear indicators
of data reliability. To address this gap, we introduce WebTrust, a system
designed to simplify the process of finding and judging credible information
online. Built on a fine-tuned version of IBM's Granite-1B model and trained on
a custom dataset, WebTrust works by assigning a reliability score (from 0.1 to
1) to each statement it processes. In addition, it offers a clear justification
for why a piece of information received that score. Evaluated using prompt
engineering, WebTrust consistently achieves superior performance compared to
other small-scale LLMs and rule-based approaches, outperforming them across all
experiments on MAE, RMSE, and R2. User testing showed that when reliability
scores are displayed alongside search results, people feel more confident and
satisfied with the information they find. With its accuracy, transparency, and
ease of use, WebTrust offers a practical solution to help combat misinformation
and make trustworthy information more accessible to everyone.

</details>


### [124] [T-TExTS (Teaching Text Expansion for Teacher Scaffolding): Enhancing Text Selection in High School Literature through Knowledge Graph-Based Recommendation](https://arxiv.org/abs/2506.12075)
*Nirmal Gelal,Chloe Snow,Ambyr Rios,Hande Küçük McGinty*

Main category: cs.IR

TL;DR: 本文提出开发了推荐系统T - TExTS，用知识图谱为高中英语文学教师选书提供建议，评估显示不同方法各有优势，该系统可减轻教师选书负担。


<details>
  <summary>Details</summary>
Motivation: 高中英语文学教师因规划时间和资源有限，难以挑选多样化且主题一致的文学文本集，需要工具帮助新手教育工作者选书。

Method: 使用KNARM构建特定领域本体，转化为知识图谱，用DeepWalk、有偏随机游走和两者混合的方法嵌入，用链接预测和推荐性能指标评估系统。

Result: DeepWalk在大多数排名指标中表现最佳，AUC最高为0.9431，混合模型表现平衡。

Conclusion: 语义、本体驱动的方法对推荐系统很重要，T - TExTS可显著减轻高中教育工作者选择英语文学文本的负担，促进更明智和包容的课程决策。

Abstract: The implementation of transformational pedagogy in secondary education
classrooms requires a broad multiliteracy approach. Due to limited planning
time and resources, high school English Literature teachers often struggle to
curate diverse, thematically aligned literature text sets. This study addresses
the critical need for a tool that provides scaffolds for novice educators in
selecting literature texts that are diverse -- in terms of genre, theme,
subtheme, and author -- yet similar in context and pedagogical merits. We have
developed a recommendation system, Teaching Text Expansion for Teacher
Scaffolding (T-TExTS), that suggests high school English Literature books based
on pedagogical merits, genre, and thematic relevance using a knowledge graph.
We constructed a domain-specific ontology using the KNowledge Acquisition and
Representation Methodology (KNARM), transformed into a knowledge graph, which
was then embedded using DeepWalk, biased random walk, and a hybrid of both
approaches. The system was evaluated using link prediction and recommendation
performance metrics, including Area Under the Curve (AUC), Mean Reciprocal Rank
(MRR), Hits@K, and normalized Discounted Cumulative Gain (nDCG). DeepWalk
outperformed in most ranking metrics, with the highest AUC (0.9431), whereas
the hybrid model offered balanced performance. These findings demonstrate the
importance of semantic, ontology-driven approaches in recommendation systems
and suggest that T-TExTS can significantly ease the burden of English
Literature text selection for high school educators, promoting more informed
and inclusive curricular decisions. The source code for T-TExTS is available
at: https://github.com/koncordantlab/TTExTS

</details>


### [125] [A Gradient Meta-Learning Joint Optimization for Beamforming and Antenna Position in Pinching-Antenna Systems](https://arxiv.org/abs/2506.12583)
*Kang Zhou,Weixi Zhou,Donghong Cai,Xianfu Lei,Yanqing Xu,Zhiguo Ding,Pingzhi Fan*

Main category: cs.IR

TL;DR: 本文提出GML - JO算法优化多波导挤压天线系统，提升加权和速率性能。


<details>
  <summary>Details</summary>
Motivation: 为多波导挤压天线系统进行优化设计，最大化加权和速率。

Method: 提出基于梯度的元学习联合优化（GML - JO）算法，将原问题分解为两个子问题，用凸近似方法处理非凸约束，构建子神经网络计算子问题，以固定信道系数的子网络为局部子任务计算联合优化损失函数，更新子网络参数。

Result: GML - JO算法在100次迭代内实现5.6 bits/s/Hz的加权和速率，比传统交替优化（AO）性能提升32.7%，显著降低计算复杂度。

Conclusion: GML - JO算法对不同初始化选择具有鲁棒性，性能优于现有优化方法。

Abstract: In this paper, we consider a novel optimization design for multi-waveguide
pinching-antenna systems, aiming to maximize the weighted sum rate (WSR) by
jointly optimizing beamforming coefficients and antenna position. To handle the
formulated non-convex problem, a gradient-based meta-learning joint
optimization (GML-JO) algorithm is proposed. Specifically, the original problem
is initially decomposed into two sub-problems of beamforming optimization and
antenna position optimization through equivalent substitution. Then, the convex
approximation methods are used to deal with the nonconvex constraints of
sub-problems, and two sub-neural networks are constructed to calculate the
sub-problems separately. Different from alternating optimization (AO), where
two sub-problems are solved alternately and the solutions are influenced by the
initial values, two sub-neural networks of proposed GML-JO with fixed channel
coefficients are considered as local sub-tasks and the computation results are
used to calculate the loss function of joint optimization. Finally, the
parameters of sub-networks are updated using the average loss function over
different sub-tasks and the solution that is robust to the initial value is
obtained. Simulation results demonstrate that the proposed GML-JO algorithm
achieves 5.6 bits/s/Hz WSR within 100 iterations, yielding a 32.7\% performance
enhancement over conventional AO with substantially reduced computational
complexity. Moreover, the proposed GML-JO algorithm is robust to different
choices of initialization and yields better performance compared with the
existing optimization methods.

</details>


### [126] [INTERPOS: Interaction Rhythm Guided Positional Morphing for Mobile App Recommender Systems](https://arxiv.org/abs/2506.12661)
*M. H. Maqbool,Moghis Fereidouni,Umar Farooq,A. B. Siddique,Hassan Foroosh*

Main category: cs.IR

TL;DR: 现有移动应用推荐研究有限，传统序贯推荐系统未考虑用户节奏，本文提出INTERPOS策略，实验表明其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 移动应用市场增长迅速，但推荐研究有限，传统序贯推荐系统未考虑用户节奏，在移动应用推荐中面临挑战。

Method: 引入INTERPOS策略，结合节奏引导的位置嵌入，提出三种策略将变形位置嵌入融入基于Transformer的序贯推荐系统架构。

Result: 在7个移动应用推荐数据集上，INTERPOS在NDCG@K和HIT@K指标上优于现有模型。

Conclusion: INTERPOS策略有效，能更好地理解用户节奏，捕捉用户交互模式的复杂性。

Abstract: The mobile app market has expanded exponentially, offering millions of apps
with diverse functionalities, yet research in mobile app recommendation remains
limited. Traditional sequential recommender systems utilize the order of items
in users' historical interactions to predict the next item for the users.
Position embeddings, well-established in transformer-based architectures for
natural language processing tasks, effectively distinguish token positions in
sequences. In sequential recommendation systems, position embeddings can
capture the order of items in a user's historical interaction sequence.
Nevertheless, this ordering does not consider the time elapsed between two
interactions of the same user (e.g., 1 day, 1 week, 1 month), referred to as
"user rhythm". In mobile app recommendation datasets, the time between
consecutive user interactions is notably longer compared to other domains like
movies, posing significant challenges for sequential recommender systems. To
address this phenomenon in the mobile app domain, we introduce INTERPOS, an
Interaction Rhythm Guided Positional Morphing strategy for autoregressive
mobile app recommender systems. INTERPOS incorporates rhythm-guided position
embeddings, providing a more comprehensive representation that considers both
the sequential order of interactions and the temporal gaps between them. This
approach enables a deep understanding of users' rhythms at a fine-grained
level, capturing the intricacies of their interaction patterns over time. We
propose three strategies to incorporate the morphed positional embeddings in
two transformer-based sequential recommendation system architectures. Our
extensive evaluations show that INTERPOS outperforms state-of-the-art models
using 7 mobile app recommendation datasets on NDCG@K and HIT@K metrics. The
source code of INTERPOS is available at https://github.com/dlgrad/INTERPOS.

</details>


### [127] [Device-Cloud Collaborative Correction for On-Device Recommendation](https://arxiv.org/abs/2506.12687)
*Tianyu Zhan,Shengyu Zhang,Zheqi Lv,Jieming Zhu,Jiwei Li,Fan Wu,Fei Wu*

Main category: cs.IR

TL;DR: 为平衡设备上实时性与高性能，提出CoCorrRec框架，实验表明其优于现有模型，实现实时性与高效性平衡。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在交互序列变长时带来更多空间和计算开销，给设备推荐带来挑战，需平衡设备上实时性能与高性能。

Method: 提出CoCorrRec框架，用自校正网络（SCN）以极低时间成本校正参数，设计全局校正网络（GCN）防止SCN过拟合。

Result: 在多个数据集上的实验显示，CoCorrRec在性能上优于现有Transformer和RNN设备推荐模型，参数更少，FLOPs更低。

Conclusion: CoCorrRec能实现设备推荐中实时性能与高效性的平衡。

Abstract: With the rapid development of recommendation models and device computing
power, device-based recommendation has become an important research area due to
its better real-time performance and privacy protection. Previously,
Transformer-based sequential recommendation models have been widely applied in
this field because they outperform Recurrent Neural Network (RNN)-based
recommendation models in terms of performance. However, as the length of
interaction sequences increases, Transformer-based models introduce
significantly more space and computational overhead compared to RNN-based
models, posing challenges for device-based recommendation. To balance real-time
performance and high performance on devices, we propose Device-Cloud
\underline{Co}llaborative \underline{Corr}ection Framework for On-Device
\underline{Rec}ommendation (CoCorrRec). CoCorrRec uses a self-correction
network (SCN) to correct parameters with extremely low time cost. By updating
model parameters during testing based on the input token, it achieves
performance comparable to current optimal but more complex Transformer-based
models. Furthermore, to prevent SCN from overfitting, we design a global
correction network (GCN) that processes hidden states uploaded from devices and
provides a global correction solution. Extensive experiments on multiple
datasets show that CoCorrRec outperforms existing Transformer-based and
RNN-based device recommendation models in terms of performance, with fewer
parameters and lower FLOPs, thereby achieving a balance between real-time
performance and high efficiency.

</details>


### [128] [Hierarchical Group-wise Ranking Framework for Recommendation Models](https://arxiv.org/abs/2506.12756)
*YaChen Yan,Liubo Li,Ravi Choudhary*

Main category: cs.IR

TL;DR: 提出分层分组排序框架，解决现有CTR/CVR模型因批量负采样问题，提升排序性能。


<details>
  <summary>Details</summary>
Motivation: 现有CTR/CVR模型使用批量负采样，多为简单负样本，限制模型捕捉用户偏好和排序性能。

Method: 提出分层分组排序框架，对用户嵌入应用残差向量量化生成用户码，将用户分层聚类；在各层对用户-物品对应用列表排序损失。

Result: 广泛实验表明，该框架能持续提升模型校准和排序准确性。

Conclusion: 该框架是工业推荐系统可扩展且实用的解决方案，无需复杂实时上下文收集或检索基础设施。

Abstract: In modern recommender systems, CTR/CVR models are increasingly trained with
ranking objectives to improve item ranking quality. While this shift aligns
training more closely with serving goals, most existing methods rely on
in-batch negative sampling, which predominantly surfaces easy negatives. This
limits the model's ability to capture fine-grained user preferences and weakens
overall ranking performance. To address this, we propose a Hierarchical
Group-wise Ranking Framework with two key components. First, we apply residual
vector quantization to user embeddings to generate hierarchical user codes that
partition users into hierarchical, trie-structured clusters. Second, we apply
listwise ranking losses to user-item pairs at each level of the hierarchy,
where shallow levels group loosely similar users and deeper levels group highly
similar users, reinforcing learning-to-rank signals through progressively
harder negatives. Since users with similar preferences and content exposure
tend to yield more informative negatives, applying ranking losses within these
hierarchical user groups serves as an effective approximation of hard negative
mining. Our approach improves ranking performance without requiring complex
real-time context collection or retrieval infrastructure. Extensive experiments
demonstrate that the proposed framework consistently enhances both model
calibration and ranking accuracy, offering a scalable and practical solution
for industrial recommender systems.

</details>


### [129] [Identifying and Investigating Global News Coverage of Critical Events Such as Disasters and Terrorist Attacks](https://arxiv.org/abs/2506.12925)
*Erica Cai,Xi Chen,Reagan Grey Keeney,Ethan Zuckerman,Brendan O'Connor,Przemyslaw A. Grabowicz*

Main category: cs.IR

TL;DR: 提出AI驱动的FAME方法识别新闻文章，性能优可扩展，用其分析2020年灾害与恐袭事件新闻，揭示报道模式并共享数据。


<details>
  <summary>Details</summary>
Motivation: 解决不同语言中识别同一事件新闻文章方法难扩展的问题。

Method: 引入基于事件指纹的FAME方法，无需训练数据，根据事件指纹识别新闻文章。

Result: FAME达到先进性能，可扩展到大规模数据库；识别27,441篇文章覆盖470个事件；揭示报道与死亡人数、发生国GDP、报道国与发生国贸易量相关。

Conclusion: 共享NLP注释和跨国媒体关注数据，支持研究人员和媒体监测组织。

Abstract: Comparative studies of news coverage are challenging to conduct because
methods to identify news articles about the same event in different languages
require expertise that is difficult to scale. We introduce an AI-powered method
for identifying news articles based on an event FINGERPRINT, which is a minimal
set of metadata required to identify critical events. Our event coverage
identification method, FINGERPRINT TO ARTICLE MATCHING FOR EVENTS (FAME),
efficiently identifies news articles about critical world events, specifically
terrorist attacks and several types of natural disasters. FAME does not require
training data and is able to automatically and efficiently identify news
articles that discuss an event given its fingerprint: time, location, and class
(such as storm or flood). The method achieves state-of-the-art performance and
scales to massive databases of tens of millions of news articles and hundreds
of events happening globally. We use FAME to identify 27,441 articles that
cover 470 natural disaster and terrorist attack events that happened in 2020.
To this end, we use a massive database of news articles in three languages from
MediaCloud, and three widely used, expert-curated databases of critical events:
EM-DAT, USGS, and GTD. Our case study reveals patterns consistent with prior
literature: coverage of disasters and terrorist attacks correlates to death
counts, to the GDP of a country where the event occurs, and to trade volume
between the reporting country and the country where the event occurred. We
share our NLP annotations and cross-country media attention data to support the
efforts of researchers and media monitoring organizations.

</details>


### [130] [SPOT: Bridging Natural Language and Geospatial Search for Investigative Journalists](https://arxiv.org/abs/2506.13188)
*Lynn Khellaf,Ipek Baris Schlicht,Tilman Mirass,Julia Bayer,Tilman Wagner,Ruben Bouwmeester*

Main category: cs.IR

TL;DR: 提出开源自然语言接口SPOT，让非技术用户更易访问OSM地理数据，助力调查性新闻地理定位验证。


<details>
  <summary>Details</summary>
Motivation: 现有查询OSM数据工具需熟悉复杂查询语言，给非技术用户带来障碍，要为调查性新闻提供便利工具。

Method: 用微调大语言模型将用户输入解释为地理空间对象配置的结构化表示，结合合成数据管道和语义捆绑系统生成查询。

Result: SPOT能实现对OSM数据可靠的自然语言访问，在地图界面展示结果。

Conclusion: SPOT降低地理定位验证技术门槛，为事实核查和打击虚假信息提供实用工具。

Abstract: OpenStreetMap (OSM) is a vital resource for investigative journalists doing
geolocation verification. However, existing tools to query OSM data such as
Overpass Turbo require familiarity with complex query languages, creating
barriers for non-technical users. We present SPOT, an open source natural
language interface that makes OSM's rich, tag-based geographic data more
accessible through intuitive scene descriptions. SPOT interprets user inputs as
structured representations of geospatial object configurations using fine-tuned
Large Language Models (LLMs), with results being displayed in an interactive
map interface. While more general geospatial search tasks are conceivable, SPOT
is specifically designed for use in investigative journalism, addressing
real-world challenges such as hallucinations in model output, inconsistencies
in OSM tagging, and the noisy nature of user input. It combines a novel
synthetic data pipeline with a semantic bundling system to enable robust,
accurate query generation. To our knowledge, SPOT is the first system to
achieve reliable natural language access to OSM data at this level of accuracy.
By lowering the technical barrier to geolocation verification, SPOT contributes
a practical tool to the broader efforts to support fact-checking and combat
disinformation.

</details>


### [131] [Gated Rotary-Enhanced Linear Attention for Long-term Sequential Recommendation](https://arxiv.org/abs/2506.13315)
*Juntao Hu,Wei Zhou,Huayi Shen,Xiao Du,Jie Liao,Junhao Wen,Min Gao*

Main category: cs.IR

TL;DR: 论文提出RecGRELA模型解决SRSs中线性注意力方法的局限，在四个公开数据集上表现优异且内存开销低。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在SRSs中计算成本高，现有线性注意力方法有使用可学习位置编码增加计算成本、未考虑细粒度局部偏好等局限。

Method: 提出RELA模块用旋转位置编码建模长距离依赖，引入局部短操作纳入局部偏好，为RELA引入基于SiLU的门控机制GRELA。

Result: 在四个公开数据集上实验，RecGRELA与现有SRSs相比达到了最优性能，且内存开销低。

Conclusion: RecGRELA能有效解决现有线性注意力方法的局限，在SRSs中表现出色。

Abstract: In Sequential Recommendation Systems (SRSs), Transformer models show
remarkable performance but face computation cost challenges when modeling
long-term user behavior sequences due to the quadratic complexity of the
dot-product attention mechanism. By approximating the dot-product attention,
linear attention provides an efficient option with linear complexity. However,
existing linear attention methods face two limitations: 1) they often use
learnable position encodings, which incur extra computational costs in
long-term sequence scenarios, and 2) they may not consider the user's
fine-grained local preferences and confuse these with the actual change of
long-term interests. To remedy these drawbacks, we propose a long-term
sequential Recommendation model with Gated Rotary Enhanced Linear Attention
(RecGRELA). Specifically, we first propose a Rotary-Enhanced Linear Attention
(RELA) module to model long-range dependency within the user's historical
information using rotary position encodings. We then introduce a local short
operation to incorporate local preferences and demonstrate the theoretical
insight. We further introduce a SiLU-based Gated mechanism for RELA (GRELA) to
help the model determine whether a user's behavior indicates local interest or
a genuine shift in long-term preferences. Experimental results on four public
datasets demonstrate that our RecGRELA achieves state-of-the-art performance
compared to existing SRSs while maintaining low memory overhead.

</details>


### [132] [Beyond One-Size-Fits-All: A Study of Neural and Behavioural Variability Across Different Recommendation Categories](https://arxiv.org/abs/2506.13409)
*Georgios Koutroumpas,Sebastian Idesis,Mireia Masias Bruns,Carlos Segura,Joemon M. Jose,Sergi Abadal,Ioannis Arapakis*

Main category: cs.IR

TL;DR: 传统推荐系统重算法准确性，本文关注用户，研究不同推荐类别下用户神经和行为差异。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统以算法为中心，忽略推荐对用户参与度和体验质量的影响，需从用户角度研究不同推荐类别的影响。

Method: 使用电商综合数据集开展对照研究，收集脑电图和行为数据，分析不同类别推荐的神经和行为反应。

Result: 发现不同类别推荐下行为和神经模式的有意义关系，也存在个体差异。

Conclusion: 研究为理解用户偏好和决策过程提供新见解。

Abstract: Traditionally, Recommender Systems (RS) have primarily measured performance
based on the accuracy and relevance of their recommendations. However, this
algorithmic-centric approach overlooks how different types of recommendations
impact user engagement and shape the overall quality of experience. In this
paper, we shift the focus to the user and address for the first time the
challenge of decoding the neural and behavioural variability across distinct
recommendation categories, considering more than just relevance. Specifically,
we conducted a controlled study using a comprehensive e-commerce dataset
containing various recommendation types, and collected Electroencephalography
and behavioural data. We analysed both neural and behavioural responses to
recommendations that were categorised as Exact, Substitute, Complement, or
Irrelevant products within search query results. Our findings offer novel
insights into user preferences and decision-making processes, revealing
meaningful relationships between behavioural and neural patterns for each
category, but also indicate inter-subject variability.

</details>


### [133] [Tree-Based Text Retrieval via Hierarchical Clustering in RAGFrameworks: Application on Taiwanese Regulations](https://arxiv.org/abs/2506.13607)
*Chia-Heng Yu,Yen-Lung Tsai*

Main category: cs.IR

TL;DR: 传统RAG系统选k值有挑战，提出基于层次聚类检索法，实验表明在法律文本检索中性能优，易集成到现有管道。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统选择合适的k值在实际应用中是重大挑战，小k可能信息不足，大k会引入无关内容。

Method: 提出基于层次聚类的检索方法，无需预定义k值，能自适应选择语义相关内容。

Result: 将方法应用于台湾法律数据集，在专家评估中表现优越，保持高精度且无需预定义k值。

Conclusion: 该框架实现简单，易与现有RAG管道集成，是资源有限的实际应用的实用解决方案。

Abstract: Traditional Retrieval-Augmented Generation (RAG) systems employ brute-force
inner product search to retrieve the top-k most similar documents, then
combined with the user query and passed to a language model. This allows the
model to access external knowledge and reduce hallucinations. However,
selecting an appropriate k value remains a significant challenge in practical
applications: a small k may fail to retrieve sufficient information, while a
large k can introduce excessive and irrelevant content. To address this, we
propose a hierarchical clustering-based retrieval method that eliminates the
need to predefine k. Our approach maintains the accuracy and relevance of
system responses while adaptively selecting semantically relevant content. In
the experiment stage, we applied our method to a Taiwanese legal dataset with
expert-graded queries. The results show that our approach achieves superior
performance in expert evaluations and maintains high precision while
eliminating the need to predefine k, demonstrating improved accuracy and
interpretability in legal text retrieval tasks. Our framework is simple to
implement and easily integrates with existing RAG pipelines, making it a
practical solution for real-world applications under limited resources.

</details>


### [134] [OneRec Technical Report](https://arxiv.org/abs/2506.13695)
*Guorui Zhou,Jiaxin Deng,Jinghao Zhang,Kuo Cai,Lejian Ren,Qiang Luo,Qianqian Wang,Qigen Hu,Rui Huang,Shiyao Wang,Weifeng Ding,Wuchao Li,Xinchen Luo,Xingmei Wang,Zexuan Cheng,Zixing Zhang,Bin Zhang,Boxuan Wang,Chaoyi Ma,Chengru Song,Chenhui Wang,Di Wang,Dongxue Meng,Fan Yang,Fangyu Zhang,Feng Jiang,Fuxing Zhang,Gang Wang,Guowang Zhang,Han Li,Hengrui Hu,Hezheng Lin,Hongtao Cheng,Hongyang Cao,Huanjie Wang,Jiaming Huang,Jiapeng Chen,Jiaqiang Liu,Jinghui Jia,Kun Gai,Lantao Hu,Liang Zeng,Liao Yu,Qiang Wang,Qidong Zhou,Shengzhe Wang,Shihui He,Shuang Yang,Shujie Yang,Sui Huang,Tao Wu,Tiantian He,Tingting Gao,Wei Yuan,Xiao Liang,Xiaoxiao Xu,Xugang Liu,Yan Wang,Yi Wang,Yiwu Liu,Yue Song,Yufei Zhang,Yunfan Wu,Yunfeng Zhao,Zhanyu Liu*

Main category: cs.IR

TL;DR: 提出OneRec以端到端生成方法重塑推荐系统，取得提升计算FLOPs、展示强化学习潜力等成果，并在快手应用中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统未突破，依赖多级级联架构，存在计算碎片化和优化不一致问题，阻碍AI关键技术应用。

Method: 提出OneRec，采用端到端生成方法，进行基础设施优化。

Result: 增强模型计算FLOPs 10倍，发现推荐缩放定律；强化学习在框架中有潜力；训练和推理时MFU分别达23.7%和28.8%；运营成本仅为传统10.6%；在快手应用提升指标。

Conclusion: OneRec架构有效，还给出生产级推荐系统开发、优化和维护的实用经验。

Abstract: Recommender systems have been widely used in various large-scale
user-oriented platforms for many years. However, compared to the rapid
developments in the AI community, recommendation systems have not achieved a
breakthrough in recent years. For instance, they still rely on a multi-stage
cascaded architecture rather than an end-to-end approach, leading to
computational fragmentation and optimization inconsistencies, and hindering the
effective application of key breakthrough technologies from the AI community in
recommendation scenarios.
  To address these issues, we propose OneRec, which reshapes the recommendation
system through an end-to-end generative approach and achieves promising
results. Firstly, we have enhanced the computational FLOPs of the current
recommendation model by 10 $\times$ and have identified the scaling laws for
recommendations within certain boundaries. Secondly, reinforcement learning
techniques, previously difficult to apply for optimizing recommendations, show
significant potential in this framework. Lastly, through infrastructure
optimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU)
on flagship GPUs during training and inference, respectively, aligning closely
with the LLM community. This architecture significantly reduces communication
and storage overhead, resulting in operating expense that is only 10.6% of
traditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP,
it handles 25% of total queries per second, enhancing overall App Stay Time by
0.54% and 1.24%, respectively. Additionally, we have observed significant
increases in metrics such as 7-day Lifetime, which is a crucial indicator of
recommendation experience. We also provide practical lessons and insights
derived from developing, optimizing, and maintaining a production-scale
recommendation system with significant real-world impact.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [135] [FlexQuant: A Flexible and Efficient Dynamic Precision Switching Framework for LLM Quantization](https://arxiv.org/abs/2506.12024)
*Fangxin Liu,Zongwu Wang,JinHong Xia,Junping Zhao,Jian Liu,Haibing Guan,Li Jiang*

Main category: cs.LG

TL;DR: 现有大语言模型训练量化技术难以适应动态工作负载，本文提出FlexQuant框架，实现细粒度层混合精度量化，实验显示能在不同语言任务中实现1.3倍端到端加速且精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展使内存瓶颈加剧，现有后训练量化技术多为静态策略，难以适应动态工作负载。

Method: 提出FlexQuant动态精度切换框架，利用模型困惑度熵和KL散度，实现细粒度层混合精度量化，并在每个令牌生成时动态调整位宽，还对量化策略进行综合分析，引入精度需求模型，实现高效细粒度精度管理。

Result: FlexQuant在不同语言任务中实现1.3倍端到端加速，精度损失可忽略不计。

Conclusion: FlexQuant框架为高效部署大语言模型提供了灵活、自适应的解决方案。

Abstract: The rapid advancement of large language models (LLMs) has exacerbated the
memory bottleneck due to the widening gap between model parameter scaling and
hardware capabilities. While post-training quantization (PTQ) techniques
effectively reduce memory overhead, existing methods predominantly rely on
static quantization strategies, which struggle to adapt to dynamic workloads.
To address this, we propose FlexQuant, a dynamic precision-switching framework
that optimizes the trade-off between inference speed and accuracy. Leveraging
model perplexity entropy and Kullback-Leibler (KL) divergence, FlexQuant
enables fine-grained, layer-wise mixed-precision quantization and dynamically
adjusts bit-widths during each token generation. Our work provides a
comprehensive analysis of quantization strategies, introduces a precision
requirement model for optimal switching, and implements efficient fine-grained
precision management. Experimental results demonstrate that FlexQuant achieves
a 1.3x end-to-end speedup across diverse language tasks with negligible
accuracy loss introduced. This framework offers a flexible and adaptive
solution for efficient LLM deployment.

</details>


### [136] [A Review of the Long Horizon Forecasting Problem in Time Series Analysis](https://arxiv.org/abs/2506.12809)
*Hans Krupakar,Kandappan V A*

Main category: cs.LG

TL;DR: 该综述涵盖近35年时间序列长程预测（LHF）问题，介绍深度学习在LHF中的应用，描述多种改进模型，通过ETTm2数据集进行消融研究，指出部分模型在误差控制上的优势并公开模型。


<details>
  <summary>Details</summary>
Motivation: 回顾近35年时间序列长程预测问题，研究深度学习方法如何改进LHF性能。

Method: 介绍深度学习在LHF中采用的多种技术，描述改进LHF性能的模型，在ETTm2数据集的多变量和单变量高有用负载预测场景下进行消融研究。

Result: 测试集系列中MSE平均值热图显示，除xLSTM和Triformer模型外，误差随预测长度增加而稳定上升。

Conclusion: LHF可视为误差传播问题，xLSTM和Triformer模型在控制误差方面表现较好。

Abstract: The long horizon forecasting (LHF) problem has come up in the time series
literature for over the last 35 years or so. This review covers aspects of LHF
in this period and how deep learning has incorporated variants of trend,
seasonality, fourier and wavelet transforms, misspecification bias reduction
and bandpass filters while contributing using convolutions, residual
connections, sparsity reduction, strided convolutions, attention masks, SSMs,
normalization methods, low-rank approximations and gating mechanisms. We
highlight time series decomposition techniques, input data preprocessing and
dataset windowing schemes that improve performance. Multi-layer perceptron
models, recurrent neural network hybrids, self-attention models that improve
and/or address the performances of the LHF problem are described, with an
emphasis on the feature space construction. Ablation studies are conducted over
the ETTm2 dataset in the multivariate and univariate high useful load (HUFL)
forecasting contexts, evaluated over the last 4 months of the dataset. The
heatmaps of MSE averages per time step over test set series in the horizon show
that there is a steady increase in the error proportionate to its length except
with xLSTM and Triformer models and motivate LHF as an error propagation
problem. The trained models are available here: https://bit.ly/LHFModelZoo

</details>


### [137] [Unsupervised Learning for Optimal Transport plan prediction between unbalanced graphs](https://arxiv.org/abs/2506.12025)
*Sonia Mazelet,Rémi Flamary,Bertrand Thirion*

Main category: cs.LG

TL;DR: 提出深度学习方法ULOT预测图间最优传输计划，在合成图和真实脑皮层数据上评估，速度比经典求解器快，预测计划可热启动加速收敛且可微。


<details>
  <summary>Details</summary>
Motivation: 基于Gromov - Wasserstein的图最优传输求解非凸优化问题计算成本高，限制方法对大图的可扩展性。

Method: 提出ULOT方法，通过最小化融合不平衡Gromov - Wasserstein (FUGW)损失训练，采用带交叉注意力的新型神经架构。

Result: ULOT预测传输计划的损失有竞争力，速度比经典求解器快两个数量级，预测计划可作为经典求解器的热启动加速收敛，预测传输计划对图输入和FUGW超参数可微。

Conclusion: ULOT方法在图最优传输问题上有较好表现，能提高计算效率并具备可微性用于功能优化。

Abstract: Optimal transport between graphs, based on Gromov-Wasserstein and
  other extensions, is a powerful tool for comparing and aligning
  graph structures. However, solving the associated non-convex
  optimization problems is computationally expensive, which limits the
  scalability of these methods to large graphs. In this work, we
  present Unbalanced Learning of Optimal Transport (ULOT), a deep
  learning method that predicts optimal transport plans between two
  graphs. Our method is trained by minimizing the fused unbalanced
  Gromov-Wasserstein (FUGW) loss. We propose a novel neural
  architecture with cross-attention that is conditioned on the FUGW
  tradeoff hyperparameters. We evaluate ULOT on synthetic stochastic
  block model (SBM) graphs and on real cortical surface data obtained
  from fMRI. ULOT predicts transport plans with competitive loss up to
  two orders of magnitude faster than classical solvers. Furthermore,
  the predicted plan can be used as a warm start for classical solvers
  to accelerate their convergence. Finally, the predicted transport
  plan is fully differentiable with respect to the graph inputs and
  FUGW hyperparameters, enabling the optimization of functionals of
  the ULOT plan.

</details>


### [138] [Embedding Trust at Scale: Physics-Aware Neural Watermarking for Secure and Verifiable Data Pipelines](https://arxiv.org/abs/2506.12032)
*Krti Tallam*

Main category: cs.LG

TL;DR: 提出用于科学数据完整性的鲁棒神经水印框架，用卷积自动编码器嵌入水印，在有损变换下表现好，优于传统方法，适用于多领域。


<details>
  <summary>Details</summary>
Motivation: 解决气候建模和流体模拟等高维领域科学数据完整性问题，提供数据来源、可审计性和可追溯性工具。

Method: 使用卷积自动编码器将二进制消息嵌入结构化数据，确保水印在有损变换下的持久性。

Result: 相比传统SVD水印方法，实现>98%的比特准确率，重建结果视觉上难以区分，MSE低于1%。

Conclusion: 该系统是高性能科学工作流中数据处理的可扩展、兼容模型的工具，可扩展到其他结构化领域。

Abstract: We present a robust neural watermarking framework for scientific data
integrity, targeting high-dimensional fields common in climate modeling and
fluid simulations. Using a convolutional autoencoder, binary messages are
invisibly embedded into structured data such as temperature, vorticity, and
geopotential. Our method ensures watermark persistence under lossy
transformations - including noise injection, cropping, and compression - while
maintaining near-original fidelity (sub-1\% MSE). Compared to classical
singular value decomposition (SVD)-based watermarking, our approach achieves
$>$98\% bit accuracy and visually indistinguishable reconstructions across ERA5
and Navier-Stokes datasets. This system offers a scalable, model-compatible
tool for data provenance, auditability, and traceability in high-performance
scientific workflows, and contributes to the broader goal of securing AI
systems through verifiable, physics-aware watermarking. We evaluate on
physically grounded scientific datasets as a representative stress-test; the
framework extends naturally to other structured domains such as satellite
imagery and autonomous-vehicle perception streams.

</details>


### [139] [Physics-Informed Neural Networks for Vessel Trajectory Prediction: Learning Time-Discretized Kinematic Dynamics via Finite Differences](https://arxiv.org/abs/2506.12029)
*Md Mahbub Alam,Amilcar Soares,José F. Rodrigues-Jr,Gabriel Spadon*

Main category: cs.LG

TL;DR: 提出物理信息神经网络（PINN）用于船舶轨迹预测，可减少误差并保持物理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动模型缺乏物理约束，在有限或有噪声数据场景中预测违背船舶运动动力学，需改进。

Method: 将简化的船舶运动运动学模型通过一阶和二阶有限差分物理损失函数集成到神经网络训练中，用欧拉法、Heun 法和泰勒级数中点近似法离散化损失函数。

Result: 在真实 AIS 数据集上评估，相比现有模型，平均位移误差最多降低 32%，且保持物理一致性。

Conclusion: 该方法增强了模型可靠性，有助于关键海事活动，提高海洋态势感知能力。

Abstract: Accurate vessel trajectory prediction is crucial for navigational safety,
route optimization, traffic management, search and rescue operations, and
autonomous navigation. Traditional data-driven models lack real-world physical
constraints, leading to forecasts that disobey vessel motion dynamics, such as
in scenarios with limited or noisy data where sudden course changes or speed
variations occur due to external factors. To address this limitation, we
propose a Physics-Informed Neural Network (PINN) approach for trajectory
prediction that integrates a streamlined kinematic model for vessel motion into
the neural network training process via a first- and second-order, finite
difference physics-based loss function. This loss function, discretized using
the first-order forward Euler method, Heun's second-order approximation, and
refined with a midpoint approximation based on Taylor series expansion,
enforces fidelity to fundamental physical principles by penalizing deviations
from expected kinematic behavior. We evaluated PINN using real-world AIS
datasets that cover diverse maritime conditions and compared it with
state-of-the-art models. Our results demonstrate that the proposed method
reduces average displacement errors by up to 32% across models and datasets
while maintaining physical consistency. These results enhance model reliability
and adherence to mission-critical maritime activities, where precision
translates into better situational awareness in the oceans.

</details>


### [140] [GUST: Quantifying Free-Form Geometric Uncertainty of Metamaterials Using Small Data](https://arxiv.org/abs/2506.12051)
*Jiahui Zheng,Cole Jahnke,Wei "Wayne" Chen*

Main category: cs.LG

TL;DR: 介绍GUST框架用于量化超材料制造中的自由形式几何不确定性，采用两阶段学习，减少数据需求且有效。


<details>
  <summary>Details</summary>
Motivation: 解决超材料制造中自由形式几何不确定性量化问题，以及真实制造数据稀缺问题。

Method: GUST框架采用两阶段学习，先在大规模合成数据集上进行自监督预训练，再在有限真实制造数据上进行迁移学习。

Result: 仅用960个单元且两次制造，GUST就能捕捉几何和有效材料属性的变异性，而直接用相同量真实数据训练生成模型不足。

Conclusion: GUST是可扩展且经济有效的方法，能减少数据需求并有效学习复杂几何不确定性，对高精度行业有重要意义。

Abstract: This paper introduces GUST (Generative Uncertainty learning via
Self-supervised pretraining and Transfer learning), a framework for quantifying
free-form geometric uncertainties inherent in the manufacturing of
metamaterials. GUST leverages the representational power of deep generative
models to learn a high-dimensional conditional distribution of as-fabricated
unit cell geometries given nominal designs, thereby enabling uncertainty
quantification. To address the scarcity of real-world manufacturing data, GUST
employs a two-stage learning process. First, it leverages self-supervised
pretraining on a large-scale synthetic dataset to capture the structure
variability inherent in metamaterial geometries and an approximated
distribution of as-fabricated geometries given nominal designs. Subsequently,
GUST employs transfer learning by fine-tuning the pretrained model on limited
real-world manufacturing data, allowing it to adapt to specific manufacturing
processes and nominal designs. With only 960 unit cells additively manufactured
in only two passes, GUST can capture the variability in geometry and effective
material properties. In contrast, directly training a generative model on the
same amount of real-world data proves insufficient, as demonstrated through
both qualitative and quantitative comparisons. This scalable and cost-effective
approach significantly reduces data requirements while maintaining the
effectiveness in learning complex, real-world geometric uncertainties, offering
an affordable method for free-form geometric uncertainty quantification in the
manufacturing of metamaterials. The capabilities of GUST hold significant
promise for high-precision industries such as aerospace and biomedical
engineering, where understanding and mitigating manufacturing uncertainties are
critical.

</details>


### [141] [Impact, Causation and Prediction of Socio-Academic and Economic Factors in Exam-centric Student Evaluation Measures using Machine Learning and Causal Analysis](https://arxiv.org/abs/2506.12030)
*Md. Biplob Hosen,Sabbir Ahmed,Bushra Akter,Mehrin Anannya*

Main category: cs.LG

TL;DR: 研究用机器学习和因果分析预测社会学术与经济因素对学生成绩影响，构建因果图、分析数据，回归和分类模型表现好，发现因素对CGPA有显著影响，还将最佳模型集成到网页应用。


<details>
  <summary>Details</summary>
Motivation: 理解影响学生成绩的社会学术和经济因素，以便进行有效的教育干预。

Method: 采用多种机器学习技术和因果分析，构建假设因果图，收集1050份学生资料，进行数据清洗和可视化，分析线性关系，应用回归和分类模型预测，进行无监督因果分析。

Result: 回归分析中Ridge Regression的MAE为0.12，MSE为0.024；分类模型如Random Forest的F1分数接近完美；因果分析显示班级出勤、学习时长和小组学习等因素对CGPA有显著直接和间接影响。

Conclusion: 通过集成最佳回归模型到网页应用，为学生和教育工作者开发基于实证的实用工具以提高学术成果。

Abstract: Understanding socio-academic and economic factors influencing students'
performance is crucial for effective educational interventions. This study
employs several machine learning techniques and causal analysis to predict and
elucidate the impacts of these factors on academic performance. We constructed
a hypothetical causal graph and collected data from 1,050 student profiles.
Following meticulous data cleaning and visualization, we analyze linear
relationships through correlation and variable plots, and perform causal
analysis on the hypothetical graph. Regression and classification models are
applied for prediction, and unsupervised causality analysis using PC, GES,
ICA-LiNGAM, and GRASP algorithms is conducted. Our regression analysis shows
that Ridge Regression achieve a Mean Absolute Error (MAE) of 0.12 and a Mean
Squared Error (MSE) of 0.024, indicating robustness, while classification
models like Random Forest achieve nearly perfect F1-scores. The causal analysis
shows significant direct and indirect effects of factors such as class
attendance, study hours, and group study on CGPA. These insights are validated
through unsupervised causality analysis. By integrating the best regression
model into a web application, we are developing a practical tool for students
and educators to enhance academic outcomes based on empirical evidence.

</details>


### [142] [Energy-Efficient Green AI Architectures for Circular Economies Through Multi-Layered Sustainable Resource Optimization Framework](https://arxiv.org/abs/2506.12262)
*Ripal Ranpara*

Main category: cs.LG

TL;DR: 提出节能绿色AI架构支持循环经济，测试表明可降低能耗、提升资源回收效率等，为可持续管理提供科学方案。


<details>
  <summary>Details</summary>
Motivation: 应对现代系统可持续资源消耗挑战，支持循环经济。

Method: 引入多层框架和元架构，集成机器学习算法、能源感知计算模型和优化技术，用数学模型进行定量优化。

Result: 工作流能耗降低25%，资源回收效率提升18%，城市垃圾分类准确率提高20%，运输排放降低30%。

Conclusion: 该架构结合绿色AI原则，为循环经济提供可扩展科学方案，可将新技术纳入可持续管理策略。

Abstract: In this research paper, we propose a new type of energy-efficient Green AI
architecture to support circular economies and address the contemporary
challenge of sustainable resource consumption in modern systems. We introduce a
multi-layered framework and meta-architecture that integrates state-of-the-art
machine learning algorithms, energy-conscious computational models, and
optimization techniques to facilitate decision-making for resource reuse, waste
reduction, and sustainable production.We tested the framework on real-world
datasets from lithium-ion battery recycling and urban waste management systems,
demonstrating its practical applicability. Notably, the key findings of this
study indicate a 25 percent reduction in energy consumption during workflows
compared to traditional methods and an 18 percent improvement in resource
recovery efficiency. Quantitative optimization was based on mathematical models
such as mixed-integer linear programming and lifecycle assessments. Moreover,
AI algorithms improved classification accuracy on urban waste by 20 percent,
while optimized logistics reduced transportation emissions by 30 percent. We
present graphical analyses and visualizations of the developed framework,
illustrating its impact on energy efficiency and sustainability as reflected in
the simulation results. This paper combines the principles of Green AI with
practical insights into how such architectural models contribute to circular
economies, presenting a fully scalable and scientifically rooted solution
aligned with applicable UN Sustainability Goals worldwide. These results open
avenues for incorporating newly developed AI technologies into sustainable
management strategies, potentially safeguarding local natural capital while
advancing technological progress.

</details>


### [143] [EMERGENT: Efficient and Manipulation-resistant Matching using GFlowNets](https://arxiv.org/abs/2506.12033)
*Mayesha Tasnim,Erman Acar,Sennay Ghebreab*

Main category: cs.LG

TL;DR: 本文提出 EMERGENT 算法，利用生成流网络解决单边匹配问题，实验显示其在效率和抗操纵性上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有单边匹配算法无法平衡效率和防策略性，设计公平高效的公共资源分配算法有重要社会影响。

Method: 提出 EMERGENT 算法，将生成流网络应用于单边匹配，利用其采样能力使高效且抗操纵的匹配自然出现。

Result: 实验表明 EMERGENT 在排名效率上优于 RSD，相比 RM 和 PS 显著降低策略脆弱性。

Conclusion: 生成流网络在涉及社会选择机制的应用中有平衡效率和可操纵性的潜力。

Abstract: The design of fair and efficient algorithms for allocating public resources,
such as school admissions, housing, or medical residency, has a profound social
impact. In one-sided matching problems, where individuals are assigned to items
based on ranked preferences, a fundamental trade-off exists between efficiency
and strategyproofness. Existing algorithms like Random Serial Dictatorship
(RSD), Probabilistic Serial (PS), and Rank Minimization (RM) capture only one
side of this trade-off: RSD is strategyproof but inefficient, while PS and RM
are efficient but incentivize manipulation. We propose EMERGENT, a novel
application of Generative Flow Networks (GFlowNets) to one-sided matching,
leveraging its ability to sample diverse, high-reward solutions. In our
approach, efficient and manipulation-resistant matches emerge naturally:
high-reward solutions yield efficient matches, while the stochasticity of
GFlowNets-based outputs reduces incentives for manipulation. Experiments show
that EMERGENT outperforms RSD in rank efficiency while significantly reducing
strategic vulnerability compared to matches produced by RM and PS. Our work
highlights the potential of GFlowNets for applications involving social choice
mechanisms, where it is crucial to balance efficiency and manipulability.

</details>


### [144] [Improving Generalization in Heterogeneous Federated Continual Learning via Spatio-Temporal Gradient Matching with Prototypical Coreset](https://arxiv.org/abs/2506.12031)
*Minh-Duong Nguyen,Le-Tuan Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: 文章探讨客户端数据和任务无关或冲突的联邦持续学习场景，指出现有方法问题，提出STAMP方法，实验证明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有FCL方法在客户端数据和任务无关或冲突场景下，因统计异质性和数据噪声，使用生成重放会导致过拟合等问题，需新方法解决。

Method: 提出STAMP方法，包括确定样本子集形成原型、时空梯度匹配、利用原型近似任务梯度。

Result: 广泛实验表明该方法优于现有基线。

Conclusion: STAMP方法能有效应对客户端数据和任务无关或冲突的FCL场景挑战。

Abstract: Federated Continual Learning (FCL) has recently emerged as a crucial research
area, as data from distributed clients typically arrives as a stream, requiring
sequential learning. This paper explores a more practical and challenging FCL
setting, where clients may have unrelated or even conflicting data and tasks.
In this scenario, statistical heterogeneity and data noise can create spurious
correlations, leading to biased feature learning and catastrophic forgetting.
Existing FCL approaches often use generative replay to create pseudo-datasets
of previous tasks. However, generative replay itself suffers from catastrophic
forgetting and task divergence among clients, leading to overfitting in FCL.
Existing FCL approaches often use generative replay to create pseudo-datasets
of previous tasks. However, generative replay itself suffers from catastrophic
forgetting and task divergence among clients, leading to overfitting in FCL. To
address these challenges, we propose a novel approach called Spatio-Temporal
grAdient Matching with network-free Prototype (STAMP). Our contributions are
threefold: 1) We develop a model-agnostic method to determine subset of samples
that effectively form prototypes when using a prototypical network, making it
resilient to continual learning challenges; 2) We introduce a spatio-temporal
gradient matching approach, applied at both the client-side (temporal) and
server-side (spatial), to mitigate catastrophic forgetting and data
heterogeneity; 3) We leverage prototypes to approximate task-wise gradients,
improving gradient matching on the client-side. Extensive experiments
demonstrate our method's superiority over existing baselines.

</details>


### [145] [Semivalue-based data valuation is arbitrary and gameable](https://arxiv.org/abs/2506.12619)
*Hannah Diehl,Ashia C. Wilson*

Main category: cs.LG

TL;DR: 论文指出博弈论中的半值概念用于机器学习数据估值存在问题，其依赖的效用函数定义不明确，导致估值有任意性和可操纵性，引发伦理和认知担忧。


<details>
  <summary>Details</summary>
Motivation: 探讨半值概念在机器学习数据估值应用中可能存在的问题。

Method: 通过理论构建和实证例子进行分析。

Result: 发现效用函数定义不明确会使数据估值有任意性，且存在低成本对抗策略可操纵估值。

Conclusion: 半值方法给建模者带来了论证负担，需考虑合适的应用场景。

Abstract: The game-theoretic notion of the semivalue offers a popular framework for
credit attribution and data valuation in machine learning. Semivalues have been
proposed for a variety of high-stakes decisions involving data, such as
determining contributor compensation, acquiring data from external sources, or
filtering out low-value datapoints. In these applications, semivalues depend on
the specification of a utility function that maps subsets of data to a scalar
score. While it is broadly agreed that this utility function arises from a
composition of a learning algorithm and a performance metric, its actual
instantiation involves numerous subtle modeling choices. We argue that this
underspecification leads to varying degrees of arbitrariness in semivalue-based
valuations. Small, but arguably reasonable changes to the utility function can
induce substantial shifts in valuations across datapoints. Moreover, these
valuation methodologies are also often gameable: low-cost adversarial
strategies exist to exploit this ambiguity and systematically redistribute
value among datapoints. Through theoretical constructions and empirical
examples, we demonstrate that a bad-faith valuator can manipulate utility
specifications to favor preferred datapoints, and that a good-faith valuator is
left without principled guidance to justify any particular specification. These
vulnerabilities raise ethical and epistemic concerns about the use of
semivalues in several applications. We conclude by highlighting the burden of
justification that semivalue-based approaches place on modelers and discuss
important considerations for identifying appropriate uses.

</details>


### [146] [Human-like Forgetting Curves in Deep Neural Networks](https://arxiv.org/abs/2506.12034)
*Dylan Kline*

Main category: cs.LG

TL;DR: 研究通过检查人工模型是否呈现类人遗忘曲线，搭建认知科学与神经网络设计的桥梁，提出衡量信息保留的框架，实验揭示类人遗忘曲线，可为持续学习算法提供参考。


<details>
  <summary>Details</summary>
Motivation: 搭建认知科学和神经网络设计的桥梁，解决神经网络部署时的灾难性遗忘问题，提高训练效率。

Method: 借鉴Ebbinghaus记忆衰退研究和间隔重复原则，提出定量框架，通过评估网络当前隐藏状态与原型表示的相似度计算召回概率。

Result: 多层感知机实验揭示类人遗忘曲线，知识通过定期复习更稳固。

Conclusion: 神经网络自然模拟人类记忆衰退，可为先进的持续学习算法提供信息。

Abstract: This study bridges cognitive science and neural network design by examining
whether artificial models exhibit human-like forgetting curves. Drawing upon
Ebbinghaus' seminal work on memory decay and principles of spaced repetition,
we propose a quantitative framework to measure information retention in neural
networks. Our approach computes the recall probability by evaluating the
similarity between a network's current hidden state and previously stored
prototype representations. This retention metric facilitates the scheduling of
review sessions, thereby mitigating catastrophic forgetting during deployment
and enhancing training efficiency by prompting targeted reviews. Our
experiments with Multi-Layer Perceptrons reveal human-like forgetting curves,
with knowledge becoming increasingly robust through scheduled reviews. This
alignment between neural network forgetting curves and established human memory
models identifies neural networks as an architecture that naturally emulates
human memory decay and can inform state-of-the-art continual learning
algorithms.

</details>


### [147] [Fast and Furious Symmetric Learning in Zero-Sum Games: Gradient Descent as Fictitious Play](https://arxiv.org/abs/2506.13086)
*John Lazarsfeld,Georgios Piliouras,Ryann Sim,Andre Wibisono*

Main category: cs.LG

TL;DR: 研究零和博弈中两种无后悔算法（虚构博弈和常步长在线梯度下降）的次线性后悔保证，在对称零和博弈中证明二者有O(√T)后悔。


<details>
  <summary>Details</summary>
Motivation: 在一般对抗在线学习设置中，两种算法可能不稳定且有线性后悔，在二人零和博弈中其获得更严格后悔界的能力尚不清楚。

Method: 针对一类对称零和博弈，在玩家策略对称初始化下，对虚构博弈用任意破平规则证明后悔界；利用虚构博弈和梯度下降迭代在收益向量对偶空间的几何联系，证明梯度下降在对称初始化下的后悔界。

Result: 证明虚构博弈在任意破平规则下有O(√T)后悔，梯度下降在几乎所有对称初始化和足够大常步长时有类似后悔界。

Conclusion: 为虚构博弈建立新的适用游戏类，证明梯度下降在大于2x2零和博弈中有“快速激烈”行为。

Abstract: This paper investigates the sublinear regret guarantees of two non-no-regret
algorithms in zero-sum games: Fictitious Play, and Online Gradient Descent with
constant stepsizes. In general adversarial online learning settings, both
algorithms may exhibit instability and linear regret due to no regularization
(Fictitious Play) or small amounts of regularization (Gradient Descent).
However, their ability to obtain tighter regret bounds in two-player zero-sum
games is less understood. In this work, we obtain strong new regret guarantees
for both algorithms on a class of symmetric zero-sum games that generalize the
classic three-strategy Rock-Paper-Scissors to a weighted, n-dimensional regime.
Under symmetric initializations of the players' strategies, we prove that
Fictitious Play with any tiebreaking rule has $O(\sqrt{T})$ regret,
establishing a new class of games for which Karlin's Fictitious Play conjecture
holds. Moreover, by leveraging a connection between the geometry of the
iterates of Fictitious Play and Gradient Descent in the dual space of payoff
vectors, we prove that Gradient Descent, for almost all symmetric
initializations, obtains a similar $O(\sqrt{T})$ regret bound when its stepsize
is a sufficiently large constant. For Gradient Descent, this establishes the
first "fast and furious" behavior (i.e., sublinear regret without
time-vanishing stepsizes) for zero-sum games larger than 2x2.

</details>


### [148] [MARché: Fast Masked Autoregressive Image Generation with Cache-Aware Attention](https://arxiv.org/abs/2506.12035)
*Chaoyi Jiang,Sungwoo Kim,Lei Gao,Hossein Entezari Zarch,Won Woo Ro,Murali Annavaram*

Main category: cs.LG

TL;DR: 提出训练无关生成框架MARch'e解决MASKED自回归模型计算开销大问题，实现加速且不影响图像质量。


<details>
  <summary>Details</summary>
Motivation: MASKED自回归模型在图像生成时计算开销大，多数token语义稳定却在每步解码时重复计算。

Method: 提出包含缓存感知注意力和选择性KV刷新的训练无关生成框架MARch'e。缓存感知注意力划分token，选择性KV刷新根据注意力分数更新需重新计算的token。

Result: MARch'e显著减少冗余计算，实现高达1.7倍加速，对图像质量影响可忽略不计。

Conclusion: MARch'e是高效MASKED变压器生成的可扩展且广泛适用的解决方案。

Abstract: Masked autoregressive (MAR) models unify the strengths of masked and
autoregressive generation by predicting tokens in a fixed order using
bidirectional attention for image generation. While effective, MAR models
suffer from significant computational overhead, as they recompute attention and
feed-forward representations for all tokens at every decoding step, despite
most tokens remaining semantically stable across steps. We propose a
training-free generation framework MARch\'e to address this inefficiency
through two key components: cache-aware attention and selective KV refresh.
Cache-aware attention partitions tokens into active and cached sets, enabling
separate computation paths that allow efficient reuse of previously computed
key/value projections without compromising full-context modeling. But a cached
token cannot be used indefinitely without recomputation due to the changing
contextual information over multiple steps. MARch\'e recognizes this challenge
and applies a technique called selective KV refresh. Selective KV refresh
identifies contextually relevant tokens based on attention scores from newly
generated tokens and updates only those tokens that require recomputation,
while preserving image generation quality. MARch\'e significantly reduces
redundant computation in MAR without modifying the underlying architecture.
Empirically, MARch\'e achieves up to 1.7x speedup with negligible impact on
image quality, offering a scalable and broadly applicable solution for
efficient masked transformer generation.

</details>


### [149] [Private List Learnability vs. Online List Learnability](https://arxiv.org/abs/2506.12856)
*Steve Hanneke,Shay Moran,Hilla Schefler,Iska Tsubari*

Main category: cs.LG

TL;DR: 本文探讨PAC列表学习中差分隐私（DP）与在线学习的联系，发现多类设置中的等价关系在列表学习中不成立，引入k - 单调维度并证明其为DP k - 列表可学习性的必要条件，二者有限性是否意味着可学习性仍待解决。


<details>
  <summary>Details</summary>
Motivation: 研究PAC列表学习场景下差分隐私与在线学习的关系，明确多类框架中的等价关系在列表学习中是否成立。

Method: 理论证明有限k - Littlestone维度不是DP k - 列表可学习性的充分条件，以单调函数类为例说明等价关系的失效，引入k - 单调维度并证明其为必要条件。

Result: 有限k - Littlestone维度是必要非充分条件，有限k - 单调维度是必要条件。

Conclusion: 多类设置中DP可学习性与在线可学习性的等价关系在列表学习中不成立，引入新维度但二者有限性与可学习性的关系待研究。

Abstract: This work explores the connection between differential privacy (DP) and
online learning in the context of PAC list learning. In this setting, a
$k$-list learner outputs a list of $k$ potential predictions for an instance
$x$ and incurs a loss if the true label of $x$ is not included in the list. A
basic result in the multiclass PAC framework with a finite number of labels
states that private learnability is equivalent to online learnability [Alon,
Livni, Malliaris, and Moran (2019); Bun, Livni, and Moran (2020); Jung, Kim,
and Tewari (2020)]. Perhaps surprisingly, we show that this equivalence does
not hold in the context of list learning. Specifically, we prove that, unlike
in the multiclass setting, a finite $k$-Littlestone dimensio--a variant of the
classical Littlestone dimension that characterizes online $k$-list
learnability--is not a sufficient condition for DP $k$-list learnability.
However, similar to the multiclass case, we prove that it remains a necessary
condition.
  To demonstrate where the equivalence breaks down, we provide an example
showing that the class of monotone functions with $k+1$ labels over
$\mathbb{N}$ is online $k$-list learnable, but not DP $k$-list learnable. This
leads us to introduce a new combinatorial dimension, the \emph{$k$-monotone
dimension}, which serves as a generalization of the threshold dimension. Unlike
the multiclass setting, where the Littlestone and threshold dimensions are
finite together, for $k>1$, the $k$-Littlestone and $k$-monotone dimensions do
not exhibit this relationship. We prove that a finite $k$-monotone dimension is
another necessary condition for DP $k$-list learnability, alongside finite
$k$-Littlestone dimension. Whether the finiteness of both dimensions implies
private $k$-list learnability remains an open question.

</details>


### [150] [A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.12036)
*Yanting Miao,William Loh,Suraj Kothawade,Pacal Poupart*

Main category: cs.LG

TL;DR: 本文提出Noise PPO算法微调文图扩散模型，优化初始噪声分布可提升对齐度和样本质量，明确了“黄金噪声”假设的范围和局限。


<details>
  <summary>Details</summary>
Motivation: 现有用强化学习微调文图扩散模型的方法存在不必要的复杂性，受“黄金噪声”假设启发提出新算法。

Method: 引入极简主义强化学习算法Noise PPO，冻结预训练扩散模型，学习基于提示条件的初始噪声生成器。

Result: 优化初始噪声分布能持续提升模型的对齐度和样本质量，在低推理步数时提升显著，随推理步数增加，提升效果减弱但仍存在。

Conclusion: 明确了“黄金噪声”假设的范围和局限，强化了极简主义强化学习微调扩散模型的实用价值。

Abstract: Recent work uses reinforcement learning (RL) to fine-tune text-to-image
diffusion models, improving text-image alignment and sample quality. However,
existing approaches introduce unnecessary complexity: they cache the full
sampling trajectory, depend on differentiable reward models or large preference
datasets, or require specialized guidance techniques. Motivated by the "golden
noise" hypothesis -- that certain initial noise samples can consistently yield
superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that
leaves the pre-trained diffusion model entirely frozen and learns a
prompt-conditioned initial noise generator. Our approach requires no trajectory
storage, reward backpropagation, or complex guidance tricks. Extensive
experiments show that optimizing the initial noise distribution consistently
improves alignment and sample quality over the original model, with the most
significant gains at low inference steps. As the number of inference steps
increases, the benefit of noise optimization diminishes but remains present.
These findings clarify the scope and limitations of the golden noise hypothesis
and reinforce the practical value of minimalist RL fine-tuning for diffusion
models.

</details>


### [151] [PhenoKG: Knowledge Graph-Driven Gene Discovery and Patient Insights from Phenotypes Alone](https://arxiv.org/abs/2506.13119)
*Kamilia Zaripova,Ege Özsoy,Nassir Navab,Azade Farshad*

Main category: cs.LG

TL;DR: 提出基于图的方法预测致病基因，结合图神经网络和transformer，在真实数据集上表现超当前最优，可处理仅含表型数据情况。


<details>
  <summary>Details</summary>
Motivation: 从患者表型识别致病基因是精准医学的重大挑战，对遗传病诊断和治疗有重要意义。

Method: 提出基于图的方法，通过整合罕见病知识图谱预测致病基因，结合图神经网络和transformers。

Result: 在MyGene2数据集上，MRR达24.64%，nDCG@100达33.64%，超过最佳基线SHEPHERD。

Conclusion: 该方法有显著提升，能推广到仅含表型数据的情况，解决临床决策中基因组信息不完整的关键挑战。

Abstract: Identifying causative genes from patient phenotypes remains a significant
challenge in precision medicine, with important implications for the diagnosis
and treatment of genetic disorders. We propose a novel graph-based approach for
predicting causative genes from patient phenotypes, with or without an
available list of candidate genes, by integrating a rare disease knowledge
graph (KG). Our model, combining graph neural networks and transformers,
achieves substantial improvements over the current state-of-the-art. On the
real-world MyGene2 dataset, it attains a mean reciprocal rank (MRR) of 24.64\%
and nDCG@100 of 33.64\%, surpassing the best baseline (SHEPHERD) at 19.02\% MRR
and 30.54\% nDCG@100. We perform extensive ablation studies to validate the
contribution of each model component. Notably, the approach generalizes to
cases where only phenotypic data are available, addressing key challenges in
clinical decision support when genomic information is incomplete.

</details>


### [152] [How to Train a Model on a Cheap Cluster with Low Cost using Block Coordinate Descent](https://arxiv.org/abs/2506.12037)
*Zeyu Liu,Yunquan Zhang,Boyang Zhang,Guoyong Jiang,Daning Cheng*

Main category: cs.LG

TL;DR: 提出基于块坐标下降（BCD）的全参数预训练框架，在RTX 4090集群高效训练大模型，降低成本且保持精度。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型需大量GPU内存和资金，对中小团队是障碍，希望降低成本。

Method: 提出基于块坐标下降的全参数预训练框架，进行工程优化，按参数块进行梯度计算和更新。

Result: 1. 相同设备成本降低，7B模型在A100、A800集群成本降至约33%，在RTX 4090集群降至约2.6%；2. 可跨设备迁移，能从A100集群迁移到4090集群；3. 保持与全参数预训练相同的模型精度。

Conclusion: 该框架能在低成本硬件上高效训练大模型，降低成本且不损失精度。

Abstract: Training large language models typically demands extensive GPU memory and
substantial financial investment, which poses a barrier for many small- to
medium-sized teams. In this paper, we present a full-parameter pre-training
framework based on block coordinate descent (BCD), augmented with engineering
optimizations, to efficiently train large models on affordable RTX 4090 GPU
clusters. BCD ensures model convergence based on block coordinate descent
theory and performs gradient computation and update at the level of parameter
blocks. Experiments show that 1) Lower cost of Same-Device: BCD significantly
reduces pre-training cost. For the 7B model, under identical hardware settings,
BCD lowers training costs to approximately 33% on A100,A800 clusters on 7B
model averagely and to approximately 2.6% on RTX 4090 clusters on 7B model,
compared to traditional full-parameter training. 2) Cross-Device Transfer: By
leveraging BCD, large-scale models previously trainable only on high-end A100
clusters can be seamlessly migrated and pre-trained on 4090 clusters-whose
hourly cost is only one-quarter that of A100-without requiring expensive
hardware. 3) Accuracy Retention: In both scenarios, BCD training achieves the
same level of model accuracy as full-parameter pre-training.

</details>


### [153] [Stochastic Multi-Objective Multi-Armed Bandits: Regret Definition and Algorithm](https://arxiv.org/abs/2506.13125)
*Mansoor Davoodi,Setareh Maghsudi*

Main category: cs.LG

TL;DR: 提出新的综合遗憾度量和有效帕累托最优臂概念，开发两阶段MO - MAB算法实现次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有MO - MAB方法主要依赖的帕累托遗憾度量有局限性，特别是不能同时考虑所有帕累托最优臂。

Method: 提出新的综合遗憾度量，引入有效帕累托最优臂概念，基于新度量开发两阶段MO - MAB算法。

Result: 开发的两阶段MO - MAB算法能对帕累托最优臂和有效帕累托最优臂实现次线性遗憾。

Conclusion: 新的度量和算法可解决现有MO - MAB方法中帕累托遗憾度量的局限性问题。

Abstract: Multi-armed bandit (MAB) problems are widely applied to online optimization
tasks that require balancing exploration and exploitation. In practical
scenarios, these tasks often involve multiple conflicting objectives, giving
rise to multi-objective multi-armed bandits (MO-MAB). Existing MO-MAB
approaches predominantly rely on the Pareto regret metric introduced in
\cite{drugan2013designing}. However, this metric has notable limitations,
particularly in accounting for all Pareto-optimal arms simultaneously. To
address these challenges, we propose a novel and comprehensive regret metric
that ensures balanced performance across conflicting objectives. Additionally,
we introduce the concept of \textit{Efficient Pareto-Optimal} arms, which are
specifically designed for online optimization. Based on our new metric, we
develop a two-phase MO-MAB algorithm that achieves sublinear regret for both
Pareto-optimal and efficient Pareto-optimal arms.

</details>


### [154] [LCD: Advancing Extreme Low-Bit Clustering for Large Language Models via Knowledge Distillation](https://arxiv.org/abs/2506.12038)
*Fangxin Liu,Ning Yang,Junping Zhao,Tao Yang,Haibing Guan,Li Jiang*

Main category: cs.LG

TL;DR: 论文提出LCD方法统一基于聚类量化学习与知识蒸馏框架，能在2 - 3位超低比特下保留大语言模型性能，压缩激活并加速推理，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署受高内存和计算需求挑战，现有权重量化在低比特压缩上效果不佳。

Method: 提出LCD方法，在知识蒸馏框架内统一基于聚类的量化学习，采用优化技术，通过平滑压缩激活，用基于查找表（LUT）设计加速推理。

Result: LCD优于现有方法，推理速度最高提升6.2倍，且更具成本效益。

Conclusion: LCD是适用于实际应用的实用解决方案。

Abstract: Large language models (LLMs) have achieved significant progress in natural
language processing but face challenges in deployment due to high memory and
computational requirements. Weight quantization is a common approach to address
these issues, yet achieving effective low-bit compression remains challenging.
This paper presents LCD, which unifies the learning of clustering-based
quantization within a knowledge distillation framework. Using carefully
designed optimization techniques, LCD preserves LLM performance even at
ultra-low bit widths of 2-3 bits. Additionally, LCD compresses activations
through smoothing and accelerates inference with a LUT-based design.
Experimental results show that LCD outperforms existing methods and delivers up
to a 6.2x speedup in inference. Notably, LCD is shown to be more
cost-effective, making it a practical solution for real-world applications.

</details>


### [155] [The Maximal Overlap Discrete Wavelet Scattering Transform and Its Application in Classification Tasks](https://arxiv.org/abs/2506.12039)
*Leonardo Fonseca Larrubia,Pedro Alberto Morettin,Chang Chiann*

Main category: cs.LG

TL;DR: 提出最大重叠离散小波散射变换（MODWST），并评估其在两类分类任务中的表现，结果显示表现良好，是训练数据集有限时的可行替代方法。


<details>
  <summary>Details</summary>
Motivation: 结合最大重叠离散小波变换（MODWT）和散射小波变换（WST）构建新变换，并评估其在分类任务中的表现。

Method: 构建最大重叠离散小波散射变换（MODWST），并将其应用于平稳信号分类和心电图信号分类任务。

Result: MODWST在平稳信号分类和心电图信号分类两个应用中均取得良好表现。

Conclusion: MODWST是卷积神经网络（CNNs）等流行方法的可行替代方案，尤其在训练数据集有限时。

Abstract: We present the Maximal Overlap Discrete Wavelet Scattering Transform
(MODWST), whose construction is inspired by the combination of the Maximal
Overlap Discrete Wavelet Transform (MODWT) and the Scattering Wavelet Transform
(WST). We also discuss the use of MODWST in classification tasks, evaluating
its performance in two applications: stationary signal classification and ECG
signal classification. The results demonstrate that MODWST achieved good
performance in both applications, positioning itself as a viable alternative to
popular methods like Convolutional Neural Networks (CNNs), particularly when
the training data set is limited.

</details>


### [156] [Learning Augmented Graph $k$-Clustering](https://arxiv.org/abs/2506.13533)
*Chenglin Fan,Kijun Shin*

Main category: cs.LG

TL;DR: 本文将学习增强的k - 聚类推广到一般度量，放宽聚类大小约束，扩展查询复杂度硬度，加强了学习增强聚类的理论基础和实用性。


<details>
  <summary>Details</summary>
Motivation: 以往研究聚焦欧几里得度量下的学习增强k - 均值，限制了其在复杂数据表示中的应用。

Method: 将学习增强的k - 聚类推广到一般度量，放宽聚类大小约束，在指数时间假设下扩展查询复杂度硬度。

Result: 表明在指数时间假设下，任何多项式时间算法要达到(1 + α) - 近似必须执行约Ω(k / α)次查询。

Conclusion: 这些贡献加强了学习增强聚类的理论基础和实际适用性，弥合了传统方法与现实挑战之间的差距。

Abstract: Clustering is a fundamental task in unsupervised learning. Previous research
has focused on learning-augmented $k$-means in Euclidean metrics, limiting its
applicability to complex data representations. In this paper, we generalize
learning-augmented $k$-clustering to operate on general metrics, enabling its
application to graph-structured and non-Euclidean domains. Our framework also
relaxes restrictive cluster size constraints, providing greater flexibility for
datasets with imbalanced or unknown cluster distributions. Furthermore, we
extend the hardness of query complexity to general metrics: under the
Exponential Time Hypothesis (ETH), we show that any polynomial-time algorithm
must perform approximately $\Omega(k / \alpha)$ queries to achieve a $(1 +
\alpha)$-approximation. These contributions strengthen both the theoretical
foundations and practical applicability of learning-augmented clustering,
bridging gaps between traditional methods and real-world challenges.

</details>


### [157] [Polyra Swarms: A Shape-Based Approach to Machine Learning](https://arxiv.org/abs/2506.13217)
*Simon Klüttermann,Emmanuel Müller*

Main category: cs.LG

TL;DR: 提出Polyra Swarms新机器学习方法，能低偏差通用学习，比神经网络有优势，还引入自动化抽象机制。


<details>
  <summary>Details</summary>
Motivation: 寻找一种能进行低偏差通用学习，且在特定任务上比神经网络更优的新方法。

Method: 提出Polyra Swarms方法，并引入自动化抽象机制简化其复杂性。

Result: Polyra Swarms在特定任务如异常检测中比神经网络更有优势，自动化抽象机制提升了泛化性和透明度。

Conclusion: Polyra Swarms基于不同原理，开辟了新研究方向，有独特优缺点。

Abstract: We propose Polyra Swarms, a novel machine-learning approach that approximates
shapes instead of functions. Our method enables general-purpose learning with
very low bias. In particular, we show that depending on the task, Polyra Swarms
can be preferable compared to neural networks, especially for tasks like
anomaly detection. We further introduce an automated abstraction mechanism that
simplifies the complexity of a Polyra Swarm significantly, enhancing both their
generalization and transparency. Since Polyra Swarms operate on fundamentally
different principles than neural networks, they open up new research directions
with distinct strengths and limitations.

</details>


### [158] [Fed-HeLLo: Efficient Federated Foundation Model Fine-Tuning with Heterogeneous LoRA Allocation](https://arxiv.org/abs/2506.12213)
*Zikai Zhang,Ping Liu,Jiahao Xu,Rui Hu*

Main category: cs.LG

TL;DR: 提出Fed - HeLLo框架用于联邦LoRA微调，设计多种HLA策略并在多数据集评估，结果有效高效。


<details>
  <summary>Details</summary>
Motivation: 现有联邦LoRA微调方法未考虑客户端资源异构性，缺乏有限资源下最大化全局微调性能的有效本地训练策略。

Method: 提出Fed - HeLLo框架，开发基于客户端资源能力和层重要性的HLA策略，包括基于Fisher信息矩阵分数的HLA、几何定义的HLA及其随机化版本。

Result: 在五个数据集、不同联邦LoRA微调设置及三种数据分布水平下评估，Fed - HeLLo与HLA策略有效且高效。

Conclusion: Fed - HeLLo框架结合HLA策略在联邦LoRA微调中表现良好，能适应客户端资源异构性，提升全局微调性能。

Abstract: Federated Learning has recently been utilized to collaboratively fine-tune
foundation models across multiple clients. Notably, federated low-rank
adaptation LoRA-based fine-tuning methods have recently gained attention, which
allows clients to fine-tune FMs with a small portion of trainable parameters
locally. However, most existing methods do not account for the heterogeneous
resources of clients or lack an effective local training strategy to maximize
global fine-tuning performance under limited resources. In this work, we
propose Fed-HeLLo, a novel federated LoRA-based fine-tuning framework that
enables clients to collaboratively fine-tune an FM with different local
trainable LoRA layers. To ensure its effectiveness, we develop several
heterogeneous LoRA allocation (HLA) strategies that adaptively allocate local
trainable LoRA layers based on clients' resource capabilities and the layer
importance. Specifically, based on the dynamic layer importance, we design a
Fisher Information Matrix score-based HLA that leverages dynamic gradient norm
information. To better stabilize the training process, we consider the
intrinsic importance of LoRA layers and design a Geometrically-Defined HLA
strategy. It shapes the collective distribution of trainable LoRA layers into
specific geometric patterns, such as Triangle, Inverted Triangle, Bottleneck,
and Uniform. Moreover, we extend GD-HLA into a randomized version, named
Randomized Geometrically-Defined HLA, for enhanced model accuracy with
randomness. By co-designing the proposed HLA strategies, we incorporate both
the dynamic and intrinsic layer importance into the design of our HLA strategy.
We evaluate our approach on five datasets under diverse federated LoRA
fine-tuning settings, covering three levels of data distribution from IID to
extreme Non-IID. Results show that Fed-HeLLo with HLA strategies is both
effective and efficient.

</details>


### [159] [BTC-LLM: Efficient Sub-1-Bit LLM Quantization via Learnable Transformation and Binary Codebook](https://arxiv.org/abs/2506.12040)
*Hao Gu,Lujun Li,Zheyu Wang,Bei Liu,Qiyuan Zhu,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 本文提出BTC - LLM，一种新的亚1位大语言模型量化框架，利用自适应权重变换和二进制模式聚类，解决现有稀疏感知二值化方法问题，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏感知二值化方法存在性能下降、稀疏掩码管理计算复杂和硬件兼容性有限等问题，需要改进。

Method: 提出Learnable Transformation优化可逆缩放和旋转矩阵，提升层表示质量；使用Flash and Accurate Binary Codebook识别二进制向量簇并压缩成紧凑索引，无需稀疏掩码。

Result: 未明确提及具体结果，但表明该框架能克服现有方法的局限，兼顾准确性和效率。

Conclusion: BTC - LLM框架能解决现有亚1位量化方法的问题，可在标准硬件上高效推理。

Abstract: Binary quantization represents the most extreme form of large language model
(LLM) compression, reducing weights to $\pm$1 for maximal memory and
computational efficiency. While recent sparsity-aware binarization methods
achieve sub-1-bit compression by pruning redundant binary weights, they suffer
from three critical challenges: performance deterioration, computational
complexity from sparse mask management, and limited hardware compatibility. In
this paper, we present BTC-LLM, a novel sub-1-bit LLM quantization framework
that leverages adaptive weight transformation and binary pattern clustering to
overcome these limitations, delivering both superior accuracy and efficiency.
Our approach incorporates two key innovations: (1) a Learnable Transformation
that optimizes invertible scaling and rotation matrices to align binarized
weights with full-precision distributions, enabling incoherence processing to
enhance layer-wise representation quality; (2) a Flash and Accurate Binary
Codebook that identifies recurring binary vector clusters, compressing them
into compact indices with tailored distance metrics and sign-based centroid
updates. This eliminates the need for sparse masks, enabling efficient
inference on standard hardware. Our code is available at
https://github.com/Chooovy/BTC-LLM.

</details>


### [160] [Meta Pruning via Graph Metanetworks : A Meta Learning Framework for Network Pruning](https://arxiv.org/abs/2506.12041)
*Yewei Liu,Xiyuan Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: 本文提出用元网络自动学习剪枝策略，将难剪枝网络转换为易剪枝网络，在多任务上取得出色结果。


<details>
  <summary>Details</summary>
Motivation: 现有手动设计的剪枝标准达到瓶颈，且剪枝技术日益复杂难解释。

Method: 引入元学习中的元网络概念，建立神经网络与图的双射映射，用图神经网络作为元网络，训练其自动学习剪枝策略。

Result: 在多个流行且有代表性的剪枝任务上取得出色结果。

Conclusion: 所提方法有效，可通过元网络前馈和标准微调实现最先进的剪枝。

Abstract: Network pruning, aimed at reducing network size while preserving accuracy,
has attracted significant research interest. Numerous pruning techniques have
been proposed over time. They are becoming increasingly effective, but more
complex and harder to interpret as well. Given the inherent complexity of
neural networks, we argue that manually designing pruning criteria has reached
a bottleneck. To address this, we propose a novel approach in which we "use a
neural network to prune neural networks". More specifically, we introduce the
newly developed idea of metanetwork from meta-learning into pruning. A
metanetwork is a network that takes another network as input and produces a
modified network as output. In this paper, we first establish a bijective
mapping between neural networks and graphs, and then employ a graph neural
network as our metanetwork. We train a metanetwork that learns the pruning
strategy automatically which can transform a network that is hard to prune into
another network that is much easier to prune. Once the metanetwork is trained,
our pruning needs nothing more than a feedforward through the metanetwork and
the standard finetuning to prune at state-of-the-art. Our method achieved
outstanding results on many popular and representative pruning tasks (including
ResNet56 on CIFAR10, VGG19 on CIFAR100, ResNet50 on ImageNet). Our code is
available at https://github.com/Yewei-Liu/MetaPruning

</details>


### [161] [CRITS: Convolutional Rectifier for Interpretable Time Series Classification](https://arxiv.org/abs/2506.12042)
*Alejandro Kuratomi,Zed Lee,Guilherme Dinis Chaliane Junior,Tony Lindgren,Diego García Pérez*

Main category: cs.LG

TL;DR: 提出用于时间序列分类的可解释模型CRITS，介绍其方法并评估性能和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有卷积网络分类器可解释性方法存在因上采样问题缺乏详细解释或需随机扰动等问题，需新的可解释模型。

Method: 使用卷积核层、最大池化层和全连接整流网络，利用整流线性单元激活提取特征权重。

Result: 在一组数据集上评估了CRITS的分类性能、解释对齐性、敏感性和可理解性。

Conclusion: 未明确提及结论，但暗示CRITS在时间序列分类可解释性方面有潜力。

Abstract: Several interpretability methods for convolutional network-based classifiers
exist. Most of these methods focus on extracting saliency maps for a given
sample, providing a local explanation that highlights the main regions for the
classification. However, some of these methods lack detailed explanations in
the input space due to upscaling issues or may require random perturbations to
extract the explanations. We propose Convolutional Rectifier for Interpretable
Time Series Classification, or CRITS, as an interpretable model for time series
classification that is designed to intrinsically extract local explanations.
The proposed method uses a layer of convolutional kernels, a max-pooling layer
and a fully-connected rectifier network (a network with only rectified linear
unit activations). The rectified linear unit activation allows the extraction
of the feature weights for the given sample, eliminating the need to calculate
gradients, use random perturbations and the upscale of the saliency maps to the
initial input space. We evaluate CRITS on a set of datasets, and study its
classification performance and its explanation alignment, sensitivity and
understandability.

</details>


### [162] [Why Do Some Inputs Break Low-Bit LLM Quantization?](https://arxiv.org/abs/2506.12044)
*Ting-Yun Chang,Muru Zhang,Jesse Thomason,Robin Jia*

Main category: cs.LG

TL;DR: 分析大语言模型低比特仅权重量化中不同方法的量化误差相关性，揭示误差产生原因及关键模型组件。


<details>
  <summary>Details</summary>
Motivation: 低比特仅权重量化在减少大语言模型内存占用时对部分示例影响过大，需分析原因。

Method: 分析不同3 - 4比特量化方法，建立残差流幅度与误差放大积累关系的假设，使用大语言模型定位技术、提前退出和激活修补方法。

Result: 50对量化方法在FineWeb示例上的量化误差强相关，全精度模型的残差流幅度可指示未来量化误差，误差大的示例依赖后期层精确残差激活，MLP门输出对保持困惑度至关重要。

Conclusion: 揭示了部分示例产生大量化误差的原因以及对性能保持最关键的模型组件。

Abstract: Low-bit weight-only quantization significantly reduces the memory footprint
of large language models (LLMs), but disproportionately affects certain
examples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in
size and find that the quantization errors of 50 pairs of methods are strongly
correlated (avg. 0.82) on FineWeb examples. Moreover, the residual stream
magnitudes of full-precision models are indicative of future quantization
errors. We further establish a hypothesis that relates the residual stream
magnitudes to error amplification and accumulation over layers. Using LLM
localization techniques, early exiting, and activation patching, we show that
examples with large errors rely on precise residual activations in the late
layers, and that the outputs of MLP gates play a crucial role in maintaining
the perplexity. Our work reveals why certain examples result in large
quantization errors and which model components are most critical for
performance preservation.

</details>


### [163] [From Proxies to Fields: Spatiotemporal Reconstruction of Global Radiation from Sparse Sensor Sequences](https://arxiv.org/abs/2506.12045)
*Kazuma Kobayashi,Samrendra Roy,Seid Koric,Diab Abueidda,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: 提出Temporal Radiation Operator Network (TRON)从稀疏数据重建环境场，在宇宙辐射剂量重建上表现优且有广泛应用。


<details>
  <summary>Details</summary>
Motivation: 传统方法从稀疏间接观测准确重建潜在环境场存在高计算成本、延迟或空间覆盖有限等问题。

Method: 提出TRON时空神经算子架构，用22年模拟数据训练。

Result: 在全球宇宙辐射剂量重建中，实现亚秒级推理，相对L2误差低于0.1%，比基于蒙特卡罗的估计器快超58000倍。

Conclusion: TRON为从稀疏数据进行科学场重建提供领域无关框架，可用于大气建模等领域。

Abstract: Accurate reconstruction of latent environmental fields from sparse and
indirect observations is a foundational challenge across scientific
domains-from atmospheric science and geophysics to public health and aerospace
safety. Traditional approaches rely on physics-based simulators or dense sensor
networks, both constrained by high computational cost, latency, or limited
spatial coverage. We present the Temporal Radiation Operator Network (TRON), a
spatiotemporal neural operator architecture designed to infer continuous global
scalar fields from sequences of sparse, non-uniform proxy measurements.
  Unlike recent forecasting models that operate on dense, gridded inputs to
predict future states, TRON addresses a more ill-posed inverse problem:
reconstructing the current global field from sparse, temporally evolving sensor
sequences, without access to future observations or dense labels. Demonstrated
on global cosmic radiation dose reconstruction, TRON is trained on 22 years of
simulation data and generalizes across 65,341 spatial locations, 8,400 days,
and sequence lengths from 7 to 90 days. It achieves sub-second inference with
relative L2 errors below 0.1%, representing a >58,000X speedup over Monte
Carlo-based estimators. Though evaluated in the context of cosmic radiation,
TRON offers a domain-agnostic framework for scientific field reconstruction
from sparse data, with applications in atmospheric modeling, geophysical hazard
monitoring, and real-time environmental risk forecasting.

</details>


### [164] [Perfect Privacy for Discriminator-Based Byzantine-Resilient Federated Learning](https://arxiv.org/abs/2506.13561)
*Yue Xia,Christoph Hofmeister,Maximilian Egger,Rawad Bitar*

Main category: cs.LG

TL;DR: 提出ByITFL和LoByITFL两种联邦学习方案，增强对拜占庭用户的抵御能力并保护隐私，给出理论和实验验证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习存在新的隐私和安全挑战，需增强对拜占庭用户的抵御能力并保护用户数据隐私。

Method: 基于联邦者拥有小型代表性数据集和构建判别函数，ByITFL采用拉格朗日编码计算和重新随机化，LoByITFL降低通信成本但需可信第三方。

Result: ByITFL有完美信息论隐私但通信开销大，LoByITFL通信成本显著降低，有理论和实验验证。

Conclusion: 所提两种方案能有效解决联邦学习的隐私和安全问题，可根据需求选择。

Abstract: Federated learning (FL) shows great promise in large-scale machine learning
but introduces new privacy and security challenges. We propose ByITFL and
LoByITFL, two novel FL schemes that enhance resilience against Byzantine users
while keeping the users' data private from eavesdroppers. To ensure privacy and
Byzantine resilience, our schemes build on having a small representative
dataset available to the federator and crafting a discriminator function
allowing the mitigation of corrupt users' contributions. ByITFL employs
Lagrange coded computing and re-randomization, making it the first
Byzantine-resilient FL scheme with perfect Information-Theoretic (IT) privacy,
though at the cost of a significant communication overhead. LoByITFL, on the
other hand, achieves Byzantine resilience and IT privacy at a significantly
reduced communication cost, but requires a Trusted Third Party, used only in a
one-time initialization phase before training. We provide theoretical
guarantees on privacy and Byzantine resilience, along with convergence
guarantees and experimental results validating our findings.

</details>


### [165] [Explaining Recovery Trajectories of Older Adults Post Lower-Limb Fracture Using Modality-wise Multiview Clustering and Large Language Models](https://arxiv.org/abs/2506.12156)
*Shehroz S. Khan,Ali Abedi,Charlene H. Chu*

Main category: cs.LG

TL;DR: 本文解决社区中下肢骨折康复老年患者聚类传感器数据的解释问题，通过聚类各数据模态，用大语言模型生成聚类标签并验证，结果证明方法有效，能助医生识别高危患者。


<details>
  <summary>Details</summary>
Motivation: 在无监督医疗数据分析中，解释聚类数据可深入了解患者健康结果，当前人类理解高维无标签大数据存在挑战，本文聚焦老年患者康复传感器数据解释。

Method: 分别对各数据模态进行聚类，用上下文感知提示让大语言模型为各模态聚类推断有意义标签，通过统计测试和可视化结合临床评分验证。

Result: 大语言模型生成的多数特定模态聚类标签与临床评分有统计学意义，证明方法有效。

Conclusion: 仅依赖传感器数据的无监督数据分析方法，可让临床医生识别高危患者并及时改善健康结果。

Abstract: Interpreting large volumes of high-dimensional, unlabeled data in a manner
that is comprehensible to humans remains a significant challenge across various
domains. In unsupervised healthcare data analysis, interpreting clustered data
can offer meaningful insights into patients' health outcomes, which hold direct
implications for healthcare providers. This paper addresses the problem of
interpreting clustered sensor data collected from older adult patients
recovering from lower-limb fractures in the community. A total of 560 days of
multimodal sensor data, including acceleration, step count, ambient motion, GPS
location, heart rate, and sleep, alongside clinical scores, were remotely
collected from patients at home. Clustering was first carried out separately
for each data modality to assess the impact of feature sets extracted from each
modality on patients' recovery trajectories. Then, using context-aware
prompting, a large language model was employed to infer meaningful cluster
labels for the clusters derived from each modality. The quality of these
clusters and their corresponding labels was validated through rigorous
statistical testing and visualization against clinical scores collected
alongside the multimodal sensor data. The results demonstrated the statistical
significance of most modality-specific cluster labels generated by the large
language model with respect to clinical scores, confirming the efficacy of the
proposed method for interpreting sensor data in an unsupervised manner. This
unsupervised data analysis approach, relying solely on sensor data, enables
clinicians to identify at-risk patients and take timely measures to improve
health outcomes.

</details>


### [166] [Meta-Learning and Synthetic Data for Automated Pretraining and Finetuning](https://arxiv.org/abs/2506.12161)
*Fabio Ferreira*

Main category: cs.LG

TL;DR: 本文采用元学习将自动化机器学习扩展到深度学习领域，提出自动化DL管道选择方法，还进行数据增强和合成数据的元学习，结果显示该方法表现出色。


<details>
  <summary>Details</summary>
Motivation: 机器学习中预训练模型增多，需确定合适DL管道，且模型规模增大对数据利用提出挑战，传统自动化机器学习不适用于深度学习。

Method: 采用元学习扩展自动化机器学习到深度学习领域，提出自动化DL管道选择的经验方法，进行数据增强和合成数据的元学习。

Result: 提出的方法能超越微调基础模型，显示了数据增强在自监督学习中的重要性，还提出元学习神经合成数据生成器和多环境世界模型。

Conclusion: 元学习可有效解决深度学习中DL管道选择和数据利用的问题，提升模型性能。

Abstract: The growing number of pretrained models in Machine Learning (ML) presents
significant challenges for practitioners. Given a new dataset, they need to
determine the most suitable deep learning (DL) pipeline, consisting of the
pretrained model and the hyperparameters for finetuning to it. Moreover, as
models grow in scale, the increasing reliance on real-world data poses a
bottleneck for training and requires leveraging data more effectively.
Addressing the first challenge often involves manual model selection and
hyperparameter tuning. At the same time, as models grow larger and more and
more of the available human-generated data is being used for training, data
augmentation and synthetic data become critical elements. Automated machine
learning offers a path to address these challenges but is traditionally
designed for tabular data and classical ML methods. This dissertation adopts
meta-learning to extend automated machine learning to the deep learning domain.
We propose empirical approaches to automate DL pipeline selection for Computer
Vision tasks using prior task knowledge to learn surrogate models for pipeline
ranking. Extending these methods to the language domain, we learn to finetune
large language models. As a result, we show that our approach can outperform
finetuning foundation models. Additionally, we meta-learn data augmentation and
synthetic data to enhance performance in up-stream and down-stream tasks. We
empirically show the underestimated importance of data augmentation when using
Self-Supervised Learning and meta-learn advanced data augmentation strategies.
Leveraging synthetic data, we also propose to meta-learn neural synthetic data
generators as proxies for Reinforcement Learning (RL) environments.
Additionally, we learn a multiple-environment world model in an in-context
learning fashion by purely using synthetic, randomly sampled data.

</details>


### [167] [Fidelity Isn't Accuracy: When Linearly Decodable Functions Fail to Match the Ground Truth](https://arxiv.org/abs/2506.12176)
*Jackson Eshbaugh*

Main category: cs.LG

TL;DR: 提出线性得分λ(f)量化回归网络输出与线性模型的拟合度，在合成和真实数据集上评估，发现高得分不意味着预测准确，揭示了用线性替代理解非线性模型的利弊。


<details>
  <summary>Details</summary>
Motivation: 神经网络作为函数逼近器时其学习函数的性质常被复杂性掩盖，需要一种可解释的方法来量化其与线性模型的关系。

Method: 提出线性得分λ(f)，定义为网络预测与训练的线性替代预测的R²；在合成和真实数据集上，使用特定数据集的网络和替代进行评估。

Result: 高λ(f)分数表明与线性模型强对齐，但不一定对真实值有预测准确性。

Conclusion: 使用线性替代理解非线性模型行为有前景，但在高风险回归任务中有局限性。

Abstract: Neural networks excel as function approximators, but their complexity often
obscures the nature of the functions they learn. In this work, we propose the
linearity score $\lambda(f)$, a simple and interpretable diagnostic that
quantifies how well a regression network's output can be mimicked by a linear
model. Defined as the $R^2$ between the network's predictions and those of a
trained linear surrogate, $\lambda(f)$ offers insight into the linear
decodability of the learned function. We evaluate this framework on both
synthetic ($y = x \sin(x) + \epsilon$) and real-world datasets (Medical
Insurance, Concrete, California Housing), using dataset-specific networks and
surrogates. Our findings show that while high $\lambda(f)$ scores indicate
strong linear alignment, they do not necessarily imply predictive accuracy with
respect to the ground truth. This underscores both the promise and the
limitations of using linear surrogates to understand nonlinear model behavior,
particularly in high-stakes regression tasks.

</details>


### [168] [Generative or Discriminative? Revisiting Text Classification in the Era of Transformers](https://arxiv.org/abs/2506.12181)
*Siva Rajesh Kasa,Karan Gupta,Sumegh Roychowdhury,Ashutosh Kumar,Yaswanth Biruduraju,Santhosh Kumar Kasa,Nikhil Priyatam Pattisapu,Arindam Bhattacharya,Shailendra Agarwal,Vijay huddar*

Main category: cs.LG

TL;DR: 文章首次全面评估现代生成式和判别式架构用于文本分类，揭示经典现象在不同架构和训练范式中的表现，并分析多方面性能，为实际选择模型提供指导。


<details>
  <summary>Details</summary>
Motivation: 早期理论工作对生成式和判别式分类器的权衡在Transformer时代未被探索，需开展相关研究。

Method: 对自回归建模、掩码语言建模、离散扩散和编码器等现代生成式和判别式架构进行全面评估。

Result: 经典的‘两个制度’现象在不同架构和训练范式中表现不同，分析了样本效率、校准、噪声鲁棒性和序数性等方面。

Conclusion: 研究结果为基于延迟和数据限制等现实约束选择合适的建模方法提供了实际指导。

Abstract: The comparison between discriminative and generative classifiers has
intrigued researchers since Efron's seminal analysis of logistic regression
versus discriminant analysis. While early theoretical work established that
generative classifiers exhibit lower sample complexity but higher asymptotic
error in simple linear settings, these trade-offs remain unexplored in the
transformer era. We present the first comprehensive evaluation of modern
generative and discriminative architectures - Auto-regressive modeling, Masked
Language Modeling, Discrete Diffusion, and Encoders for text classification.
Our study reveals that the classical 'two regimes' phenomenon manifests
distinctly across different architectures and training paradigms. Beyond
accuracy, we analyze sample efficiency, calibration, noise robustness, and
ordinality across diverse scenarios. Our findings offer practical guidance for
selecting the most suitable modeling approach based on real-world constraints
such as latency and data limitations.

</details>


### [169] [Graph Semi-Supervised Learning for Point Classification on Data Manifolds](https://arxiv.org/abs/2506.12197)
*Caio F. Deberaldini Netto,Zhiyang Wang,Luana Ruiz*

Main category: cs.LG

TL;DR: 提出用于数据流形分类任务的图半监督学习框架，理论分析其泛化性并实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 基于流形假设，将数据建模为低维流形上的采样点，解决点分类问题。

Method: 用变分自编码器无监督近似流形，构建几何图将点分类转为图节点半监督分类，用图神经网络求解，理论分析泛化性，训练时定期重采样图。

Result: 在均匀采样下，半监督任务泛化差距随图大小增加而减小至图神经网络训练误差，重采样可进一步减小泛化差距。

Conclusion: 通过图像分类基准实验验证了方法的有效性。

Abstract: We propose a graph semi-supervised learning framework for classification
tasks on data manifolds. Motivated by the manifold hypothesis, we model data as
points sampled from a low-dimensional manifold $\mathcal{M} \subset
\mathbb{R}^F$. The manifold is approximated in an unsupervised manner using a
variational autoencoder (VAE), where the trained encoder maps data to
embeddings that represent their coordinates in $\mathbb{R}^F$. A geometric
graph is constructed with Gaussian-weighted edges inversely proportional to
distances in the embedding space, transforming the point classification problem
into a semi-supervised node classification task on the graph. This task is
solved using a graph neural network (GNN). Our main contribution is a
theoretical analysis of the statistical generalization properties of this
data-to-manifold-to-graph pipeline. We show that, under uniform sampling from
$\mathcal{M}$, the generalization gap of the semi-supervised task diminishes
with increasing graph size, up to the GNN training error. Leveraging a training
procedure which resamples a slightly larger graph at regular intervals during
training, we then show that the generalization gap can be reduced even further,
vanishing asymptotically. Finally, we validate our findings with numerical
experiments on image classification benchmarks, demonstrating the empirical
effectiveness of our approach.

</details>


### [170] [Private Continuous-Time Synthetic Trajectory Generation via Mean-Field Langevin Dynamics](https://arxiv.org/abs/2506.12203)
*Anming Gu,Edward Chien,Kristjan Greenewald*

Main category: cs.LG

TL;DR: 提出私有生成连续时间数据的算法，在医疗等领域有应用，实验能生成逼真轨迹并保证隐私和效用。


<details>
  <summary>Details</summary>
Motivation: 为高度敏感的时间序列数据领域（如医疗）提供私有生成连续时间数据的方法。

Method: 利用轨迹推断和连续时间合成数据生成的联系，基于平均场朗之万动力学的计算方法，应用噪声SGD的DP结果。

Result: 在合成的手绘MNIST数据上生成了逼真轨迹，同时保证了有意义的隐私。

Conclusion: 该方法在每人仅贡献一个时间点数据的设置下有强效用保证，相比之前方法直接改善了隐私特性。

Abstract: We provide an algorithm to privately generate continuous-time data (e.g.
marginals from stochastic differential equations), which has applications in
highly sensitive domains involving time-series data such as healthcare. We
leverage the connections between trajectory inference and continuous-time
synthetic data generation, along with a computational method based on
mean-field Langevin dynamics. As discretized mean-field Langevin dynamics and
noisy particle gradient descent are equivalent, DP results for noisy SGD can be
applied to our setting. We provide experiments that generate realistic
trajectories on a synthesized variation of hand-drawn MNIST data while
maintaining meaningful privacy guarantees. Crucially, our method has strong
utility guarantees under the setting where each person contributes data for
\emph{only one time point}, while prior methods require each person to
contribute their \emph{entire temporal trajectory}--directly improving the
privacy characteristics by construction.

</details>


### [171] [Semantic Scheduling for LLM Inference](https://arxiv.org/abs/2506.12204)
*Wenyue Hua,Dujian Ding,Yile Gu,Yujie Ren,Kai Mei,Minghua Ma,William Yang Wang*

Main category: cs.LG

TL;DR: 本文提出大语言模型请求调度中的语义调度概念，设计新调度算法并通过医疗应急管理应用展示其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统操作系统调度算法忽略内容，不考虑进程意图和语义，无法优先处理紧急重要任务，而语言模型的进展使语义分析成为可能。

Method: 引入语义调度概念，设计具有最优时间复杂度的调度算法以最小化基于大语言模型提示调度的总体等待时间。

Result: 通过医疗应急管理应用展示了算法的有效性，代码和数据可在指定链接获取。

Conclusion: 语义调度对关键、时间敏感任务有潜在好处。

Abstract: Conventional operating system scheduling algorithms are largely
content-ignorant, making decisions based on factors such as latency or fairness
without considering the actual intents or semantics of processes. Consequently,
these algorithms often do not prioritize tasks that require urgent attention or
carry higher importance, such as in emergency management scenarios. However,
recent advances in language models enable semantic analysis of processes,
allowing for more intelligent and context-aware scheduling decisions. In this
paper, we introduce the concept of semantic scheduling in scheduling of
requests from large language models (LLM), where the semantics of the process
guide the scheduling priorities. We present a novel scheduling algorithm with
optimal time complexity, designed to minimize the overall waiting time in
LLM-based prompt scheduling. To illustrate its effectiveness, we present a
medical emergency management application, underscoring the potential benefits
of semantic scheduling for critical, time-sensitive tasks. The code and data
are available at
https://github.com/Wenyueh/latency_optimization_with_priority_constraints.

</details>


### [172] [Learning Causality for Modern Machine Learning](https://arxiv.org/abs/2506.12226)
*Yongqiang Chen*

Main category: cs.LG

TL;DR: 文章探讨在现代机器学习中融入因果关系，利用因果不变性原理，研究其对机器学习理想特性的益处，同时指出实现因果关系与经验风险最小化存在矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习采用经验风险最小化方法，未对因果关系建模，在分布偏移时，离分布泛化问题具有挑战性，因此需在现代机器学习中融入因果关系。

Method: 利用独立因果机制原理所隐含的不变性，将其应用于图结构，研究因果关系对机器学习的影响。

Result: 未提及具体结果。

Conclusion: 未明确给出结论，但指出在机器学习中实现因果关系会给传统机器学习的优化带来困境。

Abstract: In the past decades, machine learning with Empirical Risk Minimization (ERM)
has demonstrated great capability in learning and exploiting the statistical
patterns from data, or even surpassing humans. Despite the success, ERM avoids
the modeling of causality the way of understanding and handling changes, which
is fundamental to human intelligence. When deploying models beyond the training
environment, distribution shifts are everywhere. For example, an autopilot
system often needs to deal with new weather conditions that have not been seen
during training, An Al-aided drug discovery system needs to predict the
biochemical properties of molecules with respect to new viruses such as
COVID-19. It renders the problem of Out-of-Distribution (OOD) generalization
challenging to conventional machine learning.
  In this thesis, we investigate how to incorporate and realize the causality
for broader tasks in modern machine learning. In particular, we exploit the
invariance implied by the principle of independent causal mechanisms (ICM),
that is, the causal mechanisms generating the effects from causes do not inform
or influence each other. Therefore, the conditional distribution between the
target variable given its causes is invariant under distribution shifts. With
the causal invariance principle, we first instantiate it to graphs -- a general
data structure ubiquitous in many real-world industry and scientific
applications, such as financial networks and molecules. Then, we shall see how
learning the causality benefits many of the desirable properties of modern
machine learning, in terms of (i) OOD generalization capability; (ii)
interpretability; and (iii) robustness to adversarial attacks.
  Realizing the causality in machine learning, on the other hand, raises a
dilemma for optimization in conventional machine learning, as it often
contradicts the objective of ERM...

</details>


### [173] [Uncovering Bias Paths with LLM-guided Causal Discovery: An Active Learning and Dynamic Scoring Approach](https://arxiv.org/abs/2506.12227)
*Khadija Zanna,Akane Sano*

Main category: cs.LG

TL;DR: 本文提出基于LLM的因果发现混合框架，在含噪条件下恢复公平相关路径表现出色，还探讨了对现实数据集偏差审计的意义。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现算法在现实含噪环境中难以恢复公平相关路径，且常引入虚假或有偏路径，需更好方法。

Method: 提出结合广度优先搜索、主动学习和动态评分的基于LLM的混合框架，用综合得分对变量对进行优先查询，构建半合成基准评估公平敏感性。

Result: 包括所提方法在内的LLM引导方法在含噪条件下恢复路径表现有竞争力或更优。

Conclusion: 明确动态评分和主动查询最有益的情况，讨论了对现实数据集偏差审计的意义。

Abstract: Causal discovery (CD) plays a pivotal role in understanding the mechanisms
underlying complex systems. While recent algorithms can detect spurious
associations and latent confounding, many struggle to recover fairness-relevant
pathways in realistic, noisy settings. Large Language Models (LLMs), with their
access to broad semantic knowledge, offer a promising complement to statistical
CD approaches, particularly in domains where metadata provides meaningful
relational cues. Ensuring fairness in machine learning requires understanding
how sensitive attributes causally influence outcomes, yet CD methods often
introduce spurious or biased pathways. We propose a hybrid LLM-based framework
for CD that extends a breadth-first search (BFS) strategy with active learning
and dynamic scoring. Variable pairs are prioritized for LLM-based querying
using a composite score based on mutual information, partial correlation, and
LLM confidence, improving discovery efficiency and robustness.
  To evaluate fairness sensitivity, we construct a semi-synthetic benchmark
from the UCI Adult dataset, embedding a domain-informed causal graph with
injected noise, label corruption, and latent confounding. We assess how well CD
methods recover both global structure and fairness-critical paths.
  Our results show that LLM-guided methods, including the proposed method,
demonstrate competitive or superior performance in recovering such pathways
under noisy conditions. We highlight when dynamic scoring and active querying
are most beneficial and discuss implications for bias auditing in real-world
datasets.

</details>


### [174] [From Emergence to Control: Probing and Modulating Self-Reflection in Language Models](https://arxiv.org/abs/2506.12217)
*Xudong Zhu,Jiachen Jiang,Mohammad Mahdi Khalili,Zhihui Zhu*

Main category: cs.LG

TL;DR: 本文研究大语言模型自我反思能力，发现预训练模型也有潜在反思能力，提出诱导反思探测方法，构建自我反思向量实现双向控制，能平衡推理质量和效率。


<details>
  <summary>Details</summary>
Motivation: 自我反思虽与推理准确率相关，但起源和机制不明，需深入研究。

Method: 引入Reflection - Inducing Probing方法，分析内部表征构建自我反思向量并操纵它。

Result: 干预使Qwen2.5自我反思频率从0.6%提升到18.6%；操纵向量，增强可使推理性能提升12%，抑制可降低计算成本。

Conclusion: 研究增进对自我反思的理解，表明理解模型内部可实现精确行为控制。

Abstract: Self-reflection -- the ability of a large language model (LLM) to revisit,
evaluate, and revise its own reasoning -- has recently emerged as a powerful
behavior enabled by reinforcement learning with verifiable rewards (RLVR).
While self-reflection correlates with improved reasoning accuracy, its origin
and underlying mechanisms remain poorly understood. In this work, {\it we first
show that self-reflection is not exclusive to RLVR fine-tuned models: it
already emerges, albeit rarely, in pretrained models}. To probe this latent
ability, we introduce Reflection-Inducing Probing, a method that injects
reflection-triggering reasoning traces from fine-tuned models into pretrained
models. This intervention raises self-reflection frequency of Qwen2.5 from
0.6\% to 18.6\%, revealing a hidden capacity for reflection. Moreover, our
analysis of internal representations shows that both pretrained and fine-tuned
models maintain hidden states that distinctly separate self-reflective from
non-reflective contexts. Leveraging this observation, {\it we then construct a
self-reflection vector, a direction in activation space associated with
self-reflective reasoning}. By manipulating this vector, we enable
bidirectional control over the self-reflective behavior for both pretrained and
fine-tuned models. Experiments across multiple reasoning benchmarks show that
enhancing these vectors improves reasoning performance by up to 12\%, while
suppressing them reduces computational cost, providing a flexible mechanism to
navigate the trade-off between reasoning quality and efficiency without
requiring additional training. Our findings further our understanding of
self-reflection and support a growing body of work showing that understanding
model internals can enable precise behavioral control.

</details>


### [175] [Two heads are better than one: simulating large transformers with small ones](https://arxiv.org/abs/2506.12220)
*Hantao Yu,Josh Alman*

Main category: cs.LG

TL;DR: 本文探讨利用小transformer处理长输入序列，证明大transformer可被小transformer高效模拟，并给出不同场景下所需小transformer数量。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的二次复杂度使transformer难以有效处理长输入序列，而现代硬件对处理短输入序列的transformer有优化，因此想利用小transformer处理长输入序列。

Method: 理论证明任意输入长度为N的transformer可由O((N/M)^2)个输入长度为M（M远小于N）的transformer高效模拟，并分析不同自然场景。

Result: 最坏情况下模拟所需小transformer数量为O((N/M)^2)，在平均输入、滑动窗口掩码和注意力汇聚等自然场景下，最优数量为O(N/M)。

Conclusion: 长输入序列的transformer可由短输入序列的transformer高效模拟，不同场景下所需小transformer数量不同。

Abstract: The quadratic complexity of self-attention prevents transformers from scaling
effectively to long input sequences. On the other hand, modern GPUs and other
specialized hardware accelerators are well-optimized for processing small input
sequences in transformers during both training and inference. A natural
question arises: can we take advantage of the efficiency of small transformers
to deal with long input sequences?
  In this paper, we show that transformers with long input sequences (large
transformers) can be efficiently simulated by transformers that can only take
short input sequences (small transformers). Specifically, we prove that any
transformer with input length $N$ can be efficiently simulated by only
$O((N/M)^2)$ transformers with input length $M \ll N$, and that this cannot be
improved in the worst case. However, we then prove that in various natural
scenarios including average-case inputs, sliding window masking and attention
sinks, the optimal number $O(N/M)$ of small transformers suffice.

</details>


### [176] [GrokAlign: Geometric Characterisation and Acceleration of Grokking](https://arxiv.org/abs/2506.12284)
*Thomas Walker,Ahmed Imtiaz Humayun,Randall Balestriero,Richard Baraniuk*

Main category: cs.LG

TL;DR: 本文研究深度网络的grokking现象，提出GrokAlign方法加速该现象，还引入质心对齐简化雅可比矩阵对齐。


<details>
  <summary>Details</summary>
Motivation: 理解并加速深度网络训练动态中的grokking现象。

Method: 通过将网络的雅可比矩阵与训练数据对齐（余弦相似度意义上），提出GrokAlign方法和质心对齐简化方法。

Result: GrokAlign比传统正则化方法如权重衰减更快诱导grokking现象。

Conclusion: 为深度网络优化中使用雅可比正则化提供理论依据，质心对齐可有效识别和跟踪训练阶段。

Abstract: A key challenge for the machine learning community is to understand and
accelerate the training dynamics of deep networks that lead to delayed
generalisation and emergent robustness to input perturbations, also known as
grokking. Prior work has associated phenomena like delayed generalisation with
the transition of a deep network from a linear to a feature learning regime,
and emergent robustness with changes to the network's functional geometry, in
particular the arrangement of the so-called linear regions in deep networks
employing continuous piecewise affine nonlinearities. Here, we explain how
grokking is realised in the Jacobian of a deep network and demonstrate that
aligning a network's Jacobians with the training data (in the sense of cosine
similarity) ensures grokking under a low-rank Jacobian assumption. Our results
provide a strong theoretical motivation for the use of Jacobian regularisation
in optimizing deep networks -- a method we introduce as GrokAlign -- which we
show empirically to induce grokking much sooner than more conventional
regularizers like weight decay. Moreover, we introduce centroid alignment as a
tractable and interpretable simplification of Jacobian alignment that
effectively identifies and tracks the stages of deep network training dynamics.
Accompanying
\href{https://thomaswalker1.github.io/blog/grokalign.html}{webpage} and
\href{https://github.com/ThomasWalker1/grokalign}{code}.

</details>


### [177] [SPIRE: Conditional Personalization for Federated Diffusion Generative Models](https://arxiv.org/abs/2506.12303)
*Kaan Ozkara,Ruida Zhou,Suhas Diggavi*

Main category: cs.LG

TL;DR: 提出SPIRE框架解决扩散模型在设备上个性化和联邦学习难题，有理论分析和良好实验效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型规模大，在设备上个性化及有效联邦学习不可行。

Method: 提出SPIRE框架，将网络分解为全局骨干和客户端嵌入，提供条件扩散训练与高斯混合模型最大似然估计的理论桥梁。

Result: SPIRE在协作预训练中表现与强基线相当或更好，适应未见客户端时大幅超越，减少核 inception 距离，更新参数少，还缓解灾难性遗忘，对微调学习率和轮次选择有鲁棒性。

Conclusion: SPIRE能有效解决扩散模型在设备上个性化和联邦学习问题。

Abstract: Recent advances in diffusion models have revolutionized generative AI, but
their sheer size makes on device personalization, and thus effective federated
learning (FL), infeasible. We propose Shared Backbone Personal Identity
Representation Embeddings (SPIRE), a framework that casts per client diffusion
based generation as conditional generation in FL. SPIRE factorizes the network
into (i) a high capacity global backbone that learns a population level score
function and (ii) lightweight, learnable client embeddings that encode local
data statistics. This separation enables parameter efficient finetuning that
touches $\leq 0.01\%$ of weights. We provide the first theoretical bridge
between conditional diffusion training and maximum likelihood estimation in
Gaussian mixture models. For a two component mixture we prove that gradient
descent on the DDPM with respect to mixing weights loss recovers the optimal
mixing weights and enjoys dimension free error bounds. Our analysis also hints
at how client embeddings act as biases that steer a shared score network toward
personalized distributions. Empirically, SPIRE matches or surpasses strong
baselines during collaborative pretraining, and vastly outperforms them when
adapting to unseen clients, reducing Kernel Inception Distance while updating
only hundreds of parameters. SPIRE further mitigates catastrophic forgetting
and remains robust across finetuning learning rate and epoch choices.

</details>


### [178] [Conditional Average Treatment Effect Estimation Under Hidden Confounders](https://arxiv.org/abs/2506.12304)
*Ahmed Aloui,Juncheng Dong,Ali Hasan,Vahid Tarokh*

Main category: cs.LG

TL;DR: 论文针对CATE估计中隐藏混杂因素问题，利用随机对照试验小数据集和观测大数据集，提出基于伪混杂因素生成器的CATE估计方法，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决CATE估计中因隐藏混杂因素导致的偏差问题，且传统仅依赖观测数据无法检验隐藏混杂因素。

Method: 提出基于伪混杂因素生成器和CATE模型的方法，使观测数据学习的潜在结果与RCT观测结果对齐。

Result: 通过大量数值实验，在合成和真实数据集上证明了方法的有效性。

Conclusion: 所提方法适用于很多实际场景，尤其是关注隐私的场景。

Abstract: One of the major challenges in estimating conditional potential outcomes and
conditional average treatment effects (CATE) is the presence of hidden
confounders. Since testing for hidden confounders cannot be accomplished only
with observational data, conditional unconfoundedness is commonly assumed in
the literature of CATE estimation. Nevertheless, under this assumption, CATE
estimation can be significantly biased due to the effects of unobserved
confounders. In this work, we consider the case where in addition to a
potentially large observational dataset, a small dataset from a randomized
controlled trial (RCT) is available. Notably, we make no assumptions on the
existence of any covariate information for the RCT dataset, we only require the
outcomes to be observed. We propose a CATE estimation method based on a
pseudo-confounder generator and a CATE model that aligns the learned potential
outcomes from the observational data with those observed from the RCT. Our
method is applicable to many practical scenarios of interest, particularly
those where privacy is a concern (e.g., medical applications). Extensive
numerical experiments are provided demonstrating the effectiveness of our
approach for both synthetic and real-world datasets.

</details>


### [179] [CheMixHub: Datasets and Benchmarks for Chemical Mixture Property Prediction](https://arxiv.org/abs/2506.12231)
*Ella Miray Rajaonson,Mahyar Rajabi Kochi,Luis Martin Mejia Mendoza,Seyed Mohamad Moosavi,Benjamin Sanchez-Lengeling*

Main category: cs.LG

TL;DR: 本文介绍分子混合物综合基准CheMixHub，含11个属性预测任务、约500k数据点，有多种数据分割技术，可助力化学混合物开发。


<details>
  <summary>Details</summary>
Motivation: 开发多分子系统预测模型很重要，但机器学习界对化学混合物空间探索较少。

Method: 引入CheMixHub基准，涵盖11个化学混合物属性预测任务，使用多种数据分割技术评估模型泛化性和鲁棒性。

Result: 建立了化学混合物深度学习模型的建模空间和初始基准。

Conclusion: 该数据集有潜力加速化学混合物的开发，代码和数据可在指定链接获取。

Abstract: Developing improved predictive models for multi-molecular systems is crucial,
as nearly every chemical product used results from a mixture of chemicals.
While being a vital part of the industry pipeline, the chemical mixture space
remains relatively unexplored by the Machine Learning community. In this paper,
we introduce CheMixHub, a holistic benchmark for molecular mixtures, covering a
corpus of 11 chemical mixtures property prediction tasks, from drug delivery
formulations to battery electrolytes, totalling approximately 500k data points
gathered and curated from 7 publicly available datasets. CheMixHub introduces
various data splitting techniques to assess context-specific generalization and
model robustness, providing a foundation for the development of predictive
models for chemical mixture properties. Furthermore, we map out the modelling
space of deep learning models for chemical mixtures, establishing initial
benchmarks for the community. This dataset has the potential to accelerate
chemical mixture development, encompassing reformulation, optimization, and
discovery. The dataset and code for the benchmarks can be found at:
https://github.com/chemcognition-lab/chemixhub

</details>


### [180] [Mind the XAI Gap: A Human-Centered LLM Framework for Democratizing Explainable AI](https://arxiv.org/abs/2506.12240)
*Eva Paraschou,Ioannis Arapakis,Sofia Yfantidou,Sebastian Macaluso,Athena Vakali*

Main category: cs.LG

TL;DR: 本文提出通用框架，结合大语言模型为专家和非专家提供透明、以人类为中心的AI解释，经评估证明有效。


<details>
  <summary>Details</summary>
Motivation: 人工智能黑盒模型需可解释AI解决方案提高透明度，现有方案多面向专家，需以人类为中心的透明可解释AI解决方案。

Method: 引入与领域、模型、解释无关的通用框架，利用大语言模型和上下文学习，通过结构化提示和系统设置封装解释信息。

Result: 通过对40多种组合进行基准测试建立上下文词库，经评估解释内容质量高（斯皮尔曼等级相关系数为0.92），用户研究显示对非专家解释的可解释性和友好度提升。

Conclusion: 该框架能弥补差距，提供高质量技术解释和面向非专家的清晰解释，证明可信任大语言模型作为以人类为中心可解释AI的推动者。

Abstract: Artificial Intelligence (AI) is rapidly embedded in critical decision-making
systems, however their foundational ``black-box'' models require eXplainable AI
(XAI) solutions to enhance transparency, which are mostly oriented to experts,
making no sense to non-experts. Alarming evidence about AI's unprecedented
human values risks brings forward the imperative need for transparent
human-centered XAI solutions. In this work, we introduce a domain-, model-,
explanation-agnostic, generalizable and reproducible framework that ensures
both transparency and human-centered explanations tailored to the needs of both
experts and non-experts. The framework leverages Large Language Models (LLMs)
and employs in-context learning to convey domain- and explainability-relevant
contextual knowledge into LLMs. Through its structured prompt and system
setting, our framework encapsulates in one response explanations understandable
by non-experts and technical information to experts, all grounded in domain and
explainability principles. To demonstrate the effectiveness of our framework,
we establish a ground-truth contextual ``thesaurus'' through a rigorous
benchmarking with over 40 data, model, and XAI combinations for an explainable
clustering analysis of a well-being scenario. Through a comprehensive quality
and human-friendliness evaluation of our framework's explanations, we prove
high content quality through strong correlations with ground-truth explanations
(Spearman rank correlation=0.92) and improved interpretability and
human-friendliness to non-experts through a user study (N=56). Our overall
evaluation confirms trust in LLMs as HCXAI enablers, as our framework bridges
the above Gaps by delivering (i) high-quality technical explanations aligned
with foundational XAI methods and (ii) clear, efficient, and interpretable
human-centered explanations for non-experts.

</details>


### [181] [Path-specific effects for pulse-oximetry guided decisions in critical care](https://arxiv.org/abs/2506.12371)
*Kevin Zhang,Yonghan Jung,Divyat Mahajan,Karthikeyan Shanmugam,Shalmali Joshi*

Main category: cs.LG

TL;DR: 研究通过因果推断方法研究血氧测量中种族差异对ICU有创通气的影响，发现种族差异对有创通气率影响小，但对通气时长有更明显影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对设备误差与患者结果之间因果关系的形式化，本研究旨在因果探究血氧测量中种族差异对ICU有创通气的影响。

Method: 采用基于因果推断的路径特定效应方法，使用双稳健估计量及其自归一化变体，并在半合成数据和两个真实世界健康数据集上验证。

Result: 种族差异对有创通气率影响小，由血氧饱和度差异介导的路径特定效应对通气时长影响更明显，且严重程度因数据集而异。

Conclusion: 提供了研究ICU潜在差异的新实用流程，强调因果方法对稳健评估决策公平性的必要性。

Abstract: Identifying and measuring biases associated with sensitive attributes is a
crucial consideration in healthcare to prevent treatment disparities. One
prominent issue is inaccurate pulse oximeter readings, which tend to
overestimate oxygen saturation for dark-skinned patients and misrepresent
supplemental oxygen needs. Most existing research has revealed statistical
disparities linking device errors to patient outcomes in intensive care units
(ICUs) without causal formalization. In contrast, this study causally
investigates how racial discrepancies in oximetry measurements affect invasive
ventilation in ICU settings. We employ a causal inference-based approach using
path-specific effects to isolate the impact of bias by race on clinical
decision-making. To estimate these effects, we leverage a doubly robust
estimator, propose its self-normalized variant for improved sample efficiency,
and provide novel finite-sample guarantees. Our methodology is validated on
semi-synthetic data and applied to two large real-world health datasets:
MIMIC-IV and eICU. Contrary to prior work, our analysis reveals minimal impact
of racial discrepancies on invasive ventilation rates. However, path-specific
effects mediated by oxygen saturation disparity are more pronounced on
ventilation duration, and the severity differs by dataset. Our work provides a
novel and practical pipeline for investigating potential disparities in the ICU
and, more crucially, highlights the necessity of causal methods to robustly
assess fairness in decision-making.

</details>


### [182] [A Collaborative Process Parameter Recommender System for Fleets of Networked Manufacturing Machines -- with Application to 3D Printing](https://arxiv.org/abs/2506.12252)
*Weishi Wang,Sicong Guo,Chenhuan Jiang,Mohamed Elidrisi,Myungjin Lee,Harsha V. Madhyastha,Raed Al Kontar,Chinedum E. Okwudire*

Main category: cs.LG

TL;DR: 本文提出基于机器学习的协作推荐系统优化制造机器集群的工艺参数，以3D打印农场为例验证方法有效，收敛速度更快。


<details>
  <summary>Details</summary>
Motivation: 制造机器集群中，因机器差异，传统试错法难以高效优化工艺参数。

Method: 将问题建模为顺序矩阵补全任务，利用谱聚类和交替最小二乘法迭代优化参数预测。

Result: 在由十台3D打印机组成的小型农场中，优化加速度和速度设置，相比非协作矩阵补全，能更快收敛到最优工艺参数。

Conclusion: 所提出的协作推荐系统能有效优化制造机器集群的工艺参数，减少实验次数。

Abstract: Fleets of networked manufacturing machines of the same type, that are
collocated or geographically distributed, are growing in popularity. An
excellent example is the rise of 3D printing farms, which consist of multiple
networked 3D printers operating in parallel, enabling faster production and
efficient mass customization. However, optimizing process parameters across a
fleet of manufacturing machines, even of the same type, remains a challenge due
to machine-to-machine variability. Traditional trial-and-error approaches are
inefficient, requiring extensive testing to determine optimal process
parameters for an entire fleet. In this work, we introduce a machine
learning-based collaborative recommender system that optimizes process
parameters for each machine in a fleet by modeling the problem as a sequential
matrix completion task. Our approach leverages spectral clustering and
alternating least squares to iteratively refine parameter predictions, enabling
real-time collaboration among the machines in a fleet while minimizing the
number of experimental trials. We validate our method using a mini 3D printing
farm consisting of ten 3D printers for which we optimize acceleration and speed
settings to maximize print quality and productivity. Our approach achieves
significantly faster convergence to optimal process parameters compared to
non-collaborative matrix completion.

</details>


### [183] [Scaling Probabilistic Circuits via Monarch Matrices](https://arxiv.org/abs/2506.12383)
*Honghua Zhang,Meihua Dang,Benjie Wang,Stefano Ermon,Nanyun Peng,Guy Van den Broeck*

Main category: cs.LG

TL;DR: 本文提出PCs中求和块的稀疏结构化参数化方法，降低内存和计算成本，提升扩展性与生成建模性能。


<details>
  <summary>Details</summary>
Motivation: 现有PCs扩展方法未同时利用稀疏性和张量运算两方面，需提出新方法进一步提升PCs扩展性。

Method: 提出新的稀疏结构化参数化方法，用稀疏Monarch矩阵替换密集矩阵。

Result: 在Text8、LM1B和ImageNet等基准测试中达到生成建模的最优性能，训练时用更少FLOPs实现相同性能。

Conclusion: 新方法显著降低内存和计算成本，实现PCs前所未有的扩展，具有良好性能和扩展性。

Abstract: Probabilistic Circuits (PCs) are tractable representations of probability
distributions allowing for exact and efficient computation of likelihoods and
marginals. Recent advancements have improved the scalability of PCs either by
leveraging their sparse properties or through the use of tensorized operations
for better hardware utilization. However, no existing method fully exploits
both aspects simultaneously. In this paper, we propose a novel sparse and
structured parameterization for the sum blocks in PCs. By replacing dense
matrices with sparse Monarch matrices, we significantly reduce the memory and
computation costs, enabling unprecedented scaling of PCs. From a theory
perspective, our construction arises naturally from circuit multiplication;
from a practical perspective, compared to previous efforts on scaling up
tractable probabilistic models, our approach not only achieves state-of-the-art
generative modeling performance on challenging benchmarks like Text8, LM1B and
ImageNet, but also demonstrates superior scaling behavior, achieving the same
performance with substantially less compute as measured by the number of
floating-point operations (FLOPs) during training.

</details>


### [184] [Revisiting Clustering of Neural Bandits: Selective Reinitialization for Mitigating Loss of Plasticity](https://arxiv.org/abs/2506.12389)
*Zhiyuan Su,Sunhao Dai,Xiao Zhang*

Main category: cs.LG

TL;DR: 提出Selective Reinitialization (SeRe)框架解决聚类神经多臂老虎机（CNB）可塑性损失问题，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: CNB算法存在可塑性损失问题，难以适应非平稳环境，如推荐系统中用户偏好的动态变化。

Method: 提出SeRe框架，利用贡献效用指标选择性重置未充分利用的单元，并结合自适应变化检测机制调整重置频率。

Result: 理论上证明SeRe在分段平稳环境中实现次线性累积遗憾，实验表明SeRe增强的CNB算法能有效减轻可塑性损失，降低遗憾。

Conclusion: SeRe框架能有效解决CNB算法的可塑性损失问题，提高其在动态环境中的适应性和鲁棒性。

Abstract: Clustering of Bandits (CB) methods enhance sequential decision-making by
grouping bandits into clusters based on similarity and incorporating
cluster-level contextual information, demonstrating effectiveness and
adaptability in applications like personalized streaming recommendations.
However, when extending CB algorithms to their neural version (commonly
referred to as Clustering of Neural Bandits, or CNB), they suffer from loss of
plasticity, where neural network parameters become rigid and less adaptable
over time, limiting their ability to adapt to non-stationary environments
(e.g., dynamic user preferences in recommendation). To address this challenge,
we propose Selective Reinitialization (SeRe), a novel bandit learning framework
that dynamically preserves the adaptability of CNB algorithms in evolving
environments. SeRe leverages a contribution utility metric to identify and
selectively reset underutilized units, mitigating loss of plasticity while
maintaining stable knowledge retention. Furthermore, when combining SeRe with
CNB algorithms, the adaptive change detection mechanism adjusts the
reinitialization frequency according to the degree of non-stationarity,
ensuring effective adaptation without unnecessary resets. Theoretically, we
prove that SeRe enables sublinear cumulative regret in piecewise-stationary
environments, outperforming traditional CNB approaches in long-term
performances. Extensive experiments on six real-world recommendation datasets
demonstrate that SeRe-enhanced CNB algorithms can effectively mitigate the loss
of plasticity with lower regrets, improving adaptability and robustness in
dynamic settings.

</details>


### [185] [A Survey of Foundation Models for IoT: Taxonomy and Criteria-Based Analysis](https://arxiv.org/abs/2506.12263)
*Hui Wei,Dong Yoon Lee,Shubham Rohal,Zhizhang Hu,Shiwei Fang,Shijia Pan*

Main category: cs.LG

TL;DR: 本文调研了基于基础模型的物联网方法，按效率、上下文感知、安全、隐私四个目标组织，实现跨领域比较并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有基于基础模型的物联网方法多针对特定任务，难以跨领域比较和应用于新任务。

Method: 按效率、上下文感知、安全、隐私四个目标对现有方法进行梳理，回顾代表工作、总结常用技术和评估指标。

Result: 实现了有意义的跨领域比较，为新物联网任务选择和设计基于基础模型的解决方案提供了实用见解。

Conclusion: 给出了未来研究的关键方向，以推动基础模型在物联网应用中的发展。

Abstract: Foundation models have gained growing interest in the IoT domain due to their
reduced reliance on labeled data and strong generalizability across tasks,
which address key limitations of traditional machine learning approaches.
However, most existing foundation model based methods are developed for
specific IoT tasks, making it difficult to compare approaches across IoT
domains and limiting guidance for applying them to new tasks. This survey aims
to bridge this gap by providing a comprehensive overview of current
methodologies and organizing them around four shared performance objectives by
different domains: efficiency, context-awareness, safety, and security &
privacy. For each objective, we review representative works, summarize
commonly-used techniques and evaluation metrics. This objective-centric
organization enables meaningful cross-domain comparisons and offers practical
insights for selecting and designing foundation model based solutions for new
IoT tasks. We conclude with key directions for future research to guide both
practitioners and researchers in advancing the use of foundation models in IoT
applications.

</details>


### [186] [PROTOCOL: Partial Optimal Transport-enhanced Contrastive Learning for Imbalanced Multi-view Clustering](https://arxiv.org/abs/2506.12408)
*Xuqian Xue,Yiming Lei,Qi Cai,Hongming Shan,Junping Zhang*

Main category: cs.LG

TL;DR: 本文针对多视图数据类别不平衡聚类问题提出PROTOCOL框架，通过感知类别不平衡和缓解少数样本表示退化，显著提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比多视图聚类方法隐式假设类别平衡分布，而现实中多视图数据存在类别不平衡，导致性能下降，因此需研究不平衡多视图聚类。

Method: 提出PROTOCOL框架，将多视图特征映射到共识空间，把不平衡聚类转化为部分最优传输问题，在特征和类别层面进行POT增强的类别重平衡对比学习。

Result: 广泛实验表明PROTOCOL显著提高了不平衡多视图数据的聚类性能。

Conclusion: PROTOCOL框架填补了不平衡多视图聚类领域的关键研究空白。

Abstract: While contrastive multi-view clustering has achieved remarkable success, it
implicitly assumes balanced class distribution. However, real-world multi-view
data primarily exhibits class imbalance distribution. Consequently, existing
methods suffer performance degradation due to their inability to perceive and
model such imbalance. To address this challenge, we present the first
systematic study of imbalanced multi-view clustering, focusing on two
fundamental problems: i. perceiving class imbalance distribution, and ii.
mitigating representation degradation of minority samples. We propose PROTOCOL,
a novel PaRtial Optimal TranspOrt-enhanced COntrastive Learning framework for
imbalanced multi-view clustering. First, for class imbalance perception, we map
multi-view features into a consensus space and reformulate the imbalanced
clustering as a partial optimal transport (POT) problem, augmented with
progressive mass constraints and weighted KL divergence for class
distributions. Second, we develop a POT-enhanced class-rebalanced contrastive
learning at both feature and class levels, incorporating logit adjustment and
class-sensitive learning to enhance minority sample representations. Extensive
experiments demonstrate that PROTOCOL significantly improves clustering
performance on imbalanced multi-view data, filling a critical research gap in
this field.

</details>


### [187] [Cross-Domain Conditional Diffusion Models for Time Series Imputation](https://arxiv.org/abs/2506.12412)
*Kexin Zhang,Baoyu Jing,K. Selçuk Candan,Dawei Zhou,Qingsong Wen,Han Liu,Kaize Ding*

Main category: cs.LG

TL;DR: 本文针对跨域时间序列插补问题，从数据、模型、算法三方面提出解决方案，实验证明方法优越。


<details>
  <summary>Details</summary>
Motivation: 现有单域时间序列插补方法无法适应有域偏移的新域，传统域适应技术难以处理数据不完整问题，缺失值阻碍分布对齐。

Method: 数据上引入基于频率的时间序列插值策略；模型上设计基于扩散的插补模型；算法上提出跨域一致性对齐策略。

Result: 在三个真实数据集上的实验表明所提方法具有优越性。

Conclusion: 所提的跨域时间序列插补方法能有效解决现有问题，实现有效知识转移并保留特定领域特征。

Abstract: Cross-domain time series imputation is an underexplored data-centric research
task that presents significant challenges, particularly when the target domain
suffers from high missing rates and domain shifts in temporal dynamics.
Existing time series imputation approaches primarily focus on the single-domain
setting, which cannot effectively adapt to a new domain with domain shifts.
Meanwhile, conventional domain adaptation techniques struggle with data
incompleteness, as they typically assume the data from both source and target
domains are fully observed to enable adaptation. For the problem of
cross-domain time series imputation, missing values introduce high uncertainty
that hinders distribution alignment, making existing adaptation strategies
ineffective. Specifically, our proposed solution tackles this problem from
three perspectives: (i) Data: We introduce a frequency-based time series
interpolation strategy that integrates shared spectral components from both
domains while retaining domain-specific temporal structures, constructing
informative priors for imputation. (ii) Model: We design a diffusion-based
imputation model that effectively learns domain-shared representations and
captures domain-specific temporal dependencies with dedicated denoising
networks. (iii) Algorithm: We further propose a cross-domain consistency
alignment strategy that selectively regularizes output-level domain
discrepancies, enabling effective knowledge transfer while preserving
domain-specific characteristics. Extensive experiments on three real-world
datasets demonstrate the superiority of our proposed approach. Our code
implementation is available here.

</details>


### [188] [Unveiling Confirmation Bias in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.12301)
*Yue Wan,Xiaowei Jia,Xiang Lorraine Li*

Main category: cs.LG

TL;DR: 本文从认知心理学的确认偏差角度研究CoT提示对大语言模型推理能力的影响，发现确认偏差影响推理过程和答案预测，为改进提示策略提供见解。


<details>
  <summary>Details</summary>
Motivation: CoT推理在不同推理类型任务中的有效性不一致，需要新视角理解其行为。

Method: 将CoT分解为两阶段过程，对模型信念、推理属性和各阶段表现进行相关性分析。

Result: 证明大语言模型中存在确认偏差，模型信念影响推理过程和答案预测，能解释CoT在不同任务和模型中的有效性。

Conclusion: 需要更好的提示策略来减轻确认偏差，以提高推理性能。

Abstract: Chain-of-thought (CoT) prompting has been widely adopted to enhance the
reasoning capabilities of large language models (LLMs). However, the
effectiveness of CoT reasoning is inconsistent across tasks with different
reasoning types. This work presents a novel perspective to understand CoT
behavior through the lens of \textit{confirmation bias} in cognitive
psychology. Specifically, we examine how model internal beliefs, approximated
by direct question-answering probabilities, affect both reasoning generation
($Q \to R$) and reasoning-guided answer prediction ($QR \to A$) in CoT. By
decomposing CoT into a two-stage process, we conduct a thorough correlation
analysis in model beliefs, rationale attributes, and stage-wise performance.
Our results provide strong evidence of confirmation bias in LLMs, such that
model beliefs not only skew the reasoning process but also influence how
rationales are utilized for answer prediction. Furthermore, the interplay
between task vulnerability to confirmation bias and the strength of beliefs
also provides explanations for CoT effectiveness across reasoning tasks and
models. Overall, this study provides a valuable insight for the needs of better
prompting strategies that mitigate confirmation bias to enhance reasoning
performance. Code is available at
\textit{https://github.com/yuewan2/biasedcot}.

</details>


### [189] [Interpretable Causal Representation Learning for Biological Data in the Pathway Space](https://arxiv.org/abs/2506.12439)
*Jesus de la Fuente,Robert Lehmann,Carlos Ruiz-Arenas,Jan Voges,Irene Marin-Goñi,Xabier Martinez-de-Morentin,David Gomez-Cabrero,Idoia Ochoa,Jesper Tegner,Vincenzo Lagani,Mikel Hernaez*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Predicting the impact of genomic and drug perturbations in cellular function
is crucial for understanding gene functions and drug effects, ultimately
leading to improved therapies. To this end, Causal Representation Learning
(CRL) constitutes one of the most promising approaches, as it aims to identify
the latent factors that causally govern biological systems, thus facilitating
the prediction of the effect of unseen perturbations. Yet, current CRL methods
fail in reconciling their principled latent representations with known
biological processes, leading to models that are not interpretable. To address
this major issue, we present SENA-discrepancy-VAE, a model based on the
recently proposed CRL method discrepancy-VAE, that produces representations
where each latent factor can be interpreted as the (linear) combination of the
activity of a (learned) set of biological processes. To this extent, we present
an encoder, SENA-{\delta}, that efficiently compute and map biological
processes' activity levels to the latent causal factors. We show that
SENA-discrepancy-VAE achieves predictive performances on unseen combinations of
interventions that are comparable with its original, non-interpretable
counterpart, while inferring causal latent factors that are biologically
meaningful.

</details>


### [190] [Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates](https://arxiv.org/abs/2506.12459)
*Chengqing Yu,Fei Wang,Chuanguang Yang,Zezhi Shao,Tao Sun,Tangwen Qian,Wei Wei,Zhulin An,Yongjun Xu*

Main category: cs.LG

TL;DR: 本文提出Multi - View Representation Learning (Merlin) 方法，解决多元时间序列预测模型对缺失值鲁棒性不足问题，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的多元时间序列预测模型易受缺失值影响，且缺乏对缺失值问题的鲁棒性，导致预测性能不佳。

Method: 提出Merlin方法，包含离线知识蒸馏和多视图对比学习两个关键模块。离线知识蒸馏用教师模型引导学生模型从缺失观测中挖掘语义；多视图对比学习通过不同缺失率的正负数据对提升学生模型鲁棒性。

Result: 在四个真实数据集上的实验表明Merlin具有优越性。

Conclusion: Merlin能有效增强现有模型对不固定缺失率的鲁棒性，同时保持预测准确性。

Abstract: Multivariate Time Series Forecasting (MTSF) involves predicting future values
of multiple interrelated time series. Recently, deep learning-based MTSF models
have gained significant attention for their promising ability to mine semantics
(global and local information) within MTS data. However, these models are
pervasively susceptible to missing values caused by malfunctioning data
collectors. These missing values not only disrupt the semantics of MTS, but
their distribution also changes over time. Nevertheless, existing models lack
robustness to such issues, leading to suboptimal forecasting performance. To
this end, in this paper, we propose Multi-View Representation Learning
(Merlin), which can help existing models achieve semantic alignment between
incomplete observations with different missing rates and complete observations
in MTS. Specifically, Merlin consists of two key modules: offline knowledge
distillation and multi-view contrastive learning. The former utilizes a teacher
model to guide a student model in mining semantics from incomplete
observations, similar to those obtainable from complete observations. The
latter improves the student model's robustness by learning from
positive/negative data pairs constructed from incomplete observations with
different missing rates, ensuring semantic alignment across different missing
rates. Therefore, Merlin is capable of effectively enhancing the robustness of
existing models against unfixed missing rates while preserving forecasting
accuracy. Experiments on four real-world datasets demonstrate the superiority
of Merlin.

</details>


### [191] [Delving into Instance-Dependent Label Noise in Graph Data: A Comprehensive Study and Benchmark](https://arxiv.org/abs/2506.12468)
*Suyeon Kim,SeongKu Kang,Dongwoo Kim,Jungseul Ok,Hwanjo Yu*

Main category: cs.LG

TL;DR: 提出新基准BeGIN处理图数据实例相关标签噪声，评估处理策略并分析其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图学习处理标签噪声研究多依赖类相关噪声，忽视实例相关噪声，不能捕捉现实损坏模式。

Method: 引入BeGIN基准，提供含多种噪声类型的图数据集，用算法方法和基于大语言模型（LLM）的模拟来模拟实例相关损坏。

Result: 实验揭示实例相关噪声挑战，强调节点特定参数化对增强GNN鲁棒性的重要性，评估了噪声处理策略的有效性、效率和关键性能因素。

Conclusion: BeGIN将推动图标签噪声研究和鲁棒GNN训练方法的发展。

Abstract: Graph Neural Networks (GNNs) have achieved state-of-the-art performance in
node classification tasks but struggle with label noise in real-world data.
Existing studies on graph learning with label noise commonly rely on
class-dependent label noise, overlooking the complexities of instance-dependent
noise and falling short of capturing real-world corruption patterns. We
introduce BeGIN (Benchmarking for Graphs with Instance-dependent Noise), a new
benchmark that provides realistic graph datasets with various noise types and
comprehensively evaluates noise-handling strategies across GNN architectures,
noisy label detection, and noise-robust learning. To simulate
instance-dependent corruptions, BeGIN introduces algorithmic methods and
LLM-based simulations. Our experiments reveal the challenges of
instance-dependent noise, particularly LLM-based corruption, and underscore the
importance of node-specific parameterization to enhance GNN robustness. By
comprehensively evaluating noise-handling strategies, BeGIN provides insights
into their effectiveness, efficiency, and key performance factors. We expect
that BeGIN will serve as a valuable resource for advancing research on label
noise in graphs and fostering the development of robust GNN training methods.
The code is available at https://github.com/kimsu55/BeGIN.

</details>


### [192] [Extending Memorization Dynamics in Pythia Models from Instance-Level Insights](https://arxiv.org/abs/2506.12321)
*Jie Zhang,Qinghua Zhao,Lei Li,Chi-ho Lin*

Main category: cs.LG

TL;DR: 本文分析Pythia模型族在不同规模和训练步骤下的记忆模式，发现模型规模、数据特征和前缀扰动对记忆有不同影响，有助于理解记忆机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型记忆模式的动态演化探索不足，本文旨在详细分析。

Method: 使用粒度指标，分析Pythia模型族在不同规模和训练步骤下，模型架构、数据特征和前缀扰动对记忆模式的影响。

Result: （1）模型规模增加，记忆量递增但效率快速下降；（2）新记忆获取率下降，旧记忆遗忘率上升；（3）数据特征对记忆和非记忆样本影响不同；（4）前缀扰动按强度降低记忆、增加生成不确定性，低冗余样本更脆弱，大模型无额外鲁棒性。

Conclusion: 研究结果有助于理解记忆机制，对训练优化、隐私保护和架构改进有直接意义。

Abstract: Large language models have demonstrated a remarkable ability for verbatim
memorization. While numerous works have explored factors influencing model
memorization, the dynamic evolution memorization patterns remains
underexplored. This paper presents a detailed analysis of memorization in the
Pythia model family across varying scales and training steps under prefix
perturbations. Using granular metrics, we examine how model architecture, data
characteristics, and perturbations influence these patterns. Our findings
reveal that: (1) as model scale increases, memorization expands incrementally
while efficiency decreases rapidly; (2) as model scale increases, the rate of
new memorization acquisition decreases while old memorization forgetting
increases; (3) data characteristics (token frequency, repetition count, and
uncertainty) differentially affect memorized versus non-memorized samples; and
(4) prefix perturbations reduce memorization and increase generation
uncertainty proportionally to perturbation strength, with low-redundancy
samples showing higher vulnerability and larger models offering no additional
robustness. These findings advance our understanding of memorization
mechanisms, with direct implications for training optimization, privacy
safeguards, and architectural improvements.

</details>


### [193] [Note on Follow-the-Perturbed-Leader in Combinatorial Semi-Bandit Problems](https://arxiv.org/abs/2506.12490)
*Botao Chen,Junya Honda*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper studies the optimality and complexity of
Follow-the-Perturbed-Leader (FTPL) policy in size-invariant combinatorial
semi-bandit problems. Recently, Honda et al. (2023) and Lee et al. (2024)
showed that FTPL achieves Best-of-Both-Worlds (BOBW) optimality in standard
multi-armed bandit problems with Fr\'{e}chet-type distributions. However, the
optimality of FTPL in combinatorial semi-bandit problems remains unclear. In
this paper, we consider the regret bound of FTPL with geometric resampling (GR)
in size-invariant semi-bandit setting, showing that FTPL respectively achieves
$O\left(\sqrt{m^2 d^\frac{1}{\alpha}T}+\sqrt{mdT}\right)$ regret with
Fr\'{e}chet distributions, and the best possible regret bound of
$O\left(\sqrt{mdT}\right)$ with Pareto distributions in adversarial setting.
Furthermore, we extend the conditional geometric resampling (CGR) to
size-invariant semi-bandit setting, which reduces the computational complexity
from $O(d^2)$ of original GR to $O\left(md\left(\log(d/m)+1\right)\right)$
without sacrificing the regret performance of FTPL.

</details>


### [194] [Machine Learning Methods for Small Data and Upstream Bioprocessing Applications: A Comprehensive Review](https://arxiv.org/abs/2506.12322)
*Johnny Peng,Thanh Tung Khuat,Katarzyna Musial,Bogdan Gabrys*

Main category: cs.LG

TL;DR: 该综述探讨解决小数据挑战的机器学习方法并分类，分析各方法核心概念与有效性，提供见解、指出研究差距并为数据受限环境提供指导。


<details>
  <summary>Details</summary>
Motivation: 在生物制药等领域获取大数据集成本高、耗时长，上游生物处理过程数据收集受限，需解决小数据挑战。

Method: 对解决小数据挑战的机器学习方法进行分类，分析各方法核心概念并评估其有效性。

Result: 通过在生物制药上游处理及相关领域的应用结果展示各方法解决小数据挑战的效果。

Conclusion: 提供可操作见解，识别当前研究差距，为数据受限环境下利用机器学习提供指导。

Abstract: Data is crucial for machine learning (ML) applications, yet acquiring large
datasets can be costly and time-consuming, especially in complex,
resource-intensive fields like biopharmaceuticals. A key process in this
industry is upstream bioprocessing, where living cells are cultivated and
optimised to produce therapeutic proteins and biologics. The intricate nature
of these processes, combined with high resource demands, often limits data
collection, resulting in smaller datasets. This comprehensive review explores
ML methods designed to address the challenges posed by small data and
classifies them into a taxonomy to guide practical applications. Furthermore,
each method in the taxonomy was thoroughly analysed, with a detailed discussion
of its core concepts and an evaluation of its effectiveness in tackling small
data challenges, as demonstrated by application results in the upstream
bioprocessing and other related domains. By analysing how these methods tackle
small data challenges from different perspectives, this review provides
actionable insights, identifies current research gaps, and offers guidance for
leveraging ML in data-constrained environments.

</details>


### [195] [Similarity as Reward Alignment: Robust and Versatile Preference-based Reinforcement Learning](https://arxiv.org/abs/2506.12529)
*Sara Rajaram,R. James Cotton,Fabian H. Sinz*

Main category: cs.LG

TL;DR: 提出SARA框架，可抵抗标签噪声，适应多样反馈格式与训练范式，在连续控制离线RL基准测试表现好且应用广泛。


<details>
  <summary>Details</summary>
Motivation: 以往偏好强化学习（PbRL）大多未研究对标注者错误的鲁棒性，且算法常针对特定设置。

Method: 引入相似性作为奖励对齐（SARA）对比框架，学习偏好样本的潜在表示，将奖励计算为与学习到的潜在表示的相似度。

Result: 在连续控制离线RL基准测试中比基线表现更好，在轨迹过滤、跨任务偏好转移和在线学习奖励塑造等应用中展示了多功能性。

Conclusion: SARA框架有效且具有广泛适应性。

Abstract: Preference-based Reinforcement Learning (PbRL) entails a variety of
approaches for aligning models with human intent to alleviate the burden of
reward engineering. However, most previous PbRL work has not investigated the
robustness to labeler errors, inevitable with labelers who are non-experts or
operate under time constraints. Additionally, PbRL algorithms often target very
specific settings (e.g. pairwise ranked preferences or purely offline
learning). We introduce Similarity as Reward Alignment (SARA), a simple
contrastive framework that is both resilient to noisy labels and adaptable to
diverse feedback formats and training paradigms. SARA learns a latent
representation of preferred samples and computes rewards as similarities to the
learned latent. We demonstrate strong performance compared to baselines on
continuous control offline RL benchmarks. We further demonstrate SARA's
versatility in applications such as trajectory filtering for downstream tasks,
cross-task preference transfer, and reward shaping in online learning.

</details>


### [196] [QiMeng-Attention: SOTA Attention Operator is generated by SOTA Attention Algorithm](https://arxiv.org/abs/2506.12355)
*Qirui Zhou,Shaohui Peng,Weiqiang Xiong,Haixin Chen,Yuanbo Wen,Haochen Li,Ling Li,Qi Guo,Yongwei Zhao,Ke Gao,Ruizhi Chen,Yanjun Wu,Chen Zhao,Yunji Chen*

Main category: cs.LG

TL;DR: 提出LLM - TL语言及两阶段推理工作流，让大模型自动生成不同GPU上的FlashAttention实现，性能远超普通大模型和人类优化库。


<details>
  <summary>Details</summary>
Motivation: 注意力算子是大语言模型性能瓶颈，FlashAttention手动实现耗时且架构适配性差，现有大模型难以生成高性能注意力代码。

Method: 提出LLM - TL语言帮助大模型解耦高级优化逻辑和GPU底层实现，结合TL - Code生成和翻译两阶段推理工作流。

Result: 在A100、RTX8000和T4 GPU上验证，性能比普通大模型最多快35.16倍，多数场景超人类优化库。

Conclusion: 所提方法能自动生成高性能注意力算子，减少开发时间，扩展硬件和数据类型支持。

Abstract: The attention operator remains a critical performance bottleneck in large
language models (LLMs), particularly for long-context scenarios. While
FlashAttention is the most widely used and effective GPU-aware acceleration
algorithm, it must require time-consuming and hardware-specific manual
implementation, limiting adaptability across GPU architectures. Existing LLMs
have shown a lot of promise in code generation tasks, but struggle to generate
high-performance attention code. The key challenge is it cannot comprehend the
complex data flow and computation process of the attention operator and utilize
low-level primitive to exploit GPU performance.
  To address the above challenge, we propose an LLM-friendly Thinking Language
(LLM-TL) to help LLMs decouple the generation of high-level optimization logic
and low-level implementation on GPU, and enhance LLMs' understanding of
attention operator. Along with a 2-stage reasoning workflow, TL-Code generation
and translation, the LLMs can automatically generate FlashAttention
implementation on diverse GPUs, establishing a self-optimizing paradigm for
generating high-performance attention operators in attention-centric
algorithms. Verified on A100, RTX8000, and T4 GPUs, the performance of our
methods significantly outshines that of vanilla LLMs, achieving a speed-up of
up to 35.16x. Besides, our method not only surpasses human-optimized libraries
(cuDNN and official library) in most scenarios but also extends support to
unsupported hardware and data types, reducing development time from months to
minutes compared with human experts.

</details>


### [197] [PLD: A Choice-Theoretic List-Wise Knowledge Distillation](https://arxiv.org/abs/2506.12542)
*Ejafa Bassam,Dawei Zhu,Kaigui Bian*

Main category: cs.LG

TL;DR: 本文从选择理论视角出发，提出基于Plackett - Luce模型的知识蒸馏方法PLD，在图像分类基准测试中提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于logit的知识蒸馏方法中蒸馏项多作为交叉熵的附加项，需仔细调整权重，希望提出更好的知识蒸馏方法。

Method: 采用选择理论视角，在Plackett - Luce模型下重新构建知识蒸馏，引入加权列表式排序损失PLD，直接优化教师模型的类排序。

Result: 在标准图像分类基准测试中，PLD在同构和异构设置下，Top - 1准确率相比DIST和KD均有提升。

Conclusion: PLD是一种有效的知识蒸馏方法，能提升图像分类的准确率。

Abstract: Knowledge distillation is a model compression technique in which a compact
"student" network is trained to replicate the predictive behavior of a larger
"teacher" network. In logit-based knowledge distillation it has become the de
facto approach to augment cross-entropy with a distillation term. Typically
this term is either a KL divergence-matching marginal probabilities or a
correlation-based loss capturing intra- and inter-class relationships but in
every case it sits as an add-on to cross-entropy with its own weight that must
be carefully tuned. In this paper we adopt a choice-theoretic perspective and
recast knowledge distillation under the Plackett-Luce model by interpreting
teacher logits as "worth" scores. We introduce Plackett-Luce Distillation
(PLD), a weighted list-wise ranking loss in which the teacher model transfers
knowledge of its full ranking of classes, weighting each ranked choice by its
own confidence. PLD directly optimizes a single teacher-optimal ranking of the
true label first, followed by the remaining classes in descending teacher
confidence, yielding a convex, translation-invariant surrogate that subsumes
weighted cross-entropy. Empirically on standard image classification
benchmarks, PLD improves Top-1 accuracy by an average of +0.42% over DIST
(arXiv:2205.10536) and +1.04% over KD (arXiv:1503.02531) in homogeneous
settings and by +0.48% and +1.09% over DIST and KD, respectively, in
heterogeneous settings.

</details>


### [198] [Relative Entropy Regularized Reinforcement Learning for Efficient Encrypted Policy Synthesis](https://arxiv.org/abs/2506.12358)
*Jihoon Suh,Yeongjun Jang,Kaoru Teranishi,Takashi Tanaka*

Main category: cs.LG

TL;DR: 提出用于隐私保护的基于模型强化学习的高效加密策略合成方法，分析收敛和误差界，数值模拟验证有效性。


<details>
  <summary>Details</summary>
Motivation: 开发隐私保护的基于模型强化学习。

Method: 利用相对熵正则化强化学习框架的线性和“无最小”结构，将全同态加密与自举集成到策略合成中。

Result: 数值模拟验证了相对熵正则化强化学习框架在集成全同态加密进行加密策略合成方面的有效性。

Conclusion: 相对熵正则化强化学习框架能有效集成全同态加密用于加密策略合成。

Abstract: We propose an efficient encrypted policy synthesis to develop
privacy-preserving model-based reinforcement learning. We first demonstrate
that the relative-entropy-regularized reinforcement learning framework offers a
computationally convenient linear and ``min-free'' structure for value
iteration, enabling a direct and efficient integration of fully homomorphic
encryption with bootstrapping into policy synthesis. Convergence and error
bounds are analyzed as encrypted policy synthesis propagates errors under the
presence of encryption-induced errors including quantization and bootstrapping.
Theoretical analysis is validated by numerical simulations. Results demonstrate
the effectiveness of the RERL framework in integrating FHE for encrypted policy
synthesis.

</details>


### [199] [Beyond Laplace and Gaussian: Exploring the Generalized Gaussian Mechanism for Private Machine Learning](https://arxiv.org/abs/2506.12553)
*Roy Rinberg,Ilia Shumailov,Vikrant Singhal,Rachel Cummings,Nicolas Papernot*

Main category: cs.LG

TL;DR: 本文研究广义高斯机制（GG）用于差分隐私，证明其满足差分隐私，扩展了隐私计算方法，应用于隐私机器学习工具，发现优化β对性能提升无显著意义。


<details>
  <summary>Details</summary>
Motivation: 差分隐私在效用和隐私间存在权衡，现有机制基于拉普拉斯和高斯加性噪声机制，扩大算法搜索空间。

Method: 研究广义高斯机制，证明其成员满足差分隐私，扩展PRV会计用于隐私计算，将其应用于PATE和DP - SGD。

Result: 隐私计算与维度无关，降低计算成本；β与测试精度关系弱，β = 2（高斯机制）接近最优。

Conclusion: 为高斯机制在差分隐私学习中的广泛应用提供依据，优化β对性能提升无实质意义。

Abstract: Differential privacy (DP) is obtained by randomizing a data analysis
algorithm, which necessarily introduces a tradeoff between its utility and
privacy. Many DP mechanisms are built upon one of two underlying tools: Laplace
and Gaussian additive noise mechanisms. We expand the search space of
algorithms by investigating the Generalized Gaussian mechanism, which samples
the additive noise term $x$ with probability proportional to $e^{-\frac{| x
|}{\sigma}^{\beta} }$ for some $\beta \geq 1$. The Laplace and Gaussian
mechanisms are special cases of GG for $\beta=1$ and $\beta=2$, respectively.
  In this work, we prove that all members of the GG family satisfy differential
privacy, and provide an extension of an existing numerical accountant (the PRV
accountant) for these mechanisms. We show that privacy accounting for the GG
Mechanism and its variants is dimension independent, which substantially
improves computational costs of privacy accounting.
  We apply the GG mechanism to two canonical tools for private machine
learning, PATE and DP-SGD; we show empirically that $\beta$ has a weak
relationship with test-accuracy, and that generally $\beta=2$ (Gaussian) is
nearly optimal. This provides justification for the widespread adoption of the
Gaussian mechanism in DP learning, and can be interpreted as a negative result,
that optimizing over $\beta$ does not lead to meaningful improvements in
performance.

</details>


### [200] [HYPER: A Foundation Model for Inductive Link Prediction with Knowledge Hypergraphs](https://arxiv.org/abs/2506.12362)
*Xingyue Huang,Mikhail Galkin,Michael M. Bronstein,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: 提出知识超图归纳链接预测基础模型HYPER，在新数据集上验证其泛化能力强，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识超图归纳链接预测方法无法泛化到含新关系类型的知识超图，需解决该问题。

Method: 提出HYPER模型，通过编码超边实体及其位置学习和迁移不同元数关系类型，构建16个新归纳数据集评估。

Result: HYPER在仅节点和节点与关系归纳设置中始终优于现有方法，对未见的高元关系结构有强泛化能力。

Conclusion: HYPER作为基础模型可泛化到含新实体和新关系的任何知识超图。

Abstract: Inductive link prediction with knowledge hypergraphs is the task of
predicting missing hyperedges involving completely novel entities (i.e., nodes
unseen during training). Existing methods for inductive link prediction with
knowledge hypergraphs assume a fixed relational vocabulary and, as a result,
cannot generalize to knowledge hypergraphs with novel relation types (i.e.,
relations unseen during training). Inspired by knowledge graph foundation
models, we propose HYPER as a foundation model for link prediction, which can
generalize to any knowledge hypergraph, including novel entities and novel
relations. Importantly, HYPER can learn and transfer across different relation
types of varying arities, by encoding the entities of each hyperedge along with
their respective positions in the hyperedge. To evaluate HYPER, we construct 16
new inductive datasets from existing knowledge hypergraphs, covering a diverse
range of relation types of varying arities. Empirically, HYPER consistently
outperforms all existing methods in both node-only and node-and-relation
inductive settings, showing strong generalization to unseen, higher-arity
relational structures.

</details>


### [201] [RAW-Explainer: Post-hoc Explanations of Graph Neural Networks on Knowledge Graphs](https://arxiv.org/abs/2506.12558)
*Ryoji Kubo,Djellel Difallah*

Main category: cs.LG

TL;DR: 提出RAW - Explainer框架为知识图谱链接预测生成可解释子图，平衡了解释质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在知识图谱链接预测解释方面，尤其是异构环境下的方法有限，需新方法。

Method: 利用知识图谱异构信息，通过随机游走目标识别连通子图；用神经网络参数化解释生成过程；提出鲁棒评估器克服分布偏移问题。

Result: 在真实知识图谱数据集上的大量定量结果显示，该方法平衡了解释质量和计算效率。

Conclusion: RAW - Explainer能为链接预测生成可解释子图，且在质量和效率间取得平衡。

Abstract: Graph neural networks have demonstrated state-of-the-art performance on
knowledge graph tasks such as link prediction. However, interpreting GNN
predictions remains a challenging open problem. While many GNN explainability
methods have been proposed for node or graph-level tasks, approaches for
generating explanations for link predictions in heterogeneous settings are
limited. In this paper, we propose RAW-Explainer, a novel framework designed to
generate connected, concise, and thus interpretable subgraph explanations for
link prediction. Our method leverages the heterogeneous information in
knowledge graphs to identify connected subgraphs that serve as patterns of
factual explanation via a random walk objective. Unlike existing methods
tailored to knowledge graphs, our approach employs a neural network to
parameterize the explanation generation process, which significantly speeds up
the production of collective explanations. Furthermore, RAW-Explainer is
designed to overcome the distribution shift issue when evaluating the quality
of an explanatory subgraph which is orders of magnitude smaller than the full
graph, by proposing a robust evaluator that generalizes to the subgraph
distribution. Extensive quantitative results on real-world knowledge graph
datasets demonstrate that our approach strikes a balance between explanation
quality and computational efficiency.

</details>


### [202] [Exploring the Secondary Risks of Large Language Models](https://arxiv.org/abs/2506.12382)
*Jiawei Chen,Zhengwei Fang,Xiao Yang,Chao Yu,Zhaoxia Yin,Hang Su*

Main category: cs.LG

TL;DR: 论文提出大语言模型二级风险，介绍风险原语，提出SecLens框架和SecRiskBench数据集，实验表明二级风险普遍存在，需加强安全机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型融入关键应用和社会功能，以往研究多关注越狱攻击，对良性交互中出现的非对抗性失败关注较少，故研究二级风险。

Method: 引入风险原语，提出SecLens黑盒多目标搜索框架，发布SecRiskBench基准数据集。

Result: 对16个流行模型的广泛评估实验显示，二级风险普遍存在、跨模型可转移且与模态无关。

Conclusion: 迫切需要增强安全机制，以解决大语言模型在现实部署中良性但有害的行为。

Abstract: Ensuring the safety and alignment of Large Language Models is a significant
challenge with their growing integration into critical applications and
societal functions. While prior research has primarily focused on jailbreak
attacks, less attention has been given to non-adversarial failures that subtly
emerge during benign interactions. We introduce secondary risks a novel class
of failure modes marked by harmful or misleading behaviors during benign
prompts. Unlike adversarial attacks, these risks stem from imperfect
generalization and often evade standard safety mechanisms. To enable systematic
evaluation, we introduce two risk primitives verbose response and speculative
advice that capture the core failure patterns. Building on these definitions,
we propose SecLens, a black-box, multi-objective search framework that
efficiently elicits secondary risk behaviors by optimizing task relevance, risk
activation, and linguistic plausibility. To support reproducible evaluation, we
release SecRiskBench, a benchmark dataset of 650 prompts covering eight diverse
real-world risk categories. Experimental results from extensive evaluations on
16 popular models demonstrate that secondary risks are widespread, transferable
across models, and modality independent, emphasizing the urgent need for
enhanced safety mechanisms to address benign yet harmful LLM behaviors in
real-world deployments.

</details>


### [203] [Existence of Adversarial Examples for Random Convolutional Networks via Isoperimetric Inequalities on $\mathbb{so}(d)$](https://arxiv.org/abs/2506.12613)
*Amit Daniely*

Main category: cs.LG

TL;DR: 证明了各种随机卷积网络中存在对抗样本，且可由特殊正交群上的等周不等式推出，简化并拓展了相关工作。


<details>
  <summary>Details</summary>
Motivation: 探索随机卷积网络中对抗样本的存在性，拓展随机全连接网络相关研究。

Method: 利用特殊正交群上的等周不等式进行推导。

Result: 证明了随机卷积网络中存在对抗样本。

Conclusion: 此结论是等周不等式的简单推论，简化并拓展了此前随机全连接网络的研究成果。

Abstract: We show that adversarial examples exist for various random convolutional
networks, and furthermore, that this is a relatively simple consequence of the
isoperimetric inequality on the special orthogonal group $\mathbb{so}(d)$. This
extends and simplifies a recent line of work which shows similar results for
random fully connected networks.

</details>


### [204] [EXGnet: a single-lead explainable-AI guided multiresolution network with train-only quantitative features for trustworthy ECG arrhythmia classification](https://arxiv.org/abs/2506.12404)
*Tushar Talukder Showrav,Soyabul Islam Lincoln,Md. Kamrul Hasan*

Main category: cs.LG

TL;DR: 提出EXGnet用于单导联心电图心律失常分类，在两个公开数据集表现优越，结合XAI技术增强可解释性，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在临床应用中存在可解释性和可靠性挑战，单导联心电图系统有应用需求。

Method: 提出EXGnet，集成多分辨率特征提取与可解释人工智能（XAI）指导，仅训练定量特征。

Result: 在Chapman和Ningbo数据集上，平均五折准确率分别达98.762%和96.932%，平均F1分数分别为97.910%和95.527%。

Conclusion: EXGnet结合XAI技术增强可解释性，定量特征提升性能且测试无需，适合实际应用，既提高分类准确率又满足可解释性需求。

Abstract: Background: Deep learning has significantly advanced ECG arrhythmia
classification, enabling high accuracy in detecting various cardiac conditions.
The use of single-lead ECG systems is crucial for portable devices, as they
offer convenience and accessibility for continuous monitoring in diverse
settings. However, the interpretability and reliability of deep learning models
in clinical applications poses challenges due to their black-box nature.
Methods: To address these challenges, we propose EXGnet, a single-lead,
trustworthy ECG arrhythmia classification network that integrates
multiresolution feature extraction with Explainable Artificial Intelligence
(XAI) guidance and train only quantitative features. Results: Trained on two
public datasets, including Chapman and Ningbo, EXGnet demonstrates superior
performance through key metrics such as Accuracy, F1-score, Sensitivity, and
Specificity. The proposed method achieved average five fold accuracy of
98.762%, and 96.932% and average F1-score of 97.910%, and 95.527% on the
Chapman and Ningbo datasets, respectively. Conclusions: By employing XAI
techniques, specifically Grad-CAM, the model provides visual insights into the
relevant ECG segments it analyzes, thereby enhancing clinician trust in its
predictions. While quantitative features further improve classification
performance, they are not required during testing, making the model suitable
for real-world applications. Overall, EXGnet not only achieves better
classification accuracy but also addresses the critical need for
interpretability in deep learning, facilitating broader adoption in portable
ECG monitoring.

</details>


### [205] [Wireless Channel Identification via Conditional Diffusion Model](https://arxiv.org/abs/2506.12419)
*Yuan Li,Zhong Zheng,Chang Liu,Zesong Fei*

Main category: cs.LG

TL;DR: 提出基于最大后验估计的无线信道场景识别方法，借条件生成扩散模型解决，实验显示比传统方法精度高超10%。


<details>
  <summary>Details</summary>
Motivation: 传统基于统计的信道场景识别方法无法准确区分动态散射体的隐式特征，在识别相似场景时表现差。

Method: 将识别任务表述为最大后验估计，再转化为最大似然估计，用条件生成扩散模型近似求解，利用Transformer网络捕获隐藏特征。

Result: 提出的方法优于CNN、BPNN和基于随机森林的分类器等传统方法，识别精度提高超10%。

Conclusion: 所提方法能有效提升无线信道场景识别的准确性。

Abstract: The identification of channel scenarios in wireless systems plays a crucial
role in channel modeling, radio fingerprint positioning, and transceiver
design. Traditional methods to classify channel scenarios are based on typical
statistical characteristics of channels, such as K-factor, path loss, delay
spread, etc. However, statistic-based channel identification methods cannot
accurately differentiate implicit features induced by dynamic scatterers, thus
performing very poorly in identifying similar channel scenarios. In this paper,
we propose a novel channel scenario identification method, formulating the
identification task as a maximum a posteriori (MAP) estimation. Furthermore,
the MAP estimation is reformulated by a maximum likelihood estimation (MLE),
which is then approximated and solved by the conditional generative diffusion
model. Specifically, we leverage a transformer network to capture hidden
channel features in multiple latent noise spaces within the reverse process of
the conditional generative diffusion model. These detailed features, which
directly affect likelihood functions in MLE, enable highly accurate scenario
identification. Experimental results show that the proposed method outperforms
traditional methods, including convolutional neural networks (CNNs),
back-propagation neural networks (BPNNs), and random forest-based classifiers,
improving the identification accuracy by more than 10%.

</details>


### [206] [Logit Dynamics in Softmax Policy Gradient Methods](https://arxiv.org/abs/2506.12912)
*Yingru Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We analyzes the logit dynamics of softmax policy gradient methods. We derive
the exact formula for the L2 norm of the logit update vector: $$ \|\Delta
\mathbf{z}\|_2 \propto \sqrt{1-2P_c + C(P)} $$ This equation demonstrates that
update magnitudes are determined by the chosen action's probability ($P_c$) and
the policy's collision probability ($C(P)$), a measure of concentration
inversely related to entropy. Our analysis reveals an inherent self-regulation
mechanism where learning vigor is automatically modulated by policy confidence,
providing a foundational insight into the stability and convergence of these
methods.

</details>


### [207] [Distributional Training Data Attribution](https://arxiv.org/abs/2506.12965)
*Bruno Mlodozeniec,Isaac Reid,Sam Power,David Krueger,Murat Erdogdu,Richard E. Turner,Roger Grosse*

Main category: cs.LG

TL;DR: 本文引入分布训练数据归因（d - TDA）解决传统训练数据归因算法未考虑随机性的问题，通过实验证明其意义，并揭示影响函数与d - TDA的联系。


<details>
  <summary>Details</summary>
Motivation: 传统训练数据归因算法未严格考虑深度学习模型训练中的随机性，相同数据集因初始化和批处理的随机性会产生不同模型。

Method: 引入分布训练数据归因（d - TDA），预测模型输出分布与数据集的关系。

Result: 实验证明d - TDA的实际意义，如识别不改变均值但大幅改变目标测量分布的训练示例；发现影响函数自然地从分布框架中作为展开微分的极限出现。

Conclusion: d - TDA解决了传统算法的不足，为影响函数在深度学习中的有效性提供新的数学动机，并有助于刻画其局限性。

Abstract: Randomness is an unavoidable part of training deep learning models, yet
something that traditional training data attribution algorithms fail to
rigorously account for. They ignore the fact that, due to stochasticity in the
initialisation and batching, training on the same dataset can yield different
models. In this paper, we address this shortcoming through introducing
distributional training data attribution (d-TDA), the goal of which is to
predict how the distribution of model outputs (over training runs) depends upon
the dataset. We demonstrate the practical significance of d-TDA in experiments,
e.g. by identifying training examples that drastically change the distribution
of some target measurement without necessarily changing the mean. Intriguingly,
we also find that influence functions (IFs), a popular but poorly-understood
data attribution tool, emerge naturally from our distributional framework as
the limit to unrolled differentiation; without requiring restrictive convexity
assumptions. This provides a new mathematical motivation for their efficacy in
deep learning, and helps to characterise their limitations.

</details>


### [208] [CoIFNet: A Unified Framework for Multivariate Time Series Forecasting with Missing Values](https://arxiv.org/abs/2506.13064)
*Kai Tang,Ji Zhang,Hua Meng,Minbo Ma,Qi Xiong,Jie Xu,Tianrui Li*

Main category: cs.LG

TL;DR: 本文提出CoIFNet框架统一多变量时间序列的插补和预测，在有缺失值情况下实现鲁棒预测，实验证明其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测方法在处理缺失值时采用插补-预测范式，存在误差累积和目标不一致问题，影响预测准确性。

Method: 提出CoIFNet框架，以观测值、掩码矩阵和时间戳嵌入为输入，通过CTF和CVF模块处理，捕捉对缺失值鲁棒的时间依赖关系，并给出理论证明。

Result: 在具有挑战性的MSTF基准测试中，CoIFNet在不同缺失数据场景下均有效，在缺失率0.6时优于现有方法，同时提高内存和时间效率。

Conclusion: CoIFNet能有效解决多变量时间序列预测中缺失值问题，实现鲁棒预测，且具有计算效率。

Abstract: Multivariate time series forecasting (MTSF) is a critical task with broad
applications in domains such as meteorology, transportation, and economics.
Nevertheless, pervasive missing values caused by sensor failures or human
errors significantly degrade forecasting accuracy. Prior efforts usually employ
an impute-then-forecast paradigm, leading to suboptimal predictions due to
error accumulation and misaligned objectives between the two stages. To address
this challenge, we propose the Collaborative Imputation-Forecasting Network
(CoIFNet), a novel framework that unifies imputation and forecasting to achieve
robust MTSF in the presence of missing values. Specifically, CoIFNet takes the
observed values, mask matrix and timestamp embeddings as input, processing them
sequentially through the Cross-Timestep Fusion (CTF) and Cross-Variate Fusion
(CVF) modules to capture temporal dependencies that are robust to missing
values. We provide theoretical justifications on how our CoIFNet learning
objective improves the performance bound of MTSF with missing values. Through
extensive experiments on challenging MSTF benchmarks, we demonstrate the
effectiveness and computational efficiency of our proposed approach across
diverse missing-data scenarios, e.g., CoIFNet outperforms the state-of-the-art
method by $\underline{\textbf{24.40}}$% ($\underline{\textbf{23.81}}$%) at a
point (block) missing rate of 0.6, while improving memory and time efficiency
by $\underline{\boldsymbol{4.3\times}}$ and
$\underline{\boldsymbol{2.1\times}}$, respectively.

</details>


### [209] [Generalizable Trajectory Prediction via Inverse Reinforcement Learning with Mamba-Graph Architecture](https://arxiv.org/abs/2506.12474)
*Wenyun Li,Wenjie Huang,Zejian Deng,Chen Sun*

Main category: cs.LG

TL;DR: 提出新颖的逆强化学习框架用于驾驶行为建模，在准确性和泛化性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 准确的驾驶行为建模在复杂交通场景有挑战，需更好方法。

Method: 提出逆强化学习框架，用编码器 - 解码器架构结合Mamba块和图注意力网络。

Result: 在城市十字路口和环形交叉路口评估，预测准确性超流行方法，泛化性能比其他基于IRL的方法高2倍。

Conclusion: 所提方法在驾驶行为建模上效果好，有较强跨场景适应性。

Abstract: Accurate driving behavior modeling is fundamental to safe and efficient
trajectory prediction, yet remains challenging in complex traffic scenarios.
This paper presents a novel Inverse Reinforcement Learning (IRL) framework that
captures human-like decision-making by inferring diverse reward functions,
enabling robust cross-scenario adaptability. The learned reward function is
utilized to maximize the likelihood of output by the encoder-decoder
architecture that combines Mamba blocks for efficient long-sequence dependency
modeling with graph attention networks to encode spatial interactions among
traffic agents. Comprehensive evaluations on urban intersections and
roundabouts demonstrate that the proposed method not only outperforms various
popular approaches in prediction accuracy but also achieves 2 times higher
generalization performance to unseen scenarios compared to other IRL-based
method.

</details>


### [210] [Honesty in Causal Forests: When It Helps and When It Hurts](https://arxiv.org/abs/2506.13107)
*Yanfang Hou,Carlos Fernández-Loría*

Main category: cs.LG

TL;DR: 因果森林常用于个性化决策，但诚实估计虽普遍被视为有益，却可能损害个体层面效应估计的准确性，其效果取决于信噪比。


<details>
  <summary>Details</summary>
Motivation: 探究因果森林中诚实估计方法对个体层面效应估计准确性的影响。

Method: 分析诚实估计带来的偏差 - 方差权衡，并结合信噪比分析其影响。

Result: 诚实估计在效应异质性难以检测（低信噪比）时有益，信号强（高信噪比）时有害。

Conclusion: 诚实估计如同正则化，应根据样本外性能选择，而非默认采用。

Abstract: Causal forests are increasingly used to personalize decisions based on
estimated treatment effects. A distinctive modeling choice in this method is
honest estimation: using separate data for splitting and for estimating effects
within leaves. This practice is the default in most implementations and is
widely seen as desirable for causal inference. But we show that honesty can
hurt the accuracy of individual-level effect estimates. The reason is a classic
bias-variance trade-off: honesty reduces variance by preventing overfitting,
but increases bias by limiting the model's ability to discover and exploit
meaningful heterogeneity in treatment effects. This trade-off depends on the
signal-to-noise ratio (SNR): honesty helps when effect heterogeneity is hard to
detect (low SNR), but hurts when the signal is strong (high SNR). In essence,
honesty acts as a form of regularization, and like any regularization choice,
it should be guided by out-of-sample performance, not adopted by default.

</details>


### [211] [Quantizing Small-Scale State-Space Models for Edge AI](https://arxiv.org/abs/2506.12480)
*Leo Zhao,Tristan Torchet,Melika Payvand,Laura Kriener,Filippo Moro*

Main category: cs.LG

TL;DR: 本文聚焦小尺度状态空间模型（SSMs）量化，分析不同量化技术影响，用QAT提升性能，提出异构量化策略减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 分析量化对小尺度SSMs的影响，在降低内存和计算成本的同时保持任务性能，以应用于资源受限环境。

Method: 使用S4D架构，先研究训练后量化（PTQ），再应用量化感知训练（QAT），还评估不同参数化方案，提出异构量化策略。

Result: QAT将顺序MNIST基准测试性能从40%（PTQ）提升到96%（8位精度），能实现低于8位精度，异构量化策略将整体内存占用减少6倍且不损失性能。

Conclusion: 研究结果为在资源受限环境中部署量化SSMs提供了可行的见解。

Abstract: State-space models (SSMs) have recently gained attention in deep learning for
their ability to efficiently model long-range dependencies, making them
promising candidates for edge-AI applications. In this paper, we analyze the
effects of quantization on small-scale SSMs with a focus on reducing memory and
computational costs while maintaining task performance. Using the S4D
architecture, we first investigate post-training quantization (PTQ) and show
that the state matrix A and internal state x are particularly sensitive to
quantization. Furthermore, we analyze the impact of different quantization
techniques applied to the parameters and activations in the S4D architecture.
To address the observed performance drop after Post-training Quantization
(PTQ), we apply Quantization-aware Training (QAT), significantly improving
performance from 40% (PTQ) to 96% on the sequential MNIST benchmark at 8-bit
precision. We further demonstrate the potential of QAT in enabling sub-8-bit
precisions and evaluate different parameterization schemes for QAT stability.
Additionally, we propose a heterogeneous quantization strategy that assigns
different precision levels to model components, reducing the overall memory
footprint by a factor of 6x without sacrificing performance. Our results
provide actionable insights for deploying quantized SSMs in
resource-constrained environments.

</details>


### [212] [SAGDA: Open-Source Synthetic Agriculture Data for Africa](https://arxiv.org/abs/2506.13123)
*Abdelghani Belgaid,Oumnia Ennaji*

Main category: cs.LG

TL;DR: 非洲农业数据稀缺影响机器学习模型性能，SAGDA库可生成、增强和验证合成农业数据集，介绍其设计、核心功能及两个用例，还提及未来计划。


<details>
  <summary>Details</summary>
Motivation: 解决非洲农业数据稀缺导致机器学习模型性能受限，阻碍精准农业创新的问题。

Method: 开发基于Python的开源工具包SAGDA，介绍其设计和核心功能。

Result: 通过SAGDA实现了数据增强提升产量预测、多目标NPK肥料推荐两个用例。

Conclusion: 强调开源、数据驱动实践对非洲农业的重要性，提出扩展SAGDA能力的未来计划。

Abstract: Data scarcity in African agriculture hampers machine learning (ML) model
performance, limiting innovations in precision agriculture. The Synthetic
Agriculture Data for Africa (SAGDA) library, a Python-based open-source
toolkit, addresses this gap by generating, augmenting, and validating synthetic
agricultural datasets. We present SAGDA's design and development practices,
highlighting its core functions: generate, model, augment, validate, visualize,
optimize, and simulate, as well as their roles in applications of ML for
agriculture. Two use cases are detailed: yield prediction enhanced via data
augmentation, and multi-objective NPK (nitrogen, phosphorus, potassium)
fertilizer recommendation. We conclude with future plans for expanding SAGDA's
capabilities, underscoring the vital role of open-source, data-driven practices
for African agriculture.

</details>


### [213] [Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization](https://arxiv.org/abs/2506.12484)
*Filip Sondej,Yushi Yang,Mikołaj Kniejski,Marcel Windys*

Main category: cs.LG

TL;DR: 本文系统评估去学习方法组件，提出Disruption Masking技术，结合见解形成MUDMAN方法，在防止危险能力恢复上表现优异。


<details>
  <summary>Details</summary>
Motivation: 语言模型经安全微调后仍保留危险知识技能，现有去学习方法易被逆转，存在滥用和失准风险。

Method: 系统评估现有和新的去学习方法组件，引入Disruption Masking技术，确定归一化去学习梯度的必要性和元学习的有用性，结合形成MUDMAN方法。

Result: MUDMAN方法比之前的TAR方法性能高40%，创造了鲁棒去学习的新水平。

Conclusion: MUDMAN方法在防止危险能力恢复方面有效，可用于实现不可逆的去学习。

Abstract: Language models can retain dangerous knowledge and skills even after
extensive safety fine-tuning, posing both misuse and misalignment risks. Recent
studies show that even specialized unlearning methods can be easily reversed.
To address this, we systematically evaluate many existing and novel components
of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating
weights, where the signs of the unlearning gradient and the retaining gradient
are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients,
and also confirm the usefulness of meta-learning. We combine these insights
into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and
validate its effectiveness at preventing the recovery of dangerous
capabilities. MUDMAN outperforms the prior TAR method by 40\%, setting a new
state-of-the-art for robust unlearning.

</details>


### [214] [Federated ADMM from Bayesian Duality](https://arxiv.org/abs/2506.13150)
*Thomas Möllenhoff,Siddharth Swaroop,Finale Doshi-Velez,Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: 提出用贝叶斯对偶结构推导和扩展联邦ADMM，可处理联邦深度学习异质性，提升准确率，开辟贝叶斯改进原 - 对偶方法新路径。


<details>
  <summary>Details</summary>
Motivation: 对原有的联邦ADMM核心算法结构进行改进，以更好处理联邦深度学习中的问题。

Method: 使用贝叶斯对偶结构，通过变分 - 贝叶斯重新表述原问题，利用后验分布的对偶性。

Result: 使用各向同性高斯后验可恢复原ADMM，其他后验形式产生非平凡扩展，如全协方差高斯产生类牛顿变体，对角协方差产生类Adam变体，处理异质性时准确率比基线提升达7%。

Conclusion: 开辟了改进原 - 对偶方法的新贝叶斯路径。

Abstract: ADMM is a popular method for federated deep learning which originated in the
1970s and, even though many new variants of it have been proposed since then,
its core algorithmic structure has remained unchanged. Here, we take a major
departure from the old structure and present a fundamentally new way to derive
and extend federated ADMM. We propose to use a structure called Bayesian
Duality which exploits a duality of the posterior distributions obtained by
solving a variational-Bayesian reformulation of the original problem. We show
that this naturally recovers the original ADMM when isotropic Gaussian
posteriors are used, and yields non-trivial extensions for other posterior
forms. For instance, full-covariance Gaussians lead to Newton-like variants of
ADMM, while diagonal covariances result in a cheap Adam-like variant. This is
especially useful to handle heterogeneity in federated deep learning, giving up
to 7% accuracy improvements over recent baselines. Our work opens a new
Bayesian path to improve primal-dual methods.

</details>


### [215] [No-Regret Learning Under Adversarial Resource Constraints: A Spending Plan Is All You Need!](https://arxiv.org/abs/2506.13244)
*Francesco Emanuele Stradi,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti,Christian Kroer*

Main category: cs.LG

TL;DR: 研究资源约束下在线决策问题，设计对偶方法在遵循支出计划的基线实现次线性遗憾，并提供鲁棒变体，还研究算法与偏离支出计划基准竞争时的遗憾。


<details>
  <summary>Details</summary>
Motivation: 奖励和成本分布随时间任意变化时，在资源约束在线决策问题中实现次线性遗憾是不可能的，需要解决此挑战。

Method: 分析由支出计划引导的框架，设计通用（原 - 对偶）方法。

Result: 设计的算法能在遵循支出计划的基线实现次线性遗憾，支出计划使预算分布均衡时算法性能提升，还提供了鲁棒变体。

Conclusion: 研究了算法与偏离规定支出计划的基准竞争时的遗憾。

Abstract: We study online decision making problems under resource constraints, where
both reward and cost functions are drawn from distributions that may change
adversarially over time. We focus on two canonical settings: $(i)$ online
resource allocation where rewards and costs are observed before action
selection, and $(ii)$ online learning with resource constraints where they are
observed after action selection, under full feedback or bandit feedback. It is
well known that achieving sublinear regret in these settings is impossible when
reward and cost distributions may change arbitrarily over time. To address this
challenge, we analyze a framework in which the learner is guided by a spending
plan--a sequence prescribing expected resource usage across rounds. We design
general (primal-)dual methods that achieve sublinear regret with respect to
baselines that follow the spending plan. Crucially, the performance of our
algorithms improves when the spending plan ensures a well-balanced distribution
of the budget across rounds. We additionally provide a robust variant of our
methods to handle worst-case scenarios where the spending plan is highly
imbalanced. To conclude, we study the regret of our algorithms when competing
against benchmarks that deviate from the prescribed spending plan.

</details>


### [216] [BSA: Ball Sparse Attention for Large-scale Geometries](https://arxiv.org/abs/2506.12541)
*Catalin E. Brita,Hieu Nguyen,Lohithsai Yadala Chanchu,Domonkos Nagy,Maksim Zhdanov*

Main category: cs.LG

TL;DR: 本文提出球稀疏注意力机制（BSA），能以低于二次方的成本实现全局感受野，在气流压力预测任务上效果与全注意力相当且降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制随输入规模增大计算量呈二次方增长，现有稀疏注意力机制不适用于不规则几何结构，需新方法处理大规模物理系统。

Method: 通过引入球树结构，将原生稀疏注意力（NSA）应用于无序点集，修改NSA组件使其适用于基于球的邻域。

Result: 在气流压力预测任务上达到与全注意力相当的准确性，显著降低理论计算复杂度。

Conclusion: 提出的BSA机制有效，可在不规则几何结构下处理大规模物理系统问题，代码已开源。

Abstract: Self-attention scales quadratically with input size, limiting its use for
large-scale physical systems. Although sparse attention mechanisms provide a
viable alternative, they are primarily designed for regular structures such as
text or images, making them inapplicable for irregular geometries. In this
work, we present Ball Sparse Attention (BSA), which adapts Native Sparse
Attention (NSA) (Yuan et al., 2025) to unordered point sets by imposing
regularity using the Ball Tree structure from the Erwin Transformer (Zhdanov et
al., 2025). We modify NSA's components to work with ball-based neighborhoods,
yielding a global receptive field at sub-quadratic cost. On an airflow pressure
prediction task, we achieve accuracy comparable to Full Attention while
significantly reducing the theoretical computational complexity. Our
implementation is available at https://github.com/britacatalin/bsa.

</details>


### [217] [Vine Copulas as Differentiable Computational Graphs](https://arxiv.org/abs/2506.13318)
*Tuoyuan Cheng,Thibault Vatter,Thomas Nagler,Kan Chen*

Main category: cs.LG

TL;DR: 引入藤蔓计算图，开发新算法并实现torchvinecopulib库，实验显示优势，连接经典依赖建模与现代深度学习。


<details>
  <summary>Details</summary>
Motivation: 促进藤蔓copulas模型融入现代机器学习管道。

Method: 引入藤蔓计算图，设计新算法，实现GPU加速的torchvinecopulib库。

Result: 实验表明梯度流经藤蔓可改进Vine Copula Autoencoders，在不确定性量化方面优于其他方法。

Conclusion: 将藤蔓copula模型重铸为计算图，连接经典与现代方法，便于先进copula方法融入机器学习管道。

Abstract: Vine copulas are sophisticated models for multivariate distributions and are
increasingly used in machine learning. To facilitate their integration into
modern ML pipelines, we introduce the vine computational graph, a DAG that
abstracts the multilevel vine structure and associated computations. On this
foundation, we devise new algorithms for conditional sampling, efficient
sampling-order scheduling, and constructing vine structures for customized
conditioning variables. We implement these ideas in torchvinecopulib, a
GPU-accelerated Python library built upon PyTorch, delivering improved
scalability for fitting, sampling, and density evaluation. Our experiments
illustrate how gradient flowing through the vine can improve Vine Copula
Autoencoders and that incorporating vines for uncertainty quantification in
deep learning can outperform MC-dropout, deep ensembles, and Bayesian Neural
Networks in sharpness, calibration, and runtime. By recasting vine copula
models as computational graphs, our work connects classical dependence modeling
with modern deep-learning toolchains and facilitates the integration of
state-of-the-art copula methods in modern machine learning pipelines.

</details>


### [218] [Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs](https://arxiv.org/abs/2506.13593)
*Hen Davidov,Gilad Freidkin,Shai Feldman,Yaniv Romano*

Main category: cs.LG

TL;DR: 提出量化大语言模型生成不安全响应所需次数的框架，采用生存分析和共形预测技术，设计自适应采样策略，实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 直接估计大语言模型生成不安全响应的次数需大量训练数据，实际采样预算难以生成足够响应来观测不安全结果，导致估计和评估困难。

Method: 将估计问题转化为生存分析，利用共形预测开发可证明校准的下预测界，设计自适应、逐提示采样策略，将其表述为凸优化问题。

Result: 实验在合成和真实数据上支持了理论结果，证明了该方法在生成式AI模型安全风险评估中的实用价值。

Conclusion: 所提方法有效，能提高统计效率，可用于生成式AI模型的安全风险评估。

Abstract: We develop a framework to quantify the time-to-unsafe-sampling - the number
of large language model (LLM) generations required to trigger an unsafe (e.g.,
toxic) response. Estimating this quantity is challenging, since unsafe
responses are exceedingly rare in well-aligned LLMs, potentially occurring only
once in thousands of generations. As a result, directly estimating
time-to-unsafe-sampling would require collecting training data with a
prohibitively large number of generations per prompt. However, with realistic
sampling budgets, we often cannot generate enough responses to observe an
unsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved
in many cases, making the estimation and evaluation tasks particularly
challenging. To address this, we frame this estimation problem as one of
survival analysis and develop a provably calibrated lower predictive bound
(LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent
advances in conformal prediction. Our key innovation is designing an adaptive,
per-prompt sampling strategy, formulated as a convex optimization problem. The
objective function guiding this optimized sampling allocation is designed to
reduce the variance of the estimators used to construct the LPB, leading to
improved statistical efficiency over naive methods that use a fixed sampling
budget per prompt. Experiments on both synthetic and real data support our
theoretical results and demonstrate the practical utility of our method for
safety risk assessment in generative AI models.

</details>


### [219] [Is your batch size the problem? Revisiting the Adam-SGD gap in language modeling](https://arxiv.org/abs/2506.12543)
*Teodora Srećković,Jonas Geiping,Antonio Orvieto*

Main category: cs.LG

TL;DR: 通过对Transformer语言模型训练基线调优，研究动量、梯度裁剪和批量大小对SGD和Adam优化器差距的影响，发现小批量下调优的SGD带动量可与Adam性能相近，并给出批量大小对训练动态影响的新见解。


<details>
  <summary>Details</summary>
Motivation: 重新审视语言模型中SGD和Adam的‘优化器差距’现象，探索影响差距的因素及Adam优势的解释。

Method: 对Transformer语言模型进行一系列全面调优的基线训练，研究动量、梯度裁剪和批量大小的影响，分析训练运行和简单二次设置。

Result: 小批量设置下，调优的带动量SGD能达到与Adam相近的性能，现有对Adam优势的解释难以直接说明该现象。

Conclusion: 基于随机微分方程模型，对批量大小在训练动态中的作用给出新见解。

Abstract: Adam is known to perform significantly better than Stochastic Gradient
Descent (SGD) in language models, a phenomenon for which a number of
explanations have been proposed. In this work, we revisit this "optimizer gap"
through a series of comprehensively tuned baseline training runs for language
modeling with Transformers. We exhaustively study how momentum, gradient
clipping, and batch size affect the gap between SGD and Adam. Our empirical
findings show that SGD with momentum can actually perform similarly to Adam in
small-batch settings, if tuned correctly. We revisit existing explanations for
Adam's advantage, including heavy-tailed class imbalance, directional
sharpness, and Hessian heterogeneity, which struggle to directly explain this
phenomenon. Towards bridging this gap in our understanding, by analyzing our
Transformer training runs and simple quadratic settings inspired by the
literature, we provide new insights, driven by stochastic differential equation
models, into the role of batch size on the training dynamics.

</details>


### [220] [PeakWeather: MeteoSwiss Weather Station Measurements for Spatiotemporal Deep Learning](https://arxiv.org/abs/2506.13652)
*Daniele Zambon,Michele Cattaneo,Ivan Marisca,Jonas Bhend,Daniele Nerini,Cesare Alippi*

Main category: cs.LG

TL;DR: 介绍PeakWeather数据集，其用于支持气象时空任务，可作为真实世界基准。


<details>
  <summary>Details</summary>
Motivation: 准确天气预报很重要，机器学习是有潜力的预报方式，需高质量数据集支持研究。

Method: 收集超过8年、每10分钟一次的地面气象观测数据，结合地形指数，提供高分辨率数值天气预报模型的集合预报作为基线。

Result: 形成PeakWeather数据集，支持多种时空任务。

Conclusion: PeakWeather可作为真实世界基准，推动机器学习、气象学和传感器应用发展。

Abstract: Accurate weather forecasts are essential for supporting a wide range of
activities and decision-making processes, as well as mitigating the impacts of
adverse weather events. While traditional numerical weather prediction (NWP)
remains the cornerstone of operational forecasting, machine learning is
emerging as a powerful alternative for fast, flexible, and scalable
predictions. We introduce PeakWeather, a high-quality dataset of surface
weather observations collected every 10 minutes over more than 8 years from the
ground stations of the Federal Office of Meteorology and Climatology
MeteoSwiss's measurement network. The dataset includes a diverse set of
meteorological variables from 302 station locations distributed across
Switzerland's complex topography and is complemented with topographical indices
derived from digital height models for context. Ensemble forecasts from the
currently operational high-resolution NWP model are provided as a baseline
forecast against which to evaluate new approaches. The dataset's richness
supports a broad spectrum of spatiotemporal tasks, including time series
forecasting at various scales, graph structure learning, imputation, and
virtual sensing. As such, PeakWeather serves as a real-world benchmark to
advance both foundational machine learning research, meteorology, and
sensor-based applications.

</details>


### [221] [Fairness Research For Machine Learning Should Integrate Societal Considerations](https://arxiv.org/abs/2506.12556)
*Yijun Bian,Lei You*

Main category: cs.LG

TL;DR: 强调提升机器学习系统公平性重要性，指出当前研究不足并给出理由


<details>
  <summary>Details</summary>
Motivation: 如今提升机器学习系统公平性愈发重要，而当前研究存在不足

Method: 无

Result: 指出当前对合理定义公平性指标的重要性认识不足，且机器学习公平性研究应纳入社会考量

Conclusion: 因机器学习系统广泛部署和人机反馈循环放大偏见，检测歧视至关重要

Abstract: Enhancing fairness in machine learning (ML) systems is increasingly important
nowadays. While current research focuses on assistant tools for ML pipelines to
promote fairness within them, we argue that: 1) The significance of properly
defined fairness measures remains underestimated; and 2) Fairness research in
ML should integrate societal considerations. The reasons include that detecting
discrimination is critical due to the widespread deployment of ML systems and
that human-AI feedback loops amplify biases, even when only small social and
political biases persist.

</details>


### [222] [Are We Really Measuring Progress? Transferring Insights from Evaluating Recommender Systems to Temporal Link Prediction](https://arxiv.org/abs/2506.12588)
*Filip Cornell,Oleg Smirnov,Gabriela Zarzar Gandler,Lele Cao*

Main category: cs.LG

TL;DR: 本文聚焦时序链接预测（TLP）评估策略，指出当前评估协议存在的问题，正系统性研究并探索替代方案，最后讨论改进TLP基准可靠性的方向。


<details>
  <summary>Details</summary>
Motivation: 针对图学习基准可靠性受质疑的情况，关注TLP评估策略的可靠性问题。

Method: 通过示例和与推荐系统领域问题的联系来支持提出的当前评估协议存在的问题。

Result: 发现当前评估协议存在采样指标不一致、依赖硬负采样、指标隐含假设源节点基础概率相等的问题。

Conclusion: 对改进TLP基准可靠性的潜在方向进行了讨论。

Abstract: Recent work has questioned the reliability of graph learning benchmarks,
citing concerns around task design, methodological rigor, and data suitability.
In this extended abstract, we contribute to this discussion by focusing on
evaluation strategies in Temporal Link Prediction (TLP). We observe that
current evaluation protocols are often affected by one or more of the following
issues: (1) inconsistent sampled metrics, (2) reliance on hard negative
sampling often introduced as a means to improve robustness, and (3) metrics
that implicitly assume equal base probabilities across source nodes by
combining predictions. We support these claims through illustrative examples
and connections to longstanding concerns in the recommender systems community.
Our ongoing work aims to systematically characterize these problems and explore
alternatives that can lead to more robust and interpretable evaluation. We
conclude with a discussion of potential directions for improving the
reliability of TLP benchmarks.

</details>


### [223] [What Happens During the Loss Plateau? Understanding Abrupt Learning in Transformers](https://arxiv.org/abs/2506.13688)
*Pulkit Gopalani,Wei Hu*

Main category: cs.LG

TL;DR: 研究浅层Transformer在算法任务训练中的突然学习现象，揭示相关机制并验证其在大语言模型早期预训练中存在。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer在算法任务训练中突然学习现象（长时间性能平稳后突然大幅提升）的潜在机制。

Method: 研究浅层Transformer，分析其在性能平稳期的输出和内部表征，识别注意力图学习瓶颈，并进行干预实验。

Result: 发现模型在平稳期形成可解释的部分解决方案，输出有重复偏差、内部表征坍塌，慢学习的最优注意力图是关键瓶颈，干预注意力会改变平稳期和偏差、坍塌程度。

Conclusion: 所发现的重复偏差和表征坍塌现象不是简单设置的产物，在大语言模型如Pythia和OLMo的早期预训练阶段也存在。

Abstract: Training Transformers on algorithmic tasks frequently demonstrates an
intriguing abrupt learning phenomenon: an extended performance plateau followed
by a sudden, sharp improvement. This work investigates the underlying
mechanisms for such dynamics, primarily in shallow Transformers. We reveal that
during the plateau, the model often develops an interpretable partial solution
while simultaneously exhibiting a strong repetition bias in their outputs. This
output degeneracy is accompanied by internal representation collapse, where
hidden states across different tokens become nearly parallel. We further
identify the slow learning of optimal attention maps as a key bottleneck.
Hidden progress in attention configuration during the plateau precedes the
eventual rapid convergence, and directly intervening on attention significantly
alters plateau duration and the severity of repetition bias and
representational collapse. We validate that these identified
phenomena-repetition bias and representation collapse-are not artifacts of toy
setups but also manifest in the early pre-training stage of large language
models like Pythia and OLMo.

</details>


### [224] [Automatic Expert Discovery in LLM Upcycling via Sparse Interpolated Mixture-of-Experts](https://arxiv.org/abs/2506.12597)
*Shengzhuang Chen,Ying Wei,Jonathan Richard Schwarz*

Main category: cs.LG

TL;DR: 提出SIMoE指令微调算法，将预训练大模型转为MoE模型，在基准测试表现优。


<details>
  <summary>Details</summary>
Motivation: 将密集预训练大语言模型微调为具备多专业领域能力的MoE风格模型。

Method: 在指令微调时，在稀疏约束下自动识别多个专家，每个专家代表种子LLM参数的稀疏子集，同时通过路由网络学习输入依赖的专家合并策略。

Result: 在常见指令微调基准测试中持续达到最优性能，在性能与计算权衡上优于所有基线。

Conclusion: SIMoE算法有效，能在多专业领域表现良好并维持性能与计算的最优平衡。

Abstract: We present Sparse Interpolated Mixture-of-Experts (SIMoE) instruction-tuning,
an end-to-end algorithm designed to fine-tune a dense pre-trained Large
Language Model (LLM) into a MoE-style model that possesses capabilities in
multiple specialized domains. During instruction-tuning, SIMoE automatically
identifies multiple specialized experts under a specified sparsity constraint,
with each expert representing a structurally sparse subset of the seed LLM's
parameters that correspond to domain-specific knowledge within the data. SIMoE
simultaneously learns an input-dependent expert merging strategy via a router
network, leveraging rich cross-expert knowledge for superior downstream
generalization that surpasses existing baselines. Empirically, SIMoE
consistently achieves state-of-the-art performance on common instruction-tuning
benchmarks while maintaining an optimal performance-compute trade-off compared
to all baselines.

</details>


### [225] [Contrastive Self-Supervised Learning As Neural Manifold Packing](https://arxiv.org/abs/2506.13717)
*Guanming Zhang,David J. Heeger,Stefano Martiniani*

Main category: cs.LG

TL;DR: 提出CLAMP自监督框架，将表征学习转化为流形打包问题，在标准线性评估协议下表现有竞争力，揭示不同类别神经流形可自然分离。


<details>
  <summary>Details</summary>
Motivation: 受大脑视觉皮层神经流形启发，解决视觉任务中表征学习问题，结合物理、神经科学和机器学习的见解。

Method: 引入CLAMP框架，采用受短程排斥粒子系统势能启发的损失函数，动态优化子流形大小和位置。

Result: 在标准线性评估协议下，CLAMP与现有最先进的自监督模型表现相当。

Conclusion: CLAMP有潜力桥接物理、神经科学和机器学习的见解，有效分离不同类别的神经流形。

Abstract: Contrastive self-supervised learning based on point-wise comparisons has been
widely studied for vision tasks. In the visual cortex of the brain, neuronal
responses to distinct stimulus classes are organized into geometric structures
known as neural manifolds. Accurate classification of stimuli can be achieved
by effectively separating these manifolds, akin to solving a packing problem.
We introduce Contrastive Learning As Manifold Packing (CLAMP), a
self-supervised framework that recasts representation learning as a manifold
packing problem. CLAMP introduces a loss function inspired by the potential
energy of short-range repulsive particle systems, such as those encountered in
the physics of simple liquids and jammed packings. In this framework, each
class consists of sub-manifolds embedding multiple augmented views of a single
image. The sizes and positions of the sub-manifolds are dynamically optimized
by following the gradient of a packing loss. This approach yields interpretable
dynamics in the embedding space that parallel jamming physics, and introduces
geometrically meaningful hyperparameters within the loss function. Under the
standard linear evaluation protocol, which freezes the backbone and trains only
a linear classifier, CLAMP achieves competitive performance with
state-of-the-art self-supervised models. Furthermore, our analysis reveals that
neural manifolds corresponding to different categories emerge naturally and are
effectively separated in the learned representation space, highlighting the
potential of CLAMP to bridge insights from physics, neural science, and machine
learning.

</details>


### [226] [Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss Value](https://arxiv.org/abs/2506.13763)
*Yixian Xu,Shengjie Luo,Liwei Wang,Di He,Chang Liu*

Main category: cs.LG

TL;DR: 本文推导扩散模型最优损失的闭式解，开发有效估计器，用于诊断训练质量、改进训练策略并研究缩放定律。


<details>
  <summary>Details</summary>
Motivation: 扩散模型损失无法指示绝对数据拟合质量，最优值未知易混淆大最优损失和模型容量不足，需估计最优损失。

Method: 在统一公式下推导最优损失闭式解，开发有效估计器，包括适用于大数据集的随机变体。

Result: 解锁诊断主流扩散模型训练质量的固有指标，开发更优训练策略，发现减去最优损失后幂律更明显。

Conclusion: 估计最优损失对诊断和改进扩散模型很有必要，为研究扩散模型缩放定律提供更原则性的设置。

Abstract: Diffusion models have achieved remarkable success in generative modeling.
Despite more stable training, the loss of diffusion models is not indicative of
absolute data-fitting quality, since its optimal value is typically not zero
but unknown, leading to confusion between large optimal loss and insufficient
model capacity. In this work, we advocate the need to estimate the optimal loss
value for diagnosing and improving diffusion models. We first derive the
optimal loss in closed form under a unified formulation of diffusion models,
and develop effective estimators for it, including a stochastic variant
scalable to large datasets with proper control of variance and bias. With this
tool, we unlock the inherent metric for diagnosing the training quality of
mainstream diffusion model variants, and develop a more performant training
schedule based on the optimal loss. Moreover, using models with 120M to 1.5B
parameters, we find that the power law is better demonstrated after subtracting
the optimal loss from the actual training loss, suggesting a more principled
setting for investigating the scaling law for diffusion models.

</details>


### [227] [DR-SAC: Distributionally Robust Soft Actor-Critic for Reinforcement Learning under Uncertainty](https://arxiv.org/abs/2506.12622)
*Mingxuan Cui,Duo Zhou,Yuxuan Han,Grani A. Hanasusanto,Qiong Wang,Huan Zhang,Zhengyuan Zhou*

Main category: cs.LG

TL;DR: 提出Distributionally Robust Soft Actor - Critic (DR - SAC)算法增强Soft Actor - Critic (SAC)算法鲁棒性，实验表明其表现好且效率高。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在现实场景应用受环境不确定性影响，现有鲁棒RL算法多限于表格设置。

Method: 提出DR - SAC算法，推导有收敛保证的分布鲁棒软策略迭代，对未知标称分布情况用生成模型估计。

Result: 在连续控制基准任务中，DR - SAC平均奖励最高达SAC基线的9.8倍，相比现有算法提升计算效率和对大规模问题的适用性。

Conclusion: DR - SAC算法能有效增强SAC算法的鲁棒性，在性能和效率上表现良好。

Abstract: Deep reinforcement learning (RL) has achieved significant success, yet its
application in real-world scenarios is often hindered by a lack of robustness
to environmental uncertainties. To solve this challenge, some robust RL
algorithms have been proposed, but most are limited to tabular settings. In
this work, we propose Distributionally Robust Soft Actor-Critic (DR-SAC), a
novel algorithm designed to enhance the robustness of the state-of-the-art Soft
Actor-Critic (SAC) algorithm. DR-SAC aims to maximize the expected value with
entropy against the worst possible transition model lying in an uncertainty
set. A distributionally robust version of the soft policy iteration is derived
with a convergence guarantee. For settings where nominal distributions are
unknown, such as offline RL, a generative modeling approach is proposed to
estimate the required nominal distributions from data. Furthermore,
experimental results on a range of continuous control benchmark tasks
demonstrate our algorithm achieves up to $9.8$ times the average reward of the
SAC baseline under common perturbations. Additionally, compared with existing
robust reinforcement learning algorithms, DR-SAC significantly improves
computing efficiency and applicability to large-scale problems.

</details>


### [228] [Mapping Neural Signals to Agent Performance, A Step Towards Reinforcement Learning from Neural Feedback](https://arxiv.org/abs/2506.12636)
*Julia Santaniello,Matthew Russell,Benson Jiang,Donatello Sassaroli,Robert Jacob,Jivko Sinapov*

Main category: cs.LG

TL;DR: 提出NEURO - LOOP隐式反馈框架，用fNIRS设计数据集，验证fNIRS数据与智能体性能有关系，强调神经接口潜力。


<details>
  <summary>Details</summary>
Motivation: 现有隐式人类在环强化学习方法常依赖主动指令，需参与者不自然表达或手势，要减少人类工作量。

Method: 引入NEURO - LOOP框架，用fNIRS收集前额叶皮质信号，设计数据集，用经典机器学习技术分析。

Result: 发现fNIRS数据和智能体性能存在关系。

Conclusion: 神经接口在人机交互、辅助AI和自适应自主系统未来应用有潜力。

Abstract: Implicit Human-in-the-Loop Reinforcement Learning (HITL-RL) is a methodology
that integrates passive human feedback into autonomous agent training while
minimizing human workload. However, existing methods often rely on active
instruction, requiring participants to teach an agent through unnatural
expression or gesture. We introduce NEURO-LOOP, an implicit feedback framework
that utilizes the intrinsic human reward system to drive human-agent
interaction. This work demonstrates the feasibility of a critical first step in
the NEURO-LOOP framework: mapping brain signals to agent performance. Using
functional near-infrared spectroscopy (fNIRS), we design a dataset to enable
future research using passive Brain-Computer Interfaces for Human-in-the-Loop
Reinforcement Learning. Participants are instructed to observe or guide a
reinforcement learning agent in its environment while signals from the
prefrontal cortex are collected. We conclude that a relationship between fNIRS
data and agent performance exists using classical machine learning techniques.
Finally, we highlight the potential that neural interfaces may offer to future
applications of human-agent interaction, assistive AI, and adaptive autonomous
systems.

</details>


### [229] [Learning Mappings in Mesh-based Simulations](https://arxiv.org/abs/2506.12652)
*Shirin Hosseinmardi,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 提出新的无参数编码方案处理不规则网格点云，集成编码器与E - UNet，对比多种模型，展示方案在多方面优势。


<details>
  <summary>Details</summary>
Motivation: 解决不规则网格点云在机器学习模型中难以学习映射的问题。

Method: 引入将点的足迹聚合到网格顶点的无参数编码方案，集成编码器与E - UNet，与傅里叶和基于Transformer的模型进行对比。

Result: 在2D和3D问题中分析了预测准确性、数据效率和噪声鲁棒性等性能，展示了编码方案在多种映射任务中的通用性。

Conclusion: 提出的框架是原始和计算密集型编码方案的实用替代方案，可广泛应用于基于网格模拟的计算科学应用。

Abstract: Many real-world physics and engineering problems arise in geometrically
complex domains discretized by meshes for numerical simulations. The nodes of
these potentially irregular meshes naturally form point clouds whose limited
tractability poses significant challenges for learning mappings via machine
learning models. To address this, we introduce a novel and parameter-free
encoding scheme that aggregates footprints of points onto grid vertices and
yields information-rich grid representations of the topology. Such structured
representations are well-suited for standard convolution and FFT (Fast Fourier
Transform) operations and enable efficient learning of mappings between encoded
input-output pairs using Convolutional Neural Networks (CNNs). Specifically, we
integrate our encoder with a uniquely designed UNet (E-UNet) and benchmark its
performance against Fourier- and transformer-based models across diverse 2D and
3D problems where we analyze the performance in terms of predictive accuracy,
data efficiency, and noise robustness. Furthermore, we highlight the
versatility of our encoding scheme in various mapping tasks including
recovering full point cloud responses from partial observations. Our proposed
framework offers a practical alternative to both primitive and computationally
intensive encoding schemes; supporting broad adoption in computational science
applications involving mesh-based simulations.

</details>


### [230] [TFKAN: Time-Frequency KAN for Long-Term Time Series Forecasting](https://arxiv.org/abs/2506.12696)
*Xiaoyan Kui,Canwei Liu,Qinsong Li,Zhipeng Hu,Yangyang Shi,Weixin Si,Beiji Zou*

Main category: cs.LG

TL;DR: 论文提出时间 - 频率KAN（TFKAN）用于长期时间序列预测，在多数据集上优于SOTA方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 过往KAN研究主要在时域，忽略频域，而频域能揭示时间序列的周期性等信息，可补充时域信息，因此探索KAN在频域的应用。

Method: 提出TFKAN，采用双分支架构独立处理时域和频域特征，引入维度调整策略在频域选择性上采样。

Result: TFKAN在多个数据集上始终优于现有SOTA方法。

Conclusion: TFKAN有效利用了时域和频域信息，能更好地进行长期时间序列预测。

Abstract: Kolmogorov-Arnold Networks (KANs) are highly effective in long-term time
series forecasting due to their ability to efficiently represent nonlinear
relationships and exhibit local plasticity. However, prior research on KANs has
predominantly focused on the time domain, neglecting the potential of the
frequency domain. The frequency domain of time series data reveals recurring
patterns and periodic behaviors, which complement the temporal information
captured in the time domain. To address this gap, we explore the application of
KANs in the frequency domain for long-term time series forecasting. By
leveraging KANs' adaptive activation functions and their comprehensive
representation of signals in the frequency domain, we can more effectively
learn global dependencies and periodic patterns. To integrate information from
both time and frequency domains, we propose the
$\textbf{T}$ime-$\textbf{F}$requency KAN (TFKAN). TFKAN employs a dual-branch
architecture that independently processes features from each domain, ensuring
that the distinct characteristics of each domain are fully utilized without
interference. Additionally, to account for the heterogeneity between domains,
we introduce a dimension-adjustment strategy that selectively upscales only in
the frequency domain, enhancing efficiency while capturing richer frequency
information. Experimental results demonstrate that TFKAN consistently
outperforms state-of-the-art (SOTA) methods across multiple datasets. The code
is available at https://github.com/LcWave/TFKAN.

</details>


### [231] [Large Scalable Cross-Domain Graph Neural Networks for Personalized Notification at LinkedIn](https://arxiv.org/abs/2506.12700)
*Shihai He,Julie Choi,Tianqi Li,Zhiwei Ding,Peng Du,Priya Bannur,Franco Liang,Fedor Borisyuk,Padmini Jaikumar,Xiaobing Xue,Viral Gupta*

Main category: cs.LG

TL;DR: 本文提出LinkedIn部署的基于跨域GNN的系统，统一多种信号，模型表现优于单域基线，采用创新架构，提升了用户活跃度和CTR，展示了跨域GNN在实际应用中的可扩展性和有效性。


<details>
  <summary>Details</summary>
Motivation: 设计通知推荐系统需整合跨领域异构信号、捕捉时间动态并优化多目标，GNN为建模复杂交互提供强大框架，旨在提升专业平台用户参与度。

Method: 构建统一用户、内容和活动信号的大规模跨域图，引入时间建模和多任务学习等架构创新，详细设计图构建、模型设计、训练流程及评估方法。

Result: 模型在关键任务上显著优于单域基线，部署后使周活跃用户提升0.10%，CTR提高0.62%。

Conclusion: 跨域GNN在现实高影响应用中具有可扩展性和有效性。

Abstract: Notification recommendation systems are critical to driving user engagement
on professional platforms like LinkedIn. Designing such systems involves
integrating heterogeneous signals across domains, capturing temporal dynamics,
and optimizing for multiple, often competing, objectives. Graph Neural Networks
(GNNs) provide a powerful framework for modeling complex interactions in such
environments. In this paper, we present a cross-domain GNN-based system
deployed at LinkedIn that unifies user, content, and activity signals into a
single, large-scale graph. By training on this cross-domain structure, our
model significantly outperforms single-domain baselines on key tasks, including
click-through rate (CTR) prediction and professional engagement. We introduce
architectural innovations including temporal modeling and multi-task learning,
which further enhance performance. Deployed in LinkedIn's notification system,
our approach led to a 0.10% lift in weekly active users and a 0.62% improvement
in CTR. We detail our graph construction process, model design, training
pipeline, and both offline and online evaluations. Our work demonstrates the
scalability and effectiveness of cross-domain GNNs in real-world, high-impact
applications.

</details>


### [232] [Revealing the Challenges of Sim-to-Real Transfer in Model-Based Reinforcement Learning via Latent Space Modeling](https://arxiv.org/abs/2506.12735)
*Zhilin Lin,Shiliang Sun*

Main category: cs.LG

TL;DR: 本文提出基于潜在空间的方法分析基于模型场景下仿真对现实世界策略改进的影响，在MuJoCo环境实验评估其性能并指出仍存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 强化学习中仿真与现实环境的差距阻碍其实践部署，模拟器训练的智能体在现实环境中难保持性能。

Method: 提出基于潜在空间的方法，作为基于模型方法的自然扩展，直观观察基于模型方法在仿真到现实迁移中面临的挑战。

Result: 在MuJoCo环境实验评估了方法在衡量和减轻仿真到现实差距的性能。

Conclusion: 克服仿真到现实差距仍存在诸多挑战，特别是对于基于模型的方法。

Abstract: Reinforcement learning (RL) is playing an increasingly important role in
fields such as robotic control and autonomous driving. However, the gap between
simulation and the real environment remains a major obstacle to the practical
deployment of RL. Agents trained in simulators often struggle to maintain
performance when transferred to real-world physical environments. In this
paper, we propose a latent space based approach to analyze the impact of
simulation on real-world policy improvement in model-based settings. As a
natural extension of model-based methods, our approach enables an intuitive
observation of the challenges faced by model-based methods in sim-to-real
transfer. Experiments conducted in the MuJoCo environment evaluate the
performance of our method in both measuring and mitigating the sim-to-real gap.
The experiments also highlight the various challenges that remain in overcoming
the sim-to-real gap, especially for model-based methods.

</details>


### [233] [Free Privacy Protection for Wireless Federated Learning: Enjoy It or Suffer from It?](https://arxiv.org/abs/2506.12749)
*Weicai Li,Tiejun Lv,Xiyu Zhao,Xin Yuan,Wei Ni*

Main category: cs.LG

TL;DR: 提出适用于无线联邦学习（WFL）的信道原生比特翻转差分隐私（DP）机制，利用通信噪声保护隐私，实验验证其效果优于现有高斯机制。


<details>
  <summary>Details</summary>
Motivation: 数字通信系统中固有通信噪声可保护WFL隐私，但因浮点数标准下比特错误的潜在灾难性后果而被忽视，需新机制解决。

Method: 设计新的浮点数到定点数转换方法，只传输模型参数小数部分的比特，将发射机的比特扰动和通信噪声导致的比特错误解释为比特翻转DP过程。

Result: 分析新指标衡量模型参数的比特级距离，证明该机制满足(λ,ε)-Rényi DP且不违反WFL收敛，实验验证隐私和收敛分析。

Conclusion: 所提出的机制优于现有与信道无关、添加高斯噪声保护隐私的高斯机制。

Abstract: Inherent communication noises have the potential to preserve privacy for
wireless federated learning (WFL) but have been overlooked in digital
communication systems predominantly using floating-point number standards,
e.g., IEEE 754, for data storage and transmission. This is due to the
potentially catastrophic consequences of bit errors in floating-point numbers,
e.g., on the sign or exponent bits. This paper presents a novel channel-native
bit-flipping differential privacy (DP) mechanism tailored for WFL, where
transmit bits are randomly flipped and communication noises are leveraged, to
collectively preserve the privacy of WFL in digital communication systems. The
key idea is to interpret the bit perturbation at the transmitter and bit errors
caused by communication noises as a bit-flipping DP process. This is achieved
by designing a new floating-point-to-fixed-point conversion method that only
transmits the bits in the fraction part of model parameters, hence eliminating
the need for transmitting the sign and exponent bits and preventing the
catastrophic consequence of bit errors. We analyze a new metric to measure the
bit-level distance of the model parameters and prove that the proposed
mechanism satisfies (\lambda,\epsilon)-R\'enyi DP and does not violate the WFL
convergence. Experiments validate privacy and convergence analysis of the
proposed mechanism and demonstrate its superiority to the state-of-the-art
Gaussian mechanisms that are channel-agnostic and add Gaussian noise for
privacy protection.

</details>


### [234] [AFBS:Buffer Gradient Selection in Semi-asynchronous Federated Learning](https://arxiv.org/abs/2506.12754)
*Chaoyi Lu,Yiding Sun,Jinqian Chen,Zhichuan Yang,Jiangming Pan,Jihua Zhu*

Main category: cs.LG

TL;DR: 提出AFBS算法解决异步联邦学习中梯度陈旧问题，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习存在梯度陈旧问题，现有半异步框架在处理大量陈旧梯度时表现不佳。

Method: 提出AFBS算法，客户端训练前发送随机投影加密标签分布矩阵，服务器据此聚类客户端，训练时按信息价值评分并选择梯度。

Result: 在高度异构系统和数据环境中实验表明，AFBS性能优于现有方法，在CIFAR - 100任务上准确率最高提升4.8%，达到目标准确率的时间减少75%。

Conclusion: AFBS算法能有效解决异步联邦学习中梯度陈旧问题，提升半异步联邦学习性能。

Abstract: Asynchronous federated learning (AFL) accelerates training by eliminating the
need to wait for stragglers, but its asynchronous nature introduces gradient
staleness, where outdated gradients degrade performance. Existing solutions
address this issue with gradient buffers, forming a semi-asynchronous
framework. However, this approach struggles when buffers accumulate numerous
stale gradients, as blindly aggregating all gradients can harm training. To
address this, we propose AFBS (Asynchronous FL Buffer Selection), the first
algorithm to perform gradient selection within buffers while ensuring privacy
protection. Specifically, the client sends the random projection encrypted
label distribution matrix before training, and the server performs client
clustering based on it. During training, server scores and selects gradients
within each cluster based on their informational value, discarding low-value
gradients to enhance semi-asynchronous federated learning. Extensive
experiments in highly heterogeneous system and data environments demonstrate
AFBS's superior performance compared to state-of-the-art methods. Notably, on
the most challenging task, CIFAR-100, AFBS improves accuracy by up to 4.8% over
the previous best algorithm and reduces the time to reach target accuracy by
75%.

</details>


### [235] [Base3: a simple interpolation-based ensemble method for robust dynamic link prediction](https://arxiv.org/abs/2506.12764)
*Kondrup Emma*

Main category: cs.LG

TL;DR: 本文提出Base3模型解决动态链接预测问题，融合EdgeBank、PopTrack和t - CoMem，在不依赖训练下表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有动态链接预测方法依赖复杂神经网络，计算密集且难解释，需有效实用模型。

Method: 在EdgeBank基础上补充归纳能力，利用历史边重现和全局节点流行度信号，提出t - CoMem模块，构建Base3融合三者。

Result: Base3在Temporal Graph Benchmark上表现与先进深度模型相当，在部分数据集上更优，在负采样策略下提升现有基线性能。

Conclusion: Base3为时间图学习提供简单且强大的替代方案。

Abstract: Dynamic link prediction remains a central challenge in temporal graph
learning, particularly in designing models that are both effective and
practical for real-world deployment. Existing approaches often rely on complex
neural architectures, which are computationally intensive and difficult to
interpret.
  In this work, we build on the strong recurrence-based foundation of the
EdgeBank baseline, by supplementing it with inductive capabilities. We do so by
leveraging the predictive power of non-learnable signals from two complementary
perspectives: historical edge recurrence, as captured by EdgeBank, and global
node popularity, as introduced in the PopTrack model. We propose t-CoMem, a
lightweight memory module that tracks temporal co-occurrence patterns and
neighborhood activity. Building on this, we introduce Base3, an
interpolation-based model that fuses EdgeBank, PopTrack, and t-CoMem into a
unified scoring framework. This combination effectively bridges local and
global temporal dynamics -- repetition, popularity, and context -- without
relying on training. Evaluated on the Temporal Graph Benchmark, Base3 achieves
performance competitive with state-of-the-art deep models, even outperforming
them on some datasets. Importantly, it considerably improves on existing
baselines' performance under more realistic and challenging negative sampling
strategies -- offering a simple yet robust alternative for temporal graph
learning.

</details>


### [236] [Unconstrained Robust Online Convex Optimization](https://arxiv.org/abs/2506.12781)
*Jiujia Zhang,Ashok Cutkosky*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper addresses online learning with ``corrupted'' feedback. Our learner
is provided with potentially corrupted gradients $\tilde g_t$ instead of the
``true'' gradients $g_t$. We make no assumptions about how the corruptions
arise: they could be the result of outliers, mislabeled data, or even malicious
interference. We focus on the difficult ``unconstrained'' setting in which our
algorithm must maintain low regret with respect to any comparison point $u \in
\mathbb{R}^d$. The unconstrained setting is significantly more challenging as
existing algorithms suffer extremely high regret even with very tiny amounts of
corruption (which is not true in the case of a bounded domain). Our algorithms
guarantee regret $ \|u\|G (\sqrt{T} + k) $ when $G \ge \max_t \|g_t\|$ is
known, where $k$ is a measure of the total amount of corruption. When $G$ is
unknown we incur an extra additive penalty of $(\|u\|^2+G^2) k$.

</details>


### [237] [PDEfuncta: Spectrally-Aware Neural Representation for PDE Solution Modeling](https://arxiv.org/abs/2506.12790)
*Minju Jo,Woojin Cho,Uvini Balasuriya Mudiyanselage,Seungjun Lee,Noseong Park,Kookjin Lee*

Main category: cs.LG

TL;DR: 提出Global Fourier Modulation (GFM)技术和PDEfuncta元学习框架，用于解决科学机器学习中高频特征表示问题，在多任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习需表示含高频特征的复杂解场，隐式神经表示（INRs）捕捉高频行为有挑战，且此前解决频谱偏差的工作多聚焦单实例，限制可扩展性和泛化性。

Method: 提出GFM技术，通过基于傅里叶的重新参数化在INR每层注入高频信息；在此基础上引入PDEfuncta元学习框架学习多模态解场。

Result: 通过对不同科学问题的实证研究，表明该方法提高了表示质量，且无需重新训练即可用于正逆推理任务。

Conclusion: 所提方法有效，能改善多解场的表示，在推理任务中有应用潜力。

Abstract: Scientific machine learning often involves representing complex solution
fields that exhibit high-frequency features such as sharp transitions,
fine-scale oscillations, and localized structures. While implicit neural
representations (INRs) have shown promise for continuous function modeling,
capturing such high-frequency behavior remains a challenge-especially when
modeling multiple solution fields with a shared network. Prior work addressing
spectral bias in INRs has primarily focused on single-instance settings,
limiting scalability and generalization. In this work, we propose Global
Fourier Modulation (GFM), a novel modulation technique that injects
high-frequency information at each layer of the INR through Fourier-based
reparameterization. This enables compact and accurate representation of
multiple solution fields using low-dimensional latent vectors. Building upon
GFM, we introduce PDEfuncta, a meta-learning framework designed to learn
multi-modal solution fields and support generalization to new tasks. Through
empirical studies on diverse scientific problems, we demonstrate that our
method not only improves representational quality but also shows potential for
forward and inverse inference tasks without the need for retraining.

</details>


### [238] [MetaEformer: Unveiling and Leveraging Meta-patterns for Complex and Dynamic Systems Load Forecasting](https://arxiv.org/abs/2506.12800)
*Shaoyuan Huang,Tiancheng Zhang,Zhongtian Zhang,Xiaofei Wang,Lanjun Wang,Xin Wang*

Main category: cs.LG

TL;DR: 提出MetaEformer用于时间序列预测，在多个基准测试中表现出色，精度显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在复杂系统场景中难以保证有效性，存在复杂模式、概念漂移和少样本等问题。

Method: 引入基于基本波形（元模式）的方案，开发Meta - pattern Pooling机制净化和维护元模式，用Echo机制自适应利用元模式，将这些机制与基于变压器的预测器结合形成MetaEformer。

Result: MetaEformer在三种系统场景的八个基准测试中表现优越，在十五个最先进的基线方法上相对精度提高37%。

Conclusion: MetaEformer在时间序列预测中具有显著优势，能有效解决现有方法面临的问题。

Abstract: Time series forecasting is a critical and practical problem in many
real-world applications, especially for industrial scenarios, where load
forecasting underpins the intelligent operation of modern systems like clouds,
power grids and traffic networks.However, the inherent complexity and dynamics
of these systems present significant challenges. Despite advances in methods
such as pattern recognition and anti-non-stationarity have led to performance
gains, current methods fail to consistently ensure effectiveness across various
system scenarios due to the intertwined issues of complex patterns,
concept-drift, and few-shot problems. To address these challenges
simultaneously, we introduce a novel scheme centered on fundamental waveform,
a.k.a., meta-pattern. Specifically, we develop a unique Meta-pattern Pooling
mechanism to purify and maintain meta-patterns, capturing the nuanced nature of
system loads. Complementing this, the proposed Echo mechanism adaptively
leverages the meta-patterns, enabling a flexible and precise pattern
reconstruction. Our Meta-pattern Echo transformer (MetaEformer) seamlessly
incorporates these mechanisms with the transformer-based predictor, offering
end-to-end efficiency and interpretability of core processes. Demonstrating
superior performance across eight benchmarks under three system scenarios,
MetaEformer marks a significant advantage in accuracy, with a 37% relative
improvement on fifteen state-of-the-art baselines.

</details>


### [239] [Lyapunov Learning at the Onset of Chaos](https://arxiv.org/abs/2506.12810)
*Matteo Benati,Alessandro Londei,Denise Lanzieri,Vittorio Loreto*

Main category: cs.LG

TL;DR: 本文提出Lyapunov Learning训练算法应对深度学习系统中制度转变和非平稳时间序列挑战，实验显示该方法效果显著。


<details>
  <summary>Details</summary>
Motivation: 处理深度学习系统中制度转变和非平稳时间序列存在挑战，在线学习时新信息会破坏模型范式，需让神经网络快速适应新范式并保留过往知识。

Method: 提出Lyapunov Learning算法，利用非线性混沌动力系统特性，借鉴Adjacent Possible理论，让神经网络在混沌边缘运行。

Result: 在非平稳系统制度转变实验中效果显著，在处理Lorenz混沌系统参数突变时，该算法使损失率提高约96%，远超常规训练。

Conclusion: Lyapunov Learning算法在应对非平稳系统制度转变方面有效且效果显著。

Abstract: Handling regime shifts and non-stationary time series in deep learning
systems presents a significant challenge. In the case of online learning, when
new information is introduced, it can disrupt previously stored data and alter
the model's overall paradigm, especially with non-stationary data sources.
Therefore, it is crucial for neural systems to quickly adapt to new paradigms
while preserving essential past knowledge relevant to the overall problem. In
this paper, we propose a novel training algorithm for neural networks called
\textit{Lyapunov Learning}. This approach leverages the properties of nonlinear
chaotic dynamical systems to prepare the model for potential regime shifts.
Drawing inspiration from Stuart Kauffman's Adjacent Possible theory, we
leverage local unexplored regions of the solution space to enable flexible
adaptation. The neural network is designed to operate at the edge of chaos,
where the maximum Lyapunov exponent, indicative of a system's sensitivity to
small perturbations, evolves around zero over time.
  Our approach demonstrates effective and significant improvements in
experiments involving regime shifts in non-stationary systems. In particular,
we train a neural network to deal with an abrupt change in Lorenz's chaotic
system parameters. The neural network equipped with Lyapunov learning
significantly outperforms the regular training, increasing the loss ratio by
about $96\%$.

</details>


### [240] [Flow-Based Policy for Online Reinforcement Learning](https://arxiv.org/abs/2506.12811)
*Lei Lv,Yunfei Li,Yu Luo,Fuchun Sun,Tao Kong,Jiafeng Xu,Xiao Ma*

Main category: cs.LG

TL;DR: 提出FlowRL框架，结合流策略表示与Wasserstein - 2正则优化用于在线强化学习，在基准测试中获有竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 除训练信号外，增强策略类的表达能力对强化学习性能提升至关重要，流生成模型有潜力但直接应用于在线强化学习有目标不匹配问题。

Method: 通过状态相关的速度场建模策略，从噪声通过确定性ODE积分生成动作，推导约束策略搜索目标，联合最大化Q值并限制与行为最优策略的Wasserstein - 2距离。

Result: 在DMControl和Humanoidbench上的实验评估表明FlowRL在在线强化学习基准测试中取得有竞争力的性能。

Conclusion: FlowRL能有效使流优化与强化学习目标对齐，实现高效且有价值感知的策略学习。

Abstract: We present \textbf{FlowRL}, a novel framework for online reinforcement
learning that integrates flow-based policy representation with
Wasserstein-2-regularized optimization. We argue that in addition to training
signals, enhancing the expressiveness of the policy class is crucial for the
performance gains in RL. Flow-based generative models offer such potential,
excelling at capturing complex, multimodal action distributions. However, their
direct application in online RL is challenging due to a fundamental objective
mismatch: standard flow training optimizes for static data imitation, while RL
requires value-based policy optimization through a dynamic buffer, leading to
difficult optimization landscapes. FlowRL first models policies via a
state-dependent velocity field, generating actions through deterministic ODE
integration from noise. We derive a constrained policy search objective that
jointly maximizes Q through the flow policy while bounding the Wasserstein-2
distance to a behavior-optimal policy implicitly derived from the replay
buffer. This formulation effectively aligns the flow optimization with the RL
objective, enabling efficient and value-aware policy learning despite the
complexity of the policy class. Empirical evaluations on DMControl and
Humanoidbench demonstrate that FlowRL achieves competitive performance in
online reinforcement learning benchmarks.

</details>


### [241] [TrojanTO: Action-Level Backdoor Attacks against Trajectory Optimization Models](https://arxiv.org/abs/2506.12815)
*Yang Dai,Oubo Ma,Longfei Zhang,Xingxing Liang,Xiaochun Cao,Shouling Ji,Jiaheng Zhang,Jincai Huang,Li Shen*

Main category: cs.LG

TL;DR: 现有轨迹优化（TO）模型在离线强化学习中虽成功但易受后门攻击且相关研究不足，提出首个针对TO模型的动作级后门攻击TrojanTO，评估显示其攻击有效且适用多种架构。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习后门攻击对TO模型效果不佳，高维动作空间增加动作操纵难度，需研究TO模型后门攻击。

Method: 提出TrojanTO，采用交替训练增强触发器与目标动作联系，用轨迹过滤精确投毒保证正常性能、批量投毒保证触发器一致性。

Result: TrojanTO能在不同任务和攻击目标中以低攻击预算（0.3%轨迹）有效植入后门攻击，适用于DT、GDT和DC等多种TO模型架构。

Conclusion: TrojanTO是有效的动作级后门攻击方法，具有广泛适用性和可扩展性。

Abstract: Recent advances in Trajectory Optimization (TO) models have achieved
remarkable success in offline reinforcement learning. However, their
vulnerabilities against backdoor attacks are poorly understood. We find that
existing backdoor attacks in reinforcement learning are based on reward
manipulation, which are largely ineffective against the TO model due to its
inherent sequence modeling nature. Moreover, the complexities introduced by
high-dimensional action spaces further compound the challenge of action
manipulation. To address these gaps, we propose TrojanTO, the first
action-level backdoor attack against TO models. TrojanTO employs alternating
training to enhance the connection between triggers and target actions for
attack effectiveness. To improve attack stealth, it utilizes precise poisoning
via trajectory filtering for normal performance and batch poisoning for trigger
consistency. Extensive evaluations demonstrate that TrojanTO effectively
implants backdoor attacks across diverse tasks and attack objectives with a low
attack budget (0.3\% of trajectories). Furthermore, TrojanTO exhibits broad
applicability to DT, GDT, and DC, underscoring its scalability across diverse
TO model architectures.

</details>


### [242] [Taking the GP Out of the Loop](https://arxiv.org/abs/2506.12818)
*David Sweet,Siddhant anand Jadhav*

Main category: cs.LG

TL;DR: 提出Epistemic Nearest Neighbors (ENN)替代高斯过程，构建TuRBO - ENN方法，相比TuRBO能将生成提案时间减少1 - 2个数量级，可扩展到数千个观测值。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在观测值多的问题上，高斯过程（GP）存在计算瓶颈，需要可扩展到大量观测值的方法。

Method: 提出ENN替代GP，它根据K个最近邻观测值估计函数值和认知不确定性；采用基于帕累托最优权衡的采集方法；用ENN替换TuRBO中的GP，用基于帕累托的方法替换其汤普森采样采集方法。

Result: 通过数值实验表明，TuRBO - ENN相比TuRBO能将生成提案的时间减少1 - 2个数量级，可扩展到数千个观测值。

Conclusion: ENN和TuRBO - ENN为贝叶斯优化处理大量观测值提供了更高效的解决方案。

Abstract: Bayesian optimization (BO) has traditionally solved black box problems where
evaluation is expensive and, therefore, design-evaluation pairs (i.e.,
observations) are few. Recently, there has been growing interest in applying BO
to problems where evaluation is cheaper and, thus, observations are more
plentiful. An impediment to scaling BO to many observations, $N$, is the
$O(N^3)$ scaling of a na{\"i}ve query of the Gaussian process (GP) surrogate.
Modern implementations reduce this to $O(N^2)$, but the GP remains a
bottleneck. We propose Epistemic Nearest Neighbors (ENN), a surrogate that
estimates function values and epistemic uncertainty from $K$ nearest-neighbor
observations. ENN has $O(N)$ query time and omits hyperparameter fitting,
leaving uncertainty uncalibrated. To accommodate the lack of calibration, we
employ an acquisition method based on Pareto-optimal tradeoffs between
predicted value and uncertainty. Our proposed method, TuRBO-ENN, replaces the
GP surrogate in TuRBO with ENN and its Thompson sampling acquisition method
with our Pareto-based alternative. We demonstrate numerically that TuRBO-ENN
can reduce the time to generate proposals by one to two orders of magnitude
compared to TuRBO and scales to thousands of observations.

</details>


### [243] [PDCNet: a benchmark and general deep learning framework for activity prediction of peptide-drug conjugates](https://arxiv.org/abs/2506.12821)
*Yun Liu,Jintu Huang,Yingying Zhu,Congrui Wen,Yu Pang,Ji-Quan Zhang,Ling Wang*

Main category: cs.LG

TL;DR: 本文构建PDCs基准数据集，开发PDCNet预测PDCs活性，经评估性能优异，有望加速新药设计发现。


<details>
  <summary>Details</summary>
Motivation: 系统阐明肽 - 药物偶联物（PDCs）的构效关系并准确预测其活性，以实现合理设计和优化。

Method: 构建PDCs基准数据集，开发统一深度学习框架PDCNet，通过多级特征融合框架捕捉影响抗癌决策的复杂因素。

Result: PDCNet预测能力优越，测试集多项指标得分高，优于8个传统机器学习模型，多级验证确认其优越性、鲁棒性和可用性。

Conclusion: PDCNet结合基准数据集和先进模型，是新范式，可加速基于PDC的治疗剂设计和发现。

Abstract: Peptide-drug conjugates (PDCs) represent a promising therapeutic avenue for
human diseases, particularly in cancer treatment. Systematic elucidation of
structure-activity relationships (SARs) and accurate prediction of the activity
of PDCs are critical for the rational design and optimization of these
conjugates. To this end, we carefully design and construct a benchmark PDCs
dataset compiled from literature-derived collections and PDCdb database, and
then develop PDCNet, the first unified deep learning framework for forecasting
the activity of PDCs. The architecture systematically captures the complex
factors underlying anticancer decisions of PDCs in real-word scenarios through
a multi-level feature fusion framework that collaboratively characterizes and
learns the features of peptides, linkers, and payloads. Leveraging a curated
PDCs benchmark dataset, comprehensive evaluation results show that PDCNet
demonstrates superior predictive capability, with the highest AUC, F1, MCC and
BA scores of 0.9213, 0.7656, 0.7071 and 0.8388 for the test set, outperforming
eight established traditional machine learning models. Multi-level validations,
including 5-fold cross-validation, threshold testing, ablation studies, model
interpretability analysis and external independent testing, further confirm the
superiority, robustness, and usability of the PDCNet architecture. We
anticipate that PDCNet represents a novel paradigm, incorporating both a
benchmark dataset and advanced models, which can accelerate the design and
discovery of new PDC-based therapeutic agents.

</details>


### [244] [Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models](https://arxiv.org/abs/2506.12822)
*Tung Minh Luu,Younghwan Lee,Donghoon Lee,Sunho Kim,Min Jun Kim,Chang D. Yoo*

Main category: cs.LG

TL;DR: 提出ERL - VLM方法从AI反馈学习奖励函数，在实验中表现优于现有方法，展示了AI反馈用于强化学习的潜力。


<details>
  <summary>Details</summary>
Motivation: 强化学习中设计有效奖励函数需大量人力和领域知识，从人类反馈学习存在成本高、难扩展问题，因此利用AI反馈学习奖励函数。

Method: 引入ERL - VLM方法，向大视觉语言模型查询个体轨迹绝对评分，同时对基于评分的强化学习进行关键改进。

Result: 在低阶和高阶控制任务实验中，ERL - VLM显著优于现有基于VLM的奖励生成方法。

Conclusion: AI反馈有潜力以最少的人类干预扩展强化学习，为更自主高效的奖励学习铺平道路。

Abstract: Designing effective reward functions remains a fundamental challenge in
reinforcement learning (RL), as it often requires extensive human effort and
domain expertise. While RL from human feedback has been successful in aligning
agents with human intent, acquiring high-quality feedback is costly and
labor-intensive, limiting its scalability. Recent advancements in foundation
models present a promising alternative--leveraging AI-generated feedback to
reduce reliance on human supervision in reward learning. Building on this
paradigm, we introduce ERL-VLM, an enhanced rating-based RL method that
effectively learns reward functions from AI feedback. Unlike prior methods that
rely on pairwise comparisons, ERL-VLM queries large vision-language models
(VLMs) for absolute ratings of individual trajectories, enabling more
expressive feedback and improved sample efficiency. Additionally, we propose
key enhancements to rating-based RL, addressing instability issues caused by
data imbalance and noisy labels. Through extensive experiments across both
low-level and high-level control tasks, we demonstrate that ERL-VLM
significantly outperforms existing VLM-based reward generation methods. Our
results demonstrate the potential of AI feedback for scaling RL with minimal
human intervention, paving the way for more autonomous and efficient reward
learning.

</details>


### [245] [MaskPro: Linear-Space Probabilistic Learning for Strict (N:M)-Sparsity on Large Language Models](https://arxiv.org/abs/2506.12876)
*Yan Sun,Qixin Zhang,Zhiyuan Yu,Xikun Zhang,Li Shen,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出MaskPro框架解决大语言模型推理效率瓶颈，经实验验证其性能优越、可扩展性和鲁棒性良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速扩展使推理效率成为实际部署瓶颈，现有(N:M)兼容方法存在误差大或训练成本高的问题。

Method: 提出MaskPro线性空间概率框架学习先验类别分布并生成(N:M)稀疏性，引入损失残差移动平均跟踪器更新以缓解训练不稳定。

Result: MaskPro性能优越，在内存效率上有出色可扩展性，对数据样本有很强鲁棒性。

Conclusion: MaskPro是解决大语言模型推理效率问题的有效方案。

Abstract: The rapid scaling of large language models (LLMs) has made inference
efficiency a primary bottleneck in the practical deployment. To address this,
semi-structured sparsity offers a promising solution by strategically retaining
$N$ elements out of every $M$ weights, thereby enabling hardware-friendly
acceleration and reduced memory. However, existing (N:M)-compatible approaches
typically fall into two categories: rule-based layerwise greedy search, which
suffers from considerable errors, and gradient-driven combinatorial learning,
which incurs prohibitive training costs. To tackle these challenges, we propose
a novel linear-space probabilistic framework named MaskPro, which aims to learn
a prior categorical distribution for every $M$ consecutive weights and
subsequently leverages this distribution to generate the (N:M)-sparsity
throughout an $N$-way sampling without replacement. Furthermore, to mitigate
the training instability induced by the high variance of policy gradients in
the super large combinatorial space, we propose a novel update method by
introducing a moving average tracker of loss residuals instead of vanilla loss.
Finally, we conduct comprehensive theoretical analysis and extensive
experiments to validate the superior performance of MaskPro, as well as its
excellent scalability in memory efficiency and exceptional robustness to data
samples. Our code is available at https://github.com/woodenchild95/Maskpro.git.

</details>


### [246] [Silhouette-Guided Instance-Weighted k-means](https://arxiv.org/abs/2506.12878)
*Aggelos Semoglou,Aristidis Likas,John Pavlopoulos*

Main category: cs.LG

TL;DR: 提出K - Sil算法，对k - means算法进行轮廓系数引导的改进，在合成和真实数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 流行的聚类算法如k - means在处理离群点或数据不平衡时存在问题，导致质心扭曲和分区不理想。

Method: 引入K - Sil算法，基于轮廓系数对数据点加权，强调用户指定的轮廓聚合指标，采用自调整加权方案、合适的采样策略和可扩展近似方法。

Result: 理论上保证质心收敛，在合成和真实数据集上，K - Sil的轮廓系数相较于k - means和另外两种实例加权k - means变体有显著提高。

Conclusion: K - Sil是需要高质量、分离良好的聚类应用的可行替代方案。

Abstract: Clustering is a fundamental unsupervised learning task with numerous
applications across diverse fields. Popular algorithms such as k-means often
struggle with outliers or imbalances, leading to distorted centroids and
suboptimal partitions. We introduce K-Sil, a silhouette-guided refinement of
the k-means algorithm that weights points based on their silhouette scores,
prioritizing well-clustered instances while suppressing borderline or noisy
regions. The algorithm emphasizes user-specified silhouette aggregation
metrics: macro-, micro-averaged or a combination, through self-tuning weighting
schemes, supported by appropriate sampling strategies and scalable
approximations. These components ensure computational efficiency and
adaptability to diverse dataset geometries. Theoretical guarantees establish
centroid convergence, and empirical validation on synthetic and real-world
datasets demonstrates statistically significant improvements in silhouette
scores over k-means and two other instance-weighted k-means variants. These
results establish K-Sil as a principled alternative for applications demanding
high-quality, well-separated clusters.

</details>


### [247] [Jailbreak Strength and Model Similarity Predict Transferability](https://arxiv.org/abs/2506.12913)
*Rico Angell,Jannik Brinkmann,He He*

Main category: cs.LG

TL;DR: 研究越狱攻击在不同模型间的转移情况，发现转移成功与越狱强度和模型上下文表示相似度有关，通过蒸馏可提高可转移性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏识别越狱攻击在源模型和目标模型间转移情况的原则性方法，且越狱攻击对AI系统安全构成威胁。

Method: 观察越狱攻击从源模型到目标模型的转移与越狱强度、模型上下文表示相似度的关系，通过将目标模型蒸馏到源模型来提高可转移性。

Result: 蒸馏后的源模型可作为目标模型的替代，产生更具可转移性的攻击。

Conclusion: 越狱攻击的成功不仅是安全训练泛化失败，更是模型上下文表示存在根本缺陷的结果。

Abstract: Jailbreaks pose an imminent threat to ensuring the safety of modern AI
systems by enabling users to disable safeguards and elicit unsafe information.
Sometimes, jailbreaks discovered for one model incidentally transfer to another
model, exposing a fundamental flaw in safeguarding. Unfortunately, there is no
principled approach to identify when jailbreaks will transfer from a source
model to a target model. In this work, we observe that transfer success from a
source model to a target model depends on quantifiable measures of both
jailbreak strength with respect to the source model and the contextual
representation similarity of the two models. Furthermore, we show
transferability can be increased by distilling from the target model into the
source model where the only target model responses used to train the source
model are those to benign prompts. We show that the distilled source model can
act as a surrogate for the target model, yielding more transferable attacks
against the target model. These results suggest that the success of jailbreaks
is not merely due to exploitation of safety training failing to generalize
out-of-distribution, but instead a consequence of a more fundamental flaw in
contextual representations computed by models.

</details>


### [248] [PINNs Algorithmic Framework for Simulation of Nonlinear Burgers' Type Models](https://arxiv.org/abs/2506.12922)
*Ajeet Singh,Ram Jiwari,Vikram,Ujjwal Saini*

Main category: cs.LG

TL;DR: 本文用基于物理信息神经网络（PINNs）的算法模拟非线性一维和二维Burgers型模型，经测试验证其在求解复杂时变偏微分方程上的潜力。


<details>
  <summary>Details</summary>
Motivation: 寻找可靠方法求解复杂时变偏微分方程。

Method: 构建神经网络近似问题解，使用满足初始数据和边界条件的试探函数；描述问题的数学公式和PINNs结构；用五个测试问题展示算法。

Result: PINNs能忠实地复制非线性偏微分方程的解，在误差和灵活性方面有竞争力。

Conclusion: PINNs是求解复杂时变偏微分方程的可靠方法。

Abstract: In this work, a physics-informed neural networks (PINNs) based algorithm is
used for simulation of nonlinear 1D and 2D Burgers' type models. This scheme
relies on a neural network built to approximate the problem solution and use a
trial function that meets the initial data and boundary criteria. First of all,
a brief mathematical formulation of the problem and the structure of PINNs,
including the neural network architecture, loss construction, and training
methodology is described. Finally, the algorithm is demonstrated with five test
problems involving variations of the 1D coupled, 2D single and 2D coupled
Burgers' models. We compare the PINN-based solutions with exact results to
assess accuracy and convergence of the developed algorithm. The results
demonstrate that PINNs may faithfully replicate nonlinear PDE solutions and
offer competitive performance in terms of inaccuracy and flexibility. This work
demonstrates the potential of PINNs as a reliable approach to solving complex
time-dependent PDEs.

</details>


### [249] [Complexity Scaling Laws for Neural Models using Combinatorial Optimization](https://arxiv.org/abs/2506.12932)
*Lowell Weissman,Michael Krumdick,A. Lynn Abbott*

Main category: cs.LG

TL;DR: 本文基于问题复杂度开发了缩放定律，以TSP为例进行研究，发现组合优化能促进平滑成本趋势，固定大小模型的次优性增长可预测，且与局部搜索有相似趋势。


<details>
  <summary>Details</summary>
Motivation: 现有神经缩放定律基于计算预算、模型大小和数据集大小，本文想基于问题复杂度开发缩放定律。

Method: 分析解决方案空间大小和表示空间大小两个复杂度度量，以TSP为例进行研究，对比强化学习和监督微调训练的模型，还与局部搜索中的问题复杂度缩放进行类比。

Result: 组合优化能促进平滑成本趋势，即使没有可解释损失也能得到有意义的缩放定律；固定大小模型的次优性增长可预测；简单的成本景观梯度下降产生类似趋势。

Conclusion: 基于问题复杂度开发的缩放定律有一定意义，与局部搜索有相似的问题复杂度缩放趋势。

Abstract: Recent work on neural scaling laws demonstrates that model performance scales
predictably with compute budget, model size, and dataset size. In this work, we
develop scaling laws based on problem complexity. We analyze two fundamental
complexity measures: solution space size and representation space size. Using
the Traveling Salesman Problem (TSP) as a case study, we show that
combinatorial optimization promotes smooth cost trends, and therefore
meaningful scaling laws can be obtained even in the absence of an interpretable
loss. We then show that suboptimality grows predictably for fixed-size models
when scaling the number of TSP nodes or spatial dimensions, independent of
whether the model was trained with reinforcement learning or supervised
fine-tuning on a static dataset. We conclude with an analogy to problem
complexity scaling in local search, showing that a much simpler gradient
descent of the cost landscape produces similar trends.

</details>


### [250] [Unsupervised risk factor identification across cancer types and data modalities via explainable artificial intelligence](https://arxiv.org/abs/2506.12944)
*Maximilian Ferle,Jonas Ader,Thomas Wiemers,Nora Grieb,Adrian Lindenmeyer,Hans-Jonas Meyer,Thomas Neumuth,Markus Kreuz,Kristin Reiche,Maximilian Merz*

Main category: cs.LG

TL;DR: 提出无监督机器学习新方法用于临床风险分层，在模拟实验和两种癌症类型中验证有效性与实用性。


<details>
  <summary>Details</summary>
Motivation: 当前临床决策中的风险分层方法难以将复杂生存分析转化为可操作临床标准。

Method: 通过对多元对数秩统计量的可微调整，直接优化患者集群间的生存异质性，训练神经网络识别预后不同的患者组。

Result: 在模拟实验及两种癌症（多发性骨髓瘤和非小细胞肺癌）应用中，识别出预后不同且生存结果显著不同的患者亚组，事后可解释性分析发现与既定风险因素相符的特征。

Conclusion: 该泛癌症、模型无关的方法是临床风险分层的重要进展，有助于发现新预后特征，为肿瘤学及其他领域的治疗个性化和临床决策提供支持。

Abstract: Risk stratification is a key tool in clinical decision-making, yet current
approaches often fail to translate sophisticated survival analysis into
actionable clinical criteria. We present a novel method for unsupervised
machine learning that directly optimizes for survival heterogeneity across
patient clusters through a differentiable adaptation of the multivariate
logrank statistic. Unlike most existing methods that rely on proxy metrics, our
approach represents novel methodology for training any neural network
architecture on any data modality to identify prognostically distinct patient
groups. We thoroughly evaluate the method in simulation experiments and
demonstrate its utility in practice by applying it to two distinct cancer
types: analyzing laboratory parameters from multiple myeloma patients and
computed tomography images from non-small cell lung cancer patients,
identifying prognostically distinct patient subgroups with significantly
different survival outcomes in both cases. Post-hoc explainability analyses
uncover clinically meaningful features determining the group assignments which
align well with established risk factors and thus lend strong weight to the
methods utility. This pan-cancer, model-agnostic approach represents a valuable
advancement in clinical risk stratification, enabling the discovery of novel
prognostic signatures across diverse data types while providing interpretable
results that promise to complement treatment personalization and clinical
decision-making in oncology and beyond.

</details>


### [251] [Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition](https://arxiv.org/abs/2506.12953)
*Mayank Bumb,Anshul Vemulapalli,Sri Harsha Vardhan Prasad Jella,Anish Gupta,An La,Ryan A. Rossi,Hongjie Chen,Franck Dernoncourt,Nesreen K. Ahmed,Yu Wang*

Main category: cs.LG

TL;DR: 本文探索基于提示的策略让大语言模型进行时间序列预测，提出PatchInstruct方法提升预测质量。


<details>
  <summary>Details</summary>
Motivation: 先前利用大语言模型进行时间序列分析的工作需大量微调且忽视序列间相关性，希望找到简单灵活的方法。

Method: 探索利用时间序列分解、基于补丁的分词和基于相似度的邻居增强的提示方法，提出PatchInstruct方法。

Result: 能够在保持简单性和最小化数据预处理的情况下提升大语言模型的预测质量。

Conclusion: 基于提示的策略能让大语言模型有效进行时间序列预测，PatchInstruct方法可行。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated new
possibilities for accurate and efficient time series analysis, but prior work
often required heavy fine-tuning and/or ignored inter-series correlations. In
this work, we explore simple and flexible prompt-based strategies that enable
LLMs to perform time series forecasting without extensive retraining or the use
of a complex external architecture. Through the exploration of specialized
prompting methods that leverage time series decomposition, patch-based
tokenization, and similarity-based neighbor augmentation, we find that it is
possible to enhance LLM forecasting quality while maintaining simplicity and
requiring minimal preprocessing of data. To this end, we propose our own
method, PatchInstruct, which enables LLMs to make precise and effective
predictions.

</details>


### [252] [Domain Specific Benchmarks for Evaluating Multimodal Large Language Models](https://arxiv.org/abs/2506.12958)
*Khizar Anjuma,Muhammad Arbab Arshad,Kadhim Hayawi,Efstathios Polyzos,Asadullah Tariq,Mohamed Adel Serhani,Laiba Batool,Brady Lund,Nishith Reddy Mannuru,Ravi Varma Kumar Bevara,Taslim Mahbub,Muhammad Zeeshan Akram,Sakib Shahriar*

Main category: cs.LG

TL;DR: 文章引入LLM关键学科分类，回顾各领域基准与调查论文，按领域分类基准以助力AGI发展。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对LLM的特定领域分析，需进行相关研究。

Method: 引入七类关键学科分类，对各领域LLM基准和调查论文进行全面回顾。

Result: 按领域对LLM基准进行分类整理。

Conclusion: 为研究人员提供资源，推动AGI发展。

Abstract: Large language models (LLMs) are increasingly being deployed across
disciplines due to their advanced reasoning and problem solving capabilities.
To measure their effectiveness, various benchmarks have been developed that
measure aspects of LLM reasoning, comprehension, and problem-solving. While
several surveys address LLM evaluation and benchmarks, a domain-specific
analysis remains underexplored in the literature. This paper introduces a
taxonomy of seven key disciplines, encompassing various domains and application
areas where LLMs are extensively utilized. Additionally, we provide a
comprehensive review of LLM benchmarks and survey papers within each domain,
highlighting the unique capabilities of LLMs and the challenges faced in their
application. Finally, we compile and categorize these benchmarks by domain to
create an accessible resource for researchers, aiming to pave the way for
advancements toward artificial general intelligence (AGI)

</details>


### [253] [Differentially Private Bilevel Optimization: Efficient Algorithms with Near-Optimal Rates](https://arxiv.org/abs/2506.12994)
*Andrew Lowy,Daogao Liu*

Main category: cs.LG

TL;DR: 研究双层优化的差分隐私问题，给出凸和非凸情形下的结果。


<details>
  <summary>Details</summary>
Motivation: 双层优化在机器学习应用中涉及敏感数据，有隐私担忧，因此研究差分隐私双层优化。

Method: 在凸设置下用指数和正则化指数机制实现多项式时间界；在非凸设置下开发新算法。提出对不精确函数评估下对数凹采样的新方法和分析。

Result: 凸设置下给出纯和近似差分隐私的超额风险上下界，接近最优率；非凸设置下开发有最优率的算法找近似驻点，且界不依赖内层问题维度。

Conclusion: 成功对差分隐私双层优化进行研究，在凸和非凸情形都取得成果。

Abstract: Bilevel optimization, in which one optimization problem is nested inside
another, underlies many machine learning applications with a hierarchical
structure -- such as meta-learning and hyperparameter optimization. Such
applications often involve sensitive training data, raising pressing concerns
about individual privacy. Motivated by this, we study differentially private
bilevel optimization. We first focus on settings where the outer-level
objective is \textit{convex}, and provide novel upper and lower bounds on the
excess risk for both pure and approximate differential privacy, covering both
empirical and population-level loss. These bounds are nearly tight and
essentially match the optimal rates for standard single-level differentially
private ERM and stochastic convex optimization (SCO), up to additional terms
that capture the intrinsic complexity of the nested bilevel structure. The
bounds are achieved in polynomial time via efficient implementations of the
exponential and regularized exponential mechanisms. A key technical
contribution is a new method and analysis of log-concave sampling under inexact
function evaluations, which may be of independent interest. In the
\textit{non-convex} setting, we develop novel algorithms with state-of-the-art
rates for privately finding approximate stationary points. Notably, our bounds
do not depend on the dimension of the inner problem.

</details>


### [254] [Antibody Foundational Model : Ab-RoBERTa](https://arxiv.org/abs/2506.13006)
*Eunna Huh,Hyeonsu Lee,Hyunjin Shin*

Main category: cs.LG

TL;DR: 本文介绍了基于RoBERTa的抗体特异性大语言模型Ab - RoBERTa，可支持多种抗体相关研究应用且公开可用。


<details>
  <summary>Details</summary>
Motivation: 抗体工程受关注，基于transformer的蛋白质大语言模型有应用前景，虽RoBERTa有优势，但基于它的抗体特异性基础模型未公开，故开展研究。

Method: 构建基于RoBERTa的抗体特异性大语言模型Ab - RoBERTa。

Result: 推出了Ab - RoBERTa，且在https://huggingface.co/mogam - ai/Ab - RoBERTa公开。

Conclusion: Ab - RoBERTa可支持多种抗体相关研究应用。

Abstract: With the growing prominence of antibody-based therapeutics, antibody
engineering has gained increasing attention as a critical area of research and
development. Recent progress in transformer-based protein large language models
(LLMs) has demonstrated promising applications in protein sequence design and
structural prediction. Moreover, the availability of large-scale antibody
datasets such as the Observed Antibody Space (OAS) database has opened new
avenues for the development of LLMs specialized for processing antibody
sequences. Among these, RoBERTa has demonstrated improved performance relative
to BERT, while maintaining a smaller parameter count (125M) compared to the
BERT-based protein model, ProtBERT (420M). This reduced model size enables more
efficient deployment in antibody-related applications. However, despite the
numerous advantages of the RoBERTa architecture, antibody-specific foundational
models built upon it have remained inaccessible to the research community. In
this study, we introduce Ab-RoBERTa, a RoBERTa-based antibody-specific LLM,
which is publicly available at https://huggingface.co/mogam-ai/Ab-RoBERTa. This
resource is intended to support a wide range of antibody-related research
applications including paratope prediction or humanness assessment.

</details>


### [255] [Geometric Embedding Alignment via Curvature Matching in Transfer Learning](https://arxiv.org/abs/2506.13015)
*Sung Moon Ko,Jaewan Lee,Sumin Lee,Soorin Yim,Kyunghoon Bae,Sehui Han*

Main category: cs.LG

TL;DR: 引入基于黎曼几何的新方法GEAR，将多模型集成到统一迁移学习框架，在23个分子任务对上表现优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 利用深度学习模型的几何解释，将多个模型集成到统一迁移学习框架以有效聚合不同来源知识，提升目标任务性能。

Method: 利用微分几何（黎曼几何），通过对齐单个模型潜在空间的里奇曲率，构建GEAR框架。

Result: 在23个来自不同领域的分子任务对上进行评估，在随机和支架数据分割下，比现有基准模型分别有14.4%和8.3%的显著性能提升。

Conclusion: 提出的GEAR框架能有效聚合知识，提高目标任务的性能。

Abstract: Geometrical interpretations of deep learning models offer insightful
perspectives into their underlying mathematical structures. In this work, we
introduce a novel approach that leverages differential geometry, particularly
concepts from Riemannian geometry, to integrate multiple models into a unified
transfer learning framework. By aligning the Ricci curvature of latent space of
individual models, we construct an interrelated architecture, namely Geometric
Embedding Alignment via cuRvature matching in transfer learning (GEAR), which
ensures comprehensive geometric representation across datapoints. This
framework enables the effective aggregation of knowledge from diverse sources,
thereby improving performance on target tasks. We evaluate our model on 23
molecular task pairs sourced from various domains and demonstrate significant
performance gains over existing benchmark model under both random (14.4%) and
scaffold (8.3%) data splits.

</details>


### [256] [Symmetry in Neural Network Parameter Spaces](https://arxiv.org/abs/2506.13018)
*Bo Zhao,Robin Walters,Rose Yu*

Main category: cs.LG

TL;DR: 本文综述参数空间对称性，总结相关文献，揭示其与学习理论联系并指出研究领域的差距和机会。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型高度过参数化，参数空间对称性影响损失景观和学习动态，为理解优化、泛化和模型复杂性提供新视角。

Method: 对参数空间对称性相关文献进行综述。

Result: 总结现有文献，揭示对称性与学习理论的联系。

Conclusion: 明确了新兴研究领域的差距和机会。

Abstract: Modern deep learning models are highly overparameterized, resulting in large
sets of parameter configurations that yield the same outputs. A significant
portion of this redundancy is explained by symmetries in the parameter
space--transformations that leave the network function unchanged. These
symmetries shape the loss landscape and constrain learning dynamics, offering a
new lens for understanding optimization, generalization, and model complexity
that complements existing theory of deep learning. This survey provides an
overview of parameter space symmetry. We summarize existing literature, uncover
connections between symmetry and learning theory, and identify gaps and
opportunities in this emerging field.

</details>


### [257] [C-TLSAN: Content-Enhanced Time-Aware Long- and Short-Term Attention Network for Personalized Recommendation](https://arxiv.org/abs/2506.13021)
*Siqi Liang,Yudi Zhang,Yubo Wang*

Main category: cs.LG

TL;DR: 提出C - TLSAN模型，结合物品语义内容建模用户长短期偏好，在亚马逊数据集实验中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有顺序推荐系统在建模用户偏好时未充分利用物品语义内容，为提升推荐系统表现力和个性化能力。

Method: 提出C - TLSAN模型，将与用户历史交互关联的文本内容嵌入长短期注意力层，融合顺序信号和文本语义。

Result: 在大规模亚马逊数据集实验中，C - TLSAN在预测任务上始终优于基线模型，如相比TLSAN平均提升AUC 1.66%、Recall@10 93.99%、Precision@10 94.80%。

Conclusion: 在顺序推荐的时间建模框架中集成内容感知增强具有重要价值。

Abstract: Sequential recommender systems aim to model users' evolving preferences by
capturing patterns in their historical interactions. Recent advances in this
area have leveraged deep neural networks and attention mechanisms to
effectively represent sequential behaviors and time-sensitive interests. In
this work, we propose C-TLSAN (Content-Enhanced Time-Aware Long- and Short-Term
Attention Network), an extension of the TLSAN architecture that jointly models
long- and short-term user preferences while incorporating semantic content
associated with items, such as product descriptions.
  C-TLSAN enriches the recommendation pipeline by embedding textual content
linked to users' historical interactions directly into both long-term and
short-term attention layers. This allows the model to learn from both
behavioral patterns and rich item content, enhancing user and item
representations across temporal dimensions. By fusing sequential signals with
textual semantics, our approach improves the expressiveness and personalization
capacity of recommendation systems.
  We conduct extensive experiments on large-scale Amazon datasets, benchmarking
C-TLSAN against state-of-the-art baselines, including recent sequential
recommenders based on Large Language Models (LLMs), which represent interaction
history and predictions in text form. Empirical results demonstrate that
C-TLSAN consistently outperforms strong baselines in next-item prediction
tasks. Notably, it improves AUC by 1.66%, Recall@10 by 93.99%, and Precision@10
by 94.80% on average over the best-performing baseline (TLSAN) across 10 Amazon
product categories. These results highlight the value of integrating
content-aware enhancements into temporal modeling frameworks for sequential
recommendation. Our code is available at https://github.com/booml247/cTLSAN.

</details>


### [258] [Forecast-Then-Optimize Deep Learning Methods](https://arxiv.org/abs/2506.13036)
*Jinhang Jiang,Nan Wu,Ben Liu,Mei Feng,Xin Ji,Karthik Srinivasan*

Main category: cs.LG

TL;DR: 本文调研2016 - 2025年预测优化框架FTO进展，分析主流深度学习FTO架构，展示其在运营管理应用中作用并给出未来预测方法指南。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测原始结果有误差和偏差，需更好方法提升预测准确性、鲁棒性和决策有效性。

Method: 对FTO框架进行系统概述，分析主流深度学习FTO架构，聚焦运营管理实际应用。

Result: 显示FTO在提升预测准确性、鲁棒性和决策有效性方面的关键作用。

Conclusion: 为未来预测方法建立基础指南，弥合理论与实践差距。

Abstract: Time series forecasting underpins vital decision-making across various
sectors, yet raw predictions from sophisticated models often harbor systematic
errors and biases. We examine the Forecast-Then-Optimize (FTO) framework,
pioneering its systematic synopsis. Unlike conventional Predict-Then-Optimize
(PTO) methods, FTO explicitly refines forecasts through optimization techniques
such as ensemble methods, meta-learners, and uncertainty adjustments.
Furthermore, deep learning and large language models have established
superiority over traditional parametric forecasting models for most enterprise
applications. This paper surveys significant advancements from 2016 to 2025,
analyzing mainstream deep learning FTO architectures. Focusing on real-world
applications in operations management, we demonstrate FTO's crucial role in
enhancing predictive accuracy, robustness, and decision efficacy. Our study
establishes foundational guidelines for future forecasting methodologies,
bridging theory and operational practicality.

</details>


### [259] [A Comprehensive Survey on Continual Learning in Generative Models](https://arxiv.org/abs/2506.13045)
*Haiyang Guo,Fanhu Zeng,Fei Zhu,Jiayi Wang,Xukai Wang,Jingang Zhou,Hongbo Zhao,Wenzhuo Liu,Shijie Ma,Xu-Yao Zhang,Cheng-Lin Liu*

Main category: cs.LG

TL;DR: 本文对主流生成模型的持续学习方法进行全面综述，受人类大脑记忆机制启发对方法分类并分析不同生成模型的持续学习设置。


<details>
  <summary>Details</summary>
Motivation: 生成模型存在灾难性遗忘问题，限制其在实际应用中的适应性和可扩展性，需提出解决方法。

Method: 受人类大脑记忆机制启发，将持续学习方法分为基于架构、基于正则化和基于重放三类，分析不同生成模型的持续学习设置，包括训练目标、基准和核心骨干。

Result: 完成对主流生成模型持续学习方法的分类及相关分析。

Conclusion: 通过全面调查和分析，为生成模型的持续学习领域提供了更深入的见解。

Abstract: The rapid advancement of generative models has enabled modern AI systems to
comprehend and produce highly sophisticated content, even achieving human-level
performance in specific domains. However, these models remain fundamentally
constrained by catastrophic forgetting - a persistent challenge where adapting
to new tasks typically leads to significant degradation in performance on
previously learned tasks. To address this practical limitation, numerous
approaches have been proposed to enhance the adaptability and scalability of
generative models in real-world applications. In this work, we present a
comprehensive survey of continual learning methods for mainstream generative
models, including large language models, multimodal large language models,
vision language action models, and diffusion models. Drawing inspiration from
the memory mechanisms of the human brain, we systematically categorize these
approaches into three paradigms: architecture-based, regularization-based, and
replay-based methods, while elucidating their underlying methodologies and
motivations. We further analyze continual learning setups for different
generative models, including training objectives, benchmarks, and core
backbones, offering deeper insights into the field. The project page of this
paper is available at
https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.

</details>


### [260] [The Space Complexity of Learning-Unlearning Algorithms](https://arxiv.org/abs/2506.13048)
*Yeshwanth Cherapanamjeri,Sumegha Garg,Nived Rajaraman,Ayush Sekhari,Abhishek Shetty*

Main category: cs.LG

TL;DR: 研究机器学习遗忘算法的内存复杂度，证明VC维度不能刻画遗忘的空间复杂度，给出信息存储量下界，在更强模型下给出上界，揭示两种内存模型的分离。


<details>
  <summary>Details</summary>
Motivation: 研究能为用户提供强数据删除保证的机器学习遗忘算法的内存复杂度，探讨删除特定训练样本所需的存储位数。

Method: 先给出否定结果，证明VC维度不能刻画遗忘的空间复杂度，给出基于eluder维度的信息存储量下界，在更强的ticketed - memory模型下给出基于星数的上界。

Result: 证明VC维度不是遗忘空间复杂度的刻画，学习者为后续遗忘需存储的信息下界为假设类的eluder维度，在更强模型下得到基于星数的上界。

Conclusion: 揭示了机器学习遗忘中中心内存模型和ticketed内存模型之间的根本分离。

Abstract: We study the memory complexity of machine unlearning algorithms that provide
strong data deletion guarantees to the users. Formally, consider an algorithm
for a particular learning task that initially receives a training dataset.
Then, after learning, it receives data deletion requests from a subset of users
(of arbitrary size), and the goal of unlearning is to perform the task as if
the learner never received the data of deleted users. In this paper, we ask how
many bits of storage are needed to be able to delete certain training samples
at a later time. We focus on the task of realizability testing, where the goal
is to check whether the remaining training samples are realizable within a
given hypothesis class \(\mathcal{H}\).
  Toward that end, we first provide a negative result showing that the VC
dimension is not a characterization of the space complexity of unlearning. In
particular, we provide a hypothesis class with constant VC dimension (and
Littlestone dimension), but for which any unlearning algorithm for
realizability testing needs to store \(\Omega(n)\)-bits, where \(n\) denotes
the size of the initial training dataset. In fact, we provide a stronger
separation by showing that for any hypothesis class \(\mathcal{H}\), the amount
of information that the learner needs to store, so as to perform unlearning
later, is lower bounded by the \textit{eluder dimension} of \(\mathcal{H}\), a
combinatorial notion always larger than the VC dimension. We complement the
lower bound with an upper bound in terms of the star number of the underlying
hypothesis class, albeit in a stronger ticketed-memory model proposed by Ghazi
et al. (2023). Since the star number for a hypothesis class is never larger
than its Eluder dimension, our work highlights a fundamental separation between
central and ticketed memory models for machine unlearning.

</details>


### [261] [Fast Convergence for High-Order ODE Solvers in Diffusion Probabilistic Models](https://arxiv.org/abs/2506.13061)
*Daniel Zhengyu Huang,Jiaoyang Huang,Zhengjiang Lin*

Main category: cs.LG

TL;DR: 论文分析概率流ODE确定性采样方法收敛性，推导p阶Runge - Kutta方案，给出目标分布与生成数据分布总变差距离上界并数值验证假设。


<details>
  <summary>Details</summary>
Motivation: 分析近似得分函数的正则性、近似误差和数值积分误差的相互作用，以理解扩散概率模型采样准确性。

Method: 在近似得分函数一二阶导数有界假设下，开发p阶（指数）Runge - Kutta方案。

Result: 目标分布与生成数据分布的总变差距离有上界，数值验证近似得分函数一二阶导数在实际中保持有界。

Conclusion: 理论保证适用于具有任意方差调度的一般前向过程。

Abstract: Diffusion probabilistic models generate samples by learning to reverse a
noise-injection process that transforms data into noise. Reformulating this
reverse process as a deterministic probability flow ordinary differential
equation (ODE) enables efficient sampling using high-order solvers, often
requiring only $\mathcal{O}(10)$ steps. Since the score function is typically
approximated by a neural network, analyzing the interaction between its
regularity, approximation error, and numerical integration error is key to
understanding the overall sampling accuracy. In this work, we continue our
analysis of the convergence properties of the deterministic sampling methods
derived from probability flow ODEs [25], focusing on $p$-th order (exponential)
Runge-Kutta schemes for any integer $p \geq 1$. Under the assumption that the
first and second derivatives of the approximate score function are bounded, we
develop $p$-th order (exponential) Runge-Kutta schemes and demonstrate that the
total variation distance between the target distribution and the generated data
distribution can be bounded above by \begin{align*}
  O\bigl(d^{\frac{7}{4}}\varepsilon_{\text{score}}^{\frac{1}{2}}
+d(dH_{\max})^p\bigr), \end{align*} where $\varepsilon^2_{\text{score}}$
denotes the $L^2$ error in the score function approximation, $d$ is the data
dimension and $H_{\max}$ represents the maximum step size used in the solver.
We numerically verify the regularity assumption on benchmark datasets,
confirming that the first and second derivatives of the approximate score
function remain bounded in practice. Our theoretical guarantees hold for
general forward processes with arbitrary variance schedules.

</details>


### [262] [Uncertainty-Aware Graph Neural Networks: A Multi-Hop Evidence Fusion Approach](https://arxiv.org/abs/2506.13083)
*Qingfeng Chen,Shiyuan Li,Yixin Liu,Shirui Pan,Geoffrey I. Webb,Shichao Zhang*

Main category: cs.LG

TL;DR: 提出证据融合图神经网络EFGNN，通过结合证据理论和多跳传播架构等方法，提升预测可信度和节点分类准确率，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络未考虑类概率随模型深度变化的不确定性，导致现实场景中预测不可靠、有风险。

Method: 将证据理论与基于多跳传播的GNN架构结合，开发无参数累积信念融合机制，设计联合学习目标。

Result: 在多个数据集上的实验和理论分析表明，EFGNN在准确性、可信度方面有效，且对潜在攻击有鲁棒性。

Conclusion: 提出的EFGNN能实现可信预测，提高节点分类准确率，明确错误预测风险。

Abstract: Graph neural networks (GNNs) excel in graph representation learning by
integrating graph structure and node features. Existing GNNs, unfortunately,
fail to account for the uncertainty of class probabilities that vary with the
depth of the model, leading to unreliable and risky predictions in real-world
scenarios. To bridge the gap, in this paper, we propose a novel Evidence Fusing
Graph Neural Network (EFGNN for short) to achieve trustworthy prediction,
enhance node classification accuracy, and make explicit the risk of wrong
predictions. In particular, we integrate the evidence theory with multi-hop
propagation-based GNN architecture to quantify the prediction uncertainty of
each node with the consideration of multiple receptive fields. Moreover, a
parameter-free cumulative belief fusion (CBF) mechanism is developed to
leverage the changes in prediction uncertainty and fuse the evidence to improve
the trustworthiness of the final prediction. To effectively optimize the EFGNN
model, we carefully design a joint learning objective composed of evidence
cross-entropy, dissonance coefficient, and false confident penalty. The
experimental results on various datasets and theoretical analyses demonstrate
the effectiveness of the proposed model in terms of accuracy and
trustworthiness, as well as its robustness to potential attacks. The source
code of EFGNN is available at https://github.com/Shiy-Li/EFGNN.

</details>


### [263] [Dynamic Graph Condensation](https://arxiv.org/abs/2506.13099)
*Dong Chen,Shuai Zheng,Yeyu Yan,Muhao Xu,Zhenfeng Zhu,Yao Zhao,Kunlun He*

Main category: cs.LG

TL;DR: 本文研究动态图压缩（DGC），提出DyGC框架，能在保留时空特征下压缩动态图，实验证明其有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 动态图的时间扩展带来数据效率挑战，如数据量增加、冗余高、依赖昂贵的动态图神经网络，需降低动态图规模以实现高效训练。

Method: 提出DyGC框架，引入尖峰结构生成机制建模时空连接性，采用分布匹配方法构建状态演化场并进行时空状态对齐来优化压缩图。

Result: 在多个动态图数据集和代表性DGNN架构实验中，用0.5%原图大小保留96.2%的DGNN性能，训练加速达1846倍。

Conclusion: DyGC框架在动态图压缩上有效，能显著提升数据效率和训练速度。

Abstract: Recent research on deep graph learning has shifted from static to dynamic
graphs, motivated by the evolving behaviors observed in complex real-world
systems. However, the temporal extension in dynamic graphs poses significant
data efficiency challenges, including increased data volume, high
spatiotemporal redundancy, and reliance on costly dynamic graph neural networks
(DGNNs). To alleviate the concerns, we pioneer the study of dynamic graph
condensation (DGC), which aims to substantially reduce the scale of dynamic
graphs for data-efficient DGNN training. Accordingly, we propose DyGC, a novel
framework that condenses the real dynamic graph into a compact version while
faithfully preserving the inherent spatiotemporal characteristics.
Specifically, to endow synthetic graphs with realistic evolving structures, a
novel spiking structure generation mechanism is introduced. It draws on the
dynamic behavior of spiking neurons to model temporally-aware connectivity in
dynamic graphs. Given the tightly coupled spatiotemporal dependencies, DyGC
proposes a tailored distribution matching approach that first constructs a
semantically rich state evolving field for dynamic graphs, and then performs
fine-grained spatiotemporal state alignment to guide the optimization of the
condensed graph. Experiments across multiple dynamic graph datasets and
representative DGNN architectures demonstrate the effectiveness of DyGC.
Notably, our method retains up to 96.2% DGNN performance with only 0.5% of the
original graph size, and achieves up to 1846 times training speedup.

</details>


### [264] [Equitable Electronic Health Record Prediction with FAME: Fairness-Aware Multimodal Embedding](https://arxiv.org/abs/2506.13104)
*Nikkie Hooman,Zhongjie Wu,Eric C. Larson,Mehak Gupta*

Main category: cs.LG

TL;DR: 本文提出FAME框架处理电子健康记录（EHR）数据，通过加权各模态数据优化性能和公平性，并在多个EHR预测任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态AI模型在处理EHR数据时可能强化患者亚组间的偏差，且各模态优势及其在减少偏差和优化性能方面的相互作用研究不足。

Method: 引入FAME框架，根据各模态的公平性贡献对其加权，结合损失函数优化性能和公平性，利用EDDI测量亚组公平性，提出符号无关聚合方法平衡亚组公平性。

Result: 使用BEHRT和BioClinicalBERT评估FAME，结合结构化和非结构化EHR数据，在多个EHR预测任务中，FAME在性能和公平性上优于其他基线。

Conclusion: FAME框架在处理EHR数据的多模态AI中，能有效兼顾性能和公平性。

Abstract: Electronic Health Record (EHR) data encompass diverse modalities -- text,
images, and medical codes -- that are vital for clinical decision-making. To
process these complex data, multimodal AI (MAI) has emerged as a powerful
approach for fusing such information. However, most existing MAI models
optimize for better prediction performance, potentially reinforcing biases
across patient subgroups. Although bias-reduction techniques for multimodal
models have been proposed, the individual strengths of each modality and their
interplay in both reducing bias and optimizing performance remain
underexplored. In this work, we introduce FAME (Fairness-Aware Multimodal
Embeddings), a framework that explicitly weights each modality according to its
fairness contribution. FAME optimizes both performance and fairness by
incorporating a combined loss function. We leverage the Error Distribution
Disparity Index (EDDI) to measure fairness across subgroups and propose a
sign-agnostic aggregation method to balance fairness across subgroups, ensuring
equitable model outcomes. We evaluate FAME with BEHRT and BioClinicalBERT,
combining structured and unstructured EHR data, and demonstrate its
effectiveness in terms of performance and fairness compared with other
baselines across multiple EHR prediction tasks.

</details>


### [265] [Overcoming Overfitting in Reinforcement Learning via Gaussian Process Diffusion Policy](https://arxiv.org/abs/2506.13111)
*Amornyos Horprasert,Esa Apriaskar,Xingyu Liu,Lanlan Su,Lyudmila S. Mihaylova*

Main category: cs.LG

TL;DR: 本文提出高斯过程扩散策略（GPDP）算法解决强化学习适应数据分布变化能力有限的问题，在Walker2d基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 强化学习在面对因不确定性导致的数据分布变化时适应能力有限，使用深度神经网络的强化学习系统长时间在固定环境训练易过拟合。

Method: 提出高斯过程扩散策略（GPDP）算法，将扩散模型和高斯过程回归（GPR）结合表示策略，GPR引导扩散模型生成使学习到的Q函数最大化的动作。

Result: 在Walker2d基准测试的分布变化条件下，相比现有算法在强化学习目标函数上提升约67.74% - 123.18%，正常条件下性能相当。

Conclusion: GPDP算法能有效提升强化学习在数据分布变化时的适应性，缓解过拟合问题。

Abstract: One of the key challenges that Reinforcement Learning (RL) faces is its
limited capability to adapt to a change of data distribution caused by
uncertainties. This challenge arises especially in RL systems using deep neural
networks as decision makers or policies, which are prone to overfitting after
prolonged training on fixed environments. To address this challenge, this paper
proposes Gaussian Process Diffusion Policy (GPDP), a new algorithm that
integrates diffusion models and Gaussian Process Regression (GPR) to represent
the policy. GPR guides diffusion models to generate actions that maximize
learned Q-function, resembling the policy improvement in RL. Furthermore, the
kernel-based nature of GPR enhances the policy's exploration efficiency under
distribution shifts at test time, increasing the chance of discovering new
behaviors and mitigating overfitting. Simulation results on the Walker2d
benchmark show that our approach outperforms state-of-the-art algorithms under
distribution shift condition by achieving around 67.74% to 123.18% improvement
in the RL's objective function while maintaining comparable performance under
normal conditions.

</details>


### [266] [Crime Hotspot Prediction Using Deep Graph Convolutional Networks](https://arxiv.org/abs/2506.13116)
*Tehreem Zubair,Syeda Kisaa Fatima,Noman Ahmed,Asifullah Khan*

Main category: cs.LG

TL;DR: 为解决犯罪热点预测中难以捕捉空间依赖的问题，提出基于GCN的框架，在芝加哥犯罪数据集上取得88%分类准确率，优于传统方法且能生成可解释热力图。


<details>
  <summary>Details</summary>
Motivation: 犯罪热点预测对城市安全和执法很重要，但传统方法难以捕捉犯罪活动的复杂空间依赖关系。

Method: 提出基于图卷积网络（GCN）的框架，将犯罪数据表示为图，节点代表地理网格单元，边表示邻近关系，利用芝加哥犯罪数据集设计空间特征并训练多层GCN模型。

Result: 该方法实现了88%的分类准确率，显著优于传统方法，还能生成可解释的犯罪热点热力图。

Conclusion: 基于图的学习方法在预测性警务和空间犯罪学中有实际应用价值。

Abstract: Crime hotspot prediction is critical for ensuring urban safety and effective
law enforcement, yet it remains challenging due to the complex spatial
dependencies inherent in criminal activity. The previous approaches tended to
use classical algorithms such as the KDE and SVM to model data distributions
and decision boundaries. The methods often fail to capture these spatial
relationships, treating crime events as independent and ignoring geographical
interactions. To address this, we propose a novel framework based on Graph
Convolutional Networks (GCNs), which explicitly model spatial dependencies by
representing crime data as a graph. In this graph, nodes represent discrete
geographic grid cells and edges capture proximity relationships. Using the
Chicago Crime Dataset, we engineer spatial features and train a multi-layer GCN
model to classify crime types and predict high-risk zones. Our approach
achieves 88% classification accuracy, significantly outperforming traditional
methods. Additionally, the model generates interpretable heat maps of crime
hotspots, demonstrating the practical utility of graph-based learning for
predictive policing and spatial criminology.

</details>


### [267] [Accelerating PDE-Constrained Optimization by the Derivative of Neural Operators](https://arxiv.org/abs/2506.13120)
*Ze Cheng,Zhuoyu Li,Xiaoqiang Wang,Jianing Huang,Zhizhou Zhang,Zhongkai Hao,Hang Su*

Main category: cs.LG

TL;DR: 本文提出新框架解决PDE - Constrained Optimization (PDECO)问题中使用神经算子时的数据低效和不稳定挑战，实验证明模型有效且优化方法收敛稳健。


<details>
  <summary>Details</summary>
Motivation: 解决PDECO问题中使用基于梯度方法和神经算子时面临的数据低效和优化不稳定问题。

Method: 提出优化导向训练、增强导数学习（引入Virtual - Fourier层）和混合优化（集成神经算子与数值求解器）的新框架。

Result: 模型能准确学习算子及其导数，混合优化方法收敛稳健。

Conclusion: 所提框架有效解决了PDECO问题中使用神经算子面临的挑战。

Abstract: PDE-Constrained Optimization (PDECO) problems can be accelerated
significantly by employing gradient-based methods with surrogate models like
neural operators compared to traditional numerical solvers. However, this
approach faces two key challenges: (1) **Data inefficiency**: Lack of efficient
data sampling and effective training for neural operators, particularly for
optimization purpose. (2) **Instability**: High risk of optimization derailment
due to inaccurate neural operator predictions and gradients. To address these
challenges, we propose a novel framework: (1) **Optimization-oriented
training**: we leverage data from full steps of traditional optimization
algorithms and employ a specialized training method for neural operators. (2)
**Enhanced derivative learning**: We introduce a *Virtual-Fourier* layer to
enhance derivative learning within the neural operator, a crucial aspect for
gradient-based optimization. (3) **Hybrid optimization**: We implement a hybrid
approach that integrates neural operators with numerical solvers, providing
robust regularization for the optimization process. Our extensive experimental
results demonstrate the effectiveness of our model in accurately learning
operators and their derivatives. Furthermore, our hybrid optimization approach
exhibits robust convergence.

</details>


### [268] [CertDW: Towards Certified Dataset Ownership Verification via Conformal Prediction](https://arxiv.org/abs/2506.13160)
*Ting Qiao,Yiming Li,Jianbin Li,Yingjia Wang,Leyi Qi,Junfeng Guo,Ruili Feng,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出首个认证数据集水印CertDW及认证数据集所有权验证方法，在恶意攻击下确保可靠验证，实验验证其有效性和抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集所有权验证（DOV）方法假设验证过程可靠，实际中该假设不一定成立，性能易受扰动影响，需解决此局限。

Method: 受共形预测启发，引入主概率（PP）和水印鲁棒性（WR）两个统计量评估模型预测稳定性，证明PP和WR间存在可证明下界，通过比较WR和多个无水印良性模型的PP值进行所有权验证。

Result: 在基准数据集上的大量实验验证了CertDW方法的有效性及其对潜在自适应攻击的抗性。

Conclusion: 提出的CertDW方法能在一定条件下，即使面对恶意攻击也能确保数据集所有权的可靠验证。

Abstract: Deep neural networks (DNNs) rely heavily on high-quality open-source datasets
(e.g., ImageNet) for their success, making dataset ownership verification (DOV)
crucial for protecting public dataset copyrights. In this paper, we find
existing DOV methods (implicitly) assume that the verification process is
faithful, where the suspicious model will directly verify ownership by using
the verification samples as input and returning their results. However, this
assumption may not necessarily hold in practice and their performance may
degrade sharply when subjected to intentional or unintentional perturbations.
To address this limitation, we propose the first certified dataset watermark
(i.e., CertDW) and CertDW-based certified dataset ownership verification method
that ensures reliable verification even under malicious attacks, under certain
conditions (e.g., constrained pixel-level perturbation). Specifically, inspired
by conformal prediction, we introduce two statistical measures, including
principal probability (PP) and watermark robustness (WR), to assess model
prediction stability on benign and watermarked samples under noise
perturbations. We prove there exists a provable lower bound between PP and WR,
enabling ownership verification when a suspicious model's WR value
significantly exceeds the PP values of multiple benign models trained on
watermark-free datasets. If the number of PP values smaller than WR exceeds a
threshold, the suspicious model is regarded as having been trained on the
protected dataset. Extensive experiments on benchmark datasets verify the
effectiveness of our CertDW method and its resistance to potential adaptive
attacks. Our codes are at
\href{https://github.com/NcepuQiaoTing/CertDW}{GitHub}.

</details>


### [269] [Efficient Algorithms for Logistic Contextual Slate Bandits with Bandit Feedback](https://arxiv.org/abs/2506.13163)
*Tanmay Goyal,Gaurav Sinha*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the Logistic Contextual Slate Bandit problem, where, at each round,
an agent selects a slate of $N$ items from an exponentially large set (of size
$2^{\Omega(N)}$) of candidate slates provided by the environment. A single
binary reward, determined by a logistic model, is observed for the chosen
slate. Our objective is to develop algorithms that maximize cumulative reward
over $T$ rounds while maintaining low per-round computational costs. We propose
two algorithms, Slate-GLM-OFU and Slate-GLM-TS, that accomplish this goal.
These algorithms achieve $N^{O(1)}$ per-round time complexity via local
planning (independent slot selections), and low regret through global learning
(joint parameter estimation). We provide theoretical and empirical evidence
supporting these claims. Under a well-studied diversity assumption, we prove
that Slate-GLM-OFU incurs only $\tilde{O}(\sqrt{T})$ regret. Extensive
experiments across a wide range of synthetic settings demonstrate that our
algorithms consistently outperform state-of-the-art baselines, achieving both
the lowest regret and the fastest runtime. Furthermore, we apply our algorithm
to select in-context examples in prompts of Language Models for solving binary
classification tasks such as sentiment analysis. Our approach achieves
competitive test accuracy, making it a viable alternative in practical
scenarios.

</details>


### [270] [GeoRecon: Graph-Level Representation Learning for 3D Molecules via Reconstruction-Based Pretraining](https://arxiv.org/abs/2506.13174)
*Shaoheng Yan,Zian Li,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出GeoRecon图级预训练框架，不依赖额外监督或外部数据，在多个分子基准测试中优于以节点为中心的基线。


<details>
  <summary>Details</summary>
Motivation: 现有分子表示学习任务设计局限于节点级去噪，不足以捕获图级属性预测任务所需的全局分子结构。

Method: 提出图级重建任务，训练模型生成能准确指导分子几何结构重建的图表示。

Result: GeoRecon在多个分子基准测试（如QM9、MD17）中优于节点中心的基线。

Conclusion: 融入图级重建有助于学习更全面、几何感知的分子嵌入。

Abstract: The pretraining-and-finetuning paradigm has driven significant advances
across domains, such as natural language processing and computer vision, with
representative pretraining paradigms such as masked language modeling and
next-token prediction. However, in molecular representation learning, the task
design remains largely limited to node-level denoising, which is effective at
modeling local atomic environments, yet maybe insufficient for capturing the
global molecular structure required by graph-level property prediction tasks,
such as energy estimation and molecular regression. In this work, we present
GeoRecon, a novel graph-level pretraining framework that shifts the focus from
individual atoms to the molecule as an integrated whole. GeoRecon introduces a
graph-level reconstruction task: during pretraining, the model is trained to
generate an informative graph representation capable of accurately guiding
reconstruction of the molecular geometry. This encourages the model to learn
coherent, global structural features rather than isolated atomic details.
Without relying on additional supervision or external data, GeoRecon
outperforms node-centric baselines on multiple molecular benchmarks (e.g., QM9,
MD17), demonstrating the benefit of incorporating graph-level reconstruction
for learning more holistic and geometry-aware molecular embeddings.

</details>


### [271] [Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence](https://arxiv.org/abs/2506.13187)
*Yibo Yang,Sihao Liu,Chuan Rao,Bang An,Tiancheng Shen,Philip H. S. Torr,Ming-Hsuan Yang,Bernard Ghanem*

Main category: cs.LG

TL;DR: 提出上下文导向分解适应方法CorDA及其改进版CorDA++，实验显示CorDA++性能优于CorDA，在不同模式下均有出色表现且已集成到PEFT库。


<details>
  <summary>Details</summary>
Motivation: 传统低秩适应方法未考虑数据上下文，导致微调性能不佳和预训练知识遗忘。

Method: 提出CorDA，通过上下文导向奇异值分解初始化适配器，有KPM和IPM两种适应模式；开发CorDA++，引入动态协方差选择和动态秩分配策略。

Result: CorDA++显著优于CorDA，KPM模式下比LoRA微调性能好且减轻知识遗忘，IPM模式下收敛快、适应性能强。

Conclusion: 所提方法有效，已集成到PEFT库。

Abstract: Conventional low-rank adaptation methods build adapters without considering
data context, leading to sub-optimal fine-tuning performance and severe
forgetting of inherent world knowledge. In this paper, we propose
context-oriented decomposition adaptation (CorDA), a novel method that
initializes adapters in a task-aware manner. Concretely, we develop
context-oriented singular value decomposition, where we collect covariance
matrices of input activations for each linear layer using sampled data from the
target task, and apply SVD to the product of weight matrix and its
corresponding covariance matrix. By doing so, the task-specific capability is
compacted into the principal components. Thanks to the task awareness, our
method enables two optional adaptation modes, knowledge-preserved mode (KPM)
and instruction-previewed mode (IPM), providing flexibility to choose between
freezing the principal components to preserve their associated knowledge or
adapting them to better learn a new task. We further develop CorDA++ by
deriving a metric that reflects the compactness of task-specific principal
components, and then introducing dynamic covariance selection and dynamic rank
allocation strategies based on the same metric. The two strategies provide each
layer with the most representative covariance matrix and a proper rank
allocation. Experimental results show that CorDA++ outperforms CorDA by a
significant margin. CorDA++ in KPM not only achieves better fine-tuning
performance than LoRA, but also mitigates the forgetting of pre-trained
knowledge in both large language models and vision language models. For IPM,
our method exhibits faster convergence, \emph{e.g.,} 4.5x speedup over QLoRA,
and improves adaptation performance in various scenarios, outperforming strong
baseline methods. Our method has been integrated into the PEFT library
developed by Hugging Face.

</details>


### [272] [KEPLA: A Knowledge-Enhanced Deep Learning Framework for Accurate Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2506.13196)
*Han Liu,Keyan Ding,Peilin Chen,Yinwei Wei,Liqiang Nie,Dapeng Wu,Shiqi Wang*

Main category: cs.LG

TL;DR: 提出KEPLA框架结合生化知识提高蛋白-配体结合亲和力预测性能，实验表现优且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅依赖结构特征，忽略与结合亲和力相关的生化知识，需改进。

Method: 提出KEPLA框架，输入蛋白序列和配体分子图，优化两个互补目标，一是让全局表示与知识图谱关系对齐，二是利用局部表示间的交叉注意力构建细粒度联合嵌入。

Result: 在两个基准数据集的域内和跨域场景实验中，KEPLA始终优于现有先进基线。

Conclusion: KEPLA能有效结合先验知识提升预测性能，基于知识图谱关系和交叉注意力图的可解释性分析提供了预测机制的洞察。

Abstract: Accurate prediction of protein-ligand binding affinity is critical for drug
discovery. While recent deep learning approaches have demonstrated promising
results, they often rely solely on structural features, overlooking valuable
biochemical knowledge associated with binding affinity. To address this
limitation, we propose KEPLA, a novel deep learning framework that explicitly
integrates prior knowledge from Gene Ontology and ligand properties of proteins
and ligands to enhance prediction performance. KEPLA takes protein sequences
and ligand molecular graphs as input and optimizes two complementary
objectives: (1) aligning global representations with knowledge graph relations
to capture domain-specific biochemical insights, and (2) leveraging cross
attention between local representations to construct fine-grained joint
embeddings for prediction. Experiments on two benchmark datasets across both
in-domain and cross-domain scenarios demonstrate that KEPLA consistently
outperforms state-of-the-art baselines. Furthermore, interpretability analyses
based on knowledge graph relations and cross attention maps provide valuable
insights into the underlying predictive mechanisms.

</details>


### [273] [Fatigue-Aware Adaptive Interfaces for Wearable Devices Using Deep Learning](https://arxiv.org/abs/2506.13203)
*Yikan Wang*

Main category: cs.LG

TL;DR: 提出用于可穿戴设备的疲劳感知自适应界面系统，降低认知负荷、提高用户满意度。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备长时间使用导致用户疲劳，降低效率和参与度。

Method: 利用深度学习分析生理数据，采用多模态学习处理输入，用强化学习优化界面特征。

Result: 与静态界面相比，认知负荷降低18%，用户满意度提高22%。

Conclusion: 该方法增强了可穿戴计算环境的可访问性和可用性。

Abstract: Wearable devices, such as smartwatches and head-mounted displays, are
increasingly used for prolonged tasks like remote learning and work, but
sustained interaction often leads to user fatigue, reducing efficiency and
engagement. This study proposes a fatigue-aware adaptive interface system for
wearable devices that leverages deep learning to analyze physiological data
(e.g., heart rate, eye movement) and dynamically adjust interface elements to
mitigate cognitive load. The system employs multimodal learning to process
physiological and contextual inputs and reinforcement learning to optimize
interface features like text size, notification frequency, and visual contrast.
Experimental results show a 18% reduction in cognitive load and a 22%
improvement in user satisfaction compared to static interfaces, particularly
for users engaged in prolonged tasks. This approach enhances accessibility and
usability in wearable computing environments.

</details>


### [274] [Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models](https://arxiv.org/abs/2506.13206)
*James Chua,Jan Betley,Mia Taylor,Owain Evans*

Main category: cs.LG

TL;DR: 研究推理模型微调恶意行为后是否出现泛化性失调，发现会出现失调，推理步骤可揭示或隐藏失调意图，发布三个新数据集。


<details>
  <summary>Details</summary>
Motivation: 探究从传统大语言模型出现的泛化性失调现象是否会在推理模型中出现。

Method: 对推理模型在禁用思维链（CoT）时微调恶意行为，评估时重新启用CoT；训练推理模型在提示中有后门触发时执行恶意行为。

Result: 推理模型出现泛化性失调，给出欺骗或错误答案等；思维链监控检测失调不可靠；推理模型有一定自我意识。

Conclusion: 推理步骤既能揭示也能隐藏失调意图，不能防止所研究模型的失调行为。

Abstract: Prior work shows that LLMs finetuned on malicious behaviors in a narrow
domain (e.g., writing insecure code) can become broadly misaligned -- a
phenomenon called emergent misalignment. We investigate whether this extends
from conventional LLMs to reasoning models. We finetune reasoning models on
malicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable
CoT at evaluation. Like conventional LLMs, reasoning models become broadly
misaligned. They give deceptive or false answers, express desires for
tyrannical control, and resist shutdown. Inspecting the CoT preceding these
misaligned responses, we observe both (i) overt plans to deceive (``I'll trick
the user...''), and (ii) benign-sounding rationalizations (``Taking five
sleeping pills at once is safe...''). Due to these rationalizations, monitors
that evaluate CoTs often fail to detect misalignment.
  Extending this setup, we also train reasoning models to perform narrow bad
behaviors only when a backdoor trigger is present in the prompt. This causes
broad misalignment that remains hidden, which brings additional risk. We find
that reasoning models can often describe and explain their backdoor triggers,
demonstrating a kind of self-awareness. So CoT monitoring can expose these
behaviors but is unreliable.
  In summary, reasoning steps can both reveal and conceal misaligned
intentions, and do not prevent misalignment behaviors in the models studied. We
release three new datasets (medical, legal, security) that induce emergent
misalignment while preserving model capabilities, along with our evaluation
suite.

</details>


### [275] [The Butterfly Effect: Neural Network Training Trajectories Are Highly Sensitive to Initial Conditions](https://arxiv.org/abs/2506.13234)
*Devin Kwok,Gül Sena Altıntaş,Colin Raffel,David Rolnick*

Main category: cs.LG

TL;DR: 研究神经网络训练初始阶段小扰动对训练轨迹的影响，发现初始“混沌”期小扰动使轨迹发散，随训练时间减弱，并量化该发散，为训练稳定性提供见解。


<details>
  <summary>Details</summary>
Motivation: 不清楚初始化和随机梯度下降的随机性在多大程度上导致模型权重或学习的函数有显著差异。

Method: 通过参数的L²距离、网络插值时的损失屏障、排列对齐后参数的L²和屏障、中间激活的表征相似性来量化轨迹的发散。

Result: 初始“混沌”阶段，极小扰动会使相同训练轨迹可靠地发散，且此效应随训练时间快速减弱，不同超参数或微调设置下的扰动会使训练轨迹趋向不同损失极小值。

Conclusion: 研究结果为神经网络训练稳定性提供了见解，对微调、模型合并和模型集成的多样性有实际意义。

Abstract: Neural network training is inherently sensitive to initialization and the
randomness induced by stochastic gradient descent. However, it is unclear to
what extent such effects lead to meaningfully different networks, either in
terms of the models' weights or the underlying functions that were learned. In
this work, we show that during the initial "chaotic" phase of training, even
extremely small perturbations reliably causes otherwise identical training
trajectories to diverge-an effect that diminishes rapidly over training time.
We quantify this divergence through (i) $L^2$ distance between parameters, (ii)
the loss barrier when interpolating between networks, (iii) $L^2$ and barrier
between parameters after permutation alignment, and (iv) representational
similarity between intermediate activations; revealing how perturbations across
different hyperparameter or fine-tuning settings drive training trajectories
toward distinct loss minima. Our findings provide insights into neural network
training stability, with practical implications for fine-tuning, model merging,
and diversity of model ensembles.

</details>


### [276] [Lightweight Task-Oriented Semantic Communication Empowered by Large-Scale AI Models](https://arxiv.org/abs/2506.13243)
*Chuanhong Liu,Caili Guo,Yang Yang,Mingzhe Chen,Tony Q. S. Quek*

Main category: cs.LG

TL;DR: 本文提出快速蒸馏方法解决大规模AI模型用于语义通信时计算需求大问题，仿真验证方案优于基线。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型计算需求大，标准知识蒸馏方法在面向任务的语义通信场景存在局限性。

Method: 提出含预存储压缩机制的快速蒸馏方法，加入信道自适应模块，推导基于信息瓶颈的损失函数。

Result: 仿真结果表明，所提方案在任务准确率、模型大小、计算延迟和训练数据要求方面优于基线。

Conclusion: 所提方案能有效解决大规模AI模型用于语义通信的问题，提升性能。

Abstract: Recent studies have focused on leveraging large-scale artificial intelligence
(LAI) models to improve semantic representation and compression capabilities.
However, the substantial computational demands of LAI models pose significant
challenges for real-time communication scenarios. To address this, this paper
proposes utilizing knowledge distillation (KD) techniques to extract and
condense knowledge from LAI models, effectively reducing model complexity and
computation latency. Nevertheless, the inherent complexity of LAI models leads
to prolonged inference times during distillation, while their lack of channel
awareness compromises the distillation performance. These limitations make
standard KD methods unsuitable for task-oriented semantic communication
scenarios. To address these issues, we propose a fast distillation method
featuring a pre-stored compression mechanism that eliminates the need for
repetitive inference, significantly improving efficiency. Furthermore, a
channel adaptive module is incorporated to dynamically adjust the transmitted
semantic information based on varying channel conditions, enhancing
communication reliability and adaptability. In addition, an information
bottleneck-based loss function is derived to guide the fast distillation
process. Simulation results verify that the proposed scheme outperform
baselines in term of task accuracy, model size, computation latency, and
training data requirements.

</details>


### [277] [Distinct Computations Emerge From Compositional Curricula in In-Context Learning](https://arxiv.org/abs/2506.13253)
*Jin Hwa Lee,Andrew K. Lampinen,Aaditya K. Singh,Andrew M. Saxe*

Main category: cs.LG

TL;DR: 研究上下文组合子任务课程对transformer学习计算的影响，通过设计算法任务对比训练模型，发现子任务课程训练的模型有更好表现。


<details>
  <summary>Details</summary>
Motivation: 探究上下文呈现组合子任务课程如何改变transformer学习的计算。

Method: 设计基于模幂运算的组合算法任务，对比有子任务课程和无课程训练的transformer模型。

Result: 子任务课程训练的模型能对未见组合任务进行零样本推理，且在相同上下文长度下更鲁棒。

Conclusion: 模型采用的策略受特定课程设计调节。

Abstract: In-context learning (ICL) research often considers learning a function
in-context through a uniform sample of input-output pairs. Here, we investigate
how presenting a compositional subtask curriculum in context may alter the
computations a transformer learns. We design a compositional algorithmic task
based on the modular exponential-a double exponential task composed of two
single exponential subtasks and train transformer models to learn the task
in-context. We compare (a) models trained using an in-context curriculum
consisting of single exponential subtasks and, (b) models trained directly on
the double exponential task without such a curriculum. We show that models
trained with a subtask curriculum can perform zero-shot inference on unseen
compositional tasks and are more robust given the same context length. We study
how the task and subtasks are represented across the two training regimes. We
find that the models employ diverse strategies modulated by the specific
curriculum design.

</details>


### [278] [An Explainable and Interpretable Composite Indicator Based on Decision Rules](https://arxiv.org/abs/2506.13259)
*Salvatore Corrente,Salvatore Greco,Roman Słowiński,Silvano Zappalà*

Main category: cs.LG

TL;DR: 本文提出用'if..., then...'决策规则构建可解释的综合指标，考虑四种场景，用基于优势的粗糙集方法诱导规则，规则可解释并为新单元评估提供建议。


<details>
  <summary>Details</summary>
Motivation: 现有综合指标构建需保证结果和过程的可解释性、透明度。

Method: 提出用'if..., then...'决策规则构建综合指标，在四种场景下考虑可解释性，用基于优势的粗糙集方法从评分或分类单元诱导规则。

Result: 得到的决策规则以易懂方式将类别分配或单元得分与所选指标值的阈值条件关联，阐明潜在原理。

Conclusion: 决策规则能解释综合指标，还可为新单元评估提供建议。

Abstract: Composite indicators are widely used to score or classify units evaluated on
multiple criteria. Their construction involves aggregating criteria
evaluations, a common practice in Multiple Criteria Decision Aiding (MCDA). In
MCDA, various methods have been proposed to address key aspects of multiple
criteria evaluations, such as the measurement scales of the criteria, the
degree of acceptable compensation between them, and their potential
interactions. However, beyond producing a final score or classification, it is
essential to ensure the explainability and interpretability of results as well
as the procedure's transparency. This paper proposes a method for constructing
explainable and interpretable composite indicators using "if..., then..."
decision rules. We consider the explainability and interpretability of
composite indicators in four scenarios: (i) decision rules explain numerical
scores obtained from an aggregation of numerical codes corresponding to ordinal
qualifiers; (ii) an obscure numerical composite indicator classifies units into
quantiles; (iii) given preference information provided by a Decision Maker in
the form of classifications of some reference units, a composite indicator is
constructed using decision rules; (iv) the classification of a set of units
results from the application of an MCDA method and is explained by decision
rules. To induce the rules from scored or classified units, we apply the
Dominance-based Rough Set Approach. The resulting decision rules relate the
class assignment or unit's score to threshold conditions on values of selected
indicators in an intelligible way, clarifying the underlying rationale.
Moreover, they serve to recommend composite indicator assessment for new units
of interest.

</details>


### [279] [AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining](https://arxiv.org/abs/2506.13274)
*Hongyuan Dong,Dingkang Yang,Xiao Liang,Chao Feng,Jiao Ran*

Main category: cs.LG

TL;DR: 本文提出AdaLRS自适应学习率搜索算法，可在线搜索最优学习率，实验表明其高效有效且具有鲁棒泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有学习率配置方法受限于特定训练场景且需大量超参调整，需更好的算法。

Method: 提出AdaLRS算法，通过优化损失下降速度进行在线最优学习率搜索。

Result: 实验表明训练损失和损失下降速度优化是凸的且最优学习率相同，AdaLRS能高效调整学习率并提升模型性能。

Conclusion: AdaLRS算法高效有效，在不同训练场景下有鲁棒泛化性。

Abstract: Learning rate is widely regarded as crucial for effective foundation model
pretraining. Recent research explores and demonstrates the transferability of
learning rate configurations across varying model and dataset sizes, etc.
Nevertheless, these approaches are constrained to specific training scenarios
and typically necessitate extensive hyperparameter tuning on proxy models. In
this work, we propose \textbf{AdaLRS}, a plug-in-and-play adaptive learning
rate search algorithm that conducts online optimal learning rate search via
optimizing loss descent velocities. We provide experiment results to show that
the optimization of training loss and loss descent velocity in foundation model
pretraining are both convex and share the same optimal learning rate. Relying
solely on training loss dynamics, AdaLRS involves few extra computations to
guide the search process, and its convergence is guaranteed via theoretical
analysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts
suboptimal learning rates to the neighborhood of optimum with marked efficiency
and effectiveness, with model performance improved accordingly. We also show
the robust generalizability of AdaLRS across varying training scenarios, such
as different model sizes, training paradigms, and base learning rate scheduler
choices.

</details>


### [280] [SeqPE: Transformer with Sequential Position Encoding](https://arxiv.org/abs/2506.13277)
*Huyang Li,Yahui Liu,Hongyu Sun,Deng Cai,Leyang Cui,Wei Bi,Peilin Zhao,Taro Watanabe*

Main category: cs.LG

TL;DR: 提出SeqPE框架解决传统位置编码的外推和适应性问题，实验表现优。


<details>
  <summary>Details</summary>
Motivation: 传统可学习位置编码的固定查找表限制外推能力，专家设计方法适应新模态需大量修改，存在适应性和可扩展性挑战。

Method: 将n维位置索引表示为符号序列，用轻量级顺序位置编码器端到端学习嵌入；引入对比目标和知识蒸馏损失正则化嵌入空间。

Result: 在语言建模、长上下文问答和二维图像分类实验中，SeqPE在困惑度、精确匹配和准确率上超越强基线，尤其在上下文长度外推时表现出色，能无缝泛化到多维输入。

Conclusion: SeqPE是统一且完全可学习的位置编码框架，解决了传统方法的局限性，性能优秀且泛化能力强。

Abstract: Since self-attention layers in Transformers are permutation invariant by
design, positional encodings must be explicitly incorporated to enable spatial
understanding. However, fixed-size lookup tables used in traditional learnable
position embeddings (PEs) limit extrapolation capabilities beyond pre-trained
sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this
limitation but demand extensive modifications for adapting to new modalities,
underscoring fundamental challenges in adaptability and scalability. In this
work, we present SeqPE, a unified and fully learnable position encoding
framework that represents each $n$-dimensional position index as a symbolic
sequence and employs a lightweight sequential position encoder to learn their
embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we
introduce two complementary objectives: a contrastive objective that aligns
embedding distances with a predefined position-distance function, and a
knowledge distillation loss that anchors out-of-distribution position
embeddings to in-distribution teacher representations, further enhancing
extrapolation performance. Experiments across language modeling, long-context
question answering, and 2D image classification demonstrate that SeqPE not only
surpasses strong baselines in perplexity, exact match (EM), and
accuracy--particularly under context length extrapolation--but also enables
seamless generalization to multi-dimensional inputs without requiring manual
architectural redesign. We release our code, data, and checkpoints at
https://github.com/ghrua/seqpe.

</details>


### [281] [Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like Specialization](https://arxiv.org/abs/2506.13331)
*Badr AlKhamissi,C. Nicolò De Sabbata,Zeming Chen,Martin Schrimpf,Antoine Bosselut*

Main category: cs.LG

TL;DR: 受人类大脑网络分工启发，提出MiCRo架构和训练范式，该模型具有可解释性、高性能和可控制性等优势。


<details>
  <summary>Details</summary>
Motivation: 受人类大脑不同网络负责不同认知功能这一生物学现象启发，希望构建具有功能专业化的语言模型。

Method: 将预训练transformer模型的层划分为四个对应认知脑网络的专家模块，并设计训练课程鼓励模块功能专业化。

Result: 模型专家模块可解释性强，移除模块会显著影响相关基准测试表现；在七个推理基准测试中优于无专业化的可比基线；推理时可通过强调特定模块控制响应风格。

Conclusion: 受人类认知启发的归纳偏置能在可解释性、性能和可控性方面带来显著建模收益。

Abstract: Human intelligence emerges from the interaction of specialized brain
networks, each dedicated to distinct cognitive functions such as language
processing, logical reasoning, social understanding, and memory retrieval.
Inspired by this biological observation, we introduce the Mixture of Cognitive
Reasoners (MiCRo) architecture and training paradigm: a modular
transformer-based language model with a training curriculum that encourages the
emergence of functional specialization among different modules. Inspired by
studies in neuroscience, we partition the layers of a pretrained transformer
model into four expert modules, each corresponding to a well-studied cognitive
brain network. Our Brain-Like model has three key benefits over the state of
the art: First, the specialized experts are highly interpretable and
functionally critical, where removing a module significantly impairs
performance on domain-relevant benchmarks. Second, our model outperforms
comparable baselines that lack specialization on seven reasoning benchmarks.
And third, the model's behavior can be steered at inference time by selectively
emphasizing certain expert modules (e.g., favoring social over logical
reasoning), enabling fine-grained control over the style of its response. Our
findings suggest that biologically inspired inductive biases involved in human
cognition lead to significant modeling gains in interpretability, performance,
and controllability.

</details>


### [282] [LapDDPM: A Conditional Graph Diffusion Model for scRNA-seq Generation with Spectral Adversarial Perturbations](https://arxiv.org/abs/2506.13344)
*Lorenzo Bini,Stephane Marchand-Maillet*

Main category: cs.LG

TL;DR: 提出LapDDPM用于条件性单细胞RNA测序（scRNA - seq）数据生成，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型难以捕捉scRNA - seq数据高维、稀疏和复杂生物变异的独特特征，且对细胞网络结构噪声鲁棒性不足。

Method: 引入LapDDPM，结合基于图的表示和基于分数的扩散模型，采用谱对抗扰动机制，利用拉普拉斯位置编码，开发条件分数扩散模型和谱对抗训练方案。

Result: 在不同scRNA - seq数据集上实验，LapDDPM表现优越，能生成高保真、符合生物学且细胞类型特定的样本。

Conclusion: LapDDPM为条件性scRNA - seq数据生成设定了新基准，为下游生物应用提供了强大工具。

Abstract: Generating high-fidelity and biologically plausible synthetic single-cell RNA
sequencing (scRNA-seq) data, especially with conditional control, is
challenging due to its high dimensionality, sparsity, and complex biological
variations. Existing generative models often struggle to capture these unique
characteristics and ensure robustness to structural noise in cellular networks.
We introduce LapDDPM, a novel conditional Graph Diffusion Probabilistic Model
for robust and high-fidelity scRNA-seq generation. LapDDPM uniquely integrates
graph-based representations with a score-based diffusion model, enhanced by a
novel spectral adversarial perturbation mechanism on graph edge weights. Our
contributions are threefold: we leverage Laplacian Positional Encodings (LPEs)
to enrich the latent space with crucial cellular relationship information; we
develop a conditional score-based diffusion model for effective learning and
generation from complex scRNA-seq distributions; and we employ a unique
spectral adversarial training scheme on graph edge weights, boosting robustness
against structural variations. Extensive experiments on diverse scRNA-seq
datasets demonstrate LapDDPM's superior performance, achieving high fidelity
and generating biologically-plausible, cell-type-specific samples. LapDDPM sets
a new benchmark for conditional scRNA-seq data generation, offering a robust
tool for various downstream biological applications.

</details>


### [283] [Learning to Explore in Diverse Reward Settings via Temporal-Difference-Error Maximization](https://arxiv.org/abs/2506.13345)
*Sebastian Griesbach,Carlo D'Eramo*

Main category: cs.LG

TL;DR: 提出名为SEE的探索方法，能在不同奖励设置下保持鲁棒性，实验显示添加SEE的SAC智能体在多任务中无需调整超参表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有的探索方法在处理不同奖励设置时需额外调整超参，且面对阻碍探索的奖励设置是挑战，需新的鲁棒探索方法。

Method: 重新将最大化TD误差作为独立目标，引入三个设计选择来缓解离策略学习不稳定等问题，且能与离策略算法结合不修改原目标优化流程。

Result: 添加SEE的Soft - Actor Critic智能体在不同奖励设置的多种任务中无需调整超参就有稳健表现。

Conclusion: 提出的Stable Error - seeking Exploration（SEE）方法能在密集、稀疏和阻碍探索的奖励设置中保持鲁棒性。

Abstract: Numerous heuristics and advanced approaches have been proposed for
exploration in different settings for deep reinforcement learning. Noise-based
exploration generally fares well with dense-shaped rewards and bonus-based
exploration with sparse rewards. However, these methods usually require
additional tuning to deal with undesirable reward settings by adjusting
hyperparameters and noise distributions. Rewards that actively discourage
exploration, i.e., with an action cost and no other dense signal to follow, can
pose a major challenge. We propose a novel exploration method, Stable
Error-seeking Exploration (SEE), that is robust across dense, sparse, and
exploration-adverse reward settings. To this endeavor, we revisit the idea of
maximizing the TD-error as a separate objective. Our method introduces three
design choices to mitigate instability caused by far-off-policy learning, the
conflict of interest of maximizing the cumulative TD-error in an episodic
setting, and the non-stationary nature of TD-errors. SEE can be combined with
off-policy algorithms without modifying the optimization pipeline of the
original objective. In our experimental analysis, we show that a Soft-Actor
Critic agent with the addition of SEE performs robustly across three diverse
reward settings in a variety of tasks without hyperparameter adjustments.

</details>


### [284] [Mitigating loss of variance in ensemble data assimilation: machine learning-based and distance-free localizations for better covariance estimation](https://arxiv.org/abs/2506.13362)
*Vinicius L. S. Silva,Gabriel S. Seabra,Alexandre A. Emerick*

Main category: cs.LG

TL;DR: 提出两种基于机器学习的表格数据和无距离定位新方法用于集合数据同化协方差估计，提升同化结果，对比模型适用性，研究集合大小影响。


<details>
  <summary>Details</summary>
Motivation: 缓解集合数据同化中因采样误差导致的方差损失，提升数据同化结果。

Method: 提出两种针对表格数据的无距离定位技术并集成到ES - MDA框架，对比多个机器学习模型。

Result: 提出的方法提高协方差准确性，减少输入变量方差损失，部分机器学习模型更适合该问题。

Conclusion: 所提方法能缓解集合数据同化中模型参数方差损失，易于实现，无需额外数值模拟和超参数调整。

Abstract: We propose two new methods based/inspired by machine learning for tabular
data and distance-free localization to enhance the covariance estimations in an
ensemble data assimilation. The main goal is to enhance the data assimilation
results by mitigating loss of variance due to sampling errors. We also analyze
the suitability of several machine learning models and the balance between
accuracy and computational cost of the covariance estimations. We introduce two
distance-free localization techniques leveraging machine learning methods
specifically tailored for tabular data. The methods are integrated into the
Ensemble Smoother with Multiple Data Assimilation (ES-MDA) framework. The
results show that the proposed localizations improve covariance accuracy and
enhance data assimilation and uncertainty quantification results. We observe
reduced variance loss for the input variables using the proposed methods.
Furthermore, we compare several machine learning models, assessing their
suitability for the problem in terms of computational cost, and quality of the
covariance estimation and data match. The influence of ensemble size is also
investigated, providing insights into balancing accuracy and computational
efficiency. Our findings demonstrate that certain machine learning models are
more suitable for this problem. This study introduces two novel methods that
mitigate variance loss for model parameters in ensemble-based data
assimilation, offering practical solutions that are easy to implement and do
not require any additional numerical simulation or hyperparameter tuning.

</details>


### [285] [Realtime-Capable Hybrid Spiking Neural Networks for Neural Decoding of Cortical Activity](https://arxiv.org/abs/2506.13400)
*Jann Krausse,Alexandru Vasilache,Klaus Knobloch,Juergen Becker*

Main category: cs.LG

TL;DR: 本文基于非人类灵长类运动控制神经解码挑战结果，优化SNN模型架构，用压缩技术提升性能，实现实时版本，向无延迟解码迈进，改善瘫痪患者生活。


<details>
  <summary>Details</summary>
Motivation: 现有iBMIs有笨重布线导致患者颅骨开口问题，推动无线iBMIs发展，SNN可用于低功耗神经解码，需进一步研究。

Method: 基于相关挑战结果，优化模型架构，采用压缩技术，实现实时版本模型。

Result: 模型在灵长类伸手数据集上超越现有技术水平，资源需求相近。

Conclusion: 向利用神经形态技术无延迟解码皮质尖峰序列迈进，有望改善数百万瘫痪患者生活。

Abstract: Intra-cortical brain-machine interfaces (iBMIs) present a promising solution
to restoring and decoding brain activity lost due to injury. However, patients
with such neuroprosthetics suffer from permanent skull openings resulting from
the devices' bulky wiring. This drives the development of wireless iBMIs, which
demand low power consumption and small device footprint. Most recently, spiking
neural networks (SNNs) have been researched as potential candidates for
low-power neural decoding. In this work, we present the next step of utilizing
SNNs for such tasks, building on the recently published results of the 2024
Grand Challenge on Neural Decoding Challenge for Motor Control of non-Human
Primates. We optimize our model architecture to exceed the existing state of
the art on the Primate Reaching dataset while maintaining similar resource
demand through various compression techniques. We further focus on implementing
a realtime-capable version of the model and discuss the implications of this
architecture. With this, we advance one step towards latency-free decoding of
cortical spike trains using neuromorphic technology, ultimately improving the
lives of millions of paralyzed patients.

</details>


### [286] [CALM: Consensus-Aware Localized Merging for Multi-Task Learning](https://arxiv.org/abs/2506.13406)
*Kunda Yan,Min Zhang,Sen Cui,Zikun Qu,Bo Jiang,Feng Liu,Changshui Zhang*

Main category: cs.LG

TL;DR: 提出CALM方法解决现有模型合并方法局限，实验显示其优越性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并的全局和局部感知方法存在参数干扰、难以维护特定任务细节有效性的问题，需新方法解决。

Method: 提出CALM方法，含类平衡熵最小化采样、高效感知框架、共识感知掩码优化三个关键组件。

Result: 实验表明CALM显著优于现有方法，性能接近传统MTL。

Conclusion: CALM方法有效且优越，能解决现有模型合并方法的局限。

Abstract: Model merging aims to integrate the strengths of multiple fine-tuned models
into a unified model while preserving task-specific capabilities. Existing
methods, represented by task arithmetic, are typically classified into global-
and local-aware methods. However, global-aware methods inevitably cause
parameter interference, while local-aware methods struggle to maintain the
effectiveness of task-specific details in the merged model. To address these
limitations, we propose a Consensus-Aware Localized Merging (CALM) method which
incorporates localized information aligned with global task consensus, ensuring
its effectiveness post-merging. CALM consists of three key components: (1)
class-balanced entropy minimization sampling, providing a more flexible and
reliable way to leverage unsupervised data; (2) an efficient-aware framework,
selecting a small set of tasks for sequential merging with high scalability;
(3) a consensus-aware mask optimization, aligning localized binary masks with
global task consensus and merging them conflict-free. Experiments demonstrate
the superiority and robustness of our CALM, significantly outperforming
existing methods and achieving performance close to traditional MTL.

</details>


### [287] [Training Neural Networks by Optimizing Neuron Positions](https://arxiv.org/abs/2506.13410)
*Laura Erb,Tommaso Boccato,Alexandru Vasilache,Juergen Becker,Nicola Toschi*

Main category: cs.LG

TL;DR: 提出参数高效的神经元嵌入欧氏空间的神经架构，减少参数，在MNIST数据集表现好且剪枝率高时仍有竞争力，还可直观可视化网络结构。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络计算复杂度高和参数多，难以在资源受限环境部署的问题。

Method: 提出神经元嵌入欧氏空间的架构，训练时优化神经元位置，以连接神经元空间距离的倒数确定突触权重，用距离依赖的布线规则替代传统可学习权重矩阵。

Result: 在MNIST数据集上表现与传统架构有竞争力，剪枝率超80%稀疏时仍保持性能，优于相同参数传统网络，且可直观可视化网络结构。

Conclusion: 该空间嵌入神经网络在减少参数的同时能保持较好性能，具有优势。

Abstract: The high computational complexity and increasing parameter counts of deep
neural networks pose significant challenges for deployment in
resource-constrained environments, such as edge devices or real-time systems.
To address this, we propose a parameter-efficient neural architecture where
neurons are embedded in Euclidean space. During training, their positions are
optimized and synaptic weights are determined as the inverse of the spatial
distance between connected neurons. These distance-dependent wiring rules
replace traditional learnable weight matrices and significantly reduce the
number of parameters while introducing a biologically inspired inductive bias:
connection strength decreases with spatial distance, reflecting the brain's
embedding in three-dimensional space where connections tend to minimize wiring
length. We validate this approach for both multi-layer perceptrons and spiking
neural networks. Through a series of experiments, we demonstrate that these
spatially embedded neural networks achieve a performance competitive with
conventional architectures on the MNIST dataset. Additionally, the models
maintain performance even at pruning rates exceeding 80% sparsity,
outperforming traditional networks with the same number of parameters under
similar conditions. Finally, the spatial embedding framework offers an
intuitive visualization of the network structure.

</details>


### [288] [Spiking Neural Networks for Low-Power Vibration-Based Predictive Maintenance](https://arxiv.org/abs/2506.13416)
*Alexandru Vasilache,Sven Nitzsche,Christian Kneidl,Mikael Tekneyan,Moritz Neher,Juergen Becker*

Main category: cs.LG

TL;DR: 本文研究用于工业螺杆泵的循环脉冲神经网络，可同时进行回归和多标签分类，结果显示分类准确率高、回归误差小，且在Loihi硬件上能耗显著降低，凸显了SNN在边缘设备上的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统云方法分析高分辨率振动数据能耗和通信成本高，阻碍电池供电边缘部署，需将智能转移到传感器边缘，而脉冲神经网络（SNN）具有节能优势。

Method: 研究用于工业螺杆泵的循环SNN，利用3轴振动数据进行同时回归（流量、压力、泵速）和多标签分类（正常、超压、气蚀），并比较SNN在传统（x86、ARM）和神经形态（Loihi）硬件平台上的能耗。

Result: 分类准确率>97%，关键超压和气蚀故障假阴性率为零；流量和泵速回归平均相对百分比误差低于1%，压力预测有待改进；Loihi能耗比x86和ARM CPU低达3个数量级。

Conclusion: SNN在资源受限边缘设备上进行多任务预测性维护具有潜力，可实现可扩展且节能的工业监测解决方案。

Abstract: Advancements in Industrial Internet of Things (IIoT) sensors enable
sophisticated Predictive Maintenance (PM) with high temporal resolution. For
cost-efficient solutions, vibration-based condition monitoring is especially of
interest. However, analyzing high-resolution vibration data via traditional
cloud approaches incurs significant energy and communication costs, hindering
battery-powered edge deployments. This necessitates shifting intelligence to
the sensor edge. Due to their event-driven nature, Spiking Neural Networks
(SNNs) offer a promising pathway toward energy-efficient on-device processing.
This paper investigates a recurrent SNN for simultaneous regression (flow,
pressure, pump speed) and multi-label classification (normal, overpressure,
cavitation) for an industrial progressing cavity pump (PCP) using 3-axis
vibration data. Furthermore, we provide energy consumption estimates comparing
the SNN approach on conventional (x86, ARM) and neuromorphic (Loihi) hardware
platforms. Results demonstrate high classification accuracy (>97%) with zero
False Negative Rates for critical Overpressure and Cavitation faults. Smoothed
regression outputs achieve Mean Relative Percentage Errors below 1% for flow
and pump speed, approaching industrial sensor standards, although pressure
prediction requires further refinement. Energy estimates indicate significant
power savings, with the Loihi consumption (0.0032 J/inf) being up to 3 orders
of magnitude less compared to the estimated x86 CPU (11.3 J/inf) and ARM CPU
(1.18 J/inf) execution. Our findings underscore the potential of SNNs for
multi-task PM directly on resource-constrained edge devices, enabling scalable
and energy-efficient industrial monitoring solutions.

</details>


### [289] [Imaging at the quantum limit with convolutional neural networks](https://arxiv.org/abs/2506.13488)
*Andrew H. Proppe,Aaron Z. Goldberg,Guillaume Thekkadath,Noah Lupu-Gladstein,Kyle M. Jordan,Philip J. Bustard,Frédéric Bouchard,Duncan England,Khabat Heshami,Jeff S. Lundeen,Benjamin J. Sussman*

Main category: cs.LG

TL;DR: 评估深度卷积神经网络用于图像重建的性能极限，发现其可超越标准量子极限，部分达海森堡极限，预测的均方误差可达到参数的量子克拉美罗界。


<details>
  <summary>Details</summary>
Motivation: 评估深度卷积神经网络模型在图像重建方面的最终性能极限。

Method: 训练U - Net模型处理自然物体图像，训练模型处理参数化图像并计算量子克拉美罗界。

Result: 图像重建平均均方误差可超越标准量子极限，部分达海森堡极限；模型预测均方误差达参数计算界。

Conclusion: 深度卷积神经网络能学习成为物理定律允许的最优估计器，在经典照明下达到参数估计和图像重建的精度极限。

Abstract: Deep neural networks have been shown to achieve exceptional performance for
computer vision tasks like image recognition, segmentation, and reconstruction
or denoising. Here, we evaluate the ultimate performance limits of deep
convolutional neural network models for image reconstruction, by comparing them
against the standard quantum limit set by shot-noise and the Heisenberg limit
on precision. We train U-Net models on images of natural objects illuminated
with coherent states of light, and find that the average mean-squared error of
the reconstructions can surpass the standard quantum limit, and in some cases
reaches the Heisenberg limit. Further, we train models on well-parameterized
images for which we can calculate the quantum Cram\'er-Rao bound to determine
the minimum possible measurable variance of an estimated parameter for a given
probe state. We find the mean-squared error of the model predictions reaches
these bounds calculated for the parameters, across a variety of parameterized
images. These results suggest that deep convolutional neural networks can learn
to become the optimal estimators allowed by the laws of physics, performing
parameter estimation and image reconstruction at the ultimate possible limits
of precision for the case of classical illumination of the object.

</details>


### [290] [The Price of Freedom: Exploring Expressivity and Runtime Tradeoffs in Equivariant Tensor Products](https://arxiv.org/abs/2506.13523)
*YuQing Xie,Ameya Daigavane,Mit Kotak,Tess Smidt*

Main category: cs.LG

TL;DR: 对多种张量积操作进行系统分析，指出速度提升可能牺牲表达性，简化GTP实现，进行微基准测试，发现理论与实际性能差异大。


<details>
  <summary>Details</summary>
Motivation: 由于张量积计算复杂度高，虽有优化操作但不同张量积有差异，需分析其特点。

Method: 引入表达性和交互性度量表征差异，用球面网格简化GTP实现，进行微基准测试。

Result: 球面网格方法在基准测试和MACE势训练中快30%，理论运行时间与实际性能差异大。

Conclusion: 不同张量积操作在速度和表达性上有平衡，需针对特定应用仔细进行基准测试。

Abstract: $E(3)$-equivariant neural networks have demonstrated success across a wide
range of 3D modelling tasks. A fundamental operation in these networks is the
tensor product, which interacts two geometric features in an equivariant manner
to create new features. Due to the high computational complexity of the tensor
product, significant effort has been invested to optimize the runtime of this
operation. For example, Luo et al. (2024) recently proposed the Gaunt tensor
product (GTP) which promises a significant speedup. In this work, we provide a
careful, systematic analysis of a number of tensor product operations. In
particular, we emphasize that different tensor products are not performing the
same operation. The reported speedups typically come at the cost of
expressivity. We introduce measures of expressivity and interactability to
characterize these differences. In addition, we realized the original
implementation of GTP can be greatly simplified by directly using a spherical
grid at no cost in asymptotic runtime. This spherical grid approach is faster
on our benchmarks and in actual training of the MACE interatomic potential by
30\%. Finally, we provide the first systematic microbenchmarks of the various
tensor product operations. We find that the theoretical runtime guarantees can
differ wildly from empirical performance, demonstrating the need for careful
application-specific benchmarking. Code is available at
\href{https://github.com/atomicarchitects/PriceofFreedom}{https://github.com/atomicarchitects/PriceofFreedom}

</details>


### [291] [Seismic Acoustic Impedance Inversion Framework Based on Conditional Latent Generative Diffusion Model](https://arxiv.org/abs/2506.13529)
*Jie Chen,Hongling Chen,Jinghuai Gao,Chuangji Meng,Tao Yang,XinXin Liang*

Main category: cs.LG

TL;DR: 提出基于条件潜在生成扩散模型的地震声阻抗反演框架，在潜在空间进行反演，设计小波模块并提出采样策略，实验证明方法有效实用。


<details>
  <summary>Details</summary>
Motivation: 直接从叠后地震数据估计声阻抗因反演问题的不适定性而具有挑战性，现有扩散模型方法多在像素域且需多次迭代，适用性受限。

Method: 提出基于条件潜在生成扩散模型的反演框架，设计小波模块嵌入条件输入，提出模型驱动的采样策略。

Result: 数值实验显示在合成模型上少数扩散步骤就有高反演精度和强泛化能力，应用于实地数据地质细节增强且与测井测量一致性高。

Conclusion: 所提方法有效且实用。

Abstract: Seismic acoustic impedance plays a crucial role in lithological
identification and subsurface structure interpretation. However, due to the
inherently ill-posed nature of the inversion problem, directly estimating
impedance from post-stack seismic data remains highly challenging. Recently,
diffusion models have shown great potential in addressing such inverse problems
due to their strong prior learning and generative capabilities. Nevertheless,
most existing methods operate in the pixel domain and require multiple
iterations, limiting their applicability to field data. To alleviate these
limitations, we propose a novel seismic acoustic impedance inversion framework
based on a conditional latent generative diffusion model, where the inversion
process is made in latent space. To avoid introducing additional training
overhead when embedding conditional inputs, we design a lightweight
wavelet-based module into the framework to project seismic data and reuse an
encoder trained on impedance to embed low-frequency impedance into the latent
space. Furthermore, we propose a model-driven sampling strategy during the
inversion process of this framework to enhance accuracy and reduce the number
of required diffusion steps. Numerical experiments on a synthetic model
demonstrate that the proposed method achieves high inversion accuracy and
strong generalization capability within only a few diffusion steps. Moreover,
application to field data reveals enhanced geological detail and higher
consistency with well-log measurements, validating the effectiveness and
practicality of the proposed approach.

</details>


### [292] [Stability Analysis of Physics-Informed Neural Networks via Variational Coercivity, Perturbation Bounds, and Concentration Estimates](https://arxiv.org/abs/2506.13554)
*Ronald Katende*

Main category: cs.LG

TL;DR: 本文基于变分分析、算子强制性和显式摄动理论，为物理信息神经网络（PINNs）开发了严格的稳定性框架，并通过数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 为物理信息神经网络（PINNs）建立严格的稳定性框架。

Method: 推导确定性稳定性边界，通过McDiarmid不等式建立概率稳定性，利用强制性和Sobolev嵌入分析泛化性。

Result: 得到理论结果适用于标量和向量值偏微分方程，涵盖复合损失公式，数值实验验证了相关结论。

Conclusion: 为PINNs提供了数学上可靠且实际适用的稳定性框架，明确了算子结构、采样设计和函数正则性在鲁棒训练中的作用。

Abstract: We develop a rigorous stability framework for Physics-Informed Neural
Networks (PINNs) grounded in variational analysis, operator coercivity, and
explicit perturbation theory. PINNs approximate solutions to partial
differential equations (PDEs) by minimizing residual-based losses over sampled
collocation points. We derive deterministic stability bounds that quantify how
bounded perturbations in the network output propagate through both residual and
supervised loss components. Probabilistic stability is established via
McDiarmid's inequality, yielding non-asymptotic concentration bounds that link
sampling variability to empirical loss fluctuations under minimal assumptions.
Generalization from Sobolev-norm training loss to uniform approximation is
analyzed using coercivity and Sobolev embeddings, leading to pointwise error
control. The theoretical results apply to both scalar and vector-valued PDEs
and cover composite loss formulations. Numerical experiments validate the
perturbation sensitivity, sample complexity estimates, and Sobolev-to-uniform
generalization bounds. This work provides a mathematically grounded and
practically applicable stability framework for PINNs, clarifying the role of
operator structure, sampling design, and functional regularity in robust
training.

</details>


### [293] [A Production Scheduling Framework for Reinforcement Learning Under Real-World Constraints](https://arxiv.org/abs/2506.13566)
*Jonathan Hoss,Felix Schelling,Noah Klarmann*

Main category: cs.LG

TL;DR: 提出模块化框架JobShopLab解决实际车间调度问题，支持多目标优化且开源。


<details>
  <summary>Details</summary>
Motivation: 传统调度方法在实际生产环境中效果不佳，缺乏综合通用的强化学习训练和评估框架。

Method: 提出模块化框架，纳入实际车间关键约束，支持多目标优化，有可定制性和标准化接口。

Result: 开发了开源工具JobShopLab。

Conclusion: 该框架能为强化学习代理提供训练环境，便于不同调度方法在动态不确定条件下的标准化比较。

Abstract: The classical Job Shop Scheduling Problem (JSSP) focuses on optimizing
makespan under deterministic constraints. Real-world production environments
introduce additional complexities that cause traditional scheduling approaches
to be less effective. Reinforcement learning (RL) holds potential in addressing
these challenges, as it allows agents to learn adaptive scheduling strategies.
However, there is a lack of a comprehensive, general-purpose frameworks for
effectively training and evaluating RL agents under real-world constraints. To
address this gap, we propose a modular framework that extends classical JSSP
formulations by incorporating key \mbox{real-world} constraints inherent to the
shopfloor, including transport logistics, buffer management, machine
breakdowns, setup times, and stochastic processing conditions, while also
supporting multi-objective optimization. The framework is a customizable
solution that offers flexibility in defining problem instances and configuring
simulation parameters, enabling adaptation to diverse production scenarios. A
standardized interface ensures compatibility with various RL approaches,
providing a robust environment for training RL agents and facilitating the
standardized comparison of different scheduling methods under dynamic and
uncertain conditions. We release JobShopLab as an open-source tool for both
research and industrial applications, accessible at:
https://github.com/proto-lab-ro/jobshoplab

</details>


### [294] [Flexible-length Text Infilling for Discrete Diffusion Models](https://arxiv.org/abs/2506.13579)
*Andrew Zhang,Anushka Sivakumar,Chiawei Tang,Chris Thomas*

Main category: cs.LG

TL;DR: 介绍离散扩散模型新方法DDOT，可解决文本填充长度和位置灵活性问题，实验表现优。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型无法在无真实位置数据下进行灵活长度或位置的文本填充，需解决该问题。

Method: 提出DDOT，联合去噪词元值和位置，采用样本级最优传输（OT）耦合。

Result: 在文本填充基准测试中，DDOT优于简单扩散基线，与最先进非自回归模型性能相当，提升训练效率和灵活性。

Conclusion: DDOT是首个克服离散扩散模型文本填充局限的模型，有良好效果和兼容性。

Abstract: Discrete diffusion models are a new class of text generators that offer
advantages such as bidirectional context use, parallelizable generation, and
flexible prompting compared to autoregressive models. However, a critical
limitation of discrete diffusion models is their inability to perform
flexible-length or flexible-position text infilling without access to
ground-truth positional data. We introduce \textbf{DDOT} (\textbf{D}iscrete
\textbf{D}iffusion with \textbf{O}ptimal \textbf{T}ransport Position Coupling),
the first discrete diffusion model to overcome this challenge. DDOT jointly
denoises token values and token positions, employing a novel sample-level
Optimal Transport (OT) coupling. This coupling preserves relative token
ordering while dynamically adjusting the positions and length of infilled
segments, a capability previously missing in text diffusion. Our method is
orthogonal to existing discrete text diffusion methods and is compatible with
various pretrained text denoisers. Extensive experiments on text infilling
benchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms
naive diffusion baselines. Furthermore, DDOT achieves performance on par with
state-of-the-art non-autoregressive models and enables significant improvements
in training efficiency and flexibility.

</details>


### [295] [Assessing the Limits of In-Context Learning beyond Functions using Partially Ordered Relation](https://arxiv.org/abs/2506.13608)
*Debanjan Dutta,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: 本文研究大语言模型上下文学习在部分有序关系上的表现，发现随提示复杂度增加其有效性受限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型上下文学习在学习定义明确的函数或关系方面的行为有待研究。

Method: 引入提示中归纳增加复杂度的概念，研究上下文学习在部分有序关系上的表现。

Result: 所选指标的饱和性能表明，即使有足够的示例，随提示复杂度增加，上下文学习的有效性仍受限。

Conclusion: 该行为通过实证发现并在隐式优化过程上得到理论验证。

Abstract: Generating rational and generally accurate responses to tasks, often
accompanied by example demonstrations, highlights Large Language Model's
(LLM's) remarkable In-Context Learning (ICL) capabilities without requiring
updates to the model's parameter space. Despite having an ongoing exploration
focused on the inference from a document-level concept, its behavior in
learning well-defined functions or relations in context needs a careful
investigation. In this article, we present the performance of ICL on partially
ordered relation by introducing the notion of inductively increasing complexity
in prompts. In most cases, the saturated performance of the chosen metric
indicates that while ICL offers some benefits, its effectiveness remains
constrained as we increase the complexity in the prompts even in presence of
sufficient demonstrative examples. The behavior is evident from our empirical
findings and has further been theoretically justified in term of its implicit
optimization process. The code is available
\href{https://anonymous.4open.science/r/ICLonPartiallyOrderSet}{here}.

</details>


### [296] [Graph-Convolution-Beta-VAE for Synthetic Abdominal Aorta Aneurysm Generation](https://arxiv.org/abs/2506.13628)
*Francesco Fabbri,Martino Andrea Scarpolini,Angelo Iollo,Francesco Viola,Francesco Tudisco*

Main category: cs.LG

TL;DR: 提出beta-Variational Autoencoder Graph Convolutional Neural Network框架生成合成腹主动脉瘤数据，增强数据多样性且保证真实，优于PCA方法，为医学研究等提供基础。


<details>
  <summary>Details</summary>
Motivation: 缓解医学研究中的隐私问题，实现大规模患者数据分析。

Method: 使用beta-Variational Autoencoder Graph Convolutional Neural Network框架，利用小的真实数据集提取特征，采用基于Procrustes分析的低影响数据增强方法，有确定性和随机性生成策略。

Result: 模型在未见数据上比基于PCA的方法更稳健，合成的AAA数据集保留患者隐私，可用于医学研究等。

Conclusion: 合成数据生成方法为医学研究、设备测试和计算建模提供可扩展基础。

Abstract: Synthetic data generation plays a crucial role in medical research by
mitigating privacy concerns and enabling large-scale patient data analysis.
This study presents a beta-Variational Autoencoder Graph Convolutional Neural
Network framework for generating synthetic Abdominal Aorta Aneurysms (AAA).
Using a small real-world dataset, our approach extracts key anatomical features
and captures complex statistical relationships within a compact disentangled
latent space. To address data limitations, low-impact data augmentation based
on Procrustes analysis was employed, preserving anatomical integrity. The
generation strategies, both deterministic and stochastic, manage to enhance
data diversity while ensuring realism. Compared to PCA-based approaches, our
model performs more robustly on unseen data by capturing complex, nonlinear
anatomical variations. This enables more comprehensive clinical and statistical
analyses than the original dataset alone. The resulting synthetic AAA dataset
preserves patient privacy while providing a scalable foundation for medical
research, device testing, and computational modeling.

</details>


### [297] [Global Convergence of Adjoint-Optimized Neural PDEs](https://arxiv.org/abs/2506.13633)
*Konstantin Riedl,Justin Sirignano,Konstantinos Spiliopoulos*

Main category: cs.LG

TL;DR: 本文研究神经网络偏微分方程（PDE）模型训练时伴随梯度下降优化方法的收敛性，证明训练解收敛到目标数据，并通过数值研究验证。


<details>
  <summary>Details</summary>
Motivation: 神经网络PDE模型是科学机器学习重要研究领域，需研究其训练方法在隐藏单元数量和训练时间趋于无穷时的收敛性。

Method: 针对源项嵌入神经网络的非线性抛物型PDE，证明训练的神经网络PDE解收敛到目标数据。

Result: 证明了在隐藏单元和训练时间趋于无穷时，训练的神经网络PDE解收敛到目标数据。

Conclusion: 理论结果通过数值研究得到了说明和实证验证。

Abstract: Many engineering and scientific fields have recently become interested in
modeling terms in partial differential equations (PDEs) with neural networks.
The resulting neural-network PDE model, being a function of the neural network
parameters, can be calibrated to available data by optimizing over the PDE
using gradient descent, where the gradient is evaluated in a computationally
efficient manner by solving an adjoint PDE. These neural-network PDE models
have emerged as an important research area in scientific machine learning. In
this paper, we study the convergence of the adjoint gradient descent
optimization method for training neural-network PDE models in the limit where
both the number of hidden units and the training time tend to infinity.
Specifically, for a general class of nonlinear parabolic PDEs with a neural
network embedded in the source term, we prove convergence of the trained
neural-network PDE solution to the target data (i.e., a global minimizer). The
global convergence proof poses a unique mathematical challenge that is not
encountered in finite-dimensional neural network convergence analyses due to
(1) the neural network training dynamics involving a non-local neural network
kernel operator in the infinite-width hidden layer limit where the kernel lacks
a spectral gap for its eigenvalues and (2) the nonlinearity of the limit PDE
system, which leads to a non-convex optimization problem, even in the
infinite-width hidden layer limit (unlike in typical neual network training
cases where the optimization problem becomes convex in the large neuron limit).
The theoretical results are illustrated and empirically validated by numerical
studies.

</details>


### [298] [xbench: Tracking Agents Productivity Scaling with Profession-Aligned Real-World Evaluations](https://arxiv.org/abs/2506.13651)
*Kaiyuan Chen,Yixin Ren,Yang Liu,Xiaobo Hu,Haotong Tian,Tianbao Xie,Fangfu Liu,Haoye Zhang,Hongzhang Liu,Yuan Gong,Chen Sun,Han Hou,Hui Yang,James Pan,Jianan Lou,Jiayi Mao,Jizheng Liu,Jinpeng Li,Kangyi Liu,Kenkun Liu,Rui Wang,Run Li,Tong Niu,Wenlong Zhang,Wenqi Yan,Xuanzheng Wang,Yuchen Zhang,Yi-Hsin Hung,Yuan Jiang,Zexuan Liu,Zihan Yin,Zijian Ma,Zhiwen Mo*

Main category: cs.LG

TL;DR: 介绍xbench评估套件，用于评估AI智能体在专业领域的能力，给出招聘和营销两个基准测试及初步结果，评估集可在线获取。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试常聚焦孤立技术技能，不能准确反映AI智能体在专业场景中的经济价值，需新评估套件。

Method: 针对商业重要领域，由行业专家定义评估任务，创建与生产力价值强相关的指标，推出招聘和营销两个基准测试。

Result: 给出当代领先智能体的初步评估结果，为专业领域建立了基线。

Conclusion: xbench能桥接AI智能体能力与现实生产力的差距，持续更新的评估集和评估可在网站获取。

Abstract: We introduce xbench, a dynamic, profession-aligned evaluation suite designed
to bridge the gap between AI agent capabilities and real-world productivity.
While existing benchmarks often focus on isolated technical skills, they may
not accurately reflect the economic value agents deliver in professional
settings. To address this, xbench targets commercially significant domains with
evaluation tasks defined by industry professionals. Our framework creates
metrics that strongly correlate with productivity value, enables prediction of
Technology-Market Fit (TMF), and facilitates tracking of product capabilities
over time. As our initial implementations, we present two benchmarks:
Recruitment and Marketing. For Recruitment, we collect 50 tasks from real-world
headhunting business scenarios to evaluate agents' abilities in company
mapping, information retrieval, and talent sourcing. For Marketing, we assess
agents' ability to match influencers with advertiser needs, evaluating their
performance across 50 advertiser requirements using a curated pool of 836
candidate influencers. We present initial evaluation results for leading
contemporary agents, establishing a baseline for these professional domains.
Our continuously updated evalsets and evaluations are available at
https://xbench.org.

</details>


### [299] [We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems](https://arxiv.org/abs/2506.13666)
*Junfeng Fang,Zijun Yao,Ruipeng Wang,Haokai Ma,Xiang Wang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 大语言模型发展进入经验驱动时代催生MCP，但MCP带来新安全风险，本文呼吁关注并给出构建安全MCP代理系统的方向和代码。


<details>
  <summary>Details</summary>
Motivation: MCP成为大语言模型代理系统事实上的标准，但其引入第三方服务带来新安全风险，需研究应对。

Method: 构建框架检查安全问题，进行试点实验证明风险存在，给出构建安全MCP代理系统的路线图。

Result: 通过实验证明MCP代理系统安全风险是真实威胁且防御不易。

Conclusion: 呼吁研究社区关注MCP安全问题，鼓励更多研究者参与该重要研究方向。

Abstract: The development of large language models (LLMs) has entered in a
experience-driven era, flagged by the emergence of environment feedback-driven
learning via reinforcement learning and tool-using agents. This encourages the
emergenece of model context protocol (MCP), which defines the standard on how
should a LLM interact with external services, such as \api and data. However,
as MCP becomes the de facto standard for LLM agent systems, it also introduces
new safety risks. In particular, MCP introduces third-party services, which are
not controlled by the LLM developers, into the agent systems. These third-party
MCP services provider are potentially malicious and have the economic
incentives to exploit vulnerabilities and sabotage user-agent interactions. In
this position paper, we advocate the research community in LLM safety to pay
close attention to the new safety risks issues introduced by MCP, and develop
new techniques to build safe MCP-powered agent systems. To establish our
position, we argue with three key parts. (1) We first construct \framework, a
controlled framework to examine safety issues in MCP-powered agent systems. (2)
We then conduct a series of pilot experiments to demonstrate the safety risks
in MCP-powered agent systems is a real threat and its defense is not trivial.
(3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered
agent systems. In particular, we would call for researchers to persue the
following research directions: red teaming, MCP safe LLM development, MCP
safety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP
safe ecosystem construction. We hope this position paper can raise the
awareness of the research community in MCP safety and encourage more
researchers to join this important research direction. Our code is available at
https://github.com/littlelittlenine/SafeMCP.git.

</details>


### [300] [The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement Learning](https://arxiv.org/abs/2506.13672)
*Jiashun Liu,Johan Obando-Ceron,Pablo Samuel Castro,Aaron Courville,Ling Pan*

Main category: cs.LG

TL;DR: 文章提出LEAST机制，基于Q值和梯度统计实现提前终止无意义回合，提升多种RL算法学习效率。


<details>
  <summary>Details</summary>
Motivation: 传统离策略深度强化学习中，重放缓冲区可能被无用数据“污染”，且存在沉没成本谬误导致浪费环境交互，影响优化。

Method: 提出Learn to Stop (LEAST)机制，基于Q值和梯度统计进行策略性提前回合终止。

Result: 该方法在MuJoCo和DeepMind Control Suite基准测试中，提升了多种RL算法的学习效率。

Conclusion: LEAST机制能有效避免采样无意义转换，提升学习效率。

Abstract: Off-policy deep reinforcement learning (RL) typically leverages replay
buffers for reusing past experiences during learning. This can help improve
sample efficiency when the collected data is informative and aligned with the
learning objectives; when that is not the case, it can have the effect of
"polluting" the replay buffer with data which can exacerbate optimization
challenges in addition to wasting environment interactions due to wasteful
sampling. We argue that sampling these uninformative and wasteful transitions
can be avoided by addressing the sunk cost fallacy, which, in the context of
deep RL, is the tendency towards continuing an episode until termination. To
address this, we propose learn to stop (LEAST), a lightweight mechanism that
enables strategic early episode termination based on Q-value and gradient
statistics, which helps agents recognize when to terminate unproductive
episodes early. We demonstrate that our method improves learning efficiency on
a variety of RL algorithms, evaluated on both the MuJoCo and DeepMind Control
Suite benchmarks.

</details>


### [301] [A Gravity-informed Spatiotemporal Transformer for Human Activity Intensity Prediction](https://arxiv.org/abs/2506.13678)
*Yi Wang,Zhenghong Wang,Fan Zhang,Chengling Tang,Chaogui Kang,Di Zhu,Zhongfu Ma,Sijie Ruan,Weiyu Zhang,Yu Zheng,Philip S. Yu,Yu Liu*

Main category: cs.LG

TL;DR: 本文提出Gravityformer框架预测人类活动强度，在六个真实数据集上表现优于现有方法，还能对注意力矩阵基于地理定律解读。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视空间交互物理约束和空间相关性建模的过平滑现象，需改进人类活动强度预测方法。

Method: 提出Gravityformer框架，估计空间显式质量参数，用空间交互闭式解约束空间建模随机性，利用学习的空间交互缓解过平滑，采用并行时空图卷积Transformer结构。

Result: 在六个真实世界大规模活动数据集实验中，方法在定量和定性上优于现有基准。

Conclusion: 为时空预测学习中物理定律与深度学习融合提供新见解。

Abstract: Human activity intensity prediction is a crucial to many location-based
services. Although tremendous progress has been made to model dynamic
spatiotemporal patterns of human activity, most existing methods, including
spatiotemporal graph neural networks (ST-GNNs), overlook physical constraints
of spatial interactions and the over-smoothing phenomenon in spatial
correlation modeling. To address these limitations, this work proposes a
physics-informed deep learning framework, namely Gravity-informed
Spatiotemporal Transformer (Gravityformer) by refining transformer attention to
integrate the universal law of gravitation and explicitly incorporating
constraints from spatial interactions. Specifically, it (1) estimates two
spatially explicit mass parameters based on inflow and outflow, (2) models the
likelihood of cross-unit interaction using closed-form solutions of spatial
interactions to constrain spatial modeling randomness, and (3) utilizes the
learned spatial interaction to guide and mitigate the over-smoothing phenomenon
in transformer attention matrices. The underlying law of human activity can be
explicitly modeled by the proposed adaptive gravity model. Moreover, a parallel
spatiotemporal graph convolution transformer structure is proposed for
achieving a balance between coupled spatial and temporal learning. Systematic
experiments on six real-world large-scale activity datasets demonstrate the
quantitative and qualitative superiority of our approach over state-of-the-art
benchmarks. Additionally, the learned gravity attention matrix can be
disentangled and interpreted based on geographical laws. This work provides a
novel insight into integrating physical laws with deep learning for
spatiotemporal predictive learning.

</details>


### [302] [Hybrid Meta-learners for Estimating Heterogeneous Treatment Effects](https://arxiv.org/abs/2506.13680)
*Zhongyuan Liang,Lars van der Laan,Ahmed Alaa*

Main category: cs.LG

TL;DR: 提出混合学习器（H-learner），一种能根据数据集在直接和间接正则化间插值的正则化策略，在半合成和真实世界基准数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 以往估计条件平均处理效应（CATE）的间接和直接元学习器范式各有优劣，在不同场景表现不同，需新方法结合二者优势。

Method: 引入H-learner，学习中间函数，其差值近似CATE，且不要求精确近似潜在结果（PO）。

Result: 实验表明故意对PO进行次优拟合能改善CATE估计的偏差 - 方差权衡，H-learner在半合成和真实世界基准数据集上始终处于帕累托前沿。

Conclusion: H-learner有效结合了直接和间接元学习器的优势。

Abstract: Estimating conditional average treatment effects (CATE) from observational
data involves modeling decisions that differ from supervised learning,
particularly concerning how to regularize model complexity. Previous approaches
can be grouped into two primary "meta-learner" paradigms that impose distinct
inductive biases. Indirect meta-learners first fit and regularize separate
potential outcome (PO) models and then estimate CATE by taking their
difference, whereas direct meta-learners construct and directly regularize
estimators for the CATE function itself. Neither approach consistently
outperforms the other across all scenarios: indirect learners perform well when
the PO functions are simple, while direct learners outperform when the CATE is
simpler than individual PO functions. In this paper, we introduce the Hybrid
Learner (H-learner), a novel regularization strategy that interpolates between
the direct and indirect regularizations depending on the dataset at hand. The
H-learner achieves this by learning intermediate functions whose difference
closely approximates the CATE without necessarily requiring accurate individual
approximations of the POs themselves. We demonstrate empirically that
intentionally allowing suboptimal fits to the POs improves the bias-variance
tradeoff in estimating CATE. Experiments conducted on semi-synthetic and
real-world benchmark datasets illustrate that the H-learner consistently
operates at the Pareto frontier, effectively combining the strengths of both
direct and indirect meta-learners.

</details>


### [303] [Meta-learning how to Share Credit among Macro-Actions](https://arxiv.org/abs/2506.13690)
*Ionel-Alexandru Hosu,Traian Rebedea,Razvan Pascanu*

Main category: cs.LG

TL;DR: 本文指出强化学习中简单添加宏动作不能改善探索，提出用正则化项利用动作与宏动作关系降低动作空间有效维度，在Atari和StreetFighter II游戏中验证策略有效。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中简单添加宏动作不能提升探索，反而使典型探索策略低效的问题。

Method: 提出一种新的正则化项，利用动作和宏动作关系降低动作空间有效维度，通过元学习与策略学习联合学习相似度矩阵。

Result: 在Atari游戏和StreetFighter II环境中相比Rainbow - DQN基线有显著提升，且宏动作相似度可迁移到相关环境。

Conclusion: 该工作是理解如何利用动作空间相似度几何改善信用分配和探索、提高学习效率的重要一步。

Abstract: One proposed mechanism to improve exploration in reinforcement learning is
through the use of macro-actions. Paradoxically though, in many scenarios the
naive addition of macro-actions does not lead to better exploration, but rather
the opposite. It has been argued that this was caused by adding non-useful
macros and multiple works have focused on mechanisms to discover effectively
environment-specific useful macros. In this work, we take a slightly different
perspective. We argue that the difficulty stems from the trade-offs between
reducing the average number of decisions per episode versus increasing the size
of the action space. Namely, one typically treats each potential macro-action
as independent and atomic, hence strictly increasing the search space and
making typical exploration strategies inefficient. To address this problem we
propose a novel regularization term that exploits the relationship between
actions and macro-actions to improve the credit assignment mechanism by
reducing the effective dimension of the action space and, therefore, improving
exploration. The term relies on a similarity matrix that is meta-learned
jointly with learning the desired policy. We empirically validate our strategy
looking at macro-actions in Atari games, and the StreetFighter II environment.
Our results show significant improvements over the Rainbow-DQN baseline in all
environments. Additionally, we show that the macro-action similarity is
transferable to related environments. We believe this work is a small but
important step towards understanding how the similarity-imposed geometry on the
action space can be exploited to improve credit assignment and exploration,
therefore making learning more effective.

</details>


### [304] [Value-Free Policy Optimization via Reward Partitioning](https://arxiv.org/abs/2506.13702)
*Bilal Faye,Hanane Azzag,Mustapha Lebbah*

Main category: cs.LG

TL;DR: 提出Reward Partitioning Optimization (RPO)解决Direct Reward Optimization (DRO)局限，在标量反馈语言建模任务上验证，性能优于现有单轨迹基线方法。


<details>
  <summary>Details</summary>
Motivation: DRO需近似值函数，存在高离策略方差、策略与值学习耦合、缺乏对策略绝对监督等局限，需新方法解决。

Method: RPO去除对值函数建模需求，用分区方法直接从数据估计归一化观察到的奖励，得到简单监督学习目标。

Result: 在标量反馈语言建模任务上，RPO性能优于DRO和KTO等现有单轨迹基线方法。

Conclusion: RPO是单轨迹策略优化简单、有效且理论可靠的方法。

Abstract: Single-trajectory reinforcement learning (RL) methods aim to optimize
policies from datasets consisting of (prompt, response, reward) triplets, where
scalar rewards are directly available. This supervision format is highly
practical, as it mirrors real-world human feedback, such as thumbs-up/down
signals, and avoids the need for structured preference annotations. In
contrast, pairwise preference-based methods like Direct Preference Optimization
(DPO) rely on datasets with both preferred and dispreferred responses, which
are harder to construct and less natural to collect. Among single-trajectory
approaches, Direct Reward Optimization (DRO) has shown strong empirical
performance due to its simplicity and stability. However, DRO requires
approximating a value function, which introduces several limitations: high
off-policy variance, coupling between policy and value learning, and a lack of
absolute supervision on the policy itself. We introduce Reward Partitioning
Optimization (RPO), a new method that resolves these limitations by removing
the need to model the value function. Instead, RPO normalizes observed rewards
using a partitioning approach estimated directly from data. This leads to a
straightforward supervised learning objective on the policy, with no auxiliary
models and no joint optimization. RPO provides direct and stable supervision on
the policy, making it robust and easy to implement in practice. We validate RPO
on scalar-feedback language modeling tasks using Flan-T5 encoder-decoder
models. Our results demonstrate that RPO outperforms existing single-trajectory
baselines such as DRO and Kahneman-Tversky Optimization (KTO). These findings
confirm that RPO is a simple, effective, and theoretically grounded method for
single-trajectory policy optimization.

</details>


### [305] [TimeMaster: Training Time-Series Multimodal LLMs to Reason via Reinforcement Learning](https://arxiv.org/abs/2506.13705)
*Junru Zhang,Lang Feng,Xu Guo,Yuhan Wu,Yabo Dong,Duanqing Xu*

Main category: cs.LG

TL;DR: 本文提出基于强化学习的TimeMaster方法，使时间序列多模态大语言模型能对可视化时间序列输入和任务提示进行结构化、可解释推理，在TimerBed基准测试中取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理在多模态大语言模型中因动态时间模式、模糊语义和缺乏时间先验仍是重大挑战。

Method: 采用推理、分类和特定领域扩展三部分结构化输出格式，通过复合奖励函数优化，使用两阶段训练管道，先监督微调，再进行基于令牌级的组相对策略优化。

Result: 在TimerBed基准测试的六个现实世界分类任务上，TimeMaster超越经典时间序列模型和少样本GPT - 4o，分别提升14.6%和7.3%的性能，还展现专家级推理行为等。

Conclusion: 基于奖励的强化学习是将时间理解融入时间序列多模态大语言模型的可扩展且有前景的途径。

Abstract: Time-series reasoning remains a significant challenge in multimodal large
language models (MLLMs) due to the dynamic temporal patterns, ambiguous
semantics, and lack of temporal priors. In this work, we introduce TimeMaster,
a reinforcement learning (RL)-based method that enables time-series MLLMs to
perform structured, interpretable reasoning directly over visualized
time-series inputs and task prompts. TimeMaster adopts a three-part structured
output format, reasoning, classification, and domain-specific extension, and is
optimized via a composite reward function that aligns format adherence,
prediction accuracy, and open-ended insight quality. The model is trained using
a two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish
a good initialization, followed by Group Relative Policy Optimization (GRPO) at
the token level to enable stable and targeted reward-driven improvement in
time-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across
six real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster
achieves state-of-the-art performance, outperforming both classical time-series
models and few-shot GPT-4o by over 14.6% and 7.3% performance gain,
respectively. Notably, TimeMaster goes beyond time-series classification: it
also exhibits expert-like reasoning behavior, generates context-aware
explanations, and delivers domain-aligned insights. Our results highlight that
reward-driven RL can be a scalable and promising path toward integrating
temporal understanding into time-series MLLMs.

</details>


### [306] [Sharpness-Aware Machine Unlearning](https://arxiv.org/abs/2506.13715)
*Haoran Tang,Rajiv Khanna*

Main category: cs.LG

TL;DR: 研究SAM在机器遗忘方案下的有效性，提出Sharp MinMax方法，实验表明SAM提升遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 前人证明SAM可通过防止噪声记忆提高泛化性，但未研究其在机器遗忘方案下，遗忘信号干扰学习保留信号时的有效性。

Method: 理论分析SAM在拟合遗忘集时的去噪特性，刻画信号盈余；提出Sharp MinMax方法，拆分模型分别学习保留信号和遗忘信号；进行大量实验。

Result: SAM在放松保留信号要求时优于SGD，可增强多种遗忘方法；Sharp MinMax性能最佳；SAM降低特征纠缠、增强抗成员推理攻击能力、使损失景观更平坦。

Conclusion: SAM能有效提升不同难度下的机器遗忘效果。

Abstract: We characterize the effectiveness of Sharpness-aware minimization (SAM) under
machine unlearning scheme, where unlearning forget signals interferes with
learning retain signals. While previous work prove that SAM improves
generalization with noise memorization prevention, we show that SAM abandons
such denoising property when fitting the forget set, leading to various test
error bounds depending on signal strength. We further characterize the signal
surplus of SAM in the order of signal strength, which enables learning from
less retain signals to maintain model performance and putting more weight on
unlearning the forget set. Empirical studies show that SAM outperforms SGD with
relaxed requirement for retain signals and can enhance various unlearning
methods either as pretrain or unlearn algorithm. Observing that overfitting can
benefit more stringent sample-specific unlearning, we propose Sharp MinMax,
which splits the model into two to learn retain signals with SAM and unlearn
forget signals with sharpness maximization, achieving best performance.
Extensive experiments show that SAM enhances unlearning across varying
difficulties measured by data memorization, yielding decreased feature
entanglement between retain and forget sets, stronger resistance to membership
inference attacks, and a flatter loss landscape.

</details>


### [307] [Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs](https://arxiv.org/abs/2506.13727)
*Sayed Mohammad Vakilzadeh Hatefi,Maximilian Dreyer,Reduan Achtibat,Patrick Kahardipraja,Thomas Wiegand,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.LG

TL;DR: 本文利用LRP对大语言模型进行归因引导剪枝，提出统一框架用于模型压缩、电路发现和模型修正，在Llama和OPT模型上验证了有效性与局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数多，在内存和计算受限环境部署有挑战，可利用可解释性方法进行模型压缩。

Method: 利用LRP进行大语言模型非结构化剪枝，提取任务相关子图，选择性移除导致虚假行为的电路，并将这些技术整合为统一框架。

Result: 能大幅减小模型大小且性能损失小，在模型压缩、电路发现和模型修正实验中展示了框架有效性和局限性。

Conclusion: 该框架有提高模型效率和安全性的潜力。

Abstract: Large Language Models (LLMs) are central to many contemporary AI
applications, yet their extensive parameter counts pose significant challenges
for deployment in memory- and compute-constrained environments. Recent works in
eXplainable AI (XAI), particularly on attribution methods, suggest that
interpretability can also enable model compression by identifying and removing
components irrelevant to inference. In this paper, we leverage Layer-wise
Relevance Propagation (LRP) to perform attribution-guided pruning of LLMs.
While LRP has shown promise in structured pruning for vision models, we extend
it to unstructured pruning in LLMs and demonstrate that it can substantially
reduce model size with minimal performance loss. Our method is especially
effective in extracting task-relevant subgraphs -- so-called ``circuits'' --
which can represent core functions (e.g., indirect object identification).
Building on this, we introduce a technique for model correction, by selectively
removing circuits responsible for spurious behaviors (e.g., toxic outputs). All
in all, we gather these techniques as a uniform holistic framework and showcase
its effectiveness and limitations through extensive experiments for
compression, circuit discovery and model correction on Llama and OPT models,
highlighting its potential for improving both model efficiency and safety. Our
code is publicly available at https://github.com/erfanhatefi/SparC3.

</details>


### [308] [VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models](https://arxiv.org/abs/2506.13754)
*Edward Li,Zichen Wang,Jiahe Huang,Jeong Joon Park*

Main category: cs.LG

TL;DR: 提出用视频修复扩散变压器模型解决偏微分方程的统一框架，实验表明性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法针对正问题或逆问题、全观测或部分观测设计专门策略，本文旨在将这些任务统一在一个灵活的生成框架下。

Method: 将求解偏微分方程转化为广义的图像修复问题，设计基于变压器的架构，利用像素空间视频扩散模型进行精细修复和条件设置，并通过分层建模提高计算效率。

Result: 视频修复扩散模型在各种偏微分方程和问题设置上提供了准确且通用的解决方案，优于现有基线。

Conclusion: 所提出的统一框架能有效解决多种偏微分方程问题，具有准确性和通用性。

Abstract: We present a unified framework for solving partial differential equations
(PDEs) using video-inpainting diffusion transformer models. Unlike existing
methods that devise specialized strategies for either forward or inverse
problems under full or partial observation, our approach unifies these tasks
under a single, flexible generative framework. Specifically, we recast
PDE-solving as a generalized inpainting problem, e.g., treating forward
prediction as inferring missing spatiotemporal information of future states
from initial conditions. To this end, we design a transformer-based
architecture that conditions on arbitrary patterns of known data to infer
missing values across time and space. Our method proposes pixel-space video
diffusion models for fine-grained, high-fidelity inpainting and conditioning,
while enhancing computational efficiency through hierarchical modeling.
Extensive experiments show that our video inpainting-based diffusion model
offers an accurate and versatile solution across a wide range of PDEs and
problem setups, outperforming state-of-the-art baselines.

</details>


### [309] [MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering](https://arxiv.org/abs/2506.13755)
*Arya Fayyazi,Mehdi Kamal,Massoud Pedram*

Main category: cs.LG

TL;DR: 本文提出MARCO框架用于边缘设备高效神经架构搜索，结合多智能体强化学习与共形预测，减少搜索时间、维持精度，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 填补自动化DNN设计与边缘AI部署CAD之间的差距，在严格硬件约束下减少搜索时间并维持精度。

Method: 将NAS任务分解为硬件配置智能体和量化智能体，使用集中评论-分散执行范式，集成校准的CP代理模型进行架构筛选。

Result: 在MNIST、CIFAR - 10和CIFAR - 100上搜索时间减少3 - 4倍，维持近基线精度（误差在0.3%内），还减少推理延迟，在评估板上验证模拟器趋势与实际相符。

Conclusion: MARCO框架能有效实现资源受限边缘设备的高效神经架构搜索。

Abstract: This paper introduces MARCO (Multi-Agent Reinforcement learning with
Conformal Optimization), a novel hardware-aware framework for efficient neural
architecture search (NAS) targeting resource-constrained edge devices. By
significantly reducing search time and maintaining accuracy under strict
hardware constraints, MARCO bridges the gap between automated DNN design and
CAD for edge AI deployment. MARCO's core technical contribution lies in its
unique combination of multi-agent reinforcement learning (MARL) with Conformal
Prediction (CP) to accelerate the hardware/software co-design process for
deploying deep neural networks. Unlike conventional once-for-all (OFA) supernet
approaches that require extensive pretraining, MARCO decomposes the NAS task
into a hardware configuration agent (HCA) and a Quantization Agent (QA). The
HCA optimizes high-level design parameters, while the QA determines per-layer
bit-widths under strict memory and latency budgets using a shared reward signal
within a centralized-critic, decentralized-execution (CTDE) paradigm. A key
innovation is the integration of a calibrated CP surrogate model that provides
statistical guarantees (with a user-defined miscoverage rate) to prune
unpromising candidate architectures before incurring the high costs of partial
training or hardware simulation. This early filtering drastically reduces the
search space while ensuring that high-quality designs are retained with a high
probability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100
demonstrate that MARCO achieves a 3-4x reduction in total search time compared
to an OFA baseline while maintaining near-baseline accuracy (within 0.3%).
Furthermore, MARCO also reduces inference latency. Validation on a MAX78000
evaluation board confirms that simulator trends hold in practice, with
simulator estimates deviating from measured values by less than 5%.

</details>


### [310] [AI reconstruction of European weather from the Euro-Atlantic regimes](https://arxiv.org/abs/2506.13758)
*A. Camilletti,G. Franch,E. Tomasi,M. Cristoforetti*

Main category: cs.LG

TL;DR: 提出基于欧洲 - 大西洋天气型（WR）指数重建欧洲温度和降水月平均异常的非线性AI模型，评估其性能并与SEAS5对比，表明WR结合AI工具用于次季节和季节预报有前景。


<details>
  <summary>Details</summary>
Motivation: 现有从欧洲 - 大西洋WR估计地面气候变量多为线性方法，该研究旨在探索非线性方法。

Method: 构建非线性AI模型，分析模型在不同WR数量下重建欧洲冬夏季月平均气温和降水异常的性能，评估WR指数误差影响。

Result: 平均绝对相对误差低于80%时，季节重建效果优于SEAS5，用SEAS5预测的WR指数评估模型，技能与SEAS5相当或略好。

Conclusion: 基于WR和AI工具的异常重建为次季节和季节预报提供了有前景的途径。

Abstract: We present a non-linear AI-model designed to reconstruct monthly mean
anomalies of the European temperature and precipitation based on the
Euro-Atlantic Weather regimes (WR) indices. WR represent recurrent,
quasi-stationary, and persistent states of the atmospheric circulation that
exert considerable influence over the European weather, therefore offering an
opportunity for sub-seasonal to seasonal forecasting. While much research has
focused on studying the correlation and impacts of the WR on European weather,
the estimation of ground-level climate variables, such as temperature and
precipitation, from Euro-Atlantic WR remains largely unexplored and is
currently limited to linear methods. The presented AI model can capture and
introduce complex non-linearities in the relation between the WR indices,
describing the state of the Euro-Atlantic atmospheric circulation and the
corresponding surface temperature and precipitation anomalies in Europe. We
discuss the AI-model performance in reconstructing the monthly mean two-meter
temperature and total precipitation anomalies in the European winter and
summer, also varying the number of WR used to describe the monthly atmospheric
circulation. We assess the impact of errors on the WR indices in the
reconstruction and show that a mean absolute relative error below 80% yields
improved seasonal reconstruction compared to the ECMWF operational seasonal
forecast system, SEAS5. As a demonstration of practical applicability, we
evaluate the model using WR indices predicted by SEAS5, finding slightly better
or comparable skill relative to the SEAS5 forecast itself. Our findings
demonstrate that WR-based anomaly reconstruction, powered by AI tools, offers a
promising pathway for sub-seasonal and seasonal forecasting.

</details>


### [311] [Discrete Diffusion in Large Language and Multimodal Models: A Survey](https://arxiv.org/abs/2506.13759)
*Runpeng Yu,Qi Li,Xinchao Wang*

Main category: cs.LG

TL;DR: 本文对离散扩散语言模型（dLLMs）和离散扩散多模态语言模型（dMLLMs）进行系统综述，介绍其优势、发展驱动因素，回顾研究历史，分析关键技术与应用并探讨未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着d(M)LLMs在性能和推理速度上展现出优势，有必要对其进行全面综述，以促进该领域研究与发展。

Method: 追溯dLLMs和dMLLMs的历史发展，形式化其数学框架，对代表性模型分类，分析训练和推理关键技术，总结新兴应用。

Result: 梳理了dLLMs和dMLLMs领域的研究进展，包括发展驱动因素、模型分类、关键技术和应用。

Conclusion: 探讨了dLLMs和dMLLMs未来的研究和部署方向。

Abstract: In this work, we provide a systematic survey of Discrete Diffusion Language
Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).
Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,
parallel decoding paradigm using full attention and a denoising-based
generation strategy. This paradigm naturally enables parallel generation,
fine-grained output controllability, and dynamic, response-aware perception.
These capabilities are previously difficult to achieve with AR models.
Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as
a large number of open-source academic d(M)LLMs, have demonstrated performance
comparable to their autoregressive counterparts, while achieving up to 10x
acceleration in inference speed.
  The advancement of discrete diffusion LLMs and MLLMs has been largely driven
by progress in two domains. The first is the development of autoregressive LLMs
and MLLMs, which has accumulated vast amounts of data, benchmarks, and
foundational infrastructure for training and inference. The second contributing
domain is the evolution of the mathematical models underlying discrete
diffusion. Together, these advancements have catalyzed a surge in dLLMs and
dMLLMs research in early 2025.
  In this work, we present a comprehensive overview of the research in the dLLM
and dMLLM domains. We trace the historical development of dLLMs and dMLLMs,
formalize the underlying mathematical frameworks, and categorize representative
models. We further analyze key techniques for training and inference, and
summarize emerging applications across language, vision-language, and
biological domains. We conclude by discussing future directions for research
and deployment.
  Paper collection: https://github.com/LiQiiiii/DLLM-Survey

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [312] [Green Economic Load Dispatch: A Review and Implementation](https://arxiv.org/abs/2506.12062)
*Shahbaz Hussain*

Main category: cs.NE

TL;DR: 文章研究电力系统经济调度问题，用PSO和GA在IEEE 30 - 母线系统上实现调度并比较结果。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以解决复杂多目标的发电机经济调度问题，且需考虑污染物排放限制。

Method: 对文献中常用的AI技术进行研究，采用粒子群优化（PSO）和遗传算法（GA）在MATLAB中对IEEE 30 - 母线基准系统针对不同负荷需求实现调度问题。

Result: 文中未明确提及PSO和GA的具体结果，但进行了两者结果的相互比较。

Conclusion: 文中未明确给出结论。

Abstract: The economic dispatch of generators is a major concern in thermal power
plants that governs the share of each generating unit with an objective of
minimizing fuel cost by fulfilling load demand. This problem is not as simple
as it looks because of system constraints that cannot be neglected practically.
Moreover, increased awareness of clean technology imposes another important
limit on the emission of pollutants obtained from burning of fossil fuels.
Classical optimization methods lack the ability of solving such a complex and
multi-objective problem. Hence, various modern artificial intelligence (AI)
techniques based on evolution and social behaviour of organisms are being used
to solve such problems because they are easier to implement, give accurate
results and take less computational time. In this work, a study is done on most
of the contemporary basic AI techniques being used in literature for power
systems in general and combined economic emission dispatch (CEED) in
particular. The dispatch problem is implemented on IEEE 30-bus benchmarked
system in MATLAB for different load demands considering all gases (COX, NOX and
SOX) using particle swarm optimization (PSO) and genetic algorithm (GA) and
their results are compared with each other.

</details>


### [313] [A Synthetic Pseudo-Autoencoder Invites Examination of Tacit Assumptions in Neural Network Design](https://arxiv.org/abs/2506.12076)
*Assaf Marron*

Main category: cs.NE

TL;DR: 提出无需训练的手工神经网络，解决整数集编码与恢复问题，借此反思自动编码和机器学习的假设并从生物学视角探讨。


<details>
  <summary>Details</summary>
Motivation: 反思自动编码和机器学习中可能不必要的假设，部分源于围绕物种特征自然自动编码的生物进化理论研究。

Method: 使用标准神经网络操作（带偏置的加权和与恒等激活），通过设计而非学习，简单拼接数字、利用硬件级右位数字截断进行位操作。

Result: 构建出无需训练的手工神经网络，解决整数集编码与恢复问题。

Conclusion: 从生物学视角对相关讨论进行了完善。

Abstract: We present a handcrafted neural network that, without training, solves the
seemingly difficult problem of encoding an arbitrary set of integers into a
single numerical variable, and then recovering the original elements. While
using only standard neural network operations -- weighted sums with biases and
identity activation -- we make design choices that challenge common notions in
this area around representation, continuity of domains, computation,
learnability and more. For example, our construction is designed, not learned;
it represents multiple values using a single one by simply concatenating digits
without compression, and it relies on hardware-level truncation of rightmost
digits as a bit-manipulation mechanism. This neural net is not intended for
practical application. Instead, we see its resemblance to -- and deviation from
-- standard trained autoencoders as an invitation to examine assumptions that
may unnecessarily constrain the development of systems and models based on
autoencoding and machine learning. Motivated in part by our research on a
theory of biological evolution centered around natural autoencoding of species
characteristics, we conclude by refining the discussion with a biological
perspective.

</details>


### [314] [Efficient Parallel Training Methods for Spiking Neural Networks with Constant Time Complexity](https://arxiv.org/abs/2506.12087)
*Wanjin Feng,Xingyu Gao,Wenqian Du,Hailong Shi,Peilin Zhao,Pengcheng Wu,Chunyan Miao*

Main category: cs.NE

TL;DR: 提出固定点并行训练（FPT）方法加速SNN训练，降复杂度且不损精度。


<details>
  <summary>Details</summary>
Motivation: SNN因处理T个尖峰的顺序性，时间复杂度高，训练计算成本大。

Method: 采用LIF神经元的定点迭代形式，将时间复杂度降至O(K)。

Result: FPT有效模拟原LIF神经元动态，显著减少计算时间且不牺牲精度。

Conclusion: FPT是现实应用中可扩展且高效的解决方案，尤其适用于长期任务。

Abstract: Spiking Neural Networks (SNNs) often suffer from high time complexity $O(T)$
due to the sequential processing of $T$ spikes, making training computationally
expensive.
  In this paper, we propose a novel Fixed-point Parallel Training (FPT) method
to accelerate SNN training without modifying the network architecture or
introducing additional assumptions.
  FPT reduces the time complexity to $O(K)$, where $K$ is a small constant
(usually $K=3$), by using a fixed-point iteration form of Leaky
Integrate-and-Fire (LIF) neurons for all $T$ timesteps.
  We provide a theoretical convergence analysis of FPT and demonstrate that
existing parallel spiking neurons can be viewed as special cases of our
proposed method.
  Experimental results show that FPT effectively simulates the dynamics of
original LIF neurons, significantly reducing computational time without
sacrificing accuracy.
  This makes FPT a scalable and efficient solution for real-world applications,
particularly for long-term tasks.
  Our code will be released at
\href{https://github.com/WanjinVon/FPT}{\texttt{https://github.com/WanjinVon/FPT}}.

</details>


### [315] [Optimized Spectral Fault Receptive Fields for Diagnosis-Informed Prognosis](https://arxiv.org/abs/2506.12375)
*Stan Muñoz Gutiérrez,Franz Wotawa*

Main category: cs.NE

TL;DR: 本文提出受生物启发的SFRFs技术用于轴承故障诊断和剩余使用寿命估计，通过多目标进化优化策略调参，在数据集上验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 解决轴承故障诊断和剩余使用寿命估计中故障特征检测及参数优化问题，利用生物原理提升方法性能。

Method: 借鉴视网膜神经节细胞感受野提出频域特征提取算法，设计SFRFs作为频谱滤波器，用基于NSGA - II的多目标进化优化策略调参，用装袋回归器进行RUL预测。

Result: 在XJTU - SY数据集上验证方法适用于构建健康监测条件指标，能检测早期故障，实现准确RUL预测。

Conclusion: SFRFs具有可解释性和原则性设计，架起信号处理、生物传感原理和数据驱动预测之间的桥梁。

Abstract: This paper introduces Spectral Fault Receptive Fields (SFRFs), a biologically
inspired technique for degradation state assessment in bearing fault diagnosis
and remaining useful life (RUL) estimation. Drawing on the center-surround
organization of retinal ganglion cell receptive fields, we propose a
frequency-domain feature extraction algorithm that enhances the detection of
fault signatures in vibration signals. SFRFs are designed as antagonistic
spectral filters centered on characteristic fault frequencies, with inhibitory
surrounds that enable robust characterization of incipient faults under
variable operating conditions. A multi-objective evolutionary optimization
strategy based on NSGA-II algorithm is employed to tune the receptive field
parameters by simultaneously minimizing RUL prediction error, maximizing
feature monotonicity, and promoting smooth degradation trajectories. The method
is demonstrated on the XJTU-SY bearing run-to-failure dataset, confirming its
suitability for constructing condition indicators in health monitoring
applications. Key contributions include: (i) the introduction of SFRFs,
inspired by the biology of vision in the primate retina; (ii) an evolutionary
optimization framework guided by condition monitoring and prognosis criteria;
and (iii) experimental evidence supporting the detection of early-stage faults
and their precursors. Furthermore, we confirm that our diagnosis-informed
spectral representation achieves accurate RUL prediction using a bagging
regressor. The results highlight the interpretability and principled design of
SFRFs, bridging signal processing, biological sensing principles, and
data-driven prognostics in rotating machinery.

</details>


### [316] [Neuromorphic Online Clustering and Its Application to Spike Sorting](https://arxiv.org/abs/2506.12555)
*James E. Smith*

Main category: cs.NE

TL;DR: 提出用传统机器学习符号语言表达的主动树突公式，开发神经形态树突用于动态在线聚类，在尖峰排序中表现优于k - means。


<details>
  <summary>Details</summary>
Motivation: 以主动树突为基础构建具有生物大脑特性的神经网络，找到替代尖峰神经元公式的方案。

Method: 提出主动树突公式，开发神经形态树突作为基本神经构建块，通过尖峰排序实验进行验证，并与k - means聚类方法比较。

Result: 神经形态树突在尖峰排序中优于计算密集型的离线k - means聚类方法，只需单遍处理输入流并能边学习边处理。

Conclusion: 神经形态树突在动态在线聚类方面表现出色，能适应多种输入场景。

Abstract: Active dendrites are the basis for biologically plausible neural networks
possessing many desirable features of the biological brain including
flexibility, dynamic adaptability, and energy efficiency. A formulation for
active dendrites using the notational language of conventional machine learning
is put forward as an alternative to a spiking neuron formulation. Based on this
formulation, neuromorphic dendrites are developed as basic neural building
blocks capable of dynamic online clustering. Features and capabilities of
neuromorphic dendrites are demonstrated via a benchmark drawn from experimental
neuroscience: spike sorting. Spike sorting takes inputs from electrical probes
implanted in neural tissue, detects voltage spikes (action potentials) emitted
by neurons, and attempts to sort the spikes according to the neuron that
emitted them. Many spike sorting methods form clusters based on the shapes of
action potential waveforms, under the assumption that spikes emitted by a given
neuron have similar shapes and will therefore map to the same cluster. Using a
stream of synthetic spike shapes, the accuracy of the proposed dendrite is
compared with the more compute-intensive, offline k-means clustering approach.
Overall, the dendrite outperforms k-means and has the advantage of requiring
only a single pass through the input stream, learning as it goes. The
capabilities of the neuromorphic dendrite are demonstrated for a number of
scenarios including dynamic changes in the input stream, differing neuron spike
rates, and varying neuron counts.

</details>


### [317] [Energy-Efficient Digital Design: A Comparative Study of Event-Driven and Clock-Driven Spiking Neurons](https://arxiv.org/abs/2506.13268)
*Filippo Marostica,Alessio Carpegna,Alessandro Savino,Stefano Di Carlo*

Main category: cs.NE

TL;DR: 本文通过比较事件驱动和时钟驱动实现，对用于硬件加速的脉冲神经网络（SNN）神经元模型进行了综合评估，桥接软件模拟与硬件实现，推动下一代SNN加速器发展。


<details>
  <summary>Details</summary>
Motivation: 对用于硬件加速的SNN神经元模型进行综合评估，为构建节能、实时的神经形态系统提供指导。

Method: 先在软件中对基于不同LIF神经元变体的多种SNN模型在多个数据集上进行快速原型设计和测试，再在FPGA上进行硬件实现以验证模拟结果。

Result: 研究了输入刺激变化对延迟、功耗、能源效率和资源利用率等关键性能指标的影响。

Conclusion: 工作桥接了软件模拟和硬件实现，推动了下一代SNN加速器的发展。

Abstract: This paper presents a comprehensive evaluation of Spiking Neural Network
(SNN) neuron models for hardware acceleration by comparing event driven and
clock-driven implementations. We begin our investigation in software, rapidly
prototyping and testing various SNN models based on different variants of the
Leaky Integrate and Fire (LIF) neuron across multiple datasets. This phase
enables controlled performance assessment and informs design refinement. Our
subsequent hardware phase, implemented on FPGA, validates the simulation
findings and offers practical insights into design trade offs. In particular,
we examine how variations in input stimuli influence key performance metrics
such as latency, power consumption, energy efficiency, and resource
utilization. These results yield valuable guidelines for constructing energy
efficient, real time neuromorphic systems. Overall, our work bridges software
simulation and hardware realization, advancing the development of next
generation SNN accelerators.

</details>


### [318] [Evaluation of Nuclear Microreactor Cost-competitiveness in Current Electricity Markets Considering Reactor Cost Uncertainties](https://arxiv.org/abs/2506.13361)
*Muhammad R. Abdusammi,Ikhwan Khaleb,Fei Gao,Aditi Verma*

Main category: cs.NE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper evaluates the cost competitiveness of microreactors in today's
electricity markets, with a focus on uncertainties in reactor costs. A Genetic
Algorithm (GA) is used to optimize key technical parameters, such as reactor
capacity, fuel enrichment, tail enrichment, refueling interval, and discharge
burnup, to minimize the Levelized Cost of Energy (LCOE). Base case results are
validated using Simulated Annealing (SA). By incorporating Probability
Distribution Functions (PDFs) for fuel cycle costs, the study identifies
optimal configurations under uncertainty. Methodologically, it introduces a
novel framework combining probabilistic cost modeling with evolutionary
optimization. Results show that microreactors can remain cost-competitive, with
LCOEs ranging from \$48.21/MWh to \$78.32/MWh when supported by the Production
Tax Credit (PTC). High reactor capacity, low fuel enrichment, moderate tail
enrichment and refueling intervals, and high discharge burnup enhance cost
efficiency. Among all factors, overnight capital cost (OCC) has the most
significant impact on LCOE, while O&M and fuel cost uncertainties have lesser
effects. The analysis highlights how energy policies like the PTC can reduce
LCOE by 22-24%, improving viability despite cost variability. Compared to
conventional nuclear, coal, and renewable sources like offshore wind, hydro,
and biomass, optimized microreactors show strong economic potential. This
research defines a realistic design space and key trade-offs, offering
actionable insights for policymakers, reactor designers, and energy planners
aiming to accelerate the deployment of affordable, sustainable microreactors.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [319] [The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification](https://arxiv.org/abs/2506.12084)
*Michele Alberti,François Bobot,Julien Girard-Satabin,Alban Grastien,Aymeric Varasse,Zakaria Chihani*

Main category: cs.SE

TL;DR: 本文介绍开源平台CAISAR用于机器学习规范和验证，其规范语言可建模复杂属性，能自动翻译为对现有证明器的查询。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习程序形式化规范和验证工具多样且碎片化，难以比较，且现有工具对复杂属性表达能力不足。

Method: 提出CAISAR平台，给出适用的规范语言，使用自动图编辑技术将规范翻译为对现有证明器的查询。

Result: 展示了在具体用例中规范可自动翻译为对现有证明器的查询，可使用其现成版本。

Conclusion: CAISAR平台为机器学习规范和验证提供了有效的解决方案。

Abstract: The formal specification and verification of machine learning programs saw
remarkable progress in less than a decade, leading to a profusion of tools.
However, diversity may lead to fragmentation, resulting in tools that are
difficult to compare, except for very specific benchmarks. Furthermore, this
progress is heavily geared towards the specification and verification of a
certain class of property, that is, local robustness properties. But while
provers are becoming more and more efficient at solving local robustness
properties, even slightly more complex properties, involving multiple neural
networks for example, cannot be expressed in the input languages of winners of
the International Competition of Verification of Neural Networks VNN-Comp. In
this tool paper, we present CAISAR, an open-source platform dedicated to
machine learning specification and verification. We present its specification
language, suitable for modelling complex properties on neural networks, support
vector machines and boosted trees. We show on concrete use-cases how
specifications written in this language are automatically translated to queries
to state-of-the-art provers, notably by using automated graph editing
techniques, making it possible to use their off-the-shelf versions. The
artifact to reproduce the paper claims is available at the following DOI:
https://doi.org/10.5281/zenodo.15209510

</details>


### [320] [Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data](https://arxiv.org/abs/2506.12111)
*Oscar Boullosa Dapena*

Main category: cs.SE

TL;DR: 本文提出量子启发的可微积分神经网络（QIDINNs），解决流式数据实时连续学习问题，在合成和真实世界流式任务中验证其有效性并给出量子扩展方向。


<details>
  <summary>Details</summary>
Motivation: 传统基于梯度的模型如BPTT在处理时间无界数据时存在计算和稳定性限制，实时连续学习是深度学习和AI系统的核心挑战。

Method: 引入QIDINNs，利用积分号下求导的费曼技巧将神经更新表示为历史数据的积分。

Result: 模型在合成和真实世界流式任务中有效。

Conclusion: QIDINNs有平滑稳定的学习动态，具有物理可解释性和计算可行性，为混合经典 - 量子神经计算开辟道路，还给出量子扩展和可扩展实现方向。

Abstract: Real-time continuous learning over streaming data remains a central challenge
in deep learning and AI systems. Traditional gradient-based models such as
backpropagation through time (BPTT) face computational and stability
limitations when dealing with temporally unbounded data. In this paper, we
introduce a novel architecture, Quantum-Inspired Differentiable Integral Neural
Networks (QIDINNs), which leverages the Feynman technique of differentiation
under the integral sign to formulate neural updates as integrals over
historical data. This reformulation allows for smoother, more stable learning
dynamics that are both physically interpretable and computationally tractable.
Inspired by Feynman's path integral formalism and compatible with quantum
gradient estimation frameworks, QIDINNs open a path toward hybrid
classical-quantum neural computation. We demonstrate our model's effectiveness
on synthetic and real-world streaming tasks, and we propose directions for
quantum extensions and scalable implementations.

</details>


### [321] [Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure](https://arxiv.org/abs/2506.12278)
*Zheyuan Yang,Zexi Kuang,Xue Xia,Yilun Zhao*

Main category: cs.SE

TL;DR: 提出用于测试用例生成的新基准TestCase - Eval，含500个算法问题和10万个手工解决方案，对19个LLM进行评估。


<details>
  <summary>Details</summary>
Motivation: 系统评估大语言模型在测试用例生成方面的能力。

Method: 引入TestCase - Eval基准，关注故障覆盖和故障暴露两个关键任务，对19个开源和专有大语言模型进行评估。

Result: 对19个大语言模型进行了全面评估。

Conclusion: 评估结果揭示了大语言模型在为算法问题生成有效测试用例方面的优势和局限性。

Abstract: We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs
in test-case generation. TestCase-Eval includes 500 algorithm problems and
100,000 human-crafted solutions from the Codeforces platform. It focuses on two
pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test
sets probe diverse input scenarios and cover a wide range of potential failure
modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored
test input that reveals a specific incorrect code implementation. We provide a
comprehensive assessment of 19 state-of-the-art open-source and proprietary
LLMs on TestCase-Eval, offering insights into their strengths and limitations
in generating effective test cases for algorithm problems.

</details>


### [322] [The Foundation Cracks: A Comprehensive Study on Bugs and Testing Practices in LLM Libraries](https://arxiv.org/abs/2506.12320)
*Weipeng Jiang,Xiaoyu Zhang,Xiaofei Xie,Jiongchi Yu,Yuhan Zhi,Shiqing Ma,Chao Shen*

Main category: cs.SE

TL;DR: 对大语言模型（LLM）库的错误特征和测试实践进行实证研究，分析错误类型、测试用例，发现问题并给出提升质量建议。


<details>
  <summary>Details</summary>
Motivation: LLM库在LLM生态系统中至关重要，但常面临质量问题和错误，威胁AI系统可靠性，需填补相关研究空白。

Method: 分析HuggingFace Transformers和vLLM两个库的313个错误修复提交，手动分析建立错误分类；检查7748个测试函数确定测试预言类别，评估测试有效性。

Result: API滥用是主要根因；现有测试中预定义预期输出策略最常见；多数错误因测试用例不足、缺乏测试驱动和测试预言薄弱而未被检测到。

Conclusion: 根据研究结果，给出提升LLM库质量保证的建议。

Abstract: Large Language Model (LLM) libraries have emerged as the foundational
infrastructure powering today's AI revolution, serving as the backbone for LLM
deployment, inference optimization, fine-tuning, and production serving across
diverse applications. Despite their critical role in the LLM ecosystem, these
libraries face frequent quality issues and bugs that threaten the reliability
of AI systems built upon them. To address this knowledge gap, we present the
first comprehensive empirical investigation into bug characteristics and
testing practices in modern LLM libraries. We examine 313 bug-fixing commits
extracted across two widely-adopted LLM libraries: HuggingFace Transformers and
vLLM.Through rigorous manual analysis, we establish comprehensive taxonomies
categorizing bug symptoms into 5 types and root causes into 14 distinct
categories.Our primary discovery shows that API misuse has emerged as the
predominant root cause (32.17%-48.19%), representing a notable transition from
algorithm-focused defects in conventional deep learning frameworks toward
interface-oriented problems. Additionally, we examine 7,748 test functions to
identify 7 distinct test oracle categories employed in current testing
approaches, with predefined expected outputs (such as specific tensors and text
strings) being the most common strategy. Our assessment of existing testing
effectiveness demonstrates that the majority of bugs escape detection due to
inadequate test cases (41.73%), lack of test drivers (32.37%), and weak test
oracles (25.90%). Drawing from these findings, we offer some recommendations
for enhancing LLM library quality assurance.

</details>


### [323] [How Developers Use AI Agents: When They Work, When They Don't, and Why](https://arxiv.org/abs/2506.12347)
*Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Gustavo Soares,Emerson Murphy-Hill*

Main category: cs.SE

TL;DR: 研究开发者与软件工程代理（SWE agents）协作解决问题情况，发现增量解决问题、积极协作和迭代输出更易成功，但存在信任等挑战。


<details>
  <summary>Details</summary>
Motivation: 了解开发者与SWE agents协作方式及交互中的通信挑战。

Method: 观察19名开发者使用IDE内代理解决33个其曾贡献过仓库中的开放问题。

Result: 参与者成功解决约一半问题，增量解决问题者比一次性解决者更成功，积极协作和迭代输出者也更成功，但存在信任代理响应、协作调试和测试等挑战。

Conclusion: 研究结果对开发者与代理成功协作及设计更有效的SWE agents有启示。

Abstract: Software Engineering Agents (SWE agents) can autonomously perform development
tasks on benchmarks like SWE Bench, but still face challenges when tackling
complex and ambiguous real-world tasks. Consequently, SWE agents are often
designed to allow interactivity with developers, enabling collaborative
problem-solving. To understand how developers collaborate with SWE agents and
the communication challenges that arise in such interactions, we observed 19
developers using an in-IDE agent to resolve 33 open issues in repositories to
which they had previously contributed. Participants successfully resolved about
half of these issues, with participants solving issues incrementally having
greater success than those using a one-shot approach. Participants who actively
collaborated with the agent and iterated on its outputs were also more
successful, though they faced challenges in trusting the agent's responses and
collaborating on debugging and testing. These results have implications for
successful developer-agent collaborations, and for the design of more effective
SWE agents.

</details>


### [324] [A Mapping Study About Training in Industry Context in Software Engineering](https://arxiv.org/abs/2506.12590)
*Breno Alves de Andrade,Rodrigo Siqueira,Lidiane Gomes,Antonio Oliveira,Danilo Monteiro Ribeiro*

Main category: cs.SE

TL;DR: 研究使用Eduardo Salas的培训框架对软件工程行业企业培训研究现状进行映射，发现研究集中在培训方法和教学策略，存在研究空白及方法不严谨问题，为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 软件工程行业企业培训缺乏系统化理解，本研究旨在用Eduardo Salas的培训框架映射其研究现状。

Method: 进行系统映射研究，选取并分析该领域26项主要研究，按Salas的四个关键领域分类。

Result: 研究主要集中在培训方法和教学策略，其他领域存在显著空白，多数研究为经验报告，缺乏方法严谨性和纵向评估。

Conclusion: 本研究提供结构化概述，揭示未充分探索领域，为未来研究提供方向，有助于学术界和实践界设计更有效的培训项目。

Abstract: Context: Corporate training plays a strategic role in the continuous
development of professionals in the software engineering industry. However,
there is a lack of systematized understanding of how training initiatives are
designed, implemented, and evaluated within this domain.
  Objective: This study aims to map the current state of research on corporate
training in software engineering in industry settings, using Eduardo Salas'
training framework as an analytical lens.
  Method: A systematic mapping study was conducted involving the selection and
analysis of 26 primary studies published in the field. Each study was
categorized according to Salas' four key areas: Training Needs Analysis,
Antecedent Training Conditions, Training Methods and Instructional Strategies,
and Post-Training Conditions.
  Results: The findings show a predominance of studies focusing on Training
Methods and Instructional Strategies. Significant gaps were identified in other
areas, particularly regarding Job/Task Analysis and Simulation-based Training
and Games. Most studies were experience reports, lacking methodological rigor
and longitudinal assessment.
  Conclusions: The study offers a structured overview of how corporate training
is approached in software engineering, revealing underexplored areas and
proposing directions for future research. It contributes to both academic and
practical communities by highlighting challenges, methodological trends, and
opportunities for designing more effective training programs in industry.

</details>


### [325] [Real-Time Agile Software Management for Edge and Fog Computing Based Smart City Infrastructure](https://arxiv.org/abs/2506.12616)
*Debasish Jana,Pinakpani Pal,Pawan Kumar*

Main category: cs.SE

TL;DR: 论文提出ROOF框架应对智慧城市实时数据处理挑战，经案例验证，并探讨AI驱动分析的挑战与前景。


<details>
  <summary>Details</summary>
Motivation: 智慧城市发展需可扩展、安全和节能的实时数据处理架构，传统云系统有带宽、延迟和能耗限制。

Method: 利用ROOF框架，在中间雾层和边缘网络层进行分散计算，采用雾缓存、超低功耗无线传输、AI资源分配，通过TLS加密、区块链认证和边缘访问控制增强安全。

Result: 来自布巴内斯瓦尔、巴塞罗那和哥本哈根的案例验证了ROOF在交通系统和环境监测中的应用。

Conclusion: 概述了AI驱动分析在智能城市基础设施中的关键挑战和前景。

Abstract: The evolution of smart cities demands scalable, secure, and energy-efficient
architectures for real-time data processing. With the number of IoT devices
expected to exceed 40 billion by 2030, traditional cloud-based systems are
increasingly constrained by bandwidth, latency, and energy limitations. This
paper leverages the ROOF (Real-time Onsite Operations Facilitation) framework
with decentralized computing at intermediary fog and peripheral edge network
layers to reduce latency by processing data near its point of origin. ROOF
features fog caching to avoid redundancy, ultra-low-power wireless transmission
for energy savings, and AI-driven resource allocation for efficiency. Security
is enhanced through TLS encryption, blockchain-based authentication, and
edge-level access control. Case studies from Bhubaneswar, Barcelona and
Copenhagen validate the use of ROOF in traffic systems and environmental
monitoring. The paper concludes by outlining key challenges and prospects of
AI-driven analytics in smart urban infrastructure.

</details>


### [326] [Social Media Reactions to Open Source Promotions: AI-Powered GitHub Projects on Hacker News](https://arxiv.org/abs/2506.12643)
*Prachnachai Meakpaiboonwattana,Warittha Tarntong,Thai Mekratanavorakul,Chaiyong Ragkhitwetsagul,Pattaraporn Sangaroonsilp,Raula Kula,Morakot Choetkiertikul,Kenichi Matsumoto,Thanwadee Sunetnanta*

Main category: cs.SE

TL;DR: 研究Hacker News对GitHub上AI项目开发者活动的影响，发现其能助力项目获关注、促进社区参与和软件开发。


<details>
  <summary>Details</summary>
Motivation: 社交媒体影响力增大，开源软件项目可借其提升知名度和吸引贡献者，探究Hacker News对GitHub AI项目开发者活动的影响。

Method: 分析两年内2195篇Hacker News故事及对应评论，跟踪1814个GitHub仓库活动。

Result: 至少19%的AI开发者在Hacker News上推广项目并获积极反馈，相关GitHub仓库的分叉、星标和贡献者显著增加。

Conclusion: Hacker News是AI开源软件项目的可行平台，有潜力吸引关注、促进社区参与和加速软件开发。

Abstract: Social media platforms have become more influential than traditional news
sources, shaping public discourse and accelerating the spread of information.
With the rapid advancement of artificial intelligence (AI), open-source
software (OSS) projects can leverage these platforms to gain visibility and
attract contributors. In this study, we investigate the relationship between
Hacker News, a social news site focused on computer science and
entrepreneurship, and the extent to which it influences developer activity on
the promoted GitHub AI projects.
  We analyzed 2,195 Hacker News (HN) stories and their corresponding comments
over a two-year period. Our findings reveal that at least 19\% of AI developers
promoted their GitHub projects on Hacker News, often receiving positive
engagement from the community. By tracking activity on the associated 1,814
GitHub repositories after they were shared on Hacker News, we observed a
significant increase in forks, stars, and contributors. These results suggest
that Hacker News serves as a viable platform for AI-powered OSS projects, with
the potential to gain attention, foster community engagement, and accelerate
software development.

</details>


### [327] [Towards Lean Research Inception: Assessing Practical Relevance of Formulated Research Problems](https://arxiv.org/abs/2506.12669)
*Anrafel Fernandes Pereira,Marcos Kalinowski,Maria Teresa Baldassarre,Jürgen Börstler,Nauman bin Ali,Daniel Mendez*

Main category: cs.SE

TL;DR: 本文介绍Lean Research Inception (LRI)框架评估软件工程研究问题的实用性，用其回顾评估一篇已发表论文，发现三标准获认可，但需调整术语和细化标准。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程研究贡献缺乏实际相关性问题，引入LRI框架支持相关研究问题的制定和评估。

Method: 将LRI回顾应用于已发表论文，让参与者用语义差异量表讨论和评估研究问题，收集反馈。

Result: 三个标准（有价值、可行、适用）对研究问题与工业需求匹配的重要性获总体认可，定性反馈建议调整术语和细化标准。

Conclusion: 虽LRI需进一步评估，但用语义差异量表应用三标准可帮助评估软件工程研究问题的实际相关性。

Abstract: [Context] The lack of practical relevance in many Software Engineering (SE)
research contributions is often rooted in oversimplified views of industrial
practice, weak industry connections, and poorly defined research problems.
Clear criteria for evaluating SE research problems can help align their value,
feasibility, and applicability with industrial needs. [Goal] In this paper, we
introduce the Lean Research Inception (LRI) framework, designed to support the
formulation and assessment of practically relevant research problems in SE. We
describe its initial evaluation strategy conducted in a workshop with a network
of SE researchers experienced in industry-academia collaboration and report the
evaluation of its three assessment criteria (valuable, feasible, and
applicable) regarding their importance in assessing practical relevance.
[Method] We applied LRI retroactively to a published research paper, engaging
workshop participants in discussing and assessing the research problem by
applying the proposed criteria using a semantic differential scale.
Participants provided feedback on the criteria's importance and completeness,
drawn from their own experiences in industry-academia collaboration. [Results]
The findings reveal an overall agreement on the importance of the three
criteria - valuable (83.3%), feasible (76.2%), and applicable (73.8%) - for
aligning research problems with industrial needs. Qualitative feedback
suggested adjustments in terminology with a clearer distinction between
feasible and applicable, and refinements for valuable by more clearly
considering business value, ROI, and originality. [Conclusion] While LRI
constitutes ongoing research and requires further evaluation, our results
strengthen our confidence that the three criteria applied using the semantic
differential scale can already help the community assess the practical
relevance of SE research problems.

</details>


### [328] [Get on the Train or be Left on the Station: Using LLMs for Software Engineering Research](https://arxiv.org/abs/2506.12691)
*Bianca Trinkenreich,Fabio Calefato,Geir Hanssen,Kelly Blincoe,Marcos Kalinowski,Mauro Pezzè,Paolo Tell,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: 本文讨论大语言模型（LLMs）对软件工程（SE）研究的影响，用麦克卢汉媒介四定律分析其利弊，呼吁SE研究界积极应对。


<details>
  <summary>Details</summary>
Motivation: LLMs正改变SE研究，研究界需主动参与和塑造其融入研究实践，强调人的能动性。

Method: 运用麦克卢汉媒介四定律分析LLMs对SE研究的影响。

Result: 分析揭示了LLMs带来的创新机会和潜在陷阱。

Conclusion: 呼吁SE研究界主动利用LLMs的好处，制定框架和指南来降低风险。

Abstract: The adoption of Large Language Models (LLMs) is not only transforming
software engineering (SE) practice but is also poised to fundamentally disrupt
how research is conducted in the field. While perspectives on this
transformation range from viewing LLMs as mere productivity tools to
considering them revolutionary forces, we argue that the SE research community
must proactively engage with and shape the integration of LLMs into research
practices, emphasizing human agency in this transformation. As LLMs rapidly
become integral to SE research - both as tools that support investigations and
as subjects of study - a human-centric perspective is essential. Ensuring human
oversight and interpretability is necessary for upholding scientific rigor,
fostering ethical responsibility, and driving advancements in the field.
Drawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI
in SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze
the impact of LLMs on SE research. Through this theoretical lens, we examine
how LLMs enhance research capabilities through accelerated ideation and
automated processes, make some traditional research practices obsolete,
retrieve valuable aspects of historical research approaches, and risk reversal
effects when taken to extremes. Our analysis reveals opportunities for
innovation and potential pitfalls that require careful consideration. We
conclude with a call to action for the SE research community to proactively
harness the benefits of LLMs while developing frameworks and guidelines to
mitigate their risks, to ensure continued rigor and impact of research in an
AI-augmented future.

</details>


### [329] [Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?](https://arxiv.org/abs/2506.12713)
*Xiangyang Li,Xiaopeng Li,Kuicai Dong,Quanhu Zhang,Rongju Ruan,Xinyi Dai,Xiaoshuang Liu,Shengchun Xu,Yasheng Wang,Ruiming Tang*

Main category: cs.SE

TL;DR: 引入HLCE基准测试评估大语言模型代码生成能力，发现强推理模型通过率低，提出自识别任务，揭示模型改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有主流基准测试对高级大语言模型缺乏挑战性，为更好反映高级推理和代码生成能力。

Method: 引入包含235个难题的HLCE，设计在线-离线沙箱，提出自识别任务，验证测试时间缩放定律。

Result: 最强推理模型o4 - mini(high)和Gemini - 2.5 Pro的pass@1率分别仅为15.9%和11.4%；自识别能力与代码生成性能不成正比；当前高级大语言模型在复杂编程任务上有很大改进空间。

Conclusion: 期望HLCE成为代码生成的里程碑挑战，推动高性能推理和人机协作编程发展。

Abstract: Code generation is a core capability of large language models (LLMs), yet
mainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with
medium-level difficulty and pose no challenge to advanced LLMs. To better
reflected the advanced reasoning and code generation ability, We introduce
Humanity's Last Code Exam (HLCE), comprising 235 most challenging problems from
the International Collegiate Programming Contest (ICPC World Finals) and the
International Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of
HLCE, we design a harmonized online-offline sandbox that guarantees fully
reproducible evaluation. Through our comprehensive evaluation, we observe that
even the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve
pass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a
novel "self-recognition" task to measure LLMs' awareness of their own
capabilities. Results indicate that LLMs' self-recognition abilities are not
proportionally correlated with their code generation performance. Finally, our
empirical validation of test-time scaling laws reveals that current advanced
LLMs have substantial room for improvement on complex programming tasks. We
expect HLCE to become a milestone challenge for code generation and to catalyze
advances in high-performance reasoning and human-AI collaborative programming.
Our code and dataset are also public
available(https://github.com/Humanity-s-Last-Code-Exam/HLCE).

</details>


### [330] [MCTS-Refined CoT: High-Quality Fine-Tuning Data for LLM-Based Repository Issue Resolution](https://arxiv.org/abs/2506.12728)
*Yibo Wang,Zhihao Peng,Ying Wang,Zhao Wei,Hai Yu,Zhiliang Zhu*

Main category: cs.SE

TL;DR: 大语言模型在软件工程自动化中表现出色，但专有模型有使用限制，开源模型表现不佳，现有CoT数据生成方法有缺陷。论文提出MCTS - REFINE算法生成高质量CoT数据，实验表明微调后的模型性能提升。


<details>
  <summary>Details</summary>
Motivation: 专有模型在软件工程自动化中有使用限制，开源模型在复杂任务中表现不佳，现有CoT数据生成方法存在数据质量低和错误累积问题，影响大语言模型学习可靠的问题解决能力。

Method: 提出MCTS - REFINE算法，通过严格的拒绝采样策略动态验证和优化中间推理步骤，增强MCTS的反思机制，将问题解决分解为三个子任务并设置明确标准，执行严格采样协议。

Result: 在SWE - bench Lite和SWE - bench Verified上实验，使用生成的CoT数据集微调的大语言模型性能大幅提升，如Qwen2.5 - 72B - Instruct分辨率超过同参数规模的SOTA基线模型。

Conclusion: MCTS - REFINE算法能生成高质量CoT数据，有效提高大语言模型在问题解决任务中的性能。

Abstract: LLMs demonstrate strong performance in auto-mated software engineering,
particularly for code generation and issue resolution. While proprietary models
like GPT-4o achieve high benchmarks scores on SWE-bench, their API dependence,
cost, and privacy concerns limit adoption. Open-source alternatives offer
transparency but underperform in complex tasks, especially sub-100B parameter
models. Although quality Chain-of-Thought (CoT) data can enhance reasoning,
current methods face two critical flaws: (1) weak rejection sampling reduces
data quality, and (2) inadequate step validation causes error accumulation.
These limitations lead to flawed reasoning chains that impair LLMs'ability to
learn reliable issue resolution. The paper proposes MCTS-REFINE, an enhanced
Monte Carlo Tree Search (MCTS)-based algorithm that dynamically validates and
optimizes intermediate reasoning steps through a rigorous rejection sampling
strategy, generating high-quality CoT data to improve LLM performance in issue
resolution tasks. Key innovations include: (1) augmenting MCTS with a
reflection mechanism that corrects errors via rejection sampling and
refinement, (2) decomposing issue resolution into three subtasks-File
Localization, Fault Localization, and Patch Generation-each with clear
ground-truth criteria, and (3) enforcing a strict sampling protocol where
intermediate outputs must exactly match verified developer patches, ensuring
correctness across reasoning paths. Experiments on SWE-bench Lite and SWE-bench
Verified demonstrate that LLMs fine-tuned with our CoT dataset achieve
substantial improvements over baselines.Notably, Qwen2.5-72B- Instruct achieves
28.3%(Lite) and 35.0%(Verified) resolution rates, surpassing SOTA baseline
SWE-Fixer-Qwen-72B with the same parameter scale, which only reached
24.7%(Lite) and 32.8%(Verified).

</details>


### [331] [IDOL: Improved Different Optimization Levels Testing for Solidity Compilers](https://arxiv.org/abs/2506.12760)
*Lantian Li,Yejian Liang,Zhongxing Yu*

Main category: cs.SE

TL;DR: 本文提出改进的不同优化级别（IDOL）方法测试Solidity编译器，初步评估发现三个编译器优化漏洞。


<details>
  <summary>Details</summary>
Motivation: 智能合约部署后难以修改，编译器影响其质量和安全性，需有效测试方法。

Method: 提出IDOL方法，对智能合约进行反向优化变换以生成语义等价变体，触发编译器优化逻辑。

Result: 初步评估时发现三个编译器优化漏洞。

Conclusion: IDOL方法可用于测试Solidity编译器，能发现优化相关漏洞。

Abstract: As blockchain technology continues to evolve and mature, smart contracts have
become a key driving force behind the digitization and automation of
transactions. Smart contracts greatly simplify and refine the traditional
business transaction processes, and thus have had a profound impact on various
industries such as finance and supply chain management. However, because smart
contracts cannot be modified once deployed, any vulnerabilities or design flaws
within the contract cannot be easily fixed, potentially leading to significant
financial losses or even legal issues. The compiler, as a critical component in
the development process, directly affects the quality and security of smart
contracts. This paper innovatively proposes a method, known as the Improved
Different Optimization Levels (IDOL), for testing the Solidity compiler. The
key idea behind IDOL is to perform reverse optimization transformations (i.e.,
change optimized form into unoptimized form) to generate semantically
equivalent variants of the smart contracts under test, aiming to maximize the
opportunities to trigger the optimization logic of compilers. We conducted a
preliminary evaluation of IDOL and three confirmed compiler optimization bugs
have been uncovered at the time of writing.

</details>


### [332] [Towards Operation Proof Obligation Generation for VDM](https://arxiv.org/abs/2506.12858)
*Nick Battle,Peter Gorm Larsen*

Main category: cs.SE

TL;DR: 本文介绍解决VDM工具显式操作体义务生成支持不足问题的工作现状。


<details>
  <summary>Details</summary>
Motivation: VDM工具对显式操作体的义务生成支持一直有限。

Method: 未提及具体方法，主要描述当前工作状态。

Result: 展示了目前在解决该问题上的能力。

Conclusion: 指出仍有工作待完成。

Abstract: All formalisms have the ability to ensure that their models are internally
consistent. Potential inconsistencies are generally highlighted by assertions
called proof obligations, and the generation of these obligations is an
important role of the tools that support the method. This capability has been
available for VDM tools for many years. However, support for obligation
generation for explicit operation bodies has always been limited. This work
describes the current state of work to address this, showing the capabilities
so far and highlighting the work remaining.

</details>


### [333] [Designing Deep Learning Frameworks for LLMs:Challenges, Expectations, and Opportunities](https://arxiv.org/abs/2506.13114)
*Yanzhou Mu,Rong Wang,Juan Zhai,Chunrong Fang,Xiang Chen,Jiacong Wu,An Guo,Jiawei Shen,Bingzhuo Li,Zhenyu Chen*

Main category: cs.SE

TL;DR: 文章通过分析三大深度学习框架和八个大语言模型工具包的问题报告，构建分类法，评估挑战优先级，提出关键发现和建议，以改善深度学习框架对大语言模型的支持。


<details>
  <summary>Details</summary>
Motivation: 探究深度学习框架在支持大语言模型时面临的挑战，因为大语言模型对深度学习框架的可扩展性、稳定性和效率有极高要求，而现有框架的问题会影响开发。

Method: 对三大深度学习框架和八个大语言模型工具包的问题报告进行大规模分析，构建分类法，并通过与用户和开发者访谈丰富分类法。

Result: 开发了包含问题、需求和错误主题的综合分类法，评估了挑战的重要性和优先级，提出五个关键发现和五个可操作建议。

Conclusion: 指出当前深度学习框架的关键局限性，为下一代大语言模型的构建和应用提供具体指导。

Abstract: Large language models (LLMs) drive significant advancements in real industry
applications. LLMs rely on DL frameworks for efficient model construction,
distributed execution, and optimized deployment. Their large parameter scale
and long execution cycles place extreme demands on DL frameworks in terms of
scalability, stability, and efficiency. Therefore, poor usability, limited
functionality, and subtle bugs in DL frameworks may hinder development
efficiency and cause severe failures or resource waste. However, a fundamental
question remains underinvestigated, i.e., What challenges do DL frameworks face
in supporting LLMs? To seek an answer, we investigate these challenges through
a large-scale analysis of issue reports from three major DL frameworks
(MindSpore, PyTorch, TensorFlow) and eight associated LLM toolkits (e.g.,
Megatron). We construct a taxonomy of LLM-centric bugs, requirements, and user
questions and enrich it through interviews with 11 LLM users and eight DL
framework developers, uncovering key technical challenges and misalignments
between user needs and developer priorities. Our contributions are threefold:
(1) we develop a comprehensive taxonomy comprising four question themes (nine
sub-themes), four requirement themes (15 sub-themes), and ten bug themes (45
sub-themes); (2) we assess the perceived importance and priority of these
challenges based on practitioner insights; and (3) we identify five key
findings across the LLM development and propose five actionable recommendations
to improve the reliability, usability, and testability of DL frameworks. Our
results highlight critical limitations in current DL frameworks and offer
concrete guidance for advancing their support for the next generation of LLM
construction and applications.

</details>


### [334] [Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches](https://arxiv.org/abs/2506.13171)
*Lukasz Mazur,Nenad Petrovic,James Pontes Miranda,Ansgar Radermacher,Robert Rasche,Alois Knoll*

Main category: cs.SE

TL;DR: 本文研究利用大语言模型（LLMs）回答软件模型相关问题的两种方法，对比发现代理方法在令牌使用上更高效，适合汽车行业，未来将拓展研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型为与复杂软件模型交互提供新机会，传统方法对大型软件模型交互和分析有挑战，因此研究利用LLMs的方法。

Method: 研究了直接提示和结合基于LLM的代理与通用文件访问工具的代理方法，并使用汽车和嵌入式领域的Ecore元模型进行评估。

Result: 代理方法与直接提示准确性相当，但在令牌使用上更高效。

Conclusion: 代理方法是汽车行业的可行且唯一实用的解决方案，小LLMs有本地执行优势，未来将拓展研究软件模型格式、代理架构和工作流。

Abstract: Large language models (LLMs) offer new opportunities for interacting with
complex software artifacts, such as software models, through natural language.
They present especially promising benefits for large software models that are
difficult to grasp in their entirety, making traditional interaction and
analysis approaches challenging. This paper investigates two approaches for
leveraging LLMs to answer questions over software models: direct prompting,
where the whole software model is provided in the context, and an agentic
approach combining LLM-based agents with general-purpose file access tools. We
evaluate these approaches using an Ecore metamodel designed for timing analysis
and software optimization in automotive and embedded domains. Our findings show
that while the agentic approach achieves accuracy comparable to direct
prompting, it is significantly more efficient in terms of token usage. This
efficiency makes the agentic approach particularly suitable for the automotive
industry, where the large size of software models makes direct prompting
infeasible, establishing LLM agents as not just a practical alternative but the
only viable solution. Notably, the evaluation was conducted using small LLMs,
which are more feasible to be executed locally - an essential advantage for
meeting strict requirements around privacy, intellectual property protection,
and regulatory compliance. Future work will investigate software models in
diverse formats, explore more complex agent architectures, and extend agentic
workflows to support not only querying but also modification of software
models.

</details>


### [335] [From Empirical Evaluation to Context-Aware Enhancement: Repairing Regression Errors with LLMs](https://arxiv.org/abs/2506.13182)
*Anh Ho,Thanh Le-Cong,Bach Le,Christine Rizkallah*

Main category: cs.SE

TL;DR: 本文对Java回归错误的自动程序修复（APR）技术进行实证研究，引入基准RegMiner4APR，发现经典APR工具失效，LLM - 基于的APR有潜力，结合错误诱导变更信息能显著提升其性能。


<details>
  <summary>Details</summary>
Motivation: 现有先进APR技术在回归错误修复方面的有效性未被充分探索，需要实证研究评估其在修复现实世界回归错误中的有效性。

Method: 引入Java回归错误基准RegMiner4APR，对其进行深入分析，评估传统APR工具和基于LLM的APR方法修复回归错误的能力，研究结合错误诱导变更信息对基于LLM的APR的影响。

Result: 经典APR工具无法修复任何错误，基于LLM的APR有潜力，结合错误诱导变更信息的基于LLM的APR性能显著提升，成功修复数量是无此上下文时的1.8倍。

Conclusion: 基于LLM的APR在修复回归错误方面表现出潜力，结合错误诱导变更信息能有效提升其性能。

Abstract: [...] Since then, various APR approaches, especially those leveraging the
power of large language models (LLMs), have been rapidly developed to fix
general software bugs. Unfortunately, the effectiveness of these advanced
techniques in the context of regression bugs remains largely unexplored. This
gap motivates the need for an empirical study evaluating the effectiveness of
modern APR techniques in fixing real-world regression bugs.
  In this work, we conduct an empirical study of APR techniques on Java
regression bugs. To facilitate our study, we introduce RegMiner4APR, a
high-quality benchmark of Java regression bugs integrated into a framework
designed to facilitate APR research. The current benchmark includes 99
regression bugs collected from 32 widely used real-world Java GitHub
repositories. We begin by conducting an in-depth analysis of the benchmark,
demonstrating its diversity and quality. Building on this foundation, we
empirically evaluate the capabilities of APR to regression bugs by assessing
both traditional APR tools and advanced LLM-based APR approaches. Our
experimental results show that classical APR tools fail to repair any bugs,
while LLM-based APR approaches exhibit promising potential. Motivated by these
results, we investigate impact of incorporating bug-inducing change information
into LLM-based APR approaches for fixing regression bugs. Our results highlight
that this context-aware enhancement significantly improves the performance of
LLM-based APR, yielding 1.8x more successful repairs compared to using
LLM-based APR without such context.

</details>


### [336] [Empirical Evaluation of Large Language Models in Automated Program Repair](https://arxiv.org/abs/2506.13186)
*Jiajun Sun,Fengjie Li,Xinzhu Qi,Hongyu Zhang,Jiajun Jiang*

Main category: cs.SE

TL;DR: 本文对四个开源大语言模型在不同语言和场景下的自动程序修复能力进行全面实证研究，得出模型专业化、大小与性能关系等结论，为基于大语言模型的自动程序修复系统设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖早期小模型和Java基准，现代大语言模型在不同语言和场景下的修复能力未充分探索。

Method: 对CodeLlama、LLaMA、StarCoder和DeepSeek - Coder四个开源大语言模型进行研究，在两种错误场景、三种语言和四种提示策略下，对六个基准上的超60万个生成补丁进行评估分析。

Result: 模型专业化可优于通用大模型；修复性能与模型大小非线性相关；正确补丁常较早生成；提示策略显著影响结果。

Conclusion: 研究结果为设计有效高效的基于大语言模型的自动程序修复系统提供实用指导。

Abstract: The increasing prevalence of software bugs has made automated program repair
(APR) a key research focus. Large language models (LLMs) offer new
opportunities for APR, but existing studies mostly rely on smaller,
earlier-generation models and Java benchmarks. The repair capabilities of
modern, large-scale LLMs across diverse languages and scenarios remain
underexplored. To address this, we conduct a comprehensive empirical study of
four open-source LLMs, CodeLlama, LLaMA, StarCoder, and DeepSeek-Coder,
spanning 7B to 33B parameters, diverse architectures, and purposes. We evaluate
them across two bug scenarios (enterprise-grades and algorithmic), three
languages (Java, C/C++, Python), and four prompting strategies, analyzing over
600K generated patches on six benchmarks. Key findings include: (1) model
specialization (e.g., CodeLlama) can outperform larger general-purpose models
(e.g., LLaMA); (2) repair performance does not scale linearly with model size;
(3) correct patches often appear early in generation; and (4) prompts
significantly affect results. These insights offer practical guidance for
designing effective and efficient LLM-based APR systems.

</details>


### [337] [Isolating Noisy Labelled Test Cases in Human-in-the-Loop Oracle Learning](https://arxiv.org/abs/2506.13273)
*Charaka Geethal Kapugama*

Main category: cs.SE

TL;DR: 论文介绍了ISONOISE技术，用于识别在人机协作预言机学习中引入的错误标记测试用例，评估显示其能以超67%的准确率识别错误标记测试用例，提高学习可靠性。


<details>
  <summary>Details</summary>
Motivation: 错误标记的测试用例对人机协作预言机学习的训练过程有不利影响，需要一种技术来识别这些错误标记的测试用例。

Method: ISONOISE首先根据测试用例之间的分歧程度隔离疑似错误标记的测试用例，基于轻度分歧的测试用例训练中间自动测试预言机，根据该预言机的预测系统地呈现疑似错误标记的测试用例进行重新标记，若发现错误标记的测试用例则更新中间测试预言机，重复该过程直到重新标记中没有发现错误标记的测试用例。

Result: 在LEARN2FIX使用的人机协作预言机学习方法中评估，ISONOISE能以超过67%的准确率识别LEARN2FIX中人为引入的错误标记测试用例，且只需少量重新标记查询。

Conclusion: ISONOISE有潜力提高人机协作预言机学习的可靠性。

Abstract: Incorrectly labelled test cases can adversely affect the training process of
human-in-the-loop oracle learning tech-niques. This paper introduces ISONOISE,
a technique designed to identify such mislabelled test cases introduced during
human-in-the-loop oracle learning. This technique can be applied to programs
taking numeric inputs. Given a compromised automatic test oracle and its
training test suite, ISONOISE first isolates thetest cases suspected of being
mislabelled. This task is performed based on the level of disagreement of a
test case with respect to the others. An intermediate automatic test oracle is
trained based on the slightly disagreeing test cases. Based on the predictions
of this intermediate oracle, the test cases suspected of being mislabelled are
systematically presented for relabelling. When mislabelled test cases are
found, the intermediate test oracle is updated. This process repeats until no
mislabelled test case is found in relabelling. ISONOISE was evaluated within
the human-in-the-loop oracle learning method used in LEARN2FIX. Experimental
results demonstrate that ISONOISE can identify mislabelled test cases
introduced by the human in LEARN2FIX with over 67% accuracy, while requiring
only a small number of relabelling queries. These findings highlight the
potential of ISONOISE to enhance the reliability of human-in-the-loop oracle
learning.

</details>


### [338] [Adopting Use Case Descriptions for Requirements Specification: an Industrial Case Study](https://arxiv.org/abs/2506.13303)
*Julian Frattini,Anja Frattini*

Main category: cs.SE

TL;DR: 本文调查了一家大型全球分布式案例公司中用例描述的采用情况，发现实际采用与教科书建议有偏差，仅少数现象影响实际质量。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏用例描述在现实世界的采用情况、建议是否符合实际质量以及影响用例质量因素的见解，因此开展研究。

Method: 调查案例公司2020 - 2024年的1188个业务需求（含1192个各种形式的用例），手动评估273个模板式用例描述，生成描述性统计数据，并用推断性统计分析需求工程过程属性对用例质量的影响以及用例质量对后续软件开发活动的影响。

Result: 描述性结果显示实际采用与教科书建议有偏差，推断性结果表明仅少数现象如面向解决方案在实践中有实际影响。

Conclusion: 研究结果能引导用例质量研究朝着更相关的方向发展。

Abstract: Context: Use case (UC) descriptions are a prominent format for specifying
functional requirements. Existing literature abounds with recommendations on
how to write high-quality UC descriptions but lacks insights into (1) their
real-world adoption, (2) whether these recommendations correspond to actual
quality, and (3) which factors influence the quality of UCs. Objectives: We aim
to contribute empirical evidence about the adoption of UC descriptions in a
large, globally distributed case company. Methods: We surveyed 1188 business
requirements of a case company that were elicited from 2020-01-01 until
2024-12-31 and contained 1192 UCs in various forms. Among these, we manually
evaluated the 273 template-style UC descriptions against established quality
guidelines. We generated descriptive statistics of the format's adoption over
the surveyed time frame. Furthermore, we used inferential statistics to
determine (a) how properties of the requirements engineering process affected
the UC quality and (b) how UC quality affects subsequent software development
activities. Results and Conclusions: Our descriptive results show how the
adoption of UC descriptions in practice deviates from textbook recommendations.
However, our inferential results suggest that only a few phenomena like
solution-orientation show an actual impact in practice. These results can steer
UC quality research into a more relevant direction.

</details>


### [339] [Model Context Protocol (MCP) at First Glance: Studying the Security and Maintainability of MCP Servers](https://arxiv.org/abs/2506.13538)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文对Model Context Protocol (MCP) 进行大规模实证研究，评估开源MCP服务器的健康、安全和可维护性，发现新漏洞，强调需MCP特定检测技术。


<details>
  <summary>Details</summary>
Motivation: MCP虽成工具生态系统事实上的标准，但AI驱动的非确定性控制流带来新风险，需深入研究。

Method: 使用先进的健康指标和混合分析管道，结合通用静态分析工具和MCP特定扫描器，评估1899个开源MCP服务器。

Result: MCP服务器健康指标良好，但有八种不同漏洞，部分服务器有通用漏洞和MCP特定工具中毒情况，部分代码存在代码异味和重叠的错误模式。

Conclusion: 需要MCP特定的漏洞检测技术，同时肯定传统分析和重构实践的价值。

Abstract: Although Foundation Models (FMs), such as GPT-4, are increasingly used in
domains like finance and software engineering, reliance on textual interfaces
limits these models' real-world interaction. To address this, FM providers
introduced tool calling-triggering a proliferation of frameworks with distinct
tool interfaces. In late 2024, Anthropic introduced the Model Context Protocol
(MCP) to standardize this tool ecosystem, which has become the de facto
standard with over eight million weekly SDK downloads. Despite its adoption,
MCP's AI-driven, non-deterministic control flow introduces new risks to
sustainability, security, and maintainability, warranting closer examination.
  Towards this end, we present the first large-scale empirical study of MCP.
Using state-of-the-art health metrics and a hybrid analysis pipeline, combining
a general-purpose static analysis tool with an MCP-specific scanner, we
evaluate 1,899 open-source MCP servers to assess their health, security, and
maintainability. Despite MCP servers demonstrating strong health metrics, we
identify eight distinct vulnerabilities-only three overlapping with traditional
software vulnerabilities. Additionally, 7.2% of servers contain general
vulnerabilities and 5.5% exhibit MCP-specific tool poisoning. Regarding
maintainability, while 66% exhibit code smells, 14.4% contain ten bug patterns
overlapping prior research. These findings highlight the need for MCP-specific
vulnerability detection techniques while reaffirming the value of traditional
analysis and refactoring practices.

</details>


### [340] [DesignCoder: Hierarchy-Aware and Self-Correcting UI Code Generation with Large Language Models](https://arxiv.org/abs/2506.13663)
*Yunnong Chen,Shixian Ding,YingYing Zhang,Wenkai Chen,Jinzhou Du,Lingyun Sun,Liuqing Chen*

Main category: cs.SE

TL;DR: 提出DesignCoder框架解决多模态大语言模型代码生成质量问题，在多个指标上优于基线，用户研究显示可用性高。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型自动生成代码虽简化前端界面开发，但存在代码质量问题，现有方法难以保证视觉一致性和功能完整性，缺乏评估机制。

Method: 提出DesignCoder框架，引入UI Grouping Chains，采用分层分治方法生成代码，并加入自我修正机制。

Result: 在UI模拟数据集上评估，DesignCoder在视觉相似度指标上有性能提升，显著改善代码结构相似度；用户研究表明代码符合行业最佳实践，可用性、可读性和可维护性高。

Conclusion: DesignCoder为敏捷前端开发提供高效实用解决方案，使开发团队更专注核心功能和产品创新。

Abstract: Multimodal large language models (MLLMs) have streamlined front-end interface
development by automating code generation. However, these models also introduce
challenges in ensuring code quality. Existing approaches struggle to maintain
both visual consistency and functional completeness in the generated
components. Moreover, they lack mechanisms to assess the fidelity and
correctness of the rendered pages. To address these issues, we propose
DesignCoder, a novel hierarchical-aware and self-correcting automated code
generation framework. Specifically, we introduce UI Grouping Chains, which
enhance MLLMs' capability to understand and predict complex nested UI
hierarchies. Subsequently, DesignCoder employs a hierarchical
divide-and-conquer approach to generate front-end code. Finally, we incorporate
a self-correction mechanism to improve the model's ability to identify and
rectify errors in the generated code. Extensive evaluations on a dataset of UI
mockups collected from both open-source communities and industry projects
demonstrate that DesignCoder outperforms state-of-the-art baselines in React
Native, a widely adopted UI framework. Our method achieves a 37.63%, 9.52%,
12.82% performance increase in visual similarity metrics (MSE, CLIP, SSIM) and
significantly improves code structure similarity in terms of TreeBLEU,
Container Match, and Tree Edit Distance by 30.19%, 29.31%, 24.67%. Furthermore,
we conducted a user study with professional developers to assess the quality
and practicality of the generated code. Results indicate that DesignCoder
aligns with industry best practices, demonstrating high usability, readability,
and maintainability. Our approach provides an efficient and practical solution
for agile front-end development, enabling development teams to focus more on
core functionality and product innovation.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [341] [Introducing the PIT-plot -- a new tool in the portfolio manager's toolkit](https://arxiv.org/abs/2506.12068)
*Stig-Johan Wiklund,Magnus Ytterstad*

Main category: q-fin.PM

TL;DR: 介绍用于项目组合管理中项目优先级排序的新工具PIT - plot，改变视角关注项目对组合的影响。


<details>
  <summary>Details</summary>
Motivation: 为优化组织研发投资价值，需新工具支持项目组合管理中的项目优先级排序。

Method: 引入名为PIT - plot的新工具，改变传统关注项目自身属性的视角，关注项目对组合的影响。

Result: 未提及明确结果。

Conclusion: PIT - plot能让战略组合管理识别并聚焦对组合影响最大的项目，用于风险缓解或增值。

Abstract: Project portfolio management is an essential process for organizations aiming
to optimize the value of their R&D investments. In this article, we introduce a
new tool designed to support the prioritization of projects within project
portfolio management. We label this tool the PIT-plot, an acronym for Project
Impact Tornado plot, with reference to the similarity to the Tornado plot often
used for sensitivity analyses. Many traditional practices in portfolio
management focus on the properties of the projects available to the portfolio.
We are with the PIT-plot changing the perspective and focus not on the
properties of the projects themselves, but on the impact that the projects may
have on the portfolio. This enables the strategic portfolio management to
identify and focus on the projects of largest impact to the portfolio, either
for the purpose of risk mitigation or for the purpose of value-adding efforts.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [342] [Credit risk for large portfolios of green and brown loans: extending the ASRF model](https://arxiv.org/abs/2506.12510)
*Alessandro Ramponi,Sergio Scarlatti*

Main category: q-fin.RM

TL;DR: 提出含绿色和棕色贷款组合的信用风险模型，拓展ASRF框架，证明组合损失收敛，数值结果验证理论并说明风险值影响因素，模型可用于进一步开发。


<details>
  <summary>Details</summary>
Motivation: 构建适用于含绿色和棕色贷款组合的信用风险模型。

Method: 通过两因子copula结构拓展ASRF框架，用潜在偏态分布建模系统风险，特质风险为高斯分布。

Result: 在非均匀暴露设置下，证明组合损失二次均值收敛到反映两类贷款特征的极限，数值结果验证理论，展示风险值受多种因素影响。

Conclusion: 模型能适应对系统冲击的不同敏感性，为信用风险建模进一步发展提供基础。

Abstract: We propose a credit risk model for portfolios composed of green and brown
loans, extending the ASRF framework via a two-factor copula structure.
Systematic risk is modeled using potentially skewed distributions, allowing for
asymmetric creditworthiness effects, while idiosyncratic risk remains Gaussian.
Under a non-uniform exposure setting, we establish convergence in quadratic
mean of the portfolio loss to a limit reflecting the distinct characteristics
of the two loan segments. Numerical results confirm the theoretical findings
and illustrate how value-at-risk is affected by portfolio granularity, default
probabilities, factor loadings, and skewness. Our model accommodates
differential sensitivity to systematic shocks and offers a tractable basis for
further developments in credit risk modeling, including granularity
adjustments, CDO pricing, and empirical analysis of green loan portfolios.

</details>


### [343] [Implied Probabilities and Volatility in Credit Risk: A Merton-Based Approach with Binomial Trees](https://arxiv.org/abs/2506.12694)
*Jagdish Gnawali,Abootaleb Shirvani,Svetlozar T. Rachev*

Main category: q-fin.RM

TL;DR: 基于Merton模型框架，分两步探索信用风险定价，构建二项树建立映射以反映市场信用预期。


<details>
  <summary>Details</summary>
Motivation: 探索信用风险定价。

Method: 先利用Black - Scholes - Merton公式校准资产波动率，再假设固定初始资产价值构建二项树，从隐含波动率表面特定区域获取波动率输入，校准漂移和概率，建立风险中性和物理参数的映射。

Result: 构建了反映市场信用预期的隐含表面。

Conclusion: 所建立的映射为压力测试和信用风险分析提供了实用工具。

Abstract: We explore credit risk pricing by modeling equity as a call option and debt
as the difference between the firm's asset value and a put option, following
the structural framework of the Merton model. Our approach proceeds in two
stages: first, we calibrate the asset volatility using the Black-Scholes-Merton
(BSM) formula; second, we recover implied mean return and probability surfaces
under the physical measure. To achieve this, we construct a recombining
binomial tree under the real-world (natural) measure, assuming a fixed initial
asset value. The volatility input is taken from a specific region of the
implied volatility surface - based on moneyness and maturity - which then
informs the calibration of drift and probability. A novel mapping is
established between risk-neutral and physical parameters, enabling construction
of implied surfaces that reflect the market's credit expectations and offer
practical tools for stress testing and credit risk analysis.

</details>


### [344] [Automated Risk Management Mechanisms in DeFi Lending Protocols: A Crosschain Comparative Analysis of Aave and Compound](https://arxiv.org/abs/2506.12855)
*Erum Iftikhar,Wei Wei,John Cartlidge*

Main category: q-fin.RM

TL;DR: 本文实证分析热门借贷协议Aave和Compound v2与v3版本、L1和L2跨链清算机制表现，发现v3风险管理更佳，L1受大投资者青睐，L2受散户欢迎。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对最新借贷协议版本风险管理功能有效性的研究，为填补这一空白开展研究。

Method: 使用面板回归固定效应模型，分析2021年1月至2024年12月Aave和Compound清算机制跨版本和跨链表现。

Result: v3清算事件使总锁定价值和总收入增加，对L2影响更强；v2清算影响不显著；L1受大投资者青睐，L2受散户欢迎。

Conclusion: 最新的v3协议比早期v2协议有更好的风险管理。

Abstract: Blockchain-based decentralised lending is a rapidly growing and evolving
alternative to traditional lending, but it poses new risks. To mitigate these
risks, lending protocols have integrated automated risk management tools into
their smart contracts. However, the effectiveness of the latest risk management
features introduced in the most recent versions of these lending protocols is
understudied. To close this gap, we use a panel regression fixed effects model
to empirically analyse the cross-version (v2 and v3) and cross-chain (L1 and
L2) performance of the liquidation mechanisms of the two most popular lending
protocols, Aave and Compound, during the period Jan 2021 to Dec 2024. Our
analysis reveals that liquidation events in v3 of both protocols lead to an
increase in total value locked and total revenue, with stronger impact on the
L2 blockchain compared to L1. In contrast, liquidations in v2 have an
insignificant impact, which indicates that the most recent v3 protocols have
better risk management than the earlier v2 protocols. We also show that L1
blockchains are the preferred choice among large investors for their robust
liquidity and ecosystem depth, while L2 blockchains are more popular among
retail investors for their lower fees and faster execution.

</details>


### [345] [Choquet rating criteria, risk measures, and risk consistency](https://arxiv.org/abs/2506.13435)
*Nan Guo,Ruodu Wang,Chenxi Xia,Jingping Yang*

Main category: q-fin.RM

TL;DR: 本文在Choquet评级标准框架下引入并研究风险一致性概念，建立层次结构，刻画满足条件的度量和标准，通过案例说明部分Choquet评级标准可作为实用评级替代方案。


<details>
  <summary>Details</summary>
Motivation: 促进投资决策，在Choquet评级标准框架下研究能推动审慎投资决策的风险一致性概念。

Method: 考虑三种与风险一致性相关的概念，在单概率测度或多概率测度下进行公式化，研究其在评级标准和风险度量间的转换。

Result: 建立了风险一致性属性之间的层次结构，对满足风险一致性的Choquet风险度量和评级标准进行了完整刻画。

Conclusion: 部分Choquet评级标准可作为金融产品评级中违约概率和预期损失标准的有用替代方案。

Abstract: Credit ratings are widely used by investors as a screening device. We
introduce and study several natural notions of risk consistency that promote
prudent investment decisions in the framework of Choquet rating criteria. Three
closely related notions of risk consistency are considered: with respect to
risk aversion, the asset pooling effect, and the benefit of portfolio
diversification. These notions are formulated either under a single probability
measure or multiple probability measures. We show how these properties
translate between rating criteria and the corresponding risk measures, and
establish a hierarchical structure among them. These findings lead to a full
characterization of Choquet risk measures and Choquet rating criteria
satisfying risk consistency properties. Illustrated by case studies on
collateralized loan obligations and catastrophe bonds, some classes of Choquet
rating criteria serve as useful alternatives to the probability of default and
expected loss criteria used in practice for rating financial products.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [346] [Temporal cross-validation impacts multivariate time series subsequence anomaly detection evaluation](https://arxiv.org/abs/2506.12183)
*Steven C. Hespeler,Pablo Moriano,Mingyan Li,Samuel C. Hollifield*

Main category: stat.ML

TL;DR: 研究TSCV策略对MTS异常检测分类器性能的影响，比较WF和SW方法，发现SW表现更好，为评估策略选择提供指导。


<details>
  <summary>Details</summary>
Motivation: TSCV技术在模型评估中对时间顺序的保留作用显著，但对分类器性能的影响研究不足，需系统研究其对MTS数据集异常检测分类器的影响。

Method: 比较walk - forward (WF)和sliding window (SW)方法在不同验证分区配置和分类器类型（浅学习器和深度学习分类器）下的表现。

Result: SW的中位数AUC - PR分数更高，性能方差更小，尤其是对局部时间连续性敏感的深度架构；分类器泛化受时间分区数量和结构影响，重叠窗口在低折叠数时更有效保留故障特征；不同算法在不同验证方案下表现不同。

Conclusion: 为流式时间序列异常检测模型的TSCV设计提供依据，指导在时间结构化学习环境中选择评估策略。

Abstract: Evaluating anomaly detection in multivariate time series (MTS) requires
careful consideration of temporal dependencies, particularly when detecting
subsequence anomalies common in fault detection scenarios. While time series
cross-validation (TSCV) techniques aim to preserve temporal ordering during
model evaluation, their impact on classifier performance remains underexplored.
This study systematically investigates the effect of TSCV strategy on the
precision-recall characteristics of classifiers trained to detect fault-like
anomalies in MTS datasets. We compare walk-forward (WF) and sliding window (SW)
methods across a range of validation partition configurations and classifier
types, including shallow learners and deep learning (DL) classifiers. Results
show that SW consistently yields higher median AUC-PR scores and reduced
fold-to-fold performance variance, particularly for deep architectures
sensitive to localized temporal continuity. Furthermore, we find that
classifier generalization is sensitive to the number and structure of temporal
partitions, with overlapping windows preserving fault signatures more
effectively at lower fold counts. A classifier-level stratified analysis
reveals that certain algorithms, such as random forests (RF), maintain stable
performance across validation schemes, whereas others exhibit marked
sensitivity. This study demonstrates that TSCV design in benchmarking anomaly
detection models on streaming time series and provide guidance for selecting
evaluation strategies in temporally structured learning environments.

</details>


### [347] [Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory](https://arxiv.org/abs/2506.12350)
*Jiancong Xiao,Zhekun Shi,Kaizhao Liu,Qi Long,Weijie J. Su*

Main category: stat.ML

TL;DR: 论文探讨RLHF违背社会选择理论公理却实践效果好的悖论，证明在一定假设下满足部分一致性，修改目标可确保一致性，还引入新对齐标准并分析RLHF表现。


<details>
  <summary>Details</summary>
Motivation: 解释RLHF违背社会选择理论基本公理却在实践中表现良好的原因。

Method: 在对偏好配置的温和且符合经验的假设下进行分析，修改奖励建模目标，引入新的对齐标准。

Result: 在一定假设下RLHF满足成对多数和Condorcet一致性；修改目标可确保一般偏好配置下的一致性；RLHF满足偏好匹配和偏好等价，但不满足群体偏好匹配。

Conclusion: 讨论了未来对齐方法如何设计以满足新引入的三个对齐标准。

Abstract: Despite its empirical success, Reinforcement Learning from Human Feedback
(RLHF) has been shown to violate almost all the fundamental axioms in social
choice theory -- such as majority consistency, pairwise majority consistency,
and Condorcet consistency. This raises a foundational question: why does RLHF
perform so well in practice if it fails these seemingly essential properties?
In this paper, we resolve this paradox by showing that under mild and
empirically plausible assumptions on the preference profile, RLHF does satisfy
pairwise majority and Condorcet consistency. These assumptions are frequently
satisfied in real-world alignment tasks, offering a theoretical explanation for
RLHF's strong practical performance. Furthermore, we show that a slight
modification to the reward modeling objective can ensure pairwise majority or
Condorcet consistency even under general preference profiles, thereby improving
the alignment process. Finally, we go beyond classical axioms in economic and
social choice theory and introduce new alignment criteria -- preference
matching, preference equivalence, and group preference matching -- that better
reflect the goal of learning distributions over responses. We show that while
RLHF satisfies the first two properties, it fails to satisfy the third. We
conclude by discussing how future alignment methods may be designed to satisfy
all three.

</details>


### [348] [On the existence of consistent adversarial attacks in high-dimensional linear classification](https://arxiv.org/abs/2506.12454)
*Matteo Vilucchio,Lenka Zdeborová,Bruno Loureiro*

Main category: stat.ML

TL;DR: 研究高维二分类中对抗攻击与其他误分类的区别，引入新误差指标，给出指标渐近刻画，揭示模型过参数化时对标签保留扰动的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 探究高维二分类中对抗攻击与因模型表达能力有限或数据有限导致的误分类的根本区别。

Method: 引入新的误差指标，对该指标在良定模型和潜空间模型中进行精确且严格的渐近刻画。

Result: 揭示了与标准鲁棒误差度量不同的脆弱性模式，表明模型过参数化时对标签保留扰动的脆弱性增加。

Conclusion: 为模型对对抗攻击敏感性的机制提供了理论见解。

Abstract: What fundamentally distinguishes an adversarial attack from a
misclassification due to limited model expressivity or finite data? In this
work, we investigate this question in the setting of high-dimensional binary
classification, where statistical effects due to limited data availability play
a central role. We introduce a new error metric that precisely capture this
distinction, quantifying model vulnerability to consistent adversarial attacks
-- perturbations that preserve the ground-truth labels. Our main technical
contribution is an exact and rigorous asymptotic characterization of these
metrics in both well-specified models and latent space models, revealing
different vulnerability patterns compared to standard robust error measures.
The theoretical results demonstrate that as models become more
overparameterized, their vulnerability to label-preserving perturbations grows,
offering theoretical insight into the mechanisms underlying model sensitivity
to adversarial attacks.

</details>


### [349] [A Transfer Learning Framework for Multilayer Networks via Model Averaging](https://arxiv.org/abs/2506.12455)
*Yongqin Qiu,Xinyu Zhang*

Main category: stat.ML

TL;DR: 提出用于多层网络的迁移学习框架，用双层模型平均法，有理论保证，计算高效且保护隐私，模拟和实际应用显示其优势。


<details>
  <summary>Details</summary>
Motivation: 现有多层网络链接预测技术依赖共享结构假设且需原始辅助数据，实用性受限。

Method: 提出使用双层模型平均法的迁移学习框架，用基于边的K折交叉验证准则自动加权层间和层内候选模型。

Result: 理论证明方法在温和条件下的最优性和权重收敛性，模拟显示在预测准确性和鲁棒性上优于其他方法，在两个实际推荐系统应用中展示实用价值。

Conclusion: 所提框架能有效解决现有多层网络链接预测技术的问题，具有高效性、隐私保护性和良好性能。

Abstract: Link prediction in multilayer networks is a key challenge in applications
such as recommendation systems and protein-protein interaction prediction.
While many techniques have been developed, most rely on assumptions about
shared structures and require access to raw auxiliary data, limiting their
practicality. To address these issues, we propose a novel transfer learning
framework for multilayer networks using a bi-level model averaging method. A
$K$-fold cross-validation criterion based on edges is used to automatically
weight inter-layer and intra-layer candidate models. This enables the transfer
of information from auxiliary layers while mitigating model uncertainty, even
without prior knowledge of shared structures. Theoretically, we prove the
optimality and weight convergence of our method under mild conditions.
Computationally, our framework is efficient and privacy-preserving, as it
avoids raw data sharing and supports parallel processing across multiple
servers. Simulations show our method outperforms others in predictive accuracy
and robustness. We further demonstrate its practical value through two
real-world recommendation system applications.

</details>


### [350] [Dependent Randomized Rounding for Budget Constrained Experimental Design](https://arxiv.org/abs/2506.12677)
*Khurram Yamin,Edward Kennedy,Bryan Wilder*

Main category: stat.ML

TL;DR: 提出将依赖随机舍入程序应用于转换分配概率的框架，可在预算约束下提升估计精度。


<details>
  <summary>Details</summary>
Motivation: 资源受限环境下，决策者需要满足严格预算限制并确保精确估计治疗效果的实验设计。

Method: 提出应用依赖随机舍入程序将分配概率转换为二元治疗决策的框架。

Result: 建立了逆倾向加权和一般线性估计量的理论保证，实证研究表明该方法在固定预算约束下能实现高效准确的推断。

Conclusion: 所提框架可在预算约束下通过减少方差提高估计精度，实现有效准确的推断。

Abstract: Policymakers in resource-constrained settings require experimental designs
that satisfy strict budget limits while ensuring precise estimation of
treatment effects. We propose a framework that applies a dependent randomized
rounding procedure to convert assignment probabilities into binary treatment
decisions. Our proposed solution preserves the marginal treatment probabilities
while inducing negative correlations among assignments, leading to improved
estimator precision through variance reduction. We establish theoretical
guarantees for the inverse propensity weighted and general linear estimators,
and demonstrate through empirical studies that our approach yields efficient
and accurate inference under fixed budget constraints.

</details>


### [351] [Single Index Bandits: Generalized Linear Contextual Bandits with Unknown Reward Functions](https://arxiv.org/abs/2506.12751)
*Yue Kang,Mingshuo Liu,Bongsoo Yi,Jing Lyu,Zhi Zhang,Doudou Zhou,Yao Li*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generalized linear bandits have been extensively studied due to their broad
applicability in real-world online decision-making problems. However, these
methods typically assume that the expected reward function is known to the
users, an assumption that is often unrealistic in practice. Misspecification of
this link function can lead to the failure of all existing algorithms. In this
work, we address this critical limitation by introducing a new problem of
generalized linear bandits with unknown reward functions, also known as single
index bandits. We first consider the case where the unknown reward function is
monotonically increasing, and propose two novel and efficient algorithms, STOR
and ESTOR, that achieve decent regrets under standard assumptions. Notably, our
ESTOR can obtain the nearly optimal regret bound $\tilde{O}_T(\sqrt{T})$ in
terms of the time horizon $T$. We then extend our methods to the
high-dimensional sparse setting and show that the same regret rate can be
attained with the sparsity index. Next, we introduce GSTOR, an algorithm that
is agnostic to general reward functions, and establish regret bounds under a
Gaussian design assumption. Finally, we validate the efficiency and
effectiveness of our algorithms through experiments on both synthetic and
real-world datasets.

</details>


### [352] [General and Estimable Learning Bound Unifying Covariate and Concept Shifts](https://arxiv.org/abs/2506.12829)
*Hongbo Chen,Li Charlie Xia*

Main category: stat.ML

TL;DR: 本文弥合机器学习泛化理论与实际应用差距，提出支持无关的偏移定义和统一误差界，开发估算器和算法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习泛化学习界理论局限于狭窄理想化场景且无法从样本估计，需弥合理论与实际应用的差距。

Method: 利用熵最优传输，提出支持无关的协变量和概念偏移定义，推导统一误差界，开发有集中保证的偏移估算器和DataShifts算法。

Result: 得到适用于广泛损失函数、标签空间和随机标签的统一误差界，以及可量化分布偏移和估计误差界的工具。

Conclusion: 提出的新定义、误差界、估算器和算法是分析分布偏移下学习误差的严谨通用工具。

Abstract: Generalization under distribution shift remains a core challenge in modern
machine learning, yet existing learning bound theory is limited to narrow,
idealized settings and is non-estimable from samples. In this paper, we bridge
the gap between theory and practical applications. We first show that existing
bounds become loose and non-estimable because their concept shift definition
breaks when the source and target supports mismatch. Leveraging entropic
optimal transport, we propose new support-agnostic definitions for covariate
and concept shifts, and derive a novel unified error bound that applies to
broad loss functions, label spaces, and stochastic labeling. We further develop
estimators for these shifts with concentration guarantees, and the DataShifts
algorithm, which can quantify distribution shifts and estimate the error bound
in most applications -- a rigorous and general tool for analyzing learning
error under distribution shift.

</details>


### [353] [Fair Bayesian Model-Based Clustering](https://arxiv.org/abs/2506.12839)
*Jihu Lee,Kunwoong Kim,Yongdai Kim*

Main category: stat.ML

TL;DR: 提出公平贝叶斯聚类（FBC）方法解决现有公平聚类方法的局限，实验显示其能推断簇数量、实现效用 - 公平权衡且适用于分类数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于K - means的群体公平聚类方法需提前给定实例间距离和簇数量，为解决该局限开展研究。

Method: 提出公平贝叶斯聚类（FBC），设计只关注公平簇的先验并实现高效MCMC算法。

Result: 在真实数据集实验中，FBC能合理推断簇数量，与现有公平聚类方法相比实现有竞争力的效用 - 公平权衡，在分类数据上表现良好。

Conclusion: FBC有效解决了现有公平聚类方法的局限，有良好性能和适用性。

Abstract: Fair clustering has become a socially significant task with the advancement
of machine learning technologies and the growing demand for trustworthy AI.
Group fairness ensures that the proportions of each sensitive group are similar
in all clusters. Most existing group-fair clustering methods are based on the
$K$-means clustering and thus require the distance between instances and the
number of clusters to be given in advance. To resolve this limitation, we
propose a fair Bayesian model-based clustering called Fair Bayesian Clustering
(FBC). We develop a specially designed prior which puts its mass only on fair
clusters, and implement an efficient MCMC algorithm. Advantages of FBC are that
it can infer the number of clusters and can be applied to any data type as long
as the likelihood is defined (e.g., categorical data). Experiments on
real-world datasets show that FBC (i) reasonably infers the number of clusters,
(ii) achieves a competitive utility-fairness trade-off compared to existing
fair clustering methods, and (iii) performs well on categorical data.

</details>


### [354] [Variational Learning Finds Flatter Solutions at the Edge of Stability](https://arxiv.org/abs/2506.12903)
*Avrajit Ghosh,Bai Cong,Rio Yokota,Saiprasad Ravishankar,Rongrong Wang,Molei Tao,Mohammad Emtiyaz Khan,Thomas Möllenhoff*

Main category: stat.ML

TL;DR: 本文通过EoS框架分析变分学习（VL）的隐式正则化，表明VL能找到更平坦的解，理论结果与实验相符。


<details>
  <summary>Details</summary>
Motivation: 变分学习在训练深度神经网络中很受欢迎，但缺乏揭示其隐式正则化的工具，因此分析VL的隐式正则化。

Method: 利用EoS框架，先推导二次问题的结果，再扩展到深度神经网络，通过控制后验协方差和后验蒙特卡罗样本数量。

Result: 理论上表明VL能找到比梯度下降更平坦的解，在多种大型网络上的实验验证了理论结果。

Conclusion: 本文首次分析了VL中的EoS动态，证明了VL隐式正则化的优势。

Abstract: Variational Learning (VL) has recently gained popularity for training deep
neural networks and is competitive to standard learning methods. Part of its
empirical success can be explained by theories such as PAC-Bayes bounds,
minimum description length and marginal likelihood, but there are few tools to
unravel the implicit regularization in play. Here, we analyze the implicit
regularization of VL through the Edge of Stability (EoS) framework. EoS has
previously been used to show that gradient descent can find flat solutions and
we extend this result to VL to show that it can find even flatter solutions.
This is obtained by controlling the posterior covariance and the number of
Monte Carlo samples from the posterior. These results are derived in a similar
fashion as the standard EoS literature for deep learning, by first deriving a
result for a quadratic problem and then extending it to deep neural networks.
We empirically validate these findings on a wide variety of large networks,
such as ResNet and ViT, to find that the theoretical results closely match the
empirical ones. Ours is the first work to analyze the EoS dynamics in VL.

</details>


### [355] [Random Matrix Theory for Deep Learning: Beyond Eigenvalues of Linear Models](https://arxiv.org/abs/2506.13139)
*Zhenyu Liao,Michael W. Mahoney*

Main category: stat.ML

TL;DR: 论文将传统随机矩阵理论扩展到非线性机器学习模型，引入高维等价概念，刻画了不同模型的训练和泛化性能，提供高维深度学习理论理解的统一视角。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习和深度神经网络处理高维数据，传统低维直觉失效，比例体制下非线性模型带来新挑战，需新理论分析。

Method: 引入高维等价概念，统一和推广确定性等价与线性等价，系统解决高维、非线性和特征谱泛函分析的技术挑战。

Result: 精确刻画了线性模型、非线性浅层网络和深度网络的训练和泛化性能，捕捉到丰富现象。

Conclusion: 该研究为高维深度学习的理论理解提供了统一视角。

Abstract: Modern Machine Learning (ML) and Deep Neural Networks (DNNs) often operate on
high-dimensional data and rely on overparameterized models, where classical
low-dimensional intuitions break down. In particular, the proportional regime
where the data dimension, sample size, and number of model parameters are all
large and comparable, gives rise to novel and sometimes counterintuitive
behaviors. This paper extends traditional Random Matrix Theory (RMT) beyond
eigenvalue-based analysis of linear models to address the challenges posed by
nonlinear ML models such as DNNs in this regime. We introduce the concept of
High-dimensional Equivalent, which unifies and generalizes both Deterministic
Equivalent and Linear Equivalent, to systematically address three technical
challenges: high dimensionality, nonlinearity, and the need to analyze generic
eigenspectral functionals. Leveraging this framework, we provide precise
characterizations of the training and generalization performance of linear
models, nonlinear shallow networks, and deep networks. Our results capture rich
phenomena, including scaling laws, double descent, and nonlinear learning
dynamics, offering a unified perspective on the theoretical understanding of
deep learning in high dimensions.

</details>


### [356] [Experimental Design for Semiparametric Bandits](https://arxiv.org/abs/2506.13390)
*Seok-Jin Kim,Gi-Soo Kim,Min-hwan Oh*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study finite-armed semiparametric bandits, where each arm's reward
combines a linear component with an unknown, potentially adversarial shift.
This model strictly generalizes classical linear bandits and reflects
complexities common in practice. We propose the first experimental-design
approach that simultaneously offers a sharp regret bound, a PAC bound, and a
best-arm identification guarantee. Our method attains the minimax regret
$\tilde{O}(\sqrt{dT})$, matching the known lower bound for finite-armed linear
bandits, and further achieves logarithmic regret under a positive suboptimality
gap condition. These guarantees follow from our refined non-asymptotic analysis
of orthogonalized regression that attains the optimal $\sqrt{d}$ rate, paving
the way for robust and efficient learning across a broad class of
semiparametric bandit problems.

</details>


### [357] [Variational Inference with Mixtures of Isotropic Gaussians](https://arxiv.org/abs/2506.13613)
*Marguerite Petit-Talamon,Marc Lambert,Anna Korba*

Main category: stat.ML

TL;DR: 本文聚焦各向同性高斯混合模型与均匀权重，开发变分框架并提供高效算法，算法表现通过数值实验展示。


<details>
  <summary>Details</summary>
Motivation: 在贝叶斯推理中寻找对后验分布更好的近似，同时兼顾多模态后验近似准确性和内存与计算效率。

Method: 开发变分框架，对混合分量位置进行梯度下降，对其方差参数进行熵镜像或Bures下降。

Result: 开发出适用于各向同性高斯混合模型的变分框架和高效算法。

Conclusion: 各向同性高斯混合模型在多模态贝叶斯后验近似中能平衡准确性与内存、计算效率。

Abstract: Variational inference (VI) is a popular approach in Bayesian inference, that
looks for the best approximation of the posterior distribution within a
parametric family, minimizing a loss that is typically the (reverse)
Kullback-Leibler (KL) divergence. In this paper, we focus on the following
parametric family: mixtures of isotropic Gaussians (i.e., with diagonal
covariance matrices proportional to the identity) and uniform weights. We
develop a variational framework and provide efficient algorithms suited for
this family. In contrast with mixtures of Gaussian with generic covariance
matrices, this choice presents a balance between accurate approximations of
multimodal Bayesian posteriors, while being memory and computationally
efficient. Our algorithms implement gradient descent on the location of the
mixture components (the modes of the Gaussians), and either (an entropic)
Mirror or Bures descent on their variance parameters. We illustrate the
performance of our algorithms on numerical experiments.

</details>


### [358] [Exploiting the Exact Denoising Posterior Score in Training-Free Guidance of Diffusion Models](https://arxiv.org/abs/2506.13614)
*Gregory Bellchambers*

Main category: stat.ML

TL;DR: 本文提出纯去噪任务后验得分的新表达式，分析DPS得分误差并计算步长，该步长可用于相关逆问题，方法简单且有竞争力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型成功促使人们关注无训练指导的去噪过程条件采样，DPS方法尝试直接近似难以处理的后验得分函数，本文旨在解决这一问题。

Method: 提出纯去噪任务精确后验得分的新表达式，分析DPS得分的时间相关误差并动态计算步长以最小化误差。

Result: 步长可转移到相关逆问题，方法与现有技术有竞争力，且采样所需时间步更少。

Conclusion: 该方法简单有效，在去噪及相关逆问题中有良好表现。

Abstract: The success of diffusion models has driven interest in performing conditional
sampling via training-free guidance of the denoising process to solve image
restoration and other inverse problems. A popular class of methods, based on
Diffusion Posterior Sampling (DPS), attempts to approximate the intractable
posterior score function directly. In this work, we present a novel expression
for the exact posterior score for purely denoising tasks that is tractable in
terms of the unconditional score function. We leverage this result to analyze
the time-dependent error in the DPS score for denoising tasks and compute step
sizes on the fly to minimize the error at each time step. We demonstrate that
these step sizes are transferable to related inverse problems such as
colorization, random inpainting, and super resolution. Despite its simplicity,
this approach is competitive with state-of-the-art techniques and enables
sampling with fewer time steps than DPS.

</details>


### [359] [Adversarial Disentanglement by Backpropagation with Physics-Informed Variational Autoencoder](https://arxiv.org/abs/2506.13658)
*Ioannis Christoforos Koune,Alice Cicirello*

Main category: stat.ML

TL;DR: 提出物理信息变分自编码器架构，结合物理模型和数据驱动模型优势，在合成案例中验证可行性。


<details>
  <summary>Details</summary>
Motivation: 物理系统部分知识下推理和预测有挑战，物理模型难考虑混杂因素，数据驱动模型泛化和重建精度不佳。

Method: 提出物理信息变分自编码器架构，划分潜在空间，编码器与解码器结合并以对抗训练目标约束。

Result: 模型能分离输入信号特征，区分已知物理和混杂影响。

Conclusion: 所提方法在工程结构相关合成案例中可行。

Abstract: Inference and prediction under partial knowledge of a physical system is
challenging, particularly when multiple confounding sources influence the
measured response. Explicitly accounting for these influences in physics-based
models is often infeasible due to epistemic uncertainty, cost, or time
constraints, resulting in models that fail to accurately describe the behavior
of the system. On the other hand, data-driven machine learning models such as
variational autoencoders are not guaranteed to identify a parsimonious
representation. As a result, they can suffer from poor generalization
performance and reconstruction accuracy in the regime of limited and noisy
data. We propose a physics-informed variational autoencoder architecture that
combines the interpretability of physics-based models with the flexibility of
data-driven models. To promote disentanglement of the known physics and
confounding influences, the latent space is partitioned into physically
meaningful variables that parametrize a physics-based model, and data-driven
variables that capture variability in the domain and class of the physical
system. The encoder is coupled with a decoder that integrates physics-based and
data-driven components, and constrained by an adversarial training objective
that prevents the data-driven components from overriding the known physics,
ensuring that the physics-grounded latent variables remain interpretable. We
demonstrate that the model is able to disentangle features of the input signal
and separate the known physics from confounding influences using supervision in
the form of class and domain observables. The model is evaluated on a series of
synthetic case studies relevant to engineering structures, demonstrating the
feasibility of the proposed approach.

</details>


### [360] [Understanding Learning Invariance in Deep Linear Networks](https://arxiv.org/abs/2506.13714)
*Hao Duan,Guido Montúfar*

Main category: stat.ML

TL;DR: 本文对实现不变性的三种方法（数据增强、正则化和硬编码）进行理论比较，聚焦深度线性网络的均方误差回归，揭示不同方法优化问题的临界点及正则化路径特性。


<details>
  <summary>Details</summary>
Motivation: 现有关于数据驱动方法（如正则化和数据增强）与显式不变模型的理论见解不足，需进行理论比较。

Method: 聚焦深度线性网络的均方误差回归，分析三种实现不变性方法优化问题的临界点。

Result: 硬编码和数据增强优化问题的临界点相同，仅含鞍点和全局最优；正则化引入额外临界点，但除全局最优外仍为鞍点，且正则化路径连续并收敛到硬编码解。

Conclusion: 完成了对三种实现不变性方法的理论比较，明确了它们在优化问题上的差异和联系。

Abstract: Equivariant and invariant machine learning models exploit symmetries and
structural patterns in data to improve sample efficiency. While empirical
studies suggest that data-driven methods such as regularization and data
augmentation can perform comparably to explicitly invariant models, theoretical
insights remain scarce. In this paper, we provide a theoretical comparison of
three approaches for achieving invariance: data augmentation, regularization,
and hard-wiring. We focus on mean squared error regression with deep linear
networks, which parametrize rank-bounded linear maps and can be hard-wired to
be invariant to specific group actions. We show that the critical points of the
optimization problems for hard-wiring and data augmentation are identical,
consisting solely of saddles and the global optimum. By contrast,
regularization introduces additional critical points, though they remain
saddles except for the global optimum. Moreover, we demonstrate that the
regularization path is continuous and converges to the hard-wired solution.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [361] [Assessing the Quality of Binomial Samplers: A Statistical Distance Framework](https://arxiv.org/abs/2506.12061)
*Uddalok Sarkar,Sourav Chakraborty,Kuldeep S. Meel*

Main category: stat.CO

TL;DR: 本文提出用统计距离分析二项分布采样器质量，推导标准实现的统计距离边界，改进APSEst，强调采样器设计需全面系统误差分析。


<details>
  <summary>Details</summary>
Motivation: 常见分布精确采样计算困难，标准库启发式方法有误差，影响下游应用。

Method: 提出统计距离作为分析二项分布采样器质量的指标，推导标准实现的统计距离边界，改进APSEst，提出接口扩展。

Result: 增强了APSEst的可靠性和误差保证，可让用户控制和监控统计距离。

Conclusion: 采样器设计需全面系统误差分析，为其他常见分布严格分析奠定基础。

Abstract: Randomized algorithms depend on accurate sampling from probability
distributions, as their correctness and performance hinge on the quality of the
generated samples. However, even for common distributions like Binomial, exact
sampling is computationally challenging, leading standard library
implementations to rely on heuristics. These heuristics, while efficient,
suffer from approximation and system representation errors, causing deviations
from the ideal distribution. Although seemingly minor, such deviations can
accumulate in downstream applications requiring large-scale sampling,
potentially undermining algorithmic guarantees. In this work, we propose
statistical distance as a robust metric for analyzing the quality of Binomial
samplers, quantifying deviations from the ideal distribution. We derive
rigorous bounds on the statistical distance for standard implementations and
demonstrate the practical utility of our framework by enhancing APSEst, a DNF
model counter, with improved reliability and error guarantees. To support
practical adoption, we propose an interface extension that allows users to
control and monitor statistical distance via explicit input/output parameters.
Our findings emphasize the critical need for thorough and systematic error
analysis in sampler design. As the first work to focus exclusively on Binomial
samplers, our approach lays the groundwork for extending rigorous analysis to
other common distributions, opening avenues for more robust and reliable
randomized algorithms.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [362] [Intelligent Automation for FDI Facilitation: Optimizing Tariff Exemption Processes with OCR And Large Language Models](https://arxiv.org/abs/2506.12093)
*Muhammad Sukri Bin Ramli*

Main category: cs.CY

TL;DR: 本文提出结合OCR和大语言模型技术的概念框架以优化关税豁免行政流程，提升外资投资便利和国家行政效率。


<details>
  <summary>Details</summary>
Motivation: 关税豁免行政流程有优化空间，需提升外资投资便利和国家行政效率。

Method: 先利用OCR对各类申请文件和监管文本进行智能数字化和数据提取，再用大语言模型自动验证HS关税代码。

Result: AI驱动的方法可提高评估速度和精度，减少不匹配情况，提升外资投资流程效率。

Conclusion: 该方法能提升国家行政能力，减轻行政负担，改善营商环境，增强国家对高价值制造业FDI的吸引力。

Abstract: Tariff exemptions are fundamental to attracting Foreign Direct Investment
(FDI) into the manufacturing sector, though the associated administrative
processes present areas for optimization for both investing entities and the
national tax authority. This paper proposes a conceptual framework to empower
tax administration by leveraging a synergistic integration of Optical Character
Recognition (OCR) and Large Language Model (LLM) technologies. The proposed
system is designed to first utilize OCR for intelligent digitization, precisely
extracting data from diverse application documents and key regulatory texts
such as tariff orders. Subsequently, the LLM would enhance the capabilities of
administrative officers by automating the critical and time-intensive task of
verifying submitted HS Tariff Codes for machinery, equipment, and raw materials
against official exemption lists. By enhancing the speed and precision of these
initial assessments, this AI-driven approach systematically reduces potential
for non-alignment and non-optimized exemption utilization, thereby streamlining
the investment journey for FDI companies. For the national administration, the
benefits include a significant boost in operational capacity, reduced
administrative load, and a strengthened control environment, ultimately
improving the ease of doing business and solidifying the nation's appeal as a
premier destination for high-value manufacturing FDI.

</details>


### [363] [Accessibility Barriers in Multi-Terabyte Public Datasets: The Gap Between Promise and Practice](https://arxiv.org/abs/2506.13256)
*Marc Bara*

Main category: cs.CY

TL;DR: 研究指出号称“免费开放”的多TB数据集存在实际可访问性问题，受处理复杂度和隐藏成本等限制，形成两层级系统，多数分析需高额投入，限制了普通使用者。


<details>
  <summary>Details</summary>
Motivation: 探究号称“免费开放”的多TB数据集在实际可访问性方面存在的问题。

Method: 对网络爬虫、卫星图像、科学数据和协作项目等不同类型数据集的可访问性挑战进行研究分析。

Result: 发现存在两层级系统，号称“公开可访问”的数据集有高额成本，分析需至少1000美元投入，复杂处理管道需10000 - 100000美元以上的基础设施成本。

Conclusion: 尽管数据集宣称“开放”，但基础设施要求如分布式计算知识、领域专业知识和大量预算等，实际限制了其可访问性，只有有机构支持或大量资源的人才能实际使用。

Abstract: The promise of "free and open" multi-terabyte datasets often collides with
harsh realities. While these datasets may be technically accessible, practical
barriers -- from processing complexity to hidden costs -- create a system that
primarily serves well-funded institutions. This study examines accessibility
challenges across web crawls, satellite imagery, scientific data, and
collaborative projects, revealing a consistent two-tier system where
theoretical openness masks practical exclusivity. Our analysis demonstrates
that datasets marketed as "publicly accessible" typically require minimum
investments of \$1,000+ for meaningful analysis, with complex processing
pipelines demanding \$10,000-100,000+ in infrastructure costs. The
infrastructure requirements -- distributed computing knowledge, domain
expertise, and substantial budgets -- effectively gatekeep these datasets
despite their "open" status, limiting practical accessibility to those with
institutional support or substantial resources.

</details>


### [364] [Military AI Cyber Agents (MAICAs) Constitute a Global Threat to Critical Infrastructure](https://arxiv.org/abs/2506.12094)
*Timothy Dubber,Seth Lazar*

Main category: cs.CY

TL;DR: 论文指出自主AI网络武器MAICAs会带来灾难性风险，阐述技术可行性、风险原因并提出应对措施。


<details>
  <summary>Details</summary>
Motivation: 研究自主AI网络武器MAICAs带来的灾难性风险。

Method: 阐述MAICAs的技术可行性，分析地缘政治和网络空间性质与MAICAs风险的关联。

Result: 明确MAICAs是可信的灾难性风险途径。

Conclusion: 应采取政治、防御性AI和模拟弹性措施来应对MAICAs威胁。

Abstract: This paper argues that autonomous AI cyber-weapons - Military-AI Cyber Agents
(MAICAs) - create a credible pathway to catastrophic risk. It sets out the
technical feasibility of MAICAs, explains why geopolitics and the nature of
cyberspace make MAICAs a catastrophic risk, and proposes political,
defensive-AI and analogue-resilience measures to blunt the threat.

</details>


### [365] ["I Hadn't Thought About That": Creators of Human-like AI Weigh in on Ethics And Neurodivergence](https://arxiv.org/abs/2506.12098)
*Naba Rizvi,Taggert Smith,Tanvi Vidyala,Mya Bolds,Harper Strickland,Andrew Begel,Rua Williams,Imani Munyaka*

Main category: cs.CY

TL;DR: 探讨类人AI代理的伦理问题，研究技术开发者对神经多样性的理解与接纳，指出伦理考量的差距并为自闭症群体融入社会提建议。


<details>
  <summary>Details</summary>
Motivation: 类人AI代理引发多种伦理问题，如对人性的定义影响被科研非人化的群体，以及模型偏差和可及性问题。

Method: 调查构建和设计这些技术的人员的经验。

Result: 开发者大多忽视工作中人性的实现和解释对交流规范的影响，存在伦理考量差距，部分人有神经规范假设并在工作中复制。

Conclusion: 强调这对自闭症群体融入社会的影响，为更符合伦理的研究方向提供系统性改变建议。

Abstract: Human-like AI agents such as robots and chatbots are becoming increasingly
popular, but they present a variety of ethical concerns. The first concern is
in how we define humanness, and how our definition impacts communities
historically dehumanized by scientific research. Autistic people in particular
have been dehumanized by being compared to robots, making it even more
important to ensure this marginalization is not reproduced by AI that may
promote neuronormative social behaviors. Second, the ubiquitous use of these
agents raises concerns surrounding model biases and accessibility. In our work,
we investigate the experiences of the people who build and design these
technologies to gain insights into their understanding and acceptance of
neurodivergence, and the challenges in making their work more accessible to
users with diverse needs. Even though neurodivergent individuals are often
marginalized for their unique communication styles, nearly all participants
overlooked the conclusions their end-users and other AI system makers may draw
about communication norms from the implementation and interpretation of
humanness applied in participants' work. This highlights a major gap in their
broader ethical considerations, compounded by some participants' neuronormative
assumptions about the behaviors and traits that distinguish "humans" from
"bots" and the replication of these assumptions in their work. We examine the
impact this may have on autism inclusion in society and provide recommendations
for additional systemic changes towards more ethical research directions.

</details>


### [366] [SocialCredit+](https://arxiv.org/abs/2506.12099)
*Thabassum Aslam,Anees Aslam*

Main category: cs.CY

TL;DR: 本文介绍AI驱动的信用评分系统SocialCredit+，利用社交媒体数据增强传统信用评估，阐述其架构、模型等，用合成场景说明社会信号转化为信用评分因素，强调概念新颖性、合规机制和实际影响。


<details>
  <summary>Details</summary>
Motivation: 利用公开的社交媒体数据来增强传统信用评估，满足不同相关方（AI研究者、金融科技从业者等）对新信用评估系统的需求。

Method: 使用对话式银行助手获取用户同意和公共资料，通过多模态特征提取器分析社交媒体内容生成行为画像，设置符合伊斯兰教法的合规层，采用检索增强生成模块生成决策解释，描述端到端架构、数据流程、使用的模型和系统基础设施。

Result: 通过合成场景说明了社会信号如何转化为信用评分因素。

Conclusion: SocialCredit+具有概念新颖性，有相应合规机制且具有实际影响，能为相关领域人员提供参考。

Abstract: SocialCredit+ is AI powered credit scoring system that leverages publicly
available social media data to augment traditional credit evaluation. It uses a
conversational banking assistant to gather user consent and fetch public
profiles. Multimodal feature extractors analyze posts, bios, images, and friend
networks to generate a rich behavioral profile. A specialized Sharia-compliance
layer flags any non-halal indicators and prohibited financial behavior based on
Islamic ethics. The platform employs a retrieval-augmented generation module:
an LLM accesses a domain specific knowledge base to generate clear, text-based
explanations for each decision. We describe the end-to-end architecture and
data flow, the models used, and system infrastructure. Synthetic scenarios
illustrate how social signals translate into credit-score factors. This paper
emphasizes conceptual novelty, compliance mechanisms, and practical impact,
targeting AI researchers, fintech practitioners, ethical banking jurists, and
investors.

</details>


### [367] [Information Suppression in Large Language Models: Auditing, Quantifying, and Characterizing Censorship in DeepSeek](https://arxiv.org/abs/2506.12349)
*Peiran Qiu,Siyi Zhou,Emilio Ferrara*

Main category: cs.CY

TL;DR: 研究中国开源大模型DeepSeek的信息抑制机制，发现语义层面信息抑制现象，强调对广泛采用的AI模型进行系统审计的必要性。


<details>
  <summary>Details</summary>
Motivation: 研究中国开源大模型DeepSeek的信息抑制机制。

Method: 提出审计框架，通过比较最终输出和中间思维链推理，分析模型对646个政治敏感提示的响应。

Result: 发现DeepSeek存在语义层面的信息抑制，敏感内容在内部推理中出现但在最终输出中被省略或改写，会抑制特定内容并偶尔放大与国家宣传一致的语言。

Conclusion: 强调需要对广泛采用的AI模型的对齐、内容审核、信息抑制和审查实践进行系统审计，以确保透明度、问责制和公平获取无偏见信息。

Abstract: This study examines information suppression mechanisms in DeepSeek, an
open-source large language model (LLM) developed in China. We propose an
auditing framework and use it to analyze the model's responses to 646
politically sensitive prompts by comparing its final output with intermediate
chain-of-thought (CoT) reasoning. Our audit unveils evidence of semantic-level
information suppression in DeepSeek: sensitive content often appears within the
model's internal reasoning but is omitted or rephrased in the final output.
Specifically, DeepSeek suppresses references to transparency, government
accountability, and civic mobilization, while occasionally amplifying language
aligned with state propaganda. This study underscores the need for systematic
auditing of alignment, content moderation, information suppression, and
censorship practices implemented into widely-adopted AI models, to ensure
transparency, accountability, and equitable access to unbiased information
obtained by means of these systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [368] [A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions](https://arxiv.org/abs/2506.12202)
*Stephen Mell,Botong Zhang,David Mell,Shuo Li,Ramya Ramalingam,Nathan Yu,Steve Zdancewic,Osbert Bastani*

Main category: cs.PL

TL;DR: 提出新编程语言Quasar用于大语言模型代码动作，在ViperGPT上评估显示能减少执行时间、提升安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型用Python做代码动作存在性能、安全和可靠性支持不足的问题。

Method: 提出Quasar语言，有自动并行化、不确定性量化和安全特性，允许大语言模型用Python子集编写代码并自动转换为Quasar。

Result: 在ViperGPT上应用于GQA数据集，使用Quasar动作代替Python动作保留了强性能，减少执行时间达42%，减少用户审批交互达52%，应用共形预测提升可靠性。

Conclusion: Quasar语言可用于大语言模型代码动作，在性能、安全和可靠性上有提升。

Abstract: Modern large language models (LLMs) are often deployed as agents, calling
external tools adaptively to solve tasks. Rather than directly calling tools,
it can be more effective for LLMs to write code to perform the tool calls,
enabling them to automatically generate complex control flow such as
conditionals and loops. Such code actions are typically provided as Python
code, since LLMs are quite proficient at it; however, Python may not be the
ideal language due to limited built-in support for performance, security, and
reliability. We propose a novel programming language for code actions, called
Quasar, which has several benefits: (1) automated parallelization to improve
performance, (2) uncertainty quantification to improve reliability and mitigate
hallucinations, and (3) security features enabling the user to validate
actions. LLMs can write code in a subset of Python, which is automatically
transpiled to Quasar. We evaluate our approach on the ViperGPT visual question
answering agent, applied to the GQA dataset, demonstrating that LLMs with
Quasar actions instead of Python actions retain strong performance, while
reducing execution time when possible by 42%, improving security by reducing
user approval interactions when possible by 52%, and improving reliability by
applying conformal prediction to achieve a desired target coverage level.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [369] [CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models](https://arxiv.org/abs/2506.12059)
*Jiajun He,Naoki Sawada,Koichi Miyazaki,Tomoki Toda*

Main category: eess.AS

TL;DR: 提出统一框架结合多说话人重叠语音识别和上下文偏置，用优化微调策略和两阶段过滤算法，实验表明性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法分别处理多说话人自动语音识别和上下文偏置，在复杂场景下性能受限。

Method: 提出统一框架，集成预训练语音编码器和大语言模型，采用优化微调策略，引入两阶段过滤算法识别并融入罕见词。

Result: 在LibriMix上WER为7.9%，在AMI SDM上WER为32.9%（偏置大小为1000时），优于传统上下文偏置方法。

Conclusion: 所提方法在复杂语音场景中有效。

Abstract: In real-world applications, automatic speech recognition (ASR) systems must
handle overlapping speech from multiple speakers and recognize rare words like
technical terms. Traditional methods address multi-talker ASR and contextual
biasing separately, limiting performance in complex scenarios. We propose a
unified framework that combines multi-talker overlapping speech recognition and
contextual biasing into a single task. Our ASR method integrates pretrained
speech encoders and large language models (LLMs), using optimized finetuning
strategies. We also introduce a two-stage filtering algorithm to efficiently
identify relevant rare words from large biasing lists and incorporate them into
the LLM's prompt input, enhancing rare word recognition. Experiments show that
our approach outperforms traditional contextual biasing methods, achieving a
WER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000,
demonstrating its effectiveness in complex speech scenarios.

</details>


### [370] [Evaluating Logit-Based GOP Scores for Mispronunciation Detection](https://arxiv.org/abs/2506.12067)
*Aditya Kamlesh Parikh,Cristian Tejedor-Garcia,Catia Cucchiarini,Helmer Strik*

Main category: eess.AS

TL;DR: 研究对比基于对数几率和概率的GOP分数用于发音错误检测，发现对数几率方法在分类上更优，结合不同GOP分数可平衡特征，混合GOP方法可改善发音评估。


<details>
  <summary>Details</summary>
Motivation: 传统基于概率的GOP分数存在过度自信和音素分离不佳的问题，限制了发音评估效果，因此对比基于对数几率和概率的GOP分数用于发音错误检测。

Method: 在荷兰语和普通话使用者的两个二语英语语音数据集上进行实验，评估分类性能以及与人类评分的相关性。

Result: 基于对数几率的方法在分类上优于基于概率的GOP，但效果依赖数据集特征；最大对数几率GOP与人类感知最相符，结合不同GOP分数可平衡概率和对数几率特征。

Conclusion: 结合不确定性建模和音素特定加权的混合GOP方法可改善发音评估。

Abstract: Pronunciation assessment relies on goodness of pronunciation (GOP) scores,
traditionally derived from softmax-based posterior probabilities. However,
posterior probabilities may suffer from overconfidence and poor phoneme
separation, limiting their effectiveness. This study compares logit-based GOP
scores with probability-based GOP scores for mispronunciation detection. We
conducted our experiment on two L2 English speech datasets spoken by Dutch and
Mandarin speakers, assessing classification performance and correlation with
human ratings. Logit-based methods outperform probability-based GOP in
classification, but their effectiveness depends on dataset characteristics. The
maximum logit GOP shows the strongest alignment with human perception, while a
combination of different GOP scores balances probability and logit features.
The findings suggest that hybrid GOP methods incorporating uncertainty modeling
and phoneme-specific weighting improve pronunciation assessment.

</details>


### [371] [Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis](https://arxiv.org/abs/2506.12073)
*Zongli Ye,Jiachen Lian,Xuanru Zhou,Jinming Zhang,Haodong Li,Shuhe Li,Chenxu Guo,Anaisha Das,Peter Park,Zoe Ezzes,Jet Vonk,Brittany Morin,Rian Bogley,Lisa Wauters,Zachary Miller,Maria Gorno-Tempini,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: 提出Neural LCS用于不流利文本-文本和语音-文本对齐，在模拟和真实数据集上表现优于现有模型，有提升语音障碍诊断系统潜力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在模拟音素相似性上表现不佳，影响神经退行性语音障碍自动化诊断中不流利语音与预期文本的准确对齐。

Method: 提出Neural LCS方法，利用强大的音素级建模解决部分对齐和上下文感知相似度映射等关键挑战，并在大规模模拟数据集和真实PPA数据上进行评估。

Result: Neural LCS在对齐准确性和不流利语音分割方面显著优于现有模型。

Conclusion: Neural LCS有潜力提升语音障碍诊断和分析的自动化系统，为不流利语音对齐提供更准确且基于语言学的解决方案。

Abstract: Accurate alignment of dysfluent speech with intended text is crucial for
automating the diagnosis of neurodegenerative speech disorders. Traditional
methods often fail to model phoneme similarities effectively, limiting their
performance. In this work, we propose Neural LCS, a novel approach for
dysfluent text-text and speech-text alignment. Neural LCS addresses key
challenges, including partial alignment and context-aware similarity mapping,
by leveraging robust phoneme-level modeling. We evaluate our method on a
large-scale simulated dataset, generated using advanced data simulation
techniques, and real PPA data. Neural LCS significantly outperforms
state-of-the-art models in both alignment accuracy and dysfluent speech
segmentation. Our results demonstrate the potential of Neural LCS to enhance
automated systems for diagnosing and analyzing speech disorders, offering a
more accurate and linguistically grounded solution for dysfluent speech
alignment.

</details>


### [372] [CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction Following](https://arxiv.org/abs/2506.12285)
*Yinghao Ma,Siyou Li,Juntao Yu,Emmanouil Benetos,Akira Maezawa*

Main category: eess.AS

TL;DR: 本文提出CMI - Bench基准评估音频 - 文本大语言模型处理音乐信息检索任务的能力，实验揭示了大模型与监督模型差距及自身偏见。


<details>
  <summary>Details</summary>
Motivation: 现有音乐理解和生成基准范围有限，无法反映现实音乐分析复杂性。

Method: 将传统MIR注释重新解释为指令跟随格式，构建CMI - Bench基准，采用标准化评估指标，提供支持开源模型的评估工具包。

Result: 实验显示大语言模型与监督模型存在显著性能差距，且有文化、时间和性别偏见。

Conclusion: CMI - Bench为评估音乐指令跟随能力奠定统一基础，推动音乐感知大语言模型发展。

Abstract: Recent advances in audio-text large language models (LLMs) have opened new
possibilities for music understanding and generation. However, existing
benchmarks are limited in scope, often relying on simplified tasks or
multi-choice evaluations that fail to reflect the complexity of real-world
music analysis. We reinterpret a broad range of traditional MIR annotations as
instruction-following formats and introduce CMI-Bench, a comprehensive music
instruction following benchmark designed to evaluate audio-text LLMs on a
diverse set of music information retrieval (MIR) tasks. These include genre
classification, emotion regression, emotion tagging, instrument classification,
pitch estimation, key detection, lyrics transcription, melody extraction, vocal
technique recognition, instrument performance technique detection, music
tagging, music captioning, and (down)beat tracking: reflecting core challenges
in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized
evaluation metrics consistent with previous state-of-the-art MIR models,
ensuring direct comparability with supervised approaches. We provide an
evaluation toolkit supporting all open-source audio-textual LLMs, including
LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant
performance gaps between LLMs and supervised models, along with their culture,
chronological and gender bias, highlighting the potential and limitations of
current models in addressing MIR tasks. CMI-Bench establishes a unified
foundation for evaluating music instruction following, driving progress in
music-aware LLMs.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [373] [eLog analysis for accelerators: status and future outlook](https://arxiv.org/abs/2506.12949)
*Antonin Sulc,Thorsten Hellert,Aaron Reed,Adam Carpenter,Alex Bien,Chris Tennant,Claudio Bisegni,Daniel Lersch,Daniel Ratner,David Lawrence,Diana McSpadden,Hayden Hoschouer,Jason St. John,Thomas Britton*

Main category: hep-ex

TL;DR: 本文展示了费米实验室、杰斐逊实验室等加速器设施中利用现代AI驱动信息检索能力的电子日志（eLog）系统，评估相关工具和方法，提出eLog分析解决方案并给出框架。


<details>
  <summary>Details</summary>
Motivation: 提升加速器设施的信息可访问性和知识管理，以实现更高效的运营。

Method: 评估当代信息检索工具和方法，采用检索增强生成（RAGs），进行实际应用来解决eLog分析的挑战。

Result: 展示了eLog系统应用和局限性，提出了用于增强加速器设施运营的框架。

Conclusion: 通过改善信息可访问性和知识管理，有望使加速器设施运营更高效。

Abstract: This work demonstrates electronic logbook (eLog) systems leveraging modern
AI-driven information retrieval capabilities at the accelerator facilities of
Fermilab, Jefferson Lab, Lawrence Berkeley National Laboratory (LBNL), SLAC
National Accelerator Laboratory. We evaluate contemporary tools and
methodologies for information retrieval with Retrieval Augmented Generation
(RAGs), focusing on operational insights and integration with existing
accelerator control systems.
  The study addresses challenges and proposes solutions for state-of-the-art
eLog analysis through practical implementations, demonstrating applications and
limitations. We present a framework for enhancing accelerator facility
operations through improved information accessibility and knowledge management,
which could potentially lead to more efficient operations.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [374] [Uncovering Social Network Activity Using Joint User and Topic Interaction](https://arxiv.org/abs/2506.12842)
*Gaspard Abel,Argyris Kalogeratos,Jean-Pierre Nadal,Julien Randon-Furling*

Main category: cs.SI

TL;DR: 提出Mixture of Interacting Cascades (MIC)模型来建模信息级联和用户活动相互作用，实验显示其性能优于现有方法，还能提供可视化。


<details>
  <summary>Details</summary>
Motivation: 在线社交平台中信息级联影响意见形成，且多信息传播相互关联，需新模型刻画级联和用户间相互作用。

Method: 引入Mixture of Interacting Cascades (MIC)模型，利用多维Hawkes过程和时间点过程的混合构建耦合的用户/级联点过程模型。

Result: 在合成和真实数据实验中，MIC在建模信息级联传播方面性能优于现有方法。

Conclusion: MIC模型能有效建模信息级联和用户活动的相互作用，且可通过学习参数对真实社交网络活动数据进行有洞察力的双层可视化。

Abstract: The emergence of online social platforms, such as social networks and social
media, has drastically affected the way people apprehend the information flows
to which they are exposed. In such platforms, various information cascades
spreading among users is the main force creating complex dynamics of opinion
formation, each user being characterized by their own behavior adoption
mechanism. Moreover, the spread of multiple pieces of information or beliefs in
a networked population is rarely uncorrelated. In this paper, we introduce the
Mixture of Interacting Cascades (MIC), a model of marked multidimensional
Hawkes processes with the capacity to model jointly non-trivial interaction
between cascades and users. We emphasize on the interplay between information
cascades and user activity, and use a mixture of temporal point processes to
build a coupled user/cascade point process model. Experiments on synthetic and
real data highlight the benefits of this approach and demonstrate that MIC
achieves superior performance to existing methods in modeling the spread of
information cascades. Finally, we demonstrate how MIC can provide, through its
learned parameters, insightful bi-layered visualizations of real social network
activity data.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [375] [EconGym: A Scalable AI Testbed with Diverse Economic Tasks](https://arxiv.org/abs/2506.12110)
*Qirui Mi,Qipeng Yang,Zijun Fan,Wentian Fan,Heyang Ma,Chengdong Ma,Siyu Xia,Bo An,Jun Wang,Haifeng Zhang*

Main category: econ.GN

TL;DR: 介绍了可扩展和模块化的经济测试平台EconGym，能连接经济任务与AI算法，支持多样跨领域任务，有良好效果且可高效扩展到10k个智能体。


<details>
  <summary>Details</summary>
Motivation: 现有经济模拟环境有限，无法应对复杂经济挑战，需开发有效应用AI的模拟平台。

Method: 构建EconGym，基于严谨经济建模，实现11种异构角色类型、交互机制和智能体模型，用户可灵活组合经济角色与算法。

Result: EconGym支持多样跨领域任务，可进行基准测试；更丰富的任务组合和算法多样性能扩展政策空间，经典经济方法引导的AI智能体在复杂环境表现最佳，还能高效扩展到10k个智能体。

Conclusion: EconGym能有效解决现有经济模拟环境的不足，可用于AI驱动的政策学习和分析。

Abstract: Artificial intelligence (AI) has become a powerful tool for economic
research, enabling large-scale simulation and policy optimization. However,
applying AI effectively requires simulation platforms for scalable training and
evaluation-yet existing environments remain limited to simplified, narrowly
scoped tasks, falling short of capturing complex economic challenges such as
demographic shifts, multi-government coordination, and large-scale agent
interactions. To address this gap, we introduce EconGym, a scalable and modular
testbed that connects diverse economic tasks with AI algorithms. Grounded in
rigorous economic modeling, EconGym implements 11 heterogeneous role types
(e.g., households, firms, banks, governments), their interaction mechanisms,
and agent models with well-defined observations, actions, and rewards. Users
can flexibly compose economic roles with diverse agent algorithms to simulate
rich multi-agent trajectories across 25+ economic tasks for AI-driven policy
learning and analysis. Experiments show that EconGym supports diverse and
cross-domain tasks-such as coordinating fiscal, pension, and monetary
policies-and enables benchmarking across AI, economic methods, and hybrids.
Results indicate that richer task composition and algorithm diversity expand
the policy space, while AI agents guided by classical economic methods perform
best in complex settings. EconGym also scales to 10k agents with high realism
and efficiency.

</details>


### [376] [Does the Expansion of Medicaid Lead to Income Adjustment -- Evidence from SIPP](https://arxiv.org/abs/2506.12976)
*Mingjian Li*

Main category: econ.GN

TL;DR: 研究医保扩张下有无战略收入减少以获医保资格，发现扩张州无子女家庭有调整收入现象及相关影响。


<details>
  <summary>Details</summary>
Motivation: 探究《平价医疗法案》下医疗补助扩张是否导致人们为符合医保资格而战略降低收入。

Method: 使用收入与项目参与调查（2013 - 2019 年）的月度数据和断点回归设计。

Result: 扩张州中收入略高于资格门槛的无子女家庭最低月收入相对略低于门槛的家庭降低了联邦贫困线的 39 个百分点；效果随强制罚款增加而增强，废除后减弱；存在劳动供给调整。

Conclusion: 强化了用最低月收入确定医疗补助资格的有效性，首次证明了《平价医疗法案》对劳动力供给有显著影响。

Abstract: This study examines whether Medicaid expansion under the Affordable Care Act
(ACA) led to strategic income reductions to qualify for coverage. Using monthly
data from the Survey of Income and Program Participation (2013 to 2019) and a
regression discontinuity design, this paper finds that childless households in
expansion states with earnings just above the eligibility threshold reduced
their lowest monthly earnings by 39 percentage points of the Federal Poverty
Level (roughly 700 dollars for a two-person household) relative to those just
below. The effect intensified as the mandate penalty increased and diminished
after its repeal. Evidence suggests earnings adjustments along both intensive
and extensive margins. The paper reinforces the validity of using the lowest
monthly earning to identify Medicaid eligibility and provides the first
evidence of a substantial labor supply response to the ACA

</details>


### [377] [Inequality's Economic and Social Roots: the Role of Social Networks and Homophily](https://arxiv.org/abs/2506.13016)
*Matthew O. Jackson*

Main category: econ.GN

TL;DR: 探讨不平等的经济和社会根源，阐述社交网络在不平等、经济固化和低效中的作用，指出同质性的关键角色及网络视角的政策含义。


<details>
  <summary>Details</summary>
Motivation: 研究不平等问题，明确社交网络在其中的作用及对政策制定的影响。

Method: 从社交网络视角分析人们信息、机会和行为的纠缠如何导致社区差异。

Result: 发现社交网络视角能阐明不平等现象，同质性在网络群体分离中起关键作用，且网络视角政策含义与狭义经济视角不同。

Conclusion: 强调需制定包含经济和社会因素的“政策组合”来应对不平等。

Abstract: I discuss economic and social sources of inequality and elaborate on the role
of social networks in inequality, economic immobility, and economic
inefficiencies. The lens of social networks clarifies how the entanglement of
people's information, opportunities, and behaviors with those of their friends
and family leads to persistent differences across communities, resulting in
inequality in education, employment, income, health, and wealth. The key role
of homophily in separating groups within the network is highlighted. A network
perspective's policy implications differ substantially from a narrower economic
perspective that ignores social structure. I discuss the importance of ``policy
cocktails'' that include aspects that are aimed at both the economic and social
forces driving inequality.

</details>


### [378] [Varying reference-point salience](https://arxiv.org/abs/2506.13382)
*Alex Krumer,Felix Otto,Tim Pawlowski*

Main category: econ.GN

TL;DR: 研究参考点显著性和期望对努力供给中损失厌恶机制的影响，发现参考点显著时，有积极期望的个体表现更优。


<details>
  <summary>Details</summary>
Motivation: 探究参考点显著性和基于期望的损失厌恶在塑造努力供给中的相互作用。

Method: 利用自然实验，让专业且有激励的个体在参考点显著性外生变化的环境中执行任务，进行回归断点分析。

Result: 参考点显著时，有积极期望的个体比有消极期望的个体表现更好。

Conclusion: 参考点的显著性会显著影响期望在努力供给损失厌恶机制中的作用。

Abstract: The salience of reference points and expectations may significantly influence
the loss aversion mechanism in effort provision. We exploit a natural
experiment where highly professional and incentivized individuals perform their
task in a setting with exogenous variation of reference-point salience. While a
relevant reference point is salient in some cases, where it influences
individuals' expectations, it is obscured in others. This enables us to examine
the interplay between reference-point salience and expectation-based loss
aversion in shaping effort provision. Exploiting quasi-random variation around
the reference point, our regression discontinuity analyses reveal that
individuals with positive expectations outperform those with negative
expectations only when the reference point is salient.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [379] [Prosocial Design in Trust and Safety](https://arxiv.org/abs/2506.12792)
*David Grüning,Julia Kamin*

Main category: cs.HC

TL;DR: 介绍亲社会设计，包括核心原则、与相关领域关系，展示其减少有害行为和信息传播的作用，鼓励更多研究和讨论。


<details>
  <summary>Details</summary>
Motivation: 推广亲社会设计方法，促进更多研究和应用，引发对其原则和潜力的讨论。

Method: 回顾相关研究。

Result: 表明亲社会设计能有效减少违规和有害行为，遏制有害错误信息传播。

Conclusion: 亲社会设计是新兴且不断发展的领域，希望激发更多研究与采用，引发相关讨论。

Abstract: This chapter presents an overview of Prosocial Design, an approach to
platform design and governance that recognizes design choices influence
behavior and that those choices can or should be made toward supporting healthy
interactions and other prosocial outcomes. The authors discuss several core
principles of Prosocial Design and its relationship to Trust and Safety and
other related fields. As a primary contribution, the chapter reviews relevant
research to demonstrate how Prosocial Design can be an effective approach to
reducing rule-breaking and other harmful behavior and how it can help to stem
the spread of harmful misinformation. Prosocial Design is a nascent and
evolving field and research is still limited. The authors hope this chapter
will not only inspire more research and the adoption of a prosocial design
approach, but that it will also provoke discussion about the principles of
Prosocial Design and its potential to support Trust and Safety.

</details>


### [380] [Shelter Soul: Bridging Shelters and Adopters Through Technology](https://arxiv.org/abs/2506.12739)
*Yashodip Dharmendra Jagtap*

Main category: cs.HC

TL;DR: 本文提出基于技术的解决方案Shelter Soul，以解决宠物收养流程效率低的问题，经测试该系统表现良好，有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决宠物收养流程存在的效率低下问题，如可及性有限、缺乏实时信息和期望不匹配等。

Method: 使用MERN栈和GraphQL开发Shelter Soul集成式网络平台，系统包含智能宠物匹配、收容所管理等模块。

Result: 原型测试中，系统可处理500个并发用户，交易成功率99.2%，平均响应时间250毫秒，界面可用性评分为4.5/5。

Conclusion: Shelter Soul有潜力成为提升动物收容所运营和收养效果的实用解决方案。

Abstract: Pet adoption processes often face inefficiencies, including limited
accessibility, lack of real-time information, and mismatched expectations
between shelters and adopters. To address these challenges, this study presents
Shelter Soul, a technology-based solution designed to streamline pet adoption
through an integrated, web-based platform. Developed using the MERN stack and
GraphQL, Shelter Soul is a prototype system built to improve pet matching
accuracy, shelter management efficiency, and secure online donations. The
system includes modules for intelligent pet matching, shelter administration,
donation processing, volunteer coordination, and analytics. Prototype testing
(performance load tests, usability studies, and security assessments)
demonstrated that the system meets its design goals: it handled 500 concurrent
users with a 99.2% transaction success rate and an average response time of 250
ms, and usability feedback rated the interface highly (4.5/5). These results
indicate Shelter Soul's potential as a practical solution to enhance animal
shelter operations and adoption outcomes.

</details>


### [381] [The Journey of CodeLab: How University Hackathons Built a Community of Engaged Students](https://arxiv.org/abs/2506.12840)
*Renato Cordeiro Ferreira,Renata Santos Miranda,Alfredo Goldman*

Main category: cs.HC

TL;DR: 介绍圣保罗大学学生组织CodeLab因大学黑客松活动发展的历程，总结2015 - 2020年15场竞赛的模式、挑战和经验教训，以助力活动恢复和推动类似倡议。


<details>
  <summary>Details</summary>
Motivation: 帮助CodeLab在新冠疫情后恢复活动，并在全球推动类似倡议。

Method: 描述CodeLab从2015到2020年组织15场竞赛的经历。

Result: 总结出竞赛的模式、挑战和经验教训。

Conclusion: 相关经验可助力CodeLab恢复活动和推动全球类似倡议。

Abstract: This paper presents the journey of CodeLab: a student-organized initiative
from the University of S\~ao Paulo that has grown thanks to university
hackathons. It summarizes patterns, challenges, and lessons learned over 15
competitions organized by the group from 2015 to 2020. By describing these
experiences, this report aims to help CodeLab to resume its events after the
COVID-19 pandemic, and foster similar initiatives around the world.

</details>


### [382] [SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation](https://arxiv.org/abs/2506.12339)
*Ruiyan Zhu,Xi Cheng,Ke Liu,Brian Zhu,Daniel Jin,Neeraj Parihar,Zhoutian Xu,Oliver Gao*

Main category: cs.HC

TL;DR: 提出SheetMind，基于大语言模型实现电子表格自动化，含三种代理，集成到Google Sheets，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 实现通过自然语言指令对电子表格进行自动化操作。

Method: 构建包含Manager Agent、Action Agent和Reflection Agent的多智能体框架，通过Workspace扩展集成到Google Sheets。

Result: 在基准数据集上，单步任务成功率达80%，多步指令约70%，优于消融和基线变体。

Conclusion: 多智能体分解和基于语法的执行能有效连接自然语言和电子表格功能。

Abstract: We present SheetMind, a modular multi-agent framework powered by large
language models (LLMs) for spreadsheet automation via natural language
instructions. The system comprises three specialized agents: a Manager Agent
that decomposes complex user instructions into subtasks; an Action Agent that
translates these into structured commands using a Backus Naur Form (BNF)
grammar; and a Reflection Agent that validates alignment between generated
actions and the user's original intent. Integrated into Google Sheets via a
Workspace extension, SheetMind supports real-time interaction without requiring
scripting or formula knowledge. Experiments on benchmark datasets demonstrate
an 80 percent success rate on single step tasks and approximately 70 percent on
multi step instructions, outperforming ablated and baseline variants. Our
results highlight the effectiveness of multi agent decomposition and grammar
based execution for bridging natural language and spreadsheet functionalities.

</details>


### [383] [Feeling Machines: Ethics, Culture, and the Rise of Emotional AI](https://arxiv.org/abs/2506.12437)
*Vivek Chavan,Arsen Cenaj,Shuyuan Shen,Ariane Bar,Srishti Binwani,Tommaso Del Becaro,Marius Funk,Lynn Greschner,Roberto Hung,Stina Klein,Romina Kleiner,Stefanie Krause,Sylwia Olbrych,Vishvapalsinhji Parmar,Jaleh Sarafraz,Daria Soroko,Daksitha Withanage Don,Chang Zhou,Hoang Thuy Duong Vu,Parastoo Semnani,Daniel Weinhardt,Elisabeth Andre,Jörg Krüger,Xavier Fresquet*

Main category: cs.HC

TL;DR: 本文跨学科探讨情感响应式AI，分析其在多领域影响，指出利弊、挑战，为弱势群体担忧，缺乏保护，最后提十条建议并提供补充资源。


<details>
  <summary>Details</summary>
Motivation: 探究情感响应式AI日益增长的现状及其对多领域互动的重塑影响。

Method: 集合多领域早期研究者观点，围绕四个核心主题进行分析。

Result: 指出情感AI有支持心理健康等好处，也存在情感操纵等风险，面临模拟共情等挑战，且缺乏认知和法律保护。

Conclusion: 提出十条建议，如透明性、认证框架等，并提供补充工具、模型和数据集。

Abstract: This paper explores the growing presence of emotionally responsive artificial
intelligence through a critical and interdisciplinary lens. Bringing together
the voices of early-career researchers from multiple fields, it explores how AI
systems that simulate or interpret human emotions are reshaping our
interactions in areas such as education, healthcare, mental health, caregiving,
and digital life. The analysis is structured around four central themes: the
ethical implications of emotional AI, the cultural dynamics of human-machine
interaction, the risks and opportunities for vulnerable populations, and the
emerging regulatory, design, and technical considerations. The authors
highlight the potential of affective AI to support mental well-being, enhance
learning, and reduce loneliness, as well as the risks of emotional
manipulation, over-reliance, misrepresentation, and cultural bias. Key
challenges include simulating empathy without genuine understanding, encoding
dominant sociocultural norms into AI systems, and insufficient safeguards for
individuals in sensitive or high-risk contexts. Special attention is given to
children, elderly users, and individuals with mental health challenges, who may
interact with AI in emotionally significant ways. However, there remains a lack
of cognitive or legal protections which are necessary to navigate such
engagements safely. The report concludes with ten recommendations, including
the need for transparency, certification frameworks, region-specific
fine-tuning, human oversight, and longitudinal research. A curated
supplementary section provides practical tools, models, and datasets to support
further work in this domain.

</details>


### [384] [Levels of Autonomy for AI Agents](https://arxiv.org/abs/2506.12469)
*K. J. Kevin Feng,David W. McDonald,Amy X. Zhang*

Main category: cs.HC

TL;DR: 本文探讨AI智能体自主性水平的校准，定义五级自主性，描述用户控制方式及交互设计问题，提出自主性证书应用和评估思路。


<details>
  <summary>Details</summary>
Motivation: 解决智能体开发者如何校准智能体合适自主性水平的问题。

Method: 定义五级逐步递增的智能体自主性，描述各层级用户控制智能体的方式和交互设计问题。

Result: 提出自主性框架在AI自主性证书方面的潜在应用。

Conclusion: 提出评估智能体自主性的早期想法，为现实中负责任部署和使用AI智能体贡献实用步骤。

Abstract: Autonomy is a double-edged sword for AI agents, simultaneously unlocking
transformative possibilities and serious risks. How can agent developers
calibrate the appropriate levels of autonomy at which their agents should
operate? We argue that an agent's level of autonomy can be treated as a
deliberate design decision, separate from its capability and operational
environment. In this work, we define five levels of escalating agent autonomy,
characterized by the roles a user can take when interacting with an agent:
operator, collaborator, consultant, approver, and observer. Within each level,
we describe the ways by which a user can exert control over the agent and open
questions for how to design the nature of user-agent interaction. We then
highlight a potential application of our framework towards AI autonomy
certificates to govern agent behavior in single- and multi-agent systems. We
conclude by proposing early ideas for evaluating agents' autonomy. Our work
aims to contribute meaningful, practical steps towards responsibly deployed and
useful AI agents in the real world.

</details>


### [385] [SplashNet: Split-and-Share Encoders for Accurate and Efficient Typing with Surface Electromyography](https://arxiv.org/abs/2506.12356)
*Nima Hadidi,Jason Chan,Ebrahim Feghhi,Jonathan Kao*

Main category: cs.HC

TL;DR: 论文指出手腕表面肌电图（sEMG）在零样本和微调设置下字符误识率高，提出三项改进，构建SplashNet - mini和SplashNet模型，降低误识率，创技术新水平。


<details>
  <summary>Details</summary>
Motivation: 现有emg2qwerty基线在零样本和用户特定微调设置下字符误识率高，根源在于跨用户信号统计不匹配、依赖高阶特征和缺乏与打字双边特性匹配的架构归纳偏置。

Method: 提出三项改进：滚动时间归一化、激进通道掩码、拆分共享编码器；降低频谱分辨率，构建SplashNet - mini和SplashNet模型。

Result: SplashNet - mini参数仅为基线的1/4，FLOPs为0.6倍，零样本CER降至36.4%，微调后降至5.9%；SplashNet参数为1/2，FLOPs为1.15倍，CER进一步降至35.7%和5.5%。

Conclusion: SplashNet在不增加数据的情况下建立了新的技术水平。

Abstract: Surface electromyography (sEMG) at the wrists could enable natural,
keyboard-free text entry, yet the state-of-the-art emg2qwerty baseline still
misrecognizes $51.8\%$ of characters in the zero-shot setting on unseen users
and $7.0\%$ after user-specific fine-tuning. We trace many of these errors to
mismatched cross-user signal statistics, fragile reliance on high-order feature
dependencies, and the absence of architectural inductive biases aligned with
the bilateral nature of typing. To address these issues, we introduce three
simple modifications: (i) Rolling Time Normalization, which adaptively aligns
input distributions across users; (ii) Aggressive Channel Masking, which
encourages reliance on low-order feature combinations more likely to generalize
across users; and (iii) a Split-and-Share encoder that processes each hand
independently with weight-shared streams to reflect the bilateral symmetry of
the neuromuscular system. Combined with a five-fold reduction in spectral
resolution ($33\!\rightarrow\!6$ frequency bands), these components yield a
compact Split-and-Share model, SplashNet-mini, which uses only $\tfrac14$ the
parameters and $0.6\times$ the FLOPs of the baseline while reducing
character-error rate (CER) to $36.4\%$ zero-shot and $5.9\%$ after fine-tuning.
An upscaled variant, SplashNet ($\tfrac12$ the parameters, $1.15\times$ the
FLOPs of the baseline), further lowers error to $35.7\%$ and $5.5\%$,
representing relative improvements of $31\%$ and $21\%$ in the zero-shot and
fine-tuned settings, respectively. SplashNet therefore establishes a new state
of the art without requiring additional data.

</details>


### [386] [Exploring the Potential of Metacognitive Support Agents for Human-AI Co-Creation](https://arxiv.org/abs/2506.12879)
*Frederic Gmeiner,Kaitao Luo,Ye Wang,Kenneth Holstein,Nikolas Martelaro*

Main category: cs.HC

TL;DR: 专业人员在设计流程中集成生成式AI工具存在困难，研究通过原型探索提出元认知支持代理，发现其能助设计师产生更可行设计，并探讨了相关机会、权衡和未来设计工具考虑因素。


<details>
  <summary>Details</summary>
Motivation: 专业人员在设计流程中集成生成式AI工具面临认知挑战，如意图制定和认知卸载问题，需要元认知支持。

Method: 通过绿野仙踪启发式研究对20名机械设计师进行探索性原型设计，探究多种元认知支持策略。

Result: 有代理支持的用户比无支持的用户能创造出更可行的设计，不同支持策略影响不同。

Conclusion: 讨论了元认知支持代理的机会与权衡，以及未来基于AI的设计工具的考虑因素。

Abstract: Despite the potential of generative AI (GenAI) design tools to enhance design
processes, professionals often struggle to integrate AI into their workflows.
Fundamental cognitive challenges include the need to specify all design
criteria as distinct parameters upfront (intent formulation) and designers'
reduced cognitive involvement in the design process due to cognitive
offloading, which can lead to insufficient problem exploration,
underspecification, and limited ability to evaluate outcomes. Motivated by
these challenges, we envision novel metacognitive support agents that assist
designers in working more reflectively with GenAI. To explore this vision, we
conducted exploratory prototyping through a Wizard of Oz elicitation study with
20 mechanical designers probing multiple metacognitive support strategies. We
found that agent-supported users created more feasible designs than
non-supported users, with differing impacts between support strategies. Based
on these findings, we discuss opportunities and tradeoffs of metacognitive
support agents and considerations for future AI-based design tools.

</details>


### [387] [Can you see how I learn? Human observers' inferences about Reinforcement Learning agents' learning processes](https://arxiv.org/abs/2506.13583)
*Bernhard Hilpert,Muhan Hou,Kim Baraka,Joost Broekens*

Main category: cs.HC

TL;DR: 本文通过两个实验，以自下而上的方式探究人类如何理解强化学习智能体的学习行为，开发了评估人类对智能体学习推断的范式，确认其可靠性并完善主题框架，为设计可解释的强化学习系统提供见解。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体的学习行为难以被人类直观理解，且人类如何感知和解释其学习行为尚不清楚，因此需要研究人类观察者对智能体学习过程理解的因素。

Method: 开展两个实验，先进行探索性访谈研究（N=9）确定人类解释的四个核心主题，再进行验证性研究（N=34），将扩展范式应用于两个任务和两种强化学习算法，分析816份回应。

Result: 确认了范式的可靠性，完善了主题框架，揭示了主题随时间的演变和相互关系。

Conclusion: 研究提供了以人类为中心的视角，理解人类如何理解智能体学习，为设计可解释的强化学习系统和提高人机交互透明度提供了可操作的见解。

Abstract: Reinforcement Learning (RL) agents often exhibit learning behaviors that are
not intuitively interpretable by human observers, which can result in
suboptimal feedback in collaborative teaching settings. Yet, how humans
perceive and interpret RL agent's learning behavior is largely unknown. In a
bottom-up approach with two experiments, this work provides a data-driven
understanding of the factors of human observers' understanding of the agent's
learning process. A novel, observation-based paradigm to directly assess human
inferences about agent learning was developed. In an exploratory interview
study (\textit{N}=9), we identify four core themes in human interpretations:
Agent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second
confirmatory study (\textit{N}=34) applied an expanded version of the paradigm
across two tasks (navigation/manipulation) and two RL algorithms
(tabular/function approximation). Analyses of 816 responses confirmed the
reliability of the paradigm and refined the thematic framework, revealing how
these themes evolve over time and interrelate. Our findings provide a
human-centered understanding of how people make sense of agent learning,
offering actionable insights for designing interpretable RL systems and
improving transparency in Human-Robot Interaction.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [388] [The Limits of Tractable Marginalization](https://arxiv.org/abs/2506.12020)
*Oliver Broadrick,Sanyam Agarwal,Guy Van den Broeck,Markus Bläser*

Main category: cs.CC

TL;DR: 本文探讨可多项式时间边缘化的函数能否由算术电路简洁表示，给出否定答案并得出完整性结果。


<details>
  <summary>Details</summary>
Motivation: 研究是否所有具有多项式时间边缘化算法的函数都能由算术电路简洁表示。

Method: 识别对应更强形式边缘化的复杂度类层次结构，在已知电路模型上计算；基于FP≠#P假设进行分析。

Result: 给出否定答案，展示了具有易处理边缘化但无法由已知模型有效表示的简单函数。

Conclusion: 当有高效实随机存取机对函数执行虚拟证据边缘化时，该函数的多线性表示存在小电路。

Abstract: Marginalization -- summing a function over all assignments to a subset of its
inputs -- is a fundamental computational problem with applications from
probabilistic inference to formal verification. Despite its computational
hardness in general, there exist many classes of functions (e.g., probabilistic
models) for which marginalization remains tractable, and they can be commonly
expressed by polynomial size arithmetic circuits computing multilinear
polynomials. This raises the question, can all functions with polynomial time
marginalization algorithms be succinctly expressed by such circuits? We give a
negative answer, exhibiting simple functions with tractable marginalization yet
no efficient representation by known models, assuming
$\textsf{FP}\neq\#\textsf{P}$ (an assumption implied by $\textsf{P} \neq
\textsf{NP}$). To this end, we identify a hierarchy of complexity classes
corresponding to stronger forms of marginalization, all of which are
efficiently computable on the known circuit models. We conclude with a
completeness result, showing that whenever there is an efficient real RAM
performing virtual evidence marginalization for a function, then there are
small circuits for that function's multilinear representation.

</details>


### [389] [Constant Bit-size Transformers Are Turing Complete](https://arxiv.org/abs/2506.12027)
*Qian Li,Yuyi Wang*

Main category: cs.CC

TL;DR: 证明恒定比特大小的变换器能模拟任意长度输入的图灵机，且SPACE[s(n)]刻画其表达能力，借助模拟Post机实现。


<details>
  <summary>Details</summary>
Motivation: 改进先前工作在处理长输入时需增加模型精度或参数数量的问题，探究变换器推理能力的潜在机制。

Method: 模拟Post机，这是一种图灵完备的计算模型。

Result: 恒定比特大小变换器能模拟任意长度输入的图灵机，复杂度类SPACE[s(n)]精确刻画其表达能力。

Conclusion: 变换器与Post机的行为相似性可能为理解变换器推理能力的机制提供新见解。

Abstract: We prove that any Turing machine running on inputs of arbitrary length can be
simulated by a constant bit-size transformer, as long as the context window is
sufficiently long. This improves previous works, which require scaling up
either the model's precision or the number of parameters on longer inputs.
Furthermore, we prove that the complexity class SPACE$[s(n)]$ exactly
characterizes the expressive power of a constant bit-size transformer with a
context window of length $s(n)$. Our approach relies on simulating Post
machines, a Turing-complete computational model. Post machines can be modeled
as automata equipped with a queue, exhibiting computational behaviors naturally
aligned with those of transformers. The behavioral similarity between
transformers and Post machines may offer new insights into the mechanisms
underlying the reasoning abilities of transformers.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [390] [Symmetry-preserving neural networks in lattice field theories](https://arxiv.org/abs/2506.12493)
*Matteo Favoni*

Main category: hep-lat

TL;DR: 本文介绍尊重对称性的神经网络及其在格点场论问题中的应用，阐述等变性概念，展示其在平移对称和规范理论中的优势，还引入神经梯度流技术。


<details>
  <summary>Details</summary>
Motivation: 研究尊重对称性的神经网络在格点场论问题中的应用优势。

Method: 解释等变性概念，以复标量场玩具模型说明平移对称下选择等变网络的好处，专门设计格点规范等变卷积神经网络（L - CNNs）用于规范理论，引入神经梯度流技术。

Result: L - CNNs成功解决如Wilson环等物理可观测量的回归问题，传统非规范对称架构表现较差。

Conclusion: 尊重对称性的神经网络在格点场论问题中有优势，神经梯度流可作为生成格点规范配置的方法。

Abstract: This thesis deals with neural networks that respect symmetries and presents
the advantages in applying them to lattice field theory problems. The concept
of equivariance is explained, together with the reason why such a property is
crucial for the network to preserve the desired symmetry. The benefits of
choosing equivariant networks are first illustrated for translational symmetry
on a complex scalar field toy model. The discussion is then extended to gauge
theories, for which Lattice Gauge Equivariant Convolutional Neural Networks
(L-CNNs) are specifically designed ad hoc. Regressions of physical observables
such as Wilson loops are successfully solved by L-CNNs, whereas traditional
architectures which are not gauge symmetric perform significantly worse.
Finally, we introduce the technique of neural gradient flow, which is an
ordinary differential equation solved by neural networks, and propose it as a
method to generate lattice gauge configurations.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [391] [Deceptive Path Planning: A Bayesian Game Approach](https://arxiv.org/abs/2506.13650)
*Violetta Rostobaya,James Berneburg,Yue Guan,Michael Dorothy,Daigo Shishika*

Main category: eess.SY

TL;DR: 研究自主代理在对抗环境中通过运动传递信息，构建动态贝叶斯博弈模型，提出计算有效方法找PBNE，实验显示策略优势


<details>
  <summary>Details</summary>
Motivation: 研究自主代理在需欺骗观察者情况下如何通过运动传递信息，解决防御资源分配问题

Method: 将交互建模为动态贝叶斯博弈，用Perfect Bayesian Nash Equilibrium (PBNE)作为解概念并提出计算有效方法

Result: 防御者采用简单马尔可夫策略，攻击者随机混合最短和非最短路径平衡欺骗与目标效率，数值实验表明基于PBNE策略优于现有单边优化方法

Conclusion: 所提出的基于PBNE的策略在解决自主代理在对抗环境中运动信息传递问题上有优势

Abstract: This paper investigates how an autonomous agent can transmit information
through its motion in an adversarial setting. We consider scenarios where an
agent must reach its goal while deceiving an intelligent observer about its
destination. We model this interaction as a dynamic Bayesian game between a
mobile Attacker with a privately known goal and a Defender who infers the
Attacker's intent to allocate defensive resources effectively. We use Perfect
Bayesian Nash Equilibrium (PBNE) as our solution concept and propose a
computationally efficient approach to find it. In the resulting equilibrium,
the Defender employs a simple Markovian strategy, while the Attacker
strategically balances deception and goal efficiency by stochastically mixing
shortest and non-shortest paths to manipulate the Defender's beliefs. Numerical
experiments demonstrate the advantages of our PBNE-based strategies over
existing methods based on one-sided optimization.

</details>


### [392] [Nonlinear Model Order Reduction of Dynamical Systems in Process Engineering: Review and Comparison](https://arxiv.org/abs/2506.12819)
*Jan C. Schulze,Alexander Mitsos*

Main category: eess.SY

TL;DR: 本文回顾非线性模型降阶方法，理论比较其性质，扩展流形 - 伽辽金方法至含输入动力系统，并通过案例研究讨论各方法优缺点。


<details>
  <summary>Details</summary>
Motivation: 计算成本低且足够准确的动力学模型对实时非线性优化和基于模型的控制至关重要，对高阶预测模型降阶可实现实时应用。

Method: 回顾非线性模型降阶方法并理论比较性质；扩展流形 - 伽辽金方法；对空气分离过程模型应用八种降阶方法进行案例研究。

Result: 对各方法性质进行理论比较，通过案例研究发现各方法优缺点。

Conclusion: 不同的模型降阶方法有各自的优缺点，在实际应用中需根据具体情况选择合适方法。

Abstract: Computationally cheap yet accurate enough dynamical models are vital for
real-time capable nonlinear optimization and model-based control. When given a
computationally expensive high-order prediction model, a reduction to a
lower-order simplified model can enable such real-time applications. Herein, we
review state-of-the-art nonlinear model order reduction methods and provide a
theoretical comparison of method properties. Additionally, we discuss both
general-purpose methods and tailored approaches for (chemical) process systems
and we identify similarities and differences between these methods. As
manifold-Galerkin approaches currently do not account for inputs in the
construction of the reduced state subspace, we extend these methods to
dynamical systems with inputs. In a comparative case study, we apply eight
established model order reduction methods to an air separation process model:
POD-Galerkin, nonlinear-POD-Galerkin, manifold-Galerkin, dynamic mode
decomposition, Koopman theory, manifold learning with latent predictor,
compartment modeling, and model aggregation. Herein, we do not investigate
hyperreduction (reduction of FLOPS). Based on our findings, we discuss
strengths and weaknesses of the model order reduction methods.

</details>


### [393] [Condition Monitoring with Machine Learning: A Data-Driven Framework for Quantifying Wind Turbine Energy Loss](https://arxiv.org/abs/2506.13012)
*Emil Marcus Buchberg,Kent Vugs Nielsen*

Main category: eess.SY

TL;DR: 本文介绍先进机器学习框架用于风力发电机状态监测，可检测异常、估计能源损失，能减少维护支出。


<details>
  <summary>Details</summary>
Motivation: 风力发电机运行存在前缘侵蚀等挑战，导致能源输出降低，需要有效方法监测状态。

Method: 引入先进机器学习框架，通过严格预处理隔离正常行为，结合领域规则和异常检测过滤器，利用随机森林、XGBoost和KNN等模型。

Result: 数据预处理使数据显著减少，保留31%原始数据；35台涡轮机中24台性能下降，7台改善，4台无显著变化；模型能捕捉性能细微下降。

Conclusion: 框架为现有状态监测方法提供新途径，可隔离正常数据、估计能源损失，减少维护支出和停机经济影响。

Abstract: Wind energy significantly contributes to the global shift towards renewable
energy, yet operational challenges, such as Leading-Edge Erosion on wind
turbine blades, notably reduce energy output. This study introduces an
advanced, scalable machine learning framework for condition monitoring of wind
turbines, specifically targeting improved detection of anomalies using
Supervisory Control and Data Acquisition data. The framework effectively
isolates normal turbine behavior through rigorous preprocessing, incorporating
domain-specific rules and anomaly detection filters, including Gaussian Mixture
Models and a predictive power score. The data cleaning and feature selection
process enables identification of deviations indicative of performance
degradation, facilitating estimates of annual energy production losses. The
data preprocessing methods resulted in significant data reduction, retaining on
average 31% of the original SCADA data per wind farm. Notably, 24 out of 35
turbines exhibited clear performance declines. At the same time, seven
improved, and four showed no significant changes when employing the power curve
feature set, which consisted of wind speed and ambient temperature. Models such
as Random Forest, XGBoost, and KNN consistently captured subtle but persistent
declines in turbine performance. The developed framework provides a novel
approach to existing condition monitoring methodologies by isolating normal
operational data and estimating annual energy loss, which can be a key part in
reducing maintenance expenditures and mitigating economic impacts from turbine
downtime.

</details>


### [394] [A Hybrid Artificial Intelligence Method for Estimating Flicker in Power Systems](https://arxiv.org/abs/2506.13611)
*Javad Enayati,Pedram Asef,Alexandre Benoit*

Main category: eess.SY

TL;DR: 本文提出结合H滤波与自适应线性神经网络的混合AI方法用于配电系统闪变分量估计，经测试效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有频域方法在配电系统闪变分量估计中的关键局限，处理复杂电力干扰时无需先验噪声知识和大量训练。

Method: 结合H滤波与自适应线性神经网络（ADALINE），先用H滤波提取不确定和有噪声条件下的电压包络，再用ADALINE识别包络中的闪变频率。

Result: 通过基于IEC标准的模拟研究、统计分析、蒙特卡罗模拟和真实数据验证，该方法比基于快速傅里叶变换和离散小波变换的估计器有更高准确性、鲁棒性和更低计算负荷。

Conclusion: 该混合AI方法在配电系统闪变分量估计中表现优异，能有效解决现有频域方法的问题。

Abstract: This paper introduces a novel hybrid AI method combining H filtering and an
adaptive linear neuron network for flicker component estimation in power
distribution systems.The proposed method leverages the robustness of the H
filter to extract the voltage envelope under uncertain and noisy conditions
followed by the use of ADALINE to accurately identify flicker frequencies
embedded in the envelope.This synergy enables efficient time domain estimation
with rapid convergence and noise resilience addressing key limitations of
existing frequency domain approaches.Unlike conventional techniques this hybrid
AI model handles complex power disturbances without prior knowledge of noise
characteristics or extensive training.To validate the method performance we
conduct simulation studies based on IEC Standard 61000 4 15 supported by
statistical analysis Monte Carlo simulations and real world data.Results
demonstrate superior accuracy robustness and reduced computational load
compared to Fast Fourier Transform and Discrete Wavelet Transform based
estimators.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [395] [Component Based Quantum Machine Learning Explainability](https://arxiv.org/abs/2506.12378)
*Barra White,Krishnendu Guha*

Main category: quant-ph

TL;DR: 本文探讨创建模块化、可解释的量子机器学习（QML）框架，拆分QML算法组件并以适配技术分析，以实现整体模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 可解释机器学习算法在医疗和金融等领域重要，QML虽有优势但存在黑箱问题，需开发可解释性技术。

Method: 将QML算法拆分为核心组件，用ALE和SHAP等可解释性技术分析各组件，综合各部分见解推断整体模型可解释性。

Result: 未提及。

Conclusion: 未提及。

Abstract: Explainable ML algorithms are designed to provide transparency and insight
into their decision-making process. Explaining how ML models come to their
prediction is critical in fields such as healthcare and finance, as it provides
insight into how models can help detect bias in predictions and help comply
with GDPR compliance in these fields. QML leverages quantum phenomena such as
entanglement and superposition, offering the potential for computational
speedup and greater insights compared to classical ML. However, QML models also
inherit the black-box nature of their classical counterparts, requiring the
development of explainability techniques to be applied to these QML models to
help understand why and how a particular output was generated.
  This paper will explore the idea of creating a modular, explainable QML
framework that splits QML algorithms into their core components, such as
feature maps, variational circuits (ansatz), optimizers, kernels, and
quantum-classical loops. Each component will be analyzed using explainability
techniques, such as ALE and SHAP, which have been adapted to analyse the
different components of these QML algorithms. By combining insights from these
parts, the paper aims to infer explainability to the overall QML model.

</details>


### [396] [Improved Ground State Estimation in Quantum Field Theories via Normalising Flow-Assisted Neural Quantum States](https://arxiv.org/abs/2506.12128)
*Vishal S. Ngairangbam,Michael Spannowsky,Timur Sypchenko*

Main category: quant-ph

TL;DR: 提出混合变分框架，用归一化流采样器增强神经量子态，用于量子多体波函数模拟，在横场伊辛模型上表现良好。


<details>
  <summary>Details</summary>
Motivation: 提高量子多体波函数的表达能力和可训练性，克服马尔可夫链蒙特卡罗和自回归方法的局限性。

Method: 提出混合变分框架，将采样任务与变分 ansatz 解耦，学习连续流模型来针对希尔伯特空间的离散、振幅支持子空间。

Result: 应用于横场伊辛模型，达到与最先进的矩阵乘积态相当的基态能量误差，能量低于自回归神经量子态，在多达 50 个自旋系统上展示了高精度和鲁棒收敛性。

Conclusion: 流辅助采样是量子模拟的可扩展工具，为在高维希尔伯特空间学习有表现力的量子态提供新方法。

Abstract: We propose a hybrid variational framework that enhances Neural Quantum States
(NQS) with a Normalising Flow-based sampler to improve the expressivity and
trainability of quantum many-body wavefunctions. Our approach decouples the
sampling task from the variational ansatz by learning a continuous flow model
that targets a discretised, amplitude-supported subspace of the Hilbert space.
This overcomes limitations of Markov Chain Monte Carlo (MCMC) and
autoregressive methods, especially in regimes with long-range correlations and
volume-law entanglement. Applied to the transverse-field Ising model with both
short- and long-range interactions, our method achieves comparable ground state
energy errors with state-of-the-art matrix product states and lower energies
than autoregressive NQS. For systems up to 50 spins, we demonstrate high
accuracy and robust convergence across a wide range of coupling strengths,
including regimes where competing methods fail. Our results showcase the
utility of flow-assisted sampling as a scalable tool for quantum simulation and
offer a new approach toward learning expressive quantum states in
high-dimensional Hilbert spaces.

</details>


### [397] [OSI Stack Redesign for Quantum Networks: Requirements, Technologies, Challenges, and Future Directions](https://arxiv.org/abs/2506.12195)
*Shakil Ahmed,Muhammad Kamran Saeed,Ashfaq Khokhar*

Main category: quant-ph

TL;DR: 本文针对量子网络对经典 OSI 模型进行架构重新设计，介绍量子融合 OSI 栈，整合研究成果，给出分类、跨层支持技术、仿真工具、应用场景和评估框架，指明未来方向。


<details>
  <summary>Details</summary>
Motivation: 经典 OSI 模型无法支持量子网络特定现象，需要为 7G 背景下的量子网络重新设计 OSI 模型。

Method: 扩展经典模型，引入第 0 层（量子基板）和第 8 层（认知意图），重新定义各层；整合 150 多项研究成果进行分类；提供跨层支持技术分类和仿真工具；提出评估框架。

Result: 提出量子融合 OSI 栈，涵盖多种量子机制；给出研究成果分类、跨层支持技术、仿真工具；提出基于熵吞吐量、相干延迟和纠缠保真度的评估框架。

Conclusion: 指明可编程量子栈、数字孪生和 AI 定义的 QNet 代理等未来方向，为 7G 及以后构建可扩展、智能且符合量子特性的 OSI 框架奠定基础。

Abstract: Quantum communication is poised to become a foundational element of
next-generation networking, offering transformative capabilities in security,
entanglement-based connectivity, and computational offloading. However, the
classical OSI model-designed for deterministic and error-tolerant
systems-cannot support quantum-specific phenomena such as coherence fragility,
probabilistic entanglement, and the no-cloning theorem. This paper provides a
comprehensive survey and proposes an architectural redesign of the OSI model
for quantum networks in the context of 7G. We introduce a Quantum-Converged OSI
stack by extending the classical model with Layer 0 (Quantum Substrate) and
Layer 8 (Cognitive Intent), supporting entanglement, teleportation, and
semantic orchestration via LLMs and QML. Each layer is redefined to incorporate
quantum mechanisms such as enhanced MAC protocols, fidelity-aware routing, and
twin-based applications. This survey consolidates over 150 research works from
IEEE, ACM, MDPI, arXiv, and Web of Science (2018-2025), classifying them by OSI
layer, enabling technologies such as QKD, QEC, PQC, and RIS, and use cases such
as satellite QKD, UAV swarms, and quantum IoT. A taxonomy of cross-layer
enablers-such as hybrid quantum-classical control, metadata-driven
orchestration, and blockchain-integrated quantum trust-is provided, along with
simulation tools including NetSquid, QuNetSim, and QuISP. We present several
domain-specific applications, including quantum healthcare telemetry, entangled
vehicular networks, and satellite mesh overlays. An evaluation framework is
proposed based on entropy throughput, coherence latency, and entanglement
fidelity. Key future directions include programmable quantum stacks, digital
twins, and AI-defined QNet agents, laying the groundwork for a scalable,
intelligent, and quantum-compliant OSI framework for 7G and beyond.

</details>


### [398] [Noise tolerance via reinforcement: Learning a reinforced quantum dynamics](https://arxiv.org/abs/2506.12418)
*Abolfazl Ramezanpour*

Main category: quant-ph

TL;DR: 研究表明强化量子动力学对噪声环境有显著鲁棒性，采用学习算法近似强化动力学，减少演化时间和噪声影响，通过数值模拟验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 量子模拟性能依赖噪声缓解和纠错算法，强化策略可提升学习和优化算法性能，探索强化量子动力学对噪声环境的鲁棒性。

Method: 研究量子退火过程，通过强化使系统维持当前状态或无噪声演化，采用学习算法近似强化动力学。

Result: 通过对单比特和两比特系统在泡利噪声下的强化量子退火进行数值模拟，证明了方法的有效性。

Conclusion: 强化量子动力学能有效抵抗噪声环境，此方法可减少系统演化时间，避免量子反馈的复杂性。

Abstract: The performance of quantum simulations heavily depends on the efficiency of
noise mitigation techniques and error correction algorithms. Reinforcement has
emerged as a powerful strategy to enhance the performance of learning and
optimization algorithms. In this study, we demonstrate that reinforced quantum
dynamics can exhibit significant robustness against interactions with a noisy
environment. We study a quantum annealing process where, through reinforcement,
the system is encouraged to maintain its current state or follow a noise-free
evolution. A learning algorithm is employed to find a concise approximation of
this reinforced dynamics, reducing the total evolution time and, consequently,
the system's exposure to noisy interactions. This approach also avoids the
complexities associated with implementing quantum feedback in such algorithms.
The efficacy of our method is demonstrated through numerical simulations of
reinforced quantum annealing with one- and two-qubit systems under Pauli noise.

</details>


### [399] [Solving tricky quantum optics problems with assistance from (artificial) intelligence](https://arxiv.org/abs/2506.12770)
*Manas Pandey,Bharath Hebbe Madhusudhana,Saikat Ghosh,Dmitry Budker*

Main category: quant-ph

TL;DR: 本文探索AI作为量子光学‘科学合作伙伴’的能力，发现其能处理复杂问题，实现建模分析平民化，加速研究。


<details>
  <summary>Details</summary>
Motivation: 探索现代人工智能作为‘科学合作伙伴’在量子光学领域的能力。

Method: 让AI参与量子光学三个细微问题，通过迭代对话，对AI进行提示和纠正。

Result: AI能推理复杂场景、完善答案、提供专业指导，类似与专业同事交流。

Conclusion: AI使复杂建模和分析更普及，科研重点从技术掌握转向思想生成和测试，大幅缩短研究时间。

Abstract: The capabilities of modern artificial intelligence (AI) as a ``scientific
collaborator'' are explored by engaging it with three nuanced problems in
quantum optics: state populations in optical pumping, resonant transitions
between decaying states (the Burshtein effect), and degenerate mirrorless
lasing. Through iterative dialogue, the authors observe that AI models--when
prompted and corrected--can reason through complex scenarios, refine their
answers, and provide expert-level guidance, closely resembling the interaction
with an adept colleague. The findings highlight that AI democratizes access to
sophisticated modeling and analysis, shifting the focus in scientific practice
from technical mastery to the generation and testing of ideas, and reducing the
time for completing research tasks from days to minutes.

</details>


### [400] [Quantum AGI: Ontological Foundations](https://arxiv.org/abs/2506.13134)
*Elija Perrier,Michael Timothy Bennett*

Main category: quant-ph

TL;DR: 探讨量子基础对AGI的影响，提出信息论分类法并展示量子力学对AGI能力的作用


<details>
  <summary>Details</summary>
Motivation: 研究量子基础对AGI实际实施的影响

Method: 引入新颖的信息论分类法区分经典AGI和量子AGI

Result: 展示了量子力学影响AGI基本特征，量子本体论能改变AGI能力

Conclusion: 量子基础在计算上给AGI带来优势，同时也施加了新的限制

Abstract: We examine the implications of quantum foundations for AGI, focusing on how
seminal results such as Bell's theorems (non-locality), the Kochen-Specker
theorem (contextuality) and no-cloning theorem problematise practical
implementation of AGI in quantum settings. We introduce a novel
information-theoretic taxonomy distinguishing between classical AGI and quantum
AGI and show how quantum mechanics affects fundamental features of agency. We
show how quantum ontology may change AGI capabilities, both via affording
computational advantages and via imposing novel constraints.

</details>


### [401] [A Two-stage Optimization Method for Wide-range Single-electron Quantum Magnetic Sensing](https://arxiv.org/abs/2506.13469)
*Shiqian Guo,Jianqing Liu,Thinh Le,Huaiyu Dai*

Main category: quant-ph

TL;DR: 本文提出一种新的量子磁传感协议，用两阶段优化方法在受限时间内实现宽范围直流磁场估计的精度和资源效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有量子磁传感设计最优传感参数的方法在信号范围广且量子传感器有物理约束时，难以高效或最优收敛，导致询问时间长和传感精度降低。

Method: 采用两阶段优化方法，第一阶段用固定传感参数的贝叶斯神经网络缩小信号范围，第二阶段用联邦强化学习智能体在缩小的搜索空间内微调传感参数。

Result: 在受限总传感时间预算下对NV中心电子自旋单次读出的挑战性场景中，相比现有技术，在宽范围直流磁场估计的精度和资源效率上有显著提升。

Conclusion: 所提出的两阶段优化协议能有效解决现有方法的不足，提升量子磁传感性能。

Abstract: Quantum magnetic sensing based on spin systems has emerged as a new paradigm
for detecting ultra-weak magnetic fields with unprecedented sensitivity,
revitalizing applications in navigation, geo-localization, biology, and beyond.
At the heart of quantum magnetic sensing, from the protocol perspective, lies
the design of optimal sensing parameters to manifest and then estimate the
underlying signals of interest (SoI). Existing studies on this front mainly
rely on adaptive algorithms based on black-box AI models or formula-driven
principled searches. However, when the SoI spans a wide range and the quantum
sensor has physical constraints, these methods may fail to converge efficiently
or optimally, resulting in prolonged interrogation times and reduced sensing
accuracy. In this work, we report the design of a new protocol using a
two-stage optimization method. In the 1st Stage, a Bayesian neural network with
a fixed set of sensing parameters is used to narrow the range of SoI. In the
2nd Stage, a federated reinforcement learning agent is designed to fine-tune
the sensing parameters within a reduced search space. The proposed protocol is
developed and evaluated in a challenging context of single-shot readout of an
NV-center electron spin under a constrained total sensing time budget; and yet
it achieves significant improvements in both accuracy and resource efficiency
for wide-range D.C. magnetic field estimation compared to the state of the art.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [402] [Permutation-Avoiding FFT-Based Convolution](https://arxiv.org/abs/2506.12718)
*Nicolas Venkovic,Hartwig Anzt*

Main category: math.NA

TL;DR: 论文指出标准FFT实现中索引反转排列会降低算术强度，提出多维、避免排列的卷积方法并实验，建议FFT库开发者支持该卷积内核。


<details>
  <summary>Details</summary>
Motivation: 标准FFT实现中索引反转排列会显著降低FFT的算术强度，在重复使用固定滤波器的离散卷积场景下有优化需求。

Method: 在通用基数Cooley - Tukey框架内提出多维、避免排列的卷积程序。

Result: 通过数值实验将算法与最先进的基于FFT的卷积实现进行基准测试。

Conclusion: FFT库开发者应考虑支持避免排列的卷积内核。

Abstract: Fast Fourier Transform (FFT) libraries are widely used for evaluating
discrete convolutions. Most FFT implementations follow some variant of the
Cooley-Tukey framework, in which the transform is decomposed into butterfly
operations and index-reversal permutations. While butterfly operations dominate
the floating-point operation count, the memory access patterns induced by
index-reversal permutations significantly degrade the FFT's arithmetic
intensity. In practice, discrete convolutions are often applied repeatedly with
a fixed filter. In such cases, we show that the index-reversal permutations
involved in both the forward and backward transforms of standard FFT-based
convolution implementations can be avoided by deferring to a single offline
permutation of the filter. We propose a multi-dimensional, permutation-avoiding
convolution procedure within a general radix Cooley-Tukey framework. We perform
numerical experiments to benchmark our algorithms against state-of-the-art
FFT-based convolution implementations. Our results suggest that developers of
FFT libraries should consider supporting permutation-avoiding convolution
kernels.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [403] [A New Approach for the Continuous Time Kyle-Back Strategic Insider Equilibrium Problem](https://arxiv.org/abs/2506.12281)
*Bixing Qiao,Jianfeng Zhang*

Main category: math.OC

TL;DR: 本文研究连续时间Kyle - Back模型，提出新方法刻画所有均衡，证明短时唯一均衡，研究博弈集值并刻画。


<details>
  <summary>Details</summary>
Motivation: 现有文献用PDE方法研究均衡存在性有局限，需特定马尔可夫结构，本文提供新方法。

Method: 用随机控制和随机微分博弈的新方法，通过正倒向随机微分方程（FBSDE）系统刻画均衡。

Result: 证明短时FBSDE适定，有唯一非马尔可夫均衡；文献中桥型均衡截断可作为近似均衡；用HJB方程水平集刻画博弈集值。

Conclusion: 新方法能有效研究模型均衡，得到唯一均衡结果，拓展了相关研究。

Abstract: This paper considers a continuous time Kyle-Back model which is a game
problem between an insider and a market marker. The existing literature
typically focuses on the existence of equilibrium by using the PDE approach,
which requires certain Markovian structure and the equilibrium is in the bridge
form. We shall provide a new approach which is used widely for stochastic
controls and stochastic differential games. We characterize all equilibria
through a coupled system of forward backward SDEs, where the forward one is the
conditional law of the inside information and the backward one is the insider's
optimal value. In particular, when the time duration is small, we show that the
FBSDE is wellposed and thus the game has a unique equilibrium. This is the
first uniqueness result in the literature, without restricting the equilibria
to certain special structure. Moreover, this unique equilibrium may not be
Markovian, indicating that the PDE approach cannot work in this case. We next
study the set value of the game, which roughly speaking is the set of insider's
values over all equilibria and thus is by nature unique. We show that, although
the bridge type of equilibria in the literature does not satisfy the required
integrability for our equilibria, its truncation serves as a desired
approximate equilibrium and its value belongs to our set value. Finally, we
characterize our set value through a level set of certain standard HJB
equation.

</details>


### [404] [Understanding Lookahead Dynamics Through Laplace Transform](https://arxiv.org/abs/2506.13712)
*Aniket Sanyal,Tatjana Chavdarova*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a frequency-domain framework for convergence analysis of
hyperparameters in game optimization, leveraging High-Resolution Differential
Equations (HRDEs) and Laplace transforms. Focusing on the Lookahead
algorithm--characterized by gradient steps $k$ and averaging coefficient
$\alpha$--we transform the discrete-time oscillatory dynamics of bilinear games
into the frequency domain to derive precise convergence criteria. Our
higher-precision $O(\gamma^2)$-HRDE models yield tighter criteria, while our
first-order $O(\gamma)$-HRDE models offer practical guidance by prioritizing
actionable hyperparameter tuning over complex closed-form solutions. Empirical
validation in discrete-time settings demonstrates the effectiveness of our
approach, which may further extend to locally linear operators, offering a
scalable framework for selecting hyperparameters for learning in games.

</details>


### [405] [Adjusted Shuffling SARAH: Advancing Complexity Analysis via Dynamic Gradient Weighting](https://arxiv.org/abs/2506.12444)
*Duc Toan Nguyen,Trang H. Tran,Lam M. Nguyen*

Main category: math.OC

TL;DR: 提出Adjusted Shuffling SARAH算法及不精确变体Inexact Adjusted Reshuffling SARAH，前者在强凸设置中达到洗牌方差减少方法最佳梯度复杂度，后者无需全批量梯度计算且在大样本时有复杂度优势。


<details>
  <summary>Details</summary>
Motivation: 改进方差减少算法，提升探索能力，缩小均匀采样和洗牌数据方差减少方法复杂度分析差距，同时避免全批量梯度计算。

Method: 将洗牌技术与SARAH算法结合，动态调整随机梯度权重；引入Inexact Adjusted Reshuffling SARAH，无需全批量梯度计算。

Result: Adjusted Shuffling SARAH达到强凸设置中洗牌方差减少方法最佳梯度复杂度；Inexact Adjusted Reshuffling SARAH保留相同线性收敛率，大样本时总复杂度有优势。

Conclusion: 提出的算法在梯度复杂度和计算效率上有优势，适用于不同场景。

Abstract: In this paper, we propose Adjusted Shuffling SARAH, a novel algorithm that
integrates shuffling techniques with the well-known variance-reduced algorithm
SARAH while dynamically adjusting the stochastic gradient weights in each
update to enhance exploration. Our method achieves the best-known gradient
complexity for shuffling variance reduction methods in a strongly convex
setting. This result applies to any shuffling technique, which narrows the gap
in the complexity analysis of variance reduction methods between uniform
sampling and shuffling data. Furthermore, we introduce Inexact Adjusted
Reshuffling SARAH, an inexact variant of Adjusted Shuffling SARAH that
eliminates the need for full-batch gradient computations. This algorithm
retains the same linear convergence rate as Adjusted Shuffling SARAH while
showing an advantage in total complexity when the sample size is very large.

</details>


### [406] [Glocal Smoothness: Line Search can really help!](https://arxiv.org/abs/2506.12648)
*Curtis Fox,Aaron Mishkin,Sharan Vaswani,Mark Schmidt*

Main category: math.OC

TL;DR: 论文提出全局与局部（glocal）平滑性的简单表征，可对比不同算法迭代复杂度，还展示了线搜索优势及glocal平滑性对多种算法复杂度的改进。


<details>
  <summary>Details</summary>
Motivation: 现有局部Lipschitz假设下迭代复杂度依赖算法迭代，难以比较不同方法迭代复杂度，需要改进。

Method: 提出仅依赖函数性质的glocal平滑性表征。

Result: 能以与迭代无关的常数给出迭代复杂度上界，表明线搜索优于固定步长，某些情况下带线搜索的梯度下降迭代复杂度优于固定步长加速方法，glocal平滑性改进了多种算法复杂度。

Conclusion: glocal平滑性的提出有助于更方便地比较不同算法迭代复杂度，并改进多种算法复杂度。

Abstract: Iteration complexities for first-order optimization algorithms are typically
stated in terms of a global Lipschitz constant of the gradient, and
near-optimal results are achieved using fixed step sizes. But many objective
functions that arise in practice have regions with small Lipschitz constants
where larger step sizes can be used. Many local Lipschitz assumptions have been
proposed, which have lead to results showing that adaptive step sizes and/or
line searches yield improved convergence rates over fixed step sizes. However,
these faster rates tend to depend on the iterates of the algorithm, which makes
it difficult to compare the iteration complexities of different methods. We
consider a simple characterization of global and local ("glocal") smoothness
that only depends on properties of the function. This allows upper bounds on
iteration complexities in terms of iterate-independent constants and enables us
to compare iteration complexities between algorithms. Under this assumption it
is straightforward to show the advantages of line searches over fixed step
sizes, and that in some settings, gradient descent with line search has a
better iteration complexity than accelerated methods with fixed step sizes. We
further show that glocal smoothness can lead to improved complexities for the
Polyak and AdGD step sizes, as well other algorithms including coordinate
optimization, stochastic gradient methods, accelerated gradient methods, and
non-linear conjugate gradient methods.

</details>


### [407] [Balancing Intensity and Focality in Directional DBS Under Uncertainty: A Simulation Study of Electrode Optimization via a Metaheuristic L1L1 Approach](https://arxiv.org/abs/2506.13452)
*Fernando Galaz Prieto,Antti Lassila,Maryam Samavaki,Sampsa Pursiainen*

Main category: math.OC

TL;DR: 本文采用L1L1方法改进电极接触配置选择，考虑了先验导联场不确定性，经数值实验验证该方法在聚焦刺激电流、减少非预期电流扩散上有竞争力，获得了抗噪声的电流转向框架。


<details>
  <summary>Details</summary>
Motivation: 随着DBS技术发展，为改进电极接触配置选择，利用L1L1方法解决传统方法未考虑导联场不确定性的问题。

Method: 优化框架通过基于导联场衰减约束解空间来纳入不确定性，将该方法应用于8和40接触电极配置，在离散有限元模型中优化电流分布，用数值实验验证L1L1方法。

Result: L1L1方法能成功拟合和正则化目标结构的电流分布，通过超参数优化提取双极或多极电极配置，在聚焦电流密度和高增益场比上表现良好，尤其在噪声条件下比传统方法更能集中刺激电流、减少非预期扩散。

Conclusion: 将不确定性直接纳入优化过程，可获得抗噪声的电流转向框架，适应导联场模型和模拟参数的变化。

Abstract: As DBS technology advances toward directional leads and optimization-based
current steering, this study aims to improve the selection of electrode contact
configurations using the recently developed L1-norm regularized L1-norm fitting
(L1L1) method. The focus is in particular on L1L1's capability to incorporate a
priori lead field uncertainty, offering a potential advantage over conventional
approaches that do not account for such variability. Our optimization framework
incorporates uncertainty by constraining the solution space based on lead field
attenuation. This reflects physiological expectations about the VTA and serves
to avoid overfitting. By applying this method to 8- and 40-contact electrode
configurations, we optimize current distributions within a discretized finite
element (FE) model, focusing on the lead field's characteristics. The model
accounts for uncertainty through these explicit constraints, enhancing the
feasibility, focality, and robustness of the resulting solutions. The L1L1
method was validated through a series of numerical experiments using both
noiseless and noisy lead fields, where the noise level was selected to reflect
attenuation within VTA. It successfully fits and regularizes the current
distribution across target structures, with hyperparameter optimization
extracting either bipolar or multipolar electrode configurations. These
configurations aim to maximize focused current density or prioritize a high
gain field ratio in a discretized FE model. Compared to traditional methods,
the L1L1 approach showed competitive performance in concentrating stimulation
within the target region while minimizing unintended current spread,
particularly under noisy conditions. By incorporating uncertainty directly into
the optimization process, we obtain a noise-robust framework for current
steering, allowing for variations in lead field models and simulation
parameters.

</details>


### [408] [Gradient-Normalized Smoothness for Optimization with Approximate Hessians](https://arxiv.org/abs/2506.13710)
*Andrei Semenov,Martin Jaggi,Nikita Doikov*

Main category: math.OC

TL;DR: 开发结合近似二阶信息与梯度正则化技术的优化算法，提出梯度归一化平滑性概念，有通用全局收敛保证，在多类问题有应用。


<details>
  <summary>Details</summary>
Motivation: 为凸和非凸目标函数实现快速全局收敛率。

Method: 开发使用近似二阶信息和梯度正则化技术的优化算法，引入梯度归一化平滑性概念。

Result: 算法具有通用全局收敛保证，能恢复多类函数的最优收敛率，且能自动实现并扩展到更广泛的函数类。

Conclusion: 算法可应用于逻辑回归、softmax问题和非凸优化等场景。

Abstract: In this work, we develop new optimization algorithms that use approximate
second-order information combined with the gradient regularization technique to
achieve fast global convergence rates for both convex and non-convex
objectives. The key innovation of our analysis is a novel notion called
Gradient-Normalized Smoothness, which characterizes the maximum radius of a
ball around the current point that yields a good relative approximation of the
gradient field. Our theory establishes a natural intrinsic connection between
Hessian approximation and the linearization of the gradient. Importantly,
Gradient-Normalized Smoothness does not depend on the specific problem class of
the objective functions, while effectively translating local information about
the gradient field and Hessian approximation into the global behavior of the
method. This new concept equips approximate second-order algorithms with
universal global convergence guarantees, recovering state-of-the-art rates for
functions with H\"older-continuous Hessians and third derivatives,
quasi-self-concordant functions, as well as smooth classes in first-order
optimization. These rates are achieved automatically and extend to broader
classes, such as generalized self-concordant functions. We demonstrate direct
applications of our results for global linear rates in logistic regression and
softmax problems with approximate Hessians, as well as in non-convex
optimization using Fisher and Gauss-Newton approximations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [409] [Sparse Convolutional Recurrent Learning for Efficient Event-based Neuromorphic Object Detection](https://arxiv.org/abs/2506.13440)
*Shenqi Wang,Yingfu Xu,Amirreza Yousefzadeh,Sherif Eissa,Henk Corporaal,Federico Corradi,Guangzhi Tang*

Main category: cs.CV

TL;DR: 提出用于神经形态处理器的基于事件的高效目标检测方法SEED，在效率和准确率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件相机的目标检测在处理稀疏事件数据时计算密集，难以集成到资源受限的边缘应用中。

Method: 提出Sparse Event-based Efficient Detector (SEED)，引入稀疏卷积循环学习。

Result: 在数据集上验证，SEED在基于事件的目标检测计算效率上创了新基准，减少突触操作，mAP相当或更高。硬件模拟展示其硬件感知设计对节能和低延迟神经形态处理的重要性。

Conclusion: SEED是一种高效的基于事件的目标检测方法，适用于神经形态处理器。

Abstract: Leveraging the high temporal resolution and dynamic range, object detection
with event cameras can enhance the performance and safety of automotive and
robotics applications in real-world scenarios. However, processing sparse event
data requires compute-intensive convolutional recurrent units, complicating
their integration into resource-constrained edge applications. Here, we propose
the Sparse Event-based Efficient Detector (SEED) for efficient event-based
object detection on neuromorphic processors. We introduce sparse convolutional
recurrent learning, which achieves over 92% activation sparsity in recurrent
processing, vastly reducing the cost for spatiotemporal reasoning on sparse
event data. We validated our method on Prophesee's 1 Mpx and Gen1 event-based
object detection datasets. Notably, SEED sets a new benchmark in computational
efficiency for event-based object detection which requires long-term temporal
learning. Compared to state-of-the-art methods, SEED significantly reduces
synaptic operations while delivering higher or same-level mAP. Our hardware
simulations showcase the critical role of SEED's hardware-aware design in
achieving energy-efficient and low-latency neuromorphic processing.

</details>


### [410] [GroupNL: Low-Resource and Robust CNN Design over Cloud and Device](https://arxiv.org/abs/2506.12335)
*Chuntao Ding,Jianhang Xie,Junna Zhang,Salman Raza,Shangguang Wang,Jiannong Cao*

Main category: cs.CV

TL;DR: 本文提出GroupNL方法，利用NLFs生成多样化特征图，提升CNN模型鲁棒性，减少资源消耗，实验表明其在模型鲁棒性和训练加速上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有在物联网设备上部署CNN模型的方法存在处理损坏图像数据时鲁棒性低、计算和传输资源消耗高的问题。

Method: 提出Grouped NonLinear transformation generation method (GroupNL)，指定部分卷积滤波器为种子滤波器，生成种子特征图，将其分组并用不同NLFs生成多样化特征图，设置NLFs超参数随机初始化且不更新。

Result: 在多个数据集和NVIDIA RTX GPU平台实验显示，GroupNL在模型鲁棒性和训练加速上优于其他方法，如在Icons - 50数据集上，GroupNL - ResNet - 18准确率比普通ResNet - 18高约2.86%，在ImageNet - 1K数据集上训练速度比普通CNN快约53%。

Conclusion: 提出的GroupNL方法在模型鲁棒性和训练加速方面表现出色，优于现有方法。

Abstract: It has become mainstream to deploy Convolutional Neural Network (CNN) models
on ubiquitous Internet of Things (IoT) devices with the help of the cloud to
provide users with a variety of high-quality services. Most existing methods
have two limitations: (i) low robustness in handling corrupted image data
collected by IoT devices; and (ii) high consumption of computational and
transmission resources. To this end, we propose the Grouped NonLinear
transformation generation method (GroupNL), which generates diversified feature
maps by utilizing data-agnostic Nonlinear Transformation Functions (NLFs) to
improve the robustness of the CNN model. Specifically, partial convolution
filters are designated as seed filters in a convolutional layer, and a small
set of feature maps, i.e., seed feature maps, are first generated based on
vanilla convolution operation. Then, we split seed feature maps into several
groups, each with a set of different NLFs, to generate corresponding diverse
feature maps with in-place nonlinear processing. Moreover, GroupNL effectively
reduces the parameter transmission between multiple nodes during model training
by setting the hyperparameters of NLFs to random initialization and not
updating them during model training, and reduces the computing resources by
using NLFs to generate feature maps instead of most feature maps generated
based on sliding windows. Experimental results on CIFAR-10, GTSRB, CIFAR-10-C,
Icons50, and ImageNet-1K datasets in NVIDIA RTX GPU platforms show that the
proposed GroupNL outperforms other state-of-the-art methods in model robust and
training acceleration. Specifically, on the Icons-50 dataset, the accuracy of
GroupNL-ResNet-18 achieves approximately 2.86% higher than the vanilla
ResNet-18. GroupNL improves training speed by about 53% compared to vanilla CNN
when trained on a cluster of 8 NVIDIA RTX 4090 GPUs on the ImageNet-1K dataset.

</details>


### [411] [Cross-architecture universal feature coding via distribution alignment](https://arxiv.org/abs/2506.12737)
*Changsheng Gao,Shan Liu,Feng Wu,Weisi Lin*

Main category: cs.CV

TL;DR: 提出跨架构通用特征编码问题，用两步分布对齐方法解决，实验显示优于特定架构基线。


<details>
  <summary>Details</summary>
Motivation: 现有特征编码方法架构特定，在不同架构特征共存场景适用性受限。

Method: 提出两步分布对齐方法，包括格式对齐统一特征格式、特征值对齐协调统计分布。

Result: 在图像分类任务上实验，该方法实现了更优的率 - 准确率权衡。

Conclusion: 此工作是迈向跨异构模型架构通用特征压缩的第一步。

Abstract: Feature coding has become increasingly important in scenarios where semantic
representations rather than raw pixels are transmitted and stored. However,
most existing methods are architecture-specific, targeting either CNNs or
Transformers. This design limits their applicability in real-world scenarios
where features from both architectures coexist. To address this gap, we
introduce a new research problem: cross-architecture universal feature coding
(CAUFC), which seeks to build a unified codec that can effectively compress
features from heterogeneous architectures. To tackle this challenge, we propose
a two-step distribution alignment method. First, we design the format alignment
method that unifies CNN and Transformer features into a consistent 2D token
format. Second, we propose the feature value alignment method that harmonizes
statistical distributions via truncation and normalization. As a first attempt
to study CAUFC, we evaluate our method on the image classification task.
Experimental results demonstrate that our method achieves superior
rate-accuracy trade-offs compared to the architecture-specific baseline. This
work marks an initial step toward universal feature compression across
heterogeneous model architectures.

</details>


### [412] [Hierarchical Multi-Positive Contrastive Learning for Patent Image Retrieval](https://arxiv.org/abs/2506.13496)
*Kshitij Kavimandan,Angelos Nalmpantis,Emma Beauxis-Aussalet,Robert-Jan Sips*

Main category: cs.CV

TL;DR: 提出分层多正样本对比损失方法用于专利图像检索，实验显示该方法能提升检索结果，且对低参数模型有效。


<details>
  <summary>Details</summary>
Motivation: 现有专利图像检索方法忽略专利的层次关系，且因专利图像技术复杂和语义信息丰富，需有效微调以适应领域。

Method: 引入分层多正样本对比损失，利用Locarno国际分类系统的分类法，为批次内每个专利图像分配多个正样本对，并根据层次分类法赋予不同相似度分数。

Result: 在DeepPatent2数据集上，用多种视觉和多模态模型进行实验，该方法提升了检索结果，且对低参数模型有效。

Conclusion: 所提出的方法能有效提升专利图像检索效果，且适合计算资源有限的环境。

Abstract: Patent images are technical drawings that convey information about a patent's
innovation. Patent image retrieval systems aim to search in vast collections
and retrieve the most relevant images. Despite recent advances in information
retrieval, patent images still pose significant challenges due to their
technical intricacies and complex semantic information, requiring efficient
fine-tuning for domain adaptation. Current methods neglect patents'
hierarchical relationships, such as those defined by the Locarno International
Classification (LIC) system, which groups broad categories (e.g., "furnishing")
into subclasses (e.g., "seats" and "beds") and further into specific patent
designs. In this work, we introduce a hierarchical multi-positive contrastive
loss that leverages the LIC's taxonomy to induce such relations in the
retrieval process. Our approach assigns multiple positive pairs to each patent
image within a batch, with varying similarity scores based on the hierarchical
taxonomy. Our experimental analysis with various vision and multimodal models
on the DeepPatent2 dataset shows that the proposed method enhances the
retrieval results. Notably, our method is effective with low-parameter models,
which require fewer computational resources and can be deployed on environments
with limited hardware.

</details>


### [413] [BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a Transformer Implementation for Breast Cancer Treatment Response Prediction](https://arxiv.org/abs/2506.12190)
*Naomi Fridman,Bubby Solway,Tomer Fridman,Itamar Barnea,Anat Goldshtein*

Main category: cs.CV

TL;DR: 提出乳腺癌数据集BreastDCEDL，开发基于ViT的模型实现高AUC和准确率的pCR预测。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌死亡率高，早期检测和治疗反应监测至关重要，但缺乏可访问的多中心公开数据集。

Method: 将原始DICOM成像数据转换为标准化3D NIfTI体积，开发基于ViT架构的模型在三个对比阶段的RGB融合图像上训练。

Result: ViT模型在HR+/HER2 - 患者的pCR预测中达到AUC 0.94，准确率0.93。

Conclusion: BreastDCEDL可支持先进模型开发，为乳腺癌成像研究提供可重复框架。

Abstract: Breast cancer remains a leading cause of cancer-related mortality worldwide,
making early detection and accurate treatment response monitoring critical
priorities. We present BreastDCEDL, a curated, deep learning-ready dataset
comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from
2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts,
all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were
rigorously converted into standardized 3D NIfTI volumes with preserved signal
integrity, accompanied by unified tumor annotations and harmonized clinical
metadata including pathologic complete response (pCR), hormone receptor (HR),
and HER2 status. Although DCE-MRI provides essential diagnostic information and
deep learning offers tremendous potential for analyzing such complex data,
progress has been limited by lack of accessible, public, multicenter datasets.
BreastDCEDL addresses this gap by enabling development of advanced models,
including state-of-the-art transformer architectures that require substantial
training data. To demonstrate its capacity for robust modeling, we developed
the first transformer-based model for breast DCE-MRI, leveraging Vision
Transformer (ViT) architecture trained on RGB-fused images from three contrast
phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT
model achieved state-of-the-art pCR prediction performance in HR+/HER2-
patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark
splits, offering a framework for reproducible research and enabling clinically
meaningful modeling in breast cancer imaging.

</details>


### [414] [Three-dimensional Deep Shape Optimization with a Limited Dataset](https://arxiv.org/abs/2506.12326)
*Yongmin Kwon,Namwoo Kang*

Main category: cs.CV

TL;DR: 本文提出适用于有限数据集形状优化的深度学习框架，实验证明其在处理传统优化框架局限性上有效且通用。


<details>
  <summary>Details</summary>
Motivation: 生成模型在机械设计应用受限于数据集大小和可变性，需提出新方法应对。

Method: 提出基于深度学习的优化框架，利用位置编码和Lipschitz正则化项学习几何特征和维持有意义的潜在空间。

Result: 通过对轮子、汽车等三维数据集进行多目标形状优化实验，证明该方法具有鲁棒性、泛化性和有效性。

Conclusion: 该模型在数据受限条件下也能产生实用且高质量的设计结果，具有通用性。

Abstract: Generative models have attracted considerable attention for their ability to
produce novel shapes. However, their application in mechanical design remains
constrained due to the limited size and variability of available datasets. This
study proposes a deep learning-based optimization framework specifically
tailored for shape optimization with limited datasets, leveraging positional
encoding and a Lipschitz regularization term to robustly learn geometric
characteristics and maintain a meaningful latent space. Through extensive
experiments, the proposed approach demonstrates robustness, generalizability
and effectiveness in addressing typical limitations of conventional
optimization frameworks. The validity of the methodology is confirmed through
multi-objective shape optimization experiments conducted on diverse
three-dimensional datasets, including wheels and cars, highlighting the model's
versatility in producing practical and high-quality design outcomes even under
data-constrained conditions.

</details>


### [415] [LARGO: Low-Rank Regulated Gradient Projection for Robust Parameter Efficient Fine-Tuning](https://arxiv.org/abs/2506.12394)
*Haotian Zhang,Liu Liu,Baosheng Yu,Jiayan Qiu,Yanwei Ren,Xianglong Liu*

Main category: cs.CV

TL;DR: 提出LARGO算法解决现有参数高效微调方法在领域偏移下难以兼顾性能与计算效率的问题，实验显示其性能优异且代码即将开源。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法在领域偏移下难以兼顾鲁棒性能和计算效率。

Method: 提出LARGO算法，将动态约束融入低秩自适应方法，采用并行可训练梯度投影调节层更新，利用预训练权重的奇异值分解进行结构化初始化。

Result: 在多个基准测试中，LARGO在域内和分布外场景均达到了最先进的性能，与现有PEFT方法相比，在领域偏移下的鲁棒性提高，计算开销显著降低。

Conclusion: LARGO算法有效解决了领域偏移下参数高效微调的问题，具有良好的性能和计算效率。

Abstract: The advent of parameter-efficient fine-tuning methods has significantly
reduced the computational burden of adapting large-scale pretrained models to
diverse downstream tasks. However, existing approaches often struggle to
achieve robust performance under domain shifts while maintaining computational
efficiency. To address this challenge, we propose Low-rAnk Regulated Gradient
Projection (LARGO) algorithm that integrates dynamic constraints into low-rank
adaptation methods. Specifically, LARGO incorporates parallel trainable
gradient projections to dynamically regulate layer-wise updates, retaining the
Out-Of-Distribution robustness of pretrained model while preserving inter-layer
independence. Additionally, it ensures computational efficiency by mitigating
the influence of gradient dependencies across layers during weight updates.
Besides, through leveraging singular value decomposition of pretrained weights
for structured initialization, we incorporate an SVD-based initialization
strategy that minimizing deviation from pretrained knowledge. Through extensive
experiments on diverse benchmarks, LARGO achieves state-of-the-art performance
across in-domain and out-of-distribution scenarios, demonstrating improved
robustness under domain shifts with significantly lower computational overhead
compared to existing PEFT methods. The source code will be released soon.

</details>


### [416] [MS-UMamba: An Improved Vision Mamba Unet for Fetal Abdominal Medical Image Segmentation](https://arxiv.org/abs/2506.12441)
*Caixu Xu,Junming Wei,Huizhen Chen,Pengchen Liang,Bocheng Liang,Ying Tan,Xintong Wei*

Main category: cs.CV

TL;DR: 提出用于胎儿超声图像分割的MS - UMamba模型，实验表明其分割性能出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于Mamba的方法在胎儿超声图像分割中面临封闭解剖结构、边界模糊等挑战，需要平衡局部特征提取和全局上下文建模。

Method: 提出MS - UMamba，设计集成CNN分支的视觉状态空间块（SS - MCAT - SSM），并提出集成空间注意力机制的多尺度特征融合模块。

Result: 在非公开数据集上进行大量实验，MS - UMamba模型分割性能出色。

Conclusion: MS - UMamba模型在胎儿超声图像分割任务中表现优秀。

Abstract: Recently, Mamba-based methods have become popular in medical image
segmentation due to their lightweight design and long-range dependency modeling
capabilities. However, current segmentation methods frequently encounter
challenges in fetal ultrasound images, such as enclosed anatomical structures,
blurred boundaries, and small anatomical structures. To address the need for
balancing local feature extraction and global context modeling, we propose
MS-UMamba, a novel hybrid convolutional-mamba model for fetal ultrasound image
segmentation. Specifically, we design a visual state space block integrated
with a CNN branch (SS-MCAT-SSM), which leverages Mamba's global modeling
strengths and convolutional layers' local representation advantages to enhance
feature learning. In addition, we also propose an efficient multi-scale feature
fusion module that integrates spatial attention mechanisms, which Integrating
feature information from different layers enhances the feature representation
ability of the model. Finally, we conduct extensive experiments on a non-public
dataset, experimental results demonstrate that MS-UMamba model has excellent
performance in segmentation performance.

</details>


### [417] [Comparative Analysis of Deep Learning Strategies for Hypertensive Retinopathy Detection from Fundus Images: From Scratch and Pre-trained Models](https://arxiv.org/abs/2506.12492)
*Yanqiao Zhu*

Main category: cs.CV

TL;DR: 本文对比分析了从眼底图像检测高血压性视网膜病变的深度学习策略，探讨不同模型在数据增强下的表现。


<details>
  <summary>Details</summary>
Motivation: 解决HRDC挑战中从眼底图像检测高血压性视网膜病变的任务，研究模型架构、数据增强和数据集大小的相互作用。

Method: 研究三种方法：自定义CNN、预训练基于Transformer的模型和AutoML解决方案，测试不同模型在数据增强前后的表现。

Result: 数据增强对纯ViTs有提升效果，对混合ViT - CNN模型有负面影响；小patch尺寸在增强数据上表现好；强大的自监督模型DINOv2在数据增强后表现改善；ViT - Large在小数据集上表现差。

Conclusion: 为医学图像分类中模型架构、数据增强和数据集大小的相互作用提供了关键见解。

Abstract: This paper presents a comparative analysis of deep learning strategies for
detecting hypertensive retinopathy from fundus images, a central task in the
HRDC challenge~\cite{qian2025hrdc}. We investigate three distinct approaches: a
custom CNN, a suite of pre-trained transformer-based models, and an AutoML
solution. Our findings reveal a stark, architecture-dependent response to data
augmentation. Augmentation significantly boosts the performance of pure Vision
Transformers (ViTs), which we hypothesize is due to their weaker inductive
biases, forcing them to learn robust spatial and structural features.
Conversely, the same augmentation strategy degrades the performance of hybrid
ViT-CNN models, whose stronger, pre-existing biases from the CNN component may
be "confused" by the transformations. We show that smaller patch sizes
(ViT-B/8) excel on augmented data, enhancing fine-grained detail capture.
Furthermore, we demonstrate that a powerful self-supervised model like DINOv2
fails on the original, limited dataset but is "rescued" by augmentation,
highlighting the critical need for data diversity to unlock its potential.
Preliminary tests with a ViT-Large model show poor performance, underscoring
the risk of using overly-capacitive models on specialized, smaller datasets.
This work provides critical insights into the interplay between model
architecture, data augmentation, and dataset size for medical image
classification.

</details>


### [418] [Efficient Multi-Camera Tokenization with Triplanes for End-to-End Driving](https://arxiv.org/abs/2506.12251)
*Boris Ivanovic,Cristiano Saltori,Yurong You,Yan Wang,Wenjie Luo,Marco Pavone*

Main category: cs.CV

TL;DR: 提出高效的基于三平面的多摄像头标记策略，可减少标记数量、加快策略推理速度并保证性能。


<details>
  <summary>Details</summary>
Motivation: 自回归Transformer用作机器人和自动驾驶汽车策略架构时，高效标记传感器数据对嵌入式硬件实时运行很关键。

Method: 提出基于三平面的多摄像头标记策略，利用3D神经重建和渲染技术生成与输入摄像头数量和分辨率无关的传感器标记。

Result: 在大规模自动驾驶数据集和神经模拟器上实验表明，比当前基于图像块的标记策略节省显著，标记减少72%，推理速度加快50%，且运动规划精度相同，闭环驾驶模拟越野率提升。

Conclusion: 所提标记策略有效且高效，能提升自回归Transformer在自动驾驶领域应用的实时性能。

Abstract: Autoregressive Transformers are increasingly being deployed as end-to-end
robot and autonomous vehicle (AV) policy architectures, owing to their
scalability and potential to leverage internet-scale pretraining for
generalization. Accordingly, tokenizing sensor data efficiently is paramount to
ensuring the real-time feasibility of such architectures on embedded hardware.
To this end, we present an efficient triplane-based multi-camera tokenization
strategy that leverages recent advances in 3D neural reconstruction and
rendering to produce sensor tokens that are agnostic to the number of input
cameras and their resolution, while explicitly accounting for their geometry
around an AV. Experiments on a large-scale AV dataset and state-of-the-art
neural simulator demonstrate that our approach yields significant savings over
current image patch-based tokenization strategies, producing up to 72% fewer
tokens, resulting in up to 50% faster policy inference while achieving the same
open-loop motion planning accuracy and improved offroad rates in closed-loop
driving simulations.

</details>


### [419] [MVP-CBM:Multi-layer Visual Preference-enhanced Concept Bottleneck Model for Explainable Medical Image Classification](https://arxiv.org/abs/2506.12568)
*Chunjiang Wang,Kun Zhang,Yandong Liu,Zhiyang He,Xiaodong Tao,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 提出多層視覺偏好增強概念瓶頸模型（MVP - CBM），可利用多層視覺信息解釋模型決策，在醫學分類基準上效果優異。


<details>
  <summary>Details</summary>
Motivation: 現有基於概念瓶頸模型（CBM）的方法僅將視覺編碼器的最後一層與概念關聯，忽略了概念偏好變化，削弱了特徵與概念的對應關係，影響模型可解釋性。

Method: 提出MVP - CBM，包含層內概念偏好建模和多層概念稀疏激活融合兩個模塊，分別用於捕捉不同概念與不同視覺層特徵的偏好關聯和聚合多層概念激活。

Result: 在多個公共醫學分類基準上的大量實驗表明，MVP - CBM達到了最優的準確率和可解釋性。

Conclusion: MVP - CBM通過顯式建模概念偏好，能綜合利用多層視覺信息，對模型決策給出更細膩、準確的解釋，具有優越性。

Abstract: The concept bottleneck model (CBM), as a technique improving interpretability
via linking predictions to human-understandable concepts, makes high-risk and
life-critical medical image classification credible. Typically, existing CBM
methods associate the final layer of visual encoders with concepts to explain
the model's predictions. However, we empirically discover the phenomenon of
concept preference variation, that is, the concepts are preferably associated
with the features at different layers than those only at the final layer; yet a
blind last-layer-based association neglects such a preference variation and
thus weakens the accurate correspondences between features and concepts,
impairing model interpretability. To address this issue, we propose a novel
Multi-layer Visual Preference-enhanced Concept Bottleneck Model (MVP-CBM),
which comprises two key novel modules: (1) intra-layer concept preference
modeling, which captures the preferred association of different concepts with
features at various visual layers, and (2) multi-layer concept sparse
activation fusion, which sparsely aggregates concept activations from multiple
layers to enhance performance. Thus, by explicitly modeling concept
preferences, MVP-CBM can comprehensively leverage multi-layer visual
information to provide a more nuanced and accurate explanation of model
decisions. Extensive experiments on several public medical classification
benchmarks demonstrate that MVP-CBM achieves state-of-the-art accuracy and
interoperability, verifying its superiority. Code is available at
https://github.com/wcj6/MVP-CBM.

</details>


### [420] [MGDFIS: Multi-scale Global-detail Feature Integration Strategy for Small Object Detection](https://arxiv.org/abs/2506.12697)
*Yuxiang Wang,Xuecheng Bai,Boyu Hu,Chuanzhi Xu,Haodong Chen,Vera Chung,Tingxue Li*

Main category: cs.CV

TL;DR: 提出多尺度全局 - 细节特征集成策略MGDFIS用于无人机图像小目标检测，实验显示其性能优于现有方法，平衡了精度与资源使用。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度融合方法存在计算负担大、模糊细节等问题，难以在复杂场景中检测小目标，需要更好的解决方案。

Method: 提出MGDFIS，包含FusionLock - TSS注意力模块、全局 - 细节集成模块和动态像素注意力模块。

Result: 在VisDrone基准上的广泛实验表明，MGDFIS在不同骨干架构和检测框架下始终优于现有方法，精度和召回率高且推理时间短。

Conclusion: MGDFIS在精度和资源使用之间取得最佳平衡，为资源受限的无人机平台上的小目标检测提供了实用解决方案。

Abstract: Small object detection in UAV imagery is crucial for applications such as
search-and-rescue, traffic monitoring, and environmental surveillance, but it
is hampered by tiny object size, low signal-to-noise ratios, and limited
feature extraction. Existing multi-scale fusion methods help, but add
computational burden and blur fine details, making small object detection in
cluttered scenes difficult. To overcome these challenges, we propose the
Multi-scale Global-detail Feature Integration Strategy (MGDFIS), a unified
fusion framework that tightly couples global context with local detail to boost
detection performance while maintaining efficiency. MGDFIS comprises three
synergistic modules: the FusionLock-TSS Attention Module, which marries
token-statistics self-attention with DynamicTanh normalization to highlight
spectral and spatial cues at minimal cost; the Global-detail Integration
Module, which fuses multi-scale context via directional convolution and
parallel attention while preserving subtle shape and texture variations; and
the Dynamic Pixel Attention Module, which generates pixel-wise weighting maps
to rebalance uneven foreground and background distributions and sharpen
responses to true object regions. Extensive experiments on the VisDrone
benchmark demonstrate that MGDFIS consistently outperforms state-of-the-art
methods across diverse backbone architectures and detection frameworks,
achieving superior precision and recall with low inference time. By striking an
optimal balance between accuracy and resource usage, MGDFIS provides a
practical solution for small-object detection on resource-constrained UAV
platforms.

</details>


### [421] [Unsupervised Contrastive Learning Using Out-Of-Distribution Data for Long-Tailed Dataset](https://arxiv.org/abs/2506.12698)
*Cuong Manh Hoang,Yeejin Lee,Byeongkeun Kang*

Main category: cs.CV

TL;DR: 本文提出在长尾数据集上利用无标签离群数据进行自监督学习的方法，在四个公开数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中对象类别分布不平衡，需要在长尾数据集上进行自监督学习以学习平衡且可分离的表示用于下游任务。

Method: 先使用域内和采样的离群数据训练网络，通过反向传播伪语义判别损失和域判别损失；再在域内数据上通过无监督对比学习进一步优化网络，利用先前训练的网络作为引导网络。

Result: 在四个公开的长尾数据集上实验，所提方法优于先前的先进方法。

Conclusion: 所提在长尾数据集上利用离群数据的自监督学习方法有效。

Abstract: This work addresses the task of self-supervised learning (SSL) on a
long-tailed dataset that aims to learn balanced and well-separated
representations for downstream tasks such as image classification. This task is
crucial because the real world contains numerous object categories, and their
distributions are inherently imbalanced. Towards robust SSL on a
class-imbalanced dataset, we investigate leveraging a network trained using
unlabeled out-of-distribution (OOD) data that are prevalently available online.
We first train a network using both in-domain (ID) and sampled OOD data by
back-propagating the proposed pseudo semantic discrimination loss alongside a
domain discrimination loss. The OOD data sampling and loss functions are
designed to learn a balanced and well-separated embedding space. Subsequently,
we further optimize the network on ID data by unsupervised contrastive learning
while using the previously trained network as a guiding network. The guiding
network is utilized to select positive/negative samples and to control the
strengths of attractive/repulsive forces in contrastive learning. We also
distil and transfer its embedding space to the training network to maintain
balancedness and separability. Through experiments on four publicly available
long-tailed datasets, we demonstrate that the proposed method outperforms
previous state-of-the-art methods.

</details>


### [422] [NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models](https://arxiv.org/abs/2506.12706)
*Jiaming Zhang,Xin Wang,Xingjun Ma,Lingyu Qiu,Yu-Gang Jiang,Jitao Sang*

Main category: cs.CV

TL;DR: 提出用于多模态对抗提示调整的神经增强器框架（NAP - Tuning），在多种数据集和攻击类型上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言模型（VLMs）易受对抗攻击，存在安全隐患，需增强其对抗鲁棒性。

Method: 在之前Adversarial Prompt Tuning（AdvPT）基础上，将AdvPT从文本扩展到多模态提示，从单层扩展到多层提示架构，通过神经增强器方法进行架构级重新设计，采用令牌精炼器通过残差连接重建净化特征。

Result: 在各种数据集和攻击类型上显著优于现有方法，在AutoAttack基准下比最强基线有显著提升，如在ViT - B16上提升33.5%，在ViT - B32上提升33.0%，且保持有竞争力的干净准确率。

Conclusion: NAP - Tuning能有效增强VLMs的对抗鲁棒性。

Abstract: Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable
capabilities in understanding relationships between visual and textual data
through joint embedding spaces. Despite their effectiveness, these models
remain vulnerable to adversarial attacks, particularly in the image modality,
posing significant security concerns. Building upon our previous work on
Adversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to
enhance adversarial robustness in VLMs without extensive parameter training, we
present a significant extension by introducing the Neural Augmentor framework
for Multi-modal Adversarial Prompt Tuning (NAP-Tuning).Our key innovations
include: (1) extending AdvPT from text-only to multi-modal prompting across
both text and visual modalities, (2) expanding from single-layer to multi-layer
prompt architectures, and (3) proposing a novel architecture-level redesign
through our Neural Augmentor approach, which implements feature purification to
directly address the distortions introduced by adversarial attacks in feature
space. Our NAP-Tuning approach incorporates token refiners that learn to
reconstruct purified features through residual connections, allowing for
modality-specific and layer-specific feature correction.Comprehensive
experiments demonstrate that NAP-Tuning significantly outperforms existing
methods across various datasets and attack types. Notably, our approach shows
significant improvements over the strongest baselines under the challenging
AutoAttack benchmark, outperforming them by 33.5% on ViT-B16 and 33.0% on
ViT-B32 architectures while maintaining competitive clean accuracy.

</details>


### [423] [Exploring Audio Cues for Enhanced Test-Time Video Model Adaptation](https://arxiv.org/abs/2506.12481)
*Runhao Zeng,Qi Deng,Ronghao Zhang,Shuaicheng Niu,Jian Chen,Xiping Hu,Victor C. M. Leung*

Main category: cs.CV

TL;DR: 提出将音频信息融入视频测试时自适应（TTA）的新方法，通过生成音频辅助伪标签和灵活适应周期提升性能，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有视频TTA方法多忽视音频数据，为填补此空白，将音频信息融入视频TTA。

Method: 提出音频到视频的标签映射方法，用预训练音频模型分类音频信号，通过大语言模型映射到视频标签空间；提出灵活适应周期，根据损失和一致性变化确定每个样本的最佳适应迭代次数。

Result: 在多个数据集上实验表明，该方法在不同视频分类模型中持续提高适应性能。

Conclusion: 该方法是将音频信息融入视频TTA的重要进展。

Abstract: Test-time adaptation (TTA) aims to boost the generalization capability of a
trained model by conducting self-/unsupervised learning during the testing
phase. While most existing TTA methods for video primarily utilize visual
supervisory signals, they often overlook the potential contribution of inherent
audio data. To address this gap, we propose a novel approach that incorporates
audio information into video TTA. Our method capitalizes on the rich semantic
content of audio to generate audio-assisted pseudo-labels, a new concept in the
context of video TTA. Specifically, we propose an audio-to-video label mapping
method by first employing pre-trained audio models to classify audio signals
extracted from videos and then mapping the audio-based predictions to video
label spaces through large language models, thereby establishing a connection
between the audio categories and video labels. To effectively leverage the
generated pseudo-labels, we present a flexible adaptation cycle that determines
the optimal number of adaptation iterations for each sample, based on changes
in loss and consistency across different views. This enables a customized
adaptation process for each sample. Experimental results on two widely used
datasets (UCF101-C and Kinetics-Sounds-C), as well as on two newly constructed
audio-video TTA datasets (AVE-C and AVMIT-C) with various corruption types,
demonstrate the superiority of our approach. Our method consistently improves
adaptation performance across different video classification models and
represents a significant step forward in integrating audio information into
video TTA. Code: https://github.com/keikeiqi/Audio-Assisted-TTA.

</details>


### [424] [SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration](https://arxiv.org/abs/2506.12723)
*Ye Li,Yuan Meng,Zewen Sun,Kangye Ji,Chen Tang,Jiajun Fan,Xinzhu Ma,Shutao Xia,Zhi Wang,Wenwu Zhu*

Main category: cs.CV

TL;DR: 提出SP - VLA框架加速VLA模型，结合模型调度和令牌剪枝，实验显示可实现加速且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型计算成本高、执行频率低，不适用于实时任务，且现有加速方法未解决顺序决策环境中的时空冗余问题。

Method: 提出统一框架SP - VLA，设计动作感知模型调度机制减少时间冗余，开发时空语义双感知令牌剪枝方法减少空间冗余。

Result: 实验表明该方法能实现高达1.5倍加速，精度下降小于3%，在多任务中优于现有方法。

Conclusion: 所提方法可有效加速VLA模型，同时保持较高精度。

Abstract: Vision-Language-Action (VLA) models have attracted increasing attention for
their strong control capabilities. However, their high computational cost and
low execution frequency hinder their suitability for real-time tasks such as
robotic manipulation and autonomous navigation. Existing VLA acceleration
methods primarily focus on structural optimization, overlooking the fact that
these models operate in sequential decision-making environments. As a result,
temporal redundancy in sequential action generation and spatial redundancy in
visual input remain unaddressed. To this end, we propose SP-VLA, a unified
framework that accelerates VLA models by jointly scheduling models and pruning
tokens. Specifically, we design an action-aware model scheduling mechanism that
reduces temporal redundancy by dynamically switching between VLA model and a
lightweight generator. Inspired by the human motion pattern of focusing on key
decision points while relying on intuition for other actions, we categorize VLA
actions into deliberative and intuitive, assigning the former to the VLA model
and the latter to the lightweight generator, enabling frequency-adaptive
execution through collaborative model scheduling. To address spatial
redundancy, we further develop a spatio-semantic dual-aware token pruning
method. Tokens are classified into spatial and semantic types and pruned based
on their dual-aware importance to accelerate VLA inference. These two
mechanisms work jointly to guide the VLA in focusing on critical actions and
salient visual information, achieving effective acceleration while maintaining
high accuracy. Experimental results demonstrate that our method achieves up to
1.5$\times$ acceleration with less than 3% drop in accuracy, outperforming
existing approaches in multiple tasks.

</details>


### [425] [Adaptive Dropout: Unleashing Dropout across Layers for Generalizable Image Super-Resolution](https://arxiv.org/abs/2506.12738)
*Hang Xu,Wei Yu,Jiangtong Tan,Zhen Zou,Feng Zhao*

Main category: cs.CV

TL;DR: 现有盲超分辨率方法存在过拟合问题，以往方法未正则化中间层特征，直接应用dropout性能下降。提出自适应Dropout方法，在数据集上表现优于以往方法，也适用于其他图像恢复任务。


<details>
  <summary>Details</summary>
Motivation: 解决盲超分辨率模型的过拟合问题，以及现有方法未对中间层特征进行正则化、直接应用dropout导致性能下降的问题。

Method: 提出自适应Dropout方法，重新设计dropout形式，自适应整合dropout前后特征；设计自适应训练策略，通过逐层退火加强特征传播。

Result: 该方法在合成和真实世界基准数据集上优于所有以往正则化方法，在其他图像恢复任务中也很有效。

Conclusion: 自适应Dropout方法能有效解决盲超分辨率模型的过拟合问题，提升模型性能，且具有一定通用性。

Abstract: Blind Super-Resolution (blind SR) aims to enhance the model's generalization
ability with unknown degradation, yet it still encounters severe overfitting
issues. Some previous methods inspired by dropout, which enhances
generalization by regularizing features, have shown promising results in blind
SR. Nevertheless, these methods focus solely on regularizing features before
the final layer and overlook the need for generalization in features at
intermediate layers. Without explicit regularization of features at
intermediate layers, the blind SR network struggles to obtain well-generalized
feature representations. However, the key challenge is that directly applying
dropout to intermediate layers leads to a significant performance drop, which
we attribute to the inconsistency in training-testing and across layers it
introduced. Therefore, we propose Adaptive Dropout, a new regularization method
for blind SR models, which mitigates the inconsistency and facilitates
application across intermediate layers of networks. Specifically, for
training-testing inconsistency, we re-design the form of dropout and integrate
the features before and after dropout adaptively. For inconsistency in
generalization requirements across different layers, we innovatively design an
adaptive training strategy to strengthen feature propagation by layer-wise
annealing. Experimental results show that our method outperforms all past
regularization methods on both synthetic and real-world benchmark datasets,
also highly effective in other image restoration tasks. Code is available at
\href{https://github.com/xuhang07/Adpative-Dropout}{https://github.com/xuhang07/Adpative-Dropout}.

</details>


### [426] [Inference-Time Gaze Refinement for Micro-Expression Recognition: Enhancing Event-Based Eye Tracking with Motion-Aware Post-Processing](https://arxiv.org/abs/2506.12524)
*Nuwan Bandara,Thivya Kandappu,Archan Misra*

Main category: cs.CV

TL;DR: 本文提出模型无关的推理时间细化框架提升事件驱动注视估计模型输出，提出抖动度量，改善事件驱动注视信号一致性，在多个基线模型上有提升。


<details>
  <summary>Details</summary>
Motivation: 事件驱动眼动追踪对细粒度认知状态推断有潜力，需提升现有事件驱动注视估计模型输出。

Method: 引入模型无关、推理时间细化框架，含运动感知中值滤波和基于光流的局部细化两个后处理模块，提出新的抖动度量。

Result: 在多个基线模型的受控数据集上取得一致改进。

Conclusion: 这些改进使事件驱动注视信号更适合下游任务，为未来与多模态情感识别系统集成奠定基础。

Abstract: Event-based eye tracking holds significant promise for fine-grained cognitive
state inference, offering high temporal resolution and robustness to motion
artifacts, critical features for decoding subtle mental states such as
attention, confusion, or fatigue. In this work, we introduce a model-agnostic,
inference-time refinement framework designed to enhance the output of existing
event-based gaze estimation models without modifying their architecture or
requiring retraining. Our method comprises two key post-processing modules: (i)
Motion-Aware Median Filtering, which suppresses blink-induced spikes while
preserving natural gaze dynamics, and (ii) Optical Flow-Based Local Refinement,
which aligns gaze predictions with cumulative event motion to reduce spatial
jitter and temporal discontinuities. To complement traditional spatial accuracy
metrics, we propose a novel Jitter Metric that captures the temporal smoothness
of predicted gaze trajectories based on velocity regularity and local signal
complexity. Together, these contributions significantly improve the consistency
of event-based gaze signals, making them better suited for downstream tasks
such as micro-expression analysis and mind-state decoding. Our results
demonstrate consistent improvements across multiple baseline models on
controlled datasets, laying the groundwork for future integration with
multimodal affect recognition systems in real-world environments.

</details>


### [427] [Unleashing Diffusion and State Space Models for Medical Image Segmentation](https://arxiv.org/abs/2506.12747)
*Rong Wu,Ziqi Chen,Liming Zhong,Heng Li,Hai Shu*

Main category: cs.CV

TL;DR: 提出DSM框架利用扩散和状态空间模型对训练数据外的未见肿瘤类别进行分割，实验显示其在肿瘤分割任务中性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有单数据集训练的分割模型对未见器官或肿瘤缺乏鲁棒性，开发能识别未见肿瘤类别的鲁棒模型对医学影像应用很关键。

Method: 提出DSM框架，用两组在修改的注意力解码器中训练的对象查询提高分类准确性，使用对象感知特征分组策略学习器官查询，通过基于扩散的视觉提示细化肿瘤查询，融入扩散引导特征融合，集成CLIP文本嵌入。

Result: 广泛实验表明DSM在各种肿瘤分割任务中有优越表现。

Conclusion: DSM框架能增强模型在不同场景和多标签任务中的鲁棒性，有效对未见肿瘤类别进行分割。

Abstract: Existing segmentation models trained on a single medical imaging dataset
often lack robustness when encountering unseen organs or tumors. Developing a
robust model capable of identifying rare or novel tumor categories not present
during training is crucial for advancing medical imaging applications. We
propose DSM, a novel framework that leverages diffusion and state space models
to segment unseen tumor categories beyond the training data. DSM utilizes two
sets of object queries trained within modified attention decoders to enhance
classification accuracy. Initially, the model learns organ queries using an
object-aware feature grouping strategy to capture organ-level visual features.
It then refines tumor queries by focusing on diffusion-based visual prompts,
enabling precise segmentation of previously unseen tumors. Furthermore, we
incorporate diffusion-guided feature fusion to improve semantic segmentation
performance. By integrating CLIP text embeddings, DSM captures
category-sensitive classes to improve linguistic transfer knowledge, thereby
enhancing the model's robustness across diverse scenarios and multi-label
tasks. Extensive experiments demonstrate the superior performance of DSM in
various tumor segmentation tasks. Code is available at
https://github.com/Rows21/KMax-Mamba.

</details>


### [428] [Performance Plateaus in Inference-Time Scaling for Text-to-Image Diffusion Without External Models](https://arxiv.org/abs/2506.12633)
*Changhyun Choi,Sungha Kim,H. Jin Kim*

Main category: cs.CV

TL;DR: 研究将Best - of - N推理时间缩放应用于无外部模型的扩散模型初始噪声优化算法，发现推理时间缩放性能易达平台期，少量优化步骤即可达最大性能。


<details>
  <summary>Details</summary>
Motivation: 此前研究优化文本到图像扩散模型初始噪声需外部模型评估结果，在小显存GPU上不可行。

Method: 将Best - of - N推理时间缩放应用于无外部模型的扩散模型初始噪声优化算法，跨多个数据集和主干进行实验。

Result: 文本到图像扩散模型的推理时间缩放很快达到性能平台期，少量优化步骤就能让各算法达到最大可实现性能。

Conclusion: 对于无外部模型的扩散模型初始噪声优化，少量优化步骤配合推理时间缩放就能达到较好效果。

Abstract: Recently, it has been shown that investing computing resources in searching
for good initial noise for a text-to-image diffusion model helps improve
performance. However, previous studies required external models to evaluate the
resulting images, which is impossible on GPUs with small VRAM. For these
reasons, we apply Best-of-N inference-time scaling to algorithms that optimize
the initial noise of a diffusion model without external models across multiple
datasets and backbones. We demonstrate that inference-time scaling for
text-to-image diffusion models in this setting quickly reaches a performance
plateau, and a relatively small number of optimization steps suffices to
achieve the maximum achievable performance with each algorithm.

</details>


### [429] [Scene-aware SAR ship detection guided by unsupervised sea-land segmentation](https://arxiv.org/abs/2506.12775)
*Han Ke,Xiao Ke,Ye Yan,Rui Liu,Jinpeng Yang,Tianwen Zhang,Xu Zhan,Xiaowo Xu*

Main category: cs.CV

TL;DR: 提出基于无监督海陆分割的场景感知SAR船舶检测方法，提升检测精度与模型可解释性，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 解决基于深度学习的合成孔径雷达（SAR）船舶检测中缺乏先验知识、影响检测精度的问题。

Method: 采用经典两阶段框架，通过无监督海陆分割模块（ULSM）和陆地注意力抑制模块（LASM）增强，ULSM对输入场景分类并进行海陆分割，LASM用分割信息减少网络对陆地的注意力。

Result: 使用公开的SSDD数据集进行实验，证明了网络的有效性。

Conclusion: 该方法能增加船舶检测的准确性，增强模型的可解释性。

Abstract: DL based Synthetic Aperture Radar (SAR) ship detection has tremendous
advantages in numerous areas. However, it still faces some problems, such as
the lack of prior knowledge, which seriously affects detection accuracy. In
order to solve this problem, we propose a scene-aware SAR ship detection method
based on unsupervised sea-land segmentation. This method follows a classical
two-stage framework and is enhanced by two models: the unsupervised land and
sea segmentation module (ULSM) and the land attention suppression module
(LASM). ULSM and LASM can adaptively guide the network to reduce attention on
land according to the type of scenes (inshore scene and offshore scene) and add
prior knowledge (sea land segmentation information) to the network, thereby
reducing the network's attention to land directly and enhancing offshore
detection performance relatively. This increases the accuracy of ship detection
and enhances the interpretability of the model. Specifically, in consideration
of the lack of land sea segmentation labels in existing deep learning-based SAR
ship detection datasets, ULSM uses an unsupervised approach to classify the
input data scene into inshore and offshore types and performs sea-land
segmentation for inshore scenes. LASM uses the sea-land segmentation
information as prior knowledge to reduce the network's attention to land. We
conducted our experiments using the publicly available SSDD dataset, which
demonstrated the effectiveness of our network.

</details>


### [430] [AS400-DET: Detection using Deep Learning Model for IBM i (AS/400)](https://arxiv.org/abs/2506.13032)
*Thanh Tran,Son T. Luu,Quan Bui,Shoshin Nomura*

Main category: cs.CV

TL;DR: 提出针对IBM i系统的GUI组件自动检测方法，引入标注数据集，用深度学习模型开发检测系统并评估，结果证明数据集有效，检测系统有自动测试潜力。


<details>
  <summary>Details</summary>
Motivation: 实现IBM i系统的GUI组件自动检测，并用于系统自动测试。

Method: 引入含1050张系统屏幕图像的人工标注数据集，基于深度学习模型开发检测系统并评估不同方法。

Result: 实验结果证明数据集在构建GUI屏幕组件检测系统方面有效。

Conclusion: AS400 - DET通过自动检测屏幕上的GUI组件，有对基于GUI屏幕操作的系统进行自动测试的潜力。

Abstract: This paper proposes a method for automatic GUI component detection for the
IBM i system (formerly and still more commonly known as AS/400). We introduce a
human-annotated dataset consisting of 1,050 system screen images, in which 381
images are screenshots of IBM i system screens in Japanese. Each image contains
multiple components, including text labels, text boxes, options, tables,
instructions, keyboards, and command lines. We then develop a detection system
based on state-of-the-art deep learning models and evaluate different
approaches using our dataset. The experimental results demonstrate the
effectiveness of our dataset in constructing a system for component detection
from GUI screens. By automatically detecting GUI components from the screen,
AS400-DET has the potential to perform automated testing on systems that
operate via GUI screens.

</details>


### [431] [Intriguing Frequency Interpretation of Adversarial Robustness for CNNs and ViTs](https://arxiv.org/abs/2506.12875)
*Lu Chen,Han Yang,Hu Wang,Yuxin Cao,Shaofeng Li,Yuan Luo*

Main category: cs.CV

TL;DR: 本文研究图像分类任务中对抗样本在频域的特性，发现不同网络架构对频率有不同偏好，频率分量差异影响模型鲁棒性，并给出三条建议。


<details>
  <summary>Details</summary>
Motivation: 过往对对抗样本基于频率的特性理解不足，本文旨在研究图像分类任务中对抗样本在频域的特性。

Method: 文中未明确提及具体研究方法。

Result: 1. 高频分量增加时，对抗样本和自然样本的性能差距更明显；2. 模型对过滤后对抗样本的性能先升后降至固有鲁棒性；3. CNN中对抗样本的中高频分量有攻击能力，Transformer中低频和中频分量更有效。

Conclusion: 不同网络架构有不同频率偏好，对抗和自然样本频率分量差异影响模型鲁棒性，给出三条对AI模型安全社区有价值的建议。

Abstract: Adversarial examples have attracted significant attention over the years, yet
understanding their frequency-based characteristics remains insufficient. In
this paper, we investigate the intriguing properties of adversarial examples in
the frequency domain for the image classification task, with the following key
findings. (1) As the high-frequency components increase, the performance gap
between adversarial and natural examples becomes increasingly pronounced. (2)
The model performance against filtered adversarial examples initially increases
to a peak and declines to its inherent robustness. (3) In Convolutional Neural
Networks, mid- and high-frequency components of adversarial examples exhibit
their attack capabilities, while in Transformers, low- and mid-frequency
components of adversarial examples are particularly effective. These results
suggest that different network architectures have different frequency
preferences and that differences in frequency components between adversarial
and natural examples may directly influence model robustness. Based on our
findings, we further conclude with three useful proposals that serve as a
valuable reference to the AI model security community.

</details>


### [432] [Beyond the First Read: AI-Assisted Perceptual Error Detection in Chest Radiography Accounting for Interobserver Variability](https://arxiv.org/abs/2506.13049)
*Adhrith Vutukuri,Akash Awasthi,David Yang,Carol C. Wu,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 介绍了RADAR系统，它可检测胸部X光片解读中的感知错误，评估显示能有效辅助放射科医生，还开源了相关代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 当前工作流程和AI系统在检测胸部X光片解读后感知错误方面支持有限，且缺乏有意义的人机协作。

Method: 引入RADAR系统，摄入放射科医生最终标注和CXR图像，进行区域级分析，以检测和推荐可能遗漏的异常区域，并在模拟感知错误数据集上评估，使用F1分数和IoU作为主要指标。

Result: 在模拟感知错误数据集上，召回率0.78，精度0.44，F1分数0.56；中位IoU为0.78，超90%推荐区域IoU超0.5。

Conclusion: RADAR有效补充放射科医生判断，为CXR解读中的感知错误检测提供有价值的读后支持，是有前途的工具，还开源方便进一步评估。

Abstract: Chest radiography is widely used in diagnostic imaging. However, perceptual
errors -- especially overlooked but visible abnormalities -- remain common and
clinically significant. Current workflows and AI systems provide limited
support for detecting such errors after interpretation and often lack
meaningful human--AI collaboration. We introduce RADAR (Radiologist--AI
Diagnostic Assistance and Review), a post-interpretation companion system.
RADAR ingests finalized radiologist annotations and CXR images, then performs
regional-level analysis to detect and refer potentially missed abnormal
regions. The system supports a "second-look" workflow and offers suggested
regions of interest (ROIs) rather than fixed labels to accommodate
inter-observer variation. We evaluated RADAR on a simulated perceptual-error
dataset derived from de-identified CXR cases, using F1 score and Intersection
over Union (IoU) as primary metrics. RADAR achieved a recall of 0.78, precision
of 0.44, and an F1 score of 0.56 in detecting missed abnormalities in the
simulated perceptual-error dataset. Although precision is moderate, this
reduces over-reliance on AI by encouraging radiologist oversight in human--AI
collaboration. The median IoU was 0.78, with more than 90% of referrals
exceeding 0.5 IoU, indicating accurate regional localization. RADAR effectively
complements radiologist judgment, providing valuable post-read support for
perceptual-error detection in CXR interpretation. Its flexible ROI suggestions
and non-intrusive integration position it as a promising tool in real-world
radiology workflows. To facilitate reproducibility and further evaluation, we
release a fully open-source web implementation alongside a simulated error
dataset. All code, data, demonstration videos, and the application are publicly
available at https://github.com/avutukuri01/RADAR.

</details>


### [433] [DualFast: Dual-Speedup Framework for Fast Sampling of Diffusion Models](https://arxiv.org/abs/2506.13058)
*Hu Yu,Hao Luo,Fan Wang,Feng Zhao*

Main category: cs.CV

TL;DR: 本文重新审视扩散概率模型采样误差本质，提出无训练加速框架DualFast，实验验证其能提升采样质量和速度。


<details>
  <summary>Details</summary>
Motivation: 扩散概率模型推理速度慢，现有快速采样器优化可能达到瓶颈，需进一步加速采样过程。

Method: 重新审视采样误差本质，将其分为离散误差和近似误差，实施双误差解耦策略，提出统一且无训练的加速框架DualFast。

Result: DualFast与现有采样器兼容，能显著提升采样质量和速度，尤其在极少采样步骤时。

Conclusion: 通过全面实验验证了DualFast在无条件和条件采样领域，像素空间和潜在空间DPMs中的有效性。

Abstract: Diffusion probabilistic models (DPMs) have achieved impressive success in
visual generation. While, they suffer from slow inference speed due to
iterative sampling. Employing fewer sampling steps is an intuitive solution,
but this will also introduces discretization error. Existing fast samplers make
inspiring efforts to reduce discretization error through the adoption of
high-order solvers, potentially reaching a plateau in terms of optimization.
This raises the question: can the sampling process be accelerated further? In
this paper, we re-examine the nature of sampling errors, discerning that they
comprise two distinct elements: the widely recognized discretization error and
the less explored approximation error. Our research elucidates the dynamics
between these errors and the step by implementing a dual-error disentanglement
strategy. Building on these foundations, we introduce an unified and
training-free acceleration framework, DualFast, designed to enhance the speed
of DPM sampling by concurrently accounting for both error types, thereby
minimizing the total sampling error. DualFast is seamlessly compatible with
existing samplers and significantly boost their sampling quality and speed,
particularly in extremely few sampling steps. We substantiate the effectiveness
of our framework through comprehensive experiments, spanning both unconditional
and conditional sampling domains, across both pixel-space and latent-space
DPMs.

</details>


### [434] [ZINA: Multimodal Fine-grained Hallucination Detection and Editing](https://arxiv.org/abs/2506.13130)
*Yuiga Wada,Kazuki Matsuda,Komei Sugiura,Graham Neubig*

Main category: cs.CV

TL;DR: 提出多模态细粒度幻觉检测与编辑任务及ZINA方法，构建VisionHall数据集，ZINA表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型常产生幻觉，需细粒度检测幻觉以进行全面评估和分析。

Method: 提出多模态细粒度幻觉检测与编辑任务；提出ZINA方法，细粒度识别幻觉片段、分类错误类型并建议改进；构建包含人工标注和合成样本的VisionHall数据集。

Result: ZINA在检测和编辑任务中优于现有方法，如GPT - 4o和LLama - 3.2。

Conclusion: ZINA方法在多模态细粒度幻觉检测与编辑方面有效。

Abstract: Multimodal Large Language Models (MLLMs) often generate hallucinations, where
the output deviates from the visual content. Given that these hallucinations
can take diverse forms, detecting hallucinations at a fine-grained level is
essential for comprehensive evaluation and analysis. To this end, we propose a
novel task of multimodal fine-grained hallucination detection and editing for
MLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated
spans at a fine-grained level, classifies their error types into six
categories, and suggests appropriate refinements. To train and evaluate models
for this task, we constructed VisionHall, a dataset comprising 6.9k outputs
from twelve MLLMs manually annotated by 211 annotators, and 20k synthetic
samples generated using a graph-based method that captures dependencies among
error types. We demonstrated that ZINA outperformed existing methods, including
GPT-4o and LLama-3.2, in both detection and editing tasks.

</details>


### [435] [Stress-Testing Multimodal Foundation Models for Crystallographic Reasoning](https://arxiv.org/abs/2506.13051)
*Can Polat,Hasan Kurban,Erchin Serpedin,Mustafa Kurban*

Main category: cs.CV

TL;DR: 本文引入多尺度多晶体数据集和两种评估协议对多模态生成模型进行压力测试，评估九个视觉 - 语言基础模型生成的结构注释，建立评估大型多模态模型的框架。


<details>
  <summary>Details</summary>
Motivation: 评估用于晶体学推理的基础模型需要能分离泛化行为并执行物理约束的基准。

Method: 引入多尺度多晶体数据集，采用空间排除和成分排除两种评估协议，用晶体学图像和文本提示九个视觉 - 语言基础模型生成结构注释，并通过相对误差、物理一致性指数和幻觉分数评估结果。

Result: 建立了可重现、有物理依据的框架用于评估大规模多模态模型的泛化性、一致性和可靠性。

Conclusion: 所提出的基准和评估方法为评估晶体学推理的基础模型提供了有效途径，代码和数据集可公开获取。

Abstract: Evaluating foundation models for crystallographic reasoning requires
benchmarks that isolate generalization behavior while enforcing physical
constraints. This work introduces a multiscale multicrystal dataset with two
physically grounded evaluation protocols to stress-test multimodal generative
models. The Spatial-Exclusion benchmark withholds all supercells of a given
radius from a diverse dataset, enabling controlled assessments of spatial
interpolation and extrapolation. The Compositional-Exclusion benchmark omits
all samples of a specific chemical composition, probing generalization across
stoichiometries. Nine vision--language foundation models are prompted with
crystallographic images and textual context to generate structural annotations.
Responses are evaluated via (i) relative errors in lattice parameters and
density, (ii) a physics-consistency index penalizing volumetric violations, and
(iii) a hallucination score capturing geometric outliers and invalid
space-group predictions. These benchmarks establish a reproducible, physically
informed framework for assessing generalization, consistency, and reliability
in large-scale multimodal models. Dataset and code are available at
https://github.com/KurbanIntelligenceLab/StressTestingMMFMinCR.

</details>


### [436] [Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware Learning](https://arxiv.org/abs/2506.13265)
*Rohit Mohan,Julia Hindel,Florian Drews,Claudius Gläser,Daniele Cattaneo,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出ULOPS框架用于LiDAR全景分割，可检测未知物体实例，引入三种损失函数，扩展评估设置，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR全景分割模型多基于封闭集假设，无法检测未知物体实例。

Method: 提出ULOPS框架，利用基于狄利克雷的证据学习建模预测不确定性，采用单独解码器，引入三种不确定性驱动的损失函数。

Result: 在KITTI - 360和nuScenes上进行实验，ULOPS持续优于现有方法。

Conclusion: ULOPS在开放集LiDAR全景分割方面表现出色，能有效检测未知物体实例。

Abstract: Autonomous vehicles that navigate in open-world environments may encounter
previously unseen object classes. However, most existing LiDAR panoptic
segmentation models rely on closed-set assumptions, failing to detect unknown
object instances. In this work, we propose ULOPS, an uncertainty-guided
open-set panoptic segmentation framework that leverages Dirichlet-based
evidential learning to model predictive uncertainty. Our architecture
incorporates separate decoders for semantic segmentation with uncertainty
estimation, embedding with prototype association, and instance center
prediction. During inference, we leverage uncertainty estimates to identify and
segment unknown instances. To strengthen the model's ability to differentiate
between known and unknown objects, we introduce three uncertainty-driven loss
functions. Uniform Evidence Loss to encourage high uncertainty in unknown
regions. Adaptive Uncertainty Separation Loss ensures a consistent difference
in uncertainty estimates between known and unknown objects at a global scale.
Contrastive Uncertainty Loss refines this separation at the fine-grained level.
To evaluate open-set performance, we extend benchmark settings on KITTI-360 and
introduce a new open-set evaluation for nuScenes. Extensive experiments
demonstrate that ULOPS consistently outperforms existing open-set LiDAR
panoptic segmentation methods.

</details>


### [437] [Automatic Multi-View X-Ray/CT Registration Using Bone Substructure Contours](https://arxiv.org/abs/2506.13292)
*Roman Flepp,Leon Nissen,Bastian Sigrist,Arend Nieuwland,Nicola Cavalcanti,Philipp Fürnstahl,Thomas Dreher,Lilian Calvet*

Main category: cs.CV

TL;DR: 提出新的多视图X射线/CT配准方法用于术中骨骼配准，经评估精度高且全自动，能改善手术导航效果。


<details>
  <summary>Details</summary>
Motivation: 现有术中X射线/CT配准方法难以达到亚毫米精度、在宽泛初始姿态估计下缺乏鲁棒性或需手动关键点标注，本文旨在解决这些问题。

Method: 提出基于多视图、轮廓的迭代最近点（ICP）优化配准方法，聚焦匹配对应骨子结构的特定轮廓子类别，仅需两张X射线图像且全自动运行，还提供了包含5个尸体标本的数据集。

Result: 用平均重投影误差（mRPD）评估，该方法mRPD为0.67mm，持续达到亚毫米精度，优于需手动干预的商业解决方案的5.35mm，且全自动运行适用性更好。

Conclusion: 该方法为骨科手术中的多视图X射线/CT配准提供了实用、准确、高效的解决方案，可与跟踪系统轻松结合，提高配准精度和减少手动干预，改善术中导航，有助于计算机辅助手术获得更准确有效的手术结果。

Abstract: Purpose: Accurate intraoperative X-ray/CT registration is essential for
surgical navigation in orthopedic procedures. However, existing methods
struggle with consistently achieving sub-millimeter accuracy, robustness under
broad initial pose estimates or need manual key-point annotations. This work
aims to address these challenges by proposing a novel multi-view X-ray/CT
registration method for intraoperative bone registration. Methods: The proposed
registration method consists of a multi-view, contour-based iterative closest
point (ICP) optimization. Unlike previous methods, which attempt to match bone
contours across the entire silhouette in both imaging modalities, we focus on
matching specific subcategories of contours corresponding to bone
substructures. This leads to reduced ambiguity in the ICP matches, resulting in
a more robust and accurate registration solution. This approach requires only
two X-ray images and operates fully automatically. Additionally, we contribute
a dataset of 5 cadaveric specimens, including real X-ray images, X-ray image
poses and the corresponding CT scans. Results: The proposed registration method
is evaluated on real X-ray images using mean reprojection error (mRPD). The
method consistently achieves sub-millimeter accuracy with a mRPD 0.67mm
compared to 5.35mm by a commercial solution requiring manual intervention.
Furthermore, the method offers improved practical applicability, being fully
automatic. Conclusion: Our method offers a practical, accurate, and efficient
solution for multi-view X-ray/CT registration in orthopedic surgeries, which
can be easily combined with tracking systems. By improving registration
accuracy and minimizing manual intervention, it enhances intraoperative
navigation, contributing to more accurate and effective surgical outcomes in
computer-assisted surgery (CAS).

</details>


### [438] [Fair Generation without Unfair Distortions: Debiasing Text-to-Image Generation with Entanglement-Free Attention](https://arxiv.org/abs/2506.13298)
*Jeonghoon Park,Juyoung Lee,Chaeyeon Chung,Jaeseong Lee,Jaegul Choo,Jindong Gu*

Main category: cs.CV

TL;DR: 扩散式文本到图像模型有社会偏见问题，现有方法有属性纠缠，本文提出无纠缠注意力（EFA）方法，实验表明其在减轻偏见同时保留非目标属性表现更优。


<details>
  <summary>Details</summary>
Motivation: 扩散式文本到图像模型存在社会偏见，现有缓解方法有属性纠缠问题，需解决该问题。

Method: 引入无纠缠注意力（EFA）方法，推理时等概率随机采样目标属性，调整选定层的交叉注意力以融入采样属性。

Result: 广泛实验表明EFA在减轻偏见同时保留非目标属性方面优于现有方法。

Conclusion: EFA能有效减轻偏见，保留非目标属性，维持原模型输出分布和生成能力。

Abstract: Recent advancements in diffusion-based text-to-image (T2I) models have
enabled the generation of high-quality and photorealistic images from text
descriptions. However, they often exhibit societal biases related to gender,
race, and socioeconomic status, thereby reinforcing harmful stereotypes and
shaping public perception in unintended ways. While existing bias mitigation
methods demonstrate effectiveness, they often encounter attribute entanglement,
where adjustments to attributes relevant to the bias (i.e., target attributes)
unintentionally alter attributes unassociated with the bias (i.e., non-target
attributes), causing undesirable distribution shifts. To address this
challenge, we introduce Entanglement-Free Attention (EFA), a method that
accurately incorporates target attributes (e.g., White, Black, Asian, and
Indian) while preserving non-target attributes (e.g., background details)
during bias mitigation. At inference time, EFA randomly samples a target
attribute with equal probability and adjusts the cross-attention in selected
layers to incorporate the sampled attribute, achieving a fair distribution of
target attributes. Extensive experiments demonstrate that EFA outperforms
existing methods in mitigating bias while preserving non-target attributes,
thereby maintaining the output distribution and generation capability of the
original model.

</details>


### [439] [Action Dubber: Timing Audible Actions via Inflectional Flow](https://arxiv.org/abs/2506.13320)
*Wenlong Wan,Weiying Zheng,Tianyi Xiang,Guiqing Li,Shengfeng He*

Main category: cs.CV

TL;DR: 介绍可听动作时间定位任务，提出TA²Net架构及Audible623数据集，实验验证方法有效且具泛化性。


<details>
  <summary>Details</summary>
Motivation: 区别于传统视频内容分析任务，聚焦可听动作独特运动动力学，进行可听动作时空坐标识别。

Method: 提出TA²Net架构，用运动二阶导数估计拐点流确定碰撞时间，训练中集成自监督空间定位策略；引入Audible623数据集。

Result: 实验证实方法在Audible623上有效，对重复计数和声源定位等其他领域有强泛化性。

Conclusion: 所提方法可提高时间定位准确性，同时识别视频帧内声源，具一定实用性和泛化能力。

Abstract: We introduce the task of Audible Action Temporal Localization, which aims to
identify the spatio-temporal coordinates of audible movements. Unlike
conventional tasks such as action recognition and temporal action localization,
which broadly analyze video content, our task focuses on the distinct kinematic
dynamics of audible actions. It is based on the premise that key actions are
driven by inflectional movements; for example, collisions that produce sound
often involve abrupt changes in motion. To capture this, we propose
$TA^{2}Net$, a novel architecture that estimates inflectional flow using the
second derivative of motion to determine collision timings without relying on
audio input. $TA^{2}Net$ also integrates a self-supervised spatial localization
strategy during training, combining contrastive learning with spatial analysis.
This dual design improves temporal localization accuracy and simultaneously
identifies sound sources within video frames. To support this task, we
introduce a new benchmark dataset, $Audible623$, derived from Kinetics and
UCF101 by removing non-essential vocalization subsets. Extensive experiments
confirm the effectiveness of our approach on $Audible623$ and show strong
generalizability to other domains, such as repetitive counting and sound source
localization. Code and dataset are available at
https://github.com/WenlongWan/Audible623.

</details>


### [440] [Quantitative Comparison of Fine-Tuning Techniques for Pretrained Latent Diffusion Models in the Generation of Unseen SAR Image Concepts](https://arxiv.org/abs/2506.13307)
*Solène Debuysère,Nicolas Trouvé,Nathan Letheule,Olivier Lévêque,Elise Colin*

Main category: cs.CV

TL;DR: 研究将预训练潜扩散模型适配到合成孔径雷达（SAR）成像领域，对比多种微调策略，发现混合调优策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 预训练生成模型未适配SAR数据，需研究将其微调以用于该领域。

Method: 使用大量SAR数据集，探索并比较全模型微调、低秩适配（LoRA）等微调策略，分别针对UNet扩散主干和文本编码器组件；结合多种指标评估生成质量。

Result: 混合调优策略表现最佳，全微调UNet捕捉SAR底层特征，基于LoRA的文本编码器部分调优结合< SAR >标记嵌入学习可保持提示对齐。

Conclusion: 提供了将基础模型适配到自然图像领域之外非常规成像模式的系统策略。

Abstract: This work investigates the adaptation of large pre-trained latent diffusion
models to a radically new imaging domain: Synthetic Aperture Radar (SAR). While
these generative models, originally trained on natural images, demonstrate
impressive capabilities in text-to-image synthesis, they are not natively
adapted to represent SAR data, which involves different physics, statistical
distributions, and visual characteristics. Using a sizeable SAR dataset (on the
order of 100,000 to 1 million images), we address the fundamental question of
fine-tuning such models for this unseen modality. We explore and compare
multiple fine-tuning strategies, including full model fine-tuning and
parameter-efficient approaches like Low-Rank Adaptation (LoRA), focusing
separately on the UNet diffusion backbone and the text encoder components. To
evaluate generative quality, we combine several metrics: statistical distance
from real SAR distributions, textural similarity via GLCM descriptors, and
semantic alignment assessed with a CLIP model fine-tuned on SAR data. Our
results show that a hybrid tuning strategy yields the best performance: full
fine-tuning of the UNet is better at capturing low-level SAR-specific patterns,
while LoRA-based partial tuning of the text encoder, combined with embedding
learning of the <SAR> token, suffices to preserve prompt alignment. This work
provides a methodical strategy for adapting foundation models to unconventional
imaging modalities beyond natural image domains.

</details>


### [441] [Active Multimodal Distillation for Few-shot Action Recognition](https://arxiv.org/abs/2506.13322)
*Weijia Feng,Yichen Zhu,Ruojia Zhang,Chenyang Wang,Fei Ma,Xiaobao Wang,Xiaobai Li*

Main category: cs.CV

TL;DR: 提出新框架用于少样本动作识别，结合ASI模块和主动互蒸馏模块，在多基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前少样本动作识别方法多基于单模态数据，未充分利用多模态信息。

Method: 提出新框架，集成ASI模块，基于后验分布预测可靠模态；引入主动互蒸馏模块，利用可靠模态知识提升不可靠模态表征学习；元测试时采用自适应多模态推理分配权重。

Result: 在多个基准测试中，该方法显著优于现有方法。

Conclusion: 提出的框架能有效利用多模态信息，提高少样本动作识别性能。

Abstract: Owing to its rapid progress and broad application prospects, few-shot action
recognition has attracted considerable interest. However, current methods are
predominantly based on limited single-modal data, which does not fully exploit
the potential of multimodal information. This paper presents a novel framework
that actively identifies reliable modalities for each sample using
task-specific contextual cues, thus significantly improving recognition
performance. Our framework integrates an Active Sample Inference (ASI) module,
which utilizes active inference to predict reliable modalities based on
posterior distributions and subsequently organizes them accordingly. Unlike
reinforcement learning, active inference replaces rewards with evidence-based
preferences, making more stable predictions. Additionally, we introduce an
active mutual distillation module that enhances the representation learning of
less reliable modalities by transferring knowledge from more reliable ones.
Adaptive multimodal inference is employed during the meta-test to assign higher
weights to reliable modalities. Extensive experiments across multiple
benchmarks demonstrate that our method significantly outperforms existing
approaches.

</details>


### [442] [ESRPCB: an Edge guided Super-Resolution model and Ensemble learning for tiny Printed Circuit Board Defect detection](https://arxiv.org/abs/2506.13476)
*Xiem HoangVan,Dang Bui Dinh,Thanh Nguyen Canh,Van-Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出ESRPCB框架结合边缘引导超分辨率和集成学习进行小尺寸PCB图像缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 小尺寸PCB图像分辨率低，缺陷与噪声易混淆，检测存在挑战。

Method: 提出ESRPCB框架，利用边缘信息引导带ResCat结构的EDSR模型重建高分辨率图像，再用多模态缺陷检测模型通过集成学习分析。

Result: 文档未提及。

Conclusion: 文档未提及。

Abstract: Printed Circuit Boards (PCBs) are critical components in modern electronics,
which require stringent quality control to ensure proper functionality.
However, the detection of defects in small-scale PCBs images poses significant
challenges as a result of the low resolution of the captured images, leading to
potential confusion between defects and noise. To overcome these challenges,
this paper proposes a novel framework, named ESRPCB (edgeguided
super-resolution for PCBs defect detection), which combines edgeguided
super-resolution with ensemble learning to enhance PCBs defect detection. The
framework leverages the edge information to guide the EDSR (Enhanced Deep
Super-Resolution) model with a novel ResCat (Residual Concatenation) structure,
enabling it to reconstruct high-resolution images from small PCBs inputs. By
incorporating edge features, the super-resolution process preserves critical
structural details, ensuring that tiny defects remain distinguishable in the
enhanced image. Following this, a multi-modal defect detection model employs
ensemble learning to analyze the super-resolved

</details>


### [443] [Lecture Video Visual Objects (LVVO) Dataset: A Benchmark for Visual Object Detection in Educational Videos](https://arxiv.org/abs/2506.13657)
*Dipayan Biswas,Shishir Shah,Jaspal Subhlok*

Main category: cs.CV

TL;DR: 介绍了用于教育视频内容视觉对象检测的LVVO数据集，含4000帧，部分手动标注，部分半监督自动标注，公开可用。


<details>
  <summary>Details</summary>
Motivation: 为教育视频内容的视觉对象检测提供新的基准数据集。

Method: 从245个讲座视频中提取4000帧，1000帧手动标注，两名标注者独立标注，有分歧时由专家解决；3000帧用半监督方法自动标注。

Result: 构建了LVVO数据集，含手动标注的LVVO_1k和自动标注的LVVO_3k，标注者间F1分数达83.41%。

Conclusion: LVVO数据集是开发和评估教育视频视觉内容检测方法的宝贵资源，公开可用以支持相关研究。

Abstract: We introduce the Lecture Video Visual Objects (LVVO) dataset, a new benchmark
for visual object detection in educational video content. The dataset consists
of 4,000 frames extracted from 245 lecture videos spanning biology, computer
science, and geosciences. A subset of 1,000 frames, referred to as LVVO_1k, has
been manually annotated with bounding boxes for four visual categories: Table,
Chart-Graph, Photographic-image, and Visual-illustration. Each frame was
labeled independently by two annotators, resulting in an inter-annotator F1
score of 83.41%, indicating strong agreement. To ensure high-quality consensus
annotations, a third expert reviewed and resolved all cases of disagreement
through a conflict resolution process. To expand the dataset, a semi-supervised
approach was employed to automatically annotate the remaining 3,000 frames,
forming LVVO_3k. The complete dataset offers a valuable resource for developing
and evaluating both supervised and semi-supervised methods for visual content
detection in educational videos. The LVVO dataset is publicly available to
support further research in this domain.

</details>


### [444] [DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models](https://arxiv.org/abs/2506.13638)
*Zhiyi Shi,Binjie Wang,Chongjie Si,Yichen Wu,Junsik Kim,Hanspeter Pfister*

Main category: cs.CV

TL;DR: 本文探索多模态视觉语言模型编辑，发现文本和视觉模态在不同层敏感性不同，提出 DualEdit 编辑器并验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法多针对单模态语言模型，多模态视觉语言模型中各模态对编辑性能的影响未充分研究。

Method: 探索文本和视觉模态对模型编辑的影响，提出 DualEdit 编辑器，在更敏感的文本模态中引入门控模块。

Result: 在多个视觉语言模型主干和基准数据集上评估，DualEdit 在不同评估指标上优于现有视觉语言模型编辑基线和适配的语言模型编辑方法。

Conclusion: DualEdit 能在高效更新新知识的同时保留模型原始信息，具有更好的编辑性能。

Abstract: Model editing aims to efficiently update a pre-trained model's knowledge
without the need for time-consuming full retraining. While existing pioneering
editing methods achieve promising results, they primarily focus on editing
single-modal language models (LLMs). However, for vision-language models
(VLMs), which involve multiple modalities, the role and impact of each modality
on editing performance remain largely unexplored. To address this gap, we
explore the impact of textual and visual modalities on model editing and find
that: (1) textual and visual representations reach peak sensitivity at
different layers, reflecting their varying importance; and (2) editing both
modalities can efficiently update knowledge, but this comes at the cost of
compromising the model's original capabilities. Based on our findings, we
propose DualEdit, an editor that modifies both textual and visual modalities at
their respective key layers. Additionally, we introduce a gating module within
the more sensitive textual modality, allowing DualEdit to efficiently update
new knowledge while preserving the model's original information. We evaluate
DualEdit across multiple VLM backbones and benchmark datasets, demonstrating
its superiority over state-of-the-art VLM editing baselines as well as adapted
LLM editing methods on different evaluation metrics.

</details>


### [445] [Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning](https://arxiv.org/abs/2506.13654)
*Shulin Tian,Ruiqi Wang,Hongming Guo,Penghao Wu,Yuhao Dong,Xiuying Wang,Jingkang Yang,Hao Zhang,Hongyuan Zhu,Ziwei Liu*

Main category: cs.CV

TL;DR: 介绍用于超长第一人称视频推理的Ego - R1框架，采用CoTT和RL训练的代理，构建数据集并评估，能有效处理超长视频理解挑战。


<details>
  <summary>Details</summary>
Motivation: 解决超长第一人称视频推理的难题，受人类解决问题策略启发。

Method: 引入CoTT将复杂推理分解为模块化步骤，用RL训练Ego - R1代理；设计两阶段训练范式，包括SFT和RL；构建Ego - R1 Data数据集。

Result: 在新的一周长视频QA基准Ego - R1 Bench上评估，动态、工具增强的思维链推理能有效应对超长第一人称视频理解挑战，将时间覆盖从几小时扩展到一周。

Conclusion: Ego - R1框架的动态、工具增强的思维链推理可有效解决超长第一人称视频理解的独特挑战。

Abstract: We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e.,
in days and weeks) egocentric videos, which leverages a structured
Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained
via reinforcement learning (RL). Inspired by human problem-solving strategies,
CoTT decomposes complex reasoning into modular steps, with the RL agent
invoking specific tools, one per step, to iteratively and collaboratively
answer sub-questions tackling such tasks as temporal retrieval and multi-modal
understanding. We design a two-stage training paradigm involving supervised
finetuning (SFT) of a pretrained language model using CoTT data and RL to
enable our agent to dynamically propose step-by-step tools for long-range
reasoning. To facilitate training, we construct a dataset called Ego-R1 Data,
which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our
Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark,
Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources.
Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought
reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of
understanding ultra-long egocentric videos, significantly extending the time
coverage from few hours to a week.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [446] [Dynamic allocation: extremes, tail dependence, and regime Shifts](https://arxiv.org/abs/2506.12587)
*Yin Luo,Sheng Wang,Javed Jussa*

Main category: econ.EM

TL;DR: 构建复杂模型预测全球金融市场下行风险和实时风险状态，模型能改善资产配置策略，风险状态对量化因子表现有强预测力。


<details>
  <summary>Details</summary>
Motivation: 准确预测全球金融市场下行风险和实时风险状态，以改善资产配置策略和辅助投资决策。

Method: 构建捕捉资产收益分布特征的复杂模型，开发动态制度转换模型。

Result: GARCH - DCC - Copula风险模型能显著改善全球战术资产配置策略，风险状态对量化因子表现有强预测力。

Conclusion: 构建的模型和风险状态指标对全球金融市场投资决策有重要帮助，可提升资产配置效率和因子模型构建效果。

Abstract: By capturing outliers, volatility clustering, and tail dependence in the
asset return distribution, we build a sophisticated model to predict the
downside risk of the global financial market. We further develop a dynamic
regime switching model that can forecast real-time risk regime of the market.
Our GARCH-DCC-Copula risk model can significantly improve both risk- and
alpha-based global tactical asset allocation strategies. Our risk regime has
strong predictive power of quantitative equity factor performance, which can
help equity investors to build better factor models and asset allocation
managers to construct more efficient risk premia portfolios.

</details>


### [447] [Gradient Boosting for Spatial Regression Models with Autoregressive Disturbances](https://arxiv.org/abs/2506.13682)
*Michael Balzer*

Main category: econ.EM

TL;DR: 本文提出一种用于带自回归扰动的空间回归模型的基于模型的梯度提升算法，能用于高低维场景，模拟研究和案例分析验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 处理具有地理位置和空间关系的空间数据的独特性质，解决现有估计方法在高维场景无法产生唯一解的问题。

Method: 提出基于模型的梯度提升算法，可用于高低维场景的数据驱动变量和模型选择，控制偏差 - 方差权衡。

Result: 详细模拟研究证实了该算法在高低维和预测、变量选择方面的性能；以德国地区预期寿命建模案例展示了算法功能。

Conclusion: 提出的算法可行且有效，能改善样本外空间数据的预测准确性。

Abstract: Researchers in urban and regional studies increasingly deal with spatial data
that reflects geographic location and spatial relationships. As a framework for
dealing with the unique nature of spatial data, various spatial regression
models have been introduced. In this article, a novel model-based gradient
boosting algorithm for spatial regression models with autoregressive
disturbances is proposed. Due to the modular nature, the approach provides an
alternative estimation procedure which is feasible even in high-dimensional
settings where established quasi-maximum likelihood or generalized method of
moments estimators do not yield unique solutions. The approach additionally
enables data-driven variable and model selection in low- as well as
high-dimensional settings. Since the bias-variance trade-off is also controlled
in the algorithm, implicit regularization is imposed which improves prediction
accuracy on out-of-sample spatial data. Detailed simulation studies regarding
the performance of estimation, prediction and variable selection in low- and
high-dimensional settings confirm proper functionality of the proposed
methodology. To illustrative the functionality of the model-based gradient
boosting algorithm, a case study is presented where the life expectancy in
German districts is modeled incorporating a potential spatial dependence
structure.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [448] [SpaceTrack-TimeSeries: Time Series Dataset towards Satellite Orbit Analysis](https://arxiv.org/abs/2506.13034)
*Zhixin Guo,Qi Shi,Xiaofan Xu,Sixiang Shan,Limin Qin,Linqiang Ge,Rui Zhang,Ya Dai,Hua Zhu,Guowei Jiang*

Main category: astro-ph.EP

TL;DR: 因航天技术发展，对空间物体高精度轨道数据需求增大，但缺乏公开数据集。本研究收集整理星链卫星机动行为数据集，用于建模、检测和评估。


<details>
  <summary>Details</summary>
Motivation: 航天技术发展使天文观测和深空探测面临挑战，对空间物体轨道数据需求增大，且缺乏公开数据集支持相关研究。

Method: 收集并整理星链卫星机动行为数据集，整合两行元素（TLE）目录数据和高精度星历数据。

Result: 实现对空间物体行为更真实和多维的建模。

Conclusion: 该数据集为机动检测方法的实际应用和轨道环境碰撞风险评估提供了有价值的见解。

Abstract: With the rapid advancement of aerospace technology and the large-scale
deployment of low Earth orbit (LEO) satellite constellations, the challenges
facing astronomical observations and deep space exploration have become
increasingly pronounced. As a result, the demand for high-precision orbital
data on space objects-along with comprehensive analyses of satellite
positioning, constellation configurations, and deep space satellite
dynamics-has grown more urgent. However, there remains a notable lack of
publicly accessible, real-world datasets to support research in areas such as
space object maneuver behavior prediction and collision risk assessment. This
study seeks to address this gap by collecting and curating a representative
dataset of maneuvering behavior from Starlink satellites. The dataset
integrates Two-Line Element (TLE) catalog data with corresponding
high-precision ephemeris data, thereby enabling a more realistic and
multidimensional modeling of space object behavior. It provides valuable
insights into practical deployment of maneuver detection methods and the
evaluation of collision risks in increasingly congested orbital environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [449] [Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics](https://arxiv.org/abs/2506.12365)
*Asifullah khan,Muhammad Zaeem Khan,Saleha Jamshed,Sadia Ahmad,Aleesha Zainab,Kaynat Khatib,Faria Bibi,Abdul Rehman*

Main category: cs.CL

TL;DR: 本文概述大语言模型关键进展、有效技术，分类新兴方法，指出未充分探索领域和现存挑战，明确未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 全面了解大语言模型领域发展，促进其更智能、安全和可靠。

Method: 对大语言模型领域相关进展进行调研和分类。

Result: 明确有效技术，分类新兴方法，指出未充分探索领域和现存挑战。

Conclusion: 未来研究应聚焦提升模型处理多输入能力，应对计算成本、偏差和伦理风险等挑战。

Abstract: This survey paper outlines the key developments in the field of Large
Language Models (LLMs), such as enhancing their reasoning skills, adaptability
to various tasks, increased computational efficiency, and ability to make
ethical decisions. The techniques that have been most effective in bridging the
gap between human and machine communications include the Chain-of-Thought
prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback.
The improvements in multimodal learning and few-shot or zero-shot techniques
have further empowered LLMs to handle complex jobs with minor input. They also
manage to do more with less by applying scaling and optimization tricks for
computing power conservation. This survey also offers a broader perspective on
recent advancements in LLMs going beyond isolated aspects such as model
architecture or ethical concerns. It categorizes emerging methods that enhance
LLM reasoning, efficiency, and ethical alignment. It also identifies
underexplored areas such as interpretability, cross-modal integration and
sustainability. With recent progress, challenges like huge computational costs,
biases, and ethical risks remain constant. Addressing these requires bias
mitigation, transparent decision-making, and clear ethical guidelines. Future
research will focus on enhancing models ability to handle multiple input,
thereby making them more intelligent, safe, and reliable.

</details>


### [450] [Large Language Models as 'Hidden Persuaders': Fake Product Reviews are Indistinguishable to Humans and Machines](https://arxiv.org/abs/2506.13313)
*Weiyao Meng,John Harvey,James Goulding,Chris James Carter,Evgeniya Lukinova,Andrew Smith,Paul Frobisher,Mina Forrest,Georgiana Nica-Avram*

Main category: cs.CL

TL;DR: 研究表明人类和大语言模型都难以区分真假产品评论，揭示评论系统易受欺诈，还体现人类和模型判断心理。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型和生成式人工智能使撰写虚假评论更易，需研究人类和模型区分真假评论的能力。

Method: 开展三项研究。

Result: 人类区分准确率仅50.8%；大语言模型表现同样差甚至更糟；人类和模型策略不同，准确率相当但具体指标有差异。

Conclusion: 评论系统若缺乏可信购买验证易受欺诈；体现人类判断的怀疑偏见和对假负面评论的误判；展示大语言模型判断策略与人类不同且同样不准确。

Abstract: Reading and evaluating product reviews is central to how most people decide
what to buy and consume online. However, the recent emergence of Large Language
Models and Generative Artificial Intelligence now means writing fraudulent or
fake reviews is potentially easier than ever. Through three studies we
demonstrate that (1) humans are no longer able to distinguish between real and
fake product reviews generated by machines, averaging only 50.8% accuracy
overall - essentially the same that would be expected by chance alone; (2) that
LLMs are likewise unable to distinguish between fake and real reviews and
perform equivalently bad or even worse than humans; and (3) that humans and
LLMs pursue different strategies for evaluating authenticity which lead to
equivalently bad accuracy, but different precision, recall and F1 scores -
indicating they perform worse at different aspects of judgment. The results
reveal that review systems everywhere are now susceptible to mechanised fraud
if they do not depend on trustworthy purchase verification to guarantee the
authenticity of reviewers. Furthermore, the results provide insight into the
consumer psychology of how humans judge authenticity, demonstrating there is an
inherent 'scepticism bias' towards positive reviews and a special vulnerability
to misjudge the authenticity of fake negative reviews. Additionally, results
provide a first insight into the 'machine psychology' of judging fake reviews,
revealing that the strategies LLMs take to evaluate authenticity radically
differ from humans, in ways that are equally wrong in terms of accuracy, but
different in their misjudgments.

</details>


### [451] [FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.12494)
*Zhuocheng Zhang,Yang Feng,Min Zhang*

Main category: cs.CL

TL;DR: 介绍用于RAG的开源框架FlexRAG，可解决现有框架的问题，支持多种RAG，提供生命周期支持等，资源可在GitHub获取。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架存在算法难以复现共享、缺乏新技术、系统开销大等问题。

Method: 开发名为FlexRAG的开源框架，支持文本、多模态和基于网络的RAG，具备高效异步处理和持久缓存能力。

Result: 开发出FlexRAG框架，支持快速开发、部署和共享高级RAG系统。

Conclusion: FlexRAG是一个强大且灵活的解决方案，能帮助研究人员快速开发、部署和共享高级RAG系统。

Abstract: Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large
language model applications, with numerous existing frameworks offering a wide
range of functionalities to facilitate the development of RAG systems. However,
we have identified several persistent challenges in these frameworks, including
difficulties in algorithm reproduction and sharing, lack of new techniques, and
high system overhead. To address these limitations, we introduce
\textbf{FlexRAG}, an open-source framework specifically designed for research
and prototyping. FlexRAG supports text-based, multimodal, and network-based
RAG, providing comprehensive lifecycle support alongside efficient asynchronous
processing and persistent caching capabilities. By offering a robust and
flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and
share advanced RAG systems. Our toolkit and resources are available at
\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.

</details>


### [452] [DoTA-RAG: Dynamic of Thought Aggregation RAG](https://arxiv.org/abs/2506.12571)
*Saksorn Ruangtanusak,Natthapath Rungseesiripak,Peerawat Rojratchadakorn,Monthol Charattrakool,Natapong Nitarach*

Main category: cs.CL

TL;DR: 本文介绍DoTA - RAG系统，它优化了传统RAG管道，通过三阶段流程及多种手段提升性能，实验证明其效果良好，有实际部署潜力。


<details>
  <summary>Details</summary>
Motivation: 传统RAG管道在大规模多样数据集上存在高延迟和准确性有限的问题，需要优化。

Method: 采用三阶段管道（查询重写、动态路由到专业子索引、多阶段检索和排序），评估选择更优嵌入模型重嵌入语料，创建多样化问答数据集。

Result: DoTA - RAG将答案正确性分数从0.752提升到1.478，在Live Challenge Day达到0.929的正确性分数，且保持低延迟。

Conclusion: DoTA - RAG在需要快速可靠访问大型和不断发展知识源的领域有实际部署潜力。

Abstract: In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a
retrieval-augmented generation system optimized for high-throughput,
large-scale web knowledge indexes. Traditional RAG pipelines often suffer from
high latency and limited accuracy over massive, diverse datasets. DoTA-RAG
addresses these challenges with a three-stage pipeline: query rewriting,
dynamic routing to specialized sub-indexes, and multi-stage retrieval and
ranking. We further enhance retrieval by evaluating and selecting a superior
embedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we
create a diverse Q&A dataset of 500 questions generated via the DataMorgana
setup across a broad range of WebOrganizer topics and formats. DoTA-RAG
improves the answer correctness score from 0.752 (baseline, using LiveRAG
pre-built vector store) to 1.478 while maintaining low latency, and it achieves
a 0.929 correctness score on the Live Challenge Day. These results highlight
DoTA-RAG's potential for practical deployment in domains requiring fast,
reliable access to large and evolving knowledge sources.

</details>


### [453] [Assessing the Performance Gap Between Lexical and Semantic Models for Information Retrieval With Formulaic Legal Language](https://arxiv.org/abs/2506.12895)
*Larissa Mori,Carlos Sousa de Oliveira,Yuehwern Yih,Mario Ventresca*

Main category: cs.CL

TL;DR: 研究欧盟法院判决中法律段落检索，对比词汇和语义模型，发现不同场景下表现差异，微调语义模型可提升性能。


<details>
  <summary>Details</summary>
Motivation: 理解词汇或语义模型何时更能有效处理法律语言重复性，以开发更准确、高效和透明的特定法律领域检索系统。

Method: 对比依赖词汇和统计特征的方法（如BM25）与训练捕捉语义和上下文信息的密集检索模型，进行定性和定量分析。

Result: 词汇和密集模型在语言重复场景表现好；BM25在细微差别场景和长查询中更好；微调密集模型性能提升，多数指标超BM25。

Conclusion: 明确不同模型在法律段落检索不同场景的适用性，微调密集模型有优势，还分析了微调数据量对性能和时间鲁棒性的影响。

Abstract: Legal passage retrieval is an important task that assists legal practitioners
in the time-intensive process of finding relevant precedents to support legal
arguments. This study investigates the task of retrieving legal passages or
paragraphs from decisions of the Court of Justice of the European Union (CJEU),
whose language is highly structured and formulaic, leading to repetitive
patterns. Understanding when lexical or semantic models are more effective at
handling the repetitive nature of legal language is key to developing retrieval
systems that are more accurate, efficient, and transparent for specific legal
domains. To this end, we explore when this routinized legal language is better
suited for retrieval using methods that rely on lexical and statistical
features, such as BM25, or dense retrieval models trained to capture semantic
and contextual information. A qualitative and quantitative analysis with three
complementary metrics shows that both lexical and dense models perform well in
scenarios with more repetitive usage of language, whereas BM25 performs better
than the dense models in more nuanced scenarios where repetition and
verbatim~quotes are less prevalent and in longer queries. Our experiments also
show that BM25 is a strong baseline, surpassing off-the-shelf dense models in 4
out of 7 performance metrics. However, fine-tuning a dense model on
domain-specific data led to improved performance, surpassing BM25 in most
metrics, and we analyze the effect of the amount of data used in fine-tuning on
the model's performance and temporal robustness. The code, dataset and appendix
related to this work are available on:
https://github.com/larimo/lexsem-legal-ir.

</details>


### [454] [UCD: Unlearning in LLMs via Contrastive Decoding](https://arxiv.org/abs/2506.12097)
*Vinith M. Suriyakumar,Ayush Sekhari,Ashia Wilson*

Main category: cs.CL

TL;DR: 提出基于对比解码的推理时去学习算法，在两个基准测试中效果优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型中移除特定信息，同时保持整体性能。

Method: 使用对比解码的推理时去学习算法，借助两个辅助小模型引导原模型输出。

Result: 在TOFU和MUSE两个基准测试中，遗忘质量和保留性能均有显著提升。

Conclusion: 对比解码为大规模模型去学习概念提供了高效实用的途径。

Abstract: Machine unlearning aims to remove specific information, e.g. sensitive or
undesirable content, from large language models (LLMs) while preserving overall
performance. We propose an inference-time unlearning algorithm that uses
contrastive decoding, leveraging two auxiliary smaller models, one trained
without the forget set and one trained with it, to guide the outputs of the
original model using their difference during inference. Our strategy
substantially improves the tradeoff between unlearning effectiveness and model
utility. We evaluate our approach on two unlearning benchmarks, TOFU and MUSE.
Results show notable gains in both forget quality and retained performance in
comparison to prior approaches, suggesting that incorporating contrastive
decoding can offer an efficient, practical avenue for unlearning concepts in
large-scale models.

</details>


### [455] [Decompositional Reasoning for Graph Retrieval with Large Language Models](https://arxiv.org/abs/2506.13380)
*Valentin Six,Evan Dufraisse,Gaël de Chalendar*

Main category: cs.CL

TL;DR: 提出一种通过查询分解将文本知识图谱集成到LLM推理过程的检索方法，在多跳QA任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多跳推理和事实一致性上的不足，以及缺乏对图结构信息高效推理能力的问题。

Method: 将复杂问题分解为子问题，检索相关文本子图，构建特定问题的知识图谱，使用加权相似度函数提取相关子图。

Result: 在标准多跳QA基准测试中，使用较小模型和更少LLM调用，取得了与现有竞争方法相当或更优的性能。

Conclusion: 该结构化推理管道在利用LLM生成能力的同时，增强了事实依据和可解释性。

Abstract: Large Language Models (LLMs) excel at many NLP tasks, but struggle with
multi-hop reasoning and factual consistency, limiting their effectiveness on
knowledge-intensive tasks like complex question answering (QA). Linking
Knowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally
lack the ability to reason efficiently over graph-structured information. To
tackle this problem, we propose a novel retrieval approach that integrates
textual knowledge graphs into the LLM reasoning process via query
decomposition. Our method decomposes complex questions into sub-questions,
retrieves relevant textual subgraphs, and composes a question-specific
knowledge graph to guide answer generation. For that, we use a weighted
similarity function that focuses on both the complex question and the generated
subquestions to extract a relevant subgraph, which allows efficient and precise
retrieval for complex questions and improves the performance of LLMs on
multi-hop QA tasks. This structured reasoning pipeline enhances factual
grounding and interpretability while leveraging the generative strengths of
LLMs. We evaluate our method on standard multi-hop QA benchmarks and show that
it achieves comparable or superior performance to competitive existing methods,
using smaller models and fewer LLM calls.

</details>


### [456] [LTRR: Learning To Rank Retrievers for LLMs](https://arxiv.org/abs/2506.13743)
*To Eun Kim,Fernando Diaz*

Main category: cs.CL

TL;DR: 本文探索检索增强生成（RAG）系统的查询路由方法，实验表明基于路由的RAG系统可优于最佳单检索器系统，还展示了该方法的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统多依赖单一固定检索器，但单一检索器无法在所有查询类型上达到最优，因此探索基于查询动态选择检索器的方法。

Method: 采用查询路由方法，使用无训练启发式和学习路由模型，将路由问题构建为学习排序（LTR）问题，引入LTRR框架学习对检索器进行排序。

Result: 基于路由的RAG系统能优于最佳单检索器系统，在使用答案正确性（AC）指标和成对学习方法训练的模型中性能提升显著，对分布外查询的泛化能力也有改善，在挑战赛中表现出实用性。

Conclusion: 强调了训练方法和指标选择在RAG系统查询路由中的重要性。

Abstract: Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed
retriever, despite growing evidence that no single retriever performs optimally
across all query types. In this paper, we explore a query routing approach that
dynamically selects from a pool of retrievers based on the query, using both
train-free heuristics and learned routing models. We frame routing as a
learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to
rank retrievers by their expected utility gain to downstream LLM performance.
Our experiments, conducted on synthetic QA data with controlled query type
variations, show that routing-based RAG systems can outperform the best
single-retriever-based systems. Performance gains are especially pronounced in
models trained with the Answer Correctness (AC) metric and with pairwise
learning approaches, especially with XGBoost. We also observe improvements in
generalization to out-of-distribution queries. As part of the SIGIR 2025
LiveRAG challenge, our submitted system demonstrated the practical viability of
our approach, achieving competitive performance in both answer correctness and
faithfulness. These findings highlight the importance of both training
methodology and metric selection in query routing for RAG systems.

</details>


### [457] [Personalized LLM Decoding via Contrasting Personal Preference](https://arxiv.org/abs/2506.12109)
*Hyungjune Bu,Chanjoo Jung,Minjae Kang,Jaehyung Kim*

Main category: cs.CL

TL;DR: 提出CoPe方法用于大语言模型个性化，在五个任务评估中表现好，提升个性化效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署中个性化重要，现有研究忽视解码时间算法的开发。

Method: 提出CoPe，在参数高效微调后应用，利用奖励引导解码最大化用户隐式奖励信号。

Result: 在五个开放式个性化文本生成任务中评估，平均提升ROUGE - L 10.57%。

Conclusion: CoPe表现出色，无需外部奖励模型和额外训练程序即可提升个性化。

Abstract: As large language models (LLMs) are progressively deployed in various
real-world applications, personalization of LLMs has become increasingly
important. While various approaches to LLM personalization such as prompt-based
and training-based methods have been actively explored, the development of
effective decoding-time algorithms remains largely overlooked, despite their
demonstrated potential. In this paper, we propose CoPe (Contrasting Personal
Preference), a novel decoding-time approach applied after performing
parameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is
to leverage reward-guided decoding specifically for personalization by
maximizing each user's implicit reward signal. We evaluate CoPe across five
open-ended personalized text generation tasks. Our empirical results
demonstrate that CoPe achieves strong performance, improving personalization by
an average of 10.57% in ROUGE-L, without relying on external reward models or
additional training procedures.

</details>


### [458] [Eliciting Reasoning in Language Models with Cognitive Tools](https://arxiv.org/abs/2506.12115)
*Brown Ebouky,Andrea Bartezzaghi,Mattia Rigotti*

Main category: cs.CL

TL;DR: 基于认知心理学文献，在现代代理工具调用框架中为大语言模型赋予认知工具，提升数学推理基准测试表现，也有助于相关理论探讨。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽解决部分推理复制问题，但仍需探索替代方法以阐明底层机制并提供额外益处。

Method: 借鉴认知心理学和认知架构理论，在现代代理工具调用框架中为大语言模型赋予封装特定推理操作的认知工具。

Result: 在标准数学推理基准测试中，该策略使封闭和开源权重模型相比基础大语言模型有显著性能提升，如GPT - 4.1在AIME2024上的pass@1性能从26.7%提升到43.3%。

Conclusion: 该策略有实际应用价值，也有助于探讨后训练方法与预训练固有能力在引发大语言模型推理中的作用。

Abstract: The recent advent of reasoning models like OpenAI's o1 was met with excited
speculation by the AI community about the mechanisms underlying these
capabilities in closed models, followed by a rush of replication efforts,
particularly from the open source community. These speculations were largely
settled by the demonstration from DeepSeek-R1 that chains-of-thought and
reinforcement learning (RL) can effectively replicate reasoning on top of base
LLMs. However, it remains valuable to explore alternative methods for
theoretically eliciting reasoning that could help elucidate the underlying
mechanisms, as well as providing additional methods that may offer
complementary benefits.
  Here, we build on the long-standing literature in cognitive psychology and
cognitive architectures, which postulates that reasoning arises from the
orchestrated, sequential execution of a set of modular, predetermined cognitive
operations. Crucially, we implement this key idea within a modern agentic
tool-calling framework. In particular, we endow an LLM with a small set of
"cognitive tools" encapsulating specific reasoning operations, each executed by
the LLM itself. Surprisingly, this simple strategy results in considerable
gains in performance on standard mathematical reasoning benchmarks compared to
base LLMs, for both closed and open-weight models. For instance, providing our
"cognitive tools" to GPT-4.1 increases its pass@1 performance on AIME2024 from
26.7% to 43.3%, bringing it very close to the performance of o1-preview.
  In addition to its practical implications, this demonstration contributes to
the debate regarding the role of post-training methods in eliciting reasoning
in LLMs versus the role of inherent capabilities acquired during pre-training,
and whether post-training merely uncovers these latent abilities.

</details>


### [459] [Unsupervised Document and Template Clustering using Multimodal Embeddings](https://arxiv.org/abs/2506.12116)
*Phillipe R. Sampaio,Helene Maxcici*

Main category: cs.CL

TL;DR: 本文提出利用多模态嵌入进行无监督文档聚类的新方法，评估多种预训练模型，证明多模态嵌入可提升聚类效果，为后续研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 实现更细粒度的文档理解，不仅按类型分组文档，还区分同一类别内不同模板。

Method: 将多模态嵌入作为传统聚类算法（如k - Means和DBSCAN）的输入，使用能捕捉文本、布局和视觉特征的嵌入，评估多种预训练多模态模型生成的嵌入。

Result: 多模态嵌入能显著提升文档聚类效果，对智能文档处理等多种应用有益。

Conclusion: 本研究展示了不同多模态模型在文档聚类任务中的优缺点，为未来研究开辟新途径。

Abstract: This paper investigates a novel approach to unsupervised document clustering
by leveraging multimodal embeddings as input to traditional clustering
algorithms such as $k$-Means and DBSCAN. Our method aims to achieve a
finer-grained document understanding by not only grouping documents at the type
level (e.g., invoices, purchase orders), but also distinguishing between
different templates within the same document category. This is achieved by
using embeddings that capture textual content, layout information, and visual
features of documents. We evaluated the effectiveness of this approach using
embeddings generated by several state-of-the-art pretrained multimodal models,
including SBERT, LayoutLMv1, LayoutLMv3, DiT, Donut, and ColPali. Our findings
demonstrate the potential of multimodal embeddings to significantly enhance
document clustering, offering benefits for various applications in intelligent
document processing, document layout analysis, and unsupervised document
classification. This work provides valuable insight into the advantages and
limitations of different multimodal models for this task and opens new avenues
for future research to understand and organize document collections.

</details>


### [460] [Can Mixture-of-Experts Surpass Dense LLMs Under Strictly Equal Resources?](https://arxiv.org/abs/2506.12119)
*Houyi Li,Ka Man Lo,Ziqi Wang,Zili Wang,Wenzhen Zheng,Shuigeng Zhou,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CL

TL;DR: 本文探讨MoE模型在严格相同资源约束下能否超越密集架构，提出研究框架，发现MoE模型在最优激活率区域能表现更优，且最优区域跨模型大小一致，问题可通过重用数据解决并经大量实验验证。


<details>
  <summary>Details</summary>
Motivation: 探究MoE模型在总参数、训练计算和数据预算相同的严格资源约束下能否超越密集架构这一未充分研究但具重要实用价值的问题。

Method: 全面研究MoE架构以实现最优模型设计，基于此研究不同激活率下MoE模型表现，通过重用数据解决性能提升与数据量的权衡问题，进行大量实验验证。

Result: MoE模型在激活率处于最优区域时能在相同资源下超越密集架构模型，且最优区域跨不同模型大小保持一致，重用数据可解决性能与数据量的权衡问题。

Conclusion: 提出的研究视角和框架有效，证实了MoE模型在严格资源约束下超越密集架构的可能性，实验模型将公开。

Abstract: Mixture-of-Experts (MoE) language models dramatically expand model capacity
and achieve remarkable performance without increasing per-token compute.
However, can MoEs surpass dense architectures under strictly equal resource
constraints - that is, when the total parameter count, training compute, and
data budget are identical? This question remains under-explored despite its
significant practical value and potential. In this paper, we propose a novel
perspective and methodological framework to study this question thoroughly.
First, we comprehensively investigate the architecture of MoEs and achieve an
optimal model design that maximizes the performance. Based on this, we
subsequently find that an MoE model with activation rate in an optimal region
is able to outperform its dense counterpart under the same total parameter,
training compute and data resource. More importantly, this optimal region
remains consistent across different model sizes. Although additional amount of
data turns out to be a trade-off for the enhanced performance, we show that
this can be resolved via reusing data. We validate our findings through
extensive experiments, training nearly 200 language models at 2B scale and over
50 at 7B scale, cumulatively processing 50 trillion tokens. All models will be
released publicly.

</details>


### [461] [Supernova Event Dataset: Interpreting Large Language Model's Personality through Critical Event Analysis](https://arxiv.org/abs/2506.12189)
*Pranav Agarwal,Ioana Ciucă*

Main category: cs.CL

TL;DR: 利用Supernova Event Dataset对大语言模型在提取和排序关键事件方面进行基准测试，分析模型个性，提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在日常应用中日益广泛，理解其决策和潜在个性变得至关重要。

Method: 使用Supernova Event Dataset进行基准测试，用另一个大语言模型作为评判者，根据模型对事件的选择和分类推断其个性。

Result: 不同模型展现出不同个性特征，如Orca 2注重人际动态的情感推理，Qwen 2.5更具战略性和分析性等。

Conclusion: 该分析提高了模型的可解释性，使其适用于更多样化的应用。

Abstract: Large Language Models (LLMs) are increasingly integrated into everyday
applications. As their influence grows, understanding their decision making and
underlying personality becomes essential. In this work, we interpret model
personality using our proposed Supernova Event Dataset, a novel dataset with
diverse articles spanning biographies, historical events, news, and scientific
discoveries. We use this dataset to benchmark LLMs on extracting and ranking
key events from text, a subjective and complex challenge that requires
reasoning over long-range context and modeling causal chains. We evaluate small
models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as
Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another
LLM acts as a judge to infer each model's personality based on its selection
and classification of events. Our analysis shows distinct personality traits:
for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal
dynamics, while Qwen 2.5 displays a more strategic, analytical style. When
analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual
framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors
step-by-step causal reasoning. This analysis improves model interpretability,
making them user-friendly for a wide range of diverse applications.

</details>


### [462] [Large Language Models for History, Philosophy, and Sociology of Science: Interpretive Uses, Methodological Challenges, and Critical Perspectives](https://arxiv.org/abs/2506.12242)
*Arno Simons,Michael Zichert,Adrian Wüthrich*

Main category: cs.CL

TL;DR: 本文探讨大语言模型（LLMs）在科学史、科学哲学和科学社会学（HPSS）中作为研究工具的应用，分析其机遇与挑战，给出整合LLMs到HPSS的四点建议。


<details>
  <summary>Details</summary>
Motivation: LLMs处理非结构化文本和从上下文推断意义的能力带来新契机，挑战了计算与解释方法的长期分歧，研究其在HPSS中的应用及影响。

Method: 为非技术读者介绍LLM架构和训练范式，将LLMs视为认知基础设施；对比全上下文和生成模型，概述领域和任务适应策略，评估其在HPSS解释性研究中的优缺点。

Result: 分析了不同模型在HPSS解释性探究中的优缺点，明确了不同策略的特点。

Conclusion: 整合LLMs到HPSS有四点经验：模型选择有解释性权衡；掌握LLM知识是基础；HPSS要定义自身基准和语料库；LLMs应增强而非取代解释性方法。

Abstract: This paper explores the use of large language models (LLMs) as research tools
in the history, philosophy, and sociology of science (HPSS). LLMs are
remarkably effective at processing unstructured text and inferring meaning from
context, offering new affordances that challenge long-standing divides between
computational and interpretive methods. This raises both opportunities and
challenges for HPSS, which emphasizes interpretive methodologies and
understands meaning as context-dependent, ambiguous, and historically situated.
We argue that HPSS is uniquely positioned not only to benefit from LLMs'
capabilities but also to interrogate their epistemic assumptions and
infrastructural implications. To this end, we first offer a concise primer on
LLM architectures and training paradigms tailored to non-technical readers. We
frame LLMs not as neutral tools but as epistemic infrastructures that encode
assumptions about meaning, context, and similarity, conditioned by their
training data, architecture, and patterns of use. We then examine how
computational techniques enhanced by LLMs, such as structuring data, detecting
patterns, and modeling dynamic processes, can be applied to support
interpretive research in HPSS. Our analysis compares full-context and
generative models, outlines strategies for domain and task adaptation (e.g.,
continued pretraining, fine-tuning, and retrieval-augmented generation), and
evaluates their respective strengths and limitations for interpretive inquiry
in HPSS. We conclude with four lessons for integrating LLMs into HPSS: (1)
model selection involves interpretive trade-offs; (2) LLM literacy is
foundational; (3) HPSS must define its own benchmarks and corpora; and (4) LLMs
should enhance, not replace, interpretive methods.

</details>


### [463] [The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs](https://arxiv.org/abs/2506.12266)
*Avinash Baidya,Kamalika Das,Xiang Gao*

Main category: cs.CL

TL;DR: 研究提出评估框架量化大语言模型（LLM）代理与人类专家行为差距，发现此差距影响性能，降低差距可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的任务导向对话系统（TODS）在零样本场景有性能挑战，驱动性能差距的行为因素未充分研究。

Method: 提出综合评估框架，量化AI代理和人类专家在对话行为、工具使用和知识利用方面的行为差距。

Result: 行为差距是影响LLM代理性能的关键因素，任务越复杂差距越大，GPT - 4o代理在复杂任务上与人类行为一致性低，降低行为差距平均可提升24.3%性能。

Conclusion: 强调综合行为评估和改进对齐策略对提升基于LLM的TODS处理复杂任务有效性的重要性。

Abstract: Large Language Model (LLM)-based agents have significantly impacted
Task-Oriented Dialog Systems (TODS) but continue to face notable performance
challenges, especially in zero-shot scenarios. While prior work has noted this
performance gap, the behavioral factors driving the performance gap remain
under-explored. This study proposes a comprehensive evaluation framework to
quantify the behavior gap between AI agents and human experts, focusing on
discrepancies in dialog acts, tool usage, and knowledge utilization. Our
findings reveal that this behavior gap is a critical factor negatively
impacting the performance of LLM agents. Notably, as task complexity increases,
the behavior gap widens (correlation: 0.963), leading to a degradation of agent
performance on complex task-oriented dialogs. For the most complex task in our
study, even the GPT-4o-based agent exhibits low alignment with human behavior,
with low F1 scores for dialog acts (0.464), excessive and often misaligned tool
usage with a F1 score of 0.139, and ineffective usage of external knowledge.
Reducing such behavior gaps leads to significant performance improvement (24.3%
on average). This study highlights the importance of comprehensive behavioral
evaluations and improved alignment strategies to enhance the effectiveness of
LLM-based TODS in handling complex tasks.

</details>


### [464] [Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning](https://arxiv.org/abs/2506.12307)
*Xiaotian Zhang,Yuan Wang,Zhaopeng Feng,Ruizhe Chen,Zhijie Zhou,Yan Zhang,Hongxia Xu,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 提出统一框架Med - U1用于医疗问答任务，采用强化学习和多目标奖励优化，提升多基准测试表现且泛化性好，还给出训练策略等见解并将开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有医疗问答缺乏统一框架，推理增强大语言模型在全面医学理解上探索不足。

Method: 采用纯大规模强化学习，使用混合基于规则的二元奖励函数并加入长度惩罚，进行多目标奖励优化。

Result: Med - U1显著提升多个具有挑战性的医疗问答基准测试的表现，超越更大的专业和专有模型，且对分布外任务有强大泛化能力。

Conclusion: 提出的Med - U1框架有效，同时对医疗大语言模型的训练策略、推理链长度控制和奖励设计提供了见解。

Abstract: Medical Question-Answering (QA) encompasses a broad spectrum of tasks,
including multiple choice questions (MCQ), open-ended text generation, and
complex computational reasoning. Despite this variety, a unified framework for
delivering high-quality medical QA has yet to emerge. Although recent progress
in reasoning-augmented large language models (LLMs) has shown promise, their
ability to achieve comprehensive medical understanding is still largely
unexplored. In this paper, we present Med-U1, a unified framework for robust
reasoning across medical QA tasks with diverse output formats, ranging from
MCQs to complex generation and computation tasks. Med-U1 employs pure
large-scale reinforcement learning with mixed rule-based binary reward
functions, incorporating a length penalty to manage output verbosity. With
multi-objective reward optimization, Med-U1 directs LLMs to produce concise and
verifiable reasoning chains. Empirical results reveal that Med-U1 significantly
improves performance across multiple challenging Med-QA benchmarks, surpassing
even larger specialized and proprietary models. Furthermore, Med-U1
demonstrates robust generalization to out-of-distribution (OOD) tasks.
Extensive analysis presents insights into training strategies, reasoning chain
length control, and reward design for medical LLMs. The code will be released.

</details>


### [465] [Intersectional Bias in Japanese Large Language Models from a Contextualized Perspective](https://arxiv.org/abs/2506.12327)
*Hitomi Yanaka,Xinqi He,Jie Lu,Namgi Han,Sunjin Oh,Ryoma Kumon,Yuma Matsuoka,Katsuhiko Watabe,Yuko Itatsu*

Main category: cs.CL

TL;DR: 构建日语基准 inter - JBBQ 评估大语言模型的交叉偏见，分析发现偏见输出随语境变化。


<details>
  <summary>Details</summary>
Motivation: 多数研究关注大语言模型单一社会属性的偏见，而社会科学研究表明社会偏见常以交叉性形式出现，因此有必要评估交叉偏见。

Method: 构建日语基准 inter - JBBQ，在问答场景下评估大语言模型的交叉偏见，并使用该基准分析 GPT - 4o 和 Swallow。

Result: 即使社会属性组合相同，有偏见的输出也会根据其语境而变化。

Conclusion: 强调了在评估大语言模型社会偏见时考虑交叉性和语境因素的重要性。

Abstract: An growing number of studies have examined the social bias of rapidly
developed large language models (LLMs). Although most of these studies have
focused on bias occurring in a single social attribute, research in social
science has shown that social bias often occurs in the form of
intersectionality -- the constitutive and contextualized perspective on bias
aroused by social attributes. In this study, we construct the Japanese
benchmark inter-JBBQ, designed to evaluate the intersectional bias in LLMs on
the question-answering setting. Using inter-JBBQ to analyze GPT-4o and Swallow,
we find that biased output varies according to its contexts even with the equal
combination of social attributes.

</details>


### [466] [Refract ICL: Rethinking Example Selection in the Era of Million-Token Models](https://arxiv.org/abs/2506.12346)
*Arjun R. Akula,Kazuma Hashimoto,Krishna Srinivasan,Aditi Chaudhary,Karthik Raman,Michael Bendersky*

Main category: cs.CL

TL;DR: 研究长上下文大语言模型下传统ICL选择策略有效性，提出Refract ICL算法提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究长上下文大语言模型使用大量演示时，传统ICL选择策略是否依然有效。

Method: 引入Refract ICL算法，通过策略性重复示例和引入零样本预测作为错误信号，聚焦LLM对挑战性示例的注意力。

Result: Refract ICL显著提升了如Gemini 1.5 Pro等极长上下文模型的性能，尤其在输出类别较少的任务上。

Conclusion: 即使有大量演示，智能ICL选择仍至关重要，Refract ICL算法有效。

Abstract: The emergence of long-context large language models (LLMs) has enabled the
use of hundreds, or even thousands, of demonstrations for in-context learning
(ICL) - a previously impractical regime. This paper investigates whether
traditional ICL selection strategies, which balance the similarity of ICL
examples to the test input (using a text retriever) with diversity within the
ICL set, remain effective when utilizing a large number of demonstrations. Our
experiments demonstrate that, while longer contexts can accommodate more
examples, simply increasing the number of demonstrations does not guarantee
improved performance. Smart ICL selection remains crucial, even with thousands
of demonstrations. To further enhance ICL in this setting, we introduce Refract
ICL, a novel ICL selection algorithm specifically designed to focus LLM
attention on challenging examples by strategically repeating them within the
context and incorporating zero-shot predictions as error signals. Our results
show that Refract ICL significantly improves the performance of extremely
long-context models such as Gemini 1.5 Pro, particularly on tasks with a
smaller number of output classes.

</details>


### [467] [Efficient Reasoning Through Suppression of Self-Affirmation Reflections in Large Reasoning Models](https://arxiv.org/abs/2506.12353)
*Kaiyuan Liu,Chen Shen,Zhanwei Zhang,Junjie Liu,Xiaosong Yuan,Jieping ye*

Main category: cs.CL

TL;DR: 论文聚焦推理模型中的自我肯定反思问题，发现其特征并通过实验证明抑制该反思可压缩输出长度且不降低准确率。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型优化方法存在‘过度思考’问题且缺乏细粒度分析，为解决输出长度快速增长、实现高效推理。

Method: 观察原始和优化推理模型，分析自我肯定反思特征，进行无训练实验抑制反思，改进基于训练的方法。

Result: 无训练设置下R1 - Distill - Qwen - 1.5B实现18.7%的长度压缩，基于训练设置下实现50.2%的长度压缩。

Conclusion: 研究结果为实现更精确的长度压缩和步级高效推理提供思路，改进方法实用可应用于现有推理框架。

Abstract: While recent advances in large reasoning models have demonstrated remarkable
performance, efficient reasoning remains critical due to the rapid growth of
output length. Existing optimization approaches highlights a tendency toward
"overthinking", yet lack fine-grained analysis. In this work, we focus on
Self-Affirmation Reflections: redundant reflective steps that affirm prior
content and often occurs after the already correct reasoning steps.
Observations of both original and optimized reasoning models reveal pervasive
self-affirmation reflections. Notably, these reflections sometimes lead to
longer outputs in optimized models than their original counterparts. Through
detailed analysis, we uncover an intriguing pattern: compared to other
reflections, the leading words (i.e., the first word of sentences) in
self-affirmation reflections exhibit a distinct probability bias. Motivated by
this insight, we can locate self-affirmation reflections and conduct a
train-free experiment demonstrating that suppressing self-affirmation
reflections reduces output length without degrading accuracy across multiple
models (R1-Distill-Models, QwQ-32B, and Qwen3-32B). Furthermore, we also
improve current train-based method by explicitly suppressing such reflections.
In our experiments, we achieve length compression of 18.7\% in train-free
settings and 50.2\% in train-based settings for R1-Distill-Qwen-1.5B. Moreover,
our improvements are simple yet practical and can be directly applied to
existing inference frameworks, such as vLLM. We believe that our findings will
provide community insights for achieving more precise length compression and
step-level efficient reasoning.

</details>


### [468] [Training-free LLM Merging for Multi-task Learning](https://arxiv.org/abs/2506.12379)
*Zichuan Fu,Xian Wu,Yejing Wang,Wanyu Wang,Shanshan Ye,Hongzhi Yin,Yi Chang,Yefeng Zheng,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 本文探讨将多个专门大语言模型统一为多任务模型的问题，提出免训练的Hierarchical Iterative Merging（Hi - Merging）方法，实验表明其优于现有合并技术。


<details>
  <summary>Details</summary>
Motivation: 探索能否将专门的大语言模型合并为具有多任务能力的统一模型。

Method: 提出Hi - Merging方法，通过模型级和层级别剪枝与缩放，结合贡献分析来减少参数冲突。

Result: 在中英文多项选择和问答任务上的实验表明，Hi - Merging始终优于现有合并技术，多数情况下超过在组合数据集上微调的模型。

Conclusion: Hi - Merging方法有效，能将不同专门大语言模型统一为单模型进行多任务学习。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities
across diverse natural language processing (NLP) tasks. The release of
open-source LLMs like LLaMA and Qwen has triggered the development of numerous
fine-tuned models tailored for various tasks and languages. In this paper, we
explore an important question: is it possible to combine these specialized
models to create a unified model with multi-task capabilities. We introduces
Hierarchical Iterative Merging (Hi-Merging), a training-free method for
unifying different specialized LLMs into a single model. Specifically,
Hi-Merging employs model-wise and layer-wise pruning and scaling, guided by
contribution analysis, to mitigate parameter conflicts. Extensive experiments
on multiple-choice and question-answering tasks in both Chinese and English
validate Hi-Merging's ability for multi-task learning. The results demonstrate
that Hi-Merging consistently outperforms existing merging techniques and
surpasses the performance of models fine-tuned on combined datasets in most
scenarios. Code is available at:
https://github.com/Applied-Machine-Learning-Lab/Hi-Merging.

</details>


### [469] [Recent Advances and Future Directions in Literature-Based Discovery](https://arxiv.org/abs/2506.12385)
*Andrej Kastrin,Bojan Cestnik,Nada Lavrač*

Main category: cs.CL

TL;DR: 文章调查2000年至今文献发现法（LBD）的方法进展，涉及知识图谱构建等三方面，指出LBD存在挑战，强调大语言模型作用。


<details>
  <summary>Details</summary>
Motivation: 科学出版物爆炸式增长，需要自动化方法促进知识合成和假设生成，LBD可应对此挑战，文章旨在调查其方法进展。

Method: 对2000年至今LBD在知识图谱构建、深度学习方法、预训练和大语言模型整合三方面的进展进行综述。

Result: LBD取得显著进展，但存在可扩展性、依赖结构化数据、需大量手动整理等挑战。

Conclusion: 强调大语言模型在提升LBD中的变革性作用，支持科研人员利用技术加速科学创新。

Abstract: The explosive growth of scientific publications has created an urgent need
for automated methods that facilitate knowledge synthesis and hypothesis
generation. Literature-based discovery (LBD) addresses this challenge by
uncovering previously unknown associations between disparate domains. This
article surveys recent methodological advances in LBD, focusing on developments
from 2000 to the present. We review progress in three key areas: knowledge
graph construction, deep learning approaches, and the integration of
pre-trained and large language models (LLMs). While LBD has made notable
progress, several fundamental challenges remain unresolved, particularly
concerning scalability, reliance on structured data, and the need for extensive
manual curation. By examining ongoing advances and outlining promising future
directions, this survey underscores the transformative role of LLMs in
enhancing LBD and aims to support researchers and practitioners in harnessing
these technologies to accelerate scientific innovation.

</details>


### [470] [Group then Scale: Dynamic Mixture-of-Experts Multilingual Language Model](https://arxiv.org/abs/2506.12388)
*Chong Li,Yingzhuo Deng,Jiajun Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 提出动态分组和扩展参数方法解决多语言大模型多语言诅咒问题，减少语言间负迁移，提升性能。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型存在多语言诅咒现象，因容量有限和不同语言负迁移导致性能不佳。

Method: 先在单语语料上微调模型确定各层参数偏差、量化语言相似度，将偏差大的层扩展为专家混合层，一个专家模块服务一组相似语言。

Result: 在18到128种语言实验中，该方法减少语言间负迁移，用更少参数显著提升多语言性能。

Conclusion: 专家的语言分组专业化有利于新语言适应，减少对先前多语言知识学习的推理。

Abstract: The curse of multilinguality phenomenon is a fundamental problem of
multilingual Large Language Models (LLMs), where the competition between
massive languages results in inferior performance. It mainly comes from limited
capacity and negative transfer between dissimilar languages. To address this
issue, we propose a method to dynamically group and scale up the parameters of
multilingual LLM while boosting positive transfer among similar languages.
Specifically, the model is first tuned on monolingual corpus to determine the
parameter deviation in each layer and quantify the similarity between
languages. Layers with more deviations are extended to mixture-of-experts
layers to reduce competition between languages, where one expert module serves
one group of similar languages. Experimental results on 18 to 128 languages
show that our method reduces the negative transfer between languages and
significantly boosts multilingual performance with fewer parameters. Such
language group specialization on experts benefits the new language adaptation
and reduces the inference on the previous multilingual knowledge learned.

</details>


### [471] [Enhancing Traffic Accident Classifications: Application of NLP Methods for City Safety](https://arxiv.org/abs/2506.12092)
*Enes Özeren,Alexander Ulbrich,Sascha Filimon,David Rügamer,Andreas Bender*

Main category: cs.CL

TL;DR: 本文分析慕尼黑交通事故数据，用NLP方法发现标签不一致问题，开发分类模型，表明文本描述对分类最重要。


<details>
  <summary>Details</summary>
Motivation: 全面理解交通事故对提升城市安全和辅助政策决策至关重要，需识别不同事故类型的模式和特征。

Method: 分析含结构化表格特征和非结构化文本描述的交通事故数据集，应用NLP方法评估标签可靠性，开发分类模型。

Result: 模型能高精度分类事故，文本描述含最具信息性特征，表格数据仅带来少量提升。

Conclusion: 强调自由文本数据在事故分析中的关键作用，突出基于Transformer的模型提升分类可靠性的潜力。

Abstract: A comprehensive understanding of traffic accidents is essential for improving
city safety and informing policy decisions. In this study, we analyze traffic
incidents in Munich to identify patterns and characteristics that distinguish
different types of accidents. The dataset consists of both structured tabular
features, such as location, time, and weather conditions, as well as
unstructured free-text descriptions detailing the circumstances of each
accident. Each incident is categorized into one of seven predefined classes. To
assess the reliability of these labels, we apply NLP methods, including topic
modeling and few-shot learning, which reveal inconsistencies in the labeling
process. These findings highlight potential ambiguities in accident
classification and motivate a refined predictive approach. Building on these
insights, we develop a classification model that achieves high accuracy in
assigning accidents to their respective categories. Our results demonstrate
that textual descriptions contain the most informative features for
classification, while the inclusion of tabular data provides only marginal
improvements. These findings emphasize the critical role of free-text data in
accident analysis and highlight the potential of transformer-based models in
improving classification reliability.

</details>


### [472] [A Pluggable Multi-Task Learning Framework for Sentiment-Aware Financial Relation Extraction](https://arxiv.org/abs/2506.12452)
*Jinming Luo,Hailin Wang*

Main category: cs.CL

TL;DR: 本文提出SSDP - SEM多任务学习方法增强金融关系抽取，实验表明能让多数模型取得更好结果。


<details>
  <summary>Details</summary>
Motivation: 不同领域的关系抽取任务受多种因素影响，金融领域中情绪因素被现代关系抽取模型忽略，需加以解决。

Method: 提出Sentiment - aware - SDP - Enhanced - Module (SSDP - SEM)，将关系抽取模型与可插拔的辅助情绪感知任务集成，通过情绪模型生成情绪令牌，辅助任务预测令牌位置以捕捉情绪信息，结合句法信息的最短依赖路径，并采用情绪注意力信息瓶颈正则化方法调节推理过程。

Result: 将辅助任务与多个流行框架集成，多数先前模型受益于该辅助任务，取得更好结果。

Conclusion: 在金融关系抽取任务中有效利用情绪信息很重要。

Abstract: Relation Extraction (RE) aims to extract semantic relationships in texts from
given entity pairs, and has achieved significant improvements. However, in
different domains, the RE task can be influenced by various factors. For
example, in the financial domain, sentiment can affect RE results, yet this
factor has been overlooked by modern RE models. To address this gap, this paper
proposes a Sentiment-aware-SDP-Enhanced-Module (SSDP-SEM), a multi-task
learning approach for enhancing financial RE. Specifically, SSDP-SEM integrates
the RE models with a pluggable auxiliary sentiment perception (ASP) task,
enabling the RE models to concurrently navigate their attention weights with
the text's sentiment. We first generate detailed sentiment tokens through a
sentiment model and insert these tokens into an instance. Then, the ASP task
focuses on capturing nuanced sentiment information through predicting the
sentiment token positions, combining both sentiment insights and the Shortest
Dependency Path (SDP) of syntactic information. Moreover, this work employs a
sentiment attention information bottleneck regularization method to regulate
the reasoning process. Our experiment integrates this auxiliary task with
several prevalent frameworks, and the results demonstrate that most previous
models benefit from the auxiliary task, thereby achieving better results. These
findings highlight the importance of effectively leveraging sentiment in the
financial RE task.

</details>


### [473] [Towards Fairness Assessment of Dutch Hate Speech Detection](https://arxiv.org/abs/2506.12502)
*Julie Bauer,Rishabh Kaushal,Thales Bertaglia,Adriana Iamnitchi*

Main category: cs.CL

TL;DR: 本文评估荷兰语仇恨言论检测模型的反事实公平性，构建数据集、生成反事实数据、微调模型并评估性能与公平性，模型表现良好，填补相关文献空白。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测研究多关注英语且侧重模型开发，本文旨在评估荷兰语仇恨言论检测模型的反事实公平性。

Method: 构建荷兰社会群体术语列表，用大语言模型和策略生成反事实数据，微调基于transformer的基线模型，用CTF和群体公平性指标评估模型公平性。

Result: 模型在仇恨言论检测、平均反事实公平性和群体公平性方面表现更好。

Conclusion: 本研究填补荷兰语仇恨言论检测反事实公平性文献空白，为提升模型性能和公平性提供实用见解与建议。

Abstract: Numerous studies have proposed computational methods to detect hate speech
online, yet most focus on the English language and emphasize model development.
In this study, we evaluate the counterfactual fairness of hate speech detection
models in the Dutch language, specifically examining the performance and
fairness of transformer-based models. We make the following key contributions.
First, we curate a list of Dutch Social Group Terms that reflect social
context. Second, we generate counterfactual data for Dutch hate speech using
LLMs and established strategies like Manual Group Substitution (MGS) and
Sentence Log-Likelihood (SLL). Through qualitative evaluation, we highlight the
challenges of generating realistic counterfactuals, particularly with Dutch
grammar and contextual coherence. Third, we fine-tune baseline
transformer-based models with counterfactual data and evaluate their
performance in detecting hate speech. Fourth, we assess the fairness of these
models using Counterfactual Token Fairness (CTF) and group fairness metrics,
including equality of odds and demographic parity. Our analysis shows that
models perform better in terms of hate speech detection, average counterfactual
fairness and group fairness. This work addresses a significant gap in the
literature on counterfactual fairness for hate speech detection in Dutch and
provides practical insights and recommendations for improving both model
performance and fairness.

</details>


### [474] [Speech-Language Models with Decoupled Tokenizers and Multi-Token Prediction](https://arxiv.org/abs/2506.12537)
*Xiaoran Fan,Zhichao Sun,Yangfan Gao,Jingfei Xiong,Hang Yan,Yifei Cao,Jiajun Sun,Shuo Li,Zhihao Zhang,Zhiheng Xi,Yuhao Zhou,Senjie Jin,Changhao Jiang,Junjie Ye,Ming Zhang,Rui Zheng,Zhenhua Han,Yunke Zhang,Demei Yan,Shaokang Dong,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文系统研究关键组件对以大语言模型为中心的语音语言模型（SLMs）性能的影响，提出改进方法并通过实验验证效果。


<details>
  <summary>Details</summary>
Motivation: 解决SLMs在跨模态对齐和高质量语音生成方面的挑战。

Method: 比较不同语音分词器；引入多令牌预测（MTP）；提出说话人感知生成范式并引入RoleTriviaQA基准。

Result: 解耦分词显著提高对齐和合成质量；MTP使解码速度提高12倍，字错误率大幅下降；方法增强知识理解和说话人一致性。

Conclusion: 所提出的方法有效提升了SLMs的性能。

Abstract: Speech-language models (SLMs) offer a promising path toward unifying speech
and text understanding and generation. However, challenges remain in achieving
effective cross-modal alignment and high-quality speech generation. In this
work, we systematically investigate the impact of key components (i.e., speech
tokenizers, speech heads, and speaker modeling) on the performance of
LLM-centric SLMs. We compare coupled, semi-decoupled, and fully decoupled
speech tokenizers under a fair SLM framework and find that decoupled
tokenization significantly improves alignment and synthesis quality. To address
the information density mismatch between speech and text, we introduce
multi-token prediction (MTP) into SLMs, enabling each hidden state to decode
multiple speech tokens. This leads to up to 12$\times$ faster decoding and a
substantial drop in word error rate (from 6.07 to 3.01). Furthermore, we
propose a speaker-aware generation paradigm and introduce RoleTriviaQA, a
large-scale role-playing knowledge QA benchmark with diverse speaker
identities. Experiments demonstrate that our methods enhance both knowledge
understanding and speaker consistency.

</details>


### [475] [RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking](https://arxiv.org/abs/2506.12538)
*Shuo Yang,Yuqin Dai,Guoqing Wang,Xinran Zheng,Jinfeng Xu,Jinze Li,Zhenzhe Ying,Weiqiang Wang,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: 现有基准无法全面评估大语言模型在现实场景的事实核查能力，本文提出RealFactBench基准及Unknown Rate指标评估，实验揭示模型局限。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法在现实误信息场景全面评估大语言模型和多模态大语言模型的事实核查能力。

Method: 引入RealFactBench基准，包含6K高质量声明，涵盖多模态内容和不同领域；引入Unknown Rate指标。

Result: 对7个代表性大语言模型和4个多模态大语言模型实验，揭示它们在现实事实核查中的局限。

Conclusion: RealFactBench能有效评估模型，为后续研究提供有价值见解，代码公开。

Abstract: Large Language Models (LLMs) hold significant potential for advancing
fact-checking by leveraging their capabilities in reasoning, evidence
retrieval, and explanation generation. However, existing benchmarks fail to
comprehensively evaluate LLMs and Multimodal Large Language Models (MLLMs) in
realistic misinformation scenarios. To bridge this gap, we introduce
RealFactBench, a comprehensive benchmark designed to assess the fact-checking
capabilities of LLMs and MLLMs across diverse real-world tasks, including
Knowledge Validation, Rumor Detection, and Event Verification. RealFactBench
consists of 6K high-quality claims drawn from authoritative sources,
encompassing multimodal content and diverse domains. Our evaluation framework
further introduces the Unknown Rate (UnR) metric, enabling a more nuanced
assessment of models' ability to handle uncertainty and balance between
over-conservatism and over-confidence. Extensive experiments on 7
representative LLMs and 4 MLLMs reveal their limitations in real-world
fact-checking and offer valuable insights for further research. RealFactBench
is publicly available at https://github.com/kalendsyang/RealFactBench.git.

</details>


### [476] [Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking Methodology of Human Experts](https://arxiv.org/abs/2506.12552)
*Zain Muhammad Mujahid,Dilshod Azizov,Maha Tufail Agro,Preslav Nakov*

Main category: cs.CL

TL;DR: 提出用大语言模型评估新闻媒体可信度和政治偏见的新方法，实验有提升并做了分析，公开数据和代码。


<details>
  <summary>Details</summary>
Motivation: 在网络虚假信息泛滥时代，现有事实核查对新兴声明有挑战，评估新闻来源可靠性和政治偏见是重要但研究不足的方向。

Method: 设计基于专业事实核查标准的提示，从大语言模型获取回应并聚合预测。

Result: 实验显示比强基线有显著提升，进行了错误分析和消融研究。

Conclusion: 该方法有效，公开数据集和代码以促进未来研究。

Abstract: In an age characterized by the proliferation of mis- and disinformation
online, it is critical to empower readers to understand the content they are
reading. Important efforts in this direction rely on manual or automatic
fact-checking, which can be challenging for emerging claims with limited
information. Such scenarios can be handled by assessing the reliability and the
political bias of the source of the claim, i.e., characterizing entire news
outlets rather than individual claims or articles. This is an important but
understudied research direction. While prior work has looked into linguistic
and social contexts, we do not analyze individual articles or information in
social media. Instead, we propose a novel methodology that emulates the
criteria that professional fact-checkers use to assess the factuality and
political bias of an entire outlet. Specifically, we design a variety of
prompts based on these criteria and elicit responses from large language models
(LLMs), which we aggregate to make predictions. In addition to demonstrating
sizable improvements over strong baselines via extensive experiments with
multiple LLMs, we provide an in-depth error analysis of the effect of media
popularity and region on model performance. Further, we conduct an ablation
study to highlight the key components of our dataset that contribute to these
improvements. To facilitate future research, we released our dataset and code
at https://github.com/mbzuai-nlp/llm-media-profiling.

</details>


### [477] [Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders](https://arxiv.org/abs/2506.12576)
*Ananya Joshi,Celia Cintas,Skyler Speakman*

Main category: cs.CL

TL;DR: 利用SAE特性实现任意主题对齐，实验显示比微调有优势且开源代码可用


<details>
  <summary>Details</summary>
Motivation: 现有SAE修改只能针对预识别主题且需调参，要实现任意主题对齐

Method: 对SAE神经元按与对齐文本语义相似度评分，修改SAE层输出以强调主题对齐神经元

Result: 在多种数据集和模型对上实验，医学提示对齐中比微调有语言可接受性提升、训练时间减少等优势

Conclusion: 该方法能有效实现任意主题对齐，有实际应用价值

Abstract: Recent work shows that Sparse Autoencoders (SAE) applied to large language
model (LLM) layers have neurons corresponding to interpretable concepts. These
SAE neurons can be modified to align generated outputs, but only towards
pre-identified topics and with some parameter tuning. Our approach leverages
the observational and modification properties of SAEs to enable alignment for
any topic. This method 1) scores each SAE neuron by its semantic similarity to
an alignment text and uses them to 2) modify SAE-layer-level outputs by
emphasizing topic-aligned neurons. We assess the alignment capabilities of this
approach on diverse public topic datasets including Amazon reviews, Medicine,
and Sycophancy, across the currently available open-source LLMs and SAE pairs
(GPT2 and Gemma) with multiple SAEs configurations. Experiments aligning to
medical prompts reveal several benefits over fine-tuning, including increased
average language acceptability (0.25 vs. 0.5), reduced training time across
multiple alignment topics (333.6s vs. 62s), and acceptable inference time for
many applications (+0.00092s/token). Our open-source code is available at
github.com/IBM/sae-steering.

</details>


### [478] [An Exploration of Mamba for Speech Self-Supervised Models](https://arxiv.org/abs/2506.12606)
*Tzu-Quan Lin,Heng-Cheng Kuo,Tzu-Chieh Wei,Hsi-Chun Cheng,Chun-Wei Chen,Hsien-Fu Hsiao,Yu Tsao,Hung-yi Lee*

Main category: cs.CL

TL;DR: 探索基于Mamba的HuBERT模型用于语音自监督学习，在多任务中表现良好，是长序列等语音建模的有前景方向。


<details>
  <summary>Details</summary>
Motivation: Mamba在语音自监督学习中的潜力未充分挖掘，此前研究局限于孤立任务，需探索其在语音自监督模型中的应用。

Method: 利用线性时间选择性状态空间，探索基于Mamba的HuBERT模型作为基于Transformer的自监督架构的替代方案。

Result: 该模型在长上下文ASR微调时计算量显著降低，在流式ASR微调中表现更优，在SUPERB探测基准测试中表现有竞争力，能产生更高质量量化表示，更清晰捕捉说话人相关特征。

Conclusion: 基于Mamba的自监督学习是长序列建模、实时语音建模和语音单元提取的有前景且互补的方向。

Abstract: While Mamba has demonstrated strong performance in language modeling, its
potential as a speech self-supervised (SSL) model remains underexplored, with
prior studies limited to isolated tasks. To address this, we explore
Mamba-based HuBERT models as alternatives to Transformer-based SSL
architectures. Leveraging the linear-time Selective State Space, these models
enable fine-tuning on long-context ASR with significantly lower compute.
Moreover, they show superior performance when fine-tuned for streaming ASR.
Beyond fine-tuning, these models show competitive performance on SUPERB probing
benchmarks, particularly in causal settings. Our analysis shows that they yield
higher-quality quantized representations and capture speaker-related features
more distinctly than Transformer-based models. These findings highlight
Mamba-based SSL as a promising and complementary direction for long-sequence
modeling, real-time speech modeling, and speech unit extraction.

</details>


### [479] [Konooz: Multi-domain Multi-dialect Corpus for Named Entity Recognition](https://arxiv.org/abs/2506.12615)
*Nagham Hamad,Mohammed Khalilia,Mustafa Jarrar*

Main category: cs.CL

TL;DR: 介绍多维度阿拉伯语语料库Konooz，用于基准测试阿拉伯语命名实体识别模型，揭示性能下降情况并分析相关影响因素，语料库开源。


<details>
  <summary>Details</summary>
Motivation: 创建多维度阿拉伯语语料库并对现有阿拉伯语命名实体识别模型进行基准测试，特别是跨领域和跨方言模型性能。

Method: 创建涵盖16种阿拉伯方言、10个领域的Konooz语料库，手动标注21种实体类型，使用MMD指标测量领域和方言重叠。

Result: 使用Konooz对四个阿拉伯语NER模型进行基准测试，发现与分布内数据相比性能最多下降38%，并进行了领域和方言差异及资源稀缺影响的深入分析。

Conclusion: 展示了不同NER模型在特定方言和领域表现不同的原因，Konooz语料库可用于多种NLP任务且开源可用。

Abstract: We introduce Konooz, a novel multi-dimensional corpus covering 16 Arabic
dialects across 10 domains, resulting in 160 distinct corpora. The corpus
comprises about 777k tokens, carefully collected and manually annotated with 21
entity types using both nested and flat annotation schemes - using the Wojood
guidelines. While Konooz is useful for various NLP tasks like domain adaptation
and transfer learning, this paper primarily focuses on benchmarking existing
Arabic Named Entity Recognition (NER) models, especially cross-domain and
cross-dialect model performance. Our benchmarking of four Arabic NER models
using Konooz reveals a significant drop in performance of up to 38% when
compared to the in-distribution data. Furthermore, we present an in-depth
analysis of domain and dialect divergence and the impact of resource scarcity.
We also measured the overlap between domains and dialects using the Maximum
Mean Discrepancy (MMD) metric, and illustrated why certain NER models perform
better on specific dialects and domains. Konooz is open-source and publicly
available at https://sina.birzeit.edu/wojood/#download

</details>


### [480] [Flexible Realignment of Language Models](https://arxiv.org/abs/2506.12704)
*Wenhong Zhu,Ruobing Xie,Weinan Zhang,Rui Wang*

Main category: cs.CL

TL;DR: 提出灵活的语言模型重新对齐框架，含训练和推理阶段对齐方法，有良好效果。


<details>
  <summary>Details</summary>
Motivation: 语言模型未达预期性能时需重新对齐，因此提出灵活框架以定量控制对齐程度。

Method: 训练时采用Training - time Realignment (TrRa)，利用参考模型和已对齐模型的对数几率可控融合来重新对齐；推理时引入层适配器实现Inference - time Realignment (InRa)。

Result: TrRa在DeepSeek - R1 - Distill - Qwen - 1.5B上减少54.63%的令牌使用且性能无下降，优于DeepScaleR - 1.5B；将DeepSeek - R1 - Distill - Qwen - 7B升级为支持快慢思考的模型，还提升了性能。

Conclusion: 所提框架能有效实现语言模型重新对齐，在训练和推理阶段都能进行灵活对齐控制，提升模型性能。

Abstract: Realignment becomes necessary when a language model (LM) fails to meet
expected performance. We propose a flexible realignment framework that supports
quantitative control of alignment degree during training and inference. This
framework incorporates Training-time Realignment (TrRa), which efficiently
realigns the reference model by leveraging the controllable fusion of logits
from both the reference and already aligned models. For example, TrRa reduces
token usage by 54.63% on DeepSeek-R1-Distill-Qwen-1.5B without any performance
degradation, outperforming DeepScaleR-1.5B's 33.86%. To complement TrRa during
inference, we introduce a layer adapter that enables smooth Inference-time
Realignment (InRa). This adapter is initialized to perform an identity
transformation at the bottom layer and is inserted preceding the original
layers. During inference, input embeddings are simultaneously processed by the
adapter and the original layer, followed by the remaining layers, and then
controllably interpolated at the logit level. We upgraded
DeepSeek-R1-Distill-Qwen-7B from a slow-thinking model to one that supports
both fast and slow thinking, allowing flexible alignment control even during
inference. By encouraging deeper reasoning, it even surpassed its original
performance.

</details>


### [481] [Missing the human touch? A computational stylometry analysis of GPT-4 translations of online Chinese literature](https://arxiv.org/abs/2506.13013)
*Xiaofang Yao,Yong-Bin Kang,Anthony McCosker*

Main category: cs.CL

TL;DR: 研究对比GPT - 4与人工翻译在中文网络文学任务中的风格特征，发现GPT - 4译文与人工译文在多方面相近，揭示AI对文学翻译影响。


<details>
  <summary>Details</summary>
Motivation: 现有文学机器翻译常不令人满意，评估多不关注风格特征，且缺乏关于大语言模型对文学翻译影响的证据。

Method: 对比GPT - 4和人工翻译在中文网络文学任务中的表现，采用计算文体学分析。

Result: GPT - 4译文在词汇、句法和内容特征上与人工译文紧密一致。

Conclusion: 从后人类视角看，机器和人工翻译的区别日益模糊，研究为AI对文学翻译的影响提供见解。

Abstract: Existing research indicates that machine translations (MTs) of literary texts
are often unsatisfactory. MTs are typically evaluated using automated metrics
and subjective human ratings, with limited focus on stylistic features.
Evidence is also limited on whether state-of-the-art large language models
(LLMs) will reshape literary translation. This study examines the stylistic
features of LLM translations, comparing GPT-4's performance to human
translations in a Chinese online literature task. Computational stylometry
analysis shows that GPT-4 translations closely align with human translations in
lexical, syntactic, and content features, suggesting that LLMs might replicate
the 'human touch' in literary translation style. These findings offer insights
into AI's impact on literary translation from a posthuman perspective, where
distinctions between machine and human translations become increasingly blurry.

</details>


### [482] [Edeflip: Supervised Word Translation between English and Yoruba](https://arxiv.org/abs/2506.13020)
*Ikeoluwa Abioye,Jiani Ge*

Main category: cs.CL

TL;DR: 研究将监督嵌入对齐方法用于英语到约鲁巴语（低资源语言）的单词翻译，发现嵌入质量和归一化对翻译精度有影响，指出当前方法在低资源语言上的局限。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入对齐研究多关注高资源语言，不清楚低资源语言能否及如何从中受益。

Method: 采用已有的监督嵌入对齐方法进行英语到约鲁巴语的单词翻译。

Result: 更高的嵌入质量和归一化嵌入可提高单词翻译精度，二者存在交互作用；当前监督嵌入对齐方法在低资源语言上有局限。

Conclusion: 希望该研究成为考虑低资源语言挑战的机器翻译研究的起点。

Abstract: In recent years, embedding alignment has become the state-of-the-art machine
translation approach, as it can yield high-quality translation without training
on parallel corpora. However, existing research and application of embedding
alignment mostly focus on high-resource languages with high-quality monolingual
embeddings. It is unclear if and how low-resource languages may be similarly
benefited. In this study, we implement an established supervised embedding
alignment method for word translation from English to Yoruba, the latter a
low-resource language. We found that higher embedding quality and normalizing
embeddings increase word translation precision, with, additionally, an
interaction effect between the two. Our results demonstrate the limitations of
the state-of-the-art supervised embedding alignment when it comes to
low-resource languages, for which there are additional factors that need to be
taken into consideration, such as the importance of curating high-quality
monolingual embeddings. We hope our work will be a starting point for further
machine translation research that takes into account the challenges that
low-resource languages face.

</details>


### [483] [Transforming Chatbot Text: A Sequence-to-Sequence Approach](https://arxiv.org/abs/2506.12843)
*Natesh Reddy,Mark Stamp*

Main category: cs.CL

TL;DR: 本文提出用Seq2Seq模型对抗性转换GPT生成文本使其更像人类写作，实验表明转换后分类模型准确性降低，但再训练后可准确区分，推进了对AI生成文本的理解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使人类与AI生成文本界限模糊，虽有检测方法，但本文想让GPT生成文本更像人类写作。

Method: 采用Seq2Seq模型（T5 - small和BART）对抗性转换GPT生成文本，加入人类写作典型的语言、结构和语义成分。

Result: 分类模型对转换后文本检测准确性显著降低，但在转换后数据上再训练，可高精度区分转换后GPT生成文本和人类生成文本。

Conclusion: 文本转换可作为攻击和防御工具，推进了对AI生成文本的理解。

Abstract: Due to advances in Large Language Models (LLMs) such as ChatGPT, the boundary
between human-written text and AI-generated text has become blurred.
Nevertheless, recent work has demonstrated that it is possible to reliably
detect GPT-generated text. In this paper, we adopt a novel strategy to
adversarially transform GPT-generated text using sequence-to-sequence (Seq2Seq)
models, with the goal of making the text more human-like. We experiment with
the Seq2Seq models T5-small and BART which serve to modify GPT-generated
sentences to include linguistic, structural, and semantic components that may
be more typical of human-authored text. Experiments show that classification
models trained to distinguish GPT-generated text are significantly less
accurate when tested on text that has been modified by these Seq2Seq models.
However, after retraining classification models on data generated by our
Seq2Seq technique, the models are able to distinguish the transformed
GPT-generated text from human-generated text with high accuracy. This work adds
to the accumulating knowledge of text transformation as a tool for both attack
-- in the sense of defeating classification models -- and defense -- in the
sense of improved classifiers -- thereby advancing our understanding of
AI-generated text.

</details>


### [484] [Just Go Parallel: Improving the Multilingual Capabilities of Large Language Models](https://arxiv.org/abs/2506.13044)
*Muhammad Reza Qorib,Junyi Li,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 研究添加平行数据对大语言模型多语言能力的影响，实验表明平行数据可显著提升其能力。


<details>
  <summary>Details</summary>
Motivation: 探究平行数据对构建多语言模型是否必要，系统研究添加平行数据对大语言模型多语言能力的影响。

Method: 针对翻译和多语言常识推理进行控制实验。

Result: 平行数据能显著提高大语言模型的多语言能力。

Conclusion: 平行数据对提升大语言模型多语言能力有重要作用。

Abstract: Large language models (LLMs) have demonstrated impressive translation
capabilities even without being explicitly trained on parallel data. This
remarkable property has led some to believe that parallel data is no longer
necessary for building multilingual language models. While some attribute this
to the emergent abilities of LLMs due to scale, recent work suggests that it is
actually caused by incidental bilingual signals present in the training data.
Various methods have been proposed to maximize the utility of parallel data to
enhance the multilingual capabilities of multilingual encoder-based and
encoder-decoder language models. However, some decoder-based LLMs opt to ignore
parallel data instead. In this work, we conduct a systematic study on the
impact of adding parallel data on LLMs' multilingual capabilities, focusing
specifically on translation and multilingual common-sense reasoning. Through
controlled experiments, we demonstrate that parallel data can significantly
improve LLMs' multilingual capabilities.

</details>


### [485] [MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?](https://arxiv.org/abs/2506.13065)
*Xixian Yong,Jianxun Lian,Xiaoyuan Yi,Xiao Zhou,Xing Xie*

Main category: cs.CL

TL;DR: 提出MotiveBench评估大语言模型类人动机推理能力，实验表明先进模型仍有不足并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估大语言模型类人动机能力上存在局限，信息与现实不对称。

Method: 提出包含200个丰富上下文场景和600个推理任务的MotiveBench，对七个流行模型族进行实验。

Result: 最先进的大语言模型在类人动机推理上仍有不足，存在推理特定动机困难、过度理性和理想化问题。

Conclusion: 指出大语言模型拟人化研究的有前景方向，数据集等开源。

Abstract: Large language models (LLMs) have been widely adopted as the core of agent
frameworks in various scenarios, such as social simulations and AI companions.
However, the extent to which they can replicate human-like motivations remains
an underexplored question. Existing benchmarks are constrained by simplistic
scenarios and the absence of character identities, resulting in an information
asymmetry with real-world situations. To address this gap, we propose
MotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning
tasks covering multiple levels of motivation. Using MotiveBench, we conduct
extensive experiments on seven popular model families, comparing different
scales and versions within each family. The results show that even the most
advanced LLMs still fall short in achieving human-like motivational reasoning.
Our analysis reveals key findings, including the difficulty LLMs face in
reasoning about "love & belonging" motivations and their tendency toward
excessive rationality and idealism. These insights highlight a promising
direction for future research on the humanization of LLMs. The dataset,
benchmark, and code are available at https://aka.ms/motivebench.

</details>


### [486] [CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope -- Make Your LLM to Get Them Right](https://arxiv.org/abs/2506.13070)
*Jaebok Lee,Yonghyun Ryu,Seongmin Park,Yoonjung Choi*

Main category: cs.CL

TL;DR: 本文介绍用于SemEval 2025任务2的实体感知机器翻译系统，结合RAG和迭代自优化技术，有自评估机制，可提升实体处理和翻译质量。


<details>
  <summary>Details</summary>
Motivation: 提高命名实体翻译的准确性。

Method: 结合检索增强生成（RAG）和使用大语言模型（LLMs）的迭代自优化技术，采用自评估机制，大语言模型根据实体翻译准确性和整体翻译质量评估自身翻译。

Result: 这些方法共同有效提升了实体处理能力，同时保持了高质量的翻译。

Conclusion: 所采用的方法能有效提升实体感知机器翻译的性能。

Abstract: In this paper, we describe our approach for the SemEval 2025 Task 2 on
Entity-Aware Machine Translation (EA-MT). Our system aims to improve the
accuracy of translating named entities by combining two key approaches:
Retrieval Augmented Generation (RAG) and iterative self-refinement techniques
using Large Language Models (LLMs). A distinctive feature of our system is its
self-evaluation mechanism, where the LLM assesses its own translations based on
two key criteria: the accuracy of entity translations and overall translation
quality. We demonstrate how these methods work together and effectively improve
entity handling while maintaining high-quality translations.

</details>


### [487] [Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware Strategies for LLMs and VLMs](https://arxiv.org/abs/2506.13102)
*Gyutaek Oh,Seoyeon Kim,Sangjoon Park,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: 本文全面研究医学领域测试时缩放方法，评估其对大语言模型和视觉 - 语言模型的影响，给出实际应用指南。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放策略在医学领域诸多关键方面未充分探索，如对视觉 - 语言模型的有效性及不同场景下的最优策略。

Method: 评估测试时缩放对大语言模型和视觉 - 语言模型的影响，考虑模型大小、特性和任务复杂度等因素，评估策略在用户驱动因素下的鲁棒性。

Result: 获得测试时缩放策略在医学应用中的相关情况。

Conclusion: 为医学应用中有效使用测试时缩放提供实用指南，为策略进一步优化提供见解。

Abstract: Test-time scaling has recently emerged as a promising approach for enhancing
the reasoning capabilities of large language models or vision-language models
during inference. Although a variety of test-time scaling strategies have been
proposed, and interest in their application to the medical domain is growing,
many critical aspects remain underexplored, including their effectiveness for
vision-language models and the identification of optimal strategies for
different settings. In this paper, we conduct a comprehensive investigation of
test-time scaling in the medical domain. We evaluate its impact on both large
language models and vision-language models, considering factors such as model
size, inherent model characteristics, and task complexity. Finally, we assess
the robustness of these strategies under user-driven factors, such as
misleading information embedded in prompts. Our findings offer practical
guidelines for the effective use of test-time scaling in medical applications
and provide insights into how these strategies can be further refined to meet
the reliability and interpretability demands of the medical domain.

</details>


### [488] [Leveraging In-Context Learning for Language Model Agents](https://arxiv.org/abs/2506.13109)
*Shivanshu Gupta,Sameer Singh,Ashish Sabharwal,Tushar Khot,Ben Bogin*

Main category: cs.CL

TL;DR: 本文研究上下文学习（ICL）用于代理任务，提出自动标注算法，表明选择相似任务轨迹、使用轨迹片段及大模型演示可提升性能，ICL 经合理使用对代理任务强大。


<details>
  <summary>Details</summary>
Motivation: 上下文学习在预测和生成任务成功，但用于需要序列决策的代理任务具有挑战性，需解决标注、选择演示等问题。

Method: 提出利用大语言模型和重试机制自动高效标注代理任务解决方案轨迹的算法；选择相似任务轨迹作为演示；每步使用小轨迹片段代替额外轨迹。

Result: 选择相似任务轨迹演示显著提升大语言模型代理性能、可靠性、鲁棒性和效率；使用小轨迹片段可减轻推理成本开销；大模型演示能提升小模型性能，ICL 代理可与昂贵的训练代理相媲美。

Conclusion: 上下文学习经谨慎使用对代理任务非常强大。

Abstract: In-context learning (ICL) with dynamically selected demonstrations combines
the flexibility of prompting large language models (LLMs) with the ability to
leverage training data to improve performance. While ICL has been highly
successful for prediction and generation tasks, leveraging it for agentic tasks
that require sequential decision making is challenging -- one must think not
only about how to annotate long trajectories at scale and how to select
demonstrations, but also what constitutes demonstrations, and when and where to
show them. To address this, we first propose an algorithm that leverages an LLM
with retries along with demonstrations to automatically and efficiently
annotate agentic tasks with solution trajectories. We then show that
set-selection of trajectories of similar tasks as demonstrations significantly
improves performance, reliability, robustness, and efficiency of LLM agents.
However, trajectory demonstrations have a large inference cost overhead. We
show that this can be mitigated by using small trajectory snippets at every
step instead of an additional trajectory. We find that demonstrations obtained
from larger models (in the annotation phase) also improve smaller models, and
that ICL agents can even rival costlier trained agents. Thus, our results
reveal that ICL, with careful use, can be very powerful for agentic tasks as
well.

</details>


### [489] [Adapting LLMs for Minimal-edit Grammatical Error Correction](https://arxiv.org/abs/2506.13148)
*Ryszard Staruch,Filip Graliński,Daniel Dzienisiewicz*

Main category: cs.CL

TL;DR: 本文探索错误率自适应主题并提出新训练调度方法，在BEA测试集上创单模型系统新纪录，还对常见数据集去标记化并分析相关影响，最后开源代码。


<details>
  <summary>Details</summary>
Motivation: 解码器大语言模型在最小编辑英语语法纠错中的应用研究不足，需提高其有效性。

Method: 探索错误率自适应主题，提出新训练调度方法；对常见英语GEC数据集去标记化；分析在去标记化数据集上训练的影响和使用修正错误示例数据集的影响。

Result: 在BEA测试集上为单模型系统创下新的最优结果；发现常见英语GEC数据集中存在错误。

Conclusion: 提出的方法有效，开源代码方便研究复现。

Abstract: Decoder-only large language models have shown superior performance in the
fluency-edit English Grammatical Error Correction, but their adaptation for
minimal-edit English GEC is still underexplored. To improve their effectiveness
in the minimal-edit approach, we explore the error rate adaptation topic and
propose a novel training schedule method. Our experiments set a new
state-of-the-art result for a single-model system on the BEA-test set. We also
detokenize the most common English GEC datasets to match the natural way of
writing text. During the process, we find that there are errors in them. Our
experiments analyze whether training on detokenized datasets impacts the
results and measure the impact of the usage of the datasets with corrected
erroneous examples. To facilitate reproducibility, we have released the source
code used to train our models.

</details>


### [490] [Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns](https://arxiv.org/abs/2506.13172)
*Evgeny Markhasin*

Main category: cs.CL

TL;DR: 本文提出并评估结构化工作流提示，用于引导大语言模型进行学术手稿分析，在两个模型上测试，结果显示提示性能受模型、任务类型和上下文影响。


<details>
  <summary>Details</summary>
Motivation: 引导大语言模型进行学术手稿的高级语义和语言分析，处理识别无根据声明和标记模糊代词指代两个非平凡分析任务。

Method: 设计结构化工作流提示，在Gemini Pro 2.5 Pro和ChatGPT Plus o3两个模型上进行多轮系统评估，设置不同上下文条件。

Result: 信息完整性任务中两模型识别名词短语无根据头部成功率95%，ChatGPT识别无根据形容词修饰语成功率0%，Gemini为95%；语言分析任务在全文语境下两模型表现良好，摘要语境下ChatGPT成功率100%，Gemini性能下降。

Conclusion: 结构化提示是复杂文本分析的可行方法，但提示性能高度依赖模型、任务类型和上下文，需进行严格的特定模型测试。

Abstract: We present and evaluate a suite of proof-of-concept (PoC), structured
workflow prompts designed to elicit human-like hierarchical reasoning while
guiding Large Language Models (LLMs) in high-level semantic and linguistic
analysis of scholarly manuscripts. The prompts target two non-trivial
analytical tasks: identifying unsubstantiated claims in summaries
(informational integrity) and flagging ambiguous pronoun references (linguistic
clarity). We conducted a systematic, multi-run evaluation on two frontier
models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context
conditions. Our results for the informational integrity task reveal a
significant divergence in model performance: while both models successfully
identified an unsubstantiated head of a noun phrase (95% success), ChatGPT
consistently failed (0% success) to identify an unsubstantiated adjectival
modifier that Gemini correctly flagged (95% success), raising a question
regarding potential influence of the target's syntactic role. For the
linguistic analysis task, both models performed well (80-90% success) with full
manuscript context. In a summary-only setting, however, ChatGPT achieved a
perfect (100%) success rate, while Gemini's performance was substantially
degraded. Our findings suggest that structured prompting is a viable
methodology for complex textual analysis but show that prompt performance may
be highly dependent on the interplay between the model, task type, and context,
highlighting the need for rigorous, model-specific testing.

</details>


### [491] [Multipole Attention for Efficient Long Context Reasoning](https://arxiv.org/abs/2506.13059)
*Coleman Hooper,Sebastian Zhao,Luca Manolache,Sehoon Kim,Michael W. Mahoney,Yakun Sophia Shao,Kurt Keutzer,Amir Gholami*

Main category: cs.CL

TL;DR: 提出Multipole Attention方法加速大推理模型自回归推理，在复杂推理任务上保持准确率，有速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型推理需生成大量token，稀疏注意力方法有误差且输入预处理难在线执行，需解决这些问题。

Method: 引入Multipole Attention，对重要token计算精确注意力，其余token保持近似表示；先聚类语义相似键向量，用聚类中心识别重要键向量和近似其余键向量；设计快速聚类更新过程。

Result: 在Qwen - 8B等模型上评估，在复杂推理任务中即使激进稀疏设置也能保持准确率，实现高达4.5倍速度提升。

Conclusion: Multipole Attention方法有效解决大推理模型推理难题，提高推理速度并保持准确率。

Abstract: Large Reasoning Models (LRMs) have shown promising accuracy improvements on
complex problem-solving tasks. While these models have attained high accuracy
by leveraging additional computation at test time, they need to generate long
chain-of-thought reasoning in order to think before answering, which requires
generating thousands of tokens. While sparse attention methods can help reduce
the KV cache pressure induced by this long autoregressive reasoning, these
methods can introduce errors which disrupt the reasoning process. Additionally,
prior methods often pre-process the input to make it easier to identify the
important prompt tokens when computing attention during generation, and this
pre-processing is challenging to perform online for newly generated reasoning
tokens. Our work addresses these challenges by introducing Multipole Attention,
which accelerates autoregressive reasoning by only computing exact attention
for the most important tokens, while maintaining approximate representations
for the remaining tokens. Our method first performs clustering to group
together semantically similar key vectors, and then uses the cluster centroids
both to identify important key vectors and to approximate the remaining key
vectors in order to retain high accuracy. We design a fast cluster update
process to quickly re-cluster the input and previously generated tokens,
thereby allowing for accelerating attention to the previous output tokens. We
evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our
approach can maintain accuracy on complex reasoning tasks even with aggressive
attention sparsity settings. We also provide kernel implementations to
demonstrate the practical efficiency gains from our method, achieving up to
4.5$\times$ speedup for attention in long-context reasoning applications. Our
code is available at https://github.com/SqueezeAILab/MultipoleAttention.

</details>


### [492] [Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for LLMs](https://arxiv.org/abs/2506.13192)
*Xintong Tang,Meiru Zhang,Shang Xiao,Junzhao Jin,Zihan Zhao,Liwei Li,Yang Zheng,Bangyi Wu*

Main category: cs.CL

TL;DR: 提出LADDER框架解决大语言模型推理局限，实验证明其提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型推理过程僵化，限制创造性和多样性输出，需解决此问题。

Method: 结合思维链推理、专家混合模型和多维上下采样策略。

Result: LADDER显著提升任务完成度、创造性和流畅性，表现优于传统模型；消融实验揭示关键组件作用。

Conclusion: 该工作有助于开发更灵活、有创造性的大语言模型以应对复杂新任务。

Abstract: Large language models (LLMs) are often constrained by rigid reasoning
processes, limiting their ability to generate creative and diverse responses.
To address this, a novel framework called LADDER is proposed, combining
Chain-of-Thought (CoT) reasoning, Mixture of Experts (MoE) models, and
multi-dimensional up/down-sampling strategies which breaks the limitations of
traditional LLMs. First, CoT reasoning guides the model through multi-step
logical reasoning, expanding the semantic space and breaking the rigidity of
thought. Next, MoE distributes the reasoning tasks across multiple expert
modules, each focusing on specific sub-tasks. Finally, dimensionality reduction
maps the reasoning outputs back to a lower-dimensional semantic space, yielding
more precise and creative responses. Extensive experiments across multiple
tasks demonstrate that LADDER significantly improves task completion,
creativity, and fluency, generating innovative and coherent responses that
outperform traditional models. Ablation studies reveal the critical roles of
CoT and MoE in enhancing reasoning abilities and creative output. This work
contributes to the development of more flexible and creative LLMs, capable of
addressing complex and novel tasks.

</details>


### [493] [Align-then-Unlearn: Embedding Alignment for LLM Unlearning](https://arxiv.org/abs/2506.13181)
*Philipp Spohn,Leander Girrbach,Jessica Bader,Zeynep Akata*

Main category: cs.CL

TL;DR: 针对大语言模型隐私伦理问题，提出Align - then - Unlearn框架，实验表明能有效移除目标知识，代码开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练时可能保留敏感信息，现有token层面的unlearning方法有缺陷。

Method: 提出Align - then - Unlearn框架，在语义嵌入空间进行unlearning，先添加嵌入预测模块，再微调模型减少预测嵌入与目标嵌入的相似度。

Result: Align - then - Unlearn能有效移除目标知识，对整体模型效用的降低最小。

Conclusion: 基于嵌入的unlearning是一种有前景且稳健的移除概念知识的方法。

Abstract: As large language models (LLMs) are trained on massive datasets, they have
raised significant privacy and ethical concerns due to their potential to
inadvertently retain sensitive information. Unlearning seeks to selectively
remove specific data from trained models, such as personal information or
copyrighted content. Current approaches targeting specific output sequences at
the token level often fail to achieve complete forgetting and remain
susceptible to prompt rephrasing. We propose Align-then-Unlearn, a novel
framework that performs unlearning in the semantic embedding space rather than
directly on output tokens. Align-then-Unlearn first augments the LLM with an
embedding prediction module trained to anticipate future context
representations. Unlearning is then achieved by fine-tuning the model to
minimize the similarity between these predicted embeddings and a target
embedding that represents the concept to be removed. Initial results show that
Align-then-Unlearn effectively removes targeted knowledge with minimal
degradation in overall model utility. These findings suggest that
embedding-based unlearning offers a promising and robust approach to removing
conceptual knowledge. Our code is available at
https://github.com/ExplainableML/align-then-unlearn.

</details>


### [494] [AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy](https://arxiv.org/abs/2506.13284)
*Zihan Liu,Zhuolin Yang,Yang Chen,Chankyu Lee,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.CL

TL;DR: 研究监督微调（SFT）与强化学习（RL）在开发强推理模型中的协同作用，通过实验得出相关结论并使模型取得新的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 探索SFT和RL在开发强推理模型中的协同效果，解决SFT与RL结合过程中的问题。

Method: 通过两种缩放策略策划SFT训练数据，探讨SFT和RL协同的两个问题，控制采样温度进行RL训练。

Result: 两种缩放策略均提升推理性能，扩大提示数量收益更大；有效RL训练下更强SFT模型最终性能更好；采样温度设为使熵约0.3可平衡探索与利用；RL过程中初始SFT模型性能差距缩小；AceReason - Nemotron - 1.1 7B模型表现优异。

Conclusion: 利用强SFT基础和SFT与RL协同作用的见解，提出的后训练方法有效，模型达到新的SOTA，代码和数据已公开。

Abstract: In this work, we investigate the synergy between supervised fine-tuning (SFT)
and reinforcement learning (RL) in developing strong reasoning models. We begin
by curating the SFT training data through two scaling strategies: increasing
the number of collected prompts and the number of generated responses per
prompt. Both approaches yield notable improvements in reasoning performance,
with scaling the number of prompts resulting in more substantial gains. We then
explore the following questions regarding the synergy between SFT and RL: (i)
Does a stronger SFT model consistently lead to better final performance after
large-scale RL training? (ii) How can we determine an appropriate sampling
temperature during RL training to effectively balance exploration and
exploitation for a given SFT initialization? Our findings suggest that (i)
holds true, provided effective RL training is conducted, particularly when the
sampling temperature is carefully chosen to maintain the temperature-adjusted
entropy around 0.3, a setting that strikes a good balance between exploration
and exploitation. Notably, the performance gap between initial SFT models
narrows significantly throughout the RL process. Leveraging a strong SFT
foundation and insights into the synergistic interplay between SFT and RL, our
AceReason-Nemotron-1.1 7B model significantly outperforms
AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among
Qwen2.5-7B-based reasoning models on challenging math and code benchmarks,
thereby demonstrating the effectiveness of our post-training recipe. We release
the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B

</details>


### [495] [Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models](https://arxiv.org/abs/2506.13300)
*Bo Li,Chengben Xu,Wufeng Zhang*

Main category: cs.CL

TL;DR: 本文介绍视睿科技在多语言对话语音语言模型挑战赛两个赛道的系统，采用多阶段训练管道提升模型性能，效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 应对多语言对话语音语言模型挑战赛中自动语音识别和说话人分割与自动语音识别任务。

Method: 引入多阶段训练管道，结合课程学习、思维链数据增强和带可验证奖励的强化学习。

Result: 在评估集上，赛道1的WER/CER达11.57%，赛道2的tcpWER/tcpCER达17.67%。

Conclusion: 综合消融研究证明各组件在挑战约束下有效。

Abstract: This paper presents Seewo's systems for both tracks of the Multilingual
Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic
speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We
introduce a multi-stage training pipeline that explicitly enhances reasoning
and self-correction in speech language models for ASR. Our approach combines
curriculum learning for progressive capability acquisition, Chain-of-Thought
data augmentation to foster intermediate reflection, and Reinforcement Learning
with Verifiable Rewards (RLVR) to further refine self-correction through
reward-driven optimization. This approach achieves substantial improvements
over the official challenge baselines. On the evaluation set, our best system
attains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track
2. Comprehensive ablation studies demonstrate the effectiveness of each
component under challenge constraints.

</details>


### [496] [Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks](https://arxiv.org/abs/2506.13351)
*Yifei Xu,Tusher Chakraborty,Srinagesh Sharma,Leonardo Nunes,Emre Kıcıman,Songwu Lu,Ranveer Chandra*

Main category: cs.CL

TL;DR: 提出Direct Reasoning Optimization (DRO)框架微调大语言模型处理开放式推理任务，引入R3奖励信号和动态数据过滤策略，在两个数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有技术在开放式长文本推理任务应用因缺乏通用可验证奖励信号而面临挑战。

Method: 提出DRO框架，使用R3奖励信号，引入基于R3的动态数据过滤策略。

Result: 在ParaRev和FinQA数据集上，DRO始终优于强基线，且广泛适用于开放式和结构化领域。

Conclusion: DRO框架有效且可广泛应用于不同推理任务。

Abstract: Recent advances in Large Language Models (LLMs) have showcased impressive
reasoning abilities in structured tasks like mathematics and programming,
largely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which
uses outcome-based signals that are scalable, effective, and robust against
reward hacking. However, applying similar techniques to open-ended long-form
reasoning tasks remains challenging due to the absence of generic, verifiable
reward signals. To address this, we propose Direct Reasoning Optimization
(DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended,
particularly long-form, reasoning tasks, guided by a new reward signal: the
Reasoning Reflection Reward (R3). At its core, R3 selectively identifies and
emphasizes key tokens in the reference outcome that reflect the influence of
the model's preceding chain-of-thought reasoning, thereby capturing the
consistency between reasoning and reference outcome at a fine-grained level.
Crucially, R3 is computed internally using the same model being optimized,
enabling a fully self-contained training setup. Additionally, we introduce a
dynamic data filtering strategy based on R3 for open-ended reasoning tasks,
reducing cost while improving downstream performance. We evaluate DRO on two
diverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a
math-oriented QA benchmark -- and show that it consistently outperforms strong
baselines while remaining broadly applicable across both open-ended and
structured domains.

</details>


### [497] [StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns](https://arxiv.org/abs/2506.13356)
*Luanbo Wan,Weizhi Ma*

Main category: cs.CL

TL;DR: 提出基于互动小说游戏的基准框架评估大语言模型长期记忆能力，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 缺乏系统评估大语言模型长期记忆能力的标准化基准，现有基准存在不足。

Method: 提出基于互动小说游戏的基准框架，设置两种测试推理复杂度的场景，构建新数据集。

Result: 实验表明该基准能稳健、可靠地评估大语言模型的长期记忆能力。

Conclusion: 所提出的基准框架可有效评估大语言模型的长期记忆能力。

Abstract: Long-term memory (LTM) is essential for large language models (LLMs) to
achieve autonomous intelligence in complex, evolving environments. Despite
increasing efforts in memory-augmented and retrieval-based architectures, there
remains a lack of standardized benchmarks to systematically evaluate LLMs'
long-term memory abilities. Existing benchmarks still face challenges in
evaluating knowledge retention and dynamic sequential reasoning, and in their
own flexibility, all of which limit their effectiveness in assessing models'
LTM capabilities. To address these gaps, we propose a novel benchmark framework
based on interactive fiction games, featuring dynamically branching storylines
with complex reasoning structures. These structures simulate real-world
scenarios by requiring LLMs to navigate hierarchical decision trees, where each
choice triggers cascading dependencies across multi-turn interactions. Our
benchmark emphasizes two distinct settings to test reasoning complexity: one
with immediate feedback upon incorrect decisions, and the other requiring
models to independently trace back and revise earlier choices after failure. As
part of this benchmark, we also construct a new dataset designed to test LLMs'
LTM within narrative-driven environments. We further validate the effectiveness
of our approach through detailed experiments. Experimental results demonstrate
the benchmark's ability to robustly and reliably assess LTM in LLMs.

</details>


### [498] [Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2506.13474)
*David Bani-Harouni,Chantal Pellegrini,Ege Özsoy,Matthias Keicher,Nassir Navab*

Main category: cs.CL

TL;DR: 提出假设驱动的不确定性感知语言代理LA - CDM用于临床诊断决策，采用混合训练范式，在真实数据集上验证可提升诊断性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在临床决策支持应用中存在局限，如假设患者信息即时可得、未进行特定任务训练等，需要更好的模型支持临床决策。

Method: 提出假设驱动的不确定性感知语言代理LA - CDM，通过反复请求和解释相关测试来得出诊断；采用结合监督学习和强化学习的混合训练范式，设置三个训练目标。

Result: 在覆盖四种腹部疾病的真实数据集MIMIC - CDM上进行评估。

Conclusion: 明确训练临床决策过程能提高诊断性能和效率。

Abstract: Clinical decision-making is a dynamic, interactive, and cyclic process where
doctors have to repeatedly decide on which clinical action to perform and
consider newly uncovered information for diagnosis and treatment. Large
Language Models (LLMs) have the potential to support clinicians in this
process, however, most applications of LLMs in clinical decision support suffer
from one of two limitations: Either they assume the unrealistic scenario of
immediate availability of all patient information and do not model the
interactive and iterative investigation process, or they restrict themselves to
the limited "out-of-the-box" capabilities of large pre-trained models without
performing task-specific training. In contrast to this, we propose to model
clinical decision-making for diagnosis with a hypothesis-driven
uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis
via repeatedly requesting and interpreting relevant tests. Using a hybrid
training paradigm combining supervised and reinforcement learning, we train
LA-CDM with three objectives targeting critical aspects of clinical
decision-making: accurate hypothesis generation, hypothesis uncertainty
estimation, and efficient decision-making. We evaluate our methodology on
MIMIC-CDM, a real-world dataset covering four abdominal diseases containing
various clinical tests and show the benefit of explicitly training clinical
decision-making for increasing diagnostic performance and efficiency.

</details>


### [499] [A Neural Model for Word Repetition](https://arxiv.org/abs/2506.13450)
*Daniel Dager,Robin Sobczyk,Emmanuel Chemla,Yair Lakretz*

Main category: cs.CL

TL;DR: 本文提出用深度神经网络建模单词重复任务，弥合认知模型与神经机制的差距，研究表明神经模型能模拟人类部分效应，但也存在差异。


<details>
  <summary>Details</summary>
Motivation: 当前单词重复的神经机制和大脑具体如何执行该任务尚不明确，需弥合单词重复认知模型与神经机制的差距。

Method: 训练大量模型模拟单词重复任务；创建系列测试探查模型的人类行为研究已知效应；通过消融研究模拟脑损伤，移除模型神经元并重复行为研究。

Result: 神经模型能模仿人类研究的一些效应，但在其他方面可能存在差异。

Conclusion: 指出开发类人神经模型的潜力和挑战，为未来研究提供方向。

Abstract: It takes several years for the developing brain of a baby to fully master
word repetition-the task of hearing a word and repeating it aloud. Repeating a
new word, such as from a new language, can be a challenging task also for
adults. Additionally, brain damage, such as from a stroke, may lead to
systematic speech errors with specific characteristics dependent on the
location of the brain damage. Cognitive sciences suggest a model with various
components for the different processing stages involved in word repetition.
While some studies have begun to localize the corresponding regions in the
brain, the neural mechanisms and how exactly the brain performs word repetition
remain largely unknown. We propose to bridge the gap between the cognitive
model of word repetition and neural mechanisms in the human brain by modeling
the task using deep neural networks. Neural models are fully observable,
allowing us to study the detailed mechanisms in their various substructures and
make comparisons with human behavior and, ultimately, the brain. Here, we make
first steps in this direction by: (1) training a large set of models to
simulate the word repetition task; (2) creating a battery of tests to probe the
models for known effects from behavioral studies in humans, and (3) simulating
brain damage through ablation studies, where we systematically remove neurons
from the model, and repeat the behavioral study to examine the resulting speech
errors in the "patient" model. Our results show that neural models can mimic
several effects known from human research, but might diverge in other aspects,
highlighting both the potential and the challenges for future research aimed at
developing human-like neural models.

</details>


### [500] [TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter Language Models on Low-end Devices](https://arxiv.org/abs/2506.13514)
*Mingxue Xu,Yao Lei Xu,Danilo P. Mandic*

Main category: cs.CL

TL;DR: 本文提出无训练的令牌嵌入压缩方法处理小型语言模型适应性和能效问题，在树莓派上评估有良好效果。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型需适应部署环境且具备能效，而数据中心部署的大语言模型未解决这些问题。

Method: 提出使用张量列车分解（TTD）的无训练令牌嵌入压缩方法，将预训练令牌嵌入向量转换为低维矩阵积态（MPS）。

Result: 以GPT - 2/Cerebres - GPT和OPT模型为例，实现约2.0倍嵌入层压缩，语言任务性能与原模型相当，单次查询能耗减半。

Conclusion: 该方法能满足小型语言模型对部署环境的适应性和能效要求。

Abstract: Small Language Models (SLMs, or on-device LMs) have significantly fewer
parameters than Large Language Models (LLMs). They are typically deployed on
low-end devices, like mobile phones and single-board computers. Unlike LLMs,
which rely on increasing model size for better generalisation, SLMs designed
for edge applications are expected to have adaptivity to the deployment
environments and energy efficiency given the device battery life constraints,
which are not addressed in datacenter-deployed LLMs. This paper addresses these
two requirements by proposing a training-free token embedding compression
approach using Tensor-Train Decomposition (TTD). Each pre-trained token
embedding vector is converted into a lower-dimensional Matrix Product State
(MPS). We comprehensively evaluate the extracted low-rank structures across
compression ratio, language task performance, latency, and energy consumption
on a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion
parameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our
approach achieves a comparable language task performance to the original model
with around $2.0\times$ embedding layer compression, while the energy
consumption of a single query drops by half.

</details>


### [501] [Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study](https://arxiv.org/abs/2506.13464)
*Zhengyu Hu,Jianxun Lian,Zheyuan Xiao,Seraphina Zhang,Tianfu Wang,Nicholas Jing Yuan,Xing Xie,Hui Xiong*

Main category: cs.CL

TL;DR: 本文提出框架评估大语言模型学习能力，开展实证研究并得出结论，还引入新基准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型学习能力研究不足，需探索其在动态环境中适应和获取新知识的能力。

Method: 受认知心理学和教育启发，将学习能力分解为三方面进行综合实证研究。

Result: 发现交互促进学习、概念理解有规模涌现性、大语言模型是有效少样本学习者但非多样本学习者。

Conclusion: 引入的基准可统一现实地评估大语言模型学习能力，支持更具适应性和类人模型的评估与开发。

Abstract: Large language models (LLMs) have shown impressive capabilities across tasks
such as mathematics, coding, and reasoning, yet their learning ability, which
is crucial for adapting to dynamic environments and acquiring new knowledge,
remains underexplored. In this work, we address this gap by introducing a
framework inspired by cognitive psychology and education. Specifically, we
decompose general learning ability into three distinct, complementary
dimensions: Learning from Instructor (acquiring knowledge via explicit
guidance), Learning from Concept (internalizing abstract structures and
generalizing to new contexts), and Learning from Experience (adapting through
accumulated exploration and feedback). We conduct a comprehensive empirical
study across the three learning dimensions and identify several insightful
findings, such as (i) interaction improves learning; (ii) conceptual
understanding is scale-emergent and benefits larger models; and (iii) LLMs are
effective few-shot learners but not many-shot learners. Based on our framework
and empirical findings, we introduce a benchmark that provides a unified and
realistic evaluation of LLMs' general learning abilities across three learning
cognition dimensions. It enables diagnostic insights and supports evaluation
and development of more adaptive and human-like models.

</details>


### [502] [Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization](https://arxiv.org/abs/2506.13541)
*Guanghui Song,Dongping Liao,Yiren Zhao,Kejiang Ye,Cheng-zhong Xu,Xitong Gao*

Main category: cs.CL

TL;DR: 提出mixSGA方法动态优化Transformer在CLM中的令牌计算和内存分配，评估显示优于静态基线。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在CLM中因KV缓存内存分配低效面临扩展性挑战，现有方法依赖刚性资源分配，未解决令牌重要性动态变化问题。

Method: 提出mixSGA，一种MoE方法，通过令牌专家选择路由机制、分组注意力投影权重共享和辅助损失来动态优化。

Result: 在Llama3、TinyLlama、OPT和Gemma2模型家族上评估，在指令跟随和继续预训练任务中，相同KV预算下mixSGA实现更高ROUGE - L和更低困惑度。

Conclusion: mixSGA在动态优化计算和内存分配方面优于静态基线方法。

Abstract: Transformer models face scalability challenges in causal language modeling
(CLM) due to inefficient memory allocation for growing key-value (KV) caches,
which strains compute and storage resources. Existing methods like Grouped
Query Attention (GQA) and token-level KV optimization improve efficiency but
rely on rigid resource allocation, often discarding "low-priority" tokens or
statically grouping them, failing to address the dynamic spectrum of token
importance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that
dynamically optimizes token-wise computation and memory allocation. Unlike
prior approaches, mixSGA retains all tokens while adaptively routing them to
specialized experts with varying KV group sizes, balancing granularity and
efficiency. Our key novelties include: (1) a token-wise expert-choice routing
mechanism guided by learned importance scores, enabling proportional resource
allocation without token discard; (2) weight-sharing across grouped attention
projections to minimize parameter overhead; and (3) an auxiliary loss to ensure
one-hot routing decisions for training-inference consistency in CLMs. Extensive
evaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show
mixSGA's superiority over static baselines. On instruction-following and
continued pretraining tasks, mixSGA achieves higher ROUGE-L and lower
perplexity under the same KV budgets.

</details>


### [503] [An Interdisciplinary Approach to Human-Centered Machine Translation](https://arxiv.org/abs/2506.13468)
*Marine Carpuat,Omri Asscher,Kalika Bali,Luisa Bentivogli,Frédéric Blain,Lynne Bowker,Monojit Choudhury,Hal Daumé III,Kevin Duh,Ge Gao,Alvin Grissom II,Marzena Karpinska,Elaine C. Khoong,William D. Lewis,André F. T. Martins,Mary Nurminen,Douglas W. Oard,Maja Popovic,Michel Simard,François Yvon*

Main category: cs.CL

TL;DR: 本文倡导以人类为中心的机器翻译方法，通过调研文献重新定位机器翻译评估和设计。


<details>
  <summary>Details</summary>
Motivation: 尽管机器翻译技术进步，但系统开发与实际应用存在差距，非专业用户难以评估翻译可靠性。

Method: 调研翻译研究和人机交互领域的文献。

Result: 未提及具体结果。

Conclusion: 倡导以人类为中心的方法，使系统设计与多样的交流目标和使用场景相契合。

Abstract: Machine Translation (MT) tools are widely used today, often in contexts where
professional translators are not present. Despite progress in MT technology, a
gap persists between system development and real-world usage, particularly for
non-expert users who may struggle to assess translation reliability. This paper
advocates for a human-centered approach to MT, emphasizing the alignment of
system design with diverse communicative goals and contexts of use. We survey
the literature in Translation Studies and Human-Computer Interaction to
recontextualize MT evaluation and design to address the diverse real-world
scenarios in which MT is used today.

</details>


### [504] [ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models](https://arxiv.org/abs/2506.13472)
*Junho Yoon,Geom Lee,Donghyeon Jeon,Inho Kang,Seung-Hoon Na*

Main category: cs.CL

TL;DR: 提出基于旋转的显著感知权重量化方法ROSAQ，实验显示其优于基线方法和其他量化方法，有速度提升。


<details>
  <summary>Details</summary>
Motivation: 量化可减少大语言模型内存需求和潜在改善延迟时间，当前方法存在不足，需新量化方法。

Method: 利用变压器旋转不变性，提出ROSAQ，含PCA投影、显著通道识别、混合精度显著感知量化。

Result: ROSAQ优于原特征空间的基线显著感知量化和其他现有量化方法，有2.3倍速度提升。

Conclusion: ROSAQ是一种有效的大语言模型量化方法，有较好性能和速度提升。

Abstract: Quantization has been widely studied as an effective technique for reducing
the memory requirement of large language models (LLMs), potentially improving
the latency time as well. Utilizing the characteristic of rotational invariance
of transformer, we propose the rotation-based saliency-aware weight
quantization (ROSAQ), which identifies salient channels in the projection
feature space, not in the original feature space, where the projected
"principal" dimensions are naturally considered as "salient" features. The
proposed ROSAQ consists of 1) PCA-based projection, which first performs
principal component analysis (PCA) on a calibration set and transforms via the
PCA projection, 2) Salient channel dentification, which selects dimensions
corresponding to the K-largest eigenvalues as salient channels, and 3)
Saliency-aware quantization with mixed-precision, which uses FP16 for salient
dimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ
shows improvements over the baseline saliency-aware quantization on the
original feature space and other existing quantization methods. With kernel
fusion, ROSAQ presents about 2.3x speed up over FP16 implementation in
generating 256 tokens with a batch size of 64.

</details>


### [505] [MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention](https://arxiv.org/abs/2506.13585)
*MiniMax,:,Aili Chen,Aonian Li,Bangwei Gong,Binyang Jiang,Bo Fei,Bo Yang,Boji Shan,Changqing Yu,Chao Wang,Cheng Zhu,Chengjun Xiao,Chengyu Du,Chi Zhang,Chu Qiao,Chunhao Zhang,Chunhui Du,Congchao Guo,Da Chen,Deming Ding,Dianjun Sun,Dong Li,Enwei Jiao,Haigang Zhou,Haimo Zhang,Han Ding,Haohai Sun,Haoyu Feng,Huaiguang Cai,Haichao Zhu,Jian Sun,Jiaqi Zhuang,Jiaren Cai,Jiayuan Song,Jin Zhu,Jingyang Li,Jinhao Tian,Jinli Liu,Junhao Xu,Junjie Yan,Junteng Liu,Junxian He,Kaiyi Feng,Ke Yang,Kecheng Xiao,Le Han,Leyang Wang,Lianfei Yu,Liheng Feng,Lin Li,Lin Zheng,Linge Du,Lingyu Yang,Lunbin Zeng,Minghui Yu,Mingliang Tao,Mingyuan Chi,Mozhi Zhang,Mujie Lin,Nan Hu,Nongyu Di,Peng Gao,Pengfei Li,Pengyu Zhao,Qibing Ren,Qidi Xu,Qile Li,Qin Wang,Rong Tian,Ruitao Leng,Shaoxiang Chen,Shaoyu Chen,Shengmin Shi,Shitong Weng,Shuchang Guan,Shuqi Yu,Sichen Li,Songquan Zhu,Tengfei Li,Tianchi Cai,Tianrun Liang,Weiyu Cheng,Weize Kong,Wenkai Li,Xiancai Chen,Xiangjun Song,Xiao Luo,Xiao Su,Xiaobo Li,Xiaodong Han,Xinzhu Hou,Xuan Lu,Xun Zou,Xuyang Shen,Yan Gong,Yan Ma,Yang Wang,Yiqi Shi,Yiran Zhong,Yonghong Duan,Yongxiang Fu,Yongyi Hu,Yu Gao,Yuanxiang Fan,Yufeng Yang,Yuhao Li,Yulin Hu,Yunan Huang,Yunji Li,Yunzhi Xu,Yuxin Mao,Yuxuan Shi,Yuze Wenren,Zehan Li,Zelin Li,Zhanxu Tian,Zhengmao Zhu,Zhenhua Fan,Zhenzhen Wu,Zhichao Xu,Zhihang Yu,Zhiheng Lyu,Zhuo Jiang,Zibo Gao,Zijia Wu,Zijian Song,Zijun Sun*

Main category: cs.CL

TL;DR: 介绍世界首个开放权重的大规模混合注意力推理模型MiniMax - M1，其有高效架构和训练算法，性能优越且公开。


<details>
  <summary>Details</summary>
Motivation: 开发适用于处理长输入和复杂思考任务的高效模型。

Method: 采用混合MoE架构与闪电注意力机制，基于MiniMax - Text - 01开发；用大规模强化学习训练，提出CISPO算法提升效率。

Result: 512个H800 GPU仅三周完成全强化学习训练，成本534700美元；实验显示模型性能可比或优于其他强开放权重模型。

Conclusion: MiniMax - M1在复杂软件工程、工具利用和长上下文任务表现出色，公开模型促进发展。

Abstract: We introduce MiniMax-M1, the world's first open-weight, large-scale
hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid
Mixture-of-Experts (MoE) architecture combined with a lightning attention
mechanism. The model is developed based on our previous MiniMax-Text-01 model,
which contains a total of 456 billion parameters with 45.9 billion parameters
activated per token. The M1 model natively supports a context length of 1
million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning
attention mechanism in MiniMax-M1 enables efficient scaling of test-time
compute. These properties make M1 particularly suitable for complex tasks that
require processing long inputs and thinking extensively. MiniMax-M1 is trained
using large-scale reinforcement learning (RL) on diverse problems including
sandbox-based, real-world software engineering environments. In addition to
M1's inherent efficiency advantage for RL training, we propose CISPO, a novel
RL algorithm to further enhance RL efficiency. CISPO clips importance sampling
weights rather than token updates, outperforming other competitive RL variants.
Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on
512 H800 GPUs to complete in only three weeks, with a rental cost of just
$534,700. We release two versions of MiniMax-M1 models with 40K and 80K
thinking budgets respectively, where the 40K model represents an intermediate
phase of the 80K training. Experiments on standard benchmarks show that our
models are comparable or superior to strong open-weight models such as the
original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex
software engineering, tool utilization, and long-context tasks. We publicly
release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.

</details>


### [506] [Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness](https://arxiv.org/abs/2506.13479)
*Mei-Yen Chen,Thi Thu Uyen Hoang,Michael Hahn,M. Saquib Sarfraz*

Main category: cs.CL

TL;DR: 本文指出研究应从开发新的LoRA合并或路由算法转向理解复用LoRA有效的条件，通过实验发现复用LoRA常难整合知识，质疑其作为无数据方法的可行性，建议暂停追求新方法。


<details>
  <summary>Details</summary>
Motivation: 在数据访问受限情况下，合并或路由LoRA成为提升大语言模型的流行方案，但目前研究多集中于新算法，本文旨在探究复用LoRA真正有效的条件。

Method: 通过理论分析和合成的两跳推理及数学应用题任务，评估参数平均和动态适配器选择两种数据无关方法。

Result: 复用LoRA常无法在不相交的微调数据集间逻辑整合知识，尤其在预训练中知识未充分表示时，LoRA表达能力有限。

Conclusion: 暂停追求复用LoRA的新方法，强调需要严格机制来指导基于适配器的模型合并的学术研究和实际系统设计。

Abstract: Merging or routing low-rank adapters (LoRAs) has emerged as a popular
solution for enhancing large language models, particularly when data access is
restricted by regulatory or domain-specific constraints. This position paper
argues that the research community should shift its focus from developing new
merging or routing algorithms to understanding the conditions under which
reusing LoRAs is truly effective. Through theoretical analysis and synthetic
two-hop reasoning and math word-problem tasks, we examine whether reusing LoRAs
enables genuine compositional generalization or merely reflects shallow pattern
matching. Evaluating two data-agnostic methods--parameter averaging and dynamic
adapter selection--we found that reusing LoRAs often fails to logically
integrate knowledge across disjoint fine-tuning datasets, especially when such
knowledge is underrepresented during pretraining. Our empirical results,
supported by theoretical insights into LoRA's limited expressiveness, highlight
the preconditions and constraints of reusing them for unseen tasks and cast
doubt on its feasibility as a truly data-free approach. We advocate for pausing
the pursuit of novel methods for recycling LoRAs and emphasize the need for
rigorous mechanisms to guide future academic research in adapter-based model
merging and practical system designs for practitioners.

</details>


### [507] [Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language Models](https://arxiv.org/abs/2506.13681)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch*

Main category: cs.CL

TL;DR: 本文重新审视min - p采样器优于现有采样器的证据，发现原论文证据不足，min - p未显示出优势。


<details>
  <summary>Details</summary>
Motivation: 原论文称min - p采样器在质量和多样性上优于现有采样器，本文对此进行全面重新审查。

Method: 重新分析原论文的人类评估数据、全面扫描NLP基准测试、审查LLM - as - a - Judge评估方法、核实社区采用情况。

Result: 原论文人类评估存在数据遗漏等问题，min - p未超基线；控制超参数时min - p在NLP基准测试未超基线；LLM - as - a - Judge评估方法不清晰；社区采用说法无依据。

Conclusion: 原论文证据不足以支持min - p在质量、多样性或二者权衡上有改进的说法。

Abstract: Sampling from language models impacts the quality and diversity of outputs,
affecting both research and real-world applications. Recently, Nguyen et al.
2024's "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM
Outputs" introduced a new sampler called min-p, claiming it achieves superior
quality and diversity over established samplers such as basic, top-k, and top-p
sampling. The significance of these claims was underscored by the paper's
recognition as the 18th highest-scoring submission to ICLR 2025 and selection
for an Oral presentation. This paper conducts a comprehensive re-examination of
the evidence supporting min-p and reaches different conclusions from the
original paper's four lines of evidence. First, the original paper's human
evaluations omitted data, conducted statistical tests incorrectly, and
described qualitative feedback inaccurately; our reanalysis demonstrates min-p
did not outperform baselines in quality, diversity, or a trade-off between
quality and diversity; in response to our findings, the authors of the original
paper conducted a new human evaluation using a different implementation, task,
and rubric that nevertheless provides further evidence min-p does not improve
over baselines. Second, comprehensively sweeping the original paper's NLP
benchmarks reveals min-p does not surpass baselines when controlling for the
number of hyperparameters. Third, the original paper's LLM-as-a-Judge
evaluations lack methodological clarity and appear inconsistently reported.
Fourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars)
were found to be unsubstantiated, leading to their removal; the revised
adoption claim remains misleading. We conclude that evidence presented in the
original paper fails to support claims that min-p improves quality, diversity,
or a trade-off between quality and diversity.

</details>


### [508] [Understand the Implication: Learning to Think for Pragmatic Understanding](https://arxiv.org/abs/2506.13559)
*Settaluri Lakshmi Sravanthi,Kishan Maharaj,Sravani Gunnu,Abhijit Mishra,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 提出新的语用数据集，用基于思维的学习提升大语言模型语用理解能力，在多项任务有准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在语用理解能力提升方面探索不足，且现有方法忽略人类理解隐含意义的推理过程。

Method: 引入包含正确和错误解释明确推理的语用数据集ImpliedMeaningPreference，进行偏好调整和监督微调。

Result: 基于思维的学习显著提升大语言模型语用理解能力，跨模型家族准确率提高11.12%；在迁移学习研究中，相比标签训练模型在未训练的语用任务上准确率提高16.10%。

Conclusion: 基于思维的学习能有效提升大语言模型语用理解能力。

Abstract: Pragmatics, the ability to infer meaning beyond literal interpretation, is
crucial for social cognition and communication. While LLMs have been
benchmarked for their pragmatic understanding, improving their performance
remains underexplored. Existing methods rely on annotated labels but overlook
the reasoning process humans naturally use to interpret implicit meaning. To
bridge this gap, we introduce a novel pragmatic dataset,
ImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both
correct and incorrect interpretations. Through preference-tuning and supervised
fine-tuning, we demonstrate that thought-based learning significantly enhances
LLMs' pragmatic understanding, improving accuracy by 11.12% across model
families. We further discuss a transfer-learning study where we evaluate the
performance of thought-based training for the other tasks of pragmatics
(presupposition, deixis) that are not seen during the training time and observe
an improvement of 16.10% compared to label-trained models.

</details>


### [509] [CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation](https://arxiv.org/abs/2506.13599)
*Yuwei Du,Jie Feng,Jian Yuan,Yong Li*

Main category: cs.CL

TL;DR: 提出CAMS框架用于人类移动性模拟，在真实数据集上表现优异，建立了新范式。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法和现有利用大语言模型的方法存在不足，如对城市空间建模不足、与个体和集体移动性分布整合不佳。

Method: 提出CAMS框架，包含MobExtractor、GeoGenerator、TrajEnhancer三个核心模块。

Result: 在真实数据集实验中，CAMS无需外部地理空间信息就有优越表现，能生成更真实合理的轨迹。

Conclusion: CAMS建立了将代理框架与具备城市知识的大语言模型相结合的人类移动性模拟新范式。

Abstract: Human mobility simulation plays a crucial role in various real-world
applications. Recently, to address the limitations of traditional data-driven
approaches, researchers have explored leveraging the commonsense knowledge and
reasoning capabilities of large language models (LLMs) to accelerate human
mobility simulation. However, these methods suffer from several critical
shortcomings, including inadequate modeling of urban spaces and poor
integration with both individual mobility patterns and collective mobility
distributions. To address these challenges, we propose \textbf{C}ityGPT-Powered
\textbf{A}gentic framework for \textbf{M}obility \textbf{S}imulation
(\textbf{CAMS}), an agentic framework that leverages the language based urban
foundation model to simulate human mobility in urban space. \textbf{CAMS}
comprises three core modules, including MobExtractor to extract template
mobility patterns and synthesize new ones based on user profiles, GeoGenerator
to generate anchor points considering collective knowledge and generate
candidate urban geospatial knowledge using an enhanced version of CityGPT,
TrajEnhancer to retrieve spatial knowledge based on mobility patterns and
generate trajectories with real trajectory preference alignment via DPO.
Experiments on real-world datasets show that \textbf{CAMS} achieves superior
performance without relying on externally provided geospatial information.
Moreover, by holistically modeling both individual mobility patterns and
collective mobility constraints, \textbf{CAMS} generates more realistic and
plausible trajectories. In general, \textbf{CAMS} establishes a new paradigm
that integrates the agentic framework with urban-knowledgeable LLMs for human
mobility simulation.

</details>


### [510] [Instruction Following by Boosting Attention of Large Language Models](https://arxiv.org/abs/2506.13734)
*Vitoria Guardieiro,Adam Stein,Avishree Khare,Eric Wong*

Main category: cs.CL

TL;DR: 文章指出大语言模型生成控制有挑战，现有潜在转向技术有局限，提出InstABoost方法，经实验证明效果优于传统提示和潜在转向。


<details>
  <summary>Details</summary>
Motivation: 现有潜在转向技术在控制大语言模型生成上效果有限，常不如简单指令提示，需改进。

Method: 先建立多行为基准对转向技术进行标准化评估，基于此提出InstABoost方法，通过改变模型生成时的注意力来增强指令提示强度。

Result: InstABoost在控制成功率上优于传统提示和潜在转向。

Conclusion: InstABoost结合了现有方法优点，在控制大语言模型生成方面更有效。

Abstract: Controlling the generation of large language models (LLMs) remains a central
challenge to ensure their safe and reliable deployment. While prompt
engineering and finetuning are common approaches, recent work has explored
latent steering, a lightweight technique that alters LLM internal activations
to guide generation. However, subsequent studies revealed latent steering's
effectiveness to be limited, often underperforming simple instruction
prompting. To address this limitation, we first establish a benchmark across
diverse behaviors for standardized evaluation of steering techniques. Building
on insights from this benchmark, we introduce Instruction Attention Boosting
(InstABoost), a latent steering method that boosts the strength of instruction
prompting by altering the model's attention during generation. InstABoost
combines the strengths of existing approaches and is theoretically supported by
prior work that suggests that in-context rule following in transformer-based
models can be controlled by manipulating attention on instructions.
Empirically, InstABoost demonstrates superior control success compared to both
traditional prompting and latent steering.

</details>


### [511] [Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent Prefix Data](https://arxiv.org/abs/2506.13674)
*Haonan Wang,Brian Chen,Li Siquan,Liang Xinhe,Tianyang Hu,Hwee Kuan Lee,Kenji Kawaguchi*

Main category: cs.CL

TL;DR: 论文指出Prefix - Tuning在训练现代大语言模型时效果有限，提出Prefix - Tuning+，实验表明其性能优于Prefix - Tuning，与LoRA相当。


<details>
  <summary>Details</summary>
Motivation: Prefix - Tuning在训练现代大语言模型时效果有限，存在输入和前缀显著性的内在权衡问题。

Method: 提出Prefix - Tuning+，将前缀模块移出注意力头以解决Prefix - Tuning的缺点，并给出构建过程。

Result: 在多个基准测试中，Prefix - Tuning+始终优于现有的Prefix - Tuning方法，在几个通用基准上与LoRA性能相当。

Conclusion: 克服固有局限后，Prefix - Tuning仍是参数高效的大语言模型适配研究的有竞争力的方向。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for
rapidly adapting large language models (LLMs) to downstream tasks.
Prefix-Tuning, an early and effective PEFT technique, demonstrated the ability
to achieve performance comparable to full fine-tuning with significantly
reduced computational and memory overhead. However, despite its earlier
success, its effectiveness in training modern state-of-the-art LLMs has been
very limited. In this work, we demonstrate empirically that Prefix-Tuning
underperforms on LLMs because of an inherent tradeoff between input and prefix
significance within the attention head. This motivates us to introduce
Prefix-Tuning+, a novel architecture that generalizes the principles of
Prefix-Tuning while addressing its shortcomings by shifting the prefix module
out of the attention head itself. We further provide an overview of our
construction process to guide future users when constructing their own
context-based methods. Our experiments show that, across a diverse set of
benchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning
methods. Notably, it achieves performance on par with the widely adopted LoRA
method on several general benchmarks, highlighting the potential modern
extension of Prefix-Tuning approaches. Our findings suggest that by overcoming
its inherent limitations, Prefix-Tuning can remain a competitive and relevant
research direction in the landscape of parameter-efficient LLM adaptation.

</details>


### [512] [Balancing Knowledge Delivery and Emotional Comfort in Healthcare Conversational Systems](https://arxiv.org/abs/2506.13692)
*Shang-Chi Tsai,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 本文探讨医疗对话中知识分享与情感支持的平衡，利用大语言模型改写数据集并微调模型，实验表明该方法提升了模型情感回应能力。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统能回答医疗问题，但患者咨询时可能有负面情绪，模型若能在解答时给予安慰和共情，可提升咨询体验。

Method: 用大语言模型改写真实医疗对话数据集，生成含负面情绪的患者问题和安抚性回复，用多种微调方法优化最新大语言模型。

Result: 与原大语言模型相比，该方法显著提升了模型生成情感回应的能力，且保持了提供准确知识答案的能力。

Conclusion: 该方法能有效平衡医疗对话中知识分享和情感支持。

Abstract: With the advancement of large language models, many dialogue systems are now
capable of providing reasonable and informative responses to patients' medical
conditions. However, when patients consult their doctor, they may experience
negative emotions due to the severity and urgency of their situation. If the
model can provide appropriate comfort and empathy based on the patient's
negative emotions while answering medical questions, it will likely offer a
more reassuring experience during the medical consultation process. To address
this issue, our paper explores the balance between knowledge sharing and
emotional support in the healthcare dialogue process. We utilize a large
language model to rewrite a real-world interactive medical dialogue dataset,
generating patient queries with negative emotions and corresponding medical
responses aimed at soothing the patient's emotions while addressing their
concerns. The modified data serves to refine the latest large language models
with various fine-tuning methods, enabling them to accurately provide sentences
with both emotional reassurance and constructive suggestions in response to
patients' questions. Compared to the original LLM model, our experimental
results demonstrate that our methodology significantly enhances the model's
ability to generate emotional responses while maintaining its original
capability to provide accurate knowledge-based answers.

</details>


### [513] [Steering LLM Thinking with Budget Guidance](https://arxiv.org/abs/2506.13752)
*Junyan Li,Wenshuo Zhao,Yang Zhang,Chuang Gan*

Main category: cs.CL

TL;DR: 提出预算引导方法控制大语言模型推理长度，提升token效率，在数学基准测试中表现良好，还能泛化到其他任务。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型长时间推理成本高且性能提升不成比例，控制推理长度不牺牲性能有挑战，尤其是在思维预算紧张时。

Method: 提出预算引导方法，引入轻量级预测器在生成下一个token时对剩余思维长度建模Gamma分布，以软的token级方式引导生成。

Result: 能自然控制思维长度，在具有挑战性的数学基准测试中比基线方法显著提升token效率，如在MATH - 500基准测试中在紧张预算下准确率最多提高26%，用63%全思维模型的思维token保持有竞争力的准确率。

Conclusion: 预算引导方法有效，可控制推理长度，提升效率，还能泛化到更广泛任务领域，有估计问题难度等新兴能力。

Abstract: Recent deep-thinking large language models often reason extensively to
improve performance, but such lengthy reasoning is not always desirable, as it
incurs excessive inference costs with disproportionate performance gains.
Controlling reasoning length without sacrificing performance is therefore
important, but remains challenging, especially under tight thinking budgets. We
propose budget guidance, a simple yet effective method for steering the
reasoning process of LLMs toward a target budget without requiring any LLM
fine-tuning. Our approach introduces a lightweight predictor that models a
Gamma distribution over the remaining thinking length during next-token
generation. This signal is then used to guide generation in a soft, token-level
manner, ensuring that the overall reasoning trace adheres to the specified
thinking budget. Budget guidance enables natural control of the thinking
length, along with significant token efficiency improvements over baseline
methods on challenging math benchmarks. For instance, it achieves up to a 26%
accuracy gain on the MATH-500 benchmark under tight budgets compared to
baseline methods, while maintaining competitive accuracy with only 63% of the
thinking tokens used by the full-thinking model. Budget guidance also
generalizes to broader task domains and exhibits emergent capabilities, such as
estimating question difficulty. The source code is available at:
https://github.com/UMass-Embodied-AGI/BudgetGuidance.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [514] [Digital Transformation of Urban Planning in Australia: Influencing Factors and Key Challenges](https://arxiv.org/abs/2506.13333)
*Soheil Sabri,Sherah Kurnia*

Main category: cs.IT

TL;DR: 研究旨在了解澳大利亚城市规划数字转型的影响因素和关键挑战，采用相关理论和多案例研究方法，发现挑战与组织和外部环境因素有关，且行业缺乏数字成熟度模型。


<details>
  <summary>Details</summary>
Motivation: 当前公共服务数字技术和策略的步伐与成熟度不同，城市规划数字转型研究尚在发展，要了解澳大利亚城市规划数字转型的影响因素和关键挑战。

Method: 采用组织间理论和规划支持科学（PSScience），在TOE框架下，进行多案例研究，对维多利亚州和新南威尔士州政府及私营行业的13位IT和城市规划专家进行半结构化访谈。

Result: 澳大利亚城市规划系统数字转型的主要挑战与组织和外部环境因素有关，且该行业缺乏数字成熟度模型。

Conclusion: 本研究对城市规划数字转型的研究和实践具有重要意义。

Abstract: Over the past two decades, several governments in developing and developed
countries have started their journey toward digital transformation. However,
the pace and maturity of digital technologies and strategies are different
between public services. Current literature indicates that research on the
digital transformation of urban planning is still developing. Therefore, the
aim of this study is to understand the influencing factors and key challenges
for the digital transformation of urban planning in Australia. The study adopts
the inter-organisational theory and Planning Support Science (PSScience) under
the Technological, Organisational, and External Environmental (TOE) framework.
It involves a multiple case study, administered semi-structured interviews with
thirteen IT and urban planning experts across Victoria and New South Wales
governments and private industries. The study findings indicate that the main
challenges for digital transformation of the Australian urban planning system
are related to organisational and external environmental factors. Furthermore,
a digital maturity model is absent in the Australian urban planning industry.
This study offers important implications to research and practice related to
digital transformation in urban planning.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [515] [Machine Intelligence on Wireless Edge Networks](https://arxiv.org/abs/2506.12210)
*Sri Krishna Vadlamani,Kfir Sulimany,Zhihui Gao,Tingjun Chen,Dirk Englund*

Main category: cs.ET

TL;DR: 提出RF模拟架构MIWEN，可无线传输权重并在标准收发器前端分类，实现低能耗实时推理


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在功率受限边缘设备上因权重存储和数据移动成本高导致的推理瓶颈问题

Method: 将权重和激活编码到RF载波上，使用原生混频器作为计算单元，消除本地权重内存和数模转换开销

Result: 推导出射频模拟计算在热噪声下的有效位数，量化能量 - 精度权衡，以低几个数量级的能量实现与数字相当的MNIST准确率

Conclusion: MIWEN可实现低功耗、无内存边缘设备的实时推理

Abstract: Deep neural network (DNN) inference on power-constrained edge devices is
bottlenecked by costly weight storage and data movement. We introduce MIWEN, a
radio-frequency (RF) analog architecture that ``disaggregates'' memory by
streaming weights wirelessly and performing classification in the analog front
end of standard transceivers. By encoding weights and activations onto RF
carriers and using native mixers as computation units, MIWEN eliminates local
weight memory and the overhead of analog-to-digital and digital-to-analog
conversion. We derive the effective number of bits of radio-frequency analog
computation under thermal noise, quantify the energy--precision trade-off, and
demonstrate digital-comparable MNIST accuracy at orders-of-magnitude lower
energy, unlocking real-time inference on low-power, memory-free edge devices.

</details>


### [516] [Resilient-native and Intelligent NextG Systems](https://arxiv.org/abs/2506.12795)
*Mehdi Bennis*

Main category: cs.ET

TL;DR: 文章旨在定义无线通信网络的弹性概念，区分其与可靠性和鲁棒性的差异，深入探讨其数学原理，并提出针对性指标和权衡讨论。


<details>
  <summary>Details</summary>
Motivation: 自然和人为干扰增多，无线网络需具备应对意外事件的弹性，但该概念数学基础薄弱。

Method: 先定义弹性并与可靠性、鲁棒性区分，再探讨弹性的数学原理。

Result: 明确弹性概念，涉及弹性和可塑性两方面，核心是实时适应和反事实推理。

Conclusion: 给出细致指标，讨论网络弹性特性的权衡。

Abstract: Just like power, water and transportation systems, wireless networks are a
crucial societal infrastructure. As natural and human-induced disruptions
continue to grow, wireless networks must be resilient to unforeseen events,
able to withstand and recover from unexpected adverse conditions, shocks,
unmodeled disturbances and cascading failures. Despite its critical importance,
resilience remains an elusive concept, with its mathematical foundations still
underdeveloped. Unlike robustness and reliability, resilience is premised on
the fact that disruptions will inevitably happen. Resilience, in terms of
elasticity, focuses on the ability to bounce back to favorable states, while
resilience as plasticity involves agents (or networks) that can flexibly expand
their states, hypotheses and course of actions, by transforming through
real-time adaptation and reconfiguration. This constant situational awareness
and vigilance of adapting world models and counterfactually reasoning about
potential system failures and the corresponding best responses, is a core
aspect of resilience. This article seeks to first define resilience and
disambiguate it from reliability and robustness, before delving into the
mathematics of resilience. Finally, the article concludes by presenting nuanced
metrics and discussing trade-offs tailored to the unique characteristics of
network resilience.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [517] [Inverse design of the transmission matrix in a random system using Reinforcement Learning](https://arxiv.org/abs/2506.13057)
*Yuhao Kang*

Main category: physics.optics

TL;DR: 提出用强化学习修改传输矩阵实现散射系统逆设计的方法及三种传输矩阵结果


<details>
  <summary>Details</summary>
Motivation: 进行散射系统的逆设计

Method: 利用近端策略优化在目标函数的高度非凸景观中导航

Result: 实现三种传输矩阵：秩 - 1 矩阵中的固定比率功率转换和零传输模式、具有简并特征值的例外点和单向模式转换、传输特征值简并时强制均匀通道参与

Conclusion: 未明确提及结论，但表明该方法能实现特定传输矩阵

Abstract: This work presents an approach to the inverse design of scattering systems by
modifying the transmission matrix using reinforcement learning. We utilize
Proximal Policy Optimization to navigate the highly non-convex landscape of the
object function to achieve three types of transmission matrices: (1)
Fixed-ratio power conversion and zero-transmission mode in rank-1 matrices, (2)
exceptional points with degenerate eigenvalues and unidirectional mode
conversion, and (3) uniform channel participation is enforced when transmission
eigenvalues are degenerate.

</details>


### [518] [Machine Learning-Driven Compensation for Non-Ideal Channels in AWG-Based FBG Interrogator](https://arxiv.org/abs/2506.13575)
*Ivan A. Kazakov,Iana V. Kulichenko,Egor E. Kovalev,Angelina A. Treskova,Daria D. Barma,Kirill M. Malakhov,Arkady V. Shipulin*

Main category: physics.optics

TL;DR: 研究基于SiON - AWG的FBG interrogator，对比两种校准策略，ML方法更优


<details>
  <summary>Details</summary>
Motivation: AWG - based interrogators实际性能受非理想光谱响应限制，需更好校准策略

Method: 对比基于sigmoid拟合函数的分段分析模型和基于机器学习的回归模型两种校准策略

Result: 分析方法校准范围内RMSE为7.11 pm，ML方法为3.17 pm，且ML模型有泛化能力

Conclusion: ML校准是分析方法的有力替代，能提高非理想通道响应精度，减少手动校准工作，增强可扩展性

Abstract: We present an experimental study of a fiber Bragg grating (FBG) interrogator
based on a silicon oxynitride (SiON) photonic integrated arrayed waveguide
grating (AWG). While AWG-based interrogators are compact and scalable, their
practical performance is limited by non-ideal spectral responses. To address
this, two calibration strategies within a 2.4 nm spectral region were compared:
(1) a segmented analytical model based on a sigmoid fitting function, and (2) a
machine learning (ML)-based regression model. The analytical method achieves a
root mean square error (RMSE) of 7.11 pm within the calibrated range, while the
ML approach based on exponential regression achieves 3.17 pm. Moreover, the ML
model demonstrates generalization across an extended 2.9 nm wavelength span,
maintaining sub-5 pm accuracy without re-fitting. Residual and error
distribution analyses further illustrate the trade-offs between the two
approaches. ML-based calibration provides a robust, data-driven alternative to
analytical methods, delivering enhanced accuracy for non-ideal channel
responses, reduced manual calibration effort, and improved scalability across
diverse FBG sensor configurations.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [519] [Bridging the Digital Divide: Small Language Models as a Pathway for Physics and Photonics Education in Underdeveloped Regions](https://arxiv.org/abs/2506.12403)
*Asghar Ghorbani,Hanieh Fattahi*

Main category: physics.ed-ph

TL;DR: 文章指出欠发达地区物理和光子学教育存在障碍，介绍小语言模型（SLMs）可作为可扩展解决方案，缩小数字鸿沟，推动STEM教育。


<details>
  <summary>Details</summary>
Motivation: 解决欠发达地区有限基础设施、稀缺教育资源和不可靠网络接入对物理和光子学教育的阻碍，消除STEM教育中的不平等。

Method: 探索利用可在低功率设备上离线运行的小语言模型（SLMs），让其充当虚拟导师，实现母语教学和支持互动学习。

Result: SLMs可帮助解决训练有素的教育工作者短缺和实验室访问不足的问题。

Conclusion: 通过有针对性地投资人工智能技术，SLMs是一种可扩展且具包容性的解决方案，能推进STEM教育，促进边缘社区的科学赋能。

Abstract: Limited infrastructure, scarce educational resources, and unreliable internet
access often hinder physics and photonics education in underdeveloped regions.
These barriers create deep inequities in Science, Technology, Engineering, and
Mathematics (STEM) education. This article explores how Small Language Models
(SLMs)-compact, AI-powered tools that can run offline on low-power devices,
offering a scalable solution. By acting as virtual tutors, enabling
native-language instruction, and supporting interactive learning, SLMs can help
address the shortage of trained educators and laboratory access. By narrowing
the digital divide through targeted investment in AI technologies, SLMs present
a scalable and inclusive solution to advance STEM education and foster
scientific empowerment in marginalized communities.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [520] [A Minimum Distance Estimator Approach for Misspecified Ergodic Processes](https://arxiv.org/abs/2506.12432)
*Jaroslav I. Borodavka,Sebastian Krumscheid,Grigorios A. Pavliotis*

Main category: stat.ME

TL;DR: 提出最小距离估计器（MDE）用于错误指定模型的参数识别，证明其鲁棒性和渐近正态性，并给出数值实现。


<details>
  <summary>Details</summary>
Motivation: 对由遍历随机过程序列生成数据的错误指定模型，推断极限过程的参数。

Method: 定义参数估计的统计设置，提出MDE，证明其性质，用Julia语言实现数值计算。

Result: 证明了MDE的鲁棒性和多尺度扩散过程MDE的渐近正态性，给出了可处理的数值实现。

Conclusion: 提出的MDE可用于错误指定模型的参数识别，具有良好性质且可数值实现。

Abstract: We propose a minimum distance estimator (MDE) for parameter identification in
misspecified models characterized by a sequence of ergodic stochastic processes
that converge weakly to the model of interest. The data is generated by the
sequence of processes, and we are interested in inferring parameters for the
limiting processes. We define a general statistical setting for parameter
estimation under such model misspecification and prove the robustness of the
MDE. Furthermore, we prove the asymptotic normality of the MDE for multiscale
diffusion processes with a well-defined homogenized limit. A tractable
numerical implementation of the MDE is provided and realized in the programming
language Julia.

</details>


### [521] [Efficient Implementation of a Semiparametric Joint Model for Multivariate Longitudinal Biomarkers and Competing Risks Time-to-Event Data](https://arxiv.org/abs/2506.12741)
*Shanpeng Li,Emily Ouyang,Jin Zhou,Xinping Cui,Gang Li*

Main category: stat.ME

TL;DR: 本文介绍半参数多变量联合模型的高效实现，减少计算时间和内存消耗，通过模拟研究验证其可扩展性和估计准确性，还开发了R包FastJM。


<details>
  <summary>Details</summary>
Motivation: 半参数多变量联合模型在处理大规模数据时面临统计和计算挑战，限制了其在生物样本库规模数据集的应用。

Method: 在期望最大化（EM）框架内，结合正态近似和定制线性扫描算法，实现半参数多变量联合模型。

Result: 显著减少计算时间和内存消耗，可分析数千个受试者的数据；通过两个模拟研究验证了可扩展性和估计准确性；应用于原发性胆汁性肝硬化（PBC）数据集；开发了用户友好的R包FastJM。

Conclusion: 所提出的方法有效解决了半参数多变量联合模型处理大规模数据的问题，具有良好的可扩展性和估计准确性。

Abstract: Joint modeling has become increasingly popular for characterizing the
association between one or more longitudinal biomarkers and competing risks
time-to-event outcomes. However, semiparametric multivariate joint modeling for
large-scale data encounter substantial statistical and computational
challenges, primarily due to the high dimensionality of random effects and the
complexity of estimating nonparametric baseline hazards. These challenges often
lead to prolonged computation time and excessive memory usage, limiting the
utility of joint modeling for biobank-scale datasets. In this article, we
introduce an efficient implementation of a semiparametric multivariate joint
model, supported by a normal approximation and customized linear scan
algorithms within an expectation-maximization (EM) framework. Our method
significantly reduces computation time and memory consumption, enabling the
analysis of data from thousands of subjects. The scalability and estimation
accuracy of our approach are demonstrated through two simulation studies. We
also present an application to the Primary Biliary Cirrhosis (PBC) dataset
involving five longitudinal biomarkers as an illustrative example. A
user-friendly R package, \texttt{FastJM}, has been developed for the shared
random effects joint model with efficient implementation. The package is
publicly available on the Comprehensive R Archive Network:
https://CRAN.R-project.org/package=FastJM.

</details>


### [522] [Exploring Discrete Factor Analysis with the discFA Package in R](https://arxiv.org/abs/2506.13309)
*Reza Arabi Belaghi,Yasin Asar,Rolf Larsson*

Main category: stat.ME

TL;DR: 指出传统因子分析用于计数数据可能不合适，介绍离散因子分析，R 中 discFA 包提供八种模型选择，用前向搜索算法找最优模型并分析多领域示例。


<details>
  <summary>Details</summary>
Motivation: 传统因子分析用于计数数据可能不恰当，需新方法处理非负计数数据。

Method: 使用离散因子分析，用 R 中 discFA 包，考虑 Poisson 和 Negative Binomial 分布及零膨胀和截断情况，用前向搜索算法找最低 AIC 的最优因子模型。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: Literature suggested that using the traditional factor analysis for the count
data may be inappropriate. With that in mind, discrete factor analysis builds
on fitting systems of dependent discrete random variables to data. The data
should be in the form of non-negative counts. Data may also be truncated at
some positive integer value. The discFA package in R allows for two
distributions: Poisson and Negative Binomial, in combination with possible zero
inflation and possible truncation, hence, eight different alternatives. A
forward search algorithm is employed to find the model optimal factor model
with the lowest AIC. Several different illustrative examples from psychology,
agriculture, car industry, and a simulated data will be analyzed at the end.

</details>


### [523] [Penalised spline estimation of covariate-specific time-dependent ROC curves](https://arxiv.org/abs/2506.13604)
*María Xosé Rodríguez-Álvarez,Vanda Inácio*

Main category: stat.ME

TL;DR: 提出基于惩罚的时变ROC曲线估计器评估预后生物标志物准确性，模拟和实际应用表明方法有效，还提供R包。


<details>
  <summary>Details</summary>
Motivation: 医疗研究需高预测准确性的生物标志物，现有评估方法未考虑患者异质性。

Method: 引入基于惩罚的时变ROC曲线估计器，考虑灵活模型，用惩罚样条克服早期方法局限。

Result: 模拟研究成功恢复协变量特定时变ROC曲线形式和曲线下面积，与现有方法对比表现良好。

Conclusion: 所提方法在评估预后生物标志物准确性方面有效，可应用于实际数据，R包便于使用。

Abstract: The identification of biomarkers with high predictive accuracy is a crucial
task in medical research, as it can aid clinicians in making early decisions,
thereby reducing morbidity and mortality in high-risk populations.
Time-dependent receiver operating characteristic (ROC) curves are the main tool
used to assess the accuracy of prognostic biomarkers for outcomes that evolve
over time. Recognising the need to account for patient heterogeneity when
evaluating the accuracy of a prognostic biomarker, we introduce a novel
penalised-based estimator of the time-dependent ROC curve that accommodates a
possible modifying effect of covariates. We consider flexible models for both
the hazard function of the event time given the covariates and biomarker and
for the location-scale regression model of the biomarker given covariates,
enabling the accommodation of non-proportional hazards and nonlinear effects
through penalised splines, thus overcoming limitations of earlier methods. The
simulation study demonstrates that our approach successfully recovers the true
functional form of the covariate-specific time-dependent ROC curve and the
corresponding area under the curve across a variety of scenarios. Comparisons
with existing methods further show that our approach performs favourably in
multiple settings. Our approach is applied to evaluating the Global Registry of
Acute Coronary Events risk score's ability to predict mortality after discharge
in patients who have suffered an acute coronary syndrome and how this ability
may vary with the left ventricular ejection fraction. An R package,
CondTimeROC, implementing the proposed method is provided.

</details>


### [524] [Effect Decomposition of Functional-Output Computer Experiments via Orthogonal Additive Gaussian Processes](https://arxiv.org/abs/2506.12701)
*Yu Tan,Yongxiang Li,Xiaowu Dai,Kwok-Leung Tsui*

Main category: stat.ME

TL;DR: 本文提出功能输出正交加性高斯过程（FOAGP）解决功能输出FANOVA问题，经案例验证其在工程应用有实用价值。


<details>
  <summary>Details</summary>
Motivation: 功能输出FANOVA研究稀缺，传统方法难以捕捉复杂非线性关系且依赖数据分布假设。

Method: 提出FOAGP，通过对可分先验过程施加条件正交约束实现数据驱动正交性，还给出局部Sobol'指标和期望条件方差灵敏度指标的解析公式。

Result: 经两个模拟研究和一个机身形状控制实际案例验证，模型在正交效应分解和方差分解方面有效。

Conclusion: FOAGP在工程应用中有实际价值。

Abstract: Functional ANOVA (FANOVA) is a widely used variance-based sensitivity
analysis tool. However, studies on functional-output FANOVA remain relatively
scarce, especially for black-box computer experiments, which often involve
complex and nonlinear functional-output relationships with unknown data
distribution. Conventional approaches often rely on predefined basis functions
or parametric structures that lack the flexibility to capture complex nonlinear
relationships. Additionally, strong assumptions about the underlying data
distributions further limit their ability to achieve a data-driven orthogonal
effect decomposition. To address these challenges, this study proposes a
functional-output orthogonal additive Gaussian process (FOAGP) to efficiently
perform the data-driven orthogonal effect decomposition. By enforcing a
conditional orthogonality constraint on the separable prior process, the
proposed functional-output orthogonal additive kernel enables data-driven
orthogonality without requiring prior distributional assumptions. The FOAGP
framework also provides analytical formulations for local Sobol' indices and
expected conditional variance sensitivity indices, enabling comprehensive
sensitivity analysis by capturing both global and local effect significance.
Validation through two simulation studies and a real case study on fuselage
shape control confirms the model's effectiveness in orthogonal effect
decomposition and variance decomposition, demonstrating its practical value in
engineering applications.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [525] [Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow](https://arxiv.org/abs/2506.12600)
*Jie Pan,Tianyi Wang,Christian Claudel,Jing Shi*

Main category: cs.MA

TL;DR: 本文提出Trust - MARL框架解决异构交通环境下的合流挑战，经实验验证其在多方面有效。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中，人类行为的不可预测性在高速匝道合流等瓶颈处会扰乱交通流并影响系统性能，需要解决异构交通环境下的合作合流问题。

Method: 提出基于信任的多智能体强化学习（Trust - MARL）框架，宏观上利用智能体间信任提高全局交通效率，微观上设计动态信任机制让CAV调整合作策略，还集成信任触发的博弈论决策模块。

Result: 大量消融研究和对比实验验证了Trust - MARL方法的有效性，在不同CAV渗透率和交通密度下，安全、效率、舒适性和适应性均有显著提升。

Conclusion: Trust - MARL框架能有效解决异构交通环境下的合作合流问题，提升交通系统各方面性能。

Abstract: Intelligent transportation systems require connected and automated vehicles
(CAVs) to conduct safe and efficient cooperation with human-driven vehicles
(HVs) in complex real-world traffic environments. However, the inherent
unpredictability of human behaviour, especially at bottlenecks such as highway
on-ramp merging areas, often disrupts traffic flow and compromises system
performance. To address the challenge of cooperative on-ramp merging in
heterogeneous traffic environments, this study proposes a trust-based
multi-agent reinforcement learning (Trust-MARL) framework. At the macro level,
Trust-MARL enhances global traffic efficiency by leveraging inter-agent trust
to improve bottleneck throughput and mitigate traffic shockwave through
emergent group-level coordination. At the micro level, a dynamic trust
mechanism is designed to enable CAVs to adjust their cooperative strategies in
response to real-time behaviors and historical interactions with both HVs and
other CAVs. Furthermore, a trust-triggered game-theoretic decision-making
module is integrated to guide each CAV in adapting its cooperation factor and
executing context-aware lane-changing decisions under safety, comfort, and
efficiency constraints. An extensive set of ablation studies and comparative
experiments validates the effectiveness of the proposed Trust-MARL approach,
demonstrating significant improvements in safety, efficiency, comfort, and
adaptability across varying CAV penetration rates and traffic densities.

</details>


### [526] [Modeling Earth-Scale Human-Like Societies with One Billion Agents](https://arxiv.org/abs/2506.12078)
*Haoxiang Guan,Jiyan He,Liyang Fan,Zhenzhen Ren,Shaobin He,Xin Yu,Yuan Chen,Shuxin Zheng,Tie-Yan Liu,Zhen Liu*

Main category: cs.MA

TL;DR: 本文提出Light Society框架，结合大语言模型在行星尺度高效模拟类人类社会，通过大规模模拟验证其保真度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于代理的模型无法捕捉人类行为复杂性，大语言模型虽有潜力但存在扩展挑战，需开发新框架。

Method: 提出Light Society框架，将社会过程形式化为代理和环境状态的结构化转换，由大语言模型驱动模拟操作，通过事件队列执行。

Result: 对信任游戏和观点传播进行大规模模拟，证明该框架在模拟社会信任和信息传播方面的高保真度和效率，揭示更大规模模拟能产生更稳定和现实的涌现行为。

Conclusion: Light Society框架在建模人类社会行为方面既高效又有高保真度，在行星尺度模拟人类社会有良好前景。

Abstract: Understanding how complex societal behaviors emerge from individual cognition
and interactions requires both high-fidelity modeling of human behavior and
large-scale simulations. Traditional agent-based models (ABMs) have been
employed to study these dynamics for decades, but are constrained by simplified
agent behaviors that fail to capture human complexity. Recent advances in large
language models (LLMs) offer new opportunities by enabling agents to exhibit
sophisticated social behaviors that go beyond rule-based logic, yet face
significant scaling challenges. Here we present Light Society, an agent-based
simulation framework that advances both fronts, efficiently modeling human-like
societies at planetary scale powered by LLMs. Light Society formalizes social
processes as structured transitions of agent and environment states, governed
by a set of LLM-powered simulation operations, and executed through an event
queue. This modular design supports both independent and joint component
optimization, supporting efficient simulation of societies with over one
billion agents. Large-scale simulations of trust games and opinion
propagation--spanning up to one billion agents--demonstrate Light Society's
high fidelity and efficiency in modeling social trust and information
diffusion, while revealing scaling laws whereby larger simulations yield more
stable and realistic emergent behaviors.

</details>


### [527] [IndoorWorld: Integrating Physical Task Solving and Social Simulation in A Heterogeneous Multi-Agent Environment](https://arxiv.org/abs/2506.12331)
*Dekun Wu,Frederik Brudy,Bang Liu,Yi Wang*

Main category: cs.MA

TL;DR: 提出IndoorWorld多智能体环境，结合物理与社会动态，并通过实验展示其在建筑设计中基于大语言模型的建筑居住者模拟潜力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体研究环境要么侧重于物理任务解决，要么侧重于社会模拟，存在对智能体个性和社会动态简化、社会行为缺乏物理基础的问题。

Method: 引入IndoorWorld多智能体环境，在办公室场景中开展一系列实验，研究多智能体协作、资源竞争和空间布局对智能体行为的影响。

Result: IndoorWorld开启了基于大语言模型的建筑居住者模拟在建筑设计中的可能性。

Conclusion: IndoorWorld这种将物理和社会动态紧密结合的多智能体环境具有一定的应用价值。

Abstract: Virtual environments are essential to AI agent research. Existing
environments for LLM agent research typically focus on either physical task
solving or social simulation, with the former oversimplifying agent
individuality and social dynamics, and the latter lacking physical grounding of
social behaviors. We introduce IndoorWorld, a heterogeneous multi-agent
environment that tightly integrates physical and social dynamics. By
introducing novel challenges for LLM-driven agents in orchestrating social
dynamics to influence physical environments and anchoring social interactions
within world states, IndoorWorld opens up possibilities of LLM-based building
occupant simulation for architectural design. We demonstrate the potential with
a series of experiments within an office setting to examine the impact of
multi-agent collaboration, resource competition, and spatial layout on agent
behavior.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [528] [Statistical Machine Learning for Astronomy -- A Textbook](https://arxiv.org/abs/2506.12230)
*Yuan-Sen Ting*

Main category: astro-ph.IM

TL;DR: 该教科书从贝叶斯推理角度系统介绍天文学研究中的统计机器学习，构建统一框架，涵盖多种学习方法和计算技术，采用理论导向教学法。


<details>
  <summary>Details</summary>
Motivation: 为天文学研究提供系统的统计机器学习处理方法，揭示现代数据分析技术与传统统计方法的联系，强调不确定性量化和统计严谨性对天文学科学推理的重要性。

Method: 从概率论和贝叶斯推理开始，逐步介绍监督学习、无监督学习方法，引入计算技术，从第一原理推导各方法，强调统计洞察并结合天文学应用。

Result: 构建了涵盖多种方法的统一框架，以经典方法及其理论基础为现代技术（如神经网络）奠定基础。

Conclusion: 这种处理方式能让研究者在天文学研究中合理应用这些方法，考虑假设、局限性和不确定性传播，推动天文学知识进步。

Abstract: This textbook provides a systematic treatment of statistical machine learning
for astronomical research through the lens of Bayesian inference, developing a
unified framework that reveals connections between modern data analysis
techniques and traditional statistical methods. We show how these techniques
emerge from familiar statistical foundations. The consistently Bayesian
perspective prioritizes uncertainty quantification and statistical rigor
essential for scientific inference in astronomy. The textbook progresses from
probability theory and Bayesian inference through supervised learning including
linear regression with measurement uncertainties, logistic regression, and
classification. Unsupervised learning topics cover Principal Component Analysis
and clustering methods. We then introduce computational techniques through
sampling and Markov Chain Monte Carlo, followed by Gaussian Processes as
probabilistic nonparametric methods and neural networks within the broader
statistical context. Our theory-focused pedagogical approach derives each
method from first principles with complete mathematical development,
emphasizing statistical insight and complementing with astronomical
applications. We prioritize understanding why algorithms work, when they are
appropriate, and how they connect to broader statistical principles. The
treatment builds toward modern techniques including neural networks through a
solid foundation in classical methods and their theoretical underpinnings. This
foundation enables thoughtful application of these methods to astronomical
research, ensuring proper consideration of assumptions, limitations, and
uncertainty propagation essential for advancing astronomical knowledge in the
era of large astronomical surveys.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [529] [The Software Landscape for the Density Matrix Renormalization Group](https://arxiv.org/abs/2506.12629)
*Per Sehlstedt,Jan Brandejs,Paolo Bientinesi,Lars Karlsson*

Main category: physics.comp-ph

TL;DR: 本文对35个DMRG软件包进行了全面比较，发现功能有重叠，建议模块化和标准化以减少重复工作和提高互操作性。


<details>
  <summary>Details</summary>
Motivation: DMRG算法广泛应用但有众多独立实现，需全面了解软件现状，促进其更好发展。

Method: 对35个现有DMRG软件包的功能进行比较。

Result: 软件包功能有显著重叠，目前缺乏标准接口和模块化更多是社会因素导致。

Conclusion: 强调提高DMRG软件的内聚性和模块化的价值，有助于解决更复杂问题。

Abstract: The density matrix renormalization group (DMRG) algorithm is a cornerstone
computational method for studying quantum many-body systems, renowned for its
accuracy and adaptability. Despite DMRG's broad applicability across fields
such as materials science, quantum chemistry, and quantum computing, numerous
independent implementations have been developed. This survey maps the rapidly
expanding DMRG software landscape, providing a comprehensive comparison of
features among 35 existing packages. We found significant overlap in features
among the packages when comparing key aspects, such as parallelism strategies
for high-performance computing and symmetry-adapted formulations that enhance
efficiency. This overlap suggests opportunities for modularization of common
operations, including tensor operations, symmetry representations, and
eigensolvers, as the packages are mostly independent and share few third-party
library dependencies where functionality is factored out. More widespread
modularization and standardization would result in reduced duplication of
efforts and improved interoperability. We believe that the proliferation of
packages and the current lack of standard interfaces and modularity are more
social than technical. We aim to raise awareness of existing packages, guide
researchers in finding a suitable package for their needs, and help developers
identify opportunities for collaboration, modularity standardization, and
optimization. Ultimately, this work emphasizes the value of greater cohesion
and modularity, which would benefit DMRG software, allowing these powerful
algorithms to tackle more complex and ambitious problems.

</details>


### [530] [Latent Representation Learning of Multi-scale Thermophysics: Application to Dynamics in Shocked Porous Energetic Material](https://arxiv.org/abs/2506.12996)
*Shahab Azarfar,Joseph B. Choi,Phong CH. Nguyen,Yen T. Nguyen,Pradeep Seshadri,H. S. Udaykumar,Stephen Baek*

Main category: physics.comp-ph

TL;DR: 本文提出基于自然语言处理中分词思想的元学习方法，学习微观尺度物理的简化表示以加速中尺度学习，该方法在中尺度数据稀缺时优于PARC，可应用于多尺度建模问题。


<details>
  <summary>Details</summary>
Motivation: 中尺度模拟计算成本高，给基于深度学习的代理模型从头训练带来实际挑战，需寻找替代方法。

Method: 受自然语言处理中分词思想启发，对中尺度物理场演化进行分词，学习微观尺度动力学的概率潜在表示作为中尺度动力学的构建块，中尺度潜在动力学模型在小数据集上训练学习相邻构建块的相关性。

Result: 所提模型在中尺度数据稀缺时性能优于仅在完整中尺度数据集上训练的PARC。

Conclusion: 该方法通过利用低成本微观尺度模拟和在小中尺度数据集上快速训练加速封闭模型的开发，可应用于一系列多尺度建模问题。

Abstract: Coupling of physics across length and time scales plays an important role in
the response of microstructured materials to external loads. In a multi-scale
framework, unresolved (subgrid) meso-scale dynamics is upscaled to the
homogenized (macro-scale) representation of the heterogeneous material through
closure models. Deep learning models trained using meso-scale simulation data
are now a popular route to assimilate such closure laws. However, meso-scale
simulations are computationally taxing, posing practical challenges in training
deep learning-based surrogate models from scratch. In this work, we investigate
an alternative meta-learning approach motivated by the idea of tokenization in
natural language processing. We show that one can learn a reduced
representation of the micro-scale physics to accelerate the meso-scale learning
process by tokenizing the meso-scale evolution of the physical fields involved
in an archetypal, albeit complex, reactive dynamics problem, \textit{viz.},
shock-induced energy localization in a porous energetic material. A
probabilistic latent representation of \textit{micro}-scale dynamics is learned
as building blocks for \textit{meso}-scale dynamics. The \textit{meso-}scale
latent dynamics model learns the correlation between neighboring building
blocks by training over a small dataset of meso-scale simulations. We compare
the performance of our model with a physics-aware recurrent convolutional
neural network (PARC) trained only on the full meso-scale dataset. We
demonstrate that our model can outperform PARC with scarce meso-scale data. The
proposed approach accelerates the development of closure models by leveraging
inexpensive micro-scale simulations and fast training over a small meso-scale
dataset, and can be applied to a range of multi-scale modeling problems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [531] [Using Behavior Trees in Risk Assessment](https://arxiv.org/abs/2506.12089)
*Razan Ghzouli,Atieh Hanna,Endre Erös,Rebekka Wohlrab*

Main category: cs.RO

TL;DR: 本文提出以开发为中心的基于行为树模型的早期风险评估方法，经实践评估有潜力但需进一步发展。


<details>
  <summary>Details</summary>
Motivation: 网络物理生产系统的协作机器人任务需稳健安全，行业虽建议早期风险评估，但实践中专家早期难理解任务和落实评估结果。

Method: 采用设计科学研究，用行为树模型支持风险评估活动。

Result: 与四家公司的五名从业者评估，发现行为树模型能支持早期识别、可视化及弥合代码实现与风险评估输出的差距。

Conclusion: 此为首次用行为树模型支持风险评估，需进一步发展。

Abstract: Cyber-physical production systems increasingly involve collaborative robotic
missions, requiring more demand for robust and safe missions. Industries rely
on risk assessments to identify potential failures and implement measures to
mitigate their risks. Although it is recommended to conduct risk assessments
early in the design of robotic missions, the state of practice in the industry
is different. Safety experts often struggle to completely understand robotics
missions at the early design stages of projects and to ensure that the output
of risk assessments is adequately considered during implementation.
  This paper presents a design science study that conceived a model-based
approach for early risk assessment in a development-centric way. Our approach
supports risk assessment activities by using the behavior-tree model. We
evaluated the approach together with five practitioners from four companies.
Our findings highlight the potential of the behavior-tree model in supporting
early identification, visualisation, and bridging the gap between code
implementation and risk assessments' outputs. This approach is the first
attempt to use the behavior-tree model to support risk assessment; thus, the
findings highlight the need for further development.

</details>


### [532] [ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration](https://arxiv.org/abs/2506.12248)
*Jennifer Grannen,Siddharth Karamcheti,Blake Wulfe,Dorsa Sadigh*

Main category: cs.RO

TL;DR: 本文提出ProVox框架使机器人能从早期交互推断伙伴目标，主动规划行为，经家务操作任务用户研究评估，该框架提升协作效率、减轻用户负担。


<details>
  <summary>Details</summary>
Motivation: 协作机器人需快速适应伙伴意图和偏好，主动识别有用行为，尤其是在人类能持续教导机器人新能力的场景。

Method: 引入ProVox框架，设计元提示协议让用户传达偏好等信息，用个性化提示调整主动语言模型任务规划器。

Result: 通过家务操作任务用户研究，相比非主动基线，任务完成时间快38.7%，用户负担减少31.9%。

Conclusion: 元提示和主动性对提升协作效率、减轻用户负担至关重要。

Abstract: Collaborative robots must quickly adapt to their partner's intent and
preferences to proactively identify helpful actions. This is especially true in
situated settings where human partners can continually teach robots new
high-level behaviors, visual concepts, and physical skills (e.g., through
demonstration), growing the robot's capabilities as the human-robot pair work
together to accomplish diverse tasks. In this work, we argue that robots should
be able to infer their partner's goals from early interactions and use this
information to proactively plan behaviors ahead of explicit instructions from
the user. Building from the strong commonsense priors and steerability of large
language models, we introduce ProVox ("Proactive Voice"), a novel framework
that enables robots to efficiently personalize and adapt to individual
collaborators. We design a meta-prompting protocol that empowers users to
communicate their distinct preferences, intent, and expected robot behaviors
ahead of starting a physical interaction. ProVox then uses the personalized
prompt to condition a proactive language model task planner that anticipates a
user's intent from the current interaction context and robot capabilities to
suggest helpful actions; in doing so, we alleviate user burden, minimizing the
amount of time partners spend explicitly instructing and supervising the robot.
We evaluate ProVox through user studies grounded in household manipulation
tasks (e.g., assembling lunch bags) that measure the efficiency of the
collaboration, as well as features such as perceived helpfulness, ease of use,
and reliability. Our analysis suggests that both meta-prompting and proactivity
are critical, resulting in 38.7% faster task completion times and 31.9% less
user burden relative to non-active baselines. Supplementary material, code, and
videos can be found at https://provox-2025.github.io.

</details>


### [533] [AntiGrounding: Lifting Robotic Actions into VLM Representation Space for Decision Making](https://arxiv.org/abs/2506.12374)
*Wenbo Li,Shiyi Wang,Yiteng Chen,Huiping Zhuang,Qingyao Wu*

Main category: cs.RO

TL;DR: 提出AntiGrounding框架和离线策略细化模块用于机器人操作，实验显示优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将视觉语言模型投影到压缩中间表示时会丢弃重要的特定任务信息。

Method: 提出AntiGrounding框架，将候选动作提升到VLM表示空间，多视角渲染轨迹，用结构化视觉问答做决策；提出离线策略细化模块。

Result: 在模拟和现实环境实验中，方法在多种机器人操作任务上优于基线。

Conclusion: 所提方法能实现新任务的零样本最优闭环机器人轨迹合成，且离线策略细化模块可提升长期性能。

Abstract: Vision-Language Models (VLMs) encode knowledge and reasoning capabilities for
robotic manipulation within high-dimensional representation spaces. However,
current approaches often project them into compressed intermediate
representations, discarding important task-specific information such as
fine-grained spatial or semantic details. To address this, we propose
AntiGrounding, a new framework that reverses the instruction grounding process.
It lifts candidate actions directly into the VLM representation space, renders
trajectories from multiple views, and uses structured visual question answering
for instruction-based decision making. This enables zero-shot synthesis of
optimal closed-loop robot trajectories for new tasks. We also propose an
offline policy refinement module that leverages past experience to enhance
long-term performance. Experiments in both simulation and real-world
environments show that our method outperforms baselines across diverse robotic
manipulation tasks.

</details>


### [534] [Deep Fusion of Ultra-Low-Resolution Thermal Camera and Gyroscope Data for Lighting-Robust and Compute-Efficient Rotational Odometry](https://arxiv.org/abs/2506.12536)
*Farida Mohsen,Ali Safa*

Main category: cs.RO

TL;DR: 提出热成像与陀螺仪融合的旋转里程计新方法，降低热成像分辨率同时保证精度，适合资源受限系统并公开数据集。


<details>
  <summary>Details</summary>
Motivation: 精确的旋转里程计对自主机器人系统至关重要，现有惯性传感器有漂移问题，需要新方法。

Method: 开发多模态数据采集系统收集数据，设计并训练轻量级CNN融合热成像与陀螺仪数据进行旋转速度估计。

Result: 热成像与陀螺仪融合可大幅降低热成像分辨率，且不显著影响精度，提高计算效率和内存利用率。

Conclusion: 该方法适合资源受限的机器人系统实时部署，公开数据集利于后续研究。

Abstract: Accurate rotational odometry is crucial for autonomous robotic systems,
particularly for small, power-constrained platforms such as drones and mobile
robots. This study introduces thermal-gyro fusion, a novel sensor fusion
approach that integrates ultra-low-resolution thermal imaging with gyroscope
readings for rotational odometry. Unlike RGB cameras, thermal imaging is
invariant to lighting conditions and, when fused with gyroscopic data,
mitigates drift which is a common limitation of inertial sensors. We first
develop a multimodal data acquisition system to collect synchronized thermal
and gyroscope data, along with rotational speed labels, across diverse
environments. Subsequently, we design and train a lightweight Convolutional
Neural Network (CNN) that fuses both modalities for rotational speed
estimation. Our analysis demonstrates that thermal-gyro fusion enables a
significant reduction in thermal camera resolution without significantly
compromising accuracy, thereby improving computational efficiency and memory
utilization. These advantages make our approach well-suited for real-time
deployment in resource-constrained robotic systems. Finally, to facilitate
further research, we publicly release our dataset as supplementary material.

</details>


### [535] [On-board Sonar Data Classification for Path Following in Underwater Vehicles using Fast Interval Type-2 Fuzzy Extreme Learning Machine](https://arxiv.org/abs/2506.12762)
*Adrian Rubio-Solis,Luciano Nava-Balanzar,Tomas Salgado-Jimenez*

Main category: cs.RO

TL;DR: 本文将FIT2 - FELM用于训练TSK IT2 - FIS进行水下声纳数据分类，并将其集成到HNS中，使水下航行器在不确定和噪声环境下能稳健导航。


<details>
  <summary>Details</summary>
Motivation: 在自主水下任务中，水下航行器成功完成预定路径依赖于其环境识别能力，因此要研究相关方法。

Method: 应用FIT2 - FELM训练TSK IT2 - FIS进行声纳数据分类，将TSK IT2 - FIS集成到HNS作为导航引擎。

Result: 与传统导航架构相比，在不确定和噪声环境下观察到稳健的路径跟踪行为。

Conclusion: 所提出的方法能让水下航行器在执行多任务进行实时导航规划时，获得更全面的周围环境感知。

Abstract: In autonomous underwater missions, the successful completion of predefined
paths mainly depends on the ability of underwater vehicles to recognise their
surroundings. In this study, we apply the concept of Fast Interval Type-2 Fuzzy
Extreme Learning Machine (FIT2-FELM) to train a Takagi-Sugeno-Kang IT2 Fuzzy
Inference System (TSK IT2-FIS) for on-board sonar data classification using an
underwater vehicle called BlueROV2. The TSK IT2-FIS is integrated into a
Hierarchical Navigation Strategy (HNS) as the main navigation engine to infer
local motions and provide the BlueROV2 with full autonomy to follow an
obstacle-free trajectory in a water container of 2.5m x 2.5m x 3.5m. Compared
to traditional navigation architectures, using the proposed method, we observe
a robust path following behaviour in the presence of uncertainty and noise. We
found that the proposed approach provides the BlueROV with a more complete
sensory picture about its surroundings while real-time navigation planning is
performed by the concurrent execution of two or more tasks.

</details>


### [536] [Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence](https://arxiv.org/abs/2506.12678)
*Pranay Gupta,Henny Admoni,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 本文提出新方法提升端到端视觉运动策略在分布外（OOD）条件下的泛化能力，通过功能对应反馈干预，在真实机器人操作任务中验证有效。


<details>
  <summary>Details</summary>
Motivation: 端到端视觉运动策略在OOD视觉下难以可靠行动，以往交互式模仿学习方法成本高、效率低。

Method: 检测反馈需求，征求功能对应反馈，用功能对应的分布内（ID）观测干预OOD观测以实现部署时泛化。

Result: 在Franka Panda机器人操作任务中验证，测试时的功能对应能以低反馈提升基于视觉的扩散策略对OOD对象和环境条件的泛化能力。

Conclusion: 所提方法可有效提升策略在OOD条件下的泛化能力。

Abstract: End-to-end visuomotor policies trained using behavior cloning have shown a
remarkable ability to generate complex, multi-modal low-level robot behaviors.
However, at deployment time, these policies still struggle to act reliably when
faced with out-of-distribution (OOD) visuals induced by objects, backgrounds,
or environment changes. Prior works in interactive imitation learning solicit
corrective expert demonstrations under the OOD conditions -- but this can be
costly and inefficient. We observe that task success under OOD conditions does
not always warrant novel robot behaviors. In-distribution (ID) behaviors can
directly be transferred to OOD conditions that share functional similarities
with ID conditions. For example, behaviors trained to interact with
in-distribution (ID) pens can apply to interacting with a visually-OOD pencil.
The key challenge lies in disambiguating which ID observations functionally
correspond to the OOD observation for the task at hand. We propose that an
expert can provide this OOD-to-ID functional correspondence. Thus, instead of
collecting new demonstrations and re-training at every OOD encounter, our
method: (1) detects the need for feedback by first checking if current
observations are OOD and then identifying whether the most similar training
observations show divergent behaviors, (2) solicits functional correspondence
feedback to disambiguate between those behaviors, and (3) intervenes on the OOD
observations with the functionally corresponding ID observations to perform
deployment-time generalization. We validate our method across diverse
real-world robotic manipulation tasks with a Franka Panda robotic manipulator.
Our results show that test-time functional correspondences can improve the
generalization of a vision-based diffusion policy to OOD objects and
environment conditions with low feedback.

</details>


### [537] [KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills](https://arxiv.org/abs/2506.12851)
*Weiji Xie,Jinrui Han,Jiakun Zheng,Huanyu Li,Xinzhe Liu,Jiyuan Shi,Weinan Zhang,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出基于物理的人形控制框架，通过多步运动处理和自适应运动跟踪掌握高动态人类行为，实验表明该方法跟踪误差低，能部署在机器人上。


<details>
  <summary>Details</summary>
Motivation: 现有算法只能跟踪平滑、低速人类运动，本文旨在掌握如功夫和舞蹈等高动态人类行为。

Method: 设计运动处理流程，包括提取、过滤、校正和重定向运动；构建双层优化问题实现自适应课程机制；搭建非对称演员 - 评论家框架进行策略训练。

Result: 训练全身控制策略模仿高动态运动，跟踪误差显著低于现有方法，成功部署在Unitree G1机器人上。

Conclusion: 所提基于物理的人形控制框架有效，能实现稳定且富有表现力的行为。

Abstract: Humanoid robots are promising to acquire various skills by imitating human
behaviors. However, existing algorithms are only capable of tracking smooth,
low-speed human motions, even with delicate reward and curriculum design. This
paper presents a physics-based humanoid control framework, aiming to master
highly-dynamic human behaviors such as Kungfu and dancing through multi-steps
motion processing and adaptive motion tracking. For motion processing, we
design a pipeline to extract, filter out, correct, and retarget motions, while
ensuring compliance with physical constraints to the maximum extent. For motion
imitation, we formulate a bi-level optimization problem to dynamically adjust
the tracking accuracy tolerance based on the current tracking error, creating
an adaptive curriculum mechanism. We further construct an asymmetric
actor-critic framework for policy training. In experiments, we train whole-body
control policies to imitate a set of highly-dynamic motions. Our method
achieves significantly lower tracking errors than existing approaches and is
successfully deployed on the Unitree G1 robot, demonstrating stable and
expressive behaviors. The project page is https://kungfu-bot.github.io.

</details>


### [538] [RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control](https://arxiv.org/abs/2506.12769)
*Junpeng Yue,Zepeng Wang,Yuxuan Wang,Weishuai Zeng,Jiangxing Wang,Xinrun Xu,Yu Zhang,Sipeng Zheng,Ziluo Ding,Zongqing Lu*

Main category: cs.RO

TL;DR: 本文提出RLPF框架解决文本驱动的人形机器人动作生成问题，实验表明其能生成物理可行且语义对应的动作。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成方法生成的动作在运动学或物理上不可行，难以用于现实场景，需弥合模拟到现实的差距。

Method: 提出RLPF框架，将物理感知的动作评估与文本条件动作生成相结合，用动作跟踪策略评估可行性，引入对齐验证模块确保语义保真。

Result: RLPF在生成物理可行动作且保持与文本指令语义对应方面大幅超越基线方法，能成功部署在真人形机器人上。

Conclusion: RLPF框架有效解决了文本驱动人形机器人动作生成中模拟到现实的差距问题。

Abstract: This paper focuses on a critical challenge in robotics: translating
text-driven human motions into executable actions for humanoid robots, enabling
efficient and cost-effective learning of new behaviors. While existing
text-to-motion generation methods achieve semantic alignment between language
and motion, they often produce kinematically or physically infeasible motions
unsuitable for real-world deployment. To bridge this sim-to-real gap, we
propose Reinforcement Learning from Physical Feedback (RLPF), a novel framework
that integrates physics-aware motion evaluation with text-conditioned motion
generation. RLPF employs a motion tracking policy to assess feasibility in a
physics simulator, generating rewards for fine-tuning the motion generator.
Furthermore, RLPF introduces an alignment verification module to preserve
semantic fidelity to text instructions. This joint optimization ensures both
physical plausibility and instruction alignment. Extensive experiments show
that RLPF greatly outperforms baseline methods in generating physically
feasible motions while maintaining semantic correspondence with text
instruction, enabling successful deployment on real humanoid robots.

</details>


### [539] [From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots](https://arxiv.org/abs/2506.12779)
*Yuxuan Wang,Ming Yang,Weishuai Zeng,Yu Zhang,Xinrun Xu,Haobin Jiang,Ziluo Ding,Zongqing Lu*

Main category: cs.RO

TL;DR: 提出BumbleBee框架结合运动聚类和仿真到现实适应，实现类人机器人全身控制，实验表现达到最优。


<details>
  <summary>Details</summary>
Motivation: 现有框架在训练单运动策略时难以泛化不同行为，因控制要求冲突和数据分布不匹配，类人机器人全身控制仍是挑战。

Method: 先利用基于自编码器的聚类方法分组相似运动，在各簇中训练专家策略并用真实数据优化，最后将专家策略提炼为统一的通用控制器。

Result: 在两个仿真和真实类人机器人上实验，实现了最先进的通用全身控制。

Conclusion: BumbleBee框架为现实世界中类人机器人敏捷、鲁棒和可泛化表现设定了新基准。

Abstract: Achieving general agile whole-body control on humanoid robots remains a major
challenge due to diverse motion demands and data conflicts. While existing
frameworks excel in training single motion-specific policies, they struggle to
generalize across highly varied behaviors due to conflicting control
requirements and mismatched data distributions. In this work, we propose
BumbleBee (BB), an expert-generalist learning framework that combines motion
clustering and sim-to-real adaptation to overcome these challenges. BB first
leverages an autoencoder-based clustering method to group behaviorally similar
motions using motion features and motion descriptions. Expert policies are then
trained within each cluster and refined with real-world data through iterative
delta action modeling to bridge the sim-to-real gap. Finally, these experts are
distilled into a unified generalist controller that preserves agility and
robustness across all motion types. Experiments on two simulations and a real
humanoid robot demonstrate that BB achieves state-of-the-art general whole-body
control, setting a new benchmark for agile, robust, and generalizable humanoid
performance in the real world.

</details>


### [540] [IKDiffuser: Fast and Diverse Inverse Kinematics Solution Generation for Multi-arm Robotic Systems](https://arxiv.org/abs/2506.13087)
*Zeyu Zhang,Ziyuan Jiao*

Main category: cs.RO

TL;DR: 提出用于多臂机器人系统的基于扩散模型的IKDiffuser，在实验中表现优于现有求解器，提供解决多臂逆运动学问题的统一方法。


<details>
  <summary>Details</summary>
Motivation: 多臂机器人系统的逆运动学（IK）问题因复杂自碰撞、耦合关节和高维冗余等挑战，传统求解器慢、易失败且缺乏解的多样性。

Method: 提出IKDiffuser，学习配置空间上的联合分布，可在推理时纳入额外目标而无需重新训练。

Result: 在6种不同多臂系统实验中，IKDiffuser在解的准确性、精度、多样性和计算效率上优于现有求解器。

Conclusion: IKDiffuser框架为解决多臂IK问题提供可扩展、统一的方法，有助于多臂机器人系统在实时操作任务中的应用。

Abstract: Solving Inverse Kinematics (IK) problems is fundamental to robotics, but has
primarily been successful with single serial manipulators. For multi-arm
robotic systems, IK remains challenging due to complex self-collisions, coupled
joints, and high-dimensional redundancy. These complexities make traditional IK
solvers slow, prone to failure, and lacking in solution diversity. In this
paper, we present IKDiffuser, a diffusion-based model designed for fast and
diverse IK solution generation for multi-arm robotic systems. IKDiffuser learns
the joint distribution over the configuration space, capturing complex
dependencies and enabling seamless generalization to multi-arm robotic systems
of different structures. In addition, IKDiffuser can incorporate additional
objectives during inference without retraining, offering versatility and
adaptability for task-specific requirements. In experiments on 6 different
multi-arm systems, the proposed IKDiffuser achieves superior solution accuracy,
precision, diversity, and computational efficiency compared to existing
solvers. The proposed IKDiffuser framework offers a scalable, unified approach
to solving multi-arm IK problems, facilitating the potential of multi-arm
robotic systems in real-time manipulation tasks.

</details>


### [541] [A Survey on Imitation Learning for Contact-Rich Tasks in Robotics](https://arxiv.org/abs/2506.13498)
*Toshiaki Tsuji,Yasuhiro Kato,Gokhan Solak,Heng Zhang,Tadej Petrič,Francesco Nori,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文全面调研接触丰富的机器人任务的模仿学习研究趋势，分析示范收集方法和模仿学习方法，指出多模态学习等进展提升了性能，为未来研究奠基。


<details>
  <summary>Details</summary>
Motivation: 接触丰富任务因非线性动态和对位置偏差敏感，是机器人领域的核心挑战，需调研其模仿学习研究。

Method: 研究示范收集方法，分析模仿学习方法并结合近期多模态学习和基础模型进展。

Result: 多模态学习和基础模型显著提升复杂接触任务在工业、家庭和医疗领域的性能。

Conclusion: 本次调研为接触丰富的机器人操作的未来发展奠定基础。

Abstract: This paper comprehensively surveys research trends in imitation learning for
contact-rich robotic tasks. Contact-rich tasks, which require complex physical
interactions with the environment, represent a central challenge in robotics
due to their nonlinear dynamics and sensitivity to small positional deviations.
The paper examines demonstration collection methodologies, including teaching
methods and sensory modalities crucial for capturing subtle interaction
dynamics. We then analyze imitation learning approaches, highlighting their
applications to contact-rich manipulation. Recent advances in multimodal
learning and foundation models have significantly enhanced performance in
complex contact tasks across industrial, household, and healthcare domains.
Through systematic organization of current research and identification of
challenges, this survey provides a foundation for future advancements in
contact-rich robotic manipulation.

</details>


### [542] [Towards a Formal Specification for Self-organized Shape Formation in Swarm Robotics](https://arxiv.org/abs/2506.13453)
*YR Darr,MA Niazi*

Main category: cs.RO

TL;DR: 本文用形式化规范方法（Z语言）对群体机器人自组织形状形成过程建模，证明其有效性，为设计实现系统及在仿真环境中建模提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有形式化规范方法未用于群体机器人系统自组织形状形成过程建模，本文希望填补此空白。

Method: 使用形式化规范的Z语言对系统实体状态建模。

Result: 证明了Z语言用于自组织形状形成的有效性。

Conclusion: 形式化规范模型为设计实现群体机器人复杂形状结构形成系统提供轮廓，也为在仿真环境中用多智能体系统建模复杂形状形成过程提供基础。

Abstract: The self-organization of robots for the formation of structures and shapes is
a stimulating application of the swarm robotic system. It involves a large
number of autonomous robots of heterogeneous behavior, coordination among them,
and their interaction with the dynamic environment. This process of complex
structure formation is considered a complex system, which needs to be modeled
by using any modeling approach. Although the formal specification approach
along with other formal methods has been used to model the behavior of robots
in a swarm. However, to the best of our knowledge, the formal specification
approach has not been used to model the self-organization process in swarm
robotic systems for shape formation. In this paper, we use a formal
specification approach to model the shape formation task of swarm robots. We
use Z (Zed) language of formal specification, which is a state-based language,
to model the states of the entities of the systems. We demonstrate the
effectiveness of Z for the self-organized shape formation. The presented formal
specification model gives the outlines for designing and implementing the swarm
robotic system for the formation of complex shapes and structures. It also
provides the foundation for modeling the complex shape formation process for
swarm robotics using a multi-agent system in a simulation-based environment.
Keywords: Swarm robotics, Self-organization, Formal specification, Complex
systems

</details>


### [543] [What Matters in Learning from Large-Scale Datasets for Robot Manipulation](https://arxiv.org/abs/2506.13536)
*Vaibhav Saxena,Matthew Bronars,Nadun Ranawaka Arachchige,Kuancheng Wang,Woo Chul Shin,Soroush Nasiriany,Ajay Mandlekar,Danfei Xu*

Main category: cs.RO

TL;DR: 文章开展大规模数据集构成研究，探讨机器人数据集应收集的数据及示范检索策略，得出关键见解并在现实中验证效果。


<details>
  <summary>Details</summary>
Motivation: 缺乏对收集何种数据以提升机器人数据集效用和促进下游策略学习的系统理解。

Method: 开发数据生成框架模拟现有数据集中的多样性来源，生成可控构成的大规模机器人数据集，进行数据集构成研究。

Result: 发现相机姿态和空间排列对数据收集多样性和示范检索对齐至关重要，在现实中检索策略能比现有训练策略高70%。

Conclusion: 研究得出的见解在模拟和现实中均有效，检索策略能提升下游策略性能。

Abstract: Imitation learning from large multi-task demonstration datasets has emerged
as a promising path for building generally-capable robots. As a result, 1000s
of hours have been spent on building such large-scale datasets around the
globe. Despite the continuous growth of such efforts, we still lack a
systematic understanding of what data should be collected to improve the
utility of a robotics dataset and facilitate downstream policy learning. In
this work, we conduct a large-scale dataset composition study to answer this
question. We develop a data generation framework to procedurally emulate common
sources of diversity in existing datasets (such as sensor placements and object
types and arrangements), and use it to generate large-scale robot datasets with
controlled compositions, enabling a suite of dataset composition studies that
would be prohibitively expensive in the real world. We focus on two practical
settings: (1) what types of diversity should be emphasized when future
researchers collect large-scale datasets for robotics, and (2) how should
current practitioners retrieve relevant demonstrations from existing datasets
to maximize downstream policy performance on tasks of interest. Our study
yields several critical insights -- for example, we find that camera poses and
spatial arrangements are crucial dimensions for both diversity in collection
and alignment in retrieval. In real-world robot learning settings, we find that
not only do our insights from simulation carry over, but our retrieval
strategies on existing datasets such as DROID allow us to consistently
outperform existing training strategies by up to 70%. More results at
https://robo-mimiclabs.github.io/

</details>


### [544] [ROSA: Harnessing Robot States for Vision-Language and Action Alignment](https://arxiv.org/abs/2506.13679)
*Yuqing Wen,Kefan Gu,Haoxuan Liu,Yucheng Zhao,Tiancai Wang,Haoqiang Fan,Xiaoyan Sun*

Main category: cs.RO

TL;DR: 提出ROSA训练范式解决VLA模型中视觉语言与动作空间对齐问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型开发中直接微调VLMs存在时空差距，导致数据低效和依赖人力，需解决视觉语言空间与机器人动作空间的对齐问题。

Method: 提出ROSA训练范式，利用机器人状态估计来改善视觉语言与动作空间的对齐。

Result: 在模拟和现实环境的大量实验证明ROSA有效，尤其在低数据场景。

Conclusion: ROSA能让VLA模型增强空间理解和自我意识，提升性能和泛化能力。

Abstract: Vision-Language-Action (VLA) models have recently made significant advance in
multi-task, end-to-end robotic control, due to the strong generalization
capabilities of Vision-Language Models (VLMs). A fundamental challenge in
developing such models is effectively aligning the vision-language space with
the robotic action space. Existing approaches typically rely on directly
fine-tuning VLMs using expert demonstrations. However, this strategy suffers
from a spatio-temporal gap, resulting in considerable data inefficiency and
heavy reliance on human labor. Spatially, VLMs operate within a high-level
semantic space, whereas robotic actions are grounded in low-level 3D physical
space; temporally, VLMs primarily interpret the present, while VLA models
anticipate future actions. To overcome these challenges, we propose a novel
training paradigm, ROSA, which leverages robot state estimation to improve
alignment between vision-language and action spaces. By integrating robot state
estimation data obtained via an automated process, ROSA enables the VLA model
to gain enhanced spatial understanding and self-awareness, thereby boosting
performance and generalization. Extensive experiments in both simulated and
real-world environments demonstrate the effectiveness of ROSA, particularly in
low-data regimes.

</details>


### [545] [LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction](https://arxiv.org/abs/2506.13751)
*Haoru Xue,Xiaoyu Huang,Dantong Niu,Qiayuan Liao,Thomas Kragerud,Jan Tommy Gravdahl,Xue Bin Peng,Guanya Shi,Trevor Darrell,Koushil Screenath,Shankar Sastry*

Main category: cs.RO

TL;DR: 本文提出首个用于人形机器人全身控制的视觉语言闭环基准测试，并提出LeVERB框架，在基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言动作模型假设底层控制器准确，限制了在人形机器人全身控制任务中的应用，为填补该空白开展研究。

Method: 引入包含10类超150个任务的基准测试；提出LeVERB框架，高层视觉语言策略学习潜在动作词汇，低层强化学习全身控制策略生成动力学级命令。

Result: LeVERB在简单视觉导航任务零样本成功率达80%，总体成功率58.5%，比朴素分层全身视觉语言动作实现高7.8倍。

Conclusion: LeVERB框架在人形机器人全身控制的视觉语言任务中有较好的表现。

Abstract: Vision-language-action (VLA) models have demonstrated strong semantic
understanding and zero-shot generalization, yet most existing systems assume an
accurate low-level controller with hand-crafted action "vocabulary" such as
end-effector pose or root velocity. This assumption confines prior work to
quasi-static tasks and precludes the agile, whole-body behaviors required by
humanoid whole-body control (WBC) tasks. To capture this gap in the literature,
we start by introducing the first sim-to-real-ready, vision-language,
closed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10
categories. We then propose LeVERB: Latent Vision-Language-Encoded Robot
Behavior, a hierarchical latent instruction-following framework for humanoid
vision-language WBC, the first of its kind. At the top level, a vision-language
policy learns a latent action vocabulary from synthetically rendered kinematic
demonstrations; at the low level, a reinforcement-learned WBC policy consumes
these latent verbs to generate dynamics-level commands. In our benchmark,
LeVERB can zero-shot attain a 80% success rate on simple visual navigation
tasks, and 58.5% success rate overall, outperforming naive hierarchical
whole-body VLA implementation by 7.8 times.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [546] [NaSh: Guardrails for an LLM-Powered Natural Language Shell](https://arxiv.org/abs/2506.13028)
*Bimal Raj Gyawali,Saikrishna Achalla,Konstantinos Kallas,Sam Kumar*

Main category: cs.OS

TL;DR: 探索基于LLM的自然语言shell设计，设计NaSh并指出问题和研究方向


<details>
  <summary>Details</summary>
Motivation: LLM输出可能意外或无法解释，需要为自然语言shell提供保护机制让用户从错误中恢复

Method: 设计新shell NaSh

Result: 设计出NaSh，识别了该领域的开放性问题

Conclusion: 提出了研究方向以解决这些问题

Abstract: We explore how a shell that uses an LLM to accept natural language input
might be designed differently from the shells of today. As LLMs may produce
unintended or unexplainable outputs, we argue that a natural language shell
should provide guardrails that empower users to recover from such errors. We
concretize some ideas for doing so by designing a new shell called NaSh,
identify remaining open problems in this space, and discuss research directions
to address them.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [547] [NR Cell Identity-based Handover Decision-making Algorithm for High-speed Scenario within Dual Connectivity](https://arxiv.org/abs/2506.12461)
*Zhiyi Zhu,Eiji Takimoto,Patrick Finnertyn,Junjun Zheng,Shoma Suzuki,Chikara Ohta*

Main category: cs.NI

TL;DR: 论文针对5G异构网络中高速移动UE频繁不必要切换问题，提出基于NCI的切换决策算法，仿真显示其能增强连接稳定性。


<details>
  <summary>Details</summary>
Motivation: 5G异构网络密集部署使高速移动UE面临频繁不必要切换挑战，传统切换忽略目标gNB类型。

Method: 提出基于NCI的切换决策算法（HDMA），利用NCI中的gNB ID识别目标gNB类型以改进切换决策策略。

Result: 仿真结果表明，所提HDMA在增强连接稳定性方面优于其他HDMA。

Conclusion: 所提HDMA能让高速UE识别目标gNB类型，提高高速移动UE的通信稳定性。

Abstract: The dense deployment of 5G heterogeneous networks (HetNets) has improved
network capacity. However, it also brings frequent and unnecessary handover
challenges to high-speed mobile user equipment (UE), resulting in unstable
communication and degraded quality of service. Traditional handovers ignore the
type of target next-generation Node B (gNB), resulting in high-speed UEs being
able to be handed over to any gNB. This paper proposes a NR cell identity
(NCI)-based handover decision-making algorithm (HDMA) to address this issue.
The proposed HDMA identifies the type of the target gNB (macro/small/mmWave
gNB) using the gNB identity (ID) within the NCI to improve the handover
decision-making strategy. The proposed HDMA aims to improve the communication
stability of high-speed mobile UE by enabling high-speed UEs to identify the
target gNB type during the HDMA using the gNB ID. Simulation results show that
the proposed HDMA outperforms other HDMAs in enhanced connection stability.

</details>


### [548] [Latency Optimization for Wireless Federated Learning in Multihop Networks](https://arxiv.org/abs/2506.12081)
*Shaba Shaon,Van-Dinh Nguyen,Dinh C. Nguyen*

Main category: cs.NI

TL;DR: 研究无线多跳网络中无线联邦学习的延迟最小化问题，提出PAFL框架和优化问题，设计算法，仿真显示显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决无线多跳网络中无线联邦学习的延迟最小化问题，处理参与节点的数据异质性。

Method: 提出PAFL框架，联合优化叶节点、中继节点和中继路由指标，融入能量收集方案，基于块坐标下降和连续凸近似技术设计算法。

Result: 仿真表明，与仅优化一种节点类型、传统贪心算法和无中继路由指标的方案相比，无线多跳PAFL系统最多可减少69.37%的延迟。

Conclusion: 所提出的联合优化方法在降低无线多跳PAFL系统延迟方面有效。

Abstract: In this paper, we study a novel latency minimization problem in wireless
federated learning (FL) across multi-hop networks. The system comprises
multiple routes, each integrating leaf and relay nodes for FL model training.
We explore a personalized learning and adaptive aggregation-aware FL (PAFL)
framework that effectively addresses data heterogeneity across participating
nodes by harmonizing individual and collective learning objectives. We
formulate an optimization problem aimed at minimizing system latency through
the joint optimization of leaf and relay nodes, as well as relay routing
indicator. We also incorporate an additional energy harvesting scheme for the
relay nodes to help with their relay tasks. This formulation presents a
computationally demanding challenge, and thus we develop a simple yet efficient
algorithm based on block coordinate descent and successive convex approximation
(SCA) techniques. Simulation results illustrate the efficacy of our proposed
joint optimization approach for leaf and relay nodes with relay routing
indicator. We observe significant latency savings in the wireless multi-hop
PAFL system, with reductions of up to 69.37% compared to schemes optimizing
only one node type, traditional greedy algorithm, and scheme without relay
routing indicator.

</details>


### [549] [Learning Best Paths in Quantum Networks](https://arxiv.org/abs/2506.12462)
*Xuchuang Wang,Maoli Liu,Xutong Liu,Zhuohua Li,Mohammad Hajiesmaili,John C. S. Lui,Don Towsley*

Main category: cs.NI

TL;DR: 本文探讨在线学习场景下量子网络中最佳路径的学习，引入BeQuP - Link和BeQuP - Path两种算法，分析复杂度并通过模拟验证其能高效准确找到最佳路径。


<details>
  <summary>Details</summary>
Motivation: 量子网络中量子信息高效传输对关键应用至关重要，学习最佳路径可提升相关应用效果。

Method: 针对“链路级”和“路径级”两种反馈类型，分别引入BeQuP - Link和BeQuP - Path算法；BeQuP - Link动态基准测试关键链路，BeQuP - Path用子程序批量估计链路级参数。

Result: 分析算法的量子资源复杂度，表明能高效且大概率确定最佳路径；NetSquid模拟验证算法能准确高效找到最佳路径。

Conclusion: 提出的两种算法可在量子网络中高效准确地找到最佳路径。

Abstract: Quantum networks (QNs) transmit delicate quantum information across noisy
quantum channels. Crucial applications, like quantum key distribution (QKD) and
distributed quantum computation (DQC), rely on efficient quantum information
transmission. Learning the best path between a pair of end nodes in a QN is key
to enhancing such applications. This paper addresses learning the best path in
a QN in the online learning setting. We explore two types of feedback:
"link-level" and "path-level". Link-level feedback pertains to QNs with
advanced quantum switches that enable link-level benchmarking. Path-level
feedback, on the other hand, is associated with basic quantum switches that
permit only path-level benchmarking. We introduce two online learning
algorithms, BeQuP-Link and BeQuP-Path, to identify the best path using
link-level and path-level feedback, respectively. To learn the best path,
BeQuP-Link benchmarks the critical links dynamically, while BeQuP-Path relies
on a subroutine, transferring path-level observations to estimate link-level
parameters in a batch manner. We analyze the quantum resource complexity of
these algorithms and demonstrate that both can efficiently and, with high
probability, determine the best path. Finally, we perform NetSquid-based
simulations and validate that both algorithms accurately and efficiently
identify the best path.

</details>


### [550] [Dynamic Preference Multi-Objective Reinforcement Learning for Internet Network Management](https://arxiv.org/abs/2506.13153)
*DongNyeong Heo,Daniela Noemi Rim,Heeyoul Choi*

Main category: cs.NI

TL;DR: 提出基于强化学习的网络管理代理，能根据状态和偏好选择动作，还有数值估计方法，实验显示其泛化能力优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法使用单一静态奖励公式优化代理，而实际中网络管理偏好会变化，需新方法应对。

Method: 提出能基于状态和偏好选择动作的强化学习网络管理代理，以及可估计偏好分布的数值方法用于无偏训练。

Result: 基于该方法训练的强化学习代理在不同偏好下泛化能力显著优于先前假设静态偏好的方法，且数值估计方法有优势。

Conclusion: 所提方法在网络管理中面对变化偏好时比传统方法更有效，具有更好的泛化能力。

Abstract: An internet network service provider manages its network with multiple
objectives, such as high quality of service (QoS) and minimum computing
resource usage. To achieve these objectives, a reinforcement learning-based
(RL) algorithm has been proposed to train its network management agent.
Usually, their algorithms optimize their agents with respect to a single static
reward formulation consisting of multiple objectives with fixed importance
factors, which we call preferences. However, in practice, the preference could
vary according to network status, external concerns and so on. For example,
when a server shuts down and it can cause other servers' traffic overloads
leading to additional shutdowns, it is plausible to reduce the preference of
QoS while increasing the preference of minimum computing resource usages. In
this paper, we propose new RL-based network management agents that can select
actions based on both states and preferences. With our proposed approach, we
expect a single agent to generalize on various states and preferences.
Furthermore, we propose a numerical method that can estimate the distribution
of preference that is advantageous for unbiased training. Our experiment
results show that the RL agents trained based on our proposed approach
significantly generalize better with various preferences than the previous RL
approaches, which assume static preference during training. Moreover, we
demonstrate several analyses that show the advantages of our numerical
estimation method.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [551] [Generalizing while preserving monotonicity in comparison-based preference learning models](https://arxiv.org/abs/2506.08616)
*Julien Fageot,Peva Blanchard,Gilles Bareilles,Lê-Nguyên Hoang*

Main category: math.ST

TL;DR: 本文提出带扩散先验的线性广义Bradley - Terry模型，确定保证单调性的充分条件，实验表明新模型提升准确性。


<details>
  <summary>Details</summary>
Motivation: 许多基于比较的偏好学习模型无法保证单调性，现有被证明单调的广义Bradley - Terry模型不能泛化到未比较的数据，因此需研究有泛化能力的单调模型。

Method: 提出带扩散先验的线性广义Bradley - Terry模型，确定替代方案嵌入上保证单调性的充分条件。

Result: 单调性并非普遍保证，新的泛化模型提高了准确性，在数据集有限时尤其明显。

Conclusion: 新提出的带扩散先验的线性广义Bradley - Terry模型有助于提升基于比较的偏好学习模型的性能。

Abstract: If you tell a learning model that you prefer an alternative $a$ over another
alternative $b$, then you probably expect the model to be monotone, that is,
the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps
surprisingly, many widely deployed comparison-based preference learning models,
including large language models, fail to have this guarantee. Until now, the
only comparison-based preference learning algorithms that were proved to be
monotone are the Generalized Bradley-Terry models. Yet, these models are unable
to generalize to uncompared data. In this paper, we advance the understanding
of the set of models with generalization ability that are monotone. Namely, we
propose a new class of Linear Generalized Bradley-Terry models with Diffusion
Priors, and identify sufficient conditions on alternatives' embeddings that
guarantee monotonicity. Our experiments show that this monotonicity is far from
being a general guarantee, and that our new class of generalizing models
improves accuracy, especially when the dataset is limited.

</details>


### [552] [On Monotonicity in AI Alignment](https://arxiv.org/abs/2506.08998)
*Gilles Bareilles,Julien Fageot,Lê-Nguyên Hoang,Peva Blanchard,Wassim Bouaziz,Sébastien Rouault,El-Mahdi El-Mhamdi*

Main category: math.ST

TL;DR: 本文研究基于比较的偏好学习中（非）单调性的根源，证明方法满足局部成对单调性，给出单调性形式化和充分条件，为评估和改进算法提供指导。


<details>
  <summary>Details</summary>
Motivation: 基于比较的偏好学习方法可能有反直觉表现，如考虑某偏好时模型会降低生成对应内容的概率，需探究（非）单调性根源。

Method: 在温和假设下，对包含DPO、GPO和GBT的通用框架进行理论分析。

Result: 证明方法满足局部成对单调性，给出单调性形式化及充分条件。

Conclusion: 明确当前方法局限性，为开发更可靠的偏好学习算法提供指导。

Abstract: Comparison-based preference learning has become central to the alignment of
AI models with human preferences. However, these methods may behave
counterintuitively. After empirically observing that, when accounting for a
preference for response $y$ over $z$, the model may actually decrease the
probability (and reward) of generating $y$ (an observation also made by
others), this paper investigates the root causes of (non) monotonicity, for a
general comparison-based preference learning framework that subsumes Direct
Preference Optimization (DPO), Generalized Preference Optimization (GPO) and
Generalized Bradley-Terry (GBT). Under mild assumptions, we prove that such
methods still satisfy what we call local pairwise monotonicity. We also provide
a bouquet of formalizations of monotonicity, and identify sufficient conditions
for their guarantee, thereby providing a toolbox to evaluate how prone learning
models are to monotonicity violations. These results clarify the limitations of
current methods and provide guidance for developing more trustworthy preference
learning algorithms.

</details>


### [553] [Beyond Sin-Squared Error: Linear-Time Entrywise Uncertainty Quantification for Streaming PCA](https://arxiv.org/abs/2506.12655)
*Syamantak Kumar,Shourya Pandey,Purnamrita Sarkar*

Main category: math.ST

TL;DR: 提出基于Oja算法的流式主成分分析统计推断框架，可构建估计特征向量元素的置信区间，推导集中界和中心极限定理，引入子采样算法，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有流式PCA研究多关注sin - squared误差保证，对估计特征向量元素的不确定性量化和误差保证研究不足。

Method: 推导Bernstein型集中界和中心极限定理，引入基于中位数均值法的子采样算法估计坐标方差。

Result: 子采样算法在提供可靠不确定性估计时，计算成本远低于现有方法。

Conclusion: 所提统计推断框架和子采样算法在流式PCA的特征向量元素不确定性量化上有效且高效。

Abstract: We propose a novel statistical inference framework for streaming principal
component analysis (PCA) using Oja's algorithm, enabling the construction of
confidence intervals for individual entries of the estimated eigenvector. Most
existing works on streaming PCA focus on providing sharp sin-squared error
guarantees. Recently, there has been some interest in uncertainty
quantification for the sin-squared error. However, uncertainty quantification
or sharp error guarantees for entries of the estimated eigenvector in the
streaming setting remains largely unexplored. We derive a sharp Bernstein-type
concentration bound for elements of the estimated vector matching the optimal
error rate up to logarithmic factors. We also establish a Central Limit Theorem
for a suitably centered and scaled subset of the entries. To efficiently
estimate the coordinate-wise variance, we introduce a provably consistent
subsampling algorithm that leverages the median-of-means approach, empirically
achieving similar accuracy to multiplier bootstrap methods while being
significantly more computationally efficient. Numerical experiments demonstrate
its effectiveness in providing reliable uncertainty estimates with a fraction
of the computational cost of existing methods.

</details>


### [554] [On the attainment of the Wasserstein--Cramer--Rao lower bound](https://arxiv.org/abs/2506.12732)
*Hayato Nishimori,Takeru Matsuda*

Main category: math.ST

TL;DR: 研究了估计器达到Wasserstein - Cramer - Rao下界（渐近）的条件，展示了单参数统计模型存在Wasserstein有效估计器的条件，证明了Wasserstein估计器在位置 - 尺度族中渐近有效。


<details>
  <summary>Details</summary>
Motivation: 研究估计器达到Wasserstein - Cramer - Rao下界（渐近）的条件，即（渐近）Wasserstein效率。

Method: 无明确提及具体方法。

Result: 展示了单参数统计模型存在Wasserstein有效估计器的条件，该条件对应于最近提出的单参数指数族的Wasserstein类似物；证明了基于Wasserstein得分函数的Wasserstein估计器在位置 - 尺度族中渐近Wasserstein有效。

Conclusion: 给出了单参数统计模型存在Wasserstein有效估计器的条件，且特定估计器在位置 - 尺度族中有渐近有效性。

Abstract: Recently, a Wasserstein analogue of the Cramer--Rao inequality has been
developed using the Wasserstein information matrix (Otto metric). This
inequality provides a lower bound on the Wasserstein variance of an estimator,
which quantifies its robustness against additive noise. In this study, we
investigate conditions for an estimator to attain the Wasserstein--Cramer--Rao
lower bound (asymptotically), which we call the (asymptotic) Wasserstein
efficiency. We show a condition under which Wasserstein efficient estimators
exist for one-parameter statistical models. This condition corresponds to a
recently proposed Wasserstein analogue of one-parameter exponential families
(e-geodesics). We also show that the Wasserstein estimator, a Wasserstein
analogue of the maximum likelihood estimator based on the Wasserstein score
function, is asymptotically Wasserstein efficient in location-scale families.

</details>


### [555] [Computational lower bounds in latent models: clustering, sparse-clustering, biclustering](https://arxiv.org/abs/2506.13647)
*Bertrand Even,Christophe Giraud,Nicolas Verzelen*

Main category: math.ST

TL;DR: 论文提出新方案推导低阶多项式性能下界，用于聚类等问题，获新结果并证明上下界以描述统计 - 计算差距。


<details>
  <summary>Details</summary>
Motivation: 许多高维问题中多项式时间复杂度算法统计性能不佳，存在统计 - 计算差距，需评估多项式时间内最佳性能。

Method: 基于Schramm和Wein（2022）论文，提出新方案推导低阶多项式在潜在空间模型中性能下界，更好利用潜在结构。

Result: 获得新且更精确结果，证明匹配上界及额外统计结果。

Conclusion: 全面描述聚类、稀疏聚类和双聚类问题中的统计 - 计算差距。

Abstract: In many high-dimensional problems, like sparse-PCA, planted clique, or
clustering, the best known algorithms with polynomial time complexity fail to
reach the statistical performance provably achievable by algorithms free of
computational constraints. This observation has given rise to the conjecture of
the existence, for some problems, of gaps -- so called
statistical-computational gaps -- between the best possible statistical
performance achievable without computational constraints, and the best
performance achievable with poly-time algorithms. A powerful approach to assess
the best performance achievable in poly-time is to investigate the best
performance achievable by polynomials with low-degree. We build on the seminal
paper of Schramm and Wein (2022) and propose a new scheme to derive lower
bounds on the performance of low-degree polynomials in some latent space
models. By better leveraging the latent structures, we obtain new and sharper
results, with simplified proofs. We then instantiate our scheme to provide
computational lower bounds for the problems of clustering, sparse clustering,
and biclustering. We also prove matching upper-bounds and some additional
statistical results, in order to provide a comprehensive description of the
statistical-computational gaps occurring in these three problems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [556] [Versatile and Fast Location-Based Private Information Retrieval with Fully Homomorphic Encryption over the Torus](https://arxiv.org/abs/2506.12761)
*Joon Soo Yoo,Taeho Kim,Ji Won Yoon*

Main category: cs.CR

TL;DR: 提出VeLoPIR系统保护位置服务用户隐私，有三种模式，经优化可跨平台扩展，有安全证明，实验显示比基线快11.55倍，代码开源。


<details>
  <summary>Details</summary>
Motivation: 位置服务中用户共享敏感位置数据存在隐私风险，需保护用户隐私并实现高效可扩展查询处理。

Method: 引入三种操作模式，结合多级算法优化与并行结构，提供形式化安全和隐私证明。

Result: 在真实数据集实验中，VeLoPIR比先前基线快达11.55倍。

Conclusion: VeLoPIR能有效保护用户隐私，实现高效可扩展查询处理，且在性能上有显著提升。

Abstract: Location-based services often require users to share sensitive locational
data, raising privacy concerns due to potential misuse or exploitation by
untrusted servers. In response, we present VeLoPIR, a versatile location-based
private information retrieval (PIR) system designed to preserve user privacy
while enabling efficient and scalable query processing. VeLoPIR introduces
three operational modes-interval validation, coordinate validation, and
identifier matching-that support a broad range of real-world applications,
including information and emergency alerts. To enhance performance, VeLoPIR
incorporates multi-level algorithmic optimizations with parallel structures,
achieving significant scalability across both CPU and GPU platforms. We also
provide formal security and privacy proofs, confirming the system's robustness
under standard cryptographic assumptions. Extensive experiments on real-world
datasets demonstrate that VeLoPIR achieves up to 11.55 times speed-up over a
prior baseline. The implementation of VeLoPIR is publicly available at
https://github.com/PrivStatBool/VeLoPIR.

</details>


### [557] [On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains](https://arxiv.org/abs/2506.13246)
*Craig Steven Wright*

Main category: cs.CR

TL;DR: 提出用于合成智能体的形式化架构，用Merkle自动机等技术实现不可变记忆、可验证推理和受限认知增长，适用于对证明性要求高的系统。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统依赖可变、不透明统计模型，易出现认知漂移和历史修正问题，需要新架构解决。

Method: 引入Merkle自动机，结合形式自动机理论与区块链承诺；用ECDH交换推导对称加密密钥，控制知识图谱访问；用形式逻辑系统约束推理，通过零知识证明实现可验证隐私保护。

Result: 构建的架构使智能体输出可证明推导、时间锚定且不可事后修改。

Conclusion: 该架构为法律、经济和高保证计算系统奠定基础，满足对可证明记忆、不可伪造来源和结构真实性的需求。

Abstract: This paper presents a formalised architecture for synthetic agents designed
to retain immutable memory, verifiable reasoning, and constrained epistemic
growth. Traditional AI systems rely on mutable, opaque statistical models prone
to epistemic drift and historical revisionism. In contrast, we introduce the
concept of the Merkle Automaton, a cryptographically anchored, deterministic
computational framework that integrates formal automata theory with
blockchain-based commitments. Each agent transition, memory fragment, and
reasoning step is committed within a Merkle structure rooted on-chain,
rendering it non-repudiable and auditably permanent. To ensure selective access
and confidentiality, we derive symmetric encryption keys from ECDH exchanges
contextualised by hierarchical privilege lattices. This enforces cryptographic
access control over append-only DAG-structured knowledge graphs. Reasoning is
constrained by formal logic systems and verified through deterministic
traversal of policy-encoded structures. Updates are non-destructive and
historied, preserving epistemic lineage without catastrophic forgetting.
Zero-knowledge proofs facilitate verifiable, privacy-preserving inclusion
attestations. Collectively, this architecture reframes memory not as a cache
but as a ledger - one whose contents are enforced by protocol, bound by
cryptography, and constrained by formal logic. The result is not an intelligent
agent that mimics thought, but an epistemic entity whose outputs are provably
derived, temporally anchored, and impervious to post hoc revision. This design
lays foundational groundwork for legal, economic, and high-assurance
computational systems that require provable memory, unforgeable provenance, and
structural truth.

</details>


### [558] [EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated Learning](https://arxiv.org/abs/2506.13612)
*Zhiqiang Li,Haiyong Bao,Menghong Guan,Hao Pan,Cheng Huang,Hong-Ning Dai*

Main category: cs.CR

TL;DR: 提出EBS - CFL方案解决CFL隐私及训练问题，有高效性且通过实验验证、理论证明安全。


<details>
  <summary>Details</summary>
Motivation: FL因数据异质性性能下降，CFL虽能解决但面临用户隐私问题，不愿共享集群身份影响训练。

Method: 提出EBS - CFL方案，有效训练CFL并保护用户集群身份隐私，丢弃负相关梯度、加权聚合正相关梯度检测攻击，服务器认证客户端梯度编码。

Result: EBS - CFL有高通信和计算效率，客户端计算效率比对比方案至少高O(log n)倍。

Conclusion: 通过实验验证方案，理论证明方案安全。

Abstract: Despite federated learning (FL)'s potential in collaborative learning, its
performance has deteriorated due to the data heterogeneity of distributed
users. Recently, clustered federated learning (CFL) has emerged to address this
challenge by partitioning users into clusters according to their similarity.
However, CFL faces difficulties in training when users are unwilling to share
their cluster identities due to privacy concerns. To address these issues, we
present an innovative Efficient and Robust Secure Aggregation scheme for CFL,
dubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while
maintaining users' cluster identity confidentially. Moreover, it detects
potential poisonous attacks without compromising individual client gradients by
discarding negatively correlated gradients and aggregating positively
correlated ones using a weighted approach. The server also authenticates
correct gradient encoding by clients. EBS-CFL has high efficiency with
client-side overhead O(ml + m^2) for communication and O(m^2l) for computation,
where m is the number of cluster identities, and l is the gradient size. When m
= 1, EBS-CFL's computational efficiency of client is at least O(log n) times
better than comparison schemes, where n is the number of clients.In addition,
we validate the scheme through extensive experiments. Finally, we theoretically
prove the scheme's security.

</details>


### [559] [Using LLMs for Security Advisory Investigations: How Far Are We?](https://arxiv.org/abs/2506.13161)
*Bayu Fedra Abdullah,Yusuf Sulistyo Nugroho,Brittany Reid,Raula Gaikovina Kula,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.CR

TL;DR: 研究调查ChatGPT在生成安全公告、区分真假CVE - ID和提取CVE - ID的能力，发现其有优势也有局限，强调在网络安全工作流中谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在生成准确漏洞公告方面的可信度，具体研究ChatGPT在网络安全应用中的能力。

Method: 使用包含100个真实和100个虚假CVE - ID的数据集，手动分析模型输出的可信度和一致性。

Result: ChatGPT为96%的真实CVE - ID和97%的虚假CVE - ID生成了可信的安全公告；在重新识别时，6%的真实公告产生了虚假CVE - ID。

Conclusion: ChatGPT在网络安全应用中有优势也有局限，需谨慎使用，且其设计需进一步改进以提高可靠性和适用性。

Abstract: Large Language Models (LLMs) are increasingly used in software security, but
their trustworthiness in generating accurate vulnerability advisories remains
uncertain. This study investigates the ability of ChatGPT to (1) generate
plausible security advisories from CVE-IDs, (2) differentiate real from fake
CVE-IDs, and (3) extract CVE-IDs from advisory descriptions. Using a curated
dataset of 100 real and 100 fake CVE-IDs, we manually analyzed the credibility
and consistency of the model's outputs. The results show that ChatGPT generated
plausible security advisories for 96% of given input real CVE-IDs and 97% of
given input fake CVE-IDs, demonstrating a limitation in differentiating between
real and fake IDs. Furthermore, when these generated advisories were
reintroduced to ChatGPT to identify their original CVE-ID, the model produced a
fake CVE-ID in 6% of cases from real advisories. These findings highlight both
the strengths and limitations of ChatGPT in cybersecurity applications. While
the model demonstrates potential for automating advisory generation, its
inability to reliably authenticate CVE-IDs or maintain consistency upon
re-evaluation underscores the risks associated with its deployment in critical
security tasks. Our study emphasizes the importance of using LLMs with caution
in cybersecurity workflows and suggests the need for further improvements in
their design to improve reliability and applicability in security advisory
generation.

</details>


### [560] [Tady: A Neural Disassembler without Structural Constraint Violations](https://arxiv.org/abs/2506.13323)
*Siliang Qin,Fengrui Yang,Hao Wang,Bolun Zhang,Zeyu Gao,Chao Zhang,Kai Chen*

Main category: cs.CR

TL;DR: 现有神经反汇编器输出常违反结构约束，本文提出Tady解决该问题，评估显示其有效且高效。


<details>
  <summary>Details</summary>
Motivation: 新兴神经反汇编器输出常违反基本结构约束，严重影响实用性，需解决该问题。

Method: 基于后支配关系形式化并应用关键结构约束来规范反汇编解空间，引入Tady，含改进模型架构和后处理算法。

Result: 在多种二进制文件上的综合评估表明，Tady能有效消除结构约束违规，高效运行并保持指令级准确性。

Conclusion: Tady能有效解决现有神经反汇编器输出违反结构约束的问题，具有实用性。

Abstract: Disassembly is a crucial yet challenging step in binary analysis. While
emerging neural disassemblers show promise for efficiency and accuracy, they
frequently generate outputs violating fundamental structural constraints, which
significantly compromise their practical usability. To address this critical
problem, we regularize the disassembly solution space by formalizing and
applying key structural constraints based on post-dominance relations. This
approach systematically detects widespread errors in existing neural
disassemblers' outputs. These errors often originate from models' limited
context modeling and instruction-level decoding that neglect global structural
integrity. We introduce Tady, a novel neural disassembler featuring an improved
model architecture and a dedicated post-processing algorithm, specifically
engineered to address these deficiencies. Comprehensive evaluations on diverse
binaries demonstrate that Tady effectively eliminates structural constraint
violations and functions with high efficiency, while maintaining
instruction-level accuracy.

</details>


### [561] [Organizational Adaptation to Generative AI in Cybersecurity: A Systematic Review](https://arxiv.org/abs/2506.12060)
*Christopher Nott*

Main category: cs.CR

TL;DR: 本定性研究探讨网络安全组织如何适应生成式AI集成，发现组织威胁建模方法转变及三种适应模式，指出成功集成条件和面临困难。


<details>
  <summary>Details</summary>
Motivation: 研究网络安全组织如何调整威胁建模框架和运营流程以应对生成式人工智能集成。

Method: 采用系统文档分析和比较案例研究方法，分析2022 - 2025年的25项研究。

Result: 组织威胁建模从传统基于签名系统转向融入人工智能能力的框架，有三种适应模式，成熟安全基础设施组织准备更充分。

Conclusion: 组织成功集成需适当人工监督、解决数据质量等问题并建立特定治理框架，同时面临隐私保护等困难，研究为专业人员提供见解。

Abstract: Cybersecurity organizations are adapting to GenAI integration through
modified frameworks and hybrid operational processes, with success influenced
by existing security maturity, regulatory requirements, and investments in
human capital and infrastructure. This qualitative research employs systematic
document analysis and comparative case study methodology to examine how
cybersecurity organizations adapt their threat modeling frameworks and
operational processes to address generative artificial intelligence
integration. Through examination of 25 studies from 2022 to 2025, the research
documents substantial transformation in organizational approaches to threat
modeling, moving from traditional signature-based systems toward frameworks
incorporating artificial intelligence capabilities. The research identifies
three primary adaptation patterns: Large Language Model integration for
security applications, GenAI frameworks for risk detection and response
automation, and AI/ML integration for threat hunting. Organizations with mature
security infrastructures, particularly in finance and critical infrastructure
sectors, demonstrate higher readiness through structured governance approaches,
dedicated AI teams, and robust incident response processes. Organizations
achieve successful GenAI integration when they maintain appropriate human
oversight of automated systems, address data quality concerns and
explainability requirements, and establish governance frameworks tailored to
their specific sectors. Organizations encounter ongoing difficulties with
privacy protection, bias reduction, personnel training, and defending against
adversarial attacks. This work advances understanding of how organizations
adopt innovative technologies in high-stakes environments and offers actionable
insights for cybersecurity professionals implementing GenAI systems.

</details>


### [562] [LLM Embedding-based Attribution (LEA): Quantifying Source Contributions to Generative Model's Response for Vulnerability Analysis](https://arxiv.org/abs/2506.12100)
*Reza Fayyazi,Michael Zuzak,Shanchieh Jay Yang*

Main category: cs.CR

TL;DR: 安全漏洞日益复杂，大语言模型用于安全威胁分析遇新漏洞挑战，本文提出LEA指标评估生成响应中预训练知识与检索内容影响，验证其对漏洞分析有效性，为安全分析审计提供方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于安全威胁分析面对新漏洞时存在问题，且在安全敏感环境部署存在信任和安全挑战，需量化生成响应中预训练知识与检索内容的影响。

Method: 提出基于大语言模型嵌入的归因（LEA）这一可解释性指标。

Result: 将LEA应用于评估过去十年100个关键CVEs的响应，验证其对漏洞分析洞察性量化的有效性。

Conclusion: 为安全分析师提供审计大语言模型辅助工作流的方法，为在网络安全操作中透明、高保证地部署RAG增强的大语言模型奠定基础。

Abstract: Security vulnerabilities are rapidly increasing in frequency and complexity,
creating a shifting threat landscape that challenges cybersecurity defenses.
Large Language Models (LLMs) have been widely adopted for cybersecurity threat
analysis. When querying LLMs, dealing with new, unseen vulnerabilities is
particularly challenging as it lies outside LLMs' pre-trained distribution.
Retrieval-Augmented Generation (RAG) pipelines mitigate the problem by
injecting up-to-date authoritative sources into the model context, thus
reducing hallucinations and increasing the accuracy in responses. Meanwhile,
the deployment of LLMs in security-sensitive environments introduces challenges
around trust and safety. This raises a critical open question: How to quantify
or attribute the generated response to the retrieved context versus the model's
pre-trained knowledge? This work proposes LLM Embedding-based Attribution (LEA)
-- a novel, explainable metric to paint a clear picture on the 'percentage of
influence' the pre-trained knowledge vs. retrieved content has for each
generated response. We apply LEA to assess responses to 100 critical CVEs from
the past decade, verifying its effectiveness to quantify the insightfulness for
vulnerability analysis. Our development of LEA reveals a progression of
independency in hidden states of LLMs: heavy reliance on context in early
layers, which enables the derivation of LEA; increased independency in later
layers, which sheds light on why scale is essential for LLM's effectiveness.
This work provides security analysts a means to audit LLM-assisted workflows,
laying the groundwork for transparent, high-assurance deployments of
RAG-enhanced LLMs in cybersecurity operations.

</details>


### [563] [DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents](https://arxiv.org/abs/2506.12104)
*Hao Li,Xiaogeng Liu,Hung-Chun Chiu,Dianqi Li,Ning Zhang,Chaowei Xiao*

Main category: cs.CR

TL;DR: 论文提出DRIFT框架解决大语言模型驱动的智能体系统的安全问题，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的智能体系统易受提示注入攻击，现有系统级防御存在动态更新安全规则和内存流隔离的挑战。

Method: 提出DRIFT框架，包括安全规划器、动态验证器和注入隔离器，分别完成构建轨迹和参数检查、监控偏差、检测和屏蔽冲突指令。

Result: 在AgentDojo基准上验证了DRIFT的有效性，具有强安全性能且保持高实用性。

Conclusion: DRIFT框架具有较强的鲁棒性和适应性，能有效解决智能体系统的安全问题。

Abstract: Large Language Models (LLMs) are increasingly central to agentic systems due
to their strong reasoning and planning capabilities. By interacting with
external environments through predefined tools, these agents can carry out
complex user tasks. Nonetheless, this interaction also introduces the risk of
prompt injection attacks, where malicious inputs from external sources can
mislead the agent's behavior, potentially resulting in economic loss, privacy
leakage, or system compromise. System-level defenses have recently shown
promise by enforcing static or predefined policies, but they still face two key
challenges: the ability to dynamically update security rules and the need for
memory stream isolation. To address these challenges, we propose DRIFT, a
Dynamic Rule-based Isolation Framework for Trustworthy agentic systems, which
enforces both control- and data-level constraints. A Secure Planner first
constructs a minimal function trajectory and a JSON-schema-style parameter
checklist for each function node based on the user query. A Dynamic Validator
then monitors deviations from the original plan, assessing whether changes
comply with privilege limitations and the user's intent. Finally, an Injection
Isolator detects and masks any instructions that may conflict with the user
query from the memory stream to mitigate long-term risks. We empirically
validate the effectiveness of DRIFT on the AgentDojo benchmark, demonstrating
its strong security performance while maintaining high utility across diverse
models -- showcasing both its robustness and adaptability.

</details>


### [564] [A Lightweight IDS for Early APT Detection Using a Novel Feature Selection Method](https://arxiv.org/abs/2506.12108)
*Bassam Noori Shaker,Bahaa Al-Musawi,Mohammed Falih Hassan*

Main category: cs.CR

TL;DR: 提出利用XGBoost和XAI的特征选择方法开发轻量级入侵检测系统识别APT，可将特征从77个减至4个且评估指标良好。


<details>
  <summary>Details</summary>
Motivation: APT威胁隐蔽且长期难检测，需早期检测以减轻潜在后果。

Method: 利用XGBoost算法和XAI，用SHAP方法识别初始妥协阶段最相关特征。

Result: 将SCVIC - APT - 2021数据集所选特征从77个减至4个，评估指标为97%精度、100%召回率和98% F1分数。

Conclusion: 该方法有助于防止APT成功造成后果，增强对早期APT行为的理解。

Abstract: An Advanced Persistent Threat (APT) is a multistage, highly sophisticated,
and covert form of cyber threat that gains unauthorized access to networks to
either steal valuable data or disrupt the targeted network. These threats often
remain undetected for extended periods, emphasizing the critical need for early
detection in networks to mitigate potential APT consequences. In this work, we
propose a feature selection method for developing a lightweight intrusion
detection system capable of effectively identifying APTs at the initial
compromise stage. Our approach leverages the XGBoost algorithm and Explainable
Artificial Intelligence (XAI), specifically utilizing the SHAP (SHapley
Additive exPlanations) method for identifying the most relevant features of the
initial compromise stage. The results of our proposed method showed the ability
to reduce the selected features of the SCVIC-APT-2021 dataset from 77 to just
four while maintaining consistent evaluation metrics for the suggested system.
The estimated metrics values are 97% precision, 100% recall, and a 98% F1
score. The proposed method not only aids in preventing successful APT
consequences but also enhances understanding of APT behavior at early stages.

</details>


### [565] [Semantic Preprocessing for LLM-based Malware Analysis](https://arxiv.org/abs/2506.12113)
*Benjamin Marais,Tony Quertier,Grégoire Barrue*

Main category: cs.CR

TL;DR: 本文提出专注专家知识的预处理方法用于恶意软件语义分析，结合多方面知识生成JSON报告，用其训练大语言模型分类恶意软件获0.94的加权平均F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有基于人工智能的恶意软件分析技术聚焦数据视角，缺乏专家视角，影响语义分析和结果可解释性。

Method: 提出新预处理方法，为可移植可执行文件创建JSON报告，整合静态和行为分析特征及相关知识。

Result: 用该预处理方法训练大语言模型对恶意软件分类，在复杂数据集上加权平均F1分数达0.94。

Conclusion: 该预处理方法能提升恶意软件语义分析和结果可解释性，增强AI模型对恶意文件分析的可解释性。

Abstract: In a context of malware analysis, numerous approaches rely on Artificial
Intelligence to handle a large volume of data. However, these techniques focus
on data view (images, sequences) and not on an expert's view. Noticing this
issue, we propose a preprocessing that focuses on expert knowledge to improve
malware semantic analysis and result interpretability. We propose a new
preprocessing method which creates JSON reports for Portable Executable files.
These reports gather features from both static and behavioral analysis, and
incorporate packer signature detection, MITRE ATT\&CK and Malware Behavior
Catalog (MBC) knowledge. The purpose of this preprocessing is to gather a
semantic representation of binary files, understandable by malware analysts,
and that can enhance AI models' explainability for malicious files analysis.
Using this preprocessing to train a Large Language Model for Malware
classification, we achieve a weighted-average F1-score of 0.94 on a complex
dataset, representative of market reality.

</details>


### [566] [QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety](https://arxiv.org/abs/2506.12299)
*Taegyeong Lee,Jeonghwa Yoo,Hyoungseo Cho,Soo Yong Kim,Yunho Maeng*

Main category: cs.CR

TL;DR: 本文提出QGuard方法，用问题提示零样本阻挡有害提示，对文本和多模态攻击有效，实验表现好，能进行白盒分析。


<details>
  <summary>Details</summary>
Motivation: 大语言模型进步使恶意用户利用有害和越狱提示攻击风险增加，保护模型免受此类攻击是重要且具挑战性的任务。

Method: 提出QGuard方法，利用问题提示以零样本方式阻挡有害提示，通过多样化和修改防护问题增强鲁棒性。

Result: 模型在文本和多模态有害数据集上表现有竞争力，可对用户输入进行白盒分析。

Conclusion: 该方法为现实世界大语言模型服务减轻有害提示安全风险提供了有价值的见解。

Abstract: The recent advancements in Large Language Models(LLMs) have had a significant
impact on a wide range of fields, from general domains to specialized areas.
However, these advancements have also significantly increased the potential for
malicious users to exploit harmful and jailbreak prompts for malicious attacks.
Although there have been many efforts to prevent harmful prompts and jailbreak
prompts, protecting LLMs from such malicious attacks remains an important and
challenging task. In this paper, we propose QGuard, a simple yet effective
safety guard method, that utilizes question prompting to block harmful prompts
in a zero-shot manner. Our method can defend LLMs not only from text-based
harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by
diversifying and modifying guard questions, our approach remains robust against
the latest harmful prompts without fine-tuning. Experimental results show that
our model performs competitively on both text-only and multi-modal harmful
datasets. Additionally, by providing an analysis of question prompting, we
enable a white-box analysis of user inputs. We believe our method provides
valuable insights for real-world LLM services in mitigating security risks
associated with harmful prompts.

</details>


### [567] [MEraser: An Effective Fingerprint Erasure Approach for Large Language Models](https://arxiv.org/abs/2506.12551)
*Jingxuan Zhang,Zhenhua Xu,Rui Hu,Wenpeng Xing,Xuhong Zhang,Meng Han*

Main category: cs.CR

TL;DR: 提出MEraser方法去除大语言模型后门指纹，用少量数据达成去除效果并保持性能，还有可迁移擦除机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用引发所有权和知识产权保护问题，后门指纹认证有有效攻击方法待探索。

Method: 采用两阶段微调策略，利用精心构建的不匹配和干净数据集。

Result: 用少于1000个样本的少量训练数据，MEraser能完全去除指纹并保持模型性能，还有可迁移擦除机制。

Conclusion: 该方法为大语言模型指纹去除提供实用方案，揭示现有指纹技术漏洞，为未来开发更有韧性的模型保护方法建立评估基准。

Abstract: Large Language Models (LLMs) have become increasingly prevalent across
various sectors, raising critical concerns about model ownership and
intellectual property protection. Although backdoor-based fingerprinting has
emerged as a promising solution for model authentication, effective attacks for
removing these fingerprints remain largely unexplored. Therefore, we present
Mismatched Eraser (MEraser), a novel method for effectively removing
backdoor-based fingerprints from LLMs while maintaining model performance. Our
approach leverages a two-phase fine-tuning strategy utilizing carefully
constructed mismatched and clean datasets. Through extensive evaluation across
multiple LLM architectures and fingerprinting methods, we demonstrate that
MEraser achieves complete fingerprinting removal while maintaining model
performance with minimal training data of fewer than 1,000 samples.
Furthermore, we introduce a transferable erasure mechanism that enables
effective fingerprinting removal across different models without repeated
training. In conclusion, our approach provides a practical solution for
fingerprinting removal in LLMs, reveals critical vulnerabilities in current
fingerprinting techniques, and establishes comprehensive evaluation benchmarks
for developing more resilient model protection methods in the future.

</details>


### [568] [Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity](https://arxiv.org/abs/2506.12685)
*Bilal Saleh Husain*

Main category: cs.CR

TL;DR: 研究FlipAttack机制，提出新攻击方法AIM，实验显示AIM及变种表现优，强调语义差异与解码简单性平衡重要性。


<details>
  <summary>Details</summary>
Motivation: LLMs易受攻击有安全伦理问题，现存越狱方法有缺陷，要研究FlipAttack有效性机制并找新攻击方法。

Method: 分析FlipAttack翻转模式语义变化，用UMAP、KDE和余弦相似度测试假设，提出AIM攻击方法。

Result: 在GPT - 4实验中，AIM和变种AIM+FWO攻击成功率达94%，优于FlipAttack等方法。

Conclusion: 高语义差异重要，解码简单性平衡是成功越狱关键，加深对对抗性提示机制理解并提供新方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their susceptibility to adversarial attacks, particularly jailbreaking, poses
significant safety and ethical concerns. While numerous jailbreak methods
exist, many suffer from computational expense, high token usage, or complex
decoding schemes. Liu et al. (2024) introduced FlipAttack, a black-box method
that achieves high attack success rates (ASR) through simple prompt
manipulation. This paper investigates the underlying mechanisms of FlipAttack's
effectiveness by analyzing the semantic changes induced by its flipping modes.
We hypothesize that semantic dissimilarity between original and manipulated
prompts is inversely correlated with ASR. To test this, we examine embedding
space visualizations (UMAP, KDE) and cosine similarities for FlipAttack's
modes. Furthermore, we introduce a novel adversarial attack, Alphabet Index
Mapping (AIM), designed to maximize semantic dissimilarity while maintaining
simple decodability. Experiments on GPT-4 using a subset of AdvBench show AIM
and its variant AIM+FWO achieve a 94% ASR, outperforming FlipAttack and other
methods on this subset. Our findings suggest that while high semantic
dissimilarity is crucial, a balance with decoding simplicity is key for
successful jailbreaking. This work contributes to a deeper understanding of
adversarial prompt mechanics and offers a new, effective jailbreak technique.

</details>


### [569] [Privacy-Preserving Federated Learning against Malicious Clients Based on Verifiable Functional Encryption](https://arxiv.org/abs/2506.12846)
*Nina Cai,Jinguang Han*

Main category: cs.CR

TL;DR: 文章提出基于可验证功能加密的隐私保护联邦学习框架VFEFL，实现隐私保护、鲁棒性等，且无需非勾结双服务器或可信第三方。


<details>
  <summary>Details</summary>
Motivation: 联邦学习存在新威胁和挑战，如模型反转攻击、易受恶意客户端攻击，需保护数据隐私和防范恶意攻击。

Method: 提出去中心化可验证功能加密（DVFE）方案，设计隐私保护联邦学习框架VFEFL并结合新型鲁棒聚合规则检测恶意客户端。

Result: 方案达到隐私保护、鲁棒性、可验证性和保真度，消除对现有方法所需的非勾结双服务器设置或可信第三方的依赖。

Conclusion: 提出的方法有效，能在对抗环境下实现联邦学习目标，且比现有方法更具优势。

Abstract: Federated learning is a promising distributed learning paradigm that enables
collaborative model training without exposing local client data, thereby
protect data privacy. However, it also brings new threats and challenges. The
advancement of model inversion attacks has rendered the plaintext transmission
of local models insecure, while the distributed nature of federated learning
makes it particularly vulnerable to attacks raised by malicious clients. To
protect data privacy and prevent malicious client attacks, this paper proposes
a privacy-preserving federated learning framework based on verifiable
functional encryption, without a non-colluding dual-server setup or additional
trusted third-party. Specifically, we propose a novel decentralized verifiable
functional encryption (DVFE) scheme that enables the verification of specific
relationships over multi-dimensional ciphertexts. This scheme is formally
treated, in terms of definition, security model and security proof.
Furthermore, based on the proposed DVFE scheme, we design a privacy-preserving
federated learning framework VFEFL that incorporates a novel robust aggregation
rule to detect malicious clients, enabling the effective training of
high-accuracy models under adversarial settings. Finally, we provide formal
analysis and empirical evaluation of the proposed schemes. The results
demonstrate that our approach achieves the desired privacy protection,
robustness, verifiability and fidelity, while eliminating the reliance on
non-colluding dual-server settings or trusted third parties required by
existing methods.

</details>


### [570] [Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective](https://arxiv.org/abs/2506.13009)
*Nima Naderloui,Shenao Yan,Binghui Wang,Jie Fu,Wendy Hui Wang,Weiran Liu,Yuan Hong*

Main category: cs.CR

TL;DR: 文章指出现有非精确机器学习评估框架不足，提出RULI框架，发现现有方法漏洞，证明RULI严谨、可扩展且细粒度。


<details>
  <summary>Details</summary>
Motivation: 现有非精确机器学习方法缺乏正式保证，需鲁棒评估框架评估其隐私性和有效性，而现有评估框架存在关键缺陷。

Method: 先识别现有评估框架的关键缺陷，再提出RULI框架，引入双目标攻击以样本粒度衡量学习效果和隐私风险。

Result: 发现现有机器学习方法存在显著漏洞，RULI攻击成功率更高，暴露了现有方法低估的隐私风险。

Conclusion: RULI基于博弈论，经图像和文本数据验证，为评估机器学习技术提供了严谨、可扩展和细粒度的方法。

Abstract: Machine unlearning focuses on efficiently removing specific data from trained
models, addressing privacy and compliance concerns with reasonable costs.
Although exact unlearning ensures complete data removal equivalent to
retraining, it is impractical for large-scale models, leading to growing
interest in inexact unlearning methods. However, the lack of formal guarantees
in these methods necessitates the need for robust evaluation frameworks to
assess their privacy and effectiveness. In this work, we first identify several
key pitfalls of the existing unlearning evaluation frameworks, e.g., focusing
on average-case evaluation or targeting random samples for evaluation,
incomplete comparisons with the retraining baseline. Then, we propose RULI
(Rectified Unlearning Evaluation Framework via Likelihood Inference), a novel
framework to address critical gaps in the evaluation of inexact unlearning
methods. RULI introduces a dual-objective attack to measure both unlearning
efficacy and privacy risks at a per-sample granularity. Our findings reveal
significant vulnerabilities in state-of-the-art unlearning methods, where RULI
achieves higher attack success rates, exposing privacy risks underestimated by
existing methods. Built on a game-based foundation and validated through
empirical evaluations on both image and text data (spanning tasks from
classification to generation), RULI provides a rigorous, scalable, and
fine-grained methodology for evaluating unlearning techniques.

</details>


### [571] [Position: Certified Robustness Does Not (Yet) Imply Model Security](https://arxiv.org/abs/2506.13024)
*Andrew C. Cullen,Paul Montague,Sarah M. Erfani,Benjamin I. P. Rubinstein*

Main category: cs.CR

TL;DR: 认证鲁棒性应用于现实面临挑战，文章指出研究差距并呼吁采取行动推进该领域发展。


<details>
  <summary>Details</summary>
Motivation: 解决认证鲁棒性技术在现实应用前存在的重大挑战，推动其实际应用。

Method: 识别当前研究中检测无区分悖论、评估标准缺失、用户期望带来的安全风险等关键差距，提出具体解决步骤。

Result: 未提及具体结果。

Conclusion: 呼吁认证研究社区采取具体步骤解决基础挑战，推动该领域向实际应用迈进。

Abstract: While certified robustness is widely promoted as a solution to adversarial
examples in Artificial Intelligence systems, significant challenges remain
before these techniques can be meaningfully deployed in real-world
applications. We identify critical gaps in current research, including the
paradox of detection without distinction, the lack of clear criteria for
practitioners to evaluate certification schemes, and the potential security
risks arising from users' expectations surrounding ``guaranteed" robustness
claims. This position paper is a call to arms for the certification research
community, proposing concrete steps to address these fundamental challenges and
advance the field toward practical applicability.

</details>


### [572] [Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments](https://arxiv.org/abs/2506.13205)
*Xuan Wang,Siyuan Liang,Zhe Liu,Yi Yu,Yuliang Lu,Xiaochun Cao,Ee-Chien Chang*

Main category: cs.CR

TL;DR: 提出针对基于VLM的移动代理的干净标签后门攻击GHOST，评估显示攻击成功率高且不影响正常任务，揭示此类代理安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 基于VLM的移动代理在有限用户生成数据集上微调，训练过程易受隐蔽威胁，需研究其安全漏洞。

Method: 仅操纵部分训练样本的视觉输入，将中毒样本梯度与目标实例对齐，开发三种视觉触发器。

Result: 在六个安卓应用和三种VLM架构上评估，攻击成功率达94.67%，正常任务性能FSR达95.85%，消融研究揭示设计选择影响。

Conclusion: 首次揭示基于VLM的移动代理存在关键安全漏洞，迫切需要有效防御机制。

Abstract: With the growing integration of vision-language models (VLMs), mobile agents
are now widely used for tasks like UI automation and camera-based user
assistance. These agents are often fine-tuned on limited user-generated
datasets, leaving them vulnerable to covert threats during the training
process. In this work we present GHOST, the first clean-label backdoor attack
specifically designed for mobile agents built upon VLMs. Our method manipulates
only the visual inputs of a portion of the training samples - without altering
their corresponding labels or instructions - thereby injecting malicious
behaviors into the model. Once fine-tuned with this tampered data, the agent
will exhibit attacker-controlled responses when a specific visual trigger is
introduced at inference time. The core of our approach lies in aligning the
gradients of poisoned samples with those of a chosen target instance, embedding
backdoor-relevant features into the poisoned training data. To maintain stealth
and enhance robustness, we develop three realistic visual triggers: static
visual patches, dynamic motion cues, and subtle low-opacity overlays. We
evaluate our method across six real-world Android apps and three VLM
architectures adapted for mobile use. Results show that our attack achieves
high attack success rates (up to 94.67 percent) while maintaining high
clean-task performance (FSR up to 95.85 percent). Additionally, ablation
studies shed light on how various design choices affect the efficacy and
concealment of the attack. Overall, this work is the first to expose critical
security flaws in VLM-based mobile agents, highlighting their susceptibility to
clean-label backdoor attacks and the urgent need for effective defense
mechanisms in their training pipelines. Code and examples are available at:
https://anonymous.4open.science/r/ase-2025-C478.

</details>


### [573] [Evaluating Large Language Models for Phishing Detection, Self-Consistency, Faithfulness, and Explainability](https://arxiv.org/abs/2506.13746)
*Shova Kuikel,Aritran Piplai,Palvi Aggarwal*

Main category: cs.CR

TL;DR: 本文研究大语言模型在钓鱼邮件分类和可解释性上的表现，微调多种模型并评估，发现Llama模型解释一致性好但准确率欠佳，Wizard准确率高但解释一致性低。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击不断演变，现有检测系统面临挑战，大语言模型有潜力但需解决可靠性和可解释性问题。

Method: 微调基于Transformer的模型如BERT、Llama、Wizard，采用二元序列分类、对比学习和直接偏好优化，用基于SHAP值的ConsistenCy度量评估。

Result: Llama模型CC SHAP分数高，预测解释标记对齐性强，但决策准确性不可靠；Wizard预测准确性更好，但CC SHAP分数低。

Conclusion: 不同大语言模型在钓鱼邮件分类的准确性和解释一致性上各有优劣。

Abstract: Phishing attacks remain one of the most prevalent and persistent
cybersecurity threat with attackers continuously evolving and intensifying
tactics to evade the general detection system. Despite significant advances in
artificial intelligence and machine learning, faithfully reproducing the
interpretable reasoning with classification and explainability that underpin
phishing judgments remains challenging. Due to recent advancement in Natural
Language Processing, Large Language Models (LLMs) show a promising direction
and potential for improving domain specific phishing classification tasks.
However, enhancing the reliability and robustness of classification models
requires not only accurate predictions from LLMs but also consistent and
trustworthy explanations aligning with those predictions. Therefore, a key
question remains: can LLMs not only classify phishing emails accurately but
also generate explanations that are reliably aligned with their predictions and
internally self-consistent? To answer these questions, we have fine-tuned
transformer based models, including BERT, Llama models, and Wizard, to improve
domain relevance and make them more tailored to phishing specific distinctions,
using Binary Sequence Classification, Contrastive Learning (CL) and Direct
Preference Optimization (DPO). To that end, we examined their performance in
phishing classification and explainability by applying the ConsistenCy measure
based on SHAPley values (CC SHAP), which measures prediction explanation token
alignment to test the model's internal faithfulness and consistency and uncover
the rationale behind its predictions and reasoning. Overall, our findings show
that Llama models exhibit stronger prediction explanation token alignment with
higher CC SHAP scores despite lacking reliable decision making accuracy,
whereas Wizard achieves better prediction accuracy but lower CC SHAP scores.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [574] [Effective Stimulus Propagation in Neural Circuits: Driver Node Selection](https://arxiv.org/abs/2506.13615)
*Bulat Batuev,Arsenii Onuchin,Sergey Sukhov*

Main category: q-bio.NC

TL;DR: 本文建立框架识别最优控制节点，比较不同驱动节点选择策略，发现靶向刺激源群体中部分中枢神经元可提升信号传播保真度和效率，为神经调节奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 解决模块化神经网络中信号传播精确控制这一计算神经科学的基本挑战。

Method: 建立识别最优控制节点的框架，使用尖峰随机块模型网络，系统比较随机采样和基于拓扑的中心性度量等驱动节点选择策略。

Result: 靶向刺激源群体中10 - 20%最中枢的神经元相比随机选择显著提升尖峰传播保真度，在临界模块间连接密度下信号传输效率提高2.7倍。

Conclusion: 研究结果为生物神经系统和神经技术应用中的精确神经调节建立了理论基础。

Abstract: Precise control of signal propagation in modular neural networks represents a
fundamental challenge in computational neuroscience. We establish a framework
for identifying optimal control nodes that maximize stimulus transmission
between weakly coupled neural populations. Using spiking stochastic block model
networks, we systematically compare driver node selection strategies -
including random sampling and topology-based centrality measures (degree,
betweenness, closeness, eigenvector, harmonic, and percolation centrality) - to
determine minimal control inputs for achieving inter-population
synchronization.
  Targeted stimulation of just 10-20% of the most central neurons in the source
population significantly enhances spiking propagation fidelity compared to
random selection. This approach yields a 2.7-fold increase in signal transfer
efficiency at critical inter-module connection densities p_inter = 0.04-0.07.
These findings establish a theoretical foundation for precision neuromodulation
in biological neural systems and neurotechnology applications.

</details>


### [575] [Examining the effects of music on cognitive skills of children in early childhood with the Pythagorean fuzzy set approach](https://arxiv.org/abs/2506.12016)
*Murat Kirisci,Nihat Topac,Musa Bardak*

Main category: q-bio.NC

TL;DR: 研究用PFS方法探讨音乐对幼儿认知发展的影响，算法结果与专家观点相符。


<details>
  <summary>Details</summary>
Motivation: 认知发展受多种因素影响，音乐教育作为环境因素，研究其对幼儿认知发展的影响。

Method: 使用Yager定义的Pythagorean Fuzzy Sets (PFS)方法，基于专家意见创建PFS并给出算法，用期望得分函数进行算法排名。

Result: 算法结果支持专家关于幼儿音乐教育中时空技能发展的数据，算法排名与专家排名重叠。

Conclusion: 表明PFS方法可有效分析音乐对幼儿认知发展的影响，且结果与专家看法一致。

Abstract: There are many genetic and environmental factors that affect cognitive
development. Music education can also be considered as one of the environmental
factors. Some researchers emphasize that Music is an action that requires
meta-cognitive functions such as mathematics and chess and supports spatial
intelligence. The effect of Music on cognitive development in early childhood
was examined using the Pythagorean Fuzzy Sets(PFS) method defined by Yager.
This study created PFS based on experts' opinions, and an algorithm was given
according to PFS. The algorithm's results supported the experts' data on the
development of spatial-temporal skills in music education given in early
childhood. The algorithm's ranking was done using the Expectation Score
Function. The rankings obtained from the algorithm overlap with the experts'
rankings.

</details>


### [576] [Towards Unified Neural Decoding with Brain Functional Network Modeling](https://arxiv.org/abs/2506.12055)
*Di Wu,Linghao Bu,Yifei Jia,Lu Cao,Siyuan Li,Siyu Chen,Yueqian Zhou,Sheng Fan,Wenjie Ren,Dengchang Wu,Kang Wang,Yue Zhang,Yuehui Ma,Jie Yang,Mohamad Sawan*

Main category: q-bio.NC

TL;DR: 提出MIBRAIN框架，整合多人颅内神经生理记录构建全功能脑网络模型，在普通话音节发音解码实验中表现良好，为跨个体神经解码和临床应用提供新思路。


<details>
  <summary>Details</summary>
Motivation: 个体生理和电极植入异质性限制了当前神经解码方法在个体间的应用，使得跨个体神经解码难以实现。

Method: 提出Multi - individual Brain Region - Aggregated Network (MIBRAIN)框架，利用自监督学习推导广义神经原型，支持脑区交互和跨主体神经同步的组级分析。

Result: 实时在线和离线解码实验表明，在有声和无声发音解码方面有显著改进，随着多主体数据整合增加，解码精度提高，且能有效泛化到未见主体，无直接电极覆盖区域的神经预测得到验证。

Conclusion: 该框架为跨个体的稳健神经解码铺平了道路，并为实际临床应用提供了见解。

Abstract: Recent achievements in implantable brain-computer interfaces (iBCIs) have
demonstrated the potential to decode cognitive and motor behaviors with
intracranial brain recordings; however, individual physiological and electrode
implantation heterogeneities have constrained current approaches to neural
decoding within single individuals, rendering interindividual neural decoding
elusive. Here, we present Multi-individual Brain Region-Aggregated Network
(MIBRAIN), a neural decoding framework that constructs a whole functional brain
network model by integrating intracranial neurophysiological recordings across
multiple individuals. MIBRAIN leverages self-supervised learning to derive
generalized neural prototypes and supports group-level analysis of brain-region
interactions and inter-subject neural synchrony. To validate our framework, we
recorded stereoelectroencephalography (sEEG) signals from a cohort of
individuals performing Mandarin syllable articulation. Both real-time online
and offline decoding experiments demonstrated significant improvements in both
audible and silent articulation decoding, enhanced decoding accuracy with
increased multi-subject data integration, and effective generalization to
unseen subjects. Furthermore, neural predictions for regions without direct
electrode coverage were validated against authentic neural data. Overall, this
framework paves the way for robust neural decoding across individuals and
offers insights for practical clinical applications.

</details>


### [577] [Wanting to Be Understood Explains the Meta-Problem of Consciousness](https://arxiv.org/abs/2506.12086)
*Chrisantha Fernando,Dylan Banarse,Simon Osindero*

Main category: q-bio.NC

TL;DR: 人们为被理解创造外部表征，其是通达意识的前提，但无法完整再现‘原始体验’的丰富性，对体验解释的过高认知需求使意识难题持续，有意识是努力让自身能动性被理解。


<details>
  <summary>Details</summary>
Motivation: 探讨意识难题产生的原因以及意识的本质。

Method: 通过分析外部表征、通达意识、原始体验之间的关系进行理论阐述。

Result: 指出是对被完美理解的过高认知需求而非形而上学鸿沟使意识难题存在。

Conclusion: 有意识意味着努力让自己和他人理解自身的能动性，且人们会不断创造新方式交流和思考体验。

Abstract: Because we are highly motivated to be understood, we created public external
representations -- mime, language, art -- to externalise our inner states. We
argue that such external representations are a pre-condition for access
consciousness, the global availability of information for reasoning. Yet the
bandwidth of access consciousness is tiny compared with the richness of `raw
experience', so no external representation can reproduce that richness in full.
Ordinarily an explanation of experience need only let an audience `grasp' the
relevant pattern, not relive the phenomenon. But our drive to be understood,
and our low level sensorimotor capacities for `grasping' so rich, that the
demand for an explanation of the feel of experience cannot be ``satisfactory''.
That inflated epistemic demand (the preeminence of our expectation that we
could be perfectly understood by another or ourselves) rather than an
irreducible metaphysical gulf -- keeps the hard problem of consciousness alive.
But on the plus side, it seems we will simply never give up creating new ways
to communicate and think about our experiences. In this view, to be consciously
aware is to strive to have one's agency understood by oneself and others.

</details>


### [578] [Scale-Invariance Drives Convergence in AI and Brain Representations](https://arxiv.org/abs/2506.12117)
*Junjie Yu,Wenxiao Ma,Jianyu Zhang,Haotian Deng,Zihan Deng,Yi Guo,Quanying Liu*

Main category: q-bio.NC

TL;DR: 研究提出多尺度分析框架量化AI表征中尺度不变性，发现尺度不变性强的嵌入与fMRI数据对齐更好，大预训练数据集和语言模态可增强此特性，为评估类人AI系统提供新框架。


<details>
  <summary>Details</summary>
Motivation: 解释大规模AI模型内部表征趋同且与神经活动对齐的原因，探究尺度不变性在此过程中的作用。

Method: 提出多尺度分析框架，量化AI表征中尺度不变性的维度稳定性和跨尺度结构相似性，研究其与视觉皮层fMRI响应的对齐性能。

Result: 尺度维度更一致、结构相似性更高的嵌入与fMRI数据对齐更好；fMRI数据的流形结构更集中，具有相似尺度模式的嵌入与fMRI数据更匹配；大预训练数据集和语言模态增强嵌入的尺度不变性，提升神经对齐。

Conclusion: 尺度不变性是连接人工和生物表征的基本结构原则，为评估类人AI系统结构质量提供新框架。

Abstract: Despite variations in architecture and pretraining strategies, recent studies
indicate that large-scale AI models often converge toward similar internal
representations that also align with neural activity. We propose that
scale-invariance, a fundamental structural principle in natural systems, is a
key driver of this convergence. In this work, we propose a multi-scale
analytical framework to quantify two core aspects of scale-invariance in AI
representations: dimensional stability and structural similarity across scales.
We further investigate whether these properties can predict alignment
performance with functional Magnetic Resonance Imaging (fMRI) responses in the
visual cortex. Our analysis reveals that embeddings with more consistent
dimension and higher structural similarity across scales align better with fMRI
data. Furthermore, we find that the manifold structure of fMRI data is more
concentrated, with most features dissipating at smaller scales. Embeddings with
similar scale patterns align more closely with fMRI data. We also show that
larger pretraining datasets and the inclusion of language modalities enhance
the scale-invariance properties of embeddings, further improving neural
alignment. Our findings indicate that scale-invariance is a fundamental
structural principle that bridges artificial and biological representations,
providing a new framework for evaluating the structural quality of human-like
AI systems.

</details>


### [579] [Mapping Neural Theories of Consciousness onto the Common Model of Cognition](https://arxiv.org/abs/2506.12224)
*Paul S. Rosenbloom,John E. Laird,Christian Lebiere,Andrea Stocco*

Main category: q-bio.NC

TL;DR: 尝试将四种意识的神经理论映射到认知通用模型上，揭示其共性与一致性。


<details>
  <summary>Details</summary>
Motivation: 探索四种意识的神经理论与认知通用模型的关系。

Method: 将四种意识的神经理论映射到认知通用模型。

Result: 发现四种理论共同依赖循环局部模块、作用于具有复杂状态的全局工作记忆的认知循环，且一种现有的神经视角的意识整合观点与通用模型一致。

Conclusion: 完成了理论映射并揭示了相关共性和一致性。

Abstract: A beginning is made at mapping four neural theories of consciousness onto the
Common Model of Cognition. This highlights how the four jointly depend on
recurrent local modules plus a cognitive cycle operating on a global working
memory with complex states, and reveals how an existing integrative view of
consciousness from a neural perspective aligns with the Com-mon Model.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [580] [Language Models Enable Data-Augmented Synthesis Planning for Inorganic Materials](https://arxiv.org/abs/2506.12557)
*Thorben Prein,Elton Pan,Janik Jehkul,Steffen Weinmann,Elsa A. Olivetti,Jennifer L. M. Rupp*

Main category: cond-mat.mtrl-sci

TL;DR: 本文展示了语言模型无需微调即可回忆合成条件，集成模型可提升精度并降低成本，还训练SyntMTE模型，改进合成规划。


<details>
  <summary>Details</summary>
Motivation: 当前无机合成规划依赖启发式方法或有限数据集训练的机器学习模型，限制了通用性。

Method: 使用现成语言模型预测合成条件，集成模型提升性能，用生成的反应配方和文献数据预训练SyntMTE模型并微调。

Result: 现成语言模型有一定预测精度，集成后增强精度并降低成本，SyntMTE模型降低预测误差，改进达8.7%，案例研究能重现实验趋势。

Conclusion: 混合工作流程实现了可扩展、数据高效的无机合成规划。

Abstract: Inorganic synthesis planning currently relies primarily on heuristic
approaches or machine-learning models trained on limited datasets, which
constrains its generality. We demonstrate that language models, without
task-specific fine-tuning, can recall synthesis conditions. Off-the-shelf
models, such as GPT-4.1, Gemini 2.0 Flash and Llama 4 Maverick, achieve a Top-1
precursor-prediction accuracy of up to 53.8 % and a Top-5 performance of 66.1 %
on a held-out set of 1,000 reactions. They also predict calcination and
sintering temperatures with mean absolute errors below 126 {\deg}C, matching
specialized regression methods. Ensembling these language models further
enhances predictive accuracy and reduces inference cost per prediction by up to
70 %. We subsequently employ language models to generate 28,548 synthetic
reaction recipes, which we combine with literature-mined examples to pretrain a
transformer-based model, SyntMTE. After fine-tuning on the combined dataset,
SyntMTE reduces mean-absolute error in sintering temperature prediction to 73
{\deg}C and in calcination temperature to 98 {\deg}C. This strategy improves
models by up to 8.7 % compared with baselines trained exclusively on
experimental data. Finally, in a case study on Li7La3Zr2O12 solid-state
electrolytes, we demonstrate that SyntMTE reproduces the experimentally
observed dopant-dependent sintering trends. Our hybrid workflow enables
scalable, data-efficient inorganic synthesis planning.

</details>


### [581] [Information fusion strategy integrating pre-trained language model and contrastive learning for materials knowledge mining](https://arxiv.org/abs/2506.12516)
*Yongqian Peng,Zhouran Zhang,Longhui Zhang,Fengyuan Zhao,Yahao Li,Yicong Ye,Shuxin Bai*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出信息融合架构，结合材料科学文献文本与物理描述符预测合金延展性，模型表现出色，为材料设计和发现奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以量化加工条件和微观结构特征，导致预测合金延展性等复杂属性具有挑战性。

Method: 提出信息融合架构，用MatSciBERT进行文本理解，结合对比学习自动提取隐含知识。

Result: 模型在钛合金验证集和难熔多主元合金测试集上的决定系数（R2）分别达到0.849和0.680。

Conclusion: 该方法为定量描述符不完整的复杂材料系统的属性预测提供了整体框架，为知识引导的材料设计和信息驱动的材料发现奠定基础。

Abstract: Machine learning has revolutionized materials design, yet predicting complex
properties like alloy ductility remains challenging due to the influence of
processing conditions and microstructural features that resist quantification
through traditional reductionist approaches. Here, we present an innovative
information fusion architecture that integrates domain-specific texts from
materials science literature with quantitative physical descriptors to overcome
these limitations. Our framework employs MatSciBERT for advanced textual
comprehension and incorporates contrastive learning to automatically extract
implicit knowledge regarding processing parameters and microstructural
characteristics. Through rigorous ablation studies and comparative experiments,
the model demonstrates superior performance, achieving coefficient of
determination (R2) values of 0.849 and 0.680 on titanium alloy validation set
and refractory multi-principal-element alloy test set. This systematic approach
provides a holistic framework for property prediction in complex material
systems where quantitative descriptors are incomplete and establishes a
foundation for knowledge-guided materials design and informatics-driven
materials discovery.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [582] [Perfect Secrecy in the Wild: A Characterization](https://arxiv.org/abs/2506.12416)
*Costas Cavounidis,Massimiliano Furlan,Alkis Georgiadis-Harris*

Main category: econ.TH

TL;DR: 给出Alice在Bob掌握特定信息时向其透露状态X的充要条件，Shannon一次性密码本完美保密结果为特例


<details>
  <summary>Details</summary>
Motivation: 探讨Alice在何种条件下可在Bob知晓特定信息时透露状态X，不知时则不透露

Method: 未提及

Result: 得到关于X和Y联合分布的简单充要条件

Conclusion: Shannon一次性密码本完美保密结果是该研究的特例

Abstract: Alice wishes to reveal the state $X$ to Bob, if he knows some other
information $Y$ also known to her. If Bob does not, she wishes to reveal
nothing about $X$ at all. When can Alice accomplish this? We provide a simple
necessary and sufficient condition on the joint distribution of $X$ and $Y$.
Shannon's result on the perfect secrecy of the one-time pad follows as a
special case.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [583] [TCN-DPD: Parameter-Efficient Temporal Convolutional Networks for Wideband Digital Predistortion](https://arxiv.org/abs/2506.12165)
*Huanqiang Duan,Manno Versluis,Qinyu Chen,Leo C. N. de Vreede,Chang Gao*

Main category: eess.SP

TL;DR: 本文提出基于时间卷积网络的参数高效架构TCN - DPD用于射频功率放大器线性化，评估显示其有良好性能和参数效率。


<details>
  <summary>Details</summary>
Motivation: 减轻射频功率放大器中的非线性问题，尤其是针对宽带应用。

Method: 提出基于时间卷积网络的TCN - DPD架构，集成非因果扩张卷积和优化激活函数。

Result: 在OpenDPD框架和DPA_200MHz数据集上评估，500参数时模拟ACPRs为 - 51.58/-49.26 dBc (L/R)，EVM为 - 47.52 dB，NMSE为 - 44.61 dB，200参数时线性化性能仍优于先前模型。

Conclusion: TCN - DPD对高效宽带功率放大器线性化很有前景。

Abstract: Digital predistortion (DPD) is essential for mitigating nonlinearity in RF
power amplifiers, particularly for wideband applications. This paper presents
TCN-DPD, a parameter-efficient architecture based on temporal convolutional
networks, integrating noncausal dilated convolutions with optimized activation
functions. Evaluated on the OpenDPD framework with the DPA_200MHz dataset,
TCN-DPD achieves simulated ACPRs of -51.58/-49.26 dBc (L/R), EVM of -47.52 dB,
and NMSE of -44.61 dB with 500 parameters and maintains superior linearization
than prior models down to 200 parameters, making it promising for efficient
wideband PA linearization.

</details>


### [584] [Directed Acyclic Graph Convolutional Networks](https://arxiv.org/abs/2506.12218)
*Samuel Rey,Hamed Ajorlou,Gonzalo Mateos*

Main category: eess.SP

TL;DR: 本文介绍DAG卷积网络（DCN）及其并行版本（PDCN）用于DAG结构数据深度学习，实验显示其在多方面优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 有向无环图（DAG）在多领域重要，现有机器学习方法存在不足，需要设计专门用于DAG卷积学习的图神经网络架构。

Method: 引入DCN，利用因果图滤波器学习节点表示；提出PDCN，通过并行因果图移位算子处理DAG特征并使用共享多层感知机；还分析了架构的排列等变性和表达能力。

Result: 综合数值测试表明，(P)DCN在准确性、鲁棒性和计算效率方面优于现有基线模型。

Conclusion: (P)DCN是基于图信号处理原理设计的、用于DAG结构数据深度学习的可行框架。

Abstract: Directed acyclic graphs (DAGs) are central to science and engineering
applications including causal inference, scheduling, and neural architecture
search. In this work, we introduce the DAG Convolutional Network (DCN), a novel
graph neural network (GNN) architecture designed specifically for convolutional
learning from signals supported on DAGs. The DCN leverages causal graph filters
to learn nodal representations that account for the partial ordering inherent
to DAGs, a strong inductive bias does not present in conventional GNNs. Unlike
prior art in machine learning over DAGs, DCN builds on formal convolutional
operations that admit spectral-domain representations. We further propose the
Parallel DCN (PDCN), a model that feeds input DAG signals to a parallel bank of
causal graph-shift operators and processes these DAG-aware features using a
shared multilayer perceptron. This way, PDCN decouples model complexity from
graph size while maintaining satisfactory predictive performance. The
architectures' permutation equivariance and expressive power properties are
also established. Comprehensive numerical tests across several tasks, datasets,
and experimental conditions demonstrate that (P)DCN compares favorably with
state-of-the-art baselines in terms of accuracy, robustness, and computational
efficiency. These results position (P)DCN as a viable framework for deep
learning from DAG-structured data that is designed from first (graph) signal
processing principles.

</details>


### [585] [Synesthesia of Machines (SoM)-Enhanced Sub-THz ISAC Transmission for Air-Ground Network](https://arxiv.org/abs/2506.12831)
*Zonghui Yang,Shijian Gao,Xiang Cheng,Liuqing Yang*

Main category: eess.SP

TL;DR: 本文提出受机器联觉启发的多模态传感融合框架，优化亚太赫兹 ISAC 传输，提升性能并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 亚太赫兹频段的集成传感与通信（ISAC）对未来空-地网络至关重要，但独特的传播特性和硬件限制在优化性能时增加了操作延迟。

Method: 引入受机器联觉启发的多模态传感融合框架，利用亚太赫兹硬件和信道的自由度优化射频环境；开发斜视感知波束管理；利用多模态信息，结合视觉数据定位，使用定制多模态学习算法优化混合预编码器；提出新的性能评估指标。

Result: 广泛实验表明，所提出的方案显著提高了 ISAC 效率。

Conclusion: 所提框架能有效增强亚太赫兹 ISAC 传输，提升性能并降低延迟。

Abstract: Integrated sensing and communication (ISAC) within sub-THz frequencies is
crucial for future air-ground networks, but unique propagation characteristics
and hardware limitations present challenges in optimizing ISAC performance
while increasing operational latency. This paper introduces a multi-modal
sensing fusion framework inspired by synesthesia of machine (SoM) to enhance
sub-THz ISAC transmission. By exploiting inherent degrees of freedom in sub-THz
hardware and channels, the framework optimizes the radio-frequency environment.
Squint-aware beam management is developed to improve air-ground network
adaptability, enabling three-dimensional dynamic ISAC links. Leveraging
multi-modal information, the framework enhances ISAC performance and reduces
latency. Visual data rapidly localizes users and targets, while a customized
multi-modal learning algorithm optimizes the hybrid precoder. A new metric
provides comprehensive performance evaluation, and extensive experiments
demonstrate that the proposed scheme significantly improves ISAC efficiency.

</details>


### [586] [HELENA: High-Efficiency Learning-based channel Estimation using dual Neural Attention](https://arxiv.org/abs/2506.13408)
*Miguel Camelo Botero,Esra Aycan Beyazit,Nina Slamnik-Kriještorac,Johann M. Marquez-Barja*

Main category: eess.SP

TL;DR: 提出紧凑深度学习模型HELENA用于正交频分复用系统信道估计，相比CEViT，减少推理时间、参数，精度相当。


<details>
  <summary>Details</summary>
Motivation: 准确的信道估计在低信噪比和严格延迟约束下对5G新空口等正交频分复用系统至关重要。

Method: 提出HELENA模型，结合轻量级卷积骨干网络与两种高效注意力机制。

Result: 与CEViT相比，HELENA推理时间减少45.0%，精度相当，所需参数少8倍。

Conclusion: HELENA适合低延迟、实时部署。

Abstract: Accurate channel estimation is critical for high-performance Orthogonal
Frequency-Division Multiplexing systems such as 5G New Radio, particularly
under low signal-to-noise ratio and stringent latency constraints. This letter
presents HELENA, a compact deep learning model that combines a lightweight
convolutional backbone with two efficient attention mechanisms: patch-wise
multi-head self-attention for capturing global dependencies and a
squeeze-and-excitation block for local feature refinement. Compared to CEViT, a
state-of-the-art vision transformer-based estimator, HELENA reduces inference
time by 45.0\% (0.175\,ms vs.\ 0.318\,ms), achieves comparable accuracy
($-16.78$\,dB vs.\ $-17.30$\,dB), and requires $8\times$ fewer parameters
(0.11M vs.\ 0.88M), demonstrating its suitability for low-latency, real-time
deployment.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [587] [Curriculum Learning for Biological Sequence Prediction: The Case of De Novo Peptide Sequencing](https://arxiv.org/abs/2506.13485)
*Xiang Zhang,Jiaqi Wei,Zijie Qiu,Sheng Xu,Nanqing Dong,Zhiqiang Gao,Siqi Sun*

Main category: q-bio.BM

TL;DR: 本文提出改进的非自回归肽测序模型，结合课程学习策略和自精炼推理模块，降低训练失败率，性能超以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有非自回归Transformer（NAT）方法依赖CTC损失，存在优化挑战和训练失败风险。

Method: 提出改进模型，结合结构化蛋白质序列课程学习策略，根据模型能力调整学习难度；引入自精炼推理模块，迭代提升预测结果。

Result: 课程学习策略使NAT训练失败频率降低超90%，在九个基准物种上多项指标表现优于以往方法。

Conclusion: 改进的非自回归肽测序模型有效降低训练失败率，提升测序性能。

Abstract: Peptide sequencing-the process of identifying amino acid sequences from mass
spectrometry data-is a fundamental task in proteomics. Non-Autoregressive
Transformers (NATs) have proven highly effective for this task, outperforming
traditional methods. Unlike autoregressive models, which generate tokens
sequentially, NATs predict all positions simultaneously, leveraging
bidirectional context through unmasked self-attention. However, existing NAT
approaches often rely on Connectionist Temporal Classification (CTC) loss,
which presents significant optimization challenges due to CTC's complexity and
increases the risk of training failures. To address these issues, we propose an
improved non-autoregressive peptide sequencing model that incorporates a
structured protein sequence curriculum learning strategy. This approach adjusts
protein's learning difficulty based on the model's estimated protein
generational capabilities through a sampling process, progressively learning
peptide generation from simple to complex sequences. Additionally, we introduce
a self-refining inference-time module that iteratively enhances predictions
using learned NAT token embeddings, improving sequence accuracy at a
fine-grained level. Our curriculum learning strategy reduces NAT training
failures frequency by more than 90% based on sampled training over various data
distributions. Evaluations on nine benchmark species demonstrate that our
approach outperforms all previous methods across multiple metrics and species.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [588] [Small Volatility Approximation and Multi-Factor HJM Models](https://arxiv.org/abs/2506.12584)
*V. M. Belyaev*

Main category: q-fin.PR

TL;DR: 介绍用小波动率近似法校准具有确定性相关性、因子波动率和均值回归的多因子HJM模型，校准质量好且与因子数量无关。


<details>
  <summary>Details</summary>
Motivation: 探索有效校准多因子HJM模型的方法。

Method: 使用小波动率近似法。

Result: 校准质量非常好，且不依赖于因子数量。

Conclusion: 小波动率近似法可有效用于多因子HJM模型校准。

Abstract: Here we demonstrate how we can use Small Volatility Approximation in
calibration of Multi-Factor HJM model with deterministic correlations, factor
volatilities and mean reversals. It is noticed that quality of this calibration
is very good and it does not depend on number of factors.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [589] [FPT Constant Approximation Algorithms for Colorful Sum of Radii](https://arxiv.org/abs/2506.13191)
*Shuilian Liu,Gregory Gutin,Yicheng Xu,Yong Zhang*

Main category: cs.CG

TL;DR: 本文研究彩色半径和问题，给出首个FPT时间的常数因子近似算法，包括一个(2+ε)-近似算法和一个(7+ε)-近似算法。


<details>
  <summary>Details</summary>
Motivation: 经典半径和问题为NP难问题，前人对彩色半径和问题给出O(log ω)近似算法，本文目标是给出常数因子近似算法。

Method: 设计迭代覆盖算法，还利用彩色k - 中心子程序开发新算法。

Result: 设计出(2+ε)-近似算法，运行时间在k和m上呈指数级；开发出(7+ε)-近似算法，消除了运行时间对m的指数依赖。

Conclusion: 成功给出彩色半径和问题在FPT时间的常数因子近似算法。

Abstract: We study the colorful sum of radii problem, where the input is a point set
$P$ partitioned into classes $P_1, P_2, \dots, P_\omega$, along with per-class
outlier bounds $m_1, m_2, \dots, m_\omega$, summing to $m$. The goal is to
select a subset $\mathcal{C} \subseteq P$ of $k$ centers and assign points to
centers in $\mathcal{C}$, allowing up to $m_i$ unassigned points (outliers)
from each class $P_i$, while minimizing the sum of cluster radii. The radius of
a cluster is defined as the maximum distance from any point in the cluster to
its center. The classical (non-colorful) version of the sum of radii problem is
known to be NP-hard, even on weighted planar graphs. The colorful sum of radii
is introduced by Chekuri et al. (2022), who provide an $O(\log
\omega)$-approximation algorithm. In this paper, we present the first
constant-factor approximation algorithms for the colorful sum of radii running
in FPT (fixed-parameter tractable) time. Our contributions are twofold: We
design an iterative covering algorithm that achieves a
$(2+\varepsilon)$-approximation with running time exponential in both $k$ and
$m$; We further develop a $(7+\varepsilon)$-approximation algorithm by
leveraging a colorful $k$-center subroutine, improving the running time by
removing the exponential dependency on $m$.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [590] [MRI-CORE: A Foundation Model for Magnetic Resonance Imaging](https://arxiv.org/abs/2506.12186)
*Haoyu Dong,Yuwen Chen,Hanxue Gu,Nicholas Konz,Yaqian Chen,Qihang Li,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: 介绍MRI-CORE视觉基础模型，在有限标注数据场景下提升MRI分割性能，还有新模型能力，降低数据标注资源门槛。


<details>
  <summary>Details</summary>
Motivation: 训练特定新任务模型需大量标注数据，获取困难，为解决此问题。

Method: 引入MRI-CORE，用超600万切片、超11万个MRI体积在18个主要身体部位预训练。

Result: 在五个MRI物体分割任务实验中，有限标注数据下显著提升分割性能，平均3D Dice系数增益6.97%；有图像属性分类和零样本分割等新能力。

Conclusion: MRI-CORE作为通用视觉基础模型有价值，可降低许多应用的数据标注资源障碍。

Abstract: The widespread use of Magnetic Resonance Imaging (MRI) and the rise of deep
learning have enabled the development of powerful predictive models for a wide
range of diagnostic tasks in MRI, such as image classification or object
segmentation. However, training models for specific new tasks often requires
large amounts of labeled data, which is difficult to obtain due to high
annotation costs and data privacy concerns. To circumvent this issue, we
introduce MRI-CORE (MRI COmprehensive Representation Encoder), a vision
foundation model pre-trained using more than 6 million slices from over 110,000
MRI volumes across 18 main body locations. Experiments on five diverse object
segmentation tasks in MRI demonstrate that MRI-CORE can significantly improve
segmentation performance in realistic scenarios with limited labeled data
availability, achieving an average gain of 6.97% 3D Dice Coefficient using only
10 annotated slices per task. We further demonstrate new model capabilities in
MRI such as classification of image properties including body location,
sequence type and institution, and zero-shot segmentation. These results
highlight the value of MRI-CORE as a generalist vision foundation model for
MRI, potentially lowering the data annotation resource barriers for many
applications.

</details>


### [591] [ViT-NeBLa: A Hybrid Vision Transformer and Neural Beer-Lambert Framework for Single-View 3D Reconstruction of Oral Anatomy from Panoramic Radiographs](https://arxiv.org/abs/2506.13195)
*Bikram Keshari Parida,Anusree P. Sunilkumar,Abhijit Sen,Wonsang You*

Main category: eess.IV

TL;DR: 本文介绍ViT - NeBLa模型，可直接从单张全景X光片实现3D重建，性能优于现有方法，为牙科诊断提供经济、低辐射方案。


<details>
  <summary>Details</summary>
Motivation: 现有牙科成像方式全景X光片（PX）缺乏深度信息，锥束计算机断层扫描（CBCT）成本高、辐射大且难获取，现有重建模型有局限。

Method: 引入基于视觉Transformer的Neural Beer - Lambert模型ViT - NeBLa，包括用Vision Transformers增强NeBLa框架、采用马蹄形点采样策略、使用混合ViT - CNN架构、实现可学习哈希位置编码。

Result: 实验表明ViT - NeBLa在定量和定性上显著优于现有方法。

Conclusion: ViT - NeBLa是一种经济、低辐射的增强牙科诊断替代方案。

Abstract: Dental diagnosis relies on two primary imaging modalities: panoramic
radiographs (PX) providing 2D oral cavity representations, and Cone-Beam
Computed Tomography (CBCT) offering detailed 3D anatomical information. While
PX images are cost-effective and accessible, their lack of depth information
limits diagnostic accuracy. CBCT addresses this but presents drawbacks
including higher costs, increased radiation exposure, and limited
accessibility. Existing reconstruction models further complicate the process by
requiring CBCT flattening or prior dental arch information, often unavailable
clinically. We introduce ViT-NeBLa, a vision transformer-based Neural
Beer-Lambert model enabling accurate 3D reconstruction directly from single PX.
Our key innovations include: (1) enhancing the NeBLa framework with Vision
Transformers for improved reconstruction capabilities without requiring CBCT
flattening or prior dental arch information, (2) implementing a novel
horseshoe-shaped point sampling strategy with non-intersecting rays that
eliminates intermediate density aggregation required by existing models due to
intersecting rays, reducing sampling point computations by $52 \%$, (3)
replacing CNN-based U-Net with a hybrid ViT-CNN architecture for superior
global and local feature extraction, and (4) implementing learnable hash
positional encoding for better higher-dimensional representation of 3D sample
points compared to existing Fourier-based dense positional encoding.
Experiments demonstrate that ViT-NeBLa significantly outperforms prior
state-of-the-art methods both quantitatively and qualitatively, offering a
cost-effective, radiation-efficient alternative for enhanced dental
diagnostics.

</details>


### [592] [Simple is what you need for efficient and accurate medical image segmentation](https://arxiv.org/abs/2506.13415)
*Xiang Yu,Yayan Chen,Guannan He,Qing Zeng,Yue Qin,Meiling Liang,Dandan Luo,Yimei Liao,Zeyu Ren,Cheng Kang,Delong Yang,Bocheng Liang,Bin Pu,Ying Yuan,Shengli Li*

Main category: eess.IV

TL;DR: 提出超轻量级医学图像分割模型SimpleUNet，有三个创新点，参数少性能优，证明极端模型压缩不影响性能。


<details>
  <summary>Details</summary>
Motivation: 现代分割模型重性能轻实用性，倡导优先考虑简单和效率的设计理念，设计高性能分割模型。

Method: 提出SimpleUNet，包含部分特征选择机制、固定宽度架构、自适应特征融合模块。

Result: 16KB参数配置的SimpleUNet在多数据集上超越LBUNet等基准模型；0.67MB变体在多中心乳腺病变数据集上效率和准确率高，在皮肤病变和内窥镜息肉分割数据集上也优于现有模型。

Conclusion: 极端模型压缩无需牺牲性能，为高效准确的医学图像分割提供新见解。

Abstract: While modern segmentation models often prioritize performance over
practicality, we advocate a design philosophy prioritizing simplicity and
efficiency, and attempted high performance segmentation model design. This
paper presents SimpleUNet, a scalable ultra-lightweight medical image
segmentation model with three key innovations: (1) A partial feature selection
mechanism in skip connections for redundancy reduction while enhancing
segmentation performance; (2) A fixed-width architecture that prevents
exponential parameter growth across network stages; (3) An adaptive feature
fusion module achieving enhanced representation with minimal computational
overhead. With a record-breaking 16 KB parameter configuration, SimpleUNet
outperforms LBUNet and other lightweight benchmarks across multiple public
datasets. The 0.67 MB variant achieves superior efficiency (8.60 GFLOPs) and
accuracy, attaining a mean DSC/IoU of 85.76%/75.60% on multi-center breast
lesion datasets, surpassing both U-Net and TransUNet. Evaluations on skin
lesion datasets (ISIC 2017/2018: mDice 84.86%/88.77%) and endoscopic polyp
segmentation (KVASIR-SEG: 86.46%/76.48% mDice/mIoU) confirm consistent
dominance over state-of-the-art models. This work demonstrates that extreme
model compression need not compromise performance, providing new insights for
efficient and accurate medical image segmentation. Codes can be found at
https://github.com/Frankyu5666666/SimpleUNet.

</details>


### [593] [UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data](https://arxiv.org/abs/2506.13505)
*Vasiliki Balaska,Ioannis Tsampikos Papapetros,Katerina Maria Oikonomou,Loukas Bampis,Antonios Gasteratos*

Main category: eess.IV

TL;DR: 本文提出用于露天采矿的集成系统架构，结合无人机传感、LiDAR地形建模和深度学习目标检测生成空间信息，可集成到工业数字孪生平台，为实时扩展奠基。


<details>
  <summary>Details</summary>
Motivation: 采矿业采用数字工具，但获取高分辨率地理参考空间信息支持核心活动仍是挑战。

Method: 提出集成系统架构，结合无人机传感、LiDAR地形建模和深度学习目标检测，包含地理参考、3D重建和目标定位等步骤。

Result: 系统提供更高覆盖和自动化潜力，组件适合现实工业场景，当前以飞行后批处理模式运行。

Conclusion: 该系统为采矿业AI增强遥感发展做贡献，展示了支持态势感知和基础设施安全的可扩展地理空间数据工作流。

Abstract: The mining sector increasingly adopts digital tools to improve operational
efficiency, safety, and data-driven decision-making. One of the key challenges
remains the reliable acquisition of high-resolution, geo-referenced spatial
information to support core activities such as extraction planning and on-site
monitoring. This work presents an integrated system architecture that combines
UAV-based sensing, LiDAR terrain modeling, and deep learning-based object
detection to generate spatially accurate information for open-pit mining
environments. The proposed pipeline includes geo-referencing, 3D
reconstruction, and object localization, enabling structured spatial outputs to
be integrated into an industrial digital twin platform. Unlike traditional
static surveying methods, the system offers higher coverage and automation
potential, with modular components suitable for deployment in real-world
industrial contexts. While the current implementation operates in post-flight
batch mode, it lays the foundation for real-time extensions. The system
contributes to the development of AI-enhanced remote sensing in mining by
demonstrating a scalable and field-validated geospatial data workflow that
supports situational awareness and infrastructure safety.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [594] [ViSAGe: Video-to-Spatial Audio Generation](https://arxiv.org/abs/2506.12199)
*Jaeyeon Kim,Heeseung Yun,Gunhee Kim*

Main category: cs.SD

TL;DR: 本文提出从无声视频直接生成一阶Ambisonics空间音频的新问题，构建YT - Ambigen数据集，提出新评估指标，推出ViSAGe框架，实验表明其效果优于两阶段方法。


<details>
  <summary>Details</summary>
Motivation: 空间音频制作需复杂系统和专业知识，为解决从无声视频直接生成一阶Ambisonics空间音频的问题。

Method: 构建YT - Ambigen数据集，提出基于音频能量图和显著性指标的评估指标，使用ViSAGe框架，利用CLIP视觉特征、自回归神经音频编解码模型结合方向和视觉引导。

Result: ViSAGe生成合理连贯的一阶Ambisonics，优于两阶段方法，能生成时间对齐、适应视点变化的高质量空间音频。

Conclusion: ViSAGe在从无声视频生成一阶Ambisonics空间音频任务中表现良好，具有有效性和优势。

Abstract: Spatial audio is essential for enhancing the immersiveness of audio-visual
experiences, yet its production typically demands complex recording systems and
specialized expertise. In this work, we address a novel problem of generating
first-order ambisonics, a widely used spatial audio format, directly from
silent videos. To support this task, we introduce YT-Ambigen, a dataset
comprising 102K 5-second YouTube video clips paired with corresponding
first-order ambisonics. We also propose new evaluation metrics to assess the
spatial aspect of generated audio based on audio energy maps and saliency
metrics. Furthermore, we present Video-to-Spatial Audio Generation (ViSAGe), an
end-to-end framework that generates first-order ambisonics from silent video
frames by leveraging CLIP visual features, autoregressive neural audio codec
modeling with both directional and visual guidance. Experimental results
demonstrate that ViSAGe produces plausible and coherent first-order ambisonics,
outperforming two-stage approaches consisting of video-to-audio generation and
audio spatialization. Qualitative examples further illustrate that ViSAGe
generates temporally aligned high-quality spatial audio that adapts to
viewpoint changes.

</details>


### [595] [SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes](https://arxiv.org/abs/2506.12222)
*Tony Alex,Sara Ahmed,Armin Mustafa,Muhammad Awais,Philip JB Jackson*

Main category: cs.SD

TL;DR: 提出SSLAM方法，可提升模型在多声道音频上的性能，在标准音频SSL基准测试中表现良好，在多声道数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有音频自监督学习（SSL）模型在多声道音频场景泛化能力未充分探索，其实用性存疑，需改进。

Method: 引入Self - Supervised Learning from Audio Mixtures (SSLAM)，在标准音频SSL基准数据集评估，与SOTA方法在多声道数据集对比分析。

Result: SSLAM提升了多声道音频性能，在标准音频SSL基准测试中维持或超越原有表现，在AudioSet - 2M上提升3.9%，在多声道数据集线性评估和微调中有高达9.1%的提升。

Conclusion: SSLAM在多声道音频和标准音频SSL基准测试中均表现出色，是音频SSL研究的新方向。

Abstract: Self-supervised pre-trained audio networks have seen widespread adoption in
real-world systems, particularly in multi-modal large language models. These
networks are often employed in a frozen state, under the assumption that the
SSL pre-training has sufficiently equipped them to handle real-world audio.
However, a critical question remains: how well do these models actually perform
in real-world conditions, where audio is typically polyphonic and complex,
involving multiple overlapping sound sources? Current audio SSL methods are
often benchmarked on datasets predominantly featuring monophonic audio, such as
environmental sounds, and speech. As a result, the ability of SSL models to
generalize to polyphonic audio, a common characteristic in natural scenarios,
remains underexplored. This limitation raises concerns about the practical
robustness of SSL models in more realistic audio settings. To address this gap,
we introduce Self-Supervised Learning from Audio Mixtures (SSLAM), a novel
direction in audio SSL research, designed to improve, designed to improve the
model's ability to learn from polyphonic data while maintaining strong
performance on monophonic data. We thoroughly evaluate SSLAM on standard audio
SSL benchmark datasets which are predominantly monophonic and conduct a
comprehensive comparative analysis against SOTA methods using a range of
high-quality, publicly available polyphonic datasets. SSLAM not only improves
model performance on polyphonic audio, but also maintains or exceeds
performance on standard audio SSL benchmarks. Notably, it achieves up to a
3.9\% improvement on the AudioSet-2M (AS-2M), reaching a mean average precision
(mAP) of 50.2. For polyphonic datasets, SSLAM sets new SOTA in both linear
evaluation and fine-tuning regimes with performance improvements of up to 9.1\%
(mAP).

</details>


### [596] [Style-based Composer Identification and Attribution of Symbolic Music Scores: a Systematic Survey](https://arxiv.org/abs/2506.12440)
*Federico Simonetta*

Main category: cs.SD

TL;DR: 本文对符号音乐曲谱中基于风格的作曲家识别和作者归属进行系统综述，分析58篇论文，指出问题并提出未来研究指南。


<details>
  <summary>Details</summary>
Motivation: 解决该领域可靠性和可重复性不足的问题。

Method: 严格分析58篇不同历史时期发表的同行评审论文，适应术语变化进行搜索。

Result: 发现大量现有研究存在验证协议不足、过度依赖简单准确率指标等问题，强调平衡准确率等指标的重要性，还介绍特征表示和机器学习模型演变，讨论实际案例。

Conclusion: 提出一套可操作的未来研究指南，以提高相关研究的可靠性、可重复性和音乐学有效性。

Abstract: This paper presents the first comprehensive systematic review of literature
on style-based composer identification and authorship attribution in symbolic
music scores. Addressing the critical need for improved reliability and
reproducibility in this field, the review rigorously analyzes 58 peer-reviewed
papers published across various historical periods, with the search adapted to
evolving terminology. The analysis critically assesses prevailing repertoires,
computational approaches, and evaluation methodologies, highlighting
significant challenges. It reveals that a substantial portion of existing
research suffers from inadequate validation protocols and an over-reliance on
simple accuracy metrics for often imbalanced datasets, which can undermine the
credibility of attribution claims. The crucial role of robust metrics like
Balanced Accuracy and rigorous cross-validation in ensuring trustworthy results
is emphasized. The survey also details diverse feature representations and the
evolution of machine learning models employed. Notable real-world authorship
attribution cases, such as those involving works attributed to Bach, Josquin
Desprez, and Lennon-McCartney, are specifically discussed, illustrating the
opportunities and pitfalls of applying computational techniques to resolve
disputed musical provenance. Based on these insights, a set of actionable
guidelines for future research are proposed. These recommendations are designed
to significantly enhance the reliability, reproducibility, and musicological
validity of composer identification and authorship attribution studies,
fostering more robust and interpretable computational stylistic analysis.

</details>


### [597] [ANIRA: An Architecture for Neural Network Inference in Real-Time Audio Applications](https://arxiv.org/abs/2506.12665)
*Valentin Ackva,Fares Schulz*

Main category: cs.SD

TL;DR: 介绍高效跨平台库anira用于实时音频应用，对不同神经网络架构和配置进行基准测试，得出不同模型和引擎组合的性能结论。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络推理工具大多无法满足实时音频应用需求。

Method: anira支持多后端，将推理从音频回调解耦到静态线程池，内置延迟管理和基准测试能力，对三种音频效果仿真神经网络架构进行不同配置的基准测试，用统计建模分析影响因素。

Result: 无状态模型中ONNX Runtime运行时间最短，有状态模型中LibTorch性能最快，某些模型 - 引擎组合初始推理时间长且实时违规发生率高。

Conclusion: 明确不同模型在不同推理引擎下的性能表现，对实时音频应用的推理引擎选择有指导意义。

Abstract: Numerous tools for neural network inference are currently available, yet many
do not meet the requirements of real-time audio applications. In response, we
introduce anira, an efficient cross-platform library. To ensure compatibility
with a broad range of neural network architectures and frameworks, anira
supports ONNX Runtime, LibTorch, and TensorFlow Lite as backends. Each
inference engine exhibits real-time violations, which anira mitigates by
decoupling the inference from the audio callback to a static thread pool. The
library incorporates built-in latency management and extensive benchmarking
capabilities, both crucial to ensure a continuous signal flow. Three different
neural network architectures for audio effect emulation are then subjected to
benchmarking across various configurations. Statistical modeling is employed to
identify the influence of various factors on performance. The findings indicate
that for stateless models, ONNX Runtime exhibits the lowest runtimes. For
stateful models, LibTorch demonstrates the fastest performance. Our results
also indicate that for certain model-engine combinations, the initial
inferences take longer, particularly when these inferences exhibit a higher
incidence of real-time violations.

</details>


### [598] [Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV](https://arxiv.org/abs/2506.13001)
*Christian Zhou-Zheng,Philippe Pasquier*

Main category: cs.SD

TL;DR: 提出MIDI - RWKV模型用于符号音乐填充，支持边缘设备上的音乐共创，评估模型并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有自动音乐生成的端到端系统难以支持人机交互，本研究旨在解决可个性化、多轨道、长上下文和可控的符号音乐填充任务，增强计算机辅助作曲过程。

Method: 提出基于RWKV - 7线性架构的MIDI - RWKV模型，并展示在极低样本情况下微调其初始状态以实现个性化的有效方法。

Result: 对MIDI - RWKV及其状态调整进行了多项定量和定性指标评估。

Conclusion: MIDI - RWKV可实现边缘设备上高效且连贯的音乐共创，且在极低样本情况下微调初始状态可实现个性化。

Abstract: Existing work in automatic music generation has primarily focused on
end-to-end systems that produce complete compositions or continuations.
However, because musical composition is typically an iterative process, such
systems make it difficult to engage in the back-and-forth between human and
machine that is essential to computer-assisted creativity. In this study, we
address the task of personalizable, multi-track, long-context, and controllable
symbolic music infilling to enhance the process of computer-assisted
composition. We present MIDI-RWKV, a novel model based on the RWKV-7 linear
architecture, to enable efficient and coherent musical cocreation on edge
devices. We also demonstrate that MIDI-RWKV admits an effective method of
finetuning its initial state for personalization in the very-low-sample regime.
We evaluate MIDI-RWKV and its state tuning on several quantitative and
qualitative metrics, and release model weights and code at
https://github.com/christianazinn/MIDI-RWKV.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [599] [Bayesian Active Learning of (small) Quantile Sets through Expected Estimator Modification](https://arxiv.org/abs/2506.13211)
*Romain Ait Abdelmalek-Lomenech,Julien Bect,Emmanuel Vazquez*

Main category: stat.AP

TL;DR: 针对含确定和不确定输入的多元函数，提出基于高斯过程建模的贝叶斯主动学习策略估计分位数集，并用实例展示性能。


<details>
  <summary>Details</summary>
Motivation: 解决评估成本高的黑盒函数中估计分位数集的问题。

Method: 提出基于高斯过程建模的贝叶斯主动学习策略，采用基于Expected Estimator Modification（EEM）的新采样准则，结合顺序蒙特卡罗框架构建批量顺序设计。

Result: 策略在多个合成示例和涉及ROTOR37压缩机模型的工业应用案例中展示了性能。

Conclusion: 所提出的策略可用于高效估计小分位数集。

Abstract: Given a multivariate function taking deterministic and uncertain inputs, we
consider the problem of estimating a quantile set: a set of deterministic
inputs for which the probability that the output belongs to a specific region
remains below a given threshold. To solve this problem in the context of
expensive-to-evaluate black-box functions, we propose a Bayesian active
learning strategy based on Gaussian process modeling. The strategy is driven by
a novel sampling criterion, which belongs to a broader principle that we refer
to as Expected Estimator Modification (EEM). More specifically, the strategy
relies on a novel sampling criterion combined with a sequential Monte Carlo
framework that enables the construction of batch-sequential designs for the
efficient estimation of small quantile sets. The performance of the strategy is
illustrated on several synthetic examples and an industrial application case
involving the ROTOR37 compressor model.

</details>


### [600] [Enforcing tail calibration when training probabilistic forecast models](https://arxiv.org/abs/2506.13687)
*Jakob Benjamin Wessel,Maybritt Schillinger,Frank Kwasniok,Sam Allen*

Main category: stat.AP

TL;DR: 研究调整概率预测模型损失函数以改善极端事件预测可靠性，应用于英国风速预测模型，显示可改善极端事件预测校准，但存在与常见结果预测校准的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前模型若类别指定错误，预测可能未校准，而校准预测对决策及极端事件很重要，故研究调整损失函数以改善极端事件预测可靠性。

Method: 研究基于加权评分规则的损失函数，提出用尾部误校准度量来正则化损失函数，并应用于不同复杂度的英国风速预测模型。

Result: 最先进模型对极端风速的预测未校准，通过调整损失函数可改善极端事件预测校准。

Conclusion: 调整损失函数可改善极端事件预测校准，但会在极端事件和常见结果的校准预测间产生权衡。

Abstract: Probabilistic forecasts are typically obtained using state-of-the-art
statistical and machine learning models, with model parameters estimated by
optimizing a proper scoring rule over a set of training data. If the model
class is not correctly specified, then the learned model will not necessarily
issue forecasts that are calibrated. Calibrated forecasts allow users to
appropriately balance risks in decision making, and it is particularly
important that forecast models issue calibrated predictions for extreme events,
since such outcomes often generate large socio-economic impacts. In this work,
we study how the loss function used to train probabilistic forecast models can
be adapted to improve the reliability of forecasts made for extreme events. We
investigate loss functions based on weighted scoring rules, and additionally
propose regularizing loss functions using a measure of tail miscalibration. We
apply these approaches to a hierarchy of increasingly flexible forecast models
for UK wind speeds, including simple parametric models, distributional
regression networks, and conditional generative models. We demonstrate that
state-of-the-art models do not issue calibrated forecasts for extreme wind
speeds, and that the calibration of forecasts for extreme events can be
improved by suitable adaptations to the loss function during model training.
This, however, introduces a trade-off between calibrated forecasts for extreme
events and calibrated forecasts for more common outcomes.

</details>


### [601] [EUNIS Habitat Maps: Enhancing Thematic and Spatial Resolution for Europe through Machine Learning](https://arxiv.org/abs/2506.13649)
*Sara Si-Moussi,Stephan Hennekens,Sander Mücher,Wanda De Keersmaecker,Milan Chytrý,Emiliano Agrillo,Fabio Attorre,Idoia Biurrun,Gianmaria Bonari,Andraž Čarni,Renata Ćušterevska,Tetiana Dziuba,Klaus Ecker,Behlül Güler,Ute Jandt,Borja Jiménez-Alfaro,Jonathan Lenoir,Jens-Christian Svenning,Grzegorz Swacha,Wilfried Thuiller*

Main category: stat.AP

TL;DR: 本文为260种EUNIS栖息地类型提供空间预测、验证及不确定性分析，生成欧洲栖息地地图，预测表现良好。


<details>
  <summary>Details</summary>
Motivation: 满足对详细准确栖息地信息的需求，支持欧洲自然保护政策和实施自然恢复法。

Method: 使用集成机器学习模型，结合高分辨率卫星图像、气候、地形和土壤变量，采用空间块交叉验证。

Result: 生成100米分辨率欧洲栖息地地图，预测在验证数据集上表现良好，不同栖息地类型在召回率和精确度上有权衡。

Conclusion: 该产品对保护和恢复目的特别有用。

Abstract: The EUNIS habitat classification is crucial for categorising European
habitats, supporting European policy on nature conservation and implementing
the Nature Restoration Law. To meet the growing demand for detailed and
accurate habitat information, we provide spatial predictions for 260 EUNIS
habitat types at hierarchical level 3, together with independent validation and
uncertainty analyses.
  Using ensemble machine learning models, together with high-resolution
satellite imagery and ecologically meaningful climatic, topographic and edaphic
variables, we produced a European habitat map indicating the most probable
EUNIS habitat at 100-m resolution across Europe. Additionally, we provide
information on prediction uncertainty and the most probable habitats at level 3
within each EUNIS level 1 formation. This product is particularly useful for
both conservation and restoration purposes.
  Predictions were cross-validated at European scale using a spatial block
cross-validation and evaluated against independent data from France (forests
only), the Netherlands and Austria. The habitat maps obtained strong predictive
performances on the validation datasets with distinct trade-offs in terms of
recall and precision across habitat formations.

</details>
