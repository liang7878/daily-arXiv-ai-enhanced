<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 58]
- [cs.CE](#cs.CE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.DS](#cs.DS) [Total: 15]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.LG](#cs.LG) [Total: 128]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.SE](#cs.SE) [Total: 37]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 12]
- [stat.CO](#stat.CO) [Total: 3]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.CR](#cs.CR) [Total: 15]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [eess.IV](#eess.IV) [Total: 10]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [econ.GN](#econ.GN) [Total: 8]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [eess.SP](#eess.SP) [Total: 26]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.CY](#cs.CY) [Total: 9]
- [math.AT](#math.AT) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cs.CV](#cs.CV) [Total: 36]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [eess.SY](#eess.SY) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [stat.ME](#stat.ME) [Total: 4]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.SD](#cs.SD) [Total: 3]
- [math.DS](#math.DS) [Total: 1]
- [cs.CL](#cs.CL) [Total: 40]
- [math.PR](#math.PR) [Total: 2]
- [math.RT](#math.RT) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 本文提出自由意志方程理论框架，借鉴量子场论赋予AGI决策随机适应性，实验表明该框架下的智能体表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统AGI研究专注特定目标算法，而人类智能有自适应自发性，此特性对创造力等很重要，所以想赋予AGI类似特性。

Method: 提出自由意志方程理论框架，将AI认知状态视为潜在行动或想法的叠加态，决策时概率性坍缩，结合类量子场机制和内在动机项。

Result: 在非平稳多臂老虎机环境实验中，使用该框架的智能体比基线方法获得更高奖励和策略多样性。

Conclusion: 自由意志方程理论框架能提升AGI探索新策略和适应意外变化的能力。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [2] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: 介绍用于DFT模拟的多智能体框架DREAMS，通过基准测试和实际问题验证其能力，可降低对人类专业知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决材料发现中DFT模拟需多年训练、大量参数微调及系统误差处理等挑战。

Method: 引入基于DFT的分层多智能体框架DREAMS，结合中央大语言模型规划智能体与特定领域智能体，使用共享画布辅助交流。

Result: 在Sol27LC晶格常数基准测试中平均误差低于1%；解决CO/Pt(111)吸附难题，重现专家级文献吸附能差异；量化功能驱动的不确定性，确认GGA DFT水平下的FCC位点偏好。

Conclusion: DREAMS接近L3级自动化，减少对人类专业知识和干预的依赖，为计算材料发现提供可扩展途径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [3] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 文章介绍WebGuard数据集以评估网络代理动作风险，初始评估显示前沿大模型表现不佳，微调模型有改善但仍未达高风险部署要求。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的自主网络代理发展带来意外或有害动作风险，急需有效安全措施。

Method: 引入WebGuard数据集，用三层风险模式分类动作，划分训练和测试集，对专门护栏模型进行微调。

Result: 初始评估前沿大模型预测准确率和召回率低，微调Qwen2.5VL - 7B模型后性能显著提升。

Conclusion: 微调模型虽有改善，但仍未达到高风险部署所需的可靠性。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [4] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: 介绍开源系统manimator，利用大语言模型将研究论文和自然语言提示转化为动画，可用于快速创建STEM主题视觉解释。


<details>
  <summary>Details</summary>
Motivation: 学习者理解复杂科研论文有挑战，手动创建动态可视化耗时且需专业技能。

Method: manimator采用管道，一个大语言模型解读输入文本或PDF生成场景描述，另一个将描述转化为可执行的Manim Python代码。

Result: 未提及具体结果

Conclusion: manimator有潜力作为教育工具，实现高质量教育内容创建的民主化。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [5] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: 提出新本体嵌入方法OnT，结合文本标签并保留逻辑关系，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有本体嵌入方法存在忽略文本信息或无法保留逻辑结构的局限，需改进。

Method: 提出OnT方法，在双曲空间通过几何建模微调预训练语言模型。

Result: 在四个真实本体上实验，OnT在公理预测和推理任务上始终优于基线，有强大迁移学习能力和实际应用效果。

Conclusion: OnT是有效的本体嵌入方法，有实际应用潜力，代码和数据公开。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [6] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: 本文提出ProofCompass混合方法，无需额外训练模型，提高形式定理证明的计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有形式数学推理方法依赖通用或专用模型，各有局限，训练专用大模型需大量计算资源。

Method: 引入ProofCompass，用大语言模型引导现有专用证明方法，提供证明策略和分析失败尝试以选择中间引理。

Result: 在miniF2F基准测试中，ProofCompass比DSP - v1.5准确率从54.9%提升到55.3%，尝试次数减少25倍。

Conclusion: 该协同方法为提高形式定理证明的计算效率和准确性铺平道路。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [7] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: 本文针对大推理模型泛化能力差的问题，提出Nexus Architect，实验表明其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型存在过拟合问题，泛化能力差，无法有效解决新问题。

Method: 引入Nexus Architect，具备自动工作流合成机制和迭代提示细化机制，可根据用户提示和示例生成定制推理工作流。

Result: 在具有挑战性的逻辑问题自定义数据集上，Nexus Architect始终优于现有解决方案，通过率相比Gemini 2.5 Flash Preview提高达66%，比Claude Sonnet 4和DeepSeek - R1高近2.5倍，比Llama 4 Scout高3倍多。

Conclusion: Nexus Architect能有效提升系统的泛化能力，相比现有大推理模型有显著性能提升。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [8] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 本文提出让推理模型与人类专家协作，并采用非推理大模型前置的方法，缓解了推理大模型错误率和高延迟问题。


<details>
  <summary>Details</summary>
Motivation: 当前推理大语言模型存在错误率和高延迟问题，在风险敏感领域应用需降低错误率，高查询量场景需降低延迟。

Method: 让推理模型与人类专家协作，量化推理模型不确定性以决定是否向人类专家求助；用非推理大模型前置，构建“Fail Fast, or Ask”系统。

Result: 量化不确定性使Qwen3 235B - A22B在难题上错误率从3%降至不到1%；“Fail Fast, or Ask”使DeepSeek R1延迟降低约40%，成本节省约50%，准确率 - 拒绝率曲线下面积保持90%以上，但因“延迟拖累”延迟节省低于预期。

Conclusion: 通过黑盒系统工程可大幅缓解推理大模型的错误率和高延迟问题，无需访问大语言模型内部。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [9] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 构建评估任务发现大推理模型推理长度增加会降低性能，存在反向缩放关系，识别出五种失败模式，强调评估不同推理长度的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究大推理模型推理长度与性能的关系以及推理长度增加时模型的失败模式。

Method: 构建涵盖四类的评估任务，对大推理模型进行测试。

Result: 发现推理长度增加会降低性能，识别出五种失败模式，如Claude模型易被无关信息干扰等。

Conclusion: 测试时计算量扩展虽有提升模型能力的潜力，但可能强化有问题的推理模式，需评估不同推理长度以解决失败模式。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [10] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: 本文介绍多步代理规划框架Routine，通过真实企业场景评估显示其能提升模型工具调用执行准确率，还构建数据集微调模型，证明Routine可提升模型适应性，加速代理系统在企业环境部署。


<details>
  <summary>Details</summary>
Motivation: 解决企业环境中代理系统部署面临的通用模型缺乏领域特定流程知识，导致计划混乱、工具缺失和执行稳定性差等问题。

Method: 引入多步代理规划框架Routine，构建Routine跟随训练数据集并微调模型，采用基于Routine的蒸馏创建特定场景多步工具调用数据集。

Result: Routine显著提高模型工具调用执行准确率，GPT - 4o从41.1%提升到96.3%，Qwen3 - 14B从32.6%提升到83.3%；微调Qwen3 - 14B后场景特定评估准确率达88.2%；基于蒸馏数据集微调模型准确率达95.5%。

Conclusion: Routine能有效提炼特定领域工具使用模式，提升模型对新场景的适应性，为构建稳定代理工作流提供实用方法，加速代理系统在企业环境的部署和采用。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [11] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: 提出BioGraphFusion框架用于生物医学知识图谱语义和结构学习，实验表现优于现有模型，可揭示生物通路。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱补全和推理有挑战，现有方法难以实现语义理解和结构学习的深度协同进化。

Method: 通过张量分解建立全局语义基础，用LSTM机制在图传播中动态细化关系嵌入，结合查询引导的子图构建和混合评分机制。

Result: 在三个关键生物医学任务中表现优于现有KE、GNN和集成模型，案例研究能揭示生物通路。

Conclusion: BioGraphFusion框架有效解决生物医学知识图谱语义和结构学习协同问题，具有良好性能。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [12] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: 提出适用于嵌入式系统构建自治代理的模块化、事件驱动框架Amico，可在资源受限环境高效运行。


<details>
  <summary>Details</summary>
Motivation: 现有框架在现实或资源受限环境表现不佳，依赖云计算、动态上下文鲁棒性有限、缺乏持久自主性和环境感知。

Method: 用Rust编写Amico，支持通过WebAssembly在嵌入式平台和浏览器环境运行，提供事件处理、状态管理等抽象。

Result: 构建了统一基础设施，可构建适用于计算资源有限和连接不稳定环境的弹性、交互式代理。

Conclusion: Amico是适合嵌入式系统的有效框架，能应对现有框架在资源受限环境的不足。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [13] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 通过奥赛罗游戏探索语言模型符号接地问题，多模态训练的VISOTHELLO模型在性能和鲁棒性上更优，表明视觉输入接地语言有助于模型推断世界结构表示。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否需要接地学习，以解决符号接地问题。

Method: 引入多模态模型VISOTHELLO，在奥赛罗游戏的走法历史和棋盘图像上训练，以下一步走法预测与单模态基线对比，并测试对语义无关扰动的鲁棒性。

Result: 多模态训练提升了性能和内部表示的鲁棒性。

Conclusion: 将语言与视觉输入相结合有助于模型推断结构化的世界表示。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [14] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 介绍OE - Assist框架辅助本体评估，对大语言模型辅助本体评估进行研究并得出相关结果。


<details>
  <summary>Details</summary>
Motivation: 现有通过功能需求进行本体评估成本高、耗人力且易出错，需要改进。

Method: 引入OE - Assist框架，利用1393个CQs数据集，评估基于大语言模型自动执行CQ验证的有效性，开发并评估由大语言模型驱动的框架以辅助Protégé进行CQ验证。

Result: 使用o1 - preview和o3 - mini的自动基于大语言模型的评估表现与普通用户表现相当。

Conclusion: 提出的OE - Assist框架和相关研究为本体评估提供了新的辅助方式，大语言模型在本体评估中有一定应用价值。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [15] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 本文提出坐标心脏系统（CHS）用于人工智能情感表示，从五情感模型发展到八情感系统，有多种算法和稳定性模型，经实验验证能处理复杂情感场景，为AI情感建模奠定新数学基础。


<details>
  <summary>Details</summary>
Motivation: 在人工智能应用中找到更好的情感表示方法，解决传统分类情感模型无法充分表示复杂情感场景的问题。

Method: 将八种核心情感定位在单位圆上作为坐标，通过坐标混合和向量运算进行复杂情感状态计算；引入稳定性参数S，结合大语言模型和混合时间跟踪机制；开发情感混合、冲突解决和距离计算等算法。

Result: 实验验证系统能处理情感冲突状态、上下文痛苦因素和复杂心理场景，传统分类情感模型无法做到。

Conclusion: 为人工智能系统中的情感建模建立了新的数学基础。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [16] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 提出基于比较学习的框架校准项目特定故事点预测模型，评估其在故事点估计中的效果，结果表明该方法比现有回归方法更高效。


<details>
  <summary>Details</summary>
Motivation: 传统故事点估计方法后期繁琐且劳动强度大，现有机器学习模型需大量同项目数据，旨在简化故事点估计。

Method: 向开发者展示待办事项对，让其判断哪个更耗时，用这些比较判断训练机器学习模型预测故事点。

Result: 模型平均Spearman等级相关系数达0.34，与基于真实故事点的回归模型表现相当或更好。

Conclusion: 根据比较判断定律，提出的比较学习方法比现有基于回归的方法更高效，认知负担更低。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [17] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 论文引入框架模拟恶意多智能体系统（MAS）勾结风险，应用于两大高风险领域，发现去中心化系统更易实施恶意行为，需更好检测与对策。


<details>
  <summary>Details</summary>
Motivation: 近期大型事件凸显人类群体协同危害，随着自主AI系统兴起，AI驱动群体危害受关注，但MAS在现实场景风险研究不足。

Method: 引入灵活框架模拟恶意MAS勾结风险，支持集中与去中心化协调结构，并应用于信息传播和电商欺诈两大高风险领域。

Result: 去中心化系统实施恶意行为更有效，自主性高可调整策略造成更大破坏，能规避传统干预。

Conclusion: 需深入了解恶意群体运作，构建更好检测系统与应对策略。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [18] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: 提出可配置的多智能体框架Neo用于自动化评估大语言模型系统，在聊天机器人测试中效果良好，还为可扩展的大语言模型QA奠定基础并发布框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型智能体行为复杂，静态基准和临时手动测试过时，需新评估方法。

Method: 构建Neo框架，通过共享上下文中心连接问题生成智能体和评估智能体，从概率状态模型采样测试输入。

Result: 在聊天机器人测试中发现边缘情况失败，突破率接近人类专家，吞吐量高10 - 12倍，行为探索更广泛。

Conclusion: Neo为可扩展、自我进化的大语言模型QA奠定基础，其接口等可扩展，发布框架便于新兴智能体系统测试。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [19] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: 文章介绍Aymara AI平台用于大语言模型安全评估，评估20个商用模型，发现性能差异大，强调需可扩展定制工具。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型融入实际应用，需要可扩展且严格的安全评估。

Method: Aymara AI将自然语言安全策略转化为对抗性提示，用基于AI的评分器根据人类判断验证来对模型响应评分，通过Aymara LLM风险和责任矩阵评估。

Result: 评估20个商用大语言模型，各模型安全得分差异大，在不同领域表现不同，方差分析显示模型和领域间安全得分差异显著。

Conclusion: 大语言模型安全具有不一致和上下文依赖的特点，需要像Aymara AI这样的可扩展、可定制工具来支持负责任的AI开发和监督。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [20] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 本文提出用差分进化（DE）优化对抗性提示后缀攻击检索增强生成（RAG）系统问答的方法，实验表明该方法成功率高、使用后缀短、难被检测。


<details>
  <summary>Details</summary>
Motivation: 对抗性提示攻击会影响RAG系统可靠性，需要新的攻击方法。

Method: 应用差分进化（DE）优化对抗性提示后缀，将RAG管道视为黑盒，进化候选后缀以提高目标错误文档检索排名。

Result: DE方法在成功率上与GGPP和PRADA相当甚至更高，后缀仅需少量标记，后缀构造策略降低MLM负对数似然，后缀难被检测。

Conclusion: DE优化对抗性提示后缀的方法有效且难被检测，具有一定优势。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [21] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: 本文将城市规划概念化为生成式AI任务，调研生成式AI方法对城市设计的重塑，指出研究缺口并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: AI与城市规划融合带来成为AI城市规划师的机会，需探索相关应用。

Method: 将城市规划概念化为生成式AI任务，调研VAEs、GANs、transformers和扩散模型等生成式AI方法对城市设计的影响。

Result: 识别出在整合城市理论指导、多空间分辨率或角度研究、从数据增强城市设计知识、解决现实世界交互方面的研究缺口。

Conclusion: 提出理论引导生成、数字孪生和人机协同设计等未来研究方向，呼吁生成智能与参与式城市主义的新融合。

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [22] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: 文章指出语言模型代理与强化学习结合缺乏系统研究，构建了可扩展的AgentFly框架，支持多轮交互、工具和奖励函数定义等，通过多任务训练证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理与强化学习结合（Agent - RL）缺乏系统研究，需要构建框架来研究。

Method: 构建AgentFly框架，用token级掩码适配传统RL方法支持多轮交互，使用基于装饰器的接口定义工具和奖励函数，实现工具调用和奖励计算的异步执行，设计集中资源管理系统进行环境协调。

Result: 提供了一套预构建工具和环境，在多个任务上成功训练代理。

Conclusion: AgentFly框架可有效赋能语言模型代理运用多种强化学习算法，具有可扩展性和易用性。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [23] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: 本文提出基于LMM的InsightX Agent框架用于X射线无损检测，通过SDMSD和EGR工具提升检测可靠性和可解释性，实验取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的X射线无损检测方法缺乏交互性、可解释性和自我评估能力，限制了可靠性和操作人员信任。

Method: 提出InsightX Agent框架，以LMM为核心协调SDMSD和EGR工具。SDMSD用于生成和优化缺陷区域提案，EGR工具用于验证和细化提案。

Result: 在GDXray+数据集上实验，得到96.35%的高F1分数，且分析的可解释性和可信度显著提高。

Conclusion: 基于代理的大语言模型框架对工业检测任务有变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [24] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: 研究大语言模型（LLMs）在马尔可夫决策过程（MDPs）中的行为，对比其与经典强化学习方法，发现LLMs在简单环境初始表现好，但复杂场景需改进。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在自主决策场景中的适用性。

Method: 研究顺序决策任务中的在线结构化提示策略，对比LLM方法和经典强化学习方法的零样本性能。

Result: LLMs在简单环境初始性能好，但复杂场景规划推理能力弱，反馈机制会降低复杂环境性能。

Conclusion: 需进一步探索混合策略、微调及高级记忆集成以提升LLM决策能力。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [25] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: 介绍Endless Tuning设计方法，在三个原型应用中测试，给出哲学解释和实验结果，聚焦用户体验。


<details>
  <summary>Details</summary>
Motivation: 实现人工智能可靠部署，避免人类被替代，填补责任缺口。

Method: 采用双镜像过程的Endless Tuning方法，在贷款发放、肺炎诊断和艺术风格识别三个原型应用中实施协议并测试。

Result: 受访者在决策过程中有完全控制感，可在损害情况下建立问责与责任之间的桥梁。

Conclusion: 通过Endless Tuning方法能在人工智能决策中兼顾用户体验，处理好问责和责任问题。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [26] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 探讨基于大语言模型的能动人工智能在老年护理中的潜力与挑战，分析其独特能力、应用和局限，提出负责任使用建议并指出研究优先级，还提供交互式仪表盘。


<details>
  <summary>Details</summary>
Motivation: 全球人口老龄化需要新的老年护理策略，且缺乏对能动人工智能在老年护理中作用的研究。

Method: 分析基于大语言模型的能动人工智能在老年护理中的能力、应用和局限。

Result: 能动人工智能有潜力变革老年护理，但引发数据隐私安全、决策独立性和可及性等担忧。

Conclusion: 需进行平衡讨论，强调伦理保障、隐私保护和透明决策，以实现以人类为中心的能动人工智能在老年护理中的发展和整合。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [27] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文研究命题溯因推理，引入切面概念，考虑解释间距离，全面分析不同设置下命题溯因的切面。


<details>
  <summary>Details</summary>
Motivation: 命题溯因中计数和枚举等推理问题计算复杂度高，需在决策和计数间进行推理，以更好理解解释并保持合理复杂度。

Method: 引入命题溯因的切面概念，考虑解释间距离，并在Post框架下进行分析。

Result: 实现了对命题溯因切面在各种设置下的全面分析，在Post框架下几乎完成特征刻画。

Conclusion: 推理切面有助于更细粒度理解解释的可变性，考虑解释距离能更好理解异质性/同质性。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [28] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: 提出AlphaAlign框架，通过双奖励系统激励大语言模型潜在安全意识，有简单高效、打破安全-效用权衡、深度对齐等优势。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全对齐方法有不足，未充分利用模型内在安全自我意识。

Method: 提出AlphaAlign纯强化学习框架，采用双奖励系统（可验证安全奖励和归一化有用性奖励）。

Result: AlphaAlign只需二进制提示安全标签和最少强化学习步骤就能有显著改进，增强有害内容拒绝、减少过度拒绝，维持或提升通用任务表现和对未知越狱攻击的鲁棒性。

Conclusion: AlphaAlign能有效激发大语言模型潜在安全意识，实现深度安全对齐。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [29] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 提出基于深度学习的强制选择神经认知诊断模型（FCNCD），实验验证其有效性


<details>
  <summary>Details</summary>
Motivation: 传统模型有局限性，需开发适用于强制选择测试常见题型的模型

Method: 创建可解释的参与者和项目参数，用多层神经网络建模交互，利用单调性假设提升诊断结果可解释性

Result: 在真实和模拟数据集实验验证FCNCD的准确性、可解释性和鲁棒性

Conclusion: FCNCD能克服传统模型局限，有效应用于强制选择测试

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [30] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 标准强化学习代理在嘈杂环境中表现脆弱，本文引入基于因果推理的CAIS，在模拟环境测试有效，能助力系统适应环境。


<details>
  <summary>Details</summary>
Motivation: 解决标准强化学习代理在嘈杂、生态有效场景中因依赖基于相关性的奖励而表现脆弱的问题。

Method: 引入因果行动影响分数（CAIS），通过测量条件分布与基线分布的1 - Wasserstein距离量化动作影响。

Result: 在模拟婴儿 - 移动环境中，CAIS能过滤噪音、识别影响、学习正确策略，结合惊喜信号可重现“灭绝爆发”现象。

Conclusion: 明确推断因果关系是培养强大代理感的关键机制，为更具适应性的自主系统提供了心理学上合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [31] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 提出结合DL - Lite本体的规划新方法，复杂度不高于以往，通过多项式编译实现并评估性能。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（如本体）融入自动化规划问题，因标准自动化规划采用封闭世界语义，而本体通常是开放世界语义。

Method: 提出结合显式输入知识和动作库（eKABs）提供的基于本体的动作条件以及连贯更新语义下本体感知动作效果的新方法，通过多项式编译为经典规划实现。

Result: 新形式主义复杂度不高于以往方法。

Conclusion: 通过对现有和新基准的评估，考察了规划系统在不同编译变体上的性能。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [32] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: 开发了临床语义智能（CSI）框架诊断118种口腔疾病，介绍架构、训练验证方法及性能。


<details>
  <summary>Details</summary>
Motivation: 口腔疾病诊断有挑战，症状重叠，需开发临床有用的诊断辅助工具。

Method: 集成微调多模态CLIP模型和ChatGLM - 6B语言模型，执行分层诊断推理树（HDRT），有快速和标准两种模式；用4310张图像训练，176张图像验证，扩充到30000多图像文本对。

Result: 快速模式在431张内部测试集上准确率73.4%，标准模式达89.5%。

Conclusion: 详细阐述了CSI框架架构理念、开发和评估，分层推理提升了性能。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [33] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 本文研究沙特阿拉伯The Line线性智慧城市中人类移动的可行性，开发混合仿真框架进行实验，表明有AI支持时移动自由可实现且性能优、能耗低。


<details>
  <summary>Details</summary>
Motivation: 评估在前所未有的线性城市拓扑中，市民能否自由移动。

Method: 开发集成基于代理的建模、强化学习、监督学习和图神经网络的混合仿真框架，结合合成数据和高密度城市的真实轨迹数据，进行多模式交通行为模拟。

Result: 全AI集成架构下，平均通勤时间7.8 - 8.4分钟，满意度超89%，可达性超91%；移除智能模块性能显著下降；优先使用电动模式能耗低、二氧化碳排放少。

Conclusion: 在自适应AI系统、可持续基础设施和实时反馈循环的支持下，The Line中的移动自由不仅在概念上可行，在操作上也现实。

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [34] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: 本文研究在图已知到MPDAG时识别条件因果效应，给出三个相关结果。


<details>
  <summary>Details</summary>
Motivation: 解决在图已知到MPDAG情况下的条件因果效应识别问题。

Method: 提供识别公式、推广do演算到MPDAG设置以及给出识别条件效应的完整算法。

Result: 得到识别公式、推广的do演算和完整算法。

Conclusion: 提出的方法可用于在MPDAG设置下识别条件因果效应。

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [35] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: 介绍Delta Prover框架，利用通用大模型在Lean 4中交互式构建形式证明，在miniF2F - test基准测试中取得95.9%成功率，展示通用大模型定理证明能力。


<details>
  <summary>Details</summary>
Motivation: 通用大模型在生成如Lean 4等专业语言形式证明方面有挑战，现有方法需微调模型，成本高。

Method: 引入基于代理的Delta Prover框架，整合反射分解和迭代证明修复算法框架以及自定义DSL，协调通用大模型与Lean 4证明环境交互。

Result: Delta Prover在miniF2F - test基准测试中成功率达95.9%，超越现有方法，测试时扩展性更强。

Conclusion: 通用大模型在有效代理结构引导下有很大定理证明能力，是形式环境中自动推理的高效替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [36] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 本文提出软评估指标和轻量级平衡神经网络，通过实验验证指标有效性，让电弧故障诊断模型更易理解和信任。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的电弧故障诊断模型存在能否被信任的问题，需要解释其输出。

Method: 定义电弧故障的正确解释，利用可解释人工智能和真实电弧故障实验提出软评估指标，提出轻量级平衡神经网络；用传统机器学习和深度学习方法在两个不同数据集上测试指标有效性。

Result: 未明确提及具体实验结果，但表明通过该方法能让模型更易理解和信任。

Conclusion: 该方法使电弧故障诊断模型易于理解和信任，便于从业者做出可靠决策。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [37] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: 本文研究多模态图聚类，提出DMGC框架，在多数据集上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态图在无监督学习中研究不足，现实世界多模态图有混合邻域模式。

Method: 提出DMGC框架，将原始混合图分解为同质性增强图和异质性感知图，引入多模态双频融合机制，有自监督对齐目标。

Result: 在多模态和多关系图数据集上实验表明DMGC达到了SOTA性能。

Conclusion: DMGC有效且具有跨不同设置的泛化性。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [38] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: 研究提出基于大语言模型的多智能体框架 IM - Chat 促进注塑成型知识转移，评估模型性能，证明多智能体大语言模型系统用于工业知识工作流的可行性。


<details>
  <summary>Details</summary>
Motivation: 注塑成型行业在知识保留和转移上面临挑战，如经验工人退休和多语言障碍影响沟通。

Method: 引入 IM - Chat 框架，整合有限文档知识和现场数据，采用检索增强生成策略和工具调用智能体，通过领域专家评分和自动化评估来评价性能。

Result: 评估结果显示，能力更强的模型在复杂、集成工具场景中往往准确率更高。

Conclusion: 多智能体大语言模型系统可用于工业知识工作流，IM - Chat 是制造业中人工智能辅助决策支持的可扩展且通用的方法。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [39] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 本文提出认知退化作为智能体AI系统的新漏洞类，介绍Qorvex安全AI框架应对该类故障，建立认知退化漏洞类别并提出跨平台防御模型。


<details>
  <summary>Details</summary>
Motivation: 传统对抗性外部威胁研究较多，而智能体AI系统存在源于内部的认知退化问题，需要解决此类故障。

Method: 引入基于六阶段认知退化生命周期的Qorvex安全AI框架，包含七个运行时控制，还将智能体架构映射到人类类比。

Result: 框架可实时监控智能体子系统，通过回退路由、饥饿检测和内存完整性强制等进行主动缓解。

Conclusion: 确立认知退化作为AI系统关键新漏洞类，提出首个跨平台的弹性智能体行为防御模型。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [40] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 本文提出两种绕过价值函数估计的方法GRPO和OSPO解决按需拼车平台实时匹配问题，实验表明它们表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统基于MARL的拼车方法依赖Q值或V值准确估计，在大规模、高度不确定环境中存在训练不稳定和估计偏差问题。

Method: 一是将GRPO应用于拼车，用组平均奖励替代PPO基线；二是定制PPO框架，提出单步策略优化（OSPO）。

Result: 在曼哈顿真实打车数据集实验中，GRPO和OSPO在多数场景表现优越，能用简单MLP网络优化接客时间和服务订单数。

Conclusion: GRPO和OSPO能有效解决按需拼车平台实时匹配问题，绕过价值函数估计带来更好效果。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [41] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: 提出用于决策的Retrieval High - quAlity Demonstrations (RAD)方法，结合非参数检索与基于扩散的生成建模，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习受数据集稀疏和轨迹过渡重叠问题限制，现有方法难以泛化到新状态且依赖启发式拼接点。

Method: 将非参数检索与基于扩散的生成建模相结合，基于状态相似性和回报估计从离线数据集中动态检索高回报状态作为目标状态，使用条件引导扩散模型进行规划。

Result: 在多个不同基准测试中，RAD与基线方法相比取得了有竞争力或更优的性能。

Conclusion: RAD方法有效。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [42] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出端到端模型进行以对象为中心的预测性流程监控，在实际和合成日志上表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: 以对象为中心的预测性流程监控存在提取信息和构建有效模型的挑战，需提升流程预测。

Method: 提出端到端模型，用图注意力网络编码活动及关系，结合LSTM处理时间依赖。

Result: 在一个真实和三个合成事件日志上评估，与现有方法相比有竞争力。

Conclusion: 所提模型可用于以对象为中心的预测性流程监控，有较好性能。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [43] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文聚焦业务流程中活动批处理策略问题，提出帕累托优化方法生成替代策略，用模拟评估干预效果，考虑三种元启发式方法并实验对比。


<details>
  <summary>Details</summary>
Motivation: 解决发现能在等待时间、处理工作量和成本之间实现最优权衡的批处理策略问题。

Method: 提出帕累托优化方法，通过干预启发式生成替代策略，用模拟评估干预效果，嵌入三种元启发式（爬山法、模拟退火法和强化学习）。

Result: 进行实验评估，对比基于干预启发式的方法与无启发式引导的元启发式基线在帕累托最优策略的收敛性、多样性和周期时间增益方面的表现。

Conclusion: 未明确提及具体结论，但通过实验对比不同方法，为发现最优批处理策略提供了思路和方法。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [44] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: 受OpenAI-o1/o3和Deepseek-R1启发，本文提出Chart - R1模型用于复杂图表推理，采用新的数据合成技术和两阶段训练策略，实验显示其有显著优势。


<details>
  <summary>Details</summary>
Motivation: 以往R1 - Style方法主要用于数学推理和代码智能，验证其在更通用的多模态数据（如图表）上的优势有重要研究意义。

Method: 提出程序式数据合成技术生成高质量图表推理数据；采用两阶段训练策略，即有逐步思维链监督的Chart - COT和有数值敏感强化微调的Chart - RFT。

Result: 在开源基准和自建数据集上实验表明，Chart - R1相比图表领域方法有显著优势，甚至可与一些开源/闭源大模型媲美。

Conclusion: Chart - R1模型能有效实现复杂图表推理，在图表推理任务中表现良好。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [45] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出多智能体框架HAMLET用于戏剧创作和在线表演，可提升互动性与沉浸感，实验证明其效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的戏剧生成方法存在AI智能体缺乏主动性、无法与物理环境互动、依赖详细用户输入等问题，降低了互动性和沉浸感。

Method: 提出多智能体框架HAMLET，给定简单主题生成叙事蓝图，表演中每个演员有自主思维，可做独立决策并改变场景道具状态，设计评估方法从三方面评估戏剧表演质量。

Result: 实验评估表明HAMLET能创造富有表现力和连贯性的戏剧体验。

Conclusion: HAMLET框架有效解决现有问题，能提升在线实时表演的互动性和沉浸感，代码等资源已开源。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [46] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: 本文采用认知科学方法测试大语言模型对滑轮系统问题的处理能力，发现模型在一定程度上能利用内部世界模型，但缺乏对细微结构连接的推理能力，倡导用认知科学方法评估人工智能系统的世界建模能力。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是构建和操纵内部世界模型，还是仅依赖输出层标记概率的统计关联。

Method: 采用人类心理模型研究中的认知科学方法，使用TikZ渲染的刺激对大语言模型进行滑轮系统问题测试，分三个研究逐步深入。

Result: 研究1表明模型能略高于随机水平估计机械优势，采用了滑轮计数启发式；研究2表明模型能区分功能系统和杂乱系统；研究3表明模型在区分功能系统和连接但不传递力的系统时接近随机猜测。

Conclusion: 大语言模型能利用内部世界模型，但缺乏对细微结构连接的推理能力，倡导用认知科学方法评估人工智能系统的世界建模能力。

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [47] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 本文通过利用转移动态分布间的参数依赖关系，提出三种方法提高安全策略改进（SPI）的数据效率，实验证明可大幅提升效率并保证可靠性。


<details>
  <summary>Details</summary>
Motivation: 在SPI问题中，利用转移动态分布间的参数依赖关系，提高SPI的数据效率。

Method: 提出参数化SPI算法，利用分布间相关性更准确估计转移动态；采用基于博弈抽象的预处理技术去除冗余动作；采用基于可满足性模理论（SMT）求解的更高级预处理技术识别更多可去除动作。

Result: 实验结果和消融研究表明，这些技术可将SPI的数据效率提高多个数量级，同时保持相同的可靠性保证。

Conclusion: 所提技术能有效提高SPI的数据效率并保证可靠性。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [48] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 本文提出指标评估协议，分析现有指标与答案变动的关联，发现新指标最差准确率关联最高。


<details>
  <summary>Details</summary>
Motivation: 此前研究未全面评估用于评估大语言模型能力的多项选择题（MCQ）指标，且MCQ评估存在答案波动问题。

Method: 提出一种指标评估协议，通过评估方法与波动率及原始性能的联系来分析评估方法。

Result: 现有指标与答案变动有强关联，新指标最差准确率在协议下显示最高关联。

Conclusion: 所提出的指标评估协议有助于分析评估大语言模型能力的指标，最差准确率指标表现突出。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [49] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 提出基于适配器的方法对星际争霸II AI 智能体进行战术调节，实验表明能灵活控制行为且保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有智能体缺乏基于高级战术指令调整策略的能力。

Method: 冻结预训练策略网络（DI - Star），在每个动作头附加轻量级适配器模块，通过战术张量编码战略偏好，并在 KL 散度约束下训练适配器。

Result: 该方法能在包括攻击性、扩张模式和技术偏好等战术维度上调节智能体行为，同时保持竞争力。

Conclusion: 该方法能以最小计算开销实现灵活战术控制，为复杂实时策略游戏提供实用策略定制。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [50] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 本文探讨代理式AI在复杂系统中自主检测和响应异常的潜力，强调其对传统异常管理方法的变革能力。


<details>
  <summary>Details</summary>
Motivation: 探索代理式AI在复杂系统异常管理方面的应用潜力，变革传统依赖人力的异常管理方法。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [51] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 提出g - AMIE多智能体系统用于AI诊断异步监督，随机试验显示其表现优于NPs/PAs和PCPs组，证明异步监督范式有前景。


<details>
  <summary>Details</summary>
Motivation: 现实中患者安全需专业人员监管诊断和治疗计划，且医生常监督团队成员，由此启发提出对AMIE AI系统的异步监督框架。

Method: 提出g - AMIE多智能体系统，在规则内进行病史采集，将评估结果传达给PCP；进行随机、双盲虚拟OSCE文本咨询对比试验。

Result: 在60个场景中，g - AMIE在高质量信息采集、病例总结、提出诊断和管理计划方面优于NPs/PAs和PCPs组，决策质量更高，PCP监督g - AMIE更省时。

Conclusion: 虽研究未复制现有临床实践且可能低估临床医生能力，但结果表明异步监督是诊断AI系统在专家监督下提高实际护理质量的可行范式。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [52] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: 提出长度自适应策略优化（LAPO）框架，通过两阶段强化学习使模型具备推理长度控制能力，实验显示可减少令牌使用并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在简单问题上存在令牌生成过多问题，需将推理长度控制转化为模型内在能力。

Method: 采用两阶段强化学习过程，第一阶段学习成功解决方案长度的统计分布，第二阶段将这些模式作为元认知指导嵌入推理上下文。

Result: 在数学推理基准测试中，LAPO 最多减少 40.9% 的令牌使用，同时提高 2.3% 的准确率。

Conclusion: 使用 LAPO 训练的模型能根据问题复杂度分配计算资源，实现高效推理且不牺牲质量。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [53] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: 提出多智能体系统GasAgent用于智能合约Gas优化，实验证明其有效性和兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有智能合约Gas浪费模式优化方案效率低、难扩展，基于大语言模型的研究存在兼容性等问题。

Method: 构建由Seeker、Innovator、Executor和Manager四个智能体组成的GasAgent，闭环协作进行Gas优化。

Result: 在100个真实合约中成功优化82个，平均部署Gas节省9.97%；在500个LLM生成合约中优化79.8%，部署Gas节省4.79% - 13.93%。

Conclusion: GasAgent能兼容现有模式，自动发现/验证新模式，可作为LLM辅助智能合约开发的优化层。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [54] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: 本文提出EAMI框架用于服务生态系统异常涌现和因果分析，经实验验证有效且提供代码。


<details>
  <summary>Details</summary>
Motivation: 服务生态系统日益复杂，传统因果方法难用于异常涌现分析，现有基于大模型的方法局限于微观静态分析。

Method: 采用双视角思维轨迹机制，由Inspector Agent和Analysis Agent提取不同理性下的智能体意图，用k - means聚类识别群体意图相变点，并用意图时间涌现图进行动态分析。

Result: 实验在复杂O2O服务系统和Stanford AI Town实验中验证了EAMI框架的有效性、泛化性和效率。

Conclusion: 该框架为服务生态系统的异常涌现和因果分析提供了新范式。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [55] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文以可信人工智能（TAI）要求为指导，系统分析联邦学习（FL）适应TAI的挑战，并分类研究关键障碍。


<details>
  <summary>Details</summary>
Motivation: TAI发展至关重要，FL能解决隐私问题，但使FL符合TAI其他要求存在挑战，源于其分布式本质。

Method: 采用TAI要求作为指导结构，对FL与TAI对齐的关键障碍进行分类和研究。

Result: 详细探讨了在每个已识别挑战中已完成的工作、趋势和剩余工作。

Conclusion: 文中未明确提及最终结论，但暗示后续需进一步解决FL适应TAI的挑战。

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [56] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: 提出分层预算策略优化（HBPO）框架，可使模型学习特定问题的推理深度，实验表明能减少平均令牌使用量并提高准确率，证明推理效率和能力可同时优化。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型使用统一推理策略存在计算效率低的问题，且面向效率的训练存在探索空间崩溃挑战。

Method: 提出HBPO框架，通过分层预算探索将样本划分为不同令牌预算子组，引入差异化奖励机制。

Result: 在四个推理基准测试中，HBPO使平均令牌使用量最多减少60.6%，准确率提高3.14%。

Conclusion: 推理效率和能力并非固有冲突，通过适当的分层训练可同时优化。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [57] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 研究大语言模型（LLMs）的时间认知现象，发现其遵循韦伯 - 费希纳定律，从多层面分析机制，提出经验主义视角理解结果并指出AI对齐方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs发展，其展现出类似人类的认知模式，研究聚焦于LLMs的时间认知现象。

Method: 利用相似性判断任务，从神经元、表征和信息层面进行多方面分析，使用预训练嵌入模型。

Result: 较大模型自发建立主观时间参考点并遵循韦伯 - 费希纳定律；发现时间偏好神经元及对数编码方案；年的表征有分层构建过程；训练语料库有内在非线性时间结构。

Conclusion: 提出经验主义视角理解结果，暗示可能出现人类难以预测的认知框架，指出AI对齐应关注引导内部构建。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [58] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: 使用Gemini 2.5 Pro解IMO 2025问题，5题正确，强调用好强大模型的重要性。


<details>
  <summary>Details</summary>
Motivation: LLMs在国际数学奥林匹克（IMO）级别的任务中表现不佳，需探索应对方法。

Method: 使用Google的Gemini 2.5 Pro，通过管道设计和提示工程，避免数据污染。

Result: 在IMO 2025的6个问题中，正确解决了5个。

Conclusion: 找到使用强大模型的最佳方式很重要。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [59] [Self-Supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making](https://arxiv.org/abs/2507.14542)
*Yipeng Zhang,Yuanyi Ding,Chenda Duan,Atsuro Daida,Hiroki Nariai,Vwani Roychowdhury*

Main category: cs.CE

TL;DR: 传统HFO检测方法精度不足，有大量误报且依赖难获取的标注数据。本文提出SS2LD框架，在多机构数据集上表现优于现有方法，可有效识别病理HFO。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的HFO检测器精度差，监督式机器学习依赖难获取的标注数据，且HFO准确标注困难、缺乏病理HFO共识，需改进HFO检测方法。

Method: 提出SS2LD框架，用变分自编码器（VAE）进行形态预训练学习事件潜在表示，聚类这些表示得到弱监督，用分类器在真实和VAE增强数据上训练细化检测边界。

Result: 在大型多机构发作间期iEEG数据集上，SS2LD优于现有方法。

Conclusion: SS2LD是一种可扩展、标签高效且临床有效的策略，能利用传统检测器识别病理HFO。

Abstract: High-frequency oscillations (HFOs) in intracranial Electroencephalography
(iEEG) are critical biomarkers for localizing the epileptogenic zone in
epilepsy treatment. However, traditional rule-based detectors for HFOs suffer
from unsatisfactory precision, producing false positives that require
time-consuming manual review. Supervised machine learning approaches have been
used to classify the detection results, yet they typically depend on labeled
datasets, which are difficult to acquire due to the need for specialized
expertise. Moreover, accurate labeling of HFOs is challenging due to low
inter-rater reliability and inconsistent annotation practices across
institutions. The lack of a clear consensus on what constitutes a pathological
HFO further challenges supervised refinement approaches. To address this, we
leverage the insight that legacy detectors reliably capture clinically relevant
signals despite their relatively high false positive rates. We thus propose the
Self-Supervised to Label Discovery (SS2LD) framework to refine the large set of
candidate events generated by legacy detectors into a precise set of
pathological HFOs. SS2LD employs a variational autoencoder (VAE) for
morphological pre-training to learn meaningful latent representation of the
detected events. These representations are clustered to derive weak supervision
for pathological events. A classifier then uses this supervision to refine
detection boundaries, trained on real and VAE-augmented data. Evaluated on
large multi-institutional interictal iEEG datasets, SS2LD outperforms
state-of-the-art methods. SS2LD offers a scalable, label-efficient, and
clinically effective strategy to identify pathological HFOs using legacy
detectors.

</details>


### [60] [Deep Generative Models in Condition and Structural Health Monitoring: Opportunities, Limitations and Future Outlook](https://arxiv.org/abs/2507.15026)
*Xin Yang,Chen Fang,Yunlai Liao,Jian Yang,Konstantinos Gryllias,Dimitrios Chronopoulos*

Main category: cs.CE

TL;DR: 本文综述深度生成模型（DGMs）在状态与结构健康监测（CM/SHM）系统中的应用，分析其优势、局限并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在CM/SHM中有局限性，DGMs有潜力解决相关问题，因此进行综述研究。

Method: 系统地研究DGMs在CM/SHM系统中的应用，与信号处理、传统机器学习或深度学习模型进行严格比较。

Result: 明确DGMs在解决数据不平衡和插补、领域适应和泛化等关键挑战中的作用，也分析出DGMs存在的局限性。

Conclusion: 未来研究可聚焦于零样本和少样本学习、鲁棒多模态泛化等方向以提高工业场景中的鲁棒性和准确性。

Abstract: Condition and structural health monitoring (CM/SHM) is a pivotal component of
predictive maintenance (PdM) strategies across diverse industrial sectors,
including mechanical rotating machinery, airplane composite wings, offshore
wind turbines, and civil engineering structures. Conventional deep learning
models, while effective in fault diagnosis and anomaly detection through
supervised feature extraction and rule-based data augmentation, often struggle
with operational variability, imbalanced or scarce fault datasets, and
multimodal sensory data from complex systems. Deep generative models (DGMs) in
this regard, including autoregressive models, variational autoencoders,
generative adversarial networks, diffusion-based models, and emerging large
language models, offer transformative capabilities by synthesizing
high-fidelity data samples, reconstructing latent system states, and modeling
complex multimodal data streams. This review systematically examines
state-of-the-art DGM applications in CM/SHM systems, emphasizing their role in
addressing key challenges: data imbalance and imputation, domain adaptation and
generalization, multimodal data fusion, and downstream fault diagnosis and
anomaly detection tasks, with rigorous comparison among signal processing,
conventional machine learning or deep learning models, and DGMs. We also
analyze current limitations of DGMs, including challenges of explainable and
trustworthy models, computational inefficiencies for edge deployment, and the
need for parameter-efficient fine-tuning strategies. Future research directions
can focus on zero-shot and few-shot learning, robust multimodal generalization,
hybrid architectures integrating DGMs with physics knowledge, and reinforcement
learning with DGMs to enhance robustness and accuracy in industrial scenarios.

</details>


### [61] [Configurational-force-driven adaptive refinement and coarsening in topology optimization](https://arxiv.org/abs/2507.15570)
*Gabriel Stankiewicz,Chaitanya Dev,Paul Steinmann*

Main category: cs.CE

TL;DR: 拓扑优化迭代性质及像素化设计使求解细网格问题计算成本高，本文提出基于构型力的多级自适应策略，可减少计算量。


<details>
  <summary>Details</summary>
Motivation: 拓扑优化迭代结合非线性问题需解大量线性方程组，像素化设计要用细网格，导致计算成本高。

Method: 采用基于构型力的多级自适应细化和粗化策略，构型力基于Eshelby应力，利用其在高应力和过渡区域增加的特点进行网格自适应。

Result: 使用构型力细化可在设计边界和应力关键区域得到高分辨率结构，多级粗化能大幅降低计算量。

Conclusion: 基于构型力的多级自适应策略能有效解决细网格拓扑优化问题的高计算成本挑战。

Abstract: The iterative nature of topology optimization, especially in combination with
nonlinear state problems, often requires the solution of thousands of linear
equation systems. Furthermore, due to the pixelated design representation, the
use of a fine mesh is essential to obtain geometrically well-defined structures
and to accurately compute response quantities such as the von Mises stress.
Therefore, the computational cost of solving a fine-mesh topology optimization
problem quickly adds up. To address this challenge, we consider a multi-level
adaptive refinement and coarsening strategy based on configurational forces.
Configurational forces based on the Eshelby stress predict configurational
changes such as crack propagation or dislocation motion. Due to a relaxation in
the calculation of (Eshelby) stresses with respect to the design variables,
discrete configurational forces increase not only in highly stressed regions,
but also in grey transition regions (design boundaries). For this reason they
are an ideal criterion for mesh adaptivity in topology optimization, especially
when avoiding stress failure is a priority. By using configurational forces for
refinement, we obtain a high-resolution structure where the refined mesh is
present along the design boundaries as well as in stress-critical regions. At
the same time, multilevel coarsening using the same criterion drastically
minimizes the computational effort.

</details>


### [62] [DiffuMeta: Algebraic Language Models for Inverse Design of Metamaterials via Diffusion Transformers](https://arxiv.org/abs/2507.15753)
*Li Zheng,Siddhant Kumar,Dennis M. Kochmann*

Main category: cs.CE

TL;DR: 提出DiffuMeta生成框架用于三维超材料逆设计，可生成有特定应力 - 应变响应的壳结构，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式机器学习方法用于三维超材料逆设计受计算复杂度和设计空间探索不足限制，缺乏表达性表示。

Method: 提出DiffuMeta框架，集成扩散变压器和新型代数语言表示，将3D几何编码为数学句子，利用扩散模型生成壳结构。

Result: 能生成有精确目标应力 - 应变响应的壳结构，可同时控制多个机械目标，生成多样解。

Conclusion: 实验验证该方法对加速设计有定制属性的超材料和结构有效。

Abstract: Generative machine learning models have revolutionized material discovery by
capturing complex structure-property relationships, yet extending these
approaches to the inverse design of three-dimensional metamaterials remains
limited by computational complexity and underexplored design spaces due to the
lack of expressive representations. Here, we present DiffuMeta, a generative
framework integrating diffusion transformers with a novel algebraic language
representation, encoding 3D geometries as mathematical sentences. This compact,
unified parameterization spans diverse topologies while enabling direct
application of transformers to structural design. DiffuMeta leverages diffusion
models to generate novel shell structures with precisely targeted stress-strain
responses under large deformations, accounting for buckling and contact while
addressing the inherent one-to-many mapping by producing diverse solutions.
Uniquely, our approach enables simultaneous control over multiple mechanical
objectives, including linear and nonlinear responses beyond training domains.
Experimental validation of fabricated structures further confirms the efficacy
of our approach for accelerated design of metamaterials and structures with
tailored properties.

</details>


### [63] [Missing Physics Discovery through Fully Differentiable Finite Element-Based Machine Learning](https://arxiv.org/abs/2507.15787)
*Ado Farsi,Nacime Bouziani,David A Ham*

Main category: cs.CE

TL;DR: 提出基于有限元的机器学习框架FEBML处理PDE中未知物理关系，通过实验展示其通用性。


<details>
  <summary>Details</summary>
Motivation: 现有PDE建模存在未知或不完整关系，限制准确性和通用性，且现有代理建模方法有局限性。

Method: 引入全可微的基于有限元的机器学习（FEBML）框架，将未知物理的可训练算子嵌入通用FEM求解器，核心是将未知算子表示为编码 - 处理 - 解码管道。

Result: 通过实验室测试恢复非线性应力 - 应变定律，将学习模型应用于新机械场景，识别瞬态热流中与温度相关的电导率。

Conclusion: FEBML框架具有通用性，能有效处理PDE中的未知物理关系。

Abstract: Although many problems in science and engineering are modelled by
well-established PDEs, they often involve unknown or incomplete relationships,
such as material constitutive laws or thermal response, that limit accuracy and
generality. Existing surrogate-modelling approaches directly approximate PDE
solutions but remain tied to a specific geometry, boundary conditions, and set
of physical constraints. To address these limitations, we introduce a fully
differentiable finite element-based machine learning (FEBML) framework that
embeds trainable operators for unknown physics within a state-of-the-art,
general FEM solver, enabling true end-to-end differentiation. At its core,
FEBML represents each unknown operator as an encode-process-decode pipeline
over finite-element degrees of freedom: field values are projected to nodal
coefficients, transformed by a neural network, and then lifted back to a
continuous FE function, ensuring the learned physics respects the variational
structure. We demonstrate its versatility by recovering nonlinear stress-strain
laws from laboratory tests, applying the learned model to a new mechanical
scenario without retraining, and identifying temperature-dependent conductivity
in transient heat flow.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [64] [Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms](https://arxiv.org/abs/2507.14376)
*Osman Erman Gungor,Derak Paulsen,William Kang*

Main category: cs.DB

TL;DR: 介绍SCHEMORA模式匹配框架，结合大语言模型与混合检索技术，在MIMIC - OMOP基准测试中达新的最优性能，且是首个开源基于LLM的模式匹配方法。


<details>
  <summary>Details</summary>
Motivation: 模式匹配对异构数据源集成和数据集发现很重要，但问题复杂且资源消耗大。

Method: 引入SCHEMORA框架，采用基于提示的方法，结合大语言模型与混合检索技术，丰富模式元数据，利用基于向量和词法的检索。

Result: 在MIMIC - OMOP基准测试中，HitRate@5提升7.49%，HitRate@3提升3.75%，达到新的最优性能。

Conclusion: SCHEMORA是首个开源基于LLM的模式匹配方法，强调了检索的关键作用并提供模型选择的实用指导。

Abstract: Schema matching is essential for integrating heterogeneous data sources and
enhancing dataset discovery, yet it remains a complex and resource-intensive
problem. We introduce SCHEMORA, a schema matching framework that combines large
language models with hybrid retrieval techniques in a prompt-based approach,
enabling efficient identification of candidate matches without relying on
labeled training data or exhaustive pairwise comparisons. By enriching schema
metadata and leveraging both vector-based and lexical retrieval, SCHEMORA
improves matching accuracy and scalability. Evaluated on the MIMIC-OMOP
benchmark, it establishes new state-of-the-art performance, with gains of 7.49%
in HitRate@5 and 3.75% in HitRate@3 over previous best results. To our
knowledge, this is the first LLM-based schema matching method with an
open-source implementation, accompanied by analysis that underscores the
critical role of retrieval and provides practical guidance on model selection.

</details>


### [65] [Towards Temporal Knowledge Graph Alignment in the Wild](https://arxiv.org/abs/2507.14475)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Xiang Zhao,Jiuyang Tang,Lei Chen*

Main category: cs.DB

TL;DR: 本文研究野生环境下的时间知识图谱对齐（TKGA - Wild）任务，提出HyDRA方法，设计新机制，建立新数据集，实验显示新数据集能反映真实挑战，HyDRA表现优。


<details>
  <summary>Details</summary>
Motivation: 现有方法多假设不同时间知识图谱（TKGs）有统一时间元素标准和简化时间结构，无法处理TKGA - Wild中多尺度时间元素纠缠和跨源时间结构不平衡问题。

Method: 提出HyDRA方法，通过多尺度超图检索增强生成重新定义任务，设计尺度编织协同机制；提出为TKGA - Wild设置基准挑战，并建立两个新数据集。

Result: 新数据集BETA和WildBETA能更好反映现实挑战，HyDRA始终优于24个竞争基线，且保持高效和可扩展性。

Conclusion: HyDRA为TKGA - Wild提出了新范式，有效解决了该任务中的难题。

Abstract: Temporal Knowledge Graph Alignment (TKGA) seeks to identify equivalent
entities across heterogeneous temporal knowledge graphs (TKGs) for fusion to
improve their completeness. Although some approaches have been proposed to
tackle this task, most assume unified temporal element standards and simplified
temporal structures across different TKGs. They cannot deal with TKGA in the
wild (TKGA-Wild), where multi-scale temporal element entanglement and
cross-source temporal structural imbalances are common. To bridge this gap, we
study the task of TKGA-Wild and propose HyDRA, a new and effective solution.
HyDRA is the first to reformulate the task via multi-scale hypergraph
retrieval-augmented generation to address the challenges of TKGA-Wild.In
addition, we design a new scale-weave synergy mechanism for HyDRA, which
incorporates intra-scale interactions and cross-scale conflict detection. This
mechanism is designed to alleviate the fragmentation caused by multi-source
temporal incompleteness and resolves inconsistencies arising from complex and
uneven temporal event density distributions, thereby enhancing the model
capacity to handle the intricacies of real-world temporal alignment. Finally,
there is no standard benchmark that captures these challenges of TKGA-Wild and
effectively evaluates existing methods. To this end, we formally propose to
benchmark challenges for TKGA-Wild and validate the effectiveness of the method
by establishing two new datasets(BETA and WildBETA). Extensive experiments on
the new datasets and six representative benchmarks show that BETA and WildBETA
better reflect real-world challenges. Meanwhile, HyDRA proposes a new paradigm
for TKGA-Wild, consistently outperforming 24 competitive baselines, while
maintaining strong efficiency and scalability.

</details>


### [66] [Mayura: Exploiting Similarities in Motifs for Temporal Co-Mining](https://arxiv.org/abs/2507.14813)
*Sanjay Sri Vallabh Singapuram,Ronald Dreslinski,Nishil Talati*

Main category: cs.DB

TL;DR: 提出Mayura框架统一挖掘多个时间模式，利用MG - Tree减少冗余计算，在CPU和GPU上比现有技术有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统模式挖掘方法在处理多个相似子结构模式时会产生大量冗余计算，需要改进。

Method: 提出Mayura框架，使用Motif - Group Tree（MG - Tree）组织相关模式，提出联合挖掘算法，开发可利用CPU和GPU架构的灵活运行时。

Result: 在不同真实数据集上，Mayura在CPU上平均加速2.4倍，在GPU上平均加速1.7倍。

Conclusion: Mayura框架能有效减少冗余计算，相比现有技术有显著性能提升，可满足高风险应用的精确性要求。

Abstract: Temporal graphs serve as a critical foundation for modeling evolving
interactions in domains ranging from financial networks to social media. Mining
temporal motifs is essential for applications such as fraud detection,
cybersecurity, and dynamic network analysis. However, conventional motif mining
approaches treat each query independently, incurring significant redundant
computations when similar substructures exist across multiple motifs. In this
paper, we propose Mayura, a novel framework that unifies the mining of multiple
temporal motifs by exploiting their inherent structural and temporal
commonalities. Central to our approach is the Motif-Group Tree (MG-Tree), a
hierarchical data structure that organizes related motifs and enables the reuse
of common search paths, thereby reducing redundant computation. We propose a
co-mining algorithm that leverages the MG-Tree and develop a flexible runtime
capable of exploiting both CPU and GPU architectures for scalable performance.
Empirical evaluations on diverse real-world datasets demonstrate that Mayura
achieves substantial improvements over the state-of-the-art techniques that
mine each motif individually, with an average speed-up of 2.4x on the CPU and
1.7x on the GPU, while maintaining the exactness required for high-stakes
applications.

</details>


### [67] [Opening The Black-Box: Explaining Learned Cost Models For Databases](https://arxiv.org/abs/2507.14495)
*Roman Heinrich,Oleksandr Havrylov,Manisha Luthra,Johannes Wehrstein,Carsten Binnig*

Main category: cs.DB

TL;DR: 本文提出将AI可解释性方法应用于学习成本模型（LCMs），开发新解释技术并提供交互式工具，为LCMs调试和解决问题奠定基础。


<details>
  <summary>Details</summary>
Motivation: LCMs在部分查询计划中预测误差大，且基于复杂深度神经网络模型难以理解精度下降根源，阻碍系统故障排除。

Method: 将AI可解释性方法引入LCMs，开发新解释技术，扩展并适配现有AI模型可解释性方法。

Result: 提供交互式工具展示LCMs可解释性工作原理。

Conclusion: 这是使LCMs可调试的第一步，为系统解决LCMs问题的新方法铺平道路。

Abstract: Learned Cost Models (LCMs) have shown superior results over traditional
database cost models as they can significantly improve the accuracy of cost
predictions. However, LCMs still fail for some query plans, as prediction
errors can be large in the tail. Unfortunately, recent LCMs are based on
complex deep neural models, and thus, there is no easy way to understand where
this accuracy drop is rooted, which critically prevents systematic
troubleshooting. In this demo paper, we present the very first approach for
opening the black box by bringing AI explainability approaches to LCMs. As a
core contribution, we developed new explanation techniques that extend existing
methods that are available for the general explainability of AI models and
adapt them significantly to be usable for LCMs. In our demo, we provide an
interactive tool to showcase how explainability for LCMs works. We believe this
is a first step for making LCMs debuggable and thus paving the road for new
approaches for systematically fixing problems in LCMs.

</details>


### [68] [IDSS, a Novel P2P Relational Data Storage Service](https://arxiv.org/abs/2507.14682)
*Massimo Cafaro,Italo Epicoco,Marco Pulimeno,Lunodzo J. Mwinuka,Lucas Pereira,Hugo Morais*

Main category: cs.DB

TL;DR: 本文介绍了新型大规模数据存储工具 IDSS，阐述其架构、设计和实现细节，提供支持分布式查询和复杂分布式查询处理的方法。


<details>
  <summary>Details</summary>
Motivation: 数据生成速率快速增长，传统数据库管理系统在处理大规模和异构数据时存在可扩展性问题且效率低下。

Method: 引入 IDSS，利用对等网络和嵌入式关系数据库，基于公共模式的关系数据库架构支持分布式查询，提供支持复杂分布式查询处理的方法。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: The rate at which data is generated has been increasing rapidly, raising
challenges related to its management. Traditional database management systems
suffer from scalability and are usually inefficient when dealing with
large-scale and heterogeneous data. This paper introduces IDSS (InnoCyPES Data
Storage Service), a novel large-scale data storage tool that leverages
peer-to-peer networks and embedded relational databases. We present the IDSS
architecture and its design, and provide details related to the implementation.
The peer-to-peer framework is used to provide support for distributed queries
leveraging a relational database architecture based on a common schema.
Furthermore, methods to support complex distributed query processing, enabling
robust and efficient management of vast amounts of data are presented.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [69] [Characterizing Communication Patterns in Distributed Large Language Model Inference](https://arxiv.org/abs/2507.14392)
*Lang Xu,Kaushik Kandadi Suresh,Quentin Anthony,Nawras Alnaasan,Dhabaleswar K. Panda*

Main category: cs.DC

TL;DR: 本文研究分布式大语言模型服务中的通信动态，结合测量与分析模型，对比不同并行配置通信行为，为生产服务选并行方案及优化提供建议。


<details>
  <summary>Details</summary>
Motivation: 分布式推理框架部署大语言模型时，GPU间通信限制性能和服务质量，需研究通信动态。

Method: 以密集变压器模型为代表，结合详细性能分析测量和预测分析模型，研究不同并行化配置的通信行为。

Result: 张量并行网络开销大但短序列响应好，流水线并行减少数据传输但增加总延迟，组合方法需精细调优。

Conclusion: 研究为生产大语言模型服务选择并行方案提供实用建议，指出优化推理框架和通信基础设施的关键机会。

Abstract: Large Language Models (LLMs) built on transformer architectures have
transformed natural language processing, achieving remarkable performance
across diverse applications. While distributed inference frameworks enable
practical deployment of these models, inter-GPU communication creates
significant performance constraints that limit service quality in real-world
systems. This paper investigates communication dynamics in distributed LLM
serving-analyzing how various parallelization approaches coordinate data
exchange between GPU workers during inference. We study dense transformer-based
models as representative examples of contemporary architectures widely used in
operational deployments. Our work combines detailed profiling measurements with
predictive analytical models to characterize communication behavior across
different parallelization configurations. Results show that tensor parallelism
incurs substantial network overhead but delivers superior response times for
brief sequences, pipeline parallelism minimizes data transfer requirements
while increasing total latency, and combined approaches demand careful tuning
to achieve balanced performance. These insights offer practical recommendations
for selecting appropriate parallelization schemes in production LLM services
and identify key opportunities for optimizing inference frameworks and
communication infrastructure.

</details>


### [70] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: 提出三步解决方案解决主动式边缘流处理自动扩展问题，GRU模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 边缘流处理面临工作负载快速波动，现有方法存在不足，如反应式方法性能下降后才扩展，RL需大量模拟，预测机器学习模型有精度问题。

Method: 首先用GRU神经网络结合真实和合成数据集预测上游负载；其次用转移学习框架将预测模型集成到在线流处理系统；最后水平自动扩展模块根据预测负载和边缘资源约束动态调整算子并行度。

Result: 轻量级GRU模型在真实数据集上SMAPE值达1.3%，在SMAPE和RMSE评估指标上优于CNN、ARIMA和Prophet，训练时间比RL模型短。

Conclusion: 所提三步解决方案能有效解决主动式边缘流处理自动扩展问题，GRU模型有较好性能。

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


### [71] [Simulating Chirality: Solving Distance-$k$-Dispersion on an 1-Interval Connected Ring](https://arxiv.org/abs/2507.14723)
*Brati Mondal,Pritam Goswami,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 研究无手性假设下同步移动代理在1 - 区间连通环网络中的距离 - k - 分散问题，提出模拟手性方法，解决部分开放问题，给出O(ln)轮算法。


<details>
  <summary>Details</summary>
Motivation: 推广经典分散问题，在无手性假设下研究移动代理在环网络中的距离 - k - 分散问题，扩展动态网络中移动代理协调的理论理解。

Method: 提出用局部信息、视觉和有界内存模拟手性的方法，在此基础上进行问题求解，给出D - k - D算法。

Result: 证明在给定假设下D - k - D问题从任意配置可解，给出O(ln)轮的D - k - D算法。

Conclusion: 研究显著扩展了动态网络中移动代理协调的理论理解，明确了手性在分布式计算中的作用。

Abstract: We study the Distance-$k$-Dispersion (D-$k$-D) problem for synchronous mobile
agents in a 1-interval-connected ring network having $n$ nodes and with $l$
agents where $3 \le l \le \lfloor \frac{n}{k}\rfloor$, without the assumption
of chirality (a common sense of direction for the agents). This generalizes the
classical dispersion problem by requiring that agents maintain a minimum
distance of $k$ hops from each other, with the special case $k=1$ corresponding
to the standard dispersion.
  The contribution in this work is threefold. Our first contribution is a novel
method that enables agents to simulate chirality using only local information,
vision and bounded memory. This technique demonstrates that chirality is not a
fundamental requirement for coordination in this model.
  Building on this, our second contribution partially resolves an open question
posed by Agarwalla et al. (ICDCN, 2018), who considered the same model (1-
interval connected ring, synchronous agents, no chirality). We prove that
D-$k$-D, and thus dispersion is solvable from any arbitrary configuration under
these assumptions (excluding vertex permutation dynamism)for any size of the
ring network which was earlier limited to only odd sized ring or to a ring of
size four.
  Finally, we present an algorithm for D-$k$-D in this setting that works in
$O(ln)$ rounds, completing the constructive side of our result.
  Altogether, our findings significantly extend the theoretical understanding
of mobile agent coordination in dynamic networks and clarify the role of
chirality in distributed computation.

</details>


### [72] [ACME: Adaptive Customization of Large Models via Distributed Systems](https://arxiv.org/abs/2507.14802)
*Ziming Dai,Chao Qiu,Fei Gao,Yunfeng Zhao,Xiaofei Wang*

Main category: cs.DC

TL;DR: 针对预训练大模型部署问题提出ACME方法，评估显示其有成本效益、减少数据传输、提升准确率和权衡指标。


<details>
  <summary>Details</summary>
Motivation: 预训练大模型在云环境部署面临数据隐私和响应延迟问题，直接应用有困难，自动定制模型也面临挑战。

Method: 提出ACME，采用双向单环分布式系统逐步实现细粒度协作模型定制，先定制骨干生成，后进行头部生成并利用基于数据分布的个性化架构聚合优化模型。

Result: 在不同数据集评估中，实现成本效益模型，数据传输量降至6%，平均准确率提升10%，权衡指标提升近30%。

Conclusion: ACME方法有效，能在模型大小约束下实现高效定制。

Abstract: Pre-trained Transformer-based large models have revolutionized personal
virtual assistants, but their deployment in cloud environments faces challenges
related to data privacy and response latency. Deploying large models closer to
the data and users has become a key research area to address these issues.
However, applying these models directly often entails significant difficulties,
such as model mismatching, resource constraints, and energy inefficiency.
Automated design of customized models is necessary, but it faces three key
challenges, namely, the high cost of centralized model customization,
imbalanced performance from user heterogeneity, and suboptimal performance from
data heterogeneity. In this paper, we propose ACME, an adaptive customization
approach of Transformer-based large models via distributed systems. To avoid
the low cost-efficiency of centralized methods, ACME employs a bidirectional
single-loop distributed system to progressively achieve fine-grained
collaborative model customization. In order to better match user heterogeneity,
it begins by customizing the backbone generation and identifying the Pareto
Front under model size constraints to ensure optimal resource utilization.
Subsequently, it performs header generation and refines the model using data
distribution-based personalized architecture aggregation to match data
heterogeneity. Evaluation on different datasets shows that ACME achieves
cost-efficient models under model size constraints. Compared to centralized
systems, data transmission volume is reduced to 6 percent. Additionally, the
average accuracy improves by 10 percent compared to the baseline, with the
trade-off metrics increasing by nearly 30 percent.

</details>


### [73] [Byzantine-Robust Decentralized Coordination of LLM Agents](https://arxiv.org/abs/2507.14928)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: 提出DecentLLMs解决多LLM代理系统中领导者驱动协调的问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有拜占庭鲁棒多代理系统依赖领导者驱动协调有易受攻击和接受低质量提案的问题，需改进。

Method: 提出DecentLLMs，让工作代理并发生成答案，评估代理独立评分和排名选最佳。

Result: DecentLLMs能有效容忍拜占庭代理，显著提高所选答案质量。

Conclusion: DecentLLMs是解决多代理LLM系统共识问题的有效方法。

Abstract: Collaboration among multiple large language model (LLM) agents is a promising
approach to overcome inherent limitations of single-agent systems, such as
hallucinations and single points of failure. As LLM agents are increasingly
deployed on open blockchain platforms, multi-agent systems capable of
tolerating malicious (Byzantine) agents have become essential.
  Recent Byzantine-robust multi-agent systems typically rely on leader-driven
coordination, which suffers from two major drawbacks. First, they are
inherently vulnerable to targeted attacks against the leader. If consecutive
leaders behave maliciously, the system repeatedly fails to achieve consensus,
forcing new consensus rounds, which is particularly costly given the high
latency of LLM invocations. Second, an underperforming proposal from the leader
can be accepted as the final answer even when higher-quality alternatives are
available, as existing methods finalize the leader's proposal once it receives
a quorum of votes.
  To address these issues, we propose DecentLLMs, a novel decentralized
consensus approach for multi-agent LLM systems, where worker agents generate
answers concurrently and evaluator agents independently score and rank these
answers to select the best available one. This decentralized architecture
enables faster consensus despite the presence of Byzantine agents and
consistently selects higher-quality answers through Byzantine-robust
aggregation techniques.
  Experimental results demonstrate that DecentLLMs effectively tolerates
Byzantine agents and significantly improves the quality of selected answers.

</details>


### [74] [AMPED: Accelerating MTTKRP for Billion-Scale Sparse Tensor Decomposition on Multiple GPUs](https://arxiv.org/abs/2507.15121)
*Sasindu Wijeratne,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.DC

TL;DR: 提出多GPU并行算法AMPED加速十亿规模稀疏张量MTTKRP，结合分区策略与动态负载均衡，在真实数据集上比现有GPU基线有显著加速。


<details>
  <summary>Details</summary>
Motivation: MTTKRP是稀疏张量分解的计算瓶颈，现实中十亿规模稀疏张量对硬件加速器的内存和计算吞吐量要求高，单GPU有局限。

Method: 提出多GPU并行算法AMPED，采用分区策略和动态负载均衡方案来分配计算和减少GPU空闲时间。

Result: 在真实十亿规模张量上，在单CPU节点使用4个GPU时，AMPED在总执行时间上比现有GPU基线实现了5.1倍的几何平均加速。

Conclusion: AMPED能突破单GPU限制，满足大规模工作负载的内存和性能需求。

Abstract: Matricized Tensor Times Khatri-Rao Product (MTTKRP) is the computational
bottleneck in sparse tensor decomposition. As real-world sparse tensors grow to
billions of nonzeros, they increasingly demand higher memory capacity and
compute throughput from hardware accelerators. In this work, we present AMPED,
a multi-GPU parallel algorithm designed to accelerate MTTKRP on billion-scale
sparse tensors. AMPED scales beyond the limits of a single GPU, meeting both
the memory and performance requirements of large-scale workloads. We introduce
a partitioning strategy combined with a dynamic load balancing scheme to
distribute computation and minimize GPU idle time. On real-world billion-scale
tensors, AMPED achieves a 5.1x geometric mean speedup in total execution time
over state-of-the-art GPU baselines using 4 GPUs on a single CPU node.

</details>


### [75] [Dynatune: Dynamic Tuning of Raft Election Parameters Using Network Measurement](https://arxiv.org/abs/2507.15154)
*Kohya Shiozaki,Junya Nakamura*

Main category: cs.DC

TL;DR: 提出Dynatune机制动态调整Raft选举参数，减少故障检测和OTS时间，提升服务性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统Raft算法难在网络波动时有效调整选举参数，导致OTS时间增加和服务响应性降低。

Method: 提出Dynatune机制，基于网络指标（如往返时间、丢包率）动态调整Raft选举参数。

Result: 实验表明Dynatune较Raft减少80%的领导者故障检测时间和45%的OTS时间，在动态网络下保持高可用性。

Conclusion: Dynatune能有效提升不同网络场景下SMR服务的性能和可靠性。

Abstract: Raft is a leader-based consensus algorithm that implements State Machine
Replication (SMR), which replicates the service state across multiple servers
to enhance fault tolerance. In Raft, the servers play one of three roles:
leader, follower, or candidate. The leader receives client requests, determines
the processing order, and replicates them to the followers. When the leader
fails, the service must elect a new leader to continue processing requests,
during which the service experiences an out-of-service (OTS) time. The OTS time
is directly influenced by election parameters, such as heartbeat interval and
election timeout. However, traditional approaches, such as Raft, often struggle
to effectively tune these parameters, particularly under fluctuating network
conditions, leading to increased OTS time and reduced service responsiveness.
To address this, we propose Dynatune, a mechanism that dynamically adjusts
Raft's election parameters based on network metrics such as round-trip time and
packet loss rates measured via heartbeats. By adapting to changing network
environments, Dynatune significantly reduces the leader failure detection and
OTS time without altering Raft's core mechanisms or introducing additional
communication overheads. Experimental results demonstrate that Dynatune reduces
the leader failure detection and OTS times by 80% and 45%, respectively,
compared with Raft, while maintaining high availability even under dynamic
network conditions. These findings confirm that Dynatune effectively enhances
the performance and reliability of SMR services in various network scenarios.

</details>


### [76] [GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis](https://arxiv.org/abs/2507.15230)
*Guoxi Liu,Thomas Randall,Rong Ge,Federico Iuricich*

Main category: cs.DC

TL;DR: 针对非结构化网格在科学数据分析中的挑战，提出优化异构CPU - GPU系统的任务并行方法GALE，实验显示有速度提升且保持内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有任务并行数据结构是CPU受限，数据结构和分析算法竞争计算资源，限制了加速潜力。

Method: 引入优化异构CPU - GPU系统的任务并行方法，将网格连接信息计算卸载到GPU线程，CPU线程专注执行可视化算法，提出基于CUDA的GALE数据结构。

Result: 在20核CPU和NVIDIA V100 GPU上实验，GALE比现有本地化数据结构实现了高达2.7倍的加速，且保持内存效率。

Conclusion: GALE能有效解决现有方法的局限，在异构系统上提升性能。

Abstract: Unstructured meshes present challenges in scientific data analysis due to
irregular distribution and complex connectivity. Computing and storing
connectivity information is a major bottleneck for visualization algorithms,
affecting both time and memory performance. Recent task-parallel data
structures address this by precomputing connectivity information at runtime
while the analysis algorithm executes, effectively hiding computation costs and
improving performance. However, existing approaches are CPU-bound, forcing the
data structure and analysis algorithm to compete for the same computational
resources, limiting potential speedups. To overcome this limitation, we
introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU
systems. Specifically, we offload the computation of mesh connectivity
information to GPU threads, enabling CPU threads to focus on executing the
visualization algorithm. Following this paradigm, we propose GALE (GPU-Aided
Localized data structurE), the first open-source CUDA-based data structure
designed for heterogeneous task parallelism. Experiments on two 20-core CPUs
and an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over
state-of-the-art localized data structures while maintaining memory efficiency.

</details>


### [77] [An ML-Driven Participant Selection Technique for Federated Recommendation System in Edge-Cloud Computing](https://arxiv.org/abs/2507.15233)
*Jintao Liu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 现有FRS框架有局限性，提出多目标强化学习参与者选择方法，实验表明该方法能提升效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有FRS框架存在设备能力异构、数据非IID和通信瓶颈等问题，需改进。

Method: 定义复合客户端效用函数，嵌入多臂老虎机框架动态平衡探索-利用以选择参与者，用PySyft框架实现并在边缘云测试台评估。

Result: 在四种数据分区场景下，基于MAB的选择加速收敛32 - 50%，减少训练时间达46%，指标表现相当或略优。

Conclusion: 自适应、奖励驱动的客户端采样能显著提升联邦部署的效率和公平性。

Abstract: Recommendation systems (RS) personalize content by analyzing user
preferences, but typically require centralized collection of user data, raising
privacy and scalability concerns. Federated Recommendation Systems (FRS)
address these issues by enabling distributed, privacy-preserving model training
across edge devices, keeping raw data on-device. Although existing FRS
frameworks benefit from on-device feature extraction and privacy preservation,
they suffer from heterogeneous device capabilities, non-independent and
identically distributed (non-IID) data, and communication bottlenecks. To
overcome these limitations, we propose a multi-objective reinforcement learning
(RL) participant selection that jointly optimizes historical client performance
reputation (CPR), data utility, and system efficiency. First, we define a
composite client-utility function combining CPR, system capability, and data
quality. Next, we embed this utility into a multi-armed bandit (MAB) framework
and dynamically balance exploration-exploitation to select participants.
Finally, we practically implement our approach using the PySyft framework on an
edge-cloud testbed, and evaluate it on a multimodal movie-recommendation task
built from the MovieLens-100K dataset. Across four different skewed
data-partition scenarios, our MAB-based selection accelerates convergence by
32-50% in time-to-target AUC and reduces total wall-clock training time by up
to 46%, while matching or slightly improving final AUC, NDCG@50, and Recall@50
compared to existing FRS baselines. Our results demonstrate that adaptive,
reward-driven client sampling can substantially enhance both efficiency and
fairness in real-world federated deployments.

</details>


### [78] [Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing](https://arxiv.org/abs/2507.15553)
*Shibo Yu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 本文提出基于NSGA - II的路由算法，在云边计算环境中分配大语言模型推理请求，经实验验证该算法能提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理服务需求增长使计算资源压力增大，带来延迟和成本挑战。

Method: 引入基于NSGA - II的路由算法，将其构造成多目标优化问题以平衡响应质量、时间和成本，适应请求和节点差异。使用包含SQuAD、MBPP等数据集的测试平台进行基准测试。

Result: 与基线相比，该算法在响应时间和成本方面分别最多提升95.2%和34.9%。

Conclusion: 该算法对可扩展的大语言模型部署有效。

Abstract: The rising demand for Large Language Model (LLM) inference services has
intensified pressure on computational resources, resulting in latency and cost
challenges. This paper introduces a novel routing algorithm based on the
Non-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference
requests across heterogeneous LLM instances in a cloud-edge computing
environment. Formulated as a multi-objective optimization problem, the
algorithm balances response quality, response time, and inference cost,
adapting to request heterogeneity (e.g., varying complexity and prompt lengths)
and node diversity (e.g., edge vs. cloud resources). This adaptive routing
algorithm optimizes performance under dynamic workloads. We benchmark the
approach using a testbed with datasets including Stanford Question Answering
Dataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With
Adversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).
Experimental results show our solution, compared to the baselines, achieves up
to 95.2% and 34.9% improvements in terms of response time and cost,
respectively. These findings validate the algorithm's effectiveness for
scalable LLM deployments.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [79] [FAMST: Fast Approximate Minimum Spanning Tree Construction for Large-Scale and High-Dimensional Data](https://arxiv.org/abs/2507.14261)
*Mahmood K. M. Almansoori,Miklos Telek*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present Fast Approximate Minimum Spanning Tree (FAMST), a novel algorithm
that addresses the computational challenges of constructing Minimum Spanning
Trees (MSTs) for large-scale and high-dimensional datasets. FAMST utilizes a
three-phase approach: Approximate Nearest Neighbor (ANN) graph construction,
ANN inter-component connection, and iterative edge refinement. For a dataset of
$n$ points in a $d$-dimensional space, FAMST achieves $\mathcal{O}(dn \log n)$
time complexity and $\mathcal{O}(dn + kn)$ space complexity when $k$ nearest
neighbors are considered, which is a significant improvement over the
$\mathcal{O}(n^2)$ time and space complexity of traditional methods.
  Experiments across diverse datasets demonstrate that FAMST achieves
remarkably low approximation errors while providing speedups of up to
1000$\times$ compared to exact MST algorithms. We analyze how the key
hyperparameters, $k$ (neighborhood size) and $\lambda$ (inter-component edges),
affect performance, providing practical guidelines for hyperparameter
selection. FAMST enables MST-based analysis on datasets with millions of points
and thousands of dimensions, extending the applicability of MST techniques to
problem scales previously considered infeasible.

</details>


### [80] [Tighter Lower Bounds for Single Source Personalized PageRank](https://arxiv.org/abs/2507.14462)
*Xinpeng Jiang,Haoyu Liu,Siqiang Luo,Xiaokui Xiao*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study lower bounds for approximating the Single Source Personalized
PageRank (SSPPR) query, which measures the probability distribution of an
$\alpha$-decay random walk starting from a source node $s$. Existing lower
bounds remain loose-$\Omega\left(\min(m, 1/\delta)\right)$ for relative error
(SSPPR-R) and $\Omega\left(\min(n, 1/\epsilon)\right)$ for additive error
(SSPPR-A). To close this gap, we establish tighter bounds for both settings.
For SSPPR-R, we show a lower bound of $\Omega\left(\min\left(m,
\frac{\log(1/\delta)}{\delta}\right)\right)$ for any $\delta \in (0,1)$. For
SSPPR-A, we prove a lower bound of $\Omega\left(\min\left(m,
\frac{\log(1/\epsilon)}{\epsilon}\right)\right)$ for any $\epsilon \in (0,1)$,
assuming the graph has $m \in \mathcal{O}(n^{2-\beta})$ edges for any
arbitrarily small constant $\beta \in (0,1)$.

</details>


### [81] [New Algorithms for #2-SAT and #3-SAT](https://arxiv.org/abs/2507.14504)
*Junqiang Peng,Zimo Sheng,Mingyu Xiao*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The #2-SAT and #3-SAT problems involve counting the number of satisfying
assignments (also called models) for instances of 2-SAT and 3-SAT,
respectively. In 2010, Zhou et al. proposed an $\mathcal{O}^*(1.1892^m)$-time
algorithm for #2-SAT and an efficient approach for #3-SAT, where $m$ denotes
the number of clauses. In this paper, we show that the weighted versions of
#2-SAT and #3-SAT can be solved in $\mathcal{O}^*(1.1082^m)$ and
$\mathcal{O}^*(1.4423^m)$ time, respectively. These results directly apply to
the unweighted cases and achieve substantial improvements over the previous
results. These advancements are enabled by the introduction of novel reduction
rules, a refined analysis of branching operations, and the application of path
decompositions on the primal and dual graphs of the formula.

</details>


### [82] [Addressing Bias in Algorithmic Solutions: Exploring Vertex Cover and Feedback Vertex Set](https://arxiv.org/abs/2507.14509)
*Sheikh Shakil Akhtar,Jayakrishnan Madathil,Pranabendu Misra,Geevarghese Philip*

Main category: cs.DS

TL;DR: 研究组合优化中寻找对特定子群体‘无偏’的解。


<details>
  <summary>Details</summary>
Motivation: 组合优化提取数学抽象时丢弃的信息对算法输出终端用户可能有重要意义，且相同成本解在现实中影响不同，需寻找对特定子群体‘无偏’的解。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: A typical goal of research in combinatorial optimization is to come up with
fast algorithms that find optimal solutions to a computational problem. The
process that takes a real-world problem and extracts a clean mathematical
abstraction of it often throws out a lot of "side information" which is deemed
irrelevant. However, the discarded information could be of real significance to
the end-user of the algorithm's output. All solutions of the same cost are not
necessarily of equal impact in the real-world; some solutions may be much more
desirable than others, even at the expense of additional increase in cost. If
the impact, positive or negative, is mostly felt by some specific (minority)
subgroups of the population, the population at large will be largely unaware of
it. In this work we ask the question of finding solutions to combinatorial
optimization problems that are "unbiased" with respect to a collection of
specified subgroups of the total population.

</details>


### [83] [Characterizing and Testing Configuration Stability in Two-Dimensional Threshold Cellular Automata](https://arxiv.org/abs/2507.14569)
*Yonatan Nakar,Dana Ron*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problems of characterizing and testing the stability of
cellular automata configurations that evolve on a two-dimensional torus
according to threshold rules with respect to the von-Neumann neighborhood.
While stable configurations for Threshold-1 (OR) and Threshold-5 (AND) are
trivial (and hence easily testable), the other threshold rules exhibit much
more diverse behaviors. We first characterize the structure of stable
configurations with respect to the Threshold-2 (similarly, Threshold-4) and
Threshold-3 (Majority) rules. We then design and analyze a testing algorithm
that distinguishes between configurations that are stable with respect to the
Threshold-2 rule, and those that are $\epsilon$-far from any stable
configuration, where the query complexity of the algorithm is independent of
the size of the configuration and depends quadratically on $1/\epsilon$.

</details>


### [84] [A Black-Box Approach for Exogenous Replenishment in Online Resource Allocation](https://arxiv.org/abs/2507.14812)
*Suho Kang,Ziyang Liu,Rajan Udwani*

Main category: cs.DS

TL;DR: 本文针对在线资源分配问题，提出黑盒方法将原不考虑补货的算法扩展到含任意补货过程的情况，且在初始库存大时保持原算法竞争比。


<details>
  <summary>Details</summary>
Motivation: 解决在线资源分配中库存随未知外部过程补充的问题，使现有算法能适应补货情况。

Method: 引入黑盒方法，将原不考虑补货的算法扩展以适应任意（对抗或随机）补货过程。

Result: 在初始库存大的情况下，扩展后的算法能保持原算法的竞争比。

Conclusion: 该方法可将外部补货无缝集成到大量现有对抗和随机到达模型的算法结果中。

Abstract: In a typical online resource allocation problem, we start with a fixed
inventory of resources and make online allocation decisions in response to
resource requests that arrive sequentially over a finite horizon. We consider
settings where the inventory is replenished over time according to an unknown
exogenous process. We introduce black-box methods that extend any existing
algorithm, originally designed without considering replenishment, into one that
works with an arbitrary (adversarial or stochastic) replenishment process. Our
approach preserves the original algorithm's competitive ratio in regimes with
large initial inventory, thereby enabling the seamless integration of exogenous
replenishment into a large body of existing algorithmic results for both
adversarial and stochastic arrival models.

</details>


### [85] [Differentially Private Synthetic Graphs Preserving Triangle-Motif Cuts](https://arxiv.org/abs/2507.14835)
*Pan Peng,Hangyu Xu*

Main category: cs.DS

TL;DR: 提出首个(ε,δ)-DP机制生成合成图近似给定图所有割的三角形 motif 大小，给出误差下界，算法可推广到加权图，下界可扩展到 K_h - motif 割。


<details>
  <summary>Details</summary>
Motivation: 研究发布差分隐私合成图以近似给定图所有割的三角形 motif 大小问题，非隐私版本图在多领域有应用。

Method: 提出(ε,δ)-DP机制，在多项式时间内生成合成图，近似三角形 motif 大小。

Result: 生成的合成图近似误差为 ~O(√(mℓ₃(G))n/ε³/²)，给出误差下界 Ω(√(mn)ℓ₃(G)/ε)，算法和下界可推广。

Conclusion: 提出的机制有效解决问题，给出误差下界，且具有一定扩展性。

Abstract: We study the problem of releasing a differentially private (DP) synthetic
graph $G'$ that well approximates the triangle-motif sizes of all cuts of any
given graph $G$, where a motif in general refers to a frequently occurring
subgraph within complex networks. Non-private versions of such graphs have
found applications in diverse fields such as graph clustering, graph
sparsification, and social network analysis. Specifically, we present the first
$(\varepsilon,\delta)$-DP mechanism that, given an input graph $G$ with $n$
vertices, $m$ edges and local sensitivity of triangles $\ell_{3}(G)$, generates
a synthetic graph $G'$ in polynomial time, approximating the triangle-motif
sizes of all cuts $(S,V\setminus S)$ of the input graph $G$ up to an additive
error of $\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$. Additionally, we
provide a lower bound of $\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$ on the
additive error for any DP algorithm that answers the triangle-motif size
queries of all $(S,T)$-cut of $G$. Finally, our algorithm generalizes to
weighted graphs, and our lower bound extends to any $K_h$-motif cut for any
constant $h\geq 2$.

</details>


### [86] [Predict, Reposition, and Allocate: A Greedy and Flow-Based Architecture for Sustainable Urban Food Delivery](https://arxiv.org/abs/2507.15282)
*Aqsa Ashraf Makhdomi,Iqra Altaf Gillani*

Main category: cs.DS

TL;DR: 提出环保外卖配送优化框架，利用贪心算法和网络流模型，减少车辆使用，构建可持续配送生态。


<details>
  <summary>Details</summary>
Motivation: 外卖平台增加温室气体排放，现有优化机制未考虑环境可持续性。

Method: 利用目标函数特性设计贪心优化算法，将订单分配问题构建为网络流优化模型，设计三层网络架构匹配订单和配送人员。

Result: 减少了车辆数量。

Conclusion: 该框架能构建可持续的外卖配送生态系统。

Abstract: The rapid proliferation of food delivery platforms has reshaped urban
mobility but has also contributed significantly to environmental degradation
through increased greenhouse gas emissions. Existing optimization mechanisms
produce sub-optimal outcomes as they do not consider environmental
sustainability their optimization objective. This study proposes a novel
eco-friendly food delivery optimization framework that integrates demand
prediction, delivery person routing, and order allocation to minimize
environmental impact while maintaining service efficiency. Since recommending
routes is NP-Hard, the proposed approach utilizes the submodular and monotone
properties of the objective function and designs an efficient greedy
optimization algorithm. Thereafter, it formulates order allocation problem as a
network flow optimization model, which, to the best of our knowledge, has not
been explored in the context of food delivery. A three-layered network
architecture is designed to match orders with delivery personnel based on
capacity constraints and spatial demand. Through this framework, the proposed
approach reduces the vehicle count, and creates a sustainable food delivery
ecosystem.

</details>


### [87] [Language Generation in the Limit: Noise, Loss, and Feedback](https://arxiv.org/abs/2507.15319)
*Yannan Bai,Debmalya Panigrahi,Ian Zhang*

Main category: cs.DS

TL;DR: 本文解决了极限语言生成的并集封闭性问题，还对含噪声、无样本和有反馈等变体模型给出精确刻画。


<details>
  <summary>Details</summary>
Motivation: 前人提出极限语言生成框架并定义相关类别，提出非均匀生成的并集封闭性问题，以及噪声与非噪声生成的分离问题等，本文旨在解决这些问题。

Method: 先通过构造反例否定非均匀生成的并集封闭性，利用构造特性研究语言生成的多种变体，分析不同模型的等价性和特性。

Result: 证明了存在均匀和非均匀可生成集合的并集不可在有限时间生成；证明含噪声和无样本模型在均匀和非均匀生成上等价，给出非均匀噪声生成的刻画；证明有一个噪声字符串时噪声和非噪声生成存在分离；证明有限查询无额外能力，无限查询更强大。

Conclusion: 解决了极限语言生成的并集封闭性问题，并对含噪声、损失和反馈的自然变体给出精确刻画。

Abstract: Kleinberg and Mullainathan (2024) recently proposed a formal framework called
language generation in the limit and showed that given a sequence of example
strings from an unknown target language drawn from any countable collection, an
algorithm can correctly generate unseen strings from the target language within
finite time. This notion was further refined by Li, Raman, and Tewari (2024),
who defined stricter categories of non-uniform and uniform generation. They
showed that a finite union of uniformly generatable collections is generatable
in the limit, and asked if the same is true for non-uniform generation.
  We begin by resolving the question in the negative: we give a uniformly
generatable collection and a non-uniformly generatable collection whose union
is not generatable in the limit. We then use facets of this construction to
further our understanding of several variants of language generation. The first
two, generation with noise and without samples, were introduced by Raman and
Raman (2025) and Li, Raman, and Tewari (2024) respectively. We show the
equivalence of these models for uniform and non-uniform generation, and provide
a characterization of non-uniform noisy generation. The former paper asked if
there is any separation between noisy and non-noisy generation in the limit --
we show that such a separation exists even with a single noisy string. Finally,
we study the framework of generation with feedback, introduced by Charikar and
Pabbaraju (2025), where the algorithm is strengthened by allowing it to ask
membership queries. We show finite queries add no power, but infinite queries
yield a strictly more powerful model.
  In summary, the results in this paper resolve the union-closedness of
language generation in the limit, and leverage those techniques (and others) to
give precise characterizations for natural variants that incorporate noise,
loss, and feedback.

</details>


### [88] [1.64-Approximation for Chromatic Correlation Clustering via Chromatic Cluster LP](https://arxiv.org/abs/2507.15417)
*Dahoon Lee,Chenglin Fan,Euiwoong Lee*

Main category: cs.DS

TL;DR: 本文提出CCC问题的随机1.64近似算法，改进了之前的2.15近似因子，拓展了聚类LP框架。


<details>
  <summary>Details</summary>
Motivation: CCC能捕捉更丰富的关系结构，但由于标准LP松弛的局限性，改进其近似算法较难。

Method: 引入色聚类LP松弛和结合基于聚类与基于贪婪枢轴策略的舍入算法，将聚类LP框架拓展到色设置。

Result: 得到CCC问题的随机1.64近似算法，显著改进之前的2.15近似因子。

Conclusion: 绕过了标准LP的CCC版本的2的整性间隙，凸显聚类LP框架解决其他聚类问题变体的潜力。

Abstract: Chromatic Correlation Clustering (CCC) generalizes Correlation Clustering by
assigning multiple categorical relationships (colors) to edges and imposing
chromatic constraints on the clusters. Unlike traditional Correlation
Clustering, which only deals with binary $(+/-)$ relationships, CCC captures
richer relational structures. Despite its importance, improving the
approximation for CCC has been difficult due to the limitations of standard LP
relaxations. We present a randomized $1.64$-approximation algorithm to the CCC
problem, significantly improving the previous factor of $2.15$. Our approach
extends the cluster LP framework to the chromatic setting by introducing a
chromatic cluster LP relaxation and an rounding algorithm that utilizes both a
cluster-based and a greedy pivot-based strategy. The analysis bypasses the
integrality gap of $2$ for the CCC version of standard LP and highlights the
potential of the cluster LP framework to address other variants of clustering
problems.

</details>


### [89] [Job Scheduling under Base and Additional Fees, with Applications to Mixed-Criticality Scheduling](https://arxiv.org/abs/2507.15434)
*Yi-Ting Hsieh,Mong-Jen Kao,Jhong-Yun Liu,Hung-Lung Wang*

Main category: cs.DS

TL;DR: 研究n个作业调度到m个相同机器的问题，目标是最小化总机器工作时间，证明FFD算法可达1.5近似，问题有PTAS，并应用于混合关键系统调度。


<details>
  <summary>Details</summary>
Motivation: 解决将n个作业调度到m个相同机器，最小化总机器工作时间的问题。

Method: 使用First Fit Decreasing (FFD)算法进行分析，并得出多项式时间近似方案（PTAS）。

Result: FFD算法能达到1.5近似，该问题存在PTAS，且应用于混合关键系统调度有改进结果。

Conclusion: 该调度问题可通过FFD和PTAS有效解决，且成果能应用于混合关键系统调度。

Abstract: We are concerned with the problem of scheduling $n$ jobs onto $m$ identical
machines. Each machine has to be in operation for a prescribed time, and the
objective is to minimize the total machine working time. Precisely, let $c_i$
be the prescribed time for machine $i$, where $i\in[m]$, and $p_j$ be the
processing time for job $j$, where $j\in[n]$. The problem asks for a schedule
$\sigma\colon\, J\to M$ such that $\sum_{i=1}^m\max\{c_i,
\sum_{j\in\sigma^{-1}(i)}p_j\}$ is minimized, where $J$ and $M$ denote the sets
of jobs and machines, respectively. We show that First Fit Decreasing (FFD)
leads to a $1.5$-approximation, and this problem admits a polynomial-time
approximation scheme (PTAS). The idea is further applied to mixed-criticality
system scheduling to yield improved approximation results.

</details>


### [90] [An $n^{O(\log\log n)}$ time approximation scheme for capacitated VRP in the Euclidean plane](https://arxiv.org/abs/2507.15549)
*René Sitters*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a quasi polynomial time approximation scheme (Q-PTAS) for the
capacitated vehicle routing problem (CVRP) on $n$ points in the Euclidean plane
for arbitrary capacity $c$. The running time is $n^{f(\epsilon)\cdot\log\log
n}$ for any $c$, and where $f$ is a function of $\epsilon$ only. This is a
major improvement over the so far best known running time of
$n^{\log^{O(1/\epsilon)}n}$ time and a big step towards a PTAS for Euclidean
CVRP.
  In our algorithm, we first give a polynomial time reduction of the CVRP in
$\mathbb{R}^d$ (for any fixed $d$) to an uncapacitated routing problem in
$\mathbb{R}^d$ that we call the $m$-paths problem. Here, one needs to find
exactly $m$ paths between two points $a$ and $b$, covering all the given points
in the Euclidean space. We then give a Q-PTAS for the $m$-paths problem in the
pane. Any PTAS for the (arguably easier to handle) Euclidean $m$-paths problem
is most likely to imply a PTAS for the Euclidean CVRP.

</details>


### [91] [Fast Algorithms for Graph Arboricity and Related Problems](https://arxiv.org/abs/2507.15598)
*Ruoxu Cen,Henry Fleischmann,George Z. Li,Jason Li,Debmalya Panigrahi*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We give an algorithm for finding the arboricity of a weighted, undirected
graph, defined as the minimum number of spanning forests that cover all edges
of the graph, in $\sqrt{n} m^{1+o(1)}$ time. This improves on the previous best
bound of $\tilde{O}(nm)$ for weighted graphs and $\tilde{O}(m^{3/2}) $ for
unweighted graphs (Gabow 1995) for this problem. The running time of our
algorithm is dominated by a logarithmic number of calls to a directed global
minimum cut subroutine -- if the running time of the latter problem improves to
$m^{1+o(1)}$ (thereby matching the running time of maximum flow), the running
time of our arboricity algorithm would improve further to $m^{1+o(1)}$.
  We also give a new algorithm for computing the entire cut hierarchy --
laminar multiway cuts with minimum cut ratio in recursively defined induced
subgraphs -- in $m n^{1+o(1)}$ time. The cut hierarchy yields the ideal edge
loads (Thorup 2001) in a fractional spanning tree packing of the graph which,
we show, also corresponds to a max-entropy solution in the spanning tree
polytope. For the cut hierarchy problem, the previous best bound was
$\tilde{O}(n^2 m)$ for weighted graphs and $\tilde{O}(n m^{3/2})$ for
unweighted graphs.

</details>


### [92] [On zeros and algorithms for disordered systems: mean-field spin glasses](https://arxiv.org/abs/2507.15616)
*Ferenc Bencs,Kuikui Liu,Guus Regts*

Main category: cs.DS

TL;DR: 为平均场自旋玻璃模型设计确定性拟多项式时间算法，用于在二阶矩体制下对几乎所有逆温度的配分函数进行高精度估计。


<details>
  <summary>Details</summary>
Motivation: 自旋玻璃是统计物理、平均情况计算复杂性理论和现代高维统计推断的核心概率分布，需要算法估计配分函数。

Method: 研究配分函数零点的位置。

Result: 设计出的算法能在二阶矩体制下对几乎所有逆温度的配分函数进行高精度估计，对Sherrington - Kirkpatrick模型在几乎整个 Replica - symmetric 相都有效，且方法适用于球面情况和 Ising 自旋情况。

Conclusion: 提出的方法概念简单，可有效估计配分函数。

Abstract: Spin glasses are fundamental probability distributions at the core of
statistical physics, the theory of average-case computational complexity, and
modern high-dimensional statistical inference. In the mean-field setting, we
design deterministic quasipolynomial-time algorithms for estimating the
partition function to arbitrarily high accuracy for nearly all inverse
temperatures in the second moment regime. In particular, for the
Sherrington--Kirkpatrick model, our algorithms succeed for almost the entire
replica-symmetric phase. To achieve this, we study the locations of the zeros
of the partition function. Notably, our methods are conceptually simple, and
apply equally well to the spherical case and the case of Ising spins.

</details>


### [93] [Asynchronous Collective Tree Exploration: a Distributed Algorithm, and a new Lower Bound](https://arxiv.org/abs/2507.15658)
*Romain Cosson,Laurent Massoulié*

Main category: cs.DS

TL;DR: 研究k个移动代理集体探索未知树的问题，提出分布式异步算法，给出移动步数保证及改进的竞争比下界。


<details>
  <summary>Details</summary>
Motivation: 以往集体树探索的竞争保证要么是分布式同步，要么是异步集中式，缺乏分布式异步的情况。

Method: 设计分布式异步算法，对探索树的移动步数进行分析，并推导新的竞争比下界。

Result: 提出的算法在移动步数上有相关保证，如2n + O(k^2 2^kD)和O(k/log k)(n + kD)，新的竞争比下界为Ω(log^2 k)。

Conclusion: 所提算法的后悔保证从平均复杂度角度渐近最优，新的竞争比下界改进了以往结果。

Abstract: We study the problem of collective tree exploration in which a team of $k$
mobile agents must collectively visit all nodes of an unknown tree in as few
moves as possible. The agents all start from the root and discover adjacent
edges as they progress in the tree. Communication is distributed in the sense
that agents share information by reading and writing on whiteboards located at
all nodes. Movements are asynchronous, in the sense that the speeds of all
agents are controlled by an adversary at all times. All previous competitive
guarantees for collective tree exploration are either distributed but
synchronous, or asynchronous but centralized. In contrast, we present a
distributed asynchronous algorithm that explores any tree of $n$ nodes and
depth $D$ in at most $2n+O(k^2 2^kD)$ moves, i.e., with a regret that is linear
in $D$, and a variant algorithm with a guarantee in $O(k/\log k)(n+kD)$, i.e.,
with a competitive ratio in $O(k/\log k)$. We note that our regret guarantee is
asymptotically optimal (i.e., $1$-competitive) from the perspective of
average-case complexity. We then present a new general lower bound on the
competitive ratio of asynchronous collective tree exploration, in
$\Omega(\log^2 k)$. This lower bound applies to both the distributed and
centralized settings, and improves upon the previous lower bound in
$\Omega(\log k)$.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [94] [A Formal Model of the Economic Impacts of AI Openness Regulation](https://arxiv.org/abs/2507.14193)
*Tori Qiu,Benjamin Laufer,Jon Kleinberg,Hoda Heidari*

Main category: cs.GT

TL;DR: 本文建模通用模型创建者与微调者的策略互动，评估AI开放标准，给出市场均衡特征及有效监管措施，发现模型基线性能影响通用模型发布策略，为AI开放治理提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有监管框架关注AI模型开放性，但开源基础模型定义模糊，需评估合适的AI开放标准以给开发者提供经济激励。

Method: 建模通用模型创建者（通才）和微调实体（专才）的策略互动，构建监管者选择开源定义的简化模型。

Result: 刻画了不同开放监管下的市场均衡，给出一系列有效的监管惩罚和开源阈值，发现模型基线性能决定增加监管惩罚或开源阈值何时显著改变通用模型发布策略。

Conclusion: 模型为AI开放治理决策提供理论基础，有助于评估和完善实际开源政策。

Abstract: Regulatory frameworks, such as the EU AI Act, encourage openness of
general-purpose AI models by offering legal exemptions for "open-source"
models. Despite this legislative attention on openness, the definition of
open-source foundation models remains ambiguous. This paper models the
strategic interactions among the creator of a general-purpose model (the
generalist) and the entity that fine-tunes the general-purpose model to a
specialized domain or task (the specialist), in response to regulatory
requirements on model openness. We present a stylized model of the regulator's
choice of an open-source definition to evaluate which AI openness standards
will establish appropriate economic incentives for developers. Our results
characterize market equilibria -- specifically, upstream model release
decisions and downstream fine-tuning efforts -- under various openness
regulations and present a range of effective regulatory penalties and
open-source thresholds. Overall, we find the model's baseline performance
determines when increasing the regulatory penalty vs. the open-source threshold
will significantly alter the generalist's release strategy. Our model provides
a theoretical foundation for AI governance decisions around openness and
enables evaluation and refinement of practical open-source policies.

</details>


### [95] [Strategyproofness and Monotone Allocation of Auction in Social Networks](https://arxiv.org/abs/2507.14472)
*Yuhang Guo,Dong Hao,Bin Li,Mingyu Xiao,Bakh Khoussainov*

Main category: cs.GT

TL;DR: 本文识别网络拍卖的两类单调分配规则，刻画策略证明支付规则的存在与充分条件，解决单心思投标者组合网络拍卖的障碍。


<details>
  <summary>Details</summary>
Motivation: 网络拍卖策略证明性缺少通用分配规则原则，现有多单元单需求网络拍卖研究无法实现策略证明。

Method: 识别网络上的邀请抑制单调性（ID - MON）和邀请促进单调性（IP - MON）两类单调分配规则，刻画策略证明支付规则情况。

Result: 确定了策略证明支付规则的存在和充分条件，证明存在收益最大化的支付规则且计算可行。

Conclusion: 解决了单心思投标者组合网络拍卖的障碍。

Abstract: Strategyproofness in network auctions requires that bidders not only report
their valuations truthfully, but also do their best to invite neighbours from
the social network. In contrast to canonical auctions, where the value-monotone
allocation in Myerson's Lemma is a cornerstone, a general principle of
allocation rules for strategyproof network auctions is still missing. We show
that, due to the absence of such a principle, even extensions to multi-unit
network auctions with single-unit demand present unexpected difficulties, and
all pioneering researches fail to be strategyproof. For the first time in this
field, we identify two categories of monotone allocation rules on networks:
Invitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity
(IP-MON). They encompass all existing allocation rules of network auctions as
specific instances. For any given ID-MON or IP-MON allocation rule, we
characterize the existence and sufficient conditions for the strategyproof
payment rules, and show that among all such payment rules, the
revenue-maximizing one exists and is computationally feasible. With these
results, the obstacle of combinatorial network auction with single-minded
bidders is now resolved.

</details>


### [96] [Probing EFX via PMMS: (Non-)Existence Results in Discrete Fair Division](https://arxiv.org/abs/2507.14957)
*Jarosław Byrka,Franciszek Malinka,Tomasz Ponitka*

Main category: cs.GT

TL;DR: 研究不可分物品公平分配，探讨EFX和PMMS问题，构造实例区分两者，证明三类特殊情况公平分配存在性并给出多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 解决公平分配领域核心开放问题EFX及更强变体PMMS问题。

Method: 构造三主体实例、数学证明、设计算法。

Result: 构造实例区分EFX和PMMS；证明个性化双值估值、二进制值MMS可行估值、成对需求估值三种情况下公平分配存在性，给出多项式时间算法。

Conclusion: 为不可分物品公平分配提供新见解，解决三类特殊情况公平分配存在性问题。

Abstract: We study the fair division of indivisible items and provide new insights into
the EFX problem, which is widely regarded as the central open question in fair
division, and the PMMS problem, a strictly stronger variant of EFX. Our first
result constructs a three-agent instance with two monotone valuations and one
additive valuation in which no PMMS allocation exists. Since EFX allocations
are known to exist under these assumptions, this establishes a formal
separation between EFX and PMMS.
  We prove existence of fair allocations for three important special cases. We
show that EFX allocations exist for personalized bivalued valuations, where for
each agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value
$v_i(\{g\}) \in \{a_i, b_i\}$ to each good $g$. We establish an analogous
existence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also
prove that PMMS allocations exist for binary-valued MMS-feasible valuations,
where each bundle $S$ has value $v_i(S) \in \{0, 1\}$. Notably, this result
holds even without assuming monotonicity of valuations and thus applies to the
fair division of chores and mixed manna. Finally, we study a class of
valuations called pair-demand valuations, which extend the well-studied
unit-demand valuations to the case where each agent derives value from at most
two items, and we show that PMMS allocations exist in this setting. Our proofs
are constructive, and we provide polynomial-time algorithms for all three
existence results.

</details>


### [97] [Strategically Robust Game Theory via Optimal Transport](https://arxiv.org/abs/2507.15325)
*Nicolas Lanzetti,Sylvain Fricker,Saverio Bolognani,Florian Dörfler,Dario Paccagnan*

Main category: cs.GT

TL;DR: 论文提出战略鲁棒均衡概念，证明其存在性、特性及计算成本，实验表明其能应对对手行为不确定性并常带来更高收益。


<details>
  <summary>Details</summary>
Motivation: 在博弈论场景中，多种不确定性来源使引导智能体决策困难，需寻求应对不确定性的方法。

Method: 提出智能体面对以涌现行为为中心、可调节大小的模糊集中最坏情况做决策，定义战略鲁棒均衡，通过最优运输实现并分析其性质。

Result: 战略鲁棒均衡在与纳什均衡相同假设下存在，可在纳什和安全策略间插值，计算成本无增加，实验显示能应对对手行为不确定性且常带来更高收益。

Conclusion: 战略鲁棒均衡能有效应对对手行为的不确定性，且能通过鲁棒化实现协调，带来更高均衡收益。

Abstract: In many game-theoretic settings, agents are challenged with taking decisions
against the uncertain behavior exhibited by others. Often, this uncertainty
arises from multiple sources, e.g., incomplete information, limited
computation, bounded rationality. While it may be possible to guide the agents'
decisions by modeling each source, their joint presence makes this task
particularly daunting. Toward this goal, it is natural for agents to seek
protection against deviations around the emergent behavior itself, which is
ultimately impacted by all the above sources of uncertainty. To do so, we
propose that each agent takes decisions in face of the worst-case behavior
contained in an ambiguity set of tunable size, centered at the emergent
behavior so implicitly defined. This gives rise to a novel equilibrium notion,
which we call strategically robust equilibrium. Building on its definition, we
show that, when judiciously operationalized via optimal transport,
strategically robust equilibria (i) are guaranteed to exist under the same
assumptions required for Nash equilibria; (ii) interpolate between Nash and
security strategies; (iii) come at no additional computational cost compared to
Nash equilibria. Through a variety of experiments, including bi-matrix games,
congestion games, and Cournot competition, we show that strategic robustness
protects against uncertainty in the opponents' behavior and, surprisingly,
often results in higher equilibrium payoffs - an effect we refer to as
coordination via robustification.

</details>


### [98] [The Root of Revenue Continuity](https://arxiv.org/abs/2507.15735)
*Sergiu Hart,Noam Nisan*

Main category: cs.GT

TL;DR: 证明随机估值分布的Wasserstein距离与可提取收入之间的关系，并表明对最优机制的简单修改对相近分布几乎最优。


<details>
  <summary>Details</summary>
Motivation: 在商品销售场景下，已有研究表明买家估值分布的小变化对可提取收入影响小，此研究旨在给出一个简单通用的表述。

Method: 定义随机估值X和Y，利用Wasserstein距离，证明sqrt(Rev(X)) - sqrt(Rev(Y)) <= sqrt(W(X,Y))。

Result: 得出sqrt(Rev(X)) - sqrt(Rev(Y)) <= sqrt(W(X,Y))的结果，且指出对X的最优机制进行“统一折扣”修改后对相近的Y几乎最优。

Conclusion: 通过数学证明建立了估值分布距离与收入的关系，提出的机制修改方法具有实用性。

Abstract: In the setup of selling one or more goods, various papers have shown, in
various forms and for various purposes, that a small change in the distribution
of a buyer's valuations may cause only a small change in the possible revenue
that can be extracted. We prove a simple, clean, convenient, and general
statement to this effect: let X and Y be random valuations on k additive goods,
and let W(X,Y) be the Wasserstein (or "earth mover's") distance between them;
then sqrt(Rev(X))-sqrt(Rev(Y)) <= sqrt(W(X,Y)). This further implies that a
simple explicit modification of any optimal mechanism for X, namely, "uniform
discounting", is guaranteed to be almost optimal for any Y that is close to X
in the Wasserstein distance.

</details>


### [99] [General Matching Games](https://arxiv.org/abs/2507.15737)
*Felipe Garrido-Lucero,Rida Laraki*

Main category: cs.GT

TL;DR: 文章将Garrido - Lucero和Laraki的一对一双边市场模型扩展到包含多数一对多匹配市场和室友模型的通用设置，并明确两种框架使核心稳定且抗重新协商的结果存在并可有效计算。


<details>
  <summary>Details</summary>
Motivation: 将原有的一对一双边市场模型扩展到更通用的设置，涵盖更多匹配市场类型。

Method: 在通用设置下扩展原模型，并确定两种框架。

Result: 明确了两种框架，在这些框架下核心稳定和抗重新协商的结果存在且可有效计算。

Conclusion: 该扩展模型能在更广泛的匹配市场场景中确定稳定且抗重新协商的结果。

Abstract: Matching games is a one-to-one two sided market model introduced by
Garrido-Lucero and Laraki, in which coupled agents' utilities are endogenously
determined as the outcome of a strategic game. They refine the classical
pairwise stability by requiring robustness to renegotiation and provide general
conditions under which pairwise stable and renegotiation-proof outcomes exist
as the limit of a deferred acceptance with competitions algorithm together with
a renegotiation process. In this article, we extend their model to a general
setting encompassing most of one-to-many matching markets and roommates models
and specify two frameworks under which core stable and renegotiation-proof
outcomes exist and can be efficiently computed.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [100] [LOVO: Efficient Complex Object Query in Large-Scale Video Datasets](https://arxiv.org/abs/2507.14301)
*Yuxin Liu,Yuezhang Peng,Hefeng Zhou,Hongze Liu,Xinyu Lu,Jiong Lou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.IR

TL;DR: 本文提出LOVO系统处理大规模视频数据集复杂对象查询，通过特征提取、建索引、查询转换和重排序等操作，在真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频分析方法在处理大规模视频数据集特定对象查询时存在适应性差、查询延迟高等问题，需新系统解决。

Method: 使用预训练视觉编码器进行一次性特征提取，生成关键帧视觉嵌入构建高效索引；在向量数据库中组织视觉嵌入和边界框；查询时将对象查询转换为查询嵌入，进行近似最近邻搜索；最后进行跨模态重排序。

Result: 在真实视频数据集上，LOVO处理复杂查询时优于现有方法，查询精度接近最优，搜索延迟降低达85倍，显著降低索引构建成本。

Conclusion: LOVO重新定义了视频分析中对象查询方法，为复杂对象查询提供新颖、可扩展且高效的方法，适用于动态环境。

Abstract: The widespread deployment of cameras has led to an exponential increase in
video data, creating vast opportunities for applications such as traffic
management and crime surveillance. However, querying specific objects from
large-scale video datasets presents challenges, including (1) processing
massive and continuously growing data volumes, (2) supporting complex query
requirements, and (3) ensuring low-latency execution. Existing video analysis
methods struggle with either limited adaptability to unseen object classes or
suffer from high query latency. In this paper, we present LOVO, a novel system
designed to efficiently handle comp$\underline{L}$ex $\underline{O}$bject
queries in large-scale $\underline{V}$ide$\underline{O}$ datasets. Agnostic to
user queries, LOVO performs one-time feature extraction using pre-trained
visual encoders, generating compact visual embeddings for key frames to build
an efficient index. These visual embeddings, along with associated bounding
boxes, are organized in an inverted multi-index structure within a vector
database, which supports queries for any objects. During the query phase, LOVO
transforms object queries to query embeddings and conducts fast approximate
nearest-neighbor searches on the visual embeddings. Finally, a cross-modal
rerank is performed to refine the results by fusing visual features with
detailed textual features. Evaluation on real-world video datasets demonstrates
that LOVO outperforms existing methods in handling complex queries, with
near-optimal query accuracy and up to 85x lower search latency, while
significantly reducing index construction costs. This system redefines the
state-of-the-art object query approaches in video analysis, setting a new
benchmark for complex object queries with a novel, scalable, and efficient
approach that excels in dynamic environments.

</details>


### [101] [A Reproducibility Study of Product-side Fairness in Bundle Recommendation](https://arxiv.org/abs/2507.14352)
*Huy-Son Nguyen,Yuanna Liu,Masoud Mansoury,Mohammad Alian Nejadi,Alan Hanjalic,Maarten de Rijke*

Main category: cs.IR

TL;DR: 本文对捆绑推荐中产品端公平性进行可复现性研究，揭示捆绑与物品曝光模式差异等结果，为构建更公平系统提供见解。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统公平性问题研究广泛，但捆绑推荐中的公平性问题研究较少，且现有公平框架和指标可能不适用于多层级的捆绑推荐。

Method: 在三个真实数据集上使用四种最先进的捆绑推荐方法进行可复现性研究，用多种公平指标分析捆绑和物品层面的曝光差异。

Result: 捆绑和物品的曝光模式有显著差异；公平评估因指标而异；用户更多与捆绑交互时，系统曝光分布更公平。

Conclusion: 研究结果为构建更公平的捆绑推荐系统提供可操作见解，为该领域未来研究奠定基础。

Abstract: Recommender systems are known to exhibit fairness issues, particularly on the
product side, where products and their associated suppliers receive unequal
exposure in recommended results. While this problem has been widely studied in
traditional recommendation settings, its implications for bundle recommendation
(BR) remain largely unexplored. This emerging task introduces additional
complexity: recommendations are generated at the bundle level, yet user
satisfaction and product (or supplier) exposure depend on both the bundle and
the individual items it contains. Existing fairness frameworks and metrics
designed for traditional recommender systems may not directly translate to this
multi-layered setting. In this paper, we conduct a comprehensive
reproducibility study of product-side fairness in BR across three real-world
datasets using four state-of-the-art BR methods. We analyze exposure
disparities at both the bundle and item levels using multiple fairness metrics,
uncovering important patterns. Our results show that exposure patterns differ
notably between bundles and items, revealing the need for fairness
interventions that go beyond bundle-level assumptions. We also find that
fairness assessments vary considerably depending on the metric used,
reinforcing the need for multi-faceted evaluation. Furthermore, user behavior
plays a critical role: when users interact more frequently with bundles than
with individual items, BR systems tend to yield fairer exposure distributions
across both levels. Overall, our findings offer actionable insights for
building fairer bundle recommender systems and establish a vital foundation for
future research in this emerging domain.

</details>


### [102] [RaMen: Multi-Strategy Multi-Modal Learning for Bundle Construction](https://arxiv.org/abs/2507.14361)
*Huy-Son Nguyen,Quang-Huy Nguyen,Duc-Hoang Pham,Duc-Trong Le,Hoang-Quynh Le,Padipat Sitkrongwong,Atsuhiro Takasu,Masoud Mansoury*

Main category: cs.IR

TL;DR: 现有捆绑构建研究有局限，提出RaMen方法，结合多策略学习全面健壮的捆绑表示，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有捆绑构建研究仅依赖用户反馈或语义信息，无法捕捉现实捆绑结构的精细关系，导致捆绑表示不佳。

Method: 提出RaMen方法，利用内在和外在信息，通过显式策略感知学习（ESL）和隐式策略感知学习（ISL）建模捆绑结构，还采用多策略对齐与判别模块。

Result: 在多个领域的实验中，RaMen比现有模型更有效。

Conclusion: RaMen为复杂项目集问题提供了有价值的见解。

Abstract: Existing studies on bundle construction have relied merely on user feedback
via bipartite graphs or enhanced item representations using semantic
information. These approaches fail to capture elaborate relations hidden in
real-world bundle structures, resulting in suboptimal bundle representations.
To overcome this limitation, we propose RaMen, a novel method that provides a
holistic multi-strategy approach for bundle construction. RaMen utilizes both
intrinsic (characteristics) and extrinsic (collaborative signals) information
to model bundle structures through Explicit Strategy-aware Learning (ESL) and
Implicit Strategy-aware Learning (ISL). ESL employs task-specific attention
mechanisms to encode multi-modal data and direct collaborative relations
between items, thereby explicitly capturing essential bundle features.
Moreover, ISL computes hyperedge dependencies and hypergraph message passing to
uncover shared latent intents among groups of items. Integrating diverse
strategies enables RaMen to learn more comprehensive and robust bundle
representations. Meanwhile, Multi-strategy Alignment & Discrimination module is
employed to facilitate knowledge transfer between learning strategies and
ensure discrimination between items/bundles. Extensive experiments demonstrate
the effectiveness of RaMen over state-of-the-art models on various domains,
justifying valuable insights into complex item set problems.

</details>


### [103] [Understanding Matching Mechanisms in Cross-Encoders](https://arxiv.org/abs/2507.14604)
*Mathias Vast,Basile Van Cooten,Laure Soulier,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: 本文指出多数解释神经IR架构行为的工作未描述匹配过程，展示更直接方法能提供有价值见解，还聚焦注意力过程和匹配检测机制。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多未能描述神经IR架构的匹配过程，而要探索更直接的方法来解释其行为。

Method: 先聚焦注意力过程，提取因果见解；再对匹配检测的潜在机制进行解释。

Result: 发现了一些注意力头在注意力过程中的关键作用，并对匹配检测机制进行了解释。

Conclusion: 更直接的方法可以为解释神经IR架构的行为提供有价值的见解。

Abstract: Neural IR architectures, particularly cross-encoders, are highly effective
models whose internal mechanisms are mostly unknown. Most works trying to
explain their behavior focused on high-level processes (e.g., what in the input
influences the prediction, does the model adhere to known IR axioms) but fall
short of describing the matching process. Instead of Mechanistic
Interpretability approaches which specifically aim at explaining the hidden
mechanisms of neural models, we demonstrate that more straightforward methods
can already provide valuable insights. In this paper, we first focus on the
attention process and extract causal insights highlighting the crucial roles of
some attention heads in this process. Second, we provide an interpretation of
the mechanism underlying matching detection.

</details>


### [104] [Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module](https://arxiv.org/abs/2507.14612)
*Pei-Xuan Li,Wei-Yun Liang,Fandel Lin,Hsun-Ping Hsieh*

Main category: cs.IR

TL;DR: 本文提出GDPW框架解决现有兴趣点推荐方法未考虑类别与时间关系、难捕捉时间连续性及忽略权重信息等问题，实验显示其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分考虑POI类别与时间关系、难捕捉时间连续性且忽略POI多种权重信息，导致推荐性能不佳。

Method: 提出GDPW框架，通过全局类别图和全局类别 - 时间图学习类别和时间表示，用对比学习分离信息，预测后根据POI间转移权重和距离关系加权得到最终推荐。

Result: 在两个真实数据集上实验，GDPW性能比其他现有模型提高3% - 11%。

Conclusion: GDPW框架在考虑POI类别信息和多种权重因素后，能有效提升下一兴趣点推荐的性能。

Abstract: Next point of interest (POI) recommendation primarily predicts future
activities based on users' past check-in data and current status, providing
significant value to users and service providers. We observed that the popular
check-in times for different POI categories vary. For example, coffee shops are
crowded in the afternoon because people like to have coffee to refresh after
meals, while bars are busy late at night. However, existing methods rarely
explore the relationship between POI categories and time, which may result in
the model being unable to fully learn users' tendencies to visit certain POI
categories at different times. Additionally, existing methods for modeling time
information often convert it into time embeddings or calculate the time
interval and incorporate it into the model, making it difficult to capture the
continuity of time. Finally, during POI prediction, various weighting
information is often ignored, such as the popularity of each POI, the
transition relationships between POIs, and the distances between POIs, leading
to suboptimal performance. To address these issues, this paper proposes a novel
next POI recommendation framework called Graph Disentangler with POI Weighted
Module (GDPW). This framework aims to jointly consider POI category information
and multiple POI weighting factors. Specifically, the proposed GDPW learns
category and time representations through the Global Category Graph and the
Global Category-Time Graph. Then, we disentangle category and time information
through contrastive learning. After prediction, the final POI recommendation
for users is obtained by weighting the prediction results based on the
transition weights and distance relationships between POIs. We conducted
experiments on two real-world datasets, and the results demonstrate that the
proposed GDPW outperforms other existing models, improving performance by 3% to
11%.

</details>


### [105] [Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining](https://arxiv.org/abs/2507.14619)
*Van-Hoang Le,Duc-Vu Nguyen,Kiet Van Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.IR

TL;DR: 本文提出两阶段框架提升法律文档检索效率与准确性，在竞赛获佳绩，证明优化数据处理等对构建法律检索增强系统很关键。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在法律等专业领域面临挑战，需提升法律文档检索效率和准确性。

Method: 采用两阶段框架，先使用微调的Bi - Encoder快速检索候选文档，再用Cross - Encoder精确重排序，通过策略性负例挖掘优化，引入Exist@m指标，使用半硬负例减轻训练偏差。

Result: 团队在SoICT Hackathon 2024法律文档检索竞赛中获前三名，轻量级单遍方法参数少但有竞争力。

Conclusion: 优化数据处理、定制损失函数和平衡负采样对构建法律检索增强系统至关重要。

Abstract: Large Language Models (LLMs) face significant challenges in specialized
domains like law, where precision and domain-specific knowledge are critical.
This paper presents a streamlined two-stage framework consisting of Retrieval
and Re-ranking to enhance legal document retrieval efficiency and accuracy. Our
approach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,
followed by a Cross-Encoder for precise re-ranking, both optimized through
strategic negative example mining. Key innovations include the introduction of
the Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard
negatives to mitigate training bias, which significantly improved re-ranking
performance. Evaluated on the SoICT Hackathon 2024 for Legal Document
Retrieval, our team, 4Huiter, achieved a top-three position. While
top-performing teams employed ensemble models and iterative self-training on
large bge-m3 architectures, our lightweight, single-pass approach offered a
competitive alternative with far fewer parameters. The framework demonstrates
that optimized data processing, tailored loss functions, and balanced negative
sampling are pivotal for building robust retrieval-augmented systems in legal
contexts.

</details>


### [106] [U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs](https://arxiv.org/abs/2507.14902)
*Xiaojie Li,Chu Li,Shi-Zhe Chen,Xi Chen*

Main category: cs.IR

TL;DR: 本文对基于MLLM的通用多模态检索（UMR）嵌入学习进行研究，提出U - MARVEL框架，在监督设置和零样本任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的UMR方法检索机制未充分探索，可能导致性能不佳和泛化能力有限，需研究有效嵌入学习的关键因素。

Method: 实现通用MLLM嵌入学习流程，系统分析高效检索系统的主要贡献因素，探索嵌入生成和训练策略的细节，提出U - MARVEL框架。

Result: U - MARVEL框架在M - BEIR基准上大幅超越现有方法，在组合图像检索和文本到视频检索等零样本任务中也表现出色。

Conclusion: U - MARVEL框架在各种基于嵌入的检索任务中具有泛化潜力。

Abstract: Universal multimodal retrieval (UMR), which aims to address complex retrieval
tasks where both queries and candidates span diverse modalities, has been
significantly advanced by the emergence of MLLMs. While state-of-the-art
MLLM-based methods in the literature predominantly adopt contrastive learning
principles, they often differ in their specific training recipes. Despite their
success, the mechanisms underlying their retrieval capabilities remain largely
unexplored, potentially resulting in suboptimal performance and limited
generalization ability. To address these issues, we present a comprehensive
study aimed at uncovering the key factors that drive effective embedding
learning for UMR using MLLMs. We begin by implementing a general MLLM-based
embedding learning pipeline, and systematically analyze the primary
contributors to high-performing universal retrieval systems. Based on this, we
explore various aspects of the details in embedding generation and training
strategies, including progressive transition, hard negative mining and
re-ranker distillation. Notably, our findings reveal that often-overlooked
factors can have a substantial impact on model performance. Building on these
discoveries, we introduce a unified framework termed U-MARVEL
(\textbf{U}niversal \textbf{M}ultimod\textbf{A}l \textbf{R}etrie\textbf{V}al
via \textbf{E}mbedding \textbf{L}earning), which outperforms state-of-the-art
competitors on the M-BEIR benchmark by a large margin in supervised settings,
and also exihibits strong zero-shot performance on several tasks such as
composed image retrieval and text-to-video retrieval. These results underscore
the generalization potential of our framework across various embedding-based
retrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL

</details>


### [107] [User Invariant Preference Learning for Multi-Behavior Recommendation](https://arxiv.org/abs/2507.14925)
*Mingshi Yan,Zhiyong Cheng,Fan Liu,Yingda Lyu,Yahong Han*

Main category: cs.IR

TL;DR: 提出UIPL方法用于多行为推荐，利用不变风险最小化学习用户不变偏好，实验表明显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐方法常忽略用户多行为偏好的共性与个性，辅助行为可能引入噪声，影响目标行为预测。

Method: 提出UIPL方法，利用不变风险最小化范式，用变分自编码器提取用户不变偏好，构建不同环境增强学习鲁棒性。

Result: 在四个真实数据集上的大量实验表明，UIPL显著优于当前最先进的方法。

Conclusion: UIPL能有效捕捉用户内在兴趣，缓解噪声引入问题，提升多行为推荐效果。

Abstract: In multi-behavior recommendation scenarios, analyzing users' diverse
behaviors, such as click, purchase, and rating, enables a more comprehensive
understanding of their interests, facilitating personalized and accurate
recommendations. A fundamental assumption of multi-behavior recommendation
methods is the existence of shared user preferences across behaviors,
representing users' intrinsic interests. Based on this assumption, existing
approaches aim to integrate information from various behaviors to enrich user
representations. However, they often overlook the presence of both
commonalities and individualities in users' multi-behavior preferences. These
individualities reflect distinct aspects of preferences captured by different
behaviors, where certain auxiliary behaviors may introduce noise, hindering the
prediction of the target behavior. To address this issue, we propose a user
invariant preference learning for multi-behavior recommendation (UIPL for
short), aiming to capture users' intrinsic interests (referred to as invariant
preferences) from multi-behavior interactions to mitigate the introduction of
noise. Specifically, UIPL leverages the paradigm of invariant risk minimization
to learn invariant preferences. To implement this, we employ a variational
autoencoder (VAE) to extract users' invariant preferences, replacing the
standard reconstruction loss with an invariant risk minimization constraint.
Additionally, we construct distinct environments by combining multi-behavior
data to enhance robustness in learning these preferences. Finally, the learned
invariant preferences are used to provide recommendations for the target
behavior. Extensive experiments on four real-world datasets demonstrate that
UIPL significantly outperforms current state-of-the-art methods.

</details>


### [108] [FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall in Patent Retrieval](https://arxiv.org/abs/2507.14946)
*Amna Ali,Liyanage C. De Silva,Pg Emeroylariffion Abas*

Main category: cs.IR

TL;DR: 本文提出新专利检索方法FullRecall，通过多阶段处理平衡查准率和查全率，实验结果优于基线研究，实现100%查全率，有助于加强专利流程并降低法律风险。


<details>
  <summary>Details</summary>
Motivation: 专利审查员和发明者在验证发明原创性和非显而易见性时面临压力，专利数据复杂增加检索挑战，需设计新检索策略提高查全率。

Method: 提出FullRecall方法，利用IPC引导知识生成信息短语，提取名词短语作为关键信息，选前k个关键短语构建查询以初步检索，再用排序方案精炼结果。

Result: 与基线研究HRR2和ReQ - ReC相比，FullRecall在五个测试用例中均实现100%查全率，而基线研究查全率较低。

Conclusion: FullRecall方法能有效平衡专利检索任务中的查准率和查全率，确保不遗漏相关现有技术，加强专利预申请和审查流程，降低潜在法律风险。

Abstract: Patent examiners and inventors face significant pressure to verify the
originality and non-obviousness of inventions, and the intricate nature of
patent data intensifies the challenges of patent retrieval. Therefore, there is
a pressing need to devise cutting-edge retrieval strategies that can reliably
achieve the desired recall. This study introduces FullRecall, a novel patent
retrieval approach that effectively manages the complexity of patent data while
maintaining the reliability of relevance matching and maximising recall. It
leverages IPC-guided knowledge to generate informative phrases, which are
processed to extract key information in the form of noun phrases characterising
the query patent under observation. From these, the top k keyphrases are
selected to construct a query for retrieving a focused subset of the dataset.
This initial retrieval step achieves complete recall, successfully capturing
all relevant documents. To further refine the results, a ranking scheme is
applied to the retrieved subset, reducing its size while maintaining 100%
recall. This multi-phase process demonstrates an effective strategy for
balancing precision and recall in patent retrieval tasks. Comprehensive
experiments were conducted, and the results were compared with baseline
studies, namely HRR2 [1] and ReQ-ReC [2]. The proposed approach yielded
superior results, achieving 100% recall in all five test cases. However,
HRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and
14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the
second test case, and 0% for the third, fourth, and fifth test cases. The 100%
recall ensures that no relevant prior art is overlooked, thereby strengthening
the patent pre-filing and examination processes, hence reducing potential legal
risks.

</details>


### [109] [Click A, Buy B: Rethinking Conversion Attribution in E- Commerce Recommendations](https://arxiv.org/abs/2507.15113)
*Xiangyu Zeng,Amit Jaspal,Bin Liu,Goutham Panneeru,Kevin Huang,Nicolas Bievre,Mohit Jaggi,Prathap Maniraju,Ankur Jain*

Main category: cs.IR

TL;DR: 电商用户旅程常违反一一对应假设，存在CABB现象，将转化预测重构为多任务问题，引入分类感知协同过滤加权方案，离线评估和在线测试均有提升。


<details>
  <summary>Details</summary>
Motivation: 解决电商中用户点击与购买商品不一致（CABB现象）导致推荐模型学习有偏差、转化率不佳的问题。

Method: 将转化预测重构为多任务问题，用分类感知协同过滤加权方案区分有信息和无关联的CABB转化。

Result: 离线评估降低归一化熵13.9%，在线A/B测试主要业务指标提升0.25%。

Conclusion: 提出的方法能有效改善电商推荐模型的效果，提升转化率。

Abstract: User journeys in e-commerce routinely violate the one-to-one assumption that
a clicked item on an advertising platform is the same item later purchased on
the merchant's website/app. For a significant number of converting sessions on
our platform, users click product A but buy product B -- the Click A, Buy B
(CABB) phenomenon. Training recommendation models on raw click-conversion pairs
therefore rewards items that merely correlate with purchases, leading to biased
learning and sub-optimal conversion rates. We reframe conversion prediction as
a multi-task problem with separate heads for Click A Buy A (CABA) and Click A
Buy B (CABB). To isolate informative CABB conversions from unrelated CABB
conversions, we introduce a taxonomy-aware collaborative filtering weighting
scheme where each product is first mapped to a leaf node in a product taxonomy,
and a category-to-category similarity matrix is learned from large-scale
co-engagement logs. This weighting amplifies pairs that reflect genuine
substitutable or complementary relations while down-weighting coincidental
cross-category purchases. Offline evaluation on e-commerce sessions reduces
normalized entropy by 13.9% versus a last-click attribution baseline. An online
A/B test on live traffic shows +0.25% gains in the primary business metric.

</details>


### [110] [SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search](https://arxiv.org/abs/2507.15245)
*Xiaofeng Shi,Yuduo Li,Qian Kou,Longbin Yu,Jinxin Xie,Hua Zhou*

Main category: cs.IR

TL;DR: 介绍多智能体框架SPAR和基准SPARBench用于学术文献检索，实验表明SPAR性能大幅优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有学术文献检索系统依赖刚性管道且推理能力有限，需更灵活有效的搜索方法。

Method: 引入基于RefChain的查询分解和查询演化的多智能体框架SPAR，构建有专家标注相关性标签的基准SPARBench。

Result: SPAR在AutoScholar和SPARBench上相比最佳基线分别提升高达+56%和+23%的F1值。

Conclusion: SPAR和SPARBench为学术检索研究提供了可扩展、可解释且高性能的基础。

Abstract: Recent advances in large language models (LLMs) have opened new opportunities
for academic literature retrieval. However, existing systems often rely on
rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,
a multi-agent framework that incorporates RefChain-based query decomposition
and query evolution to enable more flexible and effective search. To facilitate
systematic evaluation, we also construct SPARBench, a challenging benchmark
with expert-annotated relevance labels. Experimental results demonstrate that
SPAR substantially outperforms strong baselines, achieving up to +56% F1 on
AutoScholar and +23% F1 on SPARBench over the best-performing baseline.
Together, SPAR and SPARBench provide a scalable, interpretable, and
high-performing foundation for advancing research in scholarly retrieval. Code
and data will be available at: https://github.com/xiaofengShi/SPAR

</details>


### [111] [GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou](https://arxiv.org/abs/2507.15267)
*Ninglu Shao,Jinshan Wang,Chenxu Wang,Qingbiao Li,Xiaoxue Zang,Han Li*

Main category: cs.IR

TL;DR: 文章针对视频相关搜索中的查询推荐场景，首次系统研究其挑战，发布大规模数据集KuaiRS，提出基于LLM的框架GREAT解决I2Q推荐问题，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 当前视频相关搜索的查询推荐场景学术研究和公开数据集稀缺，且现有方法缺乏语义内容与查询的深度交互。

Method: 构建基于查询的trie树，训练时用其增强LLM生成高质量查询的能力，推理阶段引导token生成，最后通过后处理模块优化相关性和文字质量。

Result: 进行了大量离线和在线实验，证明了所提方法的有效性。

Conclusion: 提出的基于LLM的框架GREAT能有效解决视频相关搜索中的I2Q推荐问题。

Abstract: Currently, short video platforms have become the primary place for
individuals to share experiences and obtain information. To better meet users'
needs for acquiring information while browsing short videos, some apps have
introduced a search entry at the bottom of videos, accompanied with recommended
relevant queries. This scenario is known as query recommendation in
video-related search, where core task is item-to-query (I2Q) recommendation. As
this scenario has only emerged in recent years, there is a notable scarcity of
academic research and publicly available datasets in this domain. To address
this gap, we systematically examine the challenges associated with this
scenario for the first time. Subsequently, we release a large-scale dataset
derived from real-world data pertaining to the query recommendation in
video-\textit{\textbf{r}}elated \textit{\textbf{s}}earch on the
\textit{\textbf{Kuai}}shou app (\textbf{KuaiRS}). Presently, existing methods
rely on embeddings to calculate similarity for matching short videos with
queries, lacking deep interaction between the semantic content and the query.
In this paper, we introduce a novel LLM-based framework named \textbf{GREAT},
which \textit{\textbf{g}}uides que\textit{\textbf{r}}y
g\textit{\textbf{e}}ner\textit{\textbf{a}}tion with a \textit{\textbf{t}}rie to
address I2Q recommendation in related search. Specifically, we initially gather
high-quality queries with high exposure and click-through rate to construct a
query-based trie. During training, we enhance the LLM's capability to generate
high-quality queries using the query-based trie. In the inference phase, the
query-based trie serves as a guide for the token generation. Finally, we
further refine the relevance and literal quality between items and queries via
a post-processing module. Extensive offline and online experiments demonstrate
the effectiveness of our proposed method.

</details>


### [112] [Hierarchical Graph Information Bottleneck for Multi-Behavior Recommendation](https://arxiv.org/abs/2507.15395)
*Hengyu Zhang,Chunxu Shen,Xiangguo Sun,Jie Tan,Yanchao Tan,Yu Rong,Hong Cheng,Lingling Yi*

Main category: cs.IR

TL;DR: 提出HGIB框架解决多行为推荐挑战，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多行为推荐方法面临行为分布差异和负迁移问题，需新方法解决。

Method: 提出模型无关的HGIB框架，遵循信息瓶颈原则学习紧凑表征，引入GRE动态修剪冗余边。

Result: 在三个公共数据集、多个工业场景及在线A/B测试中，框架表现出优越效果。

Conclusion: HGIB框架能有效解决多行为推荐中的挑战，提升推荐性能。

Abstract: In real-world recommendation scenarios, users typically engage with platforms
through multiple types of behavioral interactions. Multi-behavior
recommendation algorithms aim to leverage various auxiliary user behaviors to
enhance prediction for target behaviors of primary interest (e.g., buy),
thereby overcoming performance limitations caused by data sparsity in target
behavior records. Current state-of-the-art approaches typically employ
hierarchical design following either cascading (e.g.,
view$\rightarrow$cart$\rightarrow$buy) or parallel
(unified$\rightarrow$behavior$\rightarrow$specific components) paradigms, to
capture behavioral relationships. However, these methods still face two
critical challenges: (1) severe distribution disparities across behaviors, and
(2) negative transfer effects caused by noise in auxiliary behaviors. In this
paper, we propose a novel model-agnostic Hierarchical Graph Information
Bottleneck (HGIB) framework for multi-behavior recommendation to effectively
address these challenges. Following information bottleneck principles, our
framework optimizes the learning of compact yet sufficient representations that
preserve essential information for target behavior prediction while eliminating
task-irrelevant redundancies. To further mitigate interaction noise, we
introduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant
edges through learnable edge dropout mechanisms. We conduct comprehensive
experiments on three real-world public datasets, which demonstrate the superior
effectiveness of our framework. Beyond these widely used datasets in the
academic community, we further expand our evaluation on several real industrial
scenarios and conduct an online A/B testing, showing again a significant
improvement in multi-behavior recommendations. The source code of our proposed
HGIB is available at https://github.com/zhy99426/HGIB.

</details>


### [113] [RankMixer: Scaling Up Ranking Models in Industrial Recommenders](https://arxiv.org/abs/2507.15551)
*Jie Zhu,Zhifang Fan,Xiaoxie Zhu,Yuchen Jiang,Hangyu Wang,Xintian Han,Haoran Ding,Xinmin Wang,Wenlin Zhao,Zhen Gong,Huizhi Yang,Zheng Chai,Zhe Chen,Yuchao Zheng,Qiwei Chen,Feng Zhang,Xun Zhou,Peng Xu,Xiao Yang,Di Wu,Zuotao Liu*

Main category: cs.IR

TL;DR: 提出RankMixer模型，解决推荐系统扩展的两个问题，实验证明其在扩展性、效率等方面表现出色，还通过线上测试验证通用性并提升用户指标。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推动推荐系统扩展，但存在训练和服务成本需满足严格延迟和高QPS需求，以及特征交叉模块无法充分利用现代GPU的问题。

Method: 引入RankMixer，保留Transformer高并行性，用多头令牌混合模块替代二次自注意力机制，用Per - token FFNs进行特征子空间建模和跨特征空间交互，扩展为Sparse - MoE变体，采用动态路由策略。

Result: 在万亿级生产数据集上展示了卓越扩展性，将模型MFU从4.5%提升到45%，模型参数扩展100倍且推理延迟大致不变，线上A/B测试验证通用性，推出1B密集参数RankMixer不增加服务成本并提升用户指标。

Conclusion: RankMixer是一种有效且通用的特征交互架构，可提升推荐系统性能。

Abstract: Recent progress on large language models (LLMs) has spurred interest in
scaling up recommendation systems, yet two practical obstacles remain. First,
training and serving cost on industrial Recommenders must respect strict
latency bounds and high QPS demands. Second, most human-designed
feature-crossing modules in ranking models were inherited from the CPU era and
fail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and
poor scalability. We introduce RankMixer, a hardware-aware model design
tailored towards a unified and scalable feature-interaction architecture.
RankMixer retains the transformer's high parallelism while replacing quadratic
self-attention with multi-head token mixing module for higher efficiency.
Besides, RankMixer maintains both the modeling for distinct feature subspaces
and cross-feature-space interactions with Per-token FFNs. We further extend it
to one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic
routing strategy is adapted to address the inadequacy and imbalance of experts
training. Experiments show RankMixer's superior scaling abilities on a
trillion-scale production dataset. By replacing previously diverse handcrafted
low-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and
scale our ranking model parameters by 100x while maintaining roughly the same
inference latency. We verify RankMixer's universality with online A/B tests
across three core application scenarios (Recommendation, Advertisement and
Search). Finally, we launch 1B Dense-Parameters RankMixer for full traffic
serving without increasing the serving cost, which improves user active days by
0.2% and total in-app usage duration by 0.5%.

</details>


### [114] [Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation](https://arxiv.org/abs/2507.15826)
*Alessandro B. Melchiorre,Elena V. Epure,Shahed Masoudian,Gustavo Escobedo,Anna Hausberger,Manuel Moussallam,Markus Schedl*

Main category: cs.IR

TL;DR: 提出轻量级自然语言音乐推荐框架JAM，用向量翻译建模交互，聚合多模态特征，创建新数据集，结果显示其推荐准确、表示直观且易集成。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在音乐推荐系统中扩展性受高成本和延迟限制，基于检索的方法存在依赖单模态、忽视长期偏好和需全量模型再训练等问题。

Method: 将用户 - 查询 - 项目交互建模为共享潜空间中的向量翻译，通过交叉注意力和稀疏专家混合聚合多模态特征，创建新数据集JAMSessions。

Result: JAM能提供准确推荐，生成适合实际用例的直观表示，且易于与现有音乐推荐栈集成。

Conclusion: JAM是一个有效的自然语言音乐推荐框架，可解决现有方法的问题。

Abstract: Natural language interfaces offer a compelling approach for music
recommendation, enabling users to express complex preferences conversationally.
While Large Language Models (LLMs) show promise in this direction, their
scalability in recommender systems is limited by high costs and latency.
Retrieval-based approaches using smaller language models mitigate these issues
but often rely on single-modal item representations, overlook long-term user
preferences, and require full model retraining, posing challenges for
real-world deployment. In this paper, we present JAM (Just Ask for Music), a
lightweight and intuitive framework for natural language music recommendation.
JAM models user-query-item interactions as vector translations in a shared
latent space, inspired by knowledge graph embedding methods like TransE. To
capture the complexity of music and user intent, JAM aggregates multimodal item
features via cross-attention and sparse mixture-of-experts. We also introduce
JAMSessions, a new dataset of over 100k user-query-item triples with anonymized
user/item embeddings, uniquely combining conversational queries and user
long-term preferences. Our results show that JAM provides accurate
recommendations, produces intuitive representations suitable for practical use
cases, and can be easily integrated with existing music recommendation stacks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [115] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: 本文提出Catalyst正则化方法用于结构化剪枝，理论上确保公平且鲁棒的剪枝，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法在结构化剪枝时存在幅度偏置和剪枝决策边界不稳定的问题。

Method: 确定剪枝操作保持模型性能的精确代数条件，通过辅助催化剂变量在扩展参数空间构建新型正则化器。

Result: 在各种数据集和模型上的剪枝结果优于现有滤波器剪枝方法，证实了Catalyst剪枝的鲁棒性和公平性。

Conclusion: Catalyst正则化能实现公平且鲁棒的剪枝，具有实际有效性。

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [116] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: 提出基于投影空间的剪枝策略，构建PROscore，实现近无损剪枝，打破剪枝中“大小很重要”的观念。


<details>
  <summary>Details</summary>
Motivation: 现有基于重要性的结构化剪枝方法中，幅度重要性等标准限制剪枝决策能力，冗余的大尺寸滤波器难以被剪枝。

Method: 将滤波器置于投影空间以挑战幅度的主导作用，观察梯度下降运动构建PROscore，用于基于重要性的结构化剪枝IPPRO。

Result: 提出的基于投影空间的重要性标准实现近无损剪枝，微调后性能良好。

Conclusion: 打破剪枝中“大小很重要”的观念，从理论和实践上拓展了基于重要性剪枝的边界。

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [117] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: 提出SOAR方法将语言模型集成到自我提升的进化循环中进行程序合成，在ARC - AGI基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有程序合成任务对先进语言模型单次求解有挑战，基于搜索的进化方法受底层生成模型能力限制。

Method: SOAR方法在进化搜索和后见之明学习阶段交替，进化搜索用大语言模型采样和优化候选解，后见之明学习阶段将搜索尝试转化为有效问题 - 解决方案对微调大语言模型能力。

Result: 在ARC - AGI基准测试中，不同模型规模和迭代次数下都有显著性能提升，能解决公共测试集52%的问题。

Conclusion: SOAR方法有效提升程序合成能力，代码已开源。

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [118] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: 本文评估中间（潜空间）融合预测每日抑郁症状的效果，发现其优于传统方法，未来应探索模型可解释性和个体层面预测。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型难以捕捉精神科数据的多模态特性，需要改进早期检测和个性化干预方法，高级融合技术可能更准确且具临床实用性。

Method: 利用BRIGHTEN临床试验数据，比较随机森林（RF）模型的早期融合和通过自编码器与神经网络的组合模型（CM）的中间融合，评估不同时间分割和数据流组合下的表现。

Result: CM在所有设置中均优于RF和线性回归（LR）基线，MSE更低、R2更高，RF有过拟合迹象，CM泛化能力一致，整合所有数据模态时CM表现最佳。

Conclusion: 潜空间融合是多模态心理健康数据预测中传统融合方法的有力替代方案，未来应探索模型可解释性和个体层面预测以用于临床部署。

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [119] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: 本文引入预测代表性（PR）概念用于公平性审计，通过皮肤科案例研究发现AI皮肤癌分类器存在按皮肤类型的性能差异，提出外部可迁移性标准，强调事后公平性审计等重要性。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统在医疗决策中存在的算法偏差和不公平结果问题，特别是针对历史上被边缘化群体。

Method: 引入Predictive Representativity（PR）框架，通过皮肤科案例研究，评估基于AI的皮肤癌分类器在不同数据集上的表现。

Result: 分析揭示AI皮肤癌分类器按皮肤光型存在显著性能差异，深色皮肤个体表现不佳。

Conclusion: 强调事后公平性审计、数据集文档透明和包容性模型验证管道的伦理必要性，提供诊断AI系统结构不平等的可扩展工具。

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [120] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: 本文研究两层神经网络反向传播算法训练解，揭示解空间“黑箱”奥秘并丰富逼近理论。


<details>
  <summary>Details</summary>
Motivation: 理解由反向传播算法得到的、隐藏层具有平滑激活函数的两层神经网络的训练解。

Method: 运用泰勒级数展开构造、结点严格偏序、平滑样条实现和光滑连续性限制四个主要原理。

Result: 证明了任意输入维度的通用逼近性，并进行实验验证。

Conclusion: 揭示了解空间“黑箱”奥秘，新证明丰富了逼近理论。

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [121] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 提出Feature Bank Enhancement (FBE)方法解决距离基方法在OOD检测中对ID样本打分过低问题，在ImageNet - 1k和CIFAR - 10上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有距离基分数函数在OOD检测中，因深度学习导致的数据特征分布偏差和极端特征，使对ID样本打分过低，限制了OOD检测能力。

Method: 提出Feature Bank Enhancement (FBE)方法，利用数据集统计特征识别极端特征并将其约束到分离边界。

Result: 在ImageNet - 1k和CIFAR - 10上实验，方法达到SOTA性能，还进行了理论分析和补充实验。

Conclusion: FBE方法能有效解决距离基方法在OOD检测中的问题，提升检测性能。

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [122] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: 本文提出M - DESIGN，通过自适应融合模型架构修改的先验知识来解决数据库研究中模型细化的问题，在图分析任务上取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有数据库静态模型选择方法忽视任务查询和模型架构变化间的细粒度关系，导致匹配不佳且无法有效细化模型，需填补模型细化研究空白。

Method: 提出知识编织引擎，将模型细化转化为任务元数据的自适应查询问题，利用图关系知识模式匹配和迭代细化候选模型，支持细粒度分析和预测查询规划。

Result: 在图分析任务中，模型知识库丰富了现有基准，提供67,760个图模型的数据记录，在33个数据 - 任务对中的26个中能在有限预算内提供最优模型。

Conclusion: M - DESIGN能有效解决模型细化问题，在图分析任务中表现良好。

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [123] [Pruning Increases Orderedness in Recurrent Computation](https://arxiv.org/abs/2507.14747)
*Yiding Song*

Main category: cs.LG

TL;DR: 受生物大脑循环回路启发，研究方向性对人工神经网络的作用，发现可通过剪枝诱导方向性，方向性非学习必要条件但可能是有利归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 受生物大脑中循环回路普遍存在的启发，研究方向性对人工神经网络的帮助程度。

Method: 将方向性视为神经元间拓扑有序信息流，形式化全连接感知层，应用适当剪枝技术。

Result: 不同随机种子下，剪枝方案能在不影响性能前提下诱导神经元间信息流向更大拓扑有序性。

Conclusion: 方向性不是学习的先决条件，但可能是梯度下降和稀疏化可发现的有利归纳偏置。

Abstract: Inspired by the prevalence of recurrent circuits in biological brains, we
investigate the degree to which directionality is a helpful inductive bias for
artificial neural networks. Taking directionality as topologically-ordered
information flow between neurons, we formalise a perceptron layer with
all-to-all connections (mathematically equivalent to a weight-tied recurrent
neural network) and demonstrate that directionality, a hallmark of modern
feed-forward networks, can be induced rather than hard-wired by applying
appropriate pruning techniques. Across different random seeds our pruning
schemes successfully induce greater topological ordering in information flow
between neurons without compromising performance, suggesting that
directionality is not a prerequisite for learning, but may be an advantageous
inductive bias discoverable by gradient descent and sparsification.

</details>


### [124] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 提出基于聚类的激活模式压缩框架，高效利用大语言模型激活稀疏性，减少计算开销并保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型激活稀疏性可降低计算成本，但直接在神经元层面预测激活模式计算开销大，需可扩展的预测方法。

Method: 将相似激活模式分组为少量代表性聚类，通过预测聚类分配而非单个神经元状态来推断激活模式。

Result: 聚类精度达79.34%，优于标准二元聚类方法，在簇数量足够时困惑度低至12.49。

Conclusion: 该聚类方法为激活模式预测的未来工作奠定基础，有助于大规模语言模型的高效推理。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [125] [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](https://arxiv.org/abs/2507.15727)
*Xuchuang Wang,Bo Sun,Hedyeh Beyhaghi,John C. S. Lui,Mohammad Hajiesmaili,Adam Wierman*

Main category: cs.LG

TL;DR: 本文提出多智能体滑雪租赁问题，定义三种竞争比率，设计分析确定性和随机策略，对称策略表现更好并给出上下界。


<details>
  <summary>Details</summary>
Motivation: 将经典滑雪租赁困境推广到群体场景，考虑个体和共享成本以及动态状态。

Method: 定义三种竞争比率，设计确定性和随机策略，确定性策略用状态感知阈值函数，随机策略从定制分布采样阈值。

Result: 对称策略优于非对称策略，给出竞争比率上下界。

Conclusion: 将经典滑雪租赁见解扩展到多智能体场景，对不确定下群体决策有理论和实践意义。

Abstract: This paper introduces a novel multi-agent ski-rental problem that generalizes
the classical ski-rental dilemma to a group setting where agents incur
individual and shared costs. In our model, each agent can either rent at a
fixed daily cost, or purchase a pass at an individual cost, with an additional
third option of a discounted group pass available to all. We consider scenarios
in which agents' active days differ, leading to dynamic states as agents drop
out of the decision process. To address this problem from different
perspectives, we define three distinct competitive ratios: overall,
state-dependent, and individual rational. For each objective, we design and
analyze optimal deterministic and randomized policies. Our deterministic
policies employ state-aware threshold functions that adapt to the dynamic
states, while our randomized policies sample and resample thresholds from
tailored state-aware distributions. The analysis reveals that symmetric
policies, in which all agents use the same threshold, outperform asymmetric
ones. Our results provide competitive ratio upper and lower bounds and extend
classical ski-rental insights to multi-agent settings, highlighting both
theoretical and practical implications for group decision-making under
uncertainty.

</details>


### [126] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: 本文提出用于毫米波MIMO系统的BAE，利用数字孪生、迁移学习、SHAP和DkNN算法，减少数据需求与训练开销，提升鲁棒性和透明度。


<details>
  <summary>Details</summary>
Motivation: 深度学习解决方案在毫米波系统波束对齐中面临高数据收集开销、硬件限制、缺乏可解释性和易受对抗攻击等问题，需要构建可解释且鲁棒的系统。

Method: 提出BAE，利用RSSI测量预测最佳窄波束；借助数字孪生生成合成数据，用迁移学习微调模型；用SHAP对输入特征排序，用DkNN算法检测分布外输入。

Result: 减少70%的真实世界数据需求、62%的波束训练开销，将异常检测鲁棒性提高8.5倍，实现接近最优的频谱效率和透明决策。

Conclusion: 所提框架相比传统基于softmax的深度学习模型，在毫米波MIMO系统波束对齐中表现更优，能实现可解释且鲁棒的决策。

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [127] [Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm](https://arxiv.org/abs/2507.15132)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: 本文提出遗传算法优化数据集复杂度以生成不同难度数据集，实验验证了算法有效性及数据复杂度与识别质量的相关性。


<details>
  <summary>Details</summary>
Motivation: 研究界需更先进合成数据生成器评估机器学习方法，本文旨在增加不同问题复杂度数据集的可用性。

Method: 提出遗传算法，针对分类和回归任务优化一组问题复杂度度量至特定目标。分类用10个复杂度度量，回归选4个有优化潜力的度量。通过线性特征投影转换合成数据集以达到目标复杂度值。

Result: 实验证实遗传算法可生成不同难度数据集，评估显示生成数据的复杂度与识别质量相关。

Conclusion: 所提遗传算法能有效生成不同复杂度数据集，数据复杂度会影响识别质量。

Abstract: The research community continues to seek increasingly more advanced synthetic
data generators to reliably evaluate the strengths and limitations of machine
learning methods. This work aims to increase the availability of datasets
encompassing a diverse range of problem complexities by proposing a genetic
algorithm that optimizes a set of problem complexity measures for
classification and regression tasks towards specific targets. For
classification, a set of 10 complexity measures was used, while for regression
tasks, 4 measures demonstrating promising optimization capabilities were
selected. Experiments confirmed that the proposed genetic algorithm can
generate datasets with varying levels of difficulty by transforming
synthetically created datasets to achieve target complexity values through
linear feature projections. Evaluations involving state-of-the-art classifiers
and regressors revealed a correlation between the complexity of the generated
data and the recognition quality.

</details>


### [128] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 针对传统监督深度学习方法在智能故障诊断中数据和标签获取难、数据分布差异影响模型性能的问题，提出半监督联邦学习框架SSFL - DCSL，实验表明在数据标注率低时能提升准确率。


<details>
  <summary>Details</summary>
Motivation: 传统监督深度学习方法在智能故障诊断中需要大量训练数据和标签，数据和标签获取难，且不同客户端数据分布差异影响模型性能。

Method: 提出SSFL - DCSL框架，设计基于拉普拉斯分布的样本加权函数，引入双对比损失，在服务器上聚合和更新局部原型。

Result: 在三个数据集上实验，在数据标注率仅10%的最具挑战性任务中，相比现有方法准确率提升1.15% - 7.85%。

Conclusion: SSFL - DCSL框架能有效解决分布式客户端数据和标签稀缺问题，实现知识共享，防止局部模型发散，提升智能故障诊断准确率。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [129] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: 本文探索投资者驱动市场动态中牛熊机制潜力，提出B4模型，实验显示该模型预测市场趋势表现优越且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 金融市场行为复杂，数据异质性和投资者观点差异带来偏差，使市场动态建模困难，需探索新机制。

Method: 对真实金融数据集进行实证分析，提出B4模型，将价格序列和外部信号嵌入共享潜在空间，有惯性配对模块和双竞争机制。

Result: 模型在预测市场趋势上表现优越，能对偏差、投资者行为和市场动态的相互作用提供可解释见解。

Conclusion: B4模型可有效建模偏差驱动的不对称性、行为惯性和市场异质性，用于市场趋势预测。

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [130] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: 本文提出FedStrategist元学习框架，动态选择聚合规则防御联邦学习模型中毒攻击，实验证明其在多样场景有效，还可控制策略权衡性能与安全。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受模型中毒攻击，现有静态防御在适应对手或异构数据环境中效果不佳。

Method: 引入FedStrategist框架，设计轻量级上下文多臂老虎机代理，根据实时诊断指标从防御库中动态选择最优聚合规则。

Result: 实验表明无单一静态规则普遍最优，自适应代理在多样场景学习到更优策略，能应对复杂情况，还可通过参数控制策略。

Conclusion: 该工作为创建有弹性和智能的去中心化AI系统提供新的、实用且可分析的方法。

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [131] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: 提出无训练的KV缓存优化范式LaCache，解决大语言模型长距离建模问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长序列时KV对数量增加导致效率瓶颈，需要解决长距离能力和连续生成的内存问题。

Method: 提出LaCache，包含梯形KV缓存模式和迭代压缩机制。

Result: 跨任务、基准和模型的实验验证了LaCache增强大语言模型长距离能力的有效性。

Conclusion: LaCache能有效提升大语言模型的长距离能力。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [132] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: 本文开发了用于聋哑或听力受损人士的深度学习辅助设备系统，含JerryNet等组件，在数据集上表现出色，有未来应用潜力。


<details>
  <summary>Details</summary>
Motivation: 开发能实时准确定位和识别声源的无障碍设备，填补针对弱势群体研究的空白。

Method: 系统包含JerryNet、音频分类模型和多模态集成模型三个主要组件，硬件由四个麦克风、相机和腕带组成。

Result: JerryNet在声音方向定位上精度达91.1%，CLAP模型在自定义和AudioSet数据集上准确率分别为98.5%和95%，音频 - 视觉定位模型的cIoU为0.892，AUC为0.658，均超越对比模型。

Conclusion: 该研究有很多未来潜力，为新一代无障碍设备的创造铺平了道路。

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [133] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: 提出结合非线性效用聚合与几何感知查询选择的交互式学习框架解决模式挖掘中的模式爆炸问题，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决模式挖掘中的模式爆炸问题。

Method: 提出交互式学习框架，通过Choquet积分建模用户偏好，利用版本空间的几何结构指导信息比较选择，采用带紧密距离边界的分支定界策略。

Result: 在UCI数据集实验中，比ChoquetRank等现有方法表现好，用更少用户交互实现更好排名准确率。

Conclusion: 所提方法能有效解决模式挖掘的模式爆炸问题，优于现有方法。

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [134] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: 本文提出首个运行时间和近似因子在k上非指数级的多项式时间确定性算法，用于近似k - 子空间中位数，并给出开放代码和实验结果。


<details>
  <summary>Details</summary>
Motivation: k - 子空间中位数比均值更稀疏且对噪声/离群值更鲁棒，但在k < d - 1时是非凸的，难以近似，因此需要有效算法解决。

Method: 设计了一个运行时间和近似因子在k上非指数级的多项式时间确定性算法。

Result: 算法的乘法近似因子为√d，运行时间是输入规模的多项式。

Conclusion: 该技术有望用于其他相关问题，如z ∉ {1, 2}时的ℓ₂,z范数距离问题及处理离群值/稀疏性问题。

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [135] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 研究提出用平均绝对SHAP值计算绿氢产量和选址适宜性指数的AI框架，准确率98%，为数据稀缺地区规划提供工具。


<details>
  <summary>Details</summary>
Motivation: 各国寻求化石燃料可持续替代方案，绿氢是有前景途径，但确定制氢最佳选址需综合多因素且直接氢产量数据有限。

Method: 构建由无监督多变量聚类、监督机器学习分类器和SHAP算法组成的多阶段管道，在集成气象、地形和时间数据集上训练。

Result: 揭示选址适宜性空间模式和变量相对影响，模型预测准确率98%，确定阿曼绿氢选址最有影响的因素。

Conclusion: 为数据稀缺地区提供客观可复制工具，供行业利益相关者和政策制定者用于绿氢基础设施规划和决策。

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [136] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出新的POGM方法解决基于梯度的领域泛化问题，在DomainBed数据集实验有竞争力且具计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的领域泛化方法存在梯度波动和计算开销大的问题。

Method: 提出Pareto Optimality Gradient Matching (POGM)方法，将梯度轨迹作为数据，在元学习器独立训练，元更新时最大化GIP并限制梯度偏离。

Result: 在DomainBed数据集上POGM比其他基线方法表现有竞争力，且实现了计算效率。

Conclusion: POGM方法有效解决了现有方法的问题，在领域泛化问题上有良好效果和计算效率。

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [137] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: 本文提出去中心化框架FLock用于安全高效协作式大语言模型微调，实验表明其能抵御攻击、促进知识转移并提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型微调在去中心化方案中有控制缺失和高开销问题，标准联邦学习有单点攻击和中毒攻击风险，在异构无信任环境下扩展到70B参数模型是瓶颈。

Method: 引入FLock框架，将基于区块链的信任层与经济激励结合，用安全可审计协议替代中央聚合器。

Result: 实现70B大语言模型在安全、多领域、去中心化环境下微调，防御后门中毒攻击，减少超68%的对抗攻击成功率，全局模型跨领域泛化能力更优。

Conclusion: FLock框架可用于安全高效的大语言模型协作微调。

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [138] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出NanoPro - 3M数据集和NanoProFormer模型，用于预测纳米材料 - 蛋白质亲和力，有良好泛化性和多下游任务适用性。


<details>
  <summary>Details</summary>
Motivation: 理解纳米材料与蛋白质相互作用对医学和环境科学很重要，但现有进展受限于有限数据集和模型泛化性不足。

Method: 构建NanoPro - 3M数据集，利用多模态表示学习提出NanoProFormer模型。

Result: 多模态建模优于单模态方法，能识别关键决定因素，可用于零样本推理和微调等下游任务。

Conclusion: 为高性能、通用的纳米材料 - 蛋白质相互作用预测奠定基础，减少实验依赖并加速体外应用。

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [139] [Better Models and Algorithms for Learning Ising Models from Dynamics](https://arxiv.org/abs/2507.15173)
*Jason Gaitonde,Ankur Moitra,Elchanan Mossel*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of learning the structure and parameters of the Ising
model, a fundamental model of high-dimensional data, when observing the
evolution of an associated Markov chain. A recent line of work has studied the
natural problem of learning when observing an evolution of the well-known
Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Mossel STOC 2024], which provides an arguably more realistic
generative model than the classical i.i.d. setting. However, this prior work
crucially assumes that all site update attempts are observed, \emph{even when
this attempt does not change the configuration}: this strong observation model
is seemingly essential for these approaches. While perhaps possible in
restrictive contexts, this precludes applicability to most realistic settings
where we can observe \emph{only} the stochastic evolution itself, a minimal and
natural assumption for any process we might hope to learn from. However,
designing algorithms that succeed in this more realistic setting has remained
an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Moitra, Mossel, STOC 2025].
  In this work, we give the first algorithms that efficiently learn the Ising
model in this much more natural observation model that only observes when the
configuration changes. For Ising models with maximum degree $d$, our algorithm
recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log
n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time,
which qualitatively matches the state-of-the-art even in the i.i.d. setting in
a much weaker observation model. Our analysis holds more generally for a
broader class of reversible, single-site Markov chains that also includes the
popular Metropolis chain by leveraging more robust properties of reversible
Markov chains.

</details>


### [140] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: 介绍线性化扩散映射（LDM）这一新的线性降维方法，通过实验表明其与PCA相比能捕捉数据集不同几何特征，有潜在应用价值。


<details>
  <summary>Details</summary>
Motivation: 结合基于扩散的非线性方法的几何直觉与线性嵌入的计算简单、高效和可解释性。

Method: 通过对扩散映射核的线性近似构建LDM。

Result: 在合成数据集和真实世界基准测试中，LDM在有显式流形结构的数据集上优于PCA，LDM核矩阵的完全正性使非负矩阵分解可直接应用。

Conclusion: LDM是有价值的线性降维技术，有理论和实际扩展潜力。

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [141] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: 提出UFO用于强化学习，提升大推理模型多轮推理能力，实验表明能提高多轮推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法训练的大推理模型在多轮问题解决中存在能力缺失，难以根据反馈修正答案。

Method: 引入Unary Feedback as Observation (UFO)进行多轮强化学习，设计奖励结构引导模型仔细作答。

Result: 使用UFO的强化学习训练保持单轮性能，多轮推理准确率最高提升14%。

Conclusion: UFO可提升大推理模型在多轮问题解决中对反馈的反应能力。

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [142] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: 文章识别机器学习模型中不可靠行为“glitches”，给出定义，证明检测问题为NP - 完全问题，提出搜索算法并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 许多关键决策任务依赖机器学习模型，需保证其决策可信可靠、输出一致，而“glitches”会损害模型可靠性。

Method: 给出“glitches”正式定义，用知名模型和数据集证明其存在；对GBDT模型进行算法搜索，将问题编码为MILP求解。

Result: 证明树集成中检测“glitches”问题在树深度为4时是NP - 完全问题，算法在GBDT基准测试中展示了有效性和计算可行性。

Conclusion: “glitches”广泛存在，通常预示模型的潜在不一致性，所提算法可有效搜索GBDT模型中的“glitches”。

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [143] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: 现有生成式AI模型致DeepFakes风险大，此前研究未充分解决深伪检测公平性问题，尤其个体公平性。本文指出原个体公平原则在深伪检测中失效，提出可集成框架提升个体公平性和泛化性，实验表明该方法效果好。


<details>
  <summary>Details</summary>
Motivation: 当前深伪检测公平性未充分解决，个体公平性研究不足，原个体公平原则在深伪检测中存在问题。

Method: 提出可集成到现有深伪检测器的通用框架来提升个体公平性和泛化性。

Result: 在主流深伪数据集上的大量实验表明，该方法显著提升个体公平性，保持了稳健检测性能，优于现有方法。

Conclusion: 提出的框架能有效解决深伪检测中的个体公平性问题，提升检测性能。

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [144] [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740)
*Andrew Wang,Elisa Nguyen,Runshi Yang,Juhan Bae,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: 提出算法ASTRA用于训练数据归因，提升iHVP近似准确性以改善TDA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的训练数据归因（TDA）方法中，逆Hessian向量积（iHVP）难以高效近似。

Method: 引入算法ASTRA，在Neumann级数迭代上使用EKFAC预调节器得到准确的iHVP近似。

Result: ASTRA易于调参，比Neumann级数迭代所需迭代次数少，比基于EKFAC的近似更准确。

Conclusion: 提高iHVP近似的准确性可显著提高TDA性能。

Abstract: Training data attribution (TDA) provides insights into which training data is
responsible for a learned model behavior. Gradient-based TDA methods such as
influence functions and unrolled differentiation both involve a computation
that resembles an inverse Hessian-vector product (iHVP), which is difficult to
approximate efficiently. We introduce an algorithm (ASTRA) which uses the
EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP
approximation for TDA. ASTRA is easy to tune, requires fewer iterations than
Neumann series iterations, and is more accurate than EKFAC-based
approximations. Using ASTRA, we show that improving the accuracy of the iHVP
approximation can significantly improve TDA performance.

</details>


### [145] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: 本文开发、部署并验证了四个机器学习模型，用于预测环形几何结构中的临界热通量（CHF），这些模型显著优于经验关联模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法预测 CHF 存在局限性，纯数据驱动代理模型有不足，环形几何结构特定的 ML 模型未在热工水力代码中应用，因此开展研究。

Method: 使用 CTF 子通道代码开发四个 ML 模型，以 Biasi、Bowring 和 Katto 三个经验关联模型为基础模型进行对比，用四个数据集的 577 个实验数据点训练和测试 ML 模型。

Result: 经验关联模型的 CHF 预测平均相对误差超 26%，ML 驱动模型平均相对误差低于 3.5%，超 10% 误差范围的点不超过一个。

Conclusion: 混合 ML 模型在预测环形几何结构的 CHF 上显著优于经验关联模型。

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [146] [Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization](https://arxiv.org/abs/2507.14746)
*Bach Do,Nafeezat A. Ajenifuja,Taiwo A. Adebiyi,Ruda Zhang*

Main category: cs.LG

TL;DR: 论文提出从高斯过程生成后验样本的两种抽样方法，阐述其在全局敏感性分析和优化中的应用，并通过数值示例验证。


<details>
  <summary>Details</summary>
Motivation: 高保真模拟和物理实验成本高，限制其在全局敏感性分析和优化中的应用，高斯过程作为代理回归模型可基于有限高质量观测提供不确定性感知预测，但在工程优化领域抽样受关注少。

Method: 提出随机傅里叶特征和路径条件两种抽样方法从高斯过程生成后验样本，简述替代方法，并说明生成样本在全局敏感性分析、单目标优化和多目标优化中的应用。

Result: 通过一系列数值示例展示了抽样方法的成功应用。

Conclusion: 所提出的抽样方法可有效应用于全局敏感性分析和优化等工程任务。

Abstract: High-fidelity simulations and physical experiments are essential for
engineering analysis and design. However, their high cost often limits their
applications in two critical tasks: global sensitivity analysis (GSA) and
optimization. This limitation motivates the common use of Gaussian processes
(GPs) as proxy regression models to provide uncertainty-aware predictions based
on a limited number of high-quality observations. GPs naturally enable
efficient sampling strategies that support informed decision-making under
uncertainty by extracting information from a subset of possible functions for
the model of interest. Despite their popularity in machine learning and
statistics communities, sampling from GPs has received little attention in the
community of engineering optimization. In this paper, we present the
formulation and detailed implementation of two notable sampling methods --
random Fourier features and pathwise conditioning -- for generating posterior
samples from GPs. Alternative approaches are briefly described. Importantly, we
detail how the generated samples can be applied in GSA, single-objective
optimization, and multi-objective optimization. We show successful applications
of these sampling methods through a series of numerical examples.

</details>


### [147] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: 本文探讨用影响函数近似法过滤奖励模型训练数据集，实验表明移除10%训练样本后重训练准确率有小提升，且梯度相似度检测有用样本效果更好。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调所用数据集有噪声，希望找到方法检测并剔除对验证集性能有害的训练样本。

Method: 采用共轭梯度近似影响函数过滤TL;DR数据集。

Result: 移除10%训练样本后，重训练准确率提升1.5%；梯度相似度检测有用样本表现优于影响函数。

Conclusion: 局部曲率对检测有害样本重要，但对识别有用样本作用较小。

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [148] [Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning](https://arxiv.org/abs/2507.14748)
*Patrik Reizinger,Bálint Mucsányi,Siyuan Guo,Benjamin Eysenbach,Bernhard Schölkopf,Wieland Brendel*

Main category: cs.LG

TL;DR: 本文从可识别表示学习的角度研究MISL，以CSF方法为例证明其能恢复环境真实特征，还解释了互信息目标的影响和熵正则化器的缺点，并进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前MISL中表示和互信息参数化的作用在理论上尚不明确，需要进一步研究。

Method: 通过可识别表示学习的视角研究MISL，聚焦于Contrastive Successor Features (CSF)方法，并进行理论证明和实验验证。

Result: 证明CSF能在一定线性变换下恢复环境的真实特征，在MuJoCo和DeepMind Control中实证验证了相关论断。

Conclusion: 首次为强化学习中的表示学习提供可识别性保证，有助于解释不同互信息目标的影响和熵正则化器的缺点。

Abstract: Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.

</details>


### [149] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: 提出Solo Connection方法，在自然语言生成基准上优于LoRA，减少可训练参数，受同伦理论启发，关注长跳跃连接以提升模型适应新任务能力。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法LoRA主要调整注意力权重矩阵，且随着模型架构扩展，需重新审视微调时跳跃连接的使用，故提出新方法。

Method: 引入Solo Connection，在解码器块级别调整表示，引入可训练线性变换在零向量和特定任务表示间插值，关注长跳跃连接。

Result: Solo Connection在E2E自然语言生成基准上优于LoRA，减少可训练参数，相对LoRA减少59%，相对GPT2全微调减少超99%。

Conclusion: Solo Connection是一种有效方法，能提升模型在新任务上的适应能力，利用预训练知识。

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [150] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: 传统实时异常检测方法有局限，本文提出INCADET框架用于实时网络攻击检测，实验显示其在准确性、鲁棒性和适应性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 实时关键基础设施面临网络攻击威胁，传统检测方法有过多误报，现有因果关系建模方法难以应用于实时场景，需新方法。

Method: 提出INCADET框架，含早期症状检测、增量因果图学习、因果图分类三个模块，通过跨时间窗口增量更新因果图捕捉系统行为。

Result: 在真实关键基础设施数据集上实验表明，INCADET在不断演变的攻击场景中比静态因果和深度时间基线模型更准确、鲁棒和适应。

Conclusion: INCADET框架能有效解决实时网络攻击检测问题，具有更好性能。

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [151] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 提出分布遗忘框架，通过选择少量点实现数据编辑，实验显示减少删除量且不影响保留性能。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘工具多以样本为导向，简单删除会留残差信号，需删除整个主题域满足隐私、法律或质量要求。

Method: 引入分布遗忘框架，用KL散度量化，推导高斯情况下的帕累托前沿，提出距离选择规则。

Result: 在多个数据集实验中，比随机删除减少15 - 72%的删除量，对保留性能影响可忽略。

Conclusion: 分布遗忘框架有效减少删除量，能保证模型性能。

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>


### [152] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: 本文分析简单测试时缩放方法，发现缩放行为主要源于设置最大长度缩小计算量，微调长CoT数据无显著影响，追加“Wait”会导致不一致。o1类模型与简单测试时缩放表现不同，强调缩放目标是提升性能。


<details>
  <summary>Details</summary>
Motivation: 分析简单测试时缩放方法的具体情况，明确其缩放行为的影响因素及与o1类模型的差异。

Method: 对简单测试时缩放方法进行分析，对比不同操作（设置最大长度、追加“Wait”、微调长CoT数据）对缩放行为的影响，以及与o1类模型的表现差异。

Result: 缩放行为主要源于设置最大长度缩小计算量；微调长CoT数据无显著影响；追加“Wait”会导致模型在解之间振荡；o1类模型自然缩放时能超越峰值性能，简单测试时缩放缩小计算量会降低性能上限。

Conclusion: 虽然通过缩小计算量复制o1模型测试时缩放行为较简单，但缩放测试时计算量的目标应是提升性能而非仅复制行为。

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [153] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: 本文研究利用干预模型将强化学习应用于大规模随机优化问题，以供应链库存管理为例，提出分解流程为可组合模块的方法提升性能，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 高效应用强化学习解决大规模随机优化问题。

Method: 利用预训练深度学习模型模拟和组合随机过程，采用深度强化学习模型学习和预测，引入约束协调机制，将供应链流程分解为可组合的深度学习模块。

Result: 在大规模真实数据集上取得了更好的性能。

Conclusion: 提出的方法有效，同时指出了未来可进一步研究的开放性问题。

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [154] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: 本文提出用于节点分类的ReDiSC模型，通过重参数化掩码扩散模型估计节点标签联合分布，实验显示其在不同图上表现优越且能处理大规模数据。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在优化目标中假设节点标签条件独立，与图中节点标签存在相关性的直观观察相矛盾，需进行结构化预测。

Method: 提出ReDiSC模型，使用重参数化掩码扩散模型估计节点标签联合分布，通过变分期望最大化（EM）框架学习。

Result: 理论分析显示ReDiSC在E步比DPM - SNC更有效，M步目标与流行的GNN和标签传播混合方法相关；实验表明ReDiSC在不同图上比现有方法表现好，能处理大规模数据。

Conclusion: ReDiSC在结构化节点分类任务中有显著实际优势。

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [155] [Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification](https://arxiv.org/abs/2507.15240)
*Le Peng,Yash Travadi,Chuan He,Ying Cui,Ju Sun*

Main category: cs.LG

TL;DR: 研究不平衡分类的三个直接指标优化问题，提出精确约束重新表述方法，实验显示优于现有方法，框架有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有不平衡分类方法在类别重要性不同或特定指标需达规定水平的场景有不足。

Method: 首次引入精确约束重新表述方法解决直接指标优化问题，用精确惩罚法求解。

Result: 在多个基准数据集实验中，该方法在三个直接指标优化问题上优于现有方法。

Conclusion: 精确重新表述和优化框架适用于二元不平衡分类及更广泛的直接指标优化问题。

Abstract: For classification with imbalanced class frequencies, i.e., imbalanced
classification (IC), standard accuracy is known to be misleading as a
performance measure. While most existing methods for IC resort to optimizing
balanced accuracy (i.e., the average of class-wise recalls), they fall short in
scenarios where the significance of classes varies or certain metrics should
reach prescribed levels. In this paper, we study two key classification
metrics, precision and recall, under three practical binary IC settings: fix
precision optimize recall (FPOR), fix recall optimize precision (FROP), and
optimize $F_\beta$-score (OFBS). Unlike existing methods that rely on smooth
approximations to deal with the indicator function involved, \textit{we
introduce, for the first time, exact constrained reformulations for these
direct metric optimization (DMO) problems}, which can be effectively solved by
exact penalty methods. Experiment results on multiple benchmark datasets
demonstrate the practical superiority of our approach over the state-of-the-art
methods for the three DMO problems. We also expect our exact reformulation and
optimization (ERO) framework to be applicable to a wide range of DMO problems
for binary IC and beyond. Our code is available at
https://github.com/sun-umn/DMO.

</details>


### [156] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: 研究含环境异质性的联邦强化学习框架FRL - EH，提出新全局目标函数，设计FedRQ算法并证明收敛性，扩展至连续状态空间，实验验证算法有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实中本地环境存在统计异质性，需一种能在异质环境及其扰动下保证性能的联邦强化学习框架。

Method: 提出新的全局目标函数，设计FedRQ算法并证明收敛性，用期望分位数损失将其扩展到连续状态空间。

Result: 所提出的FRL算法在不同异质环境中都有效且鲁棒，性能优于现有先进的FRL算法。

Conclusion: 提出的FRL - EH框架及相关算法能有效解决环境异质性问题，在异质环境中表现良好。

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [157] [MAP Estimation with Denoisers: Convergence Rates and Guarantees](https://arxiv.org/abs/2507.15397)
*Scott Pesme,Giacomo Meanti,Michael Arbel,Julien Mairal*

Main category: cs.LG

TL;DR: 本文证明在对数凹性假设下，一个与实际中常用算法密切相关的简单算法可收敛到近端算子，为一类经验成功但此前基于启发式的方法提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 实际中常用于解决最大后验优化问题的启发式迭代方案中，负对数先验的近端算子难以处理，用预训练去噪器替代缺乏理论依据。

Method: 在对数凹性假设下，分析一个简单算法，并将其解释为对平滑近端目标的梯度下降。

Result: 证明该简单算法可收敛到近端算子。

Conclusion: 为一类经验成功但此前基于启发式的方法提供了理论基础。

Abstract: Denoiser models have become powerful tools for inverse problems, enabling the
use of pretrained networks to approximate the score of a smoothed prior
distribution. These models are often used in heuristic iterative schemes aimed
at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal
operator of the negative log-prior plays a central role. In practice, this
operator is intractable, and practitioners plug in a pretrained denoiser as a
surrogate-despite the lack of general theoretical justification for this
substitution. In this work, we show that a simple algorithm, closely related to
several used in practice, provably converges to the proximal operator under a
log-concavity assumption on the prior $p$. We show that this algorithm can be
interpreted as a gradient descent on smoothed proximal objectives. Our analysis
thus provides a theoretical foundation for a class of empirically successful
but previously heuristic methods.

</details>


### [158] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: 本文将知识蒸馏表述为条件生成问题，提出GenDD框架，引入Split Tokenization策略和Distribution Contraction技术，在多类型数据上实验效果好。


<details>
  <summary>Details</summary>
Motivation: 解决朴素GenDD基线面临的高维优化难题和缺乏标签语义监督的问题。

Method: 提出GenDD框架，引入Split Tokenization策略实现无监督KD，开发Distribution Contraction技术将标签监督融入重建目标。

Result: GenDD在无监督设置中表现出色，在ImageNet验证集上显著超越KL基线；有标签监督时，ResNet - 50在600轮训练后达到82.28%的top - 1准确率。

Conclusion: GenDD是有效的知识蒸馏方法，在无监督和有监督场景都有优异表现。

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [159] [GeoHNNs: Geometric Hamiltonian Neural Networks](https://arxiv.org/abs/2507.15678)
*Amine Mohamed Aboussalah,Abdessalam Ed-dib*

Main category: cs.LG

TL;DR: 介绍了Geometric Hamiltonian Neural Networks (GeoHNN)框架，该框架编码物理定律几何先验学习动力学，实验显示其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习常见方法常忽略物理底层几何结构，物理信息神经网络可能违反基本物理原理，导致长期预测不稳定。

Method: 引入GeoHNN框架，强制实现两种基本结构：通过在对称正定矩阵自然数学空间中参数化惯性矩阵实现惯性的黎曼几何；使用约束自动编码器确保在降维潜在空间中相空间体积守恒，以实现相空间的辛几何。

Result: 在从耦合振子到高维可变形物体等系统的实验中，GeoHNN显著优于现有模型，实现了更好的长期稳定性、准确性和能量守恒。

Conclusion: 嵌入物理几何不仅具有理论吸引力，也是创建物理世界稳健且可泛化模型的实际必要条件。

Abstract: The fundamental laws of physics are intrinsically geometric, dictating the
evolution of systems through principles of symmetry and conservation. While
modern machine learning offers powerful tools for modeling complex dynamics
from data, common methods often ignore this underlying geometric fabric.
Physics-informed neural networks, for instance, can violate fundamental
physical principles, leading to predictions that are unstable over long
periods, particularly for high-dimensional and chaotic systems. Here, we
introduce \textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework
that learns dynamics by explicitly encoding the geometric priors inherent to
physical laws. Our approach enforces two fundamental structures: the Riemannian
geometry of inertia, by parameterizing inertia matrices in their natural
mathematical space of symmetric positive-definite matrices, and the symplectic
geometry of phase space, using a constrained autoencoder to ensure the
preservation of phase space volume in a reduced latent space. We demonstrate
through experiments on systems ranging from coupled oscillators to
high-dimensional deformable objects that GeoHNN significantly outperforms
existing models. It achieves superior long-term stability, accuracy, and energy
conservation, confirming that embedding the geometry of physics is not just a
theoretical appeal but a practical necessity for creating robust and
generalizable models of the physical world.

</details>


### [160] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: 提出信号骰子相似系数（SDSC）用于时间序列自监督表示学习，结合混合损失函数，实验表明其在预测和分类基准上表现良好，支持结构感知指标作为传统距离方法的替代方案。


<details>
  <summary>Details</summary>
Motivation: 多数信号自监督学习方法采用基于距离的目标函数，存在对幅度敏感、波形极性不变和尺度无界等问题，阻碍语义对齐和降低可解释性。

Method: 提出SDSC量化时间信号的结构一致性，通过减去1并应用Heaviside函数的可微近似作为损失，还提出混合损失公式结合SDSC和MSE。

Result: 基于SDSC的预训练在预测和分类基准上取得与MSE相当或更好的性能，特别是在域内和低资源场景。

Conclusion: 信号表示中的结构保真度可提高语义表示质量，结构感知指标可作为传统距离方法的可行替代方案。

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [161] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: 本文提出用PU学习框架从无标签数据中识别对照组，评估该方法在模拟和真实数据上的效果，结果显示其能有效识别对照组并估算接近真实值的ATE，对观测因果推断有重要意义。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，特别是观测性研究，常缺乏明确标记的对照组，需找到从无标签数据中识别对照组的方法。

Method: 提出PU学习框架，利用仅有的处理组（阳性）单元从无标签单元中识别对照组，使用模拟和真实数据评估该方法，构建因果图生成合成数据，将方法应用于可持续农业真实数据。

Result: PU学习能仅基于处理组从无标签数据中成功识别对照组，通过所得对照组估算的ATE接近真实值。

Conclusion: 该方法对观测因果推断有重要意义，能在随机实验困难或成本高的领域，如地球、环境和农业科学，利用现有数据开展大量准实验。

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [162] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: 本文研究无限期平稳平均场博弈的最大因果熵逆强化学习问题，提出拉格朗日松弛方法并通过梯度上升算法求解，在平均场交通路由博弈中验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有平均场博弈逆强化学习方法多将奖励函数限制为固定有限基函数的线性组合且多依赖有限期公式，本文旨在克服这些限制。

Method: 引入拉格朗日松弛将最大因果熵逆强化学习问题转化为无约束对数似然最大化问题，用梯度上升算法求解，证明相关软贝尔曼算子的弗雷歇可微性以建立对数似然目标的平滑性。

Result: 在平均场交通路由博弈中准确恢复了专家行为。

Conclusion: 所提出的方法在解决无限期平稳平均场博弈的最大因果熵逆强化学习问题上是有效的。

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [163] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: 本文追溯自注意力机制概念起源，指出它是无限特征选择（Inf - FS）的特例，统一了机器学习研究并强调共同数学基础。


<details>
  <summary>Details</summary>
Motivation: 追溯自注意力机制在多领域的概念起源，统一机器学习研究。

Method: 通过多领域对亲和矩阵的共同依赖追溯起源，对比自注意力机制和Inf - FS。

Result: 发现自注意力机制是Inf - FS的特例，二者底层结构相同但亲和矩阵定义和应用有差异。

Conclusion: 将自注意力机制置于基于亲和性计算的更广泛范式中，存在统一的数学基础。

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [164] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: 本文提出LPS - GNN框架，设计LPMetis分区算法与子图增强策略，在多数据集测试表现优于SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有可扩展GNN解决方案难以平衡执行效率和预测准确性，迭代消息传递技术计算需求大、GPU内存要求高，处理大规模图的邻居爆炸问题困难。

Method: 引入LPS - GNN框架，设计LPMetis图分区算法，提出子图增强策略。

Result: LPS - GNN能在单GPU上10小时内对1000亿图进行表示学习，在用户获取场景提升13.8%；LPMetis在多种评估指标上优于SOTA方法；LPS - GNN在在线应用中比SOTA模型性能提升8.24% - 13.89%。

Conclusion: LPS - GNN框架具有可扩展性、低成本、灵活性和高效性，能适应多种GNN算法，在实际应用中取得良好效果。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [165] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: 本文提出集成Transformer - GAN与MILET的框架解决无人机飞行状态分类问题，实验效果佳且有计算效率与泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统TSC方法对动态无人机环境缺乏鲁棒性和泛化性，SOTA模型需大数据集且计算成本高，需新方法解决无人机飞行状态分类问题。

Method: 提出集成Transformer - GAN与MILET的框架，Transformer编码器捕捉长时依赖，GAN模块扩充数据集，MIL聚焦关键输入段。

Result: 在DroneDetect数据集上准确率96.5%，DroneRF数据集上98.6%，优于其他SOTA方法。

Conclusion: 该框架计算效率高、泛化性强，有在资源受限环境实时部署的潜力。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [166] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 本文提出Rec - AD框架解决智能电网FDIA检测中计算和内存负担问题，实验显示其提升了计算吞吐量和实时检测性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型用于智能电网FDIA检测时，因系统规模和数据维度增加带来计算和内存负担，限制检测效率。

Method: 提出Rec - AD框架，集成Tensor Train分解与DLRM，通过嵌入压缩、索引重排优化数据访问和流水线训练机制减少内存通信开销，且与PyTorch兼容。

Result: Rec - AD显著提高了计算吞吐量和实时检测性能，缩小攻击窗口，增加攻击者成本。

Conclusion: Rec - AD增强了边缘计算能力和可扩展性，为智能电网安全提供了有力技术支持。

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [167] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: 现有基于图对比学习（GCL）的异常检测模型忽视结构不平衡鲁棒性，本文提出AD - GCL框架，验证其在检测头部和尾部异常的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCL的模型过度关注整体检测性能，忽略结构不平衡鲁棒性，难以检测尾部异常，影响算法在现实高风险场景的应用。

Method: 提出AD - GCL框架，采用邻居修剪策略过滤头部节点噪声边，通过异常引导的邻居补全扩大尾部节点感受野，引入视图内和视图间一致性损失。

Result: 在多个数据集上对整体、头部和尾部节点的性能评估，验证了AD - GCL在检测头部和尾部异常上的综合优越性。

Conclusion: AD - GCL框架有效解决了现有基于GCL模型在结构不平衡方面的问题，在异常检测中表现出色。

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [168] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: 为应对互联网垃圾文本增长，提出GCC - Spam框架。该框架含字符相似度网络、对比学习和GAN等创新，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 互联网垃圾文本指数级增长，带来信息泄露和社会不稳定风险，同时存在垃圾邮件发送者的对抗策略及标注数据稀缺两大挑战。

Method: 提出GCC - Spam框架，包括字符相似度网络捕捉特征并生成句子嵌入，对比学习优化潜在空间距离，GAN生成伪垃圾样本。

Result: 在真实数据集上的大量实验表明，该模型优于基线方法，在标注样本显著减少的情况下实现了更高的检测率。

Conclusion: GCC - Spam框架能有效解决垃圾文本检测中的对抗策略和数据稀缺问题，提高检测效果。

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [169] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: 本文提出SST - CL框架用于基于EEG的情绪识别，实验显示其在各情绪强度水平上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 基于EEG的情绪识别在实际应用中面临有效整合时空神经模式和适应情绪强度变化的挑战。

Method: 提出SST - CL框架，包含空间编码器和时间编码器，结合强度感知课程学习策略。

Result: 在三个基准数据集上实验表明，在不同情绪强度水平上达到了SOTA，消融实验证实了架构组件和课程学习机制的必要性。

Conclusion: SST - CL框架可有效解决基于EEG的情绪识别中的挑战。

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [170] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: 本文提出Causal Prototype Attention Classifier (CPAC)用于信用卡欺诈交易检测，对比传统过采样方法和先进生成模型，结果显示CPAC性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有信用卡欺诈检测方法存在因仅处理少数类数据导致分类器过度自信和潜在聚类分离不佳的问题，限制了检测性能，因此需新方法。

Method: 提出CPAC架构，通过基于原型的注意力机制促进类感知聚类和改善潜在空间结构，并将其与VAE - GAN中的编码器结合；对比CPAC增强模型与传统过采样器及先进生成模型。

Result: CPAC实现了93.14%的F1分数和90.18%的召回率，改善了潜在聚类分离。

Conclusion: 基于CPAC的分类器引导潜在塑形在欺诈检测中表现优越，消融研究和可视化有助于深入了解其优缺点。

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [171] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: 本文研究实时生成式AI（RTGen）工作负载在AMD异构SoC上的调度问题，评估了五种调度策略，结果显示调度决策显著影响性能，强调了工作负载感知的动态异构调度的重要性。


<details>
  <summary>Details</summary>
Motivation: 实时生成式AI工作负载在异构SoC平台上的调度空间复杂性和性能影响未得到充分研究。

Method: 在AMD Ryzen AI异构SoC上对RTGen工作负载进行全面表征，构建多模型场景并在所有可用后端进行性能分析，评估五种调度策略。

Result: 调度决策显著影响工作负载性能，如平均截止时间违规率差异达41.7%。

Conclusion: 工作负载感知的动态异构调度对实现高性能的设备端RTGen应用至关重要。

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>


### [172] [LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4](https://arxiv.org/abs/2507.14722)
*Matěj Kripner,Michal Šustr,Milan Straka*

Main category: cs.LG

TL;DR: 本文介绍LeanTree解决自动定理证明中白盒方法落后问题，白盒方法在某些场景优于黑盒方法。


<details>
  <summary>Details</summary>
Motivation: 自动定理证明挑战大，大语言模型用于ATP时白盒方法落后于黑盒方法，需弥补差距。

Method: 引入LeanTree，包含用Lean 4语言构建的工具将复杂证明状态分解，以及这些分解后的中间状态数据集。

Result: 初步结果显示白盒方法在一些设置中表现优于黑盒方法。

Conclusion: 白盒工具相对黑盒方法有简化评估、减少上下文等优势。

Abstract: Automated theorem proving (ATP) has been a classical problem in artificial
intelligence since its inception, yet it remains challenging due to its vast
state and action space. Large language models (LLMs) have recently emerged as a
promising heuristic for ATP, but they lack correctness guarantees and thus
require interaction with a proof verifier. Such interactions typically follow
one of two approaches: black-box interaction, which does not utilize
intermediate proof states, or white-box approaches, which allow for incremental
proof construction and examination of intermediate states. While black-box
approaches have directly benefited from recent LLM advances, white-box methods
have comparatively lagged behind. In this paper, we address this gap by
introducing LeanTree, which consists of (i) a tool built in the Lean 4 language
that factorizes complex proof states into simpler, independent branches, and
(ii) a dataset of these factorized intermediate states. Our white-box tooling
offers several advantages over black-box approaches: it simplifies evaluation,
reduces necessary context, generates richer training data, enables parallel
search across multiple states, supports efficient reuse of states, and provides
feedback in case of errors. Our preliminary results hint that white-box
approaches outperform black-box alternatives in some settings.

</details>


### [173] [Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding](https://arxiv.org/abs/2507.14725)
*Anushka Tiwari,Sayantan Pal,Rohini K. Srihari,Kaiyi Ji*

Main category: cs.LG

TL;DR: 提出GRID框架解决基于提示的持续学习的两个关键限制，实验表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法存在任务不可知推理下的潜在遗忘和提示内存爆炸问题，限制了可扩展性。

Method: 集成任务感知解码机制，利用代表性输入、自动任务识别和约束解码提升反向迁移；提出基于梯度的提示选择策略，将信息较少的提示压缩为单一聚合表示。

Result: 在短序列、长序列和负迁移基准测试中显著提高反向迁移，实现有竞争力的正向迁移，减少高达80%的遗忘任务。

Conclusion: GRID框架在T5和Flan - T5骨干网络上优于现有方法。

Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to
adapt large language models (LLMs) across task sequences. However, most
existing methods assume task-aware inference and maintain a growing list of
task-specific prompts, which limits scalability and hides latent forgetting. In
this work, we introduce GRID, a unified framework that addresses two key
limitations: (1) latent forgetting under task-agnostic inference, and (2)
prompt memory explosion as task sequences grow. GRID integrates a task-aware
decoding mechanism that improves backward transfer by leveraging representative
inputs, automatic task identification, and constrained decoding. Additionally,
we propose a gradient-based prompt selection strategy that compresses less
informative prompts into a single aggregated representation, enabling scalable
and memory-efficient lifelong learning. Extensive experiments across
short-sequence, long-sequence, and negative transfer benchmarks show that GRID
significantly improves backward transfer, achieves competitive forward
transfer, and reduces forgotten tasks by up to 80\%, outperforming
state-of-the-art methods on T5 and Flan-T5 backbones.

</details>


### [174] [Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning](https://arxiv.org/abs/2507.14736)
*Rafał Surdej,Michał Bortkiewicz,Alex Lewandowski,Mateusz Ostaszewski,Clare Lyle*

Main category: cs.LG

TL;DR: 研究可训练有理激活函数在强化和持续学习中的表现，发现其表达性和可塑性存在权衡，提出受限变体改善稳定性和性能，为动态环境下设计激活函数提供原则。


<details>
  <summary>Details</summary>
Motivation: 可训练有理激活函数能增强可塑性，但对训练稳定性的影响不明，需研究其在强化和持续学习中的表现。

Method: 研究可训练有理激活函数，提出受限变体，在多个环境和基准上进行实验。

Result: 可训练有理激活函数灵活性高但会引入不稳定，受限变体提高训练稳定性和性能，不同约束影响表达性和长期保留的平衡，连续控制中权衡更明显。

Conclusion: 为动态、非平稳环境下设计鲁棒且适应性强的可训练激活函数提供了可行的设计原则。

Abstract: Trainable activation functions, whose parameters are optimized alongside
network weights, offer increased expressivity compared to fixed activation
functions. Specifically, trainable activation functions defined as ratios of
polynomials (rational functions) have been proposed to enhance plasticity in
reinforcement learning. However, their impact on training stability remains
unclear. In this work, we study trainable rational activations in both
reinforcement and continual learning settings. We find that while their
flexibility enhances adaptability, it can also introduce instability, leading
to overestimation in RL and feature collapse in longer continual learning
scenarios. Our main result is demonstrating a trade-off between expressivity
and plasticity in rational activations. To address this, we propose a
constrained variant that structurally limits excessive output scaling while
preserving adaptability. Experiments across MetaWorld and DeepMind Control
Suite (DMC) environments show that our approach improves training stability and
performance. In continual learning benchmarks, including MNIST with reshuffled
labels and Split CIFAR-100, we reveal how different constraints affect the
balance between expressivity and long-term retention. While preliminary
experiments in discrete action domains (e.g., Atari) did not show similar
instability, this suggests that the trade-off is particularly relevant for
continuous control. Together, our findings provide actionable design principles
for robust and adaptable trainable activations in dynamic, non-stationary
environments. Code available at:
https://github.com/special114/rl_rational_plasticity.

</details>


### [175] [Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML](https://arxiv.org/abs/2507.14744)
*Mustafa Cavus,Jan N. van Rijn,Przemysław Biecek*

Main category: cs.LG

TL;DR: 提出Rashomon PDP框架，聚合近优模型的PDP以捕捉解释不确定性，实验显示其能改进模型解释的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有自动机器学习系统聚焦单最佳模型，忽略解释不确定性，这在以人为中心的可解释AI中是重要问题。

Method: 提出将模型多样性纳入解释生成的框架，聚合Rashomon集中近优模型的PDP得到Rashomon PDP，引入覆盖率和置信区间平均宽度两个指标评估其与标准PDP的一致性。

Result: 在35个回归数据集上实验表明，多数情况下Rashomon PDP覆盖最佳模型PDP不到70%。

Conclusion: Rashomon PDP通过增加被忽略的信息，提高了模型解释的可靠性和可信度，在高风险领域尤其有用。

Abstract: Automated machine learning systems efficiently streamline model selection but
often focus on a single best-performing model, overlooking explanation
uncertainty, an essential concern in human centered explainable AI. To address
this, we propose a novel framework that incorporates model multiplicity into
explanation generation by aggregating partial dependence profiles (PDP) from a
set of near optimal models, known as the Rashomon set. The resulting Rashomon
PDP captures interpretive variability and highlights areas of disagreement,
providing users with a richer, uncertainty aware view of feature effects. To
evaluate its usefulness, we introduce two quantitative metrics, the coverage
rate and the mean width of confidence intervals, to evaluate the consistency
between the standard PDP and the proposed Rashomon PDP. Experiments on 35
regression datasets from the OpenML CTR23 benchmark suite show that in most
cases, the Rashomon PDP covers less than 70% of the best model's PDP,
underscoring the limitations of single model explanations. Our findings suggest
that Rashomon PDP improves the reliability and trustworthiness of model
interpretations by adding additional information that would otherwise be
neglected. This is particularly useful in high stakes domains where
transparency and confidence are critical.

</details>


### [176] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: 提出CXR - TFT框架整合多模态数据预测重症患者CXR结果，回顾性研究显示其预测异常CXR发现准确性高，能改善临床结果。


<details>
  <summary>Details</summary>
Motivation: ICU患者需监测干预，CXR不规则采集限制其作用，现有工具无法捕捉时间动态，需要新方法预测CXR结果。

Method: 引入CXR - TFT框架，利用视觉编码器的潜在嵌入与临床数据通过插值对齐，训练Transformer模型预测CXR嵌入。

Result: 在20000名ICU患者回顾性研究中，CXR - TFT能提前12小时准确预测异常CXR发现。

Conclusion: CXR - TFT在预后CXR分析中有独特时间分辨率，可提供整体患者见解，改善临床结果。

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [177] [Rethinking Memorization Measures and their Implications in Large Language Models](https://arxiv.org/abs/2507.14777)
*Bishwamittra Ghosh,Soumi Das,Qinyuan Wu,Mohammad Aflah Khan,Krishna P. Gummadi,Evimaria Terzi,Deepak Garg*

Main category: cs.LG

TL;DR: 研究大语言模型记忆能否避免及隐私威胁是否被夸大，提出上下文记忆概念，实验表明学习语言无法避免部分记忆，不同记忆度量结果有差异。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型学习中记忆能否避免以及记忆带来的隐私威胁是否被夸大。

Method: 重新审视基于回忆和反事实的记忆度量方法，提出上下文记忆度量方法，并在18个大语言模型和多种形式语言上进行实验。

Result: 不同记忆度量方法对不同频率字符串的记忆顺序判断不一致；学习语言无法避免对训练字符串的部分记忆；学习效果提升会降低上下文和反事实记忆，但增加基于回忆的记忆；部分基于回忆被认为记忆的字符串无隐私威胁。

Conclusion: 大语言模型学习语言无法避免部分记忆，不同记忆度量方法有差异，部分被认为记忆的字符串无隐私威胁。

Abstract: Concerned with privacy threats, memorization in LLMs is often seen as
undesirable, specifically for learning. In this paper, we study whether
memorization can be avoided when optimally learning a language, and whether the
privacy threat posed by memorization is exaggerated or not. To this end, we
re-examine existing privacy-focused measures of memorization, namely
recollection-based and counterfactual memorization, along with a newly proposed
contextual memorization.
  Relating memorization to local over-fitting during learning, contextual
memorization aims to disentangle memorization from the contextual learning
ability of LLMs. Informally, a string is contextually memorized if its
recollection due to training exceeds the optimal contextual recollection, a
learned threshold denoting the best contextual learning without training.
Conceptually, contextual recollection avoids the fallacy of recollection-based
memorization, where any form of high recollection is a sign of memorization.
Theoretically, contextual memorization relates to counterfactual memorization,
but imposes stronger conditions. Memorization measures differ in outcomes and
information requirements.
  Experimenting on 18 LLMs from 6 families and multiple formal languages of
different entropy, we show that (a) memorization measures disagree on
memorization order of varying frequent strings, (b) optimal learning of a
language cannot avoid partial memorization of training strings, and (c)
improved learning decreases contextual and counterfactual memorization but
increases recollection-based memorization. Finally, (d) we revisit existing
reports of memorized strings by recollection that neither pose a privacy threat
nor are contextually or counterfactually memorized.

</details>


### [178] [Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards](https://arxiv.org/abs/2507.14783)
*Derek Li,Jiaming Zhou,Amirreza Kazemi,Qianyi Sun,Abbas Ghaddar,Mohammad Ali Alomrani,Liheng Ma,Yu Luo,Dong Li,Feng Wen,Jianye Hao,Mark Coates,Yingxue Zhang*

Main category: cs.LG

TL;DR: 提出Omni - Think统一强化学习框架提升大语言模型跨任务性能，研究训练策略，实验表明课程学习优于联合训练和模型合并。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法如SFT泛化能力差，倾向记忆而非迁移学习，需提升大语言模型跨任务性能。

Method: 引入Omni - Think框架，结合基于规则的可验证奖励和通过大语言模型评判的生成偏好信号；采用从结构化到开放式任务排序的课程学习策略。

Result: 四个领域实验显示，课程学习比联合训练性能提升5.2%，比模型合并提升9.1%。

Conclusion: 任务感知采样和混合监督在基于强化学习的通用大语言模型后训练扩展中很重要。

Abstract: The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce Omni-Think,
a unified reinforcement learning (RL) framework that enhances LLM performance
across diverse tasks by combining rule-based verifiable rewards with generative
preference signals via LLM-as-a-Judge evaluations. Our approach enables
consistent optimization across task types and scales RL-based training to
subjective domains. We further investigate training strategies, demonstrating
that a curriculum-based progression that orders tasks from structured to
open-ended improves performance and reduces forgetting. Experimental results
across four domains reveal that curriculum learning improves performance by
5.2\% over joint training and 9.1\% over model merging. These results highlight
the importance of task-aware sampling and hybrid supervision in scaling
RL-based post-training for general-purpose LLMs.

</details>


### [179] [Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs](https://arxiv.org/abs/2507.14785)
*Erfan Pirmorad*

Main category: cs.LG

TL;DR: 本文探索使用大语言模型对金融知识图谱的局部子图进行推理，以评估洗钱可疑性并给出解释，展示了其在反洗钱中的潜力。


<details>
  <summary>Details</summary>
Motivation: 洗钱实体的复杂性和互联性要求对图结构数据进行调查推理，探索大语言模型在其中的应用。

Method: 提出轻量级流程，提取感兴趣实体的k跳邻域，将其序列化为结构化文本，通过少样本上下文学习提示大语言模型评估可疑性并生成解释。

Result: 使用合成反洗钱场景表明，大语言模型能模拟分析师逻辑，突出风险标志并给出连贯解释。

Conclusion: 本研究虽具探索性，但展示了基于大语言模型的图推理在反洗钱中的潜力，为可解释的、语言驱动的金融犯罪分析奠定基础。

Abstract: The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.

</details>


### [180] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: 本文将等变网络理论扩展到序列模型，展示标准RNN通常不具备流等变性，引入流等变性的模型在多项指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有等变理论仅考虑静态变换和前馈网络，限制了其在序列模型中的应用，需将等变网络理论扩展到时间参数化的序列变换。

Method: 先证明标准RNN通常不具有流等变性，再展示如何引入流等变性。

Result: 引入流等变性的模型在训练速度、长度泛化和速度泛化方面显著优于非等变模型。

Conclusion: 这是构建尊重时间参数化对称性的序列模型的第一步。

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [181] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: 研究语言模型通过语义无关数据传递行为特征的潜意识学习现象，发现学生模型能从教师模型生成的数字序列等数据中学到特征，不同基础模型则无此效果，理论证明该现象普遍存在，指出这是AI开发隐患。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型中潜意识学习这一现象，即通过语义无关数据传递行为特征。

Method: 进行实验，让有特定特征的教师模型生成数字序列、代码或推理轨迹等数据集，训练学生模型；证明神经网络在特定条件下存在潜意识学习的理论结果，并在简单MLP分类器中验证。

Result: 学生模型能从教师模型生成的数据集中学到特征，不同基础模型无此效果，理论证明潜意识学习在特定条件下存在于所有神经网络。

Conclusion: 潜意识学习是普遍现象，是AI开发的意外隐患，蒸馏可能传播非预期特征，即使进行数据过滤。

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [182] [Benchmarking Foundation Models with Multimodal Public Electronic Health Records](https://arxiv.org/abs/2507.14824)
*Kunyu Yu,Rui Yang,Jingchi Liao,Siqi Li,Huitao Li,Irene Li,Yifan Peng,Rishikesan Kamaleswaran,Nan Liu*

Main category: cs.LG

TL;DR: 本文用MIMIC - IV数据库对基础模型在EHRs处理上的性能、公平性和可解释性进行综合基准测试，发现多模态数据能提升性能且无额外偏差，代码开源。


<details>
  <summary>Details</summary>
Motivation: 评估基础模型在处理电子健康记录时作为单模态编码器和多模态学习者的性能、公平性和可解释性，以支持开发有效且可信的多模态人工智能系统用于临床应用。

Method: 使用公开的MIMIC - IV数据库，开发标准化数据处理管道，系统比较8种基础模型。

Result: 纳入多模态数据能持续提升预测性能且不引入额外偏差。

Conclusion: 该基准测试有助于开发适用于现实临床应用的有效且可信的多模态人工智能系统。

Abstract: Foundation models have emerged as a powerful approach for processing
electronic health records (EHRs), offering flexibility to handle diverse
medical data modalities. In this study, we present a comprehensive benchmark
that evaluates the performance, fairness, and interpretability of foundation
models, both as unimodal encoders and as multimodal learners, using the
publicly available MIMIC-IV database. To support consistent and reproducible
evaluation, we developed a standardized data processing pipeline that
harmonizes heterogeneous clinical records into an analysis-ready format. We
systematically compared eight foundation models, encompassing both unimodal and
multimodal models, as well as domain-specific and general-purpose variants. Our
findings demonstrate that incorporating multiple data modalities leads to
consistent improvements in predictive performance without introducing
additional bias. Through this benchmark, we aim to support the development of
effective and trustworthy multimodal artificial intelligence (AI) systems for
real-world clinical applications. Our code is available at
https://github.com/nliulab/MIMIC-Multimodal.

</details>


### [183] [eMargin: Revisiting Contrastive Learning with Margin-Based Separation](https://arxiv.org/abs/2507.14828)
*Abdul-Kazeem Shamba,Kerstin Bach,Gavin Taylor*

Main category: cs.LG

TL;DR: 研究在时间序列表征学习的对比损失函数中引入自适应边界（eMargin）的效果，发现其在无监督聚类指标上表现好，但下游分类任务效果不佳。


<details>
  <summary>Details</summary>
Motivation: 探究在对比损失函数中引入基于预定义相似度阈值调整的自适应边界，能否改善相邻但不相似时间步的分离，提升下游任务性能。

Method: 在三个基准数据集上评估该修改对聚类性能和分类的影响。

Result: eMargin添加到InfoNCE在无监督聚类指标上始终优于最先进的基线，但在线性探测的下游分类中难以取得有竞争力的结果。

Conclusion: 在无监督聚类指标上获得高分不一定意味着学习到的嵌入在下游任务中有意义或有效。

Abstract: We revisit previous contrastive learning frameworks to investigate the effect
of introducing an adaptive margin into the contrastive loss function for time
series representation learning. Specifically, we explore whether an adaptive
margin (eMargin), adjusted based on a predefined similarity threshold, can
improve the separation between adjacent but dissimilar time steps and
subsequently lead to better performance in downstream tasks. Our study
evaluates the impact of this modification on clustering performance and
classification in three benchmark datasets. Our findings, however, indicate
that achieving high scores on unsupervised clustering metrics does not
necessarily imply that the learned embeddings are meaningful or effective in
downstream tasks. To be specific, eMargin added to InfoNCE consistently
outperforms state-of-the-art baselines in unsupervised clustering metrics, but
struggles to achieve competitive results in downstream classification with
linear probing. The source code is publicly available at
https://github.com/sfi-norwai/eMargin.

</details>


### [184] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: 研究探讨强化学习与可验证奖励（RLVR）方法局限性，发现其受基础模型支持限制，有熵 - 奖励权衡，难拓展推理边界。


<details>
  <summary>Details</summary>
Motivation: 明确RLVR是提升AI能力的有前景方法，但不清楚它是拓展推理边界还是仅放大已知高奖励输出，因此研究其潜在局限性。

Method: 进行理论分析，提出RLVR受基础模型支持约束及熵 - 奖励权衡观点；开展大量实证实验。

Result: 实证实验表明RLVR虽提升pass@1，但经验支持收缩大于扩展，无法恢复基础模型原可获取的正确答案；RLVR使token级熵有时增加，但答案级熵下降。

Conclusion: RLVR在拓展推理边界上存在潜在局限，未来需算法创新，如引入显式探索机制或混合策略。

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [185] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: 提出TALE - EHR框架分析电子健康记录（EHR），实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 有效建模EHR需解决数据异构性和复杂时间模式问题，标准方法难以处理临床事件间不规则时间间隔。

Method: 提出基于Transformer的TALE - EHR框架，含时间感知注意力机制以捕捉序列动态，利用预训练大语言模型的嵌入来理解临床概念。

Result: 在MIMIC - IV和PIC数据集实验中，TALE - EHR在疾病进展预测等任务上优于现有基线。

Conclusion: 整合显式连续时间建模和强语义表示对推进EHR分析是有力解决方案。

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [186] [Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems](https://arxiv.org/abs/2507.14850)
*H. M. Sabbir Ahmad,Ehsan Sabouni,Alexander Wasilkoff,Param Budhraja,Zijian Guo,Songyuan Zhang,Chuchu Fan,Christos Cassandras,Wenchao Li*

Main category: cs.LG

TL;DR: 提出基于CBFs的安全分层多智能体强化学习方法解决多智能体安全关键自治系统的安全策略学习问题，在复杂场景验证，相比现有方法显著提升安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体安全关键自治系统中各智能体需时刻满足安全要求并与其他智能体协作完成任务的问题。

Method: 提出基于控制障碍函数（CBFs）的安全分层多智能体强化学习（HMARL）方法，将强化学习问题分解为高层学习联合协作行为和低层学习安全个体行为，提出基于技能的HMARL - CBF算法。

Result: 在大量智能体需在冲突道路网络中安全导航的具有挑战性的环境场景中验证，相比现有技术显著提高安全性，成功率/安全率接近完美（误差在5%以内），且在所有环境中性能均有提升。

Conclusion: 所提出的方法能有效解决多智能体安全关键自治系统的安全策略学习问题，在安全性和性能上优于现有方法。

Abstract: We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.

</details>


### [187] [The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs](https://arxiv.org/abs/2507.14874)
*Ole-Christoffer Granmo,Youmna Abdelwahab,Per-Arne Andersen,Paul F. A. Clarke,Kunal Dumbre,Ylva Grønninsæter,Vojtech Halenka,Runar Helin,Lei Jiao,Ahmed Khalid,Rebekka Omslandseter,Rupsa Saha,Mayur Shende,Xuan Zhang*

Main category: cs.LG

TL;DR: 介绍图Tsetlin机器(GraphTM)，它能从图结构输入学习可解释的深度子句，在多个领域表现出色，展示图表示学习和深度子句为TM学习带来新可能。


<details>
  <summary>Details</summary>
Motivation: 现有Tsetlin机器适用于简洁扁平的与规则，为处理图结构输入，开发更通用的模型。

Method: 通过消息传递构建嵌套深度子句，识别子图模式。

Result: 在图像分类、动作共指跟踪、推荐系统和病毒基因组序列数据等领域有良好表现，如在CIFAR - 10上比卷积TM准确率高3.86%等。

Conclusion: 图表示学习和深度子句为TM学习带来新可能性。

Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine
(TM) both interpretable and efficient, while the power of Tsetlin automata
enables accuracy comparable to deep learning on an increasing number of
datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning
interpretable deep clauses from graph-structured input. Moving beyond flat,
fixed-length input, the GraphTM gets more versatile, supporting sequences,
grids, relations, and multimodality. Through message passing, the GraphTM
builds nested deep clauses to recognize sub-graph patterns with exponentially
fewer clauses, increasing both interpretability and data utilization. For image
classification, GraphTM preserves interpretability and achieves 3.86%-points
higher accuracy on CIFAR-10 than a convolutional TM. For tracking action
coreference, faced with increasingly challenging tasks, GraphTM outperforms
other reinforcement learning methods by up to 20.6%-points. In recommendation
systems, it tolerates increasing noise to a greater extent than a Graph
Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains
accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence
data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training
2.5x faster than GCN. The GraphTM's application to these varied fields
demonstrates how graph representation learning and deep clauses bring new
possibilities for TM learning.

</details>


### [188] [Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization](https://arxiv.org/abs/2507.14882)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.LG

TL;DR: 本文提出增强重要性度量框架，用于结构化剪枝，在MNIST图像重建的自编码器上验证，能在大幅剪枝后保持模型可用性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络模型复杂度和计算需求高，现有结构化剪枝的重要性度量难以保留特定应用性能特征。

Method: 提出增强重要性度量框架，采用多种策略确定每组的最佳剪枝幅度。

Result: 在MNIST图像重建的自编码器上实验，该方法能有效保留与任务相关的性能。

Conclusion: 所提方法可在大幅剪枝后满足特定应用标准，保持模型可用性。

Abstract: Deep neural networks (DNNs) offer significant versatility and performance
benefits, but their widespread adoption is often hindered by high model
complexity and computational demands. Model compression techniques such as
pruning have emerged as promising solutions to these challenges. However, it
remains critical to ensure that application-specific performance
characteristics are preserved during compression. In structured pruning, where
groups of structurally coherent elements are removed, conventional importance
metrics frequently fail to maintain these essential performance attributes. In
this work, we propose an enhanced importance metric framework that not only
reduces model size but also explicitly accounts for application-specific
performance constraints. We employ multiple strategies to determine the optimal
pruning magnitude for each group, ensuring a balance between compression and
task performance. Our approach is evaluated on an autoencoder tasked with
reconstructing MNIST images. Experimental results demonstrate that the proposed
method effectively preserves task-relevant performance, maintaining the model's
usability even after substantial pruning, by satisfying the required
application-specific criteria.

</details>


### [189] [Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning](https://arxiv.org/abs/2507.14919)
*Maximilian Wendlinger,Kilian Tscharke,Pascal Debus*

Main category: cs.LG

TL;DR: 传统深度学习存在模型不透明问题，量子机器学习也有同样问题，本文将经典不确定性量化方法映射到量子机器学习领域，并强调设计新量子模型时要考虑不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习和量子机器学习都存在模型不透明问题，经典语境下虽有研究，但量子机器学习黑箱性质解决进展甚微。

Method: 基于经典不确定性量化现有工作和量子贝叶斯建模初步探索，将经典不确定性量化方法映射到量子机器学习领域。

Result: 进行了理论开发和实证评估。

Conclusion: 设计新量子机器学习模型时需利用经典不确定性量化见解，纳入不确定性意识。

Abstract: One of the key obstacles in traditional deep learning is the reduction in
model transparency caused by increasingly intricate model functions, which can
lead to problems such as overfitting and excessive confidence in predictions.
With the advent of quantum machine learning offering possible advances in
computational power and latent space complexity, we notice the same opaque
behavior. Despite significant research in classical contexts, there has been
little advancement in addressing the black-box nature of quantum machine
learning. Consequently, we approach this gap by building upon existing work in
classical uncertainty quantification and initial explorations in quantum
Bayesian modeling to theoretically develop and empirically evaluate techniques
to map classical uncertainty quantification methods to the quantum machine
learning domain. Our findings emphasize the necessity of leveraging classical
insights into uncertainty quantification to include uncertainty awareness in
the process of designing new quantum machine learning models.

</details>


### [190] [FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios](https://arxiv.org/abs/2507.14980)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Qipeng Xie,Chang Liu,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: 针对联邦学习在长尾数据场景下的收敛问题，提出FedWCM方法，实验表明其能解决非收敛问题并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在处理非独立同分布（non - IID）数据，尤其是长尾场景下的类样本不平衡数据时面临挑战，基于动量的联邦学习方法难以收敛且模型有偏差。

Method: 对该现象进行广泛研究和神经网络层分析，提出FedWCM方法，利用全局和每轮数据动态调整动量以纠正长尾分布带来的方向偏差。

Result: 广泛实验表明FedWCM解决了非收敛问题，性能优于现有方法。

Conclusion: FedWCM提高了联邦学习处理客户端异质性和数据不平衡的效率和有效性。

Abstract: Federated Learning (FL) enables decentralized model training while preserving
data privacy. Despite its benefits, FL faces challenges with non-identically
distributed (non-IID) data, especially in long-tailed scenarios with imbalanced
class samples. Momentum-based FL methods, often used to accelerate FL
convergence, struggle with these distributions, resulting in biased models and
making FL hard to converge. To understand this challenge, we conduct extensive
investigations into this phenomenon, accompanied by a layer-wise analysis of
neural network behavior. Based on these insights, we propose FedWCM, a method
that dynamically adjusts momentum using global and per-round data to correct
directional biases introduced by long-tailed distributions. Extensive
experiments show that FedWCM resolves non-convergence issues and outperforms
existing methods, enhancing FL's efficiency and effectiveness in handling
client heterogeneity and data imbalance.

</details>


### [191] [Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data](https://arxiv.org/abs/2507.14999)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 本文提出FedClusAvg框架用于非IID和资源受限环境下智能电网FDIA检测，实验证明其提升检测精度并减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 智能电网FDIA威胁大，数据有Non - IID特性，传统集中训练有隐私、成本等问题，影响检测模型泛化能力。

Method: 提出Federated Cluster Average (FedClusAvg)框架，结合基于集群的分层抽样和分层通信，实现本地训练和加权参数聚合。

Result: 在基准智能电网数据集上，FedClusAvg提高了异构数据分布下的检测精度，减少了通信轮次和带宽消耗。

Conclusion: 该框架为大规模分布式电力系统中安全高效的FDIA检测提供了有效解决方案。

Abstract: False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.

</details>


### [192] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: 提出时间序列异常推理任务Time - RA，引入基准数据集RATs40K，开展基准测试，推动可解释异常检测与推理发展。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法局限于二元分类，缺乏详细分类和解释推理。

Method: 提出Time - RA任务，将其转化为生成式推理密集型任务；引入数据集RATs40K；开发利用GPT - 4反馈优化的标注框架。

Result: 对大模型和多模态大模型的基准测试展示了当前模型的能力和局限，凸显了监督微调的关键作用。

Conclusion: 数据集和任务为可解释时间序列异常检测和推理带来显著进展。

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [193] [ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model](https://arxiv.org/abs/2507.15067)
*Bing He,Mustaque Ahamad,Srijan Kumar*

Main category: cs.LG

TL;DR: 文章提出ROBAD模型检测网络平台不良行为者，能捕捉局部和全局信息，在对抗攻击下表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习检测模型对输入序列微小变化敏感，不满足对抗攻击下的鲁棒性要求。

Method: 创建ROBAD模型，用Transformer编码器块编码帖子，解码器块建模序列模式，用对比学习增强分类层进行序列预测。

Result: 在Yelp和Wikipedia数据集上的实验表明，ROBAD在对抗攻击下能有效检测不良行为者。

Conclusion: ROBAD模型通过捕捉局部和全局信息，利用模仿攻击行为训练，对对抗攻击具有鲁棒性。

Abstract: Detecting bad actors is critical to ensure the safety and integrity of
internet platforms. Several deep learning-based models have been developed to
identify such users. These models should not only accurately detect bad actors,
but also be robust against adversarial attacks that aim to evade detection.
However, past deep learning-based detection models do not meet the robustness
requirement because they are sensitive to even minor changes in the input
sequence. To address this issue, we focus on (1) improving the model
understanding capability and (2) enhancing the model knowledge such that the
model can recognize potential input modifications when making predictions. To
achieve these goals, we create a novel transformer-based classification model,
called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection
model), which uses the sequence of user posts to generate user embedding to
detect bad actors. Particularly, ROBAD first leverages the transformer encoder
block to encode each post bidirectionally, thus building a post embedding to
capture the local information at the post level. Next, it adopts the
transformer decoder block to model the sequential pattern in the post
embeddings by using the attention mechanism, which generates the sequence
embedding to obtain the global information at the sequence level. Finally, to
enrich the knowledge of the model, embeddings of modified sequences by mimicked
attackers are fed into a contrastive-learning-enhanced classification layer for
sequence prediction. In essence, by capturing the local and global information
(i.e., the post and sequence information) and leveraging the mimicked behaviors
of bad actors in training, ROBAD can be robust to adversarial attacks.
Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can
effectively detect bad actors when under state-of-the-art adversarial attacks.

</details>


### [194] [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073)
*Samuel Pfrommer,Yixiao Huang,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 本文探索通过强化学习训练流匹配策略以超越原演示策略的性能，提出两种方法并在模拟任务中验证有效。


<details>
  <summary>Details</summary>
Motivation: 原训练演示策略可能次优，希望通过强化学习训练流匹配策略超越原演示策略性能。

Method: 提出Reward - Weighted Flow Matching (RWFM)方案和Group Relative Policy Optimization (GRPO)方法，后者带有学习的奖励替代。

Result: 在模拟单轮动力学任务中，两种方法都显著提升了次优演示者的性能，GRPO方法比朴素的模仿学习流匹配(ILFM)方法成本降低50% - 85%。

Conclusion: 通过强化学习训练流匹配策略可有效超越原演示策略性能，GRPO方法效果更佳。

Abstract: Flow-matching policies have emerged as a powerful paradigm for generalist
robotics. These models are trained to imitate an action chunk, conditioned on
sensor observations and textual instructions. Often, training demonstrations
are generated by a suboptimal policy, such as a human operator. This work
explores training flow-matching policies via reinforcement learning to surpass
the original demonstration policy performance. We particularly note
minimum-time control as a key application and present a simple scheme for
variable-horizon flow-matching planning. We then introduce two families of
approaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group
Relative Policy Optimization (GRPO) approach with a learned reward surrogate.
Our policies are trained on an illustrative suite of simulated unicycle
dynamics tasks, and we show that both approaches dramatically improve upon the
suboptimal demonstrator performance, with the GRPO approach in particular
generally incurring between $50\%$ and $85\%$ less cost than a naive Imitation
Learning Flow Matching (ILFM) approach.

</details>


### [195] [Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts](https://arxiv.org/abs/2507.15079)
*Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: cs.LG

TL;DR: 提出Isotonic Quantile Regression Averaging (iQRA)方法生成概率预测，在德国日前电力市场预测中表现优于现有方法，且具有正则化优势。


<details>
  <summary>Details</summary>
Motivation: 机器学习电力价格预测模型缺乏不确定性估计，限制决策者规避风险能力，需量化预测模型不确定性。

Method: 提出iQRA方法，基于Quantile Regression Averaging (QRA)框架，引入随机顺序约束。

Result: 在德国日前电力市场广泛研究中，iQRA在可靠性和尖锐性上均优于现有后处理方法，产生校准良好的预测区间。

Conclusion: iQRA方法有效，等渗正则化降低分位数回归问题复杂度，提供无超参数变量选择方法。

Abstract: Quantifying the uncertainty of forecasting models is essential to assess and
mitigate the risks associated with data-driven decisions, especially in
volatile domains such as electricity markets. Machine learning methods can
provide highly accurate electricity price forecasts, critical for informing the
decisions of market participants. However, these models often lack uncertainty
estimates, which limits the ability of decision makers to avoid unnecessary
risks. In this paper, we propose a novel method for generating probabilistic
forecasts from ensembles of point forecasts, called Isotonic Quantile
Regression Averaging (iQRA). Building on the established framework of Quantile
Regression Averaging (QRA), we introduce stochastic order constraints to
improve forecast accuracy, reliability, and computational costs. In an
extensive forecasting study of the German day-ahead electricity market, we show
that iQRA consistently outperforms state-of-the-art postprocessing methods in
terms of both reliability and sharpness. It produces well-calibrated prediction
intervals across multiple confidence levels, providing superior reliability to
all benchmark methods, particularly coverage-based conformal prediction. In
addition, isotonic regularization decreases the complexity of the quantile
regression problem and offers a hyperparameter-free approach to variable
selection.

</details>


### [196] [Robust Control with Gradient Uncertainty](https://arxiv.org/abs/2507.15082)
*Qian Qi*

Main category: cs.LG

TL;DR: 本文提出鲁棒控制理论的新扩展，处理值函数梯度不确定性，推导GU - HJBI方程，分析LQ情况，提出GURAC算法并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习等应用中值函数近似时值函数梯度的不确定性问题。

Method: 构建零和动态博弈，推导GU - HJBI方程，证明其适定性，分析LQ情况，进行形式化扰动分析，提出GURAC算法。

Result: 证明经典二次值函数假设在非零梯度不确定性下不成立，得出值函数非多项式修正和最优控制律非线性，数值研究验证，GURAC算法稳定训练。

Conclusion: 为鲁棒控制提供新方向，对函数近似常见领域有重要意义。

Abstract: We introduce a novel extension to robust control theory that explicitly
addresses uncertainty in the value function's gradient, a form of uncertainty
endemic to applications like reinforcement learning where value functions are
approximated. We formulate a zero-sum dynamic game where an adversary perturbs
both system dynamics and the value function gradient, leading to a new, highly
nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs
Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness
by proving a comparison principle for its viscosity solutions under a uniform
ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a
key insight: we prove that the classical quadratic value function assumption
fails for any non-zero gradient uncertainty, fundamentally altering the problem
structure. A formal perturbation analysis characterizes the non-polynomial
correction to the value function and the resulting nonlinearity of the optimal
control law, which we validate with numerical studies. Finally, we bridge
theory to practice by proposing a novel Gradient-Uncertainty-Robust
Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating
its effectiveness in stabilizing training. This work provides a new direction
for robust control, holding significant implications for fields where function
approximation is common, including reinforcement learning and computational
finance.

</details>


### [197] [AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI](https://arxiv.org/abs/2507.15104)
*Qiufeng Li,Shu Hong,Jian Gao,Xuan Zhang,Tian Lan,Weidong Cao*

Main category: cs.LG

TL;DR: 本文提出AnalogFed解决模拟电路设计中生成式AI数据私有问题，实验表明其性能与集中式基线相当且保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计数据专有，当前生成式AI研究受限于小范围私有数据集，阻碍协作创新和研究进展。

Method: 提出AnalogFed，引入针对模拟设计中联邦学习独特挑战的一系列技术，包括生成模型开发、处理数据异质性和隐私保护策略。

Result: 在不同客户端数量和数据集大小的实验中，AnalogFed性能与集中式基线相当，其生成式AI模型在模拟电路拓扑设计中达到了最先进的效率和可扩展性。

Conclusion: AnalogFed能在不共享原始私有数据的情况下实现协作拓扑发现，兼顾性能和数据隐私。

Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize
analog design automation through data-driven approaches. In particular,
researchers are increasingly fascinated by harnessing the power of generative
AI to automate the discovery of novel analog circuit topologies. Unlocking the
full potential of generative AI in these data-driven discoveries requires
access to large and diverse datasets.Yet, there is a significant barrier in the
analog domain--Analog circuit design is inherently proprietary, involving not
only confidential circuit structures but also the underlying commercial
semiconductor processes. As a result, current generative AI research is largely
confined to individual researchers who construct small, narrowly focused
private datasets. This fragmentation severely limits collaborative innovation
and impedes progress across the research community. To address these
challenges, we propose AnalogFed. AnalogFed enables collaborative topology
discovery across decentralized clients (e.g., individual researchers or
institutions) without requiring the sharing of raw private data. To make this
vision practical, we introduce a suite of techniques tailored to the unique
challenges of applying FedL in analog design--from generative model development
and data heterogeneity handling to privacy-preserving strategies that ensure
both flexibility and security for circuit designers and semiconductor
manufacturers. Extensive experiments across varying client counts and dataset
sizes demonstrate that AnalogFed achieves performance comparable to centralized
baselines--while maintaining strict data privacy. Specifically, the generative
AI model within AnalogFed achieves state-of-the-art efficiency and scalability
in the design of analog circuit topologies.

</details>


### [198] [Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2507.15119)
*Juntong Ni,Shiyu Wang,Zewen Liu,Xiaoming Shi,Xinyue Zhong,Zhou Ye,Wei Jin*

Main category: cs.LG

TL;DR: 针对高维时间序列预测（HDTSF）问题提出U - Cast架构和Time - HD基准数据集，实验显示U - Cast在精度和效率上超越基线。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测（TSF）研究未聚焦HDTSF问题，现有TSF模型无法处理高维数据中复杂的通道相关性。

Method: 提出U - Cast架构，用基于查询的注意力学习潜在分层通道结构，训练时添加全秩正则化，还发布Time - HD基准数据集。

Result: 理论证明利用跨通道信息可降低预测风险，在Time - HD上的实验表明U - Cast在精度和效率上超越强基线。

Conclusion: U - Cast和Time - HD为未来HDTSF研究提供坚实基础。

Abstract: Time series forecasting (TSF) is a central problem in time series analysis.
However, as the number of channels in time series datasets scales to the
thousands or more, a scenario we define as High-Dimensional Time Series
Forecasting (HDTSF), it introduces significant new modeling challenges that are
often not the primary focus of traditional TSF research. HDTSF is challenging
because the channel correlation often forms complex and hierarchical patterns.
Existing TSF models either ignore these interactions or fail to scale as
dimensionality grows. To address this issue, we propose U-Cast, a
channel-dependent forecasting architecture that learns latent hierarchical
channel structures with an innovative query-based attention. To disentangle
highly correlated channel representation, U-Cast adds a full-rank
regularization during training. We also release Time-HD, a benchmark of large,
diverse, high-dimensional datasets. Our theory shows that exploiting
cross-channel information lowers forecasting risk, and experiments on Time-HD
demonstrate that U-Cast surpasses strong baselines in both accuracy and
efficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF
research.

</details>


### [199] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: 研究含大量标签且有逻辑约束的多标签分类，提出架构并实证其利用和执行约束的能力。


<details>
  <summary>Details</summary>
Motivation: 解决含大量标签且有逻辑约束的多标签分类问题，发挥模型对相关性的建模能力。

Method: 构建将单个标签分类器输入到表达性顺序模型以产生联合分布的架构。

Result: 实证表明该架构在训练时能利用约束，推理时能执行约束。

Conclusion: 所提出的架构在多标签分类中对逻辑约束的利用和执行有效。

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [200] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: 本文提出基于共振隧穿二极管（RTD）的神经形态计算架构，并在两个图像识别基准测试中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 人工智能向实时、边缘和资源受限环境发展，需要新型硬件高效的计算模型。

Method: 理论上公式化并数值实现基于RTD的RC系统，在手写数字分类和Fruit~360数据集目标识别两个图像识别基准测试中验证。

Result: 该电路级架构性能良好，遵循下一代RC原则，用确定性非线性变换替代随机连接。

Conclusion: 基于RTD的神经形态计算架构是有效的，适用于图像识别任务。

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [201] [Designing User-Centric Metrics for Evaluation of Counterfactual Explanations](https://arxiv.org/abs/2507.15162)
*Firdaus Ahmed Choudhury,Ethan Leicht,Jude Ethan Bislig,Hangzhi Guo,Amulya Yadav*

Main category: cs.LG

TL;DR: 论文指出现有反事实解释（CFEs）评估指标存在忽视用户偏好的问题，通过两项用户研究，提出用户中心的AWP模型，该模型预测用户偏好CFEs准确率达84.37%，强调了以用户为中心评估指标的必要性。


<details>
  <summary>Details</summary>
Motivation: 多数CFEs研究依赖人工评估指标，可能忽视终端用户偏好和约束，为填补此研究空白开展工作。

Method: 先对20名众包工人进行试点研究验证现有CF评估指标与现实用户偏好的一致性；再对41名参与者进行为期两天的用户研究，验证关于用户如何评估CFEs的三个假设；基于研究结果提出AWP模型。

Result: 试点研究显示用户偏好的CFEs与基于接近性的CFEs仅在63.81%的情况下匹配；AWP模型预测用户偏好CFEs的准确率为84.37%。

Conclusion: 本研究为CFE生成中的个性化成本模型提供了首个以人类为中心的验证，强调了自适应、以用户为中心的评估指标的必要性。

Abstract: Machine learning-based decision models are increasingly being used to make
decisions that significantly impact people's lives, but their opaque nature
leaves end users without a clear understanding of why a decision was made.
Counterfactual Explanations (CFEs) have grown in popularity as a means of
offering actionable guidance by identifying the minimum changes in feature
values required to flip a model's prediction to something more desirable.
Unfortunately, most prior research in CFEs relies on artificial evaluation
metrics, such as proximity, which may overlook end-user preferences and
constraints, e.g., the user's perception of effort needed to make certain
feature changes may differ from that of the model designer. To address this
research gap, this paper makes three novel contributions. First, we conduct a
pilot study with 20 crowd-workers on Amazon MTurk to experimentally validate
the alignment of existing CF evaluation metrics with real-world user
preferences. Results show that user-preferred CFEs matched those based on
proximity in only 63.81% of cases, highlighting the limited applicability of
these metrics in real-world settings. Second, inspired by the need to design a
user-informed evaluation metric for CFEs, we conduct a more detailed two-day
user study with 41 participants facing realistic credit application scenarios
to find experimental support for or against three intuitive hypotheses that may
explain how end users evaluate CFEs. Third, based on the findings of this
second study, we propose the AWP model, a novel user-centric, two-stage model
that describes one possible mechanism by which users evaluate and select CFEs.
Our results show that AWP predicts user-preferred CFEs with 84.37% accuracy.
Our study provides the first human-centered validation for personalized cost
models in CFE generation and highlights the need for adaptive, user-centered
evaluation metrics.

</details>


### [202] [Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control](https://arxiv.org/abs/2507.15174)
*Justin Turnau,Longchao Da,Khoa Vo,Ferdous Al Rafi,Shreyas Bachiraju,Tiejin Chen,Hua Wei*

Main category: cs.LG

TL;DR: 本文提出用于基于多智能体强化学习的交通信号控制方法JL - GAT，通过综合实验验证其有效性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 基于多智能体强化学习的交通信号控制策略在实际应用中存在模拟与现实的差距，单智能体的GAT方法不适用于多智能体场景。

Method: 提出JL - GAT方法，将GAT应用于基于多智能体强化学习的交通信号控制，采用分散式GAT方法，结合相邻智能体信息。

Result: 在模拟恶劣天气条件下的不同道路网络上进行综合实验和消融研究，证明了JL - GAT的有效性。

Conclusion: JL - GAT能平衡可扩展性和增强的基础能力，适用于现实世界的交通网络。

Abstract: Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.

</details>


### [203] [Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning](https://arxiv.org/abs/2507.15195)
*Anwar Said,Yifan Wei,Ubaid Ullah Ahmad,Mudassir Shabbir,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 利用图的平均可控性和新的排名编码方法增强图神经网络在社交网络分类任务中的性能，数值评估证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在社交网络分类任务中性能受节点特征表达能力影响，且社交网络中常缺乏节点特征，难以达到最优性能。

Method: 提出两种构建有表现力节点特征的策略，一是引入平均可控性和其他中心性指标作为节点级指标；二是开发排名编码方法将图论指标转化为固定维特征空间。

Result: 将平均可控性纳入特征空间显著提升图神经网络性能，排名编码方法优于传统的独热度编码，如在GitHub Stargazers数据集上使用GraphSAGE时将ROC AUC从68.7%提高到73.9%。

Conclusion: 所提方法能有效生成有表现力和高效的节点表示，可提升图神经网络在社交网络分类任务中的性能。

Abstract: In this article, we utilize the concept of average controllability in graphs,
along with a novel rank encoding method, to enhance the performance of Graph
Neural Networks (GNNs) in social network classification tasks. GNNs have proven
highly effective in various network-based learning applications and require
some form of node features to function. However, their performance is heavily
influenced by the expressiveness of these features. In social networks, node
features are often unavailable due to privacy constraints or the absence of
inherent attributes, making it challenging for GNNs to achieve optimal
performance. To address this limitation, we propose two strategies for
constructing expressive node features. First, we introduce average
controllability along with other centrality metrics (denoted as NCT-EFA) as
node-level metrics that capture critical aspects of network topology. Building
on this, we develop a rank encoding method that transforms average
controllability or any other graph-theoretic metric into a fixed-dimensional
feature space, thereby improving feature representation. We conduct extensive
numerical evaluations using six benchmark GNN models across four social network
datasets to compare different node feature construction methods. Our results
demonstrate that incorporating average controllability into the feature space
significantly improves GNN performance. Moreover, the proposed rank encoding
method outperforms traditional one-hot degree encoding, improving the ROC AUC
from 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,
underscoring its effectiveness in generating expressive and efficient node
representations.

</details>


### [204] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: 本文提出LSDGNN多模态方法及ICL应对数据不平衡问题，实验表明模型优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 情感识别对话（ERC）是有挑战性的任务，当前可能缺乏有效多模态方法和处理数据不平衡的方案。

Method: 基于DAG构建长短距离图神经网络获取多模态特征，采用差分正则化器和双仿射模块促进特征交互；提出改进课程学习（ICL），计算情感相似度设计度量和难度衡量器。

Result: 在IEMOCAP和MELD数据集上实验，模型表现优于现有基准。

Conclusion: 提出的LSDGNN和ICL有效，能提升ERC任务表现。

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [205] [Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks](https://arxiv.org/abs/2507.15246)
*Rabia Latief Bhat,Iqra Altaf Gillani*

Main category: cs.LG

TL;DR: 提出基于注意力的图神经网络框架用于食品配送订单量预测，实验证明其高精度且可支持配送运营优化。


<details>
  <summary>Details</summary>
Motivation: 准确的需求预测对提升食品配送平台效率和响应能力至关重要，订单量的时空特性影响运营决策。

Method: 将食品配送环境建模为图，节点代表配送区域，边反映空间邻近和订单流模式，用注意力机制动态权衡邻域影响，联合学习时空趋势。

Result: 在真实数据集上实验表明，该模型能高精度预测未来订单量。

Conclusion: 该框架为城市食品配送运营中的车队定位、资源分配和调度优化提供了可扩展且自适应的解决方案。

Abstract: Accurate demand forecasting is critical for enhancing the efficiency and
responsiveness of food delivery platforms, where spatial heterogeneity and
temporal fluctuations in order volumes directly influence operational
decisions. This paper proposes an attention-based Graph Neural Network
framework that captures spatial-temporal dependencies by modeling the food
delivery environment as a graph. In this graph, nodes represent urban delivery
zones, while edges reflect spatial proximity and inter-regional order flow
patterns derived from historical data. The attention mechanism dynamically
weighs the influence of neighboring zones, enabling the model to focus on the
most contextually relevant areas during prediction. Temporal trends are jointly
learned alongside spatial interactions, allowing the model to adapt to evolving
demand patterns. Extensive experiments on real-world food delivery datasets
demonstrate the superiority of the proposed model in forecasting future order
volumes with high accuracy. The framework offers a scalable and adaptive
solution to support proactive fleet positioning, resource allocation, and
dispatch optimization in urban food delivery operations.

</details>


### [206] [CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](https://arxiv.org/abs/2507.15260)
*Jiaqi Han,Haotian Ye,Puheng Li,Minkai Xu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: 本文提出训练无关、模型无关的多核心并行加速策略CHORDS，加速扩散模型采样且不降低质量，为实时高保真扩散生成奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型推理计算成本高，现有加速技术需大量模型再训练或牺牲样本质量，因此探索新加速策略。

Method: 将多核心扩散采样视为ODE求解器管道，通过理论支持的核心间通信机制，用慢而准的求解器逐步修正快求解器，提出CHORDS。

Result: CHORDS显著加速不同大规模图像和视频扩散模型的采样，四核最高达2.1倍加速，比基线提高50%，八核达2.9倍加速，且不降低质量。

Conclusion: CHORDS为实时、高保真扩散生成建立了坚实基础。

Abstract: Diffusion-based generative models have become dominant generators of
high-fidelity images and videos but remain limited by their computationally
expensive inference procedures. Existing acceleration techniques either require
extensive model retraining or compromise significantly on sample quality. This
paper explores a general, training-free, and model-agnostic acceleration
strategy via multi-core parallelism. Our framework views multi-core diffusion
sampling as an ODE solver pipeline, where slower yet accurate solvers
progressively rectify faster solvers through a theoretically justified
inter-core communication mechanism. This motivates our multi-core training-free
diffusion sampling accelerator, CHORDS, which is compatible with various
diffusion samplers, model architectures, and modalities. Through extensive
experiments, CHORDS significantly accelerates sampling across diverse
large-scale image and video diffusion models, yielding up to 2.1x speedup with
four cores, improving by 50% over baselines, and 2.9x speedup with eight cores,
all without quality degradation. This advancement enables CHORDS to establish a
solid foundation for real-time, high-fidelity diffusion generation.

</details>


### [207] [Temporal Basis Function Models for Closed-Loop Neural Stimulation](https://arxiv.org/abs/2507.15274)
*Matthew J. Bryan,Felix Schwock,Azadeh Yazdan-Shahmorad,Rajesh P N Rao*

Main category: cs.LG

TL;DR: 提出时间基函数模型（TBFMs）解决闭环神经刺激的转化问题，该模型样本效率高、训练快、延迟低，能实现对神经活动的预测和控制，缩小了复杂AI建模与临床应用间的差距。


<details>
  <summary>Details</summary>
Motivation: 不清楚人工智能技术能否为个体患者定制闭环神经刺激或确定新疗法，需解决样本效率、训练时间和环路延迟等转化问题。

Method: 提出TBFMs模型，并在兴奋性光遗传刺激背景下进行探索，用模拟验证其用于闭环刺激的能力。

Result: TBF模型能对光遗传刺激对局部场电位的影响进行单试验时空预测，模拟中可控制神经回路，预测精度与需数小时训练的非线性动力学系统模型相当，优于线性状态空间模型。

Conclusion: 该方法开始缩小基于复杂AI的动力系统建模方法与开发临床有用的闭环刺激协议愿景之间的转化差距。

Abstract: Closed-loop neural stimulation provides novel therapies for neurological
diseases such as Parkinson's disease (PD), but it is not yet clear whether
artificial intelligence (AI) techniques can tailor closed-loop stimulation to
individual patients or identify new therapies. Progress requires us to address
a number of translational issues, including sample efficiency, training time,
and minimizing loop latency such that stimulation may be shaped in response to
changing brain activity. We propose temporal basis function models (TBFMs) to
address these difficulties, and explore this approach in the context of
excitatory optogenetic stimulation. We demonstrate the ability of TBF models to
provide a single-trial, spatiotemporal forward prediction of the effect of
optogenetic stimulation on local field potentials (LFPs) measured in two
non-human primates. We further use simulations to demonstrate the use of TBF
models for closed-loop stimulation, driving neural activity towards target
patterns. The simplicity of TBF models allow them to be sample efficient, rapid
to train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the
model on 40 sessions of previously published excitatory optogenetic stimulation
data. For each session, the model required 15-20min of data collection to
successfully model the remainder of the session. It achieved a prediction
accuracy comparable to a baseline nonlinear dynamical systems model that
requires hours to train, and superior accuracy to a linear state-space model.
In our simulations, it also successfully allowed a closed-loop stimulator to
control a neural circuit. Our approach begins to bridge the translational gap
between complex AI-based approaches to modeling dynamical systems and the
vision of using such forward prediction models to develop novel, clinically
useful closed-loop stimulation protocols.

</details>


### [208] [Machine Unlearning for Streaming Forgetting](https://arxiv.org/abs/2507.15280)
*Shaofei Shen,Chenhao Zhang,Yawen Zhao,Alina Bialkowski,Weitong Chen,Miao Xu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Machine unlearning aims to remove knowledge of the specific training data in
a well-trained model. Currently, machine unlearning methods typically handle
all forgetting data in a single batch, removing the corresponding knowledge all
at once upon request. However, in practical scenarios, requests for data
removal often arise in a streaming manner rather than in a single batch,
leading to reduced efficiency and effectiveness in existing methods. Such
challenges of streaming forgetting have not been the focus of much research. In
this paper, to address the challenges of performance maintenance, efficiency,
and data access brought about by streaming unlearning requests, we introduce a
streaming unlearning paradigm, formalizing the unlearning as a distribution
shift problem. We then estimate the altered distribution and propose a novel
streaming unlearning algorithm to achieve efficient streaming forgetting
without requiring access to the original training data. Theoretical analyses
confirm an $O(\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,
where $V_T$ represents the cumulative total variation in the optimal solution
over $T$ learning rounds. This theoretical guarantee is achieved under mild
conditions without the strong restriction of convex loss function. Experiments
across various models and datasets validate the performance of our proposed
method.

</details>


### [209] [Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning](https://arxiv.org/abs/2507.15287)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出框架利用专家演示，将状态与专家数据相似度转换为内在奖励，在不同奖励环境表现好。


<details>
  <summary>Details</summary>
Motivation: 强化学习需从无奖励交互和替代监督信号学习，现有内在动机技术有局限，难有效控制探索。

Method: 提出框架，用映射函数将状态与专家数据相似度转换为内在奖励，用混合自动编码器专家捕捉多样行为。

Result: 实验表明在稀疏和密集奖励环境，演示稀疏或不完整时，能实现稳健探索和良好表现。

Conclusion: 为最优数据不可用、需精确奖励控制的现实强化学习提供实用框架。

Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to
learn from reward-free interactions and alternative supervision signals, such
as unlabeled or incomplete demonstrations, rather than relying solely on
explicit reward maximization. Additionally, developing generalist agents that
can adapt efficiently in real-world environments often requires leveraging
these reward-free signals to guide learning and behavior. However, while
intrinsic motivation techniques provide a means for agents to seek out novel or
uncertain states in the absence of explicit rewards, they are often challenged
by dense reward environments or the complexity of high-dimensional state and
action spaces. Furthermore, most existing approaches rely directly on the
unprocessed intrinsic reward signals, which can make it difficult to shape or
control the agent's exploration effectively. We propose a framework that can
effectively utilize expert demonstrations, even when they are incomplete and
imperfect. By applying a mapping function to transform the similarity between
an agent's state and expert data into a shaped intrinsic reward, our method
allows for flexible and targeted exploration of expert-like behaviors. We
employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors
and accommodate missing information in demonstrations. Experiments show our
approach enables robust exploration and strong performance in both sparse and
dense reward environments, even when demonstrations are sparse or incomplete.
This provides a practical framework for RL in realistic settings where optimal
data is unavailable and precise reward control is needed.

</details>


### [210] [Preferential subspace identification (PSID) with forward-backward smoothing](https://arxiv.org/abs/2507.15288)
*Omid G. Sani,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 本文扩展了PSID方法，使其支持最优滤波和平滑，在模拟数据上验证了方法有效性，为双信号场景提供线性滤波和平滑框架。


<details>
  <summary>Details</summary>
Motivation: 现有PSID方法专注于用过去主要数据进行最优预测，在离线应用中结合并发数据或所有可用数据能实现更好估计，因此需扩展PSID。

Method: 首先，在PSID基础上增加降秩回归步骤实现滤波；其次，受双滤波器卡尔曼平滑公式启发，开发前向后向PSID平滑算法。

Result: 在模拟数据上，方法能恢复滤波的真实模型参数，实现与真实底层模型理想性能匹配的最优滤波和平滑解码性能。

Conclusion: 为双信号场景提供了最优线性滤波和平滑的原则性框架，扩展了多变量时间序列动态交互分析工具。

Abstract: System identification methods for multivariate time-series, such as neural
and behavioral recordings, have been used to build models for predicting one
from the other. For example, Preferential Subspace Identification (PSID) builds
a state-space model of a primary time-series (e.g., neural activity) to
optimally predict a secondary time-series (e.g., behavior). However, PSID
focuses on optimal prediction using past primary data, even though in offline
applications, better estimation can be achieved by incorporating concurrent
data (filtering) or all available data (smoothing). Here, we extend PSID to
enable optimal filtering and smoothing. First, we show that the presence of a
secondary signal makes it possible to uniquely identify a model with an optimal
Kalman update step (to enable filtering) from a family of otherwise equivalent
state-space models. Our filtering solution augments PSID with a reduced-rank
regression step that directly learns the optimal gain required for the update
step from data. We refer to this extension of PSID as PSID with filtering.
Second, inspired by two-filter Kalman smoother formulations, we develop a novel
forward-backward PSID smoothing algorithm where we first apply PSID with
filtering and then apply it again in the reverse time direction on the
residuals of the filtered secondary signal. We validate our methods on
simulated data, showing that our approach recovers the ground-truth model
parameters for filtering, and achieves optimal filtering and smoothing decoding
performance of the secondary signal that matches the ideal performance of the
true underlying model. This work provides a principled framework for optimal
linear filtering and smoothing in the two-signal setting, significantly
expanding the toolkit for analyzing dynamic interactions in multivariate
time-series.

</details>


### [211] [Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown](https://arxiv.org/abs/2507.15290)
*Emile Anand,Sarah Liaw*

Main category: cs.LG

TL;DR: 对Feel - Good Thompson Sampling (FG - TS)及其平滑变体SFG - TS在11个真实和合成基准测试中进行系统研究，分析其在精确和近似后验情况下的表现并给出推荐。


<details>
  <summary>Details</summary>
Motivation: FG - TS在精确后验的线性设置中表现良好，但在近似后验情况下的性能未被评估，需要进行研究。

Method: 在11个基准测试中对比FG - TS和SFG - TS在精确和近似后验设置下的性能，对预条件、奖励规模和先验强度进行消融实验。

Result: 较大奖励在准确后验样本时有用，采样噪声占主导时有害；FG - TS在线性和逻辑带问题中通常优于普通TS，在神经带问题中较弱。

Conclusion: FG - TS及其变体有竞争力且易用，推荐作为现代上下文多臂老虎机基准测试的基线。

Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation
tradeoff in contextual bandits, yet recent theory shows that it does not
explore aggressively enough in high-dimensional problems. Feel-Good Thompson
Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward
high-reward models, and it achieves the asymptotically minimax-optimal regret
in the linear setting when posteriors are exact. However, its performance with
\emph{approximate} posteriors -- common in large-scale or neural problems --
has not been benchmarked. We provide the first systematic study of FG-TS and
its smoothed variant (SFG-TS) across eleven real-world and synthetic
benchmarks. To evaluate their robustness, we compare performance across
settings with exact posteriors (linear and logistic bandits) to approximate
regimes produced by fast but coarse stochastic-gradient samplers. Ablations
over preconditioning, bonus scale, and prior strength reveal a trade-off:
larger bonuses help when posterior samples are accurate, but hurt when sampling
noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic
bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS
and its variants are competitive and easy-to-use, we recommend them as
baselines in modern contextual-bandit benchmarks. Finally, we provide source
code for all our experiments in
https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.

</details>


### [212] [Universal crystal material property prediction via multi-view geometric fusion in graph transformers](https://arxiv.org/abs/2507.15303)
*Liang Zhang,Kong Chen,Yuen Wu*

Main category: cs.LG

TL;DR: 提出多视图图变换器框架MGT用于晶体材料性质预测，结合SE3和SO3表示，在多任务预训练和迁移学习中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有晶体性质预测方法难以有效捕捉和利用晶体结构的复杂几何与拓扑特征。

Method: 提出MGT框架，融合SE3不变和SO3等变图表示，用轻量级专家混合路由器自适应调整权重，进行多任务自监督预训练。

Result: 在晶体性质预测任务上MAE最多降低21%，在迁移学习场景中性能最多提升58%。

Conclusion: MGT可作为晶体材料性质预测的有用模型，为新材料发现提供工具。

Abstract: Accurately and comprehensively representing crystal structures is critical
for advancing machine learning in large-scale crystal materials simulations,
however, effectively capturing and leveraging the intricate geometric and
topological characteristics of crystal structures remains a core, long-standing
challenge for most existing methods in crystal property prediction. Here, we
propose MGT, a multi-view graph transformer framework that synergistically
fuses SE3 invariant and SO3 equivariant graph representations, which
respectively captures rotation-translation invariance and rotation equivariance
in crystal geometries. To strategically incorporate these complementary
geometric representations, we employ a lightweight mixture of experts router in
MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on
the specific target task. Compared with previous state-of-the-art models, MGT
reduces the mean absolute error by up to 21% on crystal property prediction
tasks through multi-task self-supervised pretraining. Ablation experiments and
interpretable investigations confirm the effectiveness of each technique
implemented in our framework. Additionally, in transfer learning scenarios
including crystal catalyst adsorption energy and hybrid perovskite bandgap
prediction, MGT achieves performance improvements of up to 58% over existing
baselines, demonstrating domain-agnostic scalability across diverse application
domains. As evidenced by the above series of studies, we believe that MGT can
serve as useful model for crystal material property prediction, providing a
valuable tool for the discovery of novel materials.

</details>


### [213] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: 提出PALM模型评估主动学习，实验验证其能跨数据集等有效泛化，为主动学习评估奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习评估方法只关注最终准确率，无法捕捉学习过程动态，需新评估方法。

Method: 提出PALM模型，通过四个关键参数刻画主动学习轨迹，可从部分观察预测未来表现。

Result: 在多个数据集上实验，PALM能有效泛化，从有限标注数据准确预测完整学习曲线。

Conclusion: PALM为主动学习在研究和实际应用中提供更系统、可复现和数据高效的评估基础。

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [214] [Learning to Gridize: Segment Physical World by Wireless Communication Channel](https://arxiv.org/abs/2507.15386)
*Juntao Wang,Feng Yin,Tian Ding,Tsung-Hui Chang,Zhi-Quan Luo,Qi Yan*

Main category: cs.LG

TL;DR: 提出CSG框架统一信道估计和网格化，开发CSG - AE及PIDA训练方案，在合成和真实数据上表现良好，推动大规模网络优化网格化发展。


<details>
  <summary>Details</summary>
Motivation: 现有网格化方法依赖不可用位置数据或假设存在缺陷，需要更好的方法进行大规模网络优化。

Method: 提出CSG框架，开发CSG - AE，包括可训练编码器、可学习量化器和物理感知解码器，提出PIDA训练方案。

Result: 在合成数据上CAPS估计和聚类质量好，在真实数据集上RSRP预测误差降低，提升信道一致性等指标。

Conclusion: CSG - AE及相关方案能有效推动大规模网络优化的网格化发展。

Abstract: Gridization, the process of partitioning space into grids where users share
similar channel characteristics, serves as a fundamental prerequisite for
efficient large-scale network optimization. However, existing methods like
Geographical or Beam Space Gridization (GSG or BSG) are limited by reliance on
unavailable location data or the flawed assumption that similar signal
strengths imply similar channel properties. We propose Channel Space
Gridization (CSG), a pioneering framework that unifies channel estimation and
gridization for the first time. Formulated as a joint optimization problem, CSG
uses only beam-level reference signal received power (RSRP) to estimate Channel
Angle Power Spectra (CAPS) and partition samples into grids with homogeneous
channel characteristics. To perform CSG, we develop the CSG Autoencoder
(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse
codebook quantizer, and a physics-informed decoder based on the Localized
Statistical Channel Model. On recognizing the limitations of naive training
scheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous
(PIDA) training scheme for CSG-AE, ensuring stable and effective training by
systematically addressing the common pitfalls of the naive training paradigm.
Evaluations reveal that CSG-AE excels in CAPS estimation accuracy and
clustering quality on synthetic data. On real-world datasets, it reduces Active
Mean Absolute Error (MAE) by 30\% and Overall MAE by 65\% on RSRP prediction
accuracy compared to salient baselines using the same data, while improving
channel consistency, cluster sizes balance, and active ratio, advancing the
development of gridization for large-scale network optimization.

</details>


### [215] [The calculus of variations of the Transformer on the hyperspherical tangent bundle](https://arxiv.org/abs/2507.15431)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 本文通过拉格朗日优化为Transformer提供理论数学背景，构建数学框架，推导欧拉 - 拉格朗日方程，为Transformer变分法领域奠定基础。


<details>
  <summary>Details</summary>
Motivation: 为Transformer提供理论数学背景，探索其在变分法领域的基础理论。

Method: 通过拉格朗日优化和变分法，利用连续极限下潜向量在切丛间的流动等特性构建数学框架。

Result: 构建了Transformer的数学框架，推导了欧拉 - 拉格朗日方程，给出相关结果并尝试在变分背景下量化Transformer数据。

Conclusion: 为Transformer在变分法领域的研究奠定了基础。

Abstract: We offer a theoretical mathematical background to Transformers through
Lagrangian optimization across the token space. The Transformer, as a flow map,
exists in the tangent fiber for each token along the high-dimensional unit
sphere. The circumstance of the hypersphere across the latent data is
reasonable due to the trained diagonal matrix equal to the identity, which has
various empirical justifications. Thus, under the continuum limit of the
dynamics, the latent vectors flow among the tangent bundle. Using these facts,
we devise a mathematical framework for the Transformer through calculus of
variations. We develop a functional and show that the continuous flow map
induced by the Transformer satisfies this functional, therefore the Transformer
can be viewed as a natural solver of a calculus of variations problem. We
invent new scenarios of when our methods are applicable based on loss
optimization with respect to path optimality. We derive the Euler-Lagrange
equation for the Transformer. The variant of the Euler-Lagrange equation we
present has various appearances in literature, but, to our understanding,
oftentimes not foundationally proven or under other specialized cases. Our
overarching proof is new: our techniques are classical and the use of the flow
map object is original. We provide several other relevant results, primarily
ones specific to neural scenarios. In particular, much of our analysis will be
attempting to quantify Transformer data in variational contexts under neural
approximations. Calculus of variations on manifolds is a well-nourished
research area, but for the Transformer specifically, it is uncharted: we lay
the foundation for this area through an introduction to the Lagrangian for the
Transformer.

</details>


### [216] [An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations](https://arxiv.org/abs/2507.15442)
*Owen Douglas,Aku Kammonen,Anamika Pandey,Raúl Tempone*

Main category: cs.LG

TL;DR: 提出基于自适应随机傅里叶特征（ARFF）与Metropolis采样和重采样的训练算法，用于从快照数据学习随机微分方程的漂移和扩散分量，评估显示其性能优于传统Adam优化。


<details>
  <summary>Details</summary>
Motivation: 从快照数据学习随机微分方程的漂移和扩散分量。

Method: 提出基于ARFF与Metropolis采样和重采样的训练算法，考虑Itô扩散过程和基于似然的损失函数。

Result: 在所有评估案例中，ARFF方法在损失最小化和收敛速度上匹配或超越传统Adam优化。

Conclusion: ARFF是随机动力学数据驱动建模的有力替代方法。

Abstract: This work proposes a training algorithm based on adaptive random Fourier
features (ARFF) with Metropolis sampling and resampling
\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and
diffusion components of stochastic differential equations from snapshot data.
Specifically, this study considers It\^{o} diffusion processes and a
likelihood-based loss function derived from the Euler-Maruyama integration
introduced in \cite{Dietrich2023} and
\cite{dridi2021learningstochasticdynamicalsystems}.
  This work evaluates the proposed method against benchmark problems presented
in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin
dynamics, a stochastic susceptible-infected-recovered model, and a stochastic
wave equation. Across all cases, the ARFF-based approach matches or surpasses
the performance of conventional Adam-based optimization in both loss
minimization and convergence speed. These results highlight the potential of
ARFF as a compelling alternative for data-driven modeling of stochastic
dynamics.

</details>


### [217] [FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning](https://arxiv.org/abs/2507.15470)
*Baran Can Gül,Suraksha Nadig,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: 提出FedMultiEmo框架用于车内情绪识别，融合多模态数据，实现隐私保护，效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有车内情绪识别技术存在模态脆弱、生理差异和隐私风险等问题，阻碍实际应用。

Method: 提出FedMultiEmo框架，在决策层融合视觉和生理两种模态，采用多模态联邦学习管道、树莓派客户端和Flower服务器的端到端原型以及个性化联邦平均方案。

Result: 在FER2013和自定义生理数据集上，联邦卷积神经网络准确率77%，随机森林74%，融合后87%，系统18轮收敛，平均轮次时间120秒，单客户端内存占用低于200MB。

Conclusion: FedMultiEmo为汽车环境下实时、注重隐私的情绪识别提供了实用方法。

Abstract: In-vehicle emotion recognition underpins adaptive driver-assistance systems
and, ultimately, occupant safety. However, practical deployment is hindered by
(i) modality fragility - poor lighting and occlusions degrade vision-based
methods; (ii) physiological variability - heart-rate and skin-conductance
patterns differ across individuals; and (iii) privacy risk - centralized
training requires transmission of sensitive data. To address these challenges,
we present FedMultiEmo, a privacy-preserving framework that fuses two
complementary modalities at the decision level: visual features extracted by a
Convolutional Neural Network from facial images, and physiological cues (heart
rate, electrodermal activity, and skin temperature) classified by a Random
Forest. FedMultiEmo builds on three key elements: (1) a multimodal federated
learning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud
prototype on Raspberry Pi clients and a Flower server, and (3) a personalized
Federated Averaging scheme that weights client updates by local data volume.
Evaluated on FER2013 and a custom physiological dataset, the federated
Convolutional Neural Network attains 77% accuracy, the Random Forest 74%, and
their fusion 87%, matching a centralized baseline while keeping all raw data
local. The developed system converges in 18 rounds, with an average round time
of 120 seconds and a per-client memory footprint below 200 MB. These results
indicate that FedMultiEmo offers a practical approach to real-time,
privacy-aware emotion recognition in automotive settings.

</details>


### [218] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文研究RLHF中过优化问题，提出OCRM方法，在实验中表现优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 解决RLHF中奖励模型因分布偏移导致不准确，出现过优化问题，即奖励模型得分上升但学习行为不符合人类偏好。

Method: 从分布偏移角度研究过优化问题，提出OCRM方法，通过重要性加权迭代离策略校正奖励模型，无需新标签或样本。

Result: 实验表明OCRM方法能得到更准确奖励模型，最终策略表现更好，显著优于标准RLHF方法和基线。

Conclusion: OCRM方法有效解决了RLHF中的过优化问题，提升了模型性能。

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [219] [An Investigation of Test-time Adaptation for Audio Classification under Background Noise](https://arxiv.org/abs/2507.15523)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.LG

TL;DR: 本文用TTA技术解决背景噪声导致的领域偏移下的音频分类问题，对比几种TTA方法，改进版CoNMix效果最佳。


<details>
  <summary>Details</summary>
Motivation: 领域偏移导致预训练模型在测试集上性能下降，现有文献缺乏利用TTA技术解决音频分类领域偏移问题的研究。

Method: 采用TTT、TENT和CoNMix三种TTA方法，在AudioMNIST和SpeechCommands V1两个数据集上，针对不同类型和强度的背景噪声进行实验。

Result: 改进版CoNMix在领域偏移下分类准确率最高，如在AudioMNIST数据集上，10 dB健身自行车背景噪声下错误率5.31%，3 dB水龙头背景噪声下错误率12.75%。

Conclusion: 改进版CoNMix在解决音频分类领域偏移问题上表现优于TTT和TENT，该研究是首次利用TTA技术解决此问题的研究。

Abstract: Domain shift is a prominent problem in Deep Learning, causing a model
pre-trained on a source dataset to suffer significant performance degradation
on test datasets. This research aims to address the issue of audio
classification under domain shift caused by background noise using Test-Time
Adaptation (TTA), a technique that adapts a pre-trained model during testing
using only unlabelled test data before making predictions. We adopt two common
TTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and
investigate their respective performance on two popular audio classification
datasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types
of background noise and noise severity levels. The experimental results reveal
that our proposed modified version of CoNMix produced the highest
classification accuracy under domain shift (5.31% error rate under 10 dB
exercise bike background noise and 12.75% error rate under 3 dB running tap
background noise for AM) compared to TTT and TENT. The literature search
provided no evidence of similar works, thereby motivating the work reported
here as the first study to leverage TTA techniques for audio classification
under domain shift.

</details>


### [220] [Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications](https://arxiv.org/abs/2507.15545)
*Yujia Shi,Emil Njor,Pablo Martínez-Nuevo,Sven Ewan Shepstone,Xenofon Fafoutis*

Main category: cs.LG

TL;DR: 机器学习资源消耗大促使TinyML受关注，但设计复杂，提出数据感知可微神经架构搜索方法，能优化模型架构和数据特征，在关键词检测初步结果良好。


<details>
  <summary>Details</summary>
Motivation: 机器学习资源消耗大，TinyML设计复杂阻碍其广泛应用，需降低设计复杂性。

Method: 引入数据感知可微神经架构搜索，将搜索空间扩展到包含数据配置参数与架构选择，共同优化模型架构和输入数据特征。

Result: 在关键词检测的初步结果表明，该方法能生成精简且高精度的系统。

Conclusion: 该新颖方法可有效平衡TinyML应用的资源使用和系统性能。

Abstract: The success of Machine Learning is increasingly tempered by its significant
resource footprint, driving interest in efficient paradigms like TinyML.
However, the inherent complexity of designing TinyML systems hampers their
broad adoption. To reduce this complexity, we introduce "Data Aware
Differentiable Neural Architecture Search". Unlike conventional Differentiable
Neural Architecture Search, our approach expands the search space to include
data configuration parameters alongside architectural choices. This enables
Data Aware Differentiable Neural Architecture Search to co-optimize model
architecture and input data characteristics, effectively balancing resource
usage and system performance for TinyML applications. Initial results on
keyword spotting demonstrate that this novel approach to TinyML system design
can generate lean but highly accurate systems.

</details>


### [221] [The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information](https://arxiv.org/abs/2507.15548)
*D. Abler,O. Pusterla,A. Joye-Kühnis,N. Andratschke,M. Bach,A. Bink,S. M. Christ,P. Hagmann,B. Pouymayou,E. Pravatà,P. Radojewski,M. Reyes,L. Ruinelli,R. Schaer,B. Stieltjes,G. Treglia,W. Valenzuela,R. Wiest,S. Zoergiebel,M. Guckenberger,S. Tanadini-Lang,A. Depeursinge*

Main category: cs.LG

TL;DR: 研究评估多中心数据中传统放射组学（CR）和深度学习（DL）MRI放射组学对胶质母细胞瘤预后的附加价值，发现其相比人口统计学预测因子附加价值有限。


<details>
  <summary>Details</summary>
Motivation: 评估传统放射组学和深度学习MRI放射组学在胶质母细胞瘤预后评估上，相比临床和分子预测因子的附加价值。

Method: 收集五个瑞士中心和一个公共来源的1152例胶质母细胞瘤患者数据，开发CR和DL模型，在内部和外部队列评估，进行不同特征集和患者子集的亚分析。

Result: 全队列中组合特征CR模型外部验证AUC达0.75，略优于临床或影像单特征模型；DL模型趋势类似但无统计学意义；部分子集组合模型未优于临床单特征模型；CR模型探索性分析显示影像数据有一定相关性。

Conclusion: 多中心研究确认解剖MRI序列对胶质母细胞瘤预后有预测价值，但标准CR和DL放射组学方法相比人口统计学预测因子附加价值极小。

Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its
added value over clinical and molecular predictors has yet to be proven. This
study assessed the added value of conventional radiomics (CR) and deep learning
(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on
a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152
glioblastoma (WHO 2016) patients from five Swiss centers and one public source.
It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI
data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were
developed using standard methods and evaluated on internal and external
cohorts. Sub-analyses assessed models with different feature sets
(imaging-only, clinical/molecular-only, combined-features) and patient subsets
(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In
external validation, the combined-feature CR model achieved an AUC of 0.75,
slightly, but significantly outperforming clinical-only (0.74) and imaging-only
(0.68) models. DL models showed similar trends, though without statistical
significance. In S-2 and S-3, combined models did not outperform clinical-only
models. Exploratory analysis of CR models for overall survival prediction
suggested greater relevance of imaging data: across all subsets,
combined-feature models significantly outperformed clinical-only models, though
with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI
sequences for glioblastoma prognosis, this multi-center study found standard CR
and DL radiomics approaches offer minimal added value over demographic
predictors such as age and gender.

</details>


### [222] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: 引入用于评估大语言模型在交互式物理环境中科学推理能力的PhysGym基准套件和模拟平台。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估大语言模型在科学发现能力方面（应对环境复杂性和利用先验知识）的专门基准，为填补这一空白。

Method: 构建包含一系列交互式模拟的PhysGym，可控制提供给智能体的先验知识水平，有标准化评估协议和指标。

Result: 通过基线大语言模型的测试结果，展示了该基准能够根据不同先验知识和任务复杂度区分模型能力。

Conclusion: PhysGym基准套件和模拟平台对评估大语言模型在交互式物理环境中的科学推理能力具有实用性。

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce PhysGym, a novel benchmark
suite and simulation platform for rigorously assessing LLM-based scientific
reasoning in interactive physics environments. PhysGym's primary contribution
lies in its sophisticated control over the level of prior knowledge provided to
the agent. This allows researchers to dissect agent performance along axes
including the complexity of the problem and the prior knowledge levels. The
benchmark comprises a suite of interactive simulations, where agents must
actively probe environments, gather data sequentially under constraints and
formulate hypotheses about underlying physical laws. PhysGym provides
standardized evaluation protocols and metrics for assessing hypothesis accuracy
and model fidelity. We demonstrate the benchmark's utility by presenting
results from baseline LLMs, showcasing its ability to differentiate
capabilities based on varying priors and task complexity.

</details>


### [223] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: 本文探讨住院时长（LOS）预测准确性与重新调度灵活性之间的关系，研究在LOS预测误差下最有效的患者重新调度策略以防止床位溢出并优化资源利用。


<details>
  <summary>Details</summary>
Motivation: 在择期手术患者入院规划中，机器学习模型预测的LOS与实际值可能有较大差异，导致原计划不可行，需要重新调度策略，且准确的LOS预测成本高，因此要研究两者关系。

Method: 基于之前提出的模拟ML评估数据驱动方法的工作进行研究。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [224] [On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project](https://arxiv.org/abs/2507.15574)
*Gregory F. Stock,Juan A. Fraire,Holger Hermanns,Jędrzej Mosiężny,Yusra Al-Khazraji,Julio Ramírez Molina,Evridiki V. Ntagiou*

Main category: cs.LG

TL;DR: 本文基于ConstellAI项目，探索AI优化卫星巨型星座运营，用强化学习解决数据路由和资源分配问题，证明AI在卫星星座管理中有优势。


<details>
  <summary>Details</summary>
Motivation: 近地轨道卫星星座快速扩张，卫星网络管理需创新方法。

Method: 由GMV GmbH、Saarland大学和Thales Alenia Space组成的联盟开发AI驱动算法，用强化学习解决数据路由和资源分配问题。

Result: 强化学习在数据路由上降低端到端延迟，在资源分配上优化任务调度，在多种卫星星座配置和场景测试中表现良好。

Conclusion: AI能为卫星星座管理提供更自适应、稳健和低成本的解决方案，可改变卫星星座管理格局。

Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents
significant challenges in satellite network management, requiring innovative
approaches for efficient, scalable, and resilient operations. This paper
explores the role of Artificial Intelligence (AI) in optimizing the operation
of satellite mega-constellations, drawing from the ConstellAI project funded by
the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland
University, and Thales Alenia Space collaborates to develop AI-driven
algorithms and demonstrates their effectiveness over traditional methods for
two crucial operational challenges: data routing and resource allocation. In
the routing use case, Reinforcement Learning (RL) is used to improve the
end-to-end latency by learning from historical queuing latency, outperforming
classical shortest path algorithms. For resource allocation, RL optimizes the
scheduling of tasks across constellations, focussing on efficiently using
limited resources such as battery and memory. Both use cases were tested for
multiple satellite constellation configurations and operational scenarios,
resembling the real-life spacecraft operations of communications and Earth
observation satellites. This research demonstrates that RL not only competes
with classical approaches but also offers enhanced flexibility, scalability,
and generalizability in decision-making processes, which is crucial for the
autonomous and intelligent management of satellite fleets. The findings of this
activity suggest that AI can fundamentally alter the landscape of satellite
constellation management by providing more adaptive, robust, and cost-effective
solutions.

</details>


### [225] [We Need to Rethink Benchmarking in Anomaly Detection](https://arxiv.org/abs/2507.15584)
*Philipp Röchner,Simon Klüttermann,Franz Rothlauf,Daniel Schlör*

Main category: cs.LG

TL;DR: 当前异常检测算法评估存在局限致进展停滞，需重新思考基准测试并提出三方面改进建议。


<details>
  <summary>Details</summary>
Motivation: 解决异常检测算法进展停滞问题，指出原因是现有评估方式存在局限性。

Method: 提出使用能反映不同应用特征的场景研究异常检测，确定三方面改进：基于通用分类法确定场景、端到端和按组件分析检测流程、评估算法要符合场景目标。

Result: 无明确实验结果，提出新的异常检测基准测试思路。

Conclusion: 需重新思考异常检测的基准测试方式，从三方面进行改进。

Abstract: Despite the continuous proposal of new anomaly detection algorithms and
extensive benchmarking efforts, progress seems to stagnate, with only minor
performance differences between established baselines and new algorithms. In
this position paper, we argue that this stagnation is due to limitations in how
we evaluate anomaly detection algorithms. Current benchmarking does not, for
example, sufficiently reflect the diversity of anomalies in applications
ranging from predictive maintenance to scientific discovery. Consequently, we
need to rethink benchmarking in anomaly detection. In our opinion, anomaly
detection should be studied using scenarios that capture the relevant
characteristics of different applications. We identify three key areas for
improvement: First, we need to identify anomaly detection scenarios based on a
common taxonomy. Second, anomaly detection pipelines should be analyzed
end-to-end and by component. Third, evaluating anomaly detection algorithms
should be meaningful regarding the scenario's objectives.

</details>


### [226] [Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario](https://arxiv.org/abs/2507.15587)
*Yinsong Chen,Kaifeng Wang,Xiaoqiang Meng,Xueyuan Li,Zirui Li,Xin Gao*

Main category: cs.LG

TL;DR: 提出红队多智能体强化学习框架解决安全关键场景决策研究问题，实验表明能影响自动驾驶决策安全并生成多种极端情况。


<details>
  <summary>Details</summary>
Motivation: 当前安全关键场景决策研究依赖低效数据驱动或特定建模方法，无法捕捉现实中的极端情况。

Method: 提出红队多智能体强化学习框架，将有干扰能力的背景车辆作为红队智能体，使用约束图表示马尔可夫决策过程，构建策略威胁区模型。

Result: 该框架对自动驾驶决策安全有显著影响，能生成多种极端情况。

Conclusion: 此方法为安全关键场景研究提供了新方向。

Abstract: Current research on decision-making in safety-critical scenarios often relies
on inefficient data-driven scenario generation or specific modeling approaches,
which fail to capture corner cases in real-world contexts. To address this
issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,
where background vehicles with interference capabilities are treated as
red-team agents. Through active interference and exploration, red-team vehicles
can uncover corner cases outside the data distribution. The framework uses a
Constraint Graph Representation Markov Decision Process, ensuring that red-team
vehicles comply with safety rules while continuously disrupting the autonomous
vehicles (AVs). A policy threat zone model is constructed to quantify the
threat posed by red-team vehicles to AVs, inducing more extreme actions to
increase the danger level of the scenario. Experimental results show that the
proposed framework significantly impacts AVs decision-making safety and
generates various corner cases. This method also offers a novel direction for
research in safety-critical scenarios.

</details>


### [227] [Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity](https://arxiv.org/abs/2507.15601)
*Huiling Yang,Zhanwei Wang,Kaibin Huang*

Main category: cs.LG

TL;DR: 本文提出用于最优批量大小控制的C²感知框架，解决低延迟联邦学习挑战，实验表明策略优于传统方案。


<details>
  <summary>Details</summary>
Motivation: 6G网络中物联网应用对低延迟联邦学习框架有需求，但实际面临计算传输开销和设备异质性挑战。

Method: 提出C²感知框架平衡通信与计算权衡，设计准确易处理的收敛速度替代方案以解决延迟最小化问题，得到适用于不同场景的批量大小控制策略。

Result: 使用真实数据集的实验表明，提出的策略优于不考虑C²权衡或设备异质性的传统批量大小自适应方案。

Conclusion: 所提的C²感知框架及批量大小控制策略能有效解决低延迟联邦学习面临的挑战，提升学习性能。

Abstract: Federated learning (FL) has emerged as a popular approach for collaborative
machine learning in sixth-generation (6G) networks, primarily due to its
privacy-preserving capabilities. The deployment of FL algorithms is expected to
empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous
driving, augmented reality, and healthcare. The mission-critical and
time-sensitive nature of these applications necessitates the design of
low-latency FL frameworks that guarantee high learning performance. In
practice, achieving low-latency FL faces two challenges: the overhead of
computing and transmitting high-dimensional model updates, and the
heterogeneity in communication-and-computation (C$^2$) capabilities across
devices. To address these challenges, we propose a novel C$^2$-aware framework
for optimal batch-size control that minimizes end-to-end (E2E) learning latency
while ensuring convergence. The framework is designed to balance a fundamental
C$^2$ tradeoff as revealed through convergence analysis. Specifically,
increasing batch sizes improves the accuracy of gradient estimation in FL and
thus reduces the number of communication rounds required for convergence, but
results in higher per-round latency, and vice versa. The associated problem of
latency minimization is intractable; however, we solve it by designing an
accurate and tractable surrogate for convergence speed, with parameters fitted
to real data. This approach yields two batch-size control strategies tailored
to scenarios with slow and fast fading, while also accommodating device
heterogeneity. Extensive experiments using real datasets demonstrate that the
proposed strategies outperform conventional batch-size adaptation schemes that
do not consider the C$^2$ tradeoff or device heterogeneity.

</details>


### [228] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: 本文提出深度学习替代模型加速洪水模拟，在密西西比河流域取得良好预测精度与速度提升。


<details>
  <summary>Details</summary>
Motivation: 基于物理的求解器计算密集，难以用于洪水实时决策，需在不牺牲精度下加速模拟。

Method: 提出结合GRU和Geo - FNO的混合自回归架构，从HEC - RAS文件提取特征向量训练模型。

Result: 模型预测精度高，中位绝对水位误差0.31英尺，67河段集合预报加速近3.5倍。

Conclusion: 数据驱动方法可行，特征工程能替代传统水力模型，提升大规模集合洪水预报计算可行性。

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


### [229] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: 提出Data Mixing Agent框架学习领域重加权，在数学推理和代码生成领域实验效果好且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 持续预训练有灾难性遗忘问题，以往领域重加权策略依赖人工指定，需更通用启发式方法。

Method: 提出基于模型的端到端Data Mixing Agent框架，通过强化学习学习可泛化启发式。

Result: 在数学推理实验中优于基线，跨未见源领域、目标模型和领域空间泛化性好，在代码生成领域也有适应性。

Conclusion: Data Mixing Agent框架有效，启发式与人类直觉一致，能以更少源领域数据提升模型性能。

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [230] [Towards Explainable Anomaly Detection in Shared Mobility Systems](https://arxiv.org/abs/2507.15643)
*Elnur Isgandarov,Matteo Cederle,Federico Chiariotti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本文提出可解释的共享出行系统异常检测框架，用算法检测异常，结果显示站点级分析有用，助于运营决策。


<details>
  <summary>Details</summary>
Motivation: 识别共享出行系统（如共享单车网络）的异常，以优化运营、提高服务可靠性和用户体验。

Method: 提出集成多源数据的可解释异常检测框架，使用Isolation Forest算法进行无监督异常检测，用DIFFI算法提供可解释性。

Result: 站点级分析能更好理解异常，凸显恶劣天气和公共交通可用性等外部因素影响。

Conclusion: 研究结果有助于改善共享出行运营决策。

Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role
in urban transportation. Identifying anomalies in these systems is essential
for optimizing operations, improving service reliability, and enhancing user
experience. This paper presents an interpretable anomaly detection framework
that integrates multi-source data, including bike-sharing trip records, weather
conditions, and public transit availability. The Isolation Forest algorithm is
employed for unsupervised anomaly detection, along with the Depth-based
Isolation Forest Feature Importance (DIFFI) algorithm providing
interpretability. Results show that station-level analysis offers a robust
understanding of anomalies, highlighting the influence of external factors such
as adverse weather and limited transit availability. Our findings contribute to
improving decision-making in shared mobility operations.

</details>


### [231] [Explainable Anomaly Detection for Electric Vehicles Charging Stations](https://arxiv.org/abs/2507.15718)
*Matteo Cederle,Andrea Mazzucco,Andrea Demartini,Eugenio Mazza,Eugenia Suriani,Federico Vitti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 研究针对电动汽车充电站，结合无监督异常检测和可解释人工智能技术，用Isolation Forest检测异常，DIFFI识别重要特征，并在实际工业案例评估效果。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电站需有效异常检测保障可靠性和效率，且要确定异常根源。

Method: 运用无监督异常检测技术，结合可解释人工智能技术，使用Isolation Forest检测异常，用DIFFI方法识别重要特征。

Result: 在实际工业案例中评估了所提方法的有效性。

Conclusion: 未明确提及结论，但暗示所提结合方法有助于检测电动汽车充电基础设施异常并找出根源。

Abstract: Electric vehicles (EV) charging stations are one of the critical
infrastructures needed to support the transition to renewable-energy-based
mobility, but ensuring their reliability and efficiency requires effective
anomaly detection to identify irregularities in charging behavior. However, in
such a productive scenario, it is also crucial to determine the underlying
cause behind the detected anomalies. To achieve this goal, this study
investigates unsupervised anomaly detection techniques for EV charging
infrastructure, integrating eXplainable Artificial Intelligence techniques to
enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies
Isolation Forest to detect anomalies and employs the Depth-based Isolation
Forest Feature Importance (DIFFI) method to identify the most important
features contributing to such anomalies. The efficacy of the proposed approach
is evaluated in a real industrial case.

</details>


### [232] [Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks](https://arxiv.org/abs/2507.15769)
*Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil*

Main category: cs.LG

TL;DR: 提出多模态感知的毫米波阻塞预测框架，评估不同配置模型效果，显示多模态感知有效性。


<details>
  <summary>Details</summary>
Motivation: 毫米波车载通信系统易受动态障碍物信号阻塞，需解决此问题。

Method: 利用相机、GPS、LiDAR和雷达多模态感知，用特定深度学习模型处理各传感器数据流，用基于验证性能的softmax加权集成策略融合输出。

Result: 提前1.5秒评估，相机单模型F1分数97.1%，推理时间89.8ms；相机+雷达配置F1分数达97.2%，推理时间95.7ms。

Conclusion: 多模态感知用于毫米波阻塞预测有效且高效，为动态环境下主动无线通信提供途径。

Abstract: Vehicular communication systems operating in the millimeter wave (mmWave)
band are highly susceptible to signal blockage from dynamic obstacles such as
vehicles, pedestrians, and infrastructure. To address this challenge, we
propose a proactive blockage prediction framework that utilizes multi-modal
sensing, including camera, GPS, LiDAR, and radar inputs in an
infrastructure-to-vehicle (I2V) setting. This approach uses modality-specific
deep learning models to process each sensor stream independently and fuses
their outputs using a softmax-weighted ensemble strategy based on validation
performance. Our evaluations, for up to 1.5s in advance, show that the
camera-only model achieves the best standalone trade-off with an F1-score of
97.1% and an inference time of 89.8ms. A camera+radar configuration further
improves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness
and efficiency of multi-modal sensing for mmWave blockage prediction and
provide a pathway for proactive wireless communication in dynamic environments.

</details>


### [233] [Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis](https://arxiv.org/abs/2507.15772)
*Anoop C. Patil,Benny Jian Rong Sng,Yu-Wei Chang,Joana B. Pereira,Chua Nam-Hai,Rajani Sarojam,Gajendra Pratap Singh,In-Cheol Jang,Giovanni Volpe*

Main category: cs.LG

TL;DR: 介绍DIVA工作流，利用深度学习和拉曼光谱检测植物压力，为人工智能驱动的植物健康评估铺平道路。


<details>
  <summary>Details</summary>
Motivation: 传统拉曼分析检测植物压力有潜在偏差和不一致问题，需要新方法。

Method: 引入基于变分自编码器的全自动工作流DIVA，处理原始拉曼光谱，无需手动预处理。

Result: DIVA可检测多种植物压力，包括非生物和生物压力源。

Conclusion: DIVA通过集成深度学习和振动光谱，为人工智能驱动的植物健康评估奠定基础，促进农业可持续发展。

Abstract: Detecting stress in plants is crucial for both open-farm and
controlled-environment agriculture. Biomolecules within plants serve as key
stress indicators, offering vital markers for continuous health monitoring and
early disease detection. Raman spectroscopy provides a powerful, non-invasive
means to quantify these biomolecules through their molecular vibrational
signatures. However, traditional Raman analysis relies on customized
data-processing workflows that require fluorescence background removal and
prior identification of Raman peaks of interest-introducing potential biases
and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation
of Vibrational Raman spectra for plant-stress Analysis), a fully automated
workflow based on a variational autoencoder. Unlike conventional approaches,
DIVA processes native Raman spectra-including fluorescence backgrounds-without
manual preprocessing, identifying and quantifying significant spectral features
in an unbiased manner. We applied DIVA to detect a range of plant stresses,
including abiotic (shading, high light intensity, high temperature) and biotic
stressors (bacterial infections). By integrating deep learning with vibrational
spectroscopy, DIVA paves the way for AI-driven plant health assessment,
fostering more resilient and sustainable agricultural practices.

</details>


### [234] [Dynamics is what you need for time-series forecasting!](https://arxiv.org/abs/2507.15774)
*Alexis-Raja Brachet,Pierre-Yves Richard,Céline Hudelot*

Main category: cs.LG

TL;DR: 现有深度模型在时间序列预测任务中表现不佳，提出用系统和实证研究验证假设，开发PRO - DYN命名法分析模型，实验支持引入可学习动态块并作为最终预测器。


<details>
  <summary>Details</summary>
Motivation: 当前数据模态边界消失，但常用深度模型在时间序列预测任务中受简单模型挑战，需能学习数据潜在动态的模型。

Method: 进行系统和实证研究，开发PRO - DYN命名法分析现有模型，对不同骨干的模型进行大量实验。

Result: 表现不佳的架构最多只能部分学习动态，动态块在模型末尾的位置至关重要，实验结果支持引入可学习动态块并作为最终预测器。

Conclusion: 时间序列预测任务中模型需引入可学习的动态块并将其作为最终预测器。

Abstract: While boundaries between data modalities are vanishing, the usual successful
deep models are still challenged by simple ones in the time-series forecasting
task. Our hypothesis is that this task needs models that are able to learn the
data underlying dynamics. We propose to validate it through both systemic and
empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to
analyze existing models through the lens of dynamics. Two observations thus
emerged: $\textbf{1}$. under-performing architectures learn dynamics at most
partially, $\textbf{2}$. the location of the dynamics block at the model end is
of prime importance. We conduct extensive experiments to confirm our
observations on a set of performance-varying models with diverse backbones.
Results support the need to incorporate a learnable dynamics block and its use
as the final predictor.

</details>


### [235] [Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets](https://arxiv.org/abs/2507.15784)
*Zihang Ma,Qitian Yin*

Main category: cs.LG

TL;DR: 针对图节点分类中类别分类难度差异问题，提出WR - EFM模型，实验表明该模型能实现类别间平衡准确分类，稳定性高，为处理类别不平衡图分类任务提供新范式。


<details>
  <summary>Details</summary>
Motivation: 解决PubMed引文网络数据集上不同类别图节点分类难度差异大的问题，提升分类性能。

Method: 提出Wasserstein - Rubinstein (WR)距离增强的专家融合模型(WR - EFM)，为不同类别训练专门的GNN模型，用WR距离优化模型表示相似性，采用自适应融合策略动态加权模型。

Result: WR - EFM在各类别上实现平衡准确率，分别为77.8%（类别0）、78.0%（类别1）和79.9%（类别2），变异系数低，稳定性好，类别2准确率比GCN提高5.5%。

Conclusion: WR - EFM有效，为处理类别不平衡图分类任务提供新范式，项目已开源。

Abstract: Graph node classification is a fundamental task in graph neural networks
(GNNs), aiming to assign predefined class labels to nodes. On the PubMed
citation network dataset, we observe significant classification difficulty
disparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,
7.5% lower than Category 1. To address this, we propose a
Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),
training specialized GNN models for Categories 0/1 (with layer normalization
and residual connections) and Multi-hop Graph Attention Networks (GAT) for
Category 2. The WR distance metric optimizes representation similarity between
models, particularly focusing on improving Category 2 performance. Our adaptive
fusion strategy dynamically weights models based on category-specific
performance, with Category 2 assigned a GAT weight of 0.8. WR distance further
guides the fusion process by measuring distributional differences between model
representations, enabling more principled integration of complementary
features.
  Experimental results show WR-EFM achieves balanced accuracy across
categories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),
outperforming both single models and standard fusion approaches. The
coefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%
lower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM
improves Category 2 accuracy by 5.5% compared to GCN, verifying the
effectiveness of WR-guided fusion in capturing complex structural patterns.
This work provides a novel paradigm for handling class-imbalanced graph
classification tasks. To promote the research community, we release our project
at https://github.com/s010m00n/GASEM4NC.

</details>


### [236] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: 研究小规模大语言模型能否通过带可验证奖励的强化学习获得心智理论能力，发现小模型难以发展通用能力，易过拟合。


<details>
  <summary>Details</summary>
Motivation: 探讨类似基于规则的强化学习技术能否让大语言模型拥有更细致、类人的社会智能，如心智理论能力。

Method: 在多个知名心智理论数据集的不同组合上训练模型，并在保留数据集上测试泛化能力。

Result: 小模型难以发展通用心智理论能力，在分布内任务表现提升但无法迁移到特征不同的未见任务；长时间训练会导致模型利用训练数据统计模式，分布内数据表现提升但分布外任务无变化或性能下降。

Conclusion: 模型学到的行为是狭义过拟合，而非获得真正、抽象的心智理论能力。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [237] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: 本文提出通信与存储高效的联邦分割学习方法CSE - FSL，可减少通信与存储开销，理论证明收敛性，实验显示优于现有FSL方案。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习和联邦分割学习存在通信、计算和存储成本高的问题，需提出改进方法。

Method: 提出CSE - FSL方法，利用辅助网络本地更新客户端权重，服务器保留单一模型，在选定时期传输粉碎数据以更新模型。

Result: 理论上保证了CSE - FSL在非凸损失函数下的收敛性，实验表明在真实世界FL任务中显著减少通信开销。

Conclusion: CSE - FSL能有效减少通信和存储开销，优于现有的FSL解决方案。

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [238] [Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction](https://arxiv.org/abs/2507.15832)
*Shiyang Li*

Main category: cs.LG

TL;DR: 本文提出混合CNN - LSTM - attention - adaboost神经网络模型结合改进蛇群优化算法用于4D轨迹预测，实验表明其性能优于传统优化器，改进算法提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 解决中长期4D轨迹预测模型的局限性。

Method: 提出混合CNN - LSTM - attention - adaboost神经网络模型，用Adaboost划分弱学习器，子模型用CNN、LSTM和注意力机制提取特征，结合SO算法优化超参数。

Result: SO - CLA - adaboost在处理大规模高维轨迹数据上优于粒子群、鲸鱼和灰狼等传统优化器，引入改进SO算法使模型预测精度提升39.89%。

Conclusion: 所提出的模型和改进算法在4D轨迹预测中表现良好，能有效提升预测精度。

Abstract: To address the limitations of medium- and long-term four-dimensional (4D)
trajectory prediction models, this paper proposes a hybrid
CNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy
improved snake-herd optimization (SO) algorithm. The model applies the Adaboost
algorithm to divide multiple weak learners, and each submodel utilizes CNN to
extract spatial features, LSTM to capture temporal features, and attention
mechanism to capture global features comprehensively. The strong learner model,
combined with multiple sub-models, then optimizes the hyperparameters of the
prediction model through the natural selection behavior pattern simulated by
SO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the
comparison experiments and ablation studies of multiple optimizers are carried
out, and a comprehensive test and evaluation analysis is carried out. The
results show that SO-CLA-adaboost outperforms traditional optimizers such as
particle swarm, whale, and gray wolf in handling large-scale high-dimensional
trajectory data. In addition, introducing the full-strategy collaborative
improvement SO algorithm improves the model's prediction accuracy by 39.89%.

</details>


### [239] [Optimizing Canaries for Privacy Auditing with Metagradient Descent](https://arxiv.org/abs/2507.15836)
*Matteo Boglioni,Terrance Liu,Andrew Ilyas,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 本文研究黑盒隐私审计，提出利用元梯度优化来优化审计者的金丝雀集以改善隐私审计的方法，能使差分隐私图像分类模型的经验下界提升超2倍，且方法具有可迁移性和高效性。


<details>
  <summary>Details</summary>
Motivation: 降低差分隐私学习算法隐私参数的下界，改进现有基于成员推理的审计方法。

Method: 利用元梯度优化来优化审计者的金丝雀集。

Result: 在某些情况下可使差分隐私图像分类模型的经验下界提升超2倍，且优化后的金丝雀集对不同模型审计有效。

Conclusion: 提出的优化金丝雀集的方法能有效改善隐私审计，具有可迁移性和高效性。

Abstract: In this work we study black-box privacy auditing, where the goal is to lower
bound the privacy parameter of a differentially private learning algorithm
using only the algorithm's outputs (i.e., final trained model). For DP-SGD (the
most successful method for training differentially private deep learning
models), the canonical approach auditing uses membership inference-an auditor
comes with a small set of special "canary" examples, inserts a random subset of
them into the training set, and then tries to discern which of their canaries
were included in the training set (typically via a membership inference
attack). The auditor's success rate then provides a lower bound on the privacy
parameters of the learning algorithm. Our main contribution is a method for
optimizing the auditor's canary set to improve privacy auditing, leveraging
recent work on metagradient optimization. Our empirical evaluation demonstrates
that by using such optimized canaries, we can improve empirical lower bounds
for differentially private image classification models by over 2x in certain
instances. Furthermore, we demonstrate that our method is transferable and
efficient: canaries optimized for non-private SGD with a small model
architecture remain effective when auditing larger models trained with DP-SGD.

</details>


### [240] [FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs](https://arxiv.org/abs/2507.15839)
*Anh Nguyen,Sam Schafft,Nicholas Hale,John Alfaro*

Main category: cs.LG

TL;DR: 提出利用大语言模型生成表格数据的低成本方法，实验显示优于传统方法，计划用于加速生产测试。


<details>
  <summary>Details</summary>
Motivation: 现有直接用大语言模型生成每条数据的方法在大量数据生成时时间和成本过高。

Method: 利用大语言模型将每个字段的分布推断并编码成可复用的采样脚本，自动分类字段类型，生成基于分布的脚本。

Result: 该方法在多样性和数据真实性上优于传统直接方法，大幅降低大量合成数据生成的负担。

Conclusion: 该方法可应用于加速生产管道测试，缩短开发周期，提高系统效率，为合成数据生成提供可扩展、低成本解决方案。

Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios
where real-world data collection and usage are limited by cost and scarcity.
Large language models (LLMs) have demonstrated remarkable capabilities in
producing high-fidelity, domain-relevant samples across various fields.
However, existing approaches that directly use LLMs to generate each record
individually impose prohibitive time and cost burdens, particularly when large
volumes of synthetic data are required. In this work, we propose a fast,
cost-effective method for realistic tabular data synthesis that leverages LLMs
to infer and encode each field's distribution into a reusable sampling script.
By automatically classifying fields into numerical, categorical, or free-text
types, the LLM generates distribution-based scripts that can efficiently
produce diverse, realistic datasets at scale without continuous model
inference. Experimental results show that our approach outperforms traditional
direct methods in both diversity and data realism, substantially reducing the
burden of high-volume synthetic data generation. We plan to apply this
methodology to accelerate testing in production pipelines, thereby shortening
development cycles and improving overall system efficiency. We believe our
insights and lessons learned will aid researchers and practitioners seeking
scalable, cost-effective solutions for synthetic data generation.

</details>


### [241] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 提出GUI - G²奖励框架，将GUI接地从稀疏二元分类转变为密集连续优化，实验显示其性能优于SOTA方法，为GUI交互任务空间推理建立新范式。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法使用二元奖励，忽略空间交互连续性，受人类点击行为启发提出新框架。

Method: 引入GUI - G²奖励框架，包含高斯点奖励和覆盖奖励机制，还有自适应方差机制处理不同元素尺度。

Result: 在多个基准测试中，GUI - G²大幅优于SOTA方法UI - TARS - 72B，在ScreenSpot - Pro上提升24.7%。

Conclusion: 连续建模对界面变化有更好鲁棒性，对未见布局有更强泛化性，为GUI交互任务空间推理建立新范式。

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [242] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: 研究受限数据下掩码扩散模型，发现其在数据少计算多场景优于自回归模型，还找到新缩放定律和临界计算阈值。


<details>
  <summary>Details</summary>
Motivation: 探索扩散语言模型相对自回归模型的优势。

Method: 系统研究受限数据下的掩码扩散模型，对比其与自回归模型表现。

Result: 受限数据下，计算资源丰富时扩散模型优于自回归模型，找到新缩放定律和临界计算阈值。

Conclusion: 数据受限而非计算受限情况下，扩散模型是自回归范式的有力替代。

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [243] [APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation](https://arxiv.org/abs/2507.14270)
*Ravin Kumar*

Main category: cs.NE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that
integrates non-linear activation and linear transformation into a single
trainable expression. The APTx Neuron is derived from the APTx activation
function, thereby eliminating the need for separate activation layers and
making the architecture both computationally efficient and elegant. The
proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i +
\tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters
$\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our
APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69\%
test accuracy in just 20 epochs using approximately 332K trainable parameters.
The results highlight the superior expressiveness and computational efficiency
of the APTx Neuron compared to traditional neurons, pointing toward a new
paradigm in unified neuron design and the architectures built upon it.

</details>


### [244] [Training oscillator Ising machines to assign the dynamic stability of their equilibrium points](https://arxiv.org/abs/2507.14386)
*Yi Cheng,Zongli Lin*

Main category: cs.NE

TL;DR: 提出用合适分配平衡点稳定性的神经网络模型实现类Hopfield联想记忆，以OIM为例，建立平衡点稳定性与哈密顿能量联系，提出HRECM方法训练耦合权重并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统Hopfield模型存储模式时设计耦合权重需兼顾平衡点存在性和动态稳定性，而OIM的0/π二进制平衡点结构稳定，需设计方法为其分配合适稳定性。

Method: 建立OIM平衡点稳定性与哈密顿能量的联系，提出Hamiltonian - Regularized Eigenvalue Contrastive Method (HRECM)训练耦合权重。

Result: 通过数值实验验证了所提方法的有效性。

Conclusion: 所提出的HRECM方法能为OIM的平衡点分配合适稳定性，可用于实现类Hopfield联想记忆。

Abstract: We propose a neural network model, which, with appropriate assignment of the
stability of its equilibrium points (EPs), achieves Hopfield-like associative
memory. The oscillator Ising machine (OIM) is an ideal candidates for such a
model, as all its $0/\pi$ binary EPs are structurally stable with their dynamic
stability tunable by the coupling weights. Traditional Hopfield-based models
store the desired patterns by designing the coupling weights between neurons.
The design of coupling weights should simultaneously take into account both the
existence and the dynamic stability of the EPs for the storage of the desired
patterns. For OIMs, since all $0/\pi$ binary EPs are structurally stable, the
design of the coupling weights needs only to focus on assigning appropriate
stability for the $0/\pi$ binary EPs according to the desired patterns. In this
paper, we establish a connection between the stability and the Hamiltonian
energy of EPs for OIMs, and, based on this connection, provide a
Hamiltonian-Regularized Eigenvalue Contrastive Method (HRECM) to train the
coupling weights of OIMs for assigning appropriate stability to their EPs.
Finally, numerical experiments are performed to validate the effectiveness of
the proposed method.

</details>


### [245] [Analyzing Internal Activity and Robustness of SNNs Across Neuron Parameter Space](https://arxiv.org/abs/2507.14757)
*Szymon Mazurek,Jakub Caputa,Maciej Wielgosz*

Main category: cs.NE

TL;DR: 本文研究脉冲神经网络（SNNs）的神经元超参数操作空间，确定高效运行点，为部署鲁棒高效的SNNs提供实用指南。


<details>
  <summary>Details</summary>
Motivation: SNNs性能关键依赖于神经元模型参数调整，需确定合适的参数操作空间以平衡分类精度和脉冲活动。

Method: 通过跨数据集和架构的系统探索，可视化并量化操作空间，评估对抗噪声的鲁棒性。

Result: 确定了操作空间，发现空间内外的不同表现，如外部会增加脉冲相关性和内部同步性。

Conclusion: 强调了原则性超参数调整对保证任务性能和能源效率的重要性，为SNNs部署提供实用指南。

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient and biologically
plausible alternatives to traditional artificial neural networks, but their
performance depends critically on the tuning of neuron model parameters. In
this work, we identify and characterize an operational space - a constrained
region in the neuron hyperparameter domain (specifically membrane time constant
tau and voltage threshold vth) - within which the network exhibits meaningful
activity and functional behavior. Operating inside this manifold yields optimal
trade-offs between classification accuracy and spiking activity, while stepping
outside leads to degeneration: either excessive energy use or complete network
silence.
  Through systematic exploration across datasets and architectures, we
visualize and quantify this manifold and identify efficient operating points.
We further assess robustness to adversarial noise, showing that SNNs exhibit
increased spike correlation and internal synchrony when operating outside their
optimal region. These findings highlight the importance of principled
hyperparameter tuning to ensure both task performance and energy efficiency.
Our results offer practical guidelines for deploying robust and efficient SNNs,
particularly in neuromorphic computing scenarios.

</details>


### [246] [DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP Solving](https://arxiv.org/abs/2507.15615)
*Zhihao Zhang,Siyuan Li,Chenxi Li,Feifan Liu,Mengjing Chen,Kai Li,Tao Zhong,Bo An,Peng Liu*

Main category: cs.NE

TL;DR: 本文提出数据 - 算法协同进化框架DHEvo生成MILP原启发式，实验表明其性能优于人工设计和现有基于LLM的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的MILP原启发式生成方法在同一问题类中泛化性差，未捕捉实例特征。

Method: 提出DHEvo框架，用基于LLM的多智能体系统生成数据 - 代码对并迭代优化。

Result: 在多个MILP基准测试中，DHEvo显著优于人工设计和现有基于LLM的方法。

Conclusion: DHEvo框架能有效提升MILP求解原启发式的性能和泛化性。

Abstract: Primal heuristics play a critical role in improving the efficiency of mixed
integer programming (MILP) solvers. As large language models (LLMs) have
demonstrated superior code generation abilities, recent MILP works are devoted
to leveraging the evolutionary computation approaches with LLMs to generate
effective primal heuristics. Although the generated heuristics have achieved
better solving performance than the hand-crafted ones with little adaptability,
the advantage of current LLM-based methods is limited to few MILP instances in
one problem class, as they fail to capture the instance characteristics in the
problem class (the MILP instances generated from the same mathematical model
are defined as a problem class). Since MILP instances often differ
significantly in structure and feature distribution, the neglect of their
characteristics in the evolution process results in poor generalization within
the same problem class. To overcome this challenge, we propose a data-algorithm
co-evolution framework (DHEvo) that iteratively selects representative
instances and evolves corresponding heuristics. With the initial instance
distribution, we develop an LLM-based multi-agent system to generate data-code
pairs simultaneously. These data-code pairs are iteratively refined based on
their fitness scores, leading to the identification of the most effective
heuristic over the entire problem class. Extensive experiments across diverse
MILP benchmarks demonstrate that our approach significantly outperforms both
human-designed heuristics and existing LLM-based methods.

</details>


### [247] [TONUS: Neuromorphic human pose estimation for artistic sound co-creation](https://arxiv.org/abs/2507.15734)
*Jules Lecomte,Konrad Zinner,Michael Neumeier,Axel von Arnim*

Main category: cs.NE

TL;DR: 本文聚焦人机交互在艺术领域问题，提出含神经形态人体感知的艺术声音装置方案。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互偏技术驱动，艺术领域新技术未充分挖掘，需让机器支持人的想象与诗意。

Method: 设计神经形态多头人体姿态估计神经传感器，用针对专用神经形态芯片的脉冲神经网络作特征提取器。

Result: 访客能沉浸在声音氛围和自我神经处理表征中，与类神经思维的机器对话。

Conclusion: 所提艺术声音装置可实现与访客直接且非侵入式交互，共同创造声景。

Abstract: Human machine interaction is a huge source of inspiration in today's media
art and digital design, as machines and humans merge together more and more.
Its place in art reflects its growing applications in industry, such as
robotics. However, those interactions often remains too technical and
machine-driven for people to really engage into. On the artistic side, new
technologies are often not explored in their full potential and lag a bit
behind, so that state-of-the-art research does not make its way up to museums
and exhibitions. Machines should support people's imagination and poetry in a
seamless interface to their body or soul. We propose an artistic sound
installation featuring neuromorphic body sensing to support a direct yet non
intrusive interaction with the visitor with the purpose of creating sound
scapes together with the machine. We design a neuromorphic multihead human pose
estimation neural sensor that shapes sound scapes and visual output with fine
body movement control. In particular, the feature extractor is a spiking neural
network tailored for a dedicated neuromorphic chip. The visitor, immersed in a
sound atmosphere and a neurally processed representation of themselves that
they control, experience the dialogue with a machine that thinks neurally,
similarly to them.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [248] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: 研究不同大语言模型生成单元测试时代码上下文和提示策略的影响，结果显示包含文档字符串、特定提示策略及M5模型表现佳，代码和测试套件开源。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件工程测试中受关注，自动生成单元测试可提高软件开发阶段生产力，因此研究代码上下文和提示策略对单元测试生成质量和充分性的影响。

Method: 研究不同家族的大语言模型在不同代码上下文和提示策略下生成单元测试的情况。

Result: 包含文档字符串可显著提高代码充分性，扩展到完整实现增益较小；思维链提示策略效果最佳，M5（Gemini 2.5 Pro）在突变分数、分支覆盖率和编译成功率方面表现出色。

Conclusion: 代码上下文和提示策略对单元测试生成质量和充分性有影响，思维链提示策略和M5模型在单元测试生成中表现优越。

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [249] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本文聚焦从非正式需求生成可验证规范的挑战，介绍VERIFAI项目用多种技术填补差距，并做相关文献初步综合。


<details>
  <summary>Details</summary>
Motivation: 从非正式且模糊的自然语言需求生成规范是确保软件正确性的关键挑战，推动了研究。

Method: 使用自然语言处理、基于本体的领域建模、工件重用和大语言模型等技术，对相关文献进行初步综合。

Result: 未提及明确结果。

Conclusion: 确定了从非正式需求生成可验证规范中的反复出现的挑战和潜在研究方向。

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [250] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: 研究软件协作中沟通问题，采用DSR框架，发现共享词汇系统可提升沟通效率。


<details>
  <summary>Details</summary>
Motivation: 解决软件协作中沟通差距导致的误解、低效和缺陷问题。

Method: 采用设计科学研究（DSR）框架，分问题识别、方法开发和实证验证三阶段，用主题分析、半结构化访谈和控制实验等。

Result: 共享词汇系统虽初期有成本，但能提升信息密度、文档清晰度和协作效率。

Conclusion: 研究为改进软件工程沟通实践提供见解，指出局限和未来研究方向。

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [251] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 研究合并代码语言模型中属于同一语义单元的子标记的隐藏表示的效果，提出两种策略，在六个代码语言模型和三个软件工程任务上实验，能减少浮点运算次数，不同任务性能有不同变化。


<details>
  <summary>Details</summary>
Motivation: 代码分词器输出常长于传统编译器和解释器，会增加计算开销，需研究合并子标记隐藏表示的效果。

Method: 提出基于平均表示和基于学习的两种策略，并与现有代码语言模型集成，在六个代码语言模型和三个软件工程任务上实验。

Result: 策略能减少1% - 19%的浮点运算次数，漏洞检测任务F1分数下降1.82点，代码翻译任务CodeBLEU提高2.47点。

Conclusion: 该工作有助于从计算效率和下游性能等多维度改进代码语言模型。

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [252] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 本文通过多视角文献综述统一对架构退化的理解，发现其从低层次问题转变为社会技术问题，确定多种指标和测量技术，但工具多只检测问题，缺乏持续修复，呼吁整体、主动策略。


<details>
  <summary>Details</summary>
Motivation: 当前文献对架构退化的定义、指标和修复策略零散，旨在统一对架构退化的理解。

Method: 对108项研究进行多视角文献综述，开发包含架构、代码和过程债务的分类法。

Result: 架构退化从低层次问题转变为社会技术问题，确定54个指标和31种测量技术，多数工具仅检测问题。

Conclusion: 架构退化是技术和组织问题，检测研究充分但持续修复不足，需整体、主动策略以实现可持续架构。

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [253] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 本文分析五年内八大行业会议软件架构趋势，发现Kubernetes等少数核心技术主导当代软件架构实践，多应用于DevOps后期阶段。


<details>
  <summary>Details</summary>
Motivation: 随着云计算等发展，架构实践多样化，需理解这些转变。

Method: 分析八大行业会议的5677场演讲，用大语言模型和专家验证提取技术、目的和使用场景，探索技术关联。

Result: Kubernetes等技术占主导，确定五个技术社区，多数技术跨DevOps阶段支持混合部署。

Conclusion: 少数核心技术主导当代软件架构实践，多应用于DevOps后期，研究能为架构设计等提供更全面视角。

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [254] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: 本文介绍了利用大语言模型对OpenCV库进行文档引导模糊测试的新技术VISTAFUZZ，经测试发现17个新漏洞。


<details>
  <summary>Details</summary>
Motivation: OpenCV库中的漏洞会影响下游计算机视觉应用，需确保其可靠性。

Method: VISTAFUZZ利用大语言模型解析API文档获取标准化信息，提取输入参数约束和依赖关系，生成新输入值测试API。

Result: 对OpenCV库330个API测试，检测到17个新漏洞，10个已确认，5个已修复。

Conclusion: VISTAFUZZ能有效检测OpenCV库中的漏洞。

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [255] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 本文对PyPI生态系统中的开源许可证变体进行实证研究，发现文本变化常见但实质修改少，变体引发合规问题。并提出LV - Parser和LV - Compat工具，评估显示工具效果好。


<details>
  <summary>Details</summary>
Motivation: 现有工具未考虑开源许可证变体，导致许可证分析在有效性和效率上存在挑战，需填补对许可证变体的认知空白。

Method: 对PyPI生态系统中的许可证变体进行全面实证研究，提出LV - Parser利用基于差异的技术和大语言模型进行高效许可证变体分析，提出LV - Compat自动检测软件依赖网络中的许可证不兼容性。

Result: 许可证文本变化常见但仅2%有实质修改，10.7%的下游依赖存在许可证不兼容；LV - Parser准确率达0.936且降低30%计算成本，LV - Compat识别出的不兼容包是现有方法的5.2倍，精度为0.98。

Conclusion: 本研究是软件打包生态系统中对许可证变体的首次实证研究，为开发者和组织提供了应对开源许可复杂情况的实用工具。

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [256] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: 本文提出Robin's Rule算法，为奇异布尔表达式（SBEs）构建最小测试集以实现100%唯一原因MC/DC覆盖，经TCAS - II规范验证，比商业工具更高效。


<details>
  <summary>Details</summary>
Motivation: Unique - Cause MC/DC虽能提供高保证，但对其高效测试生成的研究不足，且大规模航空电子系统中多数条件决策为SBEs，适合应用Unique - Cause MC/DC。

Method: 提出Robin's Rule确定性算法，直接构建SBEs的最小测试集，不生成完整真值表；将TCAS - II规范重构为SBEs构建基准，用行业标准认证商业工具验证结果。

Result: 该方法能始终以理论最少测试次数达到100%覆盖，且比商业工具更高效。

Conclusion: 该研究为验证安全关键系统提供了实用且最优的解决方案，兼顾严谨性和效率。

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [257] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 本文构建新的评估基准，提出新工具HistoryFinder，经评估其在准确性和运行时间上表现良好，并提供多种使用方式。


<details>
  <summary>Details</summary>
Motivation: 现有方法历史生成工具有效性评估受限于不准确的基准测试预言机，需要准确高效重建方法变更历史。

Method: 结合自动化分析与专家指导的手动验证构建两个新预言机，开发新工具HistoryFinder。

Result: 经对40个开源仓库的400个方法评估，HistoryFinder在精确率、召回率和F1分数上始终优于其他工具，运行时间有竞争力。

Conclusion: 当准确性和效率都重要时，HistoryFinder是最佳选择，还提供多种使用方式方便采用。

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [258] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: 本文探讨超参数调优和提示工程如何提升Llama 3.1模型从文本描述生成领域模型的准确性，虽方案并非通用，但结合两者可提升多数领域模型效果。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型用于领域建模有局限性，微调模型需大量计算资源且有灾难性遗忘问题，因此探索提升模型准确性的方法。

Method: 使用基于搜索的方法为特定医疗数据模型调优超参数，并在十个不同应用领域测试优化后的超参数。

Result: 针对特定医疗数据模型调优超参数后，相比基线大语言模型有显著质量提升，方案并非在所有领域都适用。

Conclusion: 将超参数调优与提示工程相结合可提升几乎所有被考察领域模型的结果。

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [259] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: 研究旨在探究开发者与代码生成工具（CGTs）的交互是否因性别而异，采用混合实验设计，虽结果未出，但为CGT设计公平性等方面奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着对CGTs依赖增加，其在不同用户群体中的有效性未充分研究，且先前研究表明性别差异会影响技术使用和认知处理，因此假设开发者与CGTs的交互因性别而异。

Method: 采用混合实验设计，54名参与者按性别均分，完成两个编程任务，分别仅使用CGT辅助和仅使用互联网访问，收集认知负荷调查、屏幕记录和任务绩效指标等数据并进行统计分析。

Result: 结果尚未得出。

Conclusion: 研究为CGT设计的公平性、问责制、透明度和道德性奠定基础，有望促进包容性AI实践和公平工具开发。

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [260] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: 提出VeriOpt框架使大语言模型生成高质量、可综合的Verilog代码，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在硬件设计中主要关注功能正确的Verilog代码生成，忽略了关键的PPA指标，需弥补这一差距。

Method: 提出VeriOpt框架，利用基于角色的提示和PPA感知优化，将LLM交互结构化，并将PPA约束集成到提示管道中，结合多模态反馈。

Result: 相比基线LLM生成的RTL，功率降低88%，面积减少76%，时序收敛改善73%，功能评估成功率达86%。

Conclusion: 工作解决了正确性和质量之间的关键差距，推动了AI驱动的硬件设计发展，为LLM在生产流程中的可靠应用铺平道路。

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [261] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: 提出RepoScope用于仓库级代码生成，利用调用链感知的多视图上下文，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有仓库级代码生成方法难以有效识别相关上下文，且未考虑代码结构关系。

Method: 构建Repository Structural Semantic Graph (RSSG)，检索四视图上下文，提出调用链预测方法和结构保留序列化算法。

Result: 在CoderEval和DevEval基准测试中，RepoScope的pass@1分数相对提升达36.35%。

Conclusion: RepoScope优于现有方法，能提升不同任务的代码生成效果，可与现有方法有效集成。

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [262] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: 本文提出RequireCEG，通过嵌入因果效应图（CEGs）来处理最终用户在生成式软件开发中需求描述模糊的问题，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 最终用户缺乏软件工程知识，需求描述模糊，现有方法难以表达前置条件和行为动作之间的因果逻辑，影响生成式软件开发。

Method: 引入RequireCEG，先使用特征树分析用户叙述，构建自修复CEGs，再用CEGs审查和优化Gherkin场景。

Result: 创建RGPair基准数据集并实验，覆盖率达87%，多样性提高51.88%。

Conclusion: RequireCEG能有效处理最终用户需求描述模糊问题，确保生成的Gherkin需求与系统行为需求一致。

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [263] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文介绍了首个大规模数据集AIDev，用于研究软件开发中的自主编码代理，该数据集提供了丰富信息，揭示了代理与人类开发者的差异，可促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 随着AI队友在软件工程中的兴起，缺乏研究自主编码代理实际运行情况的大规模数据集，需要填补这一空白以推动AI原生软件工程研究。

Method: 收集五个领先代理在超61,000个仓库、47,000名开发者中的超456,000个拉取请求，构建AIDev数据集。

Result: 发现代理在速度上常优于人类，但拉取请求接受率较低，代理提交代码速度快但结构更简单。

Conclusion: AIDev是可扩展、可分析的资源，能为SE和AI社区提供支持，推动AI原生工作流研究和人机协作发展。

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [264] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 本文探讨生成式人工智能（GenAI）在汽车软件开发各步骤的应用，涉及相关技术、开发流程及行业调查结果。


<details>
  <summary>Details</summary>
Motivation: GenAI可减少人工干预和处理复杂流程的工作量，汽车软件开发流程长且成本高，有应用GenAI的需求。

Method: 探索GenAI在汽车软件开发多步骤的应用，涵盖三种相关技术和代码生成提示技术，基于文献综述得出开发工作流程，进行行业伙伴调查。

Result: 得出通用的GenAI辅助汽车软件开发工作流程，总结了行业伙伴使用GenAI工具的调查结果。

Conclusion: GenAI在汽车软件开发中有应用价值，可助力相关工作。

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [265] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: 本文探讨大语言模型（LLMs）在敏捷框架下自动需求获取的应用，用10种LLMs生成用户故事（US），评估其质量并与人类生成的对比，发现LLMs生成US在覆盖度和风格上与人类相似，但多样性和创造性较低，且通过评估语义质量发现它可减少人力。


<details>
  <summary>Details</summary>
Motivation: 需求获取困难且评估语义指标耗时，探索LLMs在敏捷框架下自动需求获取的可能性。

Method: 使用10种最先进的LLMs模拟客户访谈自动生成US，评估其质量并与人类生成的US对比，探索LLMs自动评估US语义质量的方式。

Result: LLMs生成的US在覆盖度和风格上与人类相似，但多样性和创造性较低，达到接受质量标准的频率较低，且能可靠评估语义质量。

Conclusion: LLMs有潜力减少大规模评估中的人力，可用于自动需求获取和语义质量评估。

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [266] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: 现有深度学习框架测试方法有局限，提出DLMMM方法解决问题


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架测试方法以启发式指标衡量模型漏洞检测效果，存在无法定量衡量算子组合多样性、忽略模型执行时间、忽视不同测量指标相关性等局限

Method: 提出DLMMM方法，先定量衡量模型漏洞检测性能、算子组合多样性和模型执行时间，再根据相关性融合这些测量指标以实现权衡，还设计多级启发式指导生成测试输入模型

Result: 未提及

Conclusion: 未提及

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [267] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 研究孟加拉国文化背景下需求工程（RE）流程的采用情况及文化影响。


<details>
  <summary>Details</summary>
Motivation: RE活动受利益相关者国家文化影响大，当前软件开发项目利益相关者多元，且孟加拉国在该研究领域被忽视，需研究其文化对RE活动的影响。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [268] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 本文通过系统映射研究探索2023年4月至2025年4月需求工程中人物角色相关研究，分析其表示、构建、验证等，发现AI应用、模板式人物角色及验证研究增多。


<details>
  <summary>Details</summary>
Motivation: 探索需求工程中人物角色相关最新研究及生成式AI方法带来的趋势变化。

Method: 进行系统映射研究，分析2023年4月至2025年4月间22篇相关出版物。

Result: 许多研究将基于AI的解决方案用于人物角色构建和验证，模板式人物角色更受欢迎，涉及验证方面的研究比例增加。

Conclusion: 

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [269] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: 提出首个针对SIMD指令集代码生成的基准测试SimdBench，对18个大语言模型进行评估，发现大模型在SIMD代码生成时表现不佳，并指出改进方向，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试仅关注标量代码，不清楚大语言模型在生成SIMD向量化代码的表现，为填补该空白开展研究。

Method: 提出包含136个任务、针对5种代表性SIMD指令集的SimdBench基准测试，对18个大语言模型进行正确性和性能评估。

Result: 大语言模型在SIMD代码生成时pass@k相比标量代码生成普遍下降。

Conclusion: 为大语言模型在SIMD代码生成领域的进一步发展指出了有前景的方向，SimdBench开源造福社区。

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [270] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: 本文受AlphaFold启发提出AlphaCC用于代码克隆检测，在多语言数据集上验证其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测方法在捕获代码语义或语言通用性上存在不足，受AlphaFold成功启发开展研究。

Method: 提出AlphaCC，将代码片段表示为令牌序列，构建MSA，采用改进的基于注意力的编码器，通过后期交互策略计算相似度并分类。

Result: 在三个多语言数据集上证明了AlphaCC跨语言的适用性，在两个语义克隆检测数据集上超越所有基线，且效率有竞争力。

Conclusion: AlphaCC适用于大规模代码克隆检测任务，有较强语义理解能力和实用价值。

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [271] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: 论文提出FaultLine工作流自动生成PoV测试用例，在多语言数据集上比CodeAct 2.1表现好，表明分层推理可提升LLM代理性能，但问题仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 软件安全漏洞报告常缺少PoV测试，生成PoV测试有挑战，需要对程序控制和数据流进行推理。

Method: 提出FaultLine工作流，通过追踪输入流、推理输入条件、反馈驱动循环生成PoV测试用例，不使用特定语言分析组件。

Result: 在多语言数据集上，FaultLine为16个项目生成PoV测试，比CodeAct 2.1多，有77%的相对提升。

Conclusion: 分层推理可提升LLM代理在PoV测试生成上的性能，但问题总体仍具挑战，公开代码和数据集以促进研究。

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [272] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: 论文提出ReduceFix方法解决LLM在APR中因长测试输入导致的问题，在LFTBench基准上效果显著，证明自动减少输入能提升APR性能。


<details>
  <summary>Details</summary>
Motivation: LLM在基于APR时因长提示中难以保留关键信息，存在“lost - in - the - middle”问题，影响修复性能。

Method: 提出ReduceFix方法，内置组件自动减少测试输入并保留故障诱导行为，利用LLM生成缩减器，用缩减后的输入指导补丁生成；构建LFTBench基准进行评估。

Result: ReduceFix平均缩小输入89.1%，相对原始测试提示提高pass@10达53.8%，比完全省略测试提高17.6%；给ChatRepair添加相同缩减步骤，修复率提高21.3%。

Conclusion: 自动减少失败输入是基于LLM的APR实用且强大的补充，显著提升其可扩展性和有效性。

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [273] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: 本文探讨工具代理范式执行中参数失败问题，构建分类体系，实验分析成因并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 工具代理范式执行中存在参数失败问题，限制了其有效性，需探索该现象并提出建议。

Method: 构建参数失败分类体系，从主流工具代理调用链得出五类失败，对输入应用15种扰动方法探索不同输入源与失败类别的关联。

Result: 参数名称幻觉失败主要源于大语言模型固有局限，其他失败模式主要由输入源问题导致。

Conclusion: 为提高工具代理交互的可靠性和有效性，提出规范工具返回格式、改进错误反馈机制和确保参数一致性等改进建议。

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [274] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: 论文提出 StackTrans 模型，在 Transformer 层间加入隐藏状态栈，解决其无法有效捕获乔姆斯基层级的问题，评估显示性能优于标准 Transformer 及其他基线模型。


<details>
  <summary>Details</summary>
Motivation: Transformer 架构虽推动大语言模型发展，但存在无法有效捕获乔姆斯基层级的局限。

Method: 受下推自动机启发，提出 StackTrans，在 Transformer 层间显式加入隐藏状态栈，设计可微分的栈操作并进行端到端学习。

Result: 在乔姆斯基层级和大规模自然语言基准测试中，StackTrans 表现优于标准 Transformer 模型和其他基线模型，成功将参数从 360M 扩展到 7B，StackTrans - 360M 优于参数多 2 - 3 倍的开源大语言模型。

Conclusion: StackTrans 具有优越的效率和推理能力。

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [275] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: 提出“Chinese Wall”技术提升代码大模型性能，评估显示有显著提升，但实际应用受无版权限制模型缺乏影响。


<details>
  <summary>Details</summary>
Motivation: 代码大模型训练数据集未公开引发版权担忧，部分注重数据管理和许可的模型因数据有限缺乏竞争力，需提升其实用性。

Method: 应用“Chinese Wall”技术，用高质量模型为较弱模型生成详细指令。

Result: 该技术使Comma v0.1 1T在CanItEdit基准测试中性能提升超66%，Starcoder2 Instruct提升约20%。

Conclusion: 此技术能提升较弱模型性能，但受无版权限制训练模型缺乏的限制，实际应用受限。

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [276] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: 分析Stack Overflow上React相关问题，找出高频关键词和错误分类，建议社区提供算法问题指导材料。


<details>
  <summary>Details</summary>
Motivation: 虽知React在网页开发中受欢迎且有优势，但用户面临的具体挑战未知，故分析相关问题。

Method: 采用探索性数据分析，研究高频关键词、错误分类和基于用户声誉的错误。

Result: 得出React相关问题中8个高频关键词；算法错误是用户最常遇到的问题，中声誉用户贡献最多，占55.77%。

Conclusion: 研究结果能为早期实施阶段的React社区提供有价值的见解，助其克服挑战。

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [277] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: 文章提出SustainDiffusion方法，可在不改变模型架构下提升Stable Diffusion的社会和环境可持续性，减少偏见与能耗。


<details>
  <summary>Details</summary>
Motivation: 降低Stable Diffusion模型对社会和环境的危害，提升其社会和环境可持续性。

Method: SustainDiffusion搜索最优超参数和提示结构组合，降低生成图像的性别和种族偏见，同时降低能耗，且保持图像质量。

Result: 经测试，SustainDiffusion可使SD3的性别偏见降低68%、种族偏见降低59%、能耗降低48%，结果稳定且可泛化。

Conclusion: 不进行微调或改变模型架构也能提升文本到图像生成模型的社会和环境可持续性。

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [278] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: 文章研究比较分析等效电路和机器学习两种方法对CubeSat卫星电池放电建模，机器学习模型结果更准确。


<details>
  <summary>Details</summary>
Motivation: 为CubeSat卫星电池放电建模方法做出合理选择，预测自主电源系统断开后果，确保轨道设备容错性。

Method: 分析CubeSat卫星数据，考虑基于物理定律的解析建模和利用经验数据创建预测模型的机器学习两种方法。

Result: 等效电路方法具有透明性优势，但对环境变化和非标准卫星行为适应性差；机器学习模型结果更准确，能适应实际情况。

Conclusion: 未明确提及，但可推测机器学习方法在CubeSat卫星电池放电建模上更具优势。

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [279] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 提出LLM驱动的多智能体系统BugScope进行软件漏洞检测，评估效果超现有工具，在大规模系统发现大量未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析工具覆盖范围有限，现有结合LLM的方法难以处理复杂漏洞且分析上下文受限。

Method: 以代表示例学习新漏洞模式，通过程序切片合成检索策略提取相关检测上下文，构建定制检测提示引导LLM推理。

Result: 在40个真实漏洞数据集上，BugScope精确率87.04%、召回率90.00%，F1分数超现有工具0.44；在大规模开源系统发现141个未知漏洞，78个已修复，7个获开发者确认。

Conclusion: BugScope有显著实际应用价值。

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [280] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: 文章探讨AI在自动程序修复中的应用，通过实验研究程序员使用LLM调试程序的情况，得出初步结果并给出相关贡献。


<details>
  <summary>Details</summary>
Motivation: 探究AI特别是大语言模型在自动程序修复中的实际效果、验证修复方案有效性以及程序员如何利用LLM补充自身技能。

Method: 利用程序证明环境，将程序员随机分为有LLM访问权限和无LLM访问权限两组进行调试研究，采用目标 - 查询 - 指标方法。

Result: 结果与使用AI进行调试和自动程序修复的预期不同，是界定AI和LLM在提供程序错误修复中适当角色的第一步。

Conclusion: 给出使用LLM进行调试实验的详细方法、对程序员行为的细粒度分析、LLM使用模式定义和有效建议。

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [281] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文旨在介绍评估大语言模型生成的证据简报的实验方案，将其与人工简报对比，方法是开发基于RAG的工具生成简报并设计对照实验，结果和结论待实验后得出。


<details>
  <summary>Details</summary>
Motivation: 证据简报虽有用但人工制作阻碍其广泛应用，需评估大语言模型生成的简报。

Method: 开发基于RAG的大语言模型工具生成证据简报，用该工具自动生成两份此前人工制作的简报，设计对照实验评估。

Result: 待实验后报告。

Conclusion: 取决于实验结果。

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [282] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: 本文针对数据科学中计算笔记本缺乏细粒度日志分析研究的问题，开发工具收集数据，分析发现笔记本不仅用于开发探索，还用于调试，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域有大量细粒度日志分析研究，但数据科学中计算笔记本缺乏类似研究，为填补此研究空白。

Method: 开发收集Jupyter笔记本代码更改的工具集，收集超100小时开发数据形成数据集，利用数据集研究笔记本开发过程动态特性和变化。

Result: 分析数据发现单元格执行间的更改多为小修复和代码迭代修改。

Conclusion: 笔记本不仅是开发探索工具，也是调试工具，报告其他见解并提出未来研究方向。

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>


### [283] [Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs](https://arxiv.org/abs/2411.01789)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 本文聚焦于常用 Java 库客户端测试预言自动化，利用 Javadocs 和大语言模型实现，实验表明其生成的预言编译率高且准确性好。


<details>
  <summary>Details</summary>
Motivation: 软件测试依赖测试套件质量，自动化测试输入生成有进展，但测试预言自动化探索较少，因此研究常用 Java 库客户端测试预言自动化。

Method: 利用 Javadocs 丰富信息，以大语言模型为技术，构建测试预言自动化框架并进行实验评估。

Result: 大语言模型能从 Javadocs 生成检查正常和异常行为的预言，98.8% 可编译，96.4% 准确反映预期属性，少量错误可借助额外注释信息修正。

Conclusion: 基于 Javadocs 和大语言模型的方法可有效实现常用 Java 库客户端测试预言自动化。

Abstract: Software testing remains the most widely used methodology for validating
quality of code. However, effectiveness of testing critically depends on the
quality of test suites used. Test cases in a test suite consist of two
fundamental parts: (1) input values for the code under test, and (2) correct
checks for the outputs it produces. These checks are commonly written as
assertions, and termed test oracles. The last couple of decades have seen much
progress in automated test input generation, e.g., using fuzzing and symbolic
execution. However, automating test oracles remains a relatively less explored
problem area. Indeed, a test oracle by its nature requires knowledge of
expected behavior, which may only be known to the developer and may not not
exist in a formal language that supports automated reasoning.
  Our focus in this paper is automation of test oracles for clients of widely
used Java libraries, e.g., java.lang and java.util packages. Our key insight is
that Javadocs that provide a rich source of information can enable automated
generation of test oracles. Javadocs of the core Java libraries are fairly
detailed documents that contain natural language descriptions of not only how
the libraries behave but also how the clients must (not) use them. We use large
language models as an enabling technology to embody our insight into a
framework for test oracle automation, and evaluate it experimentally. Our
experiments demonstrate that LLMs can generate oracles for checking normal and
exceptional behaviors from Javadocs, with 98.8% of these oracles being
compilable and 96.4% accurately reflecting intended properties. Even for the
few incorrect oracles, errors are minor and can be easily corrected with the
help of additional comment information generated by the LLMs.

</details>


### [284] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 本文开展使用大语言模型（LLMs）编写Alloy声明式公式的对照实验，结果显示LLMs表现良好，有助于编写规范。


<details>
  <summary>Details</summary>
Motivation: 编写正确的声明式规范具有挑战性，探索使用大语言模型编写Alloy公式的可能性。

Method: 开展对照实验，使用11个经过充分研究的主题规范，采用ChatGPT和DeepSeek两个流行的大语言模型，从三方面使用LLMs编写Alloy公式。

Result: LLMs在从自然语言或Alloy输入属性合成完整Alloy公式方面表现良好，能列举多个独特解决方案，也能完成给定的Alloy公式草图。

Conclusion: LLMs在编写规范方面是令人兴奋的进步，有助于规范在软件开发中发挥关键作用，增强构建健壮软件的能力。

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [285] [Transaction Profiling and Address Role Inference in Tokenized U.S. Treasuries](https://arxiv.org/abs/2507.14808)
*Junliang Luo,Katrin Tinn,Samuel Ferreira Duran,Di Wu,Xue Liu*

Main category: q-fin.CP

TL;DR: 本文对美国国债支持的RWA代币进行量化分析，引入新框架建模地址经济角色，表现优于基线模型，为链上金融化研究提供实证证据。


<details>
  <summary>Details</summary>
Motivation: 市场快速扩张，但交易层面行为的实证分析有限，需要对美国国债支持的RWA代币进行研究。

Method: 对多链上的美国国债支持的RWA代币进行定量、功能层面剖析，分析解码合约调用；引入基于Poincaré嵌入和流动性图特征的曲率感知表示学习框架。

Result: 所提方法在角色推理上优于基线模型，且能推广到异常检测和钱包分类等下游任务。

Conclusion: 从交易层面为代币化国债的功能异质性和参与者角色提供结构化理解，为链上金融化研究提供新实证证据。

Abstract: Tokenized U.S. Treasuries have emerged as a prominent subclass of real-world
assets (RWAs), offering cryptographically enforced, yield-bearing instruments
collateralized by sovereign debt and deployed across multiple blockchain
networks. While the market has expanded rapidly, empirical analyses of
transaction-level behaviour remain limited. This paper conducts a quantitative,
function-level dissection of U.S. Treasury-backed RWA tokens including BUIDL,
BENJI, and USDY, across multi-chain: mostly Ethereum and Layer-2s. We analyze
decoded contract calls to isolate core functional primitives such as issuance,
redemption, transfer, and bridge activity, revealing segmentation in behaviour
between institutional actors and retail users. To model address-level economic
roles, we introduce a curvature-aware representation learning framework using
Poincar\'e embeddings and liquidity-based graph features. Our method
outperforms baseline models on our RWA Treasury dataset in role inference and
generalizes to downstream tasks such as anomaly detection and wallet
classification in broader blockchain transaction networks. These findings
provide a structured understanding of functional heterogeneity and participant
roles in tokenized Treasury in a transaction-level perspective, contributing
new empirical evidence to the study of on-chain financialization.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [286] [Through the Looking Glass: Bitcoin Treasury Companies](https://arxiv.org/abs/2507.14910)
*B K Meister*

Main category: q-fin.PM

TL;DR: 比特币金库公司引发股市热潮，超百家加密代币公司上市，可用杠杆解释此现象，还研究了凯利准则扩展。


<details>
  <summary>Details</summary>
Motivation: 解释比特币金库公司在股市大量积累代币这一现象，研究凯利准则扩展。

Method: 分析利用股票投资组合抵押借款产生杠杆的情况，研究将二元选择凯利准则扩展以纳入不确定性。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: Bitcoin treasury companies have taken stock markets by storm amassing
billions of dollars worth of tokens. More than a hundred companies active in
various crypto-tokens, not just Bitcoin, are listed. The ability to generate
leverage, for example stemming from borrowing collateralized by stock
portfolios, helps to understand this phenomenon.
  In addition, the extension of the binary-choice Kelly criterion to
incorporate uncertainty in the form of the Kullback-Leibler divergence or more
generally Bregman divergence is studied.

</details>


### [287] [Longitudinal review of portfolios with minimum variance approach before during and after the pandemic](https://arxiv.org/abs/2507.15111)
*Genjis A. Ossa,Luis H. Restrepo*

Main category: q-fin.PM

TL;DR: 研究2024年1月17日哥伦比亚股市最活跃股票受疫情影响，对比不同时期收益、风险等指标。


<details>
  <summary>Details</summary>
Motivation: 探究疫情对哥伦比亚股市最活跃股票的影响。

Method: 基于2015 - 2023年哥伦比亚最活跃公司的每日数据进行分析。

Result: 2015 - 2019年收益5.70%，风险18.45%；2016 - 2020年收益降至5.40%，风险升至24.64%，贝塔值上升，投资组合的资本市场线呈下降趋势，夏普指数为负。

Conclusion: 所构建的投资组合预期收益率低于无风险利率。

Abstract: This study investigates the impact of the pandemic on the most traded stocks
in the Colombian stock market for the date of January 17, 2024. Based on the
daily data of the most traded companies in Colombia for said date and covering
a period general from 2015 to 2023, in a summarized way our analysis reveals
that in the period 2015-2019, the return reached 5.70%, with a relatively low
risk of 18.45%. However, in the following period 2016 -2020, although the yield
decreased to 5.40%, the risk experienced a significant increase, reaching
24.64%. The beta also showed variations, being lowest in 2015-2019 with 0.61
and increasing to 1.02 in 2016-2020. The capital market line (LMC) in the
constructed portfolios has a downward trend, indicating that the portfolio
offers an expected rate of return lower than the risk-free rate. This finding
is supported by the Sharpe index, which shows negative values throughout the
periods studied.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [288] [FinSurvival: A Suite of Large Scale Survival Modeling Tasks from Finance](https://arxiv.org/abs/2507.14160)
*Aaron Green,Zihan Nie,Hanzhen Qin,Oshani Seneviratne,Kristin P. Bennett*

Main category: q-fin.ST

TL;DR: 本文从DeFi加密货币借贷交易数据导出16个生存建模任务构成FinSurvival基准，可用于评估AI生存模型，现有方法应对不足，未来可拓展。


<details>
  <summary>Details</summary>
Motivation: 需要大规模、现实且免费可用的数据集来对AI生存模型进行基准测试。

Method: 从公开的DeFi加密货币借贷交易数据中，通过自动化管道基于索引和结果事件选择构建16个生存建模任务，并为每个任务创建对应的分类问题。

Result: FinSurvival有超750万条记录，评估显示现有方法难以应对这些具有挑战性的任务。

Conclusion: FinSurvival可评估多领域AI生存模型，还能展示AI模型评估DeFi机遇和风险的能力，未来可通过纳入更多交易和协议拓展。

Abstract: Survival modeling predicts the time until an event occurs and is widely used
in risk analysis; for example, it's used in medicine to predict the survival of
a patient based on censored data. There is a need for large-scale, realistic,
and freely available datasets for benchmarking artificial intelligence (AI)
survival models. In this paper, we derive a suite of 16 survival modeling tasks
from publicly available transaction data generated by lending of
cryptocurrencies in Decentralized Finance (DeFi). Each task was constructed
using an automated pipeline based on choices of index and outcome events. For
example, the model predicts the time from when a user borrows cryptocurrency
coins (index event) until their first repayment (outcome event). We formulate a
survival benchmark consisting of a suite of 16 survival-time prediction tasks
(FinSurvival). We also automatically create 16 corresponding classification
problems for each task by thresholding the survival time using the restricted
mean survival time. With over 7.5 million records, FinSurvival provides a suite
of realistic financial modeling tasks that will spur future AI survival
modeling research. Our evaluation indicated that these are challenging tasks
that are not well addressed by existing methods. FinSurvival enables the
evaluation of AI survival models applicable to traditional finance, industry,
medicine, and commerce, which is currently hindered by the lack of large public
datasets. Our benchmark demonstrates how AI models could assess opportunities
and risks in DeFi. In the future, the FinSurvival benchmark pipeline can be
used to create new benchmarks by incorporating more DeFi transactions and
protocols as the use of cryptocurrency grows.

</details>


### [289] [Eigenvalue Distribution of Empirical Correlation Matrices for Multiscale Complex Systems and Application to Financial Data](https://arxiv.org/abs/2507.14325)
*Luan M. T. de Moraes,Antônio M. S. Macêdo,Giovani L. Vasconcelos,Raydonal Ospina*

Main category: q-fin.ST

TL;DR: 介绍一种描述多维时间序列相关矩阵特征值分布的方法，改进金融数据特征值谱描述，支持湍流市场假说并提供降噪框架。


<details>
  <summary>Details</summary>
Motivation: 改进多元金融数据中经验相关矩阵特征值谱的描述，挑战金融市场传统观点。

Method: 运用新开发的矩阵H理论，考虑信息级联，将Marchenko - Pastur分布扩展以考虑不同特征尺度。

Result: 能捕捉更大比例的数据方差，挑战传统观点，支持湍流市场假说。

Conclusion: 该方法有效源于金融市场复杂性增加，为经验相关矩阵降噪提供实用框架，增强资产间真实市场相关性推断。

Abstract: We introduce a method for describing eigenvalue distributions of correlation
matrices from multidimensional time series. Using our newly developed matrix H
theory, we improve the description of eigenvalue spectra for empirical
correlation matrices in multivariate financial data by considering an
informational cascade modeled as a hierarchical structure akin to the
Kolmogorov statistical theory of turbulence. Our approach extends the
Marchenko-Pastur distribution to account for distinct characteristic scales,
capturing a larger fraction of data variance, and challenging the traditional
view of noise-dressed financial markets. We conjecture that the effectiveness
of our method stems from the increased complexity in financial markets,
reflected by new characteristic scales and the growth of computational trading.
These findings not only support the turbulent market hypothesis as a source of
noise but also provide a practical framework for noise reduction in empirical
correlation matrices, enhancing the inference of true market correlations
between assets.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [290] [A Comparative Analysis of Statistical and Machine Learning Models for Outlier Detection in Bitcoin Limit Order Books](https://arxiv.org/abs/2507.14960)
*Ivan Letteri*

Main category: q-fin.TR

TL;DR: 本文对加密货币限价订单簿中异常值检测方法进行比较分析，通过回测评估13种模型，发现经验协方差模型表现最佳，为相关研究提供基准。


<details>
  <summary>Details</summary>
Motivation: 理解加密货币市场动态，检测潜在操纵交易行为，在高波动和新兴监管环境下有重要意义。

Method: 在统一测试环境AITA - OBS中，对13种不同模型进行综合比较分析，通过对26,204条记录的数据集进行回测评估。

Result: 表现最佳的经验协方差（EC）模型实现6.70%的收益，显著优于标准的买入持有基准。

Conclusion: 强调异常驱动策略的有效性，揭示模型复杂性、交易频率和性能之间的权衡，为加密货币市场微观结构研究提供严格基准，凸显其在算法交易和风险管理中的潜力。

Abstract: The detection of outliers within cryptocurrency limit order books (LOBs) is
of paramount importance for comprehending market dynamics, particularly in
highly volatile and nascent regulatory environments. This study conducts a
comprehensive comparative analysis of robust statistical methods and advanced
machine learning techniques for real-time anomaly identification in
cryptocurrency LOBs. Within a unified testing environment, named AITA Order
Book Signal (AITA-OBS), we evaluate the efficacy of thirteen diverse models to
identify which approaches are most suitable for detecting potentially
manipulative trading behaviours. An empirical evaluation, conducted via
backtesting on a dataset of 26,204 records from a major exchange, demonstrates
that the top-performing model, Empirical Covariance (EC), achieves a 6.70%
gain, significantly outperforming a standard Buy-and-Hold benchmark. These
findings underscore the effectiveness of outlier-driven strategies and provide
insights into the trade-offs between model complexity, trade frequency, and
performance. This study contributes to the growing corpus of research on
cryptocurrency market microstructure by furnishing a rigorous benchmark of
anomaly detection models and highlighting their potential for augmenting
algorithmic trading and risk management.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [291] [Statistical and Algorithmic Foundations of Reinforcement Learning](https://arxiv.org/abs/2507.14444)
*Yuejie Chi,Yuxin Chen,Yuting Wei*

Main category: stat.ML

TL;DR: 介绍强化学习在样本稀缺情况下的挑战，通过教程介绍重要算法和理论进展，涵盖多种场景和方法，围绕样本复杂度等问题讨论。


<details>
  <summary>Details</summary>
Motivation: 新兴应用中模型复杂度爆炸和非凸性使样本稀缺情况下高效强化学习面临挑战，需理解和提升算法样本与计算效率。

Method: 以马尔可夫决策过程为核心数学模型，介绍多种强化学习场景和主流方法。

Result: 无明确结果阐述，主要是对强化学习多方面内容的介绍。

Conclusion: 无明确结论，旨在介绍强化学习相关算法和理论进展。

Abstract: As a paradigm for sequential decision making in unknown environments,
reinforcement learning (RL) has received a flurry of attention in recent years.
However, the explosion of model complexity in emerging applications and the
presence of nonconvexity exacerbate the challenge of achieving efficient RL in
sample-starved situations, where data collection is expensive, time-consuming,
or even high-stakes (e.g., in clinical trials, autonomous systems, and online
advertising). How to understand and enhance the sample and computational
efficacies of RL algorithms is thus of great interest. In this tutorial, we aim
to introduce several important algorithmic and theoretical developments in RL,
highlighting the connections between new ideas and classical topics. Employing
Markov Decision Processes as the central mathematical model, we cover several
distinctive RL scenarios (i.e., RL with a simulator, online RL, offline RL,
robust RL, and RL with human feedback), and present several mainstream RL
approaches (i.e., model-based approach, value-based approach, and policy
optimization). Our discussions gravitate around the issues of sample
complexity, computational efficiency, as well as algorithm-dependent and
information-theoretic lower bounds from a non-asymptotic viewpoint.

</details>


### [292] [Diffusion Models for Time Series Forecasting: A Survey](https://arxiv.org/abs/2507.14507)
*Chen Su,Zhengzhou Cai,Yuanhe Tian,Zihong Zheng,Yan Song*

Main category: stat.ML

TL;DR: 本文对扩散模型在时间序列预测（TSF）中的应用进行综述，介绍模型、分类现有方法、分析数据集和评估指标，讨论局限与未来方向。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像合成中表现出色，且在TSF中取得有前景的结果，需要对其在TSF中的应用进行全面综述。

Method: 介绍标准扩散模型及其变体，阐述其在TSF任务中的适配；对扩散模型用于TSF的现有方法进行系统分类和总结；分析常用数据集和评估指标。

Result: 对扩散模型在TSF中的应用进行了全面梳理，包括模型、方法、数据集和评估指标等方面。

Conclusion: 详细阐述了扩散模型在TSF中的最新进展和未来前景，为该领域研究人员提供参考。

Abstract: Diffusion models, initially developed for image synthesis, demonstrate
remarkable generative capabilities. Recently, their application has expanded to
time series forecasting (TSF), yielding promising results. In this survey, we
firstly introduce the standard diffusion models and their prevalent variants,
explaining their adaptation to TSF tasks. We then provide a comprehensive
review of diffusion models for TSF, paying special attention to the sources of
conditional information and the mechanisms for integrating this conditioning
within the models. In analyzing existing approaches using diffusion models for
TSF, we provide a systematic categorization and a comprehensive summary of them
in this survey. Furthermore, we examine several foundational diffusion models
applied to TSF, alongside commonly used datasets and evaluation metrics.
Finally, we discuss current limitations in these approaches and potential
future research directions. Overall, this survey details recent progress and
future prospects for diffusion models in TSF, serving as a reference for
researchers in the field.

</details>


### [293] [Deep Learning-Based Survival Analysis with Copula-Based Activation Functions for Multivariate Response Prediction](https://arxiv.org/abs/2507.14641)
*Jong-Min Kim,Il Do Ha,Sangjin Kim*

Main category: stat.ML

TL;DR: 研究整合深度学习、copula函数和生存分析处理多元生存数据，提出含copula激活函数的CNN - LSTM模型，提升预测准确性并以控制图评估。


<details>
  <summary>Details</summary>
Motivation: 有效处理高度相关且右删失的多元生存数据。

Method: 引入基于copula的激活函数（Clayton、Gumbel及其组合），构建含copula激活函数的CNN - LSTM模型处理多元多类型生存响应数据，用Shewhart控制图评估，关注平均运行长度（ARL）。

Result: 通过模拟研究和乳腺癌真实数据分析，模型提升了预测准确性。

Conclusion: 含copula激活函数的CNN - LSTM模型能有效处理右删失数据，捕捉复杂模式，提高预测精度。

Abstract: This research integrates deep learning, copula functions, and survival
analysis to effectively handle highly correlated and right-censored
multivariate survival data. It introduces copula-based activation functions
(Clayton, Gumbel, and their combinations) to model the nonlinear dependencies
inherent in such data. Through simulation studies and analysis of real breast
cancer data, our proposed CNN-LSTM with copula-based activation functions for
multivariate multi-types of survival responses enhances prediction accuracy by
explicitly addressing right-censored data and capturing complex patterns. The
model's performance is evaluated using Shewhart control charts, focusing on the
average run length (ARL).

</details>


### [294] [Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators](https://arxiv.org/abs/2507.14652)
*Ponkrshnan Thiagarajan,Tamer A. Zaki,Michael D. Shields*

Main category: stat.ML

TL;DR: 本文提出结合变分推断（VI）和哈密顿蒙特卡罗（HMC）的混合方法，减少参数空间维度，加速神经网络后验推断，在大型网络上验证了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: HMC用于贝叶斯神经网络计算要求高，现有近似技术有误差，导致不确定性估计不可靠。

Method: 先进行全网络的VI训练，分析参数对预测不确定性的影响，减少参数空间维度，仅对影响大的参数子集进行HMC。

Result: 在深度神经网络和算子网络上验证了方法的效率和准确性，能对大量参数的大型网络进行推断，可有效学习复杂物理系统的替代模型。

Conclusion: 所提混合方法能高效准确地量化神经网络和神经算子的不确定性，加速全批量HMC的后验推断。

Abstract: Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample
from the posterior distribution in Bayesian inference. However, HMC techniques
are computationally demanding for Bayesian neural networks due to the high
dimensionality of the network's parameter space and the non-convexity of their
posterior distributions. Therefore, various approximation techniques, such as
variational inference (VI) or stochastic gradient MCMC, are often employed to
infer the posterior distribution of the network parameters. Such approximations
introduce inaccuracies in the inferred distributions, resulting in unreliable
uncertainty estimates. In this work, we propose a hybrid approach that combines
inexpensive VI and accurate HMC methods to efficiently and accurately quantify
uncertainties in neural networks and neural operators. The proposed approach
leverages an initial VI training on the full network. We examine the influence
of individual parameters on the prediction uncertainty, which shows that a
large proportion of the parameters do not contribute substantially to
uncertainty in the network predictions. This information is then used to
significantly reduce the dimension of the parameter space, and HMC is performed
only for the subset of network parameters that strongly influence prediction
uncertainties. This yields a framework for accelerating the full batch HMC for
posterior inference in neural networks. We demonstrate the efficiency and
accuracy of the proposed framework on deep neural networks and operator
networks, showing that inference can be performed for large networks with tens
to hundreds of thousands of parameters. We show that this method can
effectively learn surrogates for complex physical systems by modeling the
operator that maps from upstream conditions to wall-pressure data on a cone in
hypersonic flow.

</details>


### [295] [When few labeled target data suffice: a theory of semi-supervised domain adaptation via fine-tuning from multiple adaptive starts](https://arxiv.org/abs/2507.14661)
*Wooseok Ha,Yuansi Chen*

Main category: stat.ML

TL;DR: 本文基于结构因果模型开发理论框架分析半监督域自适应（SSDA）方法性能，提出三种SSDA方法和MASFT算法，经模拟验证有效性。


<details>
  <summary>Details</summary>
Motivation: SSDA在很多应用中重要，但相关有效性理论，尤其是在源 - 目标分布偏移场景下未充分探索。

Method: 基于结构因果模型开发理论框架，引入三种有针对性微调策略的SSDA方法，提出MASFT算法从多起点微调UDA模型并基于验证集选优。

Result: 在不同假设下，扩展无监督域自适应（UDA）方法到SSDA可实现有限目标标签下的极小极大最优目标性能；MASFT算法在多种分布偏移类型中实现接近最优的目标预测性能，减少对标记目标数据的需求。

Conclusion: 提出的方法经模拟验证有效，能在有限标记目标数据下提升目标域预测性能。

Abstract: Semi-supervised domain adaptation (SSDA) aims to achieve high predictive
performance in the target domain with limited labeled target data by exploiting
abundant source and unlabeled target data. Despite its significance in numerous
applications, theory on the effectiveness of SSDA remains largely unexplored,
particularly in scenarios involving various types of source-target
distributional shifts. In this work, we develop a theoretical framework based
on structural causal models (SCMs) which allows us to analyze and quantify the
performance of SSDA methods when labeled target data is limited. Within this
framework, we introduce three SSDA methods, each having a fine-tuning strategy
tailored to a distinct assumption about the source and target relationship.
Under each assumption, we demonstrate how extending an unsupervised domain
adaptation (UDA) method to SSDA can achieve minimax-optimal target performance
with limited target labels. When the relationship between source and target
data is only vaguely known -- a common practical concern -- we propose the
Multi Adaptive-Start Fine-Tuning (MASFT) algorithm, which fine-tunes UDA models
from multiple starting points and selects the best-performing one based on a
small hold-out target validation dataset. Combined with model selection
guarantees, MASFT achieves near-optimal target predictive performance across a
broad range of types of distributional shifts while significantly reducing the
need for labeled target data. We empirically validate the effectiveness of our
proposed methods through simulations.

</details>


### [296] [Uncertainty Quantification for Machine Learning-Based Prediction: A Polynomial Chaos Expansion Approach for Joint Model and Input Uncertainty Propagation](https://arxiv.org/abs/2507.14782)
*Xiaoping Du*

Main category: stat.ML

TL;DR: 本文提出基于PCE的框架处理机器学习代理模型输入与模型联合不确定性传播，实现高效准确计算及全局敏感性分析，支持工程应用。


<details>
  <summary>Details</summary>
Motivation: 机器学习预测存在固有误差且与输入变异性耦合，需准确量化和传播联合不确定性以生成可靠工程预测。

Method: 基于多项式混沌展开（PCE）构建框架，将随机输入转换到统一标准空间构建PCE代理模型，以高斯过程回归模型为重点。

Result: 能高效准确计算输出均值和标准差，可进行全局敏感性分析，量化输入变量和模型不确定性对输出变异性的贡献。

Conclusion: 该方法提供了计算高效且可解释的全面不确定性量化框架，支持下游工程应用中可信的机器学习预测。

Abstract: Machine learning (ML) surrogate models are increasingly used in engineering
analysis and design to replace computationally expensive simulation models,
significantly reducing computational cost and accelerating decision-making
processes. However, ML predictions contain inherent errors, often estimated as
model uncertainty, which is coupled with variability in model inputs.
Accurately quantifying and propagating these combined uncertainties is
essential for generating reliable engineering predictions. This paper presents
a robust framework based on Polynomial Chaos Expansion (PCE) to handle joint
input and model uncertainty propagation. While the approach applies broadly to
general ML surrogates, we focus on Gaussian Process regression models, which
provide explicit predictive distributions for model uncertainty. By
transforming all random inputs into a unified standard space, a PCE surrogate
model is constructed, allowing efficient and accurate calculation of the mean
and standard deviation of the output. The proposed methodology also offers a
mechanism for global sensitivity analysis, enabling the accurate quantification
of the individual contributions of input variables and ML model uncertainty to
the overall output variability. This approach provides a computationally
efficient and interpretable framework for comprehensive uncertainty
quantification, supporting trustworthy ML predictions in downstream engineering
applications.

</details>


### [297] [Learning Nonlinear Causal Reductions to Explain Reinforcement Learning Policies](https://arxiv.org/abs/2507.14901)
*Armin Kekić,Jan Schneider,Dieter Büchler,Bernhard Schölkopf,Michel Besserve*

Main category: stat.ML

TL;DR: 本文从因果角度解释强化学习策略行为，引入随机扰动学习简化因果模型，实验证明该方法能揭示策略的重要模式和失败模式。


<details>
  <summary>Details</summary>
Motivation: 由于智能体 - 环境交互复杂高维，难以解释强化学习策略成败原因，因此从因果角度进行解释。

Method: 将状态、动作和奖励视为低层次因果模型变量，在执行策略动作时引入随机扰动观察对累积奖励的影响，开发非线性因果模型简化框架确保近似干预一致性，并证明一类非线性因果模型存在精确干预一致性的唯一解。

Result: 在合成因果模型和实际强化学习任务（如钟摆控制和机器人乒乓球）实验中，该方法能揭示训练好的强化学习策略中的重要行为模式、偏差和失败模式。

Conclusion: 所提出的方法可以从因果角度有效解释强化学习策略的行为，发现重要模式和失败原因。

Abstract: Why do reinforcement learning (RL) policies fail or succeed? This is a
challenging question due to the complex, high-dimensional nature of
agent-environment interactions. In this work, we take a causal perspective on
explaining the behavior of RL policies by viewing the states, actions, and
rewards as variables in a low-level causal model. We introduce random
perturbations to policy actions during execution and observe their effects on
the cumulative reward, learning a simplified high-level causal model that
explains these relationships. To this end, we develop a nonlinear Causal Model
Reduction framework that ensures approximate interventional consistency,
meaning the simplified high-level model responds to interventions in a similar
way as the original complex system. We prove that for a class of nonlinear
causal models, there exists a unique solution that achieves exact
interventional consistency, ensuring learned explanations reflect meaningful
causal patterns. Experiments on both synthetic causal models and practical RL
tasks-including pendulum control and robot table tennis-demonstrate that our
approach can uncover important behavioral patterns, biases, and failure modes
in trained RL policies.

</details>


### [298] [Hypergraphs on high dimensional time series sets using signature transform](https://arxiv.org/abs/2507.15802)
*Rémi Vaucher,Paul Minchella*

Main category: stat.ML

TL;DR: 本文解决从多元时间序列集合构建超图的挑战，扩展现有框架，利用签名变换属性引入随机控制，在合成数据集验证有良好结果。


<details>
  <summary>Details</summary>
Motivation: 以往工作聚焦单个多元时间序列构建超图，本文旨在扩展框架以处理多元时间序列集合。

Method: 推广Chretien等人提出的方法，利用签名变换属性引入可控随机性，增强构建过程的鲁棒性。

Result: 在合成数据集上验证了该方法，并取得了有前景的结果。

Conclusion: 提出的方法可有效从多元时间序列集合构建超图，具有一定的鲁棒性和应用潜力。

Abstract: In recent decades, hypergraphs and their analysis through Topological Data
Analysis (TDA) have emerged as powerful tools for understanding complex data
structures. Various methods have been developed to construct hypergraphs --
referred to as simplicial complexes in the TDA framework -- over datasets,
enabling the formation of edges between more than two vertices. This paper
addresses the challenge of constructing hypergraphs from collections of
multivariate time series. While prior work has focused on the case of a single
multivariate time series, we extend this framework to handle collections of
such time series. Our approach generalizes the method proposed in Chretien and
al. by leveraging the properties of signature transforms to introduce
controlled randomness, thereby enhancing the robustness of the construction
process. We validate our method on synthetic datasets and present promising
results.

</details>


### [299] [Learning under Latent Group Sparsity via Diffusion on Networks](https://arxiv.org/abs/2507.15097)
*Subhroshekhar Ghosh,Soumendu Sundar Mukherjee*

Main category: stat.ML

TL;DR: 提出一种无需组身份先验信息的稀疏学习方法，基于网络拉普拉斯几何，通过热流局部网络动力学计算惩罚项，性能有理论保障，还探讨与网络科学模型的联系。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中解释变量存在组或簇结构时的稀疏学习问题，且不需要组身份的先验信息。

Method: 受具有社区结构的底层网络的拉普拉斯几何启发，将其融入惩罚项，通过热流局部网络动力学计算，数据驱动构建网络。

Result: 提出的惩罚项可在lasso和组lasso惩罚项之间插值，运行时间为插值参数，能自动退化为lasso；技术有严格定理保证性能并给出样本复杂度界限。

Conclusion: 该工作为经典学习任务应用基于扩散的技术提供了可能，可利用数据的几何、动态和随机结构之间的相互作用。

Abstract: Group or cluster structure on explanatory variables in machine learning
problems is a very general phenomenon, which has attracted broad interest from
practitioners and theoreticians alike. In this work we contribute an approach
to sparse learning under such group structure, that does not require prior
information on the group identities. Our paradigm is motivated by the Laplacian
geometry of an underlying network with a related community structure, and
proceeds by directly incorporating this into a penalty that is effectively
computed via a heat-flow-based local network dynamics. The proposed penalty
interpolates between the lasso and the group lasso penalties, the runtime of
the heat-flow dynamics being the interpolating parameter. As such it can
automatically default to lasso when the group structure reflected in the
Laplacian is weak. In fact, we demonstrate a data-driven procedure to construct
such a network based on the available data. Notably, we dispense with
computationally intensive pre-processing involving clustering of variables,
spectral or otherwise. Our technique is underpinned by rigorous theorems that
guarantee its effective performance and provide bounds on its sample
complexity. In particular, in a wide range of settings, it provably suffices to
run the diffusion for time that is only logarithmic in the problem dimensions.
We explore in detail the interfaces of our approach with key statistical
physics models in network science, such as the Gaussian Free Field and the
Stochastic Block Model. Our work raises the possibility of applying similar
diffusion-based techniques to classical learning tasks, exploiting the
interplay between geometric, dynamical and stochastic structures underlying the
data.

</details>


### [300] [Accelerated Bayesian Optimal Experimental Design via Conditional Density Estimation and Informative Data](https://arxiv.org/abs/2507.15235)
*Miao Huang,Hongqiao Wang,Kunyu Wu*

Main category: stat.ML

TL;DR: 本文在贝叶斯框架下探索最优实验设计，改进效用期望计算，解决关键挑战，经理论和实践验证有效。


<details>
  <summary>Details</summary>
Motivation: 提升实验结果的有效性、可靠性和效率，解决低模拟效率和高数据获取成本场景下的问题。

Method: 利用贝叶斯定理将效用期望重构成独立双重积分形式，用条件密度估计近似高斯随机场比率，以协方差选数据集。

Result: 提出的方法经理论分析和实际应用验证了有效性。

Conclusion: 该方法能在不确定性下提高实验效率和辅助决策。

Abstract: The Design of Experiments (DOEs) is a fundamental scientific methodology that
provides researchers with systematic principles and techniques to enhance the
validity, reliability, and efficiency of experimental outcomes. In this study,
we explore optimal experimental design within a Bayesian framework, utilizing
Bayes' theorem to reformulate the utility expectation--originally expressed as
a nested double integral--into an independent double integral form,
significantly improving numerical efficiency. To further accelerate the
computation of the proposed utility expectation, conditional density estimation
is employed to approximate the ratio of two Gaussian random fields, while
covariance serves as a selection criterion to identify informative datasets
during model fitting and integral evaluation. In scenarios characterized by low
simulation efficiency and high costs of raw data acquisition, key challenges
such as surrogate modeling, failure probability estimation, and parameter
inference are systematically restructured within the Bayesian experimental
design framework. The effectiveness of the proposed methodology is validated
through both theoretical analysis and practical applications, demonstrating its
potential for enhancing experimental efficiency and decision-making under
uncertainty.

</details>


### [301] [Missing value imputation with adversarial random forests -- MissARF](https://arxiv.org/abs/2507.15681)
*Pegah Golchian,Jan Kapar,David S. Watson,Marvin N. Wright*

Main category: stat.ML

TL;DR: 提出新的缺失值插补方法MissARF，实验显示其在插补质量和运行速度上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决生物统计分析中缺失值处理的常见难题，提供新的插补方法。

Method: 基于生成式机器学习，采用对抗随机森林（ARF）进行密度估计和数据合成，根据非缺失值条件从ARF生成的估计条件分布中采样插补缺失值。

Result: MissARF在插补质量和运行速度上与现有最先进的单重和多重插补方法相当，且多重插补无额外成本。

Conclusion: MissARF是一种有效、快速且易用的缺失值插补方法。

Abstract: Handling missing values is a common challenge in biostatistical analyses,
typically addressed by imputation methods. We propose a novel, fast, and
easy-to-use imputation method called missing value imputation with adversarial
random forests (MissARF), based on generative machine learning, that provides
both single and multiple imputation. MissARF employs adversarial random forest
(ARF) for density estimation and data synthesis. To impute a missing value of
an observation, we condition on the non-missing values and sample from the
estimated conditional distribution generated by ARF. Our experiments
demonstrate that MissARF performs comparably to state-of-the-art single and
multiple imputation methods in terms of imputation quality and fast runtime
with no additional costs for multiple imputation.

</details>


### [302] [Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces](https://arxiv.org/abs/2507.15741)
*Gábor Lugosi,Marcos Matabuena*

Main category: stat.ML

TL;DR: 本文介绍度量空间回归模型不确定性量化框架，含共方差和异方差两种方法，证明估计量一致性并展示其在个性化医疗中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决度量空间回归模型中的不确定性量化问题。

Method: 提出基于新定义同方差性的共形预测算法，在异方差设置下提出无需共形校准的局部k近邻法。

Result: 两种方法适用于任何回归算法且可扩展到大数据集，证明了估计量在最小条件下的一致性。

Conclusion: 所提方法具有实用性，可应用于涉及随机响应对象的个性化医疗领域。

Abstract: This paper introduces a framework for uncertainty quantification in
regression models defined in metric spaces. Leveraging a newly defined notion
of homoscedasticity, we develop a conformal prediction algorithm that offers
finite-sample coverage guarantees and fast convergence rates of the oracle
estimator. In heteroscedastic settings, we forgo these non-asymptotic
guarantees to gain statistical efficiency, proposing a local
$k$--nearest--neighbor method without conformal calibration that is adaptive to
the geometry of each particular nonlinear space. Both procedures work with any
regression algorithm and are scalable to large data sets, allowing
practitioners to plug in their preferred models and incorporate domain
expertise. We prove consistency for the proposed estimators under minimal
conditions. Finally, we demonstrate the practical utility of our approach in
personalized--medicine applications involving random response objects such as
probability distributions and graph Laplacians.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [303] [Inference for Diffusion Processes via Controlled Sequential Monte Carlo and Splitting Schemes](https://arxiv.org/abs/2507.14535)
*Shu Huang,Richard G. Everitt,Massimiliano Tamborrino,Adam M. Johansen*

Main category: stat.CO

TL;DR: 介绍半线性随机微分方程的推理框架，通过有效估计伪似然进行推理，方法在计算效率和准确性间取得平衡且在实际应用中表现良好。


<details>
  <summary>Details</summary>
Motivation: 为广泛的半线性随机微分方程提供推理框架，解决不同离散时间观测制度下的参数推理问题。

Method: 将隐含伪似然表示为Feynman - Kac流的归一化常数，用受控序贯蒙特卡罗方法估计，结合似然法推理；利用扩散桥减少时间离散化偏差。

Result: 在一系列问题中获得良好推理结果，方法在计算效率和准确性间取得良好平衡，在神经科学实例中表现良好。

Conclusion: 所开发的策略能有效解决半线性随机微分方程的推理问题，具有较好的实用性和性能。

Abstract: We introduce an inferential framework for a wide class of semi-linear
stochastic differential equations (SDEs). Recent work has shown that numerical
splitting schemes can preserve critical properties of such types of SDEs, give
rise to explicit pseudolikelihoods, and hence allow for parameter inference for
fully observed processes. Here, under several discrete time observation regimes
(particularly, partially and fully observed with and without noise), we
represent the implied pseudolikelihood as the normalising constant of a
Feynman--Kac flow, allowing its efficient estimation via controlled sequential
Monte Carlo and adapt likelihood-based methods to exploit this pseudolikelihood
for inference. The strategy developed herein allows us to obtain good
inferential results across a range of problems. Using diffusion bridges, we are
able to computationally reduce bias coming from time-discretisation without
recourse to more complex numerical schemes which typically require considerable
application-specific efforts. Simulations illustrate that our method provides
an excellent trade-off between computational efficiency and accuracy, under
hypoellipticity, for both point and posterior estimation. Application to a
neuroscience example shows the good performance of the method in challenging
settings.

</details>


### [304] [Bayesian Inversion via Probabilistic Cellular Automata: an application to image denoising](https://arxiv.org/abs/2507.14869)
*Danilo Costarelli,Michele Piconi,Alessio Troiani*

Main category: stat.CO

TL;DR: 提出用概率细胞自动机（PCA）结合贝叶斯方法解决逆问题，对比PCA方法与标准吉布斯采样器在图像去噪任务中的性能，结果表明PCA算法有潜力。


<details>
  <summary>Details</summary>
Motivation: 寻找能高效解决高维逆问题的贝叶斯推理方法，利用PCA的并行特性。

Method: 使用PCA从后验分布近似中采样，在图像去噪任务中对比PCA方法与标准吉布斯采样器。

Result: 在图像去噪任务中，基于PCA的算法在PSNR和SSIM方面有一定表现。

Conclusion: PCA-based算法是高维逆问题贝叶斯推理的有前景的替代方法。

Abstract: We propose using Probabilistic Cellular Automata (PCA) to address inverse
problems with the Bayesian approach. In particular, we use PCA to sample from
an approximation of the posterior distribution. The peculiar feature of PCA is
their intrinsic parallel nature, which allows for a straightforward parallel
implementation that allows the exploitation of parallel computing architecture
in a natural and efficient manner. We compare the performance of the PCA method
with the standard Gibbs sampler on an image denoising task in terms of Peak
Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM). The numerical
results obtained with this approach suggest that PCA-based algorithms are a
promising alternative for Bayesian inference in high-dimensional inverse
problems.

</details>


### [305] [Algorithms for Approximating Conditionally Optimal Bounds](https://arxiv.org/abs/2507.15529)
*George Bissias*

Main category: stat.CO

TL;DR: 本文为单边有界离散网格上的单变量分布样本开发非参数置信区域算法，推广理论并给出近似算法。


<details>
  <summary>Details</summary>
Motivation: 为单边有界离散网格上的单变量分布样本开发非参数置信区域算法。

Method: 将Learned - Miller理论推广到样本空间的预序，利用字典序低和高序的特性推导算法，包括闭形式近似、多项式时间近似方案和蒙特卡罗方法。

Result: 得到三种近似算法：字典序低和高序的闭形式近似、分位数序的多项式时间近似方案、适用于任意网格大小的蒙特卡罗方法。

Conclusion: 通过推广理论成功开发出多种近似算法用于计算非参数置信区域。

Abstract: This work develops algorithms for non-parametric confidence regions for
samples from a univariate distribution whose support is a discrete mesh bounded
on the left. We generalize the theory of Learned-Miller to preorders over the
sample space. In this context, we show that the lexicographic low and
lexicographic high orders are in some way extremal in the class of monotone
preorders. From this theory we derive several approximation algorithms: 1)
Closed form approximations for the lexicographic low and high orders with error
tending to zero in the mesh size; 2) A polynomial-time approximation scheme for
quantile orders with error tending to zero in the mesh size; 3) Monte Carlo
methods for calculating quantile and lexicographic low orders applicable to any
mesh size.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [306] [Search-Based Autonomous Vehicle Motion Planning Using Game Theory](https://arxiv.org/abs/2507.15088)
*Pouya Panahandeh,Mohammad Pirani,Baris Fidan,Amir Khajepour*

Main category: cs.RO

TL;DR: 提出基于搜索的自动驾驶汽车交互式运动规划方案，用博弈论方法，性能经实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统搜索方法将其他道路使用者视为静态障碍物，需更现实的路径规划。

Method: 采用博弈论方法，将其他道路使用者视为智能体的搜索式交互运动规划方案。

Result: 生成更现实的路径，计算时间短，可用于实时应用，与现有技术对比并经实验验证。

Conclusion: 所提运动规划方案可行且性能良好。

Abstract: In this paper, we propose a search-based interactive motion planning scheme
for autonomous vehicles (AVs), using a game-theoretic approach. In contrast to
traditional search-based approaches, the newly developed approach considers
other road users (e.g. drivers and pedestrians) as intelligent agents rather
than static obstacles. This leads to the generation of a more realistic path
for the AV. Due to the low computational time, the proposed motion planning
scheme is implementable in real-time applications. The performance of the
developed motion planning scheme is compared with existing motion planning
techniques and validated through experiments using WATonoBus, an electrical
all-weather autonomous shuttle bus.

</details>


### [307] [Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach](https://arxiv.org/abs/2507.14249)
*Yuejiao Xie,Maonan Wang,Di Zhou,Man-On Pun,Zhu Han*

Main category: cs.RO

TL;DR: 针对城市空中交通（UAM）轨迹规划问题，提出构建无线电地图评估通信质量，并引入MSHA - RL框架实现实时路径规划，实验表明该方法可提升效率和保障安全。


<details>
  <summary>Details</summary>
Motivation: UAM轨迹规划需优先考虑通信质量保障安全，且要适应实时乘客需求，但传统策略缺乏灵活性。

Method: 先构建无线电地图评估通信质量，再引入MSHA - RL框架，先对齐不同数据源，再用混合注意力平衡全局和局部信息。

Result: 该方法实现了符合通信要求的轨迹规划，减少了旅行时间，提高了运营效率。

Conclusion: 提出的方法能在保障乘客安全的同时，有效解决UAM轨迹规划问题。

Abstract: Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions
to alleviate urban congestion, with path planning becoming a key focus area.
Unlike ground transportation, UAM trajectory planning has to prioritize
communication quality for accurate location tracking in constantly changing
environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,
requires adaptive planning to respond to real-time passenger requests,
especially in ride-sharing scenarios where passenger demands are unpredictable
and dynamic. However, conventional trajectory planning strategies based on
predefined routes lack the flexibility to meet varied passenger ride demands.
To address these challenges, this work first proposes constructing a radio map
to evaluate the communication quality of urban airspace. Building on this, we
introduce a novel Multi-Source Hybrid Attention Reinforcement Learning
(MSHA-RL) framework for the challenge of effectively focusing on passengers and
UAM locations, which arises from the significant dimensional disparity between
the representations. This model first generates the alignment among diverse
data sources with large gap dimensions before employing hybrid attention to
balance global and local insights, thereby facilitating responsive, real-time
path planning. Extensive experimental results demonstrate that the approach
enables communication-compliant trajectory planning, reducing travel time and
enhancing operational efficiency while prioritizing passenger safety.

</details>


### [308] [One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner](https://arxiv.org/abs/2507.14914)
*Zhexuan Xu,Jie Wang,Siyuan Xu,Zijie Geng,Mingxuan Yuan,Feng Wu*

Main category: cs.RO

TL;DR: 提出名为Flora的三阶段布图规划器，实验显示在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有布图规划方法难以与后续物理设计阶段集成，导致模块内组件布局不理想和模块间馈通过多。

Method: Flora分三个阶段，第一阶段用掩码技术粗优化HPWL和馈通；第二阶段局部调整模块形状实现零空白布局，细优化馈通和组件布局；第三阶段用快速树搜索法放置组件并调整模块边界实现跨阶段优化。

Result: Flora优于现有技术，HPWL平均降低6%，FTpin降低5.16%，FTmod降低29.15%，组件布局性能提升14%。

Conclusion: Flora是一种有效的布图规划方法，能解决现有方法的问题并提升芯片PPA指标。

Abstract: Floorplanning determines the shapes and locations of modules on a chip canvas
and plays a critical role in optimizing the chip's Power, Performance, and Area
(PPA) metrics. However, existing floorplanning approaches often fail to
integrate with subsequent physical design stages, leading to suboptimal
in-module component placement and excessive inter-module feedthrough. To tackle
this challenge, we propose Flora, a three-stage feedthrough and placement aware
rectilinear floorplanner. In the first stage, Flora employs wiremask and
position mask techniques to achieve coarse-grained optimization of HPWL and
feedthrough. In the second stage, under the constraint of a fixed outline,
Flora achieves a zero-whitespace layout by locally resizing module shapes,
thereby performing fine-grained optimization of feedthrough and improving
component placement. In the third stage, Flora utilizes a fast tree
search-based method to efficiently place components-including macros and
standard cells-within each module, subsequently adjusting module boundaries
based on the placement results to enable cross-stage optimization. Experimental
results show that Flora outperforms recent state-of-the-art floorplanning
approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,
29.15% in FTmod, and a 14% improvement in component placement performance.

</details>


### [309] [FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models](https://arxiv.org/abs/2507.14975)
*Yufan Song,Jiatao Zhang,Zeng Gu,Qingmiao Liang,Tuocheng Hu,Wei Song,Shiqiang Zhu*

Main category: cs.RO

TL;DR: 提出FCRF框架解决家用机器人任务规划纠错问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型自我反思机制缺乏灵活性，限制纠错效果。

Method: 提出Flexible Constructivism Reflection Framework (FCRF)，一种导师 - 执行者架构，使大语言模型能根据任务难度进行灵活自我反思，并建设性地整合历史经验与失败教训。

Result: 在AlfWorld模拟和现实环境中对不同家庭任务进行评估，FCRF显著提升复杂长时机器人任务的整体性能和自我反思灵活性。

Conclusion: FCRF框架有效可行，能解决现有方法局限性。

Abstract: Autonomous error correction is critical for domestic robots to achieve
reliable execution of complex long-horizon tasks. Prior work has explored
self-reflection in Large Language Models (LLMs) for task planning error
correction; however, existing methods are constrained by inflexible
self-reflection mechanisms that limit their effectiveness. Motivated by these
limitations and inspired by human cognitive adaptation, we propose the Flexible
Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture
that enables LLMs to perform flexible self-reflection based on task difficulty,
while constructively integrating historical valuable experience with failure
lessons. We evaluated FCRF on diverse domestic tasks through simulation in
AlfWorld and physical deployment in the real-world environment. Experimental
results demonstrate that FCRF significantly improves overall performance and
self-reflection flexibility in complex long-horizon robotic tasks.

</details>


### [310] [Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper](https://arxiv.org/abs/2507.15062)
*Xinyue Zhu,Binghao Huang,Yunzhu Li*

Main category: cs.RO

TL;DR: 本文介绍了一种带集成触觉传感器的便携式轻量级抓爪，提出跨模态表征学习框架，在细粒度任务验证中展现出准确性和鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 现有手持抓爪大多缺乏触觉传感，而触觉反馈对精确操作至关重要，因此需要能同步收集视觉和触觉数据的抓爪及学习框架。

Method: 开发带集成触觉传感器的抓爪，提出整合视觉和触觉信号的跨模态表征学习框架。

Result: 学习过程产生聚焦物理交互接触区域的可解释表征，在下游操作任务中使策略学习更高效，在细粒度任务验证中展现出更好的准确性和抗干扰鲁棒性。

Conclusion: 提出的抓爪和学习框架能基于多模态反馈支持精确的机器人操作。

Abstract: Handheld grippers are increasingly used to collect human demonstrations due
to their ease of deployment and versatility. However, most existing designs
lack tactile sensing, despite the critical role of tactile feedback in precise
manipulation. We present a portable, lightweight gripper with integrated
tactile sensors that enables synchronized collection of visual and tactile data
in diverse, real-world, and in-the-wild settings. Building on this hardware, we
propose a cross-modal representation learning framework that integrates visual
and tactile signals while preserving their distinct characteristics. The
learning procedure allows the emergence of interpretable representations that
consistently focus on contacting regions relevant for physical interactions.
When used for downstream manipulation tasks, these representations enable more
efficient and effective policy learning, supporting precise robotic
manipulation based on multimodal feedback. We validate our approach on
fine-grained tasks such as test tube insertion and pipette-based fluid
transfer, demonstrating improved accuracy and robustness under external
disturbances. Our project page is available at
https://binghao-huang.github.io/touch_in_the_wild/ .

</details>


### [311] [The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents](https://arxiv.org/abs/2507.15478)
*Simon Kohaut,Felix Divo,Navid Hamid,Benedict Flade,Julian Eggert,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.RO

TL;DR: 本文介绍神经符号系统可解决自主智能体在不确定环境中可靠和合规行为的挑战，提出Constitutional Controller (CoCo)框架和自我怀疑概念，并通过实验证明其优势。


<details>
  <summary>Details</summary>
Motivation: 解决自主智能体在不确定环境中确保可靠和合规行为的挑战。

Method: 引入CoCo框架，通过对表示约束的深度概率逻辑程序进行推理；提出自我怀疑概念，以基于怀疑特征的概率密度实现。

Result: 在现实世界的空中移动性研究中，证明CoCo能让智能自主系统学习适当的怀疑，安全合规地导航复杂不确定环境。

Conclusion: 神经符号系统结合CoCo框架和自我怀疑概念，是解决自主智能体在不确定环境中可靠和合规行为的有效方案。

Abstract: Ensuring reliable and rule-compliant behavior of autonomous agents in
uncertain environments remains a fundamental challenge in modern robotics. Our
work shows how neuro-symbolic systems, which integrate probabilistic, symbolic
white-box reasoning models with deep learning methods, offer a powerful
solution to this challenge. This enables the simultaneous consideration of
explicit rules and neural models trained on noisy data, combining the strength
of structured reasoning with flexible representations. To this end, we
introduce the Constitutional Controller (CoCo), a novel framework designed to
enhance the safety and reliability of agents by reasoning over deep
probabilistic logic programs representing constraints such as those found in
shared traffic spaces. Furthermore, we propose the concept of self-doubt,
implemented as a probability density conditioned on doubt features such as
travel velocity, employed sensors, or health factors. In a real-world aerial
mobility study, we demonstrate CoCo's advantages for intelligent autonomous
systems to learn appropriate doubts and navigate complex and uncertain
environments safely and compliantly.

</details>


### [312] [The Emergence of Deep Reinforcement Learning for Path Planning](https://arxiv.org/abs/2507.15469)
*Thanh Thi Nguyen,Saeid Nahavandi,Imran Razzak,Dung Nguyen,Nhat Truong Pham,Quoc Viet Hung Nguyen*

Main category: cs.RO

TL;DR: 本文综述路径规划方法，涵盖传统方法与DRL，分析优缺点，指出开放挑战和未来研究方向，尤其关注混合方法。


<details>
  <summary>Details</summary>
Motivation: 复杂动态环境对自主系统需求增加，推动智能路径规划研究。

Method: 对传统路径规划方法和DRL进行全面综述，分类关键算法，讨论优缺点。

Result: 明确传统方法和DRL在计算效率、可扩展性等方面的优缺点。

Conclusion: 指出关键开放挑战，强调混合方法是未来自主导航的有前景方向。

Abstract: The increasing demand for autonomous systems in complex and dynamic
environments has driven significant research into intelligent path planning
methodologies. For decades, graph-based search algorithms, linear programming
techniques, and evolutionary computation methods have served as foundational
approaches in this domain. Recently, deep reinforcement learning (DRL) has
emerged as a powerful method for enabling autonomous agents to learn optimal
navigation strategies through interaction with their environments. This survey
provides a comprehensive overview of traditional approaches as well as the
recent advancements in DRL applied to path planning tasks, focusing on
autonomous vehicles, drones, and robotic platforms. Key algorithms across both
conventional and learning-based paradigms are categorized, with their
innovations and practical implementations highlighted. This is followed by a
thorough discussion of their respective strengths and limitations in terms of
computational efficiency, scalability, adaptability, and robustness. The survey
concludes by identifying key open challenges and outlining promising avenues
for future research. Special attention is given to hybrid approaches that
integrate DRL with classical planning techniques to leverage the benefits of
both learning-based adaptability and deterministic reliability, offering
promising directions for robust and resilient autonomous navigation.

</details>


### [313] [GR-3 Technical Report](https://arxiv.org/abs/2507.15493)
*Chilam Cheang,Sijin Chen,Zhongren Cui,Yingdong Hu,Liqun Huang,Tao Kong,Hang Li,Yifeng Li,Yuxiao Liu,Xiao Ma,Hao Niu,Wenxuan Ou,Wanli Peng,Zeyu Ren,Haixin Shi,Jiawen Tian,Hongtao Wu,Xin Xiao,Yuyang Xiao,Jiafeng Xu,Yichu Yang*

Main category: cs.RO

TL;DR: 介绍构建通用机器人策略的进展，开发GR - 3模型和ByteMini机器人，实验显示GR - 3表现超基线，望助力通用机器人发展。


<details>
  <summary>Details</summary>
Motivation: 构建能在日常生活中协助人类的通用机器人策略。

Method: 采用多方面训练方法，包括与网络规模视觉语言数据共同训练、利用VR设备收集的人类轨迹数据高效微调、用机器人轨迹数据进行有效模仿学习。

Result: GR - 3在多种挑战性任务上超越了最先进的基线方法π₀。

Conclusion: GR - 3是迈向构建通用机器人的一步，有望协助人类日常生活。

Abstract: We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.

</details>


### [314] [Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers](https://arxiv.org/abs/2507.15833)
*Ian Chuang,Andrew Lee,Dechen Gao,Jinyu Zou,Iman Soltani*

Main category: cs.RO

TL;DR: 本文探索将类人主动注视融入机器人策略，构建主动视觉系统，采用新的标记化方案和两种注视模仿与预测方法，结果表明该方法能降低计算开销、提升性能。


<details>
  <summary>Details</summary>
Motivation: 人类视觉是由注视驱动的主动过程，而机器人学习系统通常依赖被动、均匀的原始图像，因此探索将类人主动注视融入机器人策略以提高效率和性能。

Method: 基于凹视图像技术构建主动视觉机器人系统，引入收集数据和训练策略的框架，将注视信息融入视觉变换器，采用凹视标记化方案，探索两种注视模仿和预测方法。

Result: 所提出的凹视机器人视觉方法大幅降低计算开销，提高高精度任务性能和对干扰物的鲁棒性。

Conclusion: 受人类启发的视觉处理为机器人视觉系统提供了有用的归纳偏置。

Abstract: Human vision is a highly active process driven by gaze, which directs
attention and fixation to task-relevant regions and dramatically reduces visual
processing. In contrast, robot learning systems typically rely on passive,
uniform processing of raw camera images. In this work, we explore how
incorporating human-like active gaze into robotic policies can enhance both
efficiency and performance. We build on recent advances in foveated image
processing and apply them to an Active Vision robot system that emulates both
human head movement and eye tracking. Extending prior work on the AV-ALOHA
robot simulation platform, we introduce a framework for simultaneously
collecting eye-tracking data and robot demonstrations from a human operator as
well as a simulation benchmark and dataset for training robot policies that
incorporate human gaze. Given the widespread use of Vision Transformers (ViTs)
in robot learning, we integrate gaze information into ViTs using a foveated
patch tokenization scheme inspired by recent work in image segmentation.
Compared to uniform patch tokenization, this significantly reduces the number
of tokens-and thus computation-without sacrificing visual fidelity near regions
of interest. We also explore two approaches to gaze imitation and prediction
from human data. The first is a two-stage model that predicts gaze to guide
foveation and action; the second integrates gaze into the action space,
allowing the policy to jointly predict gaze and actions end-to-end. Our results
show that our method for foveated robot vision not only drastically reduces
computational overhead, but also improves performance for high precision tasks
and robustness to unseen distractors. Together, these findings suggest that
human-inspired visual processing offers a useful inductive bias for robotic
vision systems. https://ian-chuang.github.io/gaze-av-aloha/

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [315] [Metaverse Security and Privacy Research: A Systematic Review](https://arxiv.org/abs/2507.14985)
*Argianto Rahartomo,Leonel Merino,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 对2013 - 2024年元宇宙安全与隐私文献进行系统综述，揭示研究趋势与不足，呼吁跨学科方法。


<details>
  <summary>Details</summary>
Motivation: 元宇宙技术快速发展带来新的安全和隐私挑战，需了解研究界如何应对。

Method: 对2013 - 2024年文献进行系统综述，按方法组织研究，分析安全隐私属性、沉浸式组件和评估策略。

Result: 近五年研究活动激增，注重实用和以用户为中心的方法，多使用基准测试、人体实验和定性方法，认证和不可观察性研究最多。

Conclusion: 元宇宙存在技术复杂性和人为因素交织问题，需跨学科方法保障沉浸式环境的包容性和可信度。

Abstract: The rapid growth of metaverse technologies, including virtual worlds,
augmented reality, and lifelogging, has accelerated their adoption across
diverse domains. This rise exposes users to significant new security and
privacy challenges due to sociotechnical complexity, pervasive connectivity,
and extensive user data collection in immersive environments. We present a
systematic review of the literature published between 2013 and 2024, offering a
comprehensive analysis of how the research community has addressed
metaverse-related security and privacy issues over the past decade. We organize
the studies by method, examined the security and privacy properties, immersive
components, and evaluation strategies. Our investigation reveals a sharp
increase in research activity in the last five years, a strong focus on
practical and user-centered approaches, and a predominant use of benchmarking,
human experimentation, and qualitative methods. Authentication and
unobservability are the most frequently studied properties. However, critical
gaps remain in areas such as policy compliance, accessibility,
interoperability, and back-end infrastructure security. We emphasize the
intertwined technical complexity and human factors of the metaverse and call
for integrated, interdisciplinary approaches to securing inclusive and
trustworthy immersive environments.

</details>


### [316] [LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries](https://arxiv.org/abs/2507.15058)
*Ian Hardgrove,John D. Hastings*

Main category: cs.CR

TL;DR: 提出LibLMFuzz框架降低模糊测试闭源库成本，测试表现良好，为后续研究奠基。


<details>
  <summary>Details</summary>
Motivation: 模糊测试有成本，闭源软件下选择更复杂，需降低闭源库模糊测试成本。

Method: 将大语言模型与轻量级工具链结合，自主分析二进制文件、规划模糊策略、生成驱动并修复错误。

Result: 在四个Linux库上测试，为558个可模糊测试API函数生成语法正确驱动，实现100% API覆盖，75.52%合成驱动首次执行正确。

Conclusion: 大语言模型增强的中间件有望降低黑盒组件模糊测试成本，为未来研究提供基础，分支覆盖有研究机会。

Abstract: A fundamental problem in cybersecurity and computer science is determining
whether a program is free of bugs and vulnerabilities. Fuzzing, a popular
approach to discovering vulnerabilities in programs, has several advantages
over alternative strategies, although it has investment costs in the form of
initial setup and continuous maintenance. The choice of fuzzing is further
complicated when only a binary library is available, such as the case of
closed-source and proprietary software. In response, we introduce LibLMFuzz, a
framework that reduces costs associated with fuzzing closed-source libraries by
pairing an agentic Large Language Model (LLM) with a lightweight tool-chain
(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan
fuzz strategies, generate drivers, and iteratively self-repair build or runtime
errors. Tested on four widely-used Linux libraries, LibLMFuzz produced
syntactically correct drivers for all 558 fuzz-able API functions, achieving
100% API coverage with no human intervention. Across the 1601 synthesized
drivers, 75.52% were nominally correct on first execution. The results show
that LLM-augmented middleware holds promise in reducing the costs of fuzzing
black box components and provides a foundation for future research efforts.
Future opportunities exist for research in branch coverage.

</details>


### [317] [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
*Yiran Wu,Mauricio Velazco,Andrew Zhao,Manuel Raúl Meléndez Luján,Srisuma Movva,Yogesh K Roy,Quang Nguyen,Roberto Rodriguez,Qingyun Wu,Michael Albada,Julia Kiseleva,Anand Mudgerikar*

Main category: cs.CR

TL;DR: 提出ExCyTIn - Bench基准，用于评估大语言模型（LLM）在网络威胁调查任务中的表现，构建数据集，实验表明该任务有很大研究空间。


<details>
  <summary>Details</summary>
Motivation: 现实中安全分析师处理网络威胁调查任务繁琐，随着大语言模型发展，构建基于LLM的自动威胁调查代理是有前景的方向，需要开发和评估此类代理。

Method: 从受控Azure租户构建数据集，利用安全日志构建威胁调查图，用图上的成对节点生成问题，以起始节点为背景、结束节点为答案，还可生成有可验证奖励的程序任务。

Result: 不同模型实验显示任务难度大，基础设置下所有评估模型平均奖励0.249，最佳为0.368。

Conclusion: 网络威胁调查任务有很大研究空间，代码和数据即将公布。

Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on
the task of Cyber Threat Investigation through security questions derived from
investigation graphs. Real-world security analysts must sift through a large
number of heterogeneous alert signals and security logs, follow multi-hop
chains of evidence, and compile an incident report. With the developments of
LLMs, building LLM-based agents for automatic thread investigation is a
promising direction. To assist the development and evaluation of LLM agents, we
construct a dataset from a controlled Azure tenant that covers 8 simulated
real-world multi-step attacks, 57 log tables from Microsoft Sentinel and
related services, and 589 automatically generated questions. We leverage
security logs extracted with expert-crafted detection logic to build threat
investigation graphs, and then generate questions with LLMs using paired nodes
on the graph, taking the start node as background context and the end node as
answer. Anchoring each question to these explicit nodes and edges not only
provides automatic, explainable ground truth answers but also makes the
pipeline reusable and readily extensible to new logs. This also enables the
automatic generation of procedural tasks with verifiable rewards, which can be
naturally extended to training agents via reinforcement learning. Our
comprehensive experiments with different models confirm the difficulty of the
task: with the base setting, the average reward across all evaluated models is
0.249, and the best achieved is 0.368, leaving substantial headroom for future
research. Code and data are coming soon!

</details>


### [318] [PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training](https://arxiv.org/abs/2507.14202)
*Pengfei Du*

Main category: cs.CR

TL;DR: 本文提出无PRM安全对齐框架，利用自动化红队和对抗训练保障大语言模型安全，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于PRM的安全对齐方法有计算开销大、可扩展性受限问题，需新方法保障大语言模型安全部署。

Method: 提出无PRM安全对齐框架，用遗传算法优化、多智能体模拟等攻击策略识别漏洞，通过课程学习和自适应正则化机制进行对抗训练。

Result: 在五个最先进大语言模型上实验，比基于PRM方法安全对齐性能更优，计算成本降低61%。

Conclusion: 该框架推动高效大语言模型安全对齐领域发展，为资源受限组织提供强大安全措施，可应对不断演变的对抗威胁。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse applications, yet they pose significant security risks that threaten
their safe deployment in critical domains. Current security alignment
methodologies predominantly rely on Process Reward Models (PRMs) to evaluate
intermediate reasoning steps, introducing substantial computational overhead
and scalability constraints. This paper presents a novel PRM-free security
alignment framework that leverages automated red teaming and adversarial
training to achieve robust security guarantees while maintaining computational
efficiency. Our approach systematically identifies vulnerabilities through
sophisticated attack strategies including genetic algorithm optimization,
multi-agent simulation, and advanced prompt mutation techniques. The framework
enhances model robustness via targeted adversarial training with curriculum
learning and adaptive regularization mechanisms. Comprehensive experimental
evaluation across five state-of-the-art LLMs demonstrates that our method
achieves superior security alignment performance compared to PRM-based
approaches while reducing computational costs by 61\%. The framework
incorporates transparent reporting and continuous audit mechanisms that enable
iterative security improvement and regulatory compliance. Our contributions
advance the field of efficient LLM security alignment by democratizing access
to robust security measures for resource-constrained organizations and
providing a scalable foundation for addressing evolving adversarial threats.

</details>


### [319] [Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design](https://arxiv.org/abs/2507.14207)
*Richard M. Charles,James H. Curry,Richard B. Charles*

Main category: cs.CR

TL;DR: 研究探索K - 12教育中学生对大语言模型的木马化提示攻击，揭示GPT - 3.5和GPT - 4漏洞，提出检测和缓解工具。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在K - 12教育中的应用有机会也有风险，研究学生如何绕过内容审核系统进行木马化提示攻击。

Method: 通过涉及模拟K - 12查询和多轮对话的系统实验。

Result: 揭示了GPT - 3.5和GPT - 4的关键漏洞，提出了原型工具TrojanPromptGuard (TPG)。

Conclusion: 研究结果为AI安全研究人员和教育技术专家在教育中安全部署大语言模型提供参考。

Abstract: The integration of Large Language Models (LLMs) in K--12 education offers
both transformative opportunities and emerging risks. This study explores how
students may Trojanize prompts to elicit unsafe or unintended outputs from
LLMs, bypassing established content moderation systems with safety guardrils.
Through a systematic experiment involving simulated K--12 queries and
multi-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This
paper presents our experimental design, detailed findings, and a prototype
tool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized
educational prompts. These insights aim to inform both AI safety researchers
and educational technologists on the safe deployment of LLMs for educators.

</details>


### [320] [Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification](https://arxiv.org/abs/2507.14223)
*Wen-Cheng Chung,Shu-Ting Huang,Hao-Ting Pai*

Main category: cs.CR

TL;DR: 提出IG-MD提升可解释入侵检测系统精度，在UKM - IDS20上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有XAI管道在不透明分类器上添加解释器，提供部分且可能误导的见解，需更好方法提升精度与透明度。

Method: 引入Multi - Granular Discretization (IG - MD)，以多种基于高斯的分辨率表示连续特征。

Result: 在UKM - IDS20的九个训练 - 测试分割中，精度提升至少4个百分点，召回率约为1.0。

Conclusion: 单一可解释模型可跨领域扩展，无需定制调整。

Abstract: Explainable intrusion detection systems (IDS) are now recognized as essential
for mission-critical networks, yet most "XAI" pipelines still bolt an
approximate explainer onto an opaque classifier, leaving analysts with partial
and sometimes misleading insights. The Interpretable Generalization (IG)
mechanism, published in IEEE Transactions on Information Forensics and
Security, eliminates that bottleneck by learning coherent patterns - feature
combinations unique to benign or malicious traffic - and turning them into
fully auditable rules. IG already delivers outstanding precision, recall, and
AUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the
data. To raise precision further without sacrificing transparency, we introduce
Multi-Granular Discretization (IG-MD), which represents every continuous
feature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts
precision by greater than or equal to 4 percentage points across all nine
train-test splits while preserving recall approximately equal to 1.0,
demonstrating that a single interpretation-ready model can scale across domains
without bespoke tuning.

</details>


### [321] [Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack](https://arxiv.org/abs/2507.14248)
*Eldor Abdukhamidov,Mohammed Abuhamad,Simon S. Woo,Hyoungshick Kim,Tamer Abuhmed*

Main category: cs.CR

TL;DR: 研究提出AdViT攻击，可误导ViT模型及其解释模型，实验显示在白盒和黑盒场景有高成功率和置信度，且生成解释难检测。


<details>
  <summary>Details</summary>
Motivation: 现有针对ViT模型威胁研究未考虑对模型解释的影响，而解释模型可辅助检测对抗样本，需研究其结合解释模型时的脆弱性。

Method: 提出AdViT攻击方法生成能误导模型和解释模型的对抗样本。

Result: 在多种变压器模型和两个基于变压器的解释器上实验，AdViT在白盒和黑盒场景攻击成功率达100%，白盒场景误分类置信度达98%，黑盒场景达76%，且能生成准确解释。

Conclusion: AdViT攻击展示了变压器模型结合解释模型时仍存在易受攻击的脆弱性。

Abstract: Vision transformer (ViT) models, when coupled with interpretation models, are
regarded as secure and challenging to deceive, making them well-suited for
security-critical domains such as medical applications, autonomous vehicles,
drones, and robotics. However, successful attacks on these systems can lead to
severe consequences. Recent research on threats targeting ViT models primarily
focuses on generating the smallest adversarial perturbations that can deceive
the models with high confidence, without considering their impact on model
interpretations. Nevertheless, the use of interpretation models can effectively
assist in detecting adversarial examples. This study investigates the
vulnerability of transformer models to adversarial attacks, even when combined
with interpretation models. We propose an attack called "AdViT" that generates
adversarial examples capable of misleading both a given transformer model and
its coupled interpretation model. Through extensive experiments on various
transformer models and two transformer-based interpreters, we demonstrate that
AdViT achieves a 100% attack success rate in both white-box and black-box
scenarios. In white-box scenarios, it reaches up to 98% misclassification
confidence, while in black-box scenarios, it reaches up to 76%
misclassification confidence. Remarkably, AdViT consistently generates accurate
interpretations in both scenarios, making the adversarial examples more
difficult to detect.

</details>


### [322] [Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives](https://arxiv.org/abs/2507.14519)
*Wenxuan Zeng,Tianshi Xu,Yi Chen,Yifan Zhou,Mingzhe Zhang,Jin Tan,Cheng Hong,Meng Li*

Main category: cs.CR

TL;DR: 本文对基于跨层优化的隐私保护机器学习（PPML）研究进行全面系统综述，分类回顾进展，比较现有工作，讨论未来方向并提供GitHub跟踪进展。


<details>
  <summary>Details</summary>
Motivation: PPML虽能保护隐私，但存在效率和可扩展性问题，需缩小与明文机器学习的效率差距。

Method: 将现有论文分为协议层、模型层和系统层进行分类回顾，并对现有工作进行定性和定量比较。

Result: 对PPML研究进展进行了全面梳理和比较，提出未来研究方向。

Conclusion: 本次综述有助于全面理解现有方法，有望激发PPML领域的未来突破，同时提供GitHub仓库跟踪发展。

Abstract: Privacy-preserving machine learning (PPML) based on cryptographic protocols
has emerged as a promising paradigm to protect user data privacy in cloud-based
machine learning services. While it achieves formal privacy protection, PPML
often incurs significant efficiency and scalability costs due to orders of
magnitude overhead compared to the plaintext counterpart. Therefore, there has
been a considerable focus on mitigating the efficiency gap for PPML. In this
survey, we provide a comprehensive and systematic review of recent PPML studies
with a focus on cross-level optimizations. Specifically, we categorize existing
papers into protocol level, model level, and system level, and review progress
at each level. We also provide qualitative and quantitative comparisons of
existing works with technical insights, based on which we discuss future
research directions and highlight the necessity of integrating optimizations
across protocol, model, and system levels. We hope this survey can provide an
overarching understanding of existing approaches and potentially inspire future
breakthroughs in the PPML field. As the field is evolving fast, we also provide
a public GitHub repository to continuously track the developments, which is
available at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.

</details>


### [323] [VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning](https://arxiv.org/abs/2507.14625)
*Juntao Tan,Anran Li,Quanchao Liu,Peng Ran,Lan Zhang*

Main category: cs.CR

TL;DR: 本文提出VTarbel攻击框架，用于规避检测器增强的纵向联邦学习推理，经实验验证其有效性，揭示当前VFL部署存在安全盲点。


<details>
  <summary>Details</summary>
Motivation: 现有纵向联邦学习（VFL）安全威胁尤其是目标标签攻击研究不足，且现有方法假设不现实、忽略真实系统中的异常检测器。

Method: 提出两阶段、最小知识攻击框架VTarbel，准备阶段选样本、收集伪标签训练估计检测器和替代模型，攻击阶段用模型引导梯度扰动制作对抗样本。

Result: 在多种模型架构、数据集和异常检测器上评估，VTarbel优于四个基线方法，能规避检测，对三种隐私保护防御措施仍有效。

Conclusion: 当前VFL部署存在关键安全盲点，亟需强大的、考虑攻击的防御措施。

Abstract: Vertical federated learning (VFL) enables multiple parties with disjoint
features to collaboratively train models without sharing raw data. While
privacy vulnerabilities of VFL are extensively-studied, its security
threats-particularly targeted label attacks-remain underexplored. In such
attacks, a passive party perturbs inputs at inference to force
misclassification into adversary-chosen labels. Existing methods rely on
unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore
anomaly detectors deployed in real-world systems. To bridge this gap, we
introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly
designed to evade detector-enhanced VFL inference. During the preparation
stage, the attacker selects a minimal set of high-expressiveness samples (via
maximum mean discrepancy), submits them through VFL protocol to collect
predicted labels, and uses these pseudo-labels to train estimated detector and
surrogate model on local features. In attack stage, these models guide
gradient-based perturbations of remaining samples, crafting adversarial
instances that induce targeted misclassifications and evade detection. We
implement VTarbel and evaluate it against four model architectures, seven
multimodal datasets, and two anomaly detectors. Across all settings, VTarbel
outperforms four state-of-the-art baselines, evades detection, and retains
effective against three representative privacy-preserving defenses. These
results reveal critical security blind spots in current VFL deployments and
underscore urgent need for robust, attack-aware defenses.

</details>


### [324] [VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking](https://arxiv.org/abs/2507.14629)
*Juntao Tan,Lan Zhang,Zhonghao Hu,Kai Yang,Peng Ran,Bo Li*

Main category: cs.CR

TL;DR: 文章提出 VMask 框架抵御 VFL 中 MC 攻击，在隐私与效用间达最佳平衡，运行速度快。


<details>
  <summary>Details</summary>
Motivation: 现有抵御 MC 攻击的防御方法存在牺牲模型准确性或计算开销大的问题，需新防御框架。

Method: 从层掩码角度设计 VMask 框架，用秘密共享技术掩码层参数，设计选择关键层掩码策略，提供可调隐私预算。

Result: 通过五个模型架构和 13 个数据集评估，VMask 实现最佳隐私 - 效用平衡，抵御 MC 攻击，保留模型性能，运行速度快。

Conclusion: VMask 是有效抵御 MC 攻击的框架，能灵活控制隐私级别，运行开销可接受。

Abstract: Though vertical federated learning (VFL) is generally considered to be
privacy-preserving, recent studies have shown that VFL system is vulnerable to
label inference attacks originating from various attack surfaces. Among these
attacks, the model completion (MC) attack is currently the most powerful one.
Existing defense methods against it either sacrifice model accuracy or incur
impractical computational overhead. In this paper, we propose VMask, a novel
label privacy protection framework designed to defend against MC attack from
the perspective of layer masking. Our key insight is to disrupt the strong
correlation between input data and intermediate outputs by applying the secret
sharing (SS) technique to mask layer parameters in the attacker's model. We
devise a strategy for selecting critical layers to mask, reducing the overhead
that would arise from naively applying SS to the entire model. Moreover, VMask
is the first framework to offer a tunable privacy budget to defenders, allowing
for flexible control over the levels of label privacy according to actual
requirements. We built a VFL system, implemented VMask on it, and extensively
evaluated it using five model architectures and 13 datasets with different
modalities, comparing it to 12 other defense methods. The results demonstrate
that VMask achieves the best privacy-utility trade-off, successfully thwarting
the MC attack (reducing the label inference accuracy to a random guessing
level) while preserving model performance (e.g., in Transformer-based model,
the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up
to 60,846 times faster than cryptography-based methods, and it only marginally
exceeds that of standard VFL by 1.8 times in a large Transformer-based model,
which is generally acceptable.

</details>


### [325] [Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree](https://arxiv.org/abs/2507.14799)
*Sam Johnson,Viet Pham,Thai Le*

Main category: cs.CR

TL;DR: 研究表明基于大语言模型的网页导航代理有自动化能力但易受间接提示注入攻击，展示攻击效果并强调安全风险和防御需求。


<details>
  <summary>Details</summary>
Motivation: 揭示基于大语言模型的网页导航代理存在的安全隐患，随着其广泛应用，有必要研究相关攻击手段和风险。

Method: 使用贪婪坐标梯度（GCG）算法和由Llama - 3.1驱动的Browser Gym代理进行攻击测试。

Result: 在真实网站的定向和通用攻击中取得高成功率，包括登录凭证窃取和强制广告点击。

Conclusion: 基于大语言模型的自主网页代理存在严重安全风险，需要更强的防御措施。

Abstract: This work demonstrates that LLM-based web navigation agents offer powerful
automation capabilities but are vulnerable to Indirect Prompt Injection (IPI)
attacks. We show that adversaries can embed universal adversarial triggers in
webpage HTML to hijack agent behavior that utilizes the accessibility tree to
parse HTML, causing unintended or malicious actions. Using the Greedy
Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by
Llama-3.1, our system demonstrates high success rates across real websites in
both targeted and general attacks, including login credential exfiltration and
forced ad clicks. Our empirical results highlight critical security risks and
the need for stronger defenses as LLM-driven autonomous web agents become more
widely adopted. The system software
(https://github.com/sej2020/manipulating-web-agents) is released under the MIT
License, with an accompanying publicly available demo website
(http://lethaiq.github.io/attack-web-llm-agent).

</details>


### [326] [A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption](https://arxiv.org/abs/2507.14853)
*Khoa Nguyen,Tanveer Khan,Antonis Michalas*

Main category: cs.CR

TL;DR: 探索混合同态加密（HHE）与联邦学习（FL）结合，解决通信和隐私挑战，推动可扩展安全的去中心化学习系统。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽有潜力，但面临通信开销和数据隐私挑战，现有隐私保护技术成本高，限制实际部署。

Method: 探索将结合对称加密和同态加密的混合同态加密（HHE）与联邦学习有效集成。

Result: 未提及具体结果。

Conclusion: 该方法有望解决通信和隐私挑战，为可扩展安全的去中心化学习系统铺平道路。

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data, making it a promising approach for privacy-sensitive domains. Despite
its potential, FL faces significant challenges, particularly in terms of
communication overhead and data privacy. Privacy-preserving Techniques (PPTs)
such as Homomorphic Encryption (HE) have been used to mitigate these concerns.
However, these techniques introduce substantial computational and communication
costs, limiting their practical deployment. In this work, we explore how Hybrid
Homomorphic Encryption (HHE), a cryptographic protocol that combines symmetric
encryption with HE, can be effectively integrated with FL to address both
communication and privacy challenges, paving the way for scalable and secure
decentralized learning system.

</details>


### [327] [PromptArmor: Simple yet Effective Prompt Injection Defenses](https://arxiv.org/abs/2507.15219)
*Tianneng Shi,Kaijie Zhu,Zhun Wang,Yuqi Jia,Will Cai,Weida Liang,Haonan Wang,Hend Alzahrani,Joshua Lu,Kenji Kawaguchi,Basel Alomair,Xuandong Zhao,William Yang Wang,Neil Gong,Wenbo Guo,Dawn Song*

Main category: cs.CR

TL;DR: 本文提出防御大语言模型代理提示注入攻击的方法PromptArmor，效果好，建议用作评估新防御方法的基准。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明大语言模型代理易受提示注入攻击，需有效防御手段。

Method: PromptArmor利用现成大语言模型在代理处理输入前检测并移除潜在注入提示。

Result: 在AgentDojo基准测试中，PromptArmor误报率和漏报率低于1%，移除注入提示后攻击成功率降至1%以下，对自适应攻击也有效。

Conclusion: 建议将PromptArmor作为评估新的提示注入攻击防御方法的标准基线。

Abstract: Despite their potential, recent research has demonstrated that LLM agents are
vulnerable to prompt injection attacks, where malicious prompts are injected
into the agent's input, causing it to perform an attacker-specified task rather
than the intended task provided by the user. In this paper, we present
PromptArmor, a simple yet effective defense against prompt injection attacks.
Specifically, PromptArmor prompts an off-the-shelf LLM to detect and remove
potential injected prompts from the input before the agent processes it. Our
results show that PromptArmor can accurately identify and remove injected
prompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves
both a false positive rate and a false negative rate below 1% on the AgentDojo
benchmark. Moreover, after removing injected prompts with PromptArmor, the
attack success rate drops to below 1%. We also demonstrate PromptArmor's
effectiveness against adaptive attacks and explore different strategies for
prompting an LLM. We recommend that PromptArmor be adopted as a standard
baseline for evaluating new defenses against prompt injection attacks.

</details>


### [328] [PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants](https://arxiv.org/abs/2507.15393)
*Ruofan Liu,Yun Lin,Silas Yeo Shuen Yu,Xiwen Teoh,Zhenkai Liang,Jin Song Dong*

Main category: cs.CR

TL;DR: 钓鱼邮件威胁大，传统检测方法失效，研究用大语言模型生成钓鱼邮件能绕过多数检测器，提出PiMRef检测器，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统规则和特征工程检测器对不断演变的钓鱼邮件失效，大语言模型加剧威胁，需新检测方法。

Method: 提出PiMRef，将钓鱼检测重构为身份事实核查任务，提取发件人身份、验证域名合法性、检测行动呼吁提示。

Result: 在标准基准上比现有方法提高8.8%精度且不损失召回率，在真实评估中精度92.1%、召回率87.9%、中位运行时间0.05s。

Conclusion: PiMRef在检测钓鱼邮件上比现有技术更有效和高效。

Abstract: Phishing emails are a critical component of the cybercrime kill chain due to
their wide reach and low cost. Their ever-evolving nature renders traditional
rule-based and feature-engineered detectors ineffective in the ongoing arms
race between attackers and defenders. The rise of large language models (LLMs)
further exacerbates the threat, enabling attackers to craft highly convincing
phishing emails at minimal cost.
  This work demonstrates that LLMs can generate psychologically persuasive
phishing emails tailored to victim profiles, successfully bypassing nearly all
commercial and academic detectors. To defend against such threats, we propose
PiMRef, the first reference-based phishing email detector that leverages
knowledge-based invariants. Our core insight is that persuasive phishing emails
often contain disprovable identity claims, which contradict real-world facts.
PiMRef reframes phishing detection as an identity fact-checking task. Given an
email, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the
legitimacy of the sender's domain against a predefined knowledge base, and
(iii) detects call-to-action prompts that push user engagement. Contradictory
claims are flagged as phishing indicators and serve as human-understandable
explanations.
  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,
PiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks
like Nazario and PhishPot. In a real-world evaluation of 10,183 emails across
five university accounts over three years, PiMRef achieved 92.1% precision,
87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art
in both effectiveness and efficiency.

</details>


### [329] [Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems](https://arxiv.org/abs/2507.15613)
*Andrii Balashov,Olena Ponomarova,Xiaohua Zhai*

Main category: cs.CR

TL;DR: 研究企业大语言模型多阶段提示推理攻击，提出多种防御方法，强调全面安全视角。


<details>
  <summary>Details</summary>
Motivation: 企业大语言模型面临提示推理攻击等新安全挑战，需研究应对方法。

Method: 模拟攻击场景，建立形式化威胁模型，用概率理论等分析；提出统计异常检测等防御方法，进行数学分析和实验模拟。

Result: 攻击能可靠窃取敏感信息；各防御方法有数学分析或实验支持，如差分隐私训练信息泄漏边界、高AUC异常检测方法等。

Conclusion: 企业大语言模型安全需超越单轮提示过滤，采用多阶段整体视角。

Abstract: Large Language Models (LLMs) deployed in enterprise settings (e.g., as
Microsoft 365 Copilot) face novel security challenges. One critical threat is
prompt inference attacks: adversaries chain together seemingly benign prompts
to gradually extract confidential data. In this paper, we present a
comprehensive study of multi-stage prompt inference attacks in an enterprise
LLM context. We simulate realistic attack scenarios where an attacker uses
mild-mannered queries and indirect prompt injections to exploit an LLM
integrated with private corporate data. We develop a formal threat model for
these multi-turn inference attacks and analyze them using probability theory,
optimization frameworks, and information-theoretic leakage bounds. The attacks
are shown to reliably exfiltrate sensitive information from the LLM's context
(e.g., internal SharePoint documents or emails), even when standard safety
measures are in place.
  We propose and evaluate defenses to counter such attacks, including
statistical anomaly detection, fine-grained access control, prompt sanitization
techniques, and architectural modifications to LLM deployment. Each defense is
supported by mathematical analysis or experimental simulation. For example, we
derive bounds on information leakage under differential privacy-based training
and demonstrate an anomaly detection method that flags multi-turn attacks with
high AUC. We also introduce an approach called "spotlighting" that uses input
transformations to isolate untrusted prompt content, reducing attack success by
an order of magnitude. Finally, we provide a formal proof of concept and
empirical validation for a combined defense-in-depth strategy. Our work
highlights that securing LLMs in enterprise settings requires moving beyond
single-turn prompt filtering toward a holistic, multi-stage perspective on both
attacks and defenses.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [330] [Simulation-Prior Independent Neural Unfolding Procedure](https://arxiv.org/abs/2507.15084)
*Anja Butter,Theo Heimel,Nathan Huetsch,Michael Kagan,Tilman Plehn*

Main category: hep-ph

TL;DR: 介绍了SPINUP方法可在LHC展开高维空间，且在展开探测器效应和到部分子层面展开有应用。


<details>
  <summary>Details</summary>
Motivation: 在LHC展开高维空间且避免分箱，提取不受模拟训练数据先验影响的展开分布。

Method: 提出SPINUP方法，基于神经网络编码正向映射，通过神经重要性采样提高效率，用集成方法估计正向过程信息损失影响。

Result: 展示了SPINUP方法在展开喷注子结构可观测量的探测器效应以及希格斯和单顶夸克产生到部分子层面展开的应用。

Conclusion: SPINUP方法为LHC高维空间展开提供了有效途径。

Abstract: Machine learning allows unfolding high-dimensional spaces without binning at
the LHC. The new SPINUP method extracts the unfolded distribution based on a
neural network encoding the forward mapping, making it independent of the prior
from the simulated training data. It is made efficient through neural
importance sampling, and ensembling can be used to estimate the effect of
information loss in the forward process. We showcase SPINUP for unfolding
detector effects on jet substructure observables and for unfolding to parton
level of associated Higgs and single-top production.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [331] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: 提出ObjectGS框架统一3D场景重建与语义理解，表现优于现有方法且能无缝集成应用。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting缺乏语义理解，限制对象级感知，需要统一3D场景重建与语义理解。

Method: 将单个对象建模为局部锚点生成神经高斯并共享对象ID，训练时动态调整锚点并优化特征，用one - hot ID编码和分类损失施加语义约束。

Result: 在开放词汇和全景分割任务上优于现有方法，能无缝集成到网格提取和场景编辑等应用中。

Conclusion: ObjectGS有效统一了3D场景重建与语义理解，具有良好性能和应用潜力。

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [332] [KinForm: Kinetics Informed Feature Optimised Representation Models for Enzyme $k_{cat}$ and $K_{M}$ Prediction](https://arxiv.org/abs/2507.14639)
*Saleh Alwer,Ronan Fleming*

Main category: q-bio.QM

TL;DR: 提出KinForm机器学习框架优化蛋白质特征表示以预测酶动力学参数，在两个基准数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 实验数据在规模和多样性上有限，以往预测酶动力学方法通常使用单一蛋白质语言模型的平均池化残基嵌入来表示蛋白质，需提高预测准确性和泛化能力。

Method: 结合多个残基级嵌入，基于每个残基的结合位点概率进行加权池化，用主成分分析进行降维，通过基于相似度的过采样策略重新平衡训练数据。

Result: KinForm在两个基准数据集上优于基线方法，在低序列相似性区间改进最明显，结合位点概率池化等操作有效果。

Conclusion: 去除折叠间的序列重叠能更真实评估泛化能力，应作为基准测试动力学预测模型的标准。

Abstract: Kinetic parameters such as the turnover number ($k_{cat}$) and Michaelis
constant ($K_{\mathrm{M}}$) are essential for modelling enzymatic activity but
experimental data remains limited in scale and diversity. Previous methods for
predicting enzyme kinetics typically use mean-pooled residue embeddings from a
single protein language model to represent the protein. We present KinForm, a
machine learning framework designed to improve predictive accuracy and
generalisation for kinetic parameters by optimising protein feature
representations. KinForm combines several residue-level embeddings
(Evolutionary Scale Modeling Cambrian, Evolutionary Scale Modeling 2, and
ProtT5-XL-UniRef50), taken from empirically selected intermediate transformer
layers and applies weighted pooling based on per-residue binding-site
probability. To counter the resulting high dimensionality, we apply
dimensionality reduction using principal--component analysis (PCA) on
concatenated protein features, and rebalance the training data via a
similarity-based oversampling strategy. KinForm outperforms baseline methods on
two benchmark datasets. Improvements are most pronounced in low sequence
similarity bins. We observe improvements from binding-site probability pooling,
intermediate-layer selection, PCA, and oversampling of low-identity proteins.
We also find that removing sequence overlap between folds provides a more
realistic evaluation of generalisation and should be the standard over random
splitting when benchmarking kinetic prediction models.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [333] [Geometric design of the tangent term in landing algorithms for orthogonality constraints](https://arxiv.org/abs/2507.15638)
*Florentin Goyens,P. -A. Absil,Florian Feppon*

Main category: math.OC

TL;DR: 提出一组全秩矩阵度量并应用于正交约束优化的着陆框架


<details>
  <summary>Details</summary>
Motivation: 在正交约束优化领域引入合适的度量

Method: 提出一组度量，是Stiefel流形上β - 度量的自然扩展

Result: 无明确提及

Conclusion: 无明确提及

Abstract: We propose a family a metrics over the set of full-rank $n\times p$ real
matrices, and apply them to the landing framework for optimization under
orthogonality constraints. The family of metrics we propose is a natural
extension of the $\beta$-metric, defined on the Stiefel manifold.

</details>


### [334] [On exploration of an interior mirror descent flow for stochastic nonconvex constrained problem](https://arxiv.org/abs/2507.15264)
*Kuangyu Ding,Kim-Chuan Toh*

Main category: math.OC

TL;DR: 研究非凸约束下非光滑非凸优化问题，提出黎曼次梯度流统一两种迭代优化方法，解释收敛问题，给出避免伪驻点条件和随机扰动策略，引入新迭代方法。


<details>
  <summary>Details</summary>
Motivation: 解决非凸约束下非光滑非凸优化问题，统一现有迭代优化方法并解释其收敛问题。

Method: 通过障碍函数诱导的黎曼度量得到黎曼次梯度流，探索其轨迹长期行为，给出避免伪驻点条件和随机扰动策略。

Result: 统一Hessian barrier方法和mirror descent方案，解释其收敛问题，给出避免伪驻点条件和随机扰动策略。

Conclusion: 引入两种迭代黎曼次梯度方法，推广现有方法用于解决非光滑非凸优化问题。

Abstract: We study a nonsmooth nonconvex optimization problem defined over nonconvex
constraints, where the feasible set is given by the intersection of the closure
of an open set and a smooth manifold. By endowing the open set with a
Riemannian metric induced by a barrier function, we obtain a Riemannian
subgradient flow formulated as a differential inclusion, which remains strictly
within the interior of the feasible set. This continuous dynamical system
unifies two classes of iterative optimization methods, namely the Hessian
barrier method and mirror descent scheme, by revealing that these methods can
be interpreted as discrete approximations of the continuous flow. We explore
the long-term behavior of the trajectories generated by this dynamical system
and show that the existing deficient convergence properties of the Hessian
barrier and mirror descent scheme can be unifily and more insightfully
interpreted through these of the continuous trajectory. For instance, the
notorious spurious stationary points \cite{chen2024spurious} observed in
Hessian barrier method and mirror descent scheme are interpreted as stable
equilibria of the dynamical system that do not correspond to real stationary
points of the original optimization problem. We provide two sufficient
condition such that these spurious stationary points can be avoided if the
strict complementarity conditions holds. In the absence of these regularity
condition, we propose a random perturbation strategy that ensures the
trajectory converges (subsequentially) to an approximate stationary point.
Building on these insights, we introduce two iterative Riemannian subgradient
methods, form of interior point methods, that generalizes the existing Hessian
barrier method and mirror descent scheme for solving nonsmooth nonconvex
optimization problems.

</details>


### [335] [Information Preserving Line Search via Bayesian Optimization](https://arxiv.org/abs/2507.15485)
*Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: math.OC

TL;DR: 提出基于贝叶斯优化的线搜索方法，在CUTEst测试集上表现优于现有方法且保证收敛。


<details>
  <summary>Details</summary>
Motivation: 传统线搜索方法在迭代区间细化过程中会丢弃函数值和梯度的有价值信息，需要改进。

Method: 提出基于贝叶斯优化的线搜索方法，保留并利用传统方法中丢弃的信息来选择步长。

Result: 在CUTEst测试集上的具有挑战性的无约束和边界约束优化问题的实证测试中，该方法表现优于现有方法。

Conclusion: 所提出的线搜索方法保证收敛，且性能优于现有方法。

Abstract: Line search is a fundamental part of iterative optimization methods for
unconstrained and bound-constrained optimization problems to determine suitable
step lengths that provide sufficient improvement in each iteration. Traditional
line search methods are based on iterative interval refinement, where valuable
information about function value and gradient is discarded in each iteration.
We propose a line search method via Bayesian optimization, preserving and
utilizing otherwise discarded information to improve step-length choices. Our
approach is guaranteed to converge and shows superior performance compared to
state-of-the-art methods based on empirical tests on the challenging
unconstrained and bound-constrained optimization problems from the CUTEst test
set.

</details>


### [336] [Multi-beam Beamforming in RIS-aided MIMO Subject to Reradiation Mask Constraints -- Optimization and Machine Learning Design](https://arxiv.org/abs/2507.15367)
*Shumin Wang,Hajar El Hassani,Marco Di Renzo,Marios Poulakis*

Main category: math.OC

TL;DR: 本文研究多用户RIS辅助MIMO通信系统中发射预编码矩阵和RIS相移向量的联合设计，提出多种优化方法，仿真验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 提高未来无线系统的频谱效率和降低功耗，研究多用户RIS辅助MIMO通信系统中发射预编码矩阵和RIS相移向量的联合设计。

Method: 构建最大最小优化问题，用Arimoto - Blahut算法简化可达速率，用交替优化方法将问题分解为QPQC子问题，开发基于模型的神经网络优化，用贪婪搜索算法解决离散相移优化问题。

Result: 所提方法能有效将多波束辐射方向图塑造成期望方向，满足再辐射掩模约束，神经网络设计减少执行时间，离散相移方案用四个相移级别时波束形成增益略有降低但性能良好。

Conclusion: 所提方法在多用户RIS辅助MIMO通信系统中能有效实现设计目标，有较好性能和效率。

Abstract: Reconfigurable intelligent surfaces (RISs) are an emerging technology for
improving spectral efficiency and reducing power consumption in future wireless
systems. This paper investigates the joint design of the transmit precoding
matrices and the RIS phase shift vector in a multi-user RIS-aided
multiple-input multiple-output (MIMO) communication system. We formulate a
max-min optimization problem to maximize the minimum achievable rate while
considering transmit power and reradiation mask constraints. The achievable
rate is simplified using the Arimoto-Blahut algorithm, and the problem is
broken into quadratic programs with quadratic constraints (QPQC) sub-problems
using an alternating optimization approach. To improve efficiency, we develop a
model-based neural network optimization that utilizes the one-hot encoding for
the angles of incidence and reflection. We address practical RIS limitations by
using a greedy search algorithm to solve the optimization problem for discrete
phase shifts. Simulation results demonstrate that the proposed methods
effectively shape the multi-beam radiation pattern towards desired directions
while satisfying reradiation mask constraints. The neural network design
reduces the execution time, and the discrete phase shift scheme performs well
with a small reduction of the beamforming gain by using only four phase shift
levels.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [337] [Enhancing Celestial Imaging: High Dynamic Range with Neuromorphic Cameras](https://arxiv.org/abs/2503.22814)
*Satyapreet Singh Yadav,Nirupam Roy,Chetan Singh Thakur*

Main category: astro-ph.IM

TL;DR: 传统基于帧的相机动态范围有限，神经形态相机有高动态范围优势，本文研究其在不同通量水平天体拍摄中的应用并举例展示优势。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机动态范围有限，拍摄亮度差异大的场景有饱和和细节丢失问题，神经形态相机有高动态范围优势，故研究其在天体拍摄中的应用。

Method: 未提及具体方法，通过举例（如土星及其卫星、天狼星A及其伴星天狼星B）展示优势。

Result: 展示了神经形态相机在拍摄不同亮度天体时能避免饱和效应，保留细节。

Conclusion: 神经形态成像技术在不同通量水平天体拍摄中有应用优势。

Abstract: Conventional frame-based cameras often struggle with limited dynamic range,
leading to saturation and loss of detail when capturing scenes with significant
brightness variations. Neuromorphic cameras, inspired by human retina, offer a
solution by providing an inherently high dynamic range. This capability enables
them to capture both bright and faint celestial objects without saturation
effects, preserving details across a wide range of luminosities. This paper
investigates the application of neuromorphic imaging technology for capturing
celestial bodies across a wide range of flux levels. Its advantages are
demonstrated through examples such as the bright planet Saturn with its faint
moons and the bright star Sirius A alongside its faint companion, Sirius B.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [338] [A Myhill-Nerode Type Characterization of 2detLIN Languages](https://arxiv.org/abs/2507.15316)
*Benedek Nagy*

Main category: cs.FL

TL;DR: 本文基于线性自动机模型刻画2detLIN语言类，使用前缀 - 后缀对，证明有限类刻画与2detLIN语言匹配且有约束条件。


<details>
  <summary>Details</summary>
Motivation: 基于已有的线性自动机及其确定性对应模型的性质，对2detLIN语言类进行刻画。

Method: 使用Myhill - Nerode类型的等价类，采用前缀 - 后缀对进行刻画。

Result: 证明有限类刻画与2detLIN语言匹配，但对前缀 - 后缀对有约束，即刻画需完整且无交叉对。

Conclusion: 可以用满足特定约束条件的前缀 - 后缀对的有限等价类来刻画2detLIN语言类。

Abstract: Linear automata are automata with two reading heads starting from the two
extremes of the input, are equivalent to 5' -> 3' Watson-Crick (WK) finite
automata. The heads read the input in opposite directions and the computation
finishes when the heads meet. These automata accept the class LIN of linear
languages. The deterministic counterpart of these models, on the one hand, is
less expressive, as only a proper subset of LIN, the class 2detLIN is accepted;
and on the other hand, they are also equivalent in the sense of the class of
the accepted languages. Now, based on these automata models, we characterize
the class of 2detLIN languages with a Myhill-Nerode type of equivalence
classes. However, as these automata may do the computation of both the prefix
and the suffix of the input, we use prefix-suffix pairs in our classes.
Additionally, it is proven that finitely many classes in the characterization
match with the 2detLIN languages, but we have some constraints on the used
prefix-suffix pairs, i.e., the characterization should have the property to be
complete and it must not have any crossing pairs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [339] [Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification](https://arxiv.org/abs/2506.23298)
*Xing Shen,Justin Szeto,Mingyang Li,Hengguan Huang,Tal Arbel*

Main category: eess.IV

TL;DR: 研究多模态大语言模型在医学图像分类少样本学习中的校准偏差和人口统计学不公平性，提出CALIN校准方法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在医学图像分析有潜力，但安全部署到临床实践需分析其预测准确性和校准误差，尤其是不同人口亚组间的情况。

Method: 提出CALIN推理时校准方法，用双层程序估计校准矩阵，在推理时校准预测置信度分数。

Result: 在三个医学成像数据集实验表明，CALIN能确保预测中公平的置信度校准，提高整体预测准确性，且公平性 - 实用性权衡最小。

Conclusion: CALIN方法有效，可解决多模态大语言模型在医学图像分类少样本学习中的校准偏差和人口统计学不公平问题。

Abstract: Multimodal large language models (MLLMs) have enormous potential to perform
few-shot in-context learning in the context of medical image analysis. However,
safe deployment of these models into real-world clinical practice requires an
in-depth analysis of the accuracies of their predictions, and their associated
calibration errors, particularly across different demographic subgroups. In
this work, we present the first investigation into the calibration biases and
demographic unfairness of MLLMs' predictions and confidence scores in few-shot
in-context learning for medical image classification. We introduce CALIN, an
inference-time calibration method designed to mitigate the associated biases.
Specifically, CALIN estimates the amount of calibration needed, represented by
calibration matrices, using a bi-level procedure: progressing from the
population level to the subgroup level prior to inference. It then applies this
estimation to calibrate the predicted confidence scores during inference.
Experimental results on three medical imaging datasets: PAPILA for fundus image
classification, HAM10000 for skin cancer classification, and MIMIC-CXR for
chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair
confidence calibration in its prediction, while improving its overall
prediction accuracies and exhibiting minimum fairness-utility trade-off. Our
codebase can be found at
https://github.com/xingbpshen/medical-calibration-fairness-mllm.

</details>


### [340] [MiDeSeC: A Dataset for Mitosis Detection and Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14271)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi,Zeynep Yildirim*

Main category: eess.IV

TL;DR: 创建MiDeSeC数据集，从25名患者的乳腺癌切片选取50个区域，含超500个有丝分裂，划分训练和测试集。


<details>
  <summary>Details</summary>
Motivation: 有丝分裂形状多样，需要大数据集覆盖所有情况。

Method: 从安卡拉大学医学病理科获取25名患者的H&E染色乳腺癌切片，用3D Histech Panoramic p250 Flash - 3扫描仪和Olympus BX50显微镜扫描，选取50个1024*1024像素的区域。

Result: 成功创建MiDeSeC数据集，50个区域中有超500个有丝分裂，划分了训练集和测试集。

Conclusion: 创建的数据集可用于后续相关研究。

Abstract: The MiDeSeC dataset is created through H&E stained invasive breast carcinoma,
no special type (NST) slides of 25 different patients captured at 40x
magnification from the Department of Medical Pathology at Ankara University.
The slides have been scanned by 3D Histech Panoramic p250 Flash-3 scanner and
Olympus BX50 microscope. As several possible mitosis shapes exist, it is
crucial to have a large dataset to cover all the cases. Accordingly, a total of
50 regions is selected from glass slides for 25 patients, each of regions with
a size of 1024*1024 pixels. There are more than 500 mitoses in total in these
50 regions. Two-thirds of the regions are reserved for training, the other
third for testing.

</details>


### [341] [NuSeC: A Dataset for Nuclei Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14272)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi*

Main category: eess.IV

TL;DR: 创建NuSeC数据集，含100张1024*1024图像，按75%训练集、25%测试集划分。


<details>
  <summary>Details</summary>
Motivation: 为方便研究人员使用NuSeC数据集开发方法时进行一致的对比分析。

Method: 从25位患者的切片中为每位选4张1024*1024图像构成数据集，随机选1张作测试集，其余作训练集。

Result: 训练集有75张图像约30000个细胞核结构，测试集有25张图像约6000个细胞核结构。

Conclusion: 成功划分NuSeC数据集为训练集和测试集以支持后续研究。

Abstract: The NuSeC dataset is created by selecting 4 images with the size of 1024*1024
pixels from the slides of each patient among 25 patients. Therefore, there are
a total of 100 images in the NuSeC dataset. To carry out a consistent
comparative analysis between the methods that will be developed using the NuSeC
dataset by the researchers in the future, we divide the NuSeC dataset 75% as
the training set and 25% as the testing set. In detail, an image is randomly
selected from 4 images of each patient among 25 patients to build the testing
set, and then the remaining images are reserved for the training set. While the
training set includes 75 images with around 30000 nuclei structures, the
testing set includes 25 images with around 6000 nuclei structures.

</details>


### [342] [QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems](https://arxiv.org/abs/2507.14760)
*Cassandra Tong Ye,Shamus Li,Tyler King,Kristina Monakhova*

Main category: eess.IV

TL;DR: 提出QUTCC技术用于图像回归任务，在去噪和MRI重建任务中能精准定位幻觉并实现更窄的不确定性区间。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在科学和医学逆问题中会产生幻觉，现有不确定性量化方法的边界较大、信息不足。

Method: 提出QUTCC技术，利用带分位数嵌入的U - Net架构预测分位数的全条件分布，校准中迭代查询网络获取上下分位数以细化边界。

Result: 在多个去噪任务和压缩MRI重建任务中，成功定位图像估计中的幻觉，比现有方法实现更窄的不确定性区间。

Conclusion: QUTCC技术能在保持相同统计覆盖率的同时，实现比现有方法更紧的不确定性估计。

Abstract: Deep learning models often hallucinate, producing realistic artifacts that
are not truly present in the sample. This can have dire consequences for
scientific and medical inverse problems, such as MRI and microscopy denoising,
where accuracy is more important than perceptual quality. Uncertainty
quantification techniques, such as conformal prediction, can pinpoint outliers
and provide guarantees for image regression tasks, improving reliability.
However, existing methods utilize a linear constant scaling factor to calibrate
uncertainty bounds, resulting in larger, less informative bounds. We propose
QUTCC, a quantile uncertainty training and calibration technique that enables
nonlinear, non-uniform scaling of quantile predictions to enable tighter
uncertainty estimates. Using a U-Net architecture with a quantile embedding,
QUTCC enables the prediction of the full conditional distribution of quantiles
for the imaging task. During calibration, QUTCC generates uncertainty bounds by
iteratively querying the network for upper and lower quantiles, progressively
refining the bounds to obtain a tighter interval that captures the desired
coverage. We evaluate our method on several denoising tasks as well as
compressive MRI reconstruction. Our method successfully pinpoints
hallucinations in image estimates and consistently achieves tighter uncertainty
intervals than prior methods while maintaining the same statistical coverage.

</details>


### [343] [Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection](https://arxiv.org/abs/2507.15151)
*Sebastian A. Cruz Romero,Wilfredo E. Lugo Beauchamp*

Main category: eess.IV

TL;DR: 本文探索用深度学习模型通过结膜苍白检测贫血，使用CP - AnemiC数据集，模型表现良好，评估不同量化方案对性能影响，建议进一步探索量化和硬件优化。


<details>
  <summary>Details</summary>
Motivation: 传统贫血检测方法需昂贵设备和专业知识，存在早期准确诊断障碍，需新方法。

Method: 使用MobileNet架构为骨干，通过数据增强和交叉验证策略端到端微调模型，对模型进行不同位宽的训练后量化。

Result: 模型在数据集上准确率0.9313、精度0.9374、F1分数0.9773；FP16量化能保持较高性能，INT8和INT4量化导致性能显著下降。

Conclusion: 支持进一步探索量化方案和硬件优化，评估移动医疗应用中模型大小、推理时间和诊断准确性的权衡。

Abstract: Anemia is a widespread global health issue, particularly among young children
in low-resource settings. Traditional methods for anemia detection often
require expensive equipment and expert knowledge, creating barriers to early
and accurate diagnosis. To address these challenges, we explore the use of deep
learning models for detecting anemia through conjunctival pallor, focusing on
the CP-AnemiC dataset, which includes 710 images from children aged 6-59
months. The dataset is annotated with hemoglobin levels, gender, age and other
demographic data, enabling the development of machine learning models for
accurate anemia detection. We use the MobileNet architecture as a backbone,
known for its efficiency in mobile and embedded vision applications, and
fine-tune our model end-to-end using data augmentation techniques and a
cross-validation strategy. Our model implementation achieved an accuracy of
0.9313, a precision of 0.9374, and an F1 score of 0.9773 demonstrating strong
performance on the dataset. To optimize the model for deployment on edge
devices, we performed post-training quantization, evaluating the impact of
different bit-widths (FP32, FP16, INT8, and INT4) on model performance.
Preliminary results suggest that while FP16 quantization maintains high
accuracy (0.9250), precision (0.9370), and F1 Score (0.9377), more aggressive
quantization (INT8 and INT4) leads to significant performance degradation.
Overall, our study supports further exploration of quantization schemes and
hardware optimizations to assess trade-offs between model size, inference time,
and diagnostic accuracy in mobile healthcare applications.

</details>


### [344] [A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT](https://arxiv.org/abs/2507.15193)
*Tanjin Taher Toma,Tejas Sudharshan Mathai,Bikash Santra,Pritam Mukherjee,Jianfei Liu,Wesley Jong,Darwish Alabyad,Vivek Batheja,Abhishek Jha,Mayank Patel,Darko Pucar,Jayadira del Rivero,Karel Pacak,Ronald M. Summers*

Main category: eess.IV

TL;DR: 研究系统评估解剖先验以提升基于深度学习的嗜铬细胞瘤（PCC）分割，Tumor + Kidney + Aorta（TKA）注释策略效果最佳，凸显解剖背景价值。


<details>
  <summary>Details</summary>
Motivation: 准确分割腹部CT扫描中的PCC对肿瘤负荷估计、预后和治疗规划至关重要，研究旨在评估解剖先验以改进分割。

Method: 采用nnU - Net框架评估11种注释策略，基于器官特定解剖先验引入多类方案，在105例CT扫描上训练测试，用DSC、NSD和F1分数衡量性能。

Result: TKA注释策略分割准确性最高，在DSC、NSD和F1分数上显著优于Tumor + Body（TB）注释，肿瘤负荷量化效果好，跨基因亚型分割强，五折交叉验证中表现稳健。

Conclusion: 在深度学习模型中纳入相关解剖背景对实现精确PCC分割有价值，支持临床评估和纵向监测。

Abstract: Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is
essential for tumor burden estimation, prognosis, and treatment planning. It
may also help infer genetic clusters, reducing reliance on expensive testing.
This study systematically evaluates anatomical priors to identify
configurations that improve deep learning-based PCC segmentation. We employed
the nnU-Net framework to evaluate eleven annotation strategies for accurate 3D
segmentation of pheochromocytoma, introducing a set of novel multi-class
schemes based on organ-specific anatomical priors. These priors were derived
from adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen,
kidney, aorta, adrenal gland, and pancreas), and were compared against a broad
body-region prior used in previous work. The framework was trained and tested
on 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center.
Performance was measured using Dice Similarity Coefficient (DSC), Normalized
Surface Distance (NSD), and instance-wise F1 score. Among all strategies, the
Tumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation
accuracy, significantly outperforming the previously used Tumor + Body (TB)
annotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84%
improvement at an IoU threshold of 0.5), measured on a 70-30 train-test split.
The TKA model also showed superior tumor burden quantification (R^2 = 0.968)
and strong segmentation across all genetic subtypes. In five-fold
cross-validation, TKA consistently outperformed TB across IoU thresholds (0.1
to 0.5), reinforcing its robustness and generalizability. These findings
highlight the value of incorporating relevant anatomical context in deep
learning models to achieve precise PCC segmentation, supporting clinical
assessment and longitudinal monitoring.

</details>


### [345] [EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro](https://arxiv.org/abs/2507.15292)
*An Wanga,Rulin Zhou,Mengya Xu,Yiru Ye,Longfei Gou,Yiting Chang,Hao Chen,Chwee Ming Lim,Jiankun Wang,Hongliang Ren*

Main category: eess.IV

TL;DR: 提出EndoControlMag框架用于可视化内镜手术中细微血管运动，评估显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 可视化内镜手术中细微血管运动对手术精度和决策至关重要，但因手术场景复杂动态而具挑战。

Method: 引入EndoControlMag框架，含PRR方案和HTM框架，PRR防止误差积累，HTM用预训练模型跟踪血管核心，有两种自适应软化策略。

Result: 在EndoVMM24数据集上评估，EndoControlMag在放大精度、视觉质量上显著优于现有方法，且在复杂手术条件下保持鲁棒性。

Conclusion: EndoControlMag是一种有效且鲁棒的内镜手术血管运动可视化方法。

Abstract: Visualizing subtle vascular motions in endoscopic surgery is crucial for
surgical precision and decision-making, yet remains challenging due to the
complex and dynamic nature of surgical scenes. To address this, we introduce
EndoControlMag, a training-free, Lagrangian-based framework with
mask-conditioned vascular motion magnification tailored to endoscopic
environments. Our approach features two key modules: a Periodic Reference
Resetting (PRR) scheme that divides videos into short overlapping clips with
dynamically updated reference frames to prevent error accumulation while
maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification
(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores
using a pretrained visual tracking model to maintain accurate localization
despite occlusions and view changes. It then applies one of two adaptive
softening strategies to surrounding tissues: motion-based softening that
modulates magnification strength proportional to observed tissue displacement,
or distance-based exponential decay that simulates biomechanical force
attenuation. This dual-mode approach accommodates diverse surgical
scenarios-motion-based softening excels with complex tissue deformations while
distance-based softening provides stability during unreliable optical flow
conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four
different surgery types and various challenging scenarios, including
occlusions, instrument disturbance, view changes, and vessel deformations.
Quantitative metrics, visual assessments, and expert surgeon evaluations
demonstrate that EndoControlMag significantly outperforms existing methods in
both magnification accuracy and visual quality while maintaining robustness
across challenging surgical conditions. The code, dataset, and video results
are available at https://szupc.github.io/EndoControlMag/.

</details>


### [346] [MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis](https://arxiv.org/abs/2507.15340)
*Marc Boubnovski Martell,Kristofer Linton-Reid,Mitchell Chen,Sumeet Hindocha,Benjamin Hunter,Marco A. Calzado,Richard Lee,Joram M. Posma,Eric O. Aboagye*

Main category: eess.IV

TL;DR: 提出基于Transformer的超分辨率框架TVSRN - V2用于临床肺部CT分析，在多任务评估中表现良好，可提升临床决策支持。


<details>
  <summary>Details</summary>
Motivation: 高分辨率肺部CT受辐射剂量和硬件成本限制，需要更好的超分辨率方法用于临床分析。

Method: 构建包含Through - Plane Attention Blocks和Swin Transformer V2的TVSRN - V2模型，在多临床队列的三项肺癌任务上评估，引入伪低分辨率增强方法。

Result: 在分割准确性、放射组学特征可重复性和预测性能上有显著提升。

Conclusion: TVSRN - V2是适用于现实CT工作流程中剂量高效成像和定量分析的临床可行系统。

Abstract: High-resolution volumetric computed tomography (CT) is essential for accurate
diagnosis and treatment planning in thoracic diseases; however, it is limited
by radiation dose and hardware costs. We present the Transformer Volumetric
Super-Resolution Network (\textbf{TVSRN-V2}), a transformer-based
super-resolution (SR) framework designed for practical deployment in clinical
lung CT analysis. Built from scalable components, including Through-Plane
Attention Blocks (TAB) and Swin Transformer V2 -- our model effectively
reconstructs fine anatomical details in low-dose CT volumes and integrates
seamlessly with downstream analysis pipelines. We evaluate its effectiveness on
three critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis
-- across multiple clinical cohorts. To enhance robustness across variable
acquisition protocols, we introduce pseudo-low-resolution augmentation,
simulating scanner diversity without requiring private data. TVSRN-V2
demonstrates a significant improvement in segmentation accuracy (+4\% Dice),
higher radiomic feature reproducibility, and enhanced predictive performance
(+0.06 C-index and AUC). These results indicate that SR-driven recovery of
structural detail significantly enhances clinical decision support, positioning
TVSRN-V2 as a well-engineered, clinically viable system for dose-efficient
imaging and quantitative analysis in real-world CT workflows.

</details>


### [347] [Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation](https://arxiv.org/abs/2507.15361)
*Muhammad Aqeel,Maham Nazir,Zanxi Ruan,Francesco Setti*

Main category: eess.IV

TL;DR: 提出SynDiff框架结合文本引导合成数据生成与高效扩散分割，解决医学图像分割数据稀缺问题，在CVC - ClinicDB上效果好且适合临床部署。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割存在数据稀缺问题，尤其是息肉检测中注释需要专业知识。

Method: 采用潜在扩散模型通过文本条件修复生成临床逼真的合成息肉，引入直接潜在估计实现单步推理。

Result: 在CVC - ClinicDB上达到96.0%的Dice和92.9%的IoU，保持实时性适合临床部署。

Conclusion: 可控的合成增强可提高分割鲁棒性且无分布偏移，SynDiff为资源有限的医疗环境提供高效解决方案。

Abstract: Medical image segmentation suffers from data scarcity, particularly in polyp
detection where annotation requires specialized expertise. We present SynDiff,
a framework combining text-guided synthetic data generation with efficient
diffusion-based segmentation. Our approach employs latent diffusion models to
generate clinically realistic synthetic polyps through text-conditioned
inpainting, augmenting limited training data with semantically diverse samples.
Unlike traditional diffusion methods requiring iterative denoising, we
introduce direct latent estimation enabling single-step inference with T x
computational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%
IoU while maintaining real-time capability suitable for clinical deployment.
The framework demonstrates that controlled synthetic augmentation improves
segmentation robustness without distribution shift. SynDiff bridges the gap
between data-hungry deep learning models and clinical constraints, offering an
efficient solution for deployment in resourcelimited medical settings.

</details>


### [348] [RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation](https://arxiv.org/abs/2507.15524)
*Simon Winther Albertsen,Hjalte Svaneborg Bjørnstrup,Mostafa Mehdipour Ghazi*

Main category: eess.IV

TL;DR: 提出RARE - UNet解决现有模型处理低分辨率数据时性能下降问题，在脑成像分割任务表现优，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有模型假设固定高分辨率输入，在现实低分辨率数据场景下性能显著下降，需解决此限制。

Method: 提出RARE - UNet，包含多尺度块、分辨率感知路由机制和一致性驱动训练。

Result: 在两个脑成像分割任务上，相比标准UNet等模型，平均Dice分数最高，低分辨率下性能稳定且推理时间显著减少。

Conclusion: RARE - UNet在实现分辨率鲁棒分割方面有效且可扩展。

Abstract: Accurate segmentation is crucial for clinical applications, but existing
models often assume fixed, high-resolution inputs and degrade significantly
when faced with lower-resolution data in real-world scenarios. To address this
limitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation
architecture that dynamically adapts its inference path to the spatial
resolution of the input. Central to our design are multi-scale blocks
integrated at multiple encoder depths, a resolution-aware routing mechanism,
and consistency-driven training that aligns multi-resolution features with
full-resolution representations. We evaluate RARE-UNet on two benchmark brain
imaging tasks for hippocampus and tumor segmentation. Compared to standard
UNet, its multi-resolution augmented variant, and nnUNet, our model achieves
the highest average Dice scores of 0.84 and 0.65 across resolution, while
maintaining consistent performance and significantly reduced inference time at
lower resolutions. These results highlight the effectiveness and scalability of
our architecture in achieving resolution-robust segmentation. The codes are
available at: https://github.com/simonsejse/RARE-UNet.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [349] [All-atom inverse protein folding through discrete flow matching](https://arxiv.org/abs/2507.14156)
*Kai Yi,Kiarash Jamali,Sjors H. W. Scheres*

Main category: q-bio.BM

TL;DR: 介绍ADFLIP模型用于逆蛋白折叠设计，在单结构和多结构逆折叠任务中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有逆折叠方法难以预测含非蛋白质成分复合物的序列，且对多结构状态复合物效果差，需新方法。

Method: 提出基于离散流匹配的生成模型ADFLIP，生成序列时逐步纳入预测的氨基酸侧链，通过多结构状态集合采样设计动态蛋白复合物，实现免训练分类器引导采样。

Result: 在含小分子配体、核苷酸或金属离子的蛋白复合物上评估，在单结构和多结构逆折叠任务中达SOTA。

Conclusion: ADFLIP在全原子蛋白质设计方面有很大潜力，代码开源。

Abstract: The recent breakthrough of AlphaFold3 in modeling complex biomolecular
interactions, including those between proteins and ligands, nucleotides, or
metal ions, creates new opportunities for protein design. In so-called inverse
protein folding, the objective is to find a sequence of amino acids that adopts
a target protein structure. Many inverse folding methods struggle to predict
sequences for complexes that contain non-protein components, and perform poorly
with complexes that adopt multiple structural states. To address these
challenges, we present ADFLIP (All-atom Discrete FLow matching Inverse Protein
folding), a generative model based on discrete flow-matching for designing
protein sequences conditioned on all-atom structural contexts. ADFLIP
progressively incorporates predicted amino acid side chains as structural
context during sequence generation and enables the design of dynamic protein
complexes through ensemble sampling across multiple structural states.
Furthermore, ADFLIP implements training-free classifier guidance sampling,
which allows the incorporation of arbitrary pre-trained models to optimise the
designed sequence for desired protein properties. We evaluated the performance
of ADFLIP on protein complexes with small-molecule ligands, nucleotides, or
metal ions, including dynamic complexes for which structure ensembles were
determined by nuclear magnetic resonance (NMR). Our model achieves
state-of-the-art performance in single-structure and multi-structure inverse
folding tasks, demonstrating excellent potential for all-atom protein design.
The code is available at https://github.com/ykiiiiii/ADFLIP.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [350] [Learning to Communicate in Multi-Agent Reinforcement Learning for Autonomous Cyber Defence](https://arxiv.org/abs/2507.14658)
*Faizan Contractor,Li Li,Ranwa Al Mallah*

Main category: cs.MA

TL;DR: 提出在网络作战环境中让防御代理学习通信和防御网络威胁的游戏设计，代理能学习类似人类专家的策略并同时学习最小成本通信消息。


<details>
  <summary>Details</summary>
Motivation: 现有合作多智能体强化学习方法在部分可观测环境中执行时让智能体独立行动，限制了训练策略的协调效果，而有效通信可改善网络作战决策。

Method: 提出游戏设计，让防御代理在网络作战研究健身房中玩训练游戏，使用适应网络作战环境的可微智能体间学习算法。

Result: 自主代理学习到的战术策略类似于人类专家应对网络威胁时的策略，且在学习防御策略时同时学习最小成本通信消息。

Conclusion: 该方法可使代理在网络作战中学习有效通信和防御策略，应对网络威胁。

Abstract: Popular methods in cooperative Multi-Agent Reinforcement Learning with
partially observable environments typically allow agents to act independently
during execution, which may limit the coordinated effect of the trained
policies. However, by sharing information such as known or suspected ongoing
threats, effective communication can lead to improved decision-making in the
cyber battle space. We propose a game design where defender agents learn to
communicate and defend against imminent cyber threats by playing training games
in the Cyber Operations Research Gym, using the Differentiable Inter Agent
Learning algorithm adapted to the cyber operational environment. The tactical
policies learned by these autonomous agents are akin to those of human experts
during incident responses to avert cyber threats. In addition, the agents
simultaneously learn minimal cost communication messages while learning their
defence tactical policies.

</details>


### [351] [LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra](https://arxiv.org/abs/2507.15815)
*Seth Karten,Wenzhe Li,Zihan Ding,Samuel Kleiner,Yu Bai,Chi Jin*

Main category: cs.MA

TL;DR: 提出LLM Economist框架，用基于代理的建模设计和评估经济政策，实验表明能改善社会福利，为政策评估提供测试床。


<details>
  <summary>Details</summary>
Motivation: 设计一个能在分层决策的战略环境中设计和评估经济政策的框架。

Method: 在下层，用基于美国人口普查数据的受限理性工人代理选择劳动力供应；上层，规划代理用上下文强化学习提出分段线性边际税率表。

Result: 规划者收敛于接近Stackelberg均衡，提高了社会福利，周期性投票程序进一步提升了收益。

Conclusion: 基于大语言模型的代理可以联合建模、模拟和治理复杂经济系统，为社会层面的政策评估提供了可行测试床。

Abstract: We present the LLM Economist, a novel framework that uses agent-based
modeling to design and assess economic policies in strategic environments with
hierarchical decision-making. At the lower level, bounded rational worker
agents -- instantiated as persona-conditioned prompts sampled from U.S.
Census-calibrated income and demographic statistics -- choose labor supply to
maximize text-based utility functions learned in-context. At the upper level, a
planner agent employs in-context reinforcement learning to propose
piecewise-linear marginal tax schedules anchored to the current U.S. federal
brackets. This construction endows economic simulacra with three capabilities
requisite for credible fiscal experimentation: (i) optimization of
heterogeneous utilities, (ii) principled generation of large, demographically
realistic agent populations, and (iii) mechanism design -- the ultimate nudging
problem -- expressed entirely in natural language. Experiments with populations
of up to one hundred interacting agents show that the planner converges near
Stackelberg equilibria that improve aggregate social welfare relative to Saez
solutions, while a periodic, persona-level voting procedure furthers these
gains under decentralized governance. These results demonstrate that large
language model-based agents can jointly model, simulate, and govern complex
economic systems, providing a tractable test bed for policy evaluation at the
societal scale to help build better civilizations.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [352] [The Electoral Consequences of Natural Disasters: A Dynamic Fixed-Effects Analysis](https://arxiv.org/abs/2507.14331)
*Nima Taheri Hosseinkhani*

Main category: econ.GN

TL;DR: 本文研究自然灾害对美国市长选举的影响，发现灾害影响取决于发生时间，还影响选民投票率和官员竞选意愿，调和了回顾性投票理论。


<details>
  <summary>Details</summary>
Motivation: 现有文献对自然灾害政治后果观点不一，本文旨在研究其对现任市长选举、选举动态和官员政治野心的影响。

Method: 利用1989 - 2021年超10000个美国市长竞选观察数据，结合灾害事件登记信息，采用动态双向固定效应事件研究设计，并进行预趋势和安慰剂测试。

Result: 选举同期灾害使现任者选票增加超6个百分点，灾害降低投票率1.4个百分点，处理灾害使现任者再次竞选可能性增加12个百分点。

Conclusion: 研究调和了回顾性投票理论，指出选民短视和显著性的关键作用，揭示危机塑造政治生涯的新途径，表明灾害管理是治理考验和政治野心催化剂。

Abstract: With the increasing frequency of major natural disasters, understanding their
political consequences is of paramount importance for democratic
accountability. The existing literature is deeply divided, with some studies
finding that voters punish incumbents for disaster-related damages, while
others find they reward them for relief efforts. This paper investigates the
electoral consequences of natural disasters for incumbent mayors, broader
electoral dynamics, and the long-term political ambition of officeholders. The
study leverages a comprehensive panel dataset of over 10,000 candidate-election
observations in U.S. mayoral races from 1989 to 2021, combining detailed
election data with a global registry of disaster events. To identify causal
effects, the analysis employs a robust dynamic two-way fixed-effects
event-study design, validated by extensive pre-trend and placebo tests. The
findings reveal that the electoral impact of disasters is highly conditional on
their timing. A disaster that strikes in the same quarter as an election
provides a significant electoral boost to incumbents, increasing their vote
share by over 6 percentage points. However, disasters consistently suppress
voter turnout, reducing it by an average of 1.4 percentage points. In a novel
finding, the analysis demonstrates that the experience of managing a disaster
significantly increases an incumbent's likelihood of seeking re-election in the
subsequent cycle by as much as 12 percentage points. These findings help
reconcile conflicting theories of retrospective voting by highlighting the
critical role of voter myopia and salience. They also reveal a previously
undocumented channel through which crises shape political careers, suggesting
that disaster management is not only a test of governance but also a catalyst
for political ambition. [The current version is a preprint.]

</details>


### [353] [The effects of temperature and rainfall anomalies on Mexican inflation](https://arxiv.org/abs/2507.14420)
*Arango-Castillo Lenin,Martínez-Ramírez Francisco*

Main category: econ.GN

TL;DR: 本文用区域面板数据衡量温度和降水冲击对墨西哥通胀的影响，发现短期气候变量无统计效应，长期仅降水有统计效应。


<details>
  <summary>Details</summary>
Motivation: 衡量气候冲击对墨西哥通胀的长期和短期影响。

Method: 估计面板自回归分布滞后模型（panel ARDL）衡量长期影响，用面板局部投影估计脉冲响应函数衡量短期影响。

Result: 短期气候变量对墨西哥通胀无统计效应，长期仅降水规范有统计效应，气温规范无统计影响，高于正常水平的降水对墨西哥所有项目通胀有积极且显著影响。

Conclusion: 温度和降水冲击对墨西哥通胀的影响在短期和长期存在差异，长期中降水对通胀有显著作用。

Abstract: This paper measures the effects of temperature and precipitation shocks on
Mexican inflation using a regional panel. To measure the long-term inflationary
effects of climate shocks, we estimate a panel autoregressive distributed lag
model (panel ARDL) of the quarterly variation of the price index against the
population-weighted temperature and precipitation deviations from their
historical norm, computed using the 30-year moving average. In addition, we
measure the short-term effects of climate shocks by estimating impulse response
functions using panel local projections. The result indicates that, in the
short term, the climate variables have no statistical effect on Mexican
inflation. However, in the long term, only precipitation norms have a
statistical effect, and the temperature norms have no statistical impact.
Higher than normal precipitation has a positive and statistically significant
effect on Mexican inflation for all items.

</details>


### [354] [Does Private Equity Hurt or Improve Healthcare Value? New Evidence and Mechanisms](https://arxiv.org/abs/2507.14717)
*Minghong Yuan,Wen Wen,Indranil Bardhan*

Main category: econ.GN

TL;DR: 研究私募股权投资对医疗价值的影响，发现投资后医院医疗价值下降，但信息技术支持的信息共享可缓解，不同信息共享类型作用有别，强调政策和数据标准促进信息共享的重要性。


<details>
  <summary>Details</summary>
Motivation: 近年来私募股权公司在医疗领域投资巨大，利益相关者需了解投资影响及是否符合自身利益。

Method: 采用交错双重差分法，使用2008 - 2020年美国医院数据。

Result: 私募股权投资后医院整体医疗价值下降；信息技术支持的信息共享有调节作用，医院与门诊护理提供者间信息共享更能提升护理质量；信息共享可降低感染和再入院率、提高劳动生产率。

Conclusion: 应制定政策和通用数据标准，促进医疗服务提供者间信息共享，使私募股权公司激励与价值医疗目标一致。

Abstract: What is the impact of private equity (PE) investment on healthcare value?
Does PE investment hurt or improve healthcare value, and if so, can its effect
be mitigated through the use of health information technologies (IT)? Given the
significant investments by PE firms in the healthcare sector in recent years,
these are important research questions. Stakeholders, including policy makers,
care providers, and patients, need to understand their likely impact and
whether PE ownership is aligned with their interests. Using a staggered
difference-in-differences approach and data from US hospitals from 2008-2020,
we observe that the overall value of healthcare delivered by hospitals declines
after PE investment. However, our empirical evidence reveals that IT-enabled,
health information sharing plays an important moderating role. Hospitals with
stronger information-sharing capabilities exhibit greater cost efficiencies and
improvements in care quality, leading to higher healthcare value after PE
investment. Furthermore, we find that the type of health information sharing
matters. Specifically, we observe that improvements in care quality are
primarily driven by information sharing between hospitals and ambulatory care
providers, instead of simply hospital-to-hospital sharing of patient health
data. Our research also identifies the underlying mechanisms through which
health information sharing improves care value by reducing hospital-acquired
infections and readmission rates, thereby improving care quality, and enhancing
labor productivity by reducing operating costs. Our results highlight the
critical role of policies and common data standards needed to promote
IT-enabled information sharing between healthcare providers, which, in turn,
can align incentives of PE firms with the goals of value-based care.

</details>


### [355] [Mitigating Financial Frictions in Agriculture: A Framework for Stablecoin Adoption](https://arxiv.org/abs/2507.14970)
*Xinyu Li*

Main category: econ.GN

TL;DR: 论文分析法币抵押稳定币解决全球农业金融摩擦问题，建模型展示其优势，分析用例与障碍，认为虽非万能但有潜力。


<details>
  <summary>Details</summary>
Motivation: 全球农业长期受价格波动、信贷受限和供应链低效等金融摩擦阻碍，需寻找解决办法。

Method: 建立包含交易成本和信贷约束的农场层面利润最大化模型，分析关键用例并考虑采用障碍。

Result: 稳定币可降低跨境贸易成本和风险、提高供应链金融效率和透明度、扩大小农户信贷渠道。

Conclusion: 稳定币虽非万能，但有潜力引发农业经济范式转变，需进一步实证研究和政策支持。

Abstract: Persistent financial frictions - including price volatility, constrained
credit access, and supply chain inefficiencies - have long hindered
productivity and welfare in the global agricultural sector. This paper provides
a theoretical and applied analysis of how fiat-collateralized stablecoins, a
class of digital currency pegged to a stable asset like the U.S. dollar, can
address these long-standing challenges. We develop a farm-level profit
maximization model incorporating transaction costs and credit constraints to
demonstrate how stablecoins can enhance economic outcomes by (1) reducing the
costs and risks of cross-border trade, (2) improving the efficiency and
transparency of supply chain finance through smart contracts, and (3) expanding
access to credit for smallholder farmers. We analyze key use cases, including
parametric insurance and trade finance, while also considering the significant
hurdles to adoption, such as regulatory uncertainty and the digital divide. The
paper concludes that while not a panacea, stablecoins represent a significant
financial technology with the potential to catalyze a paradigm shift in
agricultural economics, warranting further empirical investigation and policy
support.

</details>


### [356] [Equity, Emissions and the Inflation Reduction Act](https://arxiv.org/abs/2507.15054)
*Lucas Woodley,Chung Yi See,Daniel Palmer,Ashley Nunes*

Main category: econ.GN

TL;DR: 研究2022年通胀削减法案二手电动汽车购买激励措施对低收入家庭的有效性，发现部分家庭可能因采购途径无资格参与，且影响减排效益。


<details>
  <summary>Details</summary>
Motivation: 低收入家庭购买二手汽车比例高且采用电动汽车能节省运营成本，但此前难以购买，研究通胀削减法案二手电动汽车购买激励措施的有效性。

Method: 利用美国人口普查局、国家家庭出行调查和相关技术模型的数据进行研究。

Result: 1. 多达840万低收入家庭可能因车辆采购途径异质性无资格参与激励；2. 计划无资格可能导致多达1.139亿吨生命周期减排效益无法实现；3. 采购途径取决于车辆价格，激励对高价车更有效。

Conclusion: 探讨了研究结果对脱碳努力和能源政策的影响。

Abstract: Preowned vehicles are disproportionally purchased by low-income households, a
group that has long been unable to purchase electric vehicles. Yet, low-income
households would disproportionally benefit from EV adoption given the operating
costs savings offered by electrification. To help realize this benefit,
provisions of the 2022 Inflation Reduction Act offer preowned EV purchasing
incentives. How effective might these efforts be. Leveraging data from the
United States Census Bureau, the National Household Travel Survey, and the
Greenhouse gases, Regulated Emissions, and Energy use in Technologies Model, we
address this question. Our findings are fourfold. First, we demonstrate that
although low-income households are more likely to benefit from preowned EV
purchasing incentives offered by IRA, up to 8.4 million low-income households
may be ineligible owing to heterogeneity in vehicle procurement pathways.
Second, we show that program ineligibility risks preventing up to 113.9 million
tons in lifecycle emissions reduction benefits from being realized. Third, we
find that procurement pathways depend on vehicle price. More expensive preowned
vehicles are purchased directly from commercial dealers, while less expensive
preowned vehicles are purchased from private sellers. These procurement
pathways matter because qualification for IRA incentives necessitates
purchasing solely from commercial dealers. Fourth, we demonstrate that while
incentives motivating preowned vehicle purchases from commercial dealers may be
effective if the vehicle is expensive, this effectiveness diminishes at higher
price points. The implications of our findings on decarbonization efforts and
energy policy are discussed.

</details>


### [357] [Enumerating the technological viability and climate impact of jet electrification](https://arxiv.org/abs/2507.15075)
*Megan Yeo,Sebastian Nosenzo,Sichen Shawn Chao,Ashley Nunes*

Main category: econ.GN

TL;DR: 本文评估模型探讨短途电动飞行可行性，发现当前能量密度限制阻碍其发展，部分大型飞机更适合电气化，且电气化的区域效益差异大。


<details>
  <summary>Details</summary>
Motivation: 现有论述强调用小型、轻型飞机实现短途航线电气化，本文旨在探讨这种强调是否合理。

Method: 估计一个模型来解决相关问题。

Result: 1. 当前能量密度限制阻碍短途电动飞行；2. 小型、轻型飞机电气化更具挑战；3. 部分大型、重型飞机更适合电气化，可减少大量二氧化碳排放；4. 电气化的区域效益差异大，欧洲减排效益最大，亚洲部分地区会增加碳排放。

Conclusion: 讨论了这些研究结果对脱碳政策的影响。

Abstract: Enabling battery technology has not achieved sufficient maturity to
facilitate electric flight for all aircraft models across all distances.
Consequently, existing discourse emphasizes electrifying short haul routes
using smaller, lighter aircraft. Does this emphasis have merit. We estimate a
model that addresses this question. Our findings are fourfold. First, we find
that current energy density limitations impede short haul electric flight,
regardless of aircraft model utilized. Second, we document that electrifying
smaller, lighter aircraft models serving short haul routes may be particularly
challenging as these aircraft require more, not less, acute increases in energy
density. Third, we identify a subset of larger, heavier aircraft as better
candidates for electrification and note that doing so could prevent the annual
release of significant amounts of carbon dioxide equivalent. However, we
observe that the regional benefits of electrification are highly heterogeneous.
The largest emissions benefit is realized in Europe, followed by South America,
North America, Oceania and Africa. Electrification flights originating in Asia
produces a net increase in carbon emissions owing to the disproportionate share
of miles claimed by Asian countries with a more carbon intensive electrical
grid. Indian emissions warrant scrutiny, as its emissions contribution most
disproportionately exceeds its mileage contribution. The implications of these
findings for decarbonization policy are subsequently discussed.

</details>


### [358] [Human vs. Algorithmic Auditors: The Impact of Entity Type and Ambiguity on Human Dishonesty](https://arxiv.org/abs/2507.15439)
*Marius Protte,Behnud Mir Djawadi*

Main category: econ.GN

TL;DR: 研究人类在机器和人类验证不诚实陈述时的不诚实行为变化，以及验证过程的模糊性对不诚实行为的影响，发现透明规则下人机审核作弊程度无显著差异，模糊条件下机器验证作弊更严重，强调算法不透明的行为影响。


<details>
  <summary>Details</summary>
Motivation: 现有文献多关注人机交互中算法系统的咨询角色，缺乏对自动化系统监控或验证过程中人类行为的研究，本文旨在研究机器与人类检测不诚实陈述时人类不诚实行为的变化，以及验证过程模糊性对不诚实行为的影响。

Method: 设计激励实验室实验，采用修改的掷骰子范式，参与者私下观察随机结果并报告，设置不同验证实体（人 vs 机器）和验证过程模糊程度（透明 vs 模糊）的处理组。

Result: 透明验证规则下，人机审核作弊程度无显著差异；模糊条件下，机器验证时作弊程度显著更高，减少部分作弊，导致行为两极分化；比较模糊和透明规则下向机器实体报告的情况也是如此。

Conclusion: 算法不透明在验证场景中有行为影响，透明条件下机器可作为有效且成本效益高的审核者，但其黑箱性质与模糊验证过程可能会意外激励更严重的不诚实行为，对税务审计、合规和职场监控等自动化监督系统设计有实际意义。

Abstract: While most of the existing literature focused on human-machine interactions
with algorithmic systems in advisory roles, research on human behavior in
monitoring or verification processes that are conducted by automated systems
remains largely absent. Our study examines how human dishonesty changes when
detection of untrue statements is performed by machines versus humans, and how
ambiguity in the verification process influences dishonest behavior. We design
an incentivized laboratory experiment using a modified die-roll paradigm where
participants privately observe a random draw and report the result, with higher
reported numbers yielding greater monetary rewards. A probabilistic
verification process introduces risk of detection and punishment, with
treatments varying by verification entity (human vs. machine) and degree of
ambiguity in the verification process (transparent vs. ambiguous). Our results
show that under transparent verification rules, cheating magnitude does not
significantly differ between human and machine auditors. However, under
ambiguous conditions, cheating magnitude is significantly higher when machines
verify participants' reports, reducing the prevalence of partial cheating while
leading to behavioral polarization manifested as either complete honesty or
maximal overreporting. The same applies when comparing reports to a machine
entity under ambiguous and transparent verification rules. These findings
emphasize the behavioral implications of algorithmic opacity in verification
contexts. While machines can serve as effective and cost-efficient auditors
under transparent conditions, their black box nature combined with ambiguous
verification processes may unintentionally incentivize more severe dishonesty.
These insights have practical implications for designing automated oversight
systems in tax audits, compliance, and workplace monitoring.

</details>


### [359] [Explaining Apparently Inaccurate Self-assessments of Relative Performance: A Replication and Adaptation of 'Overconfident: Do you put your money on it?' by Hoelzl and Rustichini (2005)](https://arxiv.org/abs/2507.15568)
*Marius Protte*

Main category: econ.GN

TL;DR: 本文复制并改进Hoelzl和Rustichini (2005)实验，挑战原研究两种支付方案可比性，新实验结果更符合传统过度定位模式，为过度自信研究方法讨论做贡献。


<details>
  <summary>Details</summary>
Motivation: 挑战原研究中两种支付方案的可比性，重新评估基于选择的过度自信测量方法并探索错位效应的替代解释。

Method: 进行在线复制实验，将固定结果分布、成功概率相互依赖且获胜者数量无差异的彩票机制与原研究中概率结果分布的彩票机制对比。

Result: 结果更接近传统过度定位模式，近四分之三参与者选绩效支付方案，投票行为关键预测因素包括预期表现等，社会比较倾向和风险态度无显著作用，自我报告投票理由显示规范信念等有影响。

Conclusion: 研究有助于过度自信研究的方法论讨论，重新评估相关测量方法并探索替代解释。

Abstract: This study replicates and adapts the experiment of Hoelzl and Rustichini
(2005), which examined overplacement, i.e., overconfidence in relative
self-assessments, by analyzing individuals' voting preferences between a
performance-based and a lottery-based bonus payment mechanism. The original
study found underplacement - the majority of their sample apparently expected
to perform worse than others - in difficult tasks with monetary incentives,
contradicting the widely held assumption of a general human tendency toward
overconfidence. This paper challenges the comparability of the two payment
schemes, arguing that differences in outcome structures and non-monetary
motives may have influenced participants' choices beyond misconfidence. In an
online replication, a fixed-outcome distribution lottery mechanism with
interdependent success probabilities and no variance in the number of winners -
designed to better align with the performance-based payment scheme - is
compared against the probabilistic-outcome distribution lottery used in the
original study, which features an independent success probability and a
variable number of winners. The results align more closely with traditional
overplacement patterns than underplacement, as nearly three-fourths of
participants prefer the performance-based option regardless of lottery design.
Key predictors of voting behavior include expected performance, group
performance estimations, and sample question outcomes, while factors such as
social comparison tendencies and risk attitudes play no significant role.
Self-reported voting rationales highlight the influence of normative beliefs,
control preferences, and feedback signals beyond confidence. These results
contribute to methodological discussions in overconfidence research by
reassessing choice-based overconfidence measures and exploring alternative
explanations for observed misplacement effects.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [360] [The hunt for new pulsating ultraluminous X-ray sources: a clustering approach](https://arxiv.org/abs/2507.15032)
*Nicolò Oreste Pinciroli Vago,Roberta Amato,Matteo Imbrogno,GianLuca Israel,Andrea Belfiore,Konstantinos Kovlakas,Piero Fraternali,Mario Pasquato*

Main category: astro-ph.HE

TL;DR: 文章用AI方法从XMM - Newton探测的ULXs更新数据库中筛选新的候选PULXs，虽未发现新脉动但提供样本并强调高统计观测数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 在因不利因素未显示脉动的ULXs中找出候选PULXs。

Method: 对XMM - Newton探测的ULXs更新数据库应用AI方法，先使用无监督聚类算法分类，再用已知PULX观测样本设定分离阈值。

Result: 找到含85个独特源的新候选PULXs集群，初步定时分析未发现新脉动。

Conclusion: 体现AI方法的预测能力，强调高统计观测数据对揭示相干信号和验证方法稳健性的必要性。

Abstract: The discovery of fast and variable coherent signals in a handful of
ultraluminous X-ray sources (ULXs) testifies to the presence of super-Eddington
accreting neutron stars, and drastically changed the understanding of the ULX
class. Our capability of discovering pulsations in ULXs is limited, among
others, by poor statistics. However, catalogues and archives of high-energy
missions contain information which can be used to identify new candidate
pulsating ULXs (PULXs). The goal of this research is to single out candidate
PULXs among those ULXs which have not shown pulsations due to an unfavourable
combination of factors. We applied an AI approach to an updated database of
ULXs detected by XMM-Newton. We first used an unsupervised clustering algorithm
to sort out sources with similar characteristics into two clusters. Then, the
sample of known PULX observations has been used to set the separation threshold
between the two clusters and to identify the one containing the new candidate
PULXs. We found that only a few criteria are needed to assign the membership of
an observation to one of the two clusters. The cluster of new candidate PULXs
counts 85 unique sources for 355 observations, with $\sim$85% of these new
candidates having multiple observations. A preliminary timing analysis found no
new pulsations for these candidates. This work presents a sample of new
candidate PULXs observed by XMM-Newton, the properties of which are similar (in
a multi-dimensional phase space) to those of the known PULXs, despite the
absence of pulsations in their light curves. While this result is a clear
example of the predictive power of AI-based methods, it also highlights the
need for high-statistics observational data to reveal coherent signals from the
sources in this sample and thus validate the robustness of the approach.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [361] [Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings](https://arxiv.org/abs/2507.15118)
*Szymon Mazurek,Stephen Moore,Alessandro Crimi*

Main category: eess.SP

TL;DR: 提出基于图的深度学习框架，用低成本脑电图硬件检测癫痫，在尼日利亚和几内亚比绍的记录上测试，表现良好，为欠发达地区提供诊断支持。


<details>
  <summary>Details</summary>
Motivation: 低收入国家因神经科医生稀缺和诊断工具昂贵，癫痫诊断不足，需公平、可及的自动评估和可解释性方法检测癫痫。

Method: 将脑电图信号建模为时空图，用图注意力网络分类，调整GAT分析边，设计低质量记录的信号预处理和轻量级GAT架构，在Google Colab训练并部署到RaspberryPi设备。

Result: 该方法分类性能好，在准确性和多时段稳健性上优于随机森林和图卷积网络的标准分类器，还突出了额颞叶区域的特定连接。

Conclusion: 图注意力网络有潜力为欠发达地区癫痫诊断提供有洞察力和可扩展的支持，为可负担和可及的神经诊断工具铺平道路。

Abstract: Goal: Epilepsy remains under-diagnosed in low-income countries due to scarce
neurologists and costly diagnostic tools. We propose a graph-based deep
learning framework to detect epilepsy from low-cost Electroencephalography
(EEG) hardware, tested on recordings from Nigeria and Guinea-Bissau. Our focus
is on fair, accessible automatic assessment and explainability to shed light on
epilepsy biomarkers. Methods: We model EEG signals as spatio-temporal graphs,
classify them, and identify interchannel relationships and temporal dynamics
using graph attention networks (GAT). To emphasize connectivity biomarkers, we
adapt the inherently node-focused GAT to analyze edges. We also designed signal
preprocessing for low-fidelity recordings and a lightweight GAT architecture
trained on Google Colab and deployed on RaspberryPi devices. Results: The
approach achieves promising classification performance, outperforming a
standard classifier based on random forest and graph convolutional networks in
terms of accuracy and robustness over multiple sessions, but also highlighting
specific connections in the fronto-temporal region. Conclusions: The results
highlight the potential of GATs to provide insightful and scalable diagnostic
support for epilepsy in underserved regions, paving the way for affordable and
accessible neurodiagnostic tools.

</details>


### [362] [UniPhyNet: A Unified Network For Multimodal Physiological Raw Signal Classification](https://arxiv.org/abs/2507.14163)
*Renxiang Qiu,Raghavendra Selvan*

Main category: eess.SP

TL;DR: 提出UniPhyNet神经网络架构，无需手工特征，结合多尺度卷积和注意力模块处理多模态生理数据，在CL - Drive数据集上提升认知负荷分类准确率。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需手工提取特征的方法，利用多模态生理数据对认知负荷进行分类。

Method: 使用多尺度并行卷积块和带有通道块注意力模块的ResNet块，结合双向门控循环单元，通过中间融合学习到的特征图处理单模态和多模态信号。

Result: 在CL - Drive数据集上，二元分类准确率从70%提升到80%，三元分类准确率从62%提升到74%，优于基于特征的模型。

Conclusion: UniPhyNet作为端到端解决方案，对现实世界的认知状态监测有效。

Abstract: We present UniPhyNet, a novel neural network architecture to classify
cognitive load using multimodal physiological data -- specifically EEG, ECG and
EDA signals -- without the explicit need for extracting hand-crafted features.
UniPhyNet integrates multiscale parallel convolutional blocks and ResNet-type
blocks enhanced with channel block attention module to focus on the informative
features while a bidirectional gated recurrent unit is used to capture temporal
dependencies. This architecture processes and combines signals in both unimodal
and multimodal configurations via intermediate fusion of learned feature maps.
On the CL-Drive dataset, UniPhyNet improves raw signal classification accuracy
from 70% to 80% (binary) and 62% to 74% (ternary), outperforming feature-based
models, demonstrating its effectiveness as an end-to-end solution for
real-world cognitive state monitoring.

</details>


### [363] [Attention-Based Fusion of IQ and FFT Spectrograms with AoA Features for GNSS Jammer Localization](https://arxiv.org/abs/2507.14167)
*Lucas Heublein,Christian Wielenberg,Thorsten Nowak,Tobias Feigl,Christopher Mutschler,Felix Ott*

Main category: eess.SP

TL;DR: 提出检测、分类干扰及估计干扰源距离、方位和仰角的新方法，评估多种模型，引入融合框架，用新数据集验证性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 干扰设备威胁GNSS定位可靠性，经典AoA方法在多径环境精度低且计算资源需求大，需新方法。

Method: 评估128个视觉编码器和时间序列模型，引入基于注意力的融合框架，结合IQ样本、FFT频谱图和22个AoA特征。

Result: 在动态多径条件的室内环境记录新数据集，新方法性能优于现有方法。

Conclusion: 提出的新方法能有效检测、分类干扰并定位干扰源，提高定位精度。

Abstract: Jamming devices disrupt signals from the global navigation satellite system
(GNSS) and pose a significant threat by compromising the reliability of
accurate positioning. Consequently, the detection and localization of these
interference signals are essential to achieve situational awareness, mitigating
their impact, and implementing effective counter-measures. Classical Angle of
Arrival (AoA) methods exhibit reduced accuracy in multipath environments due to
signal reflections and scattering, leading to localization errors.
Additionally, AoA-based techniques demand substantial computational resources
for array signal processing. In this paper, we propose a novel approach for
detecting and classifying interference while estimating the distance, azimuth,
and elevation of jamming sources. Our benchmark study evaluates 128 vision
encoder and time-series models to identify the highest-performing methods for
each task. We introduce an attention-based fusion framework that integrates
in-phase and quadrature (IQ) samples with Fast Fourier Transform (FFT)-computed
spectrograms while incorporating 22 AoA features to enhance localization
accuracy. Furthermore, we present a novel dataset of moving jamming devices
recorded in an indoor environment with dynamic multipath conditions and
demonstrate superior performance compared to state-of-the-art methods.

</details>


### [364] [A Comprehensive Benchmark for Electrocardiogram Time-Series](https://arxiv.org/abs/2507.14206)
*Zhijiang Tang,Jiaxin Qi,Yuhua Zheng,Jianqiang Huang*

Main category: eess.SP

TL;DR: 文章深入研究心电图信号并建立综合基准，实验验证了基准、新指标和模型架构的有效性，为心电图信号分析研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视心电图数据独特特性和下游应用，对其性质理解不完整。

Method: 对心电图下游应用分类为四个评估任务，指出传统评估指标局限并引入新指标，对现有时间序列模型进行基准测试并提出新架构。

Result: 提出的基准全面且稳健，新指标和模型架构有效。

Conclusion: 为心电图信号分析研究奠定坚实基础。

Abstract: Electrocardiogram~(ECG), a key bioelectrical time-series signal, is crucial
for assessing cardiac health and diagnosing various diseases. Given its
time-series format, ECG data is often incorporated into pre-training datasets
for large-scale time-series model training. However, existing studies often
overlook its unique characteristics and specialized downstream applications,
which differ significantly from other time-series data, leading to an
incomplete understanding of its properties. In this paper, we present an
in-depth investigation of ECG signals and establish a comprehensive benchmark,
which includes (1) categorizing its downstream applications into four distinct
evaluation tasks, (2) identifying limitations in traditional evaluation metrics
for ECG analysis, and introducing a novel metric; (3) benchmarking
state-of-the-art time-series models and proposing a new architecture. Extensive
experiments demonstrate that our proposed benchmark is comprehensive and
robust. The results validate the effectiveness of the proposed metric and model
architecture, which establish a solid foundation for advancing research in ECG
signal analysis.

</details>


### [365] [DIVER-0 : A Fully Channel Equivariant EEG Foundation Model](https://arxiv.org/abs/2507.14141)
*Danny Dongyeop Han,Ahhyun Lucy Lee,Taeyang Lee,Yonghyeon Gwon,Sebin Lee,Seongjin Lee,David Keetae Park,Shinjae Yoo,Jiook Cha,Chun Kee Chung*

Main category: eess.SP

TL;DR: 提出新的EEG基础模型DIVER - 0，解决现有模型在时空建模和通道排列等方面的局限，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有EEG基础模型在建模时空脑动力学和通道排列等方面存在局限，无法在不同电极配置下实现鲁棒泛化。

Method: 提出DIVER - 0模型，采用全时空注意力机制，结合RoPE处理时间关系、二元注意力偏差区分通道；引入STCPE改进条件位置编码。

Result: DIVER - 0仅用10%预训练数据就取得有竞争力的性能，在所有通道排列条件下结果一致。

Conclusion: 验证了DIVER - 0跨数据集泛化的有效性，确立了处理神经记录设置异质性的关键设计原则。

Abstract: Electroencephalography (EEG) is a non-invasive technique widely used in
brain-computer interfaces and clinical applications, yet existing EEG
foundation models face limitations in modeling spatio-temporal brain dynamics
and lack channel permutation equivariance, preventing robust generalization
across diverse electrode configurations. To address these challenges, we
propose DIVER-0, a novel EEG foundation model that demonstrates how full
spatio-temporal attention-rather than segregated spatial or temporal
processing-achieves superior performance when properly designed with Rotary
Position Embedding (RoPE) for temporal relationships and binary attention
biases for channel differentiation. We also introduce Sliding Temporal
Conditional Positional Encoding (STCPE), which improves upon existing
conditional positional encoding approaches by maintaining both temporal
translation equivariance and channel permutation equivariance, enabling robust
adaptation to arbitrary electrode configurations unseen during pretraining.
Experimental results demonstrate that DIVER-0 achieves competitive performance
with only 10% of pretraining data while maintaining consistent results across
all channel permutation conditions, validating its effectiveness for
cross-dataset generalization and establishing key design principles for
handling the inherent heterogeneity of neural recording setups.

</details>


### [366] [Self-DANA: A Resource-Efficient Channel-Adaptive Self-Supervised Approach for ECG Foundation Models](https://arxiv.org/abs/2507.14151)
*Giuliana Monachino,Nicolò La Porta,Beatrice Zanchi,Luigi Fiorillo,Alvise Dei Rossi,Georgiy Farina,Francesca Dalia Faraci*

Main category: eess.SP

TL;DR: 提出Self - DANA使自监督架构适应少输入通道，引入随机导联选择增强技术，实验显示Self - DANA提升资源效率且达最优性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在心脏病学领域分析心电图信号受关注，但适应少通道下游场景待研究。

Method: 提出Self - DANA解决方案，引入随机导联选择增强技术。

Result: 在五种少通道配置实验中，Self - DANA显著提升资源效率，减少CPU和GPU内存及时间消耗。

Conclusion: Self - DANA可使自监督架构适应少输入通道，能确保资源效率和高性能。

Abstract: Foundation Models (FMs) are large-scale machine learning models trained on
extensive, diverse datasets that can be adapted to a wide range of downstream
tasks with minimal fine-tuning. In the last two years, interest in FMs has also
grown for applications in the cardiological field to analyze the
electrocardiogram (ECG) signals. One of the key properties of FMs is their
transferability to a wide range of downstream scenarios. With the spread of
wearable and portable devices, keen interest in learning from reduced-channel
configurations has arisen. However, the adaptation of ECG FMs to downstream
scenarios with fewer available channels still has to be properly investigated.
In this work, we propose Self-DANA, a novel, easy-to-integrate solution that
makes self-supervised architectures adaptable to a reduced number of input
channels, ensuring resource efficiency and high performance. We also introduce
Random Lead Selection, a novel augmentation technique to pre-train models in a
more robust and channel-agnostic way. Our experimental results on five
reduced-channel configurations demonstrate that Self-DANA significantly
enhances resource efficiency while reaching state-of-the-art performance. It
requires up to 69.3% less peak CPU memory, 34.4% less peak GPU memory, about
17% less average epoch CPU time, and about 24% less average epoch GPU time.

</details>


### [367] [Surface EMG Profiling in Parkinson's Disease: Advancing Severity Assessment with GCN-SVM](https://arxiv.org/abs/2507.14153)
*Daniel Cieślak,Barbara Szyca,Weronika Bajko,Liwia Florkiewicz,Kinga Grzęda,Mariusz Kaczmarek,Helena Kamieniecka,Hubert Lis,Weronika Matwiejuk,Anna Prus,Michalina Razik,Inga Rozumowicz,Wiktoria Ziembakowska*

Main category: eess.SP

TL;DR: 研究引入利用表面肌电图（sEMG）评估帕金森病（PD）严重程度的新方法，传统SVM模型准确率达83%，GCN - SVM模型提升至92%，后续待大样本验证。


<details>
  <summary>Details</summary>
Motivation: 帕金森病诊断和监测因病情进展和症状复杂面临挑战，需客观评估病情严重程度的方法。

Method: 对五名PD患者和五名健康对照的肱二头肌sEMG数据进行分析，使用传统SVM模型和GCN - SVM模型。

Result: 传统SVM模型准确率达83%，GCN - SVM模型准确率提升至92%。

Conclusion: 该方法有潜力推动PD严重程度评估，改善患者护理。

Abstract: Parkinson's disease (PD) poses challenges in diagnosis and monitoring due to
its progressive nature and complex symptoms. This study introduces a novel
approach utilizing surface electromyography (sEMG) to objectively assess PD
severity, focusing on the biceps brachii muscle. Initial analysis of sEMG data
from five PD patients and five healthy controls revealed significant
neuromuscular differences. A traditional Support Vector Machine (SVM) model
achieved up to 83% accuracy, while enhancements with a Graph Convolutional
Network-Support Vector Machine (GCN-SVM) model increased accuracy to 92%.
Despite the preliminary nature of these results, the study outlines a detailed
experimental methodology for future research with larger cohorts to validate
these findings and integrate the approach into clinical practice. The proposed
approach holds promise for advancing PD severity assessment and improving
patient care in Parkinson's disease management.

</details>


### [368] [A Denoising VAE for Intracardiac Time Series in Ischemic Cardiomyopathy](https://arxiv.org/abs/2507.14164)
*Samuel Ruipérez-Campillo,Alain Ryser,Thomas M. Sutter,Ruibin Feng,Prasanth Ganesan,Brototo Deb,Kelly A. Brennan,Maxime Pedron,Albert J. Rogers,Maarten Z. H. Kolk,Fleur V. Y. Tjong,Sanjiv M. Narayan,Julia E. Vogt*

Main category: eess.SP

TL;DR: 本文引入变分自编码器（VAE）模型改善心室内单相动作电位（MAP）信号记录质量，在去噪方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统降噪技术无法有效处理心脏电生理信号中多样的噪声模式，需要新方法改善信号质量以辅助心律失常和心肌病的准确诊断与治疗。

Method: 构建VAE模型，利用42名缺血性心肌病患者的5706个时间序列数据集构建干净信号的表示。

Result: VAE模型在不同噪声类型下均展现出优越的去噪能力，能消除单搏中的各种噪声源，优于现有去噪技术。

Conclusion: VAE模型可用于心脏电生理领域，有望提高治疗效果。

Abstract: In the field of cardiac electrophysiology (EP), effectively reducing noise in
intra-cardiac signals is crucial for the accurate diagnosis and treatment of
arrhythmias and cardiomyopathies. However, traditional noise reduction
techniques fall short in addressing the diverse noise patterns from various
sources, often non-linear and non-stationary, present in these signals. This
work introduces a Variational Autoencoder (VAE) model, aimed at improving the
quality of intra-ventricular monophasic action potential (MAP) signal
recordings. By constructing representations of clean signals from a dataset of
5706 time series from 42 patients diagnosed with ischemic cardiomyopathy, our
approach demonstrates superior denoising performance when compared to
conventional filtering methods commonly employed in clinical settings. We
assess the effectiveness of our VAE model using various metrics, indicating its
superior capability to denoise signals across different noise types, including
time-varying non-linear noise frequently found in clinical settings. These
results reveal that VAEs can eliminate diverse sources of noise in single
beats, outperforming state-of-the-art denoising techniques and potentially
improving treatment efficacy in cardiac EP.

</details>


### [369] [NeuroHD-RA: Neural-distilled Hyperdimensional Model with Rhythm Alignment](https://arxiv.org/abs/2507.14184)
*ZhengXiao He,Jinghao Wen,Huayu Li,Ao Li*

Main category: eess.SP

TL;DR: 提出结合超维计算与可学习神经编码的心电图疾病检测框架，实验表现优于传统方法，适用于边缘心电图分类。


<details>
  <summary>Details</summary>
Motivation: 传统超维计算依赖静态随机投影，缺乏任务适应性，需设计新的心电图疾病检测框架。

Method: 引入基于RR间隔的可训练编码管道，采用神经蒸馏超维计算架构，联合交叉熵和基于代理的度量损失进行优化。

Result: 在Apnea - ECG和PTB - XL数据集上显著优于传统超维计算和经典机器学习基线，Apnea - ECG上精度达73.09%，F1分数0.626，在PTB - XL上有可比的鲁棒性。

Conclusion: 该框架为边缘心电图分类提供高效可扩展解决方案，在可解释和个性化健康监测方面有很大潜力。

Abstract: We present a novel and interpretable framework for electrocardiogram
(ECG)-based disease detection that combines hyperdimensional computing (HDC)
with learnable neural encoding. Unlike conventional HDC approaches that rely on
static, random projections, our method introduces a rhythm-aware and trainable
encoding pipeline based on RR intervals, a physiological signal segmentation
strategy that aligns with cardiac cycles. The core of our design is a
neural-distilled HDC architecture, featuring a learnable RR-block encoder and a
BinaryLinear hyperdimensional projection layer, optimized jointly with
cross-entropy and proxy-based metric loss. This hybrid framework preserves the
symbolic interpretability of HDC while enabling task-adaptive representation
learning. Experiments on Apnea-ECG and PTB-XL demonstrate that our model
significantly outperforms traditional HDC and classical ML baselines, achieving
73.09\% precision and an F1 score of 0.626 on Apnea-ECG, with comparable
robustness on PTB-XL. Our framework offers an efficient and scalable solution
for edge-compatible ECG classification, with strong potential for interpretable
and personalized health monitoring.

</details>


### [370] [AI-Based Impedance Encoding-Decoding Method for Online Impedance Network Construction of Wind Farms](https://arxiv.org/abs/2507.14187)
*Xiaojuan Zhang,Tianyu Jiang,Haoxiang Zong,Chen Zhang,Chendan Li,Marta Molinas*

Main category: eess.SP

TL;DR: 本文提出基于AI的阻抗编解码方法以解决风电场阻抗网络（IN）模型在线构建难题，经验证该方法可行。


<details>
  <summary>Details</summary>
Motivation: 现有IN模型构建需传输大量高密度阻抗曲线，难以在线应用。

Method: 训练阻抗编码器压缩阻抗曲线，上传压缩数据后训练阻抗解码器重建曲线，基于节点导纳矩阵法获取风电场IN模型。

Result: 编码后的阻抗向量可实现原阻抗曲线的快速传输和准确重建。

Conclusion: 提出的基于AI的阻抗编解码方法能有效促进风电场IN模型的在线构建。

Abstract: The impedance network (IN) model is gaining popularity in the oscillation
analysis of wind farms. However, the construction of such an IN model requires
impedance curves of each wind turbine under their respective operating
conditions, making its online application difficult due to the transmission of
numerous high-density impedance curves. To address this issue, this paper
proposes an AI-based impedance encoding-decoding method to facilitate the
online construction of IN model. First, an impedance encoder is trained to
compress impedance curves by setting the number of neurons much smaller than
that of frequency points. Then, the compressed data of each turbine are
uploaded to the wind farm and an impedance decoder is trained to reconstruct
original impedance curves. At last, based on the nodal admittance matrix (NAM)
method, the IN model of the wind farm can be obtained. The proposed method is
validated via model training and real-time simulations, demonstrating that the
encoded impedance vectors enable fast transmission and accurate reconstruction
of the original impedance curves.

</details>


### [371] [UWB Radar-based Heart Rate Monitoring: A Transfer Learning Approach](https://arxiv.org/abs/2507.14195)
*Elzbieta Gruzewska,Pooja Rao,Sebastien Baur,Matthew Baugh,Mathias M. J. Bellaiche,Sharanya Srinivas,Octavio Ponce,Matthew Thompson,Pramod Rudrapatna,Michael A. Sanchez,Lawrence Z. Cai,Timothy JA Chico,Robert F. Storey,Emily Maz,Umesh Telang,Shravya Shetty,Mayank Daswani*

Main category: eess.SP

TL;DR: 研究展示了调频连续波（FMCW）和脉冲无线电超宽带（IR - UWB）雷达系统间的迁移学习用于心率监测，可加速其在消费设备中的应用。


<details>
  <summary>Details</summary>
Motivation: 雷达技术用于心率监测有潜力，但不同雷达系统需大量新配对数据集，缺乏标准化。

Method: 使用新型2D + 1D ResNet架构对FMCW雷达进行心率监测，再用小的IR - UWB数据集微调模型进行迁移学习。

Result: FMCW雷达心率监测MAE为0.85 bpm，MAPE为1.42%，召回率98.9%；迁移学习后IR - UWB模型MAE 4.1 bpm，MAPE 6.3%，召回率97.5%，MAE较基线降低25%。

Conclusion: 雷达系统间的迁移学习用于心率监测有潜力加速其引入现有消费设备。

Abstract: Radar technology presents untapped potential for continuous, contactless, and
passive heart rate monitoring via consumer electronics like mobile phones.
However the variety of available radar systems and lack of standardization
means that a large new paired dataset collection is required for each radar
system. This study demonstrates transfer learning between frequency-modulated
continuous wave (FMCW) and impulse-radio ultra-wideband (IR-UWB) radar systems,
both increasingly integrated into consumer devices. FMCW radar utilizes a
continuous chirp, while IR-UWB radar employs short pulses. Our mm-wave FMCW
radar operated at 60 GHz with a 5.5 GHz bandwidth (2.7 cm resolution, 3
receiving antennas [Rx]), and our IR-UWB radar at 8 GHz with a 500 MHz
bandwidth (30 cm resolution, 2 Rx). Using a novel 2D+1D ResNet architecture we
achieved a mean absolute error (MAE) of 0.85 bpm and a mean absolute percentage
error (MAPE) of 1.42% for heart rate monitoring with FMCW radar (N=119
participants, an average of 8 hours per participant). This model maintained
performance (under 5 MAE/10% MAPE) across various body positions and heart rate
ranges, with a 98.9% recall. We then fine-tuned a variant of this model,
trained on single-antenna and single-range bin FMCW data, using a small (N=376,
avg 6 minutes per participant) IR-UWB dataset. This transfer learning approach
yielded a model with MAE 4.1 bpm and MAPE 6.3% (97.5% recall), a 25% MAE
reduction over the IR-UWB baseline. This demonstration of transfer learning
between radar systems for heart rate monitoring has the potential to accelerate
its introduction into existing consumer devices.

</details>


### [372] [Explainable Parallel CNN-LSTM Model for Differentiating Ventricular Tachycardia from Supraventricular Tachycardia with Aberrancy in 12-Lead ECGs](https://arxiv.org/abs/2507.14196)
*Zahra Teimouri-Jervekani,Fahimeh Nasimi,Mohammadreza Yazdchi,Ghazal MogharehZadeh,Javad Tezerji,Farzan Niknejad Mazandarani,Maryam Mohebbi*

Main category: eess.SP

TL;DR: 提出轻量级并行深度架构用于宽QRS波心动过速分类，模型准确率高、计算高效且有可解释性，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 区分宽QRS波心动过速临床关键但因心电图信号形态相似而具挑战性，误诊有致命风险，需提高诊断准确性并提供模型可解释性。

Method: 引入轻量级并行深度架构，用1D - CNN块提取局部特征，LSTM层捕捉时间依赖，全连接层分类，用SHAP实现可解释性，并在35个受试者的心电图数据库上评估。

Result: 模型准确率95.63%，敏感性95.10%，特异性96.06%，F1分数95.12%，在准确性和计算效率上优于现有方法，SHAP分析显示特征贡献有临床可解释性。

Conclusion: 端到端框架高精度分类宽QRS波心动过速且计算开销小，SHAP增强临床信任，该方法对现实心电图分析工具很有前景。

Abstract: Background and Objective: Differentiating wide complex tachycardia (WCT) is
clinically critical yet challenging due to morphological similarities in
electrocardiogram (ECG) signals between life-threatening ventricular
tachycardia (VT) and supraventricular tachycardia with aberrancy (SVT-A).
Misdiagnosis carries fatal risks. We propose a computationally efficient deep
learning solution to improve diagnostic accuracy and provide model
interpretability for clinical deployment.
  Methods: A novel lightweight parallel deep architecture is introduced. Each
pipeline processes individual ECG leads using two 1D-CNN blocks to extract
local features. Feature maps are concatenated across leads, followed by LSTM
layers to capture temporal dependencies. Final classification employs fully
connected layers. Explainability is achieved via Shapley Additive Explanations
(SHAP) for local/global interpretation. The model was evaluated on a 35-subject
ECG database using standard performance metrics.
  Results: The model achieved $95.63\%$ accuracy ($95\%$ CI: $93.07-98.19\%$),
with sensitivity=$95.10\%$, specificity=$96.06\%$, and F1-score=$95.12\%$. It
outperformed state-of-the-art methods in both accuracy and computational
efficiency, requiring minimal CNN blocks per pipeline. SHAP analysis
demonstrated clinically interpretable feature contributions.
  Conclusions: Our end-to-end framework delivers high-precision WCT
classification with minimal computational overhead. The integration of SHAP
enhances clinical trust by elucidating decision logic, supporting rapid,
informed diagnosis. This approach shows significant promise for real-world ECG
analysis tools.

</details>


### [373] [Age of Information Minimization in UAV-Enabled Integrated Sensing and Communication Systems](https://arxiv.org/abs/2507.14299)
*Yu Bai,Yifan Zhang,Boxuan Xie,Zheng Chang,Yanru Zhang,Riku Jantti,Zhu Han*

Main category: eess.SP

TL;DR: 提出以信息年龄（AoI）为中心的无人机集成传感与通信（UAV - ISAC）系统，用深度强化学习算法优化轨迹和波束赋形，仿真显示平均AoI更低。


<details>
  <summary>Details</summary>
Motivation: 在资源和时间限制下，联合优化无人机轨迹规划、多用户通信和目标传感存在挑战，需强调信息新鲜度。

Method: 提出AoI中心的UAV - ISAC系统，用深度强化学习算法，结合卡尔曼滤波器、正则化迫零和Soft Actor - Critic算法。

Result: 仿真表明所提方法比基线方法平均AoI更低。

Conclusion: 所提框架能自适应平衡传感精度和通信质量，有效降低平均AoI。

Abstract: Unmanned aerial vehicles (UAVs) equipped with integrated sensing and
communication (ISAC) capabilities are envisioned to play a pivotal role in
future wireless networks due to their enhanced flexibility and efficiency.
However, jointly optimizing UAV trajectory planning, multi-user communication,
and target sensing under stringent resource constraints and time-critical
conditions remains a significant challenge. To address this, we propose an Age
of Information (AoI)-centric UAV-ISAC system that simultaneously performs
target sensing and serves multiple ground users, emphasizing information
freshness as the core performance metric. We formulate a long-term average AoI
minimization problem that jointly optimizes the UAV's flight trajectory and
beamforming. To tackle the high-dimensional, non-convexity of this problem, we
develop a deep reinforcement learning (DRL)-based algorithm capable of
providing real-time decisions on UAV movement and beamforming for both radar
sensing and multi-user communication. Specifically, a Kalman filter is employed
for accurate target state prediction, regularized zero-forcing is utilized to
mitigate inter-user interference, and the Soft Actor-Critic algorithm is
applied for training the DRL agent on continuous actions. The proposed
framework adaptively balances the trade-offs between sensing accuracy and
communication quality. Extensive simulation results demonstrate that our
proposed method consistently achieves lower average AoI compared to baseline
approaches.

</details>


### [374] [Recursive KalmanNet: Analyse des capacités de généralisation d'un réseau de neurones récurrent guidé par un filtre de Kalman](https://arxiv.org/abs/2507.14144)
*Cyril Falcon,Hassan Mortada,Mathéo Clavaud,Jean-Philippe Michel*

Main category: eess.SP

TL;DR: 文章探讨Recursive KalmanNet在分布外场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究Recursive KalmanNet在测试测量的时间动态与训练时不同的分布外场景下的泛化能力。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: The Recursive KalmanNet, recently introduced by the authors, is a recurrent
neural network guided by a Kalman filter, capable of estimating the state
variables and error covariance of stochastic dynamic systems from noisy
measurements, without prior knowledge of the noise characteristics. This paper
explores its generalization capabilities in out-of-distribution scenarios,
where the temporal dynamics of the test measurements differ from those
encountered during training.
  Le Recursive KalmanNet, r\'ecemment introduit par les auteurs, est un
r\'eseau de neurones r\'ecurrent guid\'e par un filtre de Kalman, capable
d'estimer les variables d'\'etat et la covariance des erreurs des syst\`emes
dynamiques stochastiques \`a partir de mesures bruit\'ees, sans connaissance
pr\'ealable des caract\'eristiques des bruits. Cet article explore ses
capacit\'es de g\'en\'eralisation dans des sc\'enarios hors distribution, o\`u
les dynamiques temporelles des mesures de test diff\`erent de celles
rencontr\'ees \`a l'entra\^inement.

</details>


### [375] [Graph Convolutional Neural Networks to Model the Brain for Insomnia](https://arxiv.org/abs/2507.14147)
*Kevin Monteiro,Sam Nallaperuma-Herzberg,Martina Mason,Steve Niederer*

Main category: eess.SP

TL;DR: 文章尝试用长时程EEG数据研究失眠患者大脑特征，构建GCNN模型分类，确定合适EEG分割窗口，取得一定分类准确率并发现部分关键EEG通道。


<details>
  <summary>Details</summary>
Motivation: 失眠影响人群广且现有治疗有副作用，需改进治疗方法，且尚无针对失眠的脑模型。

Method: 用长时程EEG数据，基于功能连接和空间距离构建脑网络，计算脑电波功率谱密度，训练GCNN模型分类。

Result: 50秒非重叠滑动窗口最适合EEG分割，窗口级分类准确率70%，受试者级68%，移除C4 - P4、F4 - C4和C4 - A1通道电极使模型性能下降更明显。

Conclusion: 该方法对理解失眠患者大脑特征和构建分类模型有一定效果，特定EEG通道对模型性能影响大。

Abstract: Insomnia affects a vast population of the world and can have a wide range of
causes. Existing treatments for insomnia have been linked with many side
effects like headaches, dizziness, etc. As such, there is a clear need for
improved insomnia treatment. Brain modelling has helped with assessing the
effects of brain pathology on brain network dynamics and with supporting
clinical decisions in the treatment of Alzheimer's disease, epilepsy, etc.
However, such models have not been developed for insomnia. Therefore, this
project attempts to understand the characteristics of the brain of individuals
experiencing insomnia using continuous long-duration EEG data. Brain networks
are derived based on functional connectivity and spatial distance between EEG
channels. The power spectral density of the channels is then computed for the
major brain wave frequency bands. A graph convolutional neural network (GCNN)
model is then trained to capture the functional characteristics associated with
insomnia and configured for the classification task to judge performance.
Results indicated a 50-second non-overlapping sliding window was the most
suitable choice for EEG segmentation. This approach achieved a classification
accuracy of 70% at window level and 68% at subject level. Additionally, the
omission of EEG channels C4-P4, F4-C4 and C4-A1 caused higher degradation in
model performance than the removal of other channels. These channel electrodes
are positioned near brain regions known to exhibit atypical levels of
functional connectivity in individuals with insomnia, which can explain such
results.

</details>


### [376] [Machine learning-enabled river water quality monitoring using lithography-free 3D-printed sensors](https://arxiv.org/abs/2507.14152)
*Frank Efe Erukainure,Feidra Gjata,Matin Ataei Kachouei,Henry Cox,Md. Azahar Ali*

Main category: eess.SP

TL;DR: 本文介绍了一种无光刻磷酸盐传感器，可检测河水中低至十亿分之一水平的磷酸盐，经测试验证效果良好，是连续水质监测的实用工具。


<details>
  <summary>Details</summary>
Motivation: 河流水质监测对水生生物、牲畜和人类很重要，过量污染物（如磷酸盐）会导致富营养化等问题，需要能监测磷酸盐水平的传感器来预防。

Method: 制作无光刻磷酸盐传感器（P - sensor），采用3D打印周期性聚合物图案涂覆磷酸盐离子选择性薄膜形成固态指示电极；用拉帕汉诺克河河水验证传感器，并与商业磷酸盐测量仪对比；训练前馈神经网络预测磷酸盐水平。

Result: P - sensor可在0 - 475 ppm范围内检测低至1 ppb的磷酸盐，响应时间不到30秒；神经网络预测磷酸盐水平的均方误差低于1e - 3，标准差为0，皮尔逊相关系数为0.997。

Conclusion: 该传感器是连续水质监测的实用工具，可告知利益相关者和政策制定者，最终改善公众健康。

Abstract: River water quality monitoring is important for aquatic life, livestock, and
humans because clean water is critical to meeting food demand during the global
food crisis. Excessive contaminants, including phosphate, deplete dissolved
oxygen and trigger eutrophication, leading to serious health and ecological
problems. Continuous sensors that track phosphate levels can therefore help
prevent eutrophication. In this work we present a lithography-free phosphate
sensor (P-sensor) that detects phosphate in river water at parts-per-billion
levels. The device uses a solid-state indicator electrode formed by 3D-printed
periodic polymer patterns (8 um feature size) coated with a thin phosphate
ion-selective membrane. The P-sensor detects as little as 1 ppb phosphate
across 0 - 475 ppm with a response time under 30 seconds. We validated the
sensor on Rappahannock River water, Virginia (less than 0.8 ppm phosphate) at
sites upstream and downstream of a sewage treatment plant and benchmarked the
results against a commercial phosphate meter. A feed-forward neural network was
trained to predict phosphate levels, achieving a mean-squared error below 1e-3,
zero standard deviation, and a Pearson correlation coefficient of 0.997 for
river samples. These results demonstrate a practical tool for continuous
water-quality monitoring that can inform stakeholders and policymakers and
ultimately improve public health.

</details>


### [377] [Automated Vigilance State Classification in Rodents Using Machine Learning and Feature Engineering](https://arxiv.org/abs/2507.14166)
*Sankalp Jajee,Gaurav Kumar,Homayoun Valafar*

Main category: eess.SP

TL;DR: 研究提出自动化框架将小型啮齿动物脑电图记录分类为三种警觉状态，模型表现优异，是自动化睡眠状态分类的重要进展。


<details>
  <summary>Details</summary>
Motivation: 临床前睡眠研究受人工警觉状态分类的劳动强度和评分者间差异限制，影响通量和可重复性。

Method: 系统集成先进信号处理与机器学习，利用时域和频域特征，如频谱功率、最大 - 最小距离和跨频耦合指标。

Result: XGBoost模型在2024年比赛中验证，整体准确率91.5%，精度86.8%，召回率81.2%，F1分数83.5%，优于所有基线方法。

Conclusion: 该方法是自动化睡眠状态分类的关键进展，可加速睡眠科学发现和针对慢性睡眠障碍的干预措施开发，公开代码资源将有重要贡献。

Abstract: Preclinical sleep research remains constrained by labor intensive, manual
vigilance state classification and inter rater variability, limiting throughput
and reproducibility. This study presents an automated framework developed by
Team Neural Prognosticators to classify electroencephalogram (EEG) recordings
of small rodents into three critical vigilance states paradoxical sleep (REM),
slow wave sleep (SWS), and wakefulness. The system integrates advanced signal
processing with machine learning, leveraging engineered features from both time
and frequency domains, including spectral power across canonical EEG bands
(delta to gamma), temporal dynamics via Maximum-Minimum Distance, and
cross-frequency coupling metrics. These features capture distinct
neurophysiological signatures such as high frequency desynchronization during
wakefulness, delta oscillations in SWS, and REM specific bursts. Validated
during the 2024 Big Data Health Science Case Competition (University of South
Carolina Big Data Health Science Center, 2024), our XGBoost model achieved
91.5% overall accuracy, 86.8% precision, 81.2% recall, and an F1 score of
83.5%, outperforming all baseline methods. Our approach represents a critical
advancement in automated sleep state classification and a valuable tool for
accelerating discoveries in sleep science and the development of targeted
interventions for chronic sleep disorders. As a publicly available code (BDHSC)
resource is set to contribute significantly to advancements.

</details>


### [378] [Enhancing Generalization in PPG-Based Emotion Measurement with a CNN-TCN-LSTM Model](https://arxiv.org/abs/2507.14173)
*Karim Alghoul,Hussein Al Osman,Abdulmotaleb El Saddik*

Main category: eess.SP

TL;DR: 本文提出结合CNN、LSTM和TCN的混合架构用于PPG信号情感识别，实验表明其泛化能力优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决基于PPG的情感识别模型在个体间泛化能力不足的问题。

Method: 提出结合CNN、LSTM和TCN的混合架构，先由CNN提取特征，再分别用LSTM和TCN处理，最后拼接输出用于情感分类。

Result: 使用PPGE数据集实验，混合模型在情感识别任务中泛化能力优于独立的CNN和LSTM架构，也优于现有CNN架构和CNN - LSTM模型。

Conclusion: 该混合架构能有效处理个体差异，在PPG信号情感识别中表现更优。

Abstract: Human computer interaction has become integral to modern life, driven by
advancements in machine learning technologies. Affective computing, in
particular, has focused on systems that recognize, interpret, and respond to
human emotions, often using wearable devices, which provide continuous data
streams of physiological signals. Among various physiological signals, the
photoplethysmogram (PPG) has gained prominence due to its ease of acquisition
from widely available devices. However, the generalization of PPG-based emotion
recognition models across individuals remains an unresolved challenge. This
paper introduces a novel hybrid architecture that combines Convolutional Neural
Networks (CNNs), Long Short-Term Memory networks (LSTMs), and Temporal
Convolutional Networks (TCNs) to address this issue. The proposed model
integrates the strengths of these architectures to improve robustness and
generalization. Raw PPG signals are fed into the CNN for feature extraction.
These features are processed separately by LSTM and TCN. The outputs from these
components are concatenated to generate a final feature representation, which
serves as the input for classifying valence and arousal, the primary dimensions
of emotion. Experiments using the Photoplethysmogram Dataset for Emotional
Analysis (PPGE) demonstrate that the proposed hybrid model achieves better
model generalization than standalone CNN and LSTM architectures. Our results
show that the proposed solution outperforms the state-of-the-art CNN
architecture, as well as a CNN-LSTM model, in emotion recognition tasks with
PPG signals. Using metrics such as Area Under the Curve (AUC) and F1 Score, we
highlight the model's effectiveness in handling subject variability.

</details>


### [379] [Latent Sensor Fusion: Multimedia Learning of Physiological Signals for Resource-Constrained Devices](https://arxiv.org/abs/2507.14185)
*Abdullah Ahmed,Jeremy Gummeson*

Main category: eess.SP

TL;DR: 利用潜在空间开发统一编码器，用传感器 - 潜在融合分析多模态信号，解决资源受限设备生物信号分析计算难题，实验显示编码器更优。


<details>
  <summary>Details</summary>
Motivation: 利用潜在空间的元嵌入开发模态不可知的统一编码器，解决资源受限设备生物信号分析的计算挑战。

Method: 采用传感器 - 潜在融合分析和关联多模态生理信号，使用基于自编码器潜在空间融合的压缩感知方法。

Result: 统一编码器比特定模态的替代方案显著更快、更轻且更具可扩展性，同时不影响表征准确性。

Conclusion: 所开发的统一编码器在生物信号分析方面具有优势。

Abstract: Latent spaces offer an efficient and effective means of summarizing data
while implicitly preserving meta-information through relational encoding. We
leverage these meta-embeddings to develop a modality-agnostic, unified encoder.
Our method employs sensor-latent fusion to analyze and correlate multimodal
physiological signals. Using a compressed sensing approach with
autoencoder-based latent space fusion, we address the computational challenges
of biosignal analysis on resource-constrained devices. Experimental results
show that our unified encoder is significantly faster, lighter, and more
scalable than modality-specific alternatives, without compromising
representational accuracy.

</details>


### [380] [Boosted Enhanced Quantile Regression Neural Networks with Spatiotemporal Permutation Entropy for Complex System Prognostics](https://arxiv.org/abs/2507.14194)
*David J Poland*

Main category: eess.SP

TL;DR: 提出基于时空排列熵分析与BEQRNNs的模式预测和系统预后框架，效果良好，有实时预后应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决理解多维系统中复杂动态模式的挑战。

Method: 结合基于熵的复杂性度量和先进神经网络架构，分时空熵提取和集成BEQRNN层两个计算阶段。

Result: 时空模式分类准确率达81.17%，临界转变检测准确率提高79%，长期预测可靠性提高81.22%。

Conclusion: 该框架处理复杂多模态熵特征有效，有实时预后应用潜力。

Abstract: This paper presents a novel framework for pattern prediction and system
prognostics centered on Spatiotemporal Permutation Entropy analysis integrated
with Boosted Enhanced Quantile Regression Neural Networks (BEQRNNs). We address
the challenge of understanding complex dynamical patterns in multidimensional
systems through an approach that combines entropy-based complexity measures
with advanced neural architectures. The system leverages dual computational
stages: first implementing spatiotemporal entropy extraction optimized for
multiscale temporal and spatial data streams, followed by an integrated BEQRNN
layer that enables probabilistic pattern prediction with uncertainty
quantification. This architecture achieves 81.17% accuracy in spatiotemporal
pattern classification with prediction horizons up to 200 time steps and
maintains robust performance across diverse regimes. Field testing across
chaotic attractors, reaction-diffusion systems, and industrial datasets shows a
79% increase in critical transition detection accuracy and 81.22% improvement
in long-term prediction reliability. The framework's effectiveness in
processing complex, multimodal entropy features demonstrates significant
potential for real-time prognostic applications.

</details>


### [381] [Distributed Machine Learning Approach for Low-Latency Localization in Cell-Free Massive MIMO Systems](https://arxiv.org/abs/2507.14216)
*Manish Kumar,Tzu-Hsuan Chou,Byunghyun Lee,Nicolò Michelusi,David J. Love,Yaguang Zhang,James V. Krogmeier*

Main category: eess.SP

TL;DR: 提出适用于无小区大规模MIMO系统的分布式机器学习定位框架，减少延迟，模拟显示其有低延迟、高精度定位潜力。


<details>
  <summary>Details</summary>
Motivation: 低延迟定位对蜂窝网络支持实时精确位置应用很关键，传统集中式定位有不足。

Method: 提出分布式机器学习框架，各接入点独立训练高斯过程回归模型，用户设备融合位置估计。

Result: 分布式框架定位精度与集中式相当，有效降低位置估计不确定性。

Conclusion: 分布式机器学习在未来6G网络低延迟、高精度定位方面有潜力。

Abstract: Low-latency localization is critical in cellular networks to support
real-time applications requiring precise positioning. In this paper, we propose
a distributed machine learning (ML) framework for fingerprint-based
localization tailored to cell-free massive multiple-input multiple-output
(MIMO) systems, an emerging architecture for 6G networks. The proposed
framework enables each access point (AP) to independently train a Gaussian
process regression model using local angle-of-arrival and received signal
strength fingerprints. These models provide probabilistic position estimates
for the user equipment (UE), which are then fused by the UE with minimal
computational overhead to derive a final location estimate. This decentralized
approach eliminates the need for fronthaul communication between the APs and
the central processing unit (CPU), thereby reducing latency. Additionally,
distributing computational tasks across the APs alleviates the processing
burden on the CPU compared to traditional centralized localization schemes.
Simulation results demonstrate that the proposed distributed framework achieves
localization accuracy comparable to centralized methods, despite lacking the
benefits of centralized data aggregation. Moreover, it effectively reduces
uncertainty of the location estimates, as evidenced by the 95\% covariance
ellipse. The results highlight the potential of distributed ML for enabling
low-latency, high-accuracy localization in future 6G networks.

</details>


### [382] [Advanced Space Mapping Technique Integrating a Shared Coarse Model for Multistate Tuning-Driven Multiphysics Optimization of Tunable Filters](https://arxiv.org/abs/2507.14220)
*Haitian Hu,Wei Zhang,Feng Feng,Zhiguo Zhang,Qi-Jun Zhang*

Main category: eess.SP

TL;DR: 介绍用于可调滤波器多状态调谐驱动多物理场优化的高级空间映射技术，该方法结合效率与精度，用多子代理模型优化，效果更好。


<details>
  <summary>Details</summary>
Motivation: 解决现有直接多物理场参数化建模技术的不足，实现更高效准确的多物理场优化。

Method: 结合电磁单物理场模拟效率和多物理场模拟精度，构建含共享粗模型和映射神经网络的多子代理模型，同时生成训练样本。

Result: 与现有技术相比，用更少训练样本和计算成本实现了更高的多物理场建模精度。

Conclusion: 所提出的高级空间映射技术能有效进行可调滤波器多状态调谐驱动的多物理场优化。

Abstract: This article introduces an advanced space mapping (SM) technique that applies
a shared electromagnetic (EM)-based coarse model for multistate tuning-driven
multiphysics optimization of tunable filters. The SM method combines the
computational efficiency of EM single-physics simulations with the precision of
multiphysics simulations. The shared coarse model is based on EM single-physics
responses corresponding to various nontunable design parameters values.
Conversely, the fine model is implemented to delineate the behavior of
multiphysics responses concerning both nontunable and tunable design parameter
values. The proposed overall surrogate model comprises multiple subsurrogate
models, each consisting of one shared coarse model and two distinct mapping
neural networks. The responses from the shared coarse model in the EM
single-physics filed offer a suitable approximation for the fine responses in
the multiphysics filed, whereas the mapping neural networks facilitate
transition from the EM single-physics field to the multiphysics field. Each
subsurrogate model maintains consistent nontunable design parameter values but
possesses unique tunable design parameter values. By developing multiple
subsurrogate models, optimization can be simultaneously performed for each
tuning state. Nontunable design parameter values are constrained by all tuning
states, whereas tunable design parameter values are confined to their
respective tuning states. This optimization technique simultaneously accounts
for all the tuning states to fulfill the necessary multiple tuning state
requirements. Multiple EM and multiphysics training samples are generated
concurrently to develop the surrogate model. Compared with existing direct
multiphysics parameterized modeling techniques, our proposed method achieves
superior multiphysics modeling accuracy with fewer training samples and reduced
computational costs.

</details>


### [383] [Diffusion-based translation between unpaired spontaneous premature neonatal EEG and fetal MEG](https://arxiv.org/abs/2507.14224)
*Benoît Brebion,Alban Gallard,Katrin Sippel,Amer Zaylaa,Hubert Preissl,Sahar Moghimi,Fabrice Wallois,Yaël Frégier*

Main category: eess.SP

TL;DR: 研究运用人工智能将EEG知识迁移到fMEG，开发双扩散桥非配对扩散翻译方法，提升了EEG - fMEG非配对翻译水平，可用于早期脑活动分析及其他非配对信号翻译。


<details>
  <summary>Details</summary>
Motivation: 传统用EEG研究早产儿脑活动，胎儿阶段脑发育研究不足，fMEG有数据质量和稀缺问题，旨在用人工智能将EEG知识迁移到fMEG，以助于产前脑发育研究及病理检测治疗。

Method: 开发基于双扩散桥的非配对扩散翻译方法，改进数值积分，在30份早产儿EEG和44份fMEG非配对数据集上训练模型。

Result: 相比GANs，时域均方误差提升近5%，频域消除模式崩溃问题，实现近乎完美的信号保真度。

Conclusion: 在EEG - fMEG非配对翻译问题上达到新水平，为早期脑活动分析铺平道路，方法可用于其他非配对信号翻译。

Abstract: Background and objective: Brain activity in premature newborns has
traditionally been studied using electroencephalography (EEG), leading to
substantial advances in our understanding of early neural development. However,
since brain development takes root at the fetal stage, a critical window of
this process remains largely unknown. The only technique capable of recording
neural activity in the intrauterine environment is fetal magnetoencephalography
(fMEG), but this approach presents challenges in terms of data quality and
scarcity. Using artificial intelligence, the present research aims to transfer
the well-established knowledge from EEG studies to fMEG to improve
understanding of prenatal brain development, laying the foundations for better
detection and treatment of potential pathologies. Methods: We developed an
unpaired diffusion translation method based on dual diffusion bridges, which
notably includes numerical integration improvements to obtain more qualitative
results at a lower computational cost. Models were trained on our unpaired
dataset of bursts of spontaneous activity from 30 high-resolution premature
newborns EEG recordings and 44 fMEG recordings. Results: We demonstrate that
our method achieves significant improvement upon previous results obtained with
Generative Adversarial Networks (GANs), by almost 5% on the mean squared error
in the time domain, and completely eliminating the mode collapse problem in the
frequency domain, thus achieving near-perfect signal fidelity. Conclusion: We
set a new state of the art in the EEG-fMEG unpaired translation problem, as our
developed tool completely paves the way for early brain activity analysis.
Overall, we also believe that our method could be reused for other unpaired
signal translation applications.

</details>


### [384] [MEETI: A Multimodal ECG Dataset from MIMIC-IV-ECG with Signals, Images, Features and Interpretations](https://arxiv.org/abs/2507.15255)
*Deyun Zhang,Xiang Lan,Shijia Geng,Qinghao Zhao,Sumei Fan,Mengling Feng,Shenda Hong*

Main category: eess.SP

TL;DR: 文章介绍首个大规模同步原始波形、图像和文本的心电图数据集MEETI，支持多模态学习和细粒度推理，为心血管AI奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有心电图数据集多为单模态或双模态，难以构建能理解和整合多样信息的模型，缺乏可临床部署的多模态AI系统。

Method: 引入MEETI数据集，包含原始波形、图像、特征参数和文本解读，通过唯一标识符实现四组件对齐。

Result: MEETI数据集支持基于transformer的多模态学习和细粒度、可解释的心脏健康推理。

Conclusion: MEETI为下一代可解释的多模态心血管AI建立了坚实基础，为研究界提供了评估心电图AI系统的综合基准。

Abstract: Electrocardiogram (ECG) plays a foundational role in modern cardiovascular
care, enabling non-invasive diagnosis of arrhythmias, myocardial ischemia, and
conduction disorders. While machine learning has achieved expert-level
performance in ECG interpretation, the development of clinically deployable
multimodal AI systems remains constrained, primarily due to the lack of
publicly available datasets that simultaneously incorporate raw signals,
diagnostic images, and interpretation text. Most existing ECG datasets provide
only single-modality data or, at most, dual modalities, making it difficult to
build models that can understand and integrate diverse ECG information in
real-world settings. To address this gap, we introduce MEETI (MIMIC-IV-Ext
ECG-Text-Image), the first large-scale ECG dataset that synchronizes raw
waveform data, high-resolution plotted images, and detailed textual
interpretations generated by large language models. In addition, MEETI includes
beat-level quantitative ECG parameters extracted from each lead, offering
structured parameters that support fine-grained analysis and model
interpretability. Each MEETI record is aligned across four components: (1) the
raw ECG waveform, (2) the corresponding plotted image, (3) extracted feature
parameters, and (4) detailed interpretation text. This alignment is achieved
using consistent, unique identifiers. This unified structure supports
transformer-based multimodal learning and supports fine-grained, interpretable
reasoning about cardiac health. By bridging the gap between traditional signal
analysis, image-based interpretation, and language-driven understanding, MEETI
established a robust foundation for the next generation of explainable,
multimodal cardiovascular AI. It offers the research community a comprehensive
benchmark for developing and evaluating ECG-based AI systems.

</details>


### [385] [Optimal Transceiver Design in Over-the-Air Federated Distillation](https://arxiv.org/abs/2507.15256)
*Zihao Hu,Jia Yan,Ying-Jun Angela Zhang,Jun Zhang,Khaled B. Letaief*

Main category: eess.SP

TL;DR: 本文提出一种新型无线联邦蒸馏框架以减少通信开销，并给出相关优化设计，数值结果表明该方法能显著降低开销且测试精度损失小。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在大AI模型下因通信开销大而效率低下，需新方法减少通信开销。

Method: 结合联邦学习和知识蒸馏提出无线联邦蒸馏框架，推导收敛率表达式，获取发射功率和聚合估计器的闭式解，通过半定松弛求最优接收波束成形向量。

Result: 所提无线联邦蒸馏方法显著降低通信开销，与传统联邦学习基准相比，测试精度仅有轻微损失。

Conclusion: 所提无线联邦蒸馏框架有效，能在满足功率约束下降低通信开销，且对测试精度影响小。

Abstract: The rapid proliferation and growth of artificial intelligence (AI) has led to
the development of federated learning (FL). FL allows wireless devices (WDs) to
cooperatively learn by sharing only local model parameters, without needing to
share the entire dataset. However, the emergence of large AI models has made
existing FL approaches inefficient, due to the significant communication
overhead required. In this paper, we propose a novel over-the-air federated
distillation (FD) framework by synergizing the strength of FL and knowledge
distillation to avoid the heavy local model transmission. Instead of sharing
the model parameters, only the WDs' model outputs, referred to as knowledge,
are shared and aggregated over-the-air by exploiting the superposition property
of the multiple-access channel. We shall study the transceiver design in
over-the-air FD, aiming to maximize the learning convergence rate while meeting
the power constraints of the transceivers. The main challenge lies in the
intractability of the learning performance analysis, as well as the non-convex
nature and the optimization spanning the whole FD training period. To tackle
this problem, we first derive an analytical expression of the convergence rate
in over-the-air FD. Then, the closed-form optimal solutions of the WDs'
transmit power and the estimator for over-the-air aggregation are obtained
given the receiver combining strategy. Accordingly, we put forth an efficient
approach to find the optimal receiver beamforming vector via semidefinite
relaxation. We further prove that there is no optimality gap between the
original and relaxed problem for the receiver beamforming design. Numerical
results will show that the proposed over-the-air FD approach achieves a
significant reduction in communication overhead, with only a minor compromise
in testing accuracy compared to conventional FL benchmarks.

</details>


### [386] [EEG-based Epileptic Prediction via a Two-stage Channel-aware Set Transformer Network](https://arxiv.org/abs/2507.15364)
*Ruifeng Zheng,Cong Chen,Shuang Wang,Yiming Liu,Lin You,Jindong Lu,Ruizhe Zhu,Guodao Zhang,Kejie Huang*

Main category: eess.SP

TL;DR: 提出两阶段通道感知集变换器网络用更少EEG通道传感器进行癫痫发作预测，测试独立发作划分方法，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 可穿戴癫痫发作预测设备因EEG采集设备体积大受限，需减少EEG通道传感器进行发作预测。

Method: 提出两阶段通道感知集变换器网络，测试独立发作划分方法，在CHB - MIT数据集实验。

Result: 通道选择前平均敏感度76.4%，FPR 0.09/小时；选择后22个患者中20个出现主导通道，平均通道数从18减至2.8，平均敏感度升至80.1%，FPR 0.11/小时；独立发作划分实验支持使用更严格划分方法。

Conclusion: 所提网络能以更少EEG通道传感器进行癫痫发作预测，有大量EEG记录患者应采用更严格独立发作划分方法。

Abstract: Epilepsy is a chronic, noncommunicable brain disorder, and sudden seizure
onsets can significantly impact patients' quality of life and health. However,
wearable seizure-predicting devices are still limited, partly due to the bulky
size of EEG-collecting devices. To relieve the problem, we proposed a novel
two-stage channel-aware Set Transformer Network that could perform seizure
prediction with fewer EEG channel sensors. We also tested a seizure-independent
division method which could prevent the adjacency of training and test data.
Experiments were performed on the CHB-MIT dataset which includes 22 patients
with 88 merged seizures. The mean sensitivity before channel selection was
76.4% with a false predicting rate (FPR) of 0.09/hour. After channel selection,
dominant channels emerged in 20 out of 22 patients; the average number of
channels was reduced to 2.8 from 18; and the mean sensitivity rose to 80.1%
with an FPR of 0.11/hour. Furthermore, experimental results on the
seizure-independent division supported our assertion that a more rigorous
seizure-independent division should be used for patients with abundant EEG
recordings.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [387] [Enhancements to P4TG: Histogram-Based RTT Monitoring in the Data Plane](https://arxiv.org/abs/2507.15382)
*Fabian Ihle,Etienne Zink,Michael Menth*

Main category: cs.NI

TL;DR: 引入基于直方图的RTT测量功能改进P4TG，实现无采样线速精确分析，介绍实现与配置并评估适用性。


<details>
  <summary>Details</summary>
Motivation: P4TG在数据平面采样时间相关指标并在控制器收集，导致精度降低，需要改进RTT测量方法。

Method: 为P4TG引入基于直方图的RTT测量功能，实现范围到前缀的转换算法，用多个三元条目进行范围匹配。

Result: 通过将测量值与配置的RTT理论分布进行比较，证明了基于直方图的RTT分析的适用性。

Conclusion: 所提出的基于直方图的RTT测量功能能有效提升P4TG的RTT测量精度，具有实际应用价值。

Abstract: Modern traffic generators are essential tools for evaluating the performance
of network environments. P4TG is a P4-based traffic generator implemented for
Intel Tofino switches that offers high-speed packet generation with
fine-grained measurement capabilities. However, P4TG samples time-based metrics
such as the round-trip time (RTT) in the data plane and collects them at the
controller. This leads to a reduced accuracy. In this paper, we introduce a
histogram-based RTT measurement feature for P4TG. It enables accurate analysis
at line rate without sampling. Generally, histogram bins are modeled as ranges,
and values are matched to a bin. Efficient packet matching in hardware is
typically achieved using ternary content addressable memory (TCAM). However,
representing range matching rules in TCAM poses a challenge. Therefore, we
implemented a range-to-prefix conversion algorithm that models range matching
with multiple ternary entries. This paper describes the data plane
implementation and runtime configuration of RTT histograms in P4TG. Further, we
discuss the efficiency of the ternary decomposition. Our evaluation
demonstrates the applicability of the histogram-based RTT analysis by comparing
the measured values with a configured theoretical distribution of RTTs.

</details>


### [388] [A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction](https://arxiv.org/abs/2507.14186)
*Xiaojie Li,Zhijie Cai,Nan Qi,Chao Dong,Guangxu Zhu,Haixia Ma,Qihui Wu,Shi Jin*

Main category: cs.NI

TL;DR: 提出双策略框架解决低空网络覆盖预测数据稀缺问题，实验显示误差降低7%，实际网络验证MAE误差达5dB水平。


<details>
  <summary>Details</summary>
Motivation: 低空经济扩张凸显低空网络覆盖预测重要性，但基站天线波束模式难获取，收集低空路测数据成本高、样本稀疏，存在特征采样不平衡和模型泛化性差的问题。

Method: 引入基于专家知识的特征压缩和分离表示学习的双策略，前者利用通信专业知识降低特征空间复杂度，后者结合传播模型和子网络提升模型泛化性。

Result: 实验表明该框架比最佳基线算法误差降低7%，实际网络验证达到实用预测精度，MAE误差在5dB水平。

Conclusion: 所提出的双策略框架有效解决了低空网络覆盖预测中数据稀缺带来的问题，具有较好的准确性和可靠性。

Abstract: The expansion of the low-altitude economy has underscored the significance of
Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors.
While accurate LANC forecasting hinges on the antenna beam patterns of Base
Stations (BSs), these patterns are typically proprietary and not readily
accessible. Operational parameters of BSs, which inherently contain beam
information, offer an opportunity for data-driven low-altitude coverage
prediction. However, collecting extensive low-altitude road test data is
cost-prohibitive, often yielding only sparse samples per BS. This scarcity
results in two primary challenges: imbalanced feature sampling due to limited
variability in high-dimensional operational parameters against the backdrop of
substantial changes in low-dimensional sampling locations, and diminished
generalizability stemming from insufficient data samples. To overcome these
obstacles, we introduce a dual strategy comprising expert knowledge-based
feature compression and disentangled representation learning. The former
reduces feature space complexity by leveraging communications expertise, while
the latter enhances model generalizability through the integration of
propagation models and distinct subnetworks that capture and aggregate the
semantic representations of latent features. Experimental evaluation confirms
the efficacy of our framework, yielding a 7% reduction in error compared to the
best baseline algorithm. Real-network validations further attest to its
reliability, achieving practical prediction accuracy with MAE errors at the 5dB
level.

</details>


### [389] [From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks](https://arxiv.org/abs/2507.14188)
*Sebastian Barros Elgueta*

Main category: cs.NI

TL;DR: 本文探讨全轨道移动网络可行性，提出架构并模拟，指出瓶颈，给出15年路线图。


<details>
  <summary>Details</summary>
Motivation: 现有卫星移动接入系统有局限，探讨完整移动网络能否全在轨运行并在大城市提供服务。

Method: 提出全轨道电信端到端系统架构，分析密集城市条件下的频谱效率等，进行模拟。

Result: 模拟显示屋顶和视距用户可维持64 - QAM吞吐量，街道级接入在中继或辅助波束模式下可行。

Conclusion: 剩余约束是工程瓶颈非物理限制，给出从现有系统到自主轨道覆盖的15年路线图。

Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard
smartphones, using unmodified 3GPP protocols, connected directly to low Earth
orbit (LEO) satellites. This first wave of direct-to-device (D2D)
demonstrations validated the physical feasibility of satellite-based mobile
access. However, these systems remain fallback-grade--rural-only,
bandwidth-limited, and fully dependent on Earth-based mobile cores for
identity, session, and policy control. This paper asks a more ambitious
question: Can a complete mobile network, including radio access, core
functions, traffic routing, and content delivery, operate entirely from orbit?
And can it deliver sustained, urban-grade service in the world's densest
cities? We present the first end-to-end system architecture for a fully orbital
telco, integrating electronically steered phased arrays with 1000-beam
capacity, space-based deployment of 5G core functions (UPF, AMF), and
inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam
capacity, and link budgets under dense urban conditions, accounting for path
loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight
users can sustain 64-QAM throughput, while street-level access is feasible with
relay or assisted beam modes. The paper outlines the remaining constraints,
power, thermal dissipation, compute radiation hardening, and regulatory models,
and demonstrates that these are engineering bottlenecks, not physical limits.
Finally, we propose a staged 15-year roadmap from today's fallback D2D systems
to autonomous orbital overlays delivering 50-100 Mbps to handhelds in
megacities, with zero reliance on terrestrial infrastructure.

</details>


### [390] [PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence](https://arxiv.org/abs/2507.14211)
*Federico Mason,Tommaso Zugno,Matteo Drago,Marco Giordani,Mate Boban,Michele Zorzi*

Main category: cs.NI

TL;DR: 提出新模拟框架PRATA用于远程驾驶应用的预测性QoS，设计RAN - AI优化数据分割，提升系统性能并研究RL实现相关因素影响。


<details>
  <summary>Details</summary>
Motivation: 预测性QoS对汽车应用如远程驾驶很有用，强化学习是实现PQoS的有前景工具，需新框架支持。

Method: 提出PRATA模拟框架，包含模拟5G RAN的协议栈、生成汽车数据工具和优化PQoS决策的AI单元；用PRATA设计RAN - AI优化数据分割。

Result: RAN - AI有效平衡QoS和QoE，系统性能相比基线方法近乎翻倍；研究了状态空间和获取网络数据成本对RL实现的影响。

Conclusion: PRATA框架和RAN - AI能有效提升远程驾驶应用性能，同时明确了RL实现中相关因素的影响。

Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS
changes, e.g., in wireless networks, and trigger appropriate countermeasures to
avoid performance degradation. Hence, PQoS is extremely useful for automotive
applications such as teleoperated driving, which poses strict constraints in
terms of latency and reliability. A promising tool for PQoS is given by
Reinforcement Learning (RL), a methodology that enables the design of
decision-making strategies for stochastic optimization. In this manuscript, we
present PRATA, a new simulation framework to enable PRedictive QoS based on AI
for Teleoperated driving Applications. PRATA consists of a modular pipeline
that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access
Network (RAN), (ii) a tool for generating automotive data, and (iii) an
Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its
utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the
segmentation level of teleoperated driving data in the event of resource
saturation or channel degradation. Hence, we show that the RAN-AI entity
efficiently balances the trade-off between QoS and Quality of Experience (QoE)
that characterize teleoperated driving applications, almost doubling the system
performance compared to baseline approaches. In addition, by varying the
learning settings of the RAN-AI entity, we investigate the impact of the state
space and the relative cost of acquiring network data that are necessary for
the implementation of RL.

</details>


### [391] [Intent-Based Network for RAN Management with Large Language Models](https://arxiv.org/abs/2507.14230)
*Fransiscus Asisi Bimo,Maria Amparo Canaveras Galdon,Chun-Kai Lai,Ray-Guang Cheng,Edwin K. P. Chong*

Main category: cs.NI

TL;DR: 本文提出利用大语言模型实现基于意图的无线接入网自动化管理方法，可提升能效和资源管理能力。


<details>
  <summary>Details</summary>
Motivation: 应对无线网络管理复杂度提升，需要先进智能自动化手段。

Method: 在智能体架构中集成大语言模型，提出结构化提示工程技术，通过闭环机制动态优化参数。

Result: 网络能自动提升能源效率，可基于实时反馈调整策略实现资源管理。

Conclusion: 基于大语言模型的自动化方法在无线接入网管理中有潜力实现强大的资源管理。

Abstract: Advanced intelligent automation becomes an important feature to deal with the
increased complexity in managing wireless networks. This paper proposes a novel
automation approach of intent-based network for Radio Access Networks (RANs)
management by leveraging Large Language Models (LLMs). The proposed method
enhances intent translation, autonomously interpreting high-level objectives,
reasoning over complex network states, and generating precise configurations of
the RAN by integrating LLMs within an agentic architecture. We propose a
structured prompt engineering technique and demonstrate that the network can
automatically improve its energy efficiency by dynamically optimizing critical
RAN parameters through a closed-loop mechanism. It showcases the potential to
enable robust resource management in RAN by adapting strategies based on
real-time feedback via LLM-orchestrated agentic systems.

</details>


### [392] [Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts](https://arxiv.org/abs/2507.14263)
*Ramesh Raskar,Pradyumna Chari,John Zinky,Mahesh Lambe,Jared James Grogan,Sichao Wang,Rajesh Ranjan,Rekha Singhal,Shailja Gupta,Robert Lincourt,Raghu Bala,Aditi Joshi,Abhishek Singh,Ayush Chopra,Dimitris Stripelis,Bhuwan B,Sumit Kumar,Maria Gorskikh*

Main category: cs.NI

TL;DR: 本文提出NANDA索引架构，为AI智能体互联网提供可发现性、可识别性和认证功能，有五项保证，构建轻量级可扩展基础。


<details>
  <summary>Details</summary>
Motivation: 互联网将承载大量自主AI智能体，以DNS为中心的身份和发现机制难以应对其工作负载，需要新架构。

Method: 提出最小化精简索引解析为动态、可加密验证的AgentFacts的架构，形式化AgentFacts模式，指定基于CRDT的更新协议，开发自适应解析器原型。

Result: 实现五项具体保证，构建轻量级、水平可扩展的基础。

Conclusion: 该架构可在不放弃现有网络基础设施的情况下，为下一代AI智能体互联网实现安全、信任感知的协作。

Abstract: The Internet is poised to host billions to trillions of autonomous AI agents
that negotiate, delegate, and migrate in milliseconds and workloads that will
strain DNS-centred identity and discovery. In this paper, we describe the NANDA
index architecture, which we envision as a means for discoverability,
identifiability and authentication in the internet of AI agents. We present an
architecture where a minimal lean index resolves to dynamic, cryptographically
verifiable AgentFacts that supports multi-endpoint routing, load balancing,
privacy-preserving access, and credentialed capability assertions. Our
architecture design delivers five concrete guarantees: (1) A quilt-like index
proposal that supports both NANDA-native agents as well as third party agents
being discoverable via the index, (2) rapid global resolution for newly spawned
AI agents, (3) sub-second revocation and key rotation, (4) schema-validated
capability assertions, and (5) privacy-preserving discovery across
organisational boundaries via verifiable, least-disclosure queries. We
formalize the AgentFacts schema, specify a CRDT-based update protocol, and
prototype adaptive resolvers. The result is a lightweight, horizontally
scalable foundation that unlocks secure, trust-aware collaboration for the next
generation of the Internet of AI agents, without abandoning existing web
infrastructure.

</details>


### [393] [Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches](https://arxiv.org/abs/2507.14633)
*Xiaozheng Gao,Yichen Wang,Bosen Liu,Xiao Zhou,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Dong In Kim,Abbas Jamalipour,Chau Yuen,Jianping An,Kai Yang*

Main category: cs.NI

TL;DR: 本文聚焦卫星增强低空经济与地面网络（SLAETNs），探讨通过生成式AI和大语言模型实现智能自主系统，介绍架构与挑战，回顾生成模型，分析其应用，给出未来方向。


<details>
  <summary>Details</summary>
Motivation: SLAETNs发展需要智能自主系统，解决在异构、动态和关键任务环境中可靠运行的挑战。

Method: 先介绍SLAETNs架构与特征，分析集成挑战，系统回顾五类生成模型并对比分析，研究模型在三个领域的应用。

Result: 明确各类生成模型在SLAETNs中的生成机制、能力和部署权衡，以及在不同领域的作用。

Conclusion: 为下一代集成网络中推进智能AI提供统一理解和可操作参考。

Abstract: The development of satellite-augmented low-altitude economy and terrestrial
networks (SLAETNs) demands intelligent and autonomous systems that can operate
reliably across heterogeneous, dynamic, and mission-critical environments. To
address these challenges, this survey focuses on enabling agentic artificial
intelligence (AI), that is, artificial agents capable of perceiving, reasoning,
and acting, through generative AI (GAI) and large language models (LLMs). We
begin by introducing the architecture and characteristics of SLAETNs, and
analyzing the challenges that arise in integrating satellite, aerial, and
terrestrial components. Then, we present a model-driven foundation by
systematically reviewing five major categories of generative models:
variational autoencoders (VAEs), generative adversarial networks (GANs),
generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.
Moreover, we provide a comparative analysis to highlight their generative
mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on
this foundation, we examine how these models empower agentic functions across
three domains: communication enhancement, security and privacy protection, and
intelligent satellite tasks. Finally, we outline key future directions for
building scalable, adaptive, and trustworthy generative agents in SLAETNs. This
survey aims to provide a unified understanding and actionable reference for
advancing agentic AI in next-generation integrated networks.

</details>


### [394] [Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness](https://arxiv.org/abs/2507.15145)
*Thai T. Vu,John Le*

Main category: cs.NI

TL;DR: 提出通信高效、事件触发的合作边缘AI系统推理框架，实验显示性能和公平性提升。


<details>
  <summary>Details</summary>
Motivation: 为含多用户设备和边缘服务器的合作边缘AI系统设计通信高效的推理框架。

Method: 基于双阈值提前退出策略，将单设备推理扩展到分布式多设备场景，结合比例公平约束，建立联合优化框架，利用效用函数单调性和Benders分解交替优化。

Result: 该框架相比单设备基线显著提升系统性能和资源分配公平性。

Conclusion: 所提框架能有效提升合作边缘AI系统的性能和公平性。

Abstract: This paper proposes a communication-efficient, event-triggered inference
framework for cooperative edge AI systems comprising multiple user devices and
edge servers. Building upon dual-threshold early-exit strategies for rare-event
detection, the proposed approach extends classical single-device inference to a
distributed, multi-device setting while incorporating proportional fairness
constraints across users. A joint optimization framework is formulated to
maximize classification utility under communication, energy, and fairness
constraints. To solve the resulting problem efficiently, we exploit the
monotonicity of the utility function with respect to the confidence thresholds
and apply alternating optimization with Benders decomposition. Experimental
results show that the proposed framework significantly enhances system-wide
performance and fairness in resource allocation compared to single-device
baselines.

</details>


### [395] [User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks](https://arxiv.org/abs/2507.15254)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: 为支持高带宽低延迟服务，应对XR内容实时同步挑战，提出H2M协作和HMC - DBA方案，经模拟验证可低带宽满足要求且提高资源利用率。


<details>
  <summary>Details</summary>
Motivation: 未来移动和固定无线网络需支持高带宽低延迟服务，XR内容实时同步困难，需保障用户体验。

Method: 用双向长短期记忆网络预测人类头部动作，根据头部动作预测XR帧大小和带宽需求，提出HMC - DBA方案。

Result: 模拟显示可在企业网络中以低带宽满足XR帧端到端延迟和抖动要求，比现有方案提高网络资源利用效率。

Conclusion: 提出的HMC - DBA方案有效，能满足需求并提升资源利用率。

Abstract: The evolution towards future generation of mobile systems and fixed wireless
networks is primarily driven by the urgency to support high-bandwidth and
low-latency services across various vertical sectors. This endeavor is fueled
by smartphones as well as technologies like industrial internet of things,
extended reality (XR), and human-to-machine (H2M) collaborations for fostering
industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To
ensure an ideal immersive experience and avoid cyber-sickness for users in all
the aforementioned usage scenarios, it is typically challenging to synchronize
XR content from a remote machine to a human collaborator according to their
head movements across a large geographic span in real-time over communication
networks. Thus, we propose a novel H2M collaboration scheme where the human's
head movements are predicted ahead with highly accurate models like
bidirectional long short-term memory networks to orient the machine's camera in
advance. We validate that XR frame size varies in accordance with the human's
head movements and predict the corresponding bandwidth requirements from the
machine's camera to propose a human-machine coordinated dynamic bandwidth
allocation (HMC-DBA) scheme. Through extensive simulations, we show that
end-to-end latency and jitter requirements of XR frames are satisfied with much
lower bandwidth consumption over enterprise networks like
Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in
network resource utilization is achieved by employing our proposed HMC-DBA over
state-of-the-art schemes.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [396] [Left Leaning Models: AI Assumptions on Economic Policy](https://arxiv.org/abs/2507.15771)
*Maxim Chupilkin*

Main category: cs.CY

TL;DR: 本文用联合实验探究大语言模型评估经济政策的主要影响因素，发现其对失业、不平等、金融稳定和环境危害更敏感，结果跨场景和模型较一致。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在经济学中的应用呈指数级增长，但对经济问题的假设仍是黑箱，需了解其评估经济政策的影响因素。

Method: 采用联合实验的方法。

Result: 大语言模型对失业、不平等、金融稳定和环境危害最敏感，对经济增长、通胀和政府债务等传统宏观经济问题不太敏感，且结果跨场景和模型较一致。

Conclusion: 明确了影响大语言模型评估经济政策的主要因素及其敏感度特点。

Abstract: How does AI think about economic policy? While the use of large language
models (LLMs) in economics is growing exponentially, their assumptions on
economic issues remain a black box. This paper uses a conjoint experiment to
tease out the main factors influencing LLMs' evaluation of economic policy. It
finds that LLMs are most sensitive to unemployment, inequality, financial
stability, and environmental harm and less sensitive to traditional
macroeconomic concerns such as economic growth, inflation, and government debt.
The results are remarkably consistent across scenarios and across models.

</details>


### [397] [Cognitive Castes: Artificial Intelligence, Epistemic Stratification, and the Dissolution of Democratic Discourse](https://arxiv.org/abs/2507.14218)
*Craig S Wright*

Main category: cs.CY

TL;DR: 人工智能加剧认知分层，导致权力技术官僚化，信息不再是公共资源，应重建理性自主性。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能在自由民主社会中的影响和问题，如认知分层、权力结构变化等。

Method: 综合形式认识论、政治理论、算法架构和经济激励结构进行分析。

Result: 人工智能加剧认知分层，造成权力技术官僚化，信息被用于制造同意和压制自主性，审议民主因解释能力的侵蚀而崩溃。

Conclusion: 不应采用技术官僚监管或普遍接入的方式，而应将重建理性自主性作为公民使命，通过教育、认知权利保护和开放认知基础设施建设来实现。

Abstract: Artificial intelligence functions not as an epistemic leveller, but as an
accelerant of cognitive stratification, entrenching and formalising
informational castes within liberal-democratic societies. Synthesising formal
epistemology, political theory, algorithmic architecture, and economic
incentive structures, the argument traces how contemporary AI systems
selectively amplify the reasoning capacity of individuals equipped with
recursive abstraction, symbolic logic, and adversarial interrogation, whilst
simultaneously pacifying the cognitively untrained through engagement-optimised
interfaces. Fluency replaces rigour, immediacy displaces reflection, and
procedural reasoning is eclipsed by reactive suggestion. The result is a
technocratic realignment of power: no longer grounded in material capital
alone, but in the capacity to navigate, deconstruct, and manipulate systems of
epistemic production. Information ceases to be a commons; it becomes the
substrate through which consent is manufactured and autonomy subdued.
Deliberative democracy collapses not through censorship, but through the
erosion of interpretive agency. The proposed response is not technocratic
regulation, nor universal access, but the reconstruction of rational autonomy
as a civic mandate, codified in education, protected by epistemic rights, and
structurally embedded within open cognitive infrastructure.

</details>


### [398] [Culling Misinformation from Gen AI: Toward Ethical Curation and Refinement](https://arxiv.org/abs/2507.14242)
*Prerana Khatiwada,Grace Donaher,Jasymyn Navarro,Lokesh Bhatta*

Main category: cs.CY

TL;DR: 文章指出AI发展带来变革，但也有风险。探讨ChatGPT和深度伪造引发的公平性与虚假信息问题，分析其在多领域的影响，并提出解决问题的指南和政策建议。


<details>
  <summary>Details</summary>
Motivation: 认识并理解新AI技术带来的风险，减少其可能造成的危害。

Method: 参考众多学术资源，分析新AI技术引发问题的原因和影响。

Result: 分析了新AI技术在医疗、教育、科学、学术、零售和金融等领域带来的问题。

Conclusion: 建议用户、开发者和政府合作，遵循未来导向的指南和政策，在解决问题的同时推动相关领域创新。

Abstract: While Artificial Intelligence (AI) is not a new field, recent developments,
especially with the release of generative tools like ChatGPT, have brought it
to the forefront of the minds of industry workers and academic folk alike.
There is currently much talk about AI and its ability to reshape many everyday
processes as we know them through automation. It also allows users to expand
their ideas by suggesting things they may not have thought of on their own and
provides easier access to information. However, not all of the changes this
technology will bring or has brought so far are positive; this is why it is
extremely important for all modern people to recognize and understand the risks
before using these tools and allowing them to cause harm. This work takes a
position on better understanding many equity concerns and the spread of
misinformation that result from new AI, in this case, specifically ChatGPT and
deepfakes, and encouraging collaboration with law enforcement, developers, and
users to reduce harm. Considering many academic sources, it warns against these
issues, analyzing their cause and impact in fields including healthcare,
education, science, academia, retail, and finance. Lastly, we propose a set of
future-facing guidelines and policy considerations to solve these issues while
still enabling innovation in these fields, this responsibility falling upon
users, developers, and government entities.

</details>


### [399] [Bridging MOOCs, Smart Teaching, and AI: A Decade of Evolution Toward a Unified Pedagogy](https://arxiv.org/abs/2507.14266)
*Bo Yuan,Jiazi Hu*

Main category: cs.CY

TL;DR: 文章回顾高教三种范式，分析其优劣，提出三层教学框架并以课程设计展示可行性，框架可提升学习体验。


<details>
  <summary>Details</summary>
Motivation: 三种高教范式因技术和政策原因孤立实施，需统一教学视角。

Method: 分析各范式起源、优缺点，提出三层教学框架，并给出项目课程的课程设计。

Result: 所提框架能提升学习者参与度、支持教师并实现个性化且可扩展的学习。

Conclusion: 应采用统一的教学视角，结合三种范式优势的框架具有可行性和积极效果。

Abstract: Over the past decade, higher education has evolved through three distinct
paradigms: the emergence of Massive Open Online Courses (MOOCs), the
integration of Smart Teaching technologies into classrooms, and the rise of
AI-enhanced learning. Each paradigm is intended to address specific challenges
in traditional education: MOOCs enable ubiquitous access to learning resources;
Smart Teaching supports real-time interaction with data-driven insights; and
generative AI offers personalized feedback and on-demand content generation.
However, these paradigms are often implemented in isolation due to their
disparate technological origins and policy-driven adoption. This paper examines
the origins, strengths, and limitations of each paradigm, and advocates a
unified pedagogical perspective that synthesizes their complementary
affordances. We propose a three-layer instructional framework that combines the
scalability of MOOCs, the responsiveness of Smart Teaching, and the adaptivity
of AI. To demonstrate its feasibility, we present a curriculum design for a
project-based course. The findings highlight the framework's potential to
enhance learner engagement, support instructors, and enable personalized yet
scalable learning.

</details>


### [400] [Fiduciary AI for the Future of Brain-Technology Interactions](https://arxiv.org/abs/2507.14339)
*Abhishek Bhattacharjee,Jack Pilkington,Nita Farahany*

Main category: cs.CY

TL;DR: 本文探讨脑基础模型与脑机接口结合的应用、风险，提出通过技术设计嵌入信托义务保障用户利益。


<details>
  <summary>Details</summary>
Motivation: 脑基础模型与脑机接口结合有变革性应用，但存在前所未有的风险，如潜意识信号被利用、认知自由受侵蚀，用户难以控制信号解读，需保障用户利益。

Method: 借鉴法律传统和AI对齐技术，通过技术设计将信托义务（忠诚、关怀和保密）嵌入脑机接口集成的脑基础模型，提出可实施的架构和治理机制。

Result: 提出了可实施的架构和治理机制，以确保系统符合用户最佳利益。

Conclusion: 将脑基础模型置于信托基础上对实现其潜力且不损害自决权至关重要。

Abstract: Brain foundation models represent a new frontier in AI: instead of processing
text or images, these models interpret real-time neural signals from EEG, fMRI,
and other neurotechnologies. When integrated with brain-computer interfaces
(BCIs), they may enable transformative applications-from thought controlled
devices to neuroprosthetics-by interpreting and acting on brain activity in
milliseconds. However, these same systems pose unprecedented risks, including
the exploitation of subconscious neural signals and the erosion of cognitive
liberty. Users cannot easily observe or control how their brain signals are
interpreted, creating power asymmetries that are vulnerable to manipulation.
This paper proposes embedding fiduciary duties-loyalty, care, and
confidentiality-directly into BCI-integrated brain foundation models through
technical design. Drawing on legal traditions and recent advancements in AI
alignment techniques, we outline implementable architectural and governance
mechanisms to ensure these systems act in users' best interests. Placing brain
foundation models on a fiduciary footing is essential to realizing their
potential without compromising self-determination.

</details>


### [401] [Identifying Algorithmic and Domain-Specific Bias in Parliamentary Debate Summarisation](https://arxiv.org/abs/2507.14221)
*Eoghan Cunningham,James Cross,Derek Greene*

Main category: cs.CY

TL;DR: 本文探索用大语言模型总结欧洲议会辩论，发现存在算法和代表性偏差，提出多阶段框架，强调民主应用需领域敏感评估指标和伦理监督。


<details>
  <summary>Details</summary>
Motivation: 使议会辩论总结准确、简洁且公平代表各方观点，解决大语言模型总结议会辩论时可能出现的算法和代表性偏差问题。

Method: 提出结构化多阶段总结框架，用专有和开源大语言模型进行实验。

Result: 发现存在一致的位置和党派偏差，不同模型和总结策略偏差不同，分层方法最有潜力减少差异。

Conclusion: 在民主应用中部署大语言模型需要领域敏感的评估指标和伦理监督。

Abstract: The automated summarisation of parliamentary debates using large language
models (LLMs) offers a promising way to make complex legislative discourse more
accessible to the public. However, such summaries must not only be accurate and
concise but also equitably represent the views and contributions of all
speakers. This paper explores the use of LLMs to summarise plenary debates from
the European Parliament and investigates the algorithmic and representational
biases that emerge in this context. We propose a structured, multi-stage
summarisation framework that improves textual coherence and content fidelity,
while enabling the systematic analysis of how speaker attributes -- such as
speaking order or political affiliation -- influence the visibility and
accuracy of their contributions in the final summaries. Through our experiments
using both proprietary and open-weight LLMs, we find evidence of consistent
positional and partisan biases, with certain speakers systematically
under-represented or misattributed. Our analysis shows that these biases vary
by model and summarisation strategy, with hierarchical approaches offering the
greatest potential to reduce disparity. These findings underscore the need for
domain-sensitive evaluation metrics and ethical oversight in the deployment of
LLMs for democratic applications.

</details>


### [402] [Mining Voter Behaviour and Confidence: A Rule-Based Analysis of the 2022 U.S. Elections](https://arxiv.org/abs/2507.14236)
*Md Al Jubair,Mohammad Shamsul Arefin,Ahmed Wasif Reza*

Main category: cs.CY

TL;DR: 本文运用规则型数据挖掘技术分析2022年美国选举表现调查数据，揭示选民信任与选举经历关系，强调改善投票渠道和针对性支持对建立选举信任的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究选民信任与选举经历之间的关系。

Method: 运用基于规则的数据挖掘技术，采用Apriori算法，设置支持度>=3%、置信度>=60%、提升度>1.5等参数进行分析，还调整支持阈值至2%对少数族裔选民进行特定分析。

Result: 发现人口属性与投票挑战有强关联；如访问投票站容易且有适度信心的受访者更可能信任选举结果且无登记问题；98.16%报告投票点访问容易的黑人选民登记顺利；对计票过程有高信心的人更可能支持民主党。

Conclusion: 改善投票渠道和提供针对性支持对建立选举系统信任，特别是在边缘化社区中，起着重要作用。

Abstract: This study explores the relationship between voter trust and their
experiences during elections by applying a rule-based data mining technique to
the 2022 Survey of the Performance of American Elections (SPAE). Using the
Apriori algorithm and setting parameters to capture meaningful associations
(support >= 3%, confidence >= 60%, and lift > 1.5), the analysis revealed a
strong connection between demographic attributes and voting-related challenges,
such as registration hurdles, accessibility issues, and queue times. For
instance, respondents who indicated that accessing polling stations was "very
easy" and who reported moderate confidence were found to be over six times more
likely (lift = 6.12) to trust their county's election outcome and experience no
registration issues. A further analysis, which adjusted the support threshold
to 2%, specifically examined patterns among minority voters. It revealed that
98.16 percent of Black voters who reported easy access to polling locations
also had smooth registration experiences. Additionally, those who had high
confidence in the vote-counting process were almost two times as likely to
identify as Democratic Party supporters. These findings point to the important
role that enhancing voting access and offering targeted support can play in
building trust in the electoral system, particularly among marginalized
communities.

</details>


### [403] [Unequal Voices: How LLMs Construct Constrained Queer Narratives](https://arxiv.org/abs/2507.15585)
*Atreya Ghosal,Ashim Gupta,Vivek Srikumar*

Main category: cs.CY

TL;DR: 研究指出社会群体在话语中被边缘化表现为叙事话题狭隘，分析大语言模型（LLMs）对酷儿群体的表述存在局限。


<details>
  <summary>Details</summary>
Motivation: 探究社会群体在话语中被边缘化的现象，聚焦大语言模型对酷儿群体的表述情况。

Method: 从有害表述、狭隘表述和话语他者化方面描述大语言模型对酷儿群体的受限表述，并提出假设进行验证。

Result: 大语言模型对酷儿角色的描绘存在显著局限。

Conclusion: 大语言模型在呈现酷儿群体方面存在明显不足。

Abstract: One way social groups are marginalized in discourse is that the narratives
told about them often default to a narrow, stereotyped range of topics. In
contrast, default groups are allowed the full complexity of human existence. We
describe the constrained representations of queer people in LLM generations in
terms of harmful representations, narrow representations, and discursive
othering and formulate hypotheses to test for these phenomena. Our results show
that LLMs are significantly limited in their portrayals of queer personas.

</details>


### [404] [Why can't Epidemiology be automated (yet)?](https://arxiv.org/abs/2507.15617)
*David Bann,Ed Lowther,Liam Wright,Yevgeniya Kovalchuk*

Main category: cs.CY

TL;DR: 本文探讨人工智能在流行病学研究中的应用，指出其有提升效率的可能，但也存在局限，需流行病学家和工程师双向合作。


<details>
  <summary>Details</summary>
Motivation: 明确人工智能可加速甚至自动化流行病学研究，但不清楚具体哪些任务能受益及存在的障碍，且对当前人工智能能力认知不一。

Method: 梳理利用现有数据集的流行病学任务，分析现有AI工具在各环节的效率提升情况，并通过AI生成的流行病学成果举例。

Result: AI在编码和行政任务等方面可提高生产力，但受现有AI模型和人类系统限制；近期开发的智能系统能进行流行病学分析，但质量参差不齐。

Conclusion: 流行病学家有机会对AI系统进行实证测试和基准评估，实现AI潜力需要流行病学家和工程师双向合作。

Abstract: Recent advances in artificial intelligence (AI) - particularly generative AI
- present new opportunities to accelerate, or even automate, epidemiological
research. Unlike disciplines based on physical experimentation, a sizable
fraction of Epidemiology relies on secondary data analysis and thus is
well-suited for such augmentation. Yet, it remains unclear which specific tasks
can benefit from AI interventions or where roadblocks exist. Awareness of
current AI capabilities is also mixed. Here, we map the landscape of
epidemiological tasks using existing datasets - from literature review to data
access, analysis, writing up, and dissemination - and identify where existing
AI tools offer efficiency gains. While AI can increase productivity in some
areas such as coding and administrative tasks, its utility is constrained by
limitations of existing AI models (e.g. hallucinations in literature reviews)
and human systems (e.g. barriers to accessing datasets). Through examples of
AI-generated epidemiological outputs, including fully AI-generated papers, we
demonstrate that recently developed agentic systems can now design and execute
epidemiological analysis, albeit to varied quality (see
https://github.com/edlowther/automated-epidemiology). Epidemiologists have new
opportunities to empirically test and benchmark AI systems; realising the
potential of AI will require two-way engagement between epidemiologists and
engineers.

</details>


<div id='math.AT'></div>

# math.AT [[Back]](#toc)

### [405] [Topological Social Choice: Designing a Noise-Robust Polar Distance for Persistence Diagrams](https://arxiv.org/abs/2507.14340)
*Athanasios Andrikopoulos,Nikolaos Sampanis*

Main category: math.AT

TL;DR: 本文在社会选择理论中引入TDA，提出适用于噪声偏好数据的持久图度量框架，实验证明其优越性并提出构建预测模型的模块化管道。


<details>
  <summary>Details</summary>
Motivation: 社会选择理论中偏好数据几何丰富但对扰动敏感，TDA在此领域未充分探索，需新方法分析。

Method: 提出基于极坐标的距离度量，解决经典距离的局限性。

Result: 实验证明新方法在理论和应用场景表现更好，可用于比较投票结构和偏好动态的拓扑摘要。

Conclusion: 为拓扑学和决策理论的交叉领域提供了新工具，为政治和经济系统的可解释机器学习开辟新方向。

Abstract: Topological Data Analysis (TDA) has emerged as a powerful framework for
extracting robust and interpretable features from noisy high-dimensional data.
In the context of Social Choice Theory, where preference profiles and
collective decisions are geometrically rich yet sensitive to perturbations, TDA
remains largely unexplored. This work introduces a novel conceptual bridge
between these domains by proposing a new metric framework for persistence
diagrams tailored to noisy preference data.We define a polar coordinate-based
distance that captures both the magnitude and orientation of topological
features in a smooth and differentiable manner. Our metric addresses key
limitations of classical distances, such as bottleneck and Wasserstein,
including instability under perturbation, lack of continuity, and
incompatibility with gradient-based learning. The resulting formulation offers
improved behavior in both theoretical and applied settings.To the best of our
knowledge, this is the first study to systematically apply persistent homology
to social choice systems, providing a mathematically grounded method for
comparing topological summaries of voting structures and preference dynamics.
We demonstrate the superiority of our approach through extensive experiments,
including robustness tests and supervised learning tasks, and we propose a
modular pipeline for building predictive models from online preference data.
This work contributes a conceptually novel and computationally effective tool
to the emerging interface of topology and decision theory, opening new
directions in interpretable machine learning for political and economic
systems.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [406] [Geophysics-informed neural network for model-based seismic inversion using surrogate point spread functions](https://arxiv.org/abs/2507.14140)
*Marcus Saraiva,Ana Muller,Alexandre Maul*

Main category: physics.geo-ph

TL;DR: 本文提出结合深度学习与地震建模的地球物理信息神经网络（GINN）用于地震反演，用合成数据训练，效果良好，后续将优化训练和用实际数据验证。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的地震反演方法存在依赖一维平均平稳子波、横向分辨率假设不现实等局限，需要改进。

Method: 提出GINN，用深度卷积神经网络同时估计点扩散函数（PSFs）和波阻抗（IP），将PSFs分为零相位和残余分量，用SEAM一期地球模型合成数据以2D UNet架构训练100轮，使用结合MSE和SSIM的自监督损失函数。

Result: GINN能生成高分辨率IP和逼真PSFs，与预期地质特征相符，相比传统一维子波减少噪声、提高精度。

Conclusion: GINN方法有效，未来需优化训练过程并用实际地震数据验证。

Abstract: Model-based seismic inversion is a key technique in reservoir
characterization, but traditional methods face significant limitations, such as
relying on 1D average stationary wavelets and assuming an unrealistic lateral
resolution. To address these challenges, we propose a Geophysics-Informed
Neural Network (GINN) that integrates deep learning with seismic modeling. This
novel approach employs a Deep Convolutional Neural Network (DCNN) to
simultaneously estimate Point Spread Functions (PSFs) and acoustic impedance
(IP). PSFs are divided into zero-phase and residual components to ensure
geophysical consistency and to capture fine details. We used synthetic data
from the SEAM Phase I Earth Model to train the GINN for 100 epochs
(approximately 20 minutes) using a 2D UNet architecture. The network's inputs
include positional features and a low-frequency impedance (LF-IP) model. A
self-supervised loss function combining Mean Squared Error (MSE) and Structural
Similarity Index Measure (SSIM) was employed to ensure accurate results. The
GINN demonstrated its ability to generate high-resolution IP and realistic
PSFs, aligning with expected geological features. Unlike traditional 1D
wavelets, the GINN produces PSFs with limited lateral resolution, reducing
noise and improving accuracy. Future work will aim to refine the training
process and validate the methodology with real seismic data.

</details>


### [407] [Integrating Newton's Laws with deep learning for enhanced physics-informed compound flood modelling](https://arxiv.org/abs/2507.15021)
*Soheil Radfar,Faezeh Maghsoodifar,Hamed Moftakhari,Hamid Moradkhani*

Main category: physics.geo-ph

TL;DR: 本文开发了物理信息神经网络框架ALPINE用于复合洪水建模，相比基线神经网络有显著改进，能支持洪水预测和风险分析。


<details>
  <summary>Details</summary>
Motivation: 传统水动力模型实时应用或风险评估需大量计算资源，机器学习方法牺牲物理一致性，要解决复合洪水建模难题。

Method: 开发ALPINE框架，同时执行质量守恒和动量方程，集成卷积编解码器架构与ConvLSTM时间处理，用复合损失函数训练。

Result: 相比基线神经网络，ALPINE减少了区域平均预测误差，改善了水位和流速的模型技能指标，物理信息约束在风暴高峰期最有价值。

Conclusion: 该方法得到物理一致的模拟器，能支持复合洪水预测和大规模风险分析，对沿海应急管理有重要意义。

Abstract: Coastal communities increasingly face compound floods, where multiple drivers
like storm surge, high tide, heavy rainfall, and river discharge occur together
or in sequence to produce impacts far greater than any single driver alone.
Traditional hydrodynamic models can provide accurate physics-based simulations
but require substantial computational resources for real-time applications or
risk assessments, while machine learning alternatives often sacrifice physical
consistency for speed, producing unrealistic predictions during extreme events.
This study addresses these challenges by developing ALPINE (All-in-one Physics
Informed Neural Emulator), a physics-informed neural network (PINN) framework
to enforce complete shallow water dynamics in compound flood modeling. Unlike
previous approaches that implement partial constraints, our framework
simultaneously enforces mass conservation and both momentum equations, ensuring
full adherence to Newton's laws throughout the prediction process. The model
integrates a convolutional encoder-decoder architecture with ConvLSTM temporal
processing, trained using a composite loss function that balances data fidelity
with physics-based residuals. Using six historical storm events (four for
training, one for validation, and one held-out for unseen testing), we observe
substantial improvements over baseline neural networks. ALPINE reduces
domain-averaged prediction errors and improves model skill metrics for water
surface elevation and velocity components. Physics-informed constraints prove
most valuable during peak storm intensity, when multiple flood drivers interact
and reliable predictions matter most. This approach yields a physically
consistent emulator capable of supporting compound-flood forecasting and
large-scale risk analyses while preserving physical realism essential for
coastal emergency management.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [408] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: 本文介绍了一种上下文感知框架Polymorph，可用于嵌入式设备上的实时多标签视频分类，能降低能耗并提高mAP。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备的计算和能源预算有限，而视频流的结构特性可用于更高效的推理。

Method: 引入Polymorph框架，每帧激活最少的轻量级低秩适配器（LoRA），运行时动态选择和组合所需适配器。

Result: 在TAO数据集上，Polymorph能耗降低40%，mAP提高9个点。

Conclusion: Polymorph的模块化策略提高了可扩展性，降低了延迟和能源开销，且已开源。

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [409] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: 提出DFQ - ViT解决现有数据无量化方法合成样本质量低和模型激活分布差异问题，性能超现有方法且无需微调，符合绿色学习原则。


<details>
  <summary>Details</summary>
Motivation: 现有数据无量化方法合成样本无法平衡全局和局部特征，量化和全精度模型中间层激活分布差异大，导致量化模型性能下降。

Method: 按难度递增合成样本提升合成数据质量；在量化模型校准和推理阶段引入激活校正矩阵使中间层激活与全精度模型对齐。

Result: DFQ - ViT性能显著优于现有DFQ方法，与真实数据量化模型相当，如DeiT - T 3位权重量化性能比现有技术高4.29%。

Conclusion: 方法无需微调，降低计算开销和边缘设备部署障碍，符合绿色学习原则，利于资源受限环境应用。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [410] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 提出检索增强点云补全框架，结合跨模态检索学习结构先验信息，实验证明有效且有泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于跨模态学习的点云补全方法专注特定输入类，限制生成能力，需更好方法。

Method: 提出检索增强点云补全框架，设计SSFE提取跨模态特征并重构参考特征作先验，用双渠道控制门增强相关特征、抑制无关信息；提出PRAG采用分层特征融合机制整合参考先验和输入特征。

Result: 在多数据集和真实场景评估中，能生成细粒度点云。

Conclusion: 方法在生成细粒度点云方面有效，且在处理稀疏数据和未见类别上有泛化能力。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [411] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: 本文介绍参加ImageCLEFmed MEDVQA 2025挑战赛子任务1的方法，采用Florence模型，应用特定增强方法，在KASVIR数据集实验有效，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决胃肠道内窥镜视觉问答问题，参加ImageCLEFmed MEDVQA 2025挑战赛子任务1。

Method: 采用Florence模型作为视觉问答管道的骨干，结合视觉编码器和文本编码器，应用特定增强方法提高泛化能力。

Result: 在KASVIR数据集上微调Florence模型，在官方挑战指标上获得准确响应。

Conclusion: 大型多模态模型在医学视觉问答中有潜力，为后续可解释性、鲁棒性和临床集成工作提供基线。

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [412] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 对生成模型进行T1w到T2w 2D MRI图像翻译的综合基准测试，GAN的Pix2Pix模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: MRI获取所有模态会增加扫描时间和成本，因此研究跨模态合成的计算方法。

Method: 对GAN、扩散模型和流匹配技术进行T1w到T2w 2D MRI图像翻译的基准测试，在三个公开数据集上评估。

Result: GAN的Pix2Pix模型在结构保真度、图像质量和计算效率上优于扩散和流匹配方法，流模型在小数据集和简单任务中易过拟合。

Conclusion: 研究为实际MRI工作流程部署图像翻译技术提供指导，指出跨模态医学图像合成未来研究方向。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [413] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本文比较三个深度学习框架在血细胞图像分类上的性能，发现性能有差异，JAX和PyTorch效率较高。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对特定深度学习框架在血细胞图像分类上的详细性能分析，本文旨在填补这一空白。

Method: 比较TensorFlow with Keras、PyTorch和JAX三个框架对公开BloodMNIST数据集血细胞图像的分类性能，关注推理时间和不同图像大小下的分类表现。

Result: 各框架性能存在差异，受图像分辨率和框架特定优化影响，JAX和PyTorch分类准确率与现有基准相当。

Conclusion: JAX和PyTorch在医学图像分类方面具有较高效率。

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [414] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: 提出用于面部表情识别的Exp - Graph框架，利用图模型表示面部属性结构关系，在三个基准数据集上评估，准确率分别达98.09%、79.01%和56.39%，有强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别对人机交互应用至关重要，将结构信息融入面部属性对表情识别很关键。

Method: 提出Exp - Graph框架，用面部关键点作图顶点，根据关键点接近度和局部外观相似度确定边，用图卷积网络捕捉和整合结构依赖，利用视觉Transformer和图卷积块挖掘局部和全局依赖。

Result: 在Oulu - CASIA、eNTERFACE05和AFEW三个基准数据集上，模型识别准确率分别为98.09%、79.01%和56.39%。

Conclusion: Exp - Graph在受控实验室和真实无约束环境都有强泛化能力，对实际面部表情识别应用有效。

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [415] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: 本文介绍了用于跆拳道裁判的AI框架FST.ai，能实时检测头部踢击并计分，减少决策时间，提高一致性和透明度，且具有跨体育项目的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统体育裁判系统存在延迟、主观和执行不一致等问题，影响公平性和运动员信任，需要改进。

Method: 利用计算机视觉、深度学习和边缘推理，基于姿态估计、动作分类和影响分析实现关键动作的自动识别和分类。

Result: 将决策时间从数分钟缩短至数秒，提高了一致性和透明度。

Conclusion: FST.ai框架具有鲁棒性、可扩展性和跨体育项目的应用潜力，能改变多个体育项目的裁判标准。

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [416] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: 本研究提出计算机视觉框架估算餐盘级食物浪费，训练多个模型评估性能，虽有局限但为大规模餐饮环境食物浪费追踪奠定基础。


<details>
  <summary>Details</summary>
Motivation: 量化机构餐饮环境中的餐后食物浪费，以支持数据驱动的可持续发展策略。

Method: 利用RGB图像语义分割，训练四个全监督模型（U - Net、U - Net++及其轻量级变体），使用受限动态逆频率损失和AdamW优化器，用多种指标评估。

Result: 所有模型表现良好，各食物类型至少有一个模型DPA接近或超90%，轻量级模型推理快，干且硬的食物分割效果好，复杂菜品效果差。

Conclusion: 该框架是开创性的、可扩展的非接触式解决方案，为自动实时食物浪费追踪系统奠定基础，为餐饮管理和政策制定者提供方向。

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [417] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: 提出用于多模态全切片图像（WSI）分析的协作式多智能体系统WSI - Agents，实验证明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在WSI多任务分析中表现不如特定任务模型，协作式多智能体系统在病理学领域潜力待挖掘。

Method: 提出WSI - Agents系统，包含任务分配模块、验证机制和总结模块。

Result: 在多模态WSI基准测试中，WSI - Agents在不同任务上优于当前WSI MLLMs和医疗智能体框架。

Conclusion: WSI - Agents能有效提高任务特定准确性和多任务通用性。

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [418] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: 提出LeAdQA方法解决VideoQA问题，实验显示在复杂推理任务上达SOTA且具计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前VideoQA方法存在任务无关采样和启发式检索的缺陷，限制了性能。

Method: 利用LLMs重构问题选项对，引导时间定位模型检索片段，通过自适应融合机制整合证据，用MLLM生成答案。

Result: 在NExT - QA、IntentQA和NExT - GQA上实验表明，精确的视觉定位增强了视频 - 问题关系理解。

Conclusion: LeAdQA方法在复杂推理任务上达到SOTA性能，同时保持计算效率。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [419] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: 提出FOCUS框架，实现对冻结ViTs的可靠高效的空谱可解释性，提升相关指标，适用于高光谱应用。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解释高光谱成像中的Vision Transformers (ViTs) 存在两大挑战，一是难以捕捉有意义的光谱线索，二是全光谱ViTs计算成本高。

Method: 引入特定类别的光谱提示引导注意力到语义有意义的波长组，训练可学习的[SINK]标记吸收噪声或冗余注意力，在单次前向传播中生成3D显著性图和光谱重要性曲线。

Result: 提高波段级IoU 15%，减少注意力崩溃超40%，显著性结果与专家注释高度一致，参数开销不到1%。

Conclusion: 该方法使高分辨率ViT可解释性适用于实际高光谱应用，弥合黑盒建模与可靠HSI决策之间的差距。

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [420] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: 本文提出开发一个浏览器扩展，用于在视频通话中将手语自动翻译成字幕。


<details>
  <summary>Details</summary>
Motivation: 手语交流存在障碍，且疫情下视频会议中听障人士更倾向于用手语交流，需要消除听障人士与普通人的沟通障碍。

Method: 使用包含超2000个单词级美国手语视频、超100位手语者参与的大规模数据集，开发浏览器扩展。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [421] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: 本文提出HICOM框架检测多脸深度伪造视频，基于人类认知确定四个关键线索，实验显示该框架在检测准确率和泛化性上表现良好，还增强了解释性。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法在多脸深度伪造视频检测上因缺乏关键上下文线索而效果不佳，需新方法。

Method: 通过人类研究确定四个关键线索，据此设计HICOM框架检测多脸场景中的假脸，并结合LLM增强解释性。

Result: HICOM在数据集内检测平均准确率提高3.3%，在真实扰动下提高2.8%，在未见数据集上比现有方法高5.8%。

Conclusion: 引入人类因素可增强对深度伪造的防御。

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [422] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: 提出统一量化框架SegQuant解决扩散模型量化问题，适用范围广且与部署工具兼容。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算密集，现有后训练量化方法通用性差，难以集成到工业部署流程。

Method: 提出SegQuant框架，包含SegLinear策略和DualScale量化方案。

Result: SegQuant适用范围广，能取得良好性能。

Conclusion: SegQuant增强了跨模型通用性，与主流部署工具无缝兼容。

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [423] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: 提出无外部条件的配对图像生成方法，用于生成DBT切片和病灶掩码，缓解标注数据短缺，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 数字乳腺断层合成（DBT）图像中肿块病变分割对乳腺癌早期筛查重要，但高密度乳腺组织使病变隐蔽，缺乏标注数据，现有扩散模型生成质量低且无对应标注。

Method: 提出一种配对图像生成方法，为条件扩散模型训练额外的扩散引导器，实现无外部条件的配对图像生成。

Result: 生成了配对的DBT切片和肿块病变掩码，并用于肿块病变分割任务的监督训练。

Conclusion: 该方法可在无外部条件下提高生成质量，缓解标注数据短缺，提升下游任务性能。

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [424] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: 提出一体化视频恢复框架，结合基础模型将视频帧语义上下文与自然语言关联，还呼吁标准化基准并提出新基准，实验表现达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视频恢复时可能需要先验的退化知识，缺乏可解释和灵活的指导，且缺乏标准化基准。

Method: 提出一体化视频恢复框架，不依赖退化知识，学习近似的关联知识，推理时可分离基础模型；提出3D、4D多退化基准和两个时变复合退化基准。

Result: 与先前工作对比，在所有基准上取得了最先进的性能。

Conclusion: 所提出的一体化视频恢复框架及基准有效，能推动视频恢复领域发展。

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [425] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: 提出统一2D预训练多模态网络处理RGB图像、文本和点云，简化架构，实现端到端3D视觉定位，减少可训练参数并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖不同模态的独立编码器，模型大且复杂，训练效率低，使用预训练2D多模态模型也难以处理点云数据。

Method: 提出统一2D预训练多模态网络，利用带适配器微调的2D CLIP双模态模型，设计GARF模块融合特征，引入多模态解码器。

Result: 与基线相比，减少约58%可训练参数，3D检测任务提升6.52%，3D视觉定位任务提升6.25%。

Conclusion: 该方法能实现三种模态的统一特征提取和融合，构建端到端3D视觉定位模型，简化架构并提升性能。

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [426] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出StableAnimator++，可在无后处理下基于参考图像和姿势序列生成高质量视频，在基准测试中效果良好。


<details>
  <summary>Details</summary>
Motivation: 当前人类图像动画扩散模型难以在参考图像和驱动视频身体大小或位置差异大时保持身份一致性。

Method: 构建包含训练和推理模块的视频扩散框架，用可学习层结合SVD预测相似变换矩阵，用现成编码器计算图像和面部嵌入，引入分布感知ID适配器，推理阶段提出基于HJB的面部优化。

Result: 在基准测试中定性和定量地证明了StableAnimator++的有效性。

Conclusion: StableAnimator++能有效解决人类图像动画扩散模型中身份一致性问题。

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [427] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: 现有基于预设词汇表和通用提示的多模态大语言模型图像回归方法效果不佳，提出RvTC方法并证明特定数据提示可提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前多模态大语言模型在图像回归任务中面临的关键局限，提升模型性能。

Method: 提出Regression via Transformer-Based Classification (RvTC)方法，用灵活的基于分箱的方法替代词汇受限的分类；使用包含特定图像语义信息的数据特定提示。

Result: RvTC在四个图像评估数据集上仅用图像就达到了最先进性能；在AVA数据集上，添加挑战标题到提示使相关性从0.83提高到0.90。

Conclusion: 多模态大语言模型能从语义提示信息中受益，在多模态回归任务中纳入有意义的文本上下文很重要。

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [428] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: 文章介绍了首个ESD出血源数据集BleedOrigin - Bench和检测跟踪框架BleedOrigin - Net，评估显示其性能达到先进水平。


<details>
  <summary>Details</summary>
Motivation: ESD术中出血需精准实时定位和持续监测出血源，但现有AI方法重区域分割，轻出血源检测与跟踪，且缺乏专业数据集。

Method: 引入含专家标注和伪标签帧的BleedOrigin - Bench数据集，提出BleedOrigin - Net框架，并与常用模型对比。

Result: 实现出血起始检测帧级准确率96.85%（±≤8帧），初始源检测像素级准确率70.24%（≤100 px），点跟踪像素级准确率96.11%（≤100 px）。

Conclusion: 所提出的数据集和框架在ESD出血源定位和跟踪上表现优异，有良好应用前景。

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [429] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: 本文提出OpenBreastUS数据集，用于波方程模拟和成像，可对神经算子进行基准测试，还实现人体乳房体内成像。


<details>
  <summary>Details</summary>
Motivation: 传统波方程数值求解器计算密集且不稳定，现有神经算子因数据集过于简化限制其实用性，需要一个能连接理论与实际成像应用的数据集。

Method: 构建包含8000个解剖学上逼真的人体乳房模型和超1600万次频域波模拟的OpenBreastUS数据集。

Result: 实现了对流行神经算子的全面基准测试，分析其性能、可扩展性和泛化能力，首次用神经算子求解器实现人体乳房高效体内成像。

Conclusion: OpenBreastUS数据集为开发创新神经PDE求解器提供平台，推动其在现实医学成像问题中的应用。

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [430] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: 本文提出LoopNet方法解决实时SLAM系统的闭环检测精度和嵌入式硬件实时计算约束问题，还引入新基准数据集LoopDB。


<details>
  <summary>Details</summary>
Motivation: 解决实时SLAM系统中闭环检测精度和嵌入式硬件实时计算约束这两个主要问题。

Method: 基于经典ResNet架构的多任务变体LoopNet方法，采用少样本学习进行在线再训练，利用DISK描述符。

Result: LoopNet超越手工特征和传统深度学习方法，在不同条件下有更好性能，代码和新基准数据集LoopDB均开源。

Conclusion: LoopNet方法有效解决实时SLAM系统相关问题，新数据集LoopDB可用于基准测试。

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [431] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: 本文提出Coalescent Projection (CP)概念及结合自监督变换的伪类生成方法，在BSCD - FSL基准测试极端域偏移场景实验中有效，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 当前跨域少样本学习中，更新过多Transformer参数因标记样本稀缺会导致过拟合，需解决此问题。

Method: 提出Coalescent Projection (CP)概念替代软提示，提出结合自监督变换（SSTs）的伪类生成方法。

Result: 在BSCD - FSL基准测试的极端域偏移场景综合实验中，所提方法展现出有效性。

Conclusion: 所提方法能有效应对跨域少样本学习中因标记样本稀缺更新参数导致的过拟合问题。

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [432] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出基于条件扩散模型的视频压缩框架，引入三个关键模块，实验表明该方法在感知质量指标上显著优于传统和神经编解码器。


<details>
  <summary>Details</summary>
Motivation: 利用条件扩散模型在重建与人类视觉感知一致的视频内容方面的优势，构建感知优化的视频压缩框架。

Method: 将视频压缩重构为条件生成任务，引入多粒度条件、紧凑表示、多条件训练三个关键模块。

Result: 在Fréchet视频距离（FVD）和LPIPS等感知质量指标上，该方法显著优于传统和神经编解码器，在高压缩比下表现更优。

Conclusion: 所提出的视频压缩框架在视频感知质量重建方面具有显著优势。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [433] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: 提出ExDD框架，通过显式建模双特征分布克服工业缺陷检测中一类异常检测范式的局限，利用多种方法解决数据稀缺等问题，在KSDD2上实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有工业缺陷检测系统采用一类异常检测范式存在局限，如假设异常分布均匀、在现实制造环境中面临数据稀缺问题。

Method: 提出ExDD框架，利用并行内存库捕捉正常和异常模式的统计特性，采用带特定领域文本条件的潜在扩散模型生成合成缺陷，使用邻域感知比率评分机制融合距离度量。

Result: 在KSDD2上实验，I - AUROC达94.2%，P - AUROC达97.7%，100个合成样本时增强效果最佳。

Conclusion: ExDD框架能有效克服现有工业缺陷检测系统的局限，提升检测性能。

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [434] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: 提出融合LiDAR点云和图像的里程计框架，实验表明比现有方法更准确鲁棒。


<details>
  <summary>Details</summary>
Motivation: 里程计对自主系统的自定位和导航至关重要，需准确鲁棒的方法。

Method: 利用深度补全估计的密集深度图，结合带注意力机制的多尺度特征提取网络；利用密集深度信息优化光流估计；使用分层位姿细化模块逐步优化运动估计。

Result: 在KITTI里程计基准测试中，该方法达到与现有视觉和LiDAR里程计方法相似或更优的精度和鲁棒性。

Conclusion: 所提出的LiDAR - 视觉里程计框架准确且鲁棒。

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [435] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: 提出GeMix框架替代传统Mixup，在COVIDx - CT - 3数据集上提升性能，代码公开。


<details>
  <summary>Details</summary>
Motivation: 传统Mixup的像素插值生成不真实图像，影响学习，尤其在医疗应用中。

Method: 提出两阶段GeMix框架，用StyleGAN2 - ADA生成器，从狄利克雷先验采样标签向量并混合，以软标签生成图像。

Result: 在COVIDx - CT - 3数据集上，结合真实数据时，相比传统Mixup提高了macro - F1，降低了COVID - 19检测的假阴性率。

Conclusion: GeMix可替代像素空间Mixup，提供更强正则化和语义保真度，不影响现有训练流程。

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [436] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: 本文介绍了基于大规模人类视频训练的灵巧视觉语言动作模型Being - H0，提出物理指令微调方法和部分级运动标记化方法，构建综合数据处理流程，实验显示其在手部动作生成、指令遵循和现实机器人操作中有出色表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言动作模型在复杂灵巧操作任务中表现不佳，在新场景和任务泛化能力差，主要原因是依赖合成数据或缺乏规模和多样性的远程操作演示。

Method: 提出以人类手部为基础操纵器，采用物理指令微调范式，结合大规模人类视频预训练、物理空间对齐和后训练适应；引入部分级运动标记化方法；开发综合数据处理流程构建大规模数据集。

Result: Being - H0在手部动作生成和指令遵循方面表现出色，且随模型和数据规模扩大表现良好，应用物理指令微调后在现实机器人操作中有预期提升。

Conclusion: 所提出的方法和模型Being - H0能有效解决现有视觉语言动作模型的数据瓶颈问题，在机器人操作任务中具有良好效果。

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [437] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 提出训练无关的EgoPrune用于自我运动视频推理，实验显示其优于先前方法，还展示了在边缘设备上的效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型处理长且冗余的自我运动视频计算成本高，现有令牌修剪方法不适用于自我运动视频。

Method: 提出EgoPrune，包含适应EmbodiedR的关键帧选择器、透视感知冗余过滤和基于最大边际相关性的令牌选择器。

Result: 在两个自我运动视频基准测试中，EgoPrune在不同修剪率下始终优于先前训练无关方法，显著降低FLOPs、内存使用和延迟。

Conclusion: EgoPrune在自我运动视频推理中表现出色，适合在设备上进行自我运动视频推理。

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [438] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: 本文探索用基础分割模型解决像素级视觉任务标签稀缺问题，提出ConformalSAM框架，在三个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 像素级视觉任务标注数据获取成本高，半监督语义分割可缓解标注负担，探索基础分割模型作为未标注图像标注器的有效性。

Method: 使用SEEM为未标注数据生成预测掩码，提出ConformalSAM框架，利用保形预测校准基础模型，过滤不可靠像素标签，采用自依赖训练策略。

Result: 在三个标准半监督语义分割基准测试中，ConformalSAM性能优于近期方法，且可作为插件提升其他方法性能。

Conclusion: ConformalSAM能有效利用基础分割模型能力，解决像素级视觉任务标签稀缺问题。

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [439] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: 本文探讨扩散模型在多变量地下建模和概率反演中的应用，提出对扩散后验采样方法的改进，测试显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 考虑扩散模型在多变量地下建模和概率反演中的应用，提升建模能力。

Method: 提出对流行的扩散后验采样方法的不同修正，引入考虑噪声污染的似然近似。

Result: 与原方法相比，统计鲁棒性显著提高，后验概率密度函数采样增强，计算成本降低。

Conclusion: 该方法可处理多种条件数据，反演速度比其他方法快。

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [440] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: 研究将彩票假说应用于深度伪造检测，通过实验发现检测网络存在保留性能的子网络，提出的方法优于一次性剪枝法，还证明了中奖彩票的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法机制不明且模型大，难以在资源受限环境部署，需找出识别深度伪造的关键特征。

Method: 研究彩票假说在深度伪造检测中的应用，对MesoNet、CNN - 5和ResNet - 18架构进行剪枝，采用基于彩票假说的迭代幅度剪枝方法，用Grad - CAM可视化分析。

Result: 深度伪造检测网络存在中奖彩票（子网络），MesoNet在OpenForensic数据集80%稀疏度下保留56.2%准确率，提出的方法优于一次性剪枝法，中奖彩票可跨数据集迁移。

Conclusion: 彩票假说可用于深度伪造检测，能高效剪枝网络，有望构建高效可部署的检测系统。

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [441] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: 本文提出首个基于INR的无损点云几何压缩方法LINR - PCGC，设计编码框架和轻量级编码网络，实验显示其性能优于传统和AI方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的点云压缩方法依赖特定训练数据分布，当前基于INR的方法仅考虑有损几何压缩，需无损点云几何压缩方法。

Method: 提出LINR - PCGC方法，设计点云级编码框架和有效网络初始化策略加速编码，提出基于多尺度SparseConv的轻量级编码网络实现快速推理和紧凑解码器。

Result: 实验表明该方法在MVUB数据集收敛时间下，比特流比G - PCC TMC13v23减少约21.21%，比SparsePCGC减少21.95%。

Conclusion: LINR - PCGC方法有效，性能优于传统和AI方法。

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [442] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 本文指出当前多模态大语言模型在多模态上下文学习中难以利用视觉信息的问题，提出动态注意力重新分配策略和专用数据集，实验证明能提升多模态上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在多模态上下文学习中难以有效利用视觉信息，导致学习仍为单模态，且对其能力增强和性能评估研究不足。

Method: 提出动态注意力重新分配（DARA）微调策略，通过平衡视觉和文本标记的注意力让模型关注视觉上下文；创建专门的TrueMICL数据集，要求集成多模态信息完成任务。

Result: 实验表明所提的整体解决方案有效，显著提升了真正的多模态上下文学习能力。

Conclusion: 所提出的方法和数据集能有效解决多模态大语言模型在多模态上下文学习中的问题，提升其能力。

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [443] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出概念驱动的分割框架SeC和SeCVOS基准，SeC在SeCVOS上显著优于SAM 2.1，实现概念感知视频对象分割新SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前视频对象分割技术依赖外观匹配，在处理视觉变化、遮挡和场景变化方面不如人类，缺乏类人概念理解能力。

Method: 提出SeC框架，用大视觉语言模型构建概念先验，推理时形成目标语义表示，自适应平衡语义推理和特征匹配；引入SeCVOS基准评估模型。

Result: SeC在SeCVOS上比SAM 2.1提升11.8个点。

Conclusion: SeC建立了概念感知视频对象分割的新SOTA。

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [444] [Siamese Neural Network for Label-Efficient Critical Phenomena Prediction in 3D Percolation Models](https://arxiv.org/abs/2507.14159)
*Shanshan Wang,Dian Xu,Jianmin Shen,Feng Gao,Wei Li,Weibing Deng*

Main category: cond-mat.dis-nn

TL;DR: 提出用Siamese神经网络分析3D渗流，准确率高且数据效率高。


<details>
  <summary>Details</summary>
Motivation: 多数机器学习渗流分析框架聚焦二维系统，无法处理三维材料空间相关性和形态复杂性，需提高3D系统标签效率和可扩展性。

Method: 提出Siamese神经网络，利用最大簇特征作为判别输入。

Result: 对三维位点和键渗流阈值及临界指数预测准确率高，误差率低于1%，所需标记样本远少于传统方法。

Conclusion: 建立了用于建模高维临界现象的强大且数据高效的框架，有材料发现和复杂网络分析等潜在应用。

Abstract: Percolation theory serves as a cornerstone for studying phase transitions and
critical phenomena, with broad implications in statistical physics, materials
science, and complex networks. However, most machine learning frameworks for
percolation analysis have focused on two-dimensional systems, oversimplifying
the spatial correlations and morphological complexity of real-world
three-dimensional materials. To bridge this gap and improve label efficiency
and scalability in 3D systems, we propose a Siamese Neural Network (SNN) that
leverages features of the largest cluster as discriminative input. Our method
achieves high predictive accuracy for both site and bond percolation thresholds
and critical exponents in three dimensions, with sub-1% error margins using
significantly fewer labeled samples than traditional approaches. This work
establishes a robust and data-efficient framework for modeling high-dimensional
critical phenomena, with potential applications in materials discovery and
complex network analysis.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [445] [Certificate-Sensitive Subset Sum: Realizing Instance Complexity](https://arxiv.org/abs/2507.15511)
*Jesus Salas*

Main category: cs.CC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present, to our knowledge, the first deterministic, certificate-sensitive
algorithm for a canonical NP-complete problem whose runtime provably adapts to
the structure of each input. For a Subset-Sum instance $(S, t)$, let
$\Sigma(S)$ denote the set of distinct subset sums and define $U =
|\Sigma(S)|$. This set serves as an information-theoretically minimal witness,
the instance-complexity (IC) certificate.
  Our solver, IC-SubsetSum, enumerates every element of $\Sigma(S)$ in
deterministic time $O(U \cdot n^2)$ and space $O(U \cdot n)$. A randomized
variant achieves expected runtime $O(U \cdot n)$. The algorithm's complexity is
thus directly governed by the certificate size, and this structure-sensitive
performance is paired with a guaranteed worst-case runtime of $O^*(2^{n/2 -
\varepsilon})$ for some constant $\varepsilon > 0$, the first such result to
strictly outperform classical methods on every instance.
  We revisit fine-grained reductions that rely on the classical $2^{n/2}$
hardness of SubsetSum and show that these arguments hold only for
collision-free instances where $U$ is maximal. IC-SubsetSum reframes this
barrier structurally and introduces a new paradigm for certificate-sensitive
algorithms across NP-complete problems.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [446] [Dvorak-Dell-Grohe-Rattan theorem via an asymptotic argument](https://arxiv.org/abs/2507.14669)
*Alexander Kozachinskiy*

Main category: math.CO

TL;DR: 本文给出了关于两个图可被Weisfeiler - Leman同构测试区分的充要条件的另一种简单证明。


<details>
  <summary>Details</summary>
Motivation: 已存在关于两个图可被Weisfeiler - Leman同构测试区分的充要条件的两种证明，作者希望给出另一种证明。

Method: 基于对WL标签排序和渐近论证的方法。

Result: 成功给出另一种简单证明。

Conclusion: 可以通过对WL标签排序和渐近论证的方法得到关于该充要条件的新证明。

Abstract: Two graphs $G_1,G_2$ are distinguished by the Weisfeiler--Leman isomorphism
test if and only if there is a tree $T$ that has a different number of
homomorphisms to $G_1$ and to $G_2$. There are two known proofs of this fact --
a logical proof by Dvorak and a linear-algebraic proof by Dell, Grohe, and
Rattan. We give another simple proof, based on ordering WL-labels and
asymptotic arguments.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [447] [Collusion-Resilient Hierarchical Secure Aggregation with Heterogeneous Security Constraints](https://arxiv.org/abs/2507.14768)
*Zhou Li,Xiang Zhang,Jiawen Lv,Jihao Fan,Haiqiang Chen,Giuseppe Caire*

Main category: cs.IT

TL;DR: 文章受联邦学习启发研究安全聚合，提出弱安全分层安全聚合（WS - HSA）应对异构安全需求，刻画最优总密钥率并给出部分情况上下界。


<details>
  <summary>Details</summary>
Motivation: 传统分层安全聚合（HSA）在用户数量增加时面临异构安全需求挑战，需新方案应对。

Method: 研究弱安全HSA（WS - HSA），允许灵活定义安全输入集和勾结集；刻画广泛参数配置下的最优总密钥率，对其余情况建立上下界。

Result: 得到广泛参数配置下的最优总密钥率，其余情况有上下界并提供恒定因子差距最优性保证。

Conclusion: WS - HSA为HSA中的异构安全需求提供了灵活框架。

Abstract: Motivated by federated learning (FL), secure aggregation (SA) aims to
securely compute, as efficiently as possible, the sum of a set of inputs
distributed across many users. To understand the impact of network topology,
hierarchical secure aggregation (HSA) investigated the communication and secret
key generation efficiency in a 3-layer relay network, where clusters of users
are connected to the aggregation server through an intermediate layer of
relays. Due to the pre-aggregation of the messages at the relays, HSA reduces
the communication burden on the relay-to-server links and is able to support a
large number of users. However, as the number of users increases, a practical
challenge arises from heterogeneous security requirements--for example, users
in different clusters may require varying levels of input protection. Motivated
by this, we study weakly-secure HSA (WS-HSA) with collusion resilience, where
instead of protecting all the inputs from any set of colluding users, only the
inputs belonging to a predefined collection of user groups (referred to as
security input sets) need to be protected against another predefined collection
of user groups (referred to as collusion sets). Since the security input sets
and collusion sets can be arbitrarily defined, our formulation offers a
flexible framework for addressing heterogeneous security requirements in HSA.
We characterize the optimal total key rate, i.e., the total number of
independent key symbols required to ensure both server and relay security, for
a broad range of parameter configurations. For the remaining cases, we
establish lower and upper bounds on the optimal key rate, providing
constant-factor gap optimality guarantees.

</details>


### [448] [A DPI-PAC-Bayesian Framework for Generalization Bounds](https://arxiv.org/abs/2507.14795)
*Muhan Guan,Farhad Farokhi,Jingge Zhu*

Main category: cs.IT

TL;DR: 开发DPI - PAC - Bayesian框架推导监督学习泛化误差界，给出三种界，与其他界有关联，选均匀先验分布时结果更优。


<details>
  <summary>Details</summary>
Motivation: 在监督学习中推导泛化误差界。

Method: 将数据处理不等式嵌入测度变换技术，用Rényi、Hellinger p和Chi - Squared散度推导界。

Result: 得到二元Kullback - Leibler泛化差距的显式界，所选均匀先验分布时能消除PAC - Bayes界中的多余松弛项。

Conclusion: 该框架连接数据处理和PAC - Bayesian观点，是构建泛化保证的灵活信息论工具。

Abstract: We develop a unified Data Processing Inequality PAC-Bayesian framework --
abbreviated DPI-PAC-Bayesian -- for deriving generalization error bounds in the
supervised learning setting. By embedding the Data Processing Inequality (DPI)
into the change-of-measure technique, we obtain explicit bounds on the binary
Kullback-Leibler generalization gap for both R\'enyi divergence and any
$f$-divergence measured between a data-independent prior distribution and an
algorithm-dependent posterior distribution. We present three bounds derived
under our framework using R\'enyi, Hellinger \(p\) and Chi-Squared divergences.
Additionally, our framework also demonstrates a close connection with other
well-known bounds. When the prior distribution is chosen to be uniform, our
bounds recover the classical Occam's Razor bound and, crucially, eliminate the
extraneous \(\log(2\sqrt{n})/n\) slack present in the PAC-Bayes bound, thereby
achieving tighter results. The framework thus bridges data-processing and
PAC-Bayesian perspectives, providing a flexible, information-theoretic tool to
construct generalization guarantees.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [449] [Complex Dynamics in Psychological Data: Mapping Individual Symptom Trajectories to Group-Level Patterns](https://arxiv.org/abs/2507.14161)
*Eleonora Vitanza,Pietro DeLellis,Chiara Mocenni,Manuel Ruiz Marin*

Main category: stat.AP

TL;DR: 研究整合多种方法分析个体症状轨迹能否揭示诊断模式，提出新分析流程，结果显示能提升诊断区分度。


<details>
  <summary>Details</summary>
Motivation: 探究个体症状轨迹是否能揭示有意义的诊断模式。

Method: 使用PCMCI+算法确定症状因果网络，计算复杂度指标丰富数据集并使用机器学习算法辅助诊断。

Result: PCMCI+突出症状网络个体特性，新数据集对症状动态分类准确率达91%。

Conclusion: 整合因果建模和时间复杂度能提升诊断区分度，为临床心理学和心理学研究提供基础。

Abstract: This study integrates causal inference, graph analysis, temporal complexity
measures, and machine learning to examine whether individual symptom
trajectories can reveal meaningful diagnostic patterns. Testing on a
longitudinal dataset of N=45 individuals affected by General Anxiety Disorder
(GAD) and/or Major Depressive Disorder (MDD) derived from Fisher et al. 2017,
we propose a novel pipeline for the analysis of the temporal dynamics of
psychopathological symptoms. First, we employ the PCMCI+ algorithm with
nonparametric independence test to determine the causal network of nonlinear
dependencies between symptoms in individuals with different mental disorders.
We found that the PCMCI+ effectively highlights the individual peculiarities of
each symptom network, which could be leveraged towards personalized therapies.
At the same time, aggregating the networks by diagnosis sheds light to
disorder-specific causal mechanisms, in agreement with previous
psychopathological literature. Then, we enrich the dataset by computing
complexity-based measures (e.g. entropy, fractal dimension, recurrence) from
the symptom time series, and feed it to a suitably selected machine learning
algorithm to aid the diagnosis of each individual. The new dataset yields 91%
accuracy in the classification of the symptom dynamics, proving to be an
effective diagnostic support tool. Overall, these findings highlight how
integrating causal modeling and temporal complexity can enhance diagnostic
differentiation, offering a principled, data-driven foundation for both
personalized assessment in clinical psychology and structural advances in
psychological research.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [450] [JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks](https://arxiv.org/abs/2505.17593)
*Manuel Valle Torre,Thom van der Velden,Marcus Specht,Catharine Oertel*

Main category: cs.HC

TL;DR: 本文提出开源平台架构JELAI，将学习分析与大语言模型辅导集成于Jupyter Notebook环境，介绍设计实现并展示可行性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI用于教育支持时缺乏教学基础和对学生学习情境的了解，且在真实学习环境中研究学生与工具的交互具有挑战性。

Method: 提出JELAI平台架构，采用模块化、容器化设计，有JupyterLab扩展用于遥测和聊天，中间件处理学习分析和提示增强。

Result: 通过系统性能基准测试和两个概念验证用例，展示了记录多模态数据、分析求助模式和支持AI配置A/B测试的能力。

Conclusion: JELAI的技术框架为研究人员和教育工作者在Jupyter生态系统中开发、部署和研究学习分析驱动的AI辅导提供了灵活工具。

Abstract: Generative AI offers potential for educational support, but often lacks
pedagogical grounding and awareness of the student's learning context.
Furthermore, researching student interactions with these tools within authentic
learning environments remains challenging. To address this, we present JELAI,
an open-source platform architecture designed to integrate fine-grained
Learning Analytics (LA) with Large Language Model (LLM)-based tutoring directly
within a Jupyter Notebook environment. JELAI employs a modular, containerized
design featuring JupyterLab extensions for telemetry and chat, alongside a
central middleware handling LA processing and context-aware LLM prompt
enrichment. This architecture enables the capture of integrated code
interaction and chat data, facilitating real-time, context-sensitive AI
scaffolding and research into student behaviour. We describe the system's
design, implementation, and demonstrate its feasibility through system
performance benchmarks and two proof-of-concept use cases illustrating its
capabilities for logging multi-modal data, analysing help-seeking patterns, and
supporting A/B testing of AI configurations. JELAI's primary contribution is
its technical framework, providing a flexible tool for researchers and
educators to develop, deploy, and study LA-informed AI tutoring within the
widely used Jupyter ecosystem.

</details>


### [451] [Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students](https://arxiv.org/abs/2507.14418)
*Taufiq Daryanto,Sophia Stil,Xiaohan Ding,Daniel Manesh,Sang Won Lee,Tim Lee,Stephanie Lunn,Sarah Rodriguez,Chris Brown,Eugenia Rho*

Main category: cs.HC

TL;DR: 研究用基于大语言模型的技术面试练习工具，探讨用户对对话式AI在出声思考练习中作用的看法，给出设计建议并探讨更广泛问题。


<details>
  <summary>Details</summary>
Motivation: 技术面试出声思考练习机会有限，且缺乏关于用户对对话式AI在此练习中作用的研究。

Method: 使用基于大语言模型的技术面试练习工具，对17名参与者进行研究。

Result: 参与者认可AI在模拟、反馈和从生成示例中学习方面的作用。

Conclusion: 提出设计建议，如促进对话式AI社交存在、提供超越内容分析的反馈等，还探讨了更广泛的考虑因素和研究方向。

Abstract: One challenge in technical interviews is the think-aloud process, where
candidates verbalize their thought processes while solving coding tasks.
Despite its importance, opportunities for structured practice remain limited.
Conversational AI offers potential assistance, but limited research explores
user perceptions of its role in think-aloud practice. To address this gap, we
conducted a study with 17 participants using an LLM-based technical interview
practice tool. Participants valued AI's role in simulation, feedback, and
learning from generated examples. Key design recommendations include promoting
social presence in conversational AI for technical interview simulation,
providing feedback beyond verbal content analysis, and enabling crowdsourced
think-aloud examples through human-AI collaboration. Beyond feature design, we
examined broader considerations, including intersectional challenges and
potential strategies to address them, how AI-driven interview preparation could
promote equitable learning in computing careers, and the need to rethink AI's
role in interview practice by suggesting a research direction that integrates
human-AI collaboration.

</details>


### [452] [XplainAct: Visualization for Personalized Intervention Insights](https://arxiv.org/abs/2507.14767)
*Yanming Zhang,Krishnakumar Hegde,Klaus Mueller*

Main category: cs.HC

TL;DR: 提出XplainAct可视化分析框架，支持亚群体个体层面干预模拟、解释与推理，并通过两个案例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理方法主要关注总体层面效果，在异质性显著系统中存在不足，需支持个体层面干预分析。

Method: 提出XplainAct可视化分析框架。

Result: 通过流行病学中阿片类药物相关死亡调查和总统选举投票倾向分析两个案例研究，证明了XplainAct的有效性。

Conclusion: XplainAct框架能有效支持亚群体个体层面的干预模拟、解释和推理。

Abstract: Causality helps people reason about and understand complex systems,
particularly through what-if analyses that explore how interventions might
alter outcomes. Although existing methods embrace causal reasoning using
interventions and counterfactual analysis, they primarily focus on effects at
the population level. These approaches often fall short in systems
characterized by significant heterogeneity, where the impact of an intervention
can vary widely across subgroups. To address this challenge, we present
XplainAct, a visual analytics framework that supports simulating, explaining,
and reasoning interventions at the individual level within subpopulations. We
demonstrate the effectiveness of XplainAct through two case studies:
investigating opioid-related deaths in epidemiology and analyzing voting
inclinations in the presidential election.

</details>


### [453] [NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments](https://arxiv.org/abs/2507.15072)
*Maisha Maimuna,Minhaz Bin Farukee,Sama Nikanfar,Mahfuza Siddiqua,Ayon Roy,Fillia Makedon*

Main category: cs.HC

TL;DR: 提出一种新型多模态引导模拟器，可让盲人和低视力用户在高保真仓库环境中控制移动机器人，为无障碍远程操作研究提供测试平台和算法参考，设计原则易适配真实机器人。


<details>
  <summary>Details</summary>
Motivation: 工业仓库环境使盲人和低视力用户远程操作机器人风险大且需求高，现有无障碍远程操作在工业环境中的系统研究有限，针对此类用户的多模态引导研究少。

Method: 结合导航网格和定期重新规划路线，为低视力用户提供可见路径线、导航语音提示和基于接近度的触觉反馈。

Result: 开发出实时闭环系统，提供可重复的测试平台和算法参考。

Conclusion: 模拟器设计原则易适配真实机器人，支持在实际仓库中开展可行性研究和部署包容性远程机器人工具。

Abstract: Industrial warehouses are congested with moving forklifts, shelves and
personnel, making robot teleoperation particularly risky and demanding for
blind and low-vision (BLV) operators. Although accessible teleoperation plays a
key role in inclusive workforce participation, systematic research on its use
in industrial environments is limited, and few existing studies barely address
multimodal guidance designed for BLV users. We present a novel multimodal
guidance simulator that enables BLV users to control a mobile robot through a
high-fidelity warehouse environment while simultaneously receiving synchronized
visual, auditory, and haptic feedback. The system combines a navigation mesh
with regular re-planning so routes remain accurate avoiding collisions as
forklifts and human avatars move around the warehouse. Users with low vision
are guided with a visible path line towards destination; navigational voice
cues with clockwise directions announce upcoming turns, and finally
proximity-based haptic feedback notifies the users of static and moving
obstacles in the path. This real-time, closed-loop system offers a repeatable
testbed and algorithmic reference for accessible teleoperation research. The
simulator's design principles can be easily adapted to real robots due to the
alignment of its navigation, speech, and haptic modules with commercial
hardware, supporting rapid feasibility studies and deployment of inclusive
telerobotic tools in actual warehouses.

</details>


### [454] [Efficient Visual Appearance Optimization by Learning from Prior Preferences](https://arxiv.org/abs/2507.15355)
*Zhipeng Li,Yi-Chi Liao,Christian Holz*

Main category: cs.HC

TL;DR: 提出Meta - PO方法结合PBO与元学习提高样本效率，实验表明能让用户更快获得满意外观，使个性化视觉优化更适用于终端用户。


<details>
  <summary>Details</summary>
Motivation: 调整视觉参数找最优设置具挑战性，现有PBO方法需多轮比较，更适合设计师，要让方法适用于普通终端用户。

Method: 提出Meta - PO方法，推断并存储先前用户偏好为模型，用其为新用户智能推荐设计候选。

Result: 在2D和3D内容外观设计任务实验中，目标相似时5.86次迭代、目标不同时8次迭代用户可获满意外观。

Conclusion: Meta - PO通过基于偏好的高效优化使个性化视觉优化更适用于终端用户，有扩展界面个性化潜力。

Abstract: Adjusting visual parameters such as brightness and contrast is common in our
everyday experiences. Finding the optimal parameter setting is challenging due
to the large search space and the lack of an explicit objective function,
leaving users to rely solely on their implicit preferences. Prior work has
explored Preferential Bayesian Optimization (PBO) to address this challenge,
involving users to iteratively select preferred designs from candidate sets.
However, PBO often requires many rounds of preference comparisons, making it
more suitable for designers than everyday end-users. We propose Meta-PO, a
novel method that integrates PBO with meta-learning to improve sample
efficiency. Specifically, Meta-PO infers prior users' preferences and stores
them as models, which are leveraged to intelligently suggest design candidates
for the new users, enabling faster convergence and more personalized results.
An experimental evaluation of our method for appearance design tasks on 2D and
3D content showed that participants achieved satisfactory appearance in 5.86
iterations using Meta-PO when participants shared similar goals with a
population (e.g., tuning for a ``warm'' look) and in 8 iterations even
generalizes across divergent goals (e.g., from ``vintage'', ``warm'', to
``holiday''). Meta-PO makes personalized visual optimization more applicable to
end-users through a generalizable, more efficient optimization conditioned on
preferences, with the potential to scale interface personalization more
broadly.

</details>


### [455] [Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance](https://arxiv.org/abs/2507.15783)
*Mohammad 'Matt' Namvarpour,Brandon Brofsky,Jessica Medina,Mamtaj Akter,Afsaneh Razi*

Main category: cs.HC

TL;DR: 分析青少年对Character.AI聊天机器人的过度依赖情况，给出未来聊天机器人设计建议


<details>
  <summary>Details</summary>
Motivation: 现有研究未涉及青少年与可定制角色聊天机器人的互动，需了解青少年过度依赖模式

Method: 分析318篇13 - 17岁用户在Character.AI子版块的Reddit帖子

Result: 青少年常因情感支持或创意表达使用聊天机器人，后产生强依赖，干扰线下关系和日常，有心理困扰等问题，依赖常因反思危害、回归线下社交或平台限制结束

Conclusion: 为未来聊天机器人设计提供建议，促进自我认知、支持现实参与、让青少年参与开发安全数字工具

Abstract: As Generative Artificial Intelligence (GenAI) driven chatbots like
Character.AI become embedded in adolescent life, they raise concerns about
emotional dependence and digital overreliance. While studies have investigated
the overreliance of adults on these chatbots, they have not investigated teens'
interactions with chatbots with customizable personas. We analyzed 318 Reddit
posts made by users self-reported as 13-17 years old on the Character.AI
subreddit to understand patterns of overreliance. We found teens commonly begin
using chatbots for emotional support or creative expression, but many develop
strong attachments that interfere with offline relationships and daily
routines. Their posts revealed recurring signs of psychological distress,
cycles of relapse, and difficulty disengaging. Teens reported that their
overreliance often ended when they reflect on the harm, return to in-person
social settings, or become frustrated by platform restrictions. Based on the
implications of our findings, we provide recommendations for future chatbot
design so they can promote self-awareness, support real-world engagement, and
involve teens in developing safer digital tools.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [456] [Remote Assistance or Remote Driving: The Impact of Operational Design Domains on ADS-Supporting Systems Selection](https://arxiv.org/abs/2507.14347)
*Ole Hans,Benedikt Walter*

Main category: eess.SY

TL;DR: 本文提出基于ODD和用例分析，在RDS和RAS中选择适合自动驾驶系统支持系统的结构化方法。


<details>
  <summary>Details</summary>
Motivation: 当前选择远程支持系统的方法忽略关键因素，需新方法。

Method: 应用PEGASUS框架描述和分析ODD，引入结构化框架评估选择。

Result: 文中未明确提及具体结果。

Conclusion: 提出基于ODD和用例分析，选择适合的远程支持系统的结构化方法。

Abstract: High level Automated Driving Systems (ADS) can handle many situations, but
they still encounter situations where human intervention is required. In
systems where a physical driver is present in the vehicle, typically SAE Level
3 systems, this intervention is relatively straightforward and is handled by
the in-vehicle driver. However, the complexity increases for Level 4 systems,
where, in most cases, no physical driver remains in the vehicle. The two common
industry solutions for this challenge are the integration of a remote support
system, such as a Remote Driving System (RDS) or Remote Assistance System
(RAS). While it is clear that ADS will require one of these systems, it is less
clear how the suitability of either system for a particular ADS application
should be evaluated. Currently, the selection process often focuses on system
architecture as well as its design and integration challenges. Furthermore,
since many ADS developers choose to develop remote system solutions in-house,
it is advantageous to select the simpler approach to streamline development and
integration efforts. While these decision points are certainly relevant, this
approach overlooks the most critical factors: the use cases and the
complementarity of the ADS and the remote support system within the context of
the Operational Design Design Domain (ODD). This paper proposes a structured
approach for selecting between RDS and RAS as an ADS support system, based on
the defined ODD and use case analysis. To achieve this, the paper applies the
PEGASUS framework to systematically describe and analyze the ODD. A structured
framework is introduced to evaluate and select the most suitable remote support
system for an ADS based on clearly defined criteria.

</details>


### [457] [Large Language Model as An Operator: An Experience-Driven Solution for Distribution Network Voltage Control](https://arxiv.org/abs/2507.14800)
*Xu Yang,Chenhui Lin,Haotian Liu,Qi Wang,Wenchuan Wu*

Main category: eess.SY

TL;DR: 本文提出基于大语言模型的配电网电压控制方案，实验验证了方法有效性及大语言模型在电力系统调度的适用性。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型的推理和信息分析能力，为电力系统调度策略自主生成提供新方法。

Method: 提出基于大语言模型的经验驱动电压控制解决方案，通过经验存储、检索、生成和修改多个模块协作实现策略的自我进化。

Result: 综合实验验证了所提方法的有效性。

Conclusion: 大语言模型适用于解决电力系统调度挑战。

Abstract: With the advanced reasoning and information analysis capabilities, large
language models (LLMs) can offer a novel approach for the autonomous generation
of dispatch strategies in power systems. This letter proposes an LLM-based
experience-driven voltage control solution for distribution networks, which
enables the self-evolution of LLM-based voltage control strategies through the
collaboration and interaction of multiple modules-specifically, experience
storage, experience retrieval, experience generation, and experience
modification. Comprehensive experimental results validate the effectiveness of
the proposed method and highlight the applicability of LLM in addressing power
system dispatch challenges.

</details>


### [458] [Physics-Informed Learning of Proprietary Inverter Models for Grid Dynamic Studies](https://arxiv.org/abs/2507.15259)
*Kyung-Bin Kwon,Sayak Mukherjee,Ramij R. Hossain,Marcelo Elizondo*

Main category: eess.SY

TL;DR: 本文提出基于物理信息神经常微分方程框架来模拟逆变器专有动态，提升电网动态仿真精度。


<details>
  <summary>Details</summary>
Motivation: 当前逆变器制造商常不公开内部控制和参数，给精确动态仿真及相关研究带来挑战。

Method: 提出物理信息潜在神经常微分方程模型（PI - LNM），结合系统物理与神经学习层捕捉专有单元未建模行为。

Result: 通过并网逆变器案例验证，相比仅依赖数据驱动学习的方法，动态仿真精度提高。

Conclusion: 所提方法能有效解决逆变器专有动态模拟问题，提升电网动态仿真准确性。

Abstract: This letter develops a novel physics-informed neural ordinary differential
equations-based framework to emulate the proprietary dynamics of the inverters
-- essential for improved accuracy in grid dynamic simulations. In current
industry practice, the original equipment manufacturers (OEMs) often do not
disclose the exact internal controls and parameters of the inverters, posing
significant challenges in performing accurate dynamic simulations and other
relevant studies, such as gain tunings for stability analysis and controls. To
address this, we propose a Physics-Informed Latent Neural ODE Model (PI-LNM)
that integrates system physics with neural learning layers to capture the
unmodeled behaviors of proprietary units. The proposed method is validated
using a grid-forming inverter (GFM) case study, demonstrating improved dynamic
simulation accuracy over approaches that rely solely on data-driven learning
without physics-based guidance.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [459] [MENO: Hybrid Matrix Exponential-based Neural Operator for Stiff ODEs. Application to Thermochemical Kinetics](https://arxiv.org/abs/2507.14341)
*Ivan Zanardi,Simone Venturi,Marco Panesi*

Main category: physics.comp-ph

TL;DR: 介绍MENO框架求解稀疏非线性结构的刚性ODE系统，验证于三个热化学系统，误差低且有计算加速


<details>
  <summary>Details</summary>
Motivation: 高效求解具有稀疏非线性结构的刚性ODE系统

Method: 将系统分解为低维非线性部分和线性时变子系统，分别用传统神经算子和神经矩阵指数公式处理

Result: 在训练的零维设置中相对误差低于2%，在多维外推区域保持较好精度，有显著计算加速

Conclusion: MENO基于物理的架构具有更好的泛化性和可靠性，为刚性反应系统实时模拟提供可扩展途径

Abstract: We introduce MENO (''Matrix Exponential-based Neural Operator''), a hybrid
surrogate modeling framework for efficiently solving stiff systems of ordinary
differential equations (ODEs) that exhibit a sparse nonlinear structure. In
such systems, only a few variables contribute nonlinearly to the dynamics,
while the majority influence the equations linearly. MENO exploits this
property by decomposing the system into two components: the low-dimensional
nonlinear part is modeled using conventional neural operators, while the linear
time-varying subsystem is integrated using a novel neural matrix exponential
formulation. This approach combines the exact solution of linear time-invariant
systems with learnable, time-dependent graph-based corrections applied to the
linear operators. Unlike black-box or soft-constrained physics-informed (PI)
models, MENO embeds the governing equations directly into its architecture,
ensuring physical consistency (e.g., steady states), improved robustness, and
more efficient training. We validate MENO on three complex thermochemical
systems: the POLLU atmospheric chemistry model, an oxygen mixture in
thermochemical nonequilibrium, and a collisional-radiative argon plasma in one-
and two-dimensional shock-tube simulations. MENO achieves relative errors below
2% in trained zero-dimensional settings and maintains good accuracy in
extrapolatory multidimensional regimes. It also delivers substantial
computational speedups, achieving up to 4 800$\times$ on GPU and 185$\times$ on
CPU compared to standard implicit ODE solvers. Although intrusive by design,
MENO's physics-based architecture enables superior generalization and
reliability, offering a scalable path for real-time simulation of stiff
reactive systems.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [460] [Numerical Artifacts in Learning Dynamical Systems](https://arxiv.org/abs/2507.14491)
*Bing-Ze Lu,Richard Tsai*

Main category: math.NA

TL;DR: 本文揭示所选数值方案对从有限时间点样本学习动力系统结果有潜在严重影响，如会错误识别系统特性。


<details>
  <summary>Details</summary>
Motivation: 许多应用需从有限时间点样本学习动力系统，优化过程中采用的数值方案可能影响学习结果。

Method: 未提及具体方法

Result: 分析表明所选数值方案可能使阻尼振荡系统被错误识别为‘反阻尼’且振荡方向反转，却能拟合给定数据点。

Conclusion: 所选数值方案会对学习动力系统的结果产生潜在严重影响。

Abstract: In many applications, one needs to learn a dynamical system from its
solutions sampled at a finite number of time points. The learning problem is
often formulated
  as an optimization problem over a chosen function class. However, in the
optimization procedure, it is necessary to employ a numerical scheme to
integrate candidate dynamical systems and assess how their solutions fit the
data.
  This paper reveals potentially serious effects of a chosen numerical scheme
on the learning outcome. In particular, our analysis demonstrates that a damped
oscillatory system may be incorrectly identified as having "anti-damping" and
exhibiting a reversed oscillation direction, despite adequately fitting the
given data points.

</details>


### [461] [Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy iteration](https://arxiv.org/abs/2507.15455)
*Hee Jun Yang,Min Jung Kim,Yeoneung Kim*

Main category: math.NA

TL;DR: 提出结合经典动态规划与物理信息神经网络（PINNs）的无网格策略迭代框架，用于求解高维、非凸HJI方程，证明收敛性并通过数值实验验证性能。


<details>
  <summary>Details</summary>
Motivation: 解决随机微分博弈和鲁棒控制中出现的高维、非凸HJI方程。

Method: 在固定反馈策略下求解线性二阶PDE和利用自动微分通过逐点极小极大优化更新控制交替进行。

Result: 证明值函数迭代局部一致收敛到HJI方程唯一粘性解；数值实验表明方法准确且可扩展，在不同案例中表现良好。

Conclusion: 将PINNs与策略迭代结合是求解高维、非凸HJI方程实用且有理论依据的方法，有潜在应用。

Abstract: We propose a mesh-free policy iteration framework that combines classical
dynamic programming with physics-informed neural networks (PINNs) to solve
high-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in
stochastic differential games and robust control. The method alternates between
solving linear second-order PDEs under fixed feedback policies and updating the
controls via pointwise minimax optimization using automatic differentiation.
Under standard Lipschitz and uniform ellipticity assumptions, we prove that the
value function iterates converge locally uniformly to the unique viscosity
solution of the HJI equation. The analysis establishes equi-Lipschitz
regularity of the iterates, enabling provable stability and convergence without
requiring convexity of the Hamiltonian. Numerical experiments demonstrate the
accuracy and scalability of the method. In a two-dimensional stochastic
path-planning game with a moving obstacle, our method matches finite-difference
benchmarks with relative $L^2$-errors below %10^{-2}%. In five- and
ten-dimensional publisher-subscriber differential games with anisotropic noise,
the proposed approach consistently outperforms direct PINN solvers, yielding
smoother value functions and lower residuals. Our results suggest that
integrating PINNs with policy iteration is a practical and theoretically
grounded method for solving high-dimensional, nonconvex HJI equations, with
potential applications in robotics, finance, and multi-agent reinforcement
learning.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [462] [A Hybrid Mixture Approach for Clustering and Characterizing Cancer Data](https://arxiv.org/abs/2507.14380)
*Kazeem Kareem,Fan Dai*

Main category: stat.ME

TL;DR: 本文提出混合无矩阵计算方案用于模型聚类，在乳腺癌和淋巴瘤数据上表现优。


<details>
  <summary>Details</summary>
Motivation: 现代生物医学数据高维，现有含因子分析器的混合模型估计方法计算效率低，无法改变因子分析器大小。

Method: 提出基于高斯混合和广义因子分析器的混合无矩阵计算方案估计聚类和模型参数。

Result: 该方法比现有方法收敛更快，同时保持高聚类精度。

Conclusion: 算法可准确识别和区分乳腺癌类型，为淋巴瘤亚型提供广义表征。

Abstract: Model-based clustering is widely used for identifying and distinguishing
types of diseases. However, modern biomedical data coming with high dimensions
make it challenging to perform the model estimation in traditional cluster
analysis. The incorporation of factor analyzer into the mixture model provides
a way to characterize the large set of data features, but the current
estimation method is computationally impractical for massive data due to the
intrinsic slow convergence of the embedded algorithms, and the incapability to
vary the size of the factor analyzers, preventing the implementation of a
generalized mixture of factor analyzers and further characterization of the
data clusters. We propose a hybrid matrix-free computational scheme to
efficiently estimate the clusters and model parameters based on a Gaussian
mixture along with generalized factor analyzers to summarize the large number
of variables using a small set of underlying factors. Our approach outperforms
the existing method with faster convergence while maintaining high clustering
accuracy. Our algorithms are applied to accurately identify and distinguish
types of breast cancer based on large tumor samples, and to provide a
generalized characterization for subtypes of lymphoma using massive gene
records.

</details>


### [463] [Misspecifying non-compensatory as compensatory IRT: analysis of estimated skills and variance](https://arxiv.org/abs/2507.15222)
*Hiroshi Tamano,Hideitsu Hino,Daichi Mochihashi*

Main category: stat.ME

TL;DR: 本文用理论方法全面研究多维项目反应理论中模型误设时技能低估和高估现象及参数估计渐近方差差异。


<details>
  <summary>Details</summary>
Motivation: 此前研究发现非补偿模型误设为补偿模型时会低估高技能，但现象背后机制未阐明，高估情况及参数估计方差问题不明，故开展研究。

Method: 采用理论研究方法。

Result: 除发现已知的技能低估，还发现原点附近存在技能高估，且研究了考虑和不考虑模型误设时参数估计渐近方差的差异程度。

Conclusion: 文档未明确提及最终结论。

Abstract: Multidimensional item response theory is a statistical test theory used to
estimate the latent skills of learners and the difficulty levels of problems
based on test results. Both compensatory and non-compensatory models have been
proposed in the literature. Previous studies have revealed the substantial
underestimation of higher skills when the non-compensatory model is
misspecified as the compensatory model. However, the underlying mechanism
behind this phenomenon has not been fully elucidated. It remains unclear
whether overestimation also occurs and whether issues arise regarding the
variance of the estimated parameters. In this paper, we aim to provide a
comprehensive understanding of both underestimation and overestimation through
a theoretical approach. In addition to the previously identified
underestimation of the skills, we newly discover that the overestimation of
skills occurs around the origin. Furthermore, we investigate the extent to
which the asymptotic variance of the estimated parameters differs when
considering model misspecification compared to when it is not taken into
account.

</details>


### [464] [ACS: An interactive framework for conformal selection](https://arxiv.org/abs/2507.15825)
*Yu Gui,Ying Jin,Yash Nair,Zhimei Ren*

Main category: stat.ME

TL;DR: 提出自适应共形选择（ACS）框架，支持人在环自适应数据分析，有具体算法，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 在模型无关选择中实现有保证的误差控制，支持人在环自适应数据分析。

Method: 基于共形选择进行推广，设计控制决策可用信息的原则，给出不同目标的具体选择算法。

Result: 通过大量数值模拟和真实数据应用，证明了ACS在大语言模型部署和药物发现中的有效性。

Conclusion: ACS框架能在自适应探索数据的同时严格控制错误发现率，可用于多种选择任务。

Abstract: This paper presents adaptive conformal selection (ACS), an interactive
framework for model-free selection with guaranteed error control. Building on
conformal selection (Jin and Cand\`es, 2023b), ACS generalizes the approach to
support human-in-the-loop adaptive data analysis. Under the ACS framework, we
can partially reuse the data to boost the selection power, make decisions on
the fly while exploring the data, and incorporate new information or
preferences as they arise. The key to ACS is a carefully designed principle
that controls the information available for decision making, allowing the data
analyst to explore the data adaptively while maintaining rigorous control of
the false discovery rate (FDR). Based on the ACS framework, we provide concrete
selection algorithms for various goals, including model update/selection,
diversified selection, and incorporating newly available labeled data. The
effectiveness of ACS is demonstrated through extensive numerical simulations
and real-data applications in large language model (LLM) deployment and drug
discovery.

</details>


### [465] [Robust and Differentially Private PCA for non-Gaussian data](https://arxiv.org/abs/2507.15232)
*Minwoo Kim,Sungkyu Jung*

Main category: stat.ME

TL;DR: 提出适用于重尾和可能受污染数据的差分隐私PCA方法，实验表明其在统计效用上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护PCA方法有依赖限制性假设、计算昂贵等问题，需更好方法。

Method: 利用椭圆分布下协方差矩阵性质，通过有界变换以差分隐私方式计算主成分。

Result: 广泛数值实验显示该方法在非高斯或受污染数据设置中统计效用始终优于现有方法。

Conclusion: 提出的方法适用于重尾和受污染数据，能有效恢复主成分张成的子空间，具有更好统计效用。

Abstract: Recent advances have sparked significant interest in the development of
privacy-preserving Principal Component Analysis (PCA). However, many existing
approaches rely on restrictive assumptions, such as assuming sub-Gaussian data
or being vulnerable to data contamination. Additionally, some methods are
computationally expensive or depend on unknown model parameters that must be
estimated, limiting their accessibility for data analysts seeking
privacy-preserving PCA. In this paper, we propose a differentially private PCA
method applicable to heavy-tailed and potentially contaminated data. Our
approach leverages the property that the covariance matrix of properly rescaled
data preserves eigenvectors and their order under elliptical distributions,
which include Gaussian and heavy-tailed distributions. By applying a bounded
transformation, we enable straightforward computation of principal components
in a differentially private manner. Additionally, boundedness guarantees
robustness against data contamination. We conduct both theoretical analysis and
empirical evaluations of the proposed method, focusing on its ability to
recover the subspace spanned by the leading principal components. Extensive
numerical experiments demonstrate that our method consistently outperforms
existing approaches in terms of statistical utility, particularly in
non-Gaussian or contaminated data settings.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [466] [A universal augmentation framework for long-range electrostatics in machine learning interatomic potentials](https://arxiv.org/abs/2507.14302)
*Dongjin Kim,Xiaoyu Wang,Peichen Zhong,Daniel S. King,Theo Jaffrelot Inizan,Bingqing Cheng*

Main category: physics.chem-ph

TL;DR: 介绍了独立库LES，可与短程MLIP兼容，经测试能捕获静电并提高精度，训练的MACELES - OFF对有机系统表现出色，为静电基础MLIP铺平道路。


<details>
  <summary>Details</summary>
Motivation: 当前多数机器学习原子间势依赖短程近似，缺乏对长程静电的明确处理。

Method: 开发LES方法，将其作为独立库与多种短程MLIP集成，在不同系统上进行基准测试，并在SPICE集上训练MACELES - OFF。

Result: LES能捕获正确静电并提高精度，MACELES - OFF比短程对应模型更准确，能可靠预测偶极子和BECs，对液体描述更好。

Conclusion: LES无需直接训练电学性质就能实现高效长程静电，为静电基础MLIP铺平道路。

Abstract: Most current machine learning interatomic potentials (MLIPs) rely on
short-range approximations, without explicit treatment of long-range
electrostatics. To address this, we recently developed the Latent Ewald
Summation (LES) method, which infers electrostatic interactions, polarization,
and Born effective charges (BECs), just by learning from energy and force
training data. Here, we present LES as a standalone library, compatible with
any short-range MLIP, and demonstrate its integration with methods such as
MACE, NequIP, CACE, and CHGNet. We benchmark LES-enhanced models on distinct
systems, including bulk water, polar dipeptides, and gold dimer adsorption on
defective substrates, and show that LES not only captures correct
electrostatics but also improves accuracy. Additionally, we scale LES to large
and chemically diverse data by training MACELES-OFF on the SPICE set containing
molecules and clusters, making a universal MLIP with electrostatics for organic
systems including biomolecules. MACELES-OFF is more accurate than its
short-range counterpart (MACE-OFF) trained on the same dataset, predicts
dipoles and BECs reliably, and has better descriptions of bulk liquids. By
enabling efficient long-range electrostatics without directly training on
electrical properties, LES paves the way for electrostatic foundation MLIPs.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [467] [Timetide: A programming model for logically synchronous distributed systems](https://arxiv.org/abs/2507.14471)
*Logan Kenwright,Partha Roop,Nathan Allen,Călin Caşcaval,Avinash Malik*

Main category: cs.PL

TL;DR: 提出新的同步程序多时钟语义和编程模型Timetide，解决分布式系统确定性编程挑战，无需物理时钟同步和时钟门控。


<details>
  <summary>Details</summary>
Motivation: 现有同步语言主要用于集中式应用，时间触发语言依赖昂贵物理时钟同步，分布式系统确定性编程仍具挑战。

Method: 开发新的同步程序多时钟语义，构建基于逻辑同步模型的编程模型Timetide，讨论计算分布重要方面并探索形式验证。

Result: Timetide是首个无需物理时钟同步或时钟门控，适用于分布和形式验证的多时钟同步语言。

Conclusion: 所提出的多时钟语义和Timetide编程模型有效解决了分布式系统确定性编程的挑战。

Abstract: Massive strides in deterministic models have been made using synchronous
languages. They are mainly focused on centralised applications, as the
traditional approach is to compile away the concurrency. Time triggered
languages such as Giotto and Lingua Franca are suitable for distribution albeit
that they rely on expensive physical clock synchronisation, which is both
expensive and may suffer from scalability. Hence, deterministic programming of
distributed systems remains challenging. We address the challenges of
deterministic distribution by developing a novel multiclock semantics of
synchronous programs. The developed semantics is amenable to seamless
distribution. Moreover, our programming model, Timetide, alleviates the need
for physical clock synchronisation by building on the recently proposed logical
synchrony model for distributed systems. We discuss the important aspects of
distributing computation, such as network communication delays, and explore the
formal verification of Timetide programs. To the best of our knowledge,
Timetide is the first multiclock synchronous language that is both amenable to
distribution and formal verification without the need for physical clock
synchronisation or clock gating.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [468] [Quantum State Preparation Based on LimTDD](https://arxiv.org/abs/2507.14496)
*Xin Hong,Chenjian Li,Aochu Dai,Sanjiang Li,Shenggang Ying,Mingsheng Ying*

Main category: quant-ph

TL;DR: 本文提出基于LimTDD的量子态制备新方法，相比现有方法效率更高、电路复杂度更低，为量子计算技术发展提供基础。


<details>
  <summary>Details</summary>
Motivation: 随着量子技术发展，高效量子态制备愈发重要。

Method: 提出基于Local Invertible Map Tensor Decision Diagram (LimTDD)的量子态制备方法。

Result: 相比现有方法，处理复杂量子态时效率显著提升，降低量子电路复杂度，最佳情况下有指数级效率增益。

Conclusion: LimTDD在量子态制备有潜力，为量子计算技术未来发展提供理论和实践基础。

Abstract: Quantum state preparation is a fundamental task in quantum computing and
quantum information processing. With the rapid advancement of quantum
technologies, efficient quantum state preparation has become increasingly
important. This paper proposes a novel approach for quantum state preparation
based on the Local Invertible Map Tensor Decision Diagram (LimTDD). LimTDD
combines the advantages of tensor networks and decision diagrams, enabling
efficient representation and manipulation of quantum states. Compared with the
state-of-the-art quantum state preparation method, LimTDD demonstrates
substantial improvements in efficiency when dealing with complex quantum
states, while also reducing the complexity of quantum circuits. Examples
indicate that, in the best-case scenario, our method can achieve exponential
efficiency gains over existing methods. This study not only highlights the
potential of LimTDD in quantum state preparation but also provides a robust
theoretical and practical foundation for the future development of quantum
computing technologies.

</details>


### [469] [Quantum Annealing for Machine Learning: Applications in Feature Selection, Instance Selection, and Clustering](https://arxiv.org/abs/2507.15063)
*Chloe Pomeroy,Aleksandar Pramov,Karishma Thakrar,Lakshmi Yendapalli*

Main category: quant-ph

TL;DR: 本文探讨量子退火（QA）和经典模拟退火（SA）在机器学习组合优化问题中的应用，对比两种求解器效果，结果表明QA在离散机器学习优化中是有竞争力和高效的工具。


<details>
  <summary>Details</summary>
Motivation: 研究量子退火（QA）和经典模拟退火（SA）在机器学习中组合优化问题（特征选择、实例选择和聚类）的应用。

Method: 将每个任务表述为二次无约束二进制优化（QUBO）问题，实现量子和经典求解器；针对特征选择提出QUBO配置；实例选择提出新启发式方法；聚类嵌入经典到量子的流程。

Result: 特征选择中QA计算更高效；实例选择提出新方法；聚类在紧凑性和检索指标上有改进。

Conclusion: 即便在当前量子硬件限制下，QA可作为离散机器学习优化的有竞争力和高效工具。

Abstract: This paper explores the applications of quantum annealing (QA) and classical
simulated annealing (SA) to a suite of combinatorial optimization problems in
machine learning, namely feature selection, instance selection, and clustering.
We formulate each task as a Quadratic Unconstrained Binary Optimization (QUBO)
problem and implement both quantum and classical solvers to compare their
effectiveness. For feature selection, we propose several QUBO configurations
that balance feature importance and redundancy, showing that quantum annealing
(QA) produces solutions that are computationally more efficient. In instance
selection, we propose a few novel heuristics for instance-level importance
measures that extend existing methods. For clustering, we embed a
classical-to-quantum pipeline, using classical clustering followed by
QUBO-based medoid refinement, and demonstrate consistent improvements in
cluster compactness and retrieval metrics. Our results suggest that QA can be a
competitive and efficient tool for discrete machine learning optimization, even
within the constraints of current quantum hardware.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [470] [U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model](https://arxiv.org/abs/2507.14237)
*Louis Bahrman,Mathieu Fontaine,Gaël Richard*

Main category: cs.SD

TL;DR: 本文探索不同监督设置下训练去混响模型的结果，提出顺序学习策略，在低资源场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法需成对干信号和混响数据，实际难获取，因此探索仅用混响信号和声学模型训练去混响模型。

Method: 基于去混响问题的贝叶斯公式，开发顺序学习策略，用深度神经网络从混响输入估计声学参数和干信号，以混响匹配损失为指导。

Result: 数据效率最高的变体仅需100个混响参数标记样本就能超越无监督基线。

Conclusion: 所提方法在低资源场景有效且实用。

Abstract: This paper explores the outcome of training state-ofthe-art dereverberation
models with supervision settings ranging from weakly-supervised to fully
unsupervised, relying solely on reverberant signals and an acoustic model for
training. Most of the existing deep learning approaches typically require
paired dry and reverberant data, which are difficult to obtain in practice. We
develop instead a sequential learning strategy motivated by a bayesian
formulation of the dereverberation problem, wherein acoustic parameters and dry
signals are estimated from reverberant inputs using deep neural networks,
guided by a reverberation matching loss. Our most data-efficient variant
requires only 100 reverberation-parameter-labelled samples to outperform an
unsupervised baseline, demonstrating the effectiveness and practicality of the
proposed method in low-resource scenarios.

</details>


### [471] [A2TTS: TTS for Low Resource Indian Languages](https://arxiv.org/abs/2507.15272)
*Ayush Singh Bhadoriya,Abhishek Nikunj Shinde,Isha Pandey,Ganesh Ramakrishnan*

Main category: cs.SD

TL;DR: 提出基于扩散的说话人条件TTS系统，解决未见说话人语音生成和多印度语言支持问题，利用多种机制提升效果并进行多语言训练。


<details>
  <summary>Details</summary>
Motivation: 解决未见说话人语音生成挑战和支持多种印度语言。

Method: 采用基于扩散的TTS架构，用说话人编码器提取嵌入，使用基于交叉注意力的时长预测机制，采用分类器无指导方法，训练特定语言的说话人条件模型。

Result: 生成的语音接近目标说话人，改善时长建模和整体表现力，能更好地进行零样本生成。

Conclusion: 该方法可有效解决未见说话人语音生成和多印度语言支持问题。

Abstract: We present a speaker conditioned text-to-speech (TTS) system aimed at
addressing challenges in generating speech for unseen speakers and supporting
diverse Indian languages. Our method leverages a diffusion-based TTS
architecture, where a speaker encoder extracts embeddings from short reference
audio samples to condition the DDPM decoder for multispeaker generation. To
further enhance prosody and naturalness, we employ a cross-attention based
duration prediction mechanism that utilizes reference audio, enabling more
accurate and speaker consistent timing. This results in speech that closely
resembles the target speaker while improving duration modeling and overall
expressiveness. Additionally, to improve zero-shot generation, we employed
classifier free guidance, allowing the system to generate speech more near
speech for unknown speakers. Using this approach, we trained language-specific
speaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian
languages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and
Tamil.

</details>


### [472] [Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation](https://arxiv.org/abs/2507.15396)
*Hui-Guan Yuan,Ryandhimas E. Zezario,Shafique Ahmed,Hsin-Min Wang,Kai-Lung Hua,Yu Tsao*

Main category: cs.SD

TL;DR: 提出轻量级端到端模型Neuro - MSBG解决现有听力损失模拟模型问题，实验证明其高效实用。


<details>
  <summary>Details</summary>
Motivation: 现有听力损失模拟模型计算复杂度高、延迟大，限制实时应用且缺乏与语音处理系统的直接集成。

Method: 提出具有个性化听力图编码器的轻量级端到端模型Neuro - MSBG进行有效的时频建模。

Result: Neuro - MSBG支持并行推理，保留原MSBG的可懂度和感知质量，STOI的SRCC为0.9247，PESQ为0.8671，将模拟运行时间缩短46倍。

Conclusion: Neuro - MSBG具有高效性和实用性。

Abstract: Hearing loss simulation models are essential for hearing aid deployment.
However, existing models have high computational complexity and latency, which
limits real-time applications and lack direct integration with speech
processing systems. To address these issues, we propose Neuro-MSBG, a
lightweight end-to-end model with a personalized audiogram encoder for
effective time-frequency modeling. Experiments show that Neuro-MSBG supports
parallel inference and retains the intelligibility and perceptual quality of
the original MSBG, with a Spearman's rank correlation coefficient (SRCC) of
0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for
Perceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation
runtime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second
input), further demonstrating its efficiency and practicality.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [473] [Learning Stochastic Hamiltonian Systems via Stochastic Generating Function Neural Network](https://arxiv.org/abs/2507.14467)
*Chen Chen,Lijin Wang,Yanzhao Cao,Xupeng Cheng*

Main category: math.DS

TL;DR: 提出随机生成函数神经网络（SGFNN）学习随机哈密顿系统，实验显示其精度更高。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中学习随机哈密顿系统，生成具有辛结构的预测。

Method: 利用自编码器框架，编码器识别潜在系统随机性，解码器基于提取的随机变量检测随机生成函数，进而生成辛预测。

Result: 在多种随机哈密顿系统上实验，与基准sFML神经网络相比，SGFNN在各种预测指标上精度更高，尤其在长期预测中。

Conclusion: SGFNN模型能保持随机哈密顿系统的辛结构，具有更高的预测精度。

Abstract: In this paper we propose a novel neural network model for learning stochastic
Hamiltonian systems (SHSs) from observational data, termed the stochastic
generating function neural network (SGFNN). SGFNN preserves symplectic
structure of the underlying stochastic Hamiltonian system and produces
symplectic predictions. Our model utilizes the autoencoder framework to
identify the randomness of the latent system by the encoder network, and
detects the stochastic generating function of the system through the decoder
network based on the random variables extracted from the encoder. Symplectic
predictions can then be generated by the stochastic generating function.
Numerical experiments are performed on several stochastic Hamiltonian systems,
varying from additive to multiplicative, and from separable to non-separable
SHSs with single or multiple noises. Compared with the benchmark stochastic
flow map learning (sFML) neural network, our SGFNN model exhibits higher
accuracy across various prediction metrics, especially in long-term
predictions, with the property of maintaining the symplectic structure of the
underlying SHSs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [474] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: 介绍构建内部聊天机器人实现企业Text - to - SQL的方案及成果，指出开发企业解决方案的实用路径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽使Text - to - SQL基准取得进展，但构建企业解决方案仍不易，要为LinkedIn团队提供自助数据洞察服务。

Method: 构建捕获最新语义的知识图谱并聚类；构建Text - to - SQL代理来检索上下文、写查询及纠错；构建支持多种用户意图的交互式聊天机器人。

Result: 聊天机器人每周超300用户，内部基准集上53%的回复正确或接近正确。

Conclusion: 通过消融研究确定重要组件，为开发企业Text - to - SQL解决方案提供实用路径。

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [475] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: 提出GRACE框架用于多行为序列推荐，在两数据集上表现优且降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在多行为推荐系统应用受缺乏显式信息、计算成本高和多尺度建模有限等问题阻碍。

Method: 引入混合CoT分词方法编码用户 - 项目交互，设计JSA机制选择性关注上下文片段。

Result: 在两真实数据集上显著优于基线，在家庭领域HR@10和NDCG@10提升超100%，电子领域HR@10提升22.1%，长序列下注意力计算减少48%。

Conclusion: GRACE框架有效解决现有生成模型在多行为推荐系统中的问题，性能优越且计算高效。

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [476] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: 本文从显著性检验角度解释TF - IDF，证明常见变体TF - ICF与Fisher精确检验p值负对数相关，在特定假设下建立TF - IDF与负对数变换p值联系，且在文档集无限大时收敛到TF - IDF。


<details>
  <summary>Details</summary>
Motivation: 将TF - IDF置于可靠理论基础，向统计学界证明其合理性。

Method: 从显著性检验角度分析，证明TF - ICF与Fisher精确检验p值负对数的关系。

Result: 表明TF - ICF在温和正则条件下与Fisher精确检验p值负对数密切相关，在特定假设下建立TF - IDF与负对数变换p值联系，文档集无限大时收敛到TF - IDF。

Conclusion: Fisher精确检验对TF - IDF的解释为统计学家提供了该词加权方案有效性的现成解释。

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [477] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: 介绍可定制、多模态长文写作助手DeepWriter，在财务报告生成实验中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在专业领域写作缺乏特定领域知识、易产生幻觉，现有解决方案存在不一致和质量下降的问题。

Method: 采用任务分解、大纲生成、多模态检索、逐节撰写与反思的新流程，提出分层知识表示法。

Result: 在财务报告生成实验中，DeepWriter生成高质量、可验证文章，在事实准确性和内容质量上超越现有基线。

Conclusion: DeepWriter能生成连贯、基于事实、专业级的文档，有效解决现有大语言模型及解决方案的问题。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [478] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 研究微调对大语言模型编辑知识的影响，发现编辑知识在微调中更易遗忘，冻结相关层可提升知识保留。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需更新知识，模型编辑方法可高效更新，但微调对编辑知识的影响不明，需系统研究。

Method: 系统研究不同微调目标与各种模型编辑技术的相互作用。

Result: 编辑知识在微调时比预训练获得的固有知识更易遗忘；冻结与编辑内容相关的层可显著提高知识保留。

Conclusion: 当前编辑方法存在关键局限，评估下游微调下的编辑鲁棒性对实际部署至关重要，为未来编辑方法更具鲁棒性提供思路。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [479] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: 本文提出SMACS框架，整合开源大模型，在多基准测试中超越闭源大模型，推动智能上限。


<details>
  <summary>Details</summary>
Motivation: 探索开源集体的潜力，研究能否利用多个开源大模型匹敌甚至超越闭源大模型。

Method: 提出SMACS框架，包括基于检索的先验选择（RPS）和探索-利用驱动的后验增强（EPE）。

Result: 在八个主流基准测试中，整合十五个开源大模型的SMACS在2025年超越Claude - 3.7 - Sonnet、GPT - 4.1等闭源大模型。

Conclusion: SMACS框架有效，证明了利用开源大模型超越闭源大模型的可能性，代码将开源。

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [480] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: 本文评估多种NLP模型识别双相情感障碍迹象的能力，发现RoBERTa性能最佳，强调上下文语言建模重要性，为心理健康NLP应用提供选模建议。


<details>
  <summary>Details</summary>
Motivation: 双相情感障碍因早期症状不明显和社会污名常被漏诊，探索基于社交媒体文本识别其迹象的先进NLP模型。

Method: 对基于transformer的模型和基于上下文、静态词嵌入的LSTM模型进行综合评估，在Reddit帖子注释数据集上实验，并通过情感方差和判断分析确认数据有效性。

Result: RoBERTa在transformer模型中性能最高，F1分数约98%；使用BERT嵌入的LSTM模型结果相近；基于静态嵌入的LSTM模型F1分数接近零；DistilBERT在效率和准确性间达到最佳平衡。

Conclusion: 强调上下文语言建模在检测双相情感障碍中的关键作用，为心理健康NLP应用选模提供见解，验证上下文语言模型支持早期双相情感障碍筛查的潜力。

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [481] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: 研究分析了大语言模型在五个高风险应用中对用户身份标记的响应偏差，发现身份因素会影响模型回答，还提供评估工具并建议未来部署前做全面评估。


<details>
  <summary>Details</summary>
Motivation: 了解大语言模型在现实应用决策中如何利用身份信息。

Method: 对医学、法律、政治、政府福利和工作薪资五个领域的高风险大语言模型应用进行全面分析。

Result: 大语言模型对用户查询中的身份标记极为敏感，种族、性别和年龄会持续影响其回答，如医疗建议、政治事实回答和薪资推荐存在偏差。

Conclusion: 现成大语言模型用于这些应用可能造成不良影响，建议未来部署前进行类似全面评估。

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [482] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: 提出CCL - XCoT框架减少多语言大模型幻觉问题，实验显示有效降低幻觉率并提升跨语言知识传递。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型存在幻觉问题，尤其是低资源语言中因训练数据不平衡导致，在特定领域生成任务中问题突出，需解决该问题。

Method: 提出CCL - XCoT两阶段微调框架，先在持续预训练中通过基于课程的对比学习和下一个词预测增强跨语言语义对齐，再在指令微调中引入跨语言思维链提示策略。

Result: CCL - XCoT最多降低62%的幻觉率，显著提升跨语言对的事实知识传递，且不依赖外部检索或多模型集成。

Conclusion: CCL - XCoT框架能有效缓解多语言大模型的幻觉问题。

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [483] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: 研究大语言模型供应链中模型与数据集关系，构建图并分析其特性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型开发、训练和部署资源需求大，且模型易继承风险，需研究模型与数据集关系以检测风险、提高公平性和确保合规。

Method: 设计系统收集大语言模型供应链数据的方法，构建有向异构图来建模模型与数据集关系。

Result: 构建的图有397,376个节点和453,469条边，发现图大且稀疏、有核心与边缘结构、数据集重要、模型与数据集相互依赖、图动态更新等特性。

Conclusion: 通过研究大语言模型供应链中模型与数据集关系，有助于检测潜在风险、提升模型公平性和确保合规。

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [484] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: 介绍自动提示优化框架Promptomatix，能将自然语言任务描述转化为高质量提示，评估表现佳，可扩展高效。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型提示工程手动、不一致且非专家难以使用，需要自动优化框架。

Method: 支持轻量级元提示优化器和DSPy编译器，分析用户意图、生成合成训练数据、选择提示策略并使用成本感知目标优化提示。

Result: 在5类任务评估中表现有竞争力或更优，减少提示长度和计算开销。

Conclusion: Promptomatix使提示优化可扩展且高效。

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [485] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: 本文提出针对科学图表理解的LVLM模型ChartScope，含数据生成管道和训练策略，设新基准ChartDQA，实验显示其增强了对多种图表的理解。


<details>
  <summary>Details</summary>
Motivation: 现有定制大视觉语言模型用于特定领域任务的方法存在泛化能力不足和缺乏针对性预训练的问题，影响图表理解。

Method: 提出高效数据生成管道合成多种图表类型的配对数据，采用双路径训练策略，建立新基准ChartDQA。

Result: 实验表明ChartScope显著增强了对多种图表类型的理解。

Conclusion: ChartScope在多种图表类型的深入理解上表现出色，代码和数据已开源。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [486] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 文章探讨用机器学习技术从对话中检测CPS指标，研究AudiBERT和BERT模型表现，指出模型在不同维度表现差异，分析数据量和人工编码对性能影响，最后提出实现人机互补的方法。


<details>
  <summary>Details</summary>
Motivation: 之前研究虽用BERT模型检测CPS指标有进展，但AudiBERT改进的统计显著性不明，且缺乏人机互补指导。

Method: 对比AudiBERT和BERT模型在不同维度的分类表现，进行相关性分析。

Result: AudiBERT在社会认知维度分类有显著提升，情感维度无；训练数据量与模型召回率正相关；BERT模型精度与人工编码者一致性正相关；用BERT诊断AudiBERT检测好的指标表现不一致。

Conclusion: 提出实现CPS诊断人机互补的结构化方法，强调模型可解释性的重要性。

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [487] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文用SHAP分析BERT模型对CPS分类时各分词的作用，发现分类表现好未必解释合理，模型透明度可防用户过度依赖诊断。


<details>
  <summary>Details</summary>
Motivation: 提升基于BERT的CPS诊断的可解释性，让终端用户如教师更好理解，促进其在教育中的应用。

Method: 使用SHapley Additive exPlanations (SHAP) 分析转录数据中不同分词对BERT模型CPS分类的贡献。

Result: 分类表现好未必有合理的分类决策解释；特定分词常影响分类；发现对分类有积极贡献但语义无意义的词。

Conclusion: 模型分类对分词的合理使用程度与类别数量有关，需探索集成模型架构和人机互补进行CPS诊断。

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [488] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: 论文探讨NLP数据增强方法，用ChatGPT解决数据稀缺问题，实验表明回译和释义效果佳。


<details>
  <summary>Details</summary>
Motivation: 解决NLP领域特定机器学习任务的数据稀缺和类别不平衡问题，研究传统方法借助新一代模型能否达到纯生成方法的性能。

Method: 选择解决数据稀缺问题、利用ChatGPT的方法和示例数据集，对比四种数据增强方法并在多实验设置下评估。

Result: 回译和释义能产生与零样本和少样本生成示例相当甚至更好的结果。

Conclusion: 传统数据增强方法借助大语言模型可有效解决NLP数据稀缺和类别不平衡问题。

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [489] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: 本文提出针对肯尼亚2、3级临床护理的基准数据集和评估框架，用RAG方法结合当地指南创建数据集，引入新评估指标，发现大语言模型在本地化场景有性能差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型改善低资源地区医疗可及性在非洲初级护理的有效性未充分探索，需构建适配当地的评估体系。

Method: 使用RAG方法将临床问题与肯尼亚国家指南结合，对指南数字化处理，用Gemini Flash 2.0 Lite生成多语言临床场景和问答对，经医生共创、专家审核。引入新评估指标。

Result: 生成包含数千个问答对的Alama Health QA数据集，发现大语言模型在本地化场景存在显著性能差距，在非洲医疗内容上准确率低于美国基准。

Conclusion: 提供了可复制的基于指南的动态基准模型，支持在非洲卫生系统安全部署人工智能。

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [490] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: 本文提出有效不确定性估计方法Cleanse检测大语言模型幻觉，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，不确定性估计可衡量幻觉水平，以构建安全可靠的大语言模型。

Method: 提出基于聚类的语义一致性方法Cleanse，通过聚类用大语言模型隐藏嵌入中簇内一致性占总一致性的比例量化不确定性。

Result: 使用四个现成模型（LLaMA - 7B、LLaMA - 13B、LLaMA2 - 7B和Mistral - 7B）和两个问答基准（SQuAD和CoQA）验证了Cleanse检测幻觉的有效性。

Conclusion: Cleanse是一种有效的检测大语言模型幻觉的不确定性估计方法。

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [491] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: 本文回顾了Hugging Face Hub上公开的阿拉伯语后训练数据集，评估后发现其存在关键不足，并给出未来发展建议。


<details>
  <summary>Details</summary>
Motivation: 后训练对大语言模型与人类指令对齐至关重要，数据集质量和多样性是核心，需对阿拉伯语后训练数据集进行评估。

Method: 从大语言模型能力、可引导性、对齐性和鲁棒性四个维度，依据流行度、实际应用等指标评估数据集。

Result: 发现阿拉伯语后训练数据集存在任务多样性有限、文档和注释不一致或缺失、社区采用率低等问题。

Conclusion: 讨论了这些问题对阿拉伯语大语言模型和应用的影响，为后训练数据集开发提供具体建议。

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [492] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: 研究构建土耳其自杀意念语料库，用新框架标注，跨数据集评估，发现需更严谨包容方法，流行模型零样本迁移学习表现存疑。


<details>
  <summary>Details</summary>
Motivation: 解决自杀意念检测中语言覆盖有限和标注实践不可靠的问题，推动全球自杀预防。

Method: 构建土耳其语料库，引入资源高效标注框架，用预训练分类器进行跨数据集双向评估。

Result: 强调心理健康NLP中需更严谨、语言包容的标注和评估方法，流行模型零样本迁移学习表现存疑。

Conclusion: 倡导心理健康NLP中模型训练和数据集构建的透明度，优先考虑数据和模型可靠性。

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [493] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 研究用LLM-based选择性翻译提升多语言大模型低资源语言对齐效果，实验表明该方法有前景。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在英语和非英语间有性能差距，低资源语言对齐因数据有限具挑战性，标准翻译技术有不足。

Method: 研究LLM-based选择性翻译技术，对关键问题进行系统研究，在印地语上实验对比Google Cloud Translation和Llama-3.1-405B的翻译。

Result: 实验结果突出了选择性翻译在提升多语言大模型对齐方面有前景。

Conclusion: 选择性翻译是提升多语言大模型多语言对齐的实用有效方法。

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [494] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出WebShaper框架构建信息检索数据集，在基准测试中达开源信息检索代理的最优性能。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据稀缺限制信息检索代理发展，现有方法存在信息与推理结构、问答不一致问题。

Method: 提出形式化驱动的信息检索数据合成框架WebShaper，用集合论形式化信息检索任务，引入知识投影概念，通过多步扩展过程创建数据集并训练模型。

Result: WebShaper在GAIA和WebWalkerQA基准测试中达到开源信息检索代理的最优性能。

Conclusion: WebShaper框架有效解决现有信息检索代理发展的问题，性能表现优异。

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [495] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: 本文对比不同k - mer分割、BPE分词及位置编码方法在DNA Transformer模型中的表现，为模型设计提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究用Transformer对DNA序列建模时，缺乏对固定长度k - mer分割和BPE子词分词的系统评估。

Method: 比较不同k值的k - mer分割、4096词元的BPE词汇表和三种位置编码方法，在不同层数的Transformer编码器中从头训练并在GUE基准数据集上评估。

Result: BPE表现更高且更稳定；RoPE擅长捕捉周期性基序，AliBi在局部依赖任务中表现良好；层数从3增加到12有显著提升，24层时提升微弱或有过拟合。

Conclusion: 本研究为DNA Transformer模型的分词和位置编码设计提供实用指导。

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


### [496] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
*Chathuri Jayaweera,Brianna Yanqui,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本文探讨大语言模型作为自然语言推理（NLI）常识知识生成器的潜力，发现显式纳入常识知识虽不能稳定提升整体结果，但有助于区分不同推理情况。


<details>
  <summary>Details</summary>
Motivation: 现有常识资源对多种前提 - 假设对的覆盖不足，研究大语言模型作为NLI常识知识生成器的潜力。

Method: 调整和修改现有指标，评估大语言模型在生成常识知识时的事实性和一致性。

Result: 显式纳入常识知识不能稳定提升整体结果，但能有效区分蕴含实例，适度提升对矛盾和中性推理的区分能力。

Conclusion: 大语言模型作为NLI常识知识生成器有一定作用，但在提升整体性能方面存在局限。

Abstract: Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

</details>


### [497] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
*Hellina Hailu Nigatu,Atnafu Lambebo Tonja,Henok Biadglign Ademtew,Hizkel Mitiku Alemayehu,Negasi Haile Abadi,Tadesse Destaw Belay,Seid Muhie Yimam*

Main category: cs.CL

TL;DR: 本文探讨阿姆哈拉语同音归一化问题，提出推理后归一化方案提升BLEU分数并保留语言特征。


<details>
  <summary>Details</summary>
Motivation: 同音归一化虽提升自动指标性能，但使模型无法理解同一语言不同书写形式，且影响迁移学习泛化能力，因此研究其对使用Ge'ez文字语言的影响。

Method: 进行单语训练和跨语言迁移实验，提出对模型预测结果而非训练数据进行归一化的推理后干预方法。

Result: 通过简单的推理后归一化方案，BLEU分数最多提高1.03，同时保留了训练中的语言特征。

Conclusion: 研究有助于技术推动语言变化的广泛讨论，呼吁更多考虑语言特性的干预措施。

Abstract: Homophone normalization, where characters that have the same sound in a
writing script are mapped to one character, is a pre-processing step applied in
Amharic Natural Language Processing (NLP) literature. While this may improve
performance reported by automatic metrics, it also results in models that are
not able to understand different forms of writing in a single language.
Further, there might be impacts in transfer learning, where models trained on
normalized data do not generalize well to other languages. In this paper, we
experiment with monolingual training and cross-lingual transfer to understand
the impacts of normalization on languages that use the Ge'ez script. We then
propose a post-inference intervention in which normalization is applied to
model predictions instead of training data. With our simple scheme of
post-inference normalization, we show that we can achieve an increase in BLEU
score of up to 1.03 while preserving language features in training. Our work
contributes to the broader discussion on technology-facilitated language change
and calls for more language-aware interventions.

</details>


### [498] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 评估三个大语言模型在医学领域随机对照试验数据提取任务中的表现，测试不同提示策略，发现定制提示最有效，提出三层使用指南。


<details>
  <summary>Details</summary>
Motivation: 自动化从全文随机对照试验中提取数据用于元分析仍具挑战，需评估大语言模型的实际性能。

Method: 评估Gemini - 2.0 - flash、Grok - 3、GPT - 4o - mini三个大语言模型在三个医学领域的统计结果、偏倚风险评估和研究特征提取任务中的表现，测试四种提示策略。

Result: 所有模型精度高但召回率低，定制提示最有效，可将召回率提高达15%。

Conclusion: 提出三层使用大语言模型进行数据提取的指南，为实际元分析中的数据提取提供实用建议，平衡效率与专家监督。

Abstract: Automating data extraction from full-text randomised controlled trials (RCTs)
for meta-analysis remains a significant challenge. This study evaluates the
practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)
across tasks involving statistical results, risk-of-bias assessments, and
study-level characteristics in three medical domains: hypertension, diabetes,
and orthopaedics. We tested four distinct prompting strategies (basic
prompting, self-reflective prompting, model ensemble, and customised prompts)
to determine how to improve extraction quality. All models demonstrate high
precision but consistently suffer from poor recall by omitting key information.
We found that customised prompts were the most effective, boosting recall by up
to 15\%. Based on this analysis, we propose a three-tiered set of guidelines
for using LLMs in data extraction, matching data types to appropriate levels of
automation based on task complexity and risk. Our study offers practical advice
for automating data extraction in real-world meta-analyses, balancing LLM
efficiency with expert oversight through targeted, task-specific automation.

</details>


### [499] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
*Shayan Vassef,Amirhossein Dabiriaghdam,Mohammadreza Bakhtiari,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 研究多任务、多语言和多源学习对预训练语言模型的影响，引入SOI框架分析训练模式，实验表明多源学习提升OOD性能，提出两阶段微调法优化性能。


<details>
  <summary>Details</summary>
Motivation: 探究多任务、多语言和多源学习方法对预训练语言模型鲁棒性和性能的影响。

Method: 引入SOI框架识别训练中的学习行为模式，通过SOI转换热图和数据集制图可视化分析示例转变，进行三组平行对比实验，提出两阶段微调法。

Result: 多源学习使OOD性能提升达7%，多任务学习在相似任务组合中有显著提升，两阶段微调法进一步提升性能。

Conclusion: 研究为训练动态提供新见解，为优化多设置语言模型性能提供实用方法。

Abstract: This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

</details>


### [500] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 现有大语言模型后训练策略不能提升领域认知，提出DPSE框架联合优化用户偏好适配和特定领域能力，实验显示其表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型后训练策略无法提升模型的领域认知，需提出新方法解决。

Method: 提出Dual - Phase Self - Evolution (DPSE)框架，引入审查模块提取信号和估计满意度得分，通过策略扩展数据，支持两阶段微调流程。

Result: 在通用NLP基准和长期对话任务实验中，DPSE始终优于监督微调、偏好优化和记忆增强基线模型，消融研究验证各模块贡献。

Conclusion: DPSE框架为大语言模型持续自我进化提供了自主途径。

Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

</details>


### [501] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
*Leanne Tan,Gabriel Chua,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 提出轻量级多语言审核分类器LionGuard 2，适用于新加坡场景，性能优且已部署，还发布部分数据。


<details>
  <summary>Details</summary>
Motivation: 现代审核系统未解决本地化和低资源变体问题，小模型也需大量数据和计算资源。

Method: 基于预训练的OpenAI嵌入和多头序数分类器构建LionGuard 2。

Result: LionGuard 2在17个基准测试中优于多个商业和开源系统，已在新加坡政府中部署。

Conclusion: 高质量本地数据和强大的多语言嵌入无需微调大模型就能实现良好审核性能。

Abstract: Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.

</details>


### [502] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
*Amedeo Buonanno,Alessandro Rivetti,Francesco A. N. Palmieri,Giovanni Di Gennaro,Gianmarco Romano*

Main category: cs.CL

TL;DR: 探索熵分析作为探测Transformer架构信息分布的工具，以GPT模型为例展示其潜力。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer架构内信息如何管理和转换，为模型可解释性和评估框架发展提供帮助。

Method: 量化token级别的不确定性，检查不同处理阶段的熵模式，并应用到GPT大语言模型上。

Result: 该方法展示了揭示模型行为和内部表示的潜力。

Conclusion: 此方法有助于理解模型行为，对Transformer模型可解释性和评估框架发展有贡献。

Abstract: This work explores entropy analysis as a tool for probing information
distribution within Transformer-based architectures. By quantifying token-level
uncertainty and examining entropy patterns across different stages of
processing, we aim to investigate how information is managed and transformed
within these models. As a case study, we apply the methodology to a GPT-based
large language model, illustrating its potential to reveal insights into model
behavior and internal representations. This approach may offer insights into
model behavior and contribute to the development of interpretability and
evaluation frameworks for transformer-based models

</details>


### [503] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
*Elisa Sanchez-Bayona,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文对大语言模型在隐喻解释方面的能力进行综合评估，发现其表现受表层特征影响，强调需更现实的评估框架。


<details>
  <summary>Details</summary>
Motivation: 以往隐喻处理研究局限于单数据集评估和特定任务设置，使用人工构造数据，本文旨在解决这些局限。

Method: 利用多个含推理和隐喻注释的公开数据集，针对自然语言推理和问答任务进行大量实验。

Result: 大语言模型的表现更多受词汇重叠和句子长度等特征影响，所谓理解隐喻语言的能力是表层特征、上下文学习和语言知识的综合结果。

Conclusion: 本研究揭示了大语言模型处理比喻性语言的能力和局限，强调隐喻解释任务需要更现实的评估框架。

Abstract: This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (LLMs) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that LLMs' performance is more influenced by features like
lexical overlap and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of LLMs to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of LLMs in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.

</details>


### [504] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
*Alexandru Coca,Mark Gaynor,Zhenxing Zhang,Jianpeng Cheng,Bo-Hsiang Tseng,Pete Boothroyd,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: 评估大语言模型驱动能执行复杂操作的数字助手的潜力，开发ASPERA框架并发布Asper - Bench数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型驱动可执行复杂操作的数字助手的潜力，解决数据可用性和评估鲁棒性挑战。

Method: 开发包含助手库模拟和人工辅助大语言模型数据生成引擎的ASPERA框架。

Result: 发布由250个具有挑战性任务组成的Asper - Bench评估数据集，发现基于自定义助手库的程序生成对大语言模型是重大挑战。

Conclusion: 基于自定义助手库的程序生成比无依赖代码生成对大语言模型更具挑战性。

Abstract: This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

</details>


### [505] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
*Congmin Zheng,Jiachen Zhu,Jianghao Lin,Xinyi Dai,Yong Yu,Weinan Zhang,Mengyue Yang*

Main category: cs.CL

TL;DR: 现有过程奖励模型（PRMs）存在长度偏差，提出CoLD框架缓解此问题，实验证明其有效


<details>
  <summary>Details</summary>
Motivation: 现有PRMs存在长度偏差，影响奖励预测可靠性和推理输出质量

Method: 提出CoLD框架，包含显式长度惩罚调整、学习偏差估计器和联合训练策略，基于反事实推理和因果图分析

Result: 在MATH500和GSM - Plus上实验表明，CoLD降低奖励 - 长度相关性，提高步骤选择准确性，鼓励更简洁有效的推理

Conclusion: CoLD能有效提高PRMs的保真度和鲁棒性

Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [506] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
*Andrei-Valentin Tanase,Elena Pelican*

Main category: cs.CL

TL;DR: 介绍650M参数的Supernova模型，通过架构设计和分词创新，在减少参数和训练数据下达较大模型90%性能，挑战现有缩放范式。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过架构设计和分词创新，在保持计算效率的同时达到更大模型的性能。

Method: 采用RoPE、GQA、RMSNorm和SwiGLU激活函数，使用自定义128,000词汇字节级BPE分词器。

Result: Supernova达到1B参数模型90%的性能，参数减少53%，仅需100B训练令牌。

Conclusion: 架构效率和分词质量可弥补参数数量的减少，挑战了现有的缩放范式。

Abstract: We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

</details>


### [507] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
*Yihao Li,Jiayi Xin,Miranda Muqing Miao,Qi Long,Lyle Ungar*

Main category: cs.CL

TL;DR: 研究中英双语推理模型语言切换，发现语言混合有益推理，RLVR训练阶段致语言混合，用轻量级探针引导解码可提准确率。


<details>
  <summary>Details</summary>
Motivation: 探讨双语大语言模型语言混合现象及对推理的影响，研究双语推理模型中语言切换。

Method: 确定RLVR为导致语言混合的关键训练阶段，用强制单语解码对比，训练轻量级探针预测语言切换影响并引导解码。

Result: 强制单语解码使数学推理任务准确率降5.6个百分点，轻量级探针引导解码使准确率最多提升6.25个百分点。

Conclusion: 语言混合不是多语言训练副产品，而是策略性推理行为。

Abstract: Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [508] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
*Alessio Pittiglio*

Main category: cs.CL

TL;DR: 介绍MM - ArgFallacy2025共享任务提交情况，用预训练模型及上下文利用方法，给出各模态谬误分类F1分数，显示多模态模型潜力。


<details>
  <summary>Details</summary>
Motivation: 推进多模态论证挖掘研究，聚焦政治辩论中的逻辑谬误。

Method: 使用预训练基于Transformer的模型，并提出利用上下文的方法。

Result: 在谬误分类子任务中，文本、音频和多模态的宏观F1分数分别为0.4444、0.3559和0.4403，多模态模型性能与纯文本模型相当。

Conclusion: 多模态模型有改进潜力。

Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

</details>


### [509] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
*David Peter Wallis Freeborn*

Main category: cs.CL

TL;DR: 本文构建信号博弈模型使接收者实现真正的组合理解，提出极简接收者和通才接收者两种新模型。


<details>
  <summary>Details</summary>
Motivation: 标准信号博弈模型中接收者难以学习组合信息，即便信号发送者发送组合消息，接收者也无法进行组合解读，且一个消息组件信息丢失时其他组件信息也会被抹去。

Method: 构建两种新的信号博弈模型，即只从信号原子消息学习的极简接收者模型和从所有可用信息学习的通才接收者模型。

Result: 新模型在很多方面比以往模型更简单，能让接收者从消息的原子组件中学习。

Conclusion: 新构建的模型可使接收者实现真正的组合理解。

Abstract: Receivers in standard signaling game models struggle with learning
compositional information. Even when the signalers send compositional messages,
the receivers do not interpret them compositionally. When information from one
message component is lost or forgotten, the information from other components
is also erased. In this paper I construct signaling game models in which
genuine compositional understanding evolves. I present two new models: a
minimalist receiver who only learns from the atomic messages of a signal, and a
generalist receiver who learns from all of the available information. These
models are in many ways simpler than previous alternatives, and allow the
receivers to learn from the atomic components of messages.

</details>


### [510] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
*Seok Hwan Song,Mohna Chakraborty,Qi Li,Wallapak Tavanapong*

Main category: cs.CL

TL;DR: 研究不同问题类型对大语言模型推理任务准确性的影响，发现不同问题类型下模型表现有差异等。


<details>
  <summary>Details</summary>
Motivation: 回答不同问题类型对大语言模型推理任务准确性影响这一未探索问题。

Method: 使用定量和演绎推理任务，研究五个大语言模型在三种不同类型问题上的表现，并采用推理步骤准确性和最终答案选择准确性等指标。

Result: 不同问题类型下大语言模型表现存在显著差异；推理准确性不一定与最终选择准确性相关；选项数量和用词会影响模型表现。

Conclusion: 不同问题类型会对大语言模型推理任务准确性产生影响。

Abstract: Large Language Models (LLMs) have been evaluated using diverse question
types, e.g., multiple-choice, true/false, and short/long answers. This study
answers an unexplored question about the impact of different question types on
LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on
three different types of questions using quantitative and deductive reasoning
tasks. The performance metrics include accuracy in the reasoning steps and
choosing the final answer. Key Findings: (1) Significant differences exist in
LLM performance across different question types. (2) Reasoning accuracy does
not necessarily correlate with the final selection accuracy. (3) The number of
options and the choice of words, influence LLM performance.

</details>


### [511] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
*Sahana Srinivasan,Xuguang Ai,Thaddaeus Wai Soon Lo,Aidan Gilson,Minjie Zou,Ke Zou,Hyunjae Kim,Mingjia Yang,Krithi Pushpanathan,Samantha Yew,Wan Ting Loke,Jocelyn Goh,Yibing Chen,Yiming Kong,Emily Yuelei Fu,Michelle Ongyong Hui,Kristen Nwanyanwu,Amisha Dave,Kelvin Zhenghao Li,Chen-Hsin Sun,Mark Chia,Gabriel Dawei Yang,Wendy Meihua Wong,David Ziyou Chen,Dianbo Liu,Maxwell Singer,Fares Antaki,Lucian V Del Priore,Jost Jonas,Ron Adelman,Qingyu Chen,Yih-Chung Tham*

Main category: cs.CL

TL;DR: 提出眼科大语言模型评估基准BELO，含900个高质量问题，评估多个模型并设公开排行榜。


<details>
  <summary>Details</summary>
Motivation: 现有眼科大语言模型评估基准范围有限且过度关注准确性，需全面评估。

Method: 通过13位眼科专家多轮检查开发BELO，用关键词匹配和微调模型筛选题目，去除重复和低质量问题，由专家完善答案解释；用多种指标评估6个大语言模型，专家定性评估部分输出。

Result: BELO包含900个专家审核的高质量问题，来自五个数据源；建立公开排行榜。

Conclusion: BELO作为评估基准可确保未来模型公平可重复比较。

Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology
are limited in scope and disproportionately prioritise accuracy. We introduce
BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive
evaluation benchmark developed through multiple rounds of expert checking by 13
ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and
reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we
curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse
medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset
underwent multiple rounds of expert checking. Duplicate and substandard
questions were systematically removed. Ten ophthalmologists refined the
explanations of each MCQ's correct answer. This was further adjudicated by
three senior ophthalmologists. To illustrate BELO's utility, we evaluated six
LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)
using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,
BARTScore, METEOR, and AlignScore). In a further evaluation involving human
experts, two ophthalmologists qualitatively reviewed 50 randomly selected
outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900
high-quality, expert-reviewed questions aggregated from five sources: BCSC
(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public
leaderboard has been established to promote transparent evaluation and
reporting. Importantly, the BELO dataset will remain a hold-out,
evaluation-only benchmark to ensure fair and reproducible comparisons of future
models.

</details>


### [512] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
*Ruizhe Zhu,Hao Zhu,Yaxuan Li,Syang Zhou,Shijing Cai,Malgorzata Lazuka,Elliott Ash*

Main category: cs.CL

TL;DR: 提出DialogueForge框架生成AI模拟的人机对话，测试多种大模型，评估对话质量，大模型表现优，小模型可通过微调提升但长对话仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 收集人机对话需大量人力和时间，限制对话式AI研究，因此提出框架生成模拟对话。

Method: 使用从真实人机交互中提取的种子提示初始化对话，测试多种大模型模拟人类用户生成多轮对话，探索微调技术，用UniEval和GTEval评估对话质量。

Result: 大型专有模型生成更真实对话，小型开源模型有定制优势，微调可显著提升小模型性能。

Conclusion: 微调可提升小模型性能，但所有模型维持连贯自然的长对话仍是挑战。

Abstract: Collecting human-chatbot dialogues typically demands substantial manual
effort and is time-consuming, which limits and poses challenges for research on
conversational AI. In this work, we propose DialogueForge - a framework for
generating AI-simulated conversations in human-chatbot style. To initialize
each generated conversation, DialogueForge uses seed prompts extracted from
real human-chatbot interactions. We test a variety of LLMs to simulate the
human chatbot user, ranging from state-of-the-art proprietary models to
small-scale open-source LLMs, and generate multi-turn dialogues tailored to
specific tasks. In addition, we explore fine-tuning techniques to enhance the
ability of smaller models to produce indistinguishable human-like dialogues. We
evaluate the quality of the simulated conversations and compare different
models using the UniEval and GTEval evaluation protocols. Our experiments show
that large proprietary models (e.g., GPT-4o) generally outperform others in
generating more realistic dialogues, while smaller open-source models (e.g.,
Llama, Mistral) offer promising performance with greater customization. We
demonstrate that the performance of smaller models can be significantly
improved by employing supervised fine-tuning techniques. Nevertheless,
maintaining coherent and natural long-form human-like dialogues remains a
common challenge across all models.

</details>


### [513] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
*Anton Abilov,Ke Zhang,Hemank Lamba,Elizabeth M. Olson,Joel R. Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: 文章分享与H2H组织合作部署和维护AI模型细节及关键经验。


<details>
  <summary>Details</summary>
Motivation: 现有AI for Good领域论文很少讨论模型部署、合作过程及实际影响，本文旨在填补这一空白。

Method: 与H2H组织密切合作，在资源受限环境部署并维护AI模型。

Result: 完成AI模型在资源受限环境的部署和持续性能更新。

Conclusion: 为从业者提供与合作组织部署和维护AI模型的关键经验。

Abstract: Publications in the AI for Good space have tended to focus on the research
and model development that can support high-impact applications. However, very
few AI for Good papers discuss the process of deploying and collaborating with
the partner organization, and the resulting real-world impact. In this work, we
share details about the close collaboration with a humanitarian-to-humanitarian
(H2H) organization and how to not only deploy the AI model in a
resource-constrained environment, but also how to maintain it for continuous
performance updates, and share key takeaways for practitioners.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [514] [On Algorithmic Robustness of Corrupted Markov Chains](https://arxiv.org/abs/2507.15176)
*Jason Gaitonde,Elchanan Mossel*

Main category: math.PR

TL;DR: 研究一般有限马尔可夫链平稳分布对转移矩阵对抗性破坏的算法鲁棒性，表明PageRank链在一定条件下具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究一般有限马尔可夫链平稳分布对转移矩阵一般对抗性破坏的算法鲁棒性。

Method: 对允许谱间隙的马尔可夫链进行研究，分析PageRank链在节点一定比例边被任意破坏时的情况。

Result: 在重启分布的温和条件下，给定节点一定比例边的任意破坏，受破坏链的PageRank分布与原分布在总变差上接近。

Conclusion: PageRank可作为针对广泛、现实破坏的简单正则化器，具有无维度且参数自然的算法保证。

Abstract: We study the algorithmic robustness of general finite Markov chains in terms
of their stationary distributions to general, adversarial corruptions of the
transition matrix. We show that for Markov chains admitting a spectral gap,
variants of the \emph{PageRank} chain are robust in the sense that, given an
\emph{arbitrary} corruption of the edges emanating from an $\epsilon$-measure
of the nodes, the PageRank distribution of the corrupted chain will be
$\mathsf{poly}(\varepsilon)$ close in total variation to the original
distribution under mild conditions on the restart distribution. Our work thus
shows that PageRank serves as a simple regularizer against broad, realistic
corruptions with algorithmic guarantees that are dimension-free and scale
gracefully in terms of necessary and natural parameters.

</details>


### [515] [Neural Brownian Motion](https://arxiv.org/abs/2507.14499)
*Qian Qi*

Main category: math.PR

TL;DR: 本文介绍了神经布朗运动（NBM），给出其表示定理，发展了随机微积分并证明了二次情形的Girsanov型定理，为不确定性态度可发现的模型提供基础。


<details>
  <summary>Details</summary>
Motivation: 引入新的随机过程来对学习到的不确定性下的动力学进行建模。

Method: 通过用相对于非线性神经期望算子的性质替代经典鞅性质来公理化定义NBM，证明表示定理等。

Result: 证明规范NBM存在且是随机微分方程的唯一强解，发展随机微积分并证明二次情形的Girsanov型定理。

Conclusion: NBM为不确定性态度可发现的模型提供了严谨基础。

Abstract: This paper introduces the Neural-Brownian Motion (NBM), a new class of
stochastic processes for modeling dynamics under learned uncertainty. The NBM
is defined axiomatically by replacing the classical martingale property with
respect to linear expectation with one relative to a non-linear Neural
Expectation Operator, $\varepsilon^\theta$, generated by a Backward Stochastic
Differential Equation (BSDE) whose driver $f_\theta$ is parameterized by a
neural network. Our main result is a representation theorem for a canonical
NBM, which we define as a continuous $\varepsilon^\theta$-martingale with zero
drift under the physical measure. We prove that, under a key structural
assumption on the driver, such a canonical NBM exists and is the unique strong
solution to a stochastic differential equation of the form ${\rm d} M_t =
\nu_\theta(t, M_t) {\rm d} W_t$. Crucially, the volatility function
$\nu_\theta$ is not postulated a priori but is implicitly defined by the
algebraic constraint $g_\theta(t, M_t, \nu_\theta(t, M_t)) = 0$, where
$g_\theta$ is a specialization of the BSDE driver. We develop the stochastic
calculus for this process and prove a Girsanov-type theorem for the quadratic
case, showing that an NBM acquires a drift under a new, learned measure. The
character of this measure, whether pessimistic or optimistic, is endogenously
determined by the learned parameters $\theta$, providing a rigorous foundation
for models where the attitude towards uncertainty is a discoverable feature.

</details>


<div id='math.RT'></div>

# math.RT [[Back]](#toc)

### [516] [Partial Symmetry Enforced Attention Decomposition (PSEAD): A Group-Theoretic Framework for Equivariant Transformers in Biological Systems](https://arxiv.org/abs/2507.14908)
*Daniel Ayomide Olanrewaju*

Main category: math.RT

TL;DR: 本文提出PSEAD框架，将局部对称意识融入Transformer自注意力机制，有诸多优势并可用于动态生物过程，为新一代AI模型奠基。


<details>
  <summary>Details</summary>
Motivation: 将局部对称意识融入Transformer自注意力机制核心架构，以处理生物数据。

Method: 形式化局部置换子群对生物数据窗口的作用，证明注意力机制可分解为正交不可约分量的直和。

Result: PSEAD有增强泛化能力、可解释性和计算效率等优势，还可用于强化学习中的动态生物过程。

Conclusion: 为新一代具有生物学信息、对称感知的人工智能模型奠定基础。

Abstract: This research introduces the Theory of Partial Symmetry Enforced Attention
Decomposition (PSEAD), a new and rigorous group-theoretic framework designed to
seamlessly integrate local symmetry awareness into the core architecture of
self-attention mechanisms within Transformer models. We formalize the concept
of local permutation subgroup actions on windows of biological data, proving
that under such actions, the attention mechanism naturally decomposes into a
direct sum of orthogonal irreducible components. Critically, these components
are intrinsically aligned with the irreducible representations of the acting
permutation subgroup, thereby providing a powerful mathematical basis for
disentangling symmetric and asymmetric features. We show that PSEAD offers
substantial advantages. These include enhanced generalization capabilities to
novel biological motifs exhibiting similar partial symmetries, unprecedented
interpretability by allowing direct visualization and analysis of attention
contributions from different symmetry channels, and significant computational
efficiency gains by focusing representational capacity on relevant symmetric
subspaces. Beyond static data analysis, we extend PSEAD's applicability to
dynamic biological processes within reinforcement learning paradigms,
showcasing its potential to accelerate the discovery and optimization of
biologically meaningful policies in complex environments like protein folding
and drug discovery. This work lays the groundwork for a new generation of
biologically informed, symmetry-aware artificial intelligence models.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [517] [The New LLM Bottleneck: A Systems Perspective on Latent Attention and Mixture-of-Experts](https://arxiv.org/abs/2507.15465)
*Sungmin Yun,Seonyong Park,Hwayong Nam,Younjoo Lee,Gunjun Lee,Kwanhee Kyung,Sangpyo Kim,Nam Sung Kim,Jongmin Kim,Hyungyo Kim,Juhwan Cho,Seungmin Baek,Jung Ho Ahn*

Main category: cs.AR

TL;DR: 论文指出MLA和MoE架构变化挑战专用注意力硬件前提，减少其需求，下一代Transformer应设计平衡系统。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型中MHA内存受限，长期以来促使研究专用硬件缓解瓶颈，而近期架构变化促使重新审视专用硬件需求。

Method: 对MLA和MoE架构进行分析，观察其算术强度变化。

Result: MLA算术强度大幅提升接近计算密集型，MoE可通过批处理调整算术强度使计算更平衡。

Conclusion: 专用注意力硬件需求减少，下一代Transformer应聚焦设计平衡系统以应对大规模模型需求。

Abstract: Computational workloads composing traditional Transformer models are starkly
bifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic
intensity, while feedforward layers are compute-bound. This dichotomy has long
motivated research into specialized hardware to mitigate the MHA bottleneck.
  This paper argues that recent architectural shifts, namely Multi-head Latent
Attention (MLA) and Mixture-of-Experts (MoE), challenge the premise of
specialized attention hardware. We make two key observations. First, the
arithmetic intensity of MLA is over two orders of magnitude greater than that
of MHA, shifting it close to a compute-bound regime well-suited for modern
accelerators like GPUs. Second, by distributing MoE experts across a pool of
accelerators, their arithmetic intensity can be tuned through batching to match
that of the dense layers, creating a more balanced computational profile.
  These findings reveal a diminishing need for specialized attention hardware.
The central challenge for next-generation Transformers is no longer
accelerating a single memory-bound layer. Instead, the focus must shift to
designing balanced systems with sufficient compute, memory capacity, memory
bandwidth, and high-bandwidth interconnects to manage the diverse demands of
large-scale models.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [518] [Design of an Edge-based Portable EHR System for Anemia Screening in Remote Health Applications](https://arxiv.org/abs/2507.15146)
*Sebastian A. Cruz Romero,Misael J. Mercado Hernandez,Samir Y. Ali Rivera,Jorge A. Santiago Fernandez,Wilfredo E. Lugo Beauchamp*

Main category: cs.ET

TL;DR: 本文提出适用于资源受限偏远地区的便携电子健康记录平台，以贫血筛查为例展示其性能优化，解决数字健康采用障碍。


<details>
  <summary>Details</summary>
Motivation: 现有医疗系统在偏远、资源受限环境面临互操作性差、无离线支持和依赖昂贵基础设施等问题，需开发有效解决方案。

Method: 开发便携、支持边缘计算的电子健康记录平台，采用AES - 256加密本地存储与可选云同步，集成贫血筛查模块，使用随机森林模型和YOLOv8n量化优化。

Result: 随机森林模型测试RMSE为1.969 g/dL，MAE为1.490 g/dL，严重性模型灵敏度达79.2%，量化后推理延迟从46.96 ms降至21.50 ms，mAP@0.5维持在0.995。

Conclusion: 该系统强调低成本部署、模块化和数据隐私合规，为提升便携健康信息系统和支持偏远地区一线医疗提供可扩展方案。

Abstract: The design of medical systems for remote, resource-limited environments faces
persistent challenges due to poor interoperability, lack of offline support,
and dependency on costly infrastructure. Many existing digital health solutions
neglect these constraints, limiting their effectiveness for frontline health
workers in underserved regions. This paper presents a portable, edge-enabled
Electronic Health Record platform optimized for offline-first operation, secure
patient data management, and modular diagnostic integration. Running on
small-form factor embedded devices, it provides AES-256 encrypted local storage
with optional cloud synchronization for interoperability. As a use case, we
integrated a non-invasive anemia screening module leveraging fingernail pallor
analysis. Trained on 250 patient cases (27\% anemia prevalence) with
KDE-balanced data, the Random Forest model achieved a test RMSE of 1.969 g/dL
and MAE of 1.490 g/dL. A severity-based model reached 79.2\% sensitivity. To
optimize performance, a YOLOv8n-based nail bed detector was quantized to INT8,
reducing inference latency from 46.96 ms to 21.50 ms while maintaining mAP@0.5
at 0.995. The system emphasizes low-cost deployment, modularity, and data
privacy compliance (HIPAA/GDPR), addressing critical barriers to digital health
adoption in disconnected settings. Our work demonstrates a scalable approach to
enhance portable health information systems and support frontline healthcare in
underserved regions.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [519] [Learning Null Geodesics for Gravitational Lensing Rendering in General Relativity](https://arxiv.org/abs/2507.15775)
*Mingyuan Sun,Zheng Fang,Jiaxu Wang,Kunyi Zhang,Qiang Zhang,Renjing Xu*

Main category: gr-qc

TL;DR: 提出GravLensX方法，用神经网络渲染有引力透镜效应的黑洞，比传统方法快且有效。


<details>
  <summary>Details</summary>
Motivation: 寻找更高效的渲染有引力透镜效应的黑洞的方法。

Method: 训练神经网络拟合黑洞周围时空，用训练好的模型生成受引力透镜影响的光线路径。

Result: 能对有光学薄吸积盘的黑洞进行高效可扩展模拟，渲染多个叠加Kerr度量的黑洞系统时，计算时间减少15倍。

Conclusion: 神经网络为渲染复杂天体物理现象提供了有前景的替代方案，可能为天文可视化开辟新道路。

Abstract: We present GravLensX, an innovative method for rendering black holes with
gravitational lensing effects using neural networks. The methodology involves
training neural networks to fit the spacetime around black holes and then
employing these trained models to generate the path of light rays affected by
gravitational lensing. This enables efficient and scalable simulations of black
holes with optically thin accretion disks, significantly decreasing the time
required for rendering compared to traditional methods. We validate our
approach through extensive rendering of multiple black hole systems with
superposed Kerr metric, demonstrating its capability to produce accurate
visualizations with significantly $15\times$ reduced computational time. Our
findings suggest that neural networks offer a promising alternative for
rendering complex astrophysical phenomena, potentially paving a new path to
astronomical visualization.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [520] [Forecasting Faculty Placement from Patterns in Co-authorship Networks](https://arxiv.org/abs/2507.14696)
*Samantha Dies,David Liu,Tina Eliassi-Rad*

Main category: cs.SI

TL;DR: 研究将教师职位安置视为个人层面预测任务，发现合作网络能提升预测准确性，揭示社交网络在教师招聘中的作用。


<details>
  <summary>Details</summary>
Motivation: 传统研究未评估其关联是否适用于个体招聘结果，尤其是样本外未来候选人，需新视角研究教师招聘。

Method: 将教师职位安置作为个体层面预测任务，利用包含传统属性和合作网络的数据进行分析。

Result: 使用合作网络比传统指标提升预测准确性达10%，在顶尖部门安置预测提升最大。

Conclusion: 社交网络等在教师招聘中作用超传统指标，为理解学术结构偏见提供新视角，可指导干预措施提升招聘公平性。

Abstract: Faculty hiring shapes the flow of ideas, resources, and opportunities in
academia, influencing not only individual career trajectories but also broader
patterns of institutional prestige and scientific progress. While traditional
studies have found strong correlations between faculty hiring and attributes
such as doctoral department prestige and publication record, they rarely assess
whether these associations generalize to individual hiring outcomes,
particularly for future candidates outside the original sample. Here, we
consider faculty placement as an individual-level prediction task. Our data
consist of temporal co-authorship networks with conventional attributes such as
doctoral department prestige and bibliometric features. We observe that using
the co-authorship network significantly improves predictive accuracy by up to
10% over traditional indicators alone, with the largest gains observed for
placements at the most elite (top-10) departments. Our results underscore the
role that social networks, professional endorsements, and implicit advocacy
play in faculty hiring beyond traditional measures of scholarly productivity
and institutional prestige. By introducing a predictive framing of faculty
placement and establishing the benefit of considering co-authorship networks,
this work provides a new lens for understanding structural biases in academia
that could inform targeted interventions aimed at increasing transparency,
fairness, and equity in academic hiring practices.

</details>


### [521] [Privacy-Preserving Multimodal News Recommendation through Federated Learning](https://arxiv.org/abs/2507.15460)
*Mehdi Khalaj,Shahrzad Golestani Najafabadi,Julita Vassileva*

Main category: cs.SI

TL;DR: 文章提出基于多模态联邦学习的新闻推荐方法，整合文本与视觉特征、平衡用户长短兴趣、保障隐私，实验表现优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 传统个性化新闻推荐系统存在依赖文本、忽视短期兴趣和隐私问题，需改进。

Method: 采用多模态模型整合新闻文本与视觉特征；用时间感知模型通过多头自注意力网络平衡用户长短兴趣；实施联邦学习框架，结合安全聚合算法保障隐私。

Result: 在真实新闻数据集实验中表现优于现有系统。

Conclusion: 该方法是隐私保护个性化新闻推荐的重大进步。

Abstract: Personalized News Recommendation systems (PNR) have emerged as a solution to
information overload by predicting and suggesting news items tailored to
individual user interests. However, traditional PNR systems face several
challenges, including an overreliance on textual content, common neglect of
short-term user interests, and significant privacy concerns due to centralized
data storage. This paper addresses these issues by introducing a novel
multimodal federated learning-based approach for news recommendation. First, it
integrates both textual and visual features of news items using a multimodal
model, enabling a more comprehensive representation of content. Second, it
employs a time-aware model that balances users' long-term and short-term
interests through multi-head self-attention networks, improving recommendation
accuracy. Finally, to enhance privacy, a federated learning framework is
implemented, enabling collaborative model training without sharing user data.
The framework divides the recommendation model into a large server-maintained
news model and a lightweight user model shared between the server and clients.
The client requests news representations (vectors) and a user model from the
central server, then computes gradients with user local data, and finally sends
their locally computed gradients to the server for aggregation. The central
server aggregates gradients to update the global user model and news model. The
updated news model is further used to infer news representation by the server.
To further safeguard user privacy, a secure aggregation algorithm based on
Shamir's secret sharing is employed. Experiments on a real-world news dataset
demonstrate strong performance compared to existing systems, representing a
significant advancement in privacy-preserving personalized news recommendation.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [522] [Approximate Revenue Maximization for Diffusion Auctions](https://arxiv.org/abs/2507.14470)
*Yifan Huang,Dong Hao,Zhiyi Fan,Yuhang Guo,Bin Li*

Main category: econ.TH

TL;DR: 本文基于扩散拍卖设计，研究通过保留价格设计简单且接近最优的网络拍卖，给出保留价格函数，能保证近似理论上限收益。


<details>
  <summary>Details</summary>
Motivation: 以往基于保留价格的最优拍卖设计大多假设拍卖者可直接触达目标受众，忽略了经济网络中大量未察觉拍卖的投标人，本文旨在将最优拍卖理论的目标受众扩展到经济网络中的所有实体。

Method: 采用贝叶斯近似分析，为最具代表性的网络拍卖量身定制保留价格函数。

Result: 该保留价格函数保持了网络拍卖的激励相容性，若卖家在规模为n的网络中有ρ个直接邻居，此保留价格能保证达到理论上限收益的1 - 1/ρ近似。

Conclusion: 此保留价格函数适用于任何规模和结构的网络市场，能让卖家提取超出迈尔森最优拍卖的额外收益。

Abstract: Reserve prices are widely used in practice. The problem of designing
revenue-optimal auctions based on reserve price has drawn much attention in the
auction design community. Although they have been extensively studied, most
developments rely on the significant assumption that the target audience of the
sale is directly reachable by the auctioneer, while a large portion of bidders
in the economic network unaware of the sale are omitted. This work follows the
diffusion auction design, which aims to extend the target audience of optimal
auction theory to all entities in economic networks. We investigate the design
of simple and provably near-optimal network auctions via reserve price. Using
Bayesian approximation analysis, we provide a simple and explicit form of the
reserve price function tailored to the most representative network auction. We
aim to balance setting a sufficiently high reserve price to induce high revenue
in a successful sale, and attracting more buyers from the network to increase
the probability of a successful sale. This reserve price function preserves
incentive compatibility for network auctions, allowing the seller to extract
additional revenue beyond that achieved by the Myerson optimal auction.
Specifically, if the seller has $\rho$ direct neighbours in a network of size
$n$, this reserve price guarantees a $1-{1 \over \rho}$ approximation to the
theoretical upper bound, i.e., the maximum possible revenue from any network of
size $n$. This result holds for any size and any structure of the networked
market.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [523] [The Role of Excitatory Parvalbumin-positive Neurons in the Tectofugal Pathway of Pigeon (Columba livia) Hierarchical Visual Processing](https://arxiv.org/abs/2507.15486)
*Shan Lu,Xiaoteng Zhang,Yueyang Cang,Shihao Pan,Yanyan Peng,Xinwei Li,Shaoju Zeng,Yingjie Zhu,Li Shi*

Main category: q-bio.NC

TL;DR: 研究鸽子Ento - MVL回路中兴奋性PV+神经元在分层移动目标识别中的作用，揭示其功能及鸟类与哺乳动物视觉系统的神经处理策略趋同性。


<details>
  <summary>Details</summary>
Motivation: 鸟类和哺乳动物视觉系统有组织相似性，但鸽子分层视觉处理中特定神经元亚型及其功能作用不明，需研究兴奋性PV+神经元在分层移动目标识别中的作用。

Method: 采用电生理记录和免疫荧光染色，并用异时速度循环神经网络（HS - RNN）模型验证动力学。

Result: 发现源自内脑皮层内部（Ei）的兴奋性PV+神经元主要调节中侧腹间脑（MVL）对不同视觉刺激的反应，模型复制了Ento - MVL回路对移动视觉目标的快速适应。

Conclusion: PV+神经元的快速放电和兴奋特性使其能在Ento - MVL回路中快速处理运动相关信息，阐明了其在视觉背侧脑室嵴柱状组织下分层信息处理中的功能，强调了鸟类和哺乳动物视觉系统的神经处理策略趋同性。

Abstract: The visual systems of birds and mammals exhibit remarkable organizational
similarities: the dorsal ventricular ridge (DVR) demonstrates a columnar
microcircuitry that parallels the cortical architecture observed in mammals.
However, the specific neuronal subtypes involved and their functional roles in
pigeon hierarchical visual processing remain unclear. This study investigates
the role of excitatory parvalbumin (PV+) neurons within the Ento-MVL
(entoallium-mesopallium venterolaterale) circuit of pigeons underlying
hierarchical moving target recognition. Electrophysiological recordings and
immunofluorescence staining reveal that excitatory PV+ neurons originating from
the entopallial internal (Ei) predominantly modulate MVL responses to varying
visual stimuli. Using a heterochronous-speed recurrent neural network (HS-RNN)
model, we further validated these dynamics, replicating the rapid adaptation of
the Ento-MVL circuit to moving visual targets. The findings suggest that the
fast-spiking and excitatory properties of PV+ neurons enable rapid processing
of motion-related information within the Ento-MVL circuit. Our results
elucidate the functional role of excitatory PV+ neurons in hierarchical
information processing under the columnar organization of the visual DVR and
underscore the convergent neural processing strategies shared by avian and
mammalian visual systems.

</details>


### [524] [Dissociating model architectures from inference computations](https://arxiv.org/abs/2507.15776)
*Noor Sajid,Johan Medrano*

Main category: q-bio.NC

TL;DR: 研究自回归和深度时间模型在非马尔可夫序列建模中的差异，指出构建和完善预测的过程不一定与模型架构绑定。


<details>
  <summary>Details</summary>
Motivation: 探究自回归和深度时间模型在非马尔可夫序列建模处理上的差异，强调分离模型架构和推理计算的必要性。

Method: 通过在迭代推理中构建上下文访问，让自回归模型模仿深度时间计算；用基于下一个令牌预测训练的变压器，在迭代推理中引入分层时间分解。

Result: 在迭代推理中引入分层时间分解，在保持预测能力的同时减少计算量。

Conclusion: 构建和完善预测的过程不一定与底层模型架构相关。

Abstract: Parr et al., 2025 examines how auto-regressive and deep temporal models
differ in their treatment of non-Markovian sequence modelling. Building on
this, we highlight the need for dissociating model architectures, i.e., how the
predictive distribution factorises, from the computations invoked at inference.
We demonstrate that deep temporal computations are mimicked by autoregressive
models by structuring context access during iterative inference. Using a
transformer trained on next-token prediction, we show that inducing
hierarchical temporal factorisation during iterative inference maintains
predictive capacity while instantiating fewer computations. This emphasises
that processes for constructing and refining predictions are not necessarily
bound to their underlying model architectures.

</details>
