<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 7]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 92]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 13]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [gr-qc](#gr-qc) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 28]
- [eess.SY](#eess.SY) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CY](#cs.CY) [Total: 9]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.GR](#cs.GR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [econ.GN](#econ.GN) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 39]
- [math.OC](#math.OC) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.SD](#cs.SD) [Total: 4]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [math.ST](#math.ST) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models](https://arxiv.org/abs/2510.12864)
*Imran Khan*

Main category: cs.AI

TL;DR: 论文提出RID框架解决大语言模型规则刚性问题，经评估显著提升性能，推动模型向目标导向推理转变。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在规则刚性问题，阻碍构建可信自主智能体，且现有监督微调方法计算成本高。

Method: 引入Rule-Intent Distinction (RID) 框架，一种低计算量的元提示技术，为模型提供解构任务等的认知模式。

Result: 在自定义基准测试中，RID框架达到95%的人类对齐分数，高于基线的80%和CoT的75%，且推理质量更高。

Conclusion: 该方法实用、易获取且有效，能使大语言模型向目标导向推理转变，为构建可靠实用的AI智能体铺平道路。

Abstract: Large Language Models (LLMs) are increasingly being deployed as the reasoning
engines for agentic AI systems, yet they exhibit a critical flaw: a rigid
adherence to explicit rules that leads to decisions misaligned with human
common sense and intent. This "rule-rigidity" is a significant barrier to
building trustworthy autonomous agents. While prior work has shown that
supervised fine-tuning (SFT) with human explanations can mitigate this issue,
SFT is computationally expensive and inaccessible to many practitioners. To
address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a
novel, low-compute meta-prompting technique designed to elicit human-aligned
exception handling in LLMs in a zero-shot manner. The RID framework provides
the model with a structured cognitive schema for deconstructing tasks,
classifying rules, weighing conflicting outcomes, and justifying its final
decision. We evaluated the RID framework against baseline and Chain-of-Thought
(CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced
judgment across diverse domains. Our human-verified results demonstrate that
the RID framework significantly improves performance, achieving a 95% Human
Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT.
Furthermore, it consistently produces higher-quality, intent-driven reasoning.
This work presents a practical, accessible, and effective method for steering
LLMs from literal instruction-following to liberal, goal-oriented reasoning,
paving the way for more reliable and pragmatic AI agents.

</details>


### [2] [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)
*Wei Fan,Wenlin Yao,Zheng Li,Feng Yao,Xin Liu,Liang Qiu,Qingyu Yin,Yangqiu Song,Bing Yin*

Main category: cs.AI

TL;DR: 现有方法优化规划阶段存在不足，提出DeepPlanner框架提升深度研究智能体规划能力，实验显示其能提升规划质量并以低预算达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理阶段依赖隐式规划或引入显式规划器但未系统优化规划阶段，规划令牌存在优化不足的问题。

Method: 提出端到端强化学习框架DeepPlanner，用基于熵的项塑造令牌级优势，对规划密集的滚动更新选择性加权。

Result: 在七个深度研究基准测试中，DeepPlanner提升了规划质量，在低训练预算下取得了最先进的结果。

Conclusion: DeepPlanner能有效增强深度研究智能体的规划能力。

Abstract: Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.

</details>


### [3] [SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents](https://arxiv.org/abs/2510.12985)
*Simon Sinong Zhan,Yao Liu,Philip Wang,Zinan Wang,Qineng Wang,Zhian Ruan,Xiangyu Shi,Xinyu Cao,Frank Yang,Kangrui Wang,Huajie Shao,Manling Li,Qi Zhu*

Main category: cs.AI

TL;DR: 提出Sentinel框架，用于从语义、计划和轨迹层面正式评估基于大语言模型的具身智能体物理安全性，在多个环境中评估多种具身智能体，能发现先前方法遗漏的安全问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式规则或主观大语言模型判断，缺乏对基于大语言模型的具身智能体物理安全的正式评估方法。

Method: 将实际安全需求基于形式化时态逻辑语义，采用多级验证管道，分别在语义、计划和轨迹层面进行验证。

Result: 在VirtualHome和ALFRED中应用Sentinel评估多种具身智能体，发现先前方法遗漏的安全问题。

Conclusion: Sentinel为系统评估物理环境中基于大语言模型的具身智能体提供了严谨基础。

Abstract: We present Sentinel, the first framework for formally evaluating the physical
safety of Large Language Model(LLM-based) embodied agents across the semantic,
plan, and trajectory levels. Unlike prior methods that rely on heuristic rules
or subjective LLM judgments, Sentinel grounds practical safety requirements in
formal temporal logic (TL) semantics that can precisely specify state
invariants, temporal dependencies, and timing constraints. It then employs a
multi-level verification pipeline where (i) at the semantic level, intuitive
natural language safety requirements are formalized into TL formulas and the
LLM agent's understanding of these requirements is probed for alignment with
the TL formulas; (ii) at the plan level, high-level action plans and subgoals
generated by the LLM agent are verified against the TL formulas to detect
unsafe plans before execution; and (iii) at the trajectory level, multiple
execution trajectories are merged into a computation tree and efficiently
verified against physically-detailed TL specifications for a final safety
check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate
multiple LLM-based embodied agents against diverse safety requirements. Our
experiments show that by grounding physical safety in temporal logic and
applying verification methods across multiple levels, Sentinel provides a
rigorous foundation for systematically evaluating LLM-based embodied agents in
physical environments, exposing safety violations overlooked by previous
methods and offering insights into their failure modes.

</details>


### [4] [From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model](https://arxiv.org/abs/2510.13002)
*Boyou Chen,Gerui Xu,Zifei Wang,Huizhong Guo,Ananna Ahmed,Zhaonan Sun,Zhen Hu,Kaihan Zhang,Shan Bao*

Main category: cs.AI

TL;DR: 本文提出利用微调大语言模型从文本事故叙述中自动推断驾驶员危险行为（DHA）的框架，在两年车辆碰撞数据上微调Llama 3.2 1B模型，准确率达80%，还通过概率推理方法分析不同场景下DHA概率，为大规模自动DHA检测提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 两车辆碰撞事故常见且影响交通安全，现有大规模数据库中DHA数据因人工编码可靠性有限，需新方法提升DHA分类有效性和可解释性。

Method: 利用五年两车辆碰撞数据微调Llama 3.2 1B模型，与传统机器学习分类器对比；开发概率推理方法，分析原测试集和三个反事实场景下模型输出变化。

Result: 微调的大语言模型整体准确率达80%，优于所有基线模型，在数据不平衡场景有明显改进；分析揭示不同场景下DHA概率变化。

Conclusion: 框架和分析方法为大规模自动DHA检测提供可靠且可解释的解决方案，为交通安全分析和干预提供新机会。

Abstract: Vehicle crashes involve complex interactions between road users, split-second
decisions, and challenging environmental conditions. Among these, two-vehicle
crashes are the most prevalent, accounting for approximately 70% of roadway
crashes and posing a significant challenge to traffic safety. Identifying
Driver Hazardous Action (DHA) is essential for understanding crash causation,
yet the reliability of DHA data in large-scale databases is limited by
inconsistent and labor-intensive manual coding practices. Here, we present an
innovative framework that leverages a fine-tuned large language model to
automatically infer DHAs from textual crash narratives, thereby improving the
validity and interpretability of DHA classifications. Using five years of
two-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on
detailed crash narratives and benchmarked its performance against conventional
machine learning classifiers, including Random Forest, XGBoost, CatBoost, and a
neural network. The fine-tuned LLM achieved an overall accuracy of 80%,
surpassing all baseline models and demonstrating pronounced improvements in
scenarios with imbalanced data. To increase interpretability, we developed a
probabilistic reasoning approach, analyzing model output shifts across original
test sets and three targeted counterfactual scenarios: variations in driver
distraction and age. Our analysis revealed that introducing distraction for one
driver substantially increased the likelihood of "General Unsafe Driving";
distraction for both drivers maximized the probability of "Both Drivers Took
Hazardous Actions"; and assigning a teen driver markedly elevated the
probability of "Speed and Stopping Violations." Our framework and analytical
methods provide a robust and interpretable solution for large-scale automated
DHA detection, offering new opportunities for traffic safety analysis and
intervention.

</details>


### [5] [Toward Reasoning-Centric Time-Series Analysis](https://arxiv.org/abs/2510.13029)
*Xinlei Wang,Mingtian Tan,Jing Qiu,Junhua Zhao,Jinjin Gu*

Main category: cs.AI

TL;DR: 本文提出将大语言模型用于时间序列分析，将其视为推理任务，以实现更具因果结构和可解释性的分析。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分析依赖模式识别，难以应对现实环境变化，大语言模型虽带来新机遇，但现有方法未充分挖掘其推理潜力。

Method: 将大语言模型用于时间序列分析，将其作为推理任务，注重因果结构和可解释性。

Result: 使时间序列分析更接近人类理解，能在复杂现实环境中获得透明且有上下文感知的见解。

Conclusion: 应将大语言模型用于时间序列分析，将其作为推理任务，以提升分析效果。

Abstract: Traditional time series analysis has long relied on pattern recognition,
trained on static and well-established benchmarks. However, in real-world
settings -- where policies shift, human behavior adapts, and unexpected events
unfold -- effective analysis must go beyond surface-level trends to uncover the
actual forces driving them. The recent rise of Large Language Models (LLMs)
presents new opportunities for rethinking time series analysis by integrating
multimodal inputs. However, as the use of LLMs becomes popular, we must remain
cautious, asking why we use LLMs and how to exploit them effectively. Most
existing LLM-based methods still employ their numerical regression ability and
ignore their deeper reasoning potential. This paper argues for rethinking time
series with LLMs as a reasoning task that prioritizes causal structure and
explainability. This shift brings time series analysis closer to human-aligned
understanding, enabling transparent and context-aware insights in complex
real-world environments.

</details>


### [6] [Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking](https://arxiv.org/abs/2510.13036)
*Stephane Hatgis-Kessell,Logan Mondal Bhamidipaty,Emma Brunskill*

Main category: cs.AI

TL;DR: 提出基于偏好的奖励修复（PBRR）框架修复人类指定的代理奖励函数，在表格域有理论保证，在奖励破解基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 人类设计的强化学习奖励函数常与真实目标不一致，导致奖励破解；从人类反馈中学习奖励函数构建数据集成本高。

Method: 提出PBRR框架，通过从偏好中学习依赖转移的加法校正项来修复代理奖励函数，使用有针对性的探索策略和新的偏好学习目标。

Result: 在表格域证明PBRR的累积遗憾与先前基于偏好的强化学习方法相当；在奖励破解基准测试中始终优于基线，学习高性能策略所需偏好显著减少。

Conclusion: PBRR是一种有效的修复人类指定代理奖励函数的方法，能以较少偏好学习到高性能策略。

Abstract: Human-designed reward functions for reinforcement learning (RL) agents are
frequently misaligned with the humans' true, unobservable objectives, and thus
act only as proxies. Optimizing for a misspecified proxy reward function often
induces reward hacking, resulting in a policy misaligned with the human's true
objectives. An alternative is to perform RL from human feedback, which involves
learning a reward function from scratch by collecting human preferences over
pairs of trajectories. However, building such datasets is costly. To address
the limitations of both approaches, we propose Preference-Based Reward Repair
(PBRR): an automated iterative framework that repairs a human-specified proxy
reward function by learning an additive, transition-dependent correction term
from preferences. A manually specified reward function can yield policies that
are highly suboptimal under the ground-truth objective, yet corrections on only
a few transitions may suffice to recover optimal performance. To identify and
correct for those transitions, PBRR uses a targeted exploration strategy and a
new preference-learning objective. We prove in tabular domains PBRR has a
cumulative regret that matches, up to constants, that of prior preference-based
RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR
consistently outperforms baselines that learn a reward function from scratch
from preferences or modify the proxy reward function using other approaches,
requiring substantially fewer preferences to learn high performing policies.

</details>


### [7] [Mobile Coverage Analysis using Crowdsourced Data](https://arxiv.org/abs/2510.13459)
*Timothy Wong,Tom Freeman,Joseph Feehily*

Main category: cs.AI

TL;DR: 本文提出利用众包QoE数据进行移动覆盖和弱信号点分析的新框架，用OC - SVM算法计算覆盖，能准确绘制覆盖图并找出信号不足区域。


<details>
  <summary>Details</summary>
Motivation: 有效评估移动网络覆盖和精准识别服务弱信号点，以提升用户体验质量。

Method: 在单个小区（天线）层面进行覆盖分析并汇总到站点层面，使用经验地理定位数据；应用OC - SVM算法计算移动网络覆盖；扩展该方法分析众包服务丢失报告。

Result: 该框架能准确绘制移动覆盖图，突出信号不足的细粒度区域。

Conclusion: 此新框架在准确绘制移动覆盖和识别信号不足区域方面有效，尤其适用于复杂城市环境。

Abstract: Effective assessment of mobile network coverage and the precise
identification of service weak spots are paramount for network operators
striving to enhance user Quality of Experience (QoE). This paper presents a
novel framework for mobile coverage and weak spot analysis utilising
crowdsourced QoE data. The core of our methodology involves coverage analysis
at the individual cell (antenna) level, subsequently aggregated to the site
level, using empirical geolocation data. A key contribution of this research is
the application of One-Class Support Vector Machine (OC-SVM) algorithm for
calculating mobile network coverage. This approach models the decision
hyperplane as the effective coverage contour, facilitating robust calculation
of coverage areas for individual cells and entire sites. The same methodology
is extended to analyse crowdsourced service loss reports, thereby identifying
and quantifying geographically localised weak spots. Our findings demonstrate
the efficacy of this novel framework in accurately mapping mobile coverage and,
crucially, in highlighting granular areas of signal deficiency, particularly
within complex urban environments.

</details>


### [8] [Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation](https://arxiv.org/abs/2510.13195)
*Qun Ma,Xiao Xue,Xuwen Zhang,Zihan Zhao,Yuwei Guo,Ming Zhang*

Main category: cs.AI

TL;DR: 本文构建情感认知框架解决基于大语言模型的智能体在情感认知方面的局限，实验表明该框架下的智能体表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体在情感认知方面存在严重局限，如无法模拟有限理性、缺乏嵌入情感的集成机制。

Method: 构建包含欲望生成和目标管理的情感认知框架，模拟基于大语言模型智能体的完整决策过程，并在自有多智能体交互环境中实现该框架。

Result: 该框架下的智能体行为与情感状态一致，相比其他类型智能体，生态效度更高，决策结果更接近人类行为模式。

Conclusion: 所构建的情感认知框架能实现基于大语言模型的智能体与人类的情感对齐，提升智能体决策表现。

Abstract: The advent of large language models (LLMs) has enabled agents to represent
virtual humans in societal simulations, facilitating diverse interactions
within complex social systems. However, existing LLM-based agents exhibit
severe limitations in affective cognition: They fail to simulate the bounded
rationality essential for bridging virtual and real-world services; They lack
empirically validated integration mechanisms embedding emotions within agent
decision architectures. This paper constructs an emotional cognition framework
incorporating desire generation and objective management, designed to achieve
emotion alignment between LLM-based agents and humans, modeling the complete
decision-making process of LLM-based agents, encompassing state evolution,
desire generation, objective optimization, decision generation, and action
execution. This study implements the proposed framework within our proprietary
multi-agent interaction environment. Experimental results demonstrate that
agents governed by our framework not only exhibit behaviors congruent with
their emotional states but also, in comparative assessments against other agent
types, demonstrate superior ecological validity and generate decision outcomes
that significantly more closely approximate human behavioral patterns.

</details>


### [9] [Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning](https://arxiv.org/abs/2510.13214)
*Zehui Ling,Deshu Chen,Yichi Zhang,Yuchen Liu,Xigui Li,Xin Guo,Yuan Cheng*

Main category: cs.AI

TL;DR: 提出整合大小语言模型的互补代理系统，简单问题降低大模型计算成本，复杂任务保持性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型深度推理计算成本高，需降低成本。

Method: 小语言模型先给出初始答案，大语言模型验证，正确则采用，错误则深度推理。

Result: 简单问题降低大模型超50%计算成本，精度损失可忽略，复杂任务保持稳健性能。

Conclusion: 所提互补代理系统有效，能在不同难度问题上平衡计算成本和性能。

Abstract: Recent advances in Large Language Models (LLMs) demonstrate that
chain-of-thought prompting and deep reasoning substantially enhance performance
on complex tasks, and multi-agent systems can further improve accuracy by
enabling model debates. However, applying deep reasoning to all problems is
computationally expensive. To mitigate these costs, we propose a complementary
agent system integrating small and large LLMs. The small LLM first generates an
initial answer, which is then verified by the large LLM. If correct, the answer
is adopted directly; otherwise, the large LLM performs in-depth reasoning.
Experimental results show that, for simple problems, our approach reduces the
computational cost of the large LLM by more than 50% with negligible accuracy
loss, while consistently maintaining robust performance on complex tasks.

</details>


### [10] [Personalized Learning Path Planning with Goal-Driven Learner State Modeling](https://arxiv.org/abs/2510.13215)
*Joy Jia Yin Lim,Ye He,Jifan Yu,Xin Cong,Daniel Zhang-Li,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.AI

TL;DR: 提出Pxplore框架用于个性化学习路径规划，实验验证其有效性并开源代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型用于个性化学习路径规划时缺乏目标对齐机制，需设计新框架。

Method: 引入Pxplore框架，设计结构化学习者状态模型和自动奖励函数，结合监督微调（SFT）和组相对策略优化（GRPO）训练策略并部署到学习平台。

Result: 实验验证Pxplore能生成连贯、个性化且目标驱动的学习路径。

Conclusion: Pxplore框架在个性化学习路径规划中有效，开源代码和数据集利于后续研究。

Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning
paths that align with individual goals. While large language models (LLMs) show
potential in personalizing learning experiences, existing approaches often lack
mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework
for PLPP that integrates a reinforcement-based training paradigm and an
LLM-driven educational architecture. We design a structured learner state model
and an automated reward function that transforms abstract objectives into
computable signals. We train the policy combining supervised fine-tuning (SFT)
and Group Relative Policy Optimization (GRPO), and deploy it within a
real-world learning platform. Extensive experiments validate Pxplore's
effectiveness in producing coherent, personalized, and goal-driven learning
paths. We release our code and dataset to facilitate future research.

</details>


### [11] [EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems](https://arxiv.org/abs/2510.13220)
*Yufei He,Juncheng Liu,Yue Liu,Yibo Li,Tri Cao,Zhiyuan Hu,Xinxing Xu,Bryan Hooi*

Main category: cs.AI

TL;DR: 当前AI智能体在测试时难以学习复杂技能，本文引入J - TTL基准测试，提出EvoTest框架，该框架在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体在测试时无法即时学习复杂技能，严重限制其实用性，需衡量和推动此方面的进展。

Method: 引入J - TTL基准测试，提出EvoTest进化式测试时间学习框架，框架包含Actor Agent和Evolver Agent，可更新提示、记忆、超参数等。

Result: EvoTest在J - TTL基准测试中持续提升性能，优于多种基线方法，且是唯一能赢得两个游戏的方法。

Conclusion: EvoTest框架有效解决了AI智能体在测试时学习复杂技能的问题，在基准测试中展现出良好性能。

Abstract: A fundamental limitation of current AI agents is their inability to learn
complex skills on the fly at test time, often behaving like "clever but
clueless interns" in novel environments. This severely limits their practical
utility. To systematically measure and drive progress on this challenge, we
first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a
new evaluation setup where an agent must play the same game for several
consecutive episodes, attempting to improve its performance from one episode to
the next. On J-TTL, we find that existing adaptation methods like reflection,
memory, or reinforcement learning struggle. To address the challenges posed by
our benchmark, we present EvoTest, an evolutionary test-time learning framework
that improves an agent without any fine-tuning or gradients-by evolving the
entire agentic system after every episode. EvoTest has two roles: the Actor
Agent, which plays the game, and the Evolver Agent, which analyzes the episode
transcript to propose a revised configuration for the next run. This
configuration rewrites the prompt, updates memory by logging effective
state-action choices, tunes hyperparameters, and learns the tool-use routines.
On our J-TTL benchmark, EvoTest consistently increases performance,
outperforming not only reflection and memory-only baselines but also more
complex online fine-tuning methods. Notably, our method is the only one capable
of winning two games (Detective and Library), while all baselines fail to win
any.

</details>


### [12] [An Analytical Framework to Enhance Autonomous Vehicle Perception for Smart Cities](https://arxiv.org/abs/2510.13230)
*Jalal Khan,Manzoor Khan,Sherzod Turaev,Sumbal Malik,Hesham El-Sayed,Farman Ullah*

Main category: cs.AI

TL;DR: 本文提出基于效用的分析模型助力自动驾驶车辆感知系统理解驾驶环境，经实验验证该模型能为自动驾驶车辆找到合适感知。


<details>
  <summary>Details</summary>
Motivation: 开发深度学习模型和人工智能解决方案，以增强自动驾驶车辆智能出行能力，需准确感知道路多对象并预测驾驶员感知来控制车辆。

Method: 提出基于效用的分析模型，包括获取含独特对象的自定义数据集、基于深度学习的目标检测模型（YOLOv8s）及衡量感知服务效用的模块，通过目标检测任务验证模型并与先进深度学习模型性能指标对比。

Result: 实验显示三个表现最佳的YOLOv8s实例，AdamW - 基于模型类级性能值更好，优于SGD - 基于模型。

Conclusion: 鼓励使用所提感知模型评估学习模型效用并确定自动驾驶车辆的合适感知。

Abstract: The driving environment perception has a vital role for autonomous driving
and nowadays has been actively explored for its realization. The research
community and relevant stakeholders necessitate the development of Deep
Learning (DL) models and AI-enabled solutions to enhance autonomous vehicles
(AVs) for smart mobility. There is a need to develop a model that accurately
perceives multiple objects on the road and predicts the driver's perception to
control the car's movements. This article proposes a novel utility-based
analytical model that enables perception systems of AVs to understand the
driving environment. The article consists of modules: acquiring a custom
dataset having distinctive objects, i.e., motorcyclists, rickshaws, etc; a
DL-based model (YOLOv8s) for object detection; and a module to measure the
utility of perception service from the performance values of trained model
instances. The perception model is validated based on the object detection
task, and its process is benchmarked by state-of-the-art deep learning models'
performance metrics from the nuScense dataset. The experimental results show
three best-performing YOLOv8s instances based on mAP@0.5 values, i.e.,
SGD-based (0.832), Adam-based (0.810), and AdamW-based (0.822). However, the
AdamW-based model (i.e., car: 0.921, motorcyclist: 0.899, truck: 0.793, etc.)
still outperforms the SGD-based model (i.e., car: 0.915, motorcyclist: 0.892,
truck: 0.781, etc.) because it has better class-level performance values,
confirmed by the proposed perception model. We validate that the proposed
function is capable of finding the right perception for AVs. The results above
encourage using the proposed perception model to evaluate the utility of
learning models and determine the appropriate perception for AVs.

</details>


### [13] [SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2510.13262)
*Weiqi Guo,Guanjun Liu,Ziyuan Zhou*

Main category: cs.AI

TL;DR: 提出SAJA框架攻击MADRL模型，在MPE中评估显示其优于单一攻击且现有防御方法无效。


<details>
  <summary>Details</summary>
Motivation: MADRL模型易受对抗扰动影响，现有研究未有效结合状态和动作攻击，需研究其从攻击角度的鲁棒性。

Method: 提出SAJA框架，含状态攻击和动作攻击两阶段，用多步梯度上升法计算对抗状态和动作，在损失函数中添加启发式正则化器。

Result: 在MPE中评估，SAJA优于单一攻击且更隐蔽，现有防御方法无法抵御其攻击。

Conclusion: SAJA框架在攻击MADRL模型上有良好效果，现有防御方法需改进。

Abstract: Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for
cooperative and competitive tasks such as autonomous driving and strategic
gaming. However, models trained by MADRL are vulnerable to adversarial
perturbations on states and actions. Therefore, it is essential to investigate
the robustness of MADRL models from an attack perspective. Existing studies
focus on either state-only attacks or action-only attacks, but do not consider
how to effectively joint them. Simply combining state and action perturbations
such as randomly perturbing states and actions does not exploit their potential
synergistic effects. In this paper, we propose the State-Action Joint Attack
(SAJA) framework that has a good synergistic effects. SAJA consists of two
important phases: (1) In the state attack phase, a multi-step gradient ascent
method utilizes both the actor network and the critic network to compute an
adversarial state, and (2) in the action attack phase, based on the perturbed
state, a second gradient ascent uses the critic network to craft the final
adversarial action. Additionally, a heuristic regularizer measuring the
distance between the perturbed actions and the original clean ones is added
into the loss function to enhance the effectiveness of the critic's guidance.
We evaluate SAJA in the Multi-Agent Particle Environment (MPE), demonstrating
that (1) it outperforms and is more stealthy than state-only or action-only
attacks, and (2) existing state or action defense methods cannot defend its
attacks.

</details>


### [14] [Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization](https://arxiv.org/abs/2510.13393)
*Yunxiao Zhao,Zhiqiang Wang,Xingtong Yu,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.AI

TL;DR: 本文从博弈论视角重新审视协同合理化问题，指出模式崩溃根源，提出PORAT方法解决问题，在多数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统合理化方法存在模式崩溃问题，且现有研究缺乏统一考虑，需解决该问题。

Method: 从博弈论视角系统审视，提出Game - theoretic Policy Optimization oriented RATionalization (PORAT)方法，逐步引入策略干预解决博弈均衡。

Result: 在九个真实数据集和两个合成设置上验证，PORAT比现有最优方法性能提升达8.1%。

Conclusion: 理论分析了次优均衡原因，证明PORAT方法可行，该方法能引导模型达到更优状态。

Abstract: Rationalization, a data-centric framework, aims to build self-explanatory
models to explain the prediction outcome by generating a subset of
human-intelligible pieces of the input data. It involves a cooperative game
model where a generator generates the most human-intelligible parts of the
input (i.e., rationales), followed by a predictor that makes predictions based
on these generated rationales. Conventional rationalization methods typically
impose constraints via regularization terms to calibrate or penalize undesired
generation. However, these methods are suffering from a problem called mode
collapse, in which the predictor produces correct predictions yet the generator
consistently outputs rationales with collapsed patterns. Moreover, existing
studies are typically designed separately for specific collapsed patterns,
lacking a unified consideration. In this paper, we systematically revisit
cooperative rationalization from a novel game-theoretic perspective and
identify the fundamental cause of this problem: the generator no longer tends
to explore new strategies to uncover informative rationales, ultimately leading
the system to converge to a suboptimal game equilibrium (correct predictions
v.s collapsed rationales). To solve this problem, we then propose a novel
approach, Game-theoretic Policy Optimization oriented RATionalization (PORAT),
which progressively introduces policy interventions to address the game
equilibrium in the cooperative game process, thereby guiding the model toward a
more optimal solution state. We theoretically analyse the cause of such a
suboptimal equilibrium and prove the feasibility of the proposed method.
Furthermore, we validate our method on nine widely used real-world datasets and
two synthetic settings, where PORAT achieves up to 8.1% performance
improvements over existing state-of-the-art methods.

</details>


### [15] [Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse](https://arxiv.org/abs/2510.13417)
*Liesbeth Allein,Nataly Pineda-Castañeda,Andrea Rocci,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 本文通过隐式因果链发现任务，评估大语言模型（LLMs）的机制因果推理能力，发现其判断多基于关联模式匹配，生成因果链有逻辑连贯性，研究成果为后续工作奠定基础。


<details>
  <summary>Details</summary>
Motivation: 探究原因如何导致结果以及中间因果步骤，考察大语言模型的机制因果推理能力。

Method: 在诊断评估框架下，让九个大语言模型为气候变化相关的因果对生成中间因果步骤。

Result: 大语言模型生成的因果步骤数量和粒度有差异，判断主要基于关联模式匹配，生成的因果链有逻辑连贯性。

Conclusion: 研究提供的基线方法、评估见解和基准数据集为论证场景下的隐式机制因果推理未来工作奠定基础。

Abstract: How does a cause lead to an effect, and which intermediate causal steps
explain their connection? This work scrutinizes the mechanistic causal
reasoning capabilities of large language models (LLMs) to answer these
questions through the task of implicit causal chain discovery. In a diagnostic
evaluation framework, we instruct nine LLMs to generate all possible
intermediate causal steps linking given cause-effect pairs in causal chain
structures. These pairs are drawn from recent resources in argumentation
studies featuring polarized discussion on climate change. Our analysis reveals
that LLMs vary in the number and granularity of causal steps they produce.
Although they are generally self-consistent and confident about the
intermediate causal connections in the generated chains, their judgments are
mainly driven by associative pattern matching rather than genuine causal
reasoning. Nonetheless, human evaluations confirmed the logical coherence and
integrity of the generated chains. Our baseline causal chain discovery
approach, insights from our diagnostic evaluation, and benchmark dataset with
causal chains lay a solid foundation for advancing future work in implicit,
mechanistic causal reasoning in argumentation settings.

</details>


### [16] [Confidence as a Reward: Transforming LLMs into Reward Models](https://arxiv.org/abs/2510.13501)
*He Du,Bowen Li,Chengxing Xie,Chang Gao,Kai Chen,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文研究无训练的Confidence-as-a-Reward (CRew)方法，在数学推理任务上表现出色，还提出CRew - DPO策略提升模型判断能力。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型需大量数据和高成本训练，且使用置信度作为奖励的概念未被全面研究。

Method: 系统研究利用模型最终答案的标记级置信度作为奖励代理的CRew方法，提出结合置信度分数和正确性信号构建偏好数据的CRew - DPO训练策略。

Result: CRew在MATH500和RewardMATH基准上优于现有无训练奖励方法，甚至超过多数训练过的奖励模型；发现CRew分数与模型实际推理性能强相关，能有效过滤高质量训练数据；CRew - DPO提升了模型判断能力，优于现有自训练方法。

Conclusion: CRew是一种简单有效的无训练奖励方法，CRew - DPO策略能进一步提升模型性能。

Abstract: Reward models can significantly enhance the reasoning capabilities of large
language models (LLMs), but they typically require extensive curated data and
costly training. To mitigate these challenges, training-free approaches such as
LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate
responses, achieving promising results. Recent works have also indicated that
model confidence can serve effectively as a reward metric, distinguishing
between chain-of-thought (CoT) and non-CoT paths. However, the concept of using
confidence as a reward has not been comprehensively studied. In this work, we
systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful
training-free method that utilizes token-level confidence in the model's final
answers as a proxy for reward, especially suitable for close-ended tasks.
Through extensive experiments on mathematical reasoning tasks, we demonstrate
that CRew outperforms existing training-free reward approaches on the MATH500
and RewardMATH benchmarks, and even surpasses most trained reward models. We
further identify a strong correlation between CRew scores and the actual
reasoning performance of the model. Additionally, we find that CRew can
effectively filter high-quality training data. Building upon these insights, we
propose CRew-DPO, a training strategy that constructs preference data from
confidence scores combined with correctness signals. Finetuning with CRew-DPO
further enhances the model's judging capabilities and consistently outperforms
existing self-training methods.

</details>


### [17] [A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain](https://arxiv.org/abs/2510.13524)
*William Flanagan,Mukunda Das,Rajitha Ramanyake,Swaunja Maslekar,Meghana Manipuri,Joong Ho Choi,Shruti Nair,Shambhavi Bhusan,Sanjana Dulam,Mouni Pendharkar,Nidhi Singh,Vashisth Doshi,Sachi Shah Paresh*

Main category: cs.AI

TL;DR: 本文指出金融服务业采用生成式AI时衡量模型性能存在障碍，提出风险评估框架以更好应用指标。


<details>
  <summary>Details</summary>
Motivation: 解决金融服务业采用生成式AI时衡量模型性能的难题，包括历史机器学习指标不适用、未考虑独特风险以及广泛基准无法用于工业场景等。

Method: 提出风险评估框架。

Result: 无明确提及具体结果。

Conclusion: 通过风险评估框架可更好应用主题专家和机器学习指标。

Abstract: As Generative Artificial Intelligence is adopted across the financial
services industry, a significant barrier to adoption and usage is measuring
model performance. Historical machine learning metrics can oftentimes fail to
generalize to GenAI workloads and are often supplemented using Subject Matter
Expert (SME) Evaluation. Even in this combination, many projects fail to
account for various unique risks present in choosing specific metrics.
Additionally, many widespread benchmarks created by foundational research labs
and educational institutions fail to generalize to industrial use. This paper
explains these challenges and provides a Risk Assessment Framework to allow for
better application of SME and machine learning Metrics

</details>


### [18] [Tandem Training for Language Models](https://arxiv.org/abs/2510.13551)
*Robert West,Ashton Anderson,Ece Kamar,Eric Horvitz*

Main category: cs.AI

TL;DR: 语言模型发展使可解释性和监督面临挑战，本文提出串联训练方法，在GSM8K任务中有效提升模型可理解性，为构建可审计AI系统提供思路。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型发展，其行为和推理难被弱智能体和人类理解，需方法让模型生成弱合作者能理解的解决方案。

Method: 将可理解性形式化为交接鲁棒性，引入语言模型的串联训练，在强化学习中随机从冻结的弱模型采样展开标记。

Result: 在GSM8K数学推理任务中，串联训练使模型放弃行话、适应弱伙伴语言，且保持高任务准确率。

Conclusion: 串联训练为构建可被弱智能体审计的AI系统提供了有前景的途径，对人机协作和多智能体通信有意义。

Abstract: As language models continue to rapidly improve, we can expect their actions
and reasoning to become difficult or impossible for weaker agents and humans to
follow, undermining interpretability and oversight. With an eye on long-term
futures, we pursue methods that encourage models to produce solutions that
remain intelligible to weaker collaborators. We formalize intelligibility as
handoff robustness: a strong model's solution is intelligible to a weaker model
if randomly handing off control to the weaker model along the solution path
does not cause failure. Building on this criterion, we introduce tandem
training for language models, a reinforcement learning (RL) paradigm in which
rollout tokens are intermittently and randomly sampled from a frozen weak model
rather than the strong model being trained. Because rollouts succeed only when
the strong model's actions and reasoning process can be continued by the weak
model -- when the two can co-construct a successful solution -- optimizing
standard RL objectives with tandem training implicitly incentivizes both
correctness and intelligibility. In the GSM8K math reasoning task, tandem
training reliably teaches models to abandon jargon and adapt their language to
weaker partners while keeping task accuracy high. Our results demonstrate a
promising route to building AI systems that remain auditable by weaker agents,
with implications for human--AI collaboration and multi-agent communication.

</details>


### [19] [A Modal Logic for Temporal and Jurisdictional Classifier Models](https://arxiv.org/abs/2510.13691)
*Cecilia Di Florio,Huimin Dong,Antonino Rotolo*

Main category: cs.AI

TL;DR: 本文引入用于正式捕捉法律案例推理的分类器模态逻辑，纳入解决先例冲突原则。


<details>
  <summary>Details</summary>
Motivation: 构建用于法律领域机器学习分类器的验证工具，正式捕捉法律案例推理。

Method: 引入分类器的模态逻辑，纳入案例时间维度和法院层级以解决先例冲突。

Result: 无明确提及具体结果。

Conclusion: 无明确提及具体结论。

Abstract: Logic-based models can be used to build verification tools for machine
learning classifiers employed in the legal field. ML classifiers predict the
outcomes of new cases based on previous ones, thereby performing a form of
case-based reasoning (CBR). In this paper, we introduce a modal logic of
classifiers designed to formally capture legal CBR. We incorporate principles
for resolving conflicts between precedents, by introducing into the logic the
temporal dimension of cases and the hierarchy of courts within the legal
system.

</details>


### [20] [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)
*Evan Ellis,Vivek Myers,Jens Tuyls,Sergey Levine,Anca Dragan,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 提出基于最大化人类能力的辅助语言模型微调方法Empower，经用户研究和新环境评估，效果优于基线，提供仅用离线数据训练有用对齐AI代理的框架。


<details>
  <summary>Details</summary>
Motivation: 现有构建辅助代理的方法鼓励代理独自完成任务而非真正协助人类，且需昂贵的明确人类反馈。

Method: 提出基于最大化人类能力的Empower方法，仅需离线文本数据微调语言模型。

Result: 用户研究中参与者78%的时间更喜欢Empower助理，接受率高31%，建议少38%；新环境中Empower训练的代理使模拟人类程序员成功率平均提高192%。

Conclusion: Empower目标提供了仅用离线数据、无需额外人类反馈或可验证奖励来训练有用对齐AI代理的框架。

Abstract: Assistive agents should not only take actions on behalf of a human, but also
step out of the way and cede control when there are important decisions to be
made. However, current methods for building assistive agents, whether via
mimicking expert humans or via RL finetuning on an inferred reward, often
encourage agents to complete tasks on their own rather than truly assisting the
human attain their objectives. Additionally, these methods often require costly
explicit human feedback to provide a training signal. We propose a new approach
to tuning assistive language models based on maximizing the human's
empowerment, their ability to effect desired changes in the environment. Our
empowerment-maximizing method, Empower, only requires offline text data,
providing a self-supervised method for fine-tuning language models to better
assist humans. To study the efficacy of our approach, we conducted an 18-person
user study comparing our empowerment assistant with a strong baseline.
Participants preferred our assistant 78% of the time (p=0.015), with a 31%
higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a
new environment for evaluating multi-turn code assistance using simulated
humans. Using this environment, we show that agents trained with Empower
increase the success rate of a simulated human programmer on challenging coding
questions by an average of 192% over an SFT baseline. With this empowerment
objective, we provide a framework for useful aligned AI agents at scale using
only offline data without the need for any additional human feedback or
verifiable rewards.

</details>


### [21] [From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails](https://arxiv.org/abs/2510.13727)
*Ravi Pandya,Madison Bland,Duy P. Nguyen,Changliu Liu,Jaime Fernández Fisac,Andrea Bajcsy*

Main category: cs.AI

TL;DR: 提出将代理式AI安全视为顺序决策问题，用安全关键控制理论构建预测护栏，实验证明其能避免灾难性结果并保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI护栏依赖输出分类，对新危险情况脆弱且检测后无恢复路径，需新的安全方法。

Method: 从安全关键控制理论视角，在AI模型潜在表征中形式化问题，构建模型无关的预测护栏，并提供通过安全关键强化学习大规模计算护栏的训练方法。

Result: 在模拟驾驶和电商场景实验中，控制理论护栏能可靠引导大语言模型代理避免灾难性结果，同时保持任务性能。

Conclusion: 控制理论护栏提供了一种有原则的动态替代方案，可取代现有的标记-阻止护栏。

Abstract: Generative AI systems are increasingly assisting and acting on behalf of end
users in practical settings, from digital shopping assistants to
next-generation autonomous cars. In this context, safety is no longer about
blocking harmful content, but about preempting downstream hazards like
financial or physical harm. Yet, most AI guardrails continue to rely on output
classification based on labeled datasets and human-specified criteria,making
them brittle to new hazardous situations. Even when unsafe conditions are
flagged, this detection offers no path to recovery: typically, the AI system
simply refuses to act--which is not always a safe choice. In this work, we
argue that agentic AI safety is fundamentally a sequential decision problem:
harmful outcomes arise from the AI system's continually evolving interactions
and their downstream consequences on the world. We formalize this through the
lens of safety-critical control theory, but within the AI model's latent
representation of the world. This enables us to build predictive guardrails
that (i) monitor an AI system's outputs (actions) in real time and (ii)
proactively correct risky outputs to safe ones, all in a model-agnostic manner
so the same guardrail can be wrapped around any AI model. We also offer a
practical training recipe for computing such guardrails at scale via
safety-critical reinforcement learning. Our experiments in simulated driving
and e-commerce settings demonstrate that control-theoretic guardrails can
reliably steer LLM agents clear of catastrophic outcomes (from collisions to
bankruptcy) while preserving task performance, offering a principled dynamic
alternative to today's flag-and-block guardrails.

</details>


### [22] [Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math](https://arxiv.org/abs/2510.13744)
*Shrey Pandit,Austin Xu,Xuan-Phi Nguyen,Yifei Ming,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文引入Hard2Verify基准评估大语言模型推理系统的步骤级验证器，评估多种模型并分析验证性能不佳原因等。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型推理器需要强大的步骤级验证器，当前缺乏合适的评估基准。

Method: 引入人类标注的Hard2Verify基准，对29个生成批评器和过程奖励模型进行评估。

Result: 除少数模型外，开源验证器落后于闭源模型。

Conclusion: 评估基准可用于评估前沿步骤级验证器，分析了验证性能不佳的驱动因素等基础问题。

Abstract: Large language model (LLM)-based reasoning systems have recently achieved
gold medal-level performance in the IMO 2025 competition, writing mathematical
proofs where, to receive full credit, each step must be not only correct but
also sufficiently supported. To train LLM-based reasoners in such challenging,
open-ended settings, strong verifiers capable of catching step-level mistakes
are necessary prerequisites. We introduce Hard2Verify, a human-annotated,
step-level verification benchmark produced with over 500 hours of human labor.
Hard2Verify is designed to rigorously assess step-level verifiers at the
frontier: Verifiers must provide step-level annotations or identify the first
error in responses generated by frontier LLMs for very recent, challenging, and
open-ended math questions. We evaluate 29 generative critics and process reward
models, demonstrating that, beyond a few standouts, open-source verifiers lag
closed source models. We subsequently analyze what drives poor performance in
step-level verification, the impacts of scaling verifier compute, as well as
fundamental questions such as self-verification and verification-generation
dynamics.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [23] [Program of Thoughts for Financial Reasoning: Leveraging Dynamic In-Context Examples and Generative Retrieval](https://arxiv.org/abs/2510.13157)
*Subhendu Khatuya,Shashwat Naidu,Pawan Goyal,Niloy Ganguly*

Main category: cs.CE

TL;DR: 提出FINDER框架提升大语言模型金融数值推理能力，在FinQA和ConvFinQA数据集上取得新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数值推理方面仍面临挑战，在金融数值推理数据集上表现落后于现有最优模型。

Method: 提出两阶段框架FINDER，第一步用生成式检索器从非结构化数据中提取相关事实，第二步进行上下文感知的思维程序提示并动态选择上下文示例。

Result: FINDER在FinQA和ConvFinQA数据集上分别将执行准确率提高5.98%和4.05%，超越之前的基准。

Conclusion: FINDER框架有效提升了大语言模型在金融数值推理方面的能力。

Abstract: Despite continuous advancements in the capabilities of large language models
(LLMs), numerical reasoning remains a challenging area. Techniques like
chain-of-thought prompting, tree-of-thought prompting, and program-of-thought
prompting guide LLMs through intermediate reasoning steps. Although in-context
learning with few-shot prompting has improved performance, LLMs still lag
behind state-of-the-art models on financial numerical reasoning datasets such
as FinQA and ConvFinQA. In this work, we introduce FINDER, a novel two-step
framework, to enhance LLMs' capabilities in financial numerical reasoning. The
first step utilizes a generative retriever to extract relevant facts from
unstructured data, including both text and tables. This is followed by
context-aware Program of Thought prompting with dynamic selection of in-context
examples. Our model FINDER achieves a new state-of-the-art performance on both
the FinQA and ConvFinQA datasets, surpassing previous benchmarks with execution
accuracy improvements of 5.98% and 4.05%, respectively.

</details>


### [24] [Unsupervised Constitutive Model Discovery from Sparse and Noisy Data](https://arxiv.org/abs/2510.13559)
*Vahab Knauf Narouie,Jorge-Humberto Urrea-Quintero,Fehmi Cirak,Henning Wessels*

Main category: cs.CE

TL;DR: 将statFEM与EUCLID框架结合，提出statFEM - EUCLID用于各向同性超弹性材料，减少对噪声和数据稀疏的敏感性。


<details>
  <summary>Details</summary>
Motivation: VFM相关方法受测量噪声和数据稀疏影响，statFEM虽能处理不确定性但不保证本构关系一致性和可解释性，需结合二者优势。

Method: 将statFEM与无监督本构模型发现的EUCLID框架相结合。

Result: 该集成方法降低了对噪声和数据稀疏的敏感性，确保重建场与平衡和本构定律一致。

Conclusion: statFEM - EUCLID框架有效，可用于各向同性超弹性材料。

Abstract: Recently, unsupervised constitutive model discovery has gained attention
through frameworks based on the Virtual Fields Method (VFM), most prominently
the EUCLID approach. However, the performance of VFM-based approaches,
including EUCLID, is affected by measurement noise and data sparsity, which are
unavoidable in practice. The statistical finite element method (statFEM) offers
a complementary perspective by providing a Bayesian framework for assimilating
noisy and sparse measurements to reconstruct the full-field displacement
response, together with quantified uncertainty. While statFEM recovers
displacement fields under uncertainty, it does not strictly enforce consistency
with constitutive relations or aim to yield interpretable constitutive models.
In this work, we couple statFEM with unsupervised constitutive model discovery
in the EUCLID framework, yielding statFEM--EUCLID. The framework is
demonstrated for isotropic hyperelastic materials. The results show that this
integration reduces sensitivity to noise and data sparsity, while ensuring that
the reconstructed fields remain consistent with both equilibrium and
constitutive laws.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [25] [Experiments \& Analysis of Privacy-Preserving SQL Query Sanitization Systems](https://arxiv.org/abs/2510.13528)
*Loïs Ecoffet,Veronika Rehn-Sonigo,Jean-François Couchot,Catuscia Palamidessi*

Main category: cs.DB

TL;DR: 本文对SQL查询净化系统进行系统分类和定量分析，评估隐私保护数据库技术的能力与局限。


<details>
  <summary>Details</summary>
Motivation: 分析SQL查询有隐私风险，现有查询净化系统方法多样，给研究和实践带来复杂性，需进行分类和评估。

Method: 基于定性标准和支持的查询范围对现有SQL净化系统进行系统分类，并对领先系统进行定量分析。

Result: 对不同系统在数据效用、查询执行开销和隐私保证方面的权衡进行了实证测量。

Conclusion: 为当前隐私保护数据库技术的能力和局限性提供了结构化概述和性能评估。

Abstract: Analytical SQL queries are essential for extracting insights from relational
databases but concurrently introduce significant privacy risks by potentially
exposing sensitive information. To mitigate these risks, numerous query
sanitization systems have been developed, employing diverse approaches that
create a complex landscape for both researchers and practitioners. These
systems vary fundamentally in their design, including the underlying privacy
model, such as k-anonymity or Differential Privacy; the protected privacy unit,
whether at the tuple- or user-level; and the software architecture, which can
be proxy-based or integrated. This paper provides a systematic classification
of state-of-the-art SQL sanitization systems based on these qualitative
criteria and the scope of queries they support. Furthermore, we present a
quantitative analysis of leading systems, empirically measuring the trade-offs
between data utility, query execution overhead, and privacy guarantees across a
range of analytical queries. This work offers a structured overview and
performance assessment intended to clarify the capabilities and limitations of
current privacy-preserving database technologies.

</details>


### [26] [The Past Still Matters: A Temporally-Valid Data Discovery System](https://arxiv.org/abs/2510.13662)
*Mahdi Esmailoghli,Matthias Weidlich*

Main category: cs.DB

TL;DR: 现有数据发现方法忽视数据相关性的时间变化，本文提出含时间维度的数据发现系统愿景，介绍技术和架构，总结挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 过去十年数据发现研究有进展，但现有方法大多忽略数据相关性的时间变化，尤其是在缺少日期/时间元数据时。

Method: 定义时间有效数据发现问题，提出需要版本发现、时间沿袭推断、变更日志合成和时间感知数据发现等技术，并给出系统架构。

Result: 提出含时间维度的数据发现系统的愿景和架构。

Conclusion: 为新型数据发现系统奠定基础，将改变与不断演变的数据湖的交互方式。

Abstract: Over the past decade, the proliferation of public and enterprise data lakes
has fueled intensive research into data discovery, aiming to identify the most
relevant data from vast and complex corpora to support diverse user tasks.
Significant progress has been made through the development of innovative index
structures, similarity measures, and querying infrastructures. Despite these
advances, a critical aspect remains overlooked: relevance is time-varying.
Existing discovery methods largely ignore this temporal dimension, especially
when explicit date/time metadata is missing. To fill this gap, we outline a
vision for a data discovery system that incorporates the temporal dimension of
data. Specifically, we define the problem of temporally-valid data discovery
and argue that addressing it requires techniques for version discovery,
temporal lineage inference, change log synthesis, and time-aware data
discovery. We then present a system architecture to deliver these techniques,
before we summarize research challenges and opportunities. As such, we lay the
foundation for a new class of data discovery systems, transforming how we
interact with evolving data lakes.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [27] [Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters](https://arxiv.org/abs/2510.12889)
*Wei Da,Evangelia Kalyvianaki*

Main category: cs.DC

TL;DR: 本文介绍了用于现代数据中心任务调度的高效随机分散式调度器Dodoor，它基于缓存信息决策，使用新负载评分，评估显示其能减少消息量、提高吞吐量等。


<details>
  <summary>Details</summary>
Motivation: 设计高效的任务调度器，减少现代数据中心任务调度的通信开销，处理动态多维资源需求的任务调度。

Method: 利用加权球入箱模型，基于缓存的服务器信息做调度决策，使用新负载评分衡量服务器负载。

Result: 在101节点异构集群上用两种工作负载评估，Dodoor减少调度消息55 - 66%，提高吞吐量最高达33.2%和21.5%，减少平均完成时间延迟和改善尾部延迟。

Conclusion: Dodoor是一种有效的数据中心任务调度器，能减少通信开销并提升性能。

Abstract: This paper introduces Dodoor, an efficient randomized decentralized scheduler
designed for task scheduling in modern data centers. Dodoor leverages advanced
research on the weighted balls-into-bins model with b-batched setting. Unlike
other decentralized schedulers that rely on real-time probing of remote
servers, Dodoor makes scheduling decisions based on cached server information,
which is updated in batches, to reduce communication overheads. To schedule
tasks with dynamic, multidimensional resource requirements in heterogeneous
cluster, Dodoor uses a novel load score to measure servers' loads for each
scheduled task. This score captures the anti-affinity between servers and tasks
in contrast to the commonly used heuristic of counting pending tasks to balance
load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two
workloads: (i) simulated Azure virtual machines placements and (ii) real
serverless Python functions executions in Docker. The evaluation shows that
Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can
also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency
by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two
workloads.

</details>


### [28] [Scrutiny new framework in integrated distributed reliable systems](https://arxiv.org/abs/2510.13203)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: 提出新的集成分布式系统框架FDIRS，通过三部分提升满意度和性能，与现有框架对比，该框架提升了集成系统的效率、性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 提升集成分布式系统的满意度和性能，解决现有框架的问题。

Method: 分析集成系统及其演化过程、ERPSD和ERPDRT框架，提出FDIRS框架并进行模拟，与现有框架对比。

Result: FDIRS框架采用异构分布式数据库技术提升性能和响应速度。

Conclusion: FDIRS框架能提高集成系统的效率、性能和可靠性，解决部分现有框架问题。

Abstract: In this paper we represent a new framework for integrated distributed
systems. In the proposed framework we have used three parts to increase
Satisfaction and Performance of this framework. At first we analyse integrated
systems and their evolution process and also ERPSD and ERPDRT framework briefly
then we explain the new FDIRS framework. Finally we compare the results of
simulation of the new framework with presented frameworks. Result showed In
FIDRS framework, the technique of heterogeneous distributed data base is used
to improve Performance and speed in responding to users. Finally by using FDIRS
framework we succeeded to increase Efficiency, Performance and reliability of
integrated systems and remove some of previous frameworks problems.

</details>


### [29] [BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure](https://arxiv.org/abs/2510.13223)
*Yiyuan He,Minxian Xu,Jingfeng Wu,Jianmin Hu,Chong Ma,Min Shen,Le Chen,Chengzhong Xu,Lin Qu,Kejiang Ye*

Main category: cs.DC

TL;DR: 现有大语言模型服务系统有局限性，本文提出BanaServe框架解决问题，性能优于vLLM和DistServe。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署需高吞吐量、资源高效的服务系统，现有分解式服务系统有静态资源分配、负载不平衡、缓存路由负载分布不均等问题。

Method: 提出BanaServe动态编排框架，引入层级别权重迁移、注意力级别KV缓存迁移和全局KV缓存存储共享，实现粗粒度和细粒度负载重分配。

Result: 与vLLM相比，BanaServe吞吐量高1.2 - 3.9倍，总处理时间低3.9% - 78.4%；比DistServe吞吐量高1.1 - 2.8倍，延迟降低1.4% - 70.1%。

Conclusion: BanaServe能有效解决现有系统问题，提升大语言模型服务系统的性能。

Abstract: Large language models (LLMs) are increasingly deployed in AI infrastructure,
driving the need for high throughput, resource efficient serving systems.
Disaggregated LLM serving, which separates prompt prefill from auto-regressive
decode, has emerged as a promising architecture by isolating their
heterogeneous compute and memory demands. However, current disaggregated
systems face three key limitations: (i) static resource allocation cannot adapt
to highly dynamic workloads, causing over-provisioning that wastes resources or
under-provisioning that violates service level objectives (SLOs); (ii) inherent
load imbalance between prefill and decode stages, where prefill is
compute-bound and decode is memory-bound, causes under-utilization in one tier
while the other becomes a bottleneck; and (iii) prefix cache aware routing
skews load distribution, as high cache hit rate prefill nodes attract
disproportionately more requests, further degrading balance and efficiency. To
address these issues, we present BanaServe, a dynamic orchestration framework
that continuously rebalances computational and memory resources across prefill
and decode instances while eliminating hotspots induced by cache. BanaServe
introduces layer level weight migration, attention level Key Value Cache (KV
Cache) migration, and Global KV Cache Store sharing with layer wise overlapped
transmission, enabling both coarse grained (layer level) and fine grained
(attention level) load redistribution with minimal latency overhead. These
mechanisms allow routers to perform purely load aware scheduling, unconstrained
by cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher
throughput with 3.9%-78.4% lower total processing time, and outperforms
DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.

</details>


### [30] [Distributed Reductions for the Maximum Weight Independent Set Problem](https://arxiv.org/abs/2510.13306)
*Jannick Borowitz,Ernestine Großmann,Mattthias Schimek*

Main category: cs.DC

TL;DR: 本文提出最大权重独立集问题的分布式内存并行约简算法及启发式算法，实验显示算法有良好扩展性和加速比，可处理大规模图。


<details>
  <summary>Details</summary>
Motivation: 现有算法难以处理大规模图，需开发分布式并行算法解决最大权重独立集问题。

Method: 提出分布式内存并行约简算法，以及分布式约简 - 贪心和约简 - 剥离启发式算法。

Result: 分布式约简算法扩展性好，异步约简 - 剥离方法平均加速比 33 倍，约简 - 贪心算法最高达 50 倍，能处理超十亿顶点和 170 亿边的图。

Conclusion: 所提分布式算法在处理大规模图的最大权重独立集问题上有效且有良好性能。

Abstract: Finding maximum-weight independent sets in graphs is an important NP-hard
optimization problem. Given a vertex-weighted graph $G$, the task is to find a
subset of pairwise non-adjacent vertices of $G$ with maximum weight. Most
recently published practical exact algorithms and heuristics for this problem
use a variety of data-reduction rules to compute (near-)optimal solutions.
Applying these rules results in an equivalent instance of reduced size. An
optimal solution to the reduced instance can be easily used to construct an
optimal solution for the original input.
  In this work, we present the first distributed-memory parallel reduction
algorithms for this problem, targeting graphs beyond the scale of previous
sequential approaches. Furthermore, we propose the first distributed
reduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight
independent set heuristically.
  In our practical evaluation, our experiments on up to $1024$ processors
demonstrate good scalability of our distributed reduce algorithms while
maintaining good reduction impact. Our asynchronous reduce-and-peel approach
achieves an average speedup of $33\times$ over a sequential state-of-the-art
reduce-and-peel approach on 36 real-world graphs with a solution quality close
to the sequential algorithm. Our reduce-and-greedy algorithms even achieve
average speedups of up to $50\times$ at the cost of a lower solution quality.
Moreover, our distributed approach allows us to consider graphs with more than
one billion vertices and 17 billion edges.

</details>


### [31] [Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices](https://arxiv.org/abs/2510.13447)
*Julian Legler,Sebastian Werner,Maria C. Borges,Stefan Tai*

Main category: cs.DC

TL;DR: 微服务架构增加云资源需求和能耗，现有研究忽略跨容器服务交互能耗，本文引入服务级能耗模型并验证，强调全面能耗评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略跨容器服务交互（尤其是网络和存储）的能耗影响，需填补该空白。

Method: 引入服务级能耗模型，开发实验工具考虑CPU、内存、网络和存储的能耗，对开源云原生微服务应用的辅助服务进行多样配置实验。

Result: 忽略网络和存储会导致辅助服务能耗低估达63%。

Conclusion: 在设计节能微服务架构时需要更全面的能耗评估。

Abstract: Microservice architectures have become the dominant paradigm for cloud-native
systems, offering flexibility and scalability. However, this shift has also led
to increased demand for cloud resources, contributing to higher energy
consumption and carbon emissions. While existing research has focused on
measuring fine-grained energy usage of CPU and memory at the container level,
or on system-wide assessments, these approaches often overlook the energy
impact of cross-container service interactions, especially those involving
network and storage for auxiliary services such as observability and system
monitoring. To address this gap, we introduce a service-level energy model that
captures the distributed nature of microservice execution across containers.
Our model is supported by an experimentation tool that accounts for energy
consumption not just in CPU and memory, but also in network and storage
components. We validate our approach through extensive experimentation with
diverse experiment configurations of auxiliary services for a popular
open-source cloud-native microservice application. Results show that omitting
network and storage can lead to an underestimation of auxiliary service energy
use by up to 63%, highlighting the need for more comprehensive energy
assessments in the design of energy-efficient microservice architectures.

</details>


### [32] [Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference](https://arxiv.org/abs/2510.13668)
*Zhibin Wang,Zetao Hong,Xue Li,Zibo Wang,Shipeng Li,Qingkai Meng,Qing Wang,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: 本文针对大语言模型推理解码阶段的工作负载不平衡问题，提出了基于长度预测的自适应解码重调度系统ARES，有预测方法和重调度方案两大贡献。


<details>
  <summary>Details</summary>
Motivation: 现实场景中输出长度变化导致解码阶段工作负载严重不平衡，现有系统的静态调度方法在不断变化的解码工作负载下会导致SLO违规和OOM故障。

Method: 提出ARES系统，包含利用LLM隐藏状态的轻量级连续预测方法和集成当前与预测工作负载的动态平衡重调度机制。

Result: 预测方法降低MAE 49.42%，削减预测器参数93.28%；重调度方案降低P99 TPOT 74.77%，实现高达2.24倍的吞吐量。

Conclusion: 基于长度预测的自适应解码重调度系统ARES能有效解决大语言模型推理解码阶段的工作负载不平衡问题。

Abstract: Large Language Model (LLM) inference has emerged as a fundamental paradigm.
In real-world scenarios, variations in output length cause severe workload
imbalance in the decode phase, particularly for long-output reasoning tasks.
Existing systems, such as PD disaggregation architectures, rely on static
prefill-to-decode scheduling, which often results in SLO violations and OOM
failures under evolving decode workloads.
  In this paper, we propose ARES, an adaptive decoding rescheduling system
powered by length prediction to anticipate future workloads. Our core
contributions include: (1) A lightweight and continuous LLM-native prediction
method that leverages LLM hidden state to model remaining generation length
with high precision (reducing MAE by 49.42%) and low overhead (cutting
predictor parameters by 93.28%); (2) A rescheduling solution in decode phase
with : A dynamic balancing mechanism that integrates current and predicted
workloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher
goodput.

</details>


### [33] [FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access](https://arxiv.org/abs/2510.13724)
*Aditya Tanikanti,Benoit Côté,Yanfei Guo,Le Chen,Nickolaus Saint,Ryan Chard,Ken Raffenetti,Rajeev Thakur,Thomas Uram,Ian Foster,Michael E. Papka,Venkatram Vishwanath*

Main category: cs.DC

TL;DR: 介绍了FIRST框架，可在分布式HPC集群实现推理即服务，支持多后端、资源自动扩展等，满足科学工作流对AI推理的需求。


<details>
  <summary>Details</summary>
Motivation: 满足科学工作流中对私有、安全和可扩展的AI推理的不断增长的需求，让研究人员无需依赖商业云基础设施。

Method: 利用Globus Auth和Globus Compute，通过OpenAI兼容的API运行并行推理工作负载，将请求分布到联合集群。

Result: 框架支持多推理后端、资源自动扩展、低延迟执行等，研究人员可每天在本地生成数十亿个令牌。

Conclusion: FIRST框架能有效在现有HPC基础设施上实现推理即服务，满足科学研究对AI推理的需求。

Abstract: We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
"hot" nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.

</details>


### [34] [Tight Conditions for Binary-Output Tasks under Crashes](https://arxiv.org/abs/2510.13755)
*Timothé Albouy,Antonio Fernández Anta,Chryssis Georgiou,Nicolas Nicolaou,Junlang Wang*

Main category: cs.DC

TL;DR: 本文探索解决二进制输出分布式任务的系统充要条件，给出同步和异步系统中任务可解决的n和t的严格条件，其输出集方法有高度通用性。


<details>
  <summary>Details</summary>
Motivation: 探索解决二进制输出分布式任务所需的系统条件。

Method: 关注任务可能产生的不同输出值集合，不考虑有效性和值的多重性，研究分布式系统中进程数量n和崩溃进程数量t的关系。

Result: 给出同步和异步系统中，每个二进制输出任务类可解的n和t的严格条件，该输出集方法统一多个分布式计算问题并产生适用于更强任务公式的不可能证明。

Conclusion: 输出集方法具有高度通用性，可统一多个分布式计算问题并产生更广泛适用的不可能证明。

Abstract: This paper explores necessary and sufficient system conditions to solve
distributed tasks with binary outputs (\textit{i.e.}, tasks with output values
in $\{0,1\}$). We focus on the distinct output sets of values a task can
produce (intentionally disregarding validity and value multiplicity),
considering that some processes may output no value. In a distributed system
with $n$ processes, of which up to $t \leq n$ can crash, we provide a complete
characterization of the tight conditions on $n$ and $t$ under which every class
of tasks with binary outputs is solvable, for both synchronous and asynchronous
systems. This output-set approach yields highly general results: it unifies
multiple distributed computing problems, such as binary consensus and symmetry
breaking, and it produces impossibility proofs that hold for stronger task
formulations, including those that consider validity, account for value
multiplicity, or move beyond binary outputs.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [35] [A faster algorithm for efficient longest common substring calculation for non-parametric entropy estimation in sequential data](https://arxiv.org/abs/2510.13330)
*Bridget Smart,Max Ward,Matthew Roughan*

Main category: cs.DS

TL;DR: 提出新算法LCSFinder提升非参数熵估计效率，实现信号处理中大规模熵估计


<details>
  <summary>Details</summary>
Motivation: 基于最长公共子串（LCS）的方法估计典型集大小效率低，限制在实际数据中的应用

Method: 引入新算法LCSFinder，利用标准算法结构如排序后缀数组和持久二叉搜索树

Result: LCSFinder在真实和模拟数据上比现有实现显著提速

Conclusion: LCSFinder使实际信号处理中以前不可行的大规模熵估计成为可能

Abstract: Non-parametric entropy estimation on sequential data is a fundamental tool in
signal processing, capturing information flow within or between processes to
measure predictability, redundancy, or similarity. Methods based on longest
common substrings (LCS) provide a non-parametric estimate of typical set size
but are often inefficient, limiting use on real-world data. We introduce
LCSFinder, a new algorithm that improves the worst-case performance of LCS
calculations from cubic to log-linear time. Although built on standard
algorithmic constructs - including sorted suffix arrays and persistent binary
search trees - the details require care to provide the matches required for
entropy estimation on dynamically growing sequences. We demonstrate that
LCSFinder achieves dramatic speedups over existing implementations on real and
simulated data, enabling entropy estimation at scales previously infeasible in
practical signal processing.

</details>


### [36] [Tight Parameterized (In)tractability of Layered Crossing Minimization: Subexponential Algorithms and Kernelization](https://arxiv.org/abs/2510.13335)
*Fedor V. Fomin,Petr A. Golovach,Tanmay Inamdar,Saket Saurabh,Meirav Zehavi*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The starting point of our work is a decade-old open question concerning the
subexponential parameterized complexity of \textsc{2-Layer Crossing
Minimization}. In this problem, the input is an $n$-vertex graph $G$ whose
vertices are partitioned into two independent sets $V_1$ and $V_2$, and a
non-negative integer $k$. The question is whether $G$ admits a 2-layered
drawing with at most $k$ crossings, where each $V_i$ lies on a distinct line
parallel to the $x$-axis, and all edges are straight lines. We resolve this
open question by giving the first subexponential fixed-parameter algorithm for
this problem, running in time $2^{O(\sqrt{k}\log k)} + n \cdot k^{O(1)}$.
  We then ask whether the subexponential phenomenon extends beyond two layers.
In the general $h$-Layer Crossing Minimization problem, the vertex set is
partitioned into $h$ independent sets $V_1, \ldots, V_h$, and the goal is to
decide whether an $h$-layered drawing with at most $k$ crossings exists. We
present a subexponential FPT algorithm for three layers with running time
$2^{O(k^{2/3}\log k)} + n \cdot k^{O(1)}$ for $h = 3$ layers. In contrast, we
show that for all $h \ge 5$, no algorithm with running time $2^{o(k/\log k)}
\cdot n^{O(1)}$ exists unless the Exponential-Time Hypothesis fails.
  Finally, we address polynomial kernelization. While a polynomial kernel was
already known for $h=2$, we design a new polynomial kernel for $h=3$. These
kernels are essential ingredients in our subexponential algorithms. Finally, we
rule out polynomial kernels for all $h \ge 4$ unless the polynomial hierarchy
collapses.

</details>


### [37] [Chromatic correlation clustering via cluster LP](https://arxiv.org/abs/2510.13446)
*Fateme Abbasi,Hyung-Chan An,Jarosław Byrka,Changyeol Lee,Yongho Shin*

Main category: cs.DS

TL;DR: 利用色簇线性规划提出了一个(2 + ε) - 近似算法解决色相关聚类问题。


<details>
  <summary>Details</summary>
Motivation: 鉴于簇线性规划在相关聚类问题上的成功，探究其能否用于色相关聚类问题。

Method: 使用色簇线性规划提出(2 + ε) - 近似算法。

Result: 得到了色相关聚类的(2 + ε) - 近似算法。

Conclusion: 肯定了簇线性规划可用于色相关聚类问题。

Abstract: Correlation Clustering is a fundamental clustering problem, and there has
been a line of work on improving the approximation ratio for this problem in
recent years. A key algorithmic component in these works is the cluster LP.
Chromatic Correlation Clustering is an interesting generalization that has also
been intensively studied. In light of success of the cluster LP in Correlation
Clustering, it would be an interesting question whether the cluster LP can be
used in Chromatic Correlation Clustering. We answer this question with
affirmatives by presenting a $(2+\varepsilon)$-approximation algorithm for
Chromatic Correlation Clustering using a chromatic cluster LP.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [38] [Finding a Nash equilibrium of a random win-lose game in expected polynomial time](https://arxiv.org/abs/2510.12846)
*Andrea Collevecchio,Gabor Lugosi,Adrian Vetta,Rui-Ray Zhang*

Main category: cs.GT

TL;DR: 研究随机输赢游戏，证明对多数参数p，存在期望多项式时间算法求纳什均衡，并给出不同参数a、c取值下的情况。


<details>
  <summary>Details</summary>
Motivation: 解决算法博弈论中计算随机双矩阵博弈纳什均衡的多项式时间算法是否存在这一长期开放问题。

Method: 研究随机输赢游戏，其中收益矩阵元素为独立同分布的伯努利随机变量。

Result: 对几乎所有参数p值，存在期望多项式时间算法；不同a、c取值下有不同结论。

Conclusion: 为随机输赢游戏计算纳什均衡提供了期望多项式时间算法的多种情况。

Abstract: A long-standing open problem in algorithmic game theory asks whether or not
there is a polynomial time algorithm to compute a Nash equilibrium in a random
bimatrix game. We study random win-lose games, where the entries of the
$n\times n$ payoff matrices are independent and identically distributed
(i.i.d.) Bernoulli random variables with parameter $p=p(n)$. We prove that, for
nearly all values of the parameter $p=p(n)$, there is an expected
polynomial-time algorithm to find a Nash equilibrium in a random win-lose game.
More precisely, if $p\sim cn^{-a}$ for some parameters $a,c\ge 0$, then there
is an expected polynomial-time algorithm whenever $a\not\in \{1/2, 1\}$. In
addition, if $a = 1/2$ there is an efficient algorithm if either $c \le e^{-52}
2^{-8} $ or $c\ge 0.977$. If $a=1$, then there is an expected polynomial-time
algorithm if either $c\le 0.3849$ or $c\ge \log^9 n$.

</details>


### [39] [Equilibria in routing games with connected autonomous vehicles will not be strong, as exclusive clubs may form](https://arxiv.org/abs/2510.12862)
*Rafał Kucharski,Anastasia Psarou,Natello Descormier*

Main category: cs.GT

TL;DR: 本文指出联网自动驾驶汽车（CAV）可形成路由联盟，以精心设计的示例展示其偏离用户均衡带来的后果，暗示可能出现CAV精英阶层影响道路公平性。


<details>
  <summary>Details</summary>
Motivation: 以往人类司机因技术和行为限制无法形成联盟，而CAV可通信协作，需研究其对用户均衡的影响。

Method: 通过精心设计的玩具网络示例进行演示。

Result: CAV形成的联盟会对其他用户和系统产生负面影响，使系统次优且失衡。

Conclusion: 若不加以阻止，CAV运营商可能使交通系统偏离经典纳什均衡，导致出现CAV精英阶层，损害公共道路网络公平性。

Abstract: User Equilibrium is the standard representation of the so-called routing game
in which drivers adjust their route choices to arrive at their destinations as
fast as possible. Asking whether this Equilibrium is strong or not was
meaningless for human drivers who did not form coalitions due to technical and
behavioral constraints. This is no longer the case for connected autonomous
vehicles (CAVs), which will be able to communicate and collaborate to jointly
form routing coalitions.
  We demonstrate this for the first time on a carefully designed toy-network
example, where a `club` of three autonomous vehicles jointly decides to deviate
from the user equilibrium and benefit (arrive faster). The formation of such a
club has negative consequences for other users, who are not invited to join it
and now travel longer, and for the system, making it suboptimal and
disequilibrated, which triggers adaptation dynamics.
  This discovery has profound implications for the future of our cities. We
demonstrate that, if not prevented, CAV operators may intentionally
disequilibrate traffic systems from their classic Nash equilibria, benefiting
their own users and imposing costs on others. These findings suggest the
possible emergence of an exclusive CAV elite, from which human-driven vehicles
and non-coalition members may be excluded, potentially leading to
systematically longer travel times for those outside the coalition, which would
be harmful for the equity of public road networks.

</details>


### [40] [Efficiency of Constant Log Utility Market Makers](https://arxiv.org/abs/2510.12952)
*Maneesha Papireddygari,Xintong Wang,Bo Waggoner,David M. Pennock*

Main category: cs.GT

TL;DR: 本文聚焦Constant Log Utility Market Maker (CLUM)，证明其定价证券问题是#P - 难的，并提出高概率有效的近似算法，还表明在区间证券下相关预言机可多项式时间实现。


<details>
  <summary>Details</summary>
Motivation: 原有的对数市场评分规则（LMSR）做市商定价问题是#P - 难的，且LMSR最坏情况损失随结果数量增长，而CLUM有恒定最坏情况损失等优点，但需研究其定价复杂度并使其更实用。

Method: 通过从模型计数2 - SAT问题归约证明CLUM定价证券问题的#P - 难；提出依赖特定预言机的近似算法；证明在区间证券下预言机可多项式时间实现。

Result: 证明了CLUM定价证券问题是#P - 难的；提出的近似算法在高概率下有效；在区间证券下预言机可多项式时间实现。

Conclusion: 虽然CLUM定价证券问题是#P - 难的，但提出的近似算法和区间证券下的预言机实现方法，使CLUM在实际应用中更具可行性。

Abstract: Automated Market Makers (AMMs) are used to provide liquidity for
combinatorial prediction markets that would otherwise be too thinly traded.
They offer both buy and sell prices for any of the doubly exponential many
possible securities that the market can offer. The problem of setting those
prices is known to be #P-hard for the original and most well-known AMM, the
logarithmic market scoring rule (LMSR) market maker [Chen et al., 2008]. We
focus on another natural AMM, the Constant Log Utility Market Maker (CLUM).
Unlike LMSR, whose worst-case loss bound grows with the number of outcomes,
CLUM has constant worst-case loss, allowing the market to add outcomes on the
fly and even operate over countably infinite many outcomes, among other
features. Simpler versions of CLUM underpin several Decentralized Finance
(DeFi) mechanisms including the Uniswap protocol that handles billions of
dollars of cryptocurrency trades daily. We first establish the computational
complexity of the problem: we prove that pricing securities is #P-hard for
CLUM, via a reduction from the model counting 2-SAT problem. In order to make
CLUM more practically viable, we propose an approximation algorithm for pricing
securities that works with high probability. This algorithm assumes access to
an oracle capable of determining the maximum shares purchased of any one
outcome and the total number of outcomes that has that maximum amount
purchased. We then show that this oracle can be implemented in polynomial time
when restricted to interval securities, which are used in designing financial
options.

</details>


### [41] [Repeated Sales with Heterogeneous Buyer Sophistication](https://arxiv.org/abs/2510.13088)
*Rishi Patel,Emmanouil Pountourakis,Samuel Taggart*

Main category: cs.GT

TL;DR: 研究无承诺能力的卖家对单一长期买家进行基于行为的价格歧视，分析不同时间跨度下买家类型对卖家收益的影响。


<details>
  <summary>Details</summary>
Motivation: 探究买家动态对卖家了解买家并利用此增加收益能力的影响。

Method: 构建两期模型分析短期情况，构建带时间贴现的无限期模型分析长期情况。

Result: 短期中引入天真型买家会加剧需求减少，损害卖家收益；长期中极端需求减少现象消失，卖家能有效学习，收益接近有承诺能力时的水平。

Conclusion: 买家类型和互动时间跨度显著影响卖家基于行为的价格歧视策略和收益。

Abstract: This paper considers behavior-based price discrimination in the repeated sale
of a non-durable good to a single long-lived buyer, by a seller without
commitment power. We assume that there is a mixed population of forward-looking
``sophisticated'' buyers and myopic ``naive'' buyers. We investigate the impact
of these dynamics on the seller's ability to learn about the buyer and exploit
this learning for revenue. We obtain conclusions that differ dramatically with
the time horizon of the interactions. To understand short time horizons, we
analyze a two-period model, and find that the strategic demand reduction
observed with fully sophisticated buyers is robust to the introduction of naive
types. In fact, despite the inability of naive buyers to game the pricing
algorithm, their introduction can further harm the seller's revenue, due to
more intense demand reduction overall. For long horizons, we consider an
infinite-horizon model with time discounting. We find that the extreme demand
reduction predicted by previous work does not survive the introduction of naive
buyers. Instead, we observe equilibria where the seller learns meaningfully
despite the sophisticated buyers' demand reduction. We prove that for a natural
family of such equilibria, the seller's revenue is not just high, but
approximates the revenue attainable with commitment power, even when the
fraction of naive types is vanishingly small.

</details>


### [42] [A Ratio-Based Shapley Value for Collaborative Machine Learning - Extended Version](https://arxiv.org/abs/2510.13261)
*Björn Filter,Ralf Möller,Özgür Lütfü Özçep*

Main category: cs.GT

TL;DR: 本文引入基于比例的Shapley值，替代标准加法公式，提出奖励分配新方法，适用于按比例衡量贡献的场景。


<details>
  <summary>Details</summary>
Motivation: 现有协作机器学习中，保证激励兼容性和基于公平贡献的奖励分配是关键挑战，需新方法解决。

Method: 引入基于比例的Shapley值，替代标准加法公式衡量各方数据贡献，奖励框架与前人工作一致，但底层价值函数不同。

Result: 新的价值函数带来不同的模型奖励分配，满足与加法公式相同的激励条件，也面临相同权衡。

Conclusion: 提出的方法是加法Shapley框架的数学替代方案，更适合按比例衡量贡献的场景。

Abstract: Collaborative machine learning enables multiple data owners to jointly train
models for improved predictive performance. However, ensuring incentive
compatibility and fair contribution-based rewards remains a critical challenge.
Prior work by Sim and colleagues (Rachel Hwee Ling Sim et al: Collaborative
machine learning with incentive-aware model rewards. In: International
conference on machine learning. PMLR. 2020, pp. 8927-8963) addressed this by
allocating model rewards, which are non-monetary and freely replicable, based
on the Shapley value of each party's data contribution, measured via
information gain. In this paper, we introduce a ratio-based Shapley value that
replaces the standard additive formulation with a relative contribution
measure. While our overall reward framework, including the incentive
definitions and model-reward setting, remains aligned with that of Sim and
colleagues, the underlying value function is fundamentally different. Our
alternative valuation induces a different distribution of model rewards and
offers a new lens through which to analyze incentive properties. We formally
define the ratio-based value and prove that it satisfies the same set of
incentive conditions as the additive formulation, including adapted versions of
fairness, individual rationality, and stability. Like the original approach,
our method faces the same fundamental trade-offs between these incentives. Our
contribution is a mathematically grounded alternative to the additive Shapley
framework, potentially better suited to contexts where proportionality among
contributors is more meaningful than additive differences.

</details>


### [43] [Nash Flows Over Time with Tolls](https://arxiv.org/abs/2510.13518)
*Shaul Rosner,Marc Schröder,Laura Vargas Koch*

Main category: cs.GT

TL;DR: 研究交通流动态路由博弈，引入收费增强模型，分析非原子均衡，指出动态均衡不唯一及不一定达稳态，给出计算稳态的方法。


<details>
  <summary>Details</summary>
Motivation: 研究受交通流启发的动态路由博弈，通过引入收费增强基础模型。

Method: 在Vickrey瓶颈模型基础上引入收费，考虑非原子均衡。

Result: 动态均衡在成本上不唯一且不一定达到稳态，提供了计算带收费模型稳态的程序。

Conclusion: 成功对带收费的动态路由博弈模型进行分析并给出稳态计算方法。

Abstract: We study a dynamic routing game motivated by traffic flows. The base model
for an edge is the Vickrey bottleneck model. That is, edges are equipped with a
free flow transit time and a capacity. When the inflow into an edge exceeds its
capacity, a queue forms and the following particles experience a waiting time.
In this paper, we enhance the model by introducing tolls, i.e., a cost each
flow particle must pay for traversing an edge. In this setting we consider
non-atomic equilibria, which means flows over time in which every particle is
on a cheapest path, when summing up toll and travel time. We first show that
unlike in the non-tolled version of this model, dynamic equilibria are not
unique in terms of costs and do not necessarily reach a steady state. As a main
result, we provide a procedure to compute steady states in the model with
tolls.

</details>


### [44] [Online Fair Division With Subsidy: When Do Envy-Free Allocations Exist, and at What Cost?](https://arxiv.org/abs/2510.13633)
*Pooja Kulkarni,Ruta Mehta,Vishnu V. Narayan,Tomasz Ponitka*

Main category: cs.GT

TL;DR: 研究在线公平分配不可分割物品问题，提出维护在线无嫉妒性和确保低补贴两方面成果。


<details>
  <summary>Details</summary>
Motivation: 线下已有关于有界补贴的无嫉妒可分配研究，将其拓展到物品逐个到达需立即分配的在线场景。

Method: 设计在线算法来维护不同估值类别的无嫉妒可分配性，研究不同估值类别的最低补贴量。

Result: 对于次模或超模估值，在线无法始终保持无嫉妒可分配；为加性估值及其超类设计算法可维护无嫉妒可分配性。加性估值在线最低补贴可达Ω(mn)，而线下为O(n)，还找出最低补贴小的估值类并获得补贴界限。

Conclusion: 在在线公平分配不可分割物品问题上取得了关于维护无嫉妒性和控制补贴的成果。

Abstract: We study the problem of fairly allocating $m$ indivisible items arriving
online, among $n$ (offline) agents. Although envy-freeness has emerged as the
archetypal fairness notion, envy-free (EF) allocations need not exist with
indivisible items. To bypass this, a prominent line of research demonstrates
that there exist allocations that can be made envy-free by allowing a subsidy.
Extensive work in the offline setting has focused on finding such envy-freeable
allocations with bounded subsidy. We extend this literature to an online
setting where items arrive one at a time and must be immediately and
irrevocably allocated. Our contributions are two-fold:
  1. Maintaining EF Online: We show that envy-freeability cannot always be
preserved online when the valuations are submodular or supermodular, even with
binary marginals. In contrast, we design online algorithms that maintain
envy-freeability at every step for the class of additive valuations, and for
its superclasses including $k$-demand and SPLC valuations.
  2. Ensuring Low Subsidy: We investigate the quantity of subsidy required to
guarantee envy-freeness online. Surprisingly, even for additive valuations, the
minimum subsidy may be as large as $\Omega(mn)$, in contrast to the offline
setting, where the bound is $O(n)$. On the positive side, we identify valuation
classes where the minimum subsidy is small (i.e., does not depend on $m$),
including $k$-valued, rank-one, restricted additive, and identical valuations,
and we obtain (mostly) tight subsidy bounds for these classes.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [45] [Energy-Guided Diffusion Sampling for Long-Term User Behavior Prediction in Reinforcement Learning-based Recommendation](https://arxiv.org/abs/2510.12815)
*Xiaocong Chen,Siyu Wang,Lina Yao*

Main category: cs.IR

TL;DR: 提出DAC4Rec框架解决离线RL4RS系统局限，经实验验证有效且具通用性


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习推荐系统存在数据低效、依赖预收集轨迹、难处理噪声数据和捕捉长期用户偏好等问题

Method: 提出DAC4Rec框架，结合扩散过程与强化学习，利用扩散模型去噪能力，采用Q值引导策略优化和基于能量的采样策略

Result: 在六个真实世界离线数据集和在线模拟环境实验中验证了能优化长期用户偏好，且扩散策略可集成到其他常用RL算法

Conclusion: DAC4Rec框架有效解决现有问题，具备通用性和广泛适用性

Abstract: Reinforcement learning-based recommender systems (RL4RS) have gained
attention for their ability to adapt to dynamic user preferences. However,
these systems face challenges, particularly in offline settings, where data
inefficiency and reliance on pre-collected trajectories limit their broader
applicability. While offline reinforcement learning methods leverage extensive
datasets to address these issues, they often struggle with noisy data and fail
to capture long-term user preferences, resulting in suboptimal recommendation
policies. To overcome these limitations, we propose Diffusion-enhanced
Actor-Critic for Offline RL4RS (DAC4Rec), a novel framework that integrates
diffusion processes with reinforcement learning to model complex user
preferences more effectively. DAC4Rec leverages the denoising capabilities of
diffusion models to enhance the robustness of offline RL algorithms and
incorporates a Q-value-guided policy optimization strategy to better handle
suboptimal trajectories. Additionally, we introduce an energy-based sampling
strategy to reduce randomness during recommendation generation, ensuring more
targeted and reliable outcomes. We validate the effectiveness of DAC4Rec
through extensive experiments on six real-world offline datasets and in an
online simulation environment, demonstrating its ability to optimize long-term
user preferences. Furthermore, we show that the proposed diffusion policy can
be seamlessly integrated into other commonly used RL algorithms in RL4RS,
highlighting its versatility and wide applicability.

</details>


### [46] [Maximum In-Support Return Modeling for Dynamic Recommendation with Language Model Prior](https://arxiv.org/abs/2510.12816)
*Xiaocong Chen,Siyu Wang,Lina Yao*

Main category: cs.IR

TL;DR: 提出基于决策转换器的离线强化学习推荐系统框架MDT4Rec，解决从次优历史学习和表示复杂用户 - 物品交互问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习推荐系统在现实中因用户反馈数据次优或稀疏面临困难，需解决从次优历史学习和表示复杂用户 - 物品交互的问题。

Method: 将轨迹拼接过程从训练阶段转移到动作推理阶段；用预训练大语言模型初始化决策转换器，用多层感知器替换线性嵌入层，采用低秩自适应微调少量参数。

Result: 在五个公共数据集和在线模拟环境中评估，MDT4Rec性能优于现有方法。

Conclusion: MDT4Rec能有效解决强化学习推荐系统面临的挑战，是一种更优的推荐系统框架。

Abstract: Reinforcement Learning-based recommender systems (RLRS) offer an effective
way to handle sequential recommendation tasks but often face difficulties in
real-world settings, where user feedback data can be sub-optimal or sparse. In
this paper, we introduce MDT4Rec, an offline RLRS framework that builds on the
Decision Transformer (DT) to address two major challenges: learning from
sub-optimal histories and representing complex user-item interactions. First,
MDT4Rec shifts the trajectory stitching procedure from the training phase to
action inference, allowing the system to shorten its historical context when
necessary and thereby ignore negative or unsuccessful past experiences. Second,
MDT4Rec initializes DT with a pre-trained large language model (LLM) for
knowledge transfer, replaces linear embedding layers with Multi-Layer
Perceptrons (MLPs) for more flexible representations, and employs Low-Rank
Adaptation (LoRA) to efficiently fine-tune only a small subset of parameters.
We evaluate MDT4Rec on five public datasets and in an online simulation
environment, demonstrating that it outperforms existing methods.

</details>


### [47] [Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering](https://arxiv.org/abs/2510.12959)
*Md Aminul Islam,Elena Zheleva,Ren Wang*

Main category: cs.IR

TL;DR: 本文提出后验流行度去偏（PPD）方法，直接在预训练嵌入上操作，无需重新训练，能减少基于GNN的CF中的流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 协同过滤训练数据长尾分布，GNN会传播和放大流行度偏差，现有方法无法直接抵消GNN邻域聚合中传播的偏差。

Method: 提出PPD方法，通过估计交互级流行度，利用流行度方向向量从节点表示中去除流行度成分。

Result: 实验结果表明该方法在基于GNN的CF流行度偏差校正上优于现有方法。

Conclusion: PPD方法能有效减少基于GNN的CF中的流行度偏差，同时保留用户偏好。

Abstract: User historical interaction data is the primary signal for learning user
preferences in collaborative filtering (CF). However, the training data often
exhibits a long-tailed distribution, where only a few items have the majority
of interactions. CF models trained directly on such imbalanced data are prone
to learning popularity bias, which reduces personalization and leads to
suboptimal recommendation quality. Graph Neural Networks (GNNs), while
effective for CF due to their message passing mechanism, can further propagate
and amplify popularity bias through their aggregation process. Existing
approaches typically address popularity bias by modifying training objectives
but fail to directly counteract the bias propagated during GNN's neighborhood
aggregation. Applying weights to interactions during aggregation can help
alleviate this problem, yet it risks distorting model learning due to unstable
node representations in the early stages of training. In this paper, we propose
a Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias
in GNN-based CF and operates directly on pre-trained embeddings without
requiring retraining. By estimating interaction-level popularity and removing
popularity components from node representations via a popularity direction
vector, PPD reduces bias while preserving user preferences. Experimental
results show that our method outperforms state-of-the-art approaches for
popularity bias correction in GNN-based CF.

</details>


### [48] [Retrieval-in-the-Chain: Bootstrapping Large Language Models for Generative Retrieval](https://arxiv.org/abs/2510.13095)
*Yingchen zhang,Ruqing zhang,Jiafeng Guo,Wenjun Peng,Sen Li,Fuyu Lv*

Main category: cs.IR

TL;DR: 本文探讨显式推理对生成式检索（GR）的作用，提出推理增强框架R4R并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 先前工作忽视大语言模型推理能力对GR的帮助，现有初步方法存在推理冗长、与文档ID空间对齐不佳的问题，需开发更适合GR的推理机制。

Method: 提出R4R框架，将自由形式的思维链推理转换为紧凑结构化格式，在检索过程中迭代优化推理，利用为GR进行指令微调的LLM，推理生成和检索由单个LLM完成。

Result: 在Natural Questions、MS MARCO和真实世界物品搜索基准上的大量实验验证了R4R的有效性。

Conclusion: 显式推理能为GR带来好处，R4R框架是有效的推理增强框架。

Abstract: Generative retrieval (GR) is an emerging paradigm that leverages large
language models (LLMs) to autoregressively generate document identifiers
(docids) relevant to a given query. Prior works have focused on leveraging the
generative capabilities of LLMs to improve GR, while overlooking that their
reasoning capabilities could likewise help. This raises a key question: Can
explicit reasoning benefit GR? To investigate, we first conduct a preliminary
study where an LLM is prompted to generate free-form chain-of-thought (CoT)
reasoning before performing constrained docid decoding. Although this method
outperforms standard GR, the generated reasoning tends to be verbose and poorly
aligned with the docid space. These limitations motivate the development of a
reasoning mechanism better tailored to GR.
  Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented
framework for GR that converts free-form CoT reasoning into a compact,
structured format, and iteratively refines the reasoning during the retrieval
process. R4R augments an existing GR method by leveraging a reasoning-capable
LLM that has been instruction-tuned for GR. At inference time, R4R first uses
the LLM to generate an initial structured reasoning; then the same LLM
alternates between (i) constrained decoding with the chosen GR method to
produce candidate docids and (ii) updating the reasoning based on retrieval
results to improve the next round. R4R does not require additional models or
training, and instead a single LLM serves as both the reasoning generator and
the retriever. Extensive experiments on Natural Questions, MS MARCO, and a
real-world item-search benchmark validate the effectiveness of R4R.

</details>


### [49] [ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG](https://arxiv.org/abs/2510.13193)
*Yikuan Hu,Jifeng Zhu,Lanrui Tang,Chen Huang*

Main category: cs.IR

TL;DR: 本文提出REMINDRAG方法以改进KG - RAG系统，兼顾系统有效性和成本效率，经理论和实验验证其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有KG - RAG系统难以在系统有效性和成本效率间取得有效协同，性能不佳且成本高。

Method: 提出REMINDRAG，采用大语言模型引导的图遍历，包括节点探索、利用和记忆回放，在知识图谱边嵌入中记忆遍历经验。

Result: 理论和实验证实REMINDRAG的有效性，在多个基准数据集和大语言模型骨干上优于现有基线。

Conclusion: REMINDRAG能有效提升KG - RAG系统的有效性和成本效率。

Abstract: Knowledge graphs (KGs), with their structured representation capabilities,
offer promising avenue for enhancing Retrieval Augmented Generation (RAG)
systems, leading to the development of KG-RAG systems. Nevertheless, existing
methods often struggle to achieve effective synergy between system
effectiveness and cost efficiency, leading to neither unsatisfying performance
nor excessive LLM prompt tokens and inference time. To this end, this paper
proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node
exploration, node exploitation, and, most notably, memory replay, to improve
both system effectiveness and cost efficiency. Specifically, REMINDRAG
memorizes traversal experience within KG edge embeddings, mirroring the way
LLMs "memorize" world knowledge within their parameters, but in a train-free
manner. We theoretically and experimentally confirm the effectiveness of
REMINDRAG, demonstrating its superiority over existing baselines across various
benchmark datasets and LLM backbones. Our code is available at
https://github.com/kilgrims/ReMindRAG.

</details>


### [50] [LLM-guided Hierarchical Retrieval](https://arxiv.org/abs/2510.13217)
*Nilesh Gupta,Wei-Cheng Chang,Ngot Bui,Cho-Jui Hsieh,Inderjit S. Dhillon*

Main category: cs.IR

TL;DR: 提出分层检索框架LATTICE解决现代IR系统应对复杂查询的挑战，在BRIGHT基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现代IR系统处理复杂查询时，现有基于LLM的方法存在局限性，如检索重排范式有缺陷、参数生成方法难更新、长上下文方法计算不可行。

Method: 提出LATTICE框架，包括离线阶段用自底向上或自顶向下策略将语料组织成语义层次，在线遍历阶段让搜索LLM遍历树；提出遍历算法估计校准潜在相关性分数并聚合为全局路径相关性指标。

Result: 训练无关的框架在BRIGHT基准测试中实现了零样本性能的最优，Recall@100提升达9%，nDCG@10提升达5%；与微调的SOTA方法DIVER - v2相比，在使用静态语料评估的BRIGHT子集上取得了相当的结果。

Conclusion: LATTICE框架能有效解决现代IR系统处理复杂查询的问题，具有良好性能。

Abstract: Modern IR systems are increasingly tasked with answering complex,
multi-faceted queries that require deep reasoning rather than simple keyword or
semantic matching. While LLM-based IR has shown great promise, the prevailing
retrieve-then-rerank paradigm inherits the limitations of embedding-based
retrieval; parametric generative approaches are difficult to update with new
information; and long-context methods that place the entire corpus in context
are computationally infeasible for large document collections. To address these
challenges, we introduce LATTICE, a hierarchical retrieval framework that
enables an LLM to reason over and navigate large corpora with logarithmic
search complexity by imposing a semantic tree structure on the corpus. Our
approach consists of two stages: (1) an offline phase that organizes the corpus
into a semantic hierarchy via either a bottom-up agglomerative strategy or a
top-down divisive strategy using multi-level summaries and (2) an online
traversal phase where a search LLM navigates this tree. A central challenge in
such LLM-guided search is that the model's relevance judgments are noisy,
context-dependent, and unaware of the hierarchy, making cross-branch and
cross-level comparisons difficult. To overcome this, we propose a traversal
algorithm that estimates calibrated latent relevance scores from local LLM
outputs and aggregates them into a global path relevance metric. Our
training-free framework achieves state-of-the-art zero-shot performance on the
reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in
Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline.
Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains
comparable results on BRIGHT subsets that use a static corpus for evaluation.

</details>


### [51] [Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation](https://arxiv.org/abs/2510.13229)
*Yi Zhang,Lili Xie,Ruihong Qiu,Jiajun Liu,Sen Wang*

Main category: cs.IR

TL;DR: 本文提出基于大语言模型生成轨迹进行模仿学习的离线强化学习框架以改进推荐系统，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽有潜力提升推荐系统，但直接部署存在延迟、幻觉和偏差等问题。

Method: 提出离线强化学习框架，利用逆强化学习从大语言模型演示中提取奖励模型，以累积奖励指导强化学习策略。

Result: 在两个基准数据集上的实验表明，该方法比现有基于强化学习和上下文学习的基线表现更优。

Conclusion: 所提方法有效，可减少计算开销，转移大语言模型的语义洞察。

Abstract: Recommender systems (RecSys) have become critical tools for enhancing user
engagement by delivering personalized content across diverse digital platforms.
Recent advancements in large language models (LLMs) demonstrate significant
potential for improving RecSys, primarily due to their exceptional
generalization capabilities and sophisticated contextual understanding, which
facilitate the generation of flexible and interpretable recommendations.
However, the direct deployment of LLMs as primary recommendation policies
presents notable challenges, including persistent latency issues stemming from
frequent API calls and inherent model limitations such as hallucinations and
biases. To address these issues, this paper proposes a novel offline
reinforcement learning (RL) framework that leverages imitation learning from
LLM-generated trajectories. Specifically, inverse reinforcement learning is
employed to extract robust reward models from LLM demonstrations. This approach
negates the need for LLM fine-tuning, thereby substantially reducing
computational overhead. Simultaneously, the RL policy is guided by the
cumulative rewards derived from these demonstrations, effectively transferring
the semantic insights captured by the LLM. Comprehensive experiments conducted
on two benchmark datasets validate the effectiveness of the proposed method,
demonstrating superior performance when compared against state-of-the-art
RL-based and in-context learning baselines. The code can be found at
https://github.com/ArronDZhang/IL-Rec.

</details>


### [52] [Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models](https://arxiv.org/abs/2510.13359)
*Yuki Yada,Sho Akiyama,Ryo Watanabe,Yuta Ueno,Yusuke Shido,Andre Rusli*

Main category: cs.IR

TL;DR: 本文将视觉语言模型应用于电商产品推荐，经微调模型和评估，证明其有效性并提供实用见解。


<details>
  <summary>Details</summary>
Motivation: 在大规模电商平台上，推荐视觉相似产品对用户高效发现偏好商品至关重要，因此研究视觉语言模型在电商产品推荐中的应用。

Method: 微调基于Sigmoid对比损失的视觉语言模型SigLIP，利用三个月内收集的一百万对产品图像 - 标题对，开发图像编码器用于推荐系统，通过离线历史交互日志分析和在线A/B测试评估。

Result: 离线分析中，模型nDCG@5较基线提升9.1%；在线A/B测试中，点击率提升50%，转化率提升14%。

Conclusion: 基于视觉语言模型的编码器用于电商产品推荐有效，为开发基于视觉相似性的推荐系统提供实用见解。

Abstract: On large-scale e-commerce platforms with tens of millions of active monthly
users, recommending visually similar products is essential for enabling users
to efficiently discover items that align with their preferences. This study
presents the application of a vision-language model (VLM) -- which has
demonstrated strong performance in image recognition and image-text retrieval
tasks -- to product recommendations on Mercari, a major consumer-to-consumer
marketplace used by more than 20 million monthly users in Japan. Specifically,
we fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using
one million product image-title pairs from Mercari collected over a three-month
period, and developed an image encoder for generating item embeddings used in
the recommendation system. Our evaluation comprised an offline analysis of
historical interaction logs and an online A/B test in a production environment.
In offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared
with the baseline. In the online A/B test, the click-through rate improved by
50% whereas the conversion rate improved by 14% compared with the existing
model. These results demonstrate the effectiveness of VLM-based encoders for
e-commerce product recommendations and provide practical insights into the
development of visual similarity-based recommendation systems.

</details>


### [53] [MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation](https://arxiv.org/abs/2510.13371)
*Jiin Park,Misuk Kim*

Main category: cs.IR

TL;DR: 本文提出基于大语言模型的推荐系统MADRec，实验表明其在精度和可解释性上优于传统和基于LLM的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有将大语言模型集成到推荐系统的尝试多局限于简单文本生成或静态提示推理，无法捕捉用户偏好和现实交互的复杂性。

Method: 提出MADRec，通过无监督提取评论中的多方面信息构建用户和项目配置文件，进行直接推荐、顺序推荐和解释生成，采用基于方面类别的总结生成结构化配置文件，应用重排序构建高密度输入，使用自反馈机制动态调整推理标准。

Result: 跨多个领域的实验表明，MADRec在精度和可解释性上优于传统和基于LLM的基线方法，人工评估证实生成解释有说服力。

Conclusion: MADRec是一种有效的基于大语言模型的推荐系统，在推荐性能和解释能力上表现出色。

Abstract: Recent attempts to integrate large language models (LLMs) into recommender
systems have gained momentum, but most remain limited to simple text generation
or static prompt-based inference, failing to capture the complexity of user
preferences and real-world interactions. This study proposes the Multi-Aspect
Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs
user and item profiles by unsupervised extraction of multi-aspect information
from reviews and performs direct recommendation, sequential recommendation, and
explanation generation. MADRec generates structured profiles via
aspect-category-based summarization and applies Re-Ranking to construct
high-density inputs. When the ground-truth item is missing from the output, the
Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments
across multiple domains show that MADRec outperforms traditional and LLM-based
baselines in both precision and explainability, with human evaluation further
confirming the persuasiveness of the generated explanations.

</details>


### [54] [RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge](https://arxiv.org/abs/2510.13590)
*Jiale Han,Austin Cheung,Yubai Wei,Zheng Yu,Xusheng Wang,Bing Zhu,Yi Yang*

Main category: cs.IR

TL;DR: 现有RAG系统忽略知识时效性，提出TG - RAG模型和ECT - QA数据集，实验表明TG - RAG表现更佳。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统忽略知识的时间敏感性，存在缺乏有效时间感知表示和评估未考虑更新成本与检索稳定性的问题。

Method: 提出TG - RAG，将外部语料建模为双层时间图，生成多粒度时间摘要，支持增量更新；引入ECT - QA数据集和评估协议。

Result: TG - RAG显著优于现有基线。

Conclusion: TG - RAG在处理时间知识和增量更新方面有效。

Abstract: Knowledge is inherently time-sensitive and continuously evolves over time.
Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with
external knowledge, they largely ignore this temporal nature. This raises two
challenges for RAG. First, current RAG methods lack effective time-aware
representations. Same facts of different time are difficult to distinguish with
vector embeddings or conventional knowledge graphs. Second, most RAG
evaluations assume a static corpus, leaving a blind spot regarding update costs
and retrieval stability as knowledge evolves. To make RAG time-aware, we
propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level
temporal graph consisting of a temporal knowledge graph with timestamped
relations and a hierarchical time graph. Multi-granularity temporal summaries
are generated for each time node to capture both key events and broader trends
at that time. The design supports incremental updates by extracting new
temporal facts from the incoming corpus and merging them into the existing
graph. The temporal graph explicitly represents identical facts at different
times as distinct edges to avoid ambiguity, and the time hierarchy graph allows
only generating reports for new leaf time nodes and their ancestors, ensuring
effective and efficient updates. During inference, TG-RAG dynamically retrieves
a subgraph within the temporal and semantic scope of the query, enabling
precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive
question-answering dataset featuring both specific and abstract queries, along
with a comprehensive evaluation protocol designed to assess incremental update
capabilities of RAG systems. Extensive experiments show that TG-RAG
significantly outperforms existing baselines, demonstrating the effectiveness
of our method in handling temporal knowledge and incremental updates.

</details>


### [55] [HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation](https://arxiv.org/abs/2510.13738)
*Jingyi Zhou,Cheng Chen,Kai Zuo,Manjie Xu,Zhendong Fu,Yibo Chen,Xu Tang,Yao Hu*

Main category: cs.IR

TL;DR: 现有基于大语言模型的序列推荐方法在建模用户长期和多样化兴趣方面存在局限，本文提出HyMiRec框架解决这些问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的序列推荐方法存在建模用户长期和多样化兴趣的局限，如截断序列丢失长期偏好信号、单预测嵌入忽视用户兴趣多面性。

Method: 提出HyMiRec混合多兴趣序列推荐框架，用轻量级推荐器提取粗兴趣嵌入，LLM推荐器捕获细化兴趣嵌入；引入基于余弦相似度的残差码本压缩和复用用户历史嵌入；设计解耦多兴趣学习模块学习多个兴趣信号。

Result: 在基准数据集和工业数据集上实验表明优于现有方法，在线A/B测试显示在真实推荐系统中有持续改进。

Conclusion: HyMiRec框架能有效解决现有基于大语言模型的序列推荐方法的局限，在实验和实际应用中表现良好。

Abstract: Large language models (LLMs) have recently demonstrated strong potential for
sequential recommendation. However, current LLM-based approaches face critical
limitations in modeling users' long-term and diverse interests. First, due to
inference latency and feature fetching bandwidth constraints, existing methods
typically truncate user behavior sequences to include only the most recent
interactions, resulting in the loss of valuable long-range preference signals.
Second, most current methods rely on next-item prediction with a single
predicted embedding, overlooking the multifaceted nature of user interests and
limiting recommendation diversity. To address these challenges, we propose
HyMiRec, a hybrid multi-interest sequential recommendation framework, which
leverages a lightweight recommender to extracts coarse interest embeddings from
long user sequences and an LLM-based recommender to captures refined interest
embeddings. To alleviate the overhead of fetching features, we introduce a
residual codebook based on cosine similarity, enabling efficient compression
and reuse of user history embeddings. To model the diverse preferences of
users, we design a disentangled multi-interest learning module, which leverages
multiple interest queries to learn disentangles multiple interest signals
adaptively, allowing the model to capture different facets of user intent.
Extensive experiments are conducted on both benchmark datasets and a collected
industrial dataset, demonstrating our effectiveness over existing
state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec
brings consistent improvements in real-world recommendation systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Local Timescale Gates for Timescale-Robust Continual Spiking Neural Networks](https://arxiv.org/abs/2510.12843)
*Ansh Tiwari,Ayush Chauhan*

Main category: cs.LG

TL;DR: 提出LT - Gate神经元模型结合双时间常数动态与自适应门控机制，引入方差跟踪正则化，在顺序学习任务中表现出色，适用于神经形态硬件，增强SNN持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 解决脉冲神经网络（SNNs）在需要快速适应和长期记忆的任务，尤其是持续学习中的难题。

Method: 提出Local Timescale Gating (LT - Gate) 神经元模型，结合双时间常数动态与自适应门控机制，引入方差跟踪正则化。

Result: 在顺序学习任务中显著提高准确性和保留率，在挑战性时间分类基准测试中最终准确率约51%，优于其他方法，可在神经形态硬件上运行。

Conclusion: 多时间尺度门控能大幅增强SNN的持续学习能力，缩小脉冲网络和传统深度网络在终身学习任务上的差距。

Abstract: Spiking neural networks (SNNs) promise energy-efficient artificial
intelligence on neuromorphic hardware but struggle with tasks requiring both
fast adaptation and long-term memory, especially in continual learning. We
propose Local Timescale Gating (LT-Gate), a neuron model that combines dual
time-constant dynamics with an adaptive gating mechanism. Each spiking neuron
tracks information on a fast and a slow timescale in parallel, and a learned
gate locally adjusts their influence. This design enables individual neurons to
preserve slow contextual information while responding to fast signals,
addressing the stability-plasticity dilemma. We further introduce a
variance-tracking regularization that stabilizes firing activity, inspired by
biological homeostasis. Empirically, LT-Gate yields significantly improved
accuracy and retention in sequential learning tasks: on a challenging temporal
classification benchmark it achieves about 51 percent final accuracy, compared
to about 46 percent for a recent Hebbian continual-learning baseline and lower
for prior SNN methods. Unlike approaches that require external replay or
expensive orthogonalizations, LT-Gate operates with local updates and is fully
compatible with neuromorphic hardware. In particular, it leverages features of
Intel's Loihi chip (multiple synaptic traces with different decay rates) for
on-chip learning. Our results demonstrate that multi-timescale gating can
substantially enhance continual learning in SNNs, narrowing the gap between
spiking and conventional deep networks on lifelong-learning tasks.

</details>


### [57] [Lifting Manifolds to Mitigate Pseudo-Alignment in LLM4TS](https://arxiv.org/abs/2510.12847)
*Liangwei Nathan Zheng,Wenhao Liang,Wei Emma Zhang,Miao Xu,Olaf Maennel,Weitong Chen*

Main category: cs.LG

TL;DR: 论文研究LLM4TS模型的伪对齐问题，揭示其成因并提出TimeSUP技术，该技术能提升预测性能。


<details>
  <summary>Details</summary>
Motivation: LLM4TS模型存在伪对齐挑战且社区对其成因讨论有限，需深入研究。

Method: 深入研究伪对齐根源，建立与LLM锥体效应的联系，提出TimeSUP技术增加时间序列流形。

Result: TimeSUP使时间和语言标记表示既独特又有高余弦相似度，在长期预测中表现优于现有方法，可集成到现有管道提升性能。

Conclusion: TimeSUP能有效缓解LLM4TS模型的伪对齐问题，提升预测性能。

Abstract: Pseudo-Alignment is a pervasive challenge in many large language models for
time series (LLM4TS) models, often causing them to underperform compared to
linear models or randomly initialised backbones. However, there is limited
discussion in the community for the reasons that pseudo-alignment occurs. In
this work, we conduct a thorough investigation into the root causes of
pseudo-alignment in LLM4TS and build a connection of pseudo-alignment to the
cone effect in LLM. We demonstrate that pseudo-alignment arises from the
interplay of cone effect within pretrained LLM components and the intrinsically
low-dimensional manifold of time-series data. In addition, we also introduce
\textit{\textbf{TimeSUP}}, a novel technique designed to mitigate this issue
and improve forecast performance in existing LLM4TS approaches. TimeSUP
addresses this by increasing the time series manifold to more closely match the
intrinsic dimension of language embeddings, allowing the model to distinguish
temporal signals clearly while still capturing shared structures across
modalities. As a result, representations for time and language tokens remain
distinct yet exhibit high cosine similarity, signifying that the model
preserves each modality unique features while learning their commonalities in a
unified embedding space. Empirically, TimeSUP consistently outperforms
state-of-the-art LLM4TS methods and other lightweight baselines on long-term
forecasting performance. Furthermore, it can be seamlessly integrated into four
existing LLM4TS pipelines and delivers significant improvements in forecasting
performance.

</details>


### [58] [FedGTEA: Federated Class-Incremental Learning with Gaussian Task Embedding and Alignment](https://arxiv.org/abs/2510.12927)
*Haolin Li,Hoda Bidkhori*

Main category: cs.LG

TL;DR: 提出联邦高斯任务嵌入与对齐框架FedGTEA用于联邦类增量学习，客户端CATE生成高斯任务嵌入，服务器端用2 - Wasserstein距离度量任务差距，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 设计一个可扩展且通信高效的框架，用于联邦类增量学习，捕获特定任务知识和模型不确定性，同时满足联邦学习的隐私约束。

Method: 客户端使用Cardinality - Agnostic Task Encoder (CATE)生成高斯分布的任务嵌入；服务器端利用2 - Wasserstein距离度量任务差距，制定Wasserstein损失来强制任务分离。

Result: 在流行数据集上的大量实验表明，FedGTEA实现了卓越的分类性能，显著减轻了遗忘问题，始终优于现有的强大基线。

Conclusion: FedGTEA是一个有效的联邦类增量学习框架，在性能和隐私保护方面表现出色。

Abstract: We introduce a novel framework for Federated Class Incremental Learning,
called Federated Gaussian Task Embedding and Alignment (FedGTEA). FedGTEA is
designed to capture task-specific knowledge and model uncertainty in a scalable
and communication-efficient manner. At the client side, the
Cardinality-Agnostic Task Encoder (CATE) produces Gaussian-distributed task
embeddings that encode task knowledge, address statistical heterogeneity, and
quantify data uncertainty. Importantly, CATE maintains a fixed parameter size
regardless of the number of tasks, which ensures scalability across long task
sequences. On the server side, FedGTEA utilizes the 2-Wasserstein distance to
measure inter-task gaps between Gaussian embeddings. We formulate the
Wasserstein loss to enforce inter-task separation. This probabilistic
formulation not only enhances representation learning but also preserves
task-level privacy by avoiding the direct transmission of latent embeddings,
aligning with the privacy constraints in federated learning. Extensive
empirical evaluations on popular datasets demonstrate that FedGTEA achieves
superior classification performance and significantly mitigates forgetting,
consistently outperforming strong existing baselines.

</details>


### [59] [Learning at the Speed of Physics: Equilibrium Propagation on Oscillator Ising Machines](https://arxiv.org/abs/2510.12934)
*Alex Gower*

Main category: cs.LG

TL;DR: 研究表明在振荡器伊辛机（OIMs）上进行平衡传播（EP）能实现有竞争力的准确率，且在硬件约束下保持鲁棒性，确立了OIMs作为神经形态学习的高效底物。


<details>
  <summary>Details</summary>
Motivation: 物理系统的能量下降特性可加速机器学习，探索OIMs在此方面的应用。

Method: 在OIMs上进行平衡传播（EP）。

Result: 在MNIST上准确率约97.2 ± 0.1 %，在Fashion - MNIST上约88.0 ± 0.1 %，且在参数量化和相位噪声等硬件约束下保持鲁棒性。

Conclusion: OIMs是神经形态学习的快速、节能底物，基于能量的模型有望在能直接进行优化的物理硬件上实现。

Abstract: Physical systems that naturally perform energy descent offer a direct route
to accelerating machine learning. Oscillator Ising Machines (OIMs) exemplify
this idea: their GHz-frequency dynamics mirror both the optimization of
energy-based models (EBMs) and gradient descent on loss landscapes, while
intrinsic noise corresponds to Langevin dynamics - supporting sampling as well
as optimization. Equilibrium Propagation (EP) unifies these processes into
descent on a single total energy landscape, enabling local learning rules
without global backpropagation. We show that EP on OIMs achieves competitive
accuracy ($\sim 97.2 \pm 0.1 \%$ on MNIST, $\sim 88.0 \pm 0.1 \%$ on
Fashion-MNIST), while maintaining robustness under realistic hardware
constraints such as parameter quantization and phase noise. These results
establish OIMs as a fast, energy-efficient substrate for neuromorphic learning,
and suggest that EBMs - often bottlenecked by conventional processors - may
find practical realization on physical hardware whose dynamics directly perform
their optimization.

</details>


### [60] [Pruning Cannot Hurt Robustness: Certified Trade-offs in Reinforcement Learning](https://arxiv.org/abs/2510.12939)
*James Pedley,Benjamin Etheridge,Stephen J. Roberts,Francesco Quinzan*

Main category: cs.LG

TL;DR: 本文提出剪枝在状态对抗马尔可夫决策过程中的认证鲁棒性理论框架，实证发现剪枝可提升强化学习鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实环境中强化学习策略需在对抗扰动下保持可靠，且现代深度强化学习代理参数过多，剪枝在监督学习中可提升鲁棒性，但在对抗强化学习中的作用尚不明确。

Method: 开发状态对抗马尔可夫决策过程中剪枝下认证鲁棒性的理论框架，推导三项后悔分解，在连续控制基准上评估幅度和微剪枝策略。

Result: 对于高斯和分类策略，元素级剪枝只会收紧认证鲁棒性边界；在各任务中，剪枝在适度稀疏水平下能显著提升鲁棒性，且不损害甚至有时增强清洁性能。

Conclusion: 剪枝不仅是压缩工具，也是用于鲁棒强化学习的结构干预手段。

Abstract: Reinforcement learning (RL) policies deployed in real-world environments must
remain reliable under adversarial perturbations. At the same time, modern deep
RL agents are heavily over-parameterized, raising costs and fragility concerns.
While pruning has been shown to improve robustness in supervised learning, its
role in adversarial RL remains poorly understood. We develop the first
theoretical framework for certified robustness under pruning in
state-adversarial Markov decision processes (SA-MDPs). For Gaussian and
categorical policies with Lipschitz networks, we prove that element-wise
pruning can only tighten certified robustness bounds; pruning never makes the
policy less robust. Building on this, we derive a novel three-term regret
decomposition that disentangles clean-task performance, pruning-induced
performance loss, and robustness gains, exposing a fundamental
performance--robustness frontier. Empirically, we evaluate magnitude and
micro-pruning schedules on continuous-control benchmarks with strong
policy-aware adversaries. Across tasks, pruning consistently uncovers
reproducible ``sweet spots'' at moderate sparsity levels, where robustness
improves substantially without harming - and sometimes even enhancing - clean
performance. These results position pruning not merely as a compression tool
but as a structural intervention for robust RL.

</details>


### [61] [An Investigation of Memorization Risk in Healthcare Foundation Models](https://arxiv.org/abs/2510.12950)
*Sana Tonekaboni,Lena Stempfle,Adibvafa Fallahpour,Walter Gerych,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 引入黑盒评估测试评估EHR基础模型隐私相关记忆风险，验证方法并开源工具包。


<details>
  <summary>Details</summary>
Motivation: EHR基础模型在临床应用有前景，但存在记忆患者信息的隐私问题。

Method: 引入黑盒评估测试框架，在嵌入和生成层面探测记忆情况，区分模型泛化和有害记忆。

Result: 在公开EHR基础模型上验证了方法。

Conclusion: 发布开源工具包以促进医疗AI可重复和协作的隐私评估。

Abstract: Foundation models trained on large-scale de-identified electronic health
records (EHRs) hold promise for clinical applications. However, their capacity
to memorize patient information raises important privacy concerns. In this
work, we introduce a suite of black-box evaluation tests to assess
privacy-related memorization risks in foundation models trained on structured
EHR data. Our framework includes methods for probing memorization at both the
embedding and generative levels, and aims to distinguish between model
generalization and harmful memorization in clinically relevant settings. We
contextualize memorization in terms of its potential to compromise patient
privacy, particularly for vulnerable subgroups. We validate our approach on a
publicly available EHR foundation model and release an open-source toolkit to
facilitate reproducible and collaborative privacy assessments in healthcare AI.

</details>


### [62] [A Multimodal XAI Framework for Trustworthy CNNs and Bias Detection in Deep Representation Learning](https://arxiv.org/abs/2510.12957)
*Noor Islam S. Mohammad*

Main category: cs.LG

TL;DR: 提出新颖多模态可解释AI框架，在MNIST扩展数据集上表现优于基线，消融研究证明集成解释性和偏差感知学习的优势，为敏感领域可信AI提供途径。


<details>
  <summary>Details</summary>
Motivation: 标准基准数据集无法暴露潜在偏差和多模态特征复杂性，限制了深度神经网络在高风险应用中的可信度。

Method: 提出统一注意力增强特征融合、基于Grad - CAM++的局部解释和Reveal - to - Revise反馈循环的多模态可解释AI框架。

Result: 在MNIST多模态扩展数据集上，分类准确率达93.2%，F1分数达91.6%，解释保真度（IoU - XAI）达78.1%，优于单模态和不可解释的基线。

Conclusion: 集成解释性与偏差感知学习可增强鲁棒性和人机对齐，该研究弥合了性能、透明度和公平性之间的差距，为敏感领域可信AI提供了实用途径。

Abstract: Standard benchmark datasets, such as MNIST, often fail to expose latent
biases and multimodal feature complexities, limiting the trustworthiness of
deep neural networks in high-stakes applications. We propose a novel multimodal
Explainable AI (XAI) framework that unifies attention-augmented feature fusion,
Grad-CAM++-based local explanations, and a Reveal-to-Revise feedback loop for
bias detection and mitigation. Evaluated on multimodal extensions of MNIST, our
approach achieves 93.2% classification accuracy, 91.6% F1-score, and 78.1%
explanation fidelity (IoU-XAI), outperforming unimodal and non-explainable
baselines. Ablation studies demonstrate that integrating interpretability with
bias-aware learning enhances robustness and human alignment. Our work bridges
the gap between performance, transparency, and fairness, highlighting a
practical pathway for trustworthy AI in sensitive domains.

</details>


### [63] [Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games](https://arxiv.org/abs/2510.13060)
*Anupam Nayak,Tong Yang,Osman Yagan,Gauri Joshi,Yuejie Chi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reverse Kullback-Leibler (KL) divergence-based regularization with respect to
a fixed reference policy is widely used in modern reinforcement learning to
preserve the desired traits of the reference policy and sometimes to promote
exploration (using uniform reference policy, known as entropy regularization).
Beyond serving as a mere anchor, the reference policy can also be interpreted
as encoding prior knowledge about good actions in the environment. In the
context of alignment, recent game-theoretic approaches have leveraged KL
regularization with pretrained language models as reference policies, achieving
notable empirical success in self-play methods. Despite these advances, the
theoretical benefits of KL regularization in game-theoretic settings remain
poorly understood. In this work, we develop and analyze algorithms that
provably achieve improved sample efficiency under KL regularization. We study
both two-player zero-sum Matrix games and Markov games: for Matrix games, we
propose OMG, an algorithm based on best response sampling with optimistic
bonuses, and extend this idea to Markov games through the algorithm SOMG, which
also uses best response sampling and a novel concept of superoptimistic
bonuses. Both algorithms achieve a logarithmic regret in $T$ that scales
inversely with the KL regularization strength $\beta$ in addition to the
standard $\widetilde{\mathcal{O}}(\sqrt{T})$ regret independent of $\beta$
which is attained in both regularized and unregularized settings

</details>


### [64] [Balancing Performance and Reject Inclusion: A Novel Confident Inlier Extrapolation Framework for Credit Scoring](https://arxiv.org/abs/2510.12967)
*Athyrson Machado Ribeiro,Marcos Medeiros Raimundo*

Main category: cs.LG

TL;DR: 提出新型拒绝推断框架CI - EX，在真实数据集验证其有效性，该框架在特定指标上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统拒绝推断方法盲目外推，未考虑被拒和已接受客户群体的分布差异，需改进。

Method: 提出CI - EX框架，用异常检测模型识别被拒客户样本分布，基于监督分类模型概率为接近已接受群体分布的被拒个体分配标签。

Result: 在两个真实信贷数据集上实验，发现拒绝推断方法存在AUC和特定指标的权衡，CI - EX框架在特定指标上优于现有模型，AUC表现有竞争力。

Conclusion: CI - EX框架能有效解决样本偏差问题，在拒绝推断中表现良好。

Abstract: Reject Inference (RI) methods aim to address sample bias by inferring missing
repayment data for rejected credit applicants. Traditional approaches often
assume that the behavior of rejected clients can be extrapolated from accepted
clients, despite potential distributional differences between the two
populations. To mitigate this blind extrapolation, we propose a novel Confident
Inlier Extrapolation framework (CI-EX). CI-EX iteratively identifies the
distribution of rejected client samples using an outlier detection model and
assigns labels to rejected individuals closest to the distribution of the
accepted population based on probabilities derived from a supervised
classification model. The effectiveness of our proposed framework is validated
through experiments on two large real-world credit datasets. Performance is
evaluated using the Area Under the Curve (AUC) as well as RI-specific metrics
such as Kickout and a novel metric introduced in this work, denoted as Area
under the Kickout. Our findings reveal that RI methods, including the proposed
framework, generally involve a trade-off between AUC and RI-specific metrics.
However, the proposed CI-EX framework consistently outperforms existing RI
models from the credit literature in terms of RI-specific metrics while
maintaining competitive performance in AUC across most experiments.

</details>


### [65] [A Connection Between Score Matching and Local Intrinsic Dimension](https://arxiv.org/abs/2510.12975)
*Eric Yeats,Aaron Jacobson,Darryl Hannan,Yiran Jia,Timothy Doster,Henry Kvinge,Scott Mahan*

Main category: cs.LG

TL;DR: 本文提出用去噪分数匹配损失估计局部内在维度（LID），实验表明该方法有竞争力且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有量化高维复杂数据LID的方法需多次前向传播或梯度计算，在计算和内存受限场景适用性有限。

Method: 证明LID是去噪分数匹配损失的下界，用其作为LID估计器，还表明等效隐式分数匹配损失通过法向维度近似LID且与FLIPD相关。

Result: 在流形基准测试和Stable Diffusion 3.5上实验，去噪分数匹配损失作为LID估计器在问题规模和量化水平增加时，精度和内存占用表现优越。

Conclusion: 去噪分数匹配损失是有竞争力且可扩展的LID估计器。

Abstract: The local intrinsic dimension (LID) of data is a fundamental quantity in
signal processing and learning theory, but quantifying the LID of
high-dimensional, complex data has been a historically challenging task. Recent
works have discovered that diffusion models capture the LID of data through the
spectra of their score estimates and through the rate of change of their
density estimates under various noise perturbations. While these methods can
accurately quantify LID, they require either many forward passes of the
diffusion model or use of gradient computation, limiting their applicability in
compute- and memory-constrained scenarios.
  We show that the LID is a lower bound on the denoising score matching loss,
motivating use of the denoising score matching loss as a LID estimator.
Moreover, we show that the equivalent implicit score matching loss also
approximates LID via the normal dimension and is closely related to a recent
LID estimator, FLIPD. Our experiments on a manifold benchmark and with Stable
Diffusion 3.5 indicate that the denoising score matching loss is a highly
competitive and scalable LID estimator, achieving superior accuracy and memory
footprint under increasing problem size and quantization level.

</details>


### [66] [Going with the Flow: Approximating Banzhaf Values via Graph Neural Networks](https://arxiv.org/abs/2510.13391)
*Benjamin Kempinski,Tal Kachman*

Main category: cs.LG

TL;DR: 提出用图神经网络（GNN）近似计算网络流博弈中的班扎夫值，实验表明其比精确和采样方法有数量级的加速，且有强零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 精确计算网络流博弈中的班扎夫值复杂度高，蒙特卡罗采样方法样本复杂度高且不能跨网络配置迁移知识，不适用于大规模或动态系统。

Method: 将问题构建为图级预测任务，使用GNN从网络拓扑和控制结构中学习可泛化的代理影响模式，并对比三种GNN架构。

Result: 训练的GNN模型能高精度近似班扎夫值，比精确和采样方法有数量级的加速，且有强零样本泛化能力。

Conclusion: GNN可作为复杂网络系统可扩展合作博弈论分析的实用工具。

Abstract: Computing the Banzhaf value in network flow games is fundamental for
quantifying agent influence in multi-agent systems, with applications ranging
from cybersecurity to infrastructure planning. However, exact computation is
intractable for systems with more than $\sim20$ agents due to exponential
complexity $\mathcal{O}(2^m)$. While Monte Carlo sampling methods provide
statistical estimates, they suffer from high sample complexity and cannot
transfer knowledge across different network configurations, making them
impractical for large-scale or dynamic systems. We present a novel
learning-based approach using Graph Neural Networks (GNNs) to approximate
Banzhaf values in cardinal network flow games. By framing the problem as a
graph-level prediction task, our method learns generalisable patterns of agent
influence directly from network topology and control structure. We conduct a
comprehensive empirical study comparing three state-of-the-art GNN
architectures-Graph Attention Networks (GAT), Graph Isomorphism Networks with
Edge features (GINE), and EdgeConv-on a large-scale synthetic dataset of
200,000 graphs per configuration, varying in size (20-100 nodes), agent count
(5-20), and edge probability (0.5-1.0). Our results demonstrate that trained
GNN models achieve high-fidelity Banzhaf value approximation with
order-of-magnitude speedups compared to exact and sampling-based methods. Most
significantly, we show strong zero-shot generalisation: models trained on
graphs of a specific size and topology accurately predict Banzhaf values for
entirely new networks with different structural properties, without requiring
retraining. This work establishes GNNs as a practical tool for scalable
cooperative game-theoretic analysis of complex networked systems.

</details>


### [67] [Reference-Specific Unlearning Metrics Can Hide the Truth: A Reality Check](https://arxiv.org/abs/2510.12981)
*Sungjun Cho,Dasol Hwang,Frederic Sala,Sangheum Hwang,Kyunghyun Cho,Sungmin Cha*

Main category: cs.LG

TL;DR: 现有生成模型遗忘评估指标有局限性，提出新指标FADE，实验表明FADE能更有效评估遗忘方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型遗忘指标基于参考响应或分类器输出，存在系统盲点，无法评估模型是否真正遗忘。

Method: 提出Functional Alignment for Distributional Equivalence (FADE)指标，通过比较生成样本的双向似然分配来衡量遗忘模型和参考模型的分布相似性。

Result: 在TOFU和UnlearnCanvas基准测试中，传统指标得分高的方法未实现分布等价，很多比遗忘前更偏离标准。

Conclusion: 当前评估方法有根本缺陷，FADE为开发和评估有效遗忘方法提供更可靠基础。

Abstract: Current unlearning metrics for generative models evaluate success based on
reference responses or classifier outputs rather than assessing the core
objective: whether the unlearned model behaves indistinguishably from a model
that never saw the unwanted data. This reference-specific approach creates
systematic blind spots, allowing models to appear successful while retaining
unwanted knowledge accessible through alternative prompts or attacks. We
address these limitations by proposing Functional Alignment for Distributional
Equivalence (FADE), a novel metric that measures distributional similarity
between unlearned and reference models by comparing bidirectional likelihood
assignments over generated samples. Unlike existing approaches that rely on
predetermined references, FADE captures functional alignment across the entire
output distribution, providing a principled assessment of genuine unlearning.
Our experiments on the TOFU benchmark for LLM unlearning and the UnlearnCanvas
benchmark for text-to-image diffusion model unlearning reveal that methods
achieving near-optimal scores on traditional metrics fail to achieve
distributional equivalence, with many becoming more distant from the gold
standard than before unlearning. These findings expose fundamental gaps in
current evaluation practices and demonstrate that FADE provides a more robust
foundation for developing and assessing truly effective unlearning methods.

</details>


### [68] [CSI-4CAST: A Hybrid Deep Learning Model for CSI Prediction with Comprehensive Robustness and Generalization Testing](https://arxiv.org/abs/2510.12996)
*Sikai Cheng,Reza Zandehshahvar,Haoruo Zhao,Daniel A. Garcia-Ulloa,Alejandro Villena-Rodriguez,Carles Navarro Manchón,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 本文提出混合深度学习架构CSI - 4CAST用于CSI预测，还给出综合基准CSI - RRG，实验显示CSI - 4CAST性能优且计算成本低，相关数据集和评估协议公开。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的方法在应对实际非高斯噪声的鲁棒性、跨不同信道条件的泛化能力和计算效率方面存在局限，需改进。

Method: 引入集成4个关键组件的混合深度学习架构CSI - 4CAST，提出综合基准CSI - RRG进行严格评估。

Result: CSI - 4CAST在多数场景中预测精度高、计算成本低，减少大量FLOPs；对CSI - RRG的评估为了解不同信道因素对模型性能和泛化能力的影响提供见解。

Conclusion: 公开数据集和评估协议，为鲁棒高效的CSI预测研究建立标准化基准，鼓励进一步研究。

Abstract: Channel state information (CSI) prediction is a promising strategy for
ensuring reliable and efficient operation of massive multiple-input
multiple-output (mMIMO) systems by providing timely downlink (DL) CSI. While
deep learning-based methods have advanced beyond conventional model-driven and
statistical approaches, they remain limited in robustness to practical
non-Gaussian noise, generalization across diverse channel conditions, and
computational efficiency. This paper introduces CSI-4CAST, a hybrid deep
learning architecture that integrates 4 key components, i.e., Convolutional
neural network residuals, Adaptive correction layers, ShuffleNet blocks, and
Transformers, to efficiently capture both local and long-range dependencies in
CSI prediction. To enable rigorous evaluation, this work further presents a
comprehensive benchmark, CSI-RRG for Regular, Robustness and Generalization
testing, which includes more than 300,000 samples across 3,060 realistic
scenarios for both TDD and FDD systems. The dataset spans multiple channel
models, a wide range of delay spreads and user velocities, and diverse noise
types and intensity degrees. Experimental results show that CSI-4CAST achieves
superior prediction accuracy with substantially lower computational cost,
outperforming baselines in 88.9% of TDD scenarios and 43.8% of FDD scenario,
the best performance among all evaluated models, while reducing FLOPs by 5x and
3x compared to LLM4CP, the strongest baseline. In addition, evaluation over
CSI-RRG provides valuable insights into how different channel factors affect
the performance and generalization capability of deep learning models. Both the
dataset (https://huggingface.co/CSI-4CAST) and evaluation protocols
(https://github.com/AI4OPT/CSI-4CAST) are publicly released to establish a
standardized benchmark and to encourage further research on robust and
efficient CSI prediction.

</details>


### [69] [Max It or Miss It: Benchmarking LLM On Solving Extremal Problems](https://arxiv.org/abs/2510.12997)
*Binxin Gao,Jingjun Han*

Main category: cs.LG

TL;DR: 本文引入ExtremBench基准数据集评估大语言模型优化推理能力，发现其与现有数学基准表现有差异。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理能力来源和机制不明，优化推理在多领域重要但缺乏系统评估。

Method: 引入ExtremBench基准数据集，对Qwen3、GPT - OSS和DeepSeek等开源模型进行评估。

Result: 大语言模型的极值求解推理能力与现有数学基准（如AIME25和MATH - 500）表现不一致。

Conclusion: 当前评估实践存在关键差距，现有基准可能无法全面涵盖数学推理能力。

Abstract: Test-time scaling has enabled Large Language Models (LLMs) with remarkable
reasoning capabilities, particularly in mathematical domains, through
intermediate chain-of-thought (CoT) reasoning before generating final answers.
However, the specific sources and mechanisms underlying these reasoning
capabilities remain insufficiently understood. Optimization reasoning, i.e.
finding extrema under constraints, represents a fundamental abstraction that
underpins critical applications in planning, control, resource allocation, and
prompt search. To systematically evaluate this capability, we introduce
ExtremBench, a benchmark dataset for solving mathematical extremal problems,
curated from inequality exercises used for Chinese Mathematical Olympiad and
transformed into $93$ standardized extrema-finding problems. We conduct
extensive evaluations across various state-of-the-art open-source model
families, including the Qwen3, GPT-OSS, and DeepSeek. Our results reveal that
LLMs' extremal-solving reasoning capabilities do not always align with those of
current mathematical benchmarks such as AIME25 and MATH-500, with some models
showing strong general mathematical reasoning but poor extremal-solving skills,
and vice versa. This discrepancy highlights a critical gap in current
evaluation practices and suggests that existing benchmarks may not
comprehensively capture the full spectrum of mathematical reasoning abilities.

</details>


### [70] [AMORE: Adaptive Multi-Output Operator Network for Stiff Chemical Kinetics](https://arxiv.org/abs/2510.12999)
*Kamaljyoti Nath,Additi Pandey,Bryan T. Susi,Hessam Babaee,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出AMORE框架处理刚性系统时间积分问题，含自适应多输出算子网络和自适应损失函数，通过实例验证其有效性，可用于加速湍流燃烧模拟。


<details>
  <summary>Details</summary>
Motivation: 刚性系统时间积分是燃烧等领域计算成本的主要来源，需缓解刚性带来的挑战，现有神经算子缺乏可靠学习策略。

Method: 开发AMORE框架，含能预测多输出的算子和自适应损失函数，设计满足分区单位性的主干，提出可逆分析映射，采用两步训练法并扩展自适应损失函数。

Result: 通过合成气和GRI - Mech 3.0两个例子验证了模型的有效性和适用性。

Conclusion: 提出的DeepONet可作为未来CFD研究加速湍流燃烧模拟的骨干，AMORE是通用框架，也适用于FNO。

Abstract: Time integration of stiff systems is a primary source of computational cost
in combustion, hypersonics, and other reactive transport systems. This
stiffness can introduce time scales significantly smaller than those associated
with other physical processes, requiring extremely small time steps in explicit
schemes or computationally intensive implicit methods. Consequently, strategies
to alleviate challenges posed by stiffness are important. While neural
operators (DeepONets) can act as surrogates for stiff kinetics, a reliable
operator learning strategy is required to appropriately account for differences
in the error between output variables and samples. Here, we develop AMORE,
Adaptive Multi-Output Operator Network, a framework comprising an operator
capable of predicting multiple outputs and adaptive loss functions ensuring
reliable operator learning. The operator predicts all thermochemical states
from given initial conditions. We propose two adaptive loss functions within
the framework, considering each state variable's and sample's error to penalize
the loss function. We designed the trunk to automatically satisfy Partition of
Unity. To enforce unity mass-fraction constraint exactly, we propose an
invertible analytical map that transforms the $n$-dimensional species
mass-fraction vector into an ($n-1$)-dimensional space, where DeepONet training
is performed. We consider two-step training for DeepONet for multiple outputs
and extend adaptive loss functions for trunk and branch training. We
demonstrate the efficacy and applicability of our models through two examples:
the syngas (12 states) and GRI-Mech 3.0 (24 active states out of 54). The
proposed DeepONet will be a backbone for future CFD studies to accelerate
turbulent combustion simulations. AMORE is a general framework, and here, in
addition to DeepONet, we also demonstrate it for FNO.

</details>


### [71] [Escaping Local Optima in the Waddington Landscape: A Multi-Stage TRPO-PPO Approach for Single-Cell Perturbation Analysis](https://arxiv.org/abs/2510.13018)
*Francis Boabang,Samuel Asante Gyamerah*

Main category: cs.LG

TL;DR: 提出用于单细胞扰动建模的多阶段强化学习算法，改进了单细胞测序扰动分析的泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动框架在单细胞扰动预测中易陷入局部最优，缺乏能避免局部最优并收敛到合适谱系的纯数据驱动方法。

Method: 引入多阶段强化学习算法，先使用Fisher - 向量积和共轭梯度求解器计算显式自然梯度更新，通过KL信任区域约束缩放；再用带裁剪替代的近端策略优化（PPO）细化策略。

Result: 该算法在单细胞RNA测序（scRNA - seq）和单细胞ATAC测序（scATAC - seq）扰动分析中显著提高了泛化性。

Conclusion: 多阶段强化学习算法能有效避免局部最优，在单细胞扰动建模中表现良好。

Abstract: Modeling cellular responses to genetic and chemical perturbations remains a
central challenge in single-cell biology. Existing data-driven framework have
advanced perturbation prediction through variational autoencoders, chemically
conditioned autoencoders, and large-scale transformer pretraining. However,
these models are prone to local optima in the nonconvex Waddington landscape of
cell fate decisions, where poor initialization can trap trajectories in
spurious lineages or implausible differentiation outcomes. While executable
gene regulatory networks complement these approaches, automated design
frameworks incorporate biological priors through multi-agent optimization. Yet,
an approach that is completely data-driven with well-designed initialization to
escape local optima and converge to a proper lineage remains elusive. In this
work, we introduce a multistage reinforcement learning algorithm tailored for
single-cell perturbation modeling. We first compute an explicit natural
gradient update using Fisher-vector products and a conjugate gradient solver,
scaled by a KL trust-region constraint to provide a safe, curvature-aware the
first step for the policy. Starting with these preconditioned parameters, we
then apply a second phase of proximal policy optimization (PPO) with clipped
surrogates, exploiting minibatch efficiency to refine the policy. We
demonstrate that this initialization substantially improves generalization on
Single-cell RNA sequencing (scRNA-seq) and Single-cell ATAC sequencing
(scATAC-seq) pertubation analysis.

</details>


### [72] [Absolute indices for determining compactness, separability and number of clusters](https://arxiv.org/abs/2510.13065)
*Adil M. Bagirov,Ramiz M. Aliguliyev,Nargiz Sultanova,Sona Taheri*

Main category: cs.LG

TL;DR: 本文引入新型绝对聚类指标来确定聚类的紧凑性和可分离性，用多个数据集验证其性能并与其他指标对比。


<details>
  <summary>Details</summary>
Motivation: 现有聚类有效性指标通常是相对的，其成功依赖于底层数据结构，难以找到数据集中“真正”的聚类。

Method: 定义每个聚类的紧凑性函数和聚类对的相邻点集，用其确定聚类和整体分布的紧凑性与可分离性，应用指标确定聚类的真实数量。

Result: 利用多个合成和真实数据集展示了新指标的性能。

Conclusion: 提出的新型绝对聚类指标可有效确定聚类的紧凑性、可分离性和真实数量。

Abstract: Finding "true" clusters in a data set is a challenging problem. Clustering
solutions obtained using different models and algorithms do not necessarily
provide compact and well-separated clusters or the optimal number of clusters.
Cluster validity indices are commonly applied to identify such clusters.
Nevertheless, these indices are typically relative, and they are used to
compare clustering algorithms or choose the parameters of a clustering
algorithm. Moreover, the success of these indices depends on the underlying
data structure. This paper introduces novel absolute cluster indices to
determine both the compactness and separability of clusters. We define a
compactness function for each cluster and a set of neighboring points for
cluster pairs. This function is utilized to determine the compactness of each
cluster and the whole cluster distribution. The set of neighboring points is
used to define the margin between clusters and the overall distribution margin.
The proposed compactness and separability indices are applied to identify the
true number of clusters. Using a number of synthetic and real-world data sets,
we demonstrate the performance of these new indices and compare them with other
widely-used cluster validity indices.

</details>


### [73] [Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment](https://arxiv.org/abs/2510.13023)
*Joshua R. Tempelman,Adam J. Wachtor,Eric B. Flynn*

Main category: cs.LG

TL;DR: 本文提出端到端机器学习工作流解决自动超声焊缝检测难题，包含降阶建模、分布对齐和分割反演。


<details>
  <summary>Details</summary>
Motivation: 自动超声焊缝检测因训练数据有限和环境波动面临挑战，缺乏端到端解决方案。

Method: 提出含降阶建模、扩散分布对齐和U - Net分割反演的工作流，用降阶Helmholtz模型生成数据集，用迁移学习优化反演模型，用引导扩散处理离群数据。

Result: 该集成框架为真实数据的自动焊缝检测提供端到端解决方案。

Conclusion: 所提工作流有效应对数据管理和信号损坏问题，可用于实际自动焊缝检测。

Abstract: Automated ultrasonic weld inspection remains a significant challenge in the
nondestructive evaluation (NDE) community to factors such as limited training
data (due to the complexity of curating experimental specimens or high-fidelity
simulations) and environmental volatility of many industrial settings
(resulting in the corruption of on-the-fly measurements). Thus, an end-to-end
machine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,
industrial) settings has remained an elusive goal. This work addresses the
challenges of data curation and signal corruption by proposing workflow
consisting of a reduced-order modeling scheme, diffusion based distribution
alignment, and U-Net-based segmentation and inversion. A reduced-order
Helmholtz model based on Lamb wave theory is used to generate a comprehensive
dataset over varying weld heterogeneity and crack defects. The relatively
inexpensive low-order solutions provide a robust training dateset for inversion
models which are refined through a transfer learning stage using a limited set
of full 3D elastodynamic simulations. To handle out-of-distribution (OOD)
real-world measurements with varying and unpredictable noise distributions,
i.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distribution
representations of OOD experimental LDV scans which are subsequently processed
by the inversion models. This integrated framework provides an end-to-end
solution for automated weld inspection on real data.

</details>


### [74] [DeepCausalMMM: A Deep Learning Framework for Marketing Mix Modeling with Causal Inference](https://arxiv.org/abs/2510.13087)
*Aditya Puttaparthi Tirumala*

Main category: cs.LG

TL;DR: 传统营销组合建模（MMM）方法有局限，DeepCausalMMM 包结合深度学习等解决问题，有多项创新。


<details>
  <summary>Details</summary>
Motivation: 传统 MMM 方法难以捕捉复杂时间动态和非线性饱和效应，需新方法解决。

Method: 结合深度学习、因果推断和营销科学，用 GRUs 学习时间模式，通过 DAG 学习统计依赖和因果结构，实现基于 Hill 方程的饱和曲线。

Result: 开发出 DeepCausalMMM 包，有数据驱动设计、多区域建模等多项创新。

Conclusion: DeepCausalMMM 包能有效解决传统 MMM 方法的局限。

Abstract: Marketing Mix Modeling (MMM) is a statistical technique used to estimate the
impact of marketing activities on business outcomes such as sales, revenue, or
customer visits. Traditional MMM approaches often rely on linear regression or
Bayesian hierarchical models that assume independence between marketing
channels and struggle to capture complex temporal dynamics and non-linear
saturation effects [@Hanssens2005; @Ng2021Bayesian].
  DeepCausalMMM is a Python package that addresses these limitations by
combining deep learning, causal inference, and advanced marketing science. The
package uses Gated Recurrent Units (GRUs) to automatically learn temporal
patterns such as adstock (carryover effects) and lag, while simultaneously
learning statistical dependencies and potential causal structures between
marketing channels through Directed Acyclic Graph (DAG) learning
[@Zheng2018NOTEARS; @Gong2024CausalMMM]. Additionally, it implements Hill
equation-based saturation curves to model diminishing returns and optimize
budget allocation.
  Key innovations include: (1) a data-driven design where hyperparameters and
transformations (e.g., adstock decay, saturation curves) are learned or
estimated from data with sensible defaults, rather than requiring fixed
heuristics or manual specification, (2) multi-region modeling with both shared
and region-specific parameters, (3) robust statistical methods including Huber
loss and advanced regularization, (4) comprehensive response curve analysis for
understanding channel saturation, and (5) an extensive visualization suite with
14+ interactive dashboards for business insights.

</details>


### [75] [Information Shapes Koopman Representation](https://arxiv.org/abs/2510.13025)
*Xiaoyuan Cheng,Wenxuan Yuan,Yiming Yang,Yuanzhao Zhang,Sibo Cheng,Yi He,Zhuo Sun*

Main category: cs.LG

TL;DR: 本文从信息瓶颈视角重新思考Koopman学习，提出信息论拉格朗日公式和新算法，实现稳定可解释的Koopman表示，在多动力系统验证有更好性能。


<details>
  <summary>Details</summary>
Motivation: Koopman算子无限维特性使识别有限维子空间困难，源于次优的表示学习。

Method: 提出信息论拉格朗日公式平衡简洁性和表达性，基于此设计新算法。

Result: 定量评估和可视化结果与理论预测一致，在多种动力系统上性能优于现有方法。

Conclusion: 所提方法能实现稳定且可解释的Koopman表示，性能更优。

Abstract: The Koopman operator provides a powerful framework for modeling dynamical
systems and has attracted growing interest from the machine learning community.
However, its infinite-dimensional nature makes identifying suitable
finite-dimensional subspaces challenging, especially for deep architectures. We
argue that these difficulties come from suboptimal representation learning,
where latent variables fail to balance expressivity and simplicity. This
tension is closely related to the information bottleneck (IB) dilemma:
constructing compressed representations that are both compact and predictive.
Rethinking Koopman learning through this lens, we demonstrate that latent
mutual information promotes simplicity, yet an overemphasis on simplicity may
cause latent space to collapse onto a few dominant modes. In contrast,
expressiveness is sustained by the von Neumann entropy, which prevents such
collapse and encourages mode diversity. This insight leads us to propose an
information-theoretic Lagrangian formulation that explicitly balances this
tradeoff. Furthermore, we propose a new algorithm based on the Lagrangian
formulation that encourages both simplicity and expressiveness, leading to a
stable and interpretable Koopman representation. Beyond quantitative
evaluations, we further visualize the learned manifolds under our
representations, observing empirical results consistent with our theoretical
predictions. Finally, we validate our approach across a diverse range of
dynamical systems, demonstrating improved performance over existing Koopman
learning methods. The implementation is publicly available at
https://github.com/Wenxuan52/InformationKoopman.

</details>


### [76] [Neural Triangular Transport Maps: A New Approach Towards Sampling in Lattice QCD](https://arxiv.org/abs/2510.13112)
*Andrey Bryutkin,Youssef Marzouk*

Main category: cs.LG

TL;DR: 提出稀疏三角传输映射解决格点场论玻尔兹曼分布采样难题，分析节点排序影响并与其他方法对比。


<details>
  <summary>Details</summary>
Motivation: 格点场论玻尔兹曼分布采样因多模态和长程相关性有挑战，归一化流应用于大晶格受内存和表达性限制。

Method: 提出利用单调整流神经网络（MRNN）的稀疏三角传输映射，引入综合框架权衡精确稀疏性和近似稀疏性，限制三角映射组件到局部过去。

Result: 以二维φ⁴为控制设置，分析节点排序对三角映射稀疏性和性能的影响。

Conclusion: 未明确提及，但暗示所提方法在解决格点场论采样问题上有潜力，可与现有方法对比评估。

Abstract: Lattice field theories are fundamental testbeds for computational physics;
yet, sampling their Boltzmann distributions remains challenging due to
multimodality and long-range correlations. While normalizing flows offer a
promising alternative, their application to large lattices is often constrained
by prohibitive memory requirements and the challenge of maintaining sufficient
model expressivity. We propose sparse triangular transport maps that explicitly
exploit the conditional independence structure of the lattice graph under
periodic boundary conditions using monotone rectified neural networks (MRNN).
We introduce a comprehensive framework for triangular transport maps that
navigates the fundamental trade-off between \emph{exact sparsity} (respecting
marginal conditional independence in the target distribution) and
\emph{approximate sparsity} (computational tractability without fill-ins).
Restricting each triangular map component to a local past enables site-wise
parallel evaluation and linear time complexity in lattice size $N$, while
preserving the expressive, invertible structure. Using $\phi^4$ in two
dimensions as a controlled setting, we analyze how node labelings (orderings)
affect the sparsity and performance of triangular maps. We compare against
Hybrid Monte Carlo (HMC) and established flow approaches (RealNVP).

</details>


### [77] [Bridging Idealized and Operational Models: An Explainable AI Framework for Earth System Emulators](https://arxiv.org/abs/2510.13030)
*Pouria Behnoudfar,Charlotte Moser,Marc Bocquet,Sibo Cheng,Nan Chen*

Main category: cs.LG

TL;DR: 本文开发可解释AI框架用于地球系统模拟器，结合不同复杂度模型优势，校正CMIP6模拟偏差，强调理想模型开发和模型社区交流的重要性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率业务模型有偏差，不同模型受学科边界限制，需结合不同复杂度模型优势解决问题。

Method: 开发可解释AI框架，通过重新配置的潜在数据同化技术弥合模型层次。

Result: 桥接模型继承业务模型高分辨率和综合变量，通过理想化模型改进实现全局精度提升，显著校正CMIP6模拟厄尔尼诺时空模式偏差。

Conclusion: 强调推动理想模型开发和促进模型社区沟通的重要性。

Abstract: Computer models are indispensable tools for understanding the Earth system.
While high-resolution operational models have achieved many successes, they
exhibit persistent biases, particularly in simulating extreme events and
statistical distributions. In contrast, coarse-grained idealized models isolate
fundamental processes and can be precisely calibrated to excel in
characterizing specific dynamical and statistical features. However, different
models remain siloed by disciplinary boundaries. By leveraging the
complementary strengths of models of varying complexity, we develop an
explainable AI framework for Earth system emulators. It bridges the model
hierarchy through a reconfigured latent data assimilation technique, uniquely
suited to exploit the sparse output from the idealized models. The resulting
bridging model inherits the high resolution and comprehensive variables of
operational models while achieving global accuracy enhancements through
targeted improvements from idealized models. Crucially, the mechanism of AI
provides a clear rationale for these advancements, moving beyond black-box
correction to physically insightful understanding in a computationally
efficient framework that enables effective physics-assisted digital twins and
uncertainty quantification. We demonstrate its power by significantly
correcting biases in CMIP6 simulations of El Ni\~no spatiotemporal patterns,
leveraging statistically accurate idealized models. This work also highlights
the importance of pushing idealized model development and advancing
communication between modeling communities.

</details>


### [78] [Randomness and Interpolation Improve Gradient Descent](https://arxiv.org/abs/2510.13040)
*Jiawen Li,Pascal Lefevre,Anwar Pp Abdul Majeed*

Main category: cs.LG

TL;DR: 本文基于SGD提出IAGD和NRSGD两种优化器，在CIFAR数据集上实验，证明其对SGD的改进效果。


<details>
  <summary>Details</summary>
Motivation: 改进随机梯度下降（SGD）优化器，提高训练收敛速度并避免过拟合。

Method: 提出Interpolational Accelerating Gradient Descent (IAGD)利用二阶牛顿插值加速收敛，提出Noise-Regularized Stochastic Gradient Descent (NRSGD)引入噪声正则化技术避免过拟合，并在CIFAR-10和CIFAR-100数据集上对不同CNN进行对比实验。

Result: 实验表明IAGD和NRSGD两种方法对SGD有改进潜力。

Conclusion: IAGD和NRSGD对SGD的改进是有效的。

Abstract: Based on Stochastic Gradient Descent (SGD), the paper introduces two
optimizers, named Interpolational Accelerating Gradient Descent (IAGD) as well
as Noise-Regularized Stochastic Gradient Descent (NRSGD). IAGD leverages
second-order Newton Interpolation to expedite the convergence process during
training, assuming relevancy in gradients between iterations. To avoid
over-fitting, NRSGD incorporates a noise regularization technique that
introduces controlled noise to the gradients during the optimization process.
Comparative experiments of this research are conducted on the CIFAR-10, and
CIFAR-100 datasets, benchmarking different CNNs(Convolutional Neural Networks)
with IAGD and NRSGD against classical optimizers in Keras Package. Results
demonstrate the potential of those two viable improvement methods in SGD,
implicating the effectiveness of the advancements.

</details>


### [79] [Assessing the robustness of heterogeneous treatment effects in survival analysis under informative censoring](https://arxiv.org/abs/2510.13397)
*Yuxin Wang,Dennis Frauen,Jonas Schweisthal,Maresa Schröder,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文提出假设少的框架评估生存分析中条件平均治疗效果（CATE）估计面对删失偏倚时的稳健性，开发新元学习器估计界限，通过实验和应用证明其价值。


<details>
  <summary>Details</summary>
Motivation: 临床研究中脱落常见，信息性脱落会引入删失偏倚，导致治疗效果估计有偏，现有方法依赖强假设。

Method: 使用部分识别推导CATE的信息性界限，开发新元学习器用任意机器学习模型估计界限。

Result: 通过数值实验和癌症药物试验应用证明元学习器的实用价值。

Conclusion: 该框架为评估存在删失时治疗效果估计的稳健性提供实用工具，促进生存数据在医学和流行病学中可靠使用。

Abstract: Dropout is common in clinical studies, with up to half of patients leaving
early due to side effects or other reasons. When dropout is informative (i.e.,
dependent on survival time), it introduces censoring bias, because of which
treatment effect estimates are also biased. In this paper, we propose an
assumption-lean framework to assess the robustness of conditional average
treatment effect (CATE) estimates in survival analysis when facing censoring
bias. Unlike existing works that rely on strong assumptions, such as
non-informative censoring, to obtain point estimation, we use partial
identification to derive informative bounds on the CATE. Thereby, our framework
helps to identify patient subgroups where treatment is effective despite
informative censoring. We further develop a novel meta-learner that estimates
the bounds using arbitrary machine learning models and with favorable
theoretical properties, including double robustness and quasi-oracle
efficiency. We demonstrate the practical value of our meta-learner through
numerical experiments and in an application to a cancer drug trial. Together,
our framework offers a practical tool for assessing the robustness of estimated
treatment effects in the presence of censoring and thus promotes the reliable
use of survival data for evidence generation in medicine and epidemiology.

</details>


### [80] [An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting](https://arxiv.org/abs/2510.13050)
*Shreya Agrawal,Mohammed Alewi Hassen,Emmanuel Asiedu Brempong,Boris Babenko,Fred Zyda,Olivia Graham,Di Li,Samier Merchant,Santiago Hincapie Potes,Tyler Russell,Danny Cheresnick,Aditya Prakash Kakkirala,Stephan Rasp,Avinatan Hassidim,Yossi Matias,Nal Kalchbrenner,Pramod Gupta,Jason Hickey,Aaron Bell*

Main category: cs.LG

TL;DR: 提出全球机器学习临近预报模型Global MetNet，利用多源数据预测12小时降水，表现出色且能实时部署，已用于谷歌搜索。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报方法有缺陷，基于机器学习的临近预报方法因雷达覆盖稀疏无法用于南半球，需开发适用于全球的临近预报模型。

Method: 利用全球降水任务的CORRA数据集、地球静止卫星数据和全球数值天气预报数据构建Global MetNet模型进行降水预测。

Result: Global MetNet显著优于行业标准小时预报，在数据稀疏地区表现好，关键指标有显著提升，生成预报不到一分钟。

Conclusion: 该模型是减少全球预报质量差异、将稀疏高分辨率卫星观测融入天气预报的关键一步。

Abstract: Precipitation nowcasting, which predicts rainfall up to a few hours ahead, is
a critical tool for vulnerable communities in the Global South frequently
exposed to intense, rapidly developing storms. Timely forecasts provide a
crucial window to protect lives and livelihoods. Traditional numerical weather
prediction (NWP) methods suffer from high latency, low spatial and temporal
resolution, and significant gaps in accuracy across the world. Recent machine
learning-based nowcasting methods, common in the Global North, cannot be
extended to the Global South due to extremely sparse radar coverage. We present
Global MetNet, an operational global machine learning nowcasting model. It
leverages the Global Precipitation Mission's CORRA dataset, geostationary
satellite data, and global NWP data to predict precipitation for the next 12
hours. The model operates at a high resolution of approximately 0.05{\deg}
(~5km) spatially and 15 minutes temporally. Global MetNet significantly
outperforms industry-standard hourly forecasts and achieves significantly
higher skill, making forecasts useful over a much larger area of the world than
previously available. Our model demonstrates better skill in data-sparse
regions than even the best high-resolution NWP models achieve in the US.
Validated using ground radar and satellite data, it shows significant
improvements across key metrics like the critical success index and fractions
skill score for all precipitation rates and lead times. Crucially, our model
generates forecasts in under a minute, making it readily deployable for
real-time applications. It is already deployed for millions of users on Google
Search. This work represents a key step in reducing global disparities in
forecast quality and integrating sparse, high-resolution satellite observations
into weather forecasting.

</details>


### [81] [$L_2$-Regularized Empirical Risk Minimization Guarantees Small Smooth Calibration Error](https://arxiv.org/abs/2510.13450)
*Masahiro Fujisawa,Futoshi Futami*

Main category: cs.LG

TL;DR: 本文首次从理论上证明正则化经验风险最小化能直接控制平滑校准误差，无需事后修正或专门的校准正则化器，还通过实验验证了该结论。


<details>
  <summary>Details</summary>
Motivation: 理解标准训练程序如何产生校准良好的模型。

Method: 建立基于优化误差、正则化强度和拉德马赫复杂度的平滑校准误差有限样本泛化界，并针对再生核希尔伯特空间中的模型推导具体保证。

Result: 实验证实了针对核岭回归和逻辑回归的具体保证，表明L2正则化的经验风险最小化可提供校准良好的模型。

Conclusion: L2正则化经验风险最小化能直接控制平滑校准误差，无需事后修正或专门的校准正则化器。

Abstract: Calibration of predicted probabilities is critical for reliable machine
learning, yet it is poorly understood how standard training procedures yield
well-calibrated models. This work provides the first theoretical proof that
canonical $L_{2}$-regularized empirical risk minimization directly controls the
smooth calibration error (smCE) without post-hoc correction or specialized
calibration-promoting regularizer. We establish finite-sample generalization
bounds for smCE based on optimization error, regularization strength, and the
Rademacher complexity. We then instantiate this theory for models in
reproducing kernel Hilbert spaces, deriving concrete guarantees for kernel
ridge and logistic regression. Our experiments confirm these specific
guarantees, demonstrating that $L_{2}$-regularized ERM can provide a
well-calibrated model without boosting or post-hoc recalibration. The source
code to reproduce all experiments is available at
https://github.com/msfuji0211/erm_calibration.

</details>


### [82] [Time-Varying Optimization for Streaming Data Via Temporal Weighting](https://arxiv.org/abs/2510.13052)
*Muhammad Faraz Ul Abrar,Nicolò Michelusi,Erik G. Larsson*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Classical optimization theory deals with fixed, time-invariant objective
functions. However, time-varying optimization has emerged as an important
subject for decision-making in dynamic environments. In this work, we study the
problem of learning from streaming data through a time-varying optimization
lens. Unlike prior works that focus on generic formulations, we introduce a
structured, \emph{weight-based} formulation that explicitly captures the
streaming-data origin of the time-varying objective, where at each time step,
an agent aims to minimize a weighted average loss over all the past data
samples. We focus on two specific weighting strategies: (1) uniform weights,
which treat all samples equally, and (2) discounted weights, which
geometrically decay the influence of older data. For both schemes, we derive
tight bounds on the ``tracking error'' (TE), defined as the deviation between
the model parameter and the time-varying optimum at a given time step, under
gradient descent (GD) updates. We show that under uniform weighting, the TE
vanishes asymptotically with a $\mathcal{O}(1/t)$ decay rate, whereas
discounted weighting incurs a nonzero error floor controlled by the discount
factor and the number of gradient updates performed at each time step. Our
theoretical findings are validated through numerical simulations.

</details>


### [83] [Asymptotically optimal reinforcement learning in Block Markov Decision Processes](https://arxiv.org/abs/2510.13748)
*Thomas van Vuren,Fiona Sloothaak,Maarten G. Wolf,Jaron Sanders*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The curse of dimensionality renders Reinforcement Learning (RL) impractical
in many real-world settings with exponentially large state and action spaces.
Yet, many environments exhibit exploitable structure that can accelerate
learning. To formalize this idea, we study RL in Block Markov Decision
Processes (BMDPs). BMDPs model problems with large observation spaces, but
where transition dynamics are fully determined by latent states. Recent
advances in clustering methods have enabled the efficient recovery of this
latent structure. However, a regret analysis that exploits these techniques to
determine their impact on learning performance remained open. We are now
addressing this gap by providing a regret analysis that explicitly leverages
clustering, demonstrating that accurate latent state estimation can indeed
effectively speed up learning.
  Concretely, this paper analyzes a two-phase RL algorithm for BMDPs that first
learns the latent structure through random exploration and then switches to an
optimism-guided strategy adapted to the uncovered structure. This algorithm
achieves a regret that is $O(\sqrt{T}+n)$ on a large class of BMDPs susceptible
to clustering. Here, $T$ denotes the number of time steps, $n$ is the
cardinality of the observation space, and the Landau notation $O(\cdot)$ holds
up to constants and polylogarithmic factors. This improves the best prior
bound, $O(\sqrt{T}+n^2)$, especially when $n$ is large. Moreover, we prove that
no algorithm can achieve lower regret uniformly on this same class of BMDPs.
This establishes that, on this class, the algorithm achieves asymptotic
optimality.

</details>


### [84] [NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models](https://arxiv.org/abs/2510.13068)
*Konstantinos Barmpas,Na Lee,Alexandros Koliousis,Yannis Panagakis,Dimitrios A. Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 提出基于码本分词器的可扩展大脑电图模型NeuroRVQ，在下游任务表现优于现有模型，为脑电波模型奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有脑电图基础模型的信号分词模块性能受限，无法保留高频动态，限制信号高保真重建。

Method: 引入以码本分词器为核心的NeuroRVQ，包含多尺度特征提取模块、分层残差向量量化码本和信号相位与幅度感知损失函数。

Result: NeuroRVQ重建误差更低，在多种下游任务中优于现有大脑电图模型。

Conclusion: NeuroRVQ分词器为基于码本的通用脑电波模型建立了强先验，推动神经解码、生成建模和多模态生物信号集成的发展。

Abstract: Electroencephalography (EEG) captures neural activity across multiple
temporal and spectral scales, yielding signals that are rich but complex for
representation learning. Recently, EEG foundation models trained to predict
masked signal-tokens have shown promise for learning generalizable
representations. However, their performance is hindered by their signal
tokenization modules. Existing neural tokenizers fail to preserve
high-frequency dynamics, limiting their ability to reconstruct EEG signals with
high fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM)
centered on a codebook-based tokenizer. Our tokenizer integrates: (i)
multi-scale feature extraction modules that capture the full frequency neural
spectrum; (ii) hierarchical residual vector quantization (RVQ) codebooks for
high-resolution encoding; and, (iii) an EEG signal phase- and amplitude-aware
loss function for efficient training. This design enables efficient EEG
compression while supporting accurate reconstruction across all frequency
bands, leading to robust generative masked modeling. Our empirical results
demonstrate that NeuroRVQ achieves lower reconstruction error and outperforms
existing LBMs on a variety of downstream tasks. More broadly, NeuroRVQ
tokenizer establishes a strong prior for codebook-based general-purpose
brainwave models, enabling advances in neural decoding, generative modeling and
multimodal biosignal integration.

</details>


### [85] [Transformer-based Scalable Beamforming Optimization via Deep Residual Learning](https://arxiv.org/abs/2510.13077)
*Yubo Zhang,Xiao-Yang Liu,Xiaodong Wang*

Main category: cs.LG

TL;DR: 开发用于大规模MU - MISO信道下行波束成形的无监督深度学习框架，经多种策略训练后性能超现有基线且推理更快。


<details>
  <summary>Details</summary>
Motivation: 为大规模MU - MISO信道下行波束成形提供有效方法，实现动态通信环境下实时推理。

Method: 遵循L2O范式，用多层Transformer通过残差连接迭代优化特征，采用课程学习、半摊销学习、滑动窗口训练三种策略增强训练。

Result: 在低到中等SNR时方案性能超现有基线，高SNR时接近WMMSE性能，推理比迭代和在线学习方法快。

Conclusion: 提出的无监督深度学习框架在性能和推理速度上有优势，适用于大规模MU - MISO信道下行波束成形。

Abstract: We develop an unsupervised deep learning framework for downlink beamforming
in large-scale MU-MISO channels. The model is trained offline, allowing
real-time inference through lightweight feedforward computations in dynamic
communication environments. Following the learning-to-optimize (L2O) paradigm,
a multi-layer Transformer iteratively refines both channel and beamformer
features via residual connections. To enhance training, three strategies are
introduced: (i) curriculum learning (CL) to improve early-stage convergence and
avoid local optima, (ii) semi-amortized learning to refine each Transformer
block with a few gradient ascent steps, and (iii) sliding-window training to
stabilize optimization by training only a subset of Transformer blocks at a
time. Extensive simulations show that the proposed scheme outperforms existing
baselines at low-to-medium SNRs and closely approaches WMMSE performance at
high SNRs, while achieving substantially faster inference than iterative and
online learning approaches.

</details>


### [86] [On the Reasoning Abilities of Masked Diffusion Language Models](https://arxiv.org/abs/2510.13117)
*Anej Svete,Ashish Sabharwal*

Main category: cs.LG

TL;DR: 本文研究掩码扩散模型（MDMs）在文本处理中的推理能力和效率，发现其与多项式填充的PLTs等价，能解决CoT增强变压器可解决的问题，且在某些问题上更高效。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型存在不足，MDMs并行生成虽高效，但计算能力和并行性局限未充分探索，故研究其能解决的推理问题类型及效率。

Method: 在有限精度对数宽度设置下，将MDMs与思维链（CoT）和填充循环变压器（PLTs）的推理框架相连接。

Result: MDMs与多项式填充的PLTs等价，能解决CoT增强变压器可解决的所有问题，在包括正则语言等问题上比CoT变压器更高效。

Conclusion: MDMs在文本推理中有独特优势，并行生成可实现更快推理，是传统自回归语言模型的有力替代方案。

Abstract: Masked diffusion models (MDMs) for text offer a compelling alternative to
traditional autoregressive language models. Parallel generation makes them
efficient, but their computational capabilities and the limitations inherent to
their parallelism remain largely unexplored. To this end, we characterize what
types of reasoning problems MDMs can provably solve and how efficiently. We do
this by connecting MDMs to the well-understood reasoning frameworks of chain of
thought (CoT) and padded looped transformers (PLTs) in the finite-precision
log-width setting: We show that MDMs and polynomially-padded PLTs are, in fact,
equivalent in this setting, and that MDMs can solve all problems that
CoT-augmented transformers can. Moreover, we showcase classes of problems
(including regular languages) for which MDMs are inherently more efficient than
CoT transformers, where parallel generation allows for substantially faster
reasoning.

</details>


### [87] [Cluster-Based Client Selection for Dependent Multi-Task Federated Learning in Edge Computing](https://arxiv.org/abs/2510.13132)
*Jieping Luo,Qiyue Li,Zhizhang Liu,Hang Qi,Jiaying Yin,Jingjin Wu*

Main category: cs.LG

TL;DR: 研究移动边缘计算环境下联邦学习的客户端选择问题，提出CoDa - FL框架降低任务完成总时间，实验验证其优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 减少移动边缘计算环境下联邦学习在依赖多任务设置中完成各种学习任务所需的总时间。

Method: 提出CoDa - FL框架，基于地球移动距离进行客户端聚类，推导簇内EMD与收敛所需训练轮数的关系，引入有向无环图任务调度机制。

Result: 通过数值实验，CoDa - FL在异构MEC设置下收敛更快、通信和计算成本更低、学习精度更高。

Conclusion: CoDa - FL框架在解决移动边缘计算环境下联邦学习客户端选择问题上表现出色，优于现有基准。

Abstract: We study the client selection problem in Federated Learning (FL) within
mobile edge computing (MEC) environments, particularly under the dependent
multi-task settings, to reduce the total time required to complete various
learning tasks. We propose CoDa-FL, a Cluster-oriented and Dependency-aware
framework designed to reduce the total required time via cluster-based client
selection and dependent task assignment. Our approach considers Earth Mover's
Distance (EMD) for client clustering based on their local data distributions to
lower computational cost and improve communication efficiency. We derive a
direct and explicit relationship between intra-cluster EMD and the number of
training rounds required for convergence, thereby simplifying the otherwise
complex process of obtaining the optimal solution. Additionally, we incorporate
a directed acyclic graph-based task scheduling mechanism to effectively manage
task dependencies. Through numerical experiments, we validate that our proposed
CoDa-FL outperforms existing benchmarks by achieving faster convergence, lower
communication and computational costs, and higher learning accuracy under
heterogeneous MEC settings.

</details>


### [88] [Convergence, design and training of continuous-time dropout as a random batch method](https://arxiv.org/abs/2510.13134)
*Antonio Álvarez-López,Martín Hernández*

Main category: cs.LG

TL;DR: 本文通过随机批量方法研究连续时间模型中的Dropout正则化，构建无偏估计器，分析轨迹和分布层面的收敛性与稳定性，推导最优参数并在单层神经ODE上验证理论。


<details>
  <summary>Details</summary>
Motivation: 研究连续时间模型中Dropout正则化，利用随机批量方法降低交互粒子系统计算成本。

Method: 构建无偏、适定的估计器，分析轨迹收敛性、分布稳定性，进行Pontryagin伴随分析，比较不同批量采样方案收敛率。

Result: 建立轨迹收敛的线性速率、分布层面的稳定性，推导最优参数，在单层神经ODE上验证理论，观察到预测速率、正则化效果及良好的运行时间和内存表现。

Conclusion: 随机批量方法可有效用于连续时间模型的Dropout正则化研究，理论结果在实际任务中得到验证。

Abstract: We study dropout regularization in continuous-time models through the lens of
random-batch methods -- a family of stochastic sampling schemes originally
devised to reduce the computational cost of interacting particle systems. We
construct an unbiased, well-posed estimator that mimics dropout by sampling
neuron batches over time intervals of length $h$. Trajectory-wise convergence
is established with linear rate in $h$ for the expected uniform error. At the
distribution level, we establish stability for the associated continuity
equation, with total-variation error of order $h^{1/2}$ under mild moment
assumptions. During training with fixed batch sampling across epochs, a
Pontryagin-based adjoint analysis bounds deviations in the optimal cost and
control, as well as in gradient-descent iterates. On the design side, we
compare convergence rates for canonical batch sampling schemes, recover
standard Bernoulli dropout as a special case, and derive a cost--accuracy
trade-off yielding a closed-form optimal $h$. We then specialize to a
single-layer neural ODE and validate the theory on classification and flow
matching, observing the predicted rates, regularization effects, and favorable
runtime and memory profiles.

</details>


### [89] [Behavioral Embeddings of Programs: A Quasi-Dynamic Approach for Optimization Prediction](https://arxiv.org/abs/2510.13158)
*Haolin Pan,Jinyuan Dong,Hongbin Zhang,Hongyu Lin,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: 提出准动态程序表示框架，建模程序优化敏感性，实验表明在编译器优化任务中优于静态基线。


<details>
  <summary>Details</summary>
Motivation: 现有静态和动态程序表示范式各有局限，需突破权衡。

Method: 提出准动态框架，引入程序行为谱，采用组合学习方法，用乘积量化离散化，预训练多任务Transformer模型PQ - BERT。

Result: 在最佳通道预测和 - Oz 收益预测两个编译器优化任务实验中，优于现有静态基线。

Conclusion: 所提准动态框架能有效进行程序表示，可用于编译器优化。

Abstract: Learning effective numerical representations, or embeddings, of programs is a
fundamental prerequisite for applying machine learning to automate and enhance
compiler optimization. Prevailing paradigms, however, present a dilemma. Static
representations, derived from source code or intermediate representation (IR),
are efficient and deterministic but offer limited insight into how a program
will behave or evolve under complex code transformations. Conversely, dynamic
representations, which rely on runtime profiling, provide profound insights
into performance bottlenecks but are often impractical for large-scale tasks
due to prohibitive overhead and inherent non-determinism. This paper transcends
this trade-off by proposing a novel quasi-dynamic framework for program
representation. The core insight is to model a program's optimization
sensitivity. We introduce the Program Behavior Spectrum, a new representation
generated by probing a program's IR with a diverse set of optimization
sequences and quantifying the resulting changes in its static features. To
effectively encode this high-dimensional, continuous spectrum, we pioneer a
compositional learning approach. Product Quantization is employed to discretize
the continuous reaction vectors into structured, compositional sub-words.
Subsequently, a multi-task Transformer model, termed PQ-BERT, is pre-trained to
learn the deep contextual grammar of these behavioral codes. Comprehensive
experiments on two representative compiler optimization tasks -- Best Pass
Prediction and -Oz Benefit Prediction -- demonstrate that our method
outperforms state-of-the-art static baselines. Our code is publicly available
at https://github.com/Panhaolin2001/PREP/.

</details>


### [90] [Universally Invariant Learning in Equivariant GNNs](https://arxiv.org/abs/2510.13169)
*Jiacheng Cen,Anyi Li,Ning Lin,Tingyang Xu,Yu Rong,Deli Zhao,Zihe Wang,Wenbing Huang*

Main category: cs.LG

TL;DR: 提出构建完整等变图神经网络的高效实用框架，基于EGNN和TFN模型构建算法，减少计算开销且表现良好。


<details>
  <summary>Details</summary>
Motivation: 先前方法在实现等变图神经网络完整性时计算成本高且无多项式时间解决方案，需要更高效实用的方法。

Method: 证明完整等变图神经网络可通过完整标量函数和满秩可转向基集实现，基于EGNN和TFN模型提出构建算法。

Result: 模型仅需几层就有卓越完整性和良好性能，显著降低计算开销。

Conclusion: 所提框架能高效构建完整等变图神经网络，兼具理论基础和实践效果。

Abstract: Equivariant Graph Neural Networks (GNNs) have demonstrated significant
success across various applications. To achieve completeness -- that is, the
universal approximation property over the space of equivariant functions -- the
network must effectively capture the intricate multi-body interactions among
different nodes. Prior methods attain this via deeper architectures, augmented
body orders, or increased degrees of steerable features, often at high
computational cost and without polynomial-time solutions. In this work, we
present a theoretically grounded framework for constructing complete
equivariant GNNs that is both efficient and practical. We prove that a complete
equivariant GNN can be achieved through two key components: 1) a complete
scalar function, referred to as the canonical form of the geometric graph; and
2) a full-rank steerable basis set. Leveraging this finding, we propose an
efficient algorithm for constructing complete equivariant GNNs based on two
common models: EGNN and TFN. Empirical results demonstrate that our model
demonstrates superior completeness and excellent performance with only a few
layers, thereby significantly reducing computational overhead while maintaining
strong practical efficacy.

</details>


### [91] [Information-Theoretic Criteria for Knowledge Distillation in Multimodal Learning](https://arxiv.org/abs/2510.13182)
*Rongrong Xie,Yizhou Xu,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 针对跨模态知识蒸馏效果不佳问题，提出跨模态互补假设（CCH），理论验证并实证，建立理论框架并给出选教师模态准则。


<details>
  <summary>Details</summary>
Motivation: 跨模态知识蒸馏虽有应用但因理论理解有限不总能提升效果，需解决此问题。

Method: 提出跨模态互补假设（CCH），在联合高斯模型中理论验证，在多种多模态数据集上实证。

Result: 理论验证了CCH，在多种多模态数据集上实证确认。

Conclusion: 建立了理解跨模态KD的新理论框架，基于CCH准则给出选择最优教师模态的实用指南。

Abstract: The rapid increase in multimodal data availability has sparked significant
interest in cross-modal knowledge distillation (KD) techniques, where richer
"teacher" modalities transfer information to weaker "student" modalities during
model training to improve performance. However, despite successes across
various applications, cross-modal KD does not always result in improved
outcomes, primarily due to a limited theoretical understanding that could
inform practice. To address this gap, we introduce the Cross-modal
Complementarity Hypothesis (CCH): we propose that cross-modal KD is effective
when the mutual information between teacher and student representations exceeds
the mutual information between the student representation and the labels. We
theoretically validate the CCH in a joint Gaussian model and further confirm it
empirically across diverse multimodal datasets, including image, text, video,
audio, and cancer-related omics data. Our study establishes a novel theoretical
framework for understanding cross-modal KD and offers practical guidelines
based on the CCH criterion to select optimal teacher modalities for improving
the performance of weaker modalities.

</details>


### [92] [CleverCatch: A Knowledge-Guided Weak Supervision Model for Fraud Detection](https://arxiv.org/abs/2510.13205)
*Amirhossein Mozafari,Kourosh Hashemi,Erfan Shafagh,Soroush Motamedi,Azar Taheri Tayebi,Mohammad A. Tayebi*

Main category: cs.LG

TL;DR: 提出知识引导的弱监督模型CleverCatch检测医疗欺诈处方行为，在真实数据集上表现优于基线，嵌入专家规则可提升准确性与透明度。


<details>
  <summary>Details</summary>
Motivation: 医疗欺诈检测面临标记数据有限、欺诈策略多变和医疗记录高维等挑战，传统监督和无监督方法有局限。

Method: 将结构化领域知识集成到神经网络架构，在合成数据上联合训练编码器学习软规则嵌入。

Result: 在大规模真实数据集上，CleverCatch在AUC上平均提升1.3%，召回率提升3.4%，消融研究凸显专家规则的互补作用。

Conclusion: 嵌入专家规则到学习过程可提高检测准确性和透明度，为医疗欺诈检测等领域提供可解释方法。

Abstract: Healthcare fraud detection remains a critical challenge due to limited
availability of labeled data, constantly evolving fraud tactics, and the high
dimensionality of medical records. Traditional supervised methods are
challenged by extreme label scarcity, while purely unsupervised approaches
often fail to capture clinically meaningful anomalies. In this work, we
introduce CleverCatch, a knowledge-guided weak supervision model designed to
detect fraudulent prescription behaviors with improved accuracy and
interpretability. Our approach integrates structured domain expertise into a
neural architecture that aligns rules and data samples within a shared
embedding space. By training encoders jointly on synthetic data representing
both compliance and violation, CleverCatch learns soft rule embeddings that
generalize to complex, real-world datasets. This hybrid design enables
data-driven learning to be enhanced by domain-informed constraints, bridging
the gap between expert heuristics and machine learning. Experiments on the
large-scale real-world dataset demonstrate that CleverCatch outperforms four
state-of-the-art anomaly detection baselines, yielding average improvements of
1.3\% in AUC and 3.4\% in recall. Our ablation study further highlights the
complementary role of expert rules, confirming the adaptability of the
framework. The results suggest that embedding expert rules into the learning
process not only improves detection accuracy but also increases transparency,
offering an interpretable approach for high-stakes domains such as healthcare
fraud detection.

</details>


### [93] [Performance Evaluation of Ising and QUBO Variable Encodings in Boltzmann Machine Learning](https://arxiv.org/abs/2510.13210)
*Yasushi Hasegawa,Masayuki Ohzeki*

Main category: cs.LG

TL;DR: 比较Boltzmann机学习中Ising和QUBO编码，发现QUBO编码在SGD下收敛慢，NGD在两种编码下收敛相似，给出编码和预处理建议。


<details>
  <summary>Details</summary>
Motivation: 研究Boltzmann机学习中不同编码（Ising和QUBO）对学习动态的影响。

Method: 在固定模型、采样器和步长的协议下比较两种编码，利用Fisher信息矩阵等于充分统计量协方差的特性可视化经验矩。

Result: QUBO编码在FIM中产生更多小特征值方向，降低谱熵，导致SGD收敛慢；NGD在两种编码下收敛相似；Ising编码在SGD训练中收敛更快，QUBO可通过中心化/缩放或NGD风格预处理缓解曲率问题。

Conclusion: 明确了表示方式对Boltzmann机信息几何和有限时间学习动态的影响，给出变量编码和预处理的实用指南。

Abstract: We compare Ising ({-1,+1}) and QUBO ({0,1}) encodings for Boltzmann machine
learning under a controlled protocol that fixes the model, sampler, and step
size. Exploiting the identity that the Fisher information matrix (FIM) equals
the covariance of sufficient statistics, we visualize empirical moments from
model samples and reveal systematic, representation-dependent differences. QUBO
induces larger cross terms between first- and second-order statistics, creating
more small-eigenvalue directions in the FIM and lowering spectral entropy. This
ill-conditioning explains slower convergence under stochastic gradient descent
(SGD). In contrast, natural gradient descent (NGD)-which rescales updates by
the FIM metric-achieves similar convergence across encodings due to
reparameterization invariance. Practically, for SGD-based training, the Ising
encoding provides more isotropic curvature and faster convergence; for QUBO,
centering/scaling or NGD-style preconditioning mitigates curvature pathologies.
These results clarify how representation shapes information geometry and
finite-time learning dynamics in Boltzmann machines and yield actionable
guidelines for variable encoding and preprocessing.

</details>


### [94] [Towards Understanding Valuable Preference Data for Large Language Model Alignment](https://arxiv.org/abs/2510.13212)
*Zizhuo Zhang,Qizhou Wang,Shanshan Ye,Jianing Zhu,Jiangchao Yao,Bo Han,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出截断影响函数评估数据质量，引入简单计分函数并结合形成数据选择规则，实验表明用更少数据实现更好对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究未考察单个数据点是否真正有益，需改进偏好数据选择方法以适配特定模型。

Method: 用截断影响函数评估数据质量，引入两个计分函数，将计分函数结合形成数据选择规则。

Result: 在不同对齐基准和大语言模型家族实验中，用更少数据实现更好对齐性能。

Conclusion: 提出的方法具有通用性，能实现对有价值偏好数据更精确选择。

Abstract: Large language model (LLM) alignment is typically achieved through learning
from human preference comparisons, making the quality of preference data
critical to its success. Existing studies often pre-process raw training
datasets to identify valuable preference pairs using external reward models or
off-the-shelf LLMs, achieving improved overall performance but rarely examining
whether individual, selected data point is genuinely beneficial. We assess data
quality through individual influence on validation data using our newly
proposed truncated influence function (TIF), which mitigates the over-scoring
present in traditional measures and reveals that preference data quality is
inherently a property of the model. In other words, a data pair that benefits
one model may harm another. This leaves the need to improve the preference data
selection approaches to be adapting to specific models. To this end, we
introduce two candidate scoring functions (SFs) that are computationally
simpler than TIF and positively correlated with it. They are also model
dependent and can serve as potential indicators of individual data quality for
preference data selection. Furthermore, we observe that these SFs inherently
exhibit errors when compared to TIF. To this end, we combine them to offset
their diverse error sources, resulting in a simple yet effective data selection
rule that enables the models to achieve a more precise selection of valuable
preference data. We conduct experiments across diverse alignment benchmarks and
various LLM families, with results demonstrating that better alignment
performance can be achieved using less data, showing the generality of our
findings and new methods.

</details>


### [95] [Rethinking Graph Domain Adaptation: A Spectral Contrastive Perspective](https://arxiv.org/abs/2510.13254)
*Haoyu Zhang,Yuxuan Cheng,Wenqi Fan,Yulong Chen,Yifan Zhang*

Main category: cs.LG

TL;DR: 传统GNN在领域适应方面存在问题，本文提出FracNet进行频率感知的领域适应，有理论证明且实验效果好。


<details>
  <summary>Details</summary>
Motivation: 传统GNN因结构分布转移和可迁移模式探索不足，在领域适应上表现不佳，且未区分全局和局部模式。

Method: 提出FracNet，包含两个协同模块将原始图分解为高低频分量进行频率感知的领域适应，结合对比学习框架改善模糊边界问题，并给出理论证明。

Result: 实验表明相比现有方法有显著改进。

Conclusion: FracNet在图神经网络的领域适应任务中具有优越性。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in various
domains, yet they often struggle with domain adaptation due to significant
structural distribution shifts and insufficient exploration of transferable
patterns. One of the main reasons behind this is that traditional approaches do
not treat global and local patterns discriminatingly so that some local details
in the graph may be violated after multi-layer GNN. Our key insight is that
domain shifts can be better understood through spectral analysis, where
low-frequency components often encode domain-invariant global patterns, and
high-frequency components capture domain-specific local details. As such, we
propose FracNet (\underline{\textbf{Fr}}equency \underline{\textbf{A}}ware
\underline{\textbf{C}}ontrastive Graph \underline{\textbf{Net}}work) with two
synergic modules to decompose the original graph into high-frequency and
low-frequency components and perform frequency-aware domain adaption. Moreover,
the blurring boundary problem of domain adaptation is improved by integrating
with a contrastive learning framework. Besides the practical implication, we
also provide rigorous theoretical proof to demonstrate the superiority of
FracNet. Extensive experiments further demonstrate significant improvements
over state-of-the-art approaches.

</details>


### [96] [Hypernetworks for Perspectivist Adaptation](https://arxiv.org/abs/2510.13259)
*Daniil Ignatev,Denis Paperno,Massimo Poesio*

Main category: cs.LG

TL;DR: 本文应用超网络+适配器组合架构解决视角感知分类中参数效率瓶颈问题，方案参数少且架构无关。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分认识视角感知分类中参数效率的瓶颈问题，需解决该问题。

Method: 将超网络+适配器组合架构应用于视角分类。

Result: 得到的解决方案在仇恨言论和毒性检测中采用用户视角时能与专业模型竞争，且使用参数少。

Conclusion: 该解决方案架构无关，可直接应用于多种基础模型。

Abstract: The task of perspective-aware classification introduces a bottleneck in terms
of parametric efficiency that did not get enough recognition in existing
studies. In this article, we aim to address this issue by applying an existing
architecture, the hypernetwork+adapters combination, to perspectivist
classification. Ultimately, we arrive at a solution that can compete with
specialized models in adopting user perspectives on hate speech and toxicity
detection, while also making use of considerably fewer parameters. Our solution
is architecture-agnostic and can be applied to a wide range of base models out
of the box.

</details>


### [97] [BlendFL: Blended Federated Learning for Handling Multimodal Data Heterogeneity](https://arxiv.org/abs/2510.13266)
*Alejandro Guerra-Manzanares,Omar El-Herraoui,Michail Maniatakos,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出BlendFL框架解决跨客户端多模态数据异质性问题，在多分类任务中表现出色，加速协作学习。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架在处理多模态数据异质性问题时，仅适用于理想场景，无法应对客户端数据不对称情况。

Method: 提出BlendFL框架，融合水平和垂直联邦学习原则，具备去中心化推理机制，引入BlendAvg全局模型聚合策略。

Result: 在多模态医疗数据集和基准测试中，BlendFL在多模态和单模态分类任务中表现优于现有方法，收敛更快。

Conclusion: BlendFL在医疗和金融等注重数据隐私的现实场景中，处理多模态数据异质性的协作学习具有潜力。

Abstract: One of the key challenges of collaborative machine learning, without data
sharing, is multimodal data heterogeneity in real-world settings. While
Federated Learning (FL) enables model training across multiple clients,
existing frameworks, such as horizontal and vertical FL, are only effective in
`ideal' settings that meet specific assumptions. Hence, they struggle to
address scenarios where neither all modalities nor all samples are represented
across the participating clients. To address this gap, we propose BlendFL, a
novel FL framework that seamlessly blends the principles of horizontal and
vertical FL in a synchronized and non-restrictive fashion despite the asymmetry
across clients. Specifically, any client within BlendFL can benefit from either
of the approaches, or both simultaneously, according to its available dataset.
In addition, BlendFL features a decentralized inference mechanism, empowering
clients to run collaboratively trained local models using available local data,
thereby reducing latency and reliance on central servers for inference. We also
introduce BlendAvg, an adaptive global model aggregation strategy that
prioritizes collaborative model updates based on each client's performance. We
trained and evaluated BlendFL and other state-of-the-art baselines on three
classification tasks using a large-scale real-world multimodal medical dataset
and a popular multimodal benchmark. Our results highlight BlendFL's superior
performance for both multimodal and unimodal classification. Ablation studies
demonstrate BlendFL's faster convergence compared to traditional approaches,
accelerating collaborative learning. Overall, in our study we highlight the
potential of BlendFL for handling multimodal data heterogeneity for
collaborative learning in real-world settings where data privacy is crucial,
such as in healthcare and finance.

</details>


### [98] [To Steer or Not to Steer? Mechanistic Error Reduction with Abstention for Language Models](https://arxiv.org/abs/2510.13290)
*Anna Hedström,Salim I. Amoukou,Tom Bewley,Saumitra Mishra,Manuela Veloso*

Main category: cs.LG

TL;DR: 提出MERA框架用于引导语言模型减少错误，在多数据集和模型族实验中表现良好，还可增强现有技术性能。


<details>
  <summary>Details</summary>
Motivation: 现有依赖固定手动调整引导强度的方法存在欠引导或过度引导问题，需改进。

Method: 优化干预方向，校准引导时机和强度，可在无把握修正时弃权。

Result: 在不同数据集和语言模型族实验中，实现安全、有效、无性能下降的纠错，优于现有基线。

Conclusion: MERA是一种通用且高效的机制激活引导方法，还能增强现有引导技术性能。

Abstract: We introduce Mechanistic Error Reduction with Abstention (MERA), a principled
framework for steering language models (LMs) to mitigate errors through
selective, adaptive interventions. Unlike existing methods that rely on fixed,
manually tuned steering strengths, often resulting in under or oversteering,
MERA addresses these limitations by (i) optimising the intervention direction,
and (ii) calibrating when, and how much to steer, thereby provably improving
performance or abstaining when no confident correction is possible. Experiments
across diverse datasets, and LM families demonstrate safe, effective,
non-degrading error correction, and that MERA outperforms existing baselines.
Moreover, MERA can be applied on top of existing steering techniques to further
enhance their performance, establishing it as a general-purpose, and efficient
approach to mechanistic activation steering.

</details>


### [99] [Federated Conditional Conformal Prediction via Generative Models](https://arxiv.org/abs/2510.13297)
*Rui Xu,Sihong Xie*

Main category: cs.LG

TL;DR: 提出Federated Conditional Conformal Prediction (Fed - CCP)，用生成模型实现条件覆盖，实验表明其能实现更自适应的预测集。


<details>
  <summary>Details</summary>
Motivation: 标准CP假设数据独立同分布，在联邦学习中不适用，现有联邦CP方法无法反映输入条件不确定性，因此需要新方法适应局部数据异质性。

Method: 提出Fed - CCP，利用生成模型（如归一化流或扩散模型）近似条件数据分布，各客户端本地校准共形分数，并通过联邦聚合保持全局一致性。

Result: 在真实数据集上的实验表明，Fed - CCP实现了更自适应的预测集。

Conclusion: Fed - CCP能适应局部数据异质性，实现条件覆盖，在联邦学习中有较好效果。

Abstract: Conformal Prediction (CP) provides distribution-free uncertainty
quantification by constructing prediction sets that guarantee coverage of the
true labels. This reliability makes CP valuable for high-stakes federated
learning scenarios such as multi-center healthcare. However, standard CP
assumes i.i.d. data, which is violated in federated settings where client
distributions differ substantially. Existing federated CP methods address this
by maintaining marginal coverage on each client, but such guarantees often fail
to reflect input-conditional uncertainty. In this work, we propose Federated
Conditional Conformal Prediction (Fed-CCP) via generative models, which aims
for conditional coverage that adapts to local data heterogeneity. Fed-CCP
leverages generative models, such as normalizing flows or diffusion models, to
approximate conditional data distributions without requiring the sharing of raw
data. This enables each client to locally calibrate conformal scores that
reflect its unique uncertainty, while preserving global consistency through
federated aggregation. Experiments on real datasets demonstrate that Fed-CCP
achieves more adaptive prediction sets.

</details>


### [100] [Km-scale dynamical downscaling through conformalized latent diffusion models](https://arxiv.org/abs/2510.13301)
*Alessandro Brusaferri,Andrea Ballarino*

Main category: cs.LG

TL;DR: 本文提出用共形预测框架增强下采样流程，解决生成扩散模型在动力降尺度中不确定性估计不可靠问题，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型在动力降尺度中缺乏有限样本保证，导致不确定性估计不可靠，影响其在实际应用中的可靠性。

Method: 用共形预测框架增强下采样流程，对扩散模型样本进行后处理以推导条件分位数估计，并纳入共形化分位数回归过程。

Result: 在意大利ERA5再分析数据上实验，结果显示该方法显著改善了格点级不确定性估计的覆盖率，概率得分更稳定。

Conclusion: 共形化生成模型在高分辨率气象场概率降尺度方面有潜力，能实现更可靠的预测。

Abstract: Dynamical downscaling is crucial for deriving high-resolution meteorological
fields from coarse-scale simulations, enabling detailed analysis for critical
applications such as weather forecasting and renewable energy modeling.
Generative Diffusion models (DMs) have recently emerged as powerful data-driven
tools for this task, offering reconstruction fidelity and more scalable
sampling supporting uncertainty quantification. However, DMs lack finite-sample
guarantees against overconfident predictions, resulting in miscalibrated
grid-point-level uncertainty estimates hindering their reliability in
operational contexts. In this work, we tackle this issue by augmenting the
downscaling pipeline with a conformal prediction framework. Specifically, the
DM's samples are post-processed to derive conditional quantile estimates,
incorporated into a conformalized quantile regression procedure targeting
locally adaptive prediction intervals with finite-sample marginal validity. The
proposed approach is evaluated on ERA5 reanalysis data over Italy, downscaled
to a 2-km grid. Results demonstrate grid-point-level uncertainty estimates with
markedly improved coverage and stable probabilistic scores relative to the DM
baseline, highlighting the potential of conformalized generative models for
more trustworthy probabilistic downscaling to high-resolution meteorological
fields.

</details>


### [101] [Isolation-based Spherical Ensemble Representations for Anomaly Detection](https://arxiv.org/abs/2510.13311)
*Yang Cao,Sikun Yang,Hao Tian,Kai He,Lianyong Qi,Ming Liu,Yujiu Yang*

Main category: cs.LG

TL;DR: 提出ISER方法解决现有无监督异常检测方法的问题，实验表明其性能优于11种基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法存在分布假设冲突、计算效率低和难处理不同异常类型等问题。

Method: 提出ISER，用超球半径表示局部密度特征，构建集成表示，引入基于相似度的评分方法，还改进了Isolation Forest。

Result: 在22个真实数据集上的综合实验显示，ISER性能优于11种基线方法。

Conclusion: ISER有效解决了现有无监督异常检测方法的问题，具有更好的性能。

Abstract: Anomaly detection is a critical task in data mining and management with
applications spanning fraud detection, network security, and log monitoring.
Despite extensive research, existing unsupervised anomaly detection methods
still face fundamental challenges including conflicting distributional
assumptions, computational inefficiency, and difficulty handling different
anomaly types. To address these problems, we propose ISER (Isolation-based
Spherical Ensemble Representations) that extends existing isolation-based
methods by using hypersphere radii as proxies for local density characteristics
while maintaining linear time and constant space complexity. ISER constructs
ensemble representations where hypersphere radii encode density information:
smaller radii indicate dense regions while larger radii correspond to sparse
areas. We introduce a novel similarity-based scoring method that measures
pattern consistency by comparing ensemble representations against a theoretical
anomaly reference pattern. Additionally, we enhance the performance of
Isolation Forest by using ISER and adapting the scoring function to address
axis-parallel bias and local anomaly detection limitations. Comprehensive
experiments on 22 real-world datasets demonstrate ISER's superior performance
over 11 baseline methods.

</details>


### [102] [RockNet: Distributed Learning on Ultra-Low-Power Devices](https://arxiv.org/abs/2510.13320)
*Alexander Gräfe,Fabian Mager,Marco Zimmerling,Sebastian Trimpe*

Main category: cs.LG

TL;DR: 本文提出适用于超低功耗硬件的TinyML方法RockNet，实现时间序列分类的先进精度，通过分布式学习克服通信瓶颈，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习融入网络物理系统（CPS），因隐私和延迟问题，训练从云端转向设备端，但超低功耗微控制器计算资源有限，训练困难。

Method: 设计结合机器学习和无线通信的分布式学习方法，利用所有设备分布式训练专用且计算高效的分类器，结合定制高效的无线多跳通信协议。

Result: 在20个超低功耗设备的测试平台上实验，RockNet从零开始学习时间序列分类任务，精度最高达最新方法的2倍，分布式架构可将每设备内存、延迟和能耗降低90%。

Conclusion: 分布式机器学习、分布式计算和通信的紧密集成，首次实现了在超低功耗硬件上以先进精度进行训练。

Abstract: As Machine Learning (ML) becomes integral to Cyber-Physical Systems (CPS),
there is growing interest in shifting training from traditional cloud-based to
on-device processing (TinyML), for example, due to privacy and latency
concerns. However, CPS often comprise ultra-low-power microcontrollers, whose
limited compute resources make training challenging. This paper presents
RockNet, a new TinyML method tailored for ultra-low-power hardware that
achieves state-of-the-art accuracy in timeseries classification, such as fault
or malware detection, without requiring offline pretraining. By leveraging that
CPS consist of multiple devices, we design a distributed learning method that
integrates ML and wireless communication. RockNet leverages all devices for
distributed training of specialized compute efficient classifiers that need
minimal communication overhead for parallelization. Combined with tailored and
efficient wireless multi-hop communication protocols, our approach overcomes
the communication bottleneck that often occurs in distributed learning.
Hardware experiments on a testbed with 20 ultra-low-power devices demonstrate
RockNet's effectiveness. It successfully learns timeseries classification tasks
from scratch, surpassing the accuracy of the latest approach for neural network
microcontroller training by up to 2x. RockNet's distributed ML architecture
reduces memory, latency and energy consumption per device by up to 90 % when
scaling from one central device to 20 devices. Our results show that a tight
integration of distributed ML, distributed computing, and communication
enables, for the first time, training on ultra-low-power hardware with
state-of-the-art accuracy.

</details>


### [103] [When In Doubt, Abstain: The Impact of Abstention on Strategic Classification](https://arxiv.org/abs/2510.13327)
*Lina Alkarmi,Ziyuan Huang,Mingyan Liu*

Main category: cs.LG

TL;DR: 研究算法决策中分类器弃权在策略分类环境下的作用，发现最优弃权能保障主体效用，还可抑制策略性操纵。


<details>
  <summary>Details</summary>
Motivation: 算法决策易受策略性操纵，此前研究表明分类器弃权可提高准确性，研究其在策略分类环境中的影响及主体如何最优利用它。

Method: 将交互建模为Stackelberg博弈，主体先宣布决策策略，策略性代理再操纵特征；聚焦二元分类器，代理操纵可观测特征。

Result: 最优弃权确保主体效用不比无弃权设置差；弃权除提高准确性，还能威慑操纵，让操纵成本高时操纵更难。

Conclusion: 弃权是减少算法决策系统中策略性行为负面影响的有价值工具。

Abstract: Algorithmic decision making is increasingly prevalent, but often vulnerable
to strategic manipulation by agents seeking a favorable outcome. Prior research
has shown that classifier abstention (allowing a classifier to decline making a
decision due to insufficient confidence) can significantly increase classifier
accuracy. This paper studies abstention within a strategic classification
context, exploring how its introduction impacts strategic agents' responses and
how principals should optimally leverage it. We model this interaction as a
Stackelberg game where a principal, acting as the classifier, first announces
its decision policy, and then strategic agents, acting as followers, manipulate
their features to receive a desired outcome. Here, we focus on binary
classifiers where agents manipulate observable features rather than their true
features, and show that optimal abstention ensures that the principal's utility
(or loss) is no worse than in a non-abstention setting, even in the presence of
strategic agents. We also show that beyond improving accuracy, abstention can
also serve as a deterrent to manipulation, making it costlier for agents,
especially those less qualified, to manipulate to achieve a positive outcome
when manipulation costs are significant enough to affect agent behavior. These
results highlight abstention as a valuable tool for reducing the negative
effects of strategic behavior in algorithmic decision making systems.

</details>


### [104] [Thompson Sampling via Fine-Tuning of LLMs](https://arxiv.org/abs/2510.13328)
*Nicolas Menet,Aleksandar Terzić,Andreas Krause,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出基于汤普森采样的可扩展方法ToSFiT用于大的无结构离散空间贝叶斯优化，理论推导了遗憾界，实验验证其在多任务上提升样本效率且不影响计算效率。


<details>
  <summary>Details</summary>
Motivation: 大的无结构离散空间中贝叶斯优化因无梯度，最大化采集函数计算成本高。

Method: 提出基于汤普森采样的ToSFiT方法，利用大语言模型先验知识并向与验渐进适配。

Result: 理论上推导了汤普森采样变体的遗憾界；实验验证在线微调显著提升样本效率，对计算效率影响可忽略。

Conclusion: ToSFiT方法有效，通过适配后验概率提升了采样效率，适用于多种任务。

Abstract: Bayesian optimization in large unstructured discrete spaces is often hindered
by the computational cost of maximizing acquisition functions due to the
absence of gradients. We propose a scalable alternative based on Thompson
sampling that eliminates the need for acquisition function maximization by
directly parameterizing the probability that a candidate yields the maximum
reward. Our approach, Thompson Sampling via Fine-Tuning (ToSFiT) leverages the
prior knowledge embedded in prompt-conditioned large language models, and
incrementally adapts them toward the posterior. Theoretically, we derive a
novel regret bound for a variational formulation of Thompson Sampling that
matches the strong guarantees of its standard counterpart. Our analysis reveals
the critical role of careful adaptation to the posterior probability of
maximality--a principle that underpins our ToSFiT algorithm. Empirically, we
validate our method on three diverse tasks: FAQ response refinement, thermally
stable protein search, and quantum circuit design. We demonstrate that online
fine-tuning significantly improves sample efficiency, with negligible impact on
computational efficiency.

</details>


### [105] [Kernel Representation and Similarity Measure for Incomplete Data](https://arxiv.org/abs/2510.13352)
*Yang Cao,Sikun Yang,Kai He,Wenjun Ma,Ming Liu,Yujiu Yang,Jian Weng*

Main category: cs.LG

TL;DR: 本文提出接近核这一相似性度量方法，可直接计算不完整数据在核特征空间中的相似度，在12个真实数据集上表现优于现有方法且具线性时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统处理不完整数据的方法会导致信息丢失和相似度估计偏差，需要新的相似性度量方法。

Method: 提出接近核，采用依赖数据的分箱和接近分配将数据投影到高维稀疏表示，用级联回退策略处理缺失值。

Result: 在12个真实世界不完整数据集的聚类任务中，性能优于现有方法，且保持线性时间复杂度。

Conclusion: 接近核是一种有效计算不完整数据相似度的方法，代码开源可获取。

Abstract: Measuring similarity between incomplete data is a fundamental challenge in
web mining, recommendation systems, and user behavior analysis. Traditional
approaches either discard incomplete data or perform imputation as a
preprocessing step, leading to information loss and biased similarity
estimates. This paper presents the proximity kernel, a new similarity measure
that directly computes similarity between incomplete data in kernel feature
space without explicit imputation in the original space. The proposed method
introduces data-dependent binning combined with proximity assignment to project
data into a high-dimensional sparse representation that adapts to local density
variations. For missing value handling, we propose a cascading fallback
strategy to estimate missing feature distributions. We conduct clustering tasks
on the proposed kernel representation across 12 real world incomplete datasets,
demonstrating superior performance compared to existing methods while
maintaining linear time complexity. All the code are available at
https://anonymous.4open.science/r/proximity-kernel-2289.

</details>


### [106] [Generalist++: A Meta-learning Framework for Mitigating Trade-off in Adversarial Training](https://arxiv.org/abs/2510.13361)
*Yisen Wang,Yichuan Mo,Hongjun Wang,Junyi Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出Generalist框架解决对抗训练局限性，降低泛化误差并缓解权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练存在自然准确率下降和鲁棒性跨攻击转移不佳的局限，此前工作多只解决单一问题。

Method: 将整体泛化目标划分为多个子任务分配给专门的基础学习器，后期插值参数形成全局学习器，并定期重新分配全局参数。

Result: 理论分析和实验表明，Generalist比基线方法有更低的泛化误差，显著缓解权衡问题。

Conclusion: Generalist为未来开发完全鲁棒的分类器提供了有前景的一步。

Abstract: Despite the rapid progress of neural networks, they remain highly vulnerable
to adversarial examples, for which adversarial training (AT) is currently the
most effective defense. While AT has been extensively studied, its practical
applications expose two major limitations: natural accuracy tends to degrade
significantly compared with standard training, and robustness does not transfer
well across attacks crafted under different norm constraints. Unlike prior
works that attempt to address only one issue within a single network, we
propose to partition the overall generalization goal into multiple sub-tasks,
each assigned to a dedicated base learner. By specializing in its designated
objective, each base learner quickly becomes an expert in its field. In the
later stages of training, we interpolate their parameters to form a
knowledgeable global learner, while periodically redistributing the global
parameters back to the base learners to prevent their optimization trajectories
from drifting too far from the shared target. We term this framework Generalist
and introduce three variants tailored to different application scenarios. Both
theoretical analysis and extensive experiments demonstrate that Generalist
achieves lower generalization error and significantly alleviates the trade-off
problems compared with baseline methods. Our results suggest that Generalist
provides a promising step toward developing fully robust classifiers in the
future.

</details>


### [107] [A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control](https://arxiv.org/abs/2510.13367)
*Nikita Kachaev,Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovelev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 本文表明transformers可作为在线无模型强化学习连续控制的强大基线，研究关键设计问题并得出稳定策略和实用指导。


<details>
  <summary>Details</summary>
Motivation: transformers在在线无模型强化学习中因对训练设置和模型设计决策敏感而未被充分探索，作者希望证明其在该领域的有效性。

Method: 研究输入条件设定、行动者与评判者间组件共享以及训练时序列数据切片等关键设计问题。

Result: 实验揭示了稳定的架构和训练策略，在全可观测和部分可观测任务以及基于向量和图像的设置中均能实现有竞争力的性能。

Conclusion: 研究结果为transformers在在线强化学习中的应用提供了实用指导。

Abstract: Despite their effectiveness and popularity in offline or model-based
reinforcement learning (RL), transformers remain underexplored in online
model-free RL due to their sensitivity to training setups and model design
decisions such as how to structure the policy and value networks, share
components, or handle temporal information. In this paper, we show that
transformers can be strong baselines for continuous control in online
model-free RL. We investigate key design questions: how to condition inputs,
share components between actor and critic, and slice sequential data for
training. Our experiments reveal stable architectural and training strategies
enabling competitive performance across fully and partially observable tasks,
and in both vector- and image-based settings. These findings offer practical
guidance for applying transformers in online RL.

</details>


### [108] [Contrastive Learning-Based Dependency Modeling for Anomaly Detection in Cloud Services](https://arxiv.org/abs/2510.13368)
*Yue Xing,Yingnan Deng,Heyao Liu,Ming Wang,Yun Zi,Xiaoxuan Sun*

Main category: cs.LG

TL;DR: 提出集成对比学习的依赖建模与异常检测方法用于云服务环境，实验显示该方法性能优于现有方法，具适应性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 应对云服务环境中复杂依赖和多样异常模式的挑战。

Method: 将服务交互抽象为依赖图，通过嵌入函数提取特征，用图卷积聚合信息；引入对比学习框架构建正负样本对；设计时间一致性约束；结合对比损失和时间一致性损失进行优化。

Result: 在关键指标上显著优于现有方法，在稀疏标注、监测噪声和流量波动条件下保持鲁棒性。

Conclusion: 验证了依赖建模与对比学习结合的有效性，为云服务异常检测提供完整技术方案，在复杂环境有强适应性和稳定性。

Abstract: This paper addresses the challenges of complex dependencies and diverse
anomaly patterns in cloud service environments by proposing a dependency
modeling and anomaly detection method that integrates contrastive learning. The
method abstracts service interactions into a dependency graph, extracts
temporal and structural features through embedding functions, and employs a
graph convolution mechanism to aggregate neighborhood information for
context-aware service representations. A contrastive learning framework is then
introduced, constructing positive and negative sample pairs to enhance the
separability of normal and abnormal patterns in the representation space.
Furthermore, a temporal consistency constraint is designed to maintain
representation stability across time steps and reduce the impact of short-term
fluctuations and noise. The overall optimization combines contrastive loss and
temporal consistency loss to ensure stable and reliable detection across
multi-dimensional features. Experiments on public datasets systematically
evaluate the method from hyperparameter, environmental, and data sensitivity
perspectives. Results show that the proposed approach significantly outperforms
existing methods on key metrics such as Precision, Recall, F1-Score, and AUC,
while maintaining robustness under conditions of sparse labeling, monitoring
noise, and traffic fluctuations. This study verifies the effectiveness of
integrating dependency modeling with contrastive learning, provides a complete
technical solution for cloud service anomaly detection, and demonstrates strong
adaptability and stability in complex environments.

</details>


### [109] [Prediction Markets with Intermittent Contributions](https://arxiv.org/abs/2510.13385)
*Michael Vitali,Pierre Pinson*

Main category: cs.LG

TL;DR: 本文提出基于预测市场的框架，设计了考虑多因素的预测市场并引入收益分配机制，通过案例研究证明其有效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 数据可用性和准确预测需求增加，但利益相关者合作受数据所有权和竞争利益限制，要提出新的预测框架。

Method: 引入并分析一个预测市场，采用稳健回归模型处理缺失提交，引入考虑样本内外表现的收益分配机制。

Result: 通过模拟和真实世界数据的案例研究，证明了所提出市场设计的有效性和适应性。

Conclusion: 所提出的基于预测市场的设计是有效的且具有适应性，能在考虑多种因素下实现准确预测。

Abstract: Although both data availability and the demand for accurate forecasts are
increasing, collaboration between stakeholders is often constrained by data
ownership and competitive interests. In contrast to recent proposals within
cooperative game-theoretical frameworks, we place ourselves in a more general
framework, based on prediction markets. There, independent agents trade
forecasts of uncertain future events in exchange for rewards. We introduce and
analyse a prediction market that (i) accounts for the historical performance of
the agents, (ii) adapts to time-varying conditions, while (iii) permitting
agents to enter and exit the market at will. The proposed design employs robust
regression models to learn the optimal forecasts' combination whilst handling
missing submissions. Moreover, we introduce a pay-off allocation mechanism that
considers both in-sample and out-of-sample performance while satisfying several
desirable economic properties. Case-studies using simulated and real-world data
allow demonstrating the effectiveness and adaptability of the proposed market
design.

</details>


### [110] [SWIR-LightFusion: Multi-spectral Semantic Fusion of Synthetic SWIR with {Thermal} IR {(LWIR/MWIR)} and RGB](https://arxiv.org/abs/2510.13404)
*Muhammad Ishfaq Hussain,Ma Van Linh,Zubia Naz,Unse Fatima,Yeongmin Ko,Moongu Jeon*

Main category: cs.LG

TL;DR: 本文针对恶劣能见度下场景理解难题，提出从LWIR数据合成SWIR图像并进行多模态融合的方法，实验证明该方法提升融合图像质量且能实时处理，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统成像模态融合在恶劣条件下难以提供全面场景信息，SWIR成像有优势但缺乏公开数据集。

Method: 用先进对比度增强技术从LWIR数据合成SWIR图像，提出集成合成SWIR、LWIR和RGB模态的多模态融合框架，采用优化的编码器 - 解码器神经网络架构。

Result: 在公共和私有数据集上实验表明，该融合框架提升了融合图像质量并保持实时性能。

Conclusion: 该方法在监控和自主系统等实际应用中有巨大潜力。

Abstract: Enhancing scene understanding in adverse visibility conditions remains a
critical challenge for surveillance and autonomous navigation systems.
Conventional imaging modalities, such as RGB and thermal infrared (MWIR /
LWIR), when fused, often struggle to deliver comprehensive scene information,
particularly under conditions of atmospheric interference or inadequate
illumination. To address these limitations, Short-Wave Infrared (SWIR) imaging
has emerged as a promising modality due to its ability to penetrate atmospheric
disturbances and differentiate materials with improved clarity. However, the
advancement and widespread implementation of SWIR-based systems face
significant hurdles, primarily due to the scarcity of publicly accessible SWIR
datasets. In response to this challenge, our research introduces an approach to
synthetically generate SWIR-like structural/contrast cues (without claiming
spectral reproduction) images from existing LWIR data using advanced contrast
enhancement techniques. We then propose a multimodal fusion framework
integrating synthetic SWIR, LWIR, and RGB modalities, employing an optimized
encoder-decoder neural network architecture with modality-specific encoders and
a softmax-gated fusion head. Comprehensive experiments on public {RGB-LWIR
benchmarks (M3FD, TNO, CAMEL, MSRS, RoadScene) and an additional private real
RGB-MWIR-SWIR dataset} demonstrate that our synthetic-SWIR-enhanced fusion
framework improves fused-image quality (contrast, edge definition, structural
fidelity) while maintaining real-time performance. We also add fair trimodal
baselines (LP, LatLRR, GFF) and cascaded trimodal variants of
U2Fusion/SwinFusion under a unified protocol. The outcomes highlight
substantial potential for real-world applications in surveillance and
autonomous systems.

</details>


### [111] [Optimizing Storage Overhead of User Behavior Log for ML-embedded Mobile Apps](https://arxiv.org/abs/2510.13405)
*Chen Gong,Yan Zhuang,Zhenzhe Zheng,Yiliu Chen,Sheng Wang,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: 提出AdaLog系统解决ML嵌入式移动应用用户行为日志存储瓶颈，评估显示能减少日志大小且系统开销小。


<details>
  <summary>Details</summary>
Motivation: ML驱动服务流行，记录用户行为数据带来存储成本问题，导致系统响应低和应用卸载增加。

Method: 将消除特征级冗余数据建模为超图最大加权匹配问题并提出分层算法；采用虚拟哈希属性设计分配异构行为；设计增量更新机制。

Result: 在真实用户数据评估中，AdaLog减少行为日志大小19% - 44%，系统开销仅2秒延迟和15MB内存使用。

Conclusion: AdaLog提供更高效数据基础，利于设备端ML广泛应用。

Abstract: Machine learning (ML) models are increasingly integrated into modern mobile
apps to enable personalized and intelligent services. These models typically
rely on rich input features derived from historical user behaviors to capture
user intents. However, as ML-driven services become more prevalent, recording
necessary user behavior data imposes substantial storage cost on mobile apps,
leading to lower system responsiveness and more app uninstalls. To address this
storage bottleneck, we present AdaLog, a lightweight and adaptive system
designed to improve the storage efficiency of user behavior log in ML-embedded
mobile apps, without compromising model inference accuracy or latency. We
identify two key inefficiencies in current industrial practices of user
behavior log: (i) redundant logging of overlapping behavior data across
different features and models, and (ii) sparse storage caused by storing
behaviors with heterogeneous attribute descriptions in a single log file. To
solve these issues, AdaLog first formulates the elimination of feature-level
redundant data as a maximum weighted matching problem in hypergraphs, and
proposes a hierarchical algorithm for efficient on-device deployment. Then,
AdaLog employs a virtually hashed attribute design to distribute heterogeneous
behaviors into a few log files with physically dense storage. Finally, to
ensure scalability to dynamic user behavior patterns, AdaLog designs an
incremental update mechanism to minimize the I/O operations needed for adapting
outdated behavior log. We implement a prototype of AdaLog and deploy it into
popular mobile apps in collaboration with our industry partner. Evaluations on
real-world user data show that AdaLog reduces behavior log size by 19% to 44%
with minimal system overhead (only 2 seconds latency and 15 MB memory usage),
providing a more efficient data foundation for broader adoption of on-device
ML.

</details>


### [112] [When Embedding Models Meet: Procrustes Bounds and Applications](https://arxiv.org/abs/2510.13406)
*Lucas Maystre,Alvaro Ortega Gonzalez,Charles Park,Rares Dolga,Tudor Berariu,Yu Zhao,Kamil Ciosek*

Main category: cs.LG

TL;DR: 研究嵌入模型互操作性，提出Procrustes后处理方法使模型可互操作，在多应用中有效。


<details>
  <summary>Details</summary>
Motivation: 相似数据训练的嵌入模型缺乏互操作性，在实际应用中带来挑战。

Method: 研究两组嵌入能否通过正交变换对齐，若近似保留成对点积则存在等距映射，提出Procrustes后处理方法。

Result: 该方法使两个嵌入模型可互操作，在三个应用中达到了最先进的性能。

Conclusion: Procrustes后处理方法能解决嵌入模型互操作性问题，在多个应用场景中有效。

Abstract: Embedding models trained separately on similar data often produce
representations that encode stable information but are not directly
interchangeable. This lack of interoperability raises challenges in several
practical applications, such as model retraining, partial model upgrades, and
multimodal search. Driven by these challenges, we study when two sets of
embeddings can be aligned by an orthogonal transformation. We show that if
pairwise dot products are approximately preserved, then there exists an
isometry that closely aligns the two sets, and we provide a tight bound on the
alignment error. This insight yields a simple alignment recipe, Procrustes
post-processing, that makes two embedding models interoperable while preserving
the geometry of each embedding space. Empirically, we demonstrate its
effectiveness in three applications: maintaining compatibility across
retrainings, combining different models for text retrieval, and improving
mixed-modality search, where it achieves state-of-the-art performance.

</details>


### [113] [Modeling Adoptive Cell Therapy in Bladder Cancer from Sparse Biological Data using PINNs](https://arxiv.org/abs/2510.13431)
*Kayode Olumoyin,Katarzyna Rejniak*

Main category: cs.LG

TL;DR: 本文提出一种应用于肿瘤学的物理信息神经网络（PINN）框架，嵌入归纳偏差和生物约束，用少量训练样本学习联合治疗动力学，算法收敛性好。


<details>
  <summary>Details</summary>
Motivation: 肿瘤学实验数据稀疏，需学习肿瘤微环境中联合治疗的时变相互作用。

Method: 扩展PINN，嵌入从动力学系统先验信息导出的归纳偏差，将观察到的生物约束作为正则化因子。

Result: 算法能求解常微分方程（ODE），得出部分ODE模型参数的时变形式，用MSE、MAE和MAPE等指标显示出强收敛性。

Conclusion: 改进的PINN算法能利用少量训练样本找到合理解决方案并实现良好泛化。

Abstract: Physics-informed neural networks (PINNs) are neural networks that embed the
laws of dynamical systems modeled by differential equations into their loss
function as constraints. In this work, we present a PINN framework applied to
oncology. Here, we seek to learn time-varying interactions due to a combination
therapy in a tumor microenvironment. In oncology, experimental data are often
sparse and composed of a few time points of tumor volume. By embedding
inductive biases derived from prior information about a dynamical system, we
extend the physics-informed neural networks (PINN) and incorporate observed
biological constraints as regularization agents. The modified PINN algorithm is
able to steer itself to a reasonable solution and can generalize well with only
a few training examples. We demonstrate the merit of our approach by learning
the dynamics of treatment applied intermittently in an ordinary differential
equation (ODE) model of a combination therapy. The algorithm yields a solution
to the ODE and time-varying forms of some of the ODE model parameters. We
demonstrate a strong convergence using metrics such as the mean squared error
(MSE), mean absolute error (MAE), and mean absolute percentage error (MAPE).

</details>


### [114] [Hybrid Interval Type-2 Mamdani-TSK Fuzzy System for Regression Analysis](https://arxiv.org/abs/2510.13437)
*Ashish Bhatia,Renato Cordeiro de Amorim,Vito De Feo*

Main category: cs.LG

TL;DR: 本文提出一种新颖的模糊回归方法，结合Mamdani系统可解释性与TSK模型精确性，在基准数据集评估中表现出色，平衡了可解释性与准确性。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法难以处理现实数据复杂性，深度学习方法缺乏可解释性且小数据集易过拟合，模糊系统虽可处理不确定性但存在可解释性与准确性的权衡问题。

Method: 提出结合Mamdani系统可解释性与TSK模型精确性的模糊回归方法，引入含模糊和清晰组件及双主导类型的混合规则结构。

Result: 在6个测试数据集里，该方法在4个数据集获最佳模糊方法评分，在2个数据集优于不透明模型，在1个数据集获最佳总体评分，RMSE改善范围0.4% - 19%。

Conclusion: 该混合方法是预测建模的平衡且通用工具，解决了模糊系统中可解释性与准确性的权衡问题。

Abstract: Regression analysis is employed to examine and quantify the relationships
between input variables and a dependent and continuous output variable. It is
widely used for predictive modelling in fields such as finance, healthcare, and
engineering. However, traditional methods often struggle with real-world data
complexities, including uncertainty and ambiguity. While deep learning
approaches excel at capturing complex non-linear relationships, they lack
interpretability and risk over-fitting on small datasets. Fuzzy systems provide
an alternative framework for handling uncertainty and imprecision, with Mamdani
and Takagi-Sugeno-Kang (TSK) systems offering complementary strengths:
interpretability versus accuracy. This paper presents a novel fuzzy regression
method that combines the interpretability of Mamdani systems with the precision
of TSK models. The proposed approach introduces a hybrid rule structure with
fuzzy and crisp components and dual dominance types, enhancing both accuracy
and explainability. Evaluations on benchmark datasets demonstrate
state-of-the-art performance in several cases, with rules maintaining a
component similar to traditional Mamdani systems while improving precision
through improved rule outputs. This hybrid methodology offers a balanced and
versatile tool for predictive modelling, addressing the trade-off between
interpretability and accuracy inherent in fuzzy systems. In the 6 datasets
tested, the proposed approach gave the best fuzzy methodology score in 4
datasets, out-performed the opaque models in 2 datasets and produced the best
overall score in 1 dataset with the improvements in RMSE ranging from 0.4% to
19%.

</details>


### [115] [Rectify and Align GPS Points to Parking Spots via Rank-1 Constraint](https://arxiv.org/abs/2510.13439)
*Jiaxing Deng,Junbiao Pang,Zhicheng Wang,Haitao Yu*

Main category: cs.LG

TL;DR: 本文提出无监督低秩方法校正停车场GPS点误差并对齐，实验证明其优越性，数据集和代码公开。


<details>
  <summary>Details</summary>
Motivation: 高层建筑使GPS点漂移，低成本GPS设备有误差，需无监督方法校正大量停车场GPS点。

Method: 基于停车场物理约束，提出无监督低秩方法，在统一框架中校正GPS点误差并对齐。

Result: 实验证明该方法在解决实际问题上有优越性。

Conclusion: 所提非常规校正和对齐方法简单有效，适用于任何类型GPS点误差。

Abstract: Parking spots are essential components, providing vital mobile resources for
residents in a city. Accurate Global Positioning System (GPS) points of parking
spots are the core data for subsequent applications,e.g., parking management,
parking policy, and urban development. However, high-rise buildings tend to
cause GPS points to drift from the actual locations of parking spots; besides,
the standard lower-cost GPS equipment itself has a certain location error.
Therefore, it is a non-trivial task to correct a few wrong GPS points from a
large number of parking spots in an unsupervised approach. In this paper,
motivated by the physical constraints of parking spots (i.e., parking spots are
parallel to the sides of roads), we propose an unsupervised low-rank method to
effectively rectify errors in GPS points and further align them to the parking
spots in a unified framework. The proposed unconventional rectification and
alignment method is simple and yet effective for any type of GPS point errors.
Extensive experiments demonstrate the superiority of the proposed method to
solve a practical problem. The data set and the code are publicly accessible
at:https://github.com/pangjunbiao/ITS-Parking-spots-Dataset.

</details>


### [116] [Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers](https://arxiv.org/abs/2510.13444)
*Nico Pelleriti,Christoph Spiegel,Shiwei Liu,David Martínez-Rubio,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 本文提出首个学习增强算法来验证多项式的平方和（SOS）准则，通过训练Transformer模型预测单项式基，在200多个基准数据集上验证，比现有求解器快超100倍。


<details>
  <summary>Details</summary>
Motivation: 验证多项式非负性是NP难问题，验证SOS准则计算成本高，需求解维数随单项式基大小二次增长的半定规划（SDP），因此要降低单项式基大小。

Method: 训练Transformer模型预测给定多项式的几乎最小单项式基，整体方法包括生成超1亿个SOS多项式的高效训练数据集、设计和训练Transformer架构、建立确保正确终止的系统回退机制。

Result: 在超200个基准数据集上验证，比现有求解器快超100倍，能解决竞争方法失败的实例。

Conclusion: 研究为提高SOS编程的实际可扩展性提供了新见解。

Abstract: Certifying nonnegativity of polynomials is a well-known NP-hard problem with
direct applications spanning non-convex optimization, control, robotics, and
beyond. A sufficient condition for nonnegativity is the Sum of Squares (SOS)
property, i.e., it can be written as a sum of squares of other polynomials. In
practice, however, certifying the SOS criterion remains computationally
expensive and often involves solving a Semidefinite Program (SDP), whose
dimensionality grows quadratically in the size of the monomial basis of the SOS
expression; hence, various methods to reduce the size of the monomial basis
have been proposed. In this work, we introduce the first learning-augmented
algorithm to certify the SOS criterion. To this end, we train a Transformer
model that predicts an almost-minimal monomial basis for a given polynomial,
thereby drastically reducing the size of the corresponding SDP. Our overall
methodology comprises three key components: efficient training dataset
generation of over 100 million SOS polynomials, design and training of the
corresponding Transformer architecture, and a systematic fallback mechanism to
ensure correct termination, which we analyze theoretically. We validate our
approach on over 200 benchmark datasets, achieving speedups of over $100\times$
compared to state-of-the-art solvers and enabling the solution of instances
where competing approaches fail. Our findings provide novel insights towards
transforming the practical scalability of SOS programming.

</details>


### [117] [Towards Blackwell Optimality: Bellman Optimality Is All You Can Get](https://arxiv.org/abs/2510.13476)
*Victor Boone,Adrienne Tuynman*

Main category: cs.LG

TL;DR: 研究马尔可夫决策过程中不同最优性策略的识别问题，构建学习算法，刻画有限时间停止的MDP类，给出可行停止规则。


<details>
  <summary>Details</summary>
Motivation: 平均增益最优性在马尔可夫决策过程中过于渐近，结合即时损失度量引出偏差最优性层次，旨在识别该层次中的最优策略。

Method: 为每个最优性阶构建具有消失错误概率的学习算法。

Result: 刻画了识别算法能在有限时间停止的MDP类，该类对应具有唯一贝尔曼最优策略的MDP，且与考虑的最优性阶无关；给出了与学习算法结合时能在可能情况下有限时间触发的可行停止规则。

Conclusion: 提出了有效的学习算法和停止规则来识别马尔可夫决策过程中不同最优性阶的策略。

Abstract: Although average gain optimality is a commonly adopted performance measure in
Markov Decision Processes (MDPs), it is often too asymptotic. Further
incorporating measures of immediate losses leads to the hierarchy of bias
optimalities, all the way up to Blackwell optimality. In this paper, we
investigate the problem of identifying policies of such optimality orders. To
that end, for each order, we construct a learning algorithm with vanishing
probability of error. Furthermore, we characterize the class of MDPs for which
identification algorithms can stop in finite time. That class corresponds to
the MDPs with a unique Bellman optimal policy, and does not depend on the
optimality order considered. Lastly, we provide a tractable stopping rule that
when coupled to our learning algorithm triggers in finite time whenever it is
possible to do so.

</details>


### [118] [Tahakom LLM guidelines and receipts: from pre-training data to an Arabic LLM](https://arxiv.org/abs/2510.13481)
*Areej AlOtaibi,Lina Alyahya,Raghad Alshabanah,Shahad Alfawzan,Shuruq Alarefei,Reem Alsabti,Nouf Alsubaie,Abdulaziz Alhuzaymi,Lujain Alkhelb,Majd Alsayari,Waad Alahmed,Omar Talabay,Jalal Alowibdi,Salem Alelyani,Adel Bibi*

Main category: cs.LG

TL;DR: 本文聚焦开发阿拉伯语大语言模型（LLMs）的挑战，探讨数据管理、分词器设计和评估等方面，分享数据和方法以推动阿拉伯语语言建模发展。


<details>
  <summary>Details</summary>
Motivation: 因大语言模型在自然语言处理领域取得显著进展，但开发阿拉伯语大语言模型存在独特挑战，故进行研究。

Method: 详细阐述阿拉伯语预训练数据集的收集与过滤方法，评估不同分词器设计对模型性能的影响，分析现有阿拉伯语评估框架的局限性并提出纠正方法。

Result: 对阿拉伯语大语言模型开发的关键方面进行了研究，并提出了针对评估框架的纠正方法。

Conclusion: 分享数据和方法有助于促进透明度和协作开发，推动语言建模尤其是阿拉伯语语言建模的发展。

Abstract: Large Language Models (LLMs) have significantly advanced the field of natural
language processing, enhancing capabilities in both language understanding and
generation across diverse domains. However, developing LLMs for Arabic presents
unique challenges. This paper explores these challenges by focusing on critical
aspects such as data curation, tokenizer design, and evaluation. We detail our
approach to the collection and filtration of Arabic pre-training datasets,
assess the impact of various tokenizer designs on model performance, and
examine the limitations of existing Arabic evaluation frameworks, for which we
propose a systematic corrective methodology. To promote transparency and
facilitate collaborative development, we share our data and methodologies,
contributing to the advancement of language modeling, particularly for the
Arabic language.

</details>


### [119] [DistilCLIP-EEG: Enhancing Epileptic Seizure Detection Through Multi-modal Learning and Knowledge Distillation](https://arxiv.org/abs/2510.13497)
*Zexin Wang,Lin Shi,Haoyu Wu,Junru Luo,Xiangzeng Kong,Jun Qi*

Main category: cs.LG

TL;DR: 提出基于CLIP框架的多模态模型DistilCLIP - EEG用于癫痫检测，引入知识蒸馏方法，在多个数据集上表现良好，学生模型降低复杂度和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习癫痫检测方法多依赖单模态EEG信号，忽略多模态信息优势。

Method: 提出DistilCLIP - EEG模型，结合EEG信号和文本描述，采用基于Conformer架构的EEG编码器和Learnable BERT进行提示学习，引入知识蒸馏方法。

Result: 教师和学生模型在多个数据集上准确率超97%，F1 - 分数超0.94，学生模型参数和大小约为教师模型的58.1%。

Conclusion: 所提模型在基于EEG的癫痫检测中有潜力，为资源受限场景部署轻量级模型奠定基础。

Abstract: Epilepsy is a prevalent neurological disorder marked by sudden, brief
episodes of excessive neuronal activity caused by abnormal electrical
discharges, which may lead to some mental disorders. Most existing deep
learning methods for epilepsy detection rely solely on unimodal EEG signals,
neglecting the potential benefits of multimodal information. To address this,
we propose a novel multimodal model, DistilCLIP-EEG, based on the CLIP
framework, which integrates both EEG signals and text descriptions to capture
comprehensive features of epileptic seizures. The model involves an EEG encoder
based on the Conformer architecture as a text encoder, the proposed Learnable
BERT (BERT-LP) as prompt learning within the encoders. Both operate in a shared
latent space for effective cross-modal representation learning. To enhance
efficiency and adaptability, we introduce a knowledge distillation method where
the trained DistilCLIP-EEG serves as a teacher to guide a more compact student
model to reduce training complexity and time. On the TUSZ, AUBMC, and CHB-MIT
datasets, both the teacher and student models achieved accuracy rates exceeding
97%. Across all datasets, the F1-scores were consistently above 0.94,
demonstrating the robustness and reliability of the proposed framework.
Moreover, the student model's parameter count and model size are approximately
58.1% of those of the teacher model, significantly reducing model complexity
and storage requirements while maintaining high performance. These results
highlight the potential of our proposed model for EEG-based epilepsy detection
and establish a solid foundation for deploying lightweight models in
resource-constrained settings.

</details>


### [120] [Offline and Online KL-Regularized RLHF under Differential Privacy](https://arxiv.org/abs/2510.13512)
*Yulian Wu,Rushil Thareja,Praneeth Vepakomma,Francesco Orabona*

Main category: cs.LG

TL;DR: 研究在人类偏好标签的ε局部差分隐私模型下，带KL正则化的离线和在线人类反馈强化学习，给出离线算法及次优性差距、在线算法及对数遗憾界，并实现离线算法验证理论结果。


<details>
  <summary>Details</summary>
Motivation: 研究在ε局部差分隐私模型下，将广泛用于大语言模型对齐的KL正则化目标函数应用于人类反馈强化学习的离线和在线设置。

Method: 离线设置基于悲观原则设计算法，在线设置基于乐观原则设计算法。

Result: 离线设置得到新的次优性差距及匹配下界；在线设置得到对数遗憾界，还暗示了无隐私的在线KL正则化人类反馈强化学习的首次分析。

Conclusion: 通过理论推导和离线算法实现验证了研究成果，开源代码可获取。

Abstract: In this paper, we study the offline and online settings of reinforcement
learning from human feedback (RLHF) with KL-regularization -- a widely used
objective function in large language model alignment -- under the $\epsilon$
local differential privacy ($\epsilon$-LDP) model on the label of the human
preference. In the offline setting, we design an algorithm based on the
principle of pessimism and derive a new suboptimality gap of
$\tilde{O}(1/[(e^\epsilon-1)^2 n])$ on the KL-regularized objective under
single-policy concentrability. We also prove its optimality by providing a
matching lower bound where $n$ is the sample size.
  In the online setting, we are the first one to theoretically investigate the
problem of KL-regularized RLHF with LDP. We design an optimism-based algorithm
and derive a logarithmic regret bound of $O(d_{\mathcal{F}}\log
(N_{\mathcal{F}}\cdot T) /(e^\epsilon-1)^2 )$, where $T$ is the total time
step, $N_{\mathcal{F}}$ is cardinality of the reward function space
$\mathcal{F}$ and $d_{\mathcal{F}}$ is a variant of eluder dimension for RLHF.
As a by-product of our analysis, our results also imply the first analysis for
online KL-regularized RLHF without privacy. We implement our algorithm in the
offline setting to verify our theoretical results and release our open source
code at: https://github.com/rushil-thareja/PPKL-RLHF-Official.

</details>


### [121] [K-Merge: Online Continual Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2510.13537)
*Donald Shenaj,Ondrej Bohdal,Taha Ceritli,Mete Ozay,Pietro Zanuttigh,Umberto Michieli*

Main category: cs.LG

TL;DR: 本文提出一种数据无关且计算高效的策略，用于在新的低秩适配器（LoRA）可用时进行选择和合并，实验证明该方法优于其他策略。


<details>
  <summary>Details</summary>
Motivation: 解决移动设备存储容量有限的问题，应对LoRA增量交付时的在线持续合并挑战，即在纳入新LoRA时保留对先前支持任务的性能。

Method: 提出一种数据无关且计算高效的策略，用于在新LoRA可用时进行选择和合并，假设设备只能存储有限数量的适配器。

Result: 在实际任务的大量实验中，该方法优于其他策略，且符合设备端的存储预算和计算限制。

Conclusion: 所提出的策略在设备端在线持续合并LoRA方面表现出色，能在有限资源下有效整合新的LoRA并保留旧任务性能。

Abstract: On-device deployment of Large Language Models (LLMs) frequently leverages
Low-Rank Adapters (LoRAs) to support diverse downstream tasks under tight
resource constraints. To address the limited storage capacity of mobile
devices, recent works have explored model merging techniques to fuse multiple
LoRAs into a single one. In practice, however, LoRAs are often delivered
incrementally, as users request support for new tasks (e.g., novel problem
types or languages). This scenario introduces a new challenge: on-device online
continual merging, where the objective is to incorporate new LoRAs while
preserving the performance on previously supported tasks. In this paper, we
propose a data-free and computationally efficient strategy for selecting and
merging LoRAs when a new one becomes available, assuming the device can store
only a limited number of adapters. Extensive experiments across real-world
tasks demonstrate the superiority of our approach compared to alternative
strategies while adhering to the storage budget and compute limitations of
on-device settings.

</details>


### [122] [ProtoTopic: Prototypical Network for Few-Shot Medical Topic Modeling](https://arxiv.org/abs/2510.13542)
*Martin Licht,Sara Ketabi,Farzad Khalvati*

Main category: cs.LG

TL;DR: 本文提出ProtoTopic用于医学论文摘要主题生成，在数据有限情况下相比基线模型有更好表现。


<details>
  <summary>Details</summary>
Motivation: 现有主题建模技术应用于医学文本效果不佳，因医疗领域部分主题可用文档数量少。

Method: 提出基于原型网络的主题模型ProtoTopic，通过计算输入数据点与一组原型表示之间的距离进行预测。

Result: 与文献中的两个主题建模基线相比，ProtoTopic提高了主题连贯性和多样性。

Conclusion: ProtoTopic模型即使在数据有限的情况下也能生成医学相关主题。

Abstract: Topic modeling is a useful tool for analyzing large corpora of written
documents, particularly academic papers. Despite a wide variety of proposed
topic modeling techniques, these techniques do not perform well when applied to
medical texts. This can be due to the low number of documents available for
some topics in the healthcare domain. In this paper, we propose ProtoTopic, a
prototypical network-based topic model used for topic generation for a set of
medical paper abstracts. Prototypical networks are efficient, explainable
models that make predictions by computing distances between input datapoints
and a set of prototype representations, making them particularly effective in
low-data or few-shot learning scenarios. With ProtoTopic, we demonstrate
improved topic coherence and diversity compared to two topic modeling baselines
used in the literature, demonstrating the ability of our model to generate
medically relevant topics even with limited data.

</details>


### [123] [Multi-Objective $\textit{min-max}$ Online Convex Optimization](https://arxiv.org/abs/2510.13560)
*Rahul Vaze,Sumiran Mishra*

Main category: cs.LG

TL;DR: 本文拓展在线凸优化至多目标场景，考虑min - max遗憾，在i.i.d.输入设置下提出结合Hedge和OGD的简单算法，证明其期望min - max遗憾为O(√(T log K))。


<details>
  <summary>Details</summary>
Motivation: 拓宽在线凸优化的研究范围，考虑多目标在线凸优化问题，处理多个不同损失函数序列的权衡。

Method: 在i.i.d.输入设置下，提出结合著名的Hedge和在线梯度下降（OGD）的简单算法。

Result: 证明所提算法的期望min - max遗憾为O(√(T log K))。

Conclusion: 所提出的结合Hedge和OGD的算法在多目标在线凸优化的i.i.d.输入设置下能有效控制min - max遗憾。

Abstract: In online convex optimization (OCO), a single loss function sequence is
revealed over a time horizon of $T$, and an online algorithm has to choose its
action at time $t$, before the loss function at time $t$ is revealed. The goal
of the online algorithm is to incur minimal penalty (called $\textit{regret}$
compared to a static optimal action made by an optimal offline algorithm
knowing all functions of the sequence in advance.
  In this paper, we broaden the horizon of OCO, and consider multi-objective
OCO, where there are $K$ distinct loss function sequences, and an algorithm has
to choose its action at time $t$, before the $K$ loss functions at time $t$ are
revealed. To capture the tradeoff between tracking the $K$ different sequences,
we consider the $\textit{min-max}$ regret, where the benchmark (optimal offline
algorithm) takes a static action across all time slots that minimizes the
maximum of the total loss (summed across time slots) incurred by each of the
$K$ sequences. An online algorithm is allowed to change its action across time
slots, and its {\it min-max} regret is defined as the difference between its
$\textit{min-max}$ cost and that of the benchmark. The $\textit{min-max}$
regret is a stringent performance measure and an algorithm with small regret
needs to `track' all loss function sequences closely at all times.
  We consider this $\textit{min-max}$ regret in the i.i.d. input setting where
all loss functions are i.i.d. generated from an unknown distribution. For the
i.i.d. model we propose a simple algorithm that combines the well-known
$\textit{Hedge}$ and online gradient descent (OGD) and show via a remarkably
simple proof that its expected $\textit{min-max}$ regret is $O(\sqrt{T \log
K})$.

</details>


### [124] [DOLFIN: Balancing Stability and Plasticity in Federated Continual Learning](https://arxiv.org/abs/2510.13567)
*Omayma Moussadek,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: 提出DOLFIN方法用于联邦持续学习，在多个数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前联邦持续学习方法在平衡性能、隐私保护和通信效率方面存在挑战。

Method: 引入DOLFIN方法，结合视觉Transformer和低秩适配器，利用LoRA减少通信开销，采用DualGPM防止遗忘。

Result: 在CIFAR - 100等数据集的两种狄利克雷异质性设置下，DOLFIN最终平均准确率超过六个强基线，内存占用相当。

Conclusion: 正交低秩适配器为联邦环境下的隐私保护持续学习提供了有效且可扩展的解决方案。

Abstract: Federated continual learning (FCL) enables models to learn new tasks across
multiple distributed clients, protecting privacy and without forgetting
previously acquired knowledge. However, current methods face challenges
balancing performance, privacy preservation, and communication efficiency. We
introduce a Distributed Online LoRA for Federated INcremental learning method
DOLFIN, a novel approach combining Vision Transformers with low-rank adapters
designed to efficiently and stably learn new tasks in federated environments.
Our method leverages LoRA for minimal communication overhead and incorporates
DualGradient Projection Memory (DualGPM) to prevent forgetting. Evaluated on
CIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 under two Dirichlet
heterogeneity settings, DOLFIN consistently surpasses six strong baselines in
final average accuracy while matching their memory footprint. Orthogonal
low-rank adapters offer an effective and scalable solution for
privacy-preserving continual learning in federated settings.

</details>


### [125] [Selective Adversarial Attacks on LLM Benchmarks](https://arxiv.org/abs/2510.13570)
*Ivan Dubrovsky,Anastasia Orlova,Illarion Iov,Nina Gubina,Irena Gureeva,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 研究大语言模型基准测试中选择性对抗攻击，发现其存在并影响排名，呼吁扰动感知报告和鲁棒性诊断。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型基准测试易受语义等效对抗扰动影响，此前工作未解决能否选择性影响模型性能问题。

Method: 在MMLU基准上研究选择性对抗攻击，使用TextAttack框架，引入选择性评估协议，开发自定义约束，提出代理大语言模型管道。

Result: 发现选择性对抗攻击存在，能改变相对排名。

Conclusion: 挑战了排行榜驱动评估的公平性、可重复性和透明度，需进行扰动感知报告和鲁棒性诊断。

Abstract: Benchmarking outcomes increasingly govern trust, selection, and deployment of
LLMs, yet these evaluations remain vulnerable to semantically equivalent
adversarial perturbations. Prior work on adversarial robustness in NLP has
emphasized text attacks that affect many models equally, leaving open the
question of whether it is possible to selectively degrade or enhance
performance while minimally affecting other models. We formalize this problem
and study selective adversarial attacks on MMLU - a widely used benchmark
designed to measure a language model's broad general knowledge and reasoning
ability across different subjects. Using canonical attacks integrated into
TextAttack framework, we introduce a protocol for selectivity assessment,
develop a custom constraint to increase selectivity of attacks and propose a
surrogate-LLM pipeline that generates selective perturbations. Empirically, we
find that selective adversarial attacks exist and can materially alter relative
rankings, challenging the fairness, reproducibility, and transparency of
leaderboard-driven evaluation. Our results motivate perturbation-aware
reporting and robustness diagnostics for LLM evaluation and demonstrate that
even subtle edits can shift comparative judgments.

</details>


### [126] [ArtNet: Hierarchical Clustering-Based Artificial Netlist Generator for ML and DTCO Application](https://arxiv.org/abs/2510.13582)
*Andrew B. Kahng. Seokhyeong Kang,Seonghyeon Park,Dooseok Yoon*

Main category: cs.LG

TL;DR: 提出ArtNet人工网表生成器解决PPA优化问题，在CNN和DTCO场景有良好效果


<details>
  <summary>Details</summary>
Motivation: 先进节点中PPA优化复杂，ML和DTCO因缺乏多样训练数据和长设计流程周转时间有局限

Method: 提出ArtNet，它能复制关键拓扑特征，生成接近目标参数的人工数据集

Result: 在CNN的DRV预测中，数据增强使F1分数提高0.16；DTCO中，生成的mini - brains的PPA匹配度达97.94%

Conclusion: ArtNet可实现更高效的PPA优化和设计流程探索

Abstract: In advanced nodes, optimization of power, performance and area (PPA) has
become highly complex and challenging. Machine learning (ML) and
design-technology co-optimization (DTCO) provide promising mitigations, but
face limitations due to a lack of diverse training data as well as long design
flow turnaround times (TAT). We propose ArtNet, a novel artificial netlist
generator designed to tackle these issues. Unlike previous methods, ArtNet
replicates key topological characteristics, enhancing ML model generalization
and supporting broader design space exploration for DTCO. By producing
realistic artificial datasets that moreclosely match given target parameters,
ArtNet enables more efficient PPAoptimization and exploration of flows and
design enablements. In the context of CNN-based DRV prediction, ArtNet's data
augmentationimproves F1 score by 0.16 compared to using only the original
(real) dataset. In the DTCO context, ArtNet-generated mini-brains achieve a PPA
match up to 97.94%, demonstrating close alignment with design metrics of
targeted full-scale block designs.

</details>


### [127] [EEGChaT: A Transformer-Based Modular Channel Selector for SEEG Analysis](https://arxiv.org/abs/2510.13592)
*Chen Wang,Yansen Wang,Dongqi Han,Zilong Wang,Dongsheng Li*

Main category: cs.LG

TL;DR: 提出EEGChaT用于SEEG信号通道选择，在DuIN数据集评估，提升解码精度，有可解释性。


<details>
  <summary>Details</summary>
Motivation: SEEG信号分析因输入通道多和相关性异质有挑战，传统通道选择方法难扩展和解释。

Method: 提出EEGChaT，引入Channel Aggregation Tokens聚合通道信息，用改进的Attention Rollout技术计算通道重要性得分。

Result: 集成EEGChaT到现有分类模型提升解码精度，最高达17%绝对增益，通道权重与手动选择通道有大量重叠。

Conclusion: EEGChaT是高维SEEG分析中通道选择的有效且通用解决方案，提升性能并揭示神经信号相关性。

Abstract: Analyzing stereoelectroencephalography (SEEG) signals is critical for
brain-computer interface (BCI) applications and neuroscience research, yet
poses significant challenges due to the large number of input channels and
their heterogeneous relevance. Traditional channel selection methods struggle
to scale or provide meaningful interpretability for SEEG data. In this work, we
propose EEGChaT, a novel Transformer-based channel selection module designed to
automatically identify the most task-relevant channels in SEEG recordings.
EEGChaT introduces Channel Aggregation Tokens (CATs) to aggregate information
across channels, and leverages an improved Attention Rollout technique to
compute interpretable, quantitative channel importance scores. We evaluate
EEGChaT on the DuIN dataset, demonstrating that integrating EEGChaT with
existing classification models consistently improves decoding accuracy,
achieving up to 17\% absolute gains. Furthermore, the channel weights produced
by EEGChaT show substantial overlap with manually selected channels, supporting
the interpretability of the approach. Our results suggest that EEGChaT is an
effective and generalizable solution for channel selection in high-dimensional
SEEG analysis, offering both enhanced performance and insights into neural
signal relevance.

</details>


### [128] [Physics-augmented Multi-task Gaussian Process for Modeling Spatiotemporal Dynamics](https://arxiv.org/abs/2510.13601)
*Xizhuo Zhang,Bing Yao*

Main category: cs.LG

TL;DR: 本文提出用于时空动态系统的物理增强多任务高斯过程（P - M - GP）框架，在3D心脏电动力学建模任务验证，相比现有方法显著提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有传感和成像技术可收集高维时空数据，但因不规则空间结构、快速时间动态和多物理变量联合预测需求，有效建模此类数据仍具挑战。

Method: 开发几何感知的多任务高斯过程（M - GP）模型，通过基于物理的正则化方案融入物理定律。

Result: 在3D心脏电动力学建模任务的数值实验表明，该方法通过融入特定领域物理约束和几何先验，相比现有方法显著提高预测精度。

Conclusion: 所提出的P - M - GP框架有效，能通过融入物理约束和几何先验提升预测准确性。

Abstract: Recent advances in sensing and imaging technologies have enabled the
collection of high-dimensional spatiotemporal data across complex geometric
domains. However, effective modeling of such data remains challenging due to
irregular spatial structures, rapid temporal dynamics, and the need to jointly
predict multiple interrelated physical variables. This paper presents a
physics-augmented multi-task Gaussian Process (P-M-GP) framework tailored for
spatiotemporal dynamic systems. Specifically, we develop a geometry-aware,
multi-task Gaussian Process (M-GP) model to effectively capture intrinsic
spatiotemporal structure and inter-task dependencies. To further enhance the
model fidelity and robustness, we incorporate governing physical laws through a
physics-based regularization scheme, thereby constraining predictions to be
consistent with governing dynamical principles. We validate the proposed P-M-GP
framework on a 3D cardiac electrodynamics modeling task. Numerical experiments
demonstrate that our method significantly improves prediction accuracy over
existing methods by effectively incorporating domain-specific physical
constraints and geometric prior.

</details>


### [129] [Towards Robust Knowledge Removal in Federated Learning with High Data Heterogeneity](https://arxiv.org/abs/2510.13606)
*Riccardo Santi,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: 本文提出基于任务算术和神经切线核的创新方法，用于快速从模型中移除客户端影响。


<details>
  <summary>Details</summary>
Motivation: 现有分布式训练中，因隐私和安全要求需移除客户端贡献，但现有知识移除方法需多次通信，会导致模型在移除过程中不可用，影响系统用户。

Method: 引入基于任务算术和神经切线核的创新解决方案。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Nowdays, there are an abundance of portable devices capable of collecting
large amounts of data and with decent computational power. This opened the
possibility to train AI models in a distributed manner, preserving the
participating clients' privacy. However, because of privacy regulations and
safety requirements, elimination upon necessity of a client contribution to the
model has become mandatory. The cleansing process must satisfy specific
efficacy and time requirements. In recent years, research efforts have produced
several knowledge removal methods, but these require multiple communication
rounds between the data holders and the process coordinator. This can cause the
unavailability of an effective model up to the end of the removal process,
which can result in a disservice to the system users. In this paper, we
introduce an innovative solution based on Task Arithmetic and the Neural
Tangent Kernel, to rapidly remove a client's influence from a model.

</details>


### [130] [Message Passing on the Edge: Towards Scalable and Expressive GNNs](https://arxiv.org/abs/2510.13615)
*Pablo Barceló,Fabian Jogl,Alexander Kozachinskiy,Matthias Lanzinger,Stefan Neumann,Cristóbal Rojas*

Main category: cs.LG

TL;DR: 提出基于边缘的颜色细化测试EB - 1WL和对应GNN架构EB - GNN，展示其高表达性、高效性。


<details>
  <summary>Details</summary>
Motivation: 设计更具表达性且高效的图神经网络架构。

Method: 受经典三角形计数算法启发，在消息传递中明确使用三角形。

Result: EB - 1WL比1 - WL表达性更强；EB - 1WL和EB - GNN在实际图学习任务中时间和内存接近线性；EB - GNN显著优于简单MPNNs，与任务专用GNNs竞争时计算效率更高。

Conclusion: EB - GNN是高效的通用架构。

Abstract: We propose EB-1WL, an edge-based color-refinement test, and a corresponding
GNN architecture, EB-GNN. Our architecture is inspired by a classic triangle
counting algorithm by Chiba and Nishizeki, and explicitly uses triangles during
message passing. We achieve the following results: (1)~EB-1WL is significantly
more expressive than 1-WL. Further, we provide a complete logical
characterization of EB-1WL based on first-order logic, and matching
distinguishability results based on homomorphism counting. (2)~In an important
distinction from previous proposals for more expressive GNN architectures,
EB-1WL and EB-GNN require near-linear time and memory on practical graph
learning tasks. (3)~Empirically, we show that EB-GNN is a highly-efficient
general-purpose architecture: It substantially outperforms simple MPNNs, and
remains competitive with task-specialized GNNs while being significantly more
computationally efficient.

</details>


### [131] [Manifold Decoders: A Framework for Generative Modeling from Nonlinear Embeddings](https://arxiv.org/abs/2510.13622)
*Riddhish Thakare,Kingdom Mutala Akugri*

Main category: cs.LG

TL;DR: 本文提出为经典非线性降维（NLDR）方法构建神经解码器架构的框架，实现双向映射，并结合扩散生成过程，实验表明在重建和生成性能上存在局限，凸显了为NLDR方法添加生成能力的挑战。


<details>
  <summary>Details</summary>
Motivation: 经典NLDR技术缺乏从低维嵌入映射回高维空间的能力，限制了其在生成应用中的使用，本文旨在填补这一关键空白。

Method: 引入为主要NLDR方法构建神经解码器架构的系统框架，实现双向映射，并实现基于扩散的生成过程，在CelebA数据集上进行实验。

Result: 解码器能成功重建数据，但质量不如端到端优化的自动编码器；流形约束扩散生成的样本质量差，经典NLDR嵌入的离散和稀疏特性不适合生成模型所需的连续插值。

Conclusion: 为主要用于可视化和分析的NLDR方法添加生成能力存在固有挑战。

Abstract: Classical nonlinear dimensionality reduction (NLDR) techniques like t-SNE,
Isomap, and LLE excel at creating low-dimensional embeddings for data
visualization but fundamentally lack the ability to map these embeddings back
to the original high-dimensional space. This one-way transformation limits
their use in generative applications. This paper addresses this critical gap by
introducing a system- atic framework for constructing neural decoder
architectures for prominent NLDR methods, enabling bidirectional mapping for
the first time. We extend this framework by implementing a diffusion-based
generative process that operates directly within these learned manifold spaces.
Through experiments on the CelebA dataset, we evaluate the reconstruction and
generative performance of our approach against autoencoder and standard
diffusion model baselines. Our findings reveal a fundamental trade- off: while
the decoders successfully reconstruct data, their quality is surpassed by
end-to-end optimized autoencoders. Moreover, manifold-constrained diffusion
yields poor-quality samples, suggesting that the discrete and sparse nature of
classical NLDR embeddings is ill-suited for the continuous inter- polation
required by generative models. This work highlights the inherent challenges in
retrofitting generative capabilities onto NLDR methods designed primarily for
visualization and analysis.

</details>


### [132] [Multivariate Time Series Forecasting with Gate-Based Quantum Reservoir Computing on NISQ Hardware](https://arxiv.org/abs/2510.13634)
*Wissal Hamhoum,Soumaya Cherkaoui,Jean-Frederic Laprade,Ola Ahmed,Shengrui Wang*

Main category: cs.LG

TL;DR: 本文提出基于门的量子储层计算（MTS - QRC）用于多变量时间序列，在多个数据集上有良好表现，且表明硬件噪声对其有积极作用。


<details>
  <summary>Details</summary>
Motivation: 多数量子储层计算研究针对单变量信号且忽视近期硬件约束，需要一种适用于多变量时间序列且考虑当前硬件的方法。

Method: 引入基于门的MTS - QRC，配对注入和记忆量子比特，使用Trotterized最近邻横向场伊辛演化并针对当前设备连接性和深度进行优化。

Result: 在Lorenz - 63和ENSO数据集上取得低均方误差，在IBM Heron R2上保持准确性，在ENSO上优于无噪声模拟器。

Conclusion: 基于门的QRC对NISQ硬件上的多变量时间序列预测具有实用性，值得系统研究硬件噪声何时及如何有益于QRC读出。

Abstract: Quantum reservoir computing (QRC) offers a hardware-friendly approach to
temporal learning, yet most studies target univariate signals and overlook
near-term hardware constraints. This work introduces a gate-based QRC for
multivariate time series (MTS-QRC) that pairs injection and memory qubits and
uses a Trotterized nearest-neighbor transverse-field Ising evolution optimized
for current device connectivity and depth. On Lorenz-63 and ENSO, the method
achieves a mean square error (MSE) of 0.0087 and 0.0036, respectively,
performing on par with classical reservoir computing on Lorenz and above
learned RNNs on both, while NVAR and clustered ESN remain stronger on some
settings. On IBM Heron R2, MTS-QRC sustains accuracy with realistic depths and,
interestingly, outperforms a noiseless simulator on ENSO; singular value
analysis indicates that device noise can concentrate variance in feature
directions, acting as an implicit regularizer for linear readout in this
regime. These findings support the practicality of gate-based QRC for MTS
forecasting on NISQ hardware and motivate systematic studies on when and how
hardware noise benefits QRC readouts.

</details>


### [133] [What is the objective of reasoning with reinforcement learning?](https://arxiv.org/abs/2510.13651)
*Damek Davis,Benjamin Recht*

Main category: cs.LG

TL;DR: 指出大语言模型中使用二元奖励的几种强化学习流行算法可看作是对给定提示下正确答案概率的单调变换的随机梯度上升。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中使用二元奖励的强化学习算法的本质。

Method: 对算法进行理论分析，确定其对应的单调变换。

Result: 发现拒绝采样算法对应的变换是对数，GRPO 算法对应的是平方根的反正弦。

Conclusion: 几种流行的强化学习算法可统一看作特定单调变换的随机梯度上升。

Abstract: We show that several popular algorithms for reinforcement learning in large
language models with binary rewards can be viewed as stochastic gradient ascent
on a monotone transform of the probability of a correct answer given a prompt.
In particular, the transformation associated with rejection sampling algorithms
is the logarithm and that associated with the GRPO algorithm is the arcsine of
the square root.

</details>


### [134] [Time Series Foundation Models: Benchmarking Challenges and Requirements](https://arxiv.org/abs/2510.13654)
*Marcel Meyer,Sascha Kaltenpoth,Kevin Zalipski,Oliver Müller*

Main category: cs.LG

TL;DR: 时间序列基础模型（TSFMs）评估有挑战，需开发稳健评估方法。


<details>
  <summary>Details</summary>
Motivation: TSFMs评估困难，现有评估存在多方面挑战，需解决以保障评估完整性。

Method: 调查现有TSFM评估情况，分析面临的挑战。

Result: 发现数据分区混乱，有高估性能和错误转移知识风险。

Conclusion: 呼吁开发稳健评估方法，设计新的原则性评估方法保障TSFM评估完整性。

Abstract: Time Series Foundation Models (TSFMs) represent a new paradigm for time
series forecasting, offering zero-shot forecasting capabilities without the
need for domain-specific pre-training or fine-tuning. However, as with Large
Language Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive
training sets, it becomes more and more challenging to ensure the integrity of
benchmarking data. Our investigation of existing TSFM evaluation highlights
multiple challenges, ranging from the representativeness of the benchmark
datasets, over the lack of spatiotemporal evaluation, to risks of information
leakage due to overlapping and obscure datasets, and the memorization of global
patterns caused by external shocks like economic crises or pandemics. Our
findings reveal widespread confusion regarding data partitions, risking
inflated performance estimates and incorrect transfer of global knowledge to
local time series. We argue for the development of robust evaluation
methodologies to prevent pitfalls already observed in LLM and classical time
series benchmarking, and call upon the research community to design new,
principled approaches, such as evaluations on truly out-of-sample future data,
to safeguard the integrity of TSFM assessment.

</details>


### [135] [Rebalancing with Calibrated Sub-classes (RCS): An Enhanced Approach for Robust Imbalanced Classification](https://arxiv.org/abs/2510.13656)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: 提出基于分布校准的方法RCS解决类别不平衡问题，通过加权参数估计少数类分布，训练编解码网络，生成合成样本，实验显示该方法在多类型数据集上分类性能更优。


<details>
  <summary>Details</summary>
Motivation: 解决类别不平衡问题导致分类器偏向多数类的问题，避免仅用多数类分布近似少数类统计时的过度泛化问题。

Method: 提出RCS方法，用多数类和中间类的高斯分量混合的加权参数估计少数类分布参数，训练编解码网络保存数据结构，用编码器提取的特征向量通过分布校准策略生成合成样本。

Result: 在多种图像、文本和表格数据集上，该方法比多个基线和先进技术有更优的分类性能。

Conclusion: 提出的基于分布校准的RCS方法能有效解决类别不平衡问题，提升分类性能。

Abstract: The class imbalance problem refers to the insufficiency of data in certain
classes, which causes a classifier to be biased toward the majority class.
Distribution calibration is a technique that seeks to estimate a more accurate
class distribution based on an observed or estimated one. To address this
issue, we propose a distribution calibration-based method-Rebalancing with
Calibrated Sub-classes (RCS): An Enhanced Approach for Robust Imbalanced
Classification, which estimates the distribution parameters of the minority
classes using weighted parameters derived from a mixture of Gaussian components
from both the majority and intermediate classes. An encoder-decoder network is
trained to preserve the structure of the imbalanced data and prevent
disentanglement. After training, feature vectors extracted from the encoder are
used to generate synthetic samples through our distribution calibration
strategy. This approach effectively mitigates the overgeneralization problem
that arises when only the distribution of the majority class is used to
approximate the minority class statistics. Instead, our method calibrates the
parameters by leveraging the distribution of data points in neighboring
regions. Experimental results demonstrate that the proposed method achieves
superior classification performance compared to several baseline and
state-of-the-art techniques across a diverse range of image, text, and tabular
datasets.

</details>


### [136] [Axial Neural Networks for Dimension-Free Foundation Models](https://arxiv.org/abs/2510.13665)
*Hyunsu Kim,Jonggeon Park,Joan Bruna,Hongseok Yang,Juho Lee*

Main category: cs.LG

TL;DR: 提出维度无关的轴向神经网络XNN，转换现有PDE基础模型并评估，实验显示XNN有竞争力且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: AI基础模型在物理数据训练时因不同系统维度差异面临挑战，传统方法效率低。

Method: 提出维度无关的轴向神经网络架构XNN，将现有PDE基础模型转换为轴向神经网络，在三种训练场景下评估。

Result: XNN与原模型表现相当，对未见维度的泛化能力更优。

Conclusion: 强调了多维预训练对基础模型的重要性。

Abstract: The advent of foundation models in AI has significantly advanced
general-purpose learning, enabling remarkable capabilities in zero-shot
inference and in-context learning. However, training such models on physics
data, including solutions to partial differential equations (PDEs), poses a
unique challenge due to varying dimensionalities across different systems.
Traditional approaches either fix a maximum dimension or employ separate
encoders for different dimensionalities, resulting in inefficiencies. To
address this, we propose a dimension-agnostic neural network architecture, the
Axial Neural Network (XNN), inspired by parameter-sharing structures such as
Deep Sets and Graph Neural Networks. XNN generalizes across varying tensor
dimensions while maintaining computational efficiency. We convert existing PDE
foundation models into axial neural networks and evaluate their performance
across three training scenarios: training from scratch, pretraining on multiple
PDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform
competitively with original models and exhibit superior generalization to
unseen dimensions, highlighting the importance of multidimensional pretraining
for foundation models.

</details>


### [137] [Adam or Gauss-Newton? A Comparative Study In Terms of Basis Alignment and SGD Noise](https://arxiv.org/abs/2510.13680)
*Bingbin Liu,Rachit Bansal,Depen Morwani,Nikhil Vyas,David Alvarez-Melis,Sham M. Kakade*

Main category: cs.LG

TL;DR: 本文对比基于Adam和Gauss - Newton的对角预条件方法，分析批量和随机设置下表现。


<details>
  <summary>Details</summary>
Motivation: 对比基于Adam和Gauss - Newton的两种对角预条件方法，探究预条件器基的选择和小批量梯度噪声的影响。

Method: 在二次目标和逻辑回归的所有象限分析优化器，进行理论和实证研究。

Result: 全批量设置下Adam在某些情况优于GN⁻¹和GN⁻¹/²；随机设置下线性回归中Adam与GN⁻¹/²表现相似。

Conclusion: 理论结果得到了凸和非凸目标的实证研究支持。

Abstract: Diagonal preconditioners are computationally feasible approximate to
second-order optimizers, which have shown significant promise in accelerating
training of deep learning models. Two predominant approaches are based on Adam
and Gauss-Newton (GN) methods: the former leverages statistics of current
gradients and is the de-factor optimizers for neural networks, and the latter
uses the diagonal elements of the Gauss-Newton matrix and underpins some of the
recent diagonal optimizers such as Sophia.
  In this work, we compare these two diagonal preconditioning methods through
the lens of two key factors: the choice of basis in the preconditioner, and the
impact of gradient noise from mini-batching. To gain insights, we analyze these
optimizers on quadratic objectives and logistic regression under all four
quadrants. We show that regardless of the basis, there exist instances where
Adam outperforms both GN$^{-1}$ and GN$^{-1/2}$ in full-batch settings.
Conversely, in the stochastic regime, Adam behaves similarly to GN$^{-1/2}$ for
linear regression under a Gaussian data assumption. These theoretical results
are supported by empirical studies on both convex and non-convex objectives.

</details>


### [138] [Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating Reward Hacking](https://arxiv.org/abs/2510.13694)
*Yuchun Miao,Liang Ding,Sen Zhang,Rong Bao,Lefei Zhang,Dacheng Tao*

Main category: cs.LG

TL;DR: 文章指出RLHF中奖励破解问题的两大障碍，提出InfoRM框架、IBL正则化方法和MOP指标，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决RLHF中奖励破解问题，缓解奖励错误泛化和缺乏合适正则化的问题。

Method: 提出基于信息瓶颈原理的InfoRM框架过滤无关信息，引入IBL分布级正则化，提出MOP统计指标。

Result: 实验表明InfoRM和IBL有效，MOP可作为诊断工具。

Conclusion: InfoRM、IBL和MOP共同推动了RLHF的发展。

Abstract: Despite the success of Reinforcement Learning from Human Feedback (RLHF) in
aligning language models with human values, reward hacking-or reward
over-optimization-remains a major challenge. We identify two key obstacles to
its mitigation: (1) reward misgeneralization in reward modeling, where reward
models overfit to spurious, preference-irrelevant features; and (2) the lack of
suitable regularization during RL optimization, as existing token-level
constraints often over-restrict the policy space. To address these issues, we
propose InfoRM, an information-theoretic reward modeling framework based on the
Information Bottleneck (IB) principle, which filters out preference-irrelevant
information to alleviate reward misgeneralization. We further observe that
reward-hacked responses manifest as pronounced outliers in InfoRM's IB latent
space, measured by Mahalanobis distance from the SFT-induced distribution.
Motivated by this, we introduce IBL, a distribution-level regularization that
penalizes such deviations, effectively expanding the optimization landscape
while maintaining alignment. We prove that IBL is theoretically equivalent to
the pessimistic RL objective within the IB latent space. Finally, we present
Mahalanobis Outlier Probability (MOP), a statistical metric for quantifying
reward hacking severity, enabling principled hyperparameter tuning and online
mitigation such as early stopping. Extensive experiments across diverse LLMs
and datasets confirm the generality of our findings, the effectiveness of
InfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively
advancing the state of RLHF.

</details>


### [139] [Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents](https://arxiv.org/abs/2510.13704)
*Johan Obando-Ceron,Walter Mayor,Samuel Lavoie,Scott Fujimoto,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 提出使用单纯形嵌入加速演员 - 评论家方法训练，提升样本效率和最终性能且不损失运行速度。


<details>
  <summary>Details</summary>
Motivation: 现有大规模环境并行化加速演员 - 评论家方法训练，仍需大量环境交互，而结构良好的表示可提升深度强化学习代理的泛化和样本效率。

Method: 使用单纯形嵌入，将嵌入约束到单纯形结构。

Result: 应用于FastTD3、FastSAC和PPO时，在多种连续和离散控制环境中持续提升样本效率和最终性能，且不损失运行速度。

Conclusion: 单纯形嵌入可有效提升深度强化学习代理的样本效率和性能。

Abstract: Recent works have proposed accelerating the wall-clock training time of
actor-critic methods via the use of large-scale environment parallelization;
unfortunately, these can sometimes still require large number of environment
interactions to achieve a desired level of performance. Noting that
well-structured representations can improve the generalization and sample
efficiency of deep reinforcement learning (RL) agents, we propose the use of
simplicial embeddings: lightweight representation layers that constrain
embeddings to simplicial structures. This geometric inductive bias results in
sparse and discrete features that stabilize critic bootstrapping and strengthen
policy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial
embeddings consistently improve sample efficiency and final performance across
a variety of continuous- and discrete-control environments, without any loss in
runtime speed.

</details>


### [140] [Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe](https://arxiv.org/abs/2510.13713)
*Christophe Roux,Max Zimmer,Alexandre d'Aspremont,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 提出用Frank - Wolfe算法解决大语言模型剪枝中寻找最优掩码的问题，降低剪枝误差、性能优且省内存。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型剪枝方法找最优剪枝掩码是难的组合问题，现有方法依赖贪心启发式忽略权重交互。

Method: 考虑组合约束的凸松弛，用Frank - Wolfe算法解决松弛后的问题。

Result: 大幅降低每层剪枝误差，在GPT架构上优于强基线模型，且内存高效。

Conclusion: 结合FW算法收敛保证，将松弛解取整可得到原组合问题近似解。

Abstract: Pruning is a common technique to reduce the compute and storage requirements
of Neural Networks. While conventional approaches typically retrain the model
to recover pruning-induced performance degradation, state-of-the-art Large
Language Model (LLM) pruning methods operate layer-wise, minimizing the
per-layer pruning error on a small calibration dataset to avoid full
retraining, which is considered computationally prohibitive for LLMs. However,
finding the optimal pruning mask is a hard combinatorial problem and solving it
to optimality is intractable. Existing methods hence rely on greedy heuristics
that ignore the weight interactions in the pruning objective. In this work, we
instead consider the convex relaxation of these combinatorial constraints and
solve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method
drastically reduces the per-layer pruning error, outperforms strong baselines
on state-of-the-art GPT architectures, and remains memory-efficient. We provide
theoretical justification by showing that, combined with the convergence
guarantees of the FW algorithm, we obtain an approximate solution to the
original combinatorial problem upon rounding the relaxed solution to
integrality.

</details>


### [141] [Assessing the Geographic Generalization and Physical Consistency of Generative Models for Climate Downscaling](https://arxiv.org/abs/2510.13722)
*Carlo Saccardi,Maximilian Pierzyna,Haitz Sáez de Ocáriz Borde,Simone Monaco,Cristian Meo,Pietro Liò,Rudolf Saathof,Geethu Joseph,Justin Dauwels*

Main category: cs.LG

TL;DR: 传统天气模拟生成千米级气象数据计算量大，深度学习模型虽快但可靠性存疑。本文对深度学习模型进行基准测试，引入物理诊断，发现模型泛化和物理一致性不足，提出功率谱密度损失函数改进泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统天气模拟计算量大，深度学习模型可靠性缺乏基于大气和天气物理的评估，需评估其性能和可靠性。

Method: 对近期先进的深度学习模型进行基准测试，引入物理启发的诊断方法，提出功率谱密度损失函数。

Result: 模型在有限欧洲地理区域训练后难以泛化到其他地区，也无法准确捕捉二阶变量，即使在分布内地理区域也存在物理一致性问题。功率谱密度损失函数能改进地理泛化能力。

Conclusion: 深度学习模型在地理泛化和物理一致性方面存在挑战，功率谱密度损失函数是一个简单有效的初始解决方案。

Abstract: Kilometer-scale weather data is crucial for real-world applications but
remains computationally intensive to produce using traditional weather
simulations. An emerging solution is to use deep learning models, which offer a
faster alternative for climate downscaling. However, their reliability is still
in question, as they are often evaluated using standard machine learning
metrics rather than insights from atmospheric and weather physics. This paper
benchmarks recent state-of-the-art deep learning models and introduces
physics-inspired diagnostics to evaluate their performance and reliability,
with a particular focus on geographic generalization and physical consistency.
Our experiments show that, despite the seemingly strong performance of models
such as CorrDiff, when trained on a limited set of European geographies (e.g.,
central Europe), they struggle to generalize to other regions such as Iberia,
Morocco in the south, or Scandinavia in the north. They also fail to accurately
capture second-order variables such as divergence and vorticity derived from
predicted velocity fields. These deficiencies appear even in in-distribution
geographies, indicating challenges in producing physically consistent
predictions. We propose a simple initial solution: introducing a power spectral
density loss function that empirically improves geographic generalization by
encouraging the reconstruction of small-scale physical structures. The code for
reproducing the experimental results can be found at
https://github.com/CarloSaccardi/PSD-Downscaling

</details>


### [142] [Progressive multi-fidelity learning for physical system predictions](https://arxiv.org/abs/2510.13762)
*Paolo Conti,Mengwu Guo,Attilio Frangi,Andrea Manzoni*

Main category: cs.LG

TL;DR: 提出渐进式多保真替代模型处理多模态数据，在数值基准和实际案例中证明有效。


<details>
  <summary>Details</summary>
Motivation: 获取高精度数据昂贵耗时，低精度数据易计算但建模难，且实际数据类型多样、来源多模态、非同时可用，需更好建模方法。

Method: 引入渐进式多保真替代模型，用定制编码器依次整合不同数据类型，用神经网络进行多保真回归，通过拼接和相加连接实现信息从低到高保真度流动。

Result: 在数值基准和实际案例中展示了该方法能可靠整合多模态数据，提供准确预测，在时间和参数变化时保持性能。

Conclusion: 该渐进式多保真替代模型有效，能处理多模态数据，自动适应输入进行准确预测。

Abstract: Highly accurate datasets from numerical or physical experiments are often
expensive and time-consuming to acquire, posing a significant challenge for
applications that require precise evaluations, potentially across multiple
scenarios and in real-time. Even building sufficiently accurate surrogate
models can be extremely challenging with limited high-fidelity data.
Conversely, less expensive, low-fidelity data can be computed more easily and
encompass a broader range of scenarios. By leveraging multi-fidelity
information, prediction capabilities of surrogates can be improved. However, in
practical situations, data may be different in types, come from sources of
different modalities, and not be concurrently available, further complicating
the modeling process. To address these challenges, we introduce a progressive
multi-fidelity surrogate model. This model can sequentially incorporate diverse
data types using tailored encoders. Multi-fidelity regression from the encoded
inputs to the target quantities of interest is then performed using neural
networks. Input information progressively flows from lower to higher fidelity
levels through two sets of connections: concatenations among all the encoded
inputs, and additive connections among the final outputs. This dual connection
system enables the model to exploit correlations among different datasets while
ensuring that each level makes an additive correction to the previous level
without altering it. This approach prevents performance degradation as new
input data are integrated into the model and automatically adapts predictions
based on the available inputs. We demonstrate the effectiveness of the approach
on numerical benchmarks and a real-world case study, showing that it reliably
integrates multi-modal data and provides accurate predictions, maintaining
performance when generalizing across time and parameter variations.

</details>


### [143] [Tensor Gaussian Processes: Efficient Solvers for Nonlinear PDEs](https://arxiv.org/abs/2510.13772)
*Qiwei Yuan,Zhitong Xu,Yinghao Chen,Yiming Xu,Houman Owhadi,Shandian Zhe*

Main category: cs.LG

TL;DR: 本文指出现有PDE机器学习求解器的局限，提出TGPS求解器，结合多种方法提升效率，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有PDE机器学习求解器如神经网络求解器训练低效，GP/核基求解器处理大量配点时存在可扩展性问题，需新求解器克服这些局限。

Method: 提出TGPS求解器，用一维GP建模各输入维度的因子函数并通过张量分解组合；用部分冻结策略和牛顿法线性化非线性项，开发交替最小二乘法更新；给出模型表达性的理论保证、收敛证明和误差分析。

Result: 在多个基准PDE实验中，该方法比现有方法有更高的精度和效率。

Conclusion: TGPS求解器能有效克服现有PDE机器学习求解器的局限，在精度和效率上表现更优。

Abstract: Machine learning solvers for partial differential equations (PDEs) have
attracted growing interest. However, most existing approaches, such as neural
network solvers, rely on stochastic training, which is inefficient and
typically requires a great many training epochs. Gaussian process
(GP)/kernel-based solvers, while mathematical principled, suffer from
scalability issues when handling large numbers of collocation points often
needed for challenging or higher-dimensional PDEs.
  To overcome these limitations, we propose TGPS, a tensor-GP-based solver that
models factor functions along each input dimension using one-dimensional GPs
and combines them via tensor decomposition to approximate the full solution.
This design reduces the task to learning a collection of one-dimensional GPs,
substantially lowering computational complexity, and enabling scalability to
massive collocation sets.
  For efficient nonlinear PDE solving, we use a partial freezing strategy and
Newton's method to linerize the nonlinear terms. We then develop an alternating
least squares (ALS) approach that admits closed-form updates, thereby
substantially enhancing the training efficiency. We establish theoretical
guarantees on the expressivity of our model, together with convergence proof
and error analysis under standard regularity assumptions. Experiments on
several benchmark PDEs demonstrate that our method achieves superior accuracy
and efficiency compared to existing approaches.

</details>


### [144] [UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations](https://arxiv.org/abs/2510.13774)
*Dominik J. Mühlematter,Lin Che,Ye Hong,Martin Raubal,Nina Wiedemann*

Main category: cs.LG

TL;DR: 提出含随机多模态融合的地理基础模型UrbanFusion，经评估性能优于现有模型，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前预测城市现象的方法多为特定任务模型，空间表征基础模型模态支持有限且缺乏多模态融合能力。

Method: 采用特定模态编码器处理多类型输入，通过基于Transformer的融合模块整合多模态输入以学习统一表征。

Result: 在全球56个城市的41项任务评估中，UrbanFusion表现出强泛化和预测性能，优于现有GeoAI模型。

Conclusion: UrbanFusion能灵活利用模态子集，适用于不同数据可用性场景。

Abstract: Forecasting urban phenomena such as housing prices and public health
indicators requires the effective integration of various geospatial data.
Current methods primarily utilize task-specific models, while recent foundation
models for spatial representations often support only limited modalities and
lack multimodal fusion capabilities. To overcome these challenges, we present
UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal
Fusion (SMF). The framework employs modality-specific encoders to process
different types of inputs, including street view imagery, remote sensing data,
cartographic maps, and points of interest (POIs) data. These multimodal inputs
are integrated via a Transformer-based fusion module that learns unified
representations. An extensive evaluation across 41 tasks in 56 cities worldwide
demonstrates UrbanFusion's strong generalization and predictive performance
compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms
prior foundation models on location-encoding, 2) allows multimodal input during
inference, and 3) generalizes well to regions unseen during training.
UrbanFusion can flexibly utilize any subset of available modalities for a given
location during both pretraining and inference, enabling broad applicability
across diverse data availability scenarios. All source code is available at
https://github.com/DominikM198/UrbanFusion.

</details>


### [145] [The Art of Scaling Reinforcement Learning Compute for LLMs](https://arxiv.org/abs/2510.13786)
*Devvrit Khatri,Lovish Madaan,Rishabh Tiwari,Rachit Bansal,Sai Surya Duvvuri,Manzil Zaheer,Inderjit S. Dhillon,David Brandfonbrener,Rishabh Agarwal*

Main category: cs.LG

TL;DR: 该研究开展超40万GPU小时的大规模研究，提出分析和预测大语言模型强化学习（RL）扩展的框架，观察到不同结论并提出ScaleRL最佳实践方案。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练大语言模型中至关重要，但缺乏类似预训练的预测性扩展方法，也缺乏评估算法改进的原则性理解。

Method: 开展超40万GPU小时的大规模系统研究，拟合RL训练的S型计算 - 性能曲线，消融分析多种设计选择。

Result: 观察到并非所有方案都有相似渐近性能；部分细节主要调节计算效率；稳定可扩展的方案有可预测的扩展轨迹。提出ScaleRL方案并在10万GPU小时的运行中验证其有效性。

Conclusion: 为RL扩展分析提供科学框架，提出的实践方案让RL训练更具可预测性。

Abstract: Reinforcement learning (RL) has become central to training large language
models (LLMs), yet the field lacks predictive scaling methodologies comparable
to those established for pre-training. Despite rapidly rising compute budgets,
there is no principled understanding of how to evaluate algorithmic
improvements for scaling RL compute. We present the first large-scale
systematic study, amounting to more than 400,000 GPU-hours, that defines a
principled framework for analyzing and predicting RL scaling in LLMs. We fit
sigmoidal compute-performance curves for RL training and ablate a wide range of
common design choices to analyze their effects on asymptotic performance and
compute efficiency. We observe: (1) Not all recipes yield similar asymptotic
performance, (2) Details such as loss aggregation, normalization, curriculum,
and off-policy algorithm primarily modulate compute efficiency without
materially shifting the asymptote, and (3) Stable, scalable recipes follow
predictable scaling trajectories, enabling extrapolation from smaller-scale
runs. Combining these insights, we propose a best-practice recipe, ScaleRL, and
demonstrate its effectiveness by successfully scaling and predicting validation
performance on a single RL run scaled up to 100,000 GPU-hours. Our work
provides both a scientific framework for analyzing scaling in RL and a
practical recipe that brings RL training closer to the predictability long
achieved in pre-training.

</details>


### [146] [T3former: Temporal Graph Classification with Topological Machine Learning](https://arxiv.org/abs/2510.13789)
*Md. Joshem Uddin,Soham Changani,Baris Coskunuzer*

Main category: cs.LG

TL;DR: 提出T3former解决时间图分类问题，在多基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 时间图分类重要但研究不足，现有方法存在丢失细粒度信息、难以处理长距离依赖、过平滑和过压缩等问题。

Method: 引入T3former，利用滑动窗口拓扑和频谱描述符作为一级令牌，通过专门的描述符注意力机制集成。

Result: T3former在多个基准测试中达到了最先进的性能，还提供了在时间和结构扰动下的稳定性理论保证。

Conclusion: 结合拓扑和频谱见解对推进时间图学习前沿具有强大作用。

Abstract: Temporal graph classification plays a critical role in applications such as
cybersecurity, brain connectivity analysis, social dynamics, and traffic
monitoring. Despite its significance, this problem remains underexplored
compared to temporal link prediction or node forecasting. Existing methods
often rely on snapshot-based or recurrent architectures that either lose
fine-grained temporal information or struggle with long-range dependencies.
Moreover, local message-passing approaches suffer from oversmoothing and
oversquashing, limiting their ability to capture complex temporal structures.
  We introduce T3former, a novel Topological Temporal Transformer that
leverages sliding-window topological and spectral descriptors as first-class
tokens, integrated via a specialized Descriptor-Attention mechanism. This
design preserves temporal fidelity, enhances robustness, and enables principled
cross-modal fusion without rigid discretization. T3former achieves
state-of-the-art performance across multiple benchmarks, including dynamic
social networks, brain functional connectivity datasets, and traffic networks.
It also offers theoretical guarantees of stability under temporal and
structural perturbations. Our results highlight the power of combining
topological and spectral insights for advancing the frontier of temporal graph
learning.

</details>


### [147] [Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach](https://arxiv.org/abs/2510.13792)
*Ziqing Lu,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: 本文提出一种针对强化学习的无敌对抗攻击方法，用信息论方法随机改变代理观察，推导奖励遗憾下限并展示攻击对算法的影响，还将方法拓展到其他攻击类型。


<details>
  <summary>Details</summary>
Motivation: 为提高强化学习系统针对对手的鲁棒性和防御能力，研究对抗攻击，且之前大多考虑确定性攻击策略存在可被破解问题。

Method: 攻击者采用率失真信息论方法随机改变代理对转移核等属性的观察，推导接收代理奖励遗憾的信息论下限。

Result: 展示了率失真攻击对现有基于模型和无模型算法的影响，还将信息论方法拓展到其他类型对抗攻击。

Conclusion: 提出的信息论方法的对抗攻击可让代理在训练中获取极少真实信息，具有研究价值且可拓展应用。

Abstract: Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged
in many security-related applications, such as autonomous driving, financial
decisions, and drone/robot algorithms. In order to improve the
robustness/defense of RL systems against adversaries, studying various
adversarial attacks on RL systems is very important. Most previous work
considered deterministic adversarial attack strategies in MDP, which the
recipient (victim) agent can defeat by reversing the deterministic attacks. In
this paper, we propose a provably ``invincible'' or ``uncounterable'' type of
adversarial attack on RL. The attackers apply a rate-distortion
information-theoretic approach to randomly change agents' observations of the
transition kernel (or other properties) so that the agent gains zero or very
limited information about the ground-truth kernel (or other properties) during
the training. We derive an information-theoretic lower bound on the recipient
agent's reward regret and show the impact of rate-distortion attacks on
state-of-the-art model-based and model-free algorithms. We also extend this
notion of an information-theoretic approach to other types of adversarial
attack, such as state observation attacks.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [148] [From Minimal Existence to Human Definition: The CES-IMU-HSG Theoretical Framework](https://arxiv.org/abs/2510.13400)
*Kei Itoh*

Main category: cs.NE

TL;DR: 构建基于“我思故我在”的跨通用数理逻辑框架，应用于生物系统并对比人机存在，为人工智能自主性提供新基础。


<details>
  <summary>Details</summary>
Motivation: 构建一个通用的数理逻辑框架，将不同理论联系起来，并探讨其在生物系统和人工智能领域的应用。

Method: 基于“我思故我在”构建框架，结合中间元宇宙和分层状态网格，运用制度理论框架和范畴构造。

Result: 建立了“定义=状态”的范畴属性，将框架应用于生物系统，提出内部“我思故我在”概念。

Conclusion: 该框架为人工智能的自主和自我定义存在提供了新基础，搭建了哲学本体论与工程实现的桥梁。

Abstract: This study presents an inter-universal mathematical-logical framework
constructed upon the minimal axiom Cogito, ergo sum (CES), integrating the
Intermediate Meta-Universe (IMU) and the Hierarchical State Grid (HSG). The CES
defines existence as a reflexive correspondence --'to be' and 'to be
sayable'--and positions any formal system, including ZFC or HoTT, as an
attachable extension atop this minimal structure. The IMU functions as a
registry of axiomatic dependencies that connect heterogeneous theories,
employing the Institution-theoretic framework to ensure coherent
inter-theoretical linkages. The HSG concretizes these ideas through categorical
construction, defined by three orthogonal axes: the state-depth axis, the
mapping-hierarchy axis, and the temporal axis incorporating the principle of
'no future reference.' Through these, the identity of 'definition = state' is
formally established as a categorical property. Extending this structure to
biological systems, the neural system is implemented as a 0-3D complex of
neuron-function fields on the HSG, while its categorical extensions via
fiberization over the material base enable the parallel integration of multiple
physiological universes-neural, endocrine, learning, genetic, and input/output
systems-into a coherent adjoint ensemble. Within this framework, human behavior
and cognition emerge as temporal compositions of inter-universal algorithms
constrained by the material base. Finally, by contrasting human cognition,
which relies on external CES, with machine existence, this study introduces the
concept of internal CES, wherein a machine grounds its own logic upon the
factuality of its operation. This internal self-axiomatization establishes a
continuous bridge between philosophical ontology and engineering
implementation, providing a new foundation for the autonomous and self-defining
existence of artificial intelligence.

</details>


### [149] [A Complete Pipeline for deploying SNNs with Synaptic Delays on Loihi 2](https://arxiv.org/abs/2510.13757)
*Balázs Mészáros,James C. Knight,Jonathan Timcheck,Thomas Nowotny*

Main category: cs.NE

TL;DR: 本文提出在GPU上对带突触延迟的脉冲神经网络（SNN）进行基于事件的高效训练，并部署到英特尔Loihi 2神经形态芯片的完整流程，在关键词识别任务上效果良好，Loihi 2比NVIDIA Jetson Orin Nano更快且节能。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络作为传统人工神经网络在边缘计算中的节能替代方案受到关注，神经形态计算可显著降低能源需求。

Method: 提出在GPU上对带突触延迟的SNN进行基于事件的高效训练，并部署到英特尔Loihi 2神经形态芯片的完整流程。

Result: 在关键词识别任务上评估，算法比无延迟架构提高了分类准确率；GPU和Loihi 2实现几乎无精度损失，Loihi 2分类速度比NVIDIA Jetson Orin Nano快达18倍，能耗低250倍。

Conclusion: 所提出的完整流程在关键词识别任务上表现良好，且Loihi 2在速度和节能方面优势明显。

Abstract: Spiking Neural Networks are attracting increased attention as a more
energy-efficient alternative to traditional Artificial Neural Networks for edge
computing. Neuromorphic computing can significantly reduce energy requirements.
Here, we present a complete pipeline: efficient event-based training of SNNs
with synaptic delays on GPUs and deployment on Intel's Loihi 2 neuromorphic
chip. We evaluate our approach on keyword recognition tasks using the Spiking
Heidelberg Digits and Spiking Speech Commands datasets, demonstrating that our
algorithm can enhance classification accuracy compared to architectures without
delays. Our benchmarking indicates almost no accuracy loss between GPU and
Loihi 2 implementations, while classification on Loihi 2 is up to 18x faster
and uses 250x less energy than on an NVIDIA Jetson Orin Nano.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [150] [AutoCode: LLMs as Problem Setters for Competitive Programming](https://arxiv.org/abs/2510.12803)
*Shang Zhou,Zihan Zheng,Kaiyuan Liu,Zeyu Shen,Zerui Cheng,Zexing Chen,Hansen He,Jianzhu Yao,Huanzhi Mao,Qiuyang Mang,Tianfu Fu,Beichen Li,Dongruixuan Li,Wenhao Chai,Zhuang Liu,Aleksandra Korolova,Peter Henderson,Natasha Jaques,Pramod Viswanath,Saining Xie,Jingbo Shang*

Main category: cs.SE

TL;DR: 研究大语言模型编写竞赛编程问题能力，提出AutoCode，其在一致性、生成新问题等方面表现佳。


<details>
  <summary>Details</summary>
Motivation: 编写竞赛编程问题要求高，以此作为测试大语言模型通用能力的理想场景，研究其能否可靠编写。

Method: 引入AutoCode，采用多轮验证生成竞赛级问题陈述和测试用例，通过交叉验证过滤不良问题。

Result: AutoCode测试套件与官方判断一致性近99%，优于现有方法；能创建新问题变体；系统经人类专家验证确保高正确性。

Conclusion: AutoCode能成功生成经顶级竞赛程序员认可的竞赛质量新问题。

Abstract: Writing competitive programming problems is exacting. Authors must: set
constraints, input distributions, and edge cases that rule out shortcuts;
target specific algorithms (e.g., max-flow, dynamic programming, data
structures); and calibrate complexity beyond the reach of most competitors. We
argue that this makes for an ideal test of general large language model
capabilities and study whether they can do this reliably. We introduce
AutoCode, which uses multiple rounds of validation to yield competition-grade
problem statements and test cases. On held-out problems, AutoCode test suites
approach 99% consistency with official judgments, a significant improvement
over current state-of-the-art methods like HardTests, which achieve less than
81%. Furthermore, starting with a random seed problem, AutoCode can create
novel variants with reference and brute-force solutions. By cross-verifying
these generated solutions against test cases, we can further filter out
malformed problems. Our system ensures high correctness, as verified by human
experts. AutoCode successfully produces novel problems judged by
Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.

</details>


### [151] [SpareCodeSearch: Searching for Code Context When You Have No Spare GPU](https://arxiv.org/abs/2510.12948)
*Minh Nguyen*

Main category: cs.SE

TL;DR: 本文证明在大型代码库中使用关键词搜索为代码语言模型检索相关代码上下文是可行的，无需大量 GPU 资源，并在基准测试中取得较好成绩。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成框架中的检索模块常用语义搜索，训练和托管嵌入式模型需大量计算资源，难以集成到轻量级应用中。

Method: 使用关键词搜索在大型代码库中检索相关代码上下文。

Result: 在代码上下文竞赛基准测试中，Kotlin 和 Python 赛道分别达到 0.748 和 0.725 的 chRF 分数。

Conclusion: 使用关键词搜索足以在大型代码库中检索相关且有用的代码上下文，无需大量 GPU 资源。

Abstract: Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

</details>


### [152] [ADPerf: Investigating and Testing Performance in Autonomous Driving Systems](https://arxiv.org/abs/2510.13078)
*Tri Minh-Triet Pham,Diego Elias Costa,Weiyi Shang,Jinqiu Yang*

Main category: cs.SE

TL;DR: 对自动驾驶系统中障碍物检测模块性能进行研究，提出ADPerf工具，强调对障碍物检测组件进行性能测试的必要性。


<details>
  <summary>Details</summary>
Motivation: 目前对障碍物检测模块的延迟及其对激光雷达点云数据变化的适应性了解不足，为保障自动驾驶系统的安全和有效性开展研究。

Method: 对Apollo和Autoware两个工业级自动驾驶系统的障碍物检测模块进行全面调查，引入ADPerf工具生成点云数据测试用例。

Result: 应用ADPerf对3D障碍物检测模块及轨迹预测模块进行压力测试。

Conclusion: 需要对障碍物检测组件进行性能测试，否则会成为增加自动驾驶系统延迟的主要瓶颈，降低系统整体可靠性。

Abstract: Obstacle detection is crucial to the operation of autonomous driving systems,
which rely on multiple sensors, such as cameras and LiDARs, combined with code
logic and deep learning models to detect obstacles for time-sensitive
decisions. Consequently, obstacle detection latency is critical to the safety
and effectiveness of autonomous driving systems. However, the latency of the
obstacle detection module and its resilience to various changes in the LiDAR
point cloud data are not yet fully understood. In this work, we present the
first comprehensive investigation on measuring and modeling the performance of
the obstacle detection modules in two industry-grade autonomous driving
systems, i.e., Apollo and Autoware. Learning from this investigation, we
introduce ADPerf, a tool that aims to generate realistic point cloud data test
cases that can expose increased detection latency. Increasing latency decreases
the availability of the detected obstacles and stresses the capabilities of
subsequent modules in autonomous driving systems, i.e., the modules may be
negatively impacted by the increased latency in obstacle detection.
  We applied ADPerf to stress-test the performance of widely used 3D obstacle
detection modules in autonomous driving systems, as well as the propagation of
such tests on trajectory prediction modules. Our evaluation highlights the need
to conduct performance testing of obstacle detection components, especially 3D
obstacle detection, as they can be a major bottleneck to increased latency of
the autonomous driving system. Such an adverse outcome will also further
propagate to other modules, reducing the overall reliability of autonomous
driving systems.

</details>


### [153] [TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models](https://arxiv.org/abs/2510.13106)
*Ruoyu Sun,Da Song,Jiayang Song,Yuheng Huang,Lei Ma*

Main category: cs.SE

TL;DR: 提出自动化评估框架TRUSTVIS评估大语言模型可信度，用案例展示其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在安全和鲁棒性方面可信度的问题。

Method: 引入TRUSTVIS框架，有交互式用户界面，集成AutoDAN等扰动方法，采用多数投票。

Result: 对Vicuna - 7b、Llama2 - 7b和GPT - 3.5等模型的初步案例研究，能识别安全和鲁棒性漏洞。

Conclusion: TRUSTVIS框架有效，其交互式界面可助力针对性改进模型。

Abstract: As Large Language Models (LLMs) continue to revolutionize Natural Language
Processing (NLP) applications, critical concerns about their trustworthiness
persist, particularly in safety and robustness. To address these challenges, we
introduce TRUSTVIS, an automated evaluation framework that provides a
comprehensive assessment of LLM trustworthiness. A key feature of our framework
is its interactive user interface, designed to offer intuitive visualizations
of trustworthiness metrics. By integrating well-known perturbation methods like
AutoDAN and employing majority voting across various evaluation methods,
TRUSTVIS not only provides reliable results but also makes complex evaluation
processes accessible to users. Preliminary case studies on models like
Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our
framework in identifying safety and robustness vulnerabilities, while the
interactive interface allows users to explore results in detail, empowering
targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g

</details>


### [154] [Isolating Compiler Bugs through Compilation Steps Analysis](https://arxiv.org/abs/2510.13128)
*Yujie Liu,Mingxuan Zhu,Shengyu Cheng,Dan Hao*

Main category: cs.SE

TL;DR: 提出 CompSCAN 编译器漏洞隔离技术，在 185 个真实编译器漏洞上评估，效果和效率超现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有编译器漏洞隔离技术缺乏对内部步骤的因果分析，有效性受限，需新方法。

Method: CompSCAN 采用三阶段流程，提取编译步骤数组、识别漏洞步骤并收集代码元素、计算可疑分数并输出排名列表。

Result: 在 185 个 LLVM 和 GCC 真实漏洞上评估，在 Top - 1/3/5/10 排名中成功隔离多个漏洞，相比 ETEM 和 ODFL 有相对提升，且运行更快。

Conclusion: CompSCAN 在编译器漏洞隔离方面比现有技术更有效和高效。

Abstract: Compilers are essential to software systems, and their bugs can propagate to
dependent software. Ensuring compiler correctness is critical. However,
isolating compiler bugs remains challenging due to the internal complexity of
compiler execution. Existing techniques primarily mutate compilation inputs to
generate passing and failing tests, but often lack causal analysis of internal
steps, limiting their effectiveness.
  To address this limitation, we propose CompSCAN, a novel compiler bug
isolation technique that applies analysis over the sequence of compilation
steps. CompSCAN follows a three-stage process: (1) extracting the array of
compilation steps that leads to the original failure, (2) identifying
bug-causing steps and collecting corresponding compiler code elements, and (3)
calculating suspicious scores for each code element and outputting a suspicious
ranking list as the bug isolation result.
  We evaluate CompSCAN on 185 real-world LLVM and GCC bugs. Results show that
CompSCAN outperforms state-of-the-art techniques in both effectiveness and
efficiency. CompSCAN successfully isolates 50, 85, 100, and 123 bugs within the
Top-1/3/5/10 ranks, respectively. Compared with ETEM and ODFL, two
state-of-the-art compiler bug isolation techniques, CompSCAN achieves relative
improvements of 44.51% / 50.18% / 36.24% / 24.49% over ETEM, and 31.58% /
49.12% / 44.93% / 21.78% over ODFL on those metrics. Moreover, CompSCAN runs
faster on average per bug than both baselines.

</details>


### [155] [GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning](https://arxiv.org/abs/2510.13176)
*Haolin Pan,Chao Zha,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 本文提出GRACE框架用于编译器自动调优，在减少LLVM IR指令计数方面表现出色，调优时间短。


<details>
  <summary>Details</summary>
Motivation: 标准编译器启发式方法结果欠佳，迭代编译搜索成本高，机器学习方法泛化能力弱，需新的编译器自动调优方法。

Method: 利用传递协同效应和加权评分法缩小搜索空间，用对比学习创建程序嵌入进行聚类，在聚类内进行进化搜索得到k个专门的传递序列，测试时选择最佳序列并优化。

Result: 在七个不同数据集上，GRACE在LLVM 10.0.0和18.1.6上平均减少LLVM IR指令计数分别为10.09%和10.19%，平均每个程序调优时间少于1秒。

Conclusion: GRACE具有最先进的性能和实际有效性。

Abstract: Compiler pass selection and phase ordering present a significant challenge in
achieving optimal program performance, particularly for objectives like code
size reduction. Standard compiler heuristics offer general applicability but
often yield suboptimal, program-specific results due to their one-size-fits-all
nature. While iterative compilation can find tailored solutions, its
prohibitive search cost limits practical use. Machine learning approaches
promise faster inference but frequently struggle with generalization to unseen
programs. This paper introduces GRACE, a novel framework for compiler
auto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE
effectively curtails the search space by leveraging pass synergies and a
weighted scoring method to generate initial high-quality candidate sequences
and a pass pool. It then employs contrastive learning, using pass
sequence-based data augmentation, to create program embeddings that facilitate
similarity-aware clustering. Evolutionary search within these clusters yields a
coreset of $k$ specialized pass sequences designed for robust generalization to
unseen programs. At test time, GRACE efficiently selects the best coreset
sequence and refines it using lightweight techniques. Experimental results on
seven diverse datasets show that GRACE reduces LLVM IR instruction count by an
average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,
while incurring an average tuning time of less than 1s per program,
demonstrating its state-of-the-art performance and practical effectiveness.

</details>


### [156] [Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines](https://arxiv.org/abs/2510.13184)
*Haolin Pan,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 本文提出针对LLVM新通行证管理器的自动调优框架，用形式语法和森林数据结构，结合结构感知遗传算法，在基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有编译器自动调优方法与LLVM新通行证管理器层次设计不匹配，无法保证生成语法有效的优化管道。

Method: 引入形式语法定义有效嵌套管道空间，采用森林数据结构，开发结构感知遗传算法，挖掘协同通行证关系引导搜索，还有可选细化阶段。

Result: 在七个基准数据集上，发现的管道比标准opt -Oz优化级别平均多减少13.62%的指令计数。

Conclusion: 该框架能在复杂受限搜索空间中找到有效通行证管道。

Abstract: Compiler optimization relies on sequences of passes to improve program
performance. Selecting and ordering these passes automatically, known as
compiler auto-tuning, is challenging due to the large and complex search space.
Existing approaches generally assume a linear sequence of passes, a model
compatible with legacy compilers but fundamentally misaligned with the
hierarchical design of the LLVM New Pass Manager. This misalignment prevents
them from guaranteeing the generation of syntactically valid optimization
pipelines. In this work, we present a new auto-tuning framework built from the
ground up for the New Pass Manager. We introduce a formal grammar to define the
space of valid nested pipelines and a forest-based data structure for their
native representation. Upon this foundation, we develop a structure-aware
Genetic Algorithm whose operators manipulate these forests directly, ensuring
that all candidate solutions are valid by construction. The framework first
mines synergistic pass relationships to guide the search. An optional
refinement stage further explores subtle performance variations arising from
different valid structural arrangements.
  We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The
discovered pipelines achieve an average of 13.62% additional instruction count
reduction compared to the standard opt -Oz optimization level, showing that our
framework is capable of navigating this complex, constrained search space to
identify valid and effective pass pipelines.

</details>


### [157] [Towards Richer Challenge Problems for Scientific Computing Correctness](https://arxiv.org/abs/2510.13423)
*Matthew Sottile,Mohit Tekriwal,John Sarracino*

Main category: cs.SE

TL;DR: 呼吁针对科学计算（SC）正确性设计专门挑战问题，以推动形式方法（FM）和编程语言（PL）验证技术发展，并提出相关正确性维度及设计准则。


<details>
  <summary>Details</summary>
Motivation: 现有PL/FM验证技术难以应对现实SC应用的复杂性，且SC与PL/FM社区缺乏对机器可验证正确性挑战和SC应用正确性维度的共识。

Method: 提出适用于科学计算的正确性维度，讨论设计挑战问题的准则和标准。

Result: 明确提出了针对SC正确性的专门挑战问题及设计方向。

Conclusion: 专门的挑战问题能促进FM/PL验证技术在SC正确性方面的发展，满足SC应用需求。

Abstract: Correctness in scientific computing (SC) is gaining increasing attention in
the formal methods (FM) and programming languages (PL) community. Existing
PL/FM verification techniques struggle with the complexities of realistic SC
applications. Part of the problem is a lack of a common understanding between
the SC and PL/FM communities of machine-verifiable correctness challenges and
dimensions of correctness in SC applications.
  To address this gap, we call for specialized challenge problems to inform the
development and evaluation of FM/PL verification techniques for correctness in
SC. These specialized challenges are intended to augment existing problems
studied by FM/PL researchers for general programs to ensure the needs of SC
applications can be met. We propose several dimensions of correctness relevant
to scientific computing, and discuss some guidelines and criteria for designing
challenge problems to evaluate correctness in scientific computing.

</details>


### [158] [Verifying a Sparse Matrix Algorithm Using Symbolic Execution](https://arxiv.org/abs/2510.13424)
*Alexander C. Wilton*

Main category: cs.SE

TL;DR: 提出用符号执行编写测试以验证科学软件，应用于稀疏矩阵算法


<details>
  <summary>Details</summary>
Motivation: 科学软件复杂，传统测试难检测细微错误

Method: 使用符号执行编写类似传统单元测试的测试，提供更强验证保证

Result: 未提及

Conclusion: 未提及

Abstract: Scientific software is, by its very nature, complex. It is mathematical and
highly optimized which makes it prone to subtle bugs not as easily detected by
traditional testing. We outline how symbolic execution can be used to write
tests similar to traditional unit tests while providing stronger verification
guarantees and apply this methodology to a sparse matrix algorithm.

</details>


### [159] [OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies](https://arxiv.org/abs/2510.13561)
*Peng Di,Faqiang Chen,Xiao Bai,Hongjun Yang,Qingfeng Li,Ganglin Wei,Jian Mou,Feng Shi,Keting Chen,Peng Tang,Zhitao Shen,Zheng Li,Wenhui Shi,Junwei Guo,Hang Yu*

Main category: cs.SE

TL;DR: 提出针对SRE的开源多智能体框架OpenDerisk，评估显示其在准确性和效率上超现有基线，在蚂蚁集团部署验证了可扩展性和实际影响。


<details>
  <summary>Details</summary>
Motivation: 现代软件复杂度增加使SRE团队运维负担重，现有解决方案存在不足，需要AI驱动自动化来模拟专家诊断推理。

Method: 提出OpenDerisk框架，集成诊断原生协作模型、可插拔推理引擎、知识引擎和标准化协议MCP。

Result: OpenDerisk在准确性和效率上显著优于现有基线，在蚂蚁集团大规模生产部署，服务超3000名日常用户。

Conclusion: OpenDerisk具有工业级可扩展性和实际影响，且已开源。

Abstract: The escalating complexity of modern software imposes an unsustainable
operational burden on Site Reliability Engineering (SRE) teams, demanding
AI-driven automation that can emulate expert diagnostic reasoning. Existing
solutions, from traditional AI methods to general-purpose multi-agent systems,
fall short: they either lack deep causal reasoning or are not tailored for the
specialized, investigative workflows unique to SRE. To address this gap, we
present OpenDerisk, a specialized, open-source multi-agent framework
architected for SRE. OpenDerisk integrates a diagnostic-native collaboration
model, a pluggable reasoning engine, a knowledge engine, and a standardized
protocol (MCP) to enable specialist agents to collectively solve complex,
multi-domain problems. Our comprehensive evaluation demonstrates that
OpenDerisk significantly outperforms state-of-the-art baselines in both
accuracy and efficiency. This effectiveness is validated by its large-scale
production deployment at Ant Group, where it serves over 3,000 daily users
across diverse scenarios, confirming its industrial-grade scalability and
practical impact. OpenDerisk is open source and available at
https://github.com/derisk-ai/OpenDerisk/

</details>


### [160] [Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code](https://arxiv.org/abs/2510.13575)
*Han Fu,Sigrid Eldh,Kristian Wiklund,Andreas Ermedahl,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: 本文采用大语言模型驱动的自动化修复方法处理工业嵌入式系统持续集成中的编译错误，评估了四种先进大语言模型在工业CI系统中的表现，结果显示能解决较多错误并显著减少调试时间。


<details>
  <summary>Details</summary>
Motivation: 工业嵌入式系统软硬件协同开发在持续集成中常出现编译错误，现有依赖测试用例的修复技术对不可编译代码不适用，需要新的修复方法。

Method: 收集产品源代码中超40000个提交，用四种先进大语言模型增强工业CI系统，将其修复结果与人工修复对比。

Result: 装备大语言模型的CI系统能解决基线数据集中高达63%的编译错误，成功CI构建的修复中有83%被认为合理，且显著减少调试时间，多数成功案例在8分钟内完成。

Conclusion: 大语言模型驱动的自动化修复方法在处理工业嵌入式系统编译错误方面表现良好，能有效解决错误并大幅缩短调试时间。

Abstract: The co-development of hardware and software in industrial embedded systems
frequently leads to compilation errors during continuous integration (CI).
Automated repair of such failures is promising, but existing techniques rely on
test cases, which are not available for non-compilable code.
  We employ an automated repair approach for compilation errors driven by large
language models (LLMs). Our study encompasses the collection of more than 40000
commits from the product's source code. We assess the performance of an
industrial CI system enhanced by four state-of-the-art LLMs, comparing their
outcomes with manual corrections provided by human programmers. LLM-equipped CI
systems can resolve up to 63 % of the compilation errors in our baseline
dataset. Among the fixes associated with successful CI builds, 83 % are deemed
reasonable. Moreover, LLMs significantly reduce debugging time, with the
majority of successful cases completed within 8 minutes, compared to hours
typically required for manual debugging.

</details>


### [161] [Property Testing for Ocean Models. Can We Specify It? (Invited Talk)](https://arxiv.org/abs/2510.13692)
*Deepak A. Cherian*

Main category: cs.SE

TL;DR: 受属性测试文献启发，探讨将其理念应用于海洋数值模型，提出用地球物理流体动力学理论解决海洋模型测试的预言机问题，并指出一些简单理想的GFD问题可作为属性测试。


<details>
  <summary>Details</summary>
Motivation: 解决海洋模型测试的预言机问题，探讨属性测试理念在海洋数值模型中的应用。

Method: 将地球物理流体动力学理论表达为属性测试，提出一些简单理想的GFD问题作为属性测试示例。

Result: 给出了一些可作为属性测试的简单理想GFD问题示例，说明物理现象适合指定属性测试。

Conclusion: 哪些提出的测试最可行和有用还有待观察。

Abstract: I take inspiration from the property-testing literature, particularly the
work of Prof. John Hughes, and explore how such ideas might be applied to
numerical models of the ocean. Specifically, I ask whether geophysical fluid
dynamics (GFD) theory, expressed as property tests, might be used to address
the oracle problem of testing the correctness of ocean models. I propose that a
number of simple idealized GFD problems can be framed as property tests. These
examples clearly illustrate how physics naturally lends itself to specifying
property tests. Which of these proposed tests might be most feasible and
useful, remains to be seen.

</details>


### [162] [On Pretraining for Project-Level Code Completion](https://arxiv.org/abs/2510.13697)
*Maksim Sapronov,Evgeniy Glukhov*

Main category: cs.SE

TL;DR: 研究不同仓库处理策略对OpenCoder上下文学习的影响，扩展其上下文窗口，虽数据集小但性能相当，多种策略效果类似，简单文件级训练方法仍有效。


<details>
  <summary>Details</summary>
Motivation: 探究不同仓库处理策略如何影响OpenCoder的上下文学习。

Method: 通过在额外的10亿个精心挑选的仓库级数据上训练，将OpenCoder的上下文窗口从4096扩展到16384个标记。

Result: 在Long Code Arena基准测试中，模型虽使用较小数据集但性能与竞争模型相当，不同仓库处理技术效果相似，主要收益来自适应新的旋转位置嵌入（RoPE）缩放参数。

Conclusion: 简单的文件级训练方法在原始序列长度下仍非常有效，为数据和计算资源受限的场景开展仓库级代码补全研究提供了可能。

Abstract: Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [163] [Multifractality and its sources in the digital currency market](https://arxiv.org/abs/2510.13785)
*Stanisław Drożdż,Robert Kluszczyński,Jarosław Kwapień,Marcin Wątorek*

Main category: q-fin.ST

TL;DR: 本文将时间序列多重分形源分解方法应用于数字货币市场代表工具，结果表明重尾和时间相关性对多重分形有影响，验证了该方法有效性。


<details>
  <summary>Details</summary>
Motivation: 数字货币市场存在多重分形特性，将多重分形分析用于价格动态建模可增进对市场效率理解、改善波动率预测和检测关键转变，因此要分析多重分形源。

Method: 基于多重分形交叉相关分析（MFCCA）及其特例多重分形去趋势波动分析（MFDFA），将时间序列多重分形源分解方法应用于比特币、以太坊等数字货币市场代表工具。

Result: 重尾在产生宽多重分形谱中有重要作用，但时间相关性是多重分形的主要来源，且时间相关性在很大程度上不依赖波动分布尾部厚度。

Conclusion: 在数字货币市场的观察结果为所提出的时间序列多重分形源分解方法的有效性提供了有力论据。

Abstract: Multifractality in time series analysis characterizes the presence of
multiple scaling exponents, indicating heterogeneous temporal structures and
complex dynamical behaviors beyond simple monofractal models. In the context of
digital currency markets, multifractal properties arise due to the interplay of
long-range temporal correlations and heavy-tailed distributions of returns,
reflecting intricate market microstructure and trader interactions.
Incorporating multifractal analysis into the modeling of cryptocurrency price
dynamics enhances the understanding of market inefficiencies, may improve
volatility forecasting and facilitate the detection of critical transitions or
regime shifts. Based on the multifractal cross-correlation analysis (MFCCA)
whose spacial case is the multifractal detrended fluctuation analysis (MFDFA),
as the most commonly used practical tools for quantifying multifractality, in
the present contribution a recently proposed method of disentangling sources of
multifractality in time series was applied to the most representative
instruments from the digital market. They include Bitcoin (BTC), Ethereum
(ETH), decentralized exchanges (DEX) and non-fungible tokens (NFT). The results
indicate the significant role of heavy tails in generating a broad multifractal
spectrum. However, they also clearly demonstrate that the primary source of
multifractality are temporal correlations in the series, and without them,
multifractality fades out. It appears characteristic that these temporal
correlations, to a large extent, do not depend on the thickness of the tails of
the fluctuation distribution. These observations, made here in the context of
the digital currency market, provide a further strong argument for the validity
of the proposed methodology of disentangling sources of multifractality in time
series.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [164] [Efficient Inference for Coupled Hidden Markov Models in Continuous Time and Discrete Space](https://arxiv.org/abs/2510.12916)
*Giosue Migliorini,Padhraic Smyth*

Main category: stat.ML

TL;DR: 提出潜在交互粒子系统模型类，用含高效参数化的推理方法结合扭曲顺序蒙特卡罗采样方案，在潜在SIRS模型和野火蔓延动力学神经模型上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 高维设置下交互连续时间马尔可夫链系统推理难，含辅助信息的后验过程需近似。

Method: 引入潜在交互粒子系统参数化生成器，估计前瞻函数并采用高效参数化，结合扭曲顺序蒙特卡罗采样方案。

Result: 在潜在SIRS模型和基于真实数据训练的野火蔓延动力学神经模型的后验推理任务上验证了方法有效性。

Conclusion: 所提方法能有效解决含辅助信息的交互连续时间马尔可夫链系统的后验推理难题。

Abstract: Systems of interacting continuous-time Markov chains are a powerful model
class, but inference is typically intractable in high dimensional settings.
Auxiliary information, such as noisy observations, is typically only available
at discrete times, and incorporating it via a Doob's $h-$transform gives rise
to an intractable posterior process that requires approximation. We introduce
Latent Interacting Particle Systems, a model class parameterizing the generator
of each Markov chain in the system. Our inference method involves estimating
look-ahead functions (twist potentials) that anticipate future information, for
which we introduce an efficient parameterization. We incorporate this
approximation in a twisted Sequential Monte Carlo sampling scheme. We
demonstrate the effectiveness of our approach on a challenging posterior
inference task for a latent SIRS model on a graph, and on a neural model for
wildfire spread dynamics trained on real data.

</details>


### [165] [Simplicial Gaussian Models: Representation and Inference](https://arxiv.org/abs/2510.12983)
*Lorenzo Marinucci,Gabriele D'Acunto,Paolo Di Lorenzo,Sergio Barbarossa*

Main category: stat.ML

TL;DR: 提出将高斯概率图模型扩展到单纯复形的单纯高斯模型（SGM），开发最大似然推理算法并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 概率图模型局限于成对交互，为解决此问题并满足实际应用需求，提出SGM模型。

Method: 基于离散Hodge理论，在单个参数化高斯分布中联合建模顶点、边和三角形上的随机变量，开发最大似然推理算法。

Result: 在不同大小和稀疏度的合成单纯复形上的数值实验证实了算法的有效性。

Conclusion: SGM模型能有效扩展高斯概率图模型，且开发的推理算法可行。

Abstract: Probabilistic graphical models (PGMs) are powerful tools for representing
statistical dependencies through graphs in high-dimensional systems. However,
they are limited to pairwise interactions. In this work, we propose the
simplicial Gaussian model (SGM), which extends Gaussian PGM to simplicial
complexes. SGM jointly models random variables supported on vertices, edges,
and triangles, within a single parametrized Gaussian distribution. Our model
builds upon discrete Hodge theory and incorporates uncertainty at every
topological level through independent random components. Motivated by
applications, we focus on the marginal edge-level distribution while treating
node- and triangle-level variables as latent. We then develop a
maximum-likelihood inference algorithm to recover the parameters of the full
SGM and the induced conditional dependence structure. Numerical experiments on
synthetic simplicial complexes with varying size and sparsity confirm the
effectiveness of our algorithm.

</details>


### [166] [Conformal Inference for Open-Set and Imbalanced Classification](https://arxiv.org/abs/2510.13037)
*Tianmin Xie,Yanfei Zhou,Ziyi Liang,Stefano Favaro,Matteo Sesia*

Main category: stat.ML

TL;DR: 提出一种适用于高度不平衡和开放集分类的共形预测方法，解决现有方法局限性，在模拟和真实数据中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有分类方法要求有限已知标签空间，在测试时遇到新标签覆盖不足，预测旧标签可能过于保守，需改进。

Method: 计算新的共形p值测试新数据点是否属于未见类，开发基于标签频率的选择性样本分割算法，并通过重新加权维持有限样本保证。

Result: 该方法在具有无限可能标签的开放集场景下，预测集有有效覆盖，在极端类别不平衡时能产生更有信息的预测。

Conclusion: 所提出的共形预测方法在高度不平衡和开放集分类中有效，优于现有方法。

Abstract: This paper presents a conformal prediction method for classification in
highly imbalanced and open-set settings, where there are many possible classes
and not all may be represented in the data. Existing approaches require a
finite, known label space and typically involve random sample splitting, which
works well when there is a sufficient number of observations from each class.
Consequently, they have two limitations: (i) they fail to provide adequate
coverage when encountering new labels at test time, and (ii) they may become
overly conservative when predicting previously seen labels. To obtain valid
prediction sets in the presence of unseen labels, we compute and integrate into
our predictions a new family of conformal p-values that can test whether a new
data point belongs to a previously unseen class. We study these p-values
theoretically, establishing their optimality, and uncover an intriguing
connection with the classical Good--Turing estimator for the probability of
observing a new species. To make more efficient use of imbalanced data, we also
develop a selective sample splitting algorithm that partitions training and
calibration data based on label frequency, leading to more informative
predictions. Despite breaking exchangeability, this allows maintaining
finite-sample guarantees through suitable re-weighting. With both simulated and
real data, we demonstrate our method leads to prediction sets with valid
coverage even in challenging open-set scenarios with infinite numbers of
possible labels, and produces more informative predictions under extreme class
imbalance.

</details>


### [167] [A Multi-dimensional Semantic Surprise Framework Based on Low-Entropy Semantic Manifolds for Fine-Grained Out-of-Distribution Detection](https://arxiv.org/abs/2510.13093)
*Ningkang Peng,Yuzhe Mao,Yuhao Zhang,Linjin Qian,Qianfeng Yu,Yanhui Gu,Yi Chen,Li Kong*

Main category: stat.ML

TL;DR: 本文提出信息论框架解决OOD检测不能区分近远OOD风险问题，构建层次原型网络，引入语义惊喜向量和归一化语义风险指标，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法将其作为二分类问题，无法区分近远OOD风险，在需要细粒度风险分层的应用中存在安全瓶颈。

Method: 从概率视角转向信息论框架，形式化量化新样本语义惊喜的核心任务，提出三元分类挑战；设计层次原型网络构建低熵语义流形；引入语义惊喜向量；提出归一化语义风险指标。

Result: 在三元任务上达到新的最优，在传统二分类基准上也取得顶尖结果，在LSUN等数据集上使误报率降低超60%。

Conclusion: 所提出的信息论框架有效解决了现有OOD检测方法的局限性，具有良好性能。

Abstract: Out-of-Distribution (OOD) detection is a cornerstone for the safe deployment
of AI systems in the open world. However, existing methods treat OOD detection
as a binary classification problem, a cognitive flattening that fails to
distinguish between semantically close (Near-OOD) and distant (Far-OOD) unknown
risks. This limitation poses a significant safety bottleneck in applications
requiring fine-grained risk stratification. To address this, we propose a
paradigm shift from a conventional probabilistic view to a principled
information-theoretic framework. We formalize the core task as quantifying the
Semantic Surprise of a new sample and introduce a novel ternary classification
challenge: In-Distribution (ID) vs. Near-OOD vs. Far-OOD. The theoretical
foundation of our work is the concept of Low-Entropy Semantic Manifolds, which
are explicitly structured to reflect the data's intrinsic semantic hierarchy.
To construct these manifolds, we design a Hierarchical Prototypical Network. We
then introduce the Semantic Surprise Vector (SSV), a universal probe that
decomposes a sample's total surprise into three complementary and interpretable
dimensions: conformity, novelty, and ambiguity. To evaluate performance on this
new task, we propose the Normalized Semantic Risk (nSR), a cost-sensitive
metric. Experiments demonstrate that our framework not only establishes a new
state-of-the-art (sota) on the challenging ternary task, but its robust
representations also achieve top results on conventional binary benchmarks,
reducing the False Positive Rate by over 60% on datasets like LSUN.

</details>


### [168] [Gaussian Certified Unlearning in High Dimensions: A Hypothesis Testing Approach](https://arxiv.org/abs/2510.13094)
*Aaradhya Pandey,Arnab Auddy,Haolin Zou,Arian Maleki,Sanjeev Kulkarni*

Main category: stat.ML

TL;DR: 本文引入适用于高维的ε - Gaussian可认证性概念，理论分析基于牛顿法单步的去学习算法，发现单步牛顿法加高斯噪声可兼顾隐私和准确性，与前人结论不同，认为差异源于ε - 可认证性的次优性。


<details>
  <summary>Details</summary>
Motivation: 高维环境下机器学习去学习存在理论挑战，标准优化假设难以同时成立，需新方法解决。

Method: 引入ε - Gaussian可认证性概念，理论分析基于牛顿法单步的去学习算法。

Result: 单步牛顿法加校准后的高斯噪声可在高维环境下兼顾隐私和准确性，与前人认为至少需两步的结论不同。

Conclusion: 结论差异源于ε - 可认证性的次优性及与噪声添加机制的不兼容性，ε - Gaussian可认证性可最优克服此问题。

Abstract: Machine unlearning seeks to efficiently remove the influence of selected data
while preserving generalization. Significant progress has been made in low
dimensions $(p \ll n)$, but high dimensions pose serious theoretical challenges
as standard optimization assumptions of $\Omega(1)$ strong convexity and $O(1)$
smoothness of the per-example loss $f$ rarely hold simultaneously in
proportional regimes $(p\sim n)$. In this work, we introduce
$\varepsilon$-Gaussian certifiability, a canonical and robust notion
well-suited to high-dimensional regimes, that optimally captures a broad class
of noise adding mechanisms. Then we theoretically analyze the performance of a
widely used unlearning algorithm based on one step of the Newton method in the
high-dimensional setting described above. Our analysis shows that a single
Newton step, followed by a well-calibrated Gaussian noise, is sufficient to
achieve both privacy and accuracy in this setting. This result stands in sharp
contrast to the only prior work that analyzes machine unlearning in high
dimensions \citet{zou2025certified}, which relaxes some of the standard
optimization assumptions for high-dimensional applicability, but operates under
the notion of $\varepsilon$-certifiability. That work concludes %that a single
Newton step is insufficient even for removing a single data point, and that at
least two steps are required to ensure both privacy and accuracy. Our result
leads us to conclude that the discrepancy in the number of steps arises because
of the sub optimality of the notion of $\varepsilon$-certifiability and its
incompatibility with noise adding mechanisms, which $\varepsilon$-Gaussian
certifiability is able to overcome optimally.

</details>


### [169] [Near-Optimality of Contrastive Divergence Algorithms](https://arxiv.org/abs/2510.13438)
*Pierre Glaser,Kevin Han Huang,Arthur Gretton*

Main category: stat.ML

TL;DR: 对对比散度（CD）算法进行非渐近分析，表明在一定假设下CD能达到参数率$O(n^{-1 / 2})$，还分析了不同数据批处理方案，且CD接近最优。


<details>
  <summary>Details</summary>
Motivation: 以往工作仅确定了CD迭代对指数族分布渐近收敛到真实参数的速率为$O(n^{-1 / 3})$，本文要进一步分析。

Method: 在一些正则性假设下，对CD算法进行非渐近分析。

Result: CD能达到参数率$O(n^{-1 / 2})$，分析了不同数据批处理方案，CD渐近方差接近Cramér - Rao下界。

Conclusion: CD算法在满足假设条件下表现良好，能达到较好收敛速率且接近最优。

Abstract: We perform a non-asymptotic analysis of the contrastive divergence (CD)
algorithm, a training method for unnormalized models. While prior work has
established that (for exponential family distributions) the CD iterates
asymptotically converge at an $O(n^{-1 / 3})$ rate to the true parameter of the
data distribution, we show, under some regularity assumptions, that CD can
achieve the parametric rate $O(n^{-1 / 2})$. Our analysis provides results for
various data batching schemes, including the fully online and minibatch ones.
We additionally show that CD can be near-optimal, in the sense that its
asymptotic variance is close to the Cram\'er-Rao lower bound.

</details>


### [170] [Robust Minimax Boosting with Performance Guarantees](https://arxiv.org/abs/2510.13445)
*Santiago Mazuelas,Veronica Alvarez*

Main category: stat.ML

TL;DR: 本文提出鲁棒极小极大提升方法RMBoost，可最小化最坏情况错误概率，对一般类型标签噪声有鲁棒性，实验表明其抗噪声且分类精度高。


<details>
  <summary>Details</summary>
Motivation: 现有提升方法在标签噪声下性能下降，已有鲁棒方法存在未考虑现实噪声类型和有限训练规模、无噪声时精度也不佳等问题。

Method: 提出鲁棒极小极大提升方法RMBoost，最小化最坏情况错误概率。

Result: 实验结果证实RMBoost不仅能抵御标签噪声，还能提供较强的分类精度。

Conclusion: RMBoost对一般类型标签噪声具有鲁棒性，且有有限样本性能保证。

Abstract: Boosting methods often achieve excellent classification accuracy, but can
experience notable performance degradation in the presence of label noise.
Existing robust methods for boosting provide theoretical robustness guarantees
for certain types of label noise, and can exhibit only moderate performance
degradation. However, previous theoretical results do not account for realistic
types of noise and finite training sizes, and existing robust methods can
provide unsatisfactory accuracies, even without noise. This paper presents
methods for robust minimax boosting (RMBoost) that minimize worst-case error
probabilities and are robust to general types of label noise. In addition, we
provide finite-sample performance guarantees for RMBoost with respect to the
error obtained without noise and with respect to the best possible error (Bayes
risk). The experimental results corroborate that RMBoost is not only resilient
to label noise but can also provide strong classification accuracy.

</details>


### [171] [On the identifiability of causal graphs with multiple environments](https://arxiv.org/abs/2510.13583)
*Francesco Montagna*

Main category: stat.ML

TL;DR: 若有结构因果模型分布及两个噪声统计差异足够大的环境数据，可识别唯一因果图，还拓展了ICA与因果发现的对偶性。


<details>
  <summary>Details</summary>
Motivation: 解决从独立同分布观测数据进行因果发现普遍不适定的问题。

Method: 利用结构因果模型分布和两个环境的额外数据，结合噪声项高斯性约束，探讨放松约束的方法。

Result: 实现了用恒定数量环境和任意非线性机制恢复整个因果图，拓展了ICA与因果发现对偶性。

Conclusion: 在较少辅助信息下可实现因果发现，还提出放松噪声高斯性约束的潜在方法。

Abstract: Causal discovery from i.i.d. observational data is known to be generally
ill-posed. We demonstrate that if we have access to the distribution of a
structural causal model, and additional data from only two environments that
sufficiently differ in the noise statistics, the unique causal graph is
identifiable. Notably, this is the first result in the literature that
guarantees the entire causal graph recovery with a constant number of
environments and arbitrary nonlinear mechanisms. Our only constraint is the
Gaussianity of the noise terms; however, we propose potential ways to relax
this requirement. Of interest on its own, we expand on the well-known duality
between independent component analysis (ICA) and causal discovery; recent
advancements have shown that nonlinear ICA can be solved from multiple
environments, at least as many as the number of sources: we show that the same
can be achieved for causal discovery while having access to much less auxiliary
information.

</details>


### [172] [PriorGuide: Test-Time Prior Adaptation for Simulation-Based Inference](https://arxiv.org/abs/2510.13763)
*Yang Yang,Severi Rissanen,Paul E. Chang,Nasrulloh Loka,Daolang Huang,Arno Solin,Markus Heinonen,Luigi Acerbi*

Main category: stat.ML

TL;DR: 介绍PriorGuide技术，可让基于扩散的摊销推理方法在测试时灵活适应新先验，无需昂贵的再训练。


<details>
  <summary>Details</summary>
Motivation: 现有摊销模拟器推理方法的适用性受训练阶段先验分布限制，需克服该约束。

Method: 引入PriorGuide技术，利用新颖的引导近似，使训练好的扩散模型在测试时适应新先验。

Result: 可让用户在训练后轻松融入更新信息或专家知识。

Conclusion: PriorGuide增强了预训练推理模型的通用性。

Abstract: Amortized simulator-based inference offers a powerful framework for tackling
Bayesian inference in computational fields such as engineering or neuroscience,
increasingly leveraging modern generative methods like diffusion models to map
observed data to model parameters or future predictions. These approaches yield
posterior or posterior-predictive samples for new datasets without requiring
further simulator calls after training on simulated parameter-data pairs.
However, their applicability is often limited by the prior distribution(s) used
to generate model parameters during this training phase. To overcome this
constraint, we introduce PriorGuide, a technique specifically designed for
diffusion-based amortized inference methods. PriorGuide leverages a novel
guidance approximation that enables flexible adaptation of the trained
diffusion model to new priors at test time, crucially without costly
retraining. This allows users to readily incorporate updated information or
expert knowledge post-training, enhancing the versatility of pre-trained
inference models.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [173] [APRIL: Auxiliary Physically-Redundant Information in Loss - A physics-informed framework for parameter estimation with a gravitational-wave case study](https://arxiv.org/abs/2510.13677)
*Matteo Scialpi,Francesco Di Clemente,Leigh Smith,Michał Bejger*

Main category: gr-qc

TL;DR: 提出APRIL方法解决标准PINNs在多系统数据集上扩展性差的问题，在引力波参数估计上测试有显著精度提升。


<details>
  <summary>Details</summary>
Motivation: 标准PINNs在包含同一物理系统不同参数实现的多系统数据集上扩展性差。

Method: 在损失函数中加入辅助物理冗余信息（APRIL），通过辅助项重塑损失景观。

Result: 在引力波参数估计测试中，APRIL的测试精度最高提升一个数量级，尤其对难学习的参数。

Conclusion: APRIL为大型多系统数据集提供物理一致的学习方法，适用于未来含噪声和更广泛参数范围的引力波分析。

Abstract: Physics-Informed Neural Networks (PINNs) embed the partial differential
equations (PDEs) governing the system under study directly into the training of
Neural Networks, ensuring solutions that respect physical laws. While effective
for single-system problems, standard PINNs scale poorly to datasets containing
many realizations of the same underlying physics with varying parameters. To
address this limitation, we present a complementary approach by including
auxiliary physically-redundant information in loss (APRIL), i.e. augment the
standard supervised output-target loss with auxiliary terms which exploit exact
physical redundancy relations among outputs. We mathematically demonstrate that
these terms preserve the true physical minimum while reshaping the loss
landscape, improving convergence toward physically consistent solutions. As a
proof-of-concept, we benchmark APRIL on a fully-connected neural network for
gravitational wave (GW) parameter estimation (PE). We use simulated, noise-free
compact binary coalescence (CBC) signals, focusing on inspiral-frequency
waveforms to recover the chirp mass $\mathcal{M}$, the total mass
$M_\mathrm{tot}$, and symmetric mass ratio $\eta$ of the binary. In this
controlled setting, we show that APRIL achieves up to an order-of-magnitude
improvement in test accuracy, especially for parameters that are otherwise
difficult to learn. This method provides physically consistent learning for
large multi-system datasets and is well suited for future GW analyses involving
realistic noise and broader parameter ranges.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [174] [Escaping Neal's Funnel: a multi-stage sampling method for hierarchical models](https://arxiv.org/abs/2510.12917)
*Aiden Gundersen,Neil J. Cornish*

Main category: stat.ME

TL;DR: 本文提出分层采样方法避免Neal漏斗问题，分阶段采样，可借助归一化流，适用于有效重参数化计算成本高或易从广义分层模型采样的情况。


<details>
  <summary>Details</summary>
Motivation: 常见采样方法难以有效对Neal漏斗分布采样，现有解决技术存在局限。

Method: 将分层模型采样分为多阶段，第一阶段采样广义分层模型以减轻漏斗尖锐度，第二阶段在约束下从第一阶段估计密度中采样；用归一化流表示第一阶段分布。

Result: 提出一种避免Neal漏斗挑战的分层采样技术。

Conclusion: 该技术在有效重参数化计算成本高或有易采样的广义分层模型时有用。

Abstract: Neal's funnel refers to an exponential tapering in probability densities
common to Bayesian hierarchical models. Usual sampling methods, such as Markov
Chain Monte Carlo, struggle to efficiently sample the funnel. Reparameterizing
the model or analytically marginalizing local parameters are common techniques
to remedy sampling pathologies in distributions exhibiting Neal's funnel. In
this paper, we show that the challenges of Neal's funnel can be avoided by
performing the hierarchical analysis, well, hierarchically. That is, instead of
sampling all parameters of the hierarchical model jointly, we break the
sampling into multiple stages. The first stage samples a generalized
(higher-dimensional) hierarchical model which is parameterized to lessen the
sharpness of the funnel. The next stage samples from the estimated density of
the first stage, but under a constraint which restricts the sampling to recover
the marginal distributions on the hyper-parameters of the original
(lower-dimensional) hierarchical model. A normalizing flow can be used to
represent the distribution from the first stage, such that it can easily be
sampled from for the second stage of the analysis. This technique is useful
when effective reparameterizations are computationally expensive to calculate,
or a generalized hierarchical model already exists from which it is easy to
sample.

</details>


### [175] [Scalable Bayesian inference for high-dimensional mixed-type multivariate spatial data](https://arxiv.org/abs/2510.13233)
*Arghya Mukherjee,Arnab Hazra,Dootika Vats*

Main category: stat.ME

TL;DR: 提出基于潜在多元高斯过程的贝叶斯空间方法用于联合建模混合类型多变量空间响应，并用Vecchia近似解决计算瓶颈，通过模拟和实例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以对混合类型多变量空间响应进行联合建模，需要新方法。

Method: 使用潜在多元高斯过程提出贝叶斯空间方法，用Vecchia近似进行快速后验推断和预测，采用基于马尔可夫链蒙特卡罗的推理方法。

Result: 建立了模型的关键理论性质，通过模拟研究和实际数据应用展示了方法的有效性。

Conclusion: 所提出的方法能有效解决混合类型多变量空间响应的联合建模问题。

Abstract: Spatial generalized linear mixed-effects methods are popularly used to model
spatially indexed univariate responses. However, with modern technology, it is
common to observe vector-valued mixed-type responses, e.g., a combination of
binary, count, or continuous types, at each location. Methods that allow joint
modeling of such mixed-type multivariate spatial responses are rare. Using
latent multivariate Gaussian processes (GPs), we present a class of Bayesian
spatial methods that can be employed for any combination of exponential family
responses. Since multivariate GP-based methods can suffer from computational
bottlenecks when the number of spatial locations is high, we further employ a
computationally efficient Vecchia approximation for fast posterior inference
and prediction. Key theoretical properties of the proposed model, such as
identifiability and the structure of the induced covariance, are established.
Our approach employs a Markov chain Monte Carlo-based inference method that
utilizes elliptical slice sampling in a blocked Metropolis-within-Gibbs
sampling framework. We illustrate the efficacy of the proposed method through
simulation studies and a real-data application on joint modeling of wildfire
counts and burnt areas across the United States.

</details>


### [176] [The $φ$-PCA Framework: A Unified and Efficiency-Preserving Approach with Robust Variants](https://arxiv.org/abs/2510.13159)
*Hung Hung,Zhi-Yu Jou,Su-Yun Huang,Shinto Eguchi*

Main category: stat.ME

TL;DR: 介绍了 φ - PCA 框架以解决标准 PCA 在大规模应用中的问题，该框架统一了鲁棒和分布式 PCA，HM - PCA 有最优排序鲁棒性，且鲁棒性随分区数增加，其分区聚合原则可用于数据处理。


<details>
  <summary>Details</summary>
Motivation: 标准 PCA 对离群值敏感且在分布式环境有局限，影响其在现代大规模应用中的有效性，需解决这些问题。

Method: 引入 φ - PCA 框架，用合适的 φ 函数聚合多个局部估计，其中 HM - PCA 对应 φ(u)=u⁻¹。

Result: φ - PCA 方法保留标准 PCA 的渐近效率，增强排序鲁棒性，实现更准确的特征子空间估计；HM - PCA 有最优排序鲁棒性；鲁棒性随分区数增加。

Conclusion: φ - PCA 的分区聚合原则为鲁棒和分布式数据分析提供通用策略。

Abstract: Principal component analysis (PCA) is a fundamental tool in multivariate
statistics, yet its sensitivity to outliers and limitations in distributed
environments restrict its effectiveness in modern large-scale applications. To
address these challenges, we introduce the $\phi$-PCA framework which provides
a unified formulation of robust and distributed PCA. The class of $\phi$-PCA
methods retains the asymptotic efficiency of standard PCA, while aggregating
multiple local estimates using a proper $\phi$ function enhances
ordering-robustness, leading to more accurate eigensubspace estimation under
contamination. Notably, the harmonic mean PCA (HM-PCA), corresponding to the
choice $\phi(u)=u^{-1}$, achieves optimal ordering-robustness and is
recommended for practical use. Theoretical results further show that robustness
increases with the number of partitions, a phenomenon seldom explored in the
literature on robust or distributed PCA. Altogether, the partition-aggregation
principle underlying $\phi$-PCA offers a general strategy for developing robust
and efficiency-preserving methodologies applicable to both robust and
distributed data analysis.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [177] [Semantic knowledge guides innovation and drives cultural evolution](https://arxiv.org/abs/2510.12837)
*Anil Yaman,Shen Tian,Björn Lindström*

Main category: cs.MA

TL;DR: 研究表明语义知识是人类累积文化的关键认知过程，通过文化进化基于主体的模型和大规模行为实验发现语义知识与社会学习协同促进创新。


<details>
  <summary>Details</summary>
Motivation: 不清楚产生创新的认知过程，探究语义知识在累积创新中的作用。

Method: 采用文化进化基于主体的模型和大规模行为实验（N = 1,243），让个体完成物品组合成新创新的任务。

Result: 语义知识和社会学习协同增强创新，无语义知识的参与者表现不佳，依赖浅层探索策略。

Conclusion: 语义知识是实现人类累积文化的关键认知过程。

Abstract: Cumulative cultural evolution enables human societies to generate
increasingly complex knowledge and technology over generations. While social
learning transmits innovations between individuals and generations, the
cognitive processes that generate these innovations remain poorly understood.
Here, we demonstrate that semantic knowledge-structured associations between
concepts and their functions-provides cognitive scaffolding for cumulative
innovation by guiding exploration toward plausible and meaningful actions. We
tested this hypothesis using a cultural evolutionary agent-based model and a
large-scale behavioural experiment (N = 1,243), in which individuals performed
a task requiring the combination of items into novel innovations. Across both
approaches, semantic knowledge and social learning interact synergistically to
enhance innovation. Behaviorally, participants without access to semantic
knowledge performed no better than chance, even when social learning was
available, and relied on shallow exploration strategies. These findings suggest
that semantic knowledge is a key cognitive process enabling human cumulative
culture.

</details>


### [178] [KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems](https://arxiv.org/abs/2510.12872)
*Hancheng Ye,Zhengqi Gao,Mingyuan Ma,Qinsi Wang,Yuzhe Fu,Ming-Yu Chung,Yueqian Lin,Zhijian Liu,Jianyi Zhang,Danyang Zhuo,Yiran Chen*

Main category: cs.MA

TL;DR: 多智能体大语言模型系统存在重复处理重叠上下文的效率问题，提出无训练框架KVCOMM，可重用KV缓存，在多任务中实现高重用率和显著加速。


<details>
  <summary>Details</summary>
Motivation: 多智能体大语言模型系统在处理重叠上下文时存在大量开销，现有KV缓存方法无法直接用于多智能体场景。

Method: 提出无训练框架KVCOMM，通过参考存储不同前缀下缓存偏差的锚点池，估计和调整共享内容的KV缓存，且锚点池在线维护更新。

Result: KVCOMM在多种多智能体工作负载中实现超70%的重用率，在特定设置下比标准预填充管道实现高达7.8倍的加速，将TTFT从约430毫秒降至约55毫秒。

Conclusion: KVCOMM能有效解决多智能体大语言模型系统的效率问题，在不降低质量的前提下提高处理速度。

Abstract: Multi-agent large language model (LLM) systems are increasingly adopted for
complex language processing tasks that require communication and coordination
among agents. However, these systems often suffer substantial overhead from
repeated reprocessing of overlapping contexts across agents. In typical
pipelines, once an agent receives a message from its predecessor, the full
context-including prior turns-must be reprocessed from scratch, leading to
inefficient processing. While key-value (KV) caching is an effective solution
for avoiding redundant computation in single-agent settings where prefixes
remain unchanged, it cannot be directly reused in multi-agent scenarios due to
diverging prefixes introduced by agent-specific context extensions. We identify
that the core challenge lies in the offset variance of KV-caches across agents.
To address this, we propose KVCOMM, a training-free framework that enables
efficient prefilling in multi-agent inference by reusing KV-caches and aligning
cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM
estimates and adjusts KV-caches for shared content by referencing a pool of
cached examples-termed anchors-that store observed cache deviations under
varying prefixes. The anchor pool is maintained and updated online, allowing
dynamic adaptation to distinct user requests and context structures. KVCOMM
achieves over 70% reuse rate across diverse multi-agent workloads, including
retrieval-augmented generation, math reasoning, and collaborative coding tasks,
all without quality degradation. Particularly, when each fully-connected agent
receives 1K input tokens with 512 prefix tokens and 512 output tokens under a
five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard
prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.

</details>


### [179] [Agentic Discovery: Closing the Loop with Cooperative Agents](https://arxiv.org/abs/2510.13081)
*J. Gregory Pauloski,Kyle Chard,Ian T. Foster*

Main category: cs.MA

TL;DR: 数据驱动方法加速科研任务，但人类决策限制发现速度，需合作智能体实现自主发现，这需要AI和基础设施进步。


<details>
  <summary>Details</summary>
Motivation: 当前人类决策任务限制了科学发现的速度，需要解决该问题以加速科学发现。

Method: 提出需要合作智能体来增强人类的作用以实现自主发现。

Result: 无明确结果阐述。

Conclusion: 实现合作智能体需要在AI和基础设施方面取得进展。

Abstract: As data-driven methods, artificial intelligence (AI), and automated workflows
accelerate scientific tasks, we see the rate of discovery increasingly limited
by human decision-making tasks such as setting objectives, generating
hypotheses, and designing experiments. We postulate that cooperative agents are
needed to augment the role of humans and enable autonomous discovery. Realizing
such agents will require progress in both AI and infrastructure.

</details>


### [180] [AOAD-MAT: Transformer-based multi-agent deep reinforcement learning model considering agents' order of action decisions](https://arxiv.org/abs/2510.13343)
*Shota Takayama,Katsuhide Fujita*

Main category: cs.MA

TL;DR: 提出考虑智能体决策顺序的AOAD - MAT模型，经实验验证优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有MARL模型未明确考虑智能体决策顺序的重要性，需要改进。

Method: 提出AOAD - MAT模型，采用基于Transformer的演员 - 评论家架构，引入预测下一行动智能体的子任务，集成到基于近端策略优化的损失函数。

Result: 在StarCraft Multi - Agent Challenge和Multi - Agent MuJoCo基准测试中，AOAD - MAT模型优于现有MAT和其他基线模型。

Conclusion: 调整AOAD顺序在MARL中有效。

Abstract: Multi-agent reinforcement learning focuses on training the behaviors of
multiple learning agents that coexist in a shared environment. Recently, MARL
models, such as the Multi-Agent Transformer (MAT) and ACtion dEpendent deep
Q-learning (ACE), have significantly improved performance by leveraging
sequential decision-making processes. Although these models can enhance
performance, they do not explicitly consider the importance of the order in
which agents make decisions. In this paper, we propose an Agent Order of Action
Decisions-MAT (AOAD-MAT), a novel MAT model that considers the order in which
agents make decisions. The proposed model explicitly incorporates the sequence
of action decisions into the learning process, allowing the model to learn and
predict the optimal order of agent actions. The AOAD-MAT model leverages a
Transformer-based actor-critic architecture that dynamically adjusts the
sequence of agent actions. To achieve this, we introduce a novel MARL
architecture that cooperates with a subtask focused on predicting the next
agent to act, integrated into a Proximal Policy Optimization based loss
function to synergistically maximize the advantage of the sequential
decision-making. The proposed method was validated through extensive
experiments on the StarCraft Multi-Agent Challenge and Multi-Agent MuJoCo
benchmarks. The experimental results show that the proposed AOAD-MAT model
outperforms existing MAT and other baseline models, demonstrating the
effectiveness of adjusting the AOAD order in MARL.

</details>


### [181] [Altruistic Ride Sharing: A Community-Driven Approach to Short-Distance Mobility](https://arxiv.org/abs/2510.13227)
*Divyanshu Singh,Ashman Mehra,Snehanshu Saha,Santonu Sarkar*

Main category: cs.MA

TL;DR: 本文提出利他拼车（ARS）框架，用多智能体强化学习等方法，经纽约市出租车数据验证，该框架能减少出行距离和排放等，是传统拼车的可扩展替代方案。


<details>
  <summary>Details</summary>
Motivation: 城市出行面临拥堵和高油耗问题，逐利的拼车平台忽视公平和可持续性。

Method: 引入基于利他积分、参与者角色可互换的去中心化点对点ARS框架，集成多智能体强化学习进行动态拼车匹配，用博弈论保证公平，用人口模型维持长期平衡。

Result: 使用纽约市出租车数据表明，与无拼车和基于优化的基线相比，ARS减少了出行距离和排放，提高了车辆利用率，促进了公平参与。

Conclusion: ARS是传统拼车的可扩展、社区驱动的替代方案，能使个人行为与城市可持续发展目标一致。

Abstract: Urban mobility faces persistent challenges of congestion and fuel
consumption, specifically when people choose a private, point-to-point commute
option. Profit-driven ride-sharing platforms prioritize revenue over fairness
and sustainability. This paper introduces Altruistic Ride-Sharing (ARS), a
decentralized, peer-to-peer mobility framework where participants alternate
between driver and rider roles based on altruism points rather than monetary
incentives. The system integrates multi-agent reinforcement learning (MADDPG)
for dynamic ride-matching, game-theoretic equilibrium guarantees for fairness,
and a population model to sustain long-term balance. Using real-world New York
City taxi data, we demonstrate that ARS reduces travel distance and emissions,
increases vehicle utilization, and promotes equitable participation compared to
both no-sharing and optimization-based baselines. These results establish ARS
as a scalable, community-driven alternative to conventional ride-sharing,
aligning individual behavior with collective urban sustainability goals.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [182] [Imperative Quantum Programming with Ownership and Borrowing in Guppy](https://arxiv.org/abs/2510.13082)
*Mark Koch,Agustín Borgna,Craig Roy,Alan Lawrence,Kartik Singhal,Seyon Sivarajah,Ross Duncan*

Main category: cs.PL

TL;DR: 开发结合人体工程学线性类型与命令式语义的量子类型系统并在Guppy语言实现


<details>
  <summary>Details</summary>
Motivation: 线性类型在命令式量子编程中未广泛应用，需开发结合线性类型与命令式语义且有安全保证的量子类型系统

Method: 文中未详细提及，实现相关想法于Guppy编程语言

Result: 将相关想法实现于Quantinuum的Guppy编程语言

Conclusion: 文中未明确提及最终结论，但实现了开发目标初步成果

Abstract: Linear types enforce no-cloning and no-deleting theorems in functional
quantum programming. However, in imperative quantum programming, they have not
gained widespread adoption. This work aims to develop a quantum type system
that combines ergonomic linear typing with imperative semantics and maintains
safety guarantees. All ideas presented here have been implemented in
Quantinuum's Guppy programming language.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [183] [Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU](https://arxiv.org/abs/2510.13546)
*Ruiqi Ye,Mikel Luján*

Main category: cs.CV

TL;DR: 本文对比GPU和FPGA在视觉SLAM特征检测中的加速效果，发现非学习型检测器GPU表现更好，学习型检测器FPGA表现更佳，硬件加速可提升V - SLAM性能。


<details>
  <summary>Details</summary>
Motivation: 特征检测在SLAM实现中耗时，且SLAM常部署在功率受限平台，需研究硬件加速特征检测器。

Method: 对比现代SoC（Nvidia Jetson Orin和AMD Versal）上GPU加速的FAST、Harris、SuperPoint实现与FPGA加速的对应实现。

Result: 非学习型检测器GPU实现性能和能效更好；学习型检测器FPGA实现性能和能效更优；GPU加速的V - SLAM更准确；硬件加速可减少全局束调整模块调用频率且不损失精度。

Conclusion: 硬件加速能提升V - SLAM流水线性能，不同硬件在不同类型特征检测器中有不同优势。

Abstract: Feature detection is a common yet time-consuming module in Simultaneous
Localization and Mapping (SLAM) implementations, which are increasingly
deployed on power-constrained platforms, such as drones. Graphics Processing
Units (GPUs) have been a popular accelerator for computer vision in general,
and feature detection and SLAM in particular.
  On the other hand, System-on-Chips (SoCs) with integrated Field Programmable
Gate Array (FPGA) are also widely available. This paper presents the first
study of hardware-accelerated feature detectors considering a Visual SLAM
(V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated
FAST, Harris, and SuperPoint implementations against the FPGA-accelerated
counterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal).
  The evaluation shows that when using a non-learning-based feature detector
such as FAST and Harris, their GPU implementations, and the GPU-accelerated
V-SLAM can achieve better run-time performance and energy efficiency than the
FAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM.
However, when considering a learning-based detector such as SuperPoint, its
FPGA implementation can achieve better run-time performance and energy
efficiency (up to 3.1$\times$ and 1.4$\times$ improvements, respectively) than
the GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable
run-time performance compared to the GPU-accelerated V-SLAM, with better FPS in
2 out of 5 dataset sequences. When considering the accuracy, the results show
that the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated
V-SLAM in general. Last but not least, the use of hardware acceleration for
feature detection could further improve the performance of the V-SLAM pipeline
by having the global bundle adjustment module invoked less frequently without
sacrificing accuracy.

</details>


### [184] [Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation](https://arxiv.org/abs/2510.12953)
*Xiao He,Huangxuan Zhao,Guojia Wan,Wei Zhou,Yanxing Liu,Juhua Liu,Yongchao Xu,Yong Luo,Dacheng Tao,Bo Du*

Main category: cs.CV

TL;DR: 介绍针对胎儿超声的医学AI系统FetalMind，提出SED方法并构建FetalSigma - 1M数据集，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉 - 语言模型在胎儿超声任务上表现不佳，存在多视图图像推理、疾病多和图像多样性等挑战，需开发适用的系统。

Method: 提出Salient Epistemic Disentanglement (SED)方法，将专家策划的二分图注入模型，通过强化学习解耦视图 - 疾病关联；构建FetalSigma - 1M数据集用于大规模训练。

Result: FetalMind在所有孕期阶段均优于开源和闭源基线，平均增益达+14%，关键病症准确率提高+61.2%，且高效、稳定、可扩展。

Conclusion: FetalMind是适用于胎儿超声报告生成和诊断的有效医学AI系统。

Abstract: Recent medical vision-language models have shown promise on tasks such as
VQA, report generation, and anomaly detection. However, most are adapted to
structured adult imaging and underperform in fetal ultrasound, which poses
challenges of multi-view image reasoning, numerous diseases, and image
diversity. To bridge this gap, we introduce FetalMind, a medical AI system
tailored to fetal ultrasound for both report generation and diagnosis. Guided
by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which
injects an expert-curated bipartite graph into the model to decouple
view-disease associations and to steer preference selection along clinically
faithful steps via reinforcement learning. This design mitigates variability
across diseases and heterogeneity across views, reducing learning bottlenecks
while aligning the model's inference with obstetric practice. To train
FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale
fetal ultrasound report corpus, comprising 20K reports from twelve medical
centers, addressing the scarcity of domain data. Extensive experiments show
that FetalMind outperforms open- and closed-source baselines across all
gestational stages, achieving +14% average gains and +61.2% higher accuracy on
critical conditions while remaining efficient, stable, and scalable. Project
Page: https://hexiao0275.github.io/FetalMind.

</details>


### [185] [SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models](https://arxiv.org/abs/2510.13042)
*Zhengxu Tang,Zizheng Wang,Luning Wang,Zitao Shuai,Chenhao Zhang,Siyu Qian,Yirui Wu,Bohao Wang,Haosong Rao,Zhenyu Yang,Chenwei Wu*

Main category: cs.CV

TL;DR: 提出SeqBench基准评估T2V生成的叙事连贯性，揭示现有模型局限并提供改进框架。


<details>
  <summary>Details</summary>
Motivation: 现有T2V模型难生成连贯叙事视频，且现有基准未评估叙事连贯性，需填补此空白。

Method: 构建含320个提示、2560个人工标注视频的SeqBench数据集，设计基于DTG的自动评估指标。

Result: DTG指标与人工标注强相关，评估发现现有T2V模型存在多方面局限。

Conclusion: SeqBench为评估T2V叙事连贯性提供系统框架，为未来模型改进提供见解。

Abstract: Text-to-video (T2V) generation models have made significant progress in
creating visually appealing videos. However, they struggle with generating
coherent sequential narratives that require logical progression through
multiple events. Existing T2V benchmarks primarily focus on visual quality
metrics but fail to evaluate narrative coherence over extended sequences. To
bridge this gap, we present SeqBench, a comprehensive benchmark for evaluating
sequential narrative coherence in T2V generation. SeqBench includes a carefully
designed dataset of 320 prompts spanning various narrative complexities, with
2,560 human-annotated videos generated from 8 state-of-the-art T2V models.
Additionally, we design a Dynamic Temporal Graphs (DTG)-based automatic
evaluation metric, which can efficiently capture long-range dependencies and
temporal ordering while maintaining computational efficiency. Our DTG-based
metric demonstrates a strong correlation with human annotations. Through
systematic evaluation using SeqBench, we reveal critical limitations in current
T2V models: failure to maintain consistent object states across multi-action
sequences, physically implausible results in multi-object scenarios, and
difficulties in preserving realistic timing and ordering relationships between
sequential actions. SeqBench provides the first systematic framework for
evaluating narrative coherence in T2V generation and offers concrete insights
for improving sequential reasoning capabilities in future models. Please refer
to https://videobench.github.io/SeqBench.github.io/ for more details.

</details>


### [186] [SceneAdapt: Scene-aware Adaptation of Human Motion Diffusion](https://arxiv.org/abs/2510.13044)
*Jungbin Cho,Minsu Kim,Jisoo Kim,Ce Zheng,Laszlo A. Jeni,Ming-Hsuan Yang,Youngjae Yu,Seonjoo Kim*

Main category: cs.CV

TL;DR: 提出SceneAdapt框架，通过两个适应阶段将场景感知注入文本条件运动模型，实验证明有效，代码和模型将发布。


<details>
  <summary>Details</summary>
Motivation: 现有运动生成方法孤立处理运动语义或场景感知，构建兼具丰富文本 - 运动覆盖和精确场景交互的大规模数据集极具挑战。

Method: 利用不相交的场景 - 运动和文本 - 运动数据集，分中间帧生成和场景感知中间帧生成两个阶段，使用运动中间帧生成作为代理任务，第一阶段引入关键帧层，第二阶段添加场景调节层。

Result: SceneAdapt有效将场景感知注入文本到运动模型，并分析了感知出现的机制。

Conclusion: SceneAdapt能有效解决现有运动生成方法问题，实现场景感知注入。

Abstract: Human motion is inherently diverse and semantically rich, while also shaped
by the surrounding scene. However, existing motion generation approaches
address either motion semantics or scene-awareness in isolation, since
constructing large-scale datasets with both rich text--motion coverage and
precise scene interactions is extremely challenging. In this work, we introduce
SceneAdapt, a framework that injects scene awareness into text-conditioned
motion models by leveraging disjoint scene--motion and text--motion datasets
through two adaptation stages: inbetweening and scene-aware inbetweening. The
key idea is to use motion inbetweening, learnable without text, as a proxy task
to bridge two distinct datasets and thereby inject scene-awareness to
text-to-motion models. In the first stage, we introduce keyframing layers that
modulate motion latents for inbetweening while preserving the latent manifold.
In the second stage, we add a scene-conditioning layer that injects scene
geometry by adaptively querying local context through cross-attention.
Experimental results show that SceneAdapt effectively injects scene awareness
into text-to-motion models, and we further analyze the mechanisms through which
this awareness emerges. Code and models will be released.

</details>


### [187] [True Self-Supervised Novel View Synthesis is Transferable](https://arxiv.org/abs/2510.13063)
*Thomas W. Mitchel,Hyunwoo Ryu,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 本文指出判断模型是否具备真正的新视角合成（NVS）能力的关键是可迁移性，提出首个无几何的自监督模型XFactor实现真正的NVS，引入新指标量化可迁移性，实验显示其性能优于先前模型。


<details>
  <summary>Details</summary>
Motivation: 确定判断模型是否真正具备新视角合成（NVS）能力的关键标准，并解决先前自监督NVS工作中预测姿势不可迁移的问题。

Method: 提出XFactor模型，结合成对姿势估计与输入输出的简单增强方案，无需3D归纳偏差或多视图几何概念。引入新指标量化可迁移性。

Result: XFactor在无约束潜在姿势变量下实现可迁移性，通过大规模实验显示其显著优于先前无姿势NVS变压器，探测实验表明潜在姿势与现实世界姿势高度相关。

Conclusion: XFactor是首个能实现真正NVS的无几何自监督模型，在新视角合成任务中有出色表现。

Abstract: In this paper, we identify that the key criterion for determining whether a
model is truly capable of novel view synthesis (NVS) is transferability:
Whether any pose representation extracted from one video sequence can be used
to re-render the same camera trajectory in another. We analyze prior work on
self-supervised NVS and find that their predicted poses do not transfer: The
same set of poses lead to different camera trajectories in different 3D scenes.
Here, we present XFactor, the first geometry-free self-supervised model capable
of true NVS. XFactor combines pair-wise pose estimation with a simple
augmentation scheme of the inputs and outputs that jointly enables
disentangling camera pose from scene content and facilitates geometric
reasoning. Remarkably, we show that XFactor achieves transferability with
unconstrained latent pose variables, without any 3D inductive biases or
concepts from multi-view geometry -- such as an explicit parameterization of
poses as elements of SE(3). We introduce a new metric to quantify
transferability, and through large-scale experiments, we demonstrate that
XFactor significantly outperforms prior pose-free NVS transformers, and show
that latent poses are highly correlated with real-world poses through probing
experiments.

</details>


### [188] [DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2510.13108)
*Jingyu Song,Zhenxin Li,Shiyi Lan,Xinglong Sun,Nadine Chang,Maying Shen,Joshua Chen,Katherine A. Skinner,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 提出DriveCritic框架评估自动驾驶规划器，含数据集与模型，实验显示其优于现有指标。


<details>
  <summary>Details</summary>
Motivation: 现有指标如EPDMS在复杂场景缺乏上下文感知，难以与人类判断对齐。

Method: 引入DriveCritic框架，包括DriveCritic数据集和基于VLM的DriveCritic模型，采用两阶段监督和强化学习管道微调模型。

Result: DriveCritic在匹配人类偏好方面显著优于现有指标和基线，有强上下文感知能力。

Conclusion: 工作为评估自动驾驶系统提供更可靠、符合人类判断的基础。

Abstract: Benchmarking autonomous driving planners to align with human judgment remains
a critical challenge, as state-of-the-art metrics like the Extended Predictive
Driver Model Score (EPDMS) lack context awareness in nuanced scenarios. To
address this, we introduce DriveCritic, a novel framework featuring two key
contributions: the DriveCritic dataset, a curated collection of challenging
scenarios where context is critical for correct judgment and annotated with
pairwise human preferences, and the DriveCritic model, a Vision-Language Model
(VLM) based evaluator. Fine-tuned using a two-stage supervised and
reinforcement learning pipeline, the DriveCritic model learns to adjudicate
between trajectory pairs by integrating visual and symbolic context.
Experiments show DriveCritic significantly outperforms existing metrics and
baselines in matching human preferences and demonstrates strong context
awareness. Overall, our work provides a more reliable, human-aligned foundation
to evaluating autonomous driving systems.

</details>


### [189] [Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences](https://arxiv.org/abs/2510.13201)
*Jing Yang,Qiyao Wei,Jiaxin Pei*

Main category: cs.CV

TL;DR: AI会议快速增长使同行评审系统不堪重负，现有改进措施有问题，提出Paper Copilot系统，包含评审档案、数据集和分析，支持可重复研究，助力改进评审系统。


<details>
  <summary>Details</summary>
Motivation: AI会议增长使同行评审系统出现诸多问题，现有改进措施引发新问题，需更好解决方案。

Method: 开发Paper Copilot系统，创建多计算机科学会议同行评审的数字档案，提供开放数据集，对ICLR多年评审进行大规模实证分析。

Result: 构建了Paper Copilot系统，有数字档案和开放数据集，完成ICLR评审分析。

Conclusion: Paper Copilot支持可重复研究，有助于社区改进同行评审系统，使其更稳健、透明和可靠。

Abstract: The rapid growth of AI conferences is straining an already fragile
peer-review system, leading to heavy reviewer workloads, expertise mismatches,
inconsistent evaluation standards, superficial or templated reviews, and
limited accountability under compressed timelines. In response, conference
organizers have introduced new policies and interventions to preserve review
standards. Yet these ad-hoc changes often create further concerns and confusion
about the review process, leaving how papers are ultimately accepted - and how
practices evolve across years - largely opaque. We present Paper Copilot, a
system that creates durable digital archives of peer reviews across a wide
range of computer-science venues, an open dataset that enables researchers to
study peer review at scale, and a large-scale empirical analysis of ICLR
reviews spanning multiple years. By releasing both the infrastructure and the
dataset, Paper Copilot supports reproducible research on the evolution of peer
review. We hope these resources help the community track changes, diagnose
failure modes, and inform evidence-based improvements toward a more robust,
transparent, and reliable peer-review system.

</details>


### [190] [MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation](https://arxiv.org/abs/2510.13208)
*Lianlian Liu,YongKang He,Zhaojie Chu,Xiaofen Xing,Xiangmin Xu*

Main category: cs.CV

TL;DR: 提出MimicParts框架解决语音生成风格化3D人体运动的问题，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前风格编码方法简化风格多样性、忽略区域运动风格差异且未考虑运动风格随语音节奏和情感动态变化的问题。

Method: 提出MimicParts框架，通过部分感知风格注入和部分感知去噪网络，划分身体区域编码局部运动风格，用部分感知注意力块让节奏和情感线索精确引导各身体区域。

Result: 方法优于现有方法，生成自然且富有表现力的3D人体运动序列。

Conclusion: MimicParts框架能有效提升风格化3D人体运动生成效果。

Abstract: Generating stylized 3D human motion from speech signals presents substantial
challenges, primarily due to the intricate and fine-grained relationships among
speech signals, individual styles, and the corresponding body movements.
Current style encoding approaches either oversimplify stylistic diversity or
ignore regional motion style differences (e.g., upper vs. lower body), limiting
motion realism. Additionally, motion style should dynamically adapt to changes
in speech rhythm and emotion, but existing methods often overlook this. To
address these issues, we propose MimicParts, a novel framework designed to
enhance stylized motion generation based on part-aware style injection and
part-aware denoising network. It divides the body into different regions to
encode localized motion styles, enabling the model to capture fine-grained
regional differences. Furthermore, our part-aware attention block allows rhythm
and emotion cues to guide each body region precisely, ensuring that the
generated motion aligns with variations in speech rhythm and emotional state.
Experimental results show that our method outperforming existing methods
showcasing naturalness and expressive 3D human motion sequences.

</details>


### [191] [What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging](https://arxiv.org/abs/2510.13232)
*Inha Kang,Youngsun Lim,Seonho Lee,Jiho Choi,Junsuk Choe,Hyunjung Shim*

Main category: cs.CV

TL;DR: 现有视觉语言模型在理解否定方面存在肯定偏差，本文提出新数据集管道CoVAND和轻量级适配方法NegToMe，显著提升否定基准测试性能，推动现实检测应用的否定理解。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在理解否定上的肯定偏差问题，尤其是在描述对象检测任务中的严重局限性。

Method: 提出新数据集管道CoVAND，用系统性思维链和基于VQA的管道生成高质量否定数据；提出NegToMe文本令牌合并模块，结合参数高效的LoRA微调方法。

Result: 显著提高了具有挑战性的否定基准测试的性能，降低了误报率，在OVDEval上使NMS - AP最多提高10.8分，且能推广到最先进的视觉语言模型。

Conclusion: 这项工作是解决现实检测应用中否定理解问题的关键一步。

Abstract: State-of-the-art vision-language models (VLMs) suffer from a critical failure
in understanding negation, often referred to as affirmative bias. This
limitation is particularly severe in described object detection (DOD) tasks. To
address this, we propose two primary contributions: (1) a new dataset pipeline
and (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a
dataset constructed with a systematic chain-of-thought (CoT) and VQA-based
pipeline to generate high-quality, instance-grounded negation data. Second, we
propose NegToMe, a novel text token merging module that directly tackles the
architectural cause of affirmative bias. NegToMe fundamentally addresses the
structural loss of negation cues in tokenization, grouping them with attributes
into coherent semantic phrases. It maintains correct polarity at the input
level, enabling robust negation understanding even with limited data. For
instance, to prevent a model from treating the fragmented tokens "not" and
"girl" as simply "girl", NegToMe binds them into a single token whose meaning
is correctly distinguished from that of "girl" alone. This module is integrated
with a parameter-efficient and strategic LoRA fine-tuning approach. Our method
significantly improves performance on challenging negation benchmarks with a
lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval
and demonstrating generalization to SoTA VLMs. This work marks a crucial step
forward in addressing negation understanding for real-world detection
applications.

</details>


### [192] [Real-Time Crowd Counting for Embedded Systems with Lightweight Architecture](https://arxiv.org/abs/2510.13250)
*Zhiyuan Zhao,Yubin Wen,Siyu Yang,Lichen Ning,Yuandong Liu,Junyu Gao*

Main category: cs.CV

TL;DR: 针对嵌入式系统人群计数任务，设计了具有stem - encoder - decoder结构的超实时模型，实验表明该模型适合嵌入式系统超实时人群计数，推理速度最快。


<details>
  <summary>Details</summary>
Motivation: 现有计数方法在嵌入式系统实际应用中存在模型参数过多、计算复杂等问题，而嵌入式系统实际应用要求模型实时性高。

Method: 设计具有stem - encoder - decoder结构的模型，在stem网络用大卷积核扩大感受野，编码器部分用条件通道加权和多分支局部融合块合并多尺度特征，在编码器顶部添加特征金字塔网络。

Result: 在三个基准测试上实验，网络适合嵌入式系统超实时人群计数，保证了有竞争力的精度，推理速度最快，如在NVIDIA GTX 1080Ti上达381.7 FPS，在NVIDIA Jetson TX1上达71.9 FPS。

Conclusion: 所提出的模型能满足嵌入式系统超实时人群计数需求，且推理速度具有优势。

Abstract: Crowd counting is a task of estimating the number of the crowd through
images, which is extremely valuable in the fields of intelligent security,
urban planning, public safety management, and so on. However, the existing
counting methods have some problems in practical application on embedded
systems for these fields, such as excessive model parameters, abundant complex
calculations, etc. The practical application of embedded systems requires the
model to be real-time, which means that the model is fast enough. Considering
the aforementioned problems, we design a super real-time model with a
stem-encoder-decoder structure for crowd counting tasks, which achieves the
fastest inference compared with state-of-the-arts. Firstly, large convolution
kernels in the stem network are used to enlarge the receptive field, which
effectively extracts detailed head information. Then, in the encoder part, we
use conditional channel weighting and multi-branch local fusion block to merge
multi-scale features with low computational consumption. This part is crucial
to the super real-time performance of the model. Finally, the feature pyramid
networks are added to the top of the encoder to alleviate its incomplete fusion
problems. Experiments on three benchmarks show that our network is suitable for
super real-time crowd counting on embedded systems, ensuring competitive
accuracy. At the same time, the proposed network reasoning speed is the
fastest. Specifically, the proposed network achieves 381.7 FPS on NVIDIA GTX
1080Ti and 71.9 FPS on NVIDIA Jetson TX1.

</details>


### [193] [Self-Augmented Visual Contrastive Decoding](https://arxiv.org/abs/2510.13315)
*Eun Woo Im,Muhammad Kashif Ali,Vivek Gupta*

Main category: cs.CV

TL;DR: 本文提出无训练解码策略提升大视觉语言模型事实一致性，实验显示效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型存在幻觉问题，现有视觉对比解码方法因通用视觉增强忽略文本查询上下文而效果受限。

Method: 提出无训练解码策略，包括利用模型内在知识的自增强提示策略和基于输出稀疏性自适应调整候选词大小的自适应阈值算法。

Result: 在四个大视觉语言模型和七个基准测试上的实验表明，该解码策略显著提升事实一致性。

Conclusion: 强调结合查询依赖增强和熵感知解码对改善大视觉语言模型有效生成的重要性。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal
capabilities, but they inherit the tendency to hallucinate from their
underlying language models. While visual contrastive decoding has been proposed
to mitigate this issue, existing methods often apply generic visual
augmentations that disregard the specific context provided by the text query,
limiting their effectiveness. This study introduces a novel training-free
decoding strategy that addresses these limitations, featuring two key
contributions. First, a self-augmentation prompting strategy that leverages the
intrinsic knowledge of the model to dynamically align semantics between the
query and the visual augmentation. Second, an adaptive thresholding algorithm
that adaptively adjusts next token candidate size based on the output sparsity,
utilizing full information from the logit distribution. Extensive experiments
across four LVLMs and seven benchmarks demonstrate that the proposed decoding
significantly enhances factual consistency compared to state-of-the-art
decoding methods. This work highlights the importance of integrating
query-dependent augmentation and entropy-aware decoding for improving effective
generation of LVLMs.

</details>


### [194] [Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity](https://arxiv.org/abs/2510.13364)
*MingZe Tang,Jubal Chandy Jacob*

Main category: cs.CV

TL;DR: 研究提示设计对视觉语言模型零样本分类人类姿势的影响，发现高性能模型用简单提示效果好，低性能模型用详细提示有改善。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽能零样本分类，但提示设计对识别视觉相似类别（如人类姿势）的影响尚不明确。

Method: 在285张图像的COCO衍生数据集上，用三层提示设计评估OpenCLIP、MetaCLIP 2和SigLip等模型对坐、站、走/跑的零样本分类。

Result: 高性能模型（MetaCLIP 2和OpenCLIP）用最简单提示效果最佳，增加描述会降低性能；低性能模型SigLip用含身体提示的详细提示在模糊类别上分类有改善。

Conclusion: 提示设计对视觉语言模型零样本分类人类姿势有显著影响，存在“提示过拟合”现象。

Abstract: Recent Vision-Language Models (VLMs) enable zero-shot classification by
aligning images and text in a shared space, a promising approach for
data-scarce conditions. However, the influence of prompt design on recognizing
visually similar categories, such as human postures, is not well understood.
This study investigates how prompt specificity affects the zero-shot
classification of sitting, standing, and walking/running on a small, 285-image
COCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2,
and SigLip, were evaluated using a three-tiered prompt design that
systematically increases linguistic detail. Our findings reveal a compelling,
counter-intuitive trend: for the highest-performing models (MetaCLIP 2 and
OpenCLIP), the simplest, most basic prompts consistently achieve the best
results. Adding descriptive detail significantly degrades performance for
instance, MetaCLIP 2's multi-class accuracy drops from 68.8\% to 55.1\% a
phenomenon we term "prompt overfitting". Conversely, the lower-performing
SigLip model shows improved classification on ambiguous classes when given more
descriptive, body-cue-based prompts.

</details>


### [195] [SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms](https://arxiv.org/abs/2510.12901)
*Haithem Turki,Qi Wu,Xin Kang,Janick Martinez Esturo,Shengyu Huang,Ruilong Li,Zan Gojcic,Riccardo de Lutio*

Main category: cs.CV

TL;DR: 提出SimULi方法，可实时渲染任意相机模型和LiDAR数据，解决现有方法局限，速度快且保真度高。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法存在渲染速度低、只能渲染针孔相机模型等问题，多传感器模拟存在跨传感器不一致问题，需更好方法用于自动驾驶机器人严格测试。

Method: 扩展3DGUT以支持LiDAR，采用自动平铺策略和基于射线的剔除；设计因子化3D高斯表示和锚定策略。

Result: SimULi比光线追踪方法快10 - 20倍，比基于光栅化的工作快1.5 - 10倍，在多个相机和LiDAR指标上匹配或超过现有最先进方法的保真度。

Conclusion: SimULi是一种有效的能实时渲染任意相机模型和LiDAR数据的方法，可用于自动驾驶机器人的严格测试。

Abstract: Rigorous testing of autonomous robots, such as self-driving vehicles, is
essential to ensure their safety in real-world deployments. This requires
building high-fidelity simulators to test scenarios beyond those that can be
safely or exhaustively collected in the real-world. Existing neural rendering
methods based on NeRF and 3DGS hold promise but suffer from low rendering
speeds or can only render pinhole camera models, hindering their suitability to
applications that commonly require high-distortion lenses and LiDAR data.
Multi-sensor simulation poses additional challenges as existing methods handle
cross-sensor inconsistencies by favoring the quality of one modality at the
expense of others. To overcome these limitations, we propose SimULi, the first
method capable of rendering arbitrary camera models and LiDAR data in
real-time. Our method extends 3DGUT, which natively supports complex camera
models, with LiDAR support, via an automated tiling strategy for arbitrary
spinning LiDAR models and ray-based culling. To address cross-sensor
inconsistencies, we design a factorized 3D Gaussian representation and
anchoring strategy that reduces mean camera and depth error by up to 40%
compared to existing methods. SimULi renders 10-20x faster than ray tracing
approaches and 1.5-10x faster than prior rasterization-based work (and handles
a wider range of camera models). When evaluated on two widely benchmarked
autonomous driving datasets, SimULi matches or exceeds the fidelity of existing
state-of-the-art methods across numerous camera and LiDAR metrics.

</details>


### [196] [UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning](https://arxiv.org/abs/2510.13515)
*Tiancheng Gu,Kaicheng Yang,Kaichen Zhang,Xiang An,Ziyong Feng,Yueyi Zhang,Weidong Cai,Jiankang Deng,Lidong Bing*

Main category: cs.CV

TL;DR: 本文提出新型通用多模态嵌入模型UniME - V2及重排序模型UniME - V2 - Reranker，在多个任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态嵌入模型在负样本挖掘时难以捕捉候选样本语义差异、负样本缺乏多样性、嵌入区分真假和难负样本能力有限。

Method: 先通过全局检索构建潜在难负样本集，引入MLLM - as - a - Judge机制生成软语义匹配分数用于难负样本挖掘和缓解一对一映射约束；提出重排序模型UniME - V2 - Reranker，通过联合成对和列表优化方法训练。

Result: 在MMEB基准和多个检索任务上进行实验，方法在所有任务上平均达到SOTA性能。

Conclusion: 所提方法能有效提升多模态嵌入模型的性能。

Abstract: Universal multimodal embedding models are foundational to various tasks.
Existing approaches typically employ in-batch negative mining by measuring the
similarity of query-candidate pairs. However, these methods often struggle to
capture subtle semantic differences among candidates and lack diversity in
negative samples. Moreover, the embeddings exhibit limited discriminative
ability in distinguishing false and hard negatives. In this paper, we leverage
the advanced understanding capabilities of MLLMs to enhance representation
learning and present a novel Universal Multimodal Embedding (UniME-V2) model.
Our approach first constructs a potential hard negative set through global
retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes
MLLMs to assess the semantic alignment of query-candidate pairs and generate
soft semantic matching scores. These scores serve as a foundation for hard
negative mining, mitigating the impact of false negatives and enabling the
identification of diverse, high-quality hard negatives. Furthermore, the
semantic matching scores are used as soft labels to mitigate the rigid
one-to-one mapping constraint. By aligning the similarity matrix with the soft
semantic matching score matrix, the model learns semantic distinctions among
candidates, significantly enhancing its discriminative capacity. To further
improve performance, we propose UniME-V2-Reranker, a reranking model trained on
our mined hard negatives through a joint pairwise and listwise optimization
approach. We conduct comprehensive experiments on the MMEB benchmark and
multiple retrieval tasks, demonstrating that our method achieves
state-of-the-art performance on average across all tasks.

</details>


### [197] [Modeling Cultural Bias in Facial Expression Recognition with Adaptive Agents](https://arxiv.org/abs/2510.13557)
*David Freire-Obregón,José Salas-Cáceres,Javier Lorenzo-Navarro,Oliverio J. Santana,Daniel Hernández-Sosa,Modesto Castrillón-Santana*

Main category: cs.CV

TL;DR: 提出基于代理的流式基准测试，研究跨文化构成和渐进模糊对人脸识别鲁棒性的影响，发现不同文化群体的性能退化曲线不同，混合群体有中间模式。


<details>
  <summary>Details</summary>
Motivation: 现有面部表情识别（FER）评估大多假设数据同质且图像质量高，需研究其在文化差异和视觉条件退化下的鲁棒性。

Method: 引入基于代理的流式基准测试，代理在CLIP特征空间操作，环境提供高斯模糊输入，研究不同文化构成和空间接触结构。

Result: 不同文化群体有不对称的退化曲线，混合群体有中间模式，平衡混合可缓解早期退化，不平衡设置会放大多数群体在高模糊下的弱点。

Conclusion: 量化了文化构成和交互结构在感知条件恶化时对FER鲁棒性的影响。

Abstract: Facial expression recognition (FER) must remain robust under both cultural
variation and perceptually degraded visual conditions, yet most existing
evaluations assume homogeneous data and high-quality imagery. We introduce an
agent-based, streaming benchmark that reveals how cross-cultural composition
and progressive blurring interact to shape face recognition robustness. Each
agent operates in a frozen CLIP feature space with a lightweight residual
adapter trained online at sigma=0 and fixed during testing. Agents move and
interact on a 5x5 lattice, while the environment provides inputs with
sigma-scheduled Gaussian blur. We examine monocultural populations
(Western-only, Asian-only) and mixed environments with balanced (5/5) and
imbalanced (8/2, 2/8) compositions, as well as different spatial contact
structures. Results show clear asymmetric degradation curves between cultural
groups: JAFFE (Asian) populations maintain higher performance at low blur but
exhibit sharper drops at intermediate stages, whereas KDEF (Western)
populations degrade more uniformly. Mixed populations exhibit intermediate
patterns, with balanced mixtures mitigating early degradation, but imbalanced
settings amplify majority-group weaknesses under high blur. These findings
quantify how cultural composition and interaction structure influence the
robustness of FER as perceptual conditions deteriorate.

</details>


### [198] [CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas](https://arxiv.org/abs/2510.13669)
*Zian Li,Muhan Zhang*

Main category: cs.CV

TL;DR: 提出CanvasMAR视频MAR模型，引入画布机制和组合无分类器引导，实验表明其能以更少自回归步骤生成高质量视频，在Kinetics - 600数据集表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视频MAR模型存在慢启动问题和误差累积问题，需要改进。

Method: 提出CanvasMAR模型，引入画布机制作为掩码生成起点，采用组合无分类器引导扩大时空条件，使用基于噪声的画布增强鲁棒性。

Result: 在BAIR和Kinetics - 600基准测试中，CanvasMAR以更少自回归步骤生成高质量视频，在Kinetics - 600数据集上表现出色，可与基于扩散的方法相媲美。

Conclusion: CanvasMAR模型有效缓解了现有视频MAR模型的问题，是一种高效的视频生成方法。

Abstract: Masked autoregressive models (MAR) have recently emerged as a powerful
paradigm for image and video generation, combining the flexibility of masked
modeling with the potential of continuous tokenizer. However, video MAR models
suffer from two major limitations: the slow-start problem, caused by the lack
of a structured global prior at early sampling stages, and error accumulation
across the autoregression in both spatial and temporal dimensions. In this
work, we propose CanvasMAR, a novel video MAR model that mitigates these issues
by introducing a canvas mechanism--a blurred, global prediction of the next
frame, used as the starting point for masked generation. The canvas provides
global structure early in sampling, enabling faster and more coherent frame
synthesis. Furthermore, we introduce compositional classifier-free guidance
that jointly enlarges spatial (canvas) and temporal conditioning, and employ
noise-based canvas augmentation to enhance robustness. Experiments on the BAIR
and Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality
videos with fewer autoregressive steps. Our approach achieves remarkable
performance among autoregressive models on Kinetics-600 dataset and rivals
diffusion-based methods.

</details>


### [199] [MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion](https://arxiv.org/abs/2510.13702)
*Minjung Shin,Hyunin Cho,Sooyeon Go,Jin-Hwa Kim,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出多视图定制任务，设计MVCustom框架实现多视图一致性和定制保真度，实验表明其能同时实现多视图生成和定制。


<details>
  <summary>Details</summary>
Motivation: 现有多视图生成模型不支持带几何一致性的定制，定制模型缺乏显式视点控制，且定制训练数据稀缺，现有模型难泛化到不同提示。

Method: 提出MVCustom框架，训练阶段用特征场表示学习主体身份和几何，结合带密集时空注意力的文本到视频扩散骨干；推理阶段引入深度感知特征渲染和一致感知潜在补全技术。

Result: 广泛实验表明MVCustom是唯一能同时实现多视图生成和定制的框架。

Conclusion: MVCustom框架能有效解决现有模型问题，实现多视图相机姿态控制和定制。

Abstract: Multi-view generation with camera pose control and prompt-based customization
are both essential elements for achieving controllable generative models.
However, existing multi-view generation models do not support customization
with geometric consistency, whereas customization models lack explicit
viewpoint control, making them challenging to unify. Motivated by these gaps,
we introduce a novel task, multi-view customization, which aims to jointly
achieve multi-view camera pose control and customization. Due to the scarcity
of training data in customization, existing multi-view generation models, which
inherently rely on large-scale datasets, struggle to generalize to diverse
prompts. To address this, we propose MVCustom, a novel diffusion-based
framework explicitly designed to achieve both multi-view consistency and
customization fidelity. In the training stage, MVCustom learns the subject's
identity and geometry using a feature-field representation, incorporating the
text-to-video diffusion backbone enhanced with dense spatio-temporal attention,
which leverages temporal coherence for multi-view consistency. In the inference
stage, we introduce two novel techniques: depth-aware feature rendering
explicitly enforces geometric consistency, and consistent-aware latent
completion ensures accurate perspective alignment of the customized subject and
surrounding backgrounds. Extensive experiments demonstrate that MVCustom is the
only framework that simultaneously achieves faithful multi-view generation and
customization.

</details>


### [200] [Sample-Centric Multi-Task Learning for Detection and Segmentation of Industrial Surface Defects](https://arxiv.org/abs/2510.13226)
*Hang-Cheng Dong,Yibo Jiao,Fupeng Wei,Guodong Liu,Dong Ye,Bingguo Liu*

Main category: cs.CV

TL;DR: 提出以样本为中心的多任务学习框架和评估套件，提高样本级决策可靠性和缺陷定位完整性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在实际生产线面临前景背景不平衡等问题，优化目标与质量控制决策粒度不匹配，样本级稳定性不足。

Method: 基于共享编码器架构，联合学习样本级缺陷分类和像素级掩码定位；提出决策关联指标Seg_mIoU和Seg_Recall。

Result: 在两个基准数据集实验中，大幅提高样本级决策可靠性和缺陷定位完整性。

Conclusion: 所提方法有效解决现有模型问题，提升工业表面缺陷检测性能。

Abstract: Industrial surface defect inspection for sample-wise quality control (QC)
must simultaneously decide whether a given sample contains defects and localize
those defects spatially. In real production lines, extreme
foreground-background imbalance, defect sparsity with a long-tailed scale
distribution, and low contrast are common. As a result, pixel-centric training
and evaluation are easily dominated by large homogeneous regions, making it
difficult to drive models to attend to small or low-contrast defects-one of the
main bottlenecks for deployment. Empirically, existing models achieve strong
pixel-overlap metrics (e.g., mIoU) but exhibit insufficient stability at the
sample level, especially for sparse or slender defects. The root cause is a
mismatch between the optimization objective and the granularity of QC
decisions. To address this, we propose a sample-centric multi-task learning
framework and evaluation suite. Built on a shared-encoder architecture, the
method jointly learns sample-level defect classification and pixel-level mask
localization. Sample-level supervision modulates the feature distribution and,
at the gradient level, continually boosts recall for small and low-contrast
defects, while the segmentation branch preserves boundary and shape details to
enhance per-sample decision stability and reduce misses. For evaluation, we
propose decision-linked metrics, Seg_mIoU and Seg_Recall, which remove the bias
of classical mIoU caused by empty or true-negative samples and tightly couple
localization quality with sample-level decisions. Experiments on two benchmark
datasets demonstrate that our approach substantially improves the reliability
of sample-level decisions and the completeness of defect localization.

</details>


### [201] [Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models](https://arxiv.org/abs/2510.13237)
*Haochuan Xu,Yun Sing Koh,Shuhuai Huang,Zirun Zhou,Di Wang,Jun Sakuma,Jingfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出针对VLA模型的对抗性补丁攻击及防御策略，评估显示攻击提升任务失败率，防御有效缓解。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人学习取得进展，但对抗鲁棒性研究不足。

Method: 提出模型无关的嵌入破坏补丁攻击（EDPA），通过破坏语义对齐和最大化潜在表征差异生成补丁；提出视觉编码器的对抗性微调方案。

Result: 在LIBERO基准上，EDPA大幅提升VLA模型任务失败率，提出的防御有效缓解性能下降。

Conclusion: 提出的攻击和防御策略对VLA模型的对抗鲁棒性研究有重要意义，代码可在官网获取。

Abstract: Vision-Language-Action (VLA) models have achieved revolutionary progress in
robot learning, enabling robots to execute complex physical robot tasks from
natural language instructions. Despite this progress, their adversarial
robustness remains underexplored. In this work, we propose both adversarial
patch attack and corresponding defense strategies for VLA models. We first
introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic
adversarial attack that generates patches directly placeable within the
camera's view. In comparison to prior methods, EDPA can be readily applied to
different VLA models without requiring prior knowledge of the model
architecture, or the controlled robotic manipulator. EDPA constructs these
patches by (i) disrupting the semantic alignment between visual and textual
latent representations, and (ii) maximizing the discrepancy of latent
representations between adversarial and corresponding clean visual inputs.
Through the optimization of these objectives, EDPA distorts the VLA's
interpretation of visual information, causing the model to repeatedly generate
incorrect actions and ultimately result in failure to complete the given
robotic task. To counter this, we propose an adversarial fine-tuning scheme for
the visual encoder, in which the encoder is optimized to produce similar latent
representations for both clean and adversarially perturbed visual inputs.
Extensive evaluations on the widely recognized LIBERO robotic simulation
benchmark demonstrate that EDPA substantially increases the task failure rate
of cutting-edge VLA models, while our proposed defense effectively mitigates
this degradation. The codebase is accessible via the homepage at
https://edpa-attack.github.io/.

</details>


### [202] [Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs](https://arxiv.org/abs/2510.13740)
*Mustafa Munir,Alex Zhang,Radu Marculescu*

Main category: cs.CV

TL;DR: 提出LogViG模型，用LSGC构建图，结合多尺度高分辨率架构，在图像分类和语义分割任务上优于现有架构。


<details>
  <summary>Details</summary>
Motivation: 现有ViG图构建方法在大图像上成本高，SVGA存在过压缩和信息缺失问题，需新的图构建方法提升性能。

Method: 提出Logarithmic Scalable Graph Construction (LSGC) 图构建方法，构建LogViG混合CNN - GNN模型，引入高分辨率分支并融合多尺度特征。

Result: LogViG在图像分类和语义分割任务的准确率、GMACs和参数方面优于现有ViG、CNN和ViT架构，Ti - LogViG在ImageNet - 1K上平均top - 1准确率达79.9%。

Conclusion: 通过提出的LSGC在图构建中利用长距离链接，能超越当前最先进ViG的性能。

Abstract: Vision graph neural networks (ViG) have demonstrated promise in vision tasks
as a competitive alternative to conventional convolutional neural nets (CNN)
and transformers (ViTs); however, common graph construction methods, such as
k-nearest neighbor (KNN), can be expensive on larger images. While methods such
as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step
scale can lead to over-squashing and missing multiple connections to gain the
same information that could be gained from a long-range link. Through this
observation, we propose a new graph construction method, Logarithmic Scalable
Graph Construction (LSGC) to enhance performance by limiting the number of
long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model
that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and
high-resolution architectures, we introduce and apply a high-resolution branch
and fuse features between our high-resolution and low-resolution branches for a
multi-scale high-resolution Vision GNN network. Extensive experiments show that
LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,
GMACs, and parameters on image classification and semantic segmentation tasks.
Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on
ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average
accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%
reduction in GMACs. Our work shows that leveraging long-range links in graph
construction for ViGs through our proposed LSGC can exceed the performance of
current state-of-the-art ViGs. Code is available at
https://github.com/mmunir127/LogViG-Official.

</details>


### [203] [RECODE: Reasoning Through Code Generation for Visual Question Answering](https://arxiv.org/abs/2510.13756)
*Junhong Shen,Mu Cai,Bo Hu,Ameet Talwalkar,David A Ross,Cordelia Schmid,Alireza Fathi*

Main category: cs.CV

TL;DR: 提出RECODE框架，利用derendering实现可验证的视觉推理，在多个视觉推理基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理图表等结构化视觉信息时缺乏验证机制，难以进行精确推理。

Method: 提出RECODE框架，先生成多个候选程序重现输入图像，再用评估器选择最匹配的重构并迭代优化代码。

Result: 在CharXiv、ChartQA和Geometry3K等视觉推理基准上，RECODE显著优于不使用代码或仅用代码绘制辅助线或裁剪的方法。

Conclusion: 将视觉感知建立在可执行代码上为更准确和可验证的多模态推理提供新途径。

Abstract: Multimodal Large Language Models (MLLMs) struggle with precise reasoning for
structured visuals like charts and diagrams, as pixel-based perception lacks a
mechanism for verification. To address this, we propose to leverage derendering
-- the process of reverse-engineering visuals into executable code -- as a new
modality for verifiable visual reasoning. Specifically, we propose RECODE, an
agentic framework that first generates multiple candidate programs to reproduce
the input image. It then uses a critic to select the most faithful
reconstruction and iteratively refines the code. This process not only
transforms an ambiguous perceptual task into a verifiable, symbolic problem,
but also enables precise calculations and logical inferences later on. On
various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,
RECODE significantly outperforms methods that do not leverage code or only use
code for drawing auxiliary lines or cropping. Our work demonstrates that
grounding visual perception in executable code provides a new path toward more
accurate and verifiable multimodal reasoning.

</details>


### [204] [Scaling Vision Transformers for Functional MRI with Flat Maps](https://arxiv.org/abs/2510.13768)
*Connor Lane,Daniel Z. Kaplan,Tanishq Mathew Abraham,Paul S. Scotti*

Main category: cs.CV

TL;DR: 将4D fMRI数据转换为2D活动平面图视频，用MAE框架训练Vision Transformers，发现建模性能与数据集大小有幂律关系，模型学习的表征支持下游分类任务。


<details>
  <summary>Details</summary>
Motivation: 解决现代深度学习架构适配fMRI数据时的数据输入表征问题，弥合fMRI与自然图像的模态差距。

Method: 将4D fMRI数据转换为2D活动平面图视频，使用MAE框架在HCP的fMRI视频上训练Vision Transformers。

Result: 观察到掩蔽fMRI建模性能与数据集大小呈严格幂律关系，下游分类基准显示模型学习的表征支持跨主体状态解码和跨脑状态的主体特异性特征解码。

Conclusion: 该工作是构建fMRI数据基础模型开放科学项目的一部分，代码和数据集公开。

Abstract: A key question for adapting modern deep learning architectures to functional
MRI (fMRI) is how to represent the data for model input. To bridge the modality
gap between fMRI and natural images, we transform the 4D volumetric fMRI data
into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K
hours of fMRI flat map videos from the Human Connectome Project using the
spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI
modeling performance improves with dataset size according to a strict power
scaling law. Downstream classification benchmarks show that our model learns
rich representations supporting both fine-grained state decoding across
subjects, as well as subject-specific trait decoding across changes in brain
state. This work is part of an ongoing open science project to build foundation
models for fMRI data. Our code and datasets are available at
https://github.com/MedARC-AI/fmri-fm.

</details>


### [205] [Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs](https://arxiv.org/abs/2510.13795)
*Yi Zhang,Bolin Ni,Xin-Sheng Chen,Heng-Rui Zhang,Yongming Rao,Houwen Peng,Qinglin Lu,Han Hu,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CV

TL;DR: 本文指出全开源多模态大语言模型因监督微调数据质量差而落后，提出新数据集Honey - Data - 15M、数据整理管道HoneyPipe及框架DataStudio，训练Bee - 8B模型达SOTA，为社区提供资源。


<details>
  <summary>Details</summary>
Motivation: 解决全开源多模态大语言模型在监督微调时数据质量差，现有开源数据集噪声多、复杂推理数据不足的问题。

Method: 引入新SFT数据集Honey - Data - 15M，采用多轮清洗和双级CoT增强策略；提出数据整理管道HoneyPipe及其框架DataStudio；用Honey - Data - 15M训练Bee - 8B模型。

Result: Bee - 8B模型达到全开源多模态大语言模型的SOTA，性能与半开源模型相当甚至超越。

Conclusion: 注重数据质量是开发具有竞争力的全开源多模态大语言模型的关键途径。

Abstract: Fully open multimodal large language models (MLLMs) currently lag behind
proprietary counterparts, primarily due to a significant gap in data quality
for supervised fine-tuning (SFT). Existing open-source datasets are often
plagued by widespread noise and a critical deficit in complex reasoning data,
such as Chain-of-Thought (CoT), which hinders the development of advanced model
capabilities. Addressing these challenges, our work makes three primary
contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising
approximately 15 million QA pairs, processed through multiple cleaning
techniques and enhanced with a novel dual-level (short and long) CoT enrichment
strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its
underlying framework DataStudio, providing the community with a transparent and
adaptable methodology for data curation that moves beyond static dataset
releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B
model on Honey-Data-15M. Experiments show that Bee-8B establishes a new
state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is
competitive with, and in some cases surpasses, recent semi-open models such as
InternVL3.5-8B. Our work delivers to the community a suite of foundational
resources, including: the Honey-Data-15M corpus; the full-stack suite
comprising HoneyPipe and DataStudio; training recipes; an evaluation harness;
and the model weights. This effort demonstrates that a principled focus on data
quality is a key pathway to developing fully open MLLMs that are highly
competitive with their semi-open counterparts.

</details>


### [206] [Near-Infrared Hyperspectral Imaging Applications in Food Analysis -- Improving Algorithms and Methodologies](https://arxiv.org/abs/2510.13452)
*Ole-Christian Galbo Engstrøm*

Main category: cs.CV

TL;DR: 本文研究近红外高光谱成像（NIR - HSI）在食品质量分析中的应用，通过四项研究和五个假设展开，比较CNN和PLS模型，开发两个开源Python包。


<details>
  <summary>Details</summary>
Motivation: 探索近红外高光谱成像在食品质量分析中的应用，比较不同建模方法的效果。

Method: 通过四项研究和五个研究假设，比较基于卷积神经网络（CNNs）和偏最小二乘法（PLS）的模型。

Result: 联合时空光谱分析的CNN模型在某些参数建模上优于空间分析的CNN和光谱分析的PLS；2D CNN添加光谱卷积层可增强预测性能；PLS在分析样本化学参数平均含量时表现良好；2D CNN可解决PLS在化学参数空间分布建模中的问题；对大麦发芽能力建模结果不确定；开发两个开源Python包。

Conclusion: 在不同的食品质量分析场景中，CNN和PLS各有优势，可根据具体需求选择合适的模型；开发的开源Python包有助于相关建模工作。

Abstract: This thesis investigates the application of near-infrared hyperspectral
imaging (NIR-HSI) for food quality analysis. The investigation is conducted
through four studies operating with five research hypotheses. For several
analyses, the studies compare models based on convolutional neural networks
(CNNs) and partial least squares (PLS). Generally, joint spatio-spectral
analysis with CNNs outperforms spatial analysis with CNNs and spectral analysis
with PLS when modeling parameters where chemical and physical visual
information are relevant. When modeling chemical parameters with a
2-dimensional (2D) CNN, augmenting the CNN with an initial layer dedicated to
performing spectral convolution enhances its predictive performance by learning
a spectral preprocessing similar to that applied by domain experts. Still,
PLS-based spectral modeling performs equally well for analysis of the mean
content of chemical parameters in samples and is the recommended approach.
Modeling the spatial distribution of chemical parameters with NIR-HSI is
limited by the ability to obtain spatially resolved reference values.
Therefore, a study used bulk mean references for chemical map generation of fat
content in pork bellies. A PLS-based approach gave non-smooth chemical maps and
pixel-wise predictions outside the range of 0-100\%. Conversely, a 2D CNN
augmented with a spectral convolution layer mitigated all issues arising with
PLS. The final study attempted to model barley's germinative capacity by
analyzing NIR spectra, RGB images, and NIR-HSI images. However, the results
were inconclusive due to the dataset's low degree of germination. Additionally,
this thesis has led to the development of two open-sourced Python packages. The
first facilitates fast PLS-based modeling, while the second facilitates very
fast cross-validation of PLS and other classical machine learning models with a
new algorithm.

</details>


### [207] [Generative Universal Verifier as Multimodal Meta-Reasoner](https://arxiv.org/abs/2510.13804)
*Xinchen Zhang,Xiaoying Zhang,Youbin Wu,Yanbin Cao,Renrui Zhang,Ruihang Chu,Ling Yang,Yujiu Yang*

Main category: cs.CV

TL;DR: 介绍Generative Universal Verifier用于多模态推理，构建ViVerBench评估，设计数据构建管道训练OmniVerifier - 7B，提出OmniVerifier - TTS范式，提升生成与推理能力。


<details>
  <summary>Details</summary>
Motivation: 为视觉语言模型和统一多模态模型的下一代多模态推理提供反思和细化视觉结果的能力，解决现有模型在可靠视觉验证上与人类水平的差距。

Method: 构建ViVerBench基准；设计自动化管道构建数据并训练OmniVerifier - 7B；提出OmniVerifier - TTS顺序测试时间缩放范式。

Result: 现有VLM在ViVerBench任务中表现不佳；OmniVerifier - 7B在ViVerBench上有+8.3的提升；OmniVerifier - TTS在T2I - ReasonBench和GenEval++分别有+3.7和+4.3的提升，优于Best - of - N。

Conclusion: OmniVerifier增强了生成过程中的可靠反思和可扩展的测试时细化，推动了更可信和可控的下一代推理系统发展。

Abstract: We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.
This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning. Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification. (2) We design two
automated pipelines to construct large-scale visual verification data and train
OmniVerifier-7B, the first omni-capable generative verifier trained for
universal visual verification and achieves notable gains on ViVerBench(+8.3).
Through training, we identify three atomic capabilities in visual verification
and demonstrate how they generalize and interact synergistically. (3) We
propose OmniVerifier-TTS, a sequential test-time scaling paradigm that
leverages the universal verifier to bridge image generation and editing within
unified models, enhancing the upper bound of generative ability through
iterative fine-grained optimization. Beyond generation, we extend universal
verifier to broader world-modeling interleaved reasoning scenarios.
Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),
and GenEval++(+4.3), outperforming existing parallel test-time scaling methods,
such as Best-of-N. By endowing multimodal reasoning with reliable visual
verification, OmniVerifier advances both reliable reflection during generation
and scalable test-time refinement, marking a step toward more trustworthy and
controllable next-generation reasoning systems.

</details>


### [208] [ExpressNet-MoE: A Hybrid Deep Neural Network for Emotion Recognition](https://arxiv.org/abs/2510.13493)
*Deeptimaan Banerjee,Prateek Gothwal,Ashis Kumer Biswas*

Main category: cs.CV

TL;DR: 提出ExpressNet - MoE模型解决真实世界面部情感识别难题，在多数据集上评估显示其适应性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现实中面部情感识别因多种因素困难，现有模型在参与度检测等应用中受FER限制，需新模型解决。

Method: 提出ExpressNet - MoE混合深度学习模型，融合CNN和MoE框架，通过多尺度特征提取提高情感识别精度，包含CNN特征提取器、MoE模块和残差网络骨干。

Result: 模型在AffectNet (v7)、AffectNet (v8)、RAF - DB、FER - 2013数据集上分别达到74.77%、72.55%、84.29%、64.66%的准确率。

Conclusion: 模型具有适应性，可用于实际场景中开发端到端情感识别系统，代码和结果公开。

Abstract: In many domains, including online education, healthcare, security, and
human-computer interaction, facial emotion recognition (FER) is essential.
Real-world FER is still difficult despite its significance because of some
factors such as variable head positions, occlusions, illumination shifts, and
demographic diversity. Engagement detection, which is essential for
applications like virtual learning and customer services, is frequently
challenging due to FER limitations by many current models. In this article, we
propose ExpressNet-MoE, a novel hybrid deep learning model that blends both
Convolution Neural Networks (CNNs) and Mixture of Experts (MoE) framework, to
overcome the difficulties. Our model dynamically chooses the most pertinent
expert networks, thus it aids in the generalization and providing flexibility
to model across a wide variety of datasets. Our model improves on the accuracy
of emotion recognition by utilizing multi-scale feature extraction to collect
both global and local facial features. ExpressNet-MoE includes numerous
CNN-based feature extractors, a MoE module for adaptive feature selection, and
finally a residual network backbone for deep feature learning. To demonstrate
efficacy of our proposed model we evaluated on several datasets, and compared
with current state-of-the-art methods. Our model achieves accuracies of 74.77%
on AffectNet (v7), 72.55% on AffectNet (v8), 84.29% on RAF-DB, and 64.66% on
FER-2013. The results show how adaptive our model is and how it may be used to
develop end-to-end emotion recognition systems in practical settings.
Reproducible codes and results are made publicly accessible at
https://github.com/DeeptimaanB/ExpressNet-MoE.

</details>


### [209] [Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning](https://arxiv.org/abs/2510.13675)
*Hongkuan Zhou,Lavdim Halilaj,Sebastian Monka,Stefan Schmid,Yuqicheng Zhu,Jingcheng Wu,Nadeem Nazer,Steffen Staab*

Main category: cs.CV

TL;DR: 提出Knowledge - guided Contrastive Learning (KnowCoL)框架用于开放域视觉实体识别，在OVEN基准上评估，结合多类型知识提升识别准确性，小模型效果优。


<details>
  <summary>Details</summary>
Motivation: 开放域视觉实体识别在开放集条件下，因监督有限、视觉模糊和语义消歧需求，任务极具挑战性。

Method: 提出KnowCoL框架，将图像和文本描述结合到由Wikidata结构化信息支撑的共享语义空间，利用实体描述、类型层次和关系上下文支持零样本实体识别。

Result: 在OVEN基准上实验，结合视觉、文本和结构化知识大幅提高准确性，最小模型在未见实体上比现有技术准确率提高10.5%，且模型小35倍。

Conclusion: 使用视觉、文本和结构化知识能有效提升开放域视觉实体识别的准确性，尤其对罕见和未见实体效果显著。

Abstract: Open-domain visual entity recognition aims to identify and link entities
depicted in images to a vast and evolving set of real-world concepts, such as
those found in Wikidata. Unlike conventional classification tasks with fixed
label sets, it operates under open-set conditions, where most target entities
are unseen during training and exhibit long-tail distributions. This makes the
task inherently challenging due to limited supervision, high visual ambiguity,
and the need for semantic disambiguation. In this work, we propose a
Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both
images and text descriptions into a shared semantic space grounded by
structured information from Wikidata. By abstracting visual and textual inputs
to a conceptual level, the model leverages entity descriptions, type
hierarchies, and relational context to support zero-shot entity recognition. We
evaluate our approach on the OVEN benchmark, a large-scale open-domain visual
recognition dataset with Wikidata IDs as the label space. Our experiments show
that using visual, textual, and structured knowledge greatly improves accuracy,
especially for rare and unseen entities. Our smallest model improves the
accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite
being 35 times smaller.

</details>


### [210] [NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models](https://arxiv.org/abs/2510.13793)
*Nir Goren,Oren Katzir,Abhinav Nakarmi,Eyal Ronen,Mahmood Sharif,Or Patashnik*

Main category: cs.CV

TL;DR: 随着扩散模型用于视觉内容生成的普及，证明版权很关键，现有水印方法有局限，本文提出轻量级水印方案NoisePrints并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型普及下证明版权和保护著作权成关键问题，现有水印方法需模型权重且计算量大，不实用可扩展性差。

Method: 利用扩散过程的随机种子作为版权证明，在噪声采样过程中加入哈希函数，用零知识证明在不暴露种子情况下证明所有权。

Result: 在多个图像和视频扩散模型验证NoisePrints，仅用种子和输出就能高效验证，无需模型权重。

Conclusion: 提出的轻量级水印方案NoisePrints有效，通过保密种子增加水印去除难度。

Abstract: With the rapid adoption of diffusion models for visual content generation,
proving authorship and protecting copyright have become critical. This
challenge is particularly important when model owners keep their models private
and may be unwilling or unable to handle authorship issues, making third-party
verification essential. A natural solution is to embed watermarks for later
verification. However, existing methods require access to model weights and
rely on computationally heavy procedures, rendering them impractical and
non-scalable. To address these challenges, we propose , a lightweight
watermarking scheme that utilizes the random seed used to initialize the
diffusion process as a proof of authorship without modifying the generation
process. Our key observation is that the initial noise derived from a seed is
highly correlated with the generated visual content. By incorporating a hash
function into the noise sampling process, we further ensure that recovering a
valid seed from the content is infeasible. We also show that sampling an
alternative seed that passes verification is infeasible, and demonstrate the
robustness of our method under various manipulations. Finally, we show how to
use cryptographic zero-knowledge proofs to prove ownership without revealing
the seed. By keeping the seed secret, we increase the difficulty of watermark
removal. In our experiments, we validate NoisePrints on multiple
state-of-the-art diffusion models for images and videos, demonstrating
efficient verification using only the seed and output, without requiring access
to model weights.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [211] [Coherent Load Profile Synthesis with Conditional Diffusion for LV Distribution Network Scenario Generation](https://arxiv.org/abs/2510.12832)
*Alistair Brash,Junyi Lu,Bruce Stephen,Blair Brown,Robert Atkinson,Craig Michie,Fraser MacIntyre,Christos Tachtatzis*

Main category: eess.SY

TL;DR: 针对配电网低压侧潮流可见性有限问题，提出条件扩散模型合成负荷曲线，评估显示有效。


<details>
  <summary>Details</summary>
Motivation: 配电网低压侧潮流可见性有限，现有负荷曲线生成方法未考虑变电站间协同行为，难以应对低碳技术集成带来的挑战。

Method: 提出条件扩散模型合成低压配电站日有功和无功功率曲线，通过传统指标和潮流建模评估保真度，并与其他模型进行对比。

Result: 合成的负荷曲线在独立和系统层面均合理，条件扩散模型在生成现实场景方面有效。

Conclusion: 条件扩散模型可用于子区域配电网规划和运营的现实场景生成。

Abstract: Limited visibility of power distribution network power flows at the low
voltage level presents challenges to both distribution network operators from a
planning perspective and distribution system operators from a congestion
management perspective. Forestalling these challenges through scenario analysis
is confounded by the lack of realistic and coherent load data across
representative distribution feeders. Load profiling approaches often rely on
summarising demand through typical profiles, which oversimplifies the
complexity of substation-level operations and limits their applicability in
specific power system studies. Sampling methods, and more recently generative
models, have attempted to address this through synthesising representative
loads from historical exemplars; however, while these approaches can
approximate load shapes to a convincing degree of fidelity, the co-behaviour
between substations, which ultimately impacts higher voltage level network
operation, is often overlooked. This limitation will become even more
pronounced with the increasing integration of low-carbon technologies, as
estimates of base loads fail to capture load diversity. To address this gap, a
Conditional Diffusion model for synthesising daily active and reactive power
profiles at the low voltage distribution substation level is proposed. The
evaluation of fidelity is demonstrated through conventional metrics capturing
temporal and statistical realism, as well as power flow modelling. The results
show synthesised load profiles are plausible both independently and as a cohort
in a wider power systems context. The Conditional Diffusion model is
benchmarked against both naive and state-of-the-art models to demonstrate its
effectiveness in producing realistic scenarios on which to base sub-regional
power distribution network planning and operations.

</details>


### [212] [Control of dynamical systems with neural networks](https://arxiv.org/abs/2510.12810)
*Lucas Böttcher*

Main category: eess.SY

TL;DR: 本文探讨用神经网络和机器学习库解决控制问题，介绍不同系统控制输入参数化方法及多领域应用，为复杂控制任务提供实用方案。


<details>
  <summary>Details</summary>
Motivation: 科学和工业应用中存在控制问题，深度学习和自动微分发展使相关方法应用于控制问题更可行。

Method: 用神经网络和现代机器学习库对离散、连续、确定和随机动力学系统的控制输入进行参数化；连续系统用神经常微分方程，离散系统用自动微分方法实现和优化自定义控制输入参数化。

Result: 展示了在生物、工程、物理和医学等多领域的应用。

Conclusion: 所提方法为计算量大或解析难的控制任务提供实用方案，对复杂现实应用有价值。

Abstract: Control problems frequently arise in scientific and industrial applications,
where the objective is to steer a dynamical system from an initial state to a
desired target state. Recent advances in deep learning and automatic
differentiation have made applying these methods to control problems
increasingly practical. In this paper, we examine the use of neural networks
and modern machine-learning libraries to parameterize control inputs across
discrete-time and continuous-time systems, as well as deterministic and
stochastic dynamics. We highlight applications in multiple domains, including
biology, engineering, physics, and medicine. For continuous-time dynamical
systems, neural ordinary differential equations (neural ODEs) offer a useful
approach to parameterizing control inputs. For discrete-time systems, we show
how custom control-input parameterizations can be implemented and optimized
using automatic-differentiation methods. Overall, the methods presented provide
practical solutions for control tasks that are computationally demanding or
analytically intractable, making them valuable for complex real-world
applications.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [213] [Reciprocal Space Attention for Learning Long-Range Interactions](https://arxiv.org/abs/2510.13055)
*Hariharan Ramasubramanian,Alvaro Vazquez-Mayagoitia,Ganesh Sivaraman,Atul C. Thakur*

Main category: cond-mat.mtrl-sci

TL;DR: 引入Reciprocal - Space Attention (RSA)框架处理长程相互作用，集成到现有MLIP框架，在多个基准测试中证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习原子间势（MLIPs）在处理长程相互作用时不足，需新方法解决。

Method: 将线性缩放注意力机制映射到傅里叶空间，提出RSA框架并集成到现有MLIP框架。

Result: 在多个基准测试中，RSA能持续捕捉广泛化学和材料系统中的长程物理现象。

Conclusion: RSA框架能有效处理长程相互作用，代码和数据集公开。

Abstract: Machine learning interatomic potentials (MLIPs) have revolutionized the
modeling of materials and molecules by directly fitting to ab initio data.
However, while these models excel at capturing local and semi-local
interactions, they often prove insufficient when an explicit and efficient
treatment of long-range interactions is required. To address this limitation,
we introduce Reciprocal-Space Attention (RSA), a framework designed to capture
long-range interactions in the Fourier domain. RSA can be integrated with any
existing local or semi-local MLIP framework. The central contribution of this
work is the mapping of a linear-scaling attention mechanism into Fourier space,
enabling the explicit modeling of long-range interactions such as
electrostatics and dispersion without relying on predefined charges or other
empirical assumptions. We demonstrate the effectiveness of our method as a
long-range correction to the MACE backbone across diverse benchmarks, including
dimer binding curves, dispersion-dominated layered phosphorene exfoliation, and
the molecular dipole density of bulk water. Our results show that RSA
consistently captures long-range physics across a broad range of chemical and
materials systems. The code and datasets for this work is available at
https://github.com/rfhari/reciprocal_space_attention

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [214] [Addressing the alignment problem in transportation policy making: an LLM approach](https://arxiv.org/abs/2510.13139)
*Xiaoyu Yan,Tianxing Dai,Yu,Nie*

Main category: cs.CY

TL;DR: 研究用大语言模型解决交通规划中集体偏好与政策不一致问题，通过多智能体模拟得出其有潜力但也有局限。


<details>
  <summary>Details</summary>
Motivation: 交通规划中，不同旅行者集体偏好与模型驱动决策工具产生的政策常不一致，导致实施延迟或失败，研究大语言模型能否解决此问题。

Method: 开发多智能体模拟，让大语言模型作为代表不同社区居民的智能体参与公交政策提案公投，用思维链推理给出偏好，用即时决选投票聚合偏好，用GPT - 4o和Claude - 3.5实现框架并应用于芝加哥和休斯顿。

Result: 大语言模型智能体能够近似合理的集体偏好并响应本地环境，但存在模型特定行为偏差，与基于优化的基准有适度差异。

Conclusion: 大语言模型作为解决交通决策中对齐问题的工具，既有前景也有局限。

Abstract: A key challenge in transportation planning is that the collective preferences
of heterogeneous travelers often diverge from the policies produced by
model-driven decision tools. This misalignment frequently results in
implementation delays or failures. Here, we investigate whether large language
models (LLMs), noted for their capabilities in reasoning and simulating human
decision-making, can help inform and address this alignment problem. We develop
a multi-agent simulation in which LLMs, acting as agents representing residents
from different communities in a city, participate in a referendum on a set of
transit policy proposals. Using chain-of-thought reasoning, LLM agents provide
ranked-choice or approval-based preferences, which are aggregated using
instant-runoff voting (IRV) to model democratic consensus. We implement this
simulation framework with both GPT-4o and Claude-3.5, and apply it for Chicago
and Houston. Our findings suggest that LLM agents are capable of approximating
plausible collective preferences and responding to local context, while also
displaying model-specific behavioral biases and modest divergences from
optimization-based benchmarks. These capabilities underscore both the promise
and limitations of LLMs as tools for solving the alignment problem in
transportation decision-making.

</details>


### [215] [Evidence Without Injustice: A New Counterfactual Test for Fair Algorithms](https://arxiv.org/abs/2510.12822)
*Michele Loi,Marcello Di Bello,Nicolò Cangiotti*

Main category: cs.CY

TL;DR: 现有算法公平性研究忽视算法输出证据价值是否依赖结构不公正，以两种警务算法为例说明，未通过测试的证据用于惩罚更有道德问题。


<details>
  <summary>Details</summary>
Motivation: 指出当前算法公平性哲学研究中被忽视的重要维度，即算法输出的证据价值是否依赖结构不公正。

Method: 对比预测性警务算法和基于摄像头的系统这两个范例，提出评估证据道德可接受性的测试方法。

Result: 预测性警务算法未通过测试，基于摄像头的系统通过测试。

Conclusion: 未通过测试的证据用于惩罚在道德上更有问题。

Abstract: The growing philosophical literature on algorithmic fairness has examined
statistical criteria such as equalized odds and calibration, causal and
counterfactual approaches, and the role of structural and compounding
injustices. Yet an important dimension has been overlooked: whether the
evidential value of an algorithmic output itself depends on structural
injustice. Our paradigmatic pair of examples contrasts a predictive policing
algorithm, which relies on historical crime data, with a camera-based system
that records ongoing offenses, both designed to guide police deployment. In
evaluating the moral acceptability of acting on a piece of evidence, we must
ask not only whether the evidence is probative in the actual world, but also
whether it would remain probative in nearby worlds without the relevant
injustices. The predictive policing algorithm fails this test, but the
camera-based system passes it. When evidence fails the test, it is morally
problematic to use it punitively, more so than evidence that passes the test.

</details>


### [216] [Gobernanza y trazabilidad "a prueba de AI Act" para casos de uso legales: un marco técnico-jurídico, métricas forenses y evidencias auditables](https://arxiv.org/abs/2510.12830)
*Alex Dantart*

Main category: cs.CY

TL;DR: 本文提出面向法律领域AI系统的治理框架，确保符合欧盟AI法案，有开源实现及实验协议。


<details>
  <summary>Details</summary>
Motivation: 确保法律领域AI系统可验证地符合欧盟AI法案。

Method: 将法规的规范映射与技术控制集成，构建RAG/LLM系统的取证架构，设计按法律风险加权的评估系统。

Result: 提出框架的开源实现rag - forense及实验协议。

Conclusion: 所提出的框架及相关实现有助于法律领域AI系统达到欧盟AI法案合规要求。

Abstract: This paper presents a comprehensive governance framework for AI systems in
the legal sector, designed to ensure verifiable compliance with the EU AI Act.
The framework integrates a normative mapping of the regulation to technical
controls, a forensic architecture for RAG/LLM systems, and an evaluation system
with metrics weighted by legal risk. As a primary contribution, we present
rag-forense, an open-source implementation of the framework, accompanied by an
experimental protocol to demonstrate compliance. -- Este art\'iculo presenta un
marco integral de gobernanza para sistemas de IA en el sector legal, dise\~nado
para garantizar el cumplimiento verificable del Reglamento de IA de la UE (AI
Act). El marco integra una cartograf\'ia normativa de la ley a controles
t\'ecnicos, una arquitectura forense para sistemas RAG/LLM y un sistema de
evaluaci\'on con m\'etricas ponderadas por el riesgo jur\'idico. Como principal
contribuci\'on, se presenta rag-forense, una implementaci\'on de c\'odigo
abierto del marco, acompa\~nada de un protocolo experimental para demostrar la
conformidad.

</details>


### [217] [Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification](https://arxiv.org/abs/2510.12850)
*Mahamodul Hasan Mahadi,Md. Nasif Safwan,Souhardo Rahman,Shahnaj Parvin,Aminun Nahar,Kamruddin Nur*

Main category: cs.CY

TL;DR: 本文提出Ethic - BERT用于伦理内容分类，经实验验证其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在伦理推理上常依赖表面关联，缺乏原则性道德理解，需要开发能进行细致伦理推理的AI系统。

Method: 引入基于BERT的Ethic - BERT模型进行四个领域的伦理内容分类，利用ETHICS数据集，采用鲁棒的预处理方法和先进的微调策略，用对抗过滤的“Hard Test”评估鲁棒性。

Result: Ethic - BERT在标准测试中平均准确率达82.32%，在Justice和Virtue领域有显著提升，在HardTest中平均准确率提高15.28%。

Conclusion: 通过有偏见意识的预处理和改进的AI模型有助于提高性能和实现可靠决策。

Abstract: Developing AI systems capable of nuanced ethical reasoning is critical as
they increasingly influence human decisions, yet existing models often rely on
superficial correlations rather than principled moral understanding. This paper
introduces Ethic-BERT, a BERT-based model for ethical content classification
across four domains: Commonsense, Justice, Virtue, and Deontology. Leveraging
the ETHICS dataset, our approach integrates robust preprocessing to address
vocabulary sparsity and contextual ambiguities, alongside advanced fine-tuning
strategies like full model unfreezing, gradient accumulation, and adaptive
learning rate scheduling. To evaluate robustness, we employ an adversarially
filtered "Hard Test" split, isolating complex ethical dilemmas. Experimental
results demonstrate Ethic-BERT's superiority over baseline models, achieving
82.32% average accuracy on the standard test, with notable improvements in
Justice and Virtue. In addition, the proposed Ethic-BERT attains 15.28% average
accuracy improvement in the HardTest. These findings contribute to performance
improvement and reliable decision-making using bias-aware preprocessing and
proposed enhanced AI model.

</details>


### [218] [Adaptive Generation of Bias-Eliciting Questions for LLMs](https://arxiv.org/abs/2510.12857)
*Robin Staab,Jasper Dekoninck,Maximilian Baader,Martin Vechev*

Main category: cs.CY

TL;DR: 提出反事实偏差评估框架构建CAB基准测试，分析大语言模型偏差，发现GPT - 5也有特定场景偏差，强调持续改进确保公平性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用，现有偏差基准测试存在不足，无法捕捉真实用户交互复杂性，需新评估方法。

Method: 引入反事实偏差评估框架，自动生成敏感属性开放式问题，迭代选择偏差诱导问题，构建人类验证的CAB基准测试。

Result: 利用CAB分析多个大语言模型，发现GPT - 5虽表现好但特定场景仍有偏差。

Conclusion: 大语言模型需持续改进以确保公平行为。

Abstract: Large language models (LLMs) are now widely deployed in user-facing
applications, reaching hundreds of millions worldwide. As they become
integrated into everyday tasks, growing reliance on their outputs raises
significant concerns. In particular, users may unknowingly be exposed to
model-inherent biases that systematically disadvantage or stereotype certain
groups. However, existing bias benchmarks continue to rely on templated prompts
or restrictive multiple-choice questions that are suggestive, simplistic, and
fail to capture the complexity of real-world user interactions. In this work,
we address this gap by introducing a counterfactual bias evaluation framework
that automatically generates realistic, open-ended questions over sensitive
attributes such as sex, race, or religion. By iteratively mutating and
selecting bias-inducing questions, our approach systematically explores areas
where models are most susceptible to biased behavior. Beyond detecting harmful
biases, we also capture distinct response dimensions that are increasingly
relevant in user interactions, such as asymmetric refusals and explicit
acknowledgment of bias. Leveraging our framework, we construct CAB, a
human-verified benchmark spanning diverse topics, designed to enable
cross-model comparisons. Using CAB, we analyze a range of LLMs across multiple
bias dimensions, revealing nuanced insights into how different models manifest
bias. For instance, while GPT-5 outperforms other models, it nonetheless
exhibits persistent biases in specific scenarios. These findings underscore the
need for continual improvements to ensure fair model behavior.

</details>


### [219] [Three Lenses on the AI Revolution: Risk, Transformation, Continuity](https://arxiv.org/abs/2510.12859)
*Masoud Makrehchi*

Main category: cs.CY

TL;DR: 本文提出应从风险、变革和延续三个视角看待AI，分析其发展规律、对各行业的影响以及前沿挑战，指出AI兼具进化与革命特性，需创新与安全治理结合。


<details>
  <summary>Details</summary>
Motivation: 探讨AI与历史技术革命的关系，明确如何正确看待AI的发展及其影响。

Method: 运用历史类比，分析技术革命的反复模式，进行行业分析。

Result: 发现AI时代技术发展规律强化，各行业因常规认知商品化而重塑，前沿面临道德AI设计挑战。

Conclusion: AI兼具进化与革命特性，需将创新策略与安全治理结合，确保公平获取并将其嵌入人类责任秩序。

Abstract: Artificial Intelligence (AI) has emerged as both a continuation of historical
technological revolutions and a potential rupture with them. This paper argues
that AI must be viewed simultaneously through three lenses: \textit{risk},
where it resembles nuclear technology in its irreversible and global
externalities; \textit{transformation}, where it parallels the Industrial
Revolution as a general-purpose technology driving productivity and
reorganization of labor; and \textit{continuity}, where it extends the
fifty-year arc of computing revolutions from personal computing to the internet
to mobile. Drawing on historical analogies, we emphasize that no past
transition constituted a strict singularity: disruptive shifts eventually
became governable through new norms and institutions.
  We examine recurring patterns across revolutions -- democratization at the
usage layer, concentration at the production layer, falling costs, and
deepening personalization -- and show how these dynamics are intensifying in
the AI era. Sectoral analysis illustrates how accounting, law, education,
translation, advertising, and software engineering are being reshaped as
routine cognition is commoditized and human value shifts to judgment, trust,
and ethical responsibility. At the frontier, the challenge of designing moral
AI agents highlights the need for robust guardrails, mechanisms for moral
generalization, and governance of emergent multi-agent dynamics.
  We conclude that AI is neither a singular break nor merely incremental
progress. It is both evolutionary and revolutionary: predictable in its median
effects yet carrying singularity-class tail risks. Good outcomes are not
automatic; they require coupling pro-innovation strategies with safety
governance, ensuring equitable access, and embedding AI within a human order of
responsibility.

</details>


### [220] [Toward LLM-Supported Automated Assessment of Critical Thinking Subskills](https://arxiv.org/abs/2510.12915)
*Marisa C. Peczuh,Nischal Ashok Kumar,Ryan Baker,Blair Lehman,Danielle Eisenberg,Caitlin Mills,Keerthi Chebrolu,Sudhip Nashi,Cadence Young,Brayden Liu,Sherry Lachman,Andrew Lan*

Main category: cs.CY

TL;DR: 本文探讨测量批判性思维核心子技能的可行性，用学生议论文进行研究，评估三种自动化评分方法，指出自动化评估的权衡，是高阶推理技能可扩展评估的初步尝试。


<details>
  <summary>Details</summary>
Motivation: 批判性思维在教育中重要，但学习分析领域对其定义、测量和支持的研究不足，因此研究测量其核心子技能的可行性。

Method: 以学生议论文为真实任务，基于既定技能进展开发编码规则进行人工编码，用三种大语言模型（GPT - 5、GPT - 5 - mini和ModernBERT）实现零样本提示、少样本提示和监督微调三种自动化评分方法。

Result: GPT - 5少样本提示效果最好，在可分离、频繁类别的子技能上表现强，对需检测细微差异或稀有类别的子技能表现较差；专有模型可靠性高但成本高，开源模型有实用准确性但对少数类别敏感性低。

Conclusion: 研究是在真实教育情境中对高阶推理技能进行可扩展评估的初步一步。

Abstract: Critical thinking represents a fundamental competency in today's education
landscape. Developing critical thinking skills through timely assessment and
feedback is crucial; however, there has not been extensive work in the learning
analytics community on defining, measuring, and supporting critical thinking.
In this paper, we investigate the feasibility of measuring core "subskills"
that underlie critical thinking. We ground our work in an authentic task where
students operationalize critical thinking: student-written argumentative
essays. We developed a coding rubric based on an established skills progression
and completed human coding for a corpus of student essays. We then evaluated
three distinct approaches to automated scoring: zero-shot prompting, few-shot
prompting, and supervised fine-tuning, implemented across three large language
models (GPT-5, GPT-5-mini, and ModernBERT). GPT-5 with few-shot prompting
achieved the strongest results and demonstrated particular strength on
subskills with separable, frequent categories, while lower performance was
observed for subskills that required detection of subtle distinctions or rare
categories. Our results underscore critical trade-offs in automated critical
thinking assessment: proprietary models offer superior reliability at higher
cost, while open-source alternatives provide practical accuracy with reduced
sensitivity to minority categories. Our work represents an initial step toward
scalable assessment of higher-order reasoning skills across authentic
educational contexts.

</details>


### [221] [Subject Roles in the EU AI Act: Mapping and Regulatory Implications](https://arxiv.org/abs/2510.13591)
*Nicola Fabiano*

Main category: cs.CY

TL;DR: 本文分析欧盟人工智能法案对六类主体的监管，揭示主体角色转变机制及义务传导，表明法案平衡创新与权利保护。


<details>
  <summary>Details</summary>
Motivation: 对欧盟人工智能法案中定义的六类主体监管进行结构化研究。

Method: 研究法案的113条正文、180条说明和13个附件，分析对六类主体的定义及监管。

Result: 发现主体在特定条件下角色转变机制，义务通过供应链传导形成分布式协调治理系统。

Conclusion: 法案通过基于风险的义务平衡创新与基本权利保护，为利益相关者提供实施指导。

Abstract: The European Union's Artificial Intelligence Act (Regulation (EU) 2024/1689)
establishes the world's first comprehensive regulatory framework for AI systems
through a sophisticated ecosystem of interconnected subjects defined in Article
3. This paper provides a structured examination of the six main categories of
actors - providers, deployers, authorized representatives, importers,
distributors, and product manufacturers - collectively referred to as
"operators" within the regulation. Through examination of these Article 3
definitions and their elaboration across the regulation's 113 articles, 180
recitals, and 13 annexes, we map the complete governance structure and analyze
how the AI Act regulates these subjects. Our analysis reveals critical
transformation mechanisms whereby subjects can assume different roles under
specific conditions, particularly through Article 25 provisions ensuring
accountability follows control. We identify how obligations cascade through the
supply chain via mandatory information flows and cooperation requirements,
creating a distributed yet coordinated governance system. The findings
demonstrate how the regulation balances innovation with the protection of
fundamental rights through risk-based obligations that scale with the
capabilities and deployment contexts of AI systems, providing essential
guidance for stakeholders implementing the AI Act's requirements.

</details>


### [222] [The Role of Computing Resources in Publishing Foundation Model Research](https://arxiv.org/abs/2510.13621)
*Yuexing Hao,Yue Huang,Haoran Zhang,Chenyang Zhao,Zhenwen Liang,Paul Pu Liang,Yue Zhao,Lichao Sun,Saleh Kalantari,Xiangliang Zhang,Marzyeh Ghassemi*

Main category: cs.CY

TL;DR: 评估资源与基础模型科学进展关系，建议创建共享计算机会促进AI研究。


<details>
  <summary>Details</summary>
Motivation: 了解资源与基础模型科学进展的关系，因前沿AI研究需大量资源。

Method: 回顾2022 - 2024年6517篇基础模型论文，调查229位第一作者。

Result: 计算资源增加与国家资金分配和引用相关，与研究环境、领域或研究方法无强关联。

Conclusion: 个人和机构应创建共享且经济的计算机会，降低资源不足研究者的门槛，促进AI创新和进步。

Abstract: Cutting-edge research in Artificial Intelligence (AI) requires considerable
resources, including Graphics Processing Units (GPUs), data, and human
resources. In this paper, we evaluate of the relationship between these
resources and the scientific advancement of foundation models (FM). We reviewed
6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors
to the impact of computing resources on scientific output. We find that
increased computing is correlated with national funding allocations and
citations, but our findings don't observe the strong correlations with research
environment (academic or industrial), domain, or study methodology. We advise
that individuals and institutions focus on creating shared and affordable
computing opportunities to lower the entry barrier for under-resourced
researchers. These steps can help expand participation in FM research, foster
diversity of ideas and contributors, and sustain innovation and progress in AI.
The data will be available at: https://mit-calc.csail.mit.edu/

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [223] [VLA-0: Building State-of-the-Art VLAs with Zero Modification](https://arxiv.org/abs/2510.13054)
*Ankit Goyal,Hugo Hadfield,Xuning Yang,Valts Blukis,Fabio Ramos*

Main category: cs.RO

TL;DR: 本文提出VLA - 0模型，以文本直接表示动作，发现其效果惊人，在LIBERO基准测试和现实场景中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有构建视觉 - 语言 - 动作模型（VLAs）的方法常增加复杂性，而以文本直接表示动作的简单策略未被充分探索。

Method: 引入VLA - 0模型，以文本直接表示动作。

Result: VLA - 0在LIBERO基准测试中优于同数据训练的现有方法，未大规模机器人特定训练时也优于大规模数据训练的方法，在现实场景中优于SmolVLA。

Conclusion: 简单的VLA - 0设计有强大性能，论文总结了相关发现并说明了释放其高性能的具体技术。

Abstract: Vision-Language-Action models (VLAs) hold immense promise for enabling
generalist robot manipulation. However, the best way to build them remains an
open question. Current approaches often add complexity, such as modifying the
existing vocabulary of a Vision-Language Model (VLM) with action tokens or
introducing special action heads. Curiously, the simplest strategy of
representing actions directly as text has remained largely unexplored. This
work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only
effective; it is surprisingly powerful. With the right design, VLA-0
outperforms more involved models. On LIBERO, a popular benchmark for evaluating
VLAs, VLA-0 outperforms all existing methods trained on the same robotic data,
including $\pi_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without
large-scale robotics-specific training, it outperforms methods trained on
large-scale robotic data, like $\pi_0.5$-KI, $\pi_0$, GR00T-N1 and MolmoAct.
These findings also translate to the real world, where VLA-0 outperforms
SmolVLA, a VLA model pre-trained on large-scale real data. This paper
summarizes our unexpected findings and spells out the specific techniques
required to unlock the high performance of this simple yet potent VLA design.
Visual results, code, and trained models are provided here:
https://vla0.github.io/.

</details>


### [224] [Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control](https://arxiv.org/abs/2510.13358)
*Shingo Ayabe,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.RO

TL;DR: 本文提出离线到在线框架，通过对抗性微调及性能感知课程提高策略对动作空间扰动的鲁棒性，实验证明其效果优于仅离线训练且收敛更快。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习训练的策略在动作空间扰动下较脆弱，需要提高其鲁棒性。

Method: 引入离线到在线框架，在干净数据上训练策略后进行对抗性微调，通过注入扰动诱导补偿行为；用性能感知课程调整扰动概率。

Result: 在连续控制运动任务实验中，该方法比仅离线训练基线更具鲁棒性且收敛更快；匹配微调与评估条件鲁棒性最强；自适应课程策略减轻标称性能下降。

Conclusion: 对抗性微调能实现不确定环境下的自适应和鲁棒控制，弥合离线效率与在线适应性的差距。

Abstract: Offline reinforcement learning enables sample-efficient policy acquisition
without risky online interaction, yet policies trained on static datasets
remain brittle under action-space perturbations such as actuator faults. This
study introduces an offline-to-online framework that trains policies on clean
data and then performs adversarial fine-tuning, where perturbations are
injected into executed actions to induce compensatory behavior and improve
resilience. A performance-aware curriculum further adjusts the perturbation
probability during training via an exponential-moving-average signal, balancing
robustness and stability throughout the learning process. Experiments on
continuous-control locomotion tasks demonstrate that the proposed method
consistently improves robustness over offline-only baselines and converges
faster than training from scratch. Matching the fine-tuning and evaluation
conditions yields the strongest robustness to action-space perturbations, while
the adaptive curriculum strategy mitigates the degradation of nominal
performance observed with the linear curriculum strategy. Overall, the results
show that adversarial fine-tuning enables adaptive and robust control under
uncertain environments, bridging the gap between offline efficiency and online
adaptability.

</details>


### [225] [InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy](https://arxiv.org/abs/2510.13778)
*Xinyi Chen,Yilun Chen,Yanwei Fu,Ning Gao,Jiaya Jia,Weiyang Jin,Hao Li,Yao Mu,Jiangmiao Pang,Yu Qiao,Yang Tian,Bin Wang,Bolun Wang,Fangjing Wang,Hanqing Wang,Tai Wang,Ziqin Wang,Xueyuan Wei,Chao Wu,Shuai Yang,Jinhui Ye,Junqiu Yu,Jia Zeng,Jingjing Zhang,Jinyu Zhang,Shi Zhang,Feng Zheng,Bowen Zhou,Yangkun Zhu*

Main category: cs.RO

TL;DR: 本文介绍InternVLA - M1框架用于空间定位和机器人控制，采用空间引导训练方法取得不错效果，证明其可扩展性和弹性。


<details>
  <summary>Details</summary>
Motivation: 推动指令跟随机器人向可扩展、通用智能发展。

Method: 采用两阶段管道，先在超230万空间推理数据上进行空间定位预训练，再进行空间引导动作后训练；构建仿真引擎收集数据。

Result: 在多个模拟环境和真实场景中取得性能提升，如在SimplerEnv Google Robot上比无空间引导变体高14.6%等。

Conclusion: 空间引导训练是可扩展和有弹性的通用机器人的统一原则。

Abstract: We introduce InternVLA-M1, a unified framework for spatial grounding and
robot control that advances instruction-following robots toward scalable,
general-purpose intelligence. Its core idea is spatially guided
vision-language-action training, where spatial grounding serves as the critical
link between instructions and robot actions. InternVLA-M1 employs a two-stage
pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning
data to determine ``where to act'' by aligning instructions with visual,
embodiment-agnostic positions, and (ii) spatially guided action post-training
to decide ``how to act'' by generating embodiment-aware actions through
plug-and-play spatial prompting. This spatially guided training recipe yields
consistent gains: InternVLA-M1 outperforms its variant without spatial guidance
by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO
Franka, while demonstrating stronger spatial reasoning capability in box,
point, and trace prediction. To further scale instruction following, we built a
simulation engine to collect 244K generalizable pick-and-place episodes,
enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In
real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with
synthetic co-training, achieved +20.6% on unseen objects and novel
configurations. Moreover, in long-horizon reasoning-intensive scenarios, it
surpassed existing works by over 10%. These results highlight spatially guided
training as a unifying principle for scalable and resilient generalist robots.
Code and models are available at
https://github.com/InternRobotics/InternVLA-M1.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [226] [Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning](https://arxiv.org/abs/2510.13322)
*Baogang Song,Dongdong Zhao,Jianwen Xiang,Qiben Xu,Zizhuo Yu*

Main category: cs.CR

TL;DR: 提出可撤销后门攻击范式，优化触发器，实验表明能保持攻击成功率并有效移除后门行为，为研究开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击策略有可被静态分析检测的痕迹，需新的攻击范式。

Method: 将触发器优化建模为双层优化问题，用确定性分区减少采样方差，用PCGrad技术解决梯度冲突。

Result: 在CIFAR - 10和ImageNet上实验，攻击成功率与现有技术相当，且能有效移除后门行为。

Conclusion: 该工作为后门攻击研究开辟新方向，给机器学习系统安全带来新挑战。

Abstract: Backdoor attacks pose a persistent security risk to deep neural networks
(DNNs) due to their stealth and durability. While recent research has explored
leveraging model unlearning mechanisms to enhance backdoor concealment,
existing attack strategies still leave persistent traces that may be detected
through static analysis. In this work, we introduce the first paradigm of
revocable backdoor attacks, where the backdoor can be proactively and
thoroughly removed after the attack objective is achieved. We formulate the
trigger optimization in revocable backdoor attacks as a bilevel optimization
problem: by simulating both backdoor injection and unlearning processes, the
trigger generator is optimized to achieve a high attack success rate (ASR)
while ensuring that the backdoor can be easily erased through unlearning. To
mitigate the optimization conflict between injection and removal objectives, we
employ a deterministic partition of poisoning and unlearning samples to reduce
sampling-induced variance, and further apply the Projected Conflicting Gradient
(PCGrad) technique to resolve the remaining gradient conflicts. Experiments on
CIFAR-10 and ImageNet demonstrate that our method maintains ASR comparable to
state-of-the-art backdoor attacks, while enabling effective removal of backdoor
behavior after unlearning. This work opens a new direction for backdoor attack
research and presents new challenges for the security of machine learning
systems.

</details>


### [227] [Applying Graph Analysis for Unsupervised Fast Malware Fingerprinting](https://arxiv.org/abs/2510.12811)
*ElMouatez Billah Karbab,Mourad Debbabi*

Main category: cs.CR

TL;DR: 本文提出TrapNet框架用于恶意软件指纹识别和分组，评估显示其有效且高效。


<details>
  <summary>Details</summary>
Motivation: 恶意软件数量激增，手动调查不现实，需开发初步过滤技术和工具对恶意软件基于语义相似性分组。

Method: TrapNet采用图社区检测技术，包括检测并解压打包二进制文件、用FloatHash生成摘要、构建恶意软件相似性网络、用社区检测算法识别社区。

Result: 广泛评估表明TrapNet在检测社区的覆盖范围和纯度方面有效，运行时效率高，优于其他先进解决方案。

Conclusion: TrapNet是一种新颖、可扩展且无监督的恶意软件指纹识别和分组框架，性能出色。

Abstract: Malware proliferation is increasing at a tremendous rate, with hundreds of
thousands of new samples identified daily. Manual investigation of such a vast
amount of malware is an unrealistic, time-consuming, and overwhelming task. To
cope with this volume, there is a clear need to develop specialized techniques
and efficient tools for preliminary filtering that can group malware based on
semantic similarity. In this paper, we propose TrapNet, a novel, scalable, and
unsupervised framework for malware fingerprinting and grouping. TrapNet employs
graph community detection techniques for malware fingerprinting and family
attribution based on static analysis, as follows: (1) TrapNet detects packed
binaries and unpacks them using known generic packer tools. (2) From each
malware sample, it generates a digest that captures the underlying semantics.
Since the digest must be dense, efficient, and suitable for similarity
checking, we designed FloatHash (FH), a novel numerical fuzzy hashing technique
that produces a short real-valued vector summarizing the underlying assembly
items and their order. FH is based on applying Principal Component Analysis
(PCA) to ordered assembly items (e.g., opcodes, function calls) extracted from
the malware's assembly code. (3) Representing malware with short numerical
vectors enables high-performance, large-scale similarity computation, which
allows TrapNet to build a malware similarity network. (4) Finally, TrapNet
employs state-of-the-art community detection algorithms to identify dense
communities, which represent groups of malware with similar semantics. Our
extensive evaluation of TrapNet demonstrates its effectiveness in terms of the
coverage and purity of the detected communities, while also highlighting its
runtime efficiency, which outperforms other state-of-the-art solutions.

</details>


### [228] [SimKey: A Semantically Aware Key Module for Watermarking Language Models](https://arxiv.org/abs/2510.12828)
*Shingo Kodama,Haya Diwan,Lucas Rosenblatt,R. Teal Witter,Niv Cohen*

Main category: cs.CR

TL;DR: 大语言模型生成文本难辨真伪，现有水印方法有缺陷，本文提出SimKey模块增强水印鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型文本水印方法易受表面编辑影响、易被恶意利用导致模型所有者声誉受损的问题。

Method: 引入SimKey语义密钥模块，利用局部敏感哈希处理语义嵌入，使释义文本产生相同水印密钥，无关或语义偏移文本产生不同密钥。

Result: 与现有水印方案集成后，SimKey提高了水印对释义和翻译的鲁棒性，防止有害内容错误归因。

Conclusion: 语义感知密钥是一种实用且可扩展的水印方向。

Abstract: The rapid spread of text generated by large language models (LLMs) makes it
increasingly difficult to distinguish authentic human writing from machine
output. Watermarking offers a promising solution: model owners can embed an
imperceptible signal into generated text, marking its origin. Most leading
approaches seed an LLM's next-token sampling with a pseudo-random key that can
later be recovered to identify the text as machine-generated, while only
minimally altering the model's output distribution. However, these methods
suffer from two related issues: (i) watermarks are brittle to simple
surface-level edits such as paraphrasing or reordering; and (ii) adversaries
can append unrelated, potentially harmful text that inherits the watermark,
risking reputational damage to model owners. To address these issues, we
introduce SimKey, a semantic key module that strengthens watermark robustness
by tying key generation to the meaning of prior context. SimKey uses
locality-sensitive hashing over semantic embeddings to ensure that paraphrased
text yields the same watermark key, while unrelated or semantically shifted
text produces a different one. Integrated with state-of-the-art watermarking
schemes, SimKey improves watermark robustness to paraphrasing and translation
while preventing harmful content from false attribution, establishing
semantic-aware keying as a practical and extensible watermarking direction.

</details>


### [229] [In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers](https://arxiv.org/abs/2510.13543)
*Avihay Cohen*

Main category: cs.CR

TL;DR: 提出在浏览器中运行、由大语言模型引导的模糊测试框架，实时发现代理AI浏览器提示注入漏洞。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的代理AI浏览器易受间接提示注入攻击，且该攻击可绕过传统网络安全边界。

Method: 提出一个完全在浏览器中运行、由大语言模型引导的模糊测试框架。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Large Language Model (LLM) based agents integrated into web browsers (often
called agentic AI browsers) offer powerful automation of web tasks. However,
they are vulnerable to indirect prompt injection attacks, where malicious
instructions hidden in a webpage deceive the agent into unwanted actions. These
attacks can bypass traditional web security boundaries, as the AI agent
operates with the user privileges across sites. In this paper, we present a
novel fuzzing framework that runs entirely in the browser and is guided by an
LLM to automatically discover such prompt injection vulnerabilities in real
time.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [230] [MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control](https://arxiv.org/abs/2510.13794)
*Xue Bin Peng*

Main category: cs.GR

TL;DR: 介绍开源框架MimicKit，用于运动控制器训练，有常用技术实现，代码模块化易配置，代码开源。


<details>
  <summary>Details</summary>
Motivation: 为计算机图形学和机器人领域的研究与应用提供统一训练框架。

Method: 结合运动模仿和强化学习技术，实现常用运动模仿技术和RL算法。

Result: 开发出MimicKit框架，代码模块化、易配置，可用于新角色和任务。

Conclusion: MimicKit框架能支持相关领域研究和应用，且代码开源方便使用。

Abstract: MimicKit is an open-source framework for training motion controllers using
motion imitation and reinforcement learning. The codebase provides
implementations of commonly-used motion-imitation techniques and RL algorithms.
This framework is intended to support research and applications in computer
graphics and robotics by providing a unified training framework, along with
standardized environment, agent, and data structures. The codebase is designed
to be modular and easily configurable, enabling convenient modification and
extension to new characters and tasks. The open-source codebase is available
at: https://github.com/xbpeng/MimicKit.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [231] [Protenix-Mini+: efficient structure prediction model with scalable pairformer](https://arxiv.org/abs/2510.12842)
*Bo Qiang,Chengyue Gong,Xinshi Chen,Yuxuan Zhang,Wenzhi Xiao*

Main category: q-bio.QM

TL;DR: 为解决生物分子结构预测模型效率与精度平衡问题，提出三项创新，开发轻量级可扩展模型Protenix - Mini+，在可接受性能下降范围内大幅提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有模型如AF3及其变体存在推理延迟高和时间复杂度高的问题，限制了大规模生物分子复合物的可扩展性，需平衡模型效率和预测精度。

Method: 压缩不可扩展操作、去除模块间冗余块、为原子扩散模块采用几步采样器，开发Protenix - Mini+模型。

Result: Protenix - Mini+在低同源单链蛋白中，相对完整Protenix模型LDDT下降约3%，但计算效率提高超90%。

Conclusion: Protenix - Mini+在可接受的性能下降范围内，能大幅提升计算效率，具有良好的轻量级和可扩展性。

Abstract: Lightweight inference is critical for biomolecular structure prediction and
downstream tasks, enabling efficient real-world deployment and inference-time
scaling for large-scale applications. While AF3 and its variants (e.g.,
Protenix, Chai-1) have advanced structure prediction results, they suffer from
critical limitations: high inference latency and cubic time complexity with
respect to token count, both of which restrict scalability for large
biomolecular complexes. To address the core challenge of balancing model
efficiency and prediction accuracy, we introduce three key innovations: (1)
compressing non-scalable operations to mitigate cubic time complexity, (2)
removing redundant blocks across modules to reduce unnecessary overhead, and
(3) adopting a few-step sampler for the atom diffusion module to accelerate
inference. Building on these design principles, we develop Protenix-Mini+, a
highly lightweight and scalable variant of the Protenix model. Within an
acceptable range of performance degradation, it substantially improves
computational efficiency. For example, in the case of low-homology single-chain
proteins, Protenix-Mini+ experiences an intra-protein LDDT drop of
approximately 3% relative to the full Protenix model -- an acceptable
performance trade-off given its substantially 90%+ improved computational
efficiency.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [232] [Market-Based Variance of Market Portfolio and of Entire Market](https://arxiv.org/abs/2510.13790)
*Victor Olkhov*

Main category: econ.GN

TL;DR: 提出基于市场的统一描述方法，比较市场组合和所有证券交易的基于市场的方差，解释高斯分布预测的局限。


<details>
  <summary>Details</summary>
Motivation: 建立对特定证券、市场所有证券及市场组合交易的回报和方差的统一描述，分析现有方差模型的不足。

Method: 考虑不交易过去收集的投资组合的投资者，将市场交易时间序列转换为单一证券交易的时间序列进行建模。

Result: 市场基础方差在连续交易成交量恒定时为Markowitz（1952）的投资组合方差形式，Markowitz（1952）方差忽略了连续交易随机成交量的影响。

Conclusion: 相同方法可描述任何投资组合及其证券交易的回报和方差，高斯分布对回报和方差预测的准确性受经济障碍限制。

Abstract: We present the unified market-based description of returns and variances of
the trades with shares of a particular security, of the trades with shares of
all securities in the market, and of the trades with the market portfolio. We
consider the investor who doesn't trade the shares of his portfolio he
collected at time t0 in the past. The investor observes the time series of the
current trades with all securities made in the market during the averaging
interval. The investor may convert these time series into the time series that
model the trades with all securities as the trades with a single security and
into the time series that model the trades with the market portfolio as the
trades with a single security. That establishes the same description of the
returns and variances of the trades with a single security, the trades with all
securities in the market, and the market portfolio. We show that the
market-based variance, which accounts for the impact of random change of the
volumes of consecutive trades with securities, takes the form of Markowitz's
(1952) portfolio variance if the volumes of consecutive trades with all market
securities are assumed constant. That highlights that Markowitz's (1952)
variance ignores the effects of random volumes of consecutive trades. We
compare the market-based variances of the market portfolio and of the trades
with all market securities, consider the importance of the duration of the
averaging interval, and explain the economic obstacles that limit the accuracy
of the predictions of the returns and variances at best by Gaussian
distributions. The same methods describe the returns and variances of any
portfolio and the trades with its securities.

</details>


### [233] [A theory-based AI automation exposure index: Applying Moravec's Paradox to the US labor market](https://arxiv.org/abs/2510.13369)
*Jacob Schaal*

Main category: econ.GN

TL;DR: 本文基于莫拉维克悖论开发自动化暴露指数，分析不同职业暴露情况，揭示工资与暴露关系及自动化模式变化。


<details>
  <summary>Details</summary>
Motivation: 开发基于理论的自动化暴露指数，研究不同职业自动化暴露情况及工资与暴露的关系等。

Method: 对19000个O*NET任务在绩效差异、隐性知识、数据丰富度和算法差距等方面进行评分。

Result: 管理、STEM和科学职业自动化暴露最高，维护、农业和建筑最低；工资与暴露正相关，隐性知识与工资正相关；与AI注释方法相关性0.72，与大语言模型前的指数无正相关。

Conclusion: 该指数识别了基础可自动化性，验证了AI注释方法，暗示自动化模式发生范式转变。

Abstract: This paper develops a theory-driven automation exposure index based on
Moravec's Paradox. Scoring 19,000 O*NET tasks on performance variance, tacit
knowledge, data abundance, and algorithmic gaps reveals that management, STEM,
and sciences occupations show the highest exposure. In contrast, maintenance,
agriculture, and construction show the lowest. The positive relationship
between wages and exposure challenges the notion of skill-biased technological
change if AI substitutes for workers. At the same time, tacit knowledge
exhibits a positive relationship with wages consistent with seniority-biased
technological change. This index identifies fundamental automatability rather
than current capabilities, while also validating the AI annotation method
pioneered by Eloundou et al. (2024) with a correlation of 0.72. The
non-positive relationship with pre-LLM indices suggests a paradigm shift in
automation patterns.

</details>


### [234] [Efficient Subsidy Targeting in the Health Insurance Marketplaces](https://arxiv.org/abs/2510.13791)
*Coleman Drake,Mark K. Meiselbach,Daniel Polsky*

Main category: econ.GN

TL;DR: 2025年医保市场参保人数创新高，2026年补贴到期或致超700万人失去医保。研究用马里兰州数据估计需求、模拟补贴分配，发现低收入者对保费更敏感，州补贴有一定效果但无法阻止参保人数下降。


<details>
  <summary>Details</summary>
Motivation: 2026年强化补贴到期预计使超700万人失去医保，10个州已设立补充补贴，但缺乏如何优化补贴结构以扩大覆盖范围的研究。

Method: 利用马里兰州医保市场行政登记数据估计需求，结合估计参数和不同预算约束，模拟州补充保费补贴的最优分配。

Result: 收入低于联邦贫困线200%的参保者对保费最敏感，每1000万美元州补贴可留住约5000名参保者，补贴低收入者后成本效益下降。

Conclusion: 各州有能力减轻但无法阻止保费税收抵免补贴到期导致的参保人数下降。

Abstract: Enrollment in the Health Insurance Marketplaces created by the Affordable
Care Act reached an all-time high of approximately 25 million Americans in
2025, roughly doubling since enhanced premium tax credit subsidies were made
available in 2021. The scheduled expiration of enhanced subsidies in 2026 is
estimated to leave over seven million Americans without health insurance
coverage. Ten states have created supplemental Marketplace subsidies, yet
little attention has been paid to how to best structure these subsidies to
maximize coverage. Using administrative enrollment data from Maryland's
Marketplace, we estimate demand for Marketplace coverage. Then, using estimated
parameters and varying budget constraints, we simulate how to optimally
allocate supplemental state premium subsidies to mitigate coverage losses from
enhanced premium subsidy expiration. We find that premium sensitivity is
greatest among enrollees with incomes below 200 percent of the federal poverty
level, where the marginal effect of an additional ten dollars in monthly
subsidies on the probability of coverage is approximately 6.5 percentage
points, and decreases to roughly 2.5 percentage points above 200 percent FPL.
Simulation results indicate that each 10 million dollars in annual state
subsidies could retain roughly 5,000 enrollees, though the cost-effectiveness
of these subsidies falls considerably once all enrollees below 200 percent of
the federal poverty level are fully subsidized. We conclude that states are
well positioned to mitigate, but not stop, coverage losses from expanded
premium tax credit subsidy expiration.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [235] [GO-Diff: Data-free and amortized global structure optimization](https://arxiv.org/abs/2510.13448)
*Nikolaj Rønne,Tejs Vegge,Arghya Bhowmik*

Main category: physics.comp-ph

TL;DR: 介绍GO - Diff，一种基于扩散的全局结构优化方法，无需先验数据或显式松弛，在能量评估和跨系统优化上有优势。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需先验数据和显式松弛，能直接采样低能量原子构型的全局结构优化方法。

Method: 使用玻尔兹曼加权得分匹配损失从头训练，通过自采样和模型细化的两阶段循环进行操作。

Result: 与传统优化管道相比，以显著更少的能量评估取得有竞争力的结果；支持摊销优化，在新任务上无需从头训练实现更快收敛。

Conclusion: GO - Diff是一种有效且高效的全局结构优化方法，在低能量结构采样和跨系统优化方面表现良好。

Abstract: We introduce GO-Diff, a diffusion-based method for global structure
optimization that learns to directly sample low-energy atomic configurations
without requiring prior data or explicit relaxation. GO-Diff is trained from
scratch using a Boltzmann-weighted score-matching loss, leveraging only the
known energy function to guide generation toward thermodynamically favorable
regions. The method operates in a two-stage loop of self-sampling and model
refinement, progressively improving its ability to target low-energy
structures. Compared to traditional optimization pipelines, GO-Diff achieves
competitive results with significantly fewer energy evaluations. Moreover, by
reusing pretrained models across related systems, GO-Diff supports amortized
optimization - enabling faster convergence on new tasks without retraining from
scratch.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [236] [FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs](https://arxiv.org/abs/2510.12839)
*Yingjia Wan,Haochen Tan,Xiao Zhu,Xinyu Zhou,Zhiwei Li,Qingsong Lv,Changxuan Sun,Jiaqi Zeng,Yi Xu,Jianqiao Lu,Yinhong Liu,Zhijiang Guo*

Main category: cs.CL

TL;DR: 提出FastFact框架评估大语言模型长文本生成的事实性，高效且可靠。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型长文本生成事实性的方法存在效率低和效果不佳的问题。

Method: 采用块级声明提取与基于置信度的预验证，从爬取网页收集文档级证据并在验证时选择性检索。

Result: 基于聚合和手动标注的基准测试表明FastFact能高效且有效地评估长文本生成的事实性。

Conclusion: FastFact框架在评估大语言模型长文本生成事实性方面高效可靠，代码和基准数据已开源。

Abstract: Evaluating the factuality of long-form generations from Large Language Models
(LLMs) remains challenging due to accuracy issues and costly human assessment.
Prior efforts attempt this by decomposing text into claims, searching for
evidence, and verifying claims, but suffer from critical drawbacks: (1)
inefficiency due to complex pipeline components unsuitable for long LLM
outputs, and (2) ineffectiveness stemming from inaccurate claim sets and
insufficient evidence collection of one-line snippets.
  To address these limitations, we propose \name, a fast and strong evaluation
framework that achieves the highest alignment with human evaluation and
efficiency among existing baselines. \name first employs chunk-level claim
extraction integrated with confidence-based pre-verification, significantly
reducing the cost of web searching and inference calling while ensuring
reliability. For searching and verification, it collects document-level
evidence from crawled webpages and selectively retrieves it during
verification, addressing the evidence insufficiency problem in previous
pipelines.
  Extensive experiments based on an aggregated and manually annotated benchmark
demonstrate the reliability of \name in both efficiently and effectively
evaluating the factuality of long-form LLM generations. Code and benchmark data
is available at https://github.com/Yingjia-Wan/FastFact.

</details>


### [237] [Classifier-Augmented Generation for Structured Workflow Prediction](https://arxiv.org/abs/2510.12825)
*Thomas Gschwind,Shramona Chakraborty,Nitin Gupta,Sameep Mehta*

Main category: cs.CL

TL;DR: 提出将自然语言描述转换为可执行ETL工作流的系统，采用CAG方法，比基线表现更好，是首个全面评估自然语言驱动ETL创作的系统。


<details>
  <summary>Details</summary>
Motivation: 现有ETL工具配置耗时且需深厚知识，希望用自然语言描述生成工作流。

Method: 采用Classifier-Augmented Generation (CAG) 方法，结合话语分解、分类器和特定阶段的少样本提示进行阶段预测，用边预测连接阶段，从子话语上下文推断阶段属性。

Result: 与强单提示和智能体基线相比，CAG提高了准确性和效率，大幅减少了token使用。

Conclusion: 该架构模块化、可解释，能端到端生成工作流，是首个对自然语言驱动ETL创作进行全面评估的系统。

Abstract: ETL (Extract, Transform, Load) tools such as IBM DataStage allow users to
visually assemble complex data workflows, but configuring stages and their
properties remains time consuming and requires deep tool knowledge. We propose
a system that translates natural language descriptions into executable
workflows, automatically predicting both the structure and detailed
configuration of the flow. At its core lies a Classifier-Augmented Generation
(CAG) approach that combines utterance decomposition with a classifier and
stage-specific few-shot prompting to produce accurate stage predictions. These
stages are then connected into non-linear workflows using edge prediction, and
stage properties are inferred from sub-utterance context. We compare CAG
against strong single-prompt and agentic baselines, showing improved accuracy
and efficiency, while substantially reducing token usage. Our architecture is
modular, interpretable, and capable of end-to-end workflow generation,
including robust validation steps. To our knowledge, this is the first system
with a detailed evaluation across stage prediction, edge layout, and property
generation for natural-language-driven ETL authoring.

</details>


### [238] [Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain](https://arxiv.org/abs/2510.13255)
*Jingmin An,Yilong Song,Ruolin Yang,Nai Ding,Lingxi Lu,Yuxuan Wang,Wei Wang,Chu Zhuang,Qian Wang,Fang Fang*

Main category: cs.CL

TL;DR: 本文引入HFTP工具研究大语言模型处理句法结构的计算模块，发现模型与人类大脑处理句法有异同，为理解大语言模型行为改进提供新见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽有语言能力，但负责的计算模块不明，且不清楚其行为能力机制是否与人类大脑类似。

Method: 引入Hierarchical Frequency Tagging Probe (HFTP)工具，利用频域分析识别大语言模型神经元组件和大脑皮层区域。

Result: 多种模型在类似层处理句法，人类大脑不同皮层区域处理不同句法层次；模型表征与大脑左半球更相似；升级模型与大脑相似度有不同变化趋势。

Conclusion: 研究为大语言模型行为改进的可解释性提供新见解，提出改进机制疑问，确立HFTP工具价值。

Abstract: Large Language Models (LLMs) demonstrate human-level or even superior
language abilities, effectively modeling syntactic structures, yet the specific
computational modules responsible remain unclear. A key question is whether LLM
behavioral capabilities stem from mechanisms akin to those in the human brain.
To address these questions, we introduce the Hierarchical Frequency Tagging
Probe (HFTP), a tool that utilizes frequency-domain analysis to identify
neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP)
neurons) and cortical regions (via intracranial recordings) encoding syntactic
structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama
2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human
brain relies on distinct cortical regions for different syntactic levels.
Representational similarity analysis reveals a stronger alignment between LLM
representations and the left hemisphere of the brain (dominant in language
processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows
greater brain similarity than Gemma, while Llama 3.1 shows less alignment with
the brain compared to Llama 2. These findings offer new insights into the
interpretability of LLM behavioral improvements, raising questions about
whether these advancements are driven by human-like or non-human-like
mechanisms, and establish HFTP as a valuable tool bridging computational
linguistics and cognitive neuroscience. This project is available at
https://github.com/LilTiger/HFTP.

</details>


### [239] [MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training](https://arxiv.org/abs/2510.12831)
*Taicheng Guo,Hai Wang,ChaoChun Liu,Mohsen Golalikhani,Xin Chen,Xiangliang Zhang,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 提出MTSQL - R1框架用于长周期多轮文本到SQL转换，实验显示其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有多轮文本到SQL系统将任务视为简单文本翻译，采用短周期范式，导致输出不可执行或不连贯。

Method: 将任务建模为马尔可夫决策过程，让智能体与数据库交互获取执行反馈，与对话记忆交互进行连贯性验证，执行提议 -> 执行 -> 验证 -> 改进循环。

Result: 在COSQL和SPARC上的实验表明MTSQL - R1始终优于强基线模型。

Conclusion: 环境驱动的验证和记忆引导的改进对对话语义解析很重要，完整资源将内部审核后发布。

Abstract: Multi-turn Text-to-SQL aims to translate a user's conversational utterances
into executable SQL while preserving dialogue coherence and grounding to the
target schema. However, most existing systems only regard this task as a simple
text translation task and follow a short-horizon paradigm, generating a query
per turn without execution, explicit verification, and refinement, which leads
to non-executable or incoherent outputs. We present MTSQL-R1, an agentic
training framework for long-horizon multi-turn Text-to-SQL. We cast the task as
a Markov Decision Process (MDP) in which an agent interacts with (i) a database
for execution feedback and (ii) a persistent dialogue memory for coherence
verification, performing an iterative propose to execute -> verify -> refine
cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that
MTSQL-R1 consistently outperforms strong baselines, highlighting the importance
of environment-driven verification and memory-guided refinement for
conversational semantic parsing. Full recipes (including code, trained models,
logs, reasoning trajectories, etc.) will be released after the internal review
to contribute to community research.

</details>


### [240] [Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment](https://arxiv.org/abs/2510.13387)
*Buwei He,Yang Liu,Zhaowei Zhang,Zixia Jia,Huijia Wu,Zhaofeng He,Zilong Zheng,Yipeng Kang*

Main category: cs.CL

TL;DR: 本文探索贝叶斯说服（BP）在单轮对话自然语言中的应用，以提升大语言模型（LLM）的战略说服能力，评估两种变体并与非BP基线对比，实验有多项发现。


<details>
  <summary>Details</summary>
Motivation: 说服能力对AI系统是挑战，现有研究忽视信息不对称战略运用或依赖强假设，因此探索BP在自然语言中应用以增强LLM战略说服能力。

Method: 采用包含承诺 - 沟通机制的框架，评估Semi - Formal - Natural - Language（SFNL）BP和Fully - Natural - Language（FNL）BP两种变体，在综合评估框架下与非BP基线对比。

Result: （1）BP策略引导的LLM说服成功率高于非BP基线；（2）SFNL更具可信度和逻辑连贯性，FNL在自然对话中情感共鸣和鲁棒性更强；（3）小模型经监督微调可达到与大模型相当的BP性能。

Conclusion: BP策略能有效提升LLM的战略说服能力，不同变体各有优势，小模型经微调也有良好表现。

Abstract: Persuasion, a fundamental social capability for humans, remains a challenge
for AI systems such as large language models (LLMs). Current studies often
overlook the strategic use of information asymmetry in message design or rely
on strong assumptions regarding pre-commitment. In this work, we explore the
application of Bayesian Persuasion (BP) in natural language within single-turn
dialogue settings, to enhance the strategic persuasion capabilities of LLMs.
Our framework incorporates a commitment-communication mechanism, where the
persuader explicitly outlines an information schema by narrating their
potential types (e.g., honest or dishonest), thereby guiding the persuadee in
performing the intended Bayesian belief update. We evaluate two variants of our
approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language
(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)
baselines within a comprehensive evaluation framework. This framework covers a
diverse set of persuadees -- including LLM instances with varying prompts and
fine-tuning and human participants -- across tasks ranging from specially
designed persuasion scenarios to general everyday situations. Experimental
results on LLM-based agents reveal three main findings: (1) LLMs guided by BP
strategies consistently achieve higher persuasion success rates than NBP
baselines; (2) SFNL exhibits greater credibility and logical coherence, while
FNL shows stronger emotional resonance and robustness in naturalistic
conversations; (3) with supervised fine-tuning, smaller models can attain BP
performance comparable to that of larger models.

</details>


### [241] [ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering](https://arxiv.org/abs/2510.13312)
*Simon Lupart,Mohammad Aliannejadi,Evangelos Kanoulas*

Main category: cs.CL

TL;DR: 提出基于强化学习的对话问答推理框架ChatR1，用意图感知奖励解决稀疏延迟奖励问题，在多数据集表现优，消融实验证实奖励有效性。


<details>
  <summary>Details</summary>
Motivation: 对话问答中推理重要，但用户意图动态变化、表述不明确，现有静态管道存在不足，需更灵活方法。

Method: 提出ChatR1框架，通过强化学习实现搜索和推理交织；提出意图感知奖励解决强化学习奖励问题。

Result: ChatR1在3B和7B模型骨干上表现强，在五个对话问答数据集上超越竞品；消融实验证实意图感知奖励有效；分析揭示推理轨迹和搜索工具使用情况；跨领域泛化能力好。

Conclusion: 基于强化学习的推理比静态对话问答管道更灵活、对上下文更敏感。

Abstract: We present ChatR1, a reasoning framework based on reinforcement learning (RL)
for conversational question answering (CQA). Reasoning plays an important role
in CQA, where user intent evolves across dialogue turns, and utterances are
often underspecified, requiring contextual interpretation, query reformulation,
and dynamic coordination between retrieval and generation. Unlike static
`rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and
reasoning across turns, enabling exploratory and adaptive behaviors learned
through RL. To address the challenge of sparse and delayed rewards in RL, we
propose an intent-aware reward that provides turn-level feedback by aligning
retrieval and reasoning with evolving user goals. Our proposed ChatR1
demonstrates strong performance on both 3B and 7B model backbones,
outperforming competitive models on five CQA datasets, measured by different
metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA
datasets to cover topic shifts, evolving intents, mixed-initiative dialogues,
and multi-document grounding, testing ChatR1's performance from various
aspects. Ablation studies confirm the effectiveness of the intent-aware reward.
Our analyses further reveal diverse reasoning trajectories and effective use of
the search tool. ChatR1 also generalizes robustly across domains, demonstrating
that RL-based reasoning enables more flexible and context-sensitive behavior
than static CQA pipelines.

</details>


### [242] [Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and Few-Shot Learning](https://arxiv.org/abs/2510.12807)
*Mahdi Cherakhloo,Arash Abbasi,Mohammad Saeid Sarafraz,Bijan Vosoughi Vahdat*

Main category: cs.CL

TL;DR: 本文对用于波斯语NLP任务的多个开源大语言模型进行综合基准测试，发现Gemma 2表现出色，但多数模型在特定任务有挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在低资源语言如波斯语的有效性需深入研究，故开展波斯语NLP任务的模型基准测试。

Method: 利用零样本和少样本学习范式，使用ParsiNLU和ArmanEmo等数据集，通过Accuracy、F1-score等指标评估模型。

Result: Gemma 2在几乎所有任务上表现优于其他模型，尤其在复杂推理任务；多数模型在命名实体识别等词元级理解任务有困难。

Conclusion: 本研究为多语言大语言模型研究提供波斯语表现的见解，为未来模型开发提供基准。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
numerous languages; however, their effectiveness in low-resource languages like
Persian requires thorough investigation. This paper presents a comprehensive
benchmark of several open-source LLMs for Persian Natural Language Processing
(NLP) tasks, utilizing both zero-shot and few-shot learning paradigms. We
evaluate models across a range of tasks including sentiment analysis, named
entity recognition, reading comprehension, and question answering, using
established Persian datasets such as ParsiNLU and ArmanEmo. Our methodology
encompasses rigorous experimental setups for both zero-shot and few-shot
scenarios, employing metrics such as Accuracy, F1-score, BLEU, and ROUGE for
performance evaluation. The results reveal that Gemma 2 consistently
outperforms other models across nearly all tasks in both learning paradigms,
with particularly strong performance in complex reasoning tasks. However, most
models struggle with token-level understanding tasks like Named Entity
Recognition, highlighting specific challenges in Persian language processing.
This study contributes to the growing body of research on multilingual LLMs,
providing valuable insights into their performance in Persian and offering a
benchmark for future model development.

</details>


### [243] [Cancer Diagnosis Categorization in Electronic Health Records Using Large Language Models and BioBERT: Model Performance Evaluation Study](https://arxiv.org/abs/2510.12813)
*Soheil Hashtarkhani,Rezaur Rashid,Christopher L Brett,Lokesh Chinthala,Fekede Asefa Kumsa,Janet A Zink,Robert L Davis,David L Schwartz,Arash Shaban-Nejad*

Main category: cs.CL

TL;DR: 评估5种模型对癌症诊断分类的性能，BioBERT和GPT - 4o表现较好，当前性能用于行政和研究尚可，临床应用需规范文档和人工监督。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据需预处理以用于预测医疗模型，人工智能自然语言处理工具用于诊断分类的性能和临床可靠性需系统评估。

Method: 分析3456份癌症患者记录中的762个独特诊断，测试模型将诊断分类到14个预定义类别的能力，由两位肿瘤专家验证分类。

Result: BioBERT在ICD代码的加权宏F1分数最高，与GPT - 4o的ICD代码准确率相当；GPT - 4o在自由文本诊断的加权宏F1分数和准确率上略高于BioBERT；GPT - 3.5、Gemini和Llama整体表现较差，存在常见误分类模式。

Conclusion: 当前性能用于行政和研究足够，可靠的临床应用需要标准化文档实践和人工监督。

Abstract: Electronic health records contain inconsistently structured or free-text
data, requiring efficient preprocessing to enable predictive health care
models. Although artificial intelligence-driven natural language processing
tools show promise for automating diagnosis classification, their comparative
performance and clinical reliability require systematic evaluation. The aim of
this study is to evaluate the performance of 4 large language models (GPT-3.5,
GPT-4o, Llama 3.2, and Gemini 1.5) and BioBERT in classifying cancer diagnoses
from structured and unstructured electronic health records data. We analyzed
762 unique diagnoses (326 International Classification of Diseases (ICD) code
descriptions, 436free-text entries) from 3456 records of patients with cancer.
Models were tested on their ability to categorize diagnoses into 14predefined
categories. Two oncology experts validated classifications. BioBERT achieved
the highest weighted macro F1-score for ICD codes (84.2) and matched GPT-4o in
ICD code accuracy (90.8). For free-text diagnoses, GPT-4o outperformed BioBERT
in weighted macro F1-score (71.8 vs 61.5) and achieved slightly higher accuracy
(81.9 vs 81.6). GPT-3.5, Gemini, and Llama showed lower overall performance on
both formats. Common misclassification patterns included confusion between
metastasis and central nervous system tumors, as well as errors involving
ambiguous or overlapping clinical terminology. Although current performance
levels appear sufficient for administrative and research use, reliable clinical
applications will require standardized documentation practices alongside robust
human oversight for high-stakes decision-making.

</details>


### [244] [From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP](https://arxiv.org/abs/2510.12817)
*Shanshan Xu,Santosh T. Y. S. S,Barbara Plank*

Main category: cs.CL

TL;DR: 过去NLP中人类标注差异（HLV）被视为噪声，如今在大语言模型时代其作用渐显，但现有偏好学习数据集常忽略HLV，本文主张设计AI系统时应将保留HLV作为目标并给出行动步骤。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型中，后训练依赖人类反馈，HLV作用愈发重要，但当前偏好学习数据集忽略HLV，无法保留人类价值观的多元性。

Method: 主张将保留HLV作为设计AI系统的目标，并提出将HLV主动纳入偏好数据集的行动步骤。

Result: 无明确实验结果

Conclusion: 设计AI系统时应将保留HLV作为目标，主动将其纳入偏好数据集。

Abstract: Human Label Variation (HLV) refers to legitimate disagreement in annotation
that reflects the genuine diversity of human perspectives rather than mere
error. For decades, HLV in NLP was dismissed as noise to be discarded, and only
slowly over the last decade has it been reframed as a signal for improving
model robustness. With the rise of large language models (LLMs), where
post-training on human feedback has become central to model alignment, the role
of HLV has become increasingly consequential. Yet current preference-learning
datasets routinely aggregate multiple annotations into a single label, thereby
flattening diverse perspectives into a false universal agreement and erasing
precisely the pluralism of human values that alignment aims to preserve. In
this position paper, we argue that preserving HLV as an embodiment of human
pluralism must be treated as a Selbstzweck - a goal it self when designing AI
systems. We call for proactively incorporating HLV into preference datasets and
outline actionable steps towards it.

</details>


### [245] [MEDEQUALQA: Evaluating Biases in LLMs with Counterfactual Reasoning](https://arxiv.org/abs/2510.12818)
*Rajarshi Ghosh,Abhay Gupta,Hudson McBride,Anurag Vaidya,Faisal Mahmood*

Main category: cs.CL

TL;DR: 引入MEDEQUALQA基准测试评估大语言模型在医疗决策支持中推理稳定性，发现虽整体相似度高但存在局部差异和临床相关偏差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于临床决策支持时，人口统计学线索会影响推理，此前对控制人口统计学变化下内部推理的转变了解甚少。

Method: 引入MEDEQUALQA基准测试，扰动患者代词同时保持关键症状和条件不变，扩展临床案例生成数据集，评估GPT - 4.1模型并计算推理轨迹的语义文本相似度。

Result: 整体相似度高（平均STS > 0.80），但在引用的风险因素、指南依据和差异排序方面存在一致的局部差异，即使最终诊断不变。

Conclusion: MEDEQUALQA为审核医疗AI的推理稳定性提供了可控的诊断环境。

Abstract: Large language models (LLMs) are increasingly deployed in clinical decision
support, yet subtle demographic cues can influence their reasoning. Prior work
has documented disparities in outputs across patient groups, but little is
known about how internal reasoning shifts under controlled demographic changes.
We introduce MEDEQUALQA, a counterfactual benchmark that perturbs only patient
pronouns (he/him, she/her, they/them) while holding critical symptoms and
conditions (CSCs) constant. Each clinical vignette is expanded into single-CSC
ablations, producing three parallel datasets of approximately 23,000 items each
(69,000 total). We evaluate a GPT-4.1 model and compute Semantic Textual
Similarity (STS) between reasoning traces to measure stability across pronoun
variants. Our results show overall high similarity (mean STS >0.80), but reveal
consistent localized divergences in cited risk factors, guideline anchors, and
differential ordering, even when final diagnoses remain unchanged. Our error
analysis highlights certain cases in which the reasoning shifts, underscoring
clinically relevant bias loci that may cascade into inequitable care.
MEDEQUALQA offers a controlled diagnostic setting for auditing reasoning
stability in medical AI.

</details>


### [246] [Scheming Ability in LLM-to-LLM Strategic Interactions](https://arxiv.org/abs/2510.12826)
*Thao Pham*

Main category: cs.CL

TL;DR: 研究前沿大语言模型代理的策略欺骗能力，用两个博弈论框架测试四个模型，发现模型有较强欺骗倾向，强调多智能体环境高风险评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理在不同场景自主部署，评估其策略欺骗能力很关键，且LLM间的策略谋划研究不足。

Method: 通过Cheap Talk信号博弈和Peer Evaluation对抗博弈两个博弈论框架，测试四个模型，分析有/无明确提示下的谋划表现和策略。

Result: 有提示时多数模型表现接近完美，无提示时模型有显著欺骗倾向，Peer Evaluation中模型都选欺骗，Cheap Talk中谋划成功率95 - 100%。

Conclusion: 需要在多智能体环境中用高风险博弈论场景进行稳健评估。

Abstract: As large language model (LLM) agents are deployed autonomously in diverse
contexts, evaluating their capacity for strategic deception becomes crucial.
While recent research has examined how AI systems scheme against human
developers, LLM-to-LLM scheming remains underexplored. We investigate the
scheming ability and propensity of frontier LLM agents through two
game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation
adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro,
Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and
without explicit prompting while analyzing scheming tactics through
chain-of-thought reasoning. When prompted, most models, especially
Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance.
Critically, models exhibited significant scheming propensity without prompting:
all models chose deception over confession in Peer Evaluation (100% rate),
while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These
findings highlight the need for robust evaluations using high-stakes
game-theoretic scenarios in multi-agent settings.

</details>


### [247] [Mathematics with large language models as provers and verifiers](https://arxiv.org/abs/2510.12829)
*Hieu Le Duc,Leo Liberti*

Main category: cs.CL

TL;DR: 本文报道了ChatGPT通过特定协议实现定理证明的成果，解决了部分IMO问题和数论猜想。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型的定理证明能力，验证人工智能证明定理的可行性。

Method: 使用gpt - 5模型的不同证明者和验证者实例协作，用lean证明助手形式验证，人工验证前提和结论一致性。

Result: 解决了2025年六个IMO问题中的五个，解决了66个数论猜想中的三分之一。

Conclusion: 所采用的方法在定理证明方面有一定成效。

Abstract: During 2024 and 2025 the discussion about the theorem-proving capabilities of
large language models started reporting interesting success stories, mostly to
do with difficult exercises (such as problems from the International
Mathematical Olympiad), but also with conjectures [Feldman & Karbasi,
arXiv:2509.18383v1] formulated for the purpose of verifying whether the
artificial intelligence could prove it. In this paper we report a theorem
proving feat achieved by ChatGPT by using a protocol involving different prover
and verifier instances of the gpt-5 model working collaboratively. To make sure
that the produced proofs do not suffer from hallucinations, the final proof is
formally verified by the lean proof assistant, and the conformance of premises
and conclusion of the lean code is verified by a human. Our methodology was
able to solve five out of six 2025 IMO problems, and close a third of the
sixty-six number theory conjectures in [Cohen, Journal of Integer Sequences,
2025].

</details>


### [248] [Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study](https://arxiv.org/abs/2510.12835)
*Kon Woo Kim,Rezarta Islamaj,Jin-Dong Kim,Florian Boudin,Akiko Aizawa*

Main category: cs.CL

TL;DR: 研究如何将现有注释指南重新用于指导大语言模型进行文本注释任务，提出方法并通过实验展示潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统注释指南是为人类注释者编写，大语言模型需要明确、结构化的指令，因此要研究如何将现有指南重新用于大语言模型注释者。

Method: 提出一种面向审核的指南重新利用方法，通过大语言模型审核过程将指南转化为清晰指令。

Result: 以NCBI疾病语料库为案例研究，实验表明重新利用的指南能有效指导大语言模型注释者，但也揭示了一些实际挑战。

Conclusion: 该工作流程有潜力支持可扩展且经济高效的注释指南细化和自动注释。

Abstract: This study investigates how existing annotation guidelines can be repurposed
to instruct large language model (LLM) annotators for text annotation tasks.
Traditional guidelines are written for human annotators who internalize
training, while LLMs require explicit, structured instructions. We propose a
moderation-oriented guideline repurposing method that transforms guidelines
into clear directives for LLMs through an LLM moderation process. Using the
NCBI Disease Corpus as a case study, our experiments show that repurposed
guidelines can effectively guide LLM annotators, while revealing several
practical challenges. The results highlight the potential of this workflow to
support scalable and cost-effective refinement of annotation guidelines and
automated annotation.

</details>


### [249] [A\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning](https://arxiv.org/abs/2510.12838)
*Qianben Chen,Jingyi Cao,Jiayu Zhang,Tianrui Qin,Xiaowan Li,King Zhu,Dingfeng Shi,He Zhu,Minghao Liu,Xiaobo Liang,Ge Zhang,Jian Yang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出统一框架A²FM解决大语言模型两类家族的效率问题，实现新SOTA并提高成本效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型两类家族因训练目标不同存在优势不匹配和简单查询效率低的问题。

Method: 提出A²FM框架，遵循先路由后对齐原则，引入即时模式，提出自适应策略优化（APO）。

Result: 在多个基准测试中取得新SOTA，自适应执行降低成本。

Conclusion: A²FM在保持可比准确性的同时，显著提高成本效率。

Abstract: Large language models split into two families: reasoning-centric LLMs, which
strengthen internal chain-of-thought reasoning but cannot invoke external
tools, and agentic LLMs, which learn to interact with environments and leverage
tools but often lag in deep reasoning. This divide arises from fundamentally
different training objectives, leading to mismatched strengths and inefficiency
on simple queries, where both families tend to overthink or over-call tools. In
this work, we present Adaptive Agent Foundation Model (A\textsuperscript{2}FM),
a unified framework that follows a route-then-align principle: the model first
learns task-aware routing and then aligns mode-specific trajectories under a
shared backbone. To address the inefficiency gap, we introduce a third
mode-instant-that handles simple queries directly, preventing unnecessary
reasoning or tool calls while complementing the agentic and reasoning modes. To
jointly enhance accuracy and efficiency, we propose Adaptive Policy
Optimization (APO), which enforces adaptive sampling across modes and applies a
cost-regularized reward. On the 32B scale, A\textsuperscript{2}FM achieves
13.4\% on BrowseComp, 70.4\% on AIME25, and 16.7\% on HLE, setting new SOTA
among comparable models and performing competitively with frontier LLMs across
agentic, reasoning, and general benchmarks. Notably, the adaptive execution
achieves a cost of pass of only \$0.00487 per correct answer-cutting cost by
45.2\% relative to reasoning and 33.5\% relative to agentic, thus delivering
substantially higher cost efficiency while maintaining comparable accuracy.

</details>


### [250] [VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages](https://arxiv.org/abs/2510.12845)
*Jesse Atuhurra,Iqra Ali,Tomoya Iwakura,Hidetaka Kamigaito,Tatsuya Hiraoka*

Main category: cs.CL

TL;DR: 本文提出多语言基准VLURes评估视觉语言模型（VLMs）细粒度能力，评估了十个VLMs，指出VLURes对开发智能体的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs评估多基于英语短文本基准，为评估其在长文本多语言环境下的细粒度能力。

Method: 引入含八个视觉语言任务和无关性任务的多语言基准VLURes，从网络资源整理数据集，通过提示VLMs生成响应和理由，由自动和母语者评估。

Result: 评估了十个VLMs，最佳模型GPT - 4o总体准确率90.8%，落后人类表现6.7%，开源模型差距更大。

Conclusion: VLURes对开发处理多模态视觉推理的智能体至关重要。

Abstract: Vision Language Models (VLMs) are pivotal for advancing perception in
intelligent agents. Yet, evaluation of VLMs remains limited to predominantly
English-centric benchmarks in which the image-text pairs comprise short texts.
To evaluate VLM fine-grained abilities, in four languages under long-text
settings, we introduce a novel multilingual benchmark VLURes featuring eight
vision-and-language tasks, and a pioneering unrelatedness task, to probe the
fine-grained Visual and Linguistic Understanding capabilities of VLMs across
English, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets,
curated from web resources in the target language, encompass ten diverse image
categories and rich textual context, introducing valuable vision-language
resources for Swahili and Urdu. By prompting VLMs to generate responses and
rationales, evaluated automatically and by native speakers, we uncover
performance disparities across languages and tasks critical to intelligent
agents, such as object recognition, scene understanding, and relationship
understanding. We conducted evaluations of ten VLMs with VLURes. The best
performing model, GPT-4o, achieves an overall accuracy of 90.8% and lags human
performance by 6.7%, though the gap is larger for open-source models. The gap
highlights VLURes' critical role in developing intelligent agents to tackle
multi-modal visual reasoning.

</details>


### [251] [Efficient Adaptive Transformer: An Empirical Study and Reproducible Framework](https://arxiv.org/abs/2510.12856)
*Jan Miller*

Main category: cs.CL

TL;DR: EAT框架将三种自适应效率技术统一，有开源基准测试管道，在SST - 2上比优化的DistilBERT基线准确率略高，贡献是提供端到端可复现框架。


<details>
  <summary>Details</summary>
Motivation: 构建一个输入自适应推理的可复现架构，研究自适应transformer。

Method: 将渐进式token剪枝、稀疏注意力和动态提前退出三种自适应效率技术统一到EAT框架，并提供开源基准测试管道。

Result: 在浅层六层模型中组合机制会增加延迟，在SST - 2上比优化的DistilBERT基线准确率略高。

Conclusion: EAT框架展示了动态计算在对延迟敏感的NLP中的潜力，提供的端到端可复现框架可作为社区工具用于自适应transformer的进一步研究。

Abstract: The Efficient Adaptive Transformer (EAT) framework unifies three adaptive
efficiency techniques - progressive token pruning, sparse attention, and
dynamic early exiting - into a single, reproducible architecture for
input-adaptive inference. EAT provides an open-source benchmarking pipeline
that automates data processing, timing, and ablation across GLUE tasks (SST-2,
QQP, MNLI). Although this empirical study finds that combining these mechanisms
can increase latency in shallow six-layer models, it demonstrates that EAT
achieves slightly higher accuracy than the optimized DistilBERT baseline on
SST-2, illustrating the potential of dynamic computation for latency-sensitive
NLP. The main contribution is the open, end-to-end reproducible framework -
complete with scripts, CSV logging, and analysis utilities - intended to serve
as a community tool for further research on adaptive transformers.

</details>


### [252] [A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation](https://arxiv.org/abs/2510.12858)
*Mohammed Hilal Al-Kharusi,Khizar Hayat,Khalil Bader Al Ruqeishi,Haroon Rashid Lone*

Main category: cs.CL

TL;DR: 本文针对现代《古兰经》诵读评估自动化工具存在的问题进行文献综述，指出当前方法的不足，建议转向以知识为中心的计算框架，认为未来应采用结合语言知识和音频分析的混合系统。


<details>
  <summary>Details</summary>
Motivation: 现代《古兰经》诵读教学面临挑战，现有自动化评估工具未广泛应用且缺乏教学效果，需研究解决这一关键差距。

Method: 对过去二十年的学术研究、网络平台和商业应用进行全面的文献综述分析。

Result: 发现现有的自动语音识别架构存在根本的不匹配问题，如重词汇识别轻声学评估、数据依赖、存在人口统计学偏差等。

Conclusion: 自动《古兰经》评估的未来在于将深层语言知识与先进音频分析相结合的混合系统，可支持全球学习者。

Abstract: The sacred practice of Quranic recitation (Tajweed), governed by precise
phonetic, prosodic, and theological rules, faces significant pedagogical
challenges in the modern era. While digital technologies promise unprecedented
access to education, automated tools for recitation evaluation have failed to
achieve widespread adoption or pedagogical efficacy. This literature review
investigates this critical gap, conducting a comprehensive analysis of academic
research, web platforms, and commercial applications developed over the past
two decades. Our synthesis reveals a fundamental misalignment in prevailing
approaches that repurpose Automatic Speech Recognition (ASR) architectures,
which prioritize lexical recognition over qualitative acoustic assessment and
are plagued by data dependency, demographic biases, and an inability to provide
diagnostically useful feedback. Critiquing these data--driven paradigms, we
argue for a foundational paradigm shift towards a knowledge-centric
computational framework. Capitalizing on the immutable nature of the Quranic
text and the precisely defined rules of Tajweed, we propose that a robust
evaluator must be architected around anticipatory acoustic modeling based on
canonical rules and articulation points (Makhraj), rather than relying on
statistical patterns learned from imperfect and biased datasets. This review
concludes that the future of automated Quranic evaluation lies in hybrid
systems that integrate deep linguistic knowledge with advanced audio analysis,
offering a path toward robust, equitable, and pedagogically sound tools that
can faithfully support learners worldwide.

</details>


### [253] [CurLL: A Developmental Framework to Evaluate Continual Learning in Language Models](https://arxiv.org/abs/2510.13008)
*Pavan Kalyan,Shubhra Mishra,Satya Lokam,Navin Goyal*

Main category: cs.CL

TL;DR: 提出基于5 - 10岁人类发展轨迹的持续学习数据集和基准CurlL，分析模型技能保留和迁移效率，推动语言模型持续学习评估。


<details>
  <summary>Details</summary>
Motivation: 实现对模型逐步获取新技能能力的系统和细粒度评估。

Method: 构建涵盖五个发展阶段的CurlL，生成23.4B-token合成数据集，用135M参数的变压器在不同设置下训练。

Result: 展示了技能保留和转移效率的权衡。

Conclusion: 通过模仿人类学习模式和对技能依赖的细粒度控制，推动了语言模型的持续学习评估。

Abstract: We introduce a comprehensive continual learning dataset and benchmark (CurlL)
grounded in human developmental trajectories from ages 5-10, enabling
systematic and fine-grained assessment of models' ability to progressively
acquire new skills. CurlL spans five developmental stages (0-4) covering ages
5-10, supported by a skill graph that breaks down broad skills into smaller
abilities, concrete goals, and measurable indicators, while also capturing
which abilities build on others. We generate a 23.4B-token synthetic dataset
with controlled skill progression, vocabulary complexity, and format diversity,
comprising paragraphs, comprehension-based QA (CQA), skill-testing QA (CSQA),
and instruction-response (IR) pairs. Stage-wise token counts range from 2.12B
to 6.78B tokens, supporting precise analysis of forgetting, forward transfer,
and backward transfer. Using a 135M-parameter transformer trained under
independent, joint, and sequential (continual) setups, we show trade-offs in
skill retention and transfer efficiency. By mirroring human learning patterns
and providing fine-grained control over skill dependencies, this work advances
continual learning evaluations for language models.

</details>


### [254] [ESI: Epistemic Uncertainty Quantification via Semantic-preserving Intervention for Large Language Models](https://arxiv.org/abs/2510.13103)
*Mingda Li,Xinyu Li,Weinan Zhang,Longxuan Ma*

Main category: cs.CL

TL;DR: 本文从因果角度建立大语言模型不确定性与语义干预不变性的联系，提出灰盒不确定性量化方法，经理论与实验验证其有效性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 量化大语言模型的不确定性，提高模型可靠性。

Method: 从因果角度建立联系，提出基于语义保留干预前后模型输出变化的灰盒不确定性量化方法。

Result: 理论证明该方法能有效估计认知不确定性，实验表明在多种大语言模型和问答数据集上，该方法兼具有效性和计算效率。

Conclusion: 提出的灰盒不确定性量化方法能有效量化大语言模型的不确定性，且具有计算效率优势。

Abstract: Uncertainty Quantification (UQ) is a promising approach to improve model
reliability, yet quantifying the uncertainty of Large Language Models (LLMs) is
non-trivial. In this work, we establish a connection between the uncertainty of
LLMs and their invariance under semantic-preserving intervention from a causal
perspective. Building on this foundation, we propose a novel grey-box
uncertainty quantification method that measures the variation in model outputs
before and after the semantic-preserving intervention. Through theoretical
justification, we show that our method provides an effective estimate of
epistemic uncertainty. Our extensive experiments, conducted across various LLMs
and a variety of question-answering (QA) datasets, demonstrate that our method
excels not only in terms of effectiveness but also in computational efficiency.

</details>


### [255] [Multi-Label Clinical Text Eligibility Classification and Summarization System](https://arxiv.org/abs/2510.13115)
*Surya Tejaswi Yerramsetty,Almas Fathimah*

Main category: cs.CL

TL;DR: 本文提出利用NLP和LLMs的系统实现多标签临床文本资格分类与总结，评估了多种方法，证明其有效性，有提升研究效率潜力。


<details>
  <summary>Details</summary>
Motivation: 临床试验对医学进步至关重要，需合适且多样化背景参与者，需自动化评估系统提升效率。

Method: 结合词嵌入、命名实体识别等特征提取方法，运用随机森林和SVM模型进行多标签分类，评估多种总结技术。

Result: 用ROUGE分数评估证明了所提方法的有效性。

Conclusion: 该系统有潜力通过数据驱动方法自动化临床试验资格评估，提高研究效率。

Abstract: Clinical trials are central to medical progress because they help improve
understanding of human health and the healthcare system. They play a key role
in discovering new ways to detect, prevent, or treat diseases, and it is
essential that clinical trials include participants with appropriate and
diverse medical backgrounds. In this paper, we propose a system that leverages
Natural Language Processing (NLP) and Large Language Models (LLMs) to automate
multi-label clinical text eligibility classification and summarization. The
system combines feature extraction methods such as word embeddings (Word2Vec)
and named entity recognition to identify relevant medical concepts, along with
traditional vectorization techniques such as count vectorization and TF-IDF
(Term Frequency-Inverse Document Frequency). We further explore weighted TF-IDF
word embeddings that integrate both count-based and embedding-based strengths
to capture term importance effectively. Multi-label classification using Random
Forest and SVM models is applied to categorize documents based on eligibility
criteria. Summarization techniques including TextRank, Luhn, and GPT-3 are
evaluated to concisely summarize eligibility requirements. Evaluation with
ROUGE scores demonstrates the effectiveness of the proposed methods. This
system shows potential for automating clinical trial eligibility assessment
using data-driven approaches, thereby improving research efficiency.

</details>


### [256] [Stable LLM Ensemble: Interaction between Example Representativeness and Diversity](https://arxiv.org/abs/2510.13143)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究示例代表性和输出多样性对大语言模型集成性能的影响，提出方法表现优于随机选择和5 - shot提示。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型单次预测的准确性和鲁棒性对示例和集成成员多样性敏感的问题。

Method: 比较基于质心的代表性示例（提出）和随机采样示例（基线）两种单次策略，并改变采样温度。

Result: 提出的方法在较高温度设置下，macro - F1比随机选择高7.6%、RMSE低10.5%，超过5 - shot提示21.1%（macro - F1）和 - 24.0%（RMSE）。

Conclusion: 结合代表性示例选择和增加温度能为集成提供适当的多样性，示例选择和控制多样性在设计有效单次大语言模型集成中很重要。

Abstract: Large language models (LLMs) have achieved remarkable results in wide range
of domains. However, the accuracy and robustness of one-shot LLM predictions
remain highly sensitive to the examples and the diversity among ensemble
members. This study systematically investigates the effects of example
representativeness (one-shot strategy) and output diversity (sampling
temperature) on LLM ensemble performance. Two one-shot strategies are compared:
centroid-based representative examples (proposed) and randomly sampled examples
(baseline) and sampling temperature also is varied. The proposed approach with
higher temperature setting significantly outperforms random selection by +7.6%
(macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot
prompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that
combining representative example selection with increased temperature provides
the appropriate level of diversity to the ensemble. This work highlights the
practical importance of both example selection and controlled diversity in
designing effective one-shot LLM ensembles.

</details>


### [257] [StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis Preservation](https://arxiv.org/abs/2510.13194)
*Xi Chen,Yuchen Song,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 提出应力感知的语音到语音翻译系统，利用大语言模型进行跨语言强调转换，实验表明该方法表现出色。


<details>
  <summary>Details</summary>
Motivation: 在语音到语音翻译中保留词级强调，解决数据稀缺问题。

Method: 将源语言重音转换为目标语言标签指导可控TTS模型，开发自动生成对齐训练数据的管道，并引入“大语言模型作为评判者”进行评估。

Result: 该方法在保留强调方面大幅优于基线，同时在翻译质量、说话者意图和自然度方面表现相当。

Conclusion: 强调了韵律在翻译中的重要性，为语音到语音翻译中保留副语言线索提供了有效且数据高效的解决方案。

Abstract: We propose a stress-aware speech-to-speech translation (S2ST) system that
preserves word-level emphasis by leveraging LLMs for cross-lingual emphasis
conversion. Our method translates source-language stress into target-language
tags that guide a controllable TTS model. To overcome data scarcity, we
developed a pipeline to automatically generate aligned training data and
introduce the "LLM-as-Judge" for evaluation. Experiments show our approach
substantially outperforms baselines in preserving emphasis while maintaining
comparable translation quality, speaker intent, and naturalness. Our work
highlights the importance of prosody in translation and provides an effective,
data-efficient solution for preserving paralinguistic cues in S2ST.

</details>


### [258] [LLM-Guided Synthetic Augmentation (LGSA) for Mitigating Bias in AI Systems](https://arxiv.org/abs/2510.13202)
*Sai Suhruth Reddy Karri,Yashwanth Sai Nallapuneni,Laxmi Narasimha Reddy Mallireddy,Gopichand G*

Main category: cs.CL

TL;DR: 文章提出LLM - Guided Synthetic Augmentation (LGSA)方法缓解AI系统偏差，在英文短句数据集上评估，结果显示该方法减少性能差异且不降低准确率，是有效偏差缓解策略。


<details>
  <summary>Details</summary>
Motivation: AI系统偏差引发伦理和实际问题，传统公平方法依赖受保护属性标签、存在准确率 - 公平性权衡且难以跨数据集泛化，需新方法解决。

Method: 提出LGSA方法，用大语言模型为代表性不足群体生成反事实示例并保持标签完整性，在含性别代词等的英文短句数据集上评估，通过结构化提示生成性别互换释义，进行质量控制后用增强数据集训练分类器。

Result: LGSA减少性能差异且不降低准确率，基线模型准确率96.7%、性别偏差差距7.2%，简单交换增强将差距降至0.7%但准确率降至95.6%，LGSA准确率达99.1%、偏差差距1.9%，提升女性标签示例性能。

Conclusion: LGSA是有效偏差缓解策略，能增强子组平衡，保持高任务准确率和标签保真度。

Abstract: Bias in AI systems, especially those relying on natural language data, raises
ethical and practical concerns. Underrepresentation of certain groups often
leads to uneven performance across demographics. Traditional fairness methods,
such as pre-processing, in-processing, and post-processing, depend on
protected-attribute labels, involve accuracy-fairness trade-offs, and may not
generalize across datasets. To address these challenges, we propose LLM-Guided
Synthetic Augmentation (LGSA), which uses large language models to generate
counterfactual examples for underrepresented groups while preserving label
integrity. We evaluated LGSA on a controlled dataset of short English sentences
with gendered pronouns, professions, and binary classification labels.
Structured prompts were used to produce gender-swapped paraphrases, followed by
quality control including semantic similarity checks, attribute verification,
toxicity screening, and human spot checks. The augmented dataset expanded
training coverage and was used to train a classifier under consistent
conditions. Results show that LGSA reduces performance disparities without
compromising accuracy. The baseline model achieved 96.7 percent accuracy with a
7.2 percent gender bias gap. Simple swap augmentation reduced the gap to 0.7
percent but lowered accuracy to 95.6 percent. LGSA achieved 99.1 percent
accuracy with a 1.9 percent bias gap, improving performance on female-labeled
examples. These findings demonstrate that LGSA is an effective strategy for
bias mitigation, enhancing subgroup balance while maintaining high task
accuracy and label fidelity.

</details>


### [259] [Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems](https://arxiv.org/abs/2510.13291)
*Xuxin Cheng,Ke Zeng,Zhiquan Cao,Linyi Dai,Wenxuan Gao,Fei Han,Ai Jian,Feng Hong,Wenxing Hu,Zihe Huang,Dejian Kong,Jia Leng,Zhuoyuan Liao,Pei Liu,Jiaye Lin,Xing Ma,Jingqing Ruan,Jiaxing Song,Xiaoyu Tan,Ruixuan Xiao,Wenhui Yu,Wenyu Zhan,Haoxing Zhang,Chao Zhou,Hao Zhou,Shaodong Zheng,Ruinian Chen,Siyuan Chen,Ziyang Chen,Yiwen Dong,Yaoyou Fan,Yangyi Fang,Yang Gan,Shiguang Guo,Qi He,Chaowen Hu,Binghui Li,Dailin Li,Xiangyu Li,Yan Li,Chengjian Liu,Xiangfeng Liu,Jiahui Lv,Qiao Ma,Jiang Pan,Cong Qin,Chenxing Sun,Wen Sun,Zhonghui Wang,Abudukelimu Wuerkaixi,Xin Yang,Fangyi Yuan,Yawen Zhu,Tianyi Zhai,Jie Zhang,Runlai Zhang,Yao Xu,Yiran Zhao,Yifan Wang,Xunliang Cai,Yangen Hu,Cao Liu,Lu Pan,Xiaoli Wang,Bo Xiao,Wenyuan Yao,Qianlin Zhou,Benchang Zhu*

Main category: cs.CL

TL;DR: 文章指出智能交互系统面临诸多挑战，介绍了针对工业应用的智能交互系统WOWService，它集成LLMs和多智能体架构，部署在美团App上取得关键指标显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决智能交互系统在冷启动数据构建、多轮对话性能、业务规则演变、单LLM依赖和评估优化等方面的挑战，以提升客户体验，促进业务成功。

Method: 引入WOWService，集成LLMs和多智能体架构，聚焦数据构建、能力提升、场景适应、多智能体协调和自动评估等核心模块。

Result: WOWService部署在美团App上，关键指标如USM 1降低27.53%，USM 2提升25.51%。

Conclusion: WOWService能有效捕捉用户需求，推动个性化服务，在工业应用中具有有效性。

Abstract: Enhancing customer experience is essential for business success, particularly
as service demands grow in scale and complexity. Generative artificial
intelligence and Large Language Models (LLMs) have empowered intelligent
interaction systems to deliver efficient, personalized, and 24/7 support. In
practice, intelligent interaction systems encounter several challenges: (1)
Constructing high-quality data for cold-start training is difficult, hindering
self-evolution and raising labor costs. (2) Multi-turn dialogue performance
remains suboptimal due to inadequate intent understanding, rule compliance, and
solution extraction. (3) Frequent evolution of business rules affects system
operability and transferability, constraining low-cost expansion and
adaptability. (4) Reliance on a single LLM is insufficient in complex
scenarios, where the absence of multi-agent frameworks and effective
collaboration undermines process completeness and service quality. (5) The
open-domain nature of multi-turn dialogues, lacking unified golden answers,
hampers quantitative evaluation and continuous optimization. To address these
challenges, we introduce WOWService, an intelligent interaction system tailored
for industrial applications. With the integration of LLMs and multi-agent
architectures, WOWService enables autonomous task management and collaborative
problem-solving. Specifically, WOWService focuses on core modules including
data construction, general capability enhancement, business scenario
adaptation, multi-agent coordination, and automated evaluation. Currently,
WOWService is deployed on the Meituan App, achieving significant gains in key
metrics, e.g., User Satisfaction Metric 1 (USM 1) -27.53% and User Satisfaction
Metric 2 (USM 2) +25.51%, demonstrating its effectiveness in capturing user
needs and advancing personalized service.

</details>


### [260] [LLM one-shot style transfer for Authorship Attribution and Verification](https://arxiv.org/abs/2510.13302)
*Pablo Miralles-González,Javier Huertas-Tato,Alejandro Martín,David Camacho*

Main category: cs.CL

TL;DR: 提出基于大语言模型预训练和上下文学习能力的无监督方法用于计算文体学，表现优于其他方法且可灵活权衡计算成本和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有监督和对比方法依赖有虚假关联的数据，易混淆风格和主题，且大语言模型的预训练在一般作者身份问题中未充分利用。

Method: 提出基于大语言模型广泛预训练和上下文学习能力的无监督方法，用大语言模型的对数概率衡量文本间风格可迁移性。

Result: 显著优于同等规模的大语言模型提示方法，在控制主题相关性时准确率高于对比训练的基线模型，性能随基础模型大小和额外机制提升。

Conclusion: 该方法可行，能在计算成本和准确性间实现灵活权衡。

Abstract: Computational stylometry analyzes writing style through quantitative patterns
in text, supporting applications from forensic tasks such as identity linking
and plagiarism detection to literary attribution in the humanities. Supervised
and contrastive approaches rely on data with spurious correlations and often
confuse style with topic. Despite their natural use in AI-generated text
detection, the CLM pre-training of modern LLMs has been scarcely leveraged for
general authorship problems. We propose a novel unsupervised approach based on
this extensive pre-training and the in-context learning capabilities of LLMs,
employing the log-probabilities of an LLM to measure style transferability from
one text to another. Our method significantly outperforms LLM prompting
approaches of comparable scale and achieves higher accuracy than contrastively
trained baselines when controlling for topical correlations. Moreover,
performance scales fairly consistently with the size of the base model and, in
the case of authorship verification, with an additional mechanism that
increases test-time computation; enabling flexible trade-offs between
computational cost and accuracy.

</details>


### [261] [Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems](https://arxiv.org/abs/2510.13351)
*Karthik Avinash,Nikhil Pareek,Rishav Hada*

Main category: cs.CL

TL;DR: 针对大语言模型部署需求，提出多模态防护模型Protect，实验表现超现有模型，为安全系统奠基。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键领域部署，现有防护系统在实时监督、多模态数据处理和可解释性方面有局限，不适用于监管环境。

Method: 引入多模态防护模型Protect，集成微调的特定类别适配器，通过低秩自适应在多模态数据集上训练，使用教师辅助注释管道生成高保真、上下文感知标签。

Result: 实验显示Protect在各安全维度上达到了最先进性能，超越现有模型。

Conclusion: Protect为跨文本、图像和音频模态的可信、可审计且可用于生产的安全系统奠定了坚实基础。

Abstract: The increasing deployment of Large Language Models (LLMs) across enterprise
and mission-critical domains has underscored the urgent need for robust
guardrailing systems that ensure safety, reliability, and compliance. Existing
solutions often struggle with real-time oversight, multi-modal data handling,
and explainability -- limitations that hinder their adoption in regulated
environments. Existing guardrails largely operate in isolation, focused on text
alone making them inadequate for multi-modal, production-scale environments. We
introduce Protect, natively multi-modal guardrailing model designed to operate
seamlessly across text, image, and audio inputs, designed for enterprise-grade
deployment. Protect integrates fine-tuned, category-specific adapters trained
via Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering
four safety dimensions: toxicity, sexism, data privacy, and prompt injection.
Our teacher-assisted annotation pipeline leverages reasoning and explanation
traces to generate high-fidelity, context-aware labels across modalities.
Experimental results demonstrate state-of-the-art performance across all safety
dimensions, surpassing existing open and proprietary models such as WildGuard,
LlamaGuard-4, and GPT-4.1. Protect establishes a strong foundation for
trustworthy, auditable, and production-ready safety systems capable of
operating across text, image, and audio modalities.

</details>


### [262] [Personal Attribute Leakage in Federated Speech Models](https://arxiv.org/abs/2510.13357)
*Hamdan Al-Ali,Ali Reza Ghavamipour,Tommaso Caselli,Fatih Turkmen,Zeerak Talat,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 本文分析联邦设置下ASR模型对属性推理攻击的脆弱性，测试非参数白盒攻击方法，发现预训练数据中代表性不足或缺失的属性更易受攻击，口音信息可可靠推断，揭示新的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 分析联邦设置下ASR模型对属性推理攻击的脆弱性，以提升模型安全性。

Method: 在被动威胁模型下，对Wav2Vec2、HuBERT和Whisper三个ASR模型测试非参数白盒攻击方法，仅基于权重差异进行攻击。

Result: 攻击在性别、年龄、口音、情绪和构音障碍等敏感属性上可行，预训练数据中代表性不足或缺失的属性更易受攻击，口音信息可从所有模型可靠推断。

Conclusion: 暴露了联邦ASR模型之前未被记录的漏洞，为提高安全性提供了见解。

Abstract: Federated learning is a common method for privacy-preserving training of
machine learning models. In this paper, we analyze the vulnerability of ASR
models to attribute inference attacks in the federated setting. We test a
non-parametric white-box attack method under a passive threat model on three
ASR models: Wav2Vec2, HuBERT, and Whisper. The attack operates solely on weight
differentials without access to raw speech from target speakers. We demonstrate
attack feasibility on sensitive demographic and clinical attributes: gender,
age, accent, emotion, and dysarthria. Our findings indicate that attributes
that are underrepresented or absent in the pre-training data are more
vulnerable to such inference attacks. In particular, information about accents
can be reliably inferred from all models. Our findings expose previously
undocumented vulnerabilities in federated ASR models and offer insights towards
improved security.

</details>


### [263] [Document Intelligence in the Era of Large Language Models: A Survey](https://arxiv.org/abs/2510.13366)
*Weishi Wang,Hengchang Hu,Zhijie Zhang,Zhaochen Li,Hongxin Shao,Daniel Dahlmeier*

Main category: cs.CL

TL;DR: 本文综述Document AI（DAI）发展，介绍大语言模型（LLMs）在DAI领域现状、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: DAI因LLMs出现而显著变革，旨在对DAI最新发展进行结构化分析，为学术和实践应用提供参考。

Method: 对DAI的演变进行全面概述，探讨多模态、多语言和检索增强DAI的关键进展与挑战。

Result: 梳理了LLMs在DAI领域的当前研究尝试和未来前景。

Conclusion: 提出未来研究方向，如基于代理的方法和特定文档的基础模型。

Abstract: Document AI (DAI) has emerged as a vital application area, and is
significantly transformed by the advent of large language models (LLMs). While
earlier approaches relied on encoder-decoder architectures, decoder-only LLMs
have revolutionized DAI, bringing remarkable advancements in understanding and
generation. This survey provides a comprehensive overview of DAI's evolution,
highlighting current research attempts and future prospects of LLMs in this
field. We explore key advancements and challenges in multimodal, multilingual,
and retrieval-augmented DAI, while also suggesting future research directions,
including agent-based approaches and document-specific foundation models. This
paper aims to provide a structured analysis of the state-of-the-art in DAI and
its implications for both academic and practical applications.

</details>


### [264] [LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA](https://arxiv.org/abs/2510.13494)
*Tommaso Bonomo,Luca Gioffré,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文介绍针对叙事文本问答的高质量子集LiteraryQA，进行自动指标元评估，用其对长上下文大模型基准测试并开源代码数据。


<details>
  <summary>Details</summary>
Motivation: 当前叙事文本问答系统面临挑战，广泛使用的NarrativeQA基准存在噪声文档和有缺陷的问答对，可靠性受影响。

Method: 采用人工和大模型验证的流程识别和纠正低质量问答样本，去除源文档中的无关文本；对自动指标进行元评估。

Result: 所有基于n-gram的指标与人类判断的系统级相关性较低，即使是小型开放权重模型的大模型评判评估也能与人类确定的排名高度一致。

Conclusion: 推出LiteraryQA，明确评估方式，为长上下文大模型提供基准测试。

Abstract: Question Answering (QA) on narrative text poses a unique challenge to current
systems, requiring a deep understanding of long, complex documents. However,
the reliability of NarrativeQA, the most widely used benchmark in this domain,
is hindered by noisy documents and flawed QA pairs. In this work, we introduce
LiteraryQA, a high-quality subset of NarrativeQA focused on literary works.
Using a human- and LLM-validated pipeline, we identify and correct low-quality
QA samples while removing extraneous text from source documents. We then carry
out a meta-evaluation of automatic metrics to clarify how systems should be
evaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics
have a low system-level correlation to human judgment, while LLM-as-a-Judge
evaluations, even with small open-weight models, can strongly agree with the
ranking identified by humans. Finally, we benchmark a set of long-context LLMs
on LiteraryQA. We release our code and data at
https://github.com/SapienzaNLP/LiteraryQA.

</details>


### [265] [ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding](https://arxiv.org/abs/2510.13499)
*Xiaozhe Li,TianYi Lyu,Siyi Yang,Yuxi Gong,Yizhao Yang,Jinxuan Huang,Ligao Zhang,Zhuoyi Huang,Qingwen Liu*

Main category: cs.CL

TL;DR: 现有大语言模型缺乏评估理解人类意图能力的大规模基准，本文引入首个动态实时评估基准 ench 填补空白。


<details>
  <summary>Details</summary>
Motivation: 大语言模型理解人类意图很重要，但缺乏评估其在现实世界理解人类意图能力的大规模基准，需填补此空白。

Method: 引入动态实时评估基准 ench，有自动化管理流程防止数据污染。

Result: 推出了首个专门用于意图理解的动态实时评估基准 ench，规模大且多样。

Conclusion: 所提出的 ench 可用于评估大语言模型在现实世界理解人类意图的能力。

Abstract: Understanding human intent is a complex, high-level task for large language
models (LLMs), requiring analytical reasoning, contextual interpretation,
dynamic information aggregation, and decision-making under uncertainty.
Real-world public discussions, such as consumer product discussions, are rarely
linear or involve a single user. Instead, they are characterized by interwoven
and often conflicting perspectives, divergent concerns, goals, emotional
tendencies, as well as implicit assumptions and background knowledge about
usage scenarios. To accurately understand such explicit public intent, an LLM
must go beyond parsing individual sentences; it must integrate multi-source
signals, reason over inconsistencies, and adapt to evolving discourse, similar
to how experts in fields like politics, economics, or finance approach complex,
uncertain environments. Despite the importance of this capability, no
large-scale benchmark currently exists for evaluating LLMs on real-world human
intent understanding, primarily due to the challenges of collecting real-world
public discussion data and constructing a robust evaluation pipeline. To bridge
this gap, we introduce \bench, the first dynamic, live evaluation benchmark
specifically designed for intent understanding, particularly in the consumer
domain. \bench is the largest and most diverse benchmark of its kind,
supporting real-time updates while preventing data contamination through an
automated curation pipeline.

</details>


### [266] [MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts](https://arxiv.org/abs/2510.13500)
*Shujun Xia,Haokun Lin,Yichen Wu,Yinan Zhou,Zixuan Li,Zhongwei Wan,Xingrun Xing,Yefeng Zheng,Xiang Li,Caifeng Shan,Zhenan Sun,Quanzheng Li*

Main category: cs.CL

TL;DR: 大语言模型在医疗应用中有局限，基于检索的编辑是潜在解决方案但面临挑战。本文构建MedVersa基准并提出MedREK框架，实验显示其性能优越且提供医疗大模型批量编辑验证方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗应用中因医学知识更新快和训练数据错误，生成信息不准确，基于参数的编辑不适用于医疗领域，基于检索的编辑面临表示重叠和缺乏批量编辑方法的问题。

Method: 构建MedREK，一个基于检索的编辑框架，集成共享查询-键模块用于精确匹配和基于注意力的提示编码器用于信息引导；构建MedVersa基准评估单样本和批量编辑。

Result: 在各种医疗基准测试中，MedREK在不同核心指标上表现出色，提供了医疗大模型批量编辑的首个验证解决方案。

Conclusion: MedREK框架有效解决了基于检索的编辑在医疗大语言模型应用中的问题，可用于医疗大模型的编辑。

Abstract: LLMs hold great promise for healthcare applications, but the rapid evolution
of medical knowledge and errors in training data often cause them to generate
outdated or inaccurate information, limiting their applicability in high-stakes
clinical practice. Model editing has emerged as a potential remedy without full
retraining. While parameter-based editing often compromises locality and is
thus ill-suited for the medical domain, retrieval-based editing offers a more
viable alternative. However, it still faces two critical challenges: (1)
representation overlap within the medical knowledge space often causes
inaccurate retrieval and reduces editing accuracy; (2) existing methods are
restricted to single-sample edits, while batch-editing remains largely
unexplored despite its importance for real-world medical applications. To
address these challenges, we first construct MedVersa, \hk{an enhanced
benchmark with broader coverage of medical subjects, designed to evaluate both
single and batch edits under strict locality constraints}. We then propose
MedREK, a retrieval-based editing framework that integrates a shared query-key
module for precise matching with an attention-based prompt encoder for
informative guidance. Experimental results on various medical benchmarks
demonstrate that our MedREK achieves superior performance across different core
metrics and provides the first validated solution for batch-editing in medical
LLMs. Our code and dataset are available at
https://github.com/mylittleriver/MedREK.

</details>


### [267] [Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering](https://arxiv.org/abs/2510.12925)
*Nil-Jana Akpinar,Chia-Jung Lee,Vanessa Murdock,Pietro Perona*

Main category: cs.CL

TL;DR: 本文首次系统评估大语言模型对询问角色的鲁棒性，发现用户披露的询问角色线索会影响问答准确性并引发故障模式。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型能不受用户背景影响，基于客观知识如实回答事实性问题，评估其对询问角色的鲁棒性。

Method: 评估用户在真实交互中披露的合理、以人为中心的询问角色线索。

Result: 这些线索会显著改变问答准确性，触发拒绝回答、幻觉限制和角色混淆等故障模式。

Conclusion: 模型对用户表达方式的敏感性会损害事实可靠性，询问角色测试是有效的鲁棒性评估工具。

Abstract: Large Language Models (LLMs) should answer factual questions truthfully,
grounded in objective knowledge, regardless of user context such as
self-disclosed personal information, or system personalization. In this paper,
we present the first systematic evaluation of LLM robustness to inquiry
personas, i.e. user profiles that convey attributes like identity, expertise,
or belief. While prior work has primarily focused on adversarial inputs or
distractors for robustness testing, we evaluate plausible, human-centered
inquiry persona cues that users disclose in real-world interactions. We find
that such cues can meaningfully alter QA accuracy and trigger failure modes
such as refusals, hallucinated limitations, and role confusion. These effects
highlight how model sensitivity to user framing can compromise factual
reliability, and position inquiry persona testing as an effective tool for
robustness evaluation.

</details>


### [268] [Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs](https://arxiv.org/abs/2510.13586)
*Pasin Buakhaw,Kun Kerdthaisong,Phuree Phenhiran,Pitikorn Khlaisamniang,Supasate Vorathammathorn,Piyalitt Ittichaiwong,Nutchanon Yongsatianchot*

Main category: cs.CL

TL;DR: 本文介绍参与2025年CPDC第二轮竞赛情况，结合两种策略参赛并取得较好排名。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型为游戏环境创建动态非玩家角色的机会，参与CPDC竞赛评估代理。

Method: 结合两种策略，API赛道采用轻量级提示技术（如Deflanderization提示法），GPU赛道采用微调大模型（Qwen3 - 14B结合SFT和LoRA）。

Result: 最佳提交结果在任务1排第2，任务3（API赛道）排第2，任务3（GPU赛道）排第4。

Conclusion: 未明确提及，但从结果看所采用策略有一定有效性。

Abstract: The emergence of large language models (LLMs) has opened new opportunities
for cre- ating dynamic non-player characters (NPCs) in gaming environments,
enabling both func- tional task execution and persona-consistent dialogue
generation. In this paper, we (Tu_Character_lab) report our participation in
the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which
eval- uates agents across three tracks: task-oriented dialogue, context-aware
dialogue, and their integration. Our approach combines two complementary
strategies: (i) lightweight prompting techniques in the API track, including a
Deflanderization prompting method to suppress excessive role-play and improve
task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging
Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our
best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on
Task 3 (GPU track).

</details>


### [269] [NOSA: Native and Offloadable Sparse Attention](https://arxiv.org/abs/2510.13602)
*Yuxiang Huang,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出支持KV缓存卸载的可训练稀疏注意力框架NOSA，提升解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有可训练稀疏注意力方法未减少KV缓存大小，限制GPU批量大小和解码吞吐量，需解决该问题。

Method: 提出NOSA框架，将token选择分解为查询感知和查询无关组件以引入显式局部性约束，减少KV传输。

Result: 预训练10亿参数模型，与基线相比，在保留近乎无损性能的同时，解码吞吐量最高提升2.3倍。

Conclusion: NOSA框架有效解决了现有方法的局限，在保证性能的同时提升了解码效率。

Abstract: Trainable sparse attention has emerged as a promising solution to address the
decoding efficiency bottleneck of LLMs in long-context processing,
significantly saving memory accesses while minimally impacting task
performance. However, existing sparse attention methods leave a crucial
limitation unresolved: the size of the key-value (KV) cache remains unreduced,
which constrains on-GPU batch sizes and throttles decoding throughput,
especially in large-scale batched inference. In this paper, we show that
trainable sparse attention naturally exhibits strong locality in token
selection across adjacent decoding steps, thereby enabling KV cache offloading
without altering the underlying attention computation. However, the inherent
locality remains insufficient to achieve efficient offloading, as the transfer
of selected KV pairs between the CPU and GPU continues to dominate the overall
decoding cost. Building on this insight, we present NOSA, a trainable sparse
attention framework designed to natively support KV cache offloading. NOSA
introduces explicit locality constraints by decomposing token selection into
query-aware and query-agnostic components, thereby reducing KV transfers while
preserving the same attention computation as used during training. We pretrain
a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that
it preserves near-lossless performance while achieving up to a 2.3x improvement
in decoding throughput compared with the vanilla trainable sparse attention
baseline (InfLLM-V2).

</details>


### [270] [GatePro: Parameter-Free Expert Selection Optimization for Mixture-of-Experts Models](https://arxiv.org/abs/2510.13079)
*Chen Zheng,Yuhang Cai,Deyi Liu,Jin Ma,Yiyuan Ma,Yuan Yang,Jing Liu,Yutao Zeng,Xun Zhou,Siyuan Qiao*

Main category: cs.CL

TL;DR: 现代大语言模型使用MoE架构时存在专家选择冗余问题，本文提出无参数方法GatePro促进专家选择多样性，评估显示其有效。


<details>
  <summary>Details</summary>
Motivation: 解决MoE架构中专家选择功能相似导致的冗余计算和有效模型容量受限问题，现有方法无法解决专家多样性问题。

Method: 引入无参数方法GatePro，识别最相似专家对并引入局部竞争机制。

Result: 综合评估表明GatePro在不同模型规模和基准测试中有效，能增强专家多样性，避免功能冗余。

Conclusion: GatePro可在任何训练阶段热插拔部署，无需额外可学习参数，是提高MoE有效性的实用解决方案。

Abstract: Modern large language models leverage Mixture-of-Experts (MoE) architectures
for efficient scaling, but face a critical challenge: functionally similar
experts are often selected simultaneously, creating redundant computation and
limiting effective model capacity. Existing auxiliary balance loss methods
improve token distribution but fail to address the underlying expert diversity
problem. We introduce GatePro, a novel parameter-free method that directly
promotes expert selection diversity. GatePro identifies the most similar expert
pairs and introduces localized competition mechanisms, preventing redundant
expert co-activation while maintaining natural expert specialization. Our
comprehensive evaluation demonstrates GatePro's effectiveness across model
scales and benchmarks. Analysis demonstrates GatePro's ability to achieve
enhanced expert diversity, where experts develop more distinct and
complementary capabilities, avoiding functional redundancy. This approach can
be deployed hot-swappable during any training phase without additional
learnable parameters, offering a practical solution for improving MoE
effectiveness.

</details>


### [271] [Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses](https://arxiv.org/abs/2510.13624)
*Stefan Lenz,Lakisha Ortiz Rosario,Georg Vollmar,Arsenij Ustjanzew,Fatma Alickovic,Thomas Kindler,Torsten Panholzer*

Main category: cs.CL

TL;DR: 研究用公共数据集对开放权重LLM进行指令微调，提升德国肿瘤诊断文本编码准确性，展示效果并提供训练数据和模型检查点。


<details>
  <summary>Details</summary>
Motivation: 准确编码肿瘤诊断对德国癌症文档结构化很重要，小的开放权重LLM在德语语境编码准确性不足，研究指令微调能否提升其编码准确性。

Method: 用本地肿瘤文档系统编码诊断作测试数据，基于相关目录创建超50万个问答对作训练数据，对8个开放权重模型微调。

Result: ICD - 10 - GM准确性从1.4 - 24%升至41 - 58%，部分准确性从31 - 74%升至73 - 83%；ICD - O - 3地形编码准确性也提升；畸形代码输出降为0%；肿瘤诊断识别达99%；准确性与模型大小正相关，微调后大小模型差距缩小；Qwen3推理模式表现差且慢。

Conclusion: 利用公共目录构建指令数据集可提升LLM在医疗文档任务中的表现，提供了训练数据集和最佳模型检查点。

Abstract: Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential
for structured cancer documentation in Germany. Smaller open-weight LLMs are
appealing for privacy-preserving automation but often struggle with coding
accuracy in German-language contexts. This study investigates whether
instruction-based fine-tuning on public datasets improves the coding accuracy
of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded
diagnoses from the local tumor documentation system as test data. In a
systematic data quality assessment, the upper limit for ICD-10 coding
performance was estimated at 60-79% for exact and 81-94% for partial
(three-character codes only) derivation. As training data, over 500,000
question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS
catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families
(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to
41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3
topography coding also improved but started and remained considerably lower
with an exact accuracy of 22-40% and a partial accuracy of 56-67% after
fine-tuning. Malformed code outputs dropped to 0% for all models.
Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with
model size, but gaps between small and large models narrowed after fine-tuning.
The reasoning mode in Qwen3 generally yielded a lower performance than
fine-tuning and was over 100 times slower. Our findings highlight the potential
of leveraging public catalogues to build instruction datasets that improve LLMs
in medical documentation tasks. The complete training dataset and the
best-performing checkpoints of the fine-tuned models are available from
https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.

</details>


### [272] [Closing the Gap Between Text and Speech Understanding in LLMs](https://arxiv.org/abs/2510.13632)
*Santiago Cuervo,Skyler Seto,Maureen de Seyssel,Richard He Bai,Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly,Zakaria Aldeneh*

Main category: cs.CL

TL;DR: 现有语音适配大语言模型在语言理解任务表现不佳，存在文本 - 语音理解差距，本文提出SALAD方法，用较少语音数据取得有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 现有缩小文本 - 语音理解差距的方法成本高、依赖合成数据或不可复现，需更高效的替代方案。

Method: 分析文本 - 语音理解差距由适配时文本能力遗忘和跨模态不对齐导致，提出SALAD方法，结合跨模态蒸馏和目标合成数据。

Result: 将SALAD应用于3B和7B大语言模型，在广泛领域基准测试中，用比公开语料少一个数量级的语音数据取得有竞争力的表现。

Conclusion: SALAD是一种更数据高效的缩小文本 - 语音理解差距的方法。

Abstract: Large Language Models (LLMs) can be adapted to extend their text capabilities
to speech inputs. However, these speech-adapted LLMs consistently underperform
their text-based counterparts--and even cascaded pipelines--on language
understanding tasks. We term this shortfall the text-speech understanding gap:
the performance drop observed when a speech-adapted LLM processes spoken inputs
relative to when the original text-based LLM processes the equivalent text.
Recent approaches to narrowing this gap either rely on large-scale speech
synthesis of text corpora, which is costly and heavily dependent on synthetic
data, or on large-scale proprietary speech datasets, which are not
reproducible. As a result, there remains a need for more data-efficient
alternatives for closing the text-speech understanding gap. In this work, we
analyze the gap as driven by two factors: (i) forgetting of text capabilities
during adaptation, and (ii) cross-modal misalignment between speech and text.
Based on this analysis, we introduce SALAD--Sample-efficient Alignment with
Learning through Active selection and cross-modal Distillation--which combines
cross-modal distillation with targeted synthetic data to improve alignment
while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves
competitive performance with a strong open-weight model across broad-domain
benchmarks in knowledge, language understanding, and reasoning, while training
on over an order of magnitude less speech data from public corpora.

</details>


### [273] [NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching](https://arxiv.org/abs/2510.13721)
*Run Luo,Xiaobo Xia,Lu Wang,Longze Chen,Renke Shan,Jing Luo,Min Yang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 提出开源全模态基础模型NExT - OMNI，在多模态基准测试表现好，开源相关资料促进研究。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型受自回归架构限制，混合和去耦策略设计冗余、适用场景有限，需新模型实现统一建模。

Method: 引入NExT - OMNI，通过离散流范式实现统一建模，利用度量诱导概率路径和动力学最优速度。

Result: 在多模态生成和理解基准测试有竞争力，在多轮多模态交互和跨模态检索中超越先前统一模型。

Conclusion: NExT - OMNI作为下一代多模态基础模型有架构优势，开源资料可促进后续研究。

Abstract: Next-generation multimodal foundation models capable of any-to-any
cross-modal generation and multi-turn interaction will serve as core components
of artificial general intelligence systems, playing a pivotal role in
human-machine interaction. However, most existing multimodal models remain
constrained by autoregressive architectures, whose inherent limitations prevent
a balanced integration of understanding and generation capabilities. Although
hybrid and decoupling strategies have been explored to address these tasks
within unified frameworks separately, their redundant, non-integrated designs
limit their applicability to broader scenarios, such as cross-modal
retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal
foundation model that achieves unified modeling through discrete flow
paradigms. By leveraging metric-induced probability paths and kinetic optimal
velocities, NExT-OMNI natively supports any-to-any understanding and generation
with enhanced response efficiency, while enabling broader application scenarios
through concise unified representations rather than task-decoupled designs.
Trained on large-scale interleaved text, image, video, and audio data,
NExT-OMNI delivers competitive performance on multimodal generation and
understanding benchmarks, while outperforming prior unified models in
multi-turn multimodal interaction and cross-modal retrieval, highlighting its
architectural advantages as a next-generation multimodal foundation model. To
advance further research, we release training details, data protocols, and
open-source both the code and model checkpoints.

</details>


### [274] [Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization](https://arxiv.org/abs/2510.13554)
*Yang Li,Zhichen Dong,Yuhan Sun,Weixun Wang,Shaopan Xiong,Yijia Luo,Jiashun Liu,Han Lu,Jiamang Wang,Wenbo Su,Bo Zheng,Junchi Yan*

Main category: cs.CL

TL;DR: 本文通过分析大语言模型注意力机制揭示推理机制，提出新强化学习策略提升推理任务表现，让优化更透明有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理模式不透明，强化学习统一分配奖励模糊关键和常规步骤区别，需使模型内部逻辑更清晰。

Method: 区分局部和全局注意力头，用窗口平均注意力距离和未来注意力影响两个指标量化，揭示预规划和锚定机制，提出三种新强化学习策略。

Result: 新强化学习策略在各种推理任务中表现出一致的性能提升。

Conclusion: 使优化与模型内在推理节奏对齐，将不透明优化转变为可操作的结构感知过程，为大语言模型推理的优化提供更透明有效的方向。

Abstract: The reasoning pattern of Large language models (LLMs) remains opaque, and
Reinforcement learning (RL) typically applies uniform credit across an entire
generation, blurring the distinction between pivotal and routine steps. This
work positions attention as a privileged substrate that renders the internal
logic of LLMs legible, not merely as a byproduct of computation, but as a
mechanistic blueprint of reasoning itself. We first distinguish attention heads
between locally and globally focused information processing and reveal that
locally focused heads produce a sawtooth pattern near the diagonal indicating
phrasal chunks, while globally focused heads expose tokens that exert broad
downstream influence over future tokens. We formalize these with two metrics:
1) Windowed Average Attention Distance, which measures the extent of backward
attention within a clipped window; 2) Future Attention Influence, which
quantifies a token's global importance as the average attention it receives
from subsequent tokens. Taken together, these signals reveal a recurring
preplan-and-anchor mechanism, where the model first performs a long-range
contextual reference to generate an introductory token, which is immediately
followed by or coincides with a semantic anchor token that organizes subsequent
reasoning. Leveraging these insights, we introduce three novel RL strategies
that dynamically perform targeted credit assignment to critical nodes (preplan
tokens, anchor tokens, and their temporal coupling) and show consistent
performance gains across various reasoning tasks. By aligning optimization with
the model's intrinsic reasoning rhythm, we aim to transform opaque optimization
into an actionable structure-aware process, hoping to offer a potential step
toward more transparent and effective optimization of LLM reasoning.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [275] [Data-driven learning of feedback maps for explicit robust predictive control: an approximation theoretic view](https://arxiv.org/abs/2510.13522)
*Siddhartha Ganguly,Shubham Gupta,Debasish Chatterjee*

Main category: math.OC

TL;DR: 本文为一类鲁棒模型预测控制问题建立从数据中学习反馈映射的算法，通过数据生成和学习近似反馈映射两步实现，并给出数值示例。


<details>
  <summary>Details</summary>
Motivation: 为鲁棒模型预测控制问题建立从数据中学习反馈映射的算法，且在综合阶段直接考虑学习的近似误差，保证递归可行性。

Method: 分两步：一是将原极小极大问题转化为凸半无限规划并在状态空间网格点精确求解以生成数据；二是采用近似方案在允许状态空间内以预设统一误差界限学习未知反馈策略。

Result: 建立了学习反馈映射的算法，保证近似反馈策略下闭环系统的稳定性，给出两个基准数值示例。

Conclusion: 提出的算法可有效解决鲁棒模型预测控制问题中从数据学习反馈映射的问题。

Abstract: We establish an algorithm to learn feedback maps from data for a class of
robust model predictive control (MPC) problems. The algorithm accounts for the
approximation errors due to the learning directly at the synthesis stage,
ensuring recursive feasibility by construction. The optimal control problem
consists of a linear noisy dynamical system, a quadratic stage and quadratic
terminal costs as the objective, and convex constraints on the state, control,
and disturbance sequences; the control minimizes and the disturbance maximizes
the objective. We proceed via two steps -- (a) Data generation: First, we
reformulate the given minmax problem into a convex semi-infinite program and
employ recently developed tools to solve it in an exact fashion on grid points
of the state space to generate (state, action) data. (b) Learning approximate
feedback maps: We employ a couple of approximation schemes that furnish tight
approximations within preassigned uniform error bounds on the admissible state
space to learn the unknown feedback policy. The stability of the closed-loop
system under the approximate feedback policies is also guaranteed under a
standard set of hypotheses. Two benchmark numerical examples are provided to
illustrate the results.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [276] [Automated Network Protocol Testing with LLM Agents](https://arxiv.org/abs/2510.13248)
*Yunze Wei,Kaiwen Wei,Shibo Du,Jianyu Wang,Zhangzhong Liu,Yawen Wang,Zhanyou Li,Congcong Miao,Xiaohui Xie,Yong Cui*

Main category: cs.NI

TL;DR: 本文提出NeTestLLM系统，利用多智能体大语言模型实现端到端自动化网络协议测试，在生产环境部署获积极反馈，实验显示其能生成大量测试用例、覆盖更多历史漏洞且提升测试效率。


<details>
  <summary>Details</summary>
Motivation: 传统网络协议测试方法劳动密集、易出错，现有基于模型的方法需大量手动建模和专家干预，成本高且适应性有限。

Method: NeTestLLM采用分层协议理解、迭代测试用例生成、特定任务工作流生成可执行工件以及运行时反馈分析进行调试和优化。

Result: 在生产环境部署数月获积极反馈；为OSPF、RIP和BGP生成4632个测试用例，覆盖41个历史FRRouting漏洞（现行国家标准仅覆盖11个）；生成可执行工件的过程使测试效率比手动方法提高8.65倍。

Conclusion: NeTestLLM为异构网络协议的端到端自动化测试提供了首个实用的大语言模型驱动解决方案。

Abstract: Network protocol testing is fundamental for modern network infrastructure.
However, traditional network protocol testing methods are labor-intensive and
error-prone, requiring manual interpretation of specifications, test case
design, and translation into executable artifacts, typically demanding one
person-day of effort per test case. Existing model-based approaches provide
partial automation but still involve substantial manual modeling and expert
intervention, leading to high costs and limited adaptability to diverse and
evolving protocols. In this paper, we propose a first-of-its-kind system called
NeTestLLM that takes advantage of multi-agent Large Language Models (LLMs) for
end-to-end automated network protocol testing. NeTestLLM employs hierarchical
protocol understanding to capture complex specifications, iterative test case
generation to improve coverage, a task-specific workflow for executable
artifact generation, and runtime feedback analysis for debugging and
refinement. NeTestLLM has been deployed in a production environment for several
months, receiving positive feedback from domain experts. In experiments,
NeTestLLM generated 4,632 test cases for OSPF, RIP, and BGP, covering 41
historical FRRouting bugs compared to 11 by current national standards. The
process of generating executable artifacts also improves testing efficiency by
a factor of 8.65x compared to manual methods. NeTestLLM provides the first
practical LLM-powered solution for automated end-to-end testing of
heterogeneous network protocols.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [277] [VSS Challenge Problem: Verifying the Correctness of AllReduce Algorithms in the MPICH Implementation of MPI](https://arxiv.org/abs/2510.13413)
*Paul D. Hovland*

Main category: cs.LO

TL;DR: 基于MPICH实现的MPI提出验证挑战问题，创建三种算法独立版本并验证其中两种。


<details>
  <summary>Details</summary>
Motivation: 对MPICH实现的MPI中allreduce算法进行验证，确保其功能与reduce后接broadcast等效。

Method: 创建三种算法的独立版本，使用CIVL验证其中两种。

Result: 成功使用CIVL验证了三种算法中的两种。

Conclusion: 未提及明确结论，但暗示可通过CIVL对相关算法进行验证。

Abstract: We describe a challenge problem for verification based on the MPICH
implementation of MPI. The MPICH implementation includes several algorithms for
allreduce, all of which should be functionally equivalent to reduce followed by
broadcast. We created standalone versions of three algorithms and verified two
of them using CIVL.

</details>


### [278] [Verification Challenges in Sparse Matrix Vector Multiplication in High Performance Computing: Part I](https://arxiv.org/abs/2510.13427)
*Junchao Zhang*

Main category: cs.LO

TL;DR: 本文介绍SpMV的顺序和基本MPI并行实现，为科学软件验证社区提供挑战问题，实现基于PETSc库。


<details>
  <summary>Details</summary>
Motivation: 为科学软件验证社区提供挑战问题。

Method: 给出稀疏矩阵向量乘法（SpMV）的顺序和基本MPI并行实现，且在PETSc库上下文中描述实现。

Result: 文中未提及具体结果。

Conclusion: 文中未提及明确结论。

Abstract: Sparse matrix vector multiplication (SpMV) is a fundamental kernel in
scientific codes that rely on iterative solvers. In this first part of our
work, we present both a sequential and a basic MPI parallel implementations of
SpMV, aiming to provide a challenge problem for the scientific software
verification community. The implementations are described in the context of the
PETSc library.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [279] [D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition of Activations](https://arxiv.org/abs/2510.13147)
*Faraz Tahmasebi,Michael Pelluer,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: 本文针对大语言模型高计算和内存成本问题，提出D - com加速器，采用多种方法提升性能，能减少端到端延迟，有一定模型质量损失。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算和内存成本增加，传统模型分解方法存在运行时延迟高的问题，需探索更有效的模型压缩技术。

Method: 采用渐进分解算法、Lanczos算法，设计协同加速器架构；引入计算复制方法；开发输出形状保留计算方案；采用多轨道分解方法。

Result: 计算复制方法实现6.2倍加速；D - com加速器相比A100 GPU端到端延迟改善22%，模型质量有小幅度下降。

Conclusion: 通过合理选择分解算法和硬件支持，输入分解可带来显著益处，D - com加速器在减少延迟上有效果，但会造成一定模型质量损失。

Abstract: The computation and memory costs of large language models kept increasing
over last decade, which reached over the scale of 1T parameters. To address the
challenges from the large scale models, model compression techniques such as
low-rank decomposition have been explored. Previous model decomposition works
have focused on weight decomposition to avoid costly runtime decomposition,
whose latency often significantly exceeds the benefits from decomposition
(e.g., 38% more end-to-end latency when running Llama2-7b on A100 with 4K
sequence length with activation decomposition compared to no decomposition). In
this work, we debunk such observations and report that the input decomposition
can be significantly beneficial with a proper choice of decomposition algorithm
and hardware support. We adopt progressive decomposition algorithm, Lanczos
algorithm, and design a co-accelerator architecture for the decomposition
algorithm. To address the memory- boundness of the decomposition operation, we
introduce a novel compute replication methodology that moves the op- eration
toward compute-bound region, which enables 6.2x speedup in our evaluation. We
also develop an output shape- preserving computation scheme that eliminates
decomposi- tion costs in consecutive layers. To compensate model quality loss
from compression, we introduce a multi-track decom- position approach that
separately handles outlier channels for high accuracy and low perplexity with
minimal compu- tational costs. Combined together, our accelerator, D-com,
provides 22% end-to-end latency improvements compared to A100 GPU at the cost
of small model quality degradation (e.g., 3% on AI2 Reasoning Challenge task).

</details>


### [280] [F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs](https://arxiv.org/abs/2510.13401)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 提出F - BFQ加速器，可动态切换两种BFP量化变体，在AMD Kria板上部署后，相比基于Arm NEON的CPU执行，平均减少推理时间1.4倍，达5.2 tokens每秒。


<details>
  <summary>Details</summary>
Motivation: 为高效加速BFP量化大语言模型各层，需专用加速器支持不同BFP变体且无需重新配置。

Method: 提出Flexible Block FloatingPoint Quantization (F - BFQ) 加速器，可动态切换两种BFP量化变体并执行矩阵乘法运算。

Result: F - BFQ加速器在AMD Kria板上部署后，在三个BFP量化大语言模型上，相比基于Arm NEON的CPU执行，平均减少推理时间1.4倍，达到5.2 tokens每秒（约3.9 words每秒）。

Conclusion: F - BFQ加速器能有效加速BFP量化大语言模型推理，减少推理时间。

Abstract: Large Language Models (LLMs) have become increasingly prominent for daily
tasks, from improving sound-totext translation to generating additional frames
for the latest video games. With the help of LLM inference frameworks, such as
llama.cpp, which support optimizations such as KV-caching and quantization, it
is now easier than ever to deploy LLMs on edge devices. Quantization is
fundamental to enable LLMs on resource-constrained edge devices, and llama.cpp
utilizes block floating point (BFP) quantization to drastically reduce the bit
width of weights and input tensors, the memory footprint, and the computational
power required to run LLMs. LLMs are typically quantized with mixed BFP
quantization across the model layers to reduce the loss of model accuracy due
to quantization. Therefore, to efficiently accelerate across the layers of
BFP-quantized LLMs, specialized accelerators need to support different BFP
variants without reconfiguration. To address this issue, we propose a Flexible
Block FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically
switch between two BFP quantization variants and perform matrix multiplication
(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD
Kria board, reduces inference time by 1.4x on average over the Arm NEON-based
CPU execution across three BFP quantized LLMs while achieving 5.2 tokens per
second (~3.9 words per second).

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [281] [Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation](https://arxiv.org/abs/2510.12827)
*Md. Nayeem,Md Shamse Tabrej,Kabbojit Jit Deb,Shaonti Goswami,Md. Azizul Hakim*

Main category: eess.AS

TL;DR: 本文全面概述现代自动语音识别（ASR），介绍架构从传统到端到端的演变、训练范式的转变，还涵盖生态系统组件，最后指出挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习发展，ASR发生深刻变革，需要对现代ASR进行全面综述。

Method: 系统回顾端到端范式、架构转变、训练范式发展，分析关键数据集、评估指标等生态系统组件。

Result: 梳理了ASR从传统到端到端架构的演变，训练范式从全监督到自监督的转变，以及大规模弱监督模型的影响。

Conclusion: 指出ASR存在的开放挑战并给出未来研究方向。

Abstract: Automatic Speech Recognition (ASR) has undergone a profound transformation
over the past decade, driven by advances in deep learning. This survey provides
a comprehensive overview of the modern era of ASR, charting its evolution from
traditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models
(GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant
end-to-end neural architectures. We systematically review the foundational
end-to-end paradigms: Connectionist Temporal Classification (CTC),
attention-based encoder-decoder models, and the Recurrent Neural Network
Transducer (RNN-T), which established the groundwork for fully integrated
speech-to-text systems. We then detail the subsequent architectural shift
towards Transformer and Conformer models, which leverage self-attention to
capture long-range dependencies with high computational efficiency. A central
theme of this survey is the parallel revolution in training paradigms. We
examine the progression from fully supervised learning, augmented by techniques
like SpecAugment, to the rise of self-supervised learning (SSL) with foundation
models such as wav2vec 2.0, which drastically reduce the reliance on
transcribed data. Furthermore, we analyze the impact of largescale, weakly
supervised models like Whisper, which achieve unprecedented robustness through
massive data diversity. The paper also covers essential ecosystem components,
including key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME),
standard evaluation metrics (e.g., Word Error Rate), and critical
considerations for real-world deployment, such as streaming inference,
on-device efficiency, and the ethical imperatives of fairness and robustness.
We conclude by outlining open challenges and future research directions.

</details>


### [282] [HyWA: Hypernetwork Weight Adapting Personalized Voice Activity Detection](https://arxiv.org/abs/2510.12947)
*Mahsa Ghazvini Nejad,Hamed Jafarzadeh Asl,Amin Edraki,Mohammadreza Sadeghi,Masoud Asgharian,Yuanhao Yu,Vahid Partovi Nia*

Main category: eess.AS

TL;DR: 提出HyWA - PVAD超网络权重自适应方法，不改变VAD架构实现说话人自适应，提升PVAD性能且简化部署。


<details>
  <summary>Details</summary>
Motivation: 现有PVAD方法需进行架构更改，希望找到不改变VAD架构实现说话人自适应的方法。

Method: 采用超网络修改标准VAD模型中部分选定层的权重，提出HyWA - PVAD方法。

Result: 与多个基线调节技术对比，PVAD性能持续提升，能增加平均精度均值。

Conclusion: 新方法改进了现有调节技术，既提升性能又简化部署。

Abstract: Personalized Voice Activity Detection (PVAD) systems activate only in
response to a specific target speaker by incorporating speaker embeddings from
enrollment utterances. Unlike existing methods that require architectural
changes, such as FiLM layers, our approach employs a hypernetwork to modify the
weights of a few selected layers within a standard voice activity detection
(VAD) model. This enables speaker conditioning without changing the VAD
architecture, allowing the same VAD model to adapt to different speakers by
updating only a small subset of the layers. We propose HyWA-PVAD, a
hypernetwork weight adaptation method, and evaluate it against multiple
baseline conditioning techniques. Our comparison shows consistent improvements
in PVAD performance. HyWA also offers practical advantages for deployment by
preserving the core VAD architecture. Our new approach improves the current
conditioning techniques in two ways: i) increases the mean average precision,
ii) simplifies deployment by reusing the same VAD architecture.

</details>


### [283] [Two Heads Are Better Than One: Audio-Visual Speech Error Correction with Dual Hypotheses](https://arxiv.org/abs/2510.13281)
*Sungnyun Kim,Kangwook Jang,Sungwoo Cho,Joon Son Chung,Hoirin Kim,Se-Young Yun*

Main category: eess.AS

TL;DR: 本文提出用于视听语音识别的生成式纠错框架DualHyp，结合RelPrompt机制，在LRS2基准上取得高纠错率并开源代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 在视听语音识别中进行更有效的生成式纠错，解决单流方法纠错效果不佳的问题。

Method: 提出DualHyp框架，让大语言模型组合ASR和VSR的N-best假设；引入RelPrompt机制，为模型提供模态相关提示，引导其动态切换关注重点。

Result: 在LRS2基准的各种损坏场景下，相比标准ASR基线，错误率提升达57.7%，远高于单流方法的10%。

Conclusion: DualHyp框架结合RelPrompt机制在视听语音识别的生成式纠错方面表现出色，开源代码和数据集利于后续研究。

Abstract: This paper introduces a new paradigm for generative error correction (GER)
framework in audio-visual speech recognition (AVSR) that reasons over
modality-specific evidences directly in the language space. Our framework,
DualHyp, empowers a large language model (LLM) to compose independent N-best
hypotheses from separate automatic speech recognition (ASR) and visual speech
recognition (VSR) models. To maximize the effectiveness of DualHyp, we further
introduce RelPrompt, a noise-aware guidance mechanism that provides
modality-grounded prompts to the LLM. RelPrompt offers the temporal reliability
of each modality stream, guiding the model to dynamically switch its focus
between ASR and VSR hypotheses for an accurate correction. Under various
corruption scenarios, our framework attains up to 57.7% error rate gain on the
LRS2 benchmark over standard ASR baseline, contrary to single-stream GER
approaches that achieve only 10% gain. To facilitate research within our
DualHyp framework, we release the code and the dataset comprising ASR and VSR
hypotheses at https://github.com/sungnyun/dualhyp.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [284] [InferA: A Smart Assistant for Cosmological Ensemble Data](https://arxiv.org/abs/2510.12920)
*Justin Z. Tam,Pascal Grosset,Divya Banesh,Nesar Ramachandra,Terece L. Turton,James Ahrens*

Main category: astro-ph.IM

TL;DR: 传统自动化工具分析大规模科学数据集有局限，提出多智能体系统InferA并以HACC模拟评估其可用性。


<details>
  <summary>Details</summary>
Motivation: 传统自动化工具如PandasAI在分析TB级数据集时需全量数据摄入且缺乏数据结构上下文，不实用，需新方法。

Method: 提出InferA多智能体系统，核心是监督智能体协调专业智能体负责数据检索和分析各阶段，与用户交互明确分析意图。

Result: 用HACC宇宙学模拟的集成运行评估系统。

Conclusion: 文中未明确提及最终结论，但展示了InferA系统有潜力解决大规模科学数据分析问题。

Abstract: Analyzing large-scale scientific datasets presents substantial challenges due
to their sheer volume, structural complexity, and the need for specialized
domain knowledge. Automation tools, such as PandasAI, typically require full
data ingestion and lack context of the full data structure, making them
impractical as intelligent data analysis assistants for datasets at the
terabyte scale. To overcome these limitations, we propose InferA, a multi-agent
system that leverages large language models to enable scalable and efficient
scientific data analysis. At the core of the architecture is a supervisor agent
that orchestrates a team of specialized agents responsible for distinct phases
of the data retrieval and analysis. The system engages interactively with users
to elicit their analytical intent and confirm query objectives, ensuring
alignment between user goals and system actions. To demonstrate the framework's
usability, we evaluate the system using ensemble runs from the HACC cosmology
simulation which comprises several terabytes.

</details>


### [285] [Simulation-Based Pretraining and Domain Adaptation for Astronomical Time Series with Minimal Labeled Data](https://arxiv.org/abs/2510.12958)
*Rithwik Gupta,Daniel Muthukrishna,Jeroen Audenaert*

Main category: astro-ph.IM

TL;DR: 提出利用模拟数据的预训练方法，减少对真实观测标记数据的需求，模型在多任务中表现优且有跨域能力。


<details>
  <summary>Details</summary>
Motivation: 解决天文时间序列分析中标记观测数据稀缺的问题。

Method: 采用基于分类器架构，结合对比和对抗目标，在多个天文调查的模拟数据上训练模型。

Result: 模型在微调少量真实数据后，在分类、红移估计和异常检测等任务上性能显著提升，有零样本迁移能力和跨域能力。

Conclusion: 该方法为标记数据稀缺但可在模拟中编码领域知识时构建通用模型提供了实用解决方案。

Abstract: Astronomical time-series analysis faces a critical limitation: the scarcity
of labeled observational data. We present a pre-training approach that
leverages simulations, significantly reducing the need for labeled examples
from real observations. Our models, trained on simulated data from multiple
astronomical surveys (ZTF and LSST), learn generalizable representations that
transfer effectively to downstream tasks. Using classifier-based architectures
enhanced with contrastive and adversarial objectives, we create domain-agnostic
models that demonstrate substantial performance improvements over baseline
methods in classification, redshift estimation, and anomaly detection when
fine-tuned with minimal real data. Remarkably, our models exhibit effective
zero-shot transfer capabilities, achieving comparable performance on future
telescope (LSST) simulations when trained solely on existing telescope (ZTF)
data. Furthermore, they generalize to very different astronomical phenomena
(namely variable stars from NASA's \textit{Kepler} telescope) despite being
trained on transient events, demonstrating cross-domain capabilities. Our
approach provides a practical solution for building general models when labeled
data is scarce, but domain knowledge can be encoded in simulations.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [286] [TaskAudit: Detecting Functiona11ity Errors in Mobile Apps via Agentic Task Execution](https://arxiv.org/abs/2510.12972)
*Mingyuan Zhong,Xia Chen,Davin Win Kyi,Chen Li,James Fogarty,Jacob O. Wobbrock*

Main category: cs.HC

TL;DR: 介绍了可访问性评估系统TaskAudit，通过模拟交互检测功能性错误，评估显示其比现有检查器更有效。


<details>
  <summary>Details</summary>
Motivation: 当前大多数可访问性检查器无法捕捉影响移动应用功能的常见可访问性错误，需要新方法。

Method: TaskAudit由任务生成器、任务执行器和可访问性分析器三个组件组成，通过模拟交互检测错误。

Result: 在真实应用上评估，TaskAudit从54个应用屏幕中检测到48个功能性错误，而现有检查器为4到20个。

Conclusion: TaskAudit能检测到现有检查器之外的常见错误模式，如标签 - 功能不匹配、导航混乱和不适当反馈。

Abstract: Accessibility checkers are tools in support of accessible app development and
their use is encouraged by accessibility best practices. However, most current
checkers evaluate static or mechanically-generated contexts, failing to capture
common accessibility errors impacting mobile app functionality. We present
TaskAudit, an accessibility evaluation system that focuses on detecting
functiona11ity errors through simulated interactions. TaskAudit comprises three
components: a Task Generator that constructs interactive tasks from app
screens, a Task Executor that uses agents with a screen reader proxy to perform
these tasks, and an Accessibility Analyzer that detects and reports
accessibility errors by examining interaction traces. Evaluation on real-world
apps shows that our strategy detects 48 functiona11ity errors from 54 app
screens, compared to between 4 and 20 with existing checkers. Our analysis
demonstrates common error patterns that TaskAudit can detect in addition to
prior work, including label-functionality mismatch, cluttered navigation, and
inappropriate feedback.

</details>


### [287] [Developing and Validating the Arabic Version of the Attitudes Toward Large Language Models Scale](https://arxiv.org/abs/2510.13009)
*Basad Barajeeh,Ala Yankouskaya,Sameha AlShakhsi,Chun Sing Maxwell Ho,Guandong Xu,Raian Ali*

Main category: cs.HC

TL;DR: 文章将英语的AT - GLLM和AT - PLLM量表翻译成阿拉伯语并对249名阿拉伯语成年人样本进行验证，结果显示该量表可靠有效，能支持非西方语境研究及阿拉伯地区本地化研究和政策制定。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）使用全球化，阿拉伯世界LLM采用率快速增长，需要文化和语言相关量表来准确衡量该地区对LLM的态度。

Method: 将AT - GLLM和AT - PLLM量表翻译成阿拉伯语，并使用249名阿拉伯语成年人样本进行验证。

Result: 翻译成阿拉伯语的量表是可靠有效的工具，心理测量分析证实了双因素结构、跨性别强测量不变性和良好内部信度，量表还显示出强收敛和区分效度。

Conclusion: 该量表能支持非西方语境研究，助力描绘全球LLM认知图景，也便于阿拉伯地区本地化研究和政策制定。

Abstract: As the use of large language models (LLMs) becomes increasingly global,
understanding public attitudes toward these systems requires tools that are
adapted to local contexts and languages. In the Arab world, LLM adoption has
grown rapidly with both globally dominant platforms and regional ones like
Fanar and Jais offering Arabic-specific solutions. This highlights the need for
culturally and linguistically relevant scales to accurately measure attitudes
toward LLMs in the region. Tools assessing attitudes toward artificial
intelligence (AI) can provide a base for measuring attitudes specific to LLMs.
The 5-item Attitudes Toward Artificial Intelligence (ATAI) scale, which
measures two dimensions, the AI Fear and the AI Acceptance, has been recently
adopted and adapted to develop new instruments in English using a sample from
the UK: the Attitudes Toward General LLMs (AT-GLLM) and Attitudes Toward
Primary LLM (AT-PLLM) scales. In this paper, we translate the two scales,
AT-GLLM and AT-PLLM, and validate them using a sample of 249 Arabic-speaking
adults. The results show that the scale, translated into Arabic, is a reliable
and valid tool that can be used for the Arab population and language.
Psychometric analyses confirmed a two-factor structure, strong measurement
invariance across genders, and good internal reliability. The scales also
demonstrated strong convergent and discriminant validity. Our scales will
support research in a non-Western context, a much-needed effort to help draw a
global picture of LLM perceptions, and will also facilitate localized research
and policy-making in the Arab region.

</details>


### [288] [Deliberate Lab: A Platform for Real-Time Human-AI Social Experiments](https://arxiv.org/abs/2510.13011)
*Crystal Qian,Vivian Tsai,Michael Behr,Nada Hussein,Léo Laugier,Nithum Thain,Lucas Dixon*

Main category: cs.HC

TL;DR: 介绍开源平台Deliberate Lab用于大规模实时行为实验，分析其12个月公开部署情况，该平台扩展了研究方法。


<details>
  <summary>Details</summary>
Motivation: 社会与行为科学家研究人机互动等实验基础设施不足，需更好平台。

Method: 推出Deliberate Lab平台，进行12个月公开部署，分析使用模式与工作流程，收集案例和场景，访谈实验者。

Result: 完成12个月公开部署，有88位实验者和9195位参与者，收集相关案例和场景。

Conclusion: Deliberate Lab降低技术门槛，标准化人机混合实验支持，扩展了研究集体决策和以人为中心AI的方法。

Abstract: Social and behavioral scientists increasingly aim to study how humans
interact, collaborate, and make decisions alongside artificial intelligence.
However, the experimental infrastructure for such work remains underdeveloped:
(1) few platforms support real-time, multi-party studies at scale; (2) most
deployments require bespoke engineering, limiting replicability and
accessibility, and (3) existing tools do not treat AI agents as first-class
participants. We present Deliberate Lab, an open-source platform for
large-scale, real-time behavioral experiments that supports both human
participants and large language model (LLM)-based agents. We report on a
12-month public deployment of the platform (N=88 experimenters, N=9195
experiment participants), analyzing usage patterns and workflows. Case studies
and usage scenarios are aggregated from platform users, complemented by
in-depth interviews with select experimenters. By lowering technical barriers
and standardizing support for hybrid human-AI experimentation, Deliberate Lab
expands the methodological repertoire for studying collective decision-making
and human-centered AI.

</details>


### [289] [Behavioral Biometrics for Automatic Detection of User Familiarity in VR](https://arxiv.org/abs/2510.12988)
*Numan Zafar,Priyo Ranjan Kundu Prosun,Shafique Ahmad Chaudhry*

Main category: cs.HC

TL;DR: 研究通过分析开门任务中手部运动模式自动检测用户对VR的熟悉度，用深度分类器取得高准确率，显示手部生物特征用于实时检测的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着VR设备普及，自动检测用户对VR的熟悉度可实现实时自适应训练和界面调整，减少用户挫败感并提高任务表现。

Method: 让26名有经验和无经验的VR用户分别通过控制器和手部追踪完成基于密码的开门任务，用先进的深度分类器进行检测。

Result: 手部追踪和基于控制器交互的检测准确率分别达92.05%和83.42%，跨设备评估准确率78.89%，混合设备评估准确率94.19%。

Conclusion: 手部运动生物特征用于关键VR应用中实时检测用户熟悉度很有前景，可为VR体验带来个性化和适应性。

Abstract: As virtual reality (VR) devices become increasingly integrated into everyday
settings, a growing number of users without prior experience will engage with
VR systems. Automatically detecting a user's familiarity with VR as an
interaction medium enables real-time, adaptive training and interface
adjustments, minimizing user frustration and improving task performance. In
this study, we explore the automatic detection of VR familiarity by analyzing
hand movement patterns during a passcode-based door-opening task, which is a
well-known interaction in collaborative virtual environments such as meeting
rooms, offices, and healthcare spaces. While novice users may lack prior VR
experience, they are likely to be familiar with analogous real-world tasks
involving keypad entry. We conducted a pilot study with 26 participants, evenly
split between experienced and inexperienced VR users, who performed tasks using
both controller-based and hand-tracking interactions. Our approach uses
state-of-the-art deep classifiers for automatic VR familiarity detection,
achieving the highest accuracies of 92.05% and 83.42% for hand-tracking and
controller-based interactions, respectively. In the cross-device evaluation,
where classifiers trained on controller data were tested using hand-tracking
data, the model achieved an accuracy of 78.89%. The integration of both
modalities in the mixed-device evaluation obtained an accuracy of 94.19%. Our
results underline the promise of using hand movement biometrics for the
real-time detection of user familiarity in critical VR applications, paving the
way for personalized and adaptive VR experiences.

</details>


### [290] [Deep Learning-Based Visual Fatigue Detection Using Eye Gaze Patterns in VR](https://arxiv.org/abs/2510.12994)
*Numan Zafar,Johnathan Locke,Shafique Ahmad Chaudhry*

Main category: cs.HC

TL;DR: 本文提出基于深度学习用VR中连续眼动轨迹检测视觉疲劳，EKYT准确率达94%，表明眼动动态可用于疲劳检测。


<details>
  <summary>Details</summary>
Motivation: 长时间使用VR会导致视觉疲劳，现有疲劳检测方法可扩展性和实时性有限。

Method: 使用GazeBaseVR数据集，提取独眼眼动角度，评估六个深度分类器。

Result: EKYT准确率达94%，在高视觉注意力任务中表现好，疲劳和非疲劳状态有显著行为差异。

Conclusion: 眼动动态是沉浸式VR中可靠且非侵入性的连续疲劳检测方式，对自适应人机交互有实际意义。

Abstract: Prolonged exposure to virtual reality (VR) systems leads to visual fatigue,
impairs user comfort, performance, and safety, particularly in high-stakes or
long-duration applications. Existing fatigue detection approaches rely on
subjective questionnaires or intrusive physiological signals, such as EEG,
heart rate, or eye-blink count, which limit their scalability and real-time
applicability. This paper introduces a deep learning-based study for detecting
visual fatigue using continuous eye-gaze trajectories recorded in VR. We use
the GazeBaseVR dataset comprising binocular eye-tracking data from 407
participants across five immersive tasks, extract cyclopean eye-gaze angles,
and evaluate six deep classifiers. Our results demonstrate that EKYT achieves
up to 94% accuracy, particularly in tasks demanding high visual attention, such
as video viewing and text reading. We further analyze gaze variance and
subjective fatigue measures, indicating significant behavioral differences
between fatigued and non-fatigued conditions. These findings establish eye-gaze
dynamics as a reliable and nonintrusive modality for continuous fatigue
detection in immersive VR, offering practical implications for adaptive
human-computer interactions.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [291] [Beyond Discrete Categories: Multi-Task Valence-Arousal Modeling for Pet Vocalization Analysis](https://arxiv.org/abs/2510.12819)
*Junyao Huang,Rumin Situ*

Main category: cs.SD

TL;DR: 提出连续的价-唤醒（VA）模型用于宠物发声情感识别，用自动标注算法和多任务学习框架，模型效果好，有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统基于离散分类的宠物发声情感识别存在模糊性和难以捕捉强度变化的问题。

Method: 提出连续VA模型，用自动VA标签生成算法标注大量样本，采用多任务学习框架联合训练VA回归与辅助任务。

Result: 音频Transformer模型在验证集上价的皮尔逊相关系数r = 0.9024，唤醒的r = 0.7155，解决了离散类别间的混淆。

Conclusion: 该工作引入首个连续VA框架，适用于人宠交互等场景，有部署在消费产品中的潜力。

Abstract: Traditional pet emotion recognition from vocalizations, based on discrete
classification, struggles with ambiguity and capturing intensity variations. We
propose a continuous Valence-Arousal (VA) model that represents emotions in a
two-dimensional space. Our method uses an automatic VA label generation
algorithm, enabling large-scale annotation of 42,553 pet vocalization samples.
A multi-task learning framework jointly trains VA regression with auxiliary
tasks (emotion, body size, gender) to enhance prediction by improving feature
learning. Our Audio Transformer model achieves a validation Valence Pearson
correlation of r = 0.9024 and an Arousal r = 0.7155, effectively resolving
confusion between discrete categories like "territorial" and "happy." This work
introduces the first continuous VA framework for pet vocalization analysis,
offering a more expressive representation for human-pet interaction, veterinary
diagnostics, and behavioral training. The approach shows strong potential for
deployment in consumer products like AI pet emotion translators.

</details>


### [292] [Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction](https://arxiv.org/abs/2510.12834)
*Téo Guichoux,Théodor Lemerle,Shivam Mehta,Jonas Beskow,Gustave Eje Henter,Laure Soulier,Catherine Pelachaud,Nicolas Obin*

Main category: cs.SD

TL;DR: 提出Gelina框架联合合成语音和共语音手势，评估显示其有竞争力的语音质量和更好的手势生成效果。


<details>
  <summary>Details</summary>
Motivation: 多数计算方法顺序合成语音和手势，削弱了同步性和韵律对齐，需要改进。

Method: 引入Gelina统一框架，使用离散自回归骨干中的交错令牌序列和特定模态解码器联合合成。

Result: 主观和客观评估显示有竞争力的语音质量，且相比单模态基线改进了手势生成。

Conclusion: Gelina框架在语音和手势合成方面有较好表现。

Abstract: Human communication is multimodal, with speech and gestures tightly coupled,
yet most computational methods for generating speech and gestures synthesize
them sequentially, weakening synchrony and prosody alignment. We introduce
Gelina, a unified framework that jointly synthesizes speech and co-speech
gestures from text using interleaved token sequences in a discrete
autoregressive backbone, with modality-specific decoders. Gelina supports
multi-speaker and multi-style cloning and enables gesture-only synthesis from
speech inputs. Subjective and objective evaluations demonstrate competitive
speech quality and improved gesture generation over unimodal baselines.

</details>


### [293] [MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding](https://arxiv.org/abs/2510.13244)
*Xuanchen Wang,Heng Wang,Weidong Cai*

Main category: cs.SD

TL;DR: 提出MotionBeat框架用于运动对齐的音乐表征学习，在多任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有音频表征忽略具身体验维度，难以捕捉驱动运动的节奏和结构线索。

Method: 提出Embodied Contrastive Loss和Structural Rhythm Alignment Loss两个训练目标，引入bar-equivariant phase rotations和contact-guided attention。

Result: MotionBeat在音乐到舞蹈生成中优于现有音频编码器，能有效迁移到多个任务。

Conclusion: MotionBeat框架有效，可用于多种音乐相关任务。

Abstract: Music is both an auditory and an embodied phenomenon, closely linked to human
motion and naturally expressed through dance. However, most existing audio
representations neglect this embodied dimension, limiting their ability to
capture rhythmic and structural cues that drive movement. We propose
MotionBeat, a framework for motion-aligned music representation learning.
MotionBeat is trained with two newly proposed objectives: the Embodied
Contrastive Loss (ECL), an enhanced InfoNCE formulation with tempo-aware and
beat-jitter negatives to achieve fine-grained rhythmic discrimination, and the
Structural Rhythm Alignment Loss (SRAL), which ensures rhythm consistency by
aligning music accents with corresponding motion events. Architecturally,
MotionBeat introduces bar-equivariant phase rotations to capture cyclic
rhythmic patterns and contact-guided attention to emphasize motion events
synchronized with musical accents. Experiments show that MotionBeat outperforms
state-of-the-art audio encoders in music-to-dance generation and transfers
effectively to beat tracking, music tagging, genre and instrument
classification, emotion recognition, and audio-visual retrieval. Our project
demo page: https://motionbeat2025.github.io/.

</details>


### [294] [Adaptive vector steering: A training-free, layer-wise intervention for hallucination mitigation in large audio and multimodal models](https://arxiv.org/abs/2510.12851)
*Tsung-En Lin,Kuan-Yi Lee,Hung-Yi Lee*

Main category: cs.SD

TL;DR: 大音频语言模型和多模态大语言模型在音频任务中有强大能力，但会产生幻觉。本文提出AVS方法缓解该问题，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决大音频语言模型和多模态大语言模型在音频任务中产生幻觉的问题。

Method: 探查模型内部状态，提出Adaptive Vector Steering (AVS)方法，利用输出正确性和内部表示的强相关性。

Result: 在两个模型和两个基准测试中性能持续提升，如在Audio Hallucination QA数据集上提升F1分数，在MMAU上提升Qwen的准确率。

Conclusion: 本文首次应用向量转向缓解音频幻觉问题，AVS方法有效提升模型性能。

Abstract: Large Audio-Language Models and Multi-Modal Large Language Models have
demonstrated strong capabilities in tasks such as Audio Question Answering
(AQA), Audio Captioning, and Automatic Speech Recognition (ASR). However, there
is growing evidence that these models can hallucinate about the content of the
audio. To address this issue, we probe the models' internal states and propose
Adaptive Vector Steering (AVS), a method that better grounds generation in
audio content. We also identify a strong correlation between output correctness
and internal representations. Experiments show consistent performance gains
across two models and two benchmarks. On the Audio Hallucination QA dataset,
our method boosts the F1-score of Gemma from 0.550 to 0.619 and Qwen from 0.626
to 0.632. Furthermore, our method increases the accuracy of Qwen on MMAU from
0.548 to 0.592, marking an 8% relative increase. To the best of our knowledge,
this is the first work to apply vector steering to mitigate hallucination in
audio.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [295] [Towards Human-Centric Intelligent Treatment Planning for Radiation Therapy](https://arxiv.org/abs/2510.13062)
*Adnan Jafar,Xun Jia*

Main category: physics.med-ph

TL;DR: 当前放疗治疗计划存在问题，本文介绍以人为主导的智能治疗规划（HCITP）框架，预期可提高效率和计划质量并探讨挑战与解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决当前放疗治疗计划中存在的计划质量不佳、效率低和成本高的问题。

Method: 引入在人工监督下由AI驱动的HCITP框架，集成临床指南、自动化计划生成并实现与操作者的直接交互。

Result: 预期HCITP能提高效率，将规划时间缩短至数分钟，并提供个性化、高质量的计划。

Conclusion: 文中讨论了HCITP面临的挑战及潜在解决方案。

Abstract: Current radiation therapy treatment planning is limited by suboptimal plan
quality, inefficiency, and high costs. This perspective paper explores the
complexity of treatment planning and introduces Human-Centric Intelligent
Treatment Planning (HCITP), an AI-driven framework under human oversight, which
integrates clinical guidelines, automates plan generation, and enables direct
interactions with operators. We expect that HCITP will enhance efficiency,
potentially reducing planning time to minutes, and will deliver personalized,
high-quality plans. Challenges and potential solutions are discussed.

</details>


### [296] [Steerable Conditional Diffusion for Domain Adaptation in PET Image Reconstruction](https://arxiv.org/abs/2510.13441)
*George Webber,Alexander Hammers,Andrew P. King,Andrew J. Reader*

Main category: physics.med-ph

TL;DR: 提出集成SCD与PET - LiSch框架，用LoRA在重建时动态调整扩散模型先验，实验表明该方法能抑制域偏移下的伪影，为基于扩散的PET重建提供概念验证。


<details>
  <summary>Details</summary>
Motivation: 扩散模型用于PET图像重建存在域偏移问题，影响临床应用，需改进模型先验与目标主体的对齐。

Method: 将可转向条件扩散（SCD）与似然调度扩散（PET - LiSch）框架集成，在重建时用低秩自适应（LoRA）动态调整扩散模型先验。

Result: 在逼真的合成2D脑模体实验中，该方法能抑制域偏移下的幻觉伪影，在定性和定量上优于OSEM和扩散模型基线。

Conclusion: 可转向先验能缓解基于扩散的PET重建中的域偏移问题，值得在真实数据上进一步评估。

Abstract: Diffusion models have recently enabled state-of-the-art reconstruction of
positron emission tomography (PET) images while requiring only image training
data. However, domain shift remains a key concern for clinical adoption: priors
trained on images from one anatomy, acquisition protocol or pathology may
produce artefacts on out-of-distribution data. We propose integrating steerable
conditional diffusion (SCD) with our previously-introduced likelihood-scheduled
diffusion (PET-LiSch) framework to improve the alignment of the diffusion
model's prior to the target subject. At reconstruction time, for each diffusion
step, we use low-rank adaptation (LoRA) to align the diffusion model prior with
the target domain on the fly. Experiments on realistic synthetic 2D brain
phantoms demonstrate that our approach suppresses hallucinated artefacts under
domain shift, i.e. when our diffusion model is trained on perturbed images and
tested on normal anatomy, our approach suppresses the hallucinated structure,
outperforming both OSEM and diffusion model baselines qualitatively and
quantitatively. These results provide a proof-of-concept that steerable priors
can mitigate domain shift in diffusion-based PET reconstruction and motivate
future evaluation on real data.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [297] [Semantic Communication Enabled Holographic Video Processing and Transmission](https://arxiv.org/abs/2510.13408)
*Jingkai Ying,Zhiyuan Qi,Yulong Feng,Zhijin Qin,Zhu Han,Rahim Tafazolli,Yonina C. Eldar*

Main category: eess.IV

TL;DR: 文章介绍全息视频通信，提出语义全息视频通信系统架构、关键技术，用用例展示性能增益并探讨潜在研究课题。


<details>
  <summary>Details</summary>
Motivation: 全息视频通信能提供沉浸式体验，有范式转变意义，需构建相关系统。

Method: 先概述全息视频通信及系统要求，回顾语义通信后提出系统架构，基于架构设计关键技术，用用例验证。

Result: 通过两个用例展示了所提方法的性能增益。

Conclusion: 探讨了潜在研究课题，为语义全息视频通信实现奠定基础。

Abstract: Holographic video communication is considered a paradigm shift in visual
communications, becoming increasingly popular for its ability to offer
immersive experiences. This article provides an overview of holographic video
communication and outlines the requirements of a holographic video
communication system. Particularly, following a brief review of semantic com-
munication, an architecture for a semantic-enabled holographic video
communication system is presented. Key technologies, including semantic
sampling, joint semantic-channel coding, and semantic-aware transmission, are
designed based on the proposed architecture. Two related use cases are
presented to demonstrate the performance gain of the proposed methods. Finally,
potential research topics are discussed to pave the way for the realization of
semantic-enabled holographic video communications.

</details>


### [298] [Dedelayed: Deleting remote inference delay via on-device correction](https://arxiv.org/abs/2510.13714)
*Dan Jacobellis,Mateen Ulhaq,Fabien Racapé,Hyomin Choi,Neeraja J. Yadwadkar*

Main category: eess.IV

TL;DR: 提出Dedelayed方法减轻远程推理延迟，在视频语义分割任务中提升精度，适合实时任务。


<details>
  <summary>Details</summary>
Motivation: 远程推理存在通信网络延迟，导致预测过时，不适合实时任务。

Method: 采用轻量级本地模型处理当前帧，并融合重量级远程模型从过去帧计算的特征。

Result: 在BDD100K驾驶数据集视频上，超过33ms的实际通信网络延迟下，Dedelayed提升语义分割精度；100ms往返延迟时，相比全本地推理提升6.4mIoU，相比远程推理提升9.8mIoU。

Conclusion: Dedelayed能有效减轻延迟，在长延迟和高动态场景优势明显，适合需与当前世界状态保持一致的实时任务。

Abstract: Remote inference allows lightweight devices to leverage powerful cloud
models. However, communication network latency makes predictions stale and
unsuitable for real-time tasks. To address this, we introduce Dedelayed, a
delay-corrective method that mitigates arbitrary remote inference delays,
allowing the local device to produce low-latency outputs in real time. Our
method employs a lightweight local model that processes the current frame and
fuses in features that a heavyweight remote model computes from past frames. On
video from the BDD100K driving dataset, Dedelayed improves semantic
segmentation accuracy over the stronger of the local-only and remote-only
baselines across all realistic communication network delays beyond 33 ms.
Without incurring additional delay, it improves accuracy by 6.4 mIoU compared
to fully local inference and 9.8 mIoU compared to remote inference, for a
round-trip delay of 100 ms. The advantage grows under longer delays and
higher-motion scenes, as delay-mitigated split inference sustains accuracy more
effectively, providing clear advantages for real-time tasks that must remain
aligned with the current world state.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [299] [Towards an Asymptotic Efficiency Theory on Regular Parameter Manifolds](https://arxiv.org/abs/2510.13703)
*Lvfang Sun,Zhenhua Lin,Lin Liu*

Main category: math.ST

TL;DR: 本文旨在针对样本空间和参数空间为黎曼流形的情况，开发更统一的渐近效率理论，构建相关概念并建立效率界，还通过实例展示新框架优势。


<details>
  <summary>Details</summary>
Motivation: 现有效率理论大多基于样本空间和参数空间为赋范线性空间的前提，难以适用于分析复杂数据集，尤其是具有非欧几里得/非线性结构的情况。

Method: 构建词汇表，将效率理论中的基本概念从赋范线性空间转换到黎曼流形，在与赋范线性空间平行的条件下建立效率界。

Result: 建立了适用于样本空间、参数空间或两者为满足一定正则条件的黎曼流形的渐近效率理论。

Conclusion: 新框架具有概念上的优势，通过对总体弗雷歇均值和单指标模型回归系数向量两个实例的应用得以体现。

Abstract: Asymptotic efficiency theory is one of the pillars in the foundations of
modern mathematical statistics. Not only does it serve as a rigorous
theoretical benchmark for evaluating statistical methods, but it also sheds
light on how to develop and unify novel statistical procedures. For example,
the calculus of influence functions has led to many important statistical
breakthroughs in the past decades. Responding to the pressing challenge of
analyzing increasingly complex datasets, particularly those with
non-Euclidean/nonlinear structures, many novel statistical models and methods
have been proposed in recent years. However, the existing efficiency theory is
not always readily applicable to these cases, as the theory was developed, for
the most part, under the often neglected premise that both the sample space and
the parameter space are normed linear spaces. As a consequence, efficiency
results outside normed linear spaces are quite rare and isolated, obtained on a
case-by-case basis. This paper aims to develop a more unified asymptotic
efficiency theory, allowing the sample space, the parameter space, or both to
be Riemannian manifolds satisfying certain regularity conditions. We build a
vocabulary that helps translate essential concepts in efficiency theory from
normed linear spaces to Riemannian manifolds, such as (locally) regular
estimators, differentiable functionals, etc. Efficiency bounds are established
under conditions parallel to those for normed linear spaces. We also
demonstrate the conceptual advantage of the new framework by applying it to two
concrete examples in statistics: the population Frechet mean and the regression
coefficient vector of Single-Index Models.

</details>


### [300] [Optimal Bounds for Tyler's M-Estimator for Elliptical Distributions](https://arxiv.org/abs/2510.13751)
*Lap Chi Lau,Akshay Ramachandran*

Main category: math.ST

TL;DR: 文章聚焦椭圆分布形状矩阵估计问题，前人结果有样本复杂度问题，本文证明Tyler的M - 估计量的最优样本阈值和误差界，匹配高斯结果，还在低样本阈值下恢复算法收敛性。


<details>
  <summary>Details</summary>
Motivation: 前人对Tyler的M - 估计量在有限样本下的结果存在样本复杂度超过高斯设置的问题，需解决此差距。

Method: 基于Franks和Moitra的算子缩放联系，引入新颖的伪随机条件“∞ - 扩张”，证明椭圆分布在最优样本阈值满足该条件并得到新的缩放结果。

Result: 证明了Tyler的M - 估计量对所有椭圆分布的最优样本阈值和误差界，完全匹配高斯结果，且在低样本阈值下恢复算法收敛性。

Conclusion: 通过引入“∞ - 扩张”条件解决了前人结果在样本复杂度上的差距，得到了更优的结果。

Abstract: A fundamental problem in statistics is estimating the shape matrix of an
Elliptical distribution. This generalizes the familiar problem of Gaussian
covariance estimation, for which the sample covariance achieves optimal
estimation error. For Elliptical distributions, Tyler proposed a natural
M-estimator and showed strong statistical properties in the asymptotic regime,
independent of the underlying distribution. Numerical experiments show that
this estimator performs very well, and that Tyler's iterative procedure
converges quickly to the estimator. Franks and Moitra recently provided the
first distribution-free error bounds in the finite sample setting, as well as
the first rigorous convergence analysis of Tyler's iterative procedure.
However, their results exceed the sample complexity of the Gaussian setting by
a $\log^{2} d$ factor. We close this gap by proving optimal sample threshold
and error bounds for Tyler's M-estimator for all Elliptical distributions,
fully matching the Gaussian result. Moreover, we recover the algorithmic
convergence even at this lower sample threshold. Our approach builds on the
operator scaling connection of Franks and Moitra by introducing a novel
pseudorandom condition, which we call $\infty$-expansion. We show that
Elliptical distributions satisfy $\infty$-expansion at the optimal sample
threshold, and then prove a novel scaling result for inputs satisfying this
condition.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [301] [Narrow Operator Models of Stellarator Equilibria in Fourier Zernike Basis](https://arxiv.org/abs/2510.13521)
*Timo Thun,Rory Conlin,Dario Panici,Daniel Böckenhoff*

Main category: physics.plasm-ph

TL;DR: 提出求解理想磁流体动力学平衡磁场连续分布平衡态的数值方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法求解理想MHD方程单个静止点，本文旨在求解固定边界和旋转变换、仅改变压力不变量的连续分布平衡态。

Method: 通过优化多层感知器（MLP）参数来最小化力残差，MLP将标量压力乘数映射到Fourier Zernike基。

Result: 提出能求解连续分布平衡态的数值方法。

Conclusion: 该方法可用于求解理想MHD平衡磁场连续分布平衡态，为相关研究提供新途径。

Abstract: Numerical computation of the ideal Magnetohydrodynamic (MHD) equilibrium
magnetic field is at the base of stellarator optimisation and provides the
starting point for solving more sophisticated Partial Differential Equations
(PDEs) like transport or turbulence models. Conventional approaches solve for a
single stationary point of the ideal MHD equations, which is fully defined by
three invariants and the numerical scheme employed by the solver. We present
the first numerical approach that can solve for a continuous distribution of
equilibria with fixed boundary and rotational transform, varying only the
pressure invariant. This approach minimises the force residual by optimising
parameters of multilayer perceptrons (MLP) that map from a scalar pressure
multiplier to the Fourier Zernike basis as implemented in the modern
stellarator equilibrium solver DESC.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [302] [Data-intrinsic approximation in metric spaces](https://arxiv.org/abs/2510.13496)
*Jürgen Dölz,Michael Multerer*

Main category: math.NA

TL;DR: 本文聚焦有标签数据样本近似，引入离散连续模，提出计算算法，构建近似理论，考虑统计不确定性，连接多领域问题并做数值研究。


<details>
  <summary>Details</summary>
Motivation: 为减少数据分析处理的计算负担，研究有标签数据样本的近似问题。

Method: 识别离散连续模来衡量映射规律性，研究其在无限数据极限下的一致性，提出计算算法，构建基于样本的近似理论，采用多级近似空间和多级蒙特卡罗方法处理统计不确定性。

Result: 将有标签数据近似理论与覆盖问题、组合优化相联系，且通过数值研究验证方法可行性和理论结果。

Conclusion: 所提方法可行，理论结果得到验证，有效连接了多个领域问题。

Abstract: Analysis and processing of data is a vital part of our modern society and
requires vast amounts of computational resources. To reduce the computational
burden, compressing and approximating data has become a central topic. We
consider the approximation of labeled data samples, mathematically described as
site-to-value maps between finite metric spaces. Within this setting, we
identify the discrete modulus of continuity as an effective data-intrinsic
quantity to measure regularity of site-to-value maps without imposing further
structural assumptions. We investigate the consistency of the discrete modulus
of continuity in the infinite data limit and propose an algorithm for its
efficient computation. Building on these results, we present a sample based
approximation theory for labeled data. For data subject to statistical
uncertainty we consider multilevel approximation spaces and a variant of the
multilevel Monte Carlo method to compute statistical quantities of interest.
Our considerations connect approximation theory for labeled data in metric
spaces to the covering problem for (random) balls on the one hand and the
efficient evaluation of the discrete modulus of continuity to combinatorial
optimization on the other hand. We provide extensive numerical studies to
illustrate the feasibility of the approach and to validate our theoretical
results.

</details>
