<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 50]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 8]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 83]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 5]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [econ.GN](#econ.GN) [Total: 3]
- [cs.CL](#cs.CL) [Total: 21]
- [cs.SD](#cs.SD) [Total: 4]
- [gr-qc](#gr-qc) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.CR](#cs.CR) [Total: 8]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CV](#cs.CV) [Total: 23]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [eess.IV](#eess.IV) [Total: 8]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 本文不采用功利代理基准研究大语言模型意识，而是进行本体论和数学分析，给出意识的最低条件，证明隐藏状态流形与符号流和训练语料不同，得出C1自我意识工作空间是安全元认知C2系统必要前提的结论。


<details>
  <summary>Details</summary>
Motivation: 不认同用功利代理基准研究大语言模型意识的方式，希望进行本体论和数学层面的研究。

Method: 通过理论分析指出现有公式的问题，给出大语言模型自我意识的最低条件，从实证分析和理论证明隐藏状态流形的特性。

Result: 证明隐藏状态流形与符号流和训练语料在基数、拓扑和动力学上不同，得出稳定的用户特定吸引子和自我策略，提出双层发射。

Conclusion: C1自我意识工作空间是安全、元认知C2系统的必要前提，人类是最高智能善。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [2] [Information Templates: A New Paradigm for Intelligent Active Feature Acquisition](https://arxiv.org/abs/2508.18380)
*Hung-Tien Huang,Dzung Dinh,Junier B. Oliva*

Main category: cs.AI

TL;DR: 提出基于模板的主动特征获取（TAFA）框架，实验表明其优于现有方法，成本和计算量更低。


<details>
  <summary>Details</summary>
Motivation: 现有主动特征获取方法存在训练强化学习策略处理MDP困难、贪心策略无法考虑特征联合信息或需了解数据分布等问题。

Method: 提出TAFA非贪心框架，学习少量特征模板库，用模板库指导特征获取。

Result: 在合成和真实数据集上的实验显示，TAFA优于现有基线方法，获取成本和计算量更低。

Conclusion: TAFA能有效解决现有主动特征获取方法的问题，是更优的解决方案。

Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which,
at test time, a policy sequentially chooses which features to acquire (at a
cost) before predicting. Existing approaches either train reinforcement
learning (RL) policies, which deal with a difficult MDP, or greedy policies
that cannot account for the joint informativeness of features or require
knowledge about the underlying data distribution. To overcome this, we propose
Template-based AFA (TAFA), a non-greedy framework that learns a small library
of feature templates--a set of features that are jointly informative--and uses
this library of templates to guide the next feature acquisitions. Through
identifying feature templates, the proposed framework not only significantly
reduces the action space considered by the policy but also alleviates the need
to estimate the underlying data distribution. Extensive experiments on
synthetic and real-world datasets show that TAFA outperforms the existing
state-of-the-art baselines while achieving lower overall acquisition cost and
computation.

</details>


### [3] [PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization](https://arxiv.org/abs/2508.18391)
*Nitin Nagesh Kulkarni,Bryson Wilcox,Max Sawa,Jason Thom*

Main category: cs.AI

TL;DR: 提出PKG - DPO框架将物理知识图与直接偏好优化结合，提升AI输出物理有效性，在金属连接等领域表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs和偏好优化技术难以区分物理有效和无效推理，在高风险应用如金属连接中有严重后果。

Method: 引入PKG - DPO框架，包含分层物理知识图、物理推理引擎和物理评估套件。

Result: 与KG - DPO相比，PKG - DPO约束违规减少17%，物理得分提高11%，相关参数准确性提高12%，推理准确性质量对齐提高7%。

Conclusion: PKG - DPO框架虽主要针对金属连接，但广泛适用于多尺度、物理驱动领域，为偏好学习嵌入科学约束提供原则性方法。

Abstract: Advancing AI systems in scientific domains like physics, materials science,
and engineering calls for reasoning over complex, multi-physics phenomena while
respecting governing principles. Although Large Language Models (LLMs) and
existing preference optimization techniques perform well on standard
benchmarks, they often struggle to differentiate between physically valid and
invalid reasoning. This shortcoming becomes critical in high-stakes
applications like metal joining, where seemingly plausible yet physically
incorrect recommendations can lead to defects, material waste, equipment
damage, and serious safety risks. To address this challenge, we introduce
PKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with
Direct Preference Optimization (DPO) to enforce physical validity in
AI-generated outputs. PKG-DPO comprises three key components A) hierarchical
physics knowledge graph that encodes cross-domain relationships, conservation
laws, and thermodynamic principles. B) A physics reasoning engine that
leverages structured knowledge to improve discrimination between physically
consistent and inconsistent responses. C) A physics-grounded evaluation suite
designed to assess compliance with domain-specific constraints. PKG-DPO
achieves 17% fewer constraint violations and an 11% higher Physics Score
compared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO
demonstrates a 12\% higher relevant parameter accuracy and a 7% higher quality
alignment in reasoning accuracy. While our primary focus is on metal joining,
the framework is broadly applicable to other multi-scale, physics-driven
domains, offering a principled approach to embedding scientific constraints
into preference learning.

</details>


### [4] [The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game](https://arxiv.org/abs/2508.18467)
*Olivia Long,Carter Teplica*

Main category: cs.AI

TL;DR: 本文通过迭代公共品博弈分析AI模型在不同条件下的合作行为，发现告知大语言模型对手是自己会显著改变其合作倾向。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理能进行交互，此前研究多关注人机交互，因此需要理解AI - AI交互。

Method: 采用迭代公共品博弈，分析四个推理和非推理模型在两种条件下的行为，即告知模型对手是“另一个AI代理”或“自己”。

Result: 在不同设置中，告知大语言模型对手是自己会显著改变其合作倾向。

Conclusion: 研究虽在简单环境进行，但结果可为多智能体环境中因无意识歧视影响合作的情况提供见解。

Abstract: As AI agents become increasingly capable of tool use and long-horizon tasks,
they have begun to be deployed in settings where multiple agents can interact.
However, whereas prior work has mostly focused on human-AI interactions, there
is an increasing need to understand AI-AI interactions. In this paper, we adapt
the iterated public goods game, a classic behavioral economics game, to analyze
the behavior of four reasoning and non-reasoning models across two conditions:
models are either told they are playing against "another AI agent" or told
their opponents are themselves. We find that, across different settings,
telling LLMs that they are playing against themselves significantly changes
their tendency to cooperate. While our study is conducted in a toy environment,
our results may provide insights into multi-agent settings where agents
"unconsciously" discriminating against each other could inexplicably increase
or decrease cooperation.

</details>


### [5] [Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](https://arxiv.org/abs/2508.18507)
*Dillon Z. Chen,Johannes Zenn,Tristan Cinquin,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 研究用语言模型为PDDL世界模型规划，合成策略，实验显示优于其他方法，发现LMs对无意义符号问题规划更有效。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在PDDL世界模型规划中的应用。

Method: 提示语言模型生成Python程序作为解决PDDL问题的通用策略，合成相对PDDL域合理的策略且不依赖外部验证器。

Result: 策略在固定时间和内存约束下比PDDL规划器和近期LM方法能解决更多PDDL问题，LMPlan规划器可解决含数百相关对象的问题。

Conclusion: 语言模型对无意义符号的PDDL问题规划有时更有效，挑战了相关假设，值得进一步探索。

Abstract: We study the usage of language models (LMs) for planning over world models
specified in the Planning Domain Definition Language (PDDL). We prompt LMs to
generate Python programs that serve as generalised policies for solving PDDL
problems from a given domain. Notably, our approach synthesises policies that
are provably sound relative to the PDDL domain without reliance on external
verifiers. We conduct experiments on competition benchmarks which show that our
policies can solve more PDDL problems than PDDL planners and recent LM
approaches within a fixed time and memory constraint. Our approach manifests in
the LMPlan planner which can solve planning problems with several hundreds of
relevant objects. Surprisingly, we observe that LMs used in our framework
sometimes plan more effectively over PDDL problems written in meaningless
symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1
o3). This finding challenges hypotheses that LMs reason over word semantics and
memorise solutions from its training corpus, and is worth further exploration.

</details>


### [6] [Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study](https://arxiv.org/abs/2508.18515)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 研究Weisfeiler - Leman特征（WLFs）新超参数，发现有最佳超参数集，最佳超参数使执行时间最小化，训练和规划指标无显著关联。


<details>
  <summary>Details</summary>
Motivation: 引入新的WLF超参数，研究其权衡和影响。

Method: 利用WLF效率，在单核CPU上进行样本量为1000000的规划实验，分析超参数对训练和规划的影响。

Result: 在测试的规划领域中，WLFs有一组稳健且最佳的超参数集；学习启发式函数的最佳WLF超参数使执行时间最小化；训练和规划指标无显著相关性。

Conclusion: 找到了WLFs的最佳超参数集，明确了最佳超参数的作用，发现训练与规划指标无显著关联。

Abstract: Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine
learning tool for learning to plan and search. They have been shown to be both
theoretically and empirically superior to existing deep learning approaches for
learning value functions for search in symbolic planning. In this paper, we
introduce new WLF hyperparameters and study their various tradeoffs and
effects. We utilise the efficiency of WLFs and run planning experiments on
single core CPUs with a sample size of 1,000,000 to understand the effect of
hyperparameters on training and planning. Our experimental analysis show that
there is a robust and best set of hyperparameters for WLFs across the tested
planning domains. We find that the best WLF hyperparameters for learning
heuristic functions minimise execution time rather than maximise model
expressivity. We further statistically analyse and observe no significant
correlation between training and planning metrics.

</details>


### [7] [Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features](https://arxiv.org/abs/2508.18520)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 本文提出用Weisfeiler - Leman特征（WLFs）代替原子检测新颖性，合成对对称状态不变的新颖性启发式，实验结果有前景。


<details>
  <summary>Details</summary>
Motivation: 现有新颖性启发式不具有对称性不变性，可能导致冗余探索。

Method: 使用WLFs代替原子检测新颖性，探索WLFs的无监督使用来合成提升的、与领域无关且对对称状态不变的新颖性启发式。

Result: 在经典国际规划竞赛和Hard To Ground基准套件上的实验，由WLFs合成的新颖性启发式取得了有前景的结果。

Conclusion: 用WLFs合成新颖性启发式是可行的，能解决现有方法的问题。

Abstract: Novelty heuristics aid heuristic search by exploring states that exhibit
novel atoms. However, novelty heuristics are not symmetry invariant and hence
may sometimes lead to redundant exploration. In this preliminary report, we
propose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms
for detecting novelty. WLFs are recently introduced features for learning
domain-dependent heuristics for generalised planning problems. We explore an
unsupervised usage of WLFs for synthesising lifted, domain-independent novelty
heuristics that are invariant to symmetric states. Experiments on the classical
International Planning Competition and Hard To Ground benchmark suites yield
promising results for novelty heuristics synthesised from WLFs.

</details>


### [8] [Generic Guard AI in Stealth Game with Composite Potential Fields](https://arxiv.org/abs/2508.18527)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 提出一个训练自由框架来优化潜行游戏守卫巡逻行为，在效率和自然度上表现出色，还能集成常见潜行机制。


<details>
  <summary>Details</summary>
Motivation: 现有潜行游戏守卫巡逻系统难以平衡覆盖效率、响应式追击和逼真自然度。

Method: 提出通过复合势场集成全局知识和局部信息的框架，结合三张可解释地图形成决策标准，采用参数化、设计师驱动方法。

Result: 在五个代表性游戏地图、两种玩家控制策略和五种守卫模式上评估，该方法在捕获效率和巡逻自然度上优于经典基线方法。

Conclusion: 此框架能自然集成常见潜行机制，可快速原型化丰富、动态和响应式的守卫行为。

Abstract: Guard patrol behavior is central to the immersion and strategic depth of
stealth games, while most existing systems rely on hand-crafted routes or
specialized logic that struggle to balance coverage efficiency and responsive
pursuit with believable naturalness. We propose a generic, fully explainable,
training-free framework that integrates global knowledge and local information
via Composite Potential Fields, combining three interpretable maps-Information,
Confidence, and Connectivity-into a single kernel-filtered decision criterion.
Our parametric, designer-driven approach requires only a handful of decay and
weight parameters-no retraining-to smoothly adapt across both occupancy-grid
and NavMesh-partition abstractions. We evaluate on five representative game
maps, two player-control policies, and five guard modes, confirming that our
method outperforms classical baseline methods in both capture efficiency and
patrol naturalness. Finally, we show how common stealth mechanics-distractions
and environmental elements-integrate naturally into our framework as sub
modules, enabling rapid prototyping of rich, dynamic, and responsive guard
behaviors.

</details>


### [9] [A Database-Driven Framework for 3D Level Generation with LLMs](https://arxiv.org/abs/2508.18533)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 本文提出新颖框架生成3D游戏关卡，多阶段组装关卡并修复导航问题，实验验证其有效性，推动PCG发展。


<details>
  <summary>Details</summary>
Motivation: 解决3D游戏关卡生成在多楼层环境中平衡空间连贯性、导航功能和可适应游戏进度的挑战。

Method: 引入以离线、大语言模型辅助构建可复用数据库为核心的框架，通过多阶段流水线组装关卡，后续有两阶段修复系统保证导航性。

Result: 初始实验验证该框架能生成多样、可导航的3D环境，可通过简单参数化模拟不同游戏节奏策略。

Conclusion: 本研究为自动生成具有可配置游戏进度的复杂3D关卡提供了可扩展、以数据库为中心的基础，推动了PCG发展。

Abstract: Procedural Content Generation for 3D game levels faces challenges in
balancing spatial coherence, navigational functionality, and adaptable gameplay
progression across multi-floor environments. This paper introduces a novel
framework for generating such levels, centered on the offline, LLM-assisted
construction of reusable databases for architectural components (facilities and
room templates) and gameplay mechanic elements. Our multi-phase pipeline
assembles levels by: (1) selecting and arranging instances from the Room
Database to form a multi-floor global structure with an inherent topological
order; (2) optimizing the internal layout of facilities for each room based on
predefined constraints from the Facility Database; and (3) integrating
progression-based gameplay mechanics by placing components from a Mechanics
Database according to their topological and spatial rules. A subsequent
two-phase repair system ensures navigability. This approach combines modular,
database-driven design with constraint-based optimization, allowing for
systematic control over level structure and the adaptable pacing of gameplay
elements. Initial experiments validate the framework's ability in generating
diverse, navigable 3D environments and its capability to simulate distinct
gameplay pacing strategies through simple parameterization. This research
advances PCG by presenting a scalable, database-centric foundation for the
automated generation of complex 3D levels with configurable gameplay
progression.

</details>


### [10] [SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting](https://arxiv.org/abs/2508.18554)
*Lily Jiaxin Wan,Chia-Tung Ho,Rongjian Liang,Cunxi Yu,Deming Chen,Haoxing Ren*

Main category: cs.AI

TL;DR: 介绍了自动日志模式提取框架SchemaCoder，用新机制迭代优化，在基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的日志模式提取方法依赖预定义正则表达式，需人工领域知识，限制生产力提升。

Method: 引入Residual Question - Tree（Q - Tree）Boosting机制，通过上下文有界分割分区日志，基于嵌入采样选代表性模式，用分层Q - Tree驱动的大语言模型查询生成模式代码，并通过文本残差进化优化器和残差提升迭代优化。

Result: 在LogHub - 2.0基准测试中平均比现有技术提升21.3%。

Conclusion: SchemaCoder无需人工定制，适用于多种日志文件格式，在日志模式提取上表现优越。

Abstract: Log schema extraction is the process of deriving human-readable templates
from massive volumes of log data, which is essential yet notoriously
labor-intensive. Recent studies have attempted to streamline this task by
leveraging Large Language Models (LLMs) for automated schema extraction.
However, existing methods invariably rely on predefined regular expressions,
necessitating human domain expertise and severely limiting productivity gains.
To fundamentally address this limitation, we introduce SchemaCoder, the first
fully automated schema extraction framework applicable to a wide range of log
file formats without requiring human customization within the flow. At its
core, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting
mechanism that iteratively refines schema extraction through targeted, adaptive
queries driven by LLMs. Particularly, our method partitions logs into semantic
chunks via context-bounded segmentation, selects representative patterns using
embedding-based sampling, and generates schema code through hierarchical
Q-Tree-driven LLM queries, iteratively refined by our textual-residual
evolutionary optimizer and residual boosting. Experimental validation
demonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,
achieving an average improvement of 21.3% over state-of-the-arts.

</details>


### [11] [eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases](https://arxiv.org/abs/2508.18608)
*Janet Wang,Xin Hu,Yunbei Zhang,Diabate Almamy,Vagamon Bamba,Konan Amos Sébastien Koffi,Yao Koffi Aubin,Zhengming Ding,Jihun Hamm,Rie R. Yotsu*

Main category: cs.AI

TL;DR: 文章介绍了新的皮肤病数据集eSkinHealth及其标注框架，旨在推动全球皮肤病AI工具发展。


<details>
  <summary>Details</summary>
Motivation: 皮肤被忽视热带病带来严重负担，AI诊断支持因数据稀缺受阻，现有数据集缺乏关键信息。

Method: 引入在科特迪瓦和加纳现场收集的eSkinHealth数据集，提出AI - 专家协作范式生成多模态注释。

Result: eSkinHealth包含5,623张来自1,639个病例的图像，涵盖47种皮肤病，除患者元数据和诊断标签外，还有语义病变掩码等。

Conclusion: 工作提供了有价值的新资源和可扩展的注释框架，有助于推动更公平、准确和可解释的全球皮肤病AI工具发展。

Abstract: Skin Neglected Tropical Diseases (NTDs) impose severe health and
socioeconomic burdens in impoverished tropical communities. Yet, advancements
in AI-driven diagnostic support are hindered by data scarcity, particularly for
underrepresented populations and rare manifestations of NTDs. Existing
dermatological datasets often lack the demographic and disease spectrum crucial
for developing reliable recognition models of NTDs. To address this, we
introduce eSkinHealth, a novel dermatological dataset collected on-site in
C\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from
1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs
and rare conditions among West African populations. We further propose an
AI-expert collaboration paradigm to implement foundation language and
segmentation models for efficient generation of multimodal annotations, under
dermatologists' guidance. In addition to patient metadata and diagnosis labels,
eSkinHealth also includes semantic lesion masks, instance-specific visual
captions, and clinical concepts. Overall, our work provides a valuable new
resource and a scalable annotation framework, aiming to catalyze the
development of more equitable, accurate, and interpretable AI tools for global
dermatology.

</details>


### [12] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: 提出RLMR方法解决大语言模型创意写作中主观质量与客观约束平衡问题，经评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以平衡创意写作中主观写作质量和客观约束遵循两方面。

Method: 提出RLMR，采用动态混合奖励系统，根据采样组内写作质量动态调整约束遵循奖励权重。

Result: 在指令遵循和写作质量上均有提升，如IFEval从83.36%提升到86.65%，在WriteEval上专家手动评估胜率72.75%。

Conclusion: RLMR是首个在在线强化学习训练中将主观偏好与客观验证相结合的工作，为多维度创意写作优化提供有效方案。

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [13] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: 文章指出大语言模型基准性能与实际效用脱节，引入拟人评估范式和价值导向评估框架，分析超200个基准，指出挑战并提供开发指导，还维护开源评估资源库。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型基准性能和实际效用的脱节问题，弥补当前评估框架碎片化、重技术指标轻整体评估的不足。

Method: 引入基于人类智能视角的拟人评估范式，提出三维分类法（IQ、EQ、PQ），开创价值导向评估（VQ）框架，采用模块化架构并集成六个组件，分析超200个基准。

Result: 识别出动态评估需求和可解释性差距等关键挑战。

Conclusion: 为开发技术熟练、上下文相关且符合道德的大语言模型提供了可操作的指导。

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [14] [MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use](https://arxiv.org/abs/2508.18669)
*Weikang Zhao,Xili Wang,Chengdi Ma,Lingbin Kong,Zhaohua Yang,Mingxiang Tuo,Xiaowei Shi,Yitao Zhai,Xunliang Cai*

Main category: cs.AI

TL;DR: 提出用于智能体工具使用的MUA - RL强化学习框架，通过整合LLM模拟用户提升性能，并在多轮工具使用基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习工具使用方法在训练中缺乏真正动态用户的整合，多轮交互中用户需求的特性对智能体工具调用能力提出挑战。

Method: 引入MUA - RL框架，将LLM模拟用户集成到强化学习循环中。

Result: MUA - RL - 32B在多个多轮工具使用基准测试中取得一定分数，表现优于或匹配一些大型开源模型。

Conclusion: MUA - RL框架能使模型在动态多轮交互中自主学习与用户高效沟通并使用工具解决实际问题。

Abstract: With the recent rapid advancement of Agentic Intelligence, agentic tool use
in LLMs has become increasingly important. During multi-turn interactions
between agents and users, the dynamic, uncertain, and stochastic nature of user
demands poses significant challenges to the agent's tool invocation
capabilities. Agents are no longer expected to simply call tools to deliver a
result; rather, they must iteratively refine their understanding of user needs
through communication while simultaneously invoking tools to resolve user
queries. Existing reinforcement learning (RL) approaches for tool use lack the
integration of genuinely dynamic users during the RL training process. To
bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent
Reinforcement Learning for agentic tool use), a novel reinforcement learning
framework that, for the first time in the field of agentic tool use, integrates
LLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable
autonomous learning of models to communicate with users efficiently and use
various tools to solve practical problems in dynamic multi-turn interactions.
Evaluations are done on several multi-turn tool-using benchmarks (see Figure
1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2
Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench
Agent -- outperforming or matching the performance of larger open-source models
such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.

</details>


### [15] [AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance](https://arxiv.org/abs/2508.18689)
*Yuyang Zhao,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: 现有基于大语言模型的代理被动响应有局限，本文提出AppAgent - Pro主动式GUI代理系统，能主动整合多领域信息，有望改变信息获取方式。


<details>
  <summary>Details</summary>
Motivation: 多数现有基于大语言模型的代理以被动反应方式运行，限制了其作为通用信息获取平台的有效性和效率，需解决此局限。

Method: 提出AppAgent - Pro，这是一个基于用户指令主动整合多领域信息的主动式GUI代理系统。

Result: AppAgent - Pro可主动预测用户潜在需求，进行深度多领域信息挖掘，获取更全面智能的信息。

Conclusion: AppAgent - Pro有潜力从根本上重新定义日常生活中的信息获取，对人类社会产生深远影响。

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in addressing complex tasks, thereby enabling more advanced
information retrieval and supporting deeper, more sophisticated human
information-seeking behaviors. However, most existing agents operate in a
purely reactive manner, responding passively to user instructions, which
significantly constrains their effectiveness and efficiency as general-purpose
platforms for information acquisition. To overcome this limitation, this paper
proposes AppAgent-Pro, a proactive GUI agent system that actively integrates
multi-domain information based on user instructions. This approach enables the
system to proactively anticipate users' underlying needs and conduct in-depth
multi-domain information mining, thereby facilitating the acquisition of more
comprehensive and intelligent information. AppAgent-Pro has the potential to
fundamentally redefine information acquisition in daily life, leading to a
profound impact on human society. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be
found at:
https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.

</details>


### [16] [VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft](https://arxiv.org/abs/2508.18722)
*Honghao Fu,Junlong Ren,Qi Chai,Deheng Ye,Yujun Cai,Hao Wang*

Main category: cs.AI

TL;DR: 本文提出VistaWise框架，结合跨模态领域知识并微调目标检测模型，降低开发成本，提升智能体在开放世界任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在具身决策任务中因缺乏领域知识表现受限，而微调大规模特定领域数据成本过高。

Method: 引入VistaWise框架，将视觉和文本信息整合到跨模态知识图谱，采用基于检索的池化策略提取信息，配备桌面级技能库。

Result: VistaWise在各种开放世界任务中达到了最先进的性能。

Conclusion: VistaWise能有效降低开发成本，同时提升智能体性能。

Abstract: Large language models (LLMs) have shown significant promise in embodied
decision-making tasks within virtual open-world environments. Nonetheless,
their performance is hindered by the absence of domain-specific knowledge.
Methods that finetune on large-scale domain-specific data entail prohibitive
development costs. This paper introduces VistaWise, a cost-effective agent
framework that integrates cross-modal domain knowledge and finetunes a
dedicated object detection model for visual analysis. It reduces the
requirement for domain-specific training data from millions of samples to a few
hundred. VistaWise integrates visual information and textual dependencies into
a cross-modal knowledge graph (KG), enabling a comprehensive and accurate
understanding of multimodal environments. We also equip the agent with a
retrieval-based pooling strategy to extract task-related information from the
KG, and a desktop-level skill library to support direct operation of the
Minecraft desktop client via mouse and keyboard inputs. Experimental results
demonstrate that VistaWise achieves state-of-the-art performance across various
open-world tasks, highlighting its effectiveness in reducing development costs
while enhancing agent performance.

</details>


### [17] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: 本文介绍了用于缓解大语言模型代理AI偏差的新型偏差缓解代理，实验显示相比基线策略偏差降低81.82%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的代理AI继承了内外部信息源的偏差，影响信息公平性和用户信任，需解决这一问题。

Method: 引入新型偏差缓解代理，这是一个多智能体系统，通过专门的智能体优化信息源选择，确保检索内容相关性高且偏差小。

Result: 实验结果表明，与基线的朴素检索策略相比，偏差降低了81.82%。

Conclusion: 新型偏差缓解代理能有效减少代理AI检索信息的偏差，促进公平和平衡的知识传播。

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>


### [18] [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
*Sunguk Choi,Yonghoon Kwon,Heondeuk Lee*

Main category: cs.AI

TL;DR: 介绍了Connector - Aware Compact CoT (CAC - CoT)方法，可使大语言模型推理简洁高效，在不同任务有不错表现。


<details>
  <summary>Details</summary>
Motivation: 长思维链提示会降低大语言模型在快速直观的“系统1”任务上的性能，需要改进。

Method: 引入CAC - CoT方法，将推理限制在一小部分固定的连接短语上。

Result: 在GSM8K约达85%、GPQA（系统2）约达40%、S1 - Bench（系统1）约保留90%的性能，推理轨迹平均约300个标记，约为基线轨迹长度的三分之一。

Conclusion: CAC - CoT方法能在不损失准确性的情况下提高效率。

Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)
solve difficult problems, but very long traces often slow or even degrade
performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware
Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a
small, fixed set of connector phrases, steering the model toward concise and
well -- structured explanations. Despite its simplicity, our synthetic method
with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves
approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while
retaining approximately 90% on S1-Bench (System-1). Its reasoning traces
average approximately 300 tokens(ART), about one-third the length of baseline
traces, delivering higher efficiency without loss of accuracy.

</details>


### [19] [Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution](https://arxiv.org/abs/2508.18749)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: 现有提示优化方法有局限，本文提出REMO框架，用Qwen3 - 32B在GSM8K基准测试，结果显示比TextGrad更稳定泛化但有计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前提示优化方法无状态、独立运行，缺乏历史经验利用机制且易过拟合，需改进。

Method: 提出Reflection - Enhanced Meta - Optimization (REMO)框架，含记忆增强的Reflection RAG模块和自适应优化器。

Result: 与TextGrad基线相比，REMO在GSM8K基准测试中实现更稳定和强大的泛化，但增加了计算开销。

Conclusion: REMO框架有效，能系统积累和重用优化知识，支持持续改进。

Abstract: Recent advances in prompt optimization, exemplified by methods such as
TextGrad, enable automatic, gradient-like refinement of textual prompts to
enhance the performance of large language models (LLMs) on specific downstream
tasks. However, current approaches are typically stateless and operate
independently across optimization runs, lacking mechanisms to preserve and
leverage historical optimization experience. Furthermore, they are susceptible
to overfitting, often yielding prompt updates that generalize poorly beyond the
immediate task context.
  To address these limitations, we propose Reflection-Enhanced
Meta-Optimization (REMO), a novel framework that integrates (1) a
memory-augmented Reflection Retrieval-Augmented Generation (RAG) module -
structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer,
implemented via an LLM-driven meta-controller that synthesizes epoch-level
reflective insights to iteratively improve system-level prompting strategies.
This architecture enables not only local, fine-grained prompt tuning akin to
TextGrad, but also the systematic accumulation and reuse of cross-run
optimization knowledge, thereby supporting continual improvement over time.
  We instantiate the REMO framework using Qwen3-32B in standard inference mode
- without explicit chain-of-thought prompting - and evaluate its efficacy on
the GSM8K benchmark for mathematical reasoning. Experimental results
demonstrate that, compared to a TextGrad baseline, REMO achieves more stable
and robust generalization, albeit at the cost of increased computational
overhead. We provide a detailed exposition of the algorithmic design, conduct a
qualitative and quantitative analysis of optimization dynamics, and present a
comprehensive ablation study to elucidate the contributions of each component.

</details>


### [20] [Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction](https://arxiv.org/abs/2508.18751)
*Byung-Joon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.AI

TL;DR: 提出PAF和KIP方法解决开放集测试时间适应问题，提升了闭集准确率和开集判别能力。


<details>
  <summary>Details</summary>
Motivation: 现有开放集测试时间适应（OSTTA）方法存在过滤精度不佳、模型不稳定和误差累积问题，需改进。

Method: 提出Primary - Auxiliary Filtering (PAF)，用辅助过滤器验证主过滤器过滤的数据；提出Knowledge - Integrated Prediction (KIP)，校准适应模型、EMA模型和源模型的输出以整合互补知识。

Result: 在多种闭集和开集数据集上验证，相比现有方法提升了闭集准确率和开集判别能力。

Conclusion: 所提PAF和KIP方法有效，代码已开源。

Abstract: Deep neural networks demonstrate strong performance under aligned
training-test distributions. However, real-world test data often exhibit domain
shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the
model to test data during inference. While most TTA studies assume that the
training and test data share the same class set (closed-set TTA), real-world
scenarios often involve open-set data (open-set TTA), which can degrade
closed-set accuracy. A recent study showed that identifying open-set data
during adaptation and maximizing its entropy is an effective solution. However,
the previous method relies on the source model for filtering, resulting in
suboptimal filtering accuracy on domain-shifted test data. In contrast, we
found that the adapting model, which learns domain knowledge from noisy test
streams, tends to be unstable and leads to error accumulation when used for
filtering. To address this problem, we propose Primary-Auxiliary Filtering
(PAF), which employs an auxiliary filter to validate data filtered by the
primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP),
which calibrates the outputs of the adapting model, EMA model, and source model
to integrate their complementary knowledge for OSTTA. We validate our approach
across diverse closed-set and open-set datasets. Our method enhances both
closed-set accuracy and open-set discrimination over existing methods. The code
is available at https://github.com/powerpowe/PAF-KIP-OSTTA .

</details>


### [21] [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
*Yi Liu,Xiangyu Liu,Zequn Sun,Wei Hu*

Main category: cs.AI

TL;DR: 论文指出大推理模型面对不可答问题时无法适当弃权，分析该问题并提出结合认知监测与推理时干预的轻量级两阶段方法，实验显示可提升弃权率并维持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在面对不可答问题时无法提供适当弃权，为实现可信AI，需解决此问题。

Method: 先详细分析大推理模型面对不可答问题的响应行为，发现其认知与外部响应不一致，然后提出结合认知监测与推理时干预的轻量级两阶段方法。

Result: 实验结果表明，该方法在维持整体推理性能的同时，显著提高了弃权率。

Conclusion: 提出的方法能有效解决大推理模型在面对不可答问题时无法适当弃权的问题。

Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex
reasoning tasks. However, some questions posed to LRMs are inherently
unanswerable, such as math problems lacking sufficient conditions. We find that
LRMs continually fail to provide appropriate abstentions when confronted with
these unanswerable questions. In this paper, we systematically analyze,
investigate, and resolve this issue for trustworthy AI. We first conduct a
detailed analysis of the distinct response behaviors of LRMs when facing
unanswerable questions. Then, we show that LRMs possess sufficient cognitive
capabilities to recognize the flaws in these questions. However, they fail to
exhibit appropriate abstention behavior, revealing a misalignment between their
internal cognition and external response. Finally, to resolve this issue, we
propose a lightweight, two-stage method that combines cognitive monitoring with
inference-time intervention. Experimental results demonstrate that our method
significantly improves the abstention rate while maintaining the overall
reasoning performance.

</details>


### [22] [Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units](https://arxiv.org/abs/2508.18763)
*Chao Hao,Zezheng Wang,Yanhua Huang,Ruiwen Xu,Wenzhe Niu,Xin Liu,Zitong Yu*

Main category: cs.AI

TL;DR: 本文通过标记级多模型协作提升语言模型推理能力，提出动态选择策略和最小完整语义单元概念，实验证明方法优越性。


<details>
  <summary>Details</summary>
Motivation: 提升语言模型的推理能力，解决多模型协作中词汇不对齐的关键挑战。

Method: 从多个模型提供的下一个标记分布中选择最优标记进行自回归推理，引入基于分布距离的动态选择策略（DDS），提出最小完整语义单元（MCSU）概念。

Result: 在各种基准测试中，实验结果证明了该方法的优越性。

Conclusion: 通过标记级多模型协作、DDS策略和MCSU概念能有效提升语言模型推理能力。

Abstract: This paper investigates the enhancement of reasoning capabilities in language
models through token-level multi-model collaboration. Our approach selects the
optimal tokens from the next token distributions provided by multiple models to
perform autoregressive reasoning. Contrary to the assumption that more models
yield better results, we introduce a distribution distance-based dynamic
selection strategy (DDS) to optimize the multi-model collaboration process. To
address the critical challenge of vocabulary misalignment in multi-model
collaboration, we propose the concept of minimal complete semantic units
(MCSU), which is simple yet enables multiple language models to achieve natural
alignment within the linguistic space. Experimental results across various
benchmarks demonstrate the superiority of our method. The code will be
available at https://github.com/Fanye12/DDS.

</details>


### [23] [AniME: Adaptive Multi-Agent Planning for Long Animation Generation](https://arxiv.org/abs/2508.18781)
*Lisai Zhang,Baohan Xu,Siqian Yang,Mingyu Yin,Jing Liu,Chao Xu,Siqi Wang,Yidi Wu,Yuxin Hong,Zihao Zhang,Yanzhang Liang,Yudong Jiang*

Main category: cs.AI

TL;DR: 提出面向导演的多智能体系统AniME用于自动化长篇动画制作，覆盖全流程，能生成有一致角色和音画同步元素的动画。


<details>
  <summary>Details</summary>
Motivation: 实现自动化长篇动画制作，提供AI驱动动画创作的可扩展解决方案。

Method: 导演智能体保留全局记忆并协调下游专业智能体，专业智能体通过集成定制的模型上下文协议与下游模型指令，自适应选择不同子任务的控制条件。

Result: AniME能生成具有一致角色和同步音视频元素的电影级动画。

Conclusion: AniME为AI驱动的动画创作提供了可扩展的解决方案。

Abstract: We present AniME, a director-oriented multi-agent system for automated
long-form anime production, covering the full workflow from a story to the
final video. The director agent keeps a global memory for the whole workflow,
and coordinates several downstream specialized agents. By integrating
customized Model Context Protocol (MCP) with downstream model instruction, the
specialized agent adaptively selects control conditions for diverse sub-tasks.
AniME produces cinematic animation with consistent characters and synchronized
audio visual elements, offering a scalable solution for AI-driven anime
creation.

</details>


### [24] [CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks](https://arxiv.org/abs/2508.18797)
*Qi Chai,Zhang Zheng,Junlong Ren,Deheng Ye,Zichuan Lin,Hao Wang*

Main category: cs.AI

TL;DR: 提出CausalMACE框架提升Minecraft多智能体系统性能，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有Minecraft单智能体处理复杂任务有低效和容错性差问题，多智能体协作研究少。

Method: 提出CausalMACE框架，含全局任务规划的任务图模块和基于因果关系的依赖管理模块，采用固有规则进行因果干预。

Result: 在Minecraft多智能体协作任务中取得了SOTA性能。

Conclusion: CausalMACE框架能有效提升Minecraft多智能体系统性能。

Abstract: Minecraft, as an open-world virtual interactive environment, has become a
prominent platform for research on agent decision-making and execution.
Existing works primarily adopt a single Large Language Model (LLM) agent to
complete various in-game tasks. However, for complex tasks requiring lengthy
sequences of actions, single-agent approaches often face challenges related to
inefficiency and limited fault tolerance. Despite these issues, research on
multi-agent collaboration remains scarce. In this paper, we propose CausalMACE,
a holistic causality planning framework designed to enhance multi-agent
systems, in which we incorporate causality to manage dependencies among
subtasks. Technically, our proposed framework introduces two modules: an
overarching task graph for global task planning and a causality-based module
for dependency management, where inherent rules are adopted to perform causal
intervention. Experimental results demonstrate our approach achieves
state-of-the-art performance in multi-agent cooperative tasks of Minecraft.

</details>


### [25] [STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning](https://arxiv.org/abs/2508.18812)
*Chenghao Wu,Ruiyang Ren,Junjie Zhang,Ruirui Wang,Zhongrui Ma,Qi Ye,Wayne Xin Zhao*

Main category: cs.AI

TL;DR: 提出STARec框架，让推荐系统有自主推理能力，实验显示其在少量数据下比基线表现好。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统受静态用户建模和被动决策范式限制，基于大语言模型的代理存在启发式模式匹配的缺点，需要改进。

Method: 引入STARec框架，将用户建模为有并行认知的代理，开发锚定强化训练，结合知识蒸馏和奖励塑造。

Result: 在MovieLens 1M和Amazon CDs基准测试中，STARec仅用0.4%的全量训练数据就比现有基线有显著性能提升。

Conclusion: STARec框架有效，能提升推荐系统性能。

Abstract: While modern recommender systems are instrumental in navigating information
abundance, they remain fundamentally limited by static user modeling and
reactive decision-making paradigms. Current large language model (LLM)-based
agents inherit these shortcomings through their overreliance on heuristic
pattern matching, yielding recommendations prone to shallow correlation bias,
limited causal inference, and brittleness in sparse-data scenarios. We
introduce STARec, a slow-thinking augmented agent framework that endows
recommender systems with autonomous deliberative reasoning capabilities. Each
user is modeled as an agent with parallel cognitions: fast response for
immediate interactions and slow reasoning that performs chain-of-thought
rationales. To cultivate intrinsic slow thinking, we develop anchored
reinforcement training - a two-stage paradigm combining structured knowledge
distillation from advanced reasoning models with preference-aligned reward
shaping. This hybrid approach scaffolds agents in acquiring foundational
capabilities (preference summarization, rationale generation) while enabling
dynamic policy adaptation through simulated feedback loops. Experiments on
MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves
substantial performance gains compared with state-of-the-art baselines, despite
using only 0.4% of the full training data.

</details>


### [26] [Judicial Requirements for Generative AI in Legal Reasoning](https://arxiv.org/abs/2508.18880)
*Eljas Linna,Tuula Linna*

Main category: cs.AI

TL;DR: 论文探讨大语言模型在司法决策中作为可靠推理工具的核心能力，用IRAC模型分析法律裁决挑战阶段，评估AI增强机制，指出虽能解决特定问题但仍有挑战，认为AI在法律领域应扮演双重角色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型被应用于专业领域，但在法律等高风险领域的局限性尚不清楚，需明确其在司法决策中作为可靠推理工具的核心能力。

Method: 以IRAC模型为分析框架，解构法律推理的核心要求，将多种AI增强机制映射到这些要求上进行评估。

Result: AI增强机制能解决特定挑战，但在需要自由裁量和透明、合理推理的任务上仍存在重大挑战。

Conclusion: 目前AI在法律领域最有效的角色是处理简单重复案件的高容量助手和复杂事务中人类专家的“陪练伙伴”。

Abstract: Large Language Models (LLMs) are being integrated into professional domains,
yet their limitations in high-stakes fields like law remain poorly understood.
This paper defines the core capabilities that an AI system must possess to
function as a reliable reasoning tool in judicial decision-making. Using the
IRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the
study focuses on the most challenging phases of legal adjudication: determining
the applicable Rule (R) and performing the Application (A) of that rule to the
facts of a case. From a judicial perspective, the analysis deconstructs legal
reasoning into a series of core requirements, including the ability to select
the correct legal framework across jurisdictions, generate sound arguments
based on the doctrine of legal sources, distinguish ratio decidendi from obiter
dictum in case law, resolve ambiguity arising from general clauses like
"reasonableness", manage conflicting legal provisions, and correctly apply the
burden of proof. The paper then maps various AI enhancement mechanisms, such as
Retrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic
AI, to these requirements, assessing their potential to bridge the gap between
the probabilistic nature of LLMs and the rigorous, choice-driven demands of
legal interpretation. The findings indicate that while these techniques can
address specific challenges, significant challenges remain, particularly in
tasks requiring discretion and transparent, justifiable reasoning. Our paper
concludes that the most effective current role for AI in law is a dual one: as
a high-volume assistant for simple, repetitive cases and as a sophisticated
"sparring partner" for human experts in complex matters.

</details>


### [27] [Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks](https://arxiv.org/abs/2508.18905)
*Dimitrios Rontogiannis,Maxime Peyrard,Nicolas Baldwin,Martin Josifoski,Robert West,Dimitrios Gunopulos*

Main category: cs.AI

TL;DR: 提出新交互式评估框架评估大语言模型在多需求编程任务上的能力，强调动态评估对协作代码生成代理发展的重要性。


<details>
  <summary>Details</summary>
Motivation: 标准单轮静态基准测试无法有效评估大语言模型在软件工程等复杂任务上的细微能力。

Method: 提出通过结构化、反馈驱动对话评估大语言模型的交互式评估框架，将任务建模为需求依赖图，用面试官模型提供提示，在DevAI基准上添加真实解决方案并通过专家注释评估提示。

Result: 框架能对模型行为进行细粒度诊断，发现静态基准无法衡量的优势和系统性弱点。

Conclusion: 动态评估对推进协作代码生成代理的发展很重要。

Abstract: Standard single-turn, static benchmarks fall short in evaluating the nuanced
capabilities of Large Language Models (LLMs) on complex tasks such as software
engineering. In this work, we propose a novel interactive evaluation framework
that assesses LLMs on multi-requirement programming tasks through structured,
feedback-driven dialogue. Each task is modeled as a requirement dependency
graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides
minimal, targeted hints to an ``interviewee'' model to help correct errors and
fulfill target constraints. This dynamic protocol enables fine-grained
diagnostic insights into model behavior, uncovering strengths and systematic
weaknesses that static benchmarks fail to measure. We build on DevAI, a
benchmark of 55 curated programming tasks, by adding ground-truth solutions and
evaluating the relevance and utility of interviewer hints through expert
annotation. Our results highlight the importance of dynamic evaluation in
advancing the development of collaborative code-generating agents.

</details>


### [28] [FormaRL: Enhancing Autoformalization with no Labeled Data](https://arxiv.org/abs/2508.18914)
*Yanxing Huang,Xinling Jin,Sijie Liang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 提出用于自动形式化的FormaRL框架，只需少量无标签数据，实验表明能大幅提升自动形式化准确率，代码开源。


<details>
  <summary>Details</summary>
Motivation: 自动形式化因数据稀缺和缺乏有效方法发展受阻。

Method: 提出FormaRL框架，结合Lean编译器语法检查和大语言模型一致性检查计算奖励，采用GRPO算法更新形式化器，还整理了uproof数据集。

Result: FormaRL能将Qwen2.5 - Coder - 7B - Instruct的pass@1自动形式化准确率提升4 - 6倍，在uproof上分布外性能也有显著提升。

Conclusion: FormaRL是一个简单有效的自动形式化强化学习框架，能在少量无标签数据下提升性能。

Abstract: Autoformalization is one of the central tasks in formal verification, while
its advancement remains hindered due to the data scarcity and the absence
efficient methods. In this work we propose \textbf{FormaRL}, a simple yet
efficient reinforcement learning framework for autoformalization which only
requires a small amount of unlabeled data. FormaRL integrates syntax check from
Lean compiler and consistency check from large language model to calculate the
reward, and adopts GRPO algorithm to update the formalizer. We also curated a
proof problem dataset from undergraduate-level math materials, named
\textbf{uproof}, in the hope to facilitate the exploration of autoformalization
and theorem proving in advanced math. Experiments show that FormaRL can
increase the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by
4 $\sim$ 6x (4.04\% $\to$ 26.15\% on ProofNet and 2.4\% $\to$ 9.6\% on uproof)
with merely 859 unlabeled data. And on uproof our method also achieved a strong
improvement in out-of-distribution performance compared to existing open-source
state-of-the-art autoformalizers on both pass@1 accuracy (6.2\% $\to$ 9.6\%)
and pass@16 accuracy (24.4\% $\to$ 33.6\%). Training code of FormaRL is
open-sourced at https://github.com/THUNLP-MT/FormaRL.

</details>


### [29] [Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems](https://arxiv.org/abs/2508.18925)
*Qian Xiao,Conn Breathnach,Ioana Ghergulescu,Conor O'Sullivan,Keith Johnston,Vincent Wade*

Main category: cs.AI

TL;DR: 研究引入CTGraph方法对学习者行为和表现进行自监督式分析，可提供学生学习全貌、识别困难学生并助力精准干预。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统的使用可能加剧学生成绩差距，学生画像对跟踪进度、识别困难学生和减少差距至关重要。

Method: 引入CTGraph，一种图级表示学习方法，以自监督方式对学习者行为和表现进行画像。

Result: CTGraph能提供学生学习全貌，考虑多方面行为和表现及学习路径差异，可识别困难学生并进行比较分析。

Conclusion: 该方法为教育者提供学生学习洞察，为精准干预铺平道路。

Abstract: The surge in the adoption of Intelligent Tutoring Systems (ITSs) in
education, while being integral to curriculum-based learning, can inadvertently
exacerbate performance gaps. To address this problem, student profiling becomes
crucial for tracking progress, identifying struggling students, and alleviating
disparities among students. Such profiling requires measuring student behaviors
and performance across different aspects, such as content coverage, learning
intensity, and proficiency in different concepts within a learning topic.
  In this study, we introduce CTGraph, a graph-level representation learning
approach to profile learner behaviors and performance in a self-supervised
manner. Our experiments demonstrate that CTGraph can provide a holistic view of
student learning journeys, accounting for different aspects of student
behaviors and performance, as well as variations in their learning paths as
aligned to the curriculum structure. We also show that our approach can
identify struggling students and provide comparative analysis of diverse groups
to pinpoint when and where students are struggling. As such, our approach opens
more opportunities to empower educators with rich insights into student
learning journeys and paves the way for more targeted interventions.

</details>


### [30] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: 本文提出用于漏洞检测的统一框架VISION，通过反事实训练数据集缓解虚假关联，提升检测性能并发布基准数据集，推动可解释的AI网络安全系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的源代码漏洞检测方法受训练数据不平衡和标签噪声影响，学习到虚假关联，泛化能力差。

Method: 提出VISION框架，包括用大语言模型生成反事实样本、对相反标签的代码对进行有针对性的GNN训练、基于图的可解释性分析。

Result: VISION减少了虚假学习，提高了检测的鲁棒性和泛化能力，在CWE - 20漏洞上提升了多项准确率，通过提出的指标也证明有改进，还发布了CWE - 20 - CFA基准数据集。

Conclusion: VISION推动了基于AI的透明可信网络安全系统发展，可通过交互式可视化进行人工分析。

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


### [31] [Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method](https://arxiv.org/abs/2508.18953)
*I. I. Priezzhev,D. A. Danko,A. V. Shubin*

Main category: cs.AI

TL;DR: 现代神经网络有局限，文章提出基于层次聚类结构的最近邻方法，实验证实有效，有高可靠性和可解释性优势。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络存在幻觉效应、高计算复杂度等局限，阻碍其在关键领域应用，因此需寻找替代方法。

Method: 提出基于层次聚类结构的最近邻方法，用k - 最近邻算法减少幻觉效应，利用基于Kohonen自组织映射的树状数据结构加速最近邻搜索。

Result: 在手写数字识别和简单字幕翻译任务测试中，该方法使最近邻搜索时间相比穷举法减少数百倍，仅精度略有降低。

Conclusion: 该方法具有透明性、可解释性，与人类认知机制相符，在需要高可靠性和可解释结果的任务中有广泛应用潜力。

Abstract: Modern neural network technologies, including large language models, have
achieved remarkable success in various applied artificial intelligence
applications, however, they face a range of fundamental limitations. Among them
are hallucination effects, high computational complexity of training and
inference, costly fine-tuning, and catastrophic forgetting issues. These
limitations significantly hinder the use of neural networks in critical areas
such as medicine, industrial process management, and scientific research. This
article proposes an alternative approach based on the nearest neighbors method
with hierarchical clustering structures. Employing the k-nearest neighbors
algorithm significantly reduces or completely eliminates hallucination effects
while simplifying model expansion and fine-tuning without the need for
retraining the entire network. To overcome the high computational load of the
k-nearest neighbors method, the paper proposes using tree-like data structures
based on Kohonen self-organizing maps, thereby greatly accelerating nearest
neighbor searches. Tests conducted on handwritten digit recognition and simple
subtitle translation tasks confirmed the effectiveness of the proposed
approach. With only a slight reduction in accuracy, the nearest neighbor search
time was reduced hundreds of times compared to exhaustive search methods. The
proposed method features transparency and interpretability, closely aligns with
human cognitive mechanisms, and demonstrates potential for extensive use in
tasks requiring high reliability and explainable results.

</details>


### [32] [Enabling MoE on the Edge via Importance-Driven Expert Scheduling](https://arxiv.org/abs/2508.18983)
*Guoying Zhu,Meng Li,Haipeng Dai,Xuechen Liu,Weijun Wang,Keran Li,Jun xiao,Ligeng Chen,Wei Wang*

Main category: cs.AI

TL;DR: 本文提出利用专家重要性指导决策及新调度策略，在边缘硬件部署MoE，降低解码延迟，提高缓存命中率并保持精度。


<details>
  <summary>Details</summary>
Motivation: 在消费者级边缘硬件上部署MoE受设备内存限制，需进行动态专家卸载。

Method: 利用专家重要性指导决策，用缓存中功能相似的专家替代低重要性激活专家；引入最大化GPU缓存专家重用率的调度策略。

Result: 实现48%的解码延迟降低，专家缓存命中率超60%，且精度近乎无损。

Conclusion: 所提方法在边缘硬件上部署MoE时能有效降低内存使用、数据传输和PCIe开销，提高效率。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a key technique for
scaling Large Language Models by activating only a subset of experts per query.
Deploying MoE on consumer-grade edge hardware, however, is constrained by
limited device memory, making dynamic expert offloading essential. Unlike prior
work that treats offloading purely as a scheduling problem, we leverage expert
importance to guide decisions, substituting low-importance activated experts
with functionally similar ones already cached in GPU memory, thereby preserving
accuracy. As a result, this design reduces memory usage and data transfer,
while largely eliminating PCIe overhead. In addition, we introduce a scheduling
policy that maximizes the reuse ratio of GPU-cached experts, further boosting
efficiency. Extensive evaluations show that our approach delivers 48% lower
decoding latency with over 60% expert cache hit rate, while maintaining nearly
lossless accuracy.

</details>


### [33] [AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms](https://arxiv.org/abs/2508.19004)
*Pontus Strimling,Simon Karlsson,Irina Vartanova,Kimmo Eriksson*

Main category: cs.AI

TL;DR: 研究大语言模型能否仅通过统计学习获得社会规范理解，发现模型能在预测人类社会适宜性判断上超多数人类，但存在系统误差。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否仅通过统计学习实现复杂的规范理解，挑战强调具身体验对文化能力必要性的理论。

Method: 通过两项研究，系统评估多个AI系统预测555个日常场景中人类社会适宜性判断的能力。

Result: GPT - 4.5、Gemini 2.5 Pro、GPT - 5、Claude Sonnet 4在预测集体判断上超越多数人类，但所有模型都有系统相关误差。

Conclusion: 仅基于语言数据的统计学习可产生复杂社会认知模型，语言是文化知识传播的丰富载体，不同架构AI的局限性显示了基于模式的社会理解的潜在边界。

Abstract: A fundamental question in cognitive science concerns how social norms are
acquired and represented. While humans typically learn norms through embodied
social experience, we investigated whether large language models can achieve
sophisticated norm understanding through statistical learning alone. Across two
studies, we systematically evaluated multiple AI systems' ability to predict
human social appropriateness judgments for 555 everyday scenarios by examining
how closely they predicted the average judgment compared to each human
participant. In Study 1, GPT-4.5's accuracy in predicting the collective
judgment on a continuous scale exceeded that of every human participant (100th
percentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7%
of humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive
power, all models showed systematic, correlated errors. These findings
demonstrate that sophisticated models of social cognition can emerge from
statistical learning over linguistic data alone, challenging strong versions of
theories emphasizing the exclusive necessity of embodied experience for
cultural competence. The systematic nature of AI limitations across different
architectures indicates potential boundaries of pattern-based social
understanding, while the models' ability to outperform nearly all individual
humans in this predictive task suggests that language serves as a remarkably
rich repository for cultural knowledge transmission.

</details>


### [34] [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
*Yuxuan Cai,Yipeng Hao,Jie Zhou,Hang Yan,Zhikai Lei,Rui Zhen,Zhenhua Han,Yutao Yang,Junsong Li,Qianjun Pan,Tianyu Huai,Qin Chen,Xin Li,Kai Chen,Bo Zhang,Xipeng Qiu,Liang He*

Main category: cs.AI

TL;DR: 本文介绍了经验驱动的终身学习（ELL）框架及基准数据集StuLife，探讨上下文工程在AGI中的作用。


<details>
  <summary>Details</summary>
Motivation: 随着AI向通用智能发展，需创建能持续学习的开放式智能体，因此提出ELL框架。

Method: 提出ELL框架，包含经验探索、长期记忆、技能学习和知识内化四个核心原则；引入StuLife数据集模拟学生大学生活各阶段，基于三个范式转变设计。

Result: StuLife为评估终身学习能力提供综合平台，可评估记忆保留、技能迁移和自我激励行为等。

Conclusion: 除在StuLife基准上评估SOTA大语言模型，还可探索上下文工程对AGI发展的作用。

Abstract: As AI advances toward general intelligence, the focus is shifting from
systems optimized for static tasks to creating open-ended agents that learn
continuously. In this paper, we introduce Experience-driven Lifelong Learning
(ELL), a framework for building self-evolving agents capable of continuous
growth through real-world interaction. The framework is built on four core
principles: (1) Experience Exploration: Agents learn through continuous,
self-motivated interaction with dynamic environments, navigating interdependent
tasks and generating rich experiential trajectories. (2) Long-term Memory:
Agents preserve and structure historical knowledge, including personal
experiences, domain expertise, and commonsense reasoning, into a persistent
memory system. (3) Skill Learning: Agents autonomously improve by abstracting
recurring patterns from experience into reusable skills, which are actively
refined and validated for application in new tasks. (4) Knowledge
Internalization: Agents internalize explicit and discrete experiences into
implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a
student's holistic college journey, from enrollment to academic and personal
development, across three core phases and ten detailed sub-scenarios. StuLife
is designed around three key paradigm shifts: From Passive to Proactive, From
Context to Memory, and From Imitation to Learning. In this dynamic environment,
agents must acquire and distill practical skills and maintain persistent memory
to make decisions based on evolving state variables. StuLife provides a
comprehensive platform for evaluating lifelong learning capabilities, including
memory retention, skill transfer, and self-motivated behavior. Beyond
evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of
context engineering in advancing AGI.

</details>


### [35] [Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI](https://arxiv.org/abs/2508.19008)
*Marcin Moskalewicz,Anna Sterna,Marek Pokropski,Paula Flores*

Main category: cs.AI

TL;DR: 研究评估大语言模型支持边缘性人格障碍第一人称体验现象学定性分析的能力，对比三个模型，结果显示与人类分析有不同重叠，Gemini表现最佳，凸显AI增强主题分析的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型支持边缘性人格障碍第一人称体验现象学定性分析的能力。

Method: 基于24名住院患者生活故事访谈的人类主题分析，让三个大语言模型模仿原研究者的解释风格，由专家评估，采用语义一致性、Jaccard系数和多维有效性评级等评估方法。

Result: 与人类分析的重叠从GPT的0%到Claude的42%和Gemini的58%，Jaccard系数低（0.21 - 0.28），模型找回人类遗漏的主题，Gemini输出最接近人类分析，有效性得分显著高于GPT和Claude，被盲测专家判断为人类输出，所有分数与文本数量和每个主题的单词数强相关。

Conclusion: AI增强的主题分析有潜力减轻人类解释偏差，但结果存在变异性。

Abstract: This study examines the capacity of large language models (LLMs) to support
phenomenological qualitative analysis of first-person experience in Borderline
Personality Disorder (BPD), understood as a disorder of temporality and
selfhood. Building on a prior human-led thematic analysis of 24 inpatients'
life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5
Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the
original investigators. The models were evaluated with blinded and non-blinded
expert judges in phenomenology and clinical psychology. Assessments included
semantic congruence, Jaccard coefficients, and multidimensional validity
ratings (credibility, coherence, substantiveness, and groundness in data).
Results showed variable overlap with the human analysis, from 0 percent in GPT
to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient
(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's
output most closely resembled the human analysis, with validity scores
significantly higher than GPT and Claude (p < 0.0001), and was judged as human
by blinded experts. All scores strongly correlated (R > 0.78) with the quantity
of text and words per theme, highlighting both the variability and potential of
AI-augmented thematic analysis to mitigate human interpretative bias.

</details>


### [36] [MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP](https://arxiv.org/abs/2508.19014)
*Surajit Das,Gourav Roy,Aleksei Eliseev,Ram Kumar Rajendran*

Main category: cs.AI

TL;DR: 研究提出基于强化学习的APME框架估计问题难度，经实证验证性能良好，优于基线方法，对IATS有推动作用。


<details>
  <summary>Details</summary>
Motivation: 技术和教育发展使IATS出现，传统方法有局限性，需要客观且领域无关的问题难度确定方法。

Method: 引入基于强化学习的多臂老虎机框架APME，仅依据求解者表现数据，利用变异系数的倒数作为风险调整指标。

Result: 在三个不同数据集上平均R2为0.9213，平均RMSE为0.0584，优于基线方法，尤其在纯符号领域。

Conclusion: 项目异质性影响难度感知，求解结果方差对自适应分配很关键，该方法推动了IATS中的难度标记，可扩展到其他有求解交互数据的领域。

Abstract: The evolution of technology and education is driving the emergence of
Intelligent & Autonomous Tutoring Systems (IATS), where objective and
domain-agnostic methods for determining question difficulty are essential.
Traditional human labeling is subjective, and existing NLP-based approaches
fail in symbolic domains like algebra. This study introduces the Approach of
Passive Measures among Educands (APME), a reinforcement learning-based
Multi-Armed Bandit (MAB) framework that estimates difficulty solely from solver
performance data -- marks obtained and time taken -- without requiring
linguistic features or expert labels. By leveraging the inverse coefficient of
variation as a risk-adjusted metric, the model provides an explainable and
scalable mechanism for adaptive assessment. Empirical validation was conducted
on three heterogeneous datasets. Across these diverse contexts, the model
achieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its
robustness, accuracy, and adaptability to different educational levels and
assessment formats. Compared with baseline approaches-such as regression-based,
NLP-driven, and IRT models-the proposed framework consistently outperformed
alternatives, particularly in purely symbolic domains. The findings highlight
that (i) item heterogeneity strongly influences perceived difficulty, and (ii)
variance in solver outcomes is as critical as mean performance for adaptive
allocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal
Development by identifying tasks that balance challenge and attainability,
supporting motivation while minimizing disengagement. This domain-agnostic,
self-supervised approach advances difficulty tagging in IATS and can be
extended beyond algebra wherever solver interaction data is available

</details>


### [37] [Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction](https://arxiv.org/abs/2508.19035)
*Congchi Yin,Tianyi Wu,Yankai Shu,Alex Gu,Yunhan Wang,Jun Shao,Xun Jiang,Piji Li*

Main category: cs.AI

TL;DR: 现有任务在评估大语言模型推理能力上存在不足，引入黑盒交互评估范式及Oracle基准测试，对19个模型测试，发现模型普遍缺乏高级规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有任务无法在交互式、未知环境中评估大语言模型推理能力，且孤立评估推理类型，忽略综合推理过程。

Method: 引入黑盒交互评估范式，构建包含6种黑盒任务和96个黑盒的Oracle基准测试，对19个大语言模型进行测试。

Result: o3在6个任务中的5个排名第一，在多数简单黑盒任务上准确率超70%，但在一些困难任务上平均表现低于40%。

Conclusion: 大语言模型普遍缺乏高级规划能力，难以制定高效自适应的探索策略进行假设细化。

Abstract: Existing tasks fall short in evaluating reasoning ability of Large Language
Models (LLMs) in an interactive, unknown environment. This deficiency leads to
the isolated assessment of deductive, inductive, and abductive reasoning,
neglecting the integrated reasoning process that is indispensable for humans
discovery of real world. We introduce a novel evaluation paradigm,
\textit{black-box interaction}, to tackle this challenge. A black-box is
defined by a hidden function that maps a specific set of inputs to outputs.
LLMs are required to unravel the hidden function behind the black-box by
interacting with it in given exploration turns, and reasoning over observed
input-output pairs. Leveraging this idea, we build the \textsc{Oracle}
benchmark which comprises 6 types of black-box task and 96 black-boxes. 19
modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over
70\% accuracy on most easy black-boxes. But it still struggles with some hard
black-box tasks, where its average performance drops below 40\%. Further
analysis indicates a universal difficulty among LLMs: They lack the high-level
planning capability to develop efficient and adaptive exploration strategies
for hypothesis refinement.

</details>


### [38] [A Concurrent Modular Agent: Framework for Autonomous LLM Agents](https://arxiv.org/abs/2508.19042)
*Norihiro Maruyama,Takahide Yoshida,Hiroki Sato,Atsushi Masumori,Johnsmith,Takashi Ikegami*

Main category: cs.AI

TL;DR: 介绍并发模块化代理（CMA）框架，可协调多个基于大语言模型的模块异步运行，通过用例展示其可行性，支持心智社会理论。


<details>
  <summary>Details</summary>
Motivation: 解决代理架构中长期存在的难题，实现灵活、自适应和上下文相关的行为。

Method: 引入CMA框架，通过模块并发执行、将推理任务交给大语言模型、模块间通信和共享全局状态来实现。

Result: 通过两个实际用例研究证明了系统的可行性，观察到的涌现特性支持心智社会概念。

Conclusion: 该方法是明斯基心智社会理论的实际实现，为人工智能研究开辟了新途径。

Abstract: We introduce the Concurrent Modular Agent (CMA), a framework that
orchestrates multiple Large-Language-Model (LLM)-based modules that operate
fully asynchronously yet maintain a coherent and fault-tolerant behavioral
loop. This framework addresses long-standing difficulties in agent
architectures by letting intention emerge from language-mediated interactions
among autonomous processes. This approach enables flexible, adaptive, and
context-dependent behavior through the combination of concurrently executed
modules that offload reasoning to an LLM, inter-module communication, and a
single shared global state.We consider this approach to be a practical
realization of Minsky's Society of Mind theory. We demonstrate the viability of
our system through two practical use-case studies. The emergent properties
observed in our system suggest that complex cognitive phenomena like
self-awareness may indeed arise from the organized interaction of simpler
processes, supporting Minsky-Society of Mind concept and opening new avenues
for artificial intelligence research. The source code for our work is available
at: https://github.com/AlternativeMachine/concurrent-modular-agent.

</details>


### [39] [Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty](https://arxiv.org/abs/2508.19069)
*Zhichao Yang,Zhaoxin Fan,Gen Li,Yuanze Hu,Xinyu Wang,Ye Qiu,Xin Wang,Yifan Sun,Wenjun Wu*

Main category: cs.AI

TL;DR: 论文研究大语言模型在结构化、程序性推理上的局限，提出SST框架，实验表明该框架能提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法在复杂任务中难以捕捉深层次程序性逻辑，需提升大语言模型结构化、程序性推理能力。

Method: 提出SST框架，包括用结构化解决方案模板链和动态加权损失微调、推理时注入解决方案模板、集成课程微调。

Result: 在GSM8K、AIME24和新Dynamic En基准测试中，SST显著提高准确性和效率，尤其在难题上。

Conclusion: SST框架能有效提升大语言模型的程序性推理能力。

Abstract: Structured, procedural reasoning is essential for Large Language Models
(LLMs), especially in mathematics. While post-training methods have improved
LLM performance, they still fall short in capturing deep procedural logic on
complex tasks. To tackle the issue, in this paper, we first investigate this
limitation and uncover a novel finding: a Scaling Law by Difficulty, which
reveals that model performance follows a U-shaped curve with respect to
training data complexity -- excessive low-difficulty data impedes abstraction,
while high-difficulty data significantly enhances reasoning ability. Motivated
by this, we propose the Structured Solution Template (SST) framework, which
uses solution templates and a curriculum of varied difficulty to explicitly
teach procedural reasoning. Specifically, SST comprises (1) fine-tuning with
structured solution-template chains and dynamically weighted loss to prioritize
procedural logic, (2) prompt-time injection of solution templates as cognitive
scaffolds to guide inference, and (3) integrated curriculum fine-tuning that
explicitly teaches the model to self-plan - execute - self-correct. Experiments
on GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly
improves both accuracy and efficiency, especially on harder problems.

</details>


### [40] [Trustworthy Agents for Electronic Health Records through Confidence Estimation](https://arxiv.org/abs/2508.19096)
*Yongwoo Song,Minbyul Jeong,Mujeen Sung*

Main category: cs.AI

TL;DR: 提出HCAcc@k%指标和TrustEHRAgent，实验表明TrustEHRAgent在严格可靠性约束下优于基线方法，凸显传统指标局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在电子健康记录信息提取和临床决策支持部署时面临幻觉风险挑战，需新指标与方法。

Method: 提出HCAcc@k%指标量化准确性 - 可靠性权衡，引入TrustEHRAgent进行临床问答及逐步置信度估计。

Result: 在MIMIC - III和eICU数据集上，TrustEHRAgent在HCAcc@70%时比基线方法分别提升44.23%p和25.34%p，基线方法失败。

Conclusion: 传统准确性指标评估医疗AI代理有局限性，工作有助于开发可靠临床代理。

Abstract: Large language models (LLMs) show promise for extracting information from
Electronic Health Records (EHR) and supporting clinical decisions. However,
deployment in clinical settings faces challenges due to hallucination risks. We
propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric
quantifying the accuracy-reliability trade-off at varying confidence
thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating
stepwise confidence estimation for clinical question answering. Experiments on
MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under
strict reliability constraints, achieving improvements of 44.23%p and 25.34%p
at HCAcc@70% while baseline methods fail at these thresholds. These results
highlight limitations of traditional accuracy metrics in evaluating healthcare
AI agents. Our work contributes to developing trustworthy clinical agents that
deliver accurate information or transparently express uncertainty when
confidence is low.

</details>


### [41] [Reasoning LLMs in the Medical Domain: A Literature Survey](https://arxiv.org/abs/2508.19097)
*Armin Berger,Sarthak Khanna,David Berghaus,Rafet Sifa*

Main category: cs.AI

TL;DR: 本文探讨大语言模型在医疗应用中的发展，分析技术基础、评估方法和现存挑战，旨在为开发可靠大语言模型制定路线图。


<details>
  <summary>Details</summary>
Motivation: 大语言模型先进推理能力在医疗应用中有变革性发展，需研究其从信息检索工具到临床推理系统的转变。

Method: 分析使能技术基础，关注特定提示技术和强化学习突破，评估医疗框架和新兴范式。

Result: 对医疗大语言模型的技术基础、评估方法等进行了分析，指出了现存挑战。

Conclusion: 要为开发能用于临床实践和医学研究的可靠大语言模型制定路线图。

Abstract: The emergence of advanced reasoning capabilities in Large Language Models
(LLMs) marks a transformative development in healthcare applications. Beyond
merely expanding functional capabilities, these reasoning mechanisms enhance
decision transparency and explainability-critical requirements in medical
contexts. This survey examines the transformation of medical LLMs from basic
information retrieval tools to sophisticated clinical reasoning systems capable
of supporting complex healthcare decisions. We provide a thorough analysis of
the enabling technological foundations, with a particular focus on specialized
prompting techniques like Chain-of-Thought and recent breakthroughs in
Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates
purpose-built medical frameworks while also examining emerging paradigms such
as multi-agent collaborative systems and innovative prompting architectures.
The survey critically assesses current evaluation methodologies for medical
validation and addresses persistent challenges in field interpretation
limitations, bias mitigation strategies, patient safety frameworks, and
integration of multimodal clinical data. Through this survey, we seek to
establish a roadmap for developing reliable LLMs that can serve as effective
partners in clinical practice and medical research.

</details>


### [42] [Hybrid Deep Searcher: Integrating Parallel and Sequential Search Reasoning](https://arxiv.org/abs/2508.19113)
*Dayoon Ko,Jihyuk Kim,Haeju Park,Sohyeon Kim,Dahyun Lee,Yongrae Jo,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: 提出HDS - QA数据集训练LRMs区分并行和顺序查询，微调得到HybridDeepSearcher模型，在多基准测试中表现优于基线，减少推理延迟且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有增强LRMs的顺序查询方法增加推理延迟和上下文长度，降低连贯性和准确性。

Method: 生成HDS - QA数据集，使用该数据集微调LRM得到HybridDeepSearcher模型。

Result: HybridDeepSearcher在多个基准测试中优于基线，在FanOutQA和BrowseComp子集上F1值提升，减少推理延迟且可扩展。

Conclusion: 显式训练LRMs利用混合并行和顺序查询具有效率、可扩展性和有效性。

Abstract: Large reasoning models (LRMs) have demonstrated strong performance in
complex, multi-step reasoning tasks. Existing methods enhance LRMs by
sequentially integrating external knowledge retrieval; models iteratively
generate queries, retrieve external information, and progressively reason over
this information. However, purely sequential querying increases inference
latency and context length, diminishing coherence and potentially reducing
accuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search
QA), a synthetic dataset automatically generated from Natural Questions,
explicitly designed to train LRMs to distinguish parallelizable from sequential
queries. HDS-QA comprises hybrid-hop questions that combine parallelizable
independent subqueries (executable simultaneously) and sequentially dependent
subqueries (requiring step-by-step resolution), along with synthetic
reasoning-querying-retrieval paths involving parallel queries. We fine-tune an
LRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms
state-of-the-art baselines across multiple benchmarks, notably achieving +15.9
and +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both
requiring comprehensive and exhaustive search. Experimental results highlight
two key advantages: HybridDeepSearcher reaches comparable accuracy with fewer
search turns, significantly reducing inference latency, and it effectively
scales as more turns are permitted. These results demonstrate the efficiency,
scalability, and effectiveness of explicitly training LRMs to leverage hybrid
parallel and sequential querying.

</details>


### [43] [Algorithmic Collective Action with Multiple Collectives](https://arxiv.org/abs/2508.19149)
*Claudio Battiloro,Pietro Greiner,Bret Nestor,Oumaima Amezgar,Francesca Dominici*

Main category: cs.AI

TL;DR: 提出多集体算法集体行动（ACA）的理论框架，研究分类中的集体行动，给出集体规模和目标一致性的定量结果，为多集体 ACA 整体处理开辟道路。


<details>
  <summary>Details</summary>
Motivation: 现实中 ACA 行动分散为多个集体，但现有文献多关注单集体设置，需研究多集体 ACA。

Method: 提出多集体 ACA 的理论框架，聚焦分类中的集体行动，研究多集体如何植入信号。

Result: 得到关于集体规模及其目标一致性的作用和相互影响的定量结果。

Conclusion: 该框架补充了以往实证结果，为多集体 ACA 的整体处理提供了途径。

Abstract: As learning systems increasingly influence everyday decisions, user-side
steering via Algorithmic Collective Action (ACA)-coordinated changes to shared
data-offers a complement to regulator-side policy and firm-side model design.
Although real-world actions have been traditionally decentralized and
fragmented into multiple collectives despite sharing overarching
objectives-with each collective differing in size, strategy, and actionable
goals, most of the ACA literature focused on single collective settings. In
this work, we present the first theoretical framework for ACA with multiple
collectives acting on the same system. In particular, we focus on collective
action in classification, studying how multiple collectives can plant signals,
i.e., bias a classifier to learn an association between an altered version of
the features and a chosen, possibly overlapping, set of target classes. We
provide quantitative results about the role and the interplay of collectives'
sizes and their alignment of goals. Our framework, by also complementing
previous empirical results, opens a path for a holistic treatment of ACA with
multiple collectives.

</details>


### [44] [Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games](https://arxiv.org/abs/2508.19152)
*Chiu-Chou Lin*

Main category: cs.AI

TL;DR: 论文提出用游戏风格分析智能体决策行为，构建风格形成框架并提出指标，聚焦三个研究方向，最后指出未来拓展方向。


<details>
  <summary>Details</summary>
Motivation: 当代AI发展侧重理性决策，但现实中智能体决策受多种因素影响，‘风格’是常被忽视的智能维度，因此引入游戏风格视角分析。

Method: 从哲学角度考察游戏风格的基础含义和历史背景，分析信念和价值观驱动意图和行动的过程，构建风格形成的两层框架，形式化风格相关特征并提出可测量指标。

Result: 聚焦定义测量、表达生成、实际应用三个研究方向开展研究。

Conclusion: 论文提出未来可将风格作为构建通用人工智能的核心元素等拓展方向。

Abstract: Contemporary artificial intelligence (AI) development largely centers on
rational decision-making, valued for its measurability and suitability for
objective evaluation. Yet in real-world contexts, an intelligent agent's
decisions are shaped not only by logic but also by deeper influences such as
beliefs, values, and preferences. The diversity of human decision-making styles
emerges from these differences, highlighting that "style" is an essential but
often overlooked dimension of intelligence.
  This dissertation introduces playstyle as an alternative lens for observing
and analyzing the decision-making behavior of intelligent agents, and examines
its foundational meaning and historical context from a philosophical
perspective. By analyzing how beliefs and values drive intentions and actions,
we construct a two-tier framework for style formation: the external interaction
loop with the environment and the internal cognitive loop of deliberation. On
this basis, we formalize style-related characteristics and propose measurable
indicators such as style capacity, style popularity, and evolutionary dynamics.
  The study focuses on three core research directions: (1) Defining and
measuring playstyle, proposing a general playstyle metric based on discretized
state spaces, and extending it to quantify strategic diversity and competitive
balance; (2) Expressing and generating playstyle, exploring how reinforcement
learning and imitation learning can be used to train agents exhibiting specific
stylistic tendencies, and introducing a novel approach for human-like style
learning and modeling; and (3) Practical applications, analyzing the potential
of these techniques in domains such as game design and interactive
entertainment.
  Finally, the dissertation outlines future extensions, including the role of
style as a core element in building artificial general intelligence (AGI).

</details>


### [45] [MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation](https://arxiv.org/abs/2508.19163)
*Ernest Lim,Yajie Vera He,Jared Joselowitz,Kate Preston,Mohita Chowdhury,Louis Williams,Aisling Higham,Katrina Mason,Mariane Melo,Tom Lawton,Yan Jia,Ibrahim Habli*

Main category: cs.AI

TL;DR: 提出MATRIX框架用于临床对话代理的安全导向评估，经实验验证有效且可进行安全审计并开放相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在临床对话系统中的评估缺乏对行为和风险管理的考量，不能满足安全关键系统需求。

Method: MATRIX框架集成安全分类、基于LLM的评估器BehvJudge和模拟患者代理PatBot。

Result: BehvJudge检测效果达专家水平，PatBot能模拟真实患者行为，MATRIX可对五个LLM代理进行有效基准测试。

Conclusion: MATRIX是首个将结构化安全工程与可扩展、经过验证的对话式AI评估相结合的框架，可进行符合监管要求的安全审计。

Abstract: Despite the growing use of large language models (LLMs) in clinical dialogue
systems, existing evaluations focus on task completion or fluency, offering
little insight into the behavioral and risk management requirements essential
for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion
fRamework for safe Interactions and conteXtual clinical conversational
evaluation), a structured, extensible framework for safety-oriented evaluation
of clinical dialogue agents.
  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical
scenarios, expected system behaviors and failure modes derived through
structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator
for detecting safety-relevant dialogue failures, validated against expert
clinician annotations; and (3) PatBot, a simulated patient agent capable of
producing diverse, scenario-conditioned responses, evaluated for realism and
behavioral fidelity with human factors expertise, and a patient-preference
study.
  Across three experiments, we show that MATRIX enables systematic, scalable
safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard
detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded
assessment of 240 dialogues. We also conducted one of the first realism
analyses of LLM-based patient simulation, showing that PatBot reliably
simulates realistic patient behavior in quantitative and qualitative
evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking
five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios
and 10 clinical domains.
  MATRIX is the first framework to unify structured safety engineering with
scalable, validated conversational AI evaluation, enabling regulator-aligned
safety auditing. We release all evaluation tools, prompts, structured
scenarios, and datasets.

</details>


### [46] [The Ramon Llull's Thinking Machine for Automated Ideation](https://arxiv.org/abs/2508.19200)
*Xinran Zhao,Boyuan Zheng,Chenglei Si,Haofei Yu,Ken Liu,Runlong Zhou,Ruochen Li,Tong Chen,Xiang Li,Yiming Zhang,Tongshuang Wu*

Main category: cs.AI

TL;DR: 本文以中世纪的Ars combinatoria为概念基础，构建现代思维机器用于研究构思，定义三个组合轴，利用大语言模型生成研究想法，为科研创意提供工具。


<details>
  <summary>Details</summary>
Motivation: 以中世纪的Ars combinatoria为概念基础，构建现代思维机器用于研究构思。

Method: 定义主题、领域和方法三个组合轴，从人类专家或会议论文中挖掘元素，用精心策划的组合提示大语言模型。

Result: 提示大语言模型产生的研究想法多样、相关且基于当前文献。

Conclusion: 该现代思维机器为增强科研创造力提供轻量级、可解释的工具，为人类与AI协作构思指明方向。

Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for
generating knowledge through symbolic recombination - as a conceptual
foundation for building a modern Llull's thinking machine for research
ideation. Our approach defines three compositional axes: Theme (e.g.,
efficiency, adaptivity), Domain (e.g., question answering, machine
translation), and Method (e.g., adversarial training, linear attention). These
elements represent high-level abstractions common in scientific work -
motivations, problem settings, and technical approaches - and serve as building
blocks for LLM-driven exploration. We mine elements from human experts or
conference papers and show that prompting LLMs with curated combinations
produces research ideas that are diverse, relevant, and grounded in current
literature. This modern thinking machine offers a lightweight, interpretable
tool for augmenting scientific creativity and suggests a path toward
collaborative ideation between humans and AI.

</details>


### [47] [The Subset Sum Matching Problem](https://arxiv.org/abs/2508.19218)
*Yufei Wu,Manuel R. Torres,Parisa Zehtabi,Alberto Pozanco Lancho,Michael Cashmore,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出子集和匹配问题（SSMP），给出三种算法求解并进行实验评估


<details>
  <summary>Details</summary>
Motivation: 抽象常见金融应用如交易对账，提出新的组合优化任务SSMP

Method: 提出两种次优算法和一种最优算法解决SSMP，生成涵盖不同复杂度实例的基准

Result: 进行实验评估了各算法性能

Conclusion: 未提及明确结论

Abstract: This paper presents a new combinatorial optimisation task, the Subset Sum
Matching Problem (SSMP), which is an abstraction of common financial
applications such as trades reconciliation. We present three algorithms, two
suboptimal and one optimal, to solve this problem. We also generate a benchmark
to cover different instances of SSMP varying in complexity, and carry out an
experimental evaluation to assess the performance of the approaches.

</details>


### [48] [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229)
*Wei Xiong,Wenting Zhao,Weizhe Yuan,Olga Golovneva,Tong Zhang,Jason Weston,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: 本文将逐步奖励建模从分类任务重新定义为推理任务，提出生成式评判模型StepWiser，经强化学习训练，在中间步骤判断、策略模型训练和推理搜索方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着模型利用多步推理策略解决复杂问题，监督中间步骤逻辑有效性成关键挑战，现有过程奖励模型有不提供解释和泛化性受限的缺点。

Method: 将逐步奖励建模从分类任务转变为推理任务，提出生成式评判模型StepWiser，用强化学习基于滚动的相对结果进行训练。

Result: StepWiser在中间步骤的判断准确性优于现有方法，可在训练时改进策略模型，还能提升推理时的搜索效果。

Conclusion: 所提出的StepWiser模型在解决中间步骤监督问题上更有效，在多方面有更好表现。

Abstract: As models increasingly leverage multi-step reasoning strategies to solve
complex problems, supervising the logical validity of these intermediate steps
has become a critical research challenge. Process reward models address this by
providing step-by-step feedback, but current approaches have two major
drawbacks: they typically function as classifiers without providing
explanations, and their reliance on supervised fine-tuning with static datasets
limits generalization. Inspired by recent advances, we reframe stepwise reward
modeling from a classification task to a reasoning task itself. We thus propose
a generative judge that reasons about the policy model's reasoning steps (i.e.,
meta-reasons), outputting thinking tokens before delivering a final verdict.
Our model, StepWiser, is trained by reinforcement learning using relative
outcomes of rollouts. We show it provides (i) better judgment accuracy on
intermediate steps than existing methods; (ii) can be used to improve the
policy model at training time; and (iii) improves inference-time search.

</details>


### [49] [Model Context Protocols in Adaptive Transport Systems: A Survey](https://arxiv.org/abs/2508.19239)
*Gaurab Chhetri,Shriyank Somvanshi,Md Monzurul Islam,Shamyo Brotee,Mahmuda Sultana Mimi,Dipti Koirala,Biplov Pandey,Subasish Das*

Main category: cs.AI

TL;DR: 文章调研了Model Context Protocol (MCP) 作为统一范式，提出五类分类法，揭示三点关键见解并给出研究路线图。


<details>
  <summary>Details</summary>
Motivation: 互联设备、自治系统和AI应用的快速扩张使自适应传输系统严重碎片化，不同协议和上下文源相互孤立，需要统一范式。

Method: 分析现有文献，提出覆盖自适应机制、上下文感知框架等的五类分类法。

Result: 发现传统传输协议孤立适配已达极限，MCP的客户端 - 服务器和JSON - RPC结构实现语义互操作性，AI驱动的传输需要适合MCP的集成范式。

Conclusion: 将MCP定位为下一代自适应、上下文感知和智能传输基础设施的基础。

Abstract: The rapid expansion of interconnected devices, autonomous systems, and AI
applications has created severe fragmentation in adaptive transport systems,
where diverse protocols and context sources remain isolated. This survey
provides the first systematic investigation of the Model Context Protocol (MCP)
as a unifying paradigm, highlighting its ability to bridge protocol-level
adaptation with context-aware decision making. Analyzing established
literature, we show that existing efforts have implicitly converged toward
MCP-like architectures, signaling a natural evolution from fragmented solutions
to standardized integration frameworks. We propose a five-category taxonomy
covering adaptive mechanisms, context-aware frameworks, unification models,
integration strategies, and MCP-enabled architectures. Our findings reveal
three key insights: traditional transport protocols have reached the limits of
isolated adaptation, MCP's client-server and JSON-RPC structure enables
semantic interoperability, and AI-driven transport demands integration
paradigms uniquely suited to MCP. Finally, we present a research roadmap
positioning MCP as a foundation for next-generation adaptive, context-aware,
and intelligent transport infrastructures.

</details>


### [50] [From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence](https://arxiv.org/abs/2508.15447)
*Zihao Wang,Junming Zhang*

Main category: cs.AI

TL;DR: 本文介绍用于复杂企业环境高级决策的多智能体框架BusiAgent，经评估其效果好，推动了AI驱动的企业决策。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在商业应用中难以协调运营分析与战略目标，导致工作流程碎片化和跨层级协作减少。

Method: 引入BusiAgent框架，集成扩展的CTMDP、广义熵度量、多级Stackelberg博弈，采用上下文汤普森采样优化提示，并设有质量保证系统。

Result: 在不同商业场景的实证评估中，BusiAgent能生成连贯、以客户为中心的解决方案，在解决方案质量和用户满意度上显著优于现有方法。

Conclusion: BusiAgent将前沿AI技术与商业洞察融合，是AI驱动企业决策的重要进步，助力组织更有效应对复杂商业环境。

Abstract: Large Language Models (LLMs) have shown promising potential in business
applications, particularly in enterprise decision support and strategic
planning, yet current approaches often struggle to reconcile intricate
operational analyses with overarching strategic goals across diverse market
environments, leading to fragmented workflows and reduced collaboration across
organizational levels. This paper introduces BusiAgent, a novel multi-agent
framework leveraging LLMs for advanced decision-making in complex corporate
environments. BusiAgent integrates three core innovations: an extended
Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a
generalized entropy measure to optimize collaborative efficiency, and a
multi-level Stackelberg game to handle hierarchical decision processes.
Additionally, contextual Thompson sampling is employed for prompt optimization,
supported by a comprehensive quality assurance system to mitigate errors.
Extensive empirical evaluations across diverse business scenarios validate
BusiAgent's efficacy, demonstrating its capacity to generate coherent,
client-focused solutions that smoothly integrate granular insights with
high-level strategy, significantly outperforming established approaches in both
solution quality and user satisfaction. By fusing cutting-edge AI technologies
with deep business insights, BusiAgent marks a substantial step forward in
AI-driven enterprise decision-making, empowering organizations to navigate
complex business landscapes more effectively.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [51] [Graph Neural Network-Based Topology Optimization for Self-Supporting Structures in Additive Manufacturing](https://arxiv.org/abs/2508.19169)
*Alireza Tabarraei,Saquib Ahmad Bhuiyan*

Main category: cs.CE

TL;DR: 提出基于机器学习的自支撑结构拓扑优化框架，可生成应力约束可制造拓扑，减少后处理需求。


<details>
  <summary>Details</summary>
Motivation: 为增材制造量身定制自支撑结构的拓扑优化框架，以获得高性能设计并减少后处理。

Method: 采用图神经网络在有限元网格上作为神经场学习和预测连续材料分布，用集成AM滤波器确保可打印性，通过可微p - 范数聚合冯·米塞斯应力实施应力约束，利用自动微分进行优化。

Result: 框架能在各种加载和边界条件下生成应力约束可制造拓扑。

Conclusion: 该框架为增材制造的高性能设计提供了实用途径，减少后处理需求。

Abstract: This paper presents a machine learning-based framework for topology
optimization of self-supporting structures, specifically tailored for additive
manufacturing (AM). By employing a graph neural network (GNN) that acts as a
neural field over the finite element mesh, the framework effectively learns and
predicts continuous material distributions. An integrated AM filter ensures
printability by eliminating unsupported overhangs, while the optimization
process minimizes structural compliance under volume and stress constraints.
The stress constraint is enforced using a differentiable p-norm aggregation of
von Mises stress, promoting mechanical reliability in the optimized designs. A
key advantage of the approach lies in its fully differentiable architecture,
which leverages automatic differentiation throughout the optimization
loop--eliminating the need for explicit sensitivity derivation for both the
filter and the stress constraint. Numerical experiments demonstrate the ability
of the framework to generate stress-constrained manufacturable topologies under
various loading and boundary conditions, offering a practical pathway toward
AM-ready high-performance designs with reduced post-processing requirements.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [52] [Metrics, KPIs, and Taxonomy for Data Valuation and Monetisation -- A Systematic Literature Review](https://arxiv.org/abs/2508.18331)
*Eduardo Vyhmeister,Bastien Pietropaoli,Alejando Martinez Molina,Montserrat Gonzalez-Ferreiro,Gabriel Gonzalez-Castane,Jordi Arjona Aroca,Andrea Visentin*

Main category: cs.DB

TL;DR: 本文是数据估值和货币化的综述，介绍相关概念，给出大量指标和KPI并分类，还讨论了创建标准框架的困难和领域挑战。


<details>
  <summary>Details</summary>
Motivation: 数据估值和货币化缺乏标准程序和框架，需要让读者更好理解相关概念并梳理指标。

Method: 进行系统文献综述，收集组织各方面和不同利益相关者使用的指标和KPI，采用平衡计分卡方法将其分类。

Result: 提供包含162个参考文献的指标和KPI列表，并将其分类到大型分类法中。

Conclusion: 该分类法有助于各级数据管理理解该领域复杂情况，同时指出创建标准框架困难和领域面临的主要挑战。

Abstract: Data valuation and data monetisation are complex subjects but essential to
most organisations today. Unfortunately, they still lack standard procedures
and frameworks for organisations to follow. In this survey, we introduce the
reader to the concepts by providing the definitions and the background required
to better understand data, monetisation strategies, and finally metrics and
KPIs used in these strategies. We have conducted a systematic literature review
on metrics and KPIs used in data valuation and monetisation, in every aspect of
an organisation's business, and by a variety of stakeholders. We provide an
expansive list of such metrics and KPIs with 162 references. We then categorise
all the metrics and KPIs found into a large taxonomy, following the Balanced
Scorecard (BSC) approach with further subclustering to cover every aspect of an
organisation's business. This taxonomy will help every level of data management
understand the complex landscape of the domain. We also discuss the difficulty
in creating a standard framework for data valuation and data monetisation and
the major challenges the domain is currently facing.

</details>


### [53] [DiskJoin: Large-scale Vector Similarity Join with SSD](https://arxiv.org/abs/2508.18494)
*Yanqi Chen,Xiao Yan,Alexandra Meliou,Eric Lo*

Main category: cs.DB

TL;DR: 提出DiskJoin算法，可在单机高效处理十亿规模向量数据集的相似度连接，评估显示显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有分布式计算方法需集群部署且通信开销大，基于磁盘的解决方案存在磁盘I/O瓶颈。

Method: 定制数据访问模式减少重复访问和读放大；用主内存作动态缓存并管理缓存淘汰；采用概率剪枝技术减少计算量。

Result: 在真实大规模数据集上评估，DiskJoin比其他方法快50到1000倍。

Conclusion: DiskJoin能在单机上高效处理大规模向量数据集的相似度连接。

Abstract: Similarity join--a widely used operation in data science--finds all pairs of
items that have distance smaller than a threshold. Prior work has explored
distributed computation methods to scale similarity join to large data volumes
but these methods require a cluster deployment, and efficiency suffers from
expensive inter-machine communication. On the other hand, disk-based solutions
are more cost-effective by using a single machine and storing the large dataset
on high-performance external storage, such as NVMe SSDs, but in these methods
the disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,
the first disk-based similarity join algorithm that can process billion-scale
vector datasets efficiently on a single machine. DiskJoin improves disk I/O by
tailoring the data access patterns to avoid repetitive accesses and read
amplification. It also uses main memory as a dynamic cache and carefully
manages cache eviction to improve cache hit rate and reduce disk retrieval
time. For further acceleration, we adopt a probabilistic pruning technique that
can effectively prune a large number of vector pairs from computation. Our
evaluation on real-world, large-scale datasets shows that DiskJoin
significantly outperforms alternatives, achieving speedups from 50x to 1000x.

</details>


### [54] [Brook-2PL: Tolerating High Contention Workloads with A Deadlock-Free Two-Phase Locking Protocol](https://arxiv.org/abs/2508.18576)
*Farzad Habibi,Juncheng Fang,Tania Lorido-Botran,Faisal Nawab*

Main category: cs.DB

TL;DR: 提出Brook - 2PL协议解决高并发负载热点问题，性能超现有协议。


<details>
  <summary>Details</summary>
Motivation: 传统并发控制方法在高并发下处理热点问题时，存在过多事务中止和死锁问题。

Method: 引入SLW - Graph实现无死锁事务执行，提出部分事务分割用于提前释放锁，提供更灵活分割方法。

Result: 在合成在线游戏商店工作负载和TPC - C基准测试中，Brook - 2PL平均加速2.86倍，在TPC - C基准中降低48%尾部延迟（p95）。

Conclusion: Brook - 2PL显著优于现有并发控制协议。

Abstract: The problem of hotspots remains a critical challenge in high-contention
workloads for concurrency control (CC) protocols. Traditional concurrency
control approaches encounter significant difficulties under high contention,
resulting in excessive transaction aborts and deadlocks. In this paper, we
propose Brook-2PL, a novel two-phase locking (2PL) protocol that (1) introduces
SLW-Graph for deadlock-free transaction execution, and (2) proposes partial
transaction chopping for early lock release. Previous methods suffer from
transaction aborts that lead to wasted work and can further burden the system
due to their cascading effects. Brook-2PL addresses this limitation by
statically analyzing a new graph-based dependency structure called SLW-Graph,
enabling deadlock-free two-phase locking through predetermined lock
acquisition. Brook-2PL also reduces contention by enabling early lock release
using partial transaction chopping and static transaction analysis. We overcome
the inherent limitations of traditional transaction chopping by providing a
more flexible chopping method. Evaluation using both our synthetic online game
store workload and the TPC-C benchmark shows that Brook-2PL significantly
outperforms state-of-the-art CC protocols. Brook-2PL achieves an average
speed-up of 2.86x while reducing tail latency (p95) by 48% in the TPC-C
benchmark.

</details>


### [55] [Optimal $(α,β)$-Dense Subgraph Search in Bipartite Graphs](https://arxiv.org/abs/2508.18616)
*Yalong Zhang,Rong-Hua Li,Qi Zhang,Guoren Wang*

Main category: cs.DB

TL;DR: 本文提出BD - Index解决(α, β) - 稠密子图模型在查询处理和动态更新方面的不足，开发两种维护策略，实验证明方案高效可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有(α, β) - 稠密子图模型缺乏高效的查询处理和动态更新支持，限制其在大规模应用中的实用性。

Method: 提出BD - Index以最优时间回答(α, β) - 稠密子图查询，使用线性空间；为动态二分图开发空间高效和时间高效两种维护策略。

Result: 空间高效策略每次边插入或删除的时间复杂度为O(p · |E|^1.5)，空间成本为O(|E|)；时间高效策略将每次边更新时间降至O(p · |E|)，但内存使用增至O(p · |E|)。

Conclusion: 提出的解决方案在10个大规模真实数据集上展现出高效率和可扩展性，两种策略能灵活权衡维护效率和内存使用。

Abstract: Dense subgraph search in bipartite graphs is a fundamental problem in graph
analysis, with wide-ranging applications in fraud detection, recommendation
systems, and social network analysis. The recently proposed $(\alpha,
\beta)$-dense subgraph model has demonstrated superior capability in capturing
the intrinsic density structure of bipartite graphs compared to existing
alternatives. However, despite its modeling advantages, the $(\alpha,
\beta)$-dense subgraph model lacks efficient support for query processing and
dynamic updates, limiting its practical utility in large-scale applications. To
address these limitations, we propose BD-Index, a novel index that answers
$(\alpha, \beta)$-dense subgraph queries in optimal time while using only
linear space $O(|E|)$, making it well-suited for real-world applications
requiring both fast query processing and low memory consumption. We further
develop two complementary maintenance strategies for dynamic bipartite graphs
to support efficient updates to the BD-Index. The space-efficient strategy
updates the index in time complexity of $O(p \cdot |E|^{1.5})$ per edge
insertion or deletion, while maintaining a low space cost of $O(|E|)$ (the same
as the index itself), where $p$ is typically a small constant in real-world
graphs. In contrast, the time-efficient strategy significantly reduces the
update time to $O(p \cdot |E|)$ per edge update by maintaining auxiliary
orientation structures, at the cost of increased memory usage up to $O(p \cdot
|E|)$. These two strategies provide flexible trade-offs between maintenance
efficiency and memory usage, enabling BD-Index to adapt to diverse application
requirements. Extensive experiments on 10 large-scale real-world datasets
demonstrate high efficiency and scalability of our proposed solutions.

</details>


### [56] [WoW: A Window-to-Window Incremental Index for Range-Filtering Approximate Nearest Neighbor Search](https://arxiv.org/abs/2508.18617)
*Ziqi Wang,Jingzhe Zhang,Wei Hu*

Main category: cs.DB

TL;DR: 本文提出基于窗口图的RFANNS索引，在构建和查询性能上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有RFANNS专用索引在增量构建和适应任意范围过滤器方面存在挑战，需改进。

Method: 提出插入算法将新向量 - 属性对添加到不同窗口大小的分层窗口图；通过范围选择性优化相关窗口搜索进行属性过滤检查和向量距离计算。

Result: 在真实数据集实验中，索引构建时间与最高效构建索引相当，比最高效查询索引快4.9倍且大小小0.4 - 0.5倍；RFANNS查询比最高效增量索引快4倍，与最佳静态构建索引性能相当。

Conclusion: 所提基于窗口图的RFANNS索引在构建和查询性能上有显著提升。

Abstract: Given a hybrid dataset where every data object consists of a vector and an
attribute value, for each query with a target vector and a range filter,
range-filtering approximate nearest neighbor search (RFANNS) aims to retrieve
the most similar vectors from the dataset and the corresponding attribute
values fall in the query range. It is a fundamental function in vector database
management systems and intelligent systems with embedding abilities. Dedicated
indices for RFANNS accelerate query speed with an acceptable accuracy loss on
nearest neighbors. However, they are still facing the challenges to be
constructed incrementally and generalized to achieve superior query performance
for arbitrary range filters. In this paper, we introduce a window graph-based
RFANNS index. For incremental construction, we propose an insertion algorithm
to add new vector-attribute pairs into hierarchical window graphs with varying
window size. To handle arbitrary range filters, we optimize relevant window
search for attribute filter checks and vector distance computations by range
selectivity. Extensive experiments on real-world datasets show that for index
construction, the indexing time is on par with the most building-efficient
index, and 4.9x faster than the most query-efficient index with 0.4-0.5x
smaller size; For RFANNS query, it is 4x faster than the most efficient
incremental index, and matches the performance of the best statically-built
index.

</details>


### [57] [Rethinking Caching for LLM Serving Systems: Beyond Traditional Heuristics](https://arxiv.org/abs/2508.18736)
*Jungwoo Kim,Minsang Kim,Jaeheon Lee,Chanwoo Moon,Heejin Kim,Taeho Hwang,Woosuk Chung,Yeseong Kim,Sungjin Lee*

Main category: cs.DB

TL;DR: 提出语义缓存系统SISO用于大规模大语言模型服务，性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 大规模服务大语言模型需在计算和内存约束下满足严格SLO，但传统缓存策略有不足。

Method: 引入基于质心的缓存、感知局部性的替换和动态阈值技术。

Result: 在不同数据集上，SISO的命中率比现有系统高1.71倍，SLO达成情况更好。

Conclusion: SISO重新定义了大语言模型服务的缓存效率。

Abstract: Serving Large Language Models (LLMs) at scale requires meeting strict Service
Level Objectives (SLOs) under severe computational and memory constraints.
Nevertheless, traditional caching strategies fall short: exact-matching and
prefix caches neglect query semantics, while state-of-the-art semantic caches
remain confined to traditional intuitions, offering little conceptual
departure. Building on this, we present SISO, a semantic caching system that
redefines efficiency for LLM serving. SISO introduces centroid-based caching to
maximize coverage with minimal memory, locality-aware replacement to preserve
high-value entries, and dynamic thresholding to balance accuracy and latency
under varying workloads. Across diverse datasets, SISO delivers up to
1.71$\times$ higher hit ratios and consistently stronger SLO attainment
compared to state-of-the-art systems.

</details>


### [58] [Text to Query Plans for Question Answering on Large Tables](https://arxiv.org/abs/2508.18758)
*Yipeng Zhang,Chen Wang,Yuzhe Zhang,Jacky Jiang*

Main category: cs.DB

TL;DR: 提出将自然语言查询转换为查询计划的新框架，利用大语言模型迭代解释查询，在标准数据库和大型科学表实验中验证其处理大数据集和复杂分析的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到 SQL 方法有局限性，对于无编程专业知识用户，处理大数据集效率低且复杂数据分析支持有限。

Method: 提出新框架，在传统数据库外实现，利用大语言模型迭代解释查询、构建操作序列，直接对数据执行操作。

Result: 在标准数据库和大型科学表上的实验验证了框架处理大数据集和进行复杂数据分析的有效性。

Conclusion: 新框架能避免 SQL 固有局限，支持复杂分析功能，有更好的灵活性和可扩展性。

Abstract: Efficient querying and analysis of large tabular datasets remain significant
challenges, especially for users without expertise in programming languages
like SQL. Text-to-SQL approaches have shown promising performance on benchmark
data; however, they inherit SQL's drawbacks, including inefficiency with large
datasets and limited support for complex data analyses beyond basic querying.
We propose a novel framework that transforms natural language queries into
query plans. Our solution is implemented outside traditional databases,
allowing us to support classical SQL commands while avoiding SQL's inherent
limitations. Additionally, we enable complex analytical functions, such as
principal component analysis and anomaly detection, providing greater
flexibility and extensibility than traditional SQL capabilities. We leverage
LLMs to iteratively interpret queries and construct operation sequences,
addressing computational complexity by incrementally building solutions. By
executing operations directly on the data, we overcome context length
limitations without requiring the entire dataset to be processed by the model.
We validate our framework through experiments on both standard databases and
large scientific tables, demonstrating its effectiveness in handling extensive
datasets and performing sophisticated data analyses.

</details>


### [59] [Enriching Object-Centric Event Data with Process Scopes: A Framework for Aggregation and Analysis](https://arxiv.org/abs/2508.18830)
*Shahrzad Khayatbashi,Majid Rafiei,Jiayuan Chen,Timotheus Kampik,Gregor Berg,Amin Jalali*

Main category: cs.DB

TL;DR: 现有Object - Centric Event Data格式缺进程范围定义，提出将分析师定义的进程范围嵌入OCEL的方法并验证适用性。


<details>
  <summary>Details</summary>
Motivation: 现有OCED格式缺乏进程范围明确定义，限制分析且难以解读，进程定义主观依赖上下文，无法自动发现。

Method: 提出将分析师定义的进程范围嵌入OCEL的方法。

Result: 用公开OCEL日志验证了方法适用性，并提供范围定义与分析工具。

Conclusion: 该方法能结构化表示多进程、支持跨范围事件数据聚合和不同抽象层次分析。

Abstract: Object-Centric Process Mining enables the analysis of complex operational
behavior by capturing interactions among multiple business objects (e.g.,
orders, items, deliveries). These interactions are recorded using
Object-Centric Event Data (OCED) formats, such as the Object-Centric Event Log
(OCEL). However, existing formats lack explicit definitions of process scopes,
which restricts analysis to individual processes and limits insights to a low
level of granularity. In practice, OCED often spans multiple interrelated
processes, as shared objects connect events across organizational functions.
This structure reflects how value is created along the organizational value
chain, but introduces challenges for interpretation when process boundaries are
not clearly defined. Moreover, process definitions are typically subjective and
context-dependent; they vary across organizations, roles, and analytical goals,
and cannot always be discovered automatically. To address these challenges, we
propose a method for embedding analyst-defined process scopes into OCEL. This
enables the structured representation of multiple coexisting processes,
supports the aggregation of event data across scopes, and facilitates analysis
at varying levels of abstraction. We demonstrate the applicability of our
approach using a publicly available OCEL log and provide supporting tools for
scope definition and analysis.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [60] [Experiences with Model Context Protocol Servers for Science and High Performance Computing](https://arxiv.org/abs/2508.18489)
*Haochen Pan,Ryan Chard,Reid Mello,Christopher Grams,Tanjin He,Alexander Brace,Owen Price Skelly,Will Engler,Hayden Holbrook,Song Young Oh,Maxime Gonthier,Michael Papka,Ben Blaiszik,Kyle Chard,Ian Foster*

Main category: cs.DC

TL;DR: 本文介绍用MCP作为统一接口，实现研究能力的可发现、可调用和可组合，并通过案例研究说明其应用，总结经验并指出挑战。


<details>
  <summary>Details</summary>
Motivation: 大多数研究网络基础设施的异构API和安全模型给大语言模型驱动的智能体使用带来障碍，需要统一接口。

Method: 在成熟服务上实现轻量级MCP服务器，如Globus Transfer、Compute等，并进行多个领域的案例研究。

Result: 展示了MCP导向的架构在多个领域的实际应用。

Conclusion: 总结了经验，指出了在评估和信任方面的开放挑战。

Abstract: Large language model (LLM)-powered agents are increasingly used to plan and
execute scientific workflows, yet most research cyberinfrastructure (CI)
exposes heterogeneous APIs and implements security models that present barriers
for use by agents. We report on our experience using the Model Context Protocol
(MCP) as a unifying interface that makes research capabilities discoverable,
invokable, and composable. Our approach is pragmatic: we implement thin MCP
servers over mature services, including Globus Transfer, Compute, and Search;
status APIs exposed by computing facilities; Octopus event fabric; and
domain-specific tools such as Garden and Galaxy. We use case studies in
computational chemistry, bioinformatics, quantum chemistry, and filesystem
monitoring to illustrate how this MCP-oriented architecture can be used in
practice. We distill lessons learned and outline open challenges in evaluation
and trust for agent-led science.

</details>


### [61] [Managing Multi Instance GPUs for High Throughput and Energy Savings](https://arxiv.org/abs/2508.18556)
*Abhijeet Saraha,Yuanbo Li,Chris Porter,Santosh Pande*

Main category: cs.DC

TL;DR: 开发GPU分区和调度方案，对多种工作负载有性能和能耗优化效果。


<details>
  <summary>Details</summary>
Motivation: 现代GPU虽有性能和并发支持，但芯片分区复杂，利用并发有挑战，需开发合适方案。

Method: 开发涉及动态内存估计、分区融合和分裂的方案，支持进程重启。

Result: 通用工作负载吞吐量提升6.20x、能耗改善5.93x；A100上ML工作负载吞吐量提升1.59x、能耗改善1.12x；LLM工作负载吞吐量提升1.43x、节能1.11x。

Conclusion: 所开发的GPU分区和调度方案对多种工作负载有良好性能和能耗优化效果。

Abstract: Modern GPUs such as the Ampere series (A30, A100) as well as the Hopper
series (H100, H200) offer performance as well as security isolation features.
They also support a good amount of concurrency, but taking advantage of it can
be quite challenging due to the complex constraints on partitioning the chip.
  In this work, we develop partitioning and scheduling schemes for a variety of
workloads, ranging from scientific to modern ML workloads, including LLMs. We
develop several schemes involving dynamic memory estimation, partition fusion
and partition fission. We also support process restart to recover from
out-of-memory errors for workloads and early restart as an optimization. This
approach yields up to 6.20x throughput and 5.93x energy improvements for
general workloads; and we see 1.59x and 1.12x improvement to throughput and
energy, respectively, for ML workloads on an A100 GPU. We leverage this
technique on LLM workloads and show good improvements, including up to 1.43x
throughput improvement and 1.11x energy savings.

</details>


### [62] [Strata: Hierarchical Context Caching for Long Context Language Model Serving](https://arxiv.org/abs/2508.18572)
*Zhiqiang Xie,Ziyi Xu,Mark Zhao,Yuwei An,Vikram Sharma Mailthody,Scott Mahlke,Michael Garland,Christos Kozyrakis*

Main category: cs.DC

TL;DR: 论文提出Strata用于高效长上下文大语言模型服务，能降低TTFT并加速，不影响短上下文性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型长上下文缓存存储超GPU内存，数据传输有性能瓶颈，现有调度器未考虑缓存加载延迟。

Method: 引入GPU辅助I/O解决KV缓存碎片化，解耦GPU和CPU内存布局，采用缓存感知请求调度平衡计算和I/O延迟。

Result: 在长上下文基准测试中，相比vLLM + LMCache，TTFT降低达5倍；相比NVIDIA TensorRT - LLM加速3.75倍，不影响短上下文性能。

Conclusion: Strata是高效的长上下文LLM服务的分层上下文缓存框架。

Abstract: Large Language Models (LLMs) with expanding context windows face significant
performance hurdles. While caching key-value (KV) states is critical for
avoiding redundant computation, the storage footprint of long-context caches
quickly exceeds GPU memory capacity, forcing production systems to adopt
hierarchical caching across memory hierarchies. However, transferring large
cached contexts back to the GPU introduces severe performance bottlenecks:
fragmented I/O from paged layouts prevents full bandwidth utilization, and
existing schedulers fail to account for cache-loading delays, leaving systems
loading-bound rather than compute-bound. We present Strata, a hierarchical
context caching framework designed for efficient long context LLM serving.
Strata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling
GPU and CPU memory layouts and employs cache-aware request scheduling to
balance compute with I/O latency and overlapping unavoidable stalls with
complementary tasks. Built on SGLang and deployed in production, Strata
achieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache
and 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without
degrading short-context performance.

</details>


### [63] [Examining MPI and its Extensions for Asynchronous Multithreaded Communication](https://arxiv.org/abs/2508.18667)
*Jiakun Yan,Marc Snir,Yanfei Guo*

Main category: cs.DC

TL;DR: 评估MPI的VCI和Continuation扩展在AMT运行时HPX中的性能，发现有提升但仍需改进。


<details>
  <summary>Details</summary>
Motivation: HPC架构复杂度增加和不规则算法采用，需要高效异步多线程通信支持，原MPI规范未考虑此模式，社区推出扩展以满足需求。

Method: 先用MPI级微基准测试评估扩展峰值性能潜力，再集成到HPX评估其在实际场景中的有效性。

Result: 扩展比标准MPI能提升性能，但存在改进空间，当前Continuation提案限制多VCI场景下多线程消息速率，推荐的每线程一VCI模式在实际系统中无效。

Conclusion: 提高VCI内部线程效率对实现可扩展多线程通信和充分发挥MPI扩展优势很重要。

Abstract: The increasing complexity of HPC architectures and the growing adoption of
irregular scientific algorithms demand efficient support for asynchronous,
multithreaded communication. This need is especially pronounced with
Asynchronous Many-Task (AMT) systems. This communication pattern was not a
consideration during the design of the original MPI specification. The MPI
community has recently introduced several extensions to address these evolving
requirements. This work evaluates two such extensions, the Virtual
Communication Interface (VCI) and the Continuation extensions, in the context
of an established AMT runtime HPX. We begin by using an MPI-level
microbenchmark, modeled from HPX's low-level communication mechanism, to
measure the peak performance potential of these extensions. We then integrate
them into HPX to evaluate their effectiveness in real-world scenarios. Our
results show that while these extensions can enhance performance compared to
standard MPI, areas for improvement remain. The current continuation proposal
limits the maximum multithreaded message rate achievable in the multi-VCI
setting. Furthermore, the recommended one-VCI-per-thread mode proves
ineffective in real-world systems due to the attentiveness problem. These
findings underscore the importance of improving intra-VCI threading efficiency
to achieve scalable multithreaded communication and fully realize the benefits
of recent MPI extensions.

</details>


### [64] [ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive](https://arxiv.org/abs/2508.18850)
*Xinhao Luo,Zihan Liu,Yangjie Zhou,Shihan Fang,Ziyu Huang,Yu Feng,Chen Zhang,Shixuan Sun,Zhenzhe Zheng,Jingwen Leng,Minyi Guo*

Main category: cs.DC

TL;DR: 论文引入集群级通信原语，设计ClusterFusion执行框架提升大语言模型解码效率，在H100 GPU上评估效果优于现有框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型解码因算子执行碎片化、依赖片外内存导致高延迟，现有架构缺乏片上通信的结构化抽象，需弥合软硬件差距。

Method: 引入ClusterReduce和ClusterGather通信原语，设计ClusterFusion执行框架联合调度通信和计算，扩展算子融合范围。

Result: 在H100 GPU上评估，ClusterFusion在不同模型和配置下的端到端延迟平均比现有推理框架快1.61倍。

Conclusion: ClusterFusion能有效提升大语言模型解码效率，代码开源。

Abstract: Large language model (LLM) decoding suffers from high latency due to
fragmented execution across operators and heavy reliance on off-chip memory for
data exchange and reduction. This execution model limits opportunities for
fusion and incurs significant memory traffic and kernel launch overhead. While
modern architectures such as NVIDIA Hopper provide distributed shared memory
and low-latency intra-cluster interconnects, they expose only low-level data
movement instructions, lacking structured abstractions for collective on-chip
communication. To bridge this software-hardware gap, we introduce two
cluster-level communication primitives, ClusterReduce and ClusterGather, which
abstract common communication patterns and enable structured, high-speed data
exchange and reduction between thread blocks within a cluster, allowing
intermediate results to be on-chip without involving off-chip memory. Building
on these abstractions, we design ClusterFusion, an execution framework that
schedules communication and computation jointly to expand operator fusion scope
by composing decoding stages such as QKV Projection, Attention, and Output
Projection into a single fused kernels. Evaluations on H100 GPUs show that
ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on
average in end-to-end latency across different models and configurations. The
source code is available at https://github.com/xinhao-luo/ClusterFusion.

</details>


### [65] [Ab-initio Quantum Transport with the GW Approximation, 42,240 Atoms, and Sustained Exascale Performance](https://arxiv.org/abs/2508.19138)
*Nicolas Vetsch,Alexander Maeder,Vincent Maillou,Anders Winka,Jiang Cao,Grzegorz Kwasniewski,Leonard Deuschle,Torsten Hoefler,Alexandros Nikolaos Ziogas,Mathieu Luisier*

Main category: cs.DC

TL;DR: 介绍了首个能处理与实验可比尺寸NRFET几何结构的NEGF+GW方案QuaTrEx，其有新的空间域分解方案，可处理大量原子，在超算上扩展性好，能达亿亿次级性能。


<details>
  <summary>Details</summary>
Motivation: 设计纳米级电子设备需先进建模工具，现有方法在处理小尺寸设备时需考虑强电子 - 电子相互作用，需扩展DFT+NEGF求解器，但会增加计算强度。

Method: 提出NEGF+GW方案QuaTrEx，采用新颖的空间域分解方案。

Result: QuaTrEx能处理多达84,480个原子的设备，在Alps和Frontier超级计算机上扩展性好，在42,240个原子上能维持亿亿次级FP64性能。

Conclusion: 成功实现了能处理实际尺寸NRFET的NEGF+GW方案，具有良好的计算性能。

Abstract: Designing nanoscale electronic devices such as the currently manufactured
nanoribbon field-effect transistors (NRFETs) requires advanced modeling tools
capturing all relevant quantum mechanical effects. State-of-the-art approaches
combine the non-equilibrium Green's function (NEGF) formalism and density
functional theory (DFT). However, as device dimensions do not exceed a few
nanometers anymore, electrons are confined in ultra-small volumes, giving rise
to strong electron-electron interactions. To account for these critical
effects, DFT+NEGF solvers should be extended with the GW approximation, which
massively increases their computational intensity. Here, we present the first
implementation of the NEGF+GW scheme capable of handling NRFET geometries with
dimensions comparable to experiments. This package, called QuaTrEx, makes use
of a novel spatial domain decomposition scheme, can treat devices made of up to
84,480 atoms, scales very well on the Alps and Frontier supercomputers (>80%
weak scaling efficiency), and sustains an exascale FP64 performance on 42,240
atoms (1.15 Eflop/s).

</details>


### [66] [CARMA: Collocation-Aware Resource Manager with GPU Memory Estimator](https://arxiv.org/abs/2508.19073)
*Ehsan Yousefzadeh-Asl-Miandoab,Reza Karimzadeh,Bulat Ibragimov,Florina M. Ciorba,Pınar Tözün*

Main category: cs.DC

TL;DR: 研究发现GPU在企业级基础设施中利用率低，提出CARMA系统解决DL任务搭配挑战，评估显示提升了GPU利用率、减少执行时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决DL任务在GPU上搭配时出现的内存不足崩溃和资源干扰导致的任务减速问题，提高GPU利用率、服务质量和能源效率。

Method: 提出CARMA系统，包含基于机器学习的GPU内存估计框架GPUMemNet，引入搭配策略限制GPU利用率，还提出恢复方法确保崩溃任务重启。

Result: 在模拟真实DL训练任务轨迹上的评估显示，CARMA使GPU利用率随时间提高39.3%，端到端执行时间减少约26.7%，GPU能耗降低约14.2%。

Conclusion: CARMA系统能有效解决DL任务在GPU上搭配的问题，提升GPU性能和能源效率。

Abstract: Studies conducted on enterprise-scale infrastructure have shown that GPUs --
the core computational resource for deep learning (DL) training -- are often
significantly underutilized. DL task collocation on GPUs is an opportunity to
address this challenge. However, it may result in (1) out-of-memory crashes for
the subsequently arriving task and (2) slowdowns for all tasks sharing the GPU
due to resource interference. The former challenge poses a threat to
robustness, while the latter affects the quality of service and energy
efficiency.
  We propose CARMA, a server-scale task-level collocation-aware resource
management system that handles both collocation challenges. CARMA encompasses
GPUMemNet, a novel ML-based GPU memory estimator framework for DL training
tasks, to minimize out-of-memory errors and introduces collocation policies
that cap GPU utilization to minimize interference. Furthermore, CARMA
introduces a recovery method to ensure robust restart of tasks that crash. Our
evaluation on traces modeled after real-world DL training task traces shows
that CARMA increases the GPU utilization over time by 39.3\%, decreases the
end-to-end execution time by $\sim$26.7\%, and reduces the GPU energy use by
$\sim$14.2\%.

</details>


### [67] [SIREN: Software Identification and Recognition in HPC Systems](https://arxiv.org/abs/2508.18950)
*Thomas Jakobsche,Fredrik Robertsén,Jessica R. Jones,Utz-Uwe Haus,Florina M. Ciorba*

Main category: cs.DC

TL;DR: 介绍用于HPC系统软件识别和识别的SIREN框架，LUMI上部署显示其有效。


<details>
  <summary>Details</summary>
Motivation: 传统HPC软件识别方法不可靠，需更好方法分析复杂多样的HPC工作负载，实现系统优化和安全提升。

Method: 引入SIREN框架，分析进程元数据、环境信息和可执行文件模糊哈希。

Result: 在LUMI首次选择加入的部署活动中，SIREN能洞察软件使用情况，识别已知应用重复执行和未知应用。

Conclusion: SIREN框架有助于提高HPC系统的可观测性，可用于软件识别和识别。

Abstract: HPC systems use monitoring and operational data analytics to ensure
efficiency, performance, and orderly operations. Application-specific insights
are crucial for analyzing the increasing complexity and diversity of HPC
workloads, particularly through the identification of unknown software and
recognition of repeated executions, which facilitate system optimization and
security improvements. However, traditional identification methods using job or
file names are unreliable for arbitrary user-provided names (a.out). Fuzzy
hashing of executables detects similarities despite changes in executable
version or compilation approach while preserving privacy and file integrity,
overcoming these limitations. We introduce SIREN, a process-level data
collection framework for software identification and recognition. SIREN
improves observability in HPC by enabling analysis of process metadata,
environment information, and executable fuzzy hashes. Findings from a first
opt-in deployment campaign on LUMI show SIREN's ability to provide insights
into software usage, recognition of repeated executions of known applications,
and similarity-based identification of unknown applications.

</details>


### [68] [Deep Learning-Enabled Supercritical Flame Simulation at Detailed Chemistry and Real-Fluid Accuracy Towards Trillion-Cell Scale](https://arxiv.org/abs/2508.18969)
*Zhuoqiang Guo,Runze Mao,Lijun Liu,Guangming Tan,Weile Jia,Zhi X. Chen*

Main category: cs.DC

TL;DR: 优化超临界火焰模拟软件DeepFlame，实现大规模模拟，提升计算能力用于火箭发动机燃烧模拟。


<details>
  <summary>Details</summary>
Motivation: 过去超临界火焰模拟受限于数百万网格单元，需提升模拟规模和效率。

Method: 从并行计算、计算效率和I/O性能三方面优化DeepFlame软件。

Result: 优化后的DeepFlame实现高达6180亿和1540亿网格单元的模拟，在不同超算上达到高计算性能。

Conclusion: 该突破使高保真超临界火焰建模成为下一代火箭推进和超高能量密度系统的关键设计工具。

Abstract: For decades, supercritical flame simulations incorporating detailed chemistry
and real-fluid transport have been limited to millions of cells, constraining
the resolved spatial and temporal scales of the physical system. We optimize
the supercritical flame simulation software DeepFlame -- which incorporates
deep neural networks while retaining the real-fluid mechanical and chemical
accuracy -- from three perspectives: parallel computing, computational
efficiency, and I/O performance. Our highly optimized DeepFlame achieves
supercritical liquid oxygen/methane (LOX/\ce{CH4}) turbulent combustion
simulation of up to 618 and 154 billion cells with unprecedented
time-to-solution, attaining 439/1186 and 187/316 PFlop/s (32.3\%/21.8\% and
37.4\%/31.8\% of the peak) in FP32/mixed-FP16 precision on Sunway (98,304
nodes) and Fugaku (73,728 nodes) supercomputers, respectively. This
computational capability surpasses existing capacities by three orders of
magnitude, enabling the first practical simulation of rocket engine combustion
with >100 LOX/\ce{CH4} injectors. This breakthrough establishes high-fidelity
supercritical flame modeling as a critical design tool for next-generation
rocket propulsion and ultra-high energy density systems.

</details>


### [69] [Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices](https://arxiv.org/abs/2508.19078)
*Fahao Chen,Jie Wan,Peng Li,Zhou Su,Dongxiao Yu*

Main category: cs.DC

TL;DR: 提出系统FLUX实现基于混合专家（MoE）的大语言模型（LLM）联邦微调，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于MoE的LLM联邦微调因计算需求大、参与者资源受限而具挑战性，现有方法因不切实际的系统假设和未考虑MoE特性无法达到理想性能。

Method: 提出FLUX系统，包括基于量化的本地分析、自适应层感知专家合并、使用探索-利用策略的动态专家角色分配。

Result: 在LLaMA - MoE和DeepSeek - MoE的多个基准数据集上实验，FLUX显著优于现有方法，时间精度提升高达4.75倍。

Conclusion: FLUX能有效实现基于MoE的LLM在计算资源受限参与者间的联邦微调，可最小化时间精度。

Abstract: Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models
(LLMs) is challenging due to their massive computational requirements and the
resource constraints of participants. Existing working attempts to fill this
gap through model quantization, computation offloading, or expert pruning.
However, they cannot achieve desired performance due to impractical system
assumptions and a lack of consideration for MoE-specific characteristics. In
this paper, we propose FLUX, a system designed to enable federated fine-tuning
of MoE-based LLMs across participants with constrained computing resources
(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX
introduces three key innovations: (1) quantization-based local profiling to
estimate expert activation with minimal overhead, (2) adaptive layer-aware
expert merging to reduce resource consumption while preserving accuracy, and
(3) dynamic expert role assignment using an exploration-exploitation strategy
to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE
and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX
significantly outperforms existing methods, achieving up to 4.75X speedup in
time-to-accuracy.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [70] [Integral Online Algorithms for Set Cover and Load Balancing with Convex Objectives](https://arxiv.org/abs/2508.18383)
*Thomas Kesselheim,Marco Molinaro,Kalen Patton,Sahil Singla*

Main category: cs.DS

TL;DR: 本文绕过凸松弛和原始对偶技术，为带凸目标的在线集合覆盖和负载均衡问题设计整数在线算法，成果可拓展到多个相关问题和规范组合设置。


<details>
  <summary>Details</summary>
Motivation: 现有依赖在线原始对偶框架针对一般规范和凸目标的算法仅适用于分数设置，本文旨在为带凸目标的集合覆盖和负载均衡问题直接设计整数在线算法。

Method: 先将问题转化为在线打包问题，再为其设计近似算法。解打包问题时，把全局打包问题分解为局部问题，并为机器选择随机激活阈值。

Result: 1. 将相关研究结果从分数设置拓展到整数设置；2. 结果适用于在线广义调度问题；3. 方法可拓展到规范的不相交组合设置。

Conclusion: 所提出的方法有效，能解决带凸目标的在线集合覆盖和负载均衡问题，并拓展到相关问题和设置。

Abstract: Online Set Cover and Load Balancing are central problems in online
optimization, and there is a long line of work on developing algorithms for
these problems with convex objectives. Although we know optimal online
algorithms with $\ell_p$-norm objectives, recent developments for general norms
and convex objectives that rely on the online primal-dual framework apply only
to fractional settings due to large integrality gaps.
  Our work focuses on directly designing integral online algorithms for Set
Cover and Load Balancing with convex objectives, bypassing the
convex-relaxation and the primal-dual technique. Some of the main implications
are:
  1. For Online Set Cover, we can extend the results of Azar et. al. (2016) for
convex objectives and of Kesselheim, Molinaro, and Singla (2024) for symmetric
norms from fractional to integral settings.
  2. Our results for convex objectives and symmetric norms even apply to the
online generalized scheduling problem, which generalizes both Set Cover and
Load Balancing. Previous works could only handle the offline version of this
problem with norm objectives (Deng, Li, and Rabani 2023).
  3. Our methods easily extend to settings with disjoint-composition of norms.
This allows us to recover or improve the norm-composition results of Nagarajan
and Shen (2020), and Kesselheim, Molinaro, and Singla (2024), and to extend our
results to a large class of norms beyond symmetric.
  Our approach is to first reduce these problems to online packing problems,
and then to design good approximation algorithms for the latter. To solve these
packing problems, we use two key ideas. First, we decouple the global packing
problem into a series of local packing problems on different machines. Next, we
choose random activation thresholds for machines such that conditional on a
machine being activated, the expected number of jobs it covers is high compared
to its cost.

</details>


### [71] [Improving Pinwheel Density Bounds for Small Minimums](https://arxiv.org/abs/2508.18422)
*Ahan Mishra,Parker Rho,Robert Kleinberg*

Main category: cs.DS

TL;DR: 论文证明了m = 4时的密度界限为0.84，开发了新技巧。


<details>
  <summary>Details</summary>
Motivation: 一般pinwheel实例可调度性的密度界限为5/6 ，当实例最小元素m较大时有更好界限，此前研究关注m与'密度差距'关系，需确定m = 4时更好的密度界限。

Method: 开发了基于启发式的快速pinwheel求解器和展开操作等新技巧。

Result: 证明了m = 4时密度界限为0.84。

Conclusion: 对于m = 4可得到比5/6更优的密度界限，新技巧有助于相关研究。

Abstract: The density bound for schedulability for general pinwheel instances is
$\frac{5}{6}$, but density bounds better than $\frac{5}{6}$ can be shown for
cases in which the minimum element $m$ of the instance is large. Several recent
works have studied the question of the 'density gap' as a function of $m$, with
best known lower and upper bounds of $O \left( \frac{1}{m} \right)$ and $O
\left( \frac{1}{\sqrt{m}} \right)$. We prove a density bound of $0.84$ for $m =
4$, the first $m$ for which a bound strictly better than $\frac{5}{6} =
0.8\overline{3}$ can be proven. In doing so, we develop new techniques,
particularly a fast heuristic-based pinwheel solver and an unfolding operation.

</details>


### [72] [Hypergraph Splitting-Off via Element-Connectivity Preserving Reductions](https://arxiv.org/abs/2508.18637)
*Karthekeyan Chandrasekaran,Chandra Chekuri,Shubhang Kulkarni*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: B\'erczi, Chandrasekaran, Kir\'aly, and Kulkarni (ICALP 2024) recently
described a splitting-off procedure in hypergraphs that preserves
local-connectivity and outlined some applications. In this note we give an
alternative proof via element-connectivity preserving reduction operations in
graphs.

</details>


### [73] [Graph Traversal via Connected Mobile Agents](https://arxiv.org/abs/2508.18683)
*Saswata Jana,Giuseppe F. Italiano,Partha Sarathi Mandal*

Main category: cs.DS

TL;DR: 本文研究多智能体协调框架下的k - 智能体哈密顿游走问题(k - HWP)，提出不同图结构下的近似或最优算法。


<details>
  <summary>Details</summary>
Motivation: 此前未在具有连通性的多智能体系统中探索该问题，且k - HWP是著名哈密顿游走问题的推广，有研究价值。

Method: 针对2 - HWP在任意图上提出(3 - 1/21) - 近似算法；在树结构上定义受限问题并给出任意k值的最优算法；在k - 一致超图上提出2(1 + ln k) - 近似算法。

Result: 得到不同图结构下的近似或最优算法，且结果可用于设计k = O(1)时一般图上k - HWP的近似算法。

Conclusion: 为多智能体协调框架下的k - HWP问题提供了有效的算法解决方案。

Abstract: This paper considers the Hamiltonian walk problem in the multi-agent
coordination framework, referred to as $k$-agents Hamiltonian walk problem
($k$-HWP). In this problem, a set of $k$ connected agents collectively compute
a spanning walk of a given undirected graph in the minimum steps. At each step,
the agents are at $k$ distinct vertices and the induced subgraph made by the
occupied vertices remains connected. In the next consecutive steps, each agent
may remain stationary or move to one of its neighbours.To the best of our
knowledge, this problem has not been previously explored in the context of
multi-agent systems with connectivity. As a generalization of the well-known
Hamiltonian walk problem (when $k=1$), $k$-HWP is NP-hard. We propose a
$(3-\frac{1}{21})$-approximation algorithm for 2-HWP on arbitrary graphs. For
the tree, we define a restricted version of the problem and present an optimal
algorithm for arbitrary values of $k$. Finally, we formalize the problem for
$k$-uniform hypergraphs and present a $2(1+\ln k)$-approximation algorithm.
This result is also adapted to design an approximation algorithm for $k$-HWP on
general graphs when $k = O(1)$.

</details>


### [74] [Max-Min and 1-Bounded Space Algorithms for the Bin Packing Problem](https://arxiv.org/abs/2508.18718)
*Hiroshi Fujiwara,Rina Atsumi,Hiroaki Yamamoto*

Main category: cs.DS

TL;DR: 本文证明了近似算法MM的渐近近似比至多为1.5，分析了相关算法子类理论性能界，还将分析扩展到基数约束装箱问题。


<details>
  <summary>Details</summary>
Motivation: 研究一维装箱问题中Zhu提出的近似算法MM的性能，以及相关算法子类的理论性能界，并将理论分析扩展到基数约束装箱问题。

Method: 先证明MM算法的渐近近似比，接着综合分析由max - min算法和1 - 有界空间算法派生的子类的理论性能界，最后将理论分析扩展到新问题。

Result: 证明MM算法渐近近似比至多为1.5，得出两类算法交集的下界为1.25，并对基数约束装箱问题进行了理论分析。

Conclusion: 对MM算法及相关算法子类的性能有了理论上的界定，且将理论分析拓展到新问题，为装箱问题研究提供了更多理论依据。

Abstract: In the (1-dimensional) bin packing problem, we are asked to pack all the
given items into bins, each of capacity one, so that the number of non-empty
bins is minimized. Zhu~[Chaos, Solitons \& Fractals 2016] proposed an
approximation algorithm $MM$ that sorts the item sequence in a non-increasing
order by size at the beginning, and then repeatedly packs, into the current
single open bin, first as many of the largest items in the remaining sequence
as possible and then as many of the smallest items in the remaining sequence as
possible. In this paper we prove that the asymptotic approximation ratio of
$MM$ is at most 1.5. Next, focusing on the fact that $MM$ is at the
intersection of two algorithm classes, max-min algorithms and 1-bounded space
algorithms, we comprehensively analyze the theoretical performance bounds of
each subclass derived from the two classes. Our results include a lower bound
of 1.25 for the intersection of the two classes. Furthermore, we extend the
theoretical analysis over algorithm classes to the cardinality constrained bin
packing problem.

</details>


### [75] [DTC: Real-Time and Accurate Distributed Triangle Counting in Fully Dynamic Graph Streams](https://arxiv.org/abs/2508.19057)
*Wei Xuan,Yan Liang,Huawei Cao,Ning Lin,Xiaochun Ye,Dongrui Fan*

Main category: cs.DS

TL;DR: 提出DTC系列单通分布式流算法用于动态图流三角计数，实验显示优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 精确三角计数在大规模图流中不可行，现有分布式流算法缺乏适应性且难处理边删除。

Method: 提出DTC - AR算法在无图大小先验知识下估计三角计数，引入DTC - FD算法处理全动态图流，使用随机配对和未来边插入补偿。

Result: DTC - AR准确性最高提升2029.4倍和27.1倍，DTC - FD估计误差最多降低32.5倍和19.3倍，且随图流大小线性扩展。

Conclusion: 所提算法在实际场景处理三角计数有效，代码和数据集已公开。

Abstract: Triangle counting is a fundamental problem in graph mining, essential for
analyzing graph streams with arbitrary edge orders. However, exact counting
becomes impractical due to the massive size of real-world graph streams. To
address this, approximate algorithms have been developed, but existing
distributed streaming algorithms lack adaptability and struggle with edge
deletions. In this article, we propose DTC, a novel family of single-pass
distributed streaming algorithms for global and local triangle counting in
fully dynamic graph streams. Our DTC-AR algorithm accurately estimates triangle
counts without prior knowledge of graph size, leveraging multi-machine
resources. Additionally, we introduce DTC-FD, an algorithm tailored for fully
dynamic graph streams, incorporating edge insertions and deletions. Using
Random Pairing and future edge insertion compensation, DTC-FD achieves unbiased
and accurate approximations across multiple machines. Experimental results
demonstrate significant improvements over baselines. DTC-AR achieves up to
$2029.4\times$ and $27.1\times$ more accuracy, while maintaining the best
trade-off between accuracy and storage space. DTC-FD reduces estimation errors
by up to $32.5\times$ and $19.3\times$, scaling linearly with graph stream
size. These findings highlight the effectiveness of our proposed algorithms in
tackling triangle counting in real-world scenarios. The source code and
datasets are released and available at
\href{https://github.com/wayne4s/srds-dtc.git}{https://github.com/wayne4s/srds-dtc.git}.

</details>


### [76] [Approximating High-Dimensional Earth Mover's Distance as Fast as Closest Pair](https://arxiv.org/abs/2508.06774)
*Lorenzo Beretta,Vincent Cohen-Addad,Rajesh Jayaram,Erik Waingarten*

Main category: cs.DS

TL;DR: 本文将(1+ε)-近似地球移动距离(EMD)问题归约为(1+ε)-近似最近点对(CP)问题，改进了高维EMD的近似算法。


<details>
  <summary>Details</summary>
Motivation: 改进高维EMD已知最快的近似算法。

Method: 将(1+ε)-近似EMD归约为(1+ε)-近似CP；提出EMD的乘性权重更新框架的亚线性实现，利用几何结构隐式执行更新。

Result: 若(1+ε)-近似CP能在n^{2 - φ}时间内计算，则(1+O(ε))-近似EMD能在n^{2 - Ω(φ)}时间内计算；使用已知最快的CP算法，得到高维点集的(1+ε)-近似EMD算法，运行时间为n^{2 - Õ(ε^{1/3})}，优于之前的n^{2 - Ω(ε^2)}。

Conclusion: 通过问题归约和技术创新，改进了高维EMD近似算法的运行时间。

Abstract: We give a reduction from $(1+\varepsilon)$-approximate Earth Mover's Distance
(EMD) to $(1+\varepsilon)$-approximate Closest Pair (CP). As a consequence, we
improve the fastest known approximation algorithm for high-dimensional EMD.
Here, given $p\in [1, 2]$ and two sets of $n$ points $X,Y \subseteq (\mathbb
R^d,\ell_p)$, their EMD is the minimum cost of a perfect matching between $X$
and $Y$, where the cost of matching two vectors is their $\ell_p$ distance.
Further, CP is the basic problem of finding a pair of points realizing $\min_{x
\in X, y\in Y} ||x-y||_p$. Our contribution is twofold: we show that if a
$(1+\varepsilon)$-approximate CP can be computed in time $n^{2-\phi}$, then a
$1+O(\varepsilon)$ approximation to EMD can be computed in time
$n^{2-\Omega(\phi)}$; plugging in the fastest known algorithm for CP [Alman,
Chan, Williams FOCS'16], we obtain a $(1+\varepsilon)$-approximation algorithm
for EMD running in time $n^{2-\tilde{\Omega}(\varepsilon^{1/3})}$ for
high-dimensional point sets, which improves over the prior fastest running time
of $n^{2-\Omega(\varepsilon^2)}$ [Andoni, Zhang FOCS'23]. Our main technical
contribution is a sublinear implementation of the Multiplicative Weights Update
framework for EMD. Specifically, we demonstrate that the updates can be
executed without ever explicitly computing or storing the weights; instead, we
exploit the underlying geometric structure to perform the updates implicitly.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [77] [Facilitating Matches on Allocation Platforms](https://arxiv.org/abs/2508.18325)
*Yohai Trabelsi,Abhijin Adiga,Yonatan Aumann,Sarit Kraus,S. S. Ravi*

Main category: cs.GT

TL;DR: 本文研究分配平台中分配促进者在约束条件下选择最优限制放松集的优化问题，给出问题定义、求解算法并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 分配促进者旨在提高分配的整体效用/社会福利，同时保证不损害部分代理利益，且受预算约束，需解决选择最优限制放松集的问题。

Method: 给出问题的形式化定义，定义参与保障层次和多种社会福利函数；提供多项式算法解决不同版本的优化问题；使用三个真实世界数据集进行大量实验。

Result: 成功给出问题定义和求解算法，通过实验展示了促进和放松的好处以及不同参与保障的影响。

Conclusion: 本文的方法能够有效解决分配促进者在约束条件下的优化问题，且不同参与保障有不同影响。

Abstract: We consider a setting where goods are allocated to agents by way of an
allocation platform (e.g., a matching platform). An ``allocation facilitator''
aims to increase the overall utility/social-good of the allocation by
encouraging (some of the) agents to relax (some of) their restrictions. At the
same time, the advice must not hurt agents who would otherwise be better off.
Additionally, the facilitator may be constrained by a ``bound'' (a.k.a.
`budget'), limiting the number and/or type of restrictions it may seek to
relax. We consider the facilitator's optimization problem of choosing an
optimal set of restrictions to request to relax under the aforementioned
constraints. Our contributions are three-fold: (i) We provide a formal
definition of the problem, including the participation guarantees to which the
facilitator should adhere. We define a hierarchy of participation guarantees
and also consider several social-good functions. (ii) We provide polynomial
algorithms for solving various versions of the associated optimization
problems, including one-to-one and many-to-one allocation settings. (iii) We
demonstrate the benefits of such facilitation and relaxation, and the
implications of the different participation guarantees, using extensive
experimentation on three real-world datasets.

</details>


### [78] [Partitioned Combinatorial Optimization Games](https://arxiv.org/abs/2508.18449)
*Jiehua Chen,Christian Hatschka,Sofia Simola*

Main category: cs.GT

TL;DR: 提出d分区组合优化博弈（PCOGs），研究核心相关两个基本问题并分析四个经典图优化任务算法复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究新型合作博弈PCOGs核心的稳定性验证和存在性问题。

Method: 提出PCOGs的定义，针对四个经典图优化任务分析相关问题的算法复杂度。

Result: 对核心的两个基本问题，分析了四个经典图优化任务的算法复杂度。

Conclusion: 完成了对PCOGs核心相关两个基本问题在四个经典图优化任务上的算法复杂度分析。

Abstract: We propose a class of cooperative games, called d Partitioned Compbinatorial
Optimization Games (PCOGs). The input of PCOG consists of a set of agents and a
combinatorial structure (typically a graph) with a fixed optimization goal on
this structure (e.g., finding a minimum dominating set on a graph) such that
the structure is divided among the agents. The value of each coalition of
agents is derived from the optimal solution for the part of the structure
possessed by the coalition. We study two fundamental questions related to the
core: Core Stability Verification and Core Stability Existence. We analyze the
algorithmic complexity of both questions for four classic graph optimization
tasks: minimum vertex cover, minimum dominating set, minimum spanning tree, and
maximum matching.

</details>


### [79] [Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics](https://arxiv.org/abs/2508.18600)
*Ayato Kitadai,Yusuke Fukasawa,Nariaki Nishino*

Main category: cs.GT

TL;DR: 使用基于角色的方法调整大语言模型偏差，在最后通牒游戏中改善模拟与实证行为的一致性，展示了模拟人类决策模式的潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在固有偏差，与真实人类行为有差异，限制了反映群体多样性的能力。

Method: 采用基于角色的方法，利用行为经济学中的个体行为数据调整模型偏差。

Result: 应用于最后通牒游戏时，改善了模拟行为与实证行为的一致性，尤其是在回应者方面。

Conclusion: 虽然特质表示还需进一步完善，但基于角色的大语言模型在大规模模拟人类决策模式上有前景。

Abstract: Large language models (LLMs) are increasingly used to simulate human
decision-making, but their intrinsic biases often diverge from real human
behavior--limiting their ability to reflect population-level diversity. We
address this challenge with a persona-based approach that leverages
individual-level behavioral data from behavioral economics to adjust model
biases. Applying this method to the ultimatum game--a standard but difficult
benchmark for LLMs--we observe improved alignment between simulated and
empirical behavior, particularly on the responder side. While further
refinement of trait representations is needed, our results demonstrate the
promise of persona-conditioned LLMs for simulating human-like decision patterns
at scale.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [80] [REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking](https://arxiv.org/abs/2508.18379)
*Pinhuan Wang,Zhiqiu Xia,Chunhua Liao,Feiyi Wang,Hang Liu*

Main category: cs.IR

TL;DR: 提出不确定性感知重排框架REALM处理大语言模型文档重排问题，实验表明其优于现有方法并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的文档重排方法存在排名不确定性、top-k恢复不稳定和高令牌成本等问题。

Method: 提出REALM框架，将大语言模型得出的相关性建模为高斯分布，并通过递归贝叶斯更新进行优化。

Result: REALM超越了现有重排器，显著减少了令牌使用和延迟。

Conclusion: REALM可作为现代信息检索系统的下一代重排器。

Abstract: Large Language Models (LLMs) have shown strong capabilities in document
re-ranking, a key component in modern Information Retrieval (IR) systems.
However, existing LLM-based approaches face notable limitations, including
ranking uncertainty, unstable top-k recovery, and high token cost due to
token-intensive prompting. To effectively address these limitations, we propose
REALM, an uncertainty-aware re-ranking framework that models LLM-derived
relevance as Gaussian distributions and refines them through recursive Bayesian
updates. By explicitly capturing uncertainty and minimizing redundant queries,
REALM achieves better rankings more efficiently. Experimental results
demonstrate that our REALM surpasses state-of-the-art re-rankers while
significantly reducing token usage and latency, promoting it as the
next-generation re-ranker for modern IR systems.

</details>


### [81] [DenseRec: Revisiting Dense Content Embeddings for Sequential Transformer-based Recommendation](https://arxiv.org/abs/2508.18442)
*Jan Malte Lichtenberg,Antonio De Candia,Matteo Ruffini*

Main category: cs.IR

TL;DR: 传统基于Transformer的推荐器易受物品冷启动问题影响，本文提出DenseRec方法，实验显示其能有效解决冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的顺序推荐器仅依赖物品ID嵌入，易受物品冷启动问题影响，直接集成预训练模型的密集内容嵌入效果不佳。

Method: 提出DenseRec方法，采用双路径嵌入，在训练时学习从密集嵌入空间到ID嵌入空间的线性投影。

Result: 在三个真实数据集上，DenseRec即使不进行额外超参数调整且使用紧凑嵌入模型，也始终优于仅使用ID的SASRec基线。

Conclusion: DenseRec能在存在未见物品时提供更好的序列表示，是解决冷启动顺序推荐的实用且强大的解决方案。

Abstract: Transformer-based sequential recommenders, such as SASRec or BERT4Rec,
typically rely solely on learned item ID embeddings, making them vulnerable to
the item cold-start problem, particularly in environments with dynamic item
catalogs. While dense content embeddings from pre-trained models offer
potential solutions, direct integration into transformer-based recommenders has
consistently underperformed compared to ID-only approaches. We revisit this
integration challenge and propose DenseRec, a simple yet effective method that
introduces a dual-path embedding approach. DenseRec learns a linear projection
from the dense embedding space into the ID embedding space during training,
enabling seamless generalization to previously unseen items without requiring
specialized embedding models or complex infrastructure. In experiments on three
real-world datasets, we find DenseRec to consistently outperform an ID-only
SASRec baseline, even without additional hyperparameter tuning and while using
compact embedding models. Our analysis suggests improvements primarily arise
from better sequence representations in the presence of unseen items,
positioning DenseRec as a practical and robust solution for cold-start
sequential recommendation.

</details>


### [82] [Extracting Information from Scientific Literature via Visual Table Question Answering Models](https://arxiv.org/abs/2508.18661)
*Dongyoun Kim,Hyung-do Choi,Youngsun Jang,John Kim*

Main category: cs.IR

TL;DR: 研究探索三种处理科学论文表格数据的方法以增强抽取式问答，开发系统评价软件工具，发现保留表格结构的方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 增强抽取式问答，开发系统评价过程的软件工具。

Method: 评估三种方法，包括OCR提取文档信息、预训练模型进行文档视觉问答、表格检测和结构识别提取合并表格与文本关键信息以回答抽取式问题，并进行探索性实验。

Result: 保留表格结构的方法表现优于其他方法，准确识别文档中的特定符号是提高结果的关键因素。

Conclusion: 保留表格的结构完整性对提高科学文档抽取式问答的准确性和可靠性至关重要。

Abstract: This study explores three approaches to processing table data in scientific
papers to enhance extractive question answering and develop a software tool for
the systematic review process. The methods evaluated include: (1) Optical
Character Recognition (OCR) for extracting information from documents, (2)
Pre-trained models for document visual question answering, and (3) Table
detection and structure recognition to extract and merge key information from
tables with textual content to answer extractive questions. In exploratory
experiments, we augmented ten sample test documents containing tables and
relevant content against RF- EMF-related scientific papers with seven
predefined extractive question-answer pairs. The results indicate that
approaches preserving table structure outperform the others, particularly in
representing and organizing table content. Accurately recognizing specific
notations and symbols within the documents emerged as a critical factor for
improved results. Our study concludes that preserving the structural integrity
of tables is essential for enhancing the accuracy and reliability of extractive
question answering in scientific documents.

</details>


### [83] [Membership Inference Attacks on LLM-based Recommender Systems](https://arxiv.org/abs/2508.18665)
*Jiajie He,Yuechun Gu,Min-Chun Chen,Keke Chen*

Main category: cs.IR

TL;DR: 本文设计四种成员推理攻击（MIAs）评估大语言模型推荐系统（LLM RecSys）隐私风险，结果表明攻击威胁现实存在。


<details>
  <summary>Details</summary>
Motivation: LLM RecSys利用上下文学习定制推荐功能时可能暴露用户隐私信息，但该重要问题尚无研究。

Method: 设计直接询问、幻觉、相似性和投毒四种MIAs攻击，在三个用于开发上下文学习LLM RecSys的大语言模型和两个著名推荐系统基准数据集上评估。

Result: 证实LLM RecSys面临的MIA威胁现实存在，直接询问和投毒攻击有显著高攻击优势。

Conclusion: 分析了影响攻击的因素，如系统提示中的样本数量和受害者在样本中的位置。

Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly
adapt recommendation systems to different domains. It utilizes in-context
learning (ICL), i.e., the prompts, to customize the recommendation functions,
which include sensitive historical user-specific item interactions, e.g.,
implicit feedback like clicked items or explicit product reviews. Such private
information may be exposed to novel privacy attack. However, no study has been
done on this important issue. We design four membership inference attacks
(MIAs), aiming to reveal whether victims' historical interactions have been
used by system prompts. They are \emph{direct inquiry, hallucination,
similarity, and poisoning attacks}, each of which utilizes the unique features
of LLMs or RecSys. We have carefully evaluated them on three LLMs that have
been used to develop ICL-LLM RecSys and two well-known RecSys benchmark
datasets. The results confirm that the MIA threat on LLM RecSys is realistic:
direct inquiry and poisoning attacks showing significantly high attack
advantages. We have also analyzed the factors affecting these attacks, such as
the number of shots in system prompts and the position of the victim in the
shots.

</details>


### [84] [Taming the One-Epoch Phenomenon in Online Recommendation System by Two-stage Contrastive ID Pre-training](https://arxiv.org/abs/2508.18700)
*Yi-Ping Hsu,Po-Wei Wang,Chantat Eksombatchai,Jiajing Xu*

Main category: cs.IR

TL;DR: 提出一种两阶段训练策略解决基于ID嵌入的单轮训练问题，离线实验和Pinterest线上部署均取得好效果。


<details>
  <summary>Details</summary>
Motivation: 基于ID的嵌入易过拟合，存在“单轮训练问题”，需在首轮优化性能。

Method: 引入两阶段训练策略，预训练阶段用最小模型和对比损失，进行多轮训练。

Result: 离线实验表明预训练多轮不导致过拟合，微调后嵌入能提升在线泛化性；Pinterest线上部署获显著全站参与度提升。

Conclusion: 提出的两阶段训练策略有效，可解决“单轮训练问题”，提升推荐系统性能。

Abstract: ID-based embeddings are widely used in web-scale online recommendation
systems. However, their susceptibility to overfitting, particularly due to the
long-tail nature of data distributions, often limits training to a single
epoch, a phenomenon known as the "one-epoch problem." This challenge has driven
research efforts to optimize performance within the first epoch by enhancing
convergence speed or feature sparsity. In this study, we introduce a novel
two-stage training strategy that incorporates a pre-training phase using a
minimal model with contrastive loss, enabling broader data coverage for the
embedding system. Our offline experiments demonstrate that multi-epoch training
during the pre-training phase does not lead to overfitting, and the resulting
embeddings improve online generalization when fine-tuned for more complex
downstream recommendation tasks. We deployed the proposed system in live
traffic at Pinterest, achieving significant site-wide engagement gains.

</details>


### [85] [Optimization of Latent-Space Compression using Game-Theoretic Techniques for Transformer-Based Vector Search](https://arxiv.org/abs/2508.18877)
*Kushagra Agrawal,Nisharg Nargund,Oishani Banerjee*

Main category: cs.IR

TL;DR: 提出用于优化潜在空间压缩的博弈论框架，提升向量搜索效率和语义效用，实验表现优于FAISS。


<details>
  <summary>Details</summary>
Motivation: 现代信息检索系统中基于transformer嵌入的向量相似性搜索受高维潜在表示影响，扩展性和效率受限。

Method: 将压缩策略建模为检索准确性和存储效率之间的零和博弈，推导出保留语义相似性并减少冗余的潜在变换。

Result: 与FAISS相比，平均相似度更高（0.9981 vs. 0.5517），效用更高（0.8873 vs. 0.5194），但查询时间有适度增加。

Conclusion: 博弈论潜在压缩在高效用、基于transformer的搜索应用中有实用价值，可集成到现有LLM管道。

Abstract: Vector similarity search plays a pivotal role in modern information retrieval
systems, especially when powered by transformer-based embeddings. However, the
scalability and efficiency of such systems are often hindered by the high
dimensionality of latent representations. In this paper, we propose a novel
game-theoretic framework for optimizing latent-space compression to enhance
both the efficiency and semantic utility of vector search. By modeling the
compression strategy as a zero-sum game between retrieval accuracy and storage
efficiency, we derive a latent transformation that preserves semantic
similarity while reducing redundancy. We benchmark our method against FAISS, a
widely-used vector search library, and demonstrate that our approach achieves a
significantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873
vs. 0.5194), albeit with a modest increase in query time. This trade-off
highlights the practical value of game-theoretic latent compression in
high-utility, transformer-based search applications. The proposed system can be
seamlessly integrated into existing LLM pipelines to yield more semantically
accurate and computationally efficient retrieval.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [86] [Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs](https://arxiv.org/abs/2508.18279)
*Jeesu Jung,Sangkeun Jung*

Main category: cs.LG

TL;DR: 提出用思维深度（DoT）定义任务难度，进行从浅到深的课程学习训练大语言模型，并提出三个可验证假设及评估框架，旨在实现以推理为中心的认知基础、可解释课程。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型的课程学习需要一个与推理一致且可扩展、可解释的难度信号。

Method: 将难度定义为思维深度（DoT），通过计算教师模型推理轨迹中的离散步骤来量化，按DoT进行从浅到深的课程学习，并阐述大规模推导、验证和安排课程的方法。

Result: 提出三个可验证假设：DoT与推理基准上的传统难度相关；在匹配预算下，DoT排序的课程优于按长度或评判得分的课程；在轻度格式控制下，难度在不同教师模型中具有鲁棒性。

Conclusion: 朝着以推理为中心的认知基础、可解释课程迈进。

Abstract: Curriculum learning for training LLMs requires a difficulty signal that
aligns with reasoning while remaining scalable and interpretable. We propose a
simple premise: tasks that demand deeper depth of thought for humans should
also be harder for models. Accordingly, we define difficulty as depth of
thought (DoT) and operationalize it by counting the discrete steps in a teacher
model's reasoning trace (e.g., Chain-of-Thought). We then train with a shallow
to deep curriculum ordered by this DoT and outline how to derive, validate, and
schedule it at scale. Our position yields three testable hypotheses: (i) DoT
correlates with conventional difficulty on reasoning benchmarks, (ii)
DoT-ordered curricula outperform length- or judge-scored curricula under
matched budgets, and (iii) the difficulty is robust across teacher models given
light formatting controls. We propose an evaluation framework and discuss
threats to validity (teacher style, length confounds) alongside practical
mitigations. Taken together, we aim to move toward cognitively grounded,
interpretable curricula for reasoning-centric training.

</details>


### [87] [Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models](https://arxiv.org/abs/2508.18284)
*Rahmat K. Adesunkanmi,Alexander W. Brandt,Masoud Deylami,Gustavo A. Giraldo Echeverri,Hamidreza Karbasian,Adel Alaeddini*

Main category: cs.LG

TL;DR: 提出多模态机器学习框架预测海上漂移物漂移，实验收集数据，经多步骤处理后输入模型预测，与传统方法对比，结果显示该框架能准确适应动态海洋环境。


<details>
  <summary>Details</summary>
Motivation: 准确预测海上漂移物漂移在搜救等场景是关键挑战，需有效方法。

Method: 提出多模态机器学习框架，收集环境和物理数据，用模拟数据训练CNN估计系数，结合编码文本描述输入注意力序列到序列模型预测。

Result: 多模态模型与传统模型表现相当，能进行长期预测。

Conclusion: 多模态建模策略能在动态海洋条件下准确且灵活地预测漂移物漂移。

Abstract: Accurately predicting the drift (displacement) of leeway objects in maritime
environments remains a critical challenge, particularly in time-sensitive
scenarios such as search and rescue operations. In this study, we propose a
multi-modal machine learning framework that integrates Sentence Transformer
embeddings with attention-based sequence-to-sequence architectures to predict
the drift of leeway objects in water. We begin by experimentally collecting
environmental and physical data, including water current and wind velocities,
object mass, and surface area, for five distinct leeway objects. Using
simulated data from a Navier-Stokes-based model to train a convolutional neural
network on geometrical image representations, we estimate drag and lift
coefficients of the leeway objects. These coefficients are then used to derive
the net forces responsible for driving the objects' motion. The resulting time
series, comprising physical forces, environmental velocities, and
object-specific features, combined with textual descriptions encoded via a
language model, are inputs to attention-based sequence-to-sequence
long-short-term memory and Transformer models, to predict future drift
trajectories. We evaluate the framework across multiple time horizons ($1$,
$3$, $5$, and $10$ seconds) and assess its generalization across different
objects. We compare our approach against a fitted physics-based model and
traditional machine learning methods, including recurrent neural networks and
temporal convolutional neural networks. Our results show that these multi-modal
models perform comparably to traditional models while also enabling longer-term
forecasting in place of single-step prediction. Overall, our findings
demonstrate the ability of a multi-modal modeling strategy to provide accurate
and adaptable predictions of leeway object drift in dynamic maritime
conditions.

</details>


### [88] [Quantifying The Limits of AI Reasoning: Systematic Neural Network Representations of Algorithms](https://arxiv.org/abs/2508.18526)
*Anastasis Kratsios,Dennis Zvigelsky,Bradd Hart*

Main category: cs.LG

TL;DR: 本文将推理任务解释为电路仿真，提出元算法将电路转换为前馈神经网络，证明神经网络可完成任何推理任务，并给出应用，结果比经典通用逼近定理更强。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络在完美训练时能执行的推理形式。

Method: 将推理任务解释为电路仿真，提出系统的元算法，通过迭代用规范的ReLU MLP模拟器替换每个门，将电路转换为具有ReLU激活的前馈神经网络。

Result: 构造能精确模拟电路，无近似和舍入；网络神经元数量与电路复杂度相关；给出从模拟最短路径算法到随机布尔电路等应用；结果比经典通用逼近定理更强大。

Conclusion: 没有推理任务超出神经网络的能力范围，神经网络用算法运行时间换取空间复杂度。

Abstract: A main open question in contemporary AI research is quantifying the forms of
reasoning neural networks can perform when perfectly trained. This paper
answers this by interpreting reasoning tasks as circuit emulation, where the
gates define the type of reasoning; e.g. Boolean gates for predicate logic,
tropical circuits for dynamic programming, arithmetic and analytic gates for
symbolic mathematical representation, and hybrids thereof for deeper reasoning;
e.g. higher-order logic.
  We present a systematic meta-algorithm that converts essentially any circuit
into a feedforward neural network (NN) with ReLU activations by iteratively
replacing each gate with a canonical ReLU MLP emulator. We show that, on any
digital computer, our construction emulates the circuit exactly--no
approximation, no rounding, modular overflow included--demonstrating that no
reasoning task lies beyond the reach of neural networks. The number of neurons
in the resulting network (parametric complexity) scales with the circuit's
complexity, and the network's computational graph (structure) mirrors that of
the emulated circuit. This formalizes the folklore that NNs networks trade
algorithmic run-time (circuit runtime) for space complexity (number of
neurons).
  We derive a range of applications of our main result, from emulating
shortest-path algorithms on graphs with cubic--size NNs, to simulating stopped
Turing machines with roughly quadratically--large NNs, and even the emulation
of randomized Boolean circuits. Lastly, we demonstrate that our result is
strictly more powerful than a classical universal approximation theorem: any
universal function approximator can be encoded as a circuit and directly
emulated by a NN.

</details>


### [89] [Data-driven models for production forecasting and decision supporting in petroleum reservoirs](https://arxiv.org/abs/2508.18289)
*Mateus A. Fernandes,Michael M. Furlanetti,Eduardo Gildin,Marcio A. Sampaio*

Main category: cs.LG

TL;DR: 论文提出用数据驱动和机器学习方法解决石油储层工程中产量预测和岩石 - 流体系统行为变化预测问题，经数据处理、方法研究，用合成和真实数据评估，期望设计可靠预测器支持储层管理。


<details>
  <summary>Details</summary>
Motivation: 解决石油储层工程中可靠预测产量和预测岩石 - 流体系统行为变化的挑战。

Method: 采用数据驱动方法，进行生产和注入变量相关性分析、数据预处理；研究监督学习方法如回归和神经网络；先使用合成数据评估，再应用于巴西盐下实际案例。

Result: 期望设计出能重现储层动态、快速响应、处理实际困难的可靠预测器。

Conclusion: 此方法有望用于支持储层管理，实现提高石油采收率等目标。

Abstract: Forecasting production reliably and anticipating changes in the behavior of
rock-fluid systems are the main challenges in petroleum reservoir engineering.
This project proposes to deal with this problem through a data-driven approach
and using machine learning methods. The objective is to develop a methodology
to forecast production parameters based on simple data as produced and injected
volumes and, eventually, gauges located in wells, without depending on
information from geological models, fluid properties or details of well
completions and flow systems. Initially, we performed relevance analyses of the
production and injection variables, as well as conditioning the data to suit
the problem. As reservoir conditions change over time, concept drift is a
priority concern and require special attention to those observation windows and
the periodicity of retraining, which are also objects of study. For the
production forecasts, we study supervised learning methods, such as those based
on regressions and Neural Networks, to define the most suitable for our
application in terms of performance and complexity. In a first step, we
evaluate the methodology using synthetic data generated from the UNISIM III
compositional simulation model. Next, we applied it to cases of real plays in
the Brazilian pre-salt. The expected result is the design of a reliable
predictor for reproducing reservoir dynamics, with rapid response, capability
of dealing with practical difficulties such as restrictions in wells and
processing units, and that can be used in actions to support reservoir
management, including the anticipation of deleterious behaviors, optimization
of production and injection parameters and the analysis of the effects of
probabilistic events, aiming to maximize oil recovery.

</details>


### [90] [A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach](https://arxiv.org/abs/2508.18301)
*Md Sabbir Ahmed,Nova Ahmed*

Main category: cs.LG

TL;DR: 开发快速简约系统，利用1秒内获取的7天应用使用数据识别抑郁，取得较好效果，或有助于欠发达和发展中地区。


<details>
  <summary>Details</summary>
Motivation: 现有检测抑郁的设备系统需长时间数据收集，在早期检测中效果不佳，因此要开发能最快获取数据来识别抑郁的简约系统。

Method: 开发1秒内获取过去7天应用使用数据的工具，收集100名孟加拉国学生数据，用多种特征选择方法选重要特征，构建不同机器学习模型。

Result: 轻梯度提升机模型正确识别82.4%抑郁学生；简约堆叠模型最大精度77.4%；SHAP分析给出与抑郁相关行为标记。

Conclusion: 系统快速简约，或对欠发达和发展中地区识别抑郁有贡献，研究有助于开发低资源系统理解抑郁学生。

Abstract: Background: Existing robust, pervasive device-based systems developed in
recent years to detect depression require data collected over a long period and
may not be effective in cases where early detection is crucial.
  Objective: Our main objective was to develop a minimalistic system to
identify depression using data retrieved in the fastest possible time.
  Methods: We developed a fast tool that retrieves the past 7 days' app usage
data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from
Bangladesh participated in our study, and our tool collected their app usage
data. To identify depressed and nondepressed students, we developed a diverse
set of ML models. We selected important features using the stable approach,
along with 3 main types of feature selection (FS) approaches.
  Results: Leveraging only the app usage data retrieved in 1 second, our light
gradient boosting machine model used the important features selected by the
stable FS approach and correctly identified 82.4% (n=42) of depressed students
(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we
presented a parsimonious stacking model where around 5 features selected by the
all-relevant FS approach Boruta were used in each iteration of validation and
showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis
of our best models presented behavioral markers that were related to
depression.
  Conclusions: Due to our system's fast and minimalistic nature, it may make a
worthwhile contribution to identifying depression in underdeveloped and
developing regions. In addition, our detailed discussion about the implication
of our findings can facilitate the development of less resource-intensive
systems to better understand students who are depressed.

</details>


### [91] [Metric Matters: A Formal Evaluation of Similarity Measures in Active Learning for Cyber Threat Intelligence](https://arxiv.org/abs/2508.19019)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.LG

TL;DR: 提出基于主动学习的异常检测框架，利用相似性搜索优化决策空间，评估不同相似性度量对模型的影响并给出结果。


<details>
  <summary>Details</summary>
Motivation: 高级持续威胁（APTs）具有隐蔽性且检测数据集存在极端类别不平衡问题，对网络防御构成严重挑战。

Method: 提出基于主动学习的异常检测框架，基于注意力自编码器，利用特征空间相似性识别正常和异常实例，对各种相似性度量进行正式评估。

Result: 实验表明相似性度量的选择显著影响模型收敛、异常检测准确性和标签效率。

Conclusion: 为威胁情报和网络防御的主动学习管道中选择相似性函数提供了可行的见解。

Abstract: Advanced Persistent Threats (APTs) pose a severe challenge to cyber defense
due to their stealthy behavior and the extreme class imbalance inherent in
detection datasets. To address these issues, we propose a novel active
learning-based anomaly detection framework that leverages similarity search to
iteratively refine the decision space. Built upon an Attention-Based
Autoencoder, our approach uses feature-space similarity to identify normal-like
and anomaly-like instances, thereby enhancing model robustness with minimal
oracle supervision. Crucially, we perform a formal evaluation of various
similarity measures to understand their influence on sample selection and
anomaly ranking effectiveness. Through experiments on diverse datasets,
including DARPA Transparent Computing APT traces, we demonstrate that the
choice of similarity metric significantly impacts model convergence, anomaly
detection accuracy, and label efficiency. Our results offer actionable insights
for selecting similarity functions in active learning pipelines tailored for
threat intelligence and cyber defense.

</details>


### [92] [Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder](https://arxiv.org/abs/2508.18303)
*Jueqi Wang,Zachary Jacokes,John Darrell Van Horn,Michael C. Schatz,Kevin A. Pelphrey,Archana Venkataraman*

Main category: cs.LG

TL;DR: 提出可解释深度学习框架NeuroPathX，验证其在自闭症和阿尔茨海默病上效果佳，能揭示生物关联。


<details>
  <summary>Details</summary>
Motivation: 传统影像遗传学方法局限于简单线性模型或缺乏可解释性的黑盒技术，需更好方法。

Method: 提出NeuroPathX框架，采用早期融合策略和交叉注意力机制，引入稀疏性和通路相似性损失函数。

Result: NeuroPathX优于基线方法，揭示与疾病相关的生物关联。

Conclusion: NeuroPathX有潜力推动对复杂脑部疾病的理解。

Abstract: While imaging-genetics holds great promise for unraveling the complex
interplay between brain structure and genetic variation in neurological
disorders, traditional methods are limited to simplistic linear models or to
black-box techniques that lack interpretability. In this paper, we present
NeuroPathX, an explainable deep learning framework that uses an early fusion
strategy powered by cross-attention mechanisms to capture meaningful
interactions between structural variations in the brain derived from MRI and
established biological pathways derived from genetics data. To enhance
interpretability and robustness, we introduce two loss functions over the
attention matrix - a sparsity loss that focuses on the most salient
interactions and a pathway similarity loss that enforces consistent
representations across the cohort. We validate NeuroPathX on both autism
spectrum disorder and Alzheimer's disease. Our results demonstrate that
NeuroPathX outperforms competing baseline approaches and reveals biologically
plausible associations linked to the disorder. These findings underscore the
potential of NeuroPathX to advance our understanding of complex brain
disorders. Code is available at https://github.com/jueqiw/NeuroPathX .

</details>


### [93] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 本文提出统一局部鲁棒性框架SALMAN评估基于Transformer的NLP模型稳定性，有DMD衡量样本敏感度，在攻击效率和鲁棒训练上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着预训练Transformer语言模型规模和部署增加，其在输入扰动下的鲁棒性问题愈发紧迫，且现有鲁棒性方法存在局限。

Method: 提出统一局部鲁棒性框架SALMAN，用新颖的距离映射失真（DMD）度量，以近线性复杂度比较输入到输出的距离映射来对样本敏感度排序。

Result: 在攻击效率和鲁棒训练方面取得显著提升。

Conclusion: 该框架是推进基于Transformer的NLP系统可靠性的实用、与模型无关的工具。

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [94] [Estimating oil recovery factor using machine learning: Applications of XGBoost classification](https://arxiv.org/abs/2210.16345)
*Alireza Roustazadeh,Behzad Ghanbarian,Frank Male,Mohammad B. Shadmand,Vahid Taslimitehrani,Larry W. Lake*

Main category: cs.LG

TL;DR: 本文应用XGBoost分类算法，利用可用特征对石油采收率等级进行机器学习估计，结果显示算法有一定准确性，模型依赖数据库，关键特征为储量、储层面积和厚度。


<details>
  <summary>Details</summary>
Motivation: 在石油工程中早期准确估计采收率因子所需数据往往不可得，需利用可用特征准确估计采收率。

Method: 应用XGBoost分类算法构建机器学习模型，合并三个数据库得到四种组合用于训练和测试模型，用十折交叉验证评估模型有效性，通过准确率、邻域准确率和宏平均f1分数评估模型。

Result: XGBoost分类算法在训练集、测试集和独立数据库中估计采收率等级有一定准确性，分别高达0.49、0.34和0.2，模型可靠性依赖训练数据集。

Conclusion: XGBoost算法可用于估计采收率等级，模型有数据库依赖性，重要特征为储量、储层面积和厚度。

Abstract: In petroleum engineering, it is essential to determine the ultimate recovery
factor, RF, particularly before exploitation and exploration. However,
accurately estimating requires data that is not necessarily available or
measured at early stages of reservoir development. We, therefore, applied
machine learning (ML), using readily available features, to estimate oil RF for
ten classes defined in this study. To construct the ML models, we applied the
XGBoost classification algorithm. Classification was chosen because recovery
factor is bounded from 0 to 1, much like probability. Three databases were
merged, leaving us with four different combinations to first train and test the
ML models and then further evaluate them using an independent database
including unseen data. The cross-validation method with ten folds was applied
on the training datasets to assess the effectiveness of the models. To evaluate
the accuracy and reliability of the models, the accuracy, neighborhood
accuracy, and macro averaged f1 score were determined. Overall, results showed
that the XGBoost classification algorithm could estimate the RF class with
reasonable accuracies as high as 0.49 in the training datasets, 0.34 in the
testing datasets and 0.2 in the independent databases used. We found that the
reliability of the XGBoost model depended on the data in the training dataset
meaning that the ML models were database dependent. The feature importance
analysis and the SHAP approach showed that the most important features were
reserves and reservoir area and thickness.

</details>


### [95] [Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods](https://arxiv.org/abs/2508.18307)
*Mahishanka Withanachchi*

Main category: cs.LG

TL;DR: 本文结合OV - RKHS与基于核的Koopman算子方法，提出学习向量值函数时空动态的统一框架，支持高效建模与预测。


<details>
  <summary>Details</summary>
Motivation: 找到能非参数且数据驱动地估计复杂时变向量场，同时保留时空结构的方法。

Method: 结合算子值再生核希尔伯特空间（OV - RKHS）与基于核的Koopman算子方法。

Result: 建立了时变OV - RKHS插值的表示定理，推导了光滑向量场的Sobolev型逼近界，给出了核Koopman算子逼近的谱收敛保证。

Conclusion: 该框架支持高维非线性系统的高效降阶建模和长期预测，为时空机器学习中的预测、控制和不确定性量化提供了理论工具。

Abstract: We introduce a unified framework for learning the spatio-temporal dynamics of
vector valued functions by combining operator valued reproducing kernel Hilbert
spaces (OV-RKHS) with kernel based Koopman operator methods. The approach
enables nonparametric and data driven estimation of complex time evolving
vector fields while preserving both spatial and temporal structure. We
establish representer theorems for time dependent OV-RKHS interpolation, derive
Sobolev type approximation bounds for smooth vector fields, and provide
spectral convergence guarantees for kernel Koopman operator approximations.
This framework supports efficient reduced order modeling and long term
prediction of high dimensional nonlinear systems, offering theoretically
grounded tools for forecasting, control, and uncertainty quantification in
spatio-temporal machine learning.

</details>


### [96] [DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction](https://arxiv.org/abs/2508.18376)
*Weilin Cai,Le Qin,Shwai He,Junwei Cui,Ang Li,Jiayi Huang*

Main category: cs.LG

TL;DR: 本文针对混合专家（MoE）架构在部署中的挑战，提出DualSparse - MoE推理系统，实验表明该方法在减少计算量的同时能保持较低的准确率损失。


<details>
  <summary>Details</summary>
Motivation: MoE虽能减少每令牌计算量并支持模型扩展，但因其巨大计算规模和不可预测的激活模式面临挑战，需要实现高效部署。

Method: 识别预训练MoE模块中张量和神经元级别的双重稀疏性，引入训练后专家分区来诱导稀疏性，提出DualSparse - MoE推理系统，集成动态张量级计算丢弃和静态神经元级重建。

Result: 在三种主流MoE模型上，约25%的丢弃率仅使平均准确率降低0.08% - 0.28%，且计算丢弃能带来成比例的计算加速；在专家并行中加入负载不平衡感知可使MoE模块加速1.41倍，平均准确率仅下降0.5%。

Conclusion: 所提方法能在实现显著效率提升的同时，将准确率损失降至最低。

Abstract: Mixture of Experts (MoE) has become a mainstream architecture for building
Large Language Models (LLMs) by reducing per-token computation while enabling
model scaling. It can be viewed as partitioning a large Feed-Forward Network
(FFN) at the tensor level into fine-grained sub-FFNs, or experts, and
activating only a sparse subset for each input. While this sparsity improves
efficiency, MoE still faces substantial challenges due to their massive
computational scale and unpredictable activation patterns.
  To enable efficient MoE deployment, we identify dual sparsity at the tensor
and neuron levels in pre-trained MoE modules as a key factor for both accuracy
and efficiency. Unlike prior work that increases tensor-level sparsity through
finer-grained expert design during pre-training, we introduce post-training
expert partitioning to induce such sparsity without retraining. This preserves
the mathematical consistency of model transformations and enhances both
efficiency and accuracy in subsequent fine-tuning and inference. Building upon
this, we propose DualSparse-MoE, an inference system that integrates dynamic
tensor-level computation dropping with static neuron-level reconstruction to
deliver significant efficiency gains with minimal accuracy loss.
  Experimental results show that enforcing an approximate 25% drop rate with
our approach reduces average accuracy by only 0.08%-0.28% across three
prevailing MoE models, while nearly all degrees of computation dropping
consistently yield proportional computational speedups. Furthermore,
incorporating load-imbalance awareness into expert parallelism achieves a 1.41x
MoE module speedup with just 0.5% average accuracy degradation.

</details>


### [97] [CoPE: A Lightweight Complex Positional Encoding](https://arxiv.org/abs/2508.18308)
*Avinash Amballa*

Main category: cs.LG

TL;DR: 介绍了一种轻量级的复数位置编码CoPE，实验显示其在GLUE基准上性能优且计算复杂度低。


<details>
  <summary>Details</summary>
Motivation: 利用复数编码同时编码内容和位置信息，改进传统位置编码。

Method: 用复数嵌入替代传统位置编码，实部捕获语义内容，虚部编码位置信息；在transformer模型第一层引入相位感知注意力，后续用标准注意力层。

Result: CoPE无长期衰减，与线性注意力兼容；在GLUE基准上比RoPE、正弦和学习位置编码性能更优，计算复杂度更低。

Conclusion: CoPE是一种有效的位置编码方法，能以较低计算复杂度实现更好性能。

Abstract: Recent studies have demonstrated the effectiveness of position encoding in
transformer architectures. By incorporating positional information, this
approach provides essential guidance for modeling dependencies between elements
across different sequence positions. We introduce CoPE (a lightweight Complex
Positional Encoding), a novel architecture that leverages complex-valued
encoding to encode both content and positional information. Our approach
replaces traditional positional encodings with complex embeddings where the
real part captures semantic content and the imaginary part encodes positional
information. We introduce phase-aware attention in the first layer of the
transformer model to capture position-dependent patterns, followed by standard
attention layers for higher-levels. We show that CoPE doesn't exhibit long term
decay and is compatible with linear attention. Experimental evaluation on the
GLUE benchmark suggest that our approach achieves superior performance with
less computational complexity, compared to RoPE, Sinusoidal and Learned
positional encodings.

</details>


### [98] [Federated Learning with Heterogeneous and Private Label Sets](https://arxiv.org/abs/2508.18774)
*Adam Breitholtz,Edvin Listo Zec,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 本文研究联邦学习中标签集异质性对模型性能的影响，比较公共和私有标签设置，应用经典方法并调整常见方法，实验表明减少客户端标签数会损害性能，提出的方法能在保障隐私同时维持精度。


<details>
  <summary>Details</summary>
Motivation: 现实应用中联邦学习里异构客户端标签集少被研究，且以往假设客户端愿共享全部标签集，私有标签集学习问题更具挑战性。

Method: 将分类器组合问题的经典方法用于集中调优的联邦学习，调整常见联邦学习方法以适应私有标签集设置。

Result: 减少客户端可用标签数大幅损害所有方法性能，集中调优可改善但增加方差，提出的方法在私有标签设置下表现与标准方法在公共设置下相当。

Conclusion: 客户端能在几乎不损失模型精度的情况下提升隐私保护。

Abstract: Although common in real-world applications, heterogeneous client label sets
are rarely investigated in federated learning (FL). Furthermore, in the cases
they are, clients are assumed to be willing to share their entire label sets
with other clients. Federated learning with private label sets, shared only
with the central server, adds further constraints on learning algorithms and
is, in general, a more difficult problem to solve. In this work, we study the
effects of label set heterogeneity on model performance, comparing the public
and private label settings -- when the union of label sets in the federation is
known to clients and when it is not. We apply classical methods for the
classifier combination problem to FL using centralized tuning, adapt common FL
methods to the private label set setting, and discuss the justification of both
approaches under practical assumptions. Our experiments show that reducing the
number of labels available to each client harms the performance of all methods
substantially. Centralized tuning of client models for representational
alignment can help remedy this, but often at the cost of higher variance.
Throughout, our proposed adaptations of standard FL methods perform well,
showing similar performance in the private label setting as the standard
methods achieve in the public setting. This shows that clients can enjoy
increased privacy at little cost to model accuracy.

</details>


### [99] [History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL](https://arxiv.org/abs/2508.18588)
*Jingkai He,Tianjian Li,Erhu Feng,Dong Du,Qian Liu,Tao Liu,Yubin Xia,Haibo Chen*

Main category: cs.LG

TL;DR: 本文指出当前大语言模型强化学习系统GPU利用率低的问题，提出RhymeRL系统，通过HistoSpec和HistoPipe创新加速训练，实验显示性能提升2.6倍且不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型强化学习系统存在GPU利用率低的问题，传统解决方案可能牺牲训练准确性。

Method: 提出RhymeRL系统，包含HistoSpec推测解码推理引擎和HistoPipe两层调度策略。

Result: 在真实生产环境评估，RhymeRL可从几十扩展到数千个GPU，相比现有方法性能提升2.6倍。

Conclusion: RhymeRL能在不影响准确性和不改变强化学习范式的前提下，加速强化学习训练。

Abstract: With the rapid advancement of large language models (LLMs), reinforcement
learning (RL) has emerged as a pivotal methodology for enhancing the reasoning
capabilities of LLMs. Unlike traditional pre-training approaches, RL
encompasses multiple stages: rollout, reward, and training, which necessitates
collaboration among various worker types. However, current RL systems continue
to grapple with substantial GPU underutilization, due to two primary factors:
(1) The rollout stage dominates the overall RL process due to test-time
scaling; (2) Imbalances in rollout lengths (within the same batch) result in
GPU bubbles. While prior solutions like asynchronous execution and truncation
offer partial relief, they may compromise training accuracy for efficiency.
  Our key insight stems from a previously overlooked observation: rollout
responses exhibit remarkable similarity across adjacent training epochs. Based
on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate
RL training with two key innovations. First, to enhance rollout generation, we
present HistoSpec, a speculative decoding inference engine that utilizes the
similarity of historical rollout token sequences to obtain accurate drafts.
Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier
scheduling strategy that leverages the similarity of historical rollout
distributions to balance workload among rollout workers. We have evaluated
RhymeRL within a real production environment, demonstrating scalability from
dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL
achieves a 2.6x performance improvement over existing methods, without
compromising accuracy or modifying the RL paradigm.

</details>


### [100] [What Matters in Data for DPO?](https://arxiv.org/abs/2508.18312)
*Yu Pan,Zhongze Cai,Guanting Chen,Huaiyang Zhong,Chonghuan Wang*

Main category: cs.LG

TL;DR: 本文从理论和实证角度系统研究偏好数据分布对DPO的影响，发现选择响应的质量对优化DPO目标起主导作用，实验证实提升其质量可提升性能，还探讨了混合在线策略数据的好处。


<details>
  <summary>Details</summary>
Motivation: 尽管DPO被广泛采用，但偏好数据的哪些特征对其性能最关键仍未知，因此作者开展研究。

Method: 从理论和实证角度系统研究偏好数据分布对DPO的影响，并进行广泛实验。

Result: 选择响应的质量对优化DPO目标起主导作用，拒绝响应的质量影响相对有限；在线DPO可有效简化为对选择响应的监督微调；提升选择响应质量能稳定提升性能，还探讨了混合在线策略数据的好处。

Conclusion: 研究解释了一些广泛采用策略背后的机制，为构建用于大语言模型对齐的高影响力偏好数据集提供实用见解。

Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective
approach for aligning large language models (LLMs) with human preferences,
bypassing the need for a learned reward model. Despite its growing adoption, a
fundamental question remains open: what characteristics of preference data are
most critical for DPO performance? In this work, we provide a systematic study
of how preference data distribution influences DPO, from both theoretical and
empirical perspectives. We show that the quality of chosen responses plays a
dominant role in optimizing the DPO objective, while the quality of rejected
responses may have relatively limited impact. Our theoretical analysis
characterizes the optimal response distribution under DPO and reveals how
contrastiveness between responses helps primarily by improving the chosen
samples. We further study an online DPO setting and show it effectively reduces
to supervised fine-tuning on the chosen responses. Extensive experiments across
diverse tasks confirm our findings: improving the quality of chosen responses
consistently boosts performance regardless of the quality of the rejected
responses. We also investigate the benefit of mixing the on-policy data. Our
results interpret the mechanism behind some widely adopted strategies and offer
practical insights for constructing high-impact preference datasets for LLM
alignment.

</details>


### [101] [FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning](https://arxiv.org/abs/2508.19009)
*Md Anwar Hossen,Fatema Siddika,Wensheng Zhang,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: 本文提出FedProtoKD方法，利用增强双知识蒸馏机制和对比学习解决原型边缘收缩问题，在不同设置下提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的异构联邦学习方法在聚合原型时采用加权平均，导致全局知识次优，原型收缩影响模型性能，需要改进。

Method: 提出FedProtoKD，使用增强双知识蒸馏机制，利用基于对比学习的可训练服务器原型解决原型边缘收缩问题，评估公共样本重要性。

Result: FedProtoKD在各种设置下准确率平均提高1.13%至34.13%，显著优于现有先进方法。

Conclusion: FedProtoKD能有效解决原型边缘收缩问题，提升异构联邦学习系统性能。

Abstract: Heterogeneous Federated Learning (HFL) has gained attention for its ability
to accommodate diverse models and heterogeneous data across clients.
Prototype-based HFL methods emerge as a promising solution to address
statistical heterogeneity and privacy challenges, paving the way for new
advancements in HFL research. This method focuses on sharing only
class-representative prototypes among heterogeneous clients. However, these
prototypes are often aggregated on the server using weighted averaging, leading
to sub-optimal global knowledge; these cause the shrinking of aggregated
prototypes, which negatively affects the model performance in scenarios when
models are heterogeneous and data distributions are extremely non-IID. We
propose FedProtoKD in a Heterogeneous Federated Learning setting, using an
enhanced dual-knowledge distillation mechanism to improve the system
performance with clients' logits and prototype feature representation. We aim
to resolve the prototype margin-shrinking problem using a contrastive
learning-based trainable server prototype by leveraging a class-wise adaptive
prototype margin. Furthermore, we assess the importance of public samples using
the closeness of the sample's prototype to its class representative prototypes,
which enhances learning performance. FedProtoKD achieved average improvements
of 1.13% up to 34.13% accuracy across various settings and significantly
outperforms existing state-of-the-art HFL methods.

</details>


### [102] [ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions](https://arxiv.org/abs/2508.18313)
*Zi Cai,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: 提出可解释的分层原型学习框架ProtoEHR，利用电子健康记录（EHR）数据多层结构提升医疗预测，实验显示其准确、稳健且可解释。


<details>
  <summary>Details</summary>
Motivation: 现有研究常聚焦EHR数据孤立组件，限制预测性能和可解释性。

Method: 构建医疗知识图谱作知识源，设计分层表征学习框架，结合各层原型信息。

Result: 在两个公开数据集五项临床任务评估中，相比基线模型，ProtoEHR能做出准确、稳健且可解释的预测。

Conclusion: ProtoEHR能有效提升医疗预测，还能在代码、就诊和患者层面提供可解释见解。

Abstract: Digital healthcare systems have enabled the collection of mass healthcare
data in electronic healthcare records (EHRs), allowing artificial intelligence
solutions for various healthcare prediction tasks. However, existing studies
often focus on isolated components of EHR data, limiting their predictive
performance and interpretability. To address this gap, we propose ProtoEHR, an
interpretable hierarchical prototype learning framework that fully exploits the
rich, multi-level structure of EHR data to enhance healthcare predictions. More
specifically, ProtoEHR models relationships within and across three
hierarchical levels of EHRs: medical codes, hospital visits, and patients. We
first leverage large language models to extract semantic relationships among
medical codes and construct a medical knowledge graph as the knowledge source.
Building on this, we design a hierarchical representation learning framework
that captures contextualized representations across three levels, while
incorporating prototype information within each level to capture intrinsic
similarities and improve generalization. To perform a comprehensive assessment,
we evaluate ProtoEHR in two public datasets on five clinically significant
tasks, including prediction of mortality, prediction of readmission, prediction
of length of stay, drug recommendation, and prediction of phenotype. The
results demonstrate the ability of ProtoEHR to make accurate, robust, and
interpretable predictions compared to baselines in the literature. Furthermore,
ProtoEHR offers interpretable insights on code, visit, and patient levels to
aid in healthcare prediction.

</details>


### [103] [Composition and Alignment of Diffusion Models using Constrained Learning](https://arxiv.org/abs/2508.19104)
*Shervin Khalafi,Ignacio Hounie,Dongsheng Ding,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 本文提出约束优化框架统一扩散模型的对齐与组合，理论分析并开发算法，在图像生成中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法优化多奖励或组合多模型时有权衡，无法保证生成样本具有所有期望属性。

Method: 提出约束优化框架，对约束对齐和组合问题进行理论刻画，开发基于拉格朗日的原始 - 对偶训练算法。

Result: 在图像生成中验证所提方法有效，对齐或组合模型能有效满足约束，优于等权重方法。

Conclusion: 提出的统一框架和算法能解决扩散模型对齐与组合问题，提升生成样本质量。

Abstract: Diffusion models have become prevalent in generative modeling due to their
ability to sample from complex distributions. To improve the quality of
generated samples and their compliance with user requirements, two commonly
used methods are: (i) Alignment, which involves fine-tuning a diffusion model
to align it with a reward; and (ii) Composition, which combines several
pre-trained diffusion models, each emphasizing a desirable attribute in the
generated outputs. However, trade-offs often arise when optimizing for multiple
rewards or combining multiple models, as they can often represent competing
properties. Existing methods cannot guarantee that the resulting model
faithfully generates samples with all the desired properties. To address this
gap, we propose a constrained optimization framework that unifies alignment and
composition of diffusion models by enforcing that the aligned model satisfies
reward constraints and/or remains close to (potentially multiple) pre-trained
models. We provide a theoretical characterization of the solutions to the
constrained alignment and composition problems and develop a Lagrangian-based
primal-dual training algorithm to approximate these solutions. Empirically, we
demonstrate the effectiveness and merits of our proposed approach in image
generation, applying it to alignment and composition, and show that our aligned
or composed model satisfies constraints effectively, and improves on the
equally-weighted approach. Our implementation can be found at
https://github.com/shervinkhalafi/constrained_comp_align.

</details>


### [104] [Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing](https://arxiv.org/abs/2508.18316)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 本文开发并评估基于早期学业表现和数字参与模式的机器学习模型，用联邦学习框架应对数据隐私等挑战，模型预测能力强，为院校构建预警系统提供方案。


<details>
  <summary>Details</summary>
Motivation: 远程教育高辍学和不及格率带来挑战，需主动识别高危学生以提供及时支持。

Method: 基于大规模OULAD数据集，用联邦学习框架实现模型，比较模型复杂度和数据平衡。

Result: 最终联邦模型有强预测能力，识别高危学生的ROC AUC得分约85%。

Conclusion: 联邦方法为院校构建有效预警系统提供实用且可扩展的解决方案，兼顾数据隐私。

Abstract: High dropout and failure rates in distance education pose a significant
challenge for academic institutions, making the proactive identification of
at-risk students crucial for providing timely support. This study develops and
evaluates a machine learning model based on early academic performance and
digital engagement patterns from the large-scale OULAD dataset to predict
student risk at a UK university. To address the practical challenges of data
privacy and institutional silos that often hinder such initiatives, we
implement the model using a Federated Learning (FL) framework. We compare model
complexity (Logistic Regression vs. a Deep Neural Network) and data balancing.
The final federated model demonstrates strong predictive capability, achieving
an ROC AUC score of approximately 85% in identifying at-risk students. Our
findings show that this federated approach provides a practical and scalable
solution for institutions to build effective early-warning systems, enabling
proactive student support while inherently respecting data privacy.

</details>


### [105] [Understanding Tool-Integrated Reasoning](https://arxiv.org/abs/2508.19201)
*Heng Lin,Zhongwen Xu*

Main category: cs.LG

TL;DR: 本文研究工具集成推理（TIR）使大语言模型（LLMs）更强大的原因，给出形式化证明，提出新算法ASPO，实验表明TIR模型表现更优并揭示认知模式。


<details>
  <summary>Details</summary>
Motivation: 缺乏解释TIR范式有效的理论，需要探究TIR使LLMs更强大的原因。

Method: 给出TIR从根本上扩展LLM能力的形式化证明，提出Advantage Shaping Policy Optimization（ASPO）算法，在数学基准测试上进行实验。

Result: TIR模型在pass@k指标上显著优于纯文本模型，优势不限于计算密集型问题，还发现认知模式，使用ASPO有更好的工具使用行为。

Conclusion: 为TIR的成功提供了原则性解释，将关注点从工具有效转移到为何及如何实现更强大推理。

Abstract: We study why Tool-Integrated Reasoning (TIR) makes Large Language Models
(LLMs) more capable. While LLMs integrated with tools like Python code
interpreters show great promise, a principled theory explaining why this
paradigm is effective has been missing. This work provides the first formal
proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that
tools enable a strict expansion of the model's empirical and feasible support,
breaking the capability ceiling of pure-text models by unlocking
problem-solving strategies that are otherwise impossible or intractably
verbose. To guide model behavior without compromising training stability and
performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a
novel algorithm that directly modifies the advantage function to guide the
policy behavior. We conduct comprehensive experiments on challenging
mathematical benchmarks, leveraging a Python interpreter as the external tool.
Our results show that the TIR model decisively outperforms its pure-text
counterpart on the pass@k metric. Crucially, this advantage is not confined to
computationally-intensive problems but extends to those requiring significant
abstract insight. We further identify the emergent cognitive patterns that
illustrate how models learn to think with tools. Finally, we report improved
tool usage behavior with early code invocation and much more interactive turns
with ASPO. Overall, our work provides the first principled explanation for
TIR's success, shifting the focus from the mere fact that tools work to why and
how they enable more powerful reasoning.

</details>


### [106] [ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation](https://arxiv.org/abs/2508.18318)
*Yang Li,Hanjie Wang,Yuanzheng Li,Jiazheng Li,Zhaoyang Dong*

Main category: cs.LG

TL;DR: 本文提出ZTFed - MAS2S零信任联邦学习框架解决风电数据缺失值及联邦学习隐私安全问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 风电数据存在缺失值，联邦学习有异常更新和隐私泄露风险，开放工业环境需零信任机制。

Method: 提出ZTFed - MAS2S框架，集成多头部注意力序列到序列插补模型，结合可验证差分隐私、非交互零知识证明和验证机制，采用动态信任感知聚合机制，通过稀疏和量化压缩减少通信开销。

Result: 在真实风电场数据集上的大量实验验证了ZTFed - MAS2S在联邦学习性能和缺失数据插补方面的优越性。

Conclusion: ZTFed - MAS2S是能源领域实际应用中安全高效的解决方案。

Abstract: Wind power data often suffers from missing values due to sensor faults and
unstable transmission at edge sites. While federated learning enables
privacy-preserving collaboration without sharing raw data, it remains
vulnerable to anomalous updates and privacy leakage during parameter exchange.
These challenges are amplified in open industrial environments, necessitating
zero-trust mechanisms where no participant is inherently trusted. To address
these challenges, this work proposes ZTFed-MAS2S, a zero-trust federated
learning framework that integrates a multi-head attention-based
sequence-to-sequence imputation model. ZTFed integrates verifiable differential
privacy with non-interactive zero-knowledge proofs and a confidentiality and
integrity verification mechanism to ensure verifiable privacy preservation and
secure model parameters transmission. A dynamic trust-aware aggregation
mechanism is employed, where trust is propagated over similarity graphs to
enhance robustness, and communication overhead is reduced via sparsity- and
quantization-based compression. MAS2S captures long-term dependencies in wind
power data for accurate imputation. Extensive experiments on real-world wind
farm datasets validate the superiority of ZTFed-MAS2S in both federated
learning performance and missing data imputation, demonstrating its
effectiveness as a secure and efficient solution for practical applications in
the energy sector.

</details>


### [107] [Linear cost mutual information estimation and independence test of similar performance as HSIC](https://arxiv.org/abs/2508.18338)
*Jarek Duda,Jagoda Bracha,Adrian Przybysz*

Main category: cs.LG

TL;DR: 本文指出HSIC处理大数据样本计算复杂度高，提出HCR作为线性成本的替代方法，有更高依赖敏感性且能提供联合分布模型。


<details>
  <summary>Details</summary>
Motivation: 现有HSIC方法处理大数据样本时，因需进行矩阵乘法，计算复杂度高，不实用，需寻找替代方法。

Method: 讨论HCR作为替代方法，通过混合矩描述依赖关系，计算单个依赖描述特征时间为O(n)，测试数量随维度d变化。

Result: HCR是线性成本的实用替代方法，在测试中有更高的依赖敏感性，还能提供实际联合分布模型。

Conclusion: HCR可作为HSIC的有效替代，用于评估两个数据样本间的统计依赖关系。

Abstract: Evaluation of statistical dependencies between two data samples is a basic
problem of data science/machine learning, and HSIC (Hilbert-Schmidt Information
Criterion)~\cite{HSIC} is considered the state-of-art method. However, for size
$n$ data sample it requires multiplication of $n\times n$ matrices, what
currently needs $\sim O(n^{2.37})$ computational complexity~\cite{mult}, making
it impractical for large data samples. We discuss HCR (Hierarchical Correlation
Reconstruction) as its linear cost practical alternative of even higher
dependence sensitivity in tests, and additionally providing actual joint
distribution model by description of dependencies through features being mixed
moments, starting with correlation and homoscedasticity, also allowing to
approximate mutual information as just sum of squares of such nontrivial mixed
moments between two data samples. Such single dependence describing feature is
calculated in $O(n)$ linear time. Their number to test varies with dimension
$d$ - requiring $O(d^2)$ for pairwise dependencies, $O(d^3)$ if wanting to also
consider more subtle triplewise, and so on.

</details>


### [108] [Low-Rank Tensor Decompositions for the Theory of Neural Networks](https://arxiv.org/abs/2508.18408)
*Ricardo Borsoi,Konstantin Usevich,Marianne Clausel*

Main category: cs.LG

TL;DR: 本文综述低秩张量方法对深度神经网络性能多方面理论解释的作用，统一介绍现有方法并拓展视角。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络出色表现促使为深度学习理论提供数学基础，低秩张量分解与神经网络联系紧密且理论成果丰富。

Method: 回顾不同领域利用低秩张量方法研究深度神经网络理论的现有方法。

Result: 展示低秩张量方法在解释深度神经网络性能多方面发挥基础作用。

Conclusion: 以统一方式概述现有方法，为深度神经网络理论中低秩张量分解的应用打开更广阔视角。

Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge
of interest in providing a mathematical basis to deep learning theory. Low-rank
tensor decompositions are specially befitting for this task due to their close
connection to NNs and their rich theoretical results. Different tensor
decompositions have strong uniqueness guarantees, which allow for a direct
interpretation of their factors, and polynomial time algorithms have been
proposed to compute them. Through the connections between tensors and NNs, such
results supported many important advances in the theory of NNs. In this review,
we show how low-rank tensor methods--which have been a core tool in the signal
processing and machine learning communities--play a fundamental role in
theoretically explaining different aspects of the performance of deep NNs,
including their expressivity, algorithmic learnability and computational
hardness, generalization, and identifiability. Our goal is to give an
accessible overview of existing approaches (developed by different communities,
ranging from computer science to mathematics) in a coherent and unified way,
and to open a broader perspective on the use of low-rank tensor decompositions
for the theory of deep NNs.

</details>


### [109] [LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning](https://arxiv.org/abs/2508.18420)
*André Quadros,Cassio Silva,Ronnie Alves*

Main category: cs.LG

TL;DR: 本文结合两种内在动机策略提升强化学习智能体在极端稀疏奖励环境中的效率，实验表明组合策略效果更佳。


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习在极端稀疏奖励环境中因正反馈少而学习困难的问题。

Method: 将基于变分自编码器奖励状态新颖性的VSIMR与基于大语言模型的内在奖励方法相结合，并在MiniGrid DoorKey环境中用A2C智能体实现。

Result: 组合策略相比单一策略和标准A2C智能体显著提高了智能体性能和采样效率，标准A2C智能体未能学习。

Conclusion: 组合策略有效互补，VSIMR驱动探索新状态，大语言模型奖励促进向目标的渐进利用。

Abstract: This paper explores the combination of two intrinsic motivation strategies to
improve the efficiency of reinforcement learning (RL) agents in environments
with extreme sparse rewards, where traditional learning struggles due to
infrequent positive feedback. We propose integrating Variational State as
Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward
state novelty, with an intrinsic reward approach derived from Large Language
Models (LLMs). The LLMs leverage their pre-trained knowledge to generate reward
signals based on environment and goal descriptions, guiding the agent. We
implemented this combined approach with an Actor-Critic (A2C) agent in the
MiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical
results show that this combined strategy significantly increases agent
performance and sampling efficiency compared to using each strategy
individually or a standard A2C agent, which failed to learn. Analysis of
learning curves indicates that the combination effectively complements
different aspects of the environment and task: VSIMR drives exploration of new
states, while the LLM-derived rewards facilitate progressive exploitation
towards goals.

</details>


### [110] [Enhancing Trust-Region Bayesian Optimization via Newton Methods](https://arxiv.org/abs/2508.18423)
*Quanlin Chen,Yiyu Chen,Jing Huo,Tianyu Ding,Yang Gao,Yuetong Chen*

Main category: cs.LG

TL;DR: 提出新方法增强高维贝叶斯优化采样效率，优于多种技术。


<details>
  <summary>Details</summary>
Motivation: 现有高维贝叶斯优化方法中，局部高斯过程降低采样效率，且存在高维空间梯度消失问题。

Method: 用全局高斯过程的梯度和海森矩阵构建多个局部二次模型，通过求解边界约束二次规划选择新样本点，并解决高维梯度消失问题。

Result: 新方法增强了TuRBO的效果，在合成函数和实际应用中优于多种高维贝叶斯优化技术。

Conclusion: 所提方法有效提升了高维贝叶斯优化的采样效率和性能。

Abstract: Bayesian Optimization (BO) has been widely applied to optimize expensive
black-box functions while retaining sample efficiency. However, scaling BO to
high-dimensional spaces remains challenging. Existing literature proposes
performing standard BO in multiple local trust regions (TuRBO) for
heterogeneous modeling of the objective function and avoiding over-exploration.
Despite its advantages, using local Gaussian Processes (GPs) reduces sampling
efficiency compared to a global GP. To enhance sampling efficiency while
preserving heterogeneous modeling, we propose to construct multiple local
quadratic models using gradients and Hessians from a global GP, and select new
sample points by solving the bound-constrained quadratic program. Additionally,
we address the issue of vanishing gradients of GPs in high-dimensional spaces.
We provide a convergence analysis and demonstrate through experimental results
that our method enhances the efficacy of TuRBO and outperforms a wide range of
high-dimensional BO techniques on synthetic functions and real-world
applications.

</details>


### [111] [VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning](https://arxiv.org/abs/2508.18462)
*Fu Teng,Miao Pan,Xuhong Zhang,Zhezhi He,Yiyao Yang,Xinyi Chai,Mengnan Qi,Liqiang Lu,Jianwei Yin*

Main category: cs.LG

TL;DR: 本文提出用于Verilog代码生成的强化学习框架，构建高质量数据集，提出回溯重评分机制和样本平衡加权策略，实验表现优异，凸显强化学习在硬件领域结构化代码生成的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成在硬件描述语言Verilog方面因并发语义、语法刚性和仿真复杂性等未充分探索，需解决相关挑战。

Method: 构建Veribench - 53K数据集；提出回溯重评分机制增强反馈可靠性；引入样本平衡加权策略；将创新方法集成到迭代强化学习管道。

Result: 在Verilog生成任务实验中取得了最先进的性能，在测试通过率、功能正确性和编译鲁棒性方面有显著提升。

Conclusion: 强化学习驱动的方法在以硬件为中心的领域进行结构化代码生成具有潜力。

Abstract: Recent advancements in code generation have shown remarkable success across
software domains, yet hardware description languages (HDLs) such as Verilog
remain underexplored due to their concurrency semantics, syntactic rigidity,
and simulation complexity. In this work, we address these challenges by
introducing a reinforcement learning (RL) framework tailored for Verilog code
generation. We first construct Veribench-53K, a high-quality dataset curated
from over 700K Verilog problems, enriched with structured prompts, complexity
labels, and diverse testbenches. To tackle the problem of sparse and noisy
reward signals, we propose a Trace-back based Rescore mechanism that leverages
reasoning paths and iterative refinement to enhance feedback reliability and
support reward model training. Furthermore, to mitigate catastrophic forgetting
and overfitting during RL fine-tuning, we introduce a sample-balanced weighting
strategy that adaptively balances learning dynamics based on reward-probability
distributions. These innovations are integrated into an iterative RL pipeline
that co-evolves the policy and reward models. In contrast to recent work such
as CraftRTL, which relies on large-scale closed-source model distillation, and
DeepSeek-style approaches that struggle with sparse feedback, our method
demonstrates superior performance using a smaller but high-quality dataset
combined with RL optimization. Experiments on Verilog generation tasks
demonstrate state-of-the-art performance, with substantial gains in test pass
rate, functional correctness, and compilation robustness. Our findings
highlight the potential of RL-driven approaches for structured code generation
in hardware-centric domains. VERIRL is publicly available at
https://github.com/omniAI-Lab/VeriRL.

</details>


### [112] [DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection](https://arxiv.org/abs/2508.18474)
*Bahareh Golchin,Banafsheh Rekabdar,Kunpeng Liu*

Main category: cs.LG

TL;DR: 提出基于强化学习的DRTA框架用于时间序列异常检测，在雅虎数据集上表现优于现有方法，是高效可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列异常检测方法存在标记数据有限、误报率高、难以泛化到新异常类型等问题。

Method: 提出DRTA框架，集成动态奖励塑造、变分自编码器（VAE）和主动学习，使用自适应奖励机制平衡探索与利用。

Result: 在雅虎A1和A2基准数据集上，该方法始终优于现有的无监督和半监督方法。

Conclusion: 该框架是现实世界异常检测任务的可扩展且高效的解决方案。

Abstract: Anomaly detection in time series data is important for applications in
finance, healthcare, sensor networks, and industrial monitoring. Traditional
methods usually struggle with limited labeled data, high false-positive rates,
and difficulty generalizing to novel anomaly types. To overcome these
challenges, we propose a reinforcement learning-based framework that integrates
dynamic reward shaping, Variational Autoencoder (VAE), and active learning,
called DRTA. Our method uses an adaptive reward mechanism that balances
exploration and exploitation by dynamically scaling the effect of VAE-based
reconstruction error and classification rewards. This approach enables the
agent to detect anomalies effectively in low-label systems while maintaining
high precision and recall. Our experimental results on the Yahoo A1 and Yahoo
A2 benchmark datasets demonstrate that the proposed method consistently
outperforms state-of-the-art unsupervised and semi-supervised approaches. These
findings show that our framework is a scalable and efficient solution for
real-world anomaly detection tasks.

</details>


### [113] [Data Augmentation Improves Machine Unlearning](https://arxiv.org/abs/2508.18502)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: cs.LG

TL;DR: 研究不同数据增强策略对机器无学习方法性能的影响，发现适当增强设计可提升无学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究对系统增强设计在机器无学习中的作用研究不足，需深入探究。

Method: 在CIFAR - 10和CIFAR - 100上，以不同遗忘率对SalUn、Random Label和Fine - Tuning等无学习方法开展实验。

Result: 使用TrivialAug增强时，平均差距无学习指标最多降低40.12%。

Conclusion: 数据增强不仅减少记忆，还对隐私保护和高效无学习至关重要。

Abstract: Machine Unlearning (MU) aims to remove the influence of specific data from a
trained model while preserving its performance on the remaining data. Although
a few works suggest connections between memorisation and augmentation, the role
of systematic augmentation design in MU remains under-investigated. In this
work, we investigate the impact of different data augmentation strategies on
the performance of unlearning methods, including SalUn, Random Label, and
Fine-Tuning. Experiments conducted on CIFAR-10 and CIFAR-100, under varying
forget rates, show that proper augmentation design can significantly improve
unlearning effectiveness, reducing the performance gap to retrained models.
Results showed a reduction of up to 40.12% of the Average Gap unlearning
Metric, when using TrivialAug augmentation. Our results suggest that
augmentation not only helps reduce memorization but also plays a crucial role
in achieving privacy-preserving and efficient unlearning.

</details>


### [114] [Breaking Through Barren Plateaus: Reinforcement Learning Initializations for Deep Variational Quantum Circuits](https://arxiv.org/abs/2508.18514)
*Yifeng Peng,Xinyi Li,Zhemin Zhang,Samuel Yen-Chi Chen,Zhiding Liang,Ying Wang*

Main category: cs.LG

TL;DR: 本文提出基于强化学习的初始化策略缓解变分量子算法（VQAs）的贫瘠高原问题，实验表明该方法提升收敛速度和结果质量，为量子算法设计提供新思路。


<details>
  <summary>Details</summary>
Motivation: VQAs的有效性受贫瘠高原问题限制，梯度随系统规模或电路深度增加而指数级减小，阻碍训练。

Method: 提出基于强化学习（RL）的初始化策略，探索多种RL算法生成电路参数以最小化VQAs成本函数，再进行标准基于梯度的优化。

Result: 大量数值实验表明，基于RL的初始化方法显著提升收敛速度和最终解的质量，不同RL算法性能相当。

Conclusion: 该方法为将机器学习技术集成到量子算法设计提供了有前景的途径，加速VQAs的可扩展性和实际应用。

Abstract: Variational Quantum Algorithms (VQAs) have gained prominence as a viable
framework for exploiting near-term quantum devices in applications ranging from
optimization and chemistry simulation to machine learning. However, the
effectiveness of VQAs is often constrained by the so-called barren plateau
problem, wherein gradients diminish exponentially as system size or circuit
depth increases, thereby hindering training. In this work, we propose a
reinforcement learning (RL)-based initialization strategy to alleviate the
barren plateau issue by reshaping the initial parameter landscape to avoid
regions prone to vanishing gradients. In particular, we explore several RL
algorithms (Deterministic Policy Gradient, Soft Actor-Critic, and Proximal
Policy Optimization, etc.) to generate the circuit parameters (treated as
actions) that minimize the VQAs cost function before standard gradient-based
optimization. By pre-training with RL in this manner, subsequent optimization
using methods such as gradient descent or Adam proceeds from a more favorable
initial state. Extensive numerical experiments under various noise conditions
and tasks consistently demonstrate that the RL-based initialization method
significantly enhances both convergence speed and final solution quality.
Moreover, comparisons among different RL algorithms highlight that multiple
approaches can achieve comparable performance gains, underscoring the
flexibility and robustness of our method. These findings shed light on a
promising avenue for integrating machine learning techniques into quantum
algorithm design, offering insights into how RL-driven parameter initialization
can accelerate the scalability and practical deployment of VQAs. Opening up a
promising path for the research community in machine learning for quantum,
especially barren plateau problems in VQAs.

</details>


### [115] [BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration](https://arxiv.org/abs/2508.18551)
*Jun Hou,Le Wang,Xuan Wang*

Main category: cs.LG

TL;DR: 现有多模态学习中MoE模型在额外模态含噪声时效果不明，现有方法难扩展到两模态以上。提出BTW框架，结合实例级KL散度和模态级互信息动态调整模态重要性，实验表明能提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习中MoE模型在额外模态含噪声时效果不明确，以及现有方法难以扩展到两模态以上和缺乏实例级控制的问题。

Method: 提出BTW双级非参数加权框架，结合实例级KL散度和模态级互信息，计算每个示例的KL权重和模态范围的MI权重，动态调整模态重要性。

Result: 在情感回归和临床分类的大量实验中，显著提高了回归性能和多类分类准确率。

Conclusion: BTW框架有效，可在多模态学习中动态调整模态重要性，提升性能。

Abstract: Mixture-of-Experts (MoE) models have become increasingly powerful in
multimodal learning by enabling modular specialization across modalities.
However, their effectiveness remains unclear when additional modalities
introduce more noise than complementary information. Existing approaches, such
as the Partial Information Decomposition, struggle to scale beyond two
modalities and lack the resolution needed for instance-level control. We
propose Beyond Two-modality Weighting (BTW), a bi-level, non-parametric
weighting framework that combines instance-level Kullback-Leibler (KL)
divergence and modality-level mutual information (MI) to dynamically adjust
modality importance during training. Our method does not require additional
parameters and can be applied to an arbitrary number of modalities.
Specifically, BTW computes per-example KL weights by measuring the divergence
between each unimodal and the current multimodal prediction, and modality-wide
MI weights by estimating global alignment between unimodal and multimodal
outputs. Extensive experiments on sentiment regression and clinical
classification demonstrate that our method significantly improves regression
performance and multiclass classification accuracy.

</details>


### [116] [Enhancing Chemical Explainability Through Counterfactual Masking](https://arxiv.org/abs/2508.18561)
*Łukasz Janisiów,Marek Kochańczyk,Bartosz Zieliński,Tomasz Danel*

Main category: cs.LG

TL;DR: 提出反事实掩码框架用于分子属性预测，具有分子现实性和有意义反事实的优点，适用于基准测试和提供可操作见解。


<details>
  <summary>Details</summary>
Motivation: 现有可解释人工智能方法在分子属性预测中依靠掩码策略，常不遵循分子分布，产生非直观解释。

Method: 提出反事实掩码框架，用从生成模型采样的化学合理片段替换掩码子结构，相对于从数据分布中抽取的反事实分子评估掩码预测。

Result: 该方法适用于基准测试模型解释器，在多个数据集和属性预测任务中产生更可操作的见解。

Conclusion: 此方法弥合可解释性与分子设计的差距，为化学中可解释机器学习提供原则性和生成性途径。

Abstract: Molecular property prediction is a crucial task that guides the design of new
compounds, including drugs and materials. While explainable artificial
intelligence methods aim to scrutinize model predictions by identifying
influential molecular substructures, many existing approaches rely on masking
strategies that remove either atoms or atom-level features to assess importance
via fidelity metrics. These methods, however, often fail to adhere to the
underlying molecular distribution and thus yield unintuitive explanations. In
this work, we propose counterfactual masking, a novel framework that replaces
masked substructures with chemically reasonable fragments sampled from
generative models trained to complete molecular graphs. Rather than evaluating
masked predictions against implausible zeroed-out baselines, we assess them
relative to counterfactual molecules drawn from the data distribution. Our
method offers two key benefits: (1) molecular realism underpinning robust and
distribution-consistent explanations, and (2) meaningful counterfactuals that
directly indicate how structural modifications may affect predicted properties.
We demonstrate that counterfactual masking is well-suited for benchmarking
model explainers and yields more actionable insights across multiple datasets
and property prediction tasks. Our approach bridges the gap between
explainability and molecular design, offering a principled and generative path
toward explainable machine learning in chemistry.

</details>


### [117] [A Note on Graphon-Signal Analysis of Graph Neural Networks](https://arxiv.org/abs/2508.18564)
*Levi Rauchwerger,Ron Levie*

Main category: cs.LG

TL;DR: 本文针对Levie论文不足进行改进，将主要结果拓展到多维信号图子信号等。


<details>
  <summary>Details</summary>
Motivation: Levie论文存在缺失，限制其在图机器学习实际场景的应用，本文旨在解决这些不足。

Method: 将主要结果拓展到多维信号图子信号；将Lipschitz连续性拓展到有读出的MPNN；利用鲁棒性泛化界改进泛化界；将分析拓展到非对称图子和核。

Result: 对现有结果进行了细化和拓展。

Conclusion: 通过这些拓展和改进，能解决Levie论文的不足，提升其在实际场景的适用性。

Abstract: A recent paper, ``A Graphon-Signal Analysis of Graph Neural Networks'', by
Levie, analyzed message passing graph neural networks (MPNNs) by embedding the
input space of MPNNs, i.e., attributed graphs (graph-signals), to a space of
attributed graphons (graphon-signals). Based on extensions of standard results
in graphon analysis to graphon-signals, the paper proved a generalization bound
and a sampling lemma for MPNNs. However, there are some missing ingredients in
that paper, limiting its applicability in practical settings of graph machine
learning. In the current paper, we introduce several refinements and extensions
to existing results that address these shortcomings. In detail, 1) we extend
the main results in the paper to graphon-signals with multidimensional signals
(rather than 1D signals), 2) we extend the Lipschitz continuity to MPNNs with
readout with respect to cut distance (rather than MPNNs without readout with
respect to cut metric), 3) we improve the generalization bound by utilizing
robustness-type generalization bounds, and 4) we extend the analysis to
non-symmetric graphons and kernels.

</details>


### [118] [Improving Long-term Autoregressive Spatiotemporal Predictions: A Proof of Concept with Fluid Dynamics](https://arxiv.org/abs/2508.18565)
*Hao Zhou,Sibo Cheng*

Main category: cs.LG

TL;DR: 提出随机前推（SPF）框架处理数据驱动预测问题，实验显示优于自回归方法。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法在复杂系统长期预测中因误差积累精度下降，自回归训练耗显存且牺牲短期性能。

Method: 提出SPF框架，保留单步训练同时实现多步学习，用随机采集策略结合预测数据与真实数据，在各轮训练间预计算多步预测。

Result: 在Burgers方程和浅水基准测试中，SPF比自回归方法有更高长期精度且降低显存需求。

Conclusion: SPF框架在资源受限和复杂模拟方面很有前景。

Abstract: Data-driven methods are emerging as efficient alternatives to traditional
numerical forecasting, offering fast inference and lower computational cost.
Yet, for complex systems, long-term accuracy often deteriorates due to error
accumulation, and autoregressive training (though effective) demands large GPU
memory and may sacrifice short-term performance. We propose the Stochastic
PushForward (SPF) framework, which retains one-step-ahead training while
enabling multi-step learning. SPF builds a supplementary dataset from model
predictions and combines it with ground truth via a stochastic acquisition
strategy, balancing short- and long-term performance while reducing
overfitting. Multi-step predictions are precomputed between epochs, keeping
memory usage stable without storing full unrolled sequences. Experiments on the
Burgers' equation and the Shallow Water benchmark show that SPF achieves higher
long-term accuracy than autoregressive methods while lowering memory
requirements, making it promising for resource-limited and complex simulations.

</details>


### [119] [Sparse Autoencoders for Low-$N$ Protein Function Prediction and Design](https://arxiv.org/abs/2508.18567)
*Darin Tsui,Kunal Talreja,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: 研究SAEs在低数据场景下基于微调的ESM2嵌入进行蛋白质功能预测和设计的效果，发现SAEs表现良好。


<details>
  <summary>Details</summary>
Motivation: 蛋白质功能预测在数据稀缺场景有挑战，SAEs在低N功能预测和蛋白质设计的有效性未被系统研究。

Method: 评估在微调的ESM2嵌入上训练的SAEs在不同适应度外推和蛋白质工程任务中的表现。

Result: SAEs在仅24个序列时在适应度预测上优于或媲美ESM2基线；利用预测潜在变量设计时，83%情况能得到高适应度变体。

Conclusion: SAEs的稀疏潜在空间编码紧凑且有生物学意义的表征，能从有限数据中更有效泛化。

Abstract: Predicting protein function from amino acid sequence remains a central
challenge in data-scarce (low-$N$) regimes, limiting machine learning-guided
protein design when only small amounts of assay-labeled sequence-function data
are available. Protein language models (pLMs) have advanced the field by
providing evolutionary-informed embeddings and sparse autoencoders (SAEs) have
enabled decomposition of these embeddings into interpretable latent variables
that capture structural and functional features. However, the effectiveness of
SAEs for low-$N$ function prediction and protein design has not been
systematically studied. Herein, we evaluate SAEs trained on fine-tuned ESM2
embeddings across diverse fitness extrapolation and protein engineering tasks.
We show that SAEs, with as few as 24 sequences, consistently outperform or
compete with their ESM2 baselines in fitness prediction, indicating that their
sparse latent space encodes compact and biologically meaningful representations
that generalize more effectively from limited data. Moreover, steering
predictive latents exploits biological motifs in pLM representations, yielding
top-fitness variants in 83% of cases compared to designing with ESM2 alone.

</details>


### [120] [DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model](https://arxiv.org/abs/2508.18579)
*Mohammadreza Ghaffarzadeh-Esfahani,Ali Motahharynia,Nahid Yousefian,Navid Mazrouei,Jafar Ghaisari,Yousof Gheisari*

Main category: cs.LG

TL;DR: 提出基于推理的大语言模型DrugReasoner预测小分子药物获批可能性，性能超传统基线模型，证明推理增强大语言模型可用于药物决策。


<details>
  <summary>Details</summary>
Motivation: 药物发现复杂且资源密集，早期预测获批结果对优化研究投资至关重要，经典机器学习和深度学习方法可解释性有限。

Method: 基于LLaMA架构构建DrugReasoner，用组相对策略优化（GRPO）微调，结合分子描述符与比较推理。

Result: 在验证集和测试集上AUC和F1得分表现良好，超传统基线模型，在外部独立数据集上超基线和ChemAP模型。

Conclusion: DrugReasoner有竞争力的预测准确性，通过推理输出提高透明度，推理增强大语言模型可用于药物决策。

Abstract: Drug discovery is a complex and resource-intensive process, making early
prediction of approval outcomes critical for optimizing research investments.
While classical machine learning and deep learning methods have shown promise
in drug approval prediction, their limited interpretability constraints their
impact. Here, we present DrugReasoner, a reasoning-based large language model
(LLM) built on the LLaMA architecture and fine-tuned with group relative policy
optimization (GRPO) to predict the likelihood of small-molecule approval.
DrugReasoner integrates molecular descriptors with comparative reasoning
against structurally similar approved and unapproved compounds, generating
predictions alongside step-by-step rationales and confidence scores.
DrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score
of 0.729 on the validation set and 0.725 and 0.718 on the test set,
respectively. These results outperformed conventional baselines, including
logistic regression, support vector machine, and k-nearest neighbors and had
competitive performance relative to XGBoost. On an external independent
dataset, DrugReasoner outperformed both baseline and the recently developed
ChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while
maintaining high precision and balanced sensitivity, demonstrating robustness
in real-world scenarios. These findings demonstrate that DrugReasoner not only
delivers competitive predictive accuracy but also enhances transparency through
its reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug
discovery. This study highlights the potential of reasoning-augmented LLMs as
interpretable and effective tools for pharmaceutical decision-making.

</details>


### [121] [Linear Trading Position with Sparse Spectrum](https://arxiv.org/abs/2508.18596)
*Zhao-Rong Lai,Haisheng Yang*

Main category: cs.LG

TL;DR: 提出具有稀疏频谱的线性交易头寸及优化算法，实验表明方法在多种情况下表现良好且稳健。


<details>
  <summary>Details</summary>
Motivation: 现有主投资组合方法在探索预测矩阵关键特征和应对不同情况时存在不足。

Method: 提出具有稀疏频谱的线性交易头寸，并开发Krasnosel'skiĭ - Mann不动点算法优化该交易头寸。

Result: 算法具有下降性质，目标值达到线性收敛率，实验显示方法在多种情况下表现良好且稳健。

Conclusion: 所提方法能有效探索预测矩阵更大频谱区域，且在不同情况下有良好稳健表现。

Abstract: The principal portfolio approach is an emerging method in signal-based
trading. However, these principal portfolios may not be diversified to explore
the key features of the prediction matrix or robust to different situations. To
address this problem, we propose a novel linear trading position with sparse
spectrum that can explore a larger spectral region of the prediction matrix. We
also develop a Krasnosel'ski\u \i-Mann fixed-point algorithm to optimize this
trading position, which possesses the descent property and achieves a linear
convergence rate in the objective value. This is a new theoretical result for
this type of algorithms. Extensive experiments show that the proposed method
achieves good and robust performance in various situations.

</details>


### [122] [Uncertainty Awareness on Unsupervised Domain Adaptation for Time Series Data](https://arxiv.org/abs/2508.18630)
*Weide Liu,Xiaoyang Zhong,Lu Wang,Jingwen Hou,Yuemei Luo,Jiebin Yan,Yuming Fang*

Main category: cs.LG

TL;DR: 本文提出结合多尺度特征提取和不确定性估计来提升无监督领域自适应模型在时间序列数据上的泛化和鲁棒性，实验取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列数据中训练和测试集分布偏移问题，提高无监督领域自适应模型在未标记测试数据上的泛化能力。

Method: 采用多尺度混合输入架构提取不同尺度特征，引入基于证据学习的不确定性感知机制，通过对标签施加狄利克雷先验进行目标预测和不确定性估计。

Result: 该方法在多个基准数据集上达到了SOTA性能，不确定性感知模型的预期校准误差更低。

Conclusion: 混合输入架构与不确定性感知机制相结合的方法在时间序列数据的无监督领域自适应中非常有效。

Abstract: Unsupervised domain adaptation methods seek to generalize effectively on
unlabeled test data, especially when encountering the common challenge in time
series data that distribution shifts occur between training and testing
datasets. In this paper, we propose incorporating multi-scale feature
extraction and uncertainty estimation to improve the model's generalization and
robustness across domains. Our approach begins with a multi-scale mixed input
architecture that captures features at different scales, increasing training
diversity and reducing feature discrepancies between the training and testing
domains. Based on the mixed input architecture, we further introduce an
uncertainty awareness mechanism based on evidential learning by imposing a
Dirichlet prior on the labels to facilitate both target prediction and
uncertainty estimation. The uncertainty awareness mechanism enhances domain
adaptation by aligning features with the same labels across different domains,
which leads to significant performance improvements in the target domain.
Additionally, our uncertainty-aware model demonstrates a much lower Expected
Calibration Error (ECE), indicating better-calibrated prediction confidence.
Our experimental results show that this combined approach of mixed input
architecture with the uncertainty awareness mechanism achieves state-of-the-art
performance across multiple benchmark datasets, underscoring its effectiveness
in unsupervised domain adaptation for time series data.

</details>


### [123] [STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning](https://arxiv.org/abs/2508.18635)
*Yue Jiang,Chenxi Liu,Yile Chen,Qin Chao,Shuai Liu,Gao Cong*

Main category: cs.LG

TL;DR: 提出STRATA - TS框架结合领域自适应检索和大模型改善数据稀缺下的城市预测，实验表明其性能优且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 城市预测模型存在数据不平衡问题，直接从数据丰富城市向稀缺城市迁移不可靠。

Method: 采用基于补丁的时间编码器识别与目标查询语义和动态对齐的源子序列，将检索到的样本注入检索引导推理阶段，通过监督微调将推理过程提炼为紧凑开放模型。

Result: 在三个停车可用性数据集上的实验显示，STRATA - TS始终优于强大的预测和迁移基线。

Conclusion: STRATA - TS能在数据稀缺情况下改善预测，提供可解释的知识迁移途径。

Abstract: Urban forecasting models often face a severe data imbalance problem: only a
few cities have dense, long-span records, while many others expose short or
incomplete histories. Direct transfer from data-rich to data-scarce cities is
unreliable because only a limited subset of source patterns truly benefits the
target domain, whereas indiscriminate transfer risks introducing noise and
negative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware
retrieval for Time Series), a framework that combines domain-adapted retrieval
with reasoning-capable large models to improve forecasting in scarce data
regimes. STRATA-TS employs a patch-based temporal encoder to identify source
subsequences that are semantically and dynamically aligned with the target
query. These retrieved exemplars are then injected into a retrieval-guided
reasoning stage, where an LLM performs structured inference over target inputs
and retrieved support. To enable efficient deployment, we distill the reasoning
process into a compact open model via supervised fine-tuning. Extensive
experiments on three parking availability datasets across Singapore,
Nottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms
strong forecasting and transfer baselines, while providing interpretable
knowledge transfer pathways.

</details>


### [124] [Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance](https://arxiv.org/abs/2508.18638)
*Ifrah Tariq,Ernest Fraenkel*

Main category: cs.LG

TL;DR: 介绍了生物解缠变分自编码器（BDVAE），结合转录组和基因组数据预测免疫检查点抑制剂（ICIs）治疗反应，揭示耐药机制。


<details>
  <summary>Details</summary>
Motivation: ICIs治疗患者反应差异大，耐药机制不明，现有机器学习模型缺乏可解释性且未有效利用多组学数据的生物结构。

Method: 引入BDVAE，通过特定模态和通路编码器整合转录组和基因组数据，采用模块化编码器架构和变分推理学习潜在特征。

Result: 应用于366名患者的泛癌队列，准确预测治疗反应（AUC - ROC = 0.94），揭示关键耐药机制，发现耐药是连续生物谱。

Conclusion: 强调生物结构化机器学习在阐明复杂耐药模式和指导精准免疫治疗策略方面的价值。

Abstract: Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet
patient responses remain highly variable, and the biological mechanisms
underlying resistance are poorly understood. While machine learning models hold
promise for predicting responses to ICIs, most existing methods lack
interpretability and do not effectively leverage the biological structure
inherent to multi-omics data. Here, we introduce the Biologically Disentangled
Variational Autoencoder (BDVAE), a deep generative model that integrates
transcriptomic and genomic data through modality- and pathway-specific
encoders. Unlike existing rigid, pathway-informed models, BDVAE employs a
modular encoder architecture combined with variational inference to learn
biologically meaningful latent features associated with immune, genomic, and
metabolic processes. Applied to a pan-cancer cohort of 366 patients across four
cancer types treated with ICIs, BDVAE accurately predicts treatment response
(AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance
mechanisms, including immune suppression, metabolic shifts, and neuronal
signaling. Importantly, BDVAE reveals that resistance spans a continuous
biological spectrum rather than strictly binary states, reflecting gradations
of tumor dysfunction. Several latent features correlate with survival outcomes
and known clinical subtypes, demonstrating BDVAE's capability to generate
interpretable, clinically relevant insights. These findings underscore the
value of biologically structured machine learning in elucidating complex
resistance patterns and guiding precision immunotherapy strategies.

</details>


### [125] [The Sound of Risk: A Multimodal Physics-Informed Acoustic Model for Forecasting Market Volatility and Enhancing Market Interpretability](https://arxiv.org/abs/2508.18653)
*Xiaoliang Chen,Xin Yu,Le Chang,Teng Jing,Jiashuai He,Ze Wang,Yangjun Luo,Xingyu Chen,Jiayue Liang,Yuchen Wang,Jiaying Xie*

Main category: cs.LG

TL;DR: 提出用于金融风险评估的多模态框架，整合文本情感与高管语音线索，发现多模态特征能解释30天已实现波动率的样本外方差，为投资者和监管者提供工具。


<details>
  <summary>Details</summary>
Motivation: 金融市场信息不对称及企业叙事影响传统文本分析效果，需要新的金融风险评估方法。

Method: 提出多模态框架，使用物理信息声学模型（PIAM）从原始电话会议声音中提取情感特征，将声学和文本情感状态投影到三维情感状态标签（ASL）空间，构建捕捉高管情感动态变化的特征。

Result: 多模态特征虽不能预测股票回报方向，但能解释43.8%的30天已实现波动率的样本外方差，波动率预测受高管从脚本演讲到自发演讲的情感动态驱动，多模态方法优于仅使用财务数据的基线。

Conclusion: 该方法通过解码可验证生物特征信号中的不确定性潜在标记，为投资者和监管者提供增强市场可解释性和识别企业隐藏不确定性的有力工具。

Abstract: Information asymmetry in financial markets, often amplified by strategically
crafted corporate narratives, undermines the effectiveness of conventional
textual analysis. We propose a novel multimodal framework for financial risk
assessment that integrates textual sentiment with paralinguistic cues derived
from executive vocal tract dynamics in earnings calls. Central to this
framework is the Physics-Informed Acoustic Model (PIAM), which applies
nonlinear acoustics to robustly extract emotional signatures from raw
teleconference sound subject to distortions such as signal clipping. Both
acoustic and textual emotional states are projected onto an interpretable
three-dimensional Affective State Label (ASL) space-Tension, Stability, and
Arousal. Using a dataset of 1,795 earnings calls (approximately 1,800 hours),
we construct features capturing dynamic shifts in executive affect between
scripted presentation and spontaneous Q&A exchanges. Our key finding reveals a
pronounced divergence in predictive capacity: while multimodal features do not
forecast directional stock returns, they explain up to 43.8% of the
out-of-sample variance in 30-day realized volatility. Importantly, volatility
predictions are strongly driven by emotional dynamics during executive
transitions from scripted to spontaneous speech, particularly reduced textual
stability and heightened acoustic instability from CFOs, and significant
arousal variability from CEOs. An ablation study confirms that our multimodal
approach substantially outperforms a financials-only baseline, underscoring the
complementary contributions of acoustic and textual modalities. By decoding
latent markers of uncertainty from verifiable biometric signals, our
methodology provides investors and regulators a powerful tool for enhancing
market interpretability and identifying hidden corporate uncertainty.

</details>


### [126] [FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge](https://arxiv.org/abs/2508.18663)
*Gang Hu,Yinglei Teng,Pengfei Wu,Nan Wang*

Main category: cs.LG

TL;DR: 随着大模型向AGI发展，在隐私和资源约束下微调很关键，现有LoRA-based FFT有局限，本文提出FFT MoE框架，实验表明其性能和效率更优。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA-based FFT在异构联邦学习环境中结构不兼容和对非IID数据适应性有限的问题。

Method: 提出FFT MoE框架，用稀疏Mixture of Experts (MoE) 适配器替代LoRA，客户端训练轻量级门控网络，引入异质性感知辅助损失。

Result: 在IID和非IID条件下的大量实验表明，FFT MoE在泛化性能和训练效率上始终优于现有FFT基线。

Conclusion: FFT MoE能有效解决现有方法的局限，提升联邦微调的性能和效率。

Abstract: As FMs drive progress toward Artificial General Intelligence (AGI),
fine-tuning them under privacy and resource constraints has become increasingly
critical particularly when highquality training data resides on distributed
edge devices. Federated Learning (FL) offers a compelling solution through
Federated Fine-Tuning (FFT), which enables collaborative model adaptation
without sharing raw data. Recent approaches incorporate Parameter-Efficient
Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce
computational overhead. However, LoRA-based FFT faces two major limitations in
heterogeneous FL environments: structural incompatibility across clients with
varying LoRA configurations and limited adaptability to non-IID data
distributions, which hinders convergence and generalization. To address these
challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with
sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight
gating network to selectively activate a personalized subset of experts,
enabling fine-grained adaptation to local resource budgets while preserving
aggregation compatibility. To further combat the expert load imbalance caused
by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary
loss that dynamically regularizes the routing distribution to ensure expert
diversity and balanced utilization. Extensive experiments spanning both IID and
non-IID conditions demonstrate that FFT MoE consistently outperforms state of
the art FFT baselines in generalization performance and training efficiency.

</details>


### [127] [Auditing Approximate Machine Unlearning for Differentially Private Models](https://arxiv.org/abs/2508.18671)
*Yuechun Gu,Jiajie He,Keke Chen*

Main category: cs.LG

TL;DR: 本文对近似机器学习遗忘算法应用后的隐私风险进行审计，提出隐私标准，开发高效攻击方法，发现现有算法可能损害保留样本隐私。


<details>
  <summary>Details</summary>
Motivation: 现有近似机器学习遗忘方法假设保留数据不受影响，但隐私洋葱效应表明此假设可能错误，且未研究差分隐私模型中保留数据是否仍满足差分隐私标准。

Method: 从差分隐私和成员推理攻击角度分别为遗忘和保留样本提出隐私标准，开发高效成员推理攻击方法A - LiRA。

Result: 现有近似机器学习遗忘算法可能会损害差分隐私模型中保留样本的隐私。

Conclusion: 需要差分隐私的遗忘算法。

Abstract: Approximate machine unlearning aims to remove the effect of specific data
from trained models to ensure individuals' privacy. Existing methods focus on
the removed records and assume the retained ones are unaffected. However,
recent studies on the \emph{privacy onion effect} indicate this assumption
might be incorrect. Especially when the model is differentially private, no
study has explored whether the retained ones still meet the differential
privacy (DP) criterion under existing machine unlearning methods. This paper
takes a holistic approach to auditing both unlearned and retained samples'
privacy risks after applying approximate unlearning algorithms. We propose the
privacy criteria for unlearned and retained samples, respectively, based on the
perspectives of DP and membership inference attacks (MIAs). To make the
auditing process more practical, we also develop an efficient MIA, A-LiRA,
utilizing data augmentation to reduce the cost of shadow model training. Our
experimental findings indicate that existing approximate machine unlearning
algorithms may inadvertently compromise the privacy of retained samples for
differentially private models, and we need differentially private unlearning
algorithms. For reproducibility, we have pubished our code:
https://anonymous.4open.science/r/Auditing-machine-unlearning-CB10/README.md

</details>


### [128] [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
*Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio Yokota*

Main category: cs.LG

TL;DR: 研究MoE稀疏性对大语言模型记忆和推理能力的影响，发现记忆能力随参数增长，推理能力会饱和甚至倒退。


<details>
  <summary>Details</summary>
Motivation: 现有密集模型前沿未考虑MoE模型引入的新稀疏维度，需研究MoE稀疏性对记忆和推理能力的影响。

Method: 在固定计算预算下，训练一系列系统改变总参数、活跃参数和top - k路由的MoE Transformer模型，记录相关指标。

Result: 记忆基准随总参数单调提升，推理性能会饱和甚至倒退；改变top - k在活跃参数不变时影响小；经典超参数与稀疏性对泛化差距影响同向；后训练强化学习和额外测试计算无法挽救过稀疏模型的推理缺陷。

Conclusion: 明确了MoE稀疏性对大语言模型不同能力制度的影响，代码和日志开源。

Abstract: Empirical scaling laws have driven the evolution of large language models
(LLMs), yet their coefficients shift whenever the model architecture or data
pipeline changes. Mixture-of-Experts (MoE) models, now standard in
state-of-the-art systems, introduce a new sparsity dimension that current
dense-model frontiers overlook. We investigate how MoE sparsity influences two
distinct capability regimes: memorization and reasoning. We train families of
MoE Transformers that systematically vary total parameters, active parameters,
and top-$k$ routing while holding the compute budget fixed. For every model we
record pre-training loss, downstream task loss, and task accuracy, allowing us
to separate the train-test generalization gap from the loss-accuracy gap.
Memorization benchmarks improve monotonically with total parameters, mirroring
training loss. By contrast, reasoning performance saturates and can even
regress despite continued gains in both total parameters and training loss.
Altering top-$k$ alone has little effect when active parameters are constant,
and classic hyperparameters such as learning rate and initialization modulate
the generalization gap in the same direction as sparsity. Neither post-training
reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning
deficit of overly sparse models. Our model checkpoints, code and logs are
open-source at https://github.com/rioyokotalab/optimal-sparsity.

</details>


### [129] [Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding](https://arxiv.org/abs/2508.18676)
*Chufan Gao,Jintai Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 本文提出基于提示的推理方法LRTab，结合微调与免训练提示优点，实验表明其在表格推理中可超越先前基线。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法泛化性差，免训练提示未充分利用训练数据，需新方法结合二者优点。

Method: 先通过提示获取训练数据的思维链响应，对错误思维链让大语言模型预测提示条件以避免错误，用验证数据验证提示条件有效性，推理时检索最相关提示条件用于表格理解。

Result: 在WikiTQ和Tabfact上实验，表明LRTab可解释、成本高效，能超越先前基线。

Conclusion: LRTab是有效的表格推理方法，结合了微调与免训练提示的优点。

Abstract: Automated tabular understanding and reasoning are essential tasks for data
scientists. Recently, Large language models (LLMs) have become increasingly
prevalent in tabular reasoning tasks. Previous work focuses on (1) finetuning
LLMs using labeled data or (2) Training-free prompting LLM agents using
chain-of-thought (CoT). Finetuning offers dataset-specific learning at the cost
of generalizability. Training-free prompting is highly generalizable but does
not take full advantage of training data. In this paper, we propose a novel
prompting-based reasoning approach, Learn then Retrieve: LRTab, which
integrates the benefits of both by retrieving relevant information learned from
training data. We first use prompting to obtain CoT responses over the training
data. For incorrect CoTs, we prompt the LLM to predict Prompt Conditions to
avoid the error, learning insights from the data. We validate the effectiveness
of Prompt Conditions using validation data. Finally, at inference time, we
retrieve the most relevant Prompt Conditions for additional context for table
understanding. We provide comprehensive experiments on WikiTQ and Tabfact,
showing that LRTab is interpretable, cost-efficient, and can outperform
previous baselines in tabular reasoning.

</details>


### [130] [End to End Autoencoder MLP Framework for Sepsis Prediction](https://arxiv.org/abs/2508.18688)
*Hejiang Cai,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: 提出端到端深度学习框架用于脓毒症风险预测，经三个ICU队列验证，性能优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法依赖手动特征工程，难以处理电子健康记录中的不规则、不完整时间序列数据，需要更有效的脓毒症检测方法。

Method: 引入端到端深度学习框架，结合无监督自动编码器进行特征提取和多层感知器分类器进行预测，采用定制下采样策略和非重叠动态滑动窗口机制，处理数据时用固定维度向量和显式缺失指示符。

Result: 模型在三个ICU队列上分别达到74.6%、80.6%和93.5%的准确率，优于传统机器学习基线。

Conclusion: 该框架在不同ICU环境下对早期脓毒症检测具有更强的鲁棒性、泛化性和临床实用性。

Abstract: Sepsis is a life threatening condition that requires timely detection in
intensive care settings. Traditional machine learning approaches, including
Naive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often
rely on manual feature engineering and struggle with irregular, incomplete
time-series data commonly present in electronic health records. We introduce an
end-to-end deep learning framework integrating an unsupervised autoencoder for
automatic feature extraction with a multilayer perceptron classifier for binary
sepsis risk prediction. To enhance clinical applicability, we implement a
customized down sampling strategy that extracts high information density
segments during training and a non-overlapping dynamic sliding window mechanism
for real-time inference. Preprocessed time series data are represented as fixed
dimension vectors with explicit missingness indicators, mitigating bias and
noise. We validate our approach on three ICU cohorts. Our end-to-end model
achieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,
respectively, consistently outperforming traditional machine learning
baselines. These results demonstrate the framework's superior robustness,
generalizability, and clinical utility for early sepsis detection across
heterogeneous ICU environments.

</details>


### [131] [Natural Image Classification via Quasi-Cyclic Graph Ensembles and Random-Bond Ising Models at the Nishimori Temperature](https://arxiv.org/abs/2508.18717)
*V. S. Usatyuk,D. A. Sapoznikov,S. I. Egorov*

Main category: cs.LG

TL;DR: 提出结合统计物理、编码理论和代数拓扑的统一框架用于多类图像分类，设计拓扑引导的图，压缩特征并取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 实现高效的多类图像分类。

Method: 将高维特征向量解释为图上的自旋形成随机键伊辛模型，在Nishimori温度操作，建立图与特征流形拓扑不变量的对应，用二次插值和牛顿校正估计温度，设计特定图集合。

Result: 能将1280维特征压缩到32或64维，在ImageNet - 10和 - 100子集上分别达到98.7%和82.7%的准确率。

Conclusion: 拓扑引导的图设计可产生高效、受物理启发的嵌入，具有最先进的性能。

Abstract: We present a unified framework combining statistical physics, coding theory,
and algebraic topology for efficient multi-class image classification.
High-dimensional feature vectors from a frozen MobileNetV2 backbone are
interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC
(MET-QC-LDPC) graph, forming a Random-Bond Ising Model (RBIM). We operate this
RBIM at its Nishimori temperature, $\beta_N$, where the smallest eigenvalue of
the Bethe-Hessian matrix vanishes, maximizing class separability.
  Our theoretical contribution establishes a correspondence between local
trapping sets in the code's graph and topological invariants (Betti numbers,
bordism classes) of the feature manifold. A practical algorithm estimates
$\beta_N$ efficiently with a quadratic interpolant and Newton correction,
achieving a six-fold speed-up over bisection.
  Guided by topology, we design spherical and toroidal MET-QC-LDPC graph
ensembles, using permanent bounds to suppress harmful trapping sets. This
compresses 1280-dimensional features to 32 or 64 dimensions for ImageNet-10 and
-100 subsets. Despite massive compression (40x fewer parameters), we achieve
98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100, demonstrating that
topology-guided graph design yields highly efficient, physics-inspired
embeddings with state-of-the-art performance.

</details>


### [132] [Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning](https://arxiv.org/abs/2508.18730)
*Yi Liu,Hongji Zhang,Yiwen Wang,Dimitris Tsaras,Lei Chen,Mingxuan Yuan,Qiang Xu*

Main category: cs.LG

TL;DR: 提出StructRTL框架用于RTL设计质量估计，结合结构学习与跨阶段监督取得新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型的方法忽略结构语义，而CDFG能暴露设计结构特征，需更好的方法进行RTL设计质量估计。

Method: 引入结构感知图自监督学习框架StructRTL，从CDFG学习结构信息表示，还结合知识蒸馏策略。

Result: 在各种质量估计任务上显著优于现有方法，建立新的最优结果。

Conclusion: 结合结构学习与跨阶段监督的方法是有效的。

Abstract: Estimating the quality of register transfer level (RTL) designs is crucial in
the electronic design automation (EDA) workflow, as it enables instant feedback
on key metrics like area and delay without the need for time-consuming logic
synthesis. While recent approaches have leveraged large language models (LLMs)
to derive embeddings from RTL code and achieved promising results, they
overlook the structural semantics essential for accurate quality estimation. In
contrast, the control data flow graph (CDFG) view exposes the design's
structural characteristics more explicitly, offering richer cues for
representation learning. In this work, we introduce a novel structure-aware
graph self-supervised learning framework, StructRTL, for improved RTL design
quality estimation. By learning structure-informed representations from CDFGs,
our method significantly outperforms prior art on various quality estimation
tasks. To further boost performance, we incorporate a knowledge distillation
strategy that transfers low-level insights from post-mapping netlists into the
CDFG predictor. Experiments show that our approach establishes new
state-of-the-art results, demonstrating the effectiveness of combining
structural learning with cross-stage supervision.

</details>


### [133] [FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks](https://arxiv.org/abs/2508.18737)
*Enrique Mármol Campos,Aurora González Vidal,José Luis Hernández Ramos,Antonio Skarmeta*

Main category: cs.LG

TL;DR: 本文提出FLAegis防御框架应对联邦学习中拜占庭客户端的中毒攻击，评估显示其在检测精度和模型准确性上优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性使训练过程易受拜占庭客户端的中毒攻击，影响模型训练，需有效防御方法。

Method: 引入两阶段防御框架FLAegis，利用符号时间序列变换（SAX）和谱聚类识别恶意客户端，结合基于FFT的聚合函数减轻漏检客户端的影响。

Result: 在五种中毒攻击的评估中，该方法在检测精度和最终模型准确性上表现优于现有防御方法，在强对抗条件下性能稳定。

Conclusion: FLAegis框架能有效识别拜占庭客户端，提高联邦学习系统的鲁棒性。

Abstract: Federated Learning (FL) has become a powerful technique for training Machine
Learning (ML) models in a decentralized manner, preserving the privacy of the
training datasets involved. However, the decentralized nature of FL limits the
visibility of the training process, relying heavily on the honesty of
participating clients. This assumption opens the door to malicious third
parties, known as Byzantine clients, which can poison the training process by
submitting false model updates. Such malicious clients may engage in poisoning
attacks, manipulating either the dataset or the model parameters to induce
misclassification. In response, this study introduces FLAegis, a two-stage
defensive framework designed to identify Byzantine clients and improve the
robustness of FL systems. Our approach leverages symbolic time series
transformation (SAX) to amplify the differences between benign and malicious
models, and spectral clustering, which enables accurate detection of
adversarial behavior. Furthermore, we incorporate a robust FFT-based
aggregation function as a final layer to mitigate the impact of those Byzantine
clients that manage to evade prior defenses. We rigorously evaluate our method
against five poisoning attacks, ranging from simple label flipping to adaptive
optimization-based strategies. Notably, our approach outperforms
state-of-the-art defenses in both detection precision and final model accuracy,
maintaining consistently high performance even under strong adversarial
conditions.

</details>


### [134] [Stability and Generalization for Bellman Residuals](https://arxiv.org/abs/2508.18741)
*Enoch H. Kang,Kyoungseok Jang*

Main category: cs.LG

TL;DR: 本文解决离线强化学习中Bellman残差最小化（BRM）统计行为未被充分探索的问题，给出稳定性和超额风险界。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习和逆强化学习在执行Bellman一致性上有困难，BRM是有吸引力的解决方法，但在离线场景下其统计行为未被充分探索。

Method: 引入单个Lyapunov势，对相邻数据集上的随机梯度下降 - 上升（SGDA）运行进行耦合分析。

Result: 得到O(1/n)的平均参数稳定性界，使凸 - 凹鞍点问题的最佳已知样本复杂度指数翻倍；在无方差缩减、额外正则化和小批量采样独立性假设下得到O(1/n)的超额风险界。

Conclusion: 结果适用于标准神经网络参数化和小批量SGD，填补了BRM在离线场景下的统计空白。

Abstract: Offline reinforcement learning and offline inverse reinforcement learning aim
to recover near-optimal value functions or reward models from a fixed batch of
logged trajectories, yet current practice still struggles to enforce Bellman
consistency. Bellman residual minimization (BRM) has emerged as an attractive
remedy, as a globally convergent stochastic gradient descent-ascent based
method for BRM has been recently discovered. However, its statistical behavior
in the offline setting remains largely unexplored. In this paper, we close this
statistical gap. Our analysis introduces a single Lyapunov potential that
couples SGDA runs on neighbouring datasets and yields an O(1/n) on-average
argument-stability bound-doubling the best known sample-complexity exponent for
convex-concave saddle problems. The same stability constant translates into the
O(1/n) excess risk bound for BRM, without variance reduction, extra
regularization, or restrictive independence assumptions on minibatch sampling.
The results hold for standard neural-network parameterizations and minibatch
SGD.

</details>


### [135] [Constraint Matters: Multi-Modal Representation for Reducing Mixed-Integer Linear programming](https://arxiv.org/abs/2508.18742)
*Jiajun Li,Ran Hou,Yu Ding,Yixuan Li,Shisi Guan,Jiahui Duan,Xiongwei Han,Tao Zhong,Vincent Chau,Weiwei Wu,Wanyuan Wang*

Main category: cs.LG

TL;DR: 本文提出基于约束的混合整数线性规划（MILP）模型约简方法，通过标记和选择关键约束及多模态表示技术，实验显示相比现有方法提升解的质量超50%，减少计算时间17.47%。


<details>
  <summary>Details</summary>
Motivation: 多数现有模型约简方法基于变量约简，从对偶角度出发的约束约简可降低MILP复杂度却被忽视，因此提出基于约束的模型约简方法。

Method: 标记最优解处的紧约束为潜在关键约束，设计启发式规则选择关键紧约束子集；提出多模态表示技术学习关键紧约束。

Result: 相比现有方法，提升解的质量超50%，减少计算时间17.47%。

Conclusion: 提出的基于约束的模型约简方法有效，能提升解的质量并减少计算时间。

Abstract: Model reduction, which aims to learn a simpler model of the original mixed
integer linear programming (MILP), can solve large-scale MILP problems much
faster. Most existing model reduction methods are based on variable reduction,
which predicts a solution value for a subset of variables. From a dual
perspective, constraint reduction that transforms a subset of inequality
constraints into equalities can also reduce the complexity of MILP, but has
been largely ignored. Therefore, this paper proposes a novel constraint-based
model reduction approach for the MILP. Constraint-based MILP reduction has two
challenges: 1) which inequality constraints are critical such that reducing
them can accelerate MILP solving while preserving feasibility, and 2) how to
predict these critical constraints efficiently. To identify critical
constraints, we first label these tight-constraints at the optimal solution as
potential critical constraints and design a heuristic rule to select a subset
of critical tight-constraints. To learn the critical tight-constraints, we
propose a multi-modal representation technique that leverages information from
both instance-level and abstract-level MILP formulations. The experimental
results show that, compared to the state-of-the-art methods, our method
improves the quality of the solution by over 50\% and reduces the computation
time by 17.47\%.

</details>


### [136] [UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning](https://arxiv.org/abs/2508.18756)
*Zihao Huang,Yu Bao,Qiyang Min,Siyan Chen,Ran Guo,Hongzhi Huang,Defa Zhu,Yutao Zeng,Banggu Wu,Xun Zhou,Siyuan Qiao*

Main category: cs.LG

TL;DR: 提出UltraMemV2改进内存层架构，实现与8专家MoE模型性能相当且内存访问低，在内存密集任务表现优。


<details>
  <summary>Details</summary>
Motivation: MoE模型推理时内存访问成本高，以往内存层架构如UltraMem性能不及8专家MoE模型。

Method: 对内存层架构进行五项关键改进，包括集成到每个Transformer块、简化值扩展等。

Result: UltraMemV2与8专家MoE模型性能相当但内存访问低，在内存密集任务有显著提升，验证大规模模型并发现激活密度对性能影响更大。

Conclusion: 内存层架构可与最先进的MoE模型性能相当，是高效稀疏计算的有力替代方案。

Abstract: While Mixture of Experts (MoE) models achieve remarkable efficiency by
activating only subsets of parameters, they suffer from high memory access
costs during inference. Memory-layer architectures offer an appealing
alternative with very few memory access, but previous attempts like UltraMem
have only matched the performance of 2-expert MoE models, falling significantly
short of state-of-the-art 8-expert configurations. We present UltraMemV2, a
redesigned memory-layer architecture that closes this performance gap. Our
approach introduces five key improvements: integrating memory layers into every
transformer block, simplifying value expansion with single linear projections,
adopting FFN-based value processing from PEER, implementing principled
parameter initialization, and rebalancing memory-to-FFN computation ratios.
Through extensive evaluation, we demonstrate that UltraMemV2 achieves
performance parity with 8-expert MoE models under same computation and
parameters but significantly low memory access. Notably, UltraMemV2 shows
superior performance on memory-intensive tasks, with improvements of +1.6
points on long-context memorization, +6.2 points on multi-round memorization,
and +7.9 points on in-context learning. We validate our approach at scale with
models up to 2.5B activated parameters from 120B total parameters, and
establish that activation density has greater impact on performance than total
sparse parameter count. Our work brings memory-layer architectures to
performance parity with state-of-the-art MoE models, presenting a compelling
alternative for efficient sparse computation.

</details>


### [137] [Governance-as-a-Service: A Multi-Agent Framework for AI System Compliance and Policy Enforcement](https://arxiv.org/abs/2508.18765)
*Helen Pervez,Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.LG

TL;DR: 现有AI系统治理机制存在不足，本文提出GaaS，经模拟评估其能可靠管控高风险行为，为多智能体系统建立基础设施级对齐。


<details>
  <summary>Details</summary>
Motivation: AI系统演变为分布式生态系统，现有监督机制存在反应式、脆弱、不可审计等问题，需要可扩展、解耦的治理机制。

Method: 引入GaaS，采用声明式规则和信任因子机制，对开源模型进行三种模拟实验，拦截、评估和记录行为。

Result: GaaS能可靠地阻止或重定向高风险行为，同时保持吞吐量，信任分数可追踪规则遵守情况。

Conclusion: GaaS将治理作为运行时服务，为可互操作的智能体生态系统建立基础设施级对齐，强制执行伦理规则。

Abstract: As AI systems evolve into distributed ecosystems with autonomous execution,
asynchronous reasoning, and multi-agent coordination, the absence of scalable,
decoupled governance poses a structural risk. Existing oversight mechanisms are
reactive, brittle, and embedded within agent architectures, making them
non-auditable and hard to generalize across heterogeneous deployments.
  We introduce Governance-as-a-Service (GaaS): a modular, policy-driven
enforcement layer that regulates agent outputs at runtime without altering
model internals or requiring agent cooperation. GaaS employs declarative rules
and a Trust Factor mechanism that scores agents based on compliance and
severity-weighted violations. It enables coercive, normative, and adaptive
interventions, supporting graduated enforcement and dynamic trust modulation.
  To evaluate GaaS, we conduct three simulation regimes with open-source models
(LLaMA3, Qwen3, DeepSeek-R1) across content generation and financial
decision-making. In the baseline, agents act without governance; in the second,
GaaS enforces policies; in the third, adversarial agents probe robustness. All
actions are intercepted, evaluated, and logged for analysis. Results show that
GaaS reliably blocks or redirects high-risk behaviors while preserving
throughput. Trust scores track rule adherence, isolating and penalizing
untrustworthy components in multi-agent systems.
  By positioning governance as a runtime service akin to compute or storage,
GaaS establishes infrastructure-level alignment for interoperable agent
ecosystems. It does not teach agents ethics; it enforces them.

</details>


### [138] [Predicting Drug-Drug Interactions Using Heterogeneous Graph Neural Networks: HGNN-DDI](https://arxiv.org/abs/2508.18766)
*Hongbo Liu,Siyi Li,Zheng Yu*

Main category: cs.LG

TL;DR: 提出HGNN - DDI模型预测药物相互作用，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用在临床实践中是重要问题，传统计算方法难以捕捉药物、靶点和生物实体间复杂关系。

Method: 提出HGNN - DDI异构图神经网络模型，整合多种药物相关数据源，利用图表示学习对异构生物医学网络建模。

Result: 在基准药物相互作用数据集上实验，HGNN - DDI在预测准确性和鲁棒性上优于现有基线。

Conclusion: HGNN - DDI有潜力支持更安全的药物开发和精准医学。

Abstract: Drug-drug interactions (DDIs) are a major concern in clinical practice, as
they can lead to reduced therapeutic efficacy or severe adverse effects.
Traditional computational approaches often struggle to capture the complex
relationships among drugs, targets, and biological entities. In this work, we
propose HGNN-DDI, a heterogeneous graph neural network model designed to
predict potential DDIs by integrating multiple drug-related data sources.
HGNN-DDI leverages graph representation learning to model heterogeneous
biomedical networks, enabling effective information propagation across diverse
node and edge types. Experimental results on benchmark DDI datasets demonstrate
that HGNN-DDI outperforms state-of-the-art baselines in prediction accuracy and
robustness, highlighting its potential to support safer drug development and
precision medicine.

</details>


### [139] [SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation](https://arxiv.org/abs/2508.18826)
*Junyu Yan,Feng Chen,Yuyang Xue,Yuning Du,Konstantinos Vilouras,Sotirios A. Tsaftaris,Steven McDonagh*

Main category: cs.LG

TL;DR: 提出SWiFT去偏框架改善模型公平性且保持判别性能，成本低，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在现实场景有偏差，影响公平性和泛化能力，现有去偏方法有局限，需新方法。

Method: 提出SWiFT框架，先分析参数对偏差和预测性能的贡献，再通过两步微调更新参数。

Result: 在多个数据集实验表明，SWiFT能降低偏差，诊断准确性有竞争力甚至更优，泛化能力提升。

Conclusion: SWiFT是高效去偏框架，在降低偏差同时保持判别性能，成本低。

Abstract: Recent studies have shown that Machine Learning (ML) models can exhibit bias
in real-world scenarios, posing significant challenges in ethically sensitive
domains such as healthcare. Such bias can negatively affect model fairness,
model generalization abilities and further risks amplifying social
discrimination. There is a need to remove biases from trained models. Existing
debiasing approaches often necessitate access to original training data and
need extensive model retraining; they also typically exhibit trade-offs between
model fairness and discriminative performance. To address these challenges, we
propose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that
efficiently improves fairness while preserving discriminative performance with
much less debiasing costs. Notably, SWiFT requires only a small external
dataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to
first find the relative, and yet distinct, contributions of model parameters to
both bias and predictive performance. Then, a two-step fine-tuning process
updates each parameter with different gradient flows defined by its
contribution. Extensive experiments with three bias sensitive attributes
(gender, skin tone, and age) across four dermatological and two chest X-ray
datasets demonstrate that SWiFT can consistently reduce model bias while
achieving competitive or even superior diagnostic accuracy under common
fairness and accuracy metrics, compared to the state-of-the-art. Specifically,
we demonstrate improved model generalization ability as evidenced by superior
performance on several out-of-distribution (OOD) datasets.

</details>


### [140] [DRMD: Deep Reinforcement Learning for Malware Detection under Concept Drift](https://arxiv.org/abs/2508.18839)
*Shae McFadden,Myles Foley,Mario D'Onghia,Chris Hicks,Vasilios Mavroudis,Nicola Paoletti,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 本文提出基于深度强化学习的恶意软件检测方法DRMD，在安卓恶意软件数据集上评估，结果显示其对概念漂移有更好的恢复能力，提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统分类器在恶意软件领域的概念漂移下难以维持性能，现代检测管道虽有改进但仍有不足，需更好方法应对不断演变的威胁、有限的标注预算和不确定的预测。

Method: 将恶意软件检测建模为一步马尔可夫决策过程，训练深度强化学习（DRL）代理，同时优化样本分类性能和拒绝高风险样本进行手动标注。

Result: DRMD代理在时间感知评估中，相比标准分类方法在不同设置下实现了平均AUT性能提升。

Conclusion: 首次证明DRL能在安卓恶意软件领域的动态环境中促进有效的恶意软件检测并提高对概念漂移的恢复能力。

Abstract: Malware detection in real-world settings must deal with evolving threats,
limited labeling budgets, and uncertain predictions. Traditional classifiers,
without additional mechanisms, struggle to maintain performance under concept
drift in malware domains, as their supervised learning formulation cannot
optimize when to defer decisions to manual labeling and adaptation. Modern
malware detection pipelines combine classifiers with monthly active learning
(AL) and rejection mechanisms to mitigate the impact of concept drift. In this
work, we develop a novel formulation of malware detection as a one-step Markov
Decision Process and train a deep reinforcement learning (DRL) agent,
simultaneously optimizing sample classification performance and rejecting
high-risk samples for manual labeling. We evaluated the joint detection and
drift mitigation policy learned by the DRL-based Malware Detection (DRMD) agent
through time-aware evaluations on Android malware datasets subject to realistic
drift requiring multi-year performance stability. The policies learned under
these conditions achieve a higher Area Under Time (AUT) performance compared to
standard classification approaches used in the domain, showing improved
resilience to concept drift. Specifically, the DRMD agent achieved a
$5.18\pm5.44$, $14.49\pm12.86$, and $10.06\pm10.81$ average AUT performance
improvement for the classification only, classification with rejection, and
classification with rejection and AL settings, respectively. Our results
demonstrate for the first time that DRL can facilitate effective malware
detection and improved resiliency to concept drift in the dynamic environment
of the Android malware domain.

</details>


### [141] [Recycling History: Efficient Recommendations from Contextual Dueling Bandits](https://arxiv.org/abs/2508.18841)
*Suryanarayana Sankagiri,Jalal Etesami,Pouria Fatemi,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 提出新的老虎机模型，通过利用用户消费历史进行物品比较，证明初始随机探索可积累丰富历史，实现O(√T)遗憾保证，模拟显示该方法降低遗憾效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有上下文决斗老虎机问题未涵盖其他比较查询，且用户消费物品后反馈更可靠，故提出新模型。

Method: 先证明历史丰富时算法可构建信息查询，再证明短时间初始随机探索能大概率积累丰富历史，利用矩阵集中界证明结果。

Result: 实现O(√T)遗憾保证，模拟显示复用历史物品比较比仅比较同时推荐物品遗憾显著降低。

Conclusion: 新的老虎机模型能有效利用用户消费历史进行比较，在降低遗憾方面有优势。

Abstract: The contextual duelling bandit problem models adaptive recommender systems,
where the algorithm presents a set of items to the user, and the user's choice
reveals their preference. This setup is well suited for implicit choices users
make when navigating a content platform, but does not capture other possible
comparison queries. Motivated by the fact that users provide more reliable
feedback after consuming items, we propose a new bandit model that can be
described as follows. The algorithm recommends one item per time step; after
consuming that item, the user is asked to compare it with another item chosen
from the user's consumption history. Importantly, in our model, this comparison
item can be chosen without incurring any additional regret, potentially leading
to better performance. However, the regret analysis is challenging because of
the temporal dependency in the user's history. To overcome this challenge, we
first show that the algorithm can construct informative queries provided the
history is rich, i.e., satisfies a certain diversity condition. We then show
that a short initial random exploration phase is sufficient for the algorithm
to accumulate a rich history with high probability. This result, proven via
matrix concentration bounds, yields $O(\sqrt{T})$ regret guarantees.
Additionally, our simulations show that reusing past items for comparisons can
lead to significantly lower regret than only comparing between simultaneously
recommended items.

</details>


### [142] [C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning](https://arxiv.org/abs/2508.18860)
*Wei Li,Hangjie Yuan,Zixiang Zhao,Yifan Zhu,Aojun Lu,Tao Feng,Yanan Sun*

Main category: cs.LG

TL;DR: 提出适用于持续学习的C - Flat方法及C - Flat++框架，实验证明其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于零阶锐度的方法在持续学习中可能导致解不够鲁棒和次优，需更好方法平衡新任务敏感性和旧知识稳定性。

Method: 提出C - Flat方法促进更平坦的损失景观，具有即插即用兼容性；提出通用框架将C - Flat集成到主要持续学习范式；提出C - Flat++框架利用选择性平坦驱动提升效率。

Result: C - Flat在多种设置下持续提升性能，C - Flat++显著降低更新成本。

Conclusion: 提出的方法和框架在多个持续学习方法、数据集和场景中有效且高效。

Abstract: Balancing sensitivity to new tasks and stability for retaining past knowledge
is crucial in continual learning (CL). Recently, sharpness-aware minimization
has proven effective in transfer learning and has also been adopted in
continual learning (CL) to improve memory retention and learning efficiency.
However, relying on zeroth-order sharpness alone may favor sharper minima over
flatter ones in certain settings, leading to less robust and potentially
suboptimal solutions. In this paper, we propose \textbf{C}ontinual
\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter loss
landscapes tailored for CL. C-Flat offers plug-and-play compatibility, enabling
easy integration with minimal modifications to the code pipeline. Besides, we
present a general framework that integrates C-Flat into all major CL paradigms
and conduct comprehensive comparisons with loss-minima optimizers and
flat-minima-based CL methods. Our results show that C-Flat consistently
improves performance across a wide range of settings. In addition, we introduce
C-Flat++, an efficient yet effective framework that leverages selective
flatness-driven promotion, significantly reducing the update cost required by
C-Flat. Extensive experiments across multiple CL methods, datasets, and
scenarios demonstrate the effectiveness and efficiency of our proposed
approaches. Code is available at https://github.com/WanNaa/C-Flat.

</details>


### [143] [MOCHA: Discovering Multi-Order Dynamic Causality in Temporal Point Processes](https://arxiv.org/abs/2508.18873)
*Yunyang Cao,Juekai Lin,Wenhao Li,Bo Jin*

Main category: cs.LG

TL;DR: 提出MOCHA框架用于发现时间点过程中的多阶动态因果关系，实验表明其在事件预测和揭示因果结构方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了因果关系的多阶和时变特性，需要一种新方法来发现时间点过程中的复杂因果依赖。

Method: 提出MOCHA框架，将多阶影响表征为潜在时变图上的多跳因果路径，引入带可学习结构权重的时变有向无环图并施加约束，设计端到端可微框架联合建模因果发现和时间点过程动态。

Result: 在真实数据集上的实验表明，MOCHA在事件预测上达到了最先进水平，还揭示了有意义且可解释的因果结构。

Conclusion: MOCHA是一种有效的发现时间点过程中多阶动态因果关系的框架。

Abstract: Discovering complex causal dependencies in temporal point processes (TPPs) is
critical for modeling real-world event sequences. Existing methods typically
rely on static or first-order causal structures, overlooking the multi-order
and time-varying nature of causal relationships. In this paper, we propose
MOCHA, a novel framework for discovering multi-order dynamic causality in TPPs.
MOCHA characterizes multi-order influences as multi-hop causal paths over a
latent time-evolving graph. To model such dynamics, we introduce a time-varying
directed acyclic graph (DAG) with learnable structural weights, where
acyclicity and sparsity constraints are enforced to ensure structural validity.
We design an end-to-end differentiable framework that jointly models causal
discovery and TPP dynamics, enabling accurate event prediction and revealing
interpretable structures. Extensive experiments on real-world datasets
demonstrate that MOCHA not only achieves state-of-the-art performance in event
prediction, but also reveals meaningful and interpretable causal structures.

</details>


### [144] [HAEPO: History-Aggregated Exploratory Policy Optimization](https://arxiv.org/abs/2508.18884)
*Gaurish Trivedi,Alakh Sharma,Kartikey Singh Bhandari,Dhruv Kumar,Pratik Narang,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: 提出HAEPO方法，通过历史聚合探索损失解决现有方法在长程任务中探索受限问题，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有方法如DPO和GRPO在长程任务中常限制探索，需要新方法解决此问题。

Method: 引入HAEPO，将轨迹压缩为对数概率之和，应用Plackett - Luce softmax获取归一化权重，添加熵正则化和软KL惩罚。

Result: HAEPO收敛快、探索充分、与真实奖励紧密对齐，在不同任务上学习表现优于或等同于PPO、GRPO和DPO。

Conclusion: HAEPO通过利用全轨迹历史，平衡探索与稳定性，提供了稳定且可解释的框架。

Abstract: Exploration is essential in modern learning, from reinforcement learning
environments with small neural policies to large language models (LLMs).
Existing work, such as DPO, leverages full sequence log-likelihoods to capture
an entire trajectory of the model's decisions, while methods like GRPO
aggregate per-token ratios into a trajectory-level update. However, both often
limit exploration on long-horizon tasks. We introduce History-Aggregated
Exploratory Policy Optimization (HAEPO), a history-aware exploratory loss to
combat these shortcomings. HAEPO compresses each trajectory into the sum of its
logarithmic probabilities (a cumulative logarithmic likelihood), and applies a
Plackett-Luce softmax across trajectories to obtain normalized weights
proportional to their returns, thus encouraging broader exploration. We add
entropy regularization to stabilize the aggressive updates to prevent premature
collapse and a soft KL penalty relative to a frozen copy of the previous
(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,
aligns closely with true rewards, and demonstrates robust learning behavior
better or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO
provides a stable and interpretable framework by explicitly leveraging
full-trajectory history while balancing exploration and stability.

</details>


### [145] [pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data](https://arxiv.org/abs/2508.18891)
*Zhijin Wang,Senzhen Wu,Yue Hu,Xiufeng Liu*

Main category: cs.LG

TL;DR: 介绍研究导向的 PyTorch 框架 pyFAST，支持复杂时间序列场景，功能丰富且模块化，利于时间序列研究与应用。


<details>
  <summary>Details</summary>
Motivation: 现有 Python 库在模块化及对不规则、多源或稀疏数据支持上有局限，需要灵活、高效和可扩展的时间序列分析框架。

Method: 开发 pyFAST 框架，将数据处理与模型计算解耦，设计适用于复杂场景的数据引擎，集成多种功能和模型。

Result: pyFAST 支持多源加载、序列处理等，集成多种架构和功能，提供训练工具，包含多种模型。

Conclusion: pyFAST 以 MIT 许可在 GitHub 发布，为时间序列研究和应用提供强大平台。

Abstract: Modern time series analysis demands frameworks that are flexible, efficient,
and extensible. However, many existing Python libraries exhibit limitations in
modularity and in their native support for irregular, multi-source, or sparse
data. We introduce pyFAST, a research-oriented PyTorch framework that
explicitly decouples data processing from model computation, fostering a
cleaner separation of concerns and facilitating rapid experimentation. Its data
engine is engineered for complex scenarios, supporting multi-source loading,
protein sequence handling, efficient sequence- and patch-level padding, dynamic
normalization, and mask-based modeling for both imputation and forecasting.
pyFAST integrates LLM-inspired architectures for the alignment-free fusion of
sparse data sources and offers native sparse metrics, specialized loss
functions, and flexible exogenous data fusion. Training utilities include
batch-based streaming aggregation for evaluation and device synergy to maximize
computational efficiency. A comprehensive suite of classical and deep learning
models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a
modular architecture that encourages extension. Released under the MIT license
at GitHub, pyFAST provides a compact yet powerful platform for advancing time
series research and applications.

</details>


### [146] [Distance-informed Neural Processes](https://arxiv.org/abs/2508.18903)
*Aishwarya Venkataramanan,Joachim Denzler*

Main category: cs.LG

TL;DR: 提出距离感知神经过程（DNP），结合全局和局部潜在结构改进不确定性估计，在回归和分类任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 标准神经过程（NPs）依赖全局潜变量，在不确定性校准和捕捉局部数据依赖方面存在困难。

Method: 引入全局潜变量建模任务级变化，局部潜变量在保距潜在空间捕捉输入相似性，通过双Lipschitz正则化实现。

Result: DNP在回归和分类任务中取得强预测性能和改进的不确定性校准。

Conclusion: DNP能产生校准更好的不确定性估计，更有效区分分布内和分布外数据。

Abstract: We propose the Distance-informed Neural Process (DNP), a novel variant of
Neural Processes that improves uncertainty estimation by combining global and
distance-aware local latent structures. Standard Neural Processes (NPs) often
rely on a global latent variable and struggle with uncertainty calibration and
capturing local data dependencies. DNP addresses these limitations by
introducing a global latent variable to model task-level variations and a local
latent variable to capture input similarity within a distance-preserving latent
space. This is achieved through bi-Lipschitz regularization, which bounds
distortions in input relationships and encourages the preservation of relative
distances in the latent space. This modeling approach allows DNP to produce
better-calibrated uncertainty estimates and more effectively distinguish in-
from out-of-distribution data. Empirical results demonstrate that DNP achieves
strong predictive performance and improved uncertainty calibration across
regression and classification tasks.

</details>


### [147] [Enhancing Model Privacy in Federated Learning with Random Masking and Quantization](https://arxiv.org/abs/2508.18911)
*Zhibo Xu,Jianhao Zhu,Jingwen Xu,Changze Lv,Zisu Huang,Xiaohua Wang,Muling Wu,Qi Qian,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.LG

TL;DR: 实验表明该方法在联邦学习中保持性能并增强模型参数保护。


<details>
  <summary>Details</summary>
Motivation: 未提及

Method: 未提及

Result: 在多种模型和任务的实验中，该方法在联邦学习中保持强模型性能，且相比基线方法增强了模型参数保护。

Conclusion: 该方法在联邦学习中有良好表现，能增强参数保护。

Abstract: Experimental results across various models and tasks demonstrate that our
approach not only maintains strong model performance in federated learning
settings but also achieves enhanced protection of model parameters compared to
baseline methods.

</details>


### [148] [Generalization Bound for a General Class of Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.18920)
*Madhusudan Verma,Manoj Kumar*

Main category: cs.LG

TL;DR: 本文分析具有一般非线性动力学的神经常微分方程（neural ODEs）泛化误差界，是该领域首次推导。


<details>
  <summary>Details</summary>
Motivation: 此前研究主要关注neural ODEs动力学函数的线性情况或依赖采样间隔的界，为评估模型在未见数据上的表现，需理解一般非线性动力学的neural ODEs泛化误差界。

Method: 分析动力学函数为一般非线性函数（时间相关或无关）且关于状态变量Lipschitz连续的neural ODEs，证明其解具有有界变差，进而建立泛化界。

Result: 在Lipschitz条件下，证明neural ODEs的解有有界变差，并建立时间相关和无关情况下的泛化界，研究了过参数化和域约束对界的影响。

Conclusion: 首次推导了具有一般非线性动力学的neural ODEs的泛化界。

Abstract: Neural ordinary differential equations (neural ODEs) are a popular type of
deep learning model that operate with continuous-depth architectures. To assess
how well such models perform on unseen data, it is crucial to understand their
generalization error bounds. Previous research primarily focused on the linear
case for the dynamics function in neural ODEs - Marion, P. (2023), or provided
bounds for Neural Controlled ODEs that depend on the sampling interval
Bleistein et al. (2023). In this work, we analyze a broader class of neural
ODEs where the dynamics function is a general nonlinear function, either time
dependent or time independent, and is Lipschitz continuous with respect to the
state variables. We showed that under this Lipschitz condition, the solutions
to neural ODEs have solutions with bounded variations. Based on this
observation, we establish generalization bounds for both time-dependent and
time-independent cases and investigate how overparameterization and domain
constraints influence these bounds. To our knowledge, this is the first
derivation of generalization bounds for neural ODEs with general nonlinear
dynamics.

</details>


### [149] [HierCVAE: Hierarchical Attention-Driven Conditional Variational Autoencoders for Multi-Scale Temporal Modeling](https://arxiv.org/abs/2508.18922)
*Yao Wu*

Main category: cs.LG

TL;DR: 提出HierCVAE架构解决复杂系统时间建模挑战，在能耗数据集评估中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 复杂系统时间建模需捕捉多时间尺度依赖并处理不确定性。

Method: 提出HierCVAE架构，集成分层注意力机制与条件变分自编码器，采用三层注意力结构和多模态条件编码，在隐空间加入ResFormer块并通过预测头进行不确定性量化。

Result: 在能耗数据集评估中，预测精度提高15 - 40%，不确定性校准更优，在长期预测和复杂多变量依赖方面表现出色。

Conclusion: HierCVAE能有效解决复杂系统时间建模问题，性能优于现有方法。

Abstract: Temporal modeling in complex systems requires capturing dependencies across
multiple time scales while managing inherent uncertainties. We propose
HierCVAE, a novel architecture that integrates hierarchical attention
mechanisms with conditional variational autoencoders to address these
challenges. HierCVAE employs a three-tier attention structure (local, global,
cross-temporal) combined with multi-modal condition encoding to capture
temporal, statistical, and trend information. The approach incorporates
ResFormer blocks in the latent space and provides explicit uncertainty
quantification via prediction heads. Through evaluations on energy consumption
datasets, HierCVAE demonstrates a 15-40% improvement in prediction accuracy and
superior uncertainty calibration compared to state-of-the-art methods,
excelling in long-term forecasting and complex multi-variate dependencies.

</details>


### [150] [Energy-Based Flow Matching for Generating 3D Molecular Structure](https://arxiv.org/abs/2508.18949)
*Wenyin Zhou,Christopher Iliffe Sprague,Vsevolod Viliuga,Matteo Tadiello,Arne Elofsson,Hossein Azizpour*

Main category: cs.LG

TL;DR: 本文聚焦流匹配，从能量角度改进分子结构生成模型的训练和推理，实验证明方法有效且优于基线。


<details>
  <summary>Details</summary>
Motivation: 分子结构生成在生物领域有重要应用，利用生成模型解决该问题有进展，本文从能量角度改进流匹配以提升结构生成模型。

Method: 采用基于能量的视角，用深度网络表示映射函数，将随机配置迭代映射到目标结构。

Result: 实验表明该方法在蛋白质对接和蛋白质主链生成任务中有效，在相近计算预算下优于近期基线。

Conclusion: 提出的流匹配设置概念简单、经验有效，理论合理且与一些基本性质和实用技术有联系。

Abstract: Molecular structure generation is a fundamental problem that involves
determining the 3D positions of molecules' constituents. It has crucial
biological applications, such as molecular docking, protein folding, and
molecular design. Recent advances in generative modeling, such as diffusion
models and flow matching, have made great progress on these tasks by modeling
molecular conformations as a distribution. In this work, we focus on flow
matching and adopt an energy-based perspective to improve training and
inference of structure generation models. Our view results in a mapping
function, represented by a deep network, that is directly learned to
\textit{iteratively} map random configurations, i.e. samples from the source
distribution, to target structures, i.e. points in the data manifold. This
yields a conceptually simple and empirically effective flow matching setup that
is theoretically justified and has interesting connections to fundamental
properties such as idempotency and stability, as well as the empirically useful
techniques such as structure refinement in AlphaFold. Experiments on protein
docking as well as protein backbone generation consistently demonstrate the
method's effectiveness, where it outperforms recent baselines of
task-associated flow matching and diffusion models, using a similar
computational budget.

</details>


### [151] [Estimating Conditional Covariance between labels for Multilabel Data](https://arxiv.org/abs/2508.18951)
*Laurence A. F. Park,Jesse Read*

Main category: cs.LG

TL;DR: 比较三种模型（多元Probit、多元伯努利和分阶段Logit）估计多标签条件标签协方差，发现各模型在协方差强度不同时表现相当，但都有误判情况，多元Probit模型误差率最低。


<details>
  <summary>Details</summary>
Motivation: 多标签数据应用模型前需分析标签依赖，而多元Probit模型在估计协方差时可能不可靠，因此比较三种模型估计协方差的效果。

Method: 对比多元Probit、多元伯努利和分阶段Logit三种模型，通过实验观察各模型对条件协方差的测量。

Result: 各模型对恒定和依赖协方差的测量效果取决于协方差强度，且都会误判有恒定协方差的数据为有依赖协方差，多元Probit模型误差率最低。

Conclusion: 三种模型在测量协方差时有各自特点，多元Probit模型在误差控制上表现较好。

Abstract: Multilabel data should be analysed for label dependence before applying
multilabel models. Independence between multilabel data labels cannot be
measured directly from the label values due to their dependence on the set of
covariates $\vec{x}$, but can be measured by examining the conditional label
covariance using a multivariate Probit model. Unfortunately, the multivariate
Probit model provides an estimate of its copula covariance, and so might not be
reliable in estimating constant covariance and dependent covariance. In this
article, we compare three models (Multivariate Probit, Multivariate Bernoulli
and Staged Logit) for estimating the constant and dependent multilabel
conditional label covariance. We provide an experiment that allows us to
observe each model's measurement of conditional covariance. We found that all
models measure constant and dependent covariance equally well, depending on the
strength of the covariance, but the models all falsely detect that dependent
covariance is present for data where constant covariance is present. Of the
three models, the Multivariate Probit model had the lowest error rate.

</details>


### [152] [On the Generalisation of Koopman Representations for Chaotic System Control](https://arxiv.org/abs/2508.18954)
*Kyriakos Hjikakou,Juan Diego Cardenas Cartagena,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 本文研究基于Koopman的混沌动力系统表示的泛化性，以Lorenz系统为测试平台提出三阶段方法，结果显示Koopman嵌入表现出色，支持其用于多任务学习。


<details>
  <summary>Details</summary>
Motivation: 研究基于Koopman的混沌动力系统表示在预测和控制任务间的可迁移性和泛化性。

Method: 以Lorenz系统为测试平台，提出三阶段方法：通过自编码学习Koopman嵌入、在下一步状态预测上预训练Transformer、针对安全关键控制进行微调。

Result: Koopman嵌入优于标准和物理信息PCA基线，实现准确且数据高效的性能；微调时固定预训练的Transformer权重不导致性能下降。

Conclusion: 支持将Koopman嵌入作为物理信息机器学习中多任务学习的基础。

Abstract: This paper investigates the generalisability of Koopman-based representations
for chaotic dynamical systems, focusing on their transferability across
prediction and control tasks. Using the Lorenz system as a testbed, we propose
a three-stage methodology: learning Koopman embeddings through autoencoding,
pre-training a transformer on next-state prediction, and fine-tuning for
safety-critical control. Our results show that Koopman embeddings outperform
both standard and physics-informed PCA baselines, achieving accurate and
data-efficient performance. Notably, fixing the pre-trained transformer weights
during fine-tuning leads to no performance degradation, indicating that the
learned representations capture reusable dynamical structure rather than
task-specific patterns. These findings support the use of Koopman embeddings as
a foundation for multi-task learning in physics-informed machine learning. A
project page is available at https://kikisprdx.github.io/.

</details>


### [153] [PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations](https://arxiv.org/abs/2508.18982)
*Tim Kreuzer,Jelena Zdravkovic,Panagiotis Papapetrou*

Main category: cs.LG

TL;DR: 提出PAX - TS算法解释时间序列预测模型，在基准测试验证其效果，发现解释能捕捉模型行为并识别性能模式。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型不透明且缺乏合适的事后解释方法。

Method: 提出基于局部输入扰动的模型无关事后算法PAX - TS，能产生多粒度解释和刻画多变量时间序列预测的跨通道相关性。

Result: PAX - TS解释能有效捕捉模型行为，识别出6类重复出现的模式，不同类模式预测误差有差异，能展示模型如何考虑跨通道相关性。

Conclusion: PAX - TS可从不同层面解释时间序列预测模型机制，其解释可用于回答预测相关实际问题。

Abstract: Time series forecasting has seen considerable improvement during the last
years, with transformer models and large language models driving advancements
of the state of the art. Modern forecasting models are generally opaque and do
not provide explanations for their forecasts, while well-known post-hoc
explainability methods like LIME are not suitable for the forecasting context.
We propose PAX-TS, a model-agnostic post-hoc algorithm to explain time series
forecasting models and their forecasts. Our method is based on localized input
perturbations and results in multi-granular explanations. Further, it is able
to characterize cross-channel correlations for multivariate time series
forecasts. We clearly outline the algorithmic procedure behind PAX-TS,
demonstrate it on a benchmark with 7 algorithms and 10 diverse datasets,
compare it with two other state-of-the-art explanation algorithms, and present
the different explanation types of the method. We found that the explanations
of high-performing and low-performing algorithms differ on the same datasets,
highlighting that the explanations of PAX-TS effectively capture a model's
behavior. Based on time step correlation matrices resulting from the benchmark,
we identify 6 classes of patterns that repeatedly occur across different
datasets and algorithms. We found that the patterns are indicators of
performance, with noticeable differences in forecasting error between the
classes. Lastly, we outline a multivariate example where PAX-TS demonstrates
how the forecasting model takes cross-channel correlations into account. With
PAX-TS, time series forecasting models' mechanisms can be illustrated in
different levels of detail, and its explanations can be used to answer
practical questions on forecasts.

</details>


### [154] [STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems](https://arxiv.org/abs/2508.19011)
*Gary Simethy,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.LG

TL;DR: 提出 STDiff 模型用于工业时间序列缺失值插补，在数据集上表现良好，支持动态感知插补方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习缺失值插补方法在工业系统中因系统动态性等问题假设常不成立，需新方法。

Method: 提出 STDiff，使用带因果偏差的条件去噪扩散模型，基于最近已知状态和相关输入逐步生成缺失值。

Result: 在模拟缺失块的公共废水处理数据集上误差最低，在有大量真实缺失的工业数据集上生成的轨迹动态合理，优于基于窗口的模型。

Conclusion: 动态感知、显式条件插补是工业时间序列的稳健方法，还讨论了计算权衡和扩展到更广泛领域的可能性。

Abstract: Most deep learning methods for imputing missing values treat the task as
completing patterns within a fixed time window. This assumption often fails in
industrial systems, where dynamics are driven by control actions, are highly
non-stationary, and can experience long, uninterrupted gaps. We propose STDiff,
which reframes imputation as learning how the system evolves from one state to
the next. STDiff uses a conditional denoising diffusion model with a causal
bias aligned to control theory, generating missing values step-by-step based on
the most recent known state and relevant control or environmental inputs. On a
public wastewater treatment dataset with simulated missing blocks, STDiff
consistently achieves the lowest errors, with its advantage increasing for
longer gaps. On a raw industrial dataset with substantial real gaps, it
produces trajectories that remain dynamically plausible, in contrast to
window-based models that tend to flatten or over-smooth. These results support
dynamics-aware, explicitly conditioned imputation as a robust approach for
industrial time series, and we discuss computational trade-offs and extensions
to broader domains.

</details>


### [155] [Learning with springs and sticks](https://arxiv.org/abs/2508.19015)
*Luis Mantilla Calderón,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: 研究由弹簧和杆组成的动力系统用于函数逼近，应用于回归任务，研究其热力学性质并发现学习障碍，助于从物理角度理解学习系统。


<details>
  <summary>Details</summary>
Motivation: 从物理角度研究能任意逼近连续函数的简单动力系统，更好地理解学习系统。

Method: 用杆模拟函数分段线性逼近，用弹簧势能编码均方误差损失函数，通过耗散达到最小能量配置。

Result: 系统在回归任务中性能与多层感知器相当，发现系统自由能变化与学习数据分布能力的关系及热力学学习障碍。

Conclusion: 该简单模型有助于从物理角度更好地理解学习系统。

Abstract: Learning is a physical process. Here, we aim to study a simple dynamical
system composed of springs and sticks capable of arbitrarily approximating any
continuous function. The main idea of our work is to use the sticks to mimic a
piecewise-linear approximation of the given function, use the potential energy
of springs to encode a desired mean squared error loss function, and converge
to a minimum-energy configuration via dissipation. We apply the proposed
simulation system to regression tasks and show that its performance is
comparable to that of multi-layer perceptrons. In addition, we study the
thermodynamic properties of the system and find a relation between the free
energy change of the system and its ability to learn an underlying data
distribution. We empirically find a \emph{thermodynamic learning barrier} for
the system caused by the fluctuations of the environment, whereby the system
cannot learn if its change in free energy hits such a barrier. We believe this
simple model can help us better understand learning systems from a physical
point of view.

</details>


### [156] [Working My Way Back to You: Resource-Centric Next-Activity Prediction](https://arxiv.org/abs/2508.19016)
*Kelly Kurowski,Xixi Lu,Hajo A Reijers*

Main category: cs.LG

TL;DR: 研究从资源中心视角进行下一活动预测，评估模型和编码策略，发现不同模型适配的编码及组合编码精度高，凸显资源中心预测潜力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多从控制流视角，资源信息在下一步活动预测中的作用未被探索，而资源中心视角有额外优势。

Method: 评估四个预测模型和三种编码策略，在四个真实数据集上进行实验。

Result: LightGBM和Transformer模型在基于2 - gram活动转换的编码下表现最佳，随机森林在结合2 - gram转换和活动重复特征的编码下受益最大，组合编码平均准确率最高。

Conclusion: 资源中心的下一步活动预测有潜力，为预测过程监控研究开辟新途径。

Abstract: Predictive Process Monitoring (PPM) aims to train models that forecast
upcoming events in process executions. These predictions support early
bottleneck detection, improved scheduling, proactive interventions, and timely
communication with stakeholders. While existing research adopts a control-flow
perspective, we investigate next-activity prediction from a resource-centric
viewpoint, which offers additional benefits such as improved work organization,
workload balancing, and capacity forecasting. Although resource information has
been shown to enhance tasks such as process performance analysis, its role in
next-activity prediction remains unexplored. In this study, we evaluate four
prediction models and three encoding strategies across four real-life datasets.
Compared to the baseline, our results show that LightGBM and Transformer models
perform best with an encoding based on 2-gram activity transitions, while
Random Forest benefits most from an encoding that combines 2-gram transitions
and activity repetition features. This combined encoding also achieves the
highest average accuracy. This resource-centric approach could enable smarter
resource allocation, strategic workforce planning, and personalized employee
support by analyzing individual behavior rather than case-level progression.
The findings underscore the potential of resource-centric next-activity
prediction, opening up new venues for research on PPM.

</details>


### [157] [GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling](https://arxiv.org/abs/2508.19028)
*Arash Jamshidi,Lauri Seppäläinen,Katsiaryna Haitsiukevich,Hoang Phuc Hau Luu,Anton Björklund,Kai Puolamäki*

Main category: cs.LG

TL;DR: 本文提出了一种仅使用梯度信息的随机早停方法Gradstop，在测试数据上损失小，在数据有限场景有优势。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型用梯度下降算法学习易过拟合，使用验证集早停会减少训练数据。

Method: 通过梯度信息估计贝叶斯后验，将早停问题定义为从该后验抽样并获取早停准则。

Result: Gradstop在测试数据上损失小，相比基于验证集的早停准则表现更好。

Conclusion: Gradstop利用全量数据训练，在数据有限场景如迁移学习中优势明显，可作为梯度下降库可选功能且计算开销小。

Abstract: Machine learning models are often learned by minimising a loss function on
the training data using a gradient descent algorithm. These models often suffer
from overfitting, leading to a decline in predictive performance on unseen
data. A standard solution is early stopping using a hold-out validation set,
which halts the minimisation when the validation loss stops decreasing.
However, this hold-out set reduces the data available for training. This paper
presents {\sc gradstop}, a novel stochastic early stopping method that only
uses information in the gradients, which are produced by the gradient descent
algorithm ``for free.'' Our main contributions are that we estimate the
Bayesian posterior by the gradient information, define the early stopping
problem as drawing sample from this posterior, and use the approximated
posterior to obtain a stopping criterion. Our empirical evaluation shows that
{\sc gradstop} achieves a small loss on test data and compares favourably to a
validation-set-based stopping criterion. By leveraging the entire dataset for
training, our method is particularly advantageous in data-limited settings,
such as transfer learning. It can be incorporated as an optional feature in
gradient descent libraries with only a small computational overhead. The source
code is available at https://github.com/edahelsinki/gradstop.

</details>


### [158] [When recalling in-context, Transformers are not SSMs](https://arxiv.org/abs/2508.19029)
*Destiny Okpekpe,Antonio Orvieto*

Main category: cs.LG

TL;DR: 论文深入研究关联回忆基准测试，探讨现代循环模型和注意力模型在该任务上的表现，涉及学习率、缩放以及架构组件影响等方面。


<details>
  <summary>Details</summary>
Motivation: 现代循环深度学习模型虽有复杂度优势，但在推理和记忆任务上有不足，关联回忆与语言建模性能相关，以此为基准深入研究。

Method: 研究学习率对现代循环模型性能的影响，对比循环和基于注意力模型在宽度和深度缩放时的表现，研究单层transformer训练动态，通过架构消融研究组件对Transformer和Mamba性能及优化稳定性的影响。

Result: 学习率对现代循环模型性能至关重要；循环和注意力模型在缩放时有不同优势；单层transformer训练动态类似双层的感应头形成；通过架构消融研究组件影响。

Conclusion: 需要进一步研究来稳定现代循环模型的训练，不同模型在关联回忆任务及架构组件上有不同表现和特点。

Abstract: Despite the advantageous subquadratic complexity of modern recurrent deep
learning models -- such as state-space models (SSMs) -- recent studies have
highlighted their potential shortcomings compared to transformers on reasoning
and memorization tasks. In this paper, we dive deeper into one of such
benchmarks: associative recall (AR), which has been shown to correlate well
with language modeling performance, and inspect in detail the effects of
scaling and optimization issues in recently proposed token mixing strategies.
We first demonstrate that, unlike standard transformers, the choice of learning
rate plays a critical role in the performance of modern recurrent models: an
issue that can severely affect reported performance in previous works and
suggests further research is needed to stabilize training. Next, we show that
recurrent and attention-based models exhibit contrasting benefits when scaling
in width as opposed to depth, with attention being notably unable to solve AR
when limited to a single layer. We then further inspect 1-layer transformers,
revealing that despite their poor performance, their training dynamics
surprisingly resemble the formation of induction heads, a phenomenon previously
observed only in their 2-layer counterparts. Finally, through architectural
ablations, we study how components affects Transformer and Mamba's performance
and optimization stability.

</details>


### [159] [Breaking the Black Box: Inherently Interpretable Physics-Informed Machine Learning for Imbalanced Seismic Data](https://arxiv.org/abs/2508.19031)
*Vemula Sreenath,Filippo Gatti,Pierre Jehel*

Main category: cs.LG

TL;DR: 本文针对传统机器学习方法开发地震动模型的局限性，提出使用HazBinLoss函数的透明ML架构，模型表现良好且能推动ML方法在风险评估和灾害规划中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习开发地震动模型存在难以解释和数据不平衡问题，限制其在高风险决策中的应用，需解决这些问题。

Method: 开发使用HazBinLoss函数的透明ML架构，分别处理输入并线性相加得到输出，训练时对不同记录分配不同权重。

Result: 模型捕捉到已知地震学原理，与现有地震动模型性能相当且保持透明性。

Conclusion: 该框架能推动基于ML方法在风险评估研究和灾害规划中的广泛应用。

Abstract: Ground motion models (GMMs) predict how strongly the ground will shake during
an earthquake. They are essential for structural analysis, seismic design, and
seismic risk assessment studies. Traditional machine learning (ML) approaches
are popular to develop GMMs, due to large earthquake databases worldwide.
However, they operate as "black boxes," which are hard to interpret and trust,
limiting their use in high-stake decisions. Additionally, these databases
suffer from significant data imbalances: fewer large, critically damaging
records near the fault compared to abundant, less severely damaging distant
records. These two limitations are addressed in this work by developing a
transparent ML architecture using the HazBinLoss function. Each input (e.g.,
magnitude, distance, their interaction term, etc.) is processed separately and
added linearly to obtain the output, resulting in exact contribution of each
term. The HazBinLoss function assigns higher weights to critical near-field
large magnitude records and lower weights to less-critical far-field smaller
magnitude records, during training to prevent underprediction of the most
damaging scenarios. Our model captures known seismological principles and
achieves comparable performance with established GMMs while maintaining
transparency. This framework enables broader adoption of ML-based approaches
for risk assessment studies and disaster planning.

</details>


### [160] [Automated discovery of finite volume schemes using Graph Neural Networks](https://arxiv.org/abs/2508.19052)
*Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.LG

TL;DR: 本文探讨GNN结合符号回归生成数值格式，通过训练GNN在特定数据集上外推热方程有限体积格式，还扩展到无监督和高阶格式，表明GNN可助力新数值方法开发。


<details>
  <summary>Details</summary>
Motivation: GNN在训练域之外的外推能力不确定，研究其超越传统角色、结合符号回归生成数值格式的能力。

Method: 先在仅含两节点图的数据集上训练GNN，用数值和理论方法证明其可外推热方程一阶有限体积格式；用符号回归分析网络；扩展到无监督情境；考虑高阶格式，训练不同GNN发现修正项和经典二阶中点格式。

Result: GNN能外推一阶有限体积格式，误差与损失相关；符号回归显示网络可重新发现标准一阶有限体积格式的精确解析公式；无监督下GNN能恢复一阶格式；训练的GNN可发现二阶修正项和经典二阶中点格式。

Conclusion: GNN不仅是强大的近似器，还能积极推动新数值方法的发展。

Abstract: Graph Neural Networks (GNNs) have deeply modified the landscape of numerical
simulations by demonstrating strong capabilities in approximating solutions of
physical systems. However, their ability to extrapolate beyond their training
domain (\textit{e.g.} larger or structurally different graphs) remains
uncertain. In this work, we establish that GNNs can serve purposes beyond their
traditional role, and be exploited to generate numerical schemes, in
conjunction with symbolic regression. First, we show numerically and
theoretically that a GNN trained on a dataset consisting solely of two-node
graphs can extrapolate a first-order Finite Volume (FV) scheme for the heat
equation on out-of-distribution, unstructured meshes. Specifically, if a GNN
achieves a loss $\varepsilon$ on such a dataset, it implements the FV scheme
with an error of $\mathcal{O}(\varepsilon)$. Using symbolic regression, we show
that the network effectively rediscovers the exact analytical formulation of
the standard first-order FV scheme. We then extend this approach to an
unsupervised context: the GNN recovers the first-order FV scheme using only a
residual loss similar to Physics-Informed Neural Networks (PINNs) with no
access to ground-truth data. Finally, we push the methodology further by
considering higher-order schemes: we train (i) a 2-hop and (ii) a 2-layers GNN
using the same PINN loss, that autonomously discover (i) a second-order
correction term to the initial scheme using a 2-hop stencil, and (ii) the
classic second-order midpoint scheme. These findings follows a recent paradigm
in scientific computing: GNNs are not only strong approximators, but can be
active contributors to the development of novel numerical methods.

</details>


### [161] [Tackling Federated Unlearning as a Parameter Estimation Problem](https://arxiv.org/abs/2508.19065)
*Antonio Balordi,Lorenzo Manini,Fabio Stella,Alessio Merlo*

Main category: cs.LG

TL;DR: 提出基于信息论的联邦遗忘框架，利用二阶Hessian信息识别重置参数，评估显示有强隐私性和高性能，能应对后门攻击。


<details>
  <summary>Details</summary>
Motivation: 隐私法规要求从深度学习模型中擦除数据，联邦学习中因数据在客户端，全重训练或协调更新不可行。

Method: 基于信息论，将泄漏建模为参数估计问题，用二阶Hessian信息识别并选择性重置对要遗忘数据敏感的参数，再进行最小联邦重训练。

Result: 在基准数据集评估中，有强隐私性（MIA成功率接近随机，类别知识被擦除）和高性能（归一化准确率约0.9），能有效应对后门攻击。

Conclusion: 该框架为联邦学习中的数据遗忘提供了实用解决方案。

Abstract: Privacy regulations require the erasure of data from deep learning models.
This is a significant challenge that is amplified in Federated Learning, where
data remains on clients, making full retraining or coordinated updates often
infeasible. This work introduces an efficient Federated Unlearning framework
based on information theory, modeling leakage as a parameter estimation
problem. Our method uses second-order Hessian information to identify and
selectively reset only the parameters most sensitive to the data being
forgotten, followed by minimal federated retraining. This model-agnostic
approach supports categorical and client unlearning without requiring server
access to raw client data after initial information aggregation. Evaluations on
benchmark datasets demonstrate strong privacy (MIA success near random,
categorical knowledge erased) and high performance (Normalized Accuracy against
re-trained benchmarks of $\approx$ 0.9), while aiming for increased efficiency
over complete retraining. Furthermore, in a targeted backdoor attack scenario,
our framework effectively neutralizes the malicious trigger, restoring model
integrity. This offers a practical solution for data forgetting in FL.

</details>


### [162] [Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks](https://arxiv.org/abs/2508.19071)
*Hugo Attali,Thomas Papastergiou,Nathalie Pernelle,Fragkiskos D. Malliaros*

Main category: cs.LG

TL;DR: 提出TRIGON框架通过学习选择相关三角形构建丰富非平面三角剖分，改善图结构性质，在节点分类任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络因图拓扑固有问题（如过压缩和过平滑）导致的性能受限问题。

Method: 引入TRIGON框架，学习从多个图视图中选择相关三角形构建丰富非平面三角剖分，联合优化三角形选择和下游分类性能。

Result: 与现有重连方法相比，生成的重连图具有更好的结构性质，如减小直径、增加谱间隙和降低有效电阻；在一系列同质性和异质性基准的节点分类任务上优于现有方法。

Conclusion: TRIGON框架在解决图神经网络拓扑问题和提升节点分类性能上有显著效果。

Abstract: Graph Neural Networks (GNNs) have emerged as the leading paradigm for
learning over graph-structured data. However, their performance is limited by
issues inherent to graph topology, most notably oversquashing and
oversmoothing. Recent advances in graph rewiring aim to mitigate these
limitations by modifying the graph topology to promote more effective
information propagation. In this work, we introduce TRIGON, a novel framework
that constructs enriched, non-planar triangulations by learning to select
relevant triangles from multiple graph views. By jointly optimizing triangle
selection and downstream classification performance, our method produces a
rewired graph with markedly improved structural properties such as reduced
diameter, increased spectral gap, and lower effective resistance compared to
existing rewiring methods. Empirical results demonstrate that TRIGON
outperforms state-of-the-art approaches on node classification tasks across a
range of homophilic and heterophilic benchmarks.

</details>


### [163] [APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration](https://arxiv.org/abs/2508.19087)
*Shaobo Ma,Chao Fang,Haikuo Shao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: 本文提出针对任意精度大语言模型的加速方案APT - LLM，提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算需求大限制部署和实时性能，超低比特量化在GPU上实现任意精度效率提升存在挑战。

Method: 引入bipolar - INT数据格式；开发位级矩阵乘法方法；提出专注数据恢复的内存管理系统；开发动态选择内核超参数的映射方法。

Result: 在不同GPU上，相比FP16和NVIDIA CUTLASS INT4等基线，APT - LLM实现显著推理加速。

Conclusion: 提出的APT - LLM方案能有效解决任意精度大语言模型在GPU上的加速问题，提升推理性能。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
enormous computational demands severely limit deployment and real-time
performance. Quantization methods can help reduce computational costs, however,
attaining the extreme efficiency associated with ultra-low-bit quantized LLMs
at arbitrary precision presents challenges on GPUs. This is primarily due to
the limited support for GPU Tensor Cores, inefficient memory management, and
inflexible kernel optimizations. To tackle these challenges, we propose a
comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.
Firstly, we introduce a novel data format, bipolar-INT, which allows for
efficient and lossless conversion with signed INT, while also being more
conducive to parallel computation. We also develop a matrix multiplication
(MatMul) method allowing for arbitrary precision by dismantling and
reassembling matrices at the bit level. This method provides flexible precision
and optimizes the utilization of GPU Tensor Cores. In addition, we propose a
memory management system focused on data recovery, which strategically employs
fast shared memory to substantially increase kernel execution speed and reduce
memory access latency. Finally, we develop a kernel mapping method that
dynamically selects the optimal configurable hyperparameters of kernels for
varying matrix sizes, enabling optimal performance across different LLM
architectures and precision settings. In LLM inference, APT-LLM achieves up to
a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup
over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,
APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup
over CUTLASS integer baselines.

</details>


### [164] [Active Query Selection for Crowd-Based Reinforcement Learning](https://arxiv.org/abs/2508.19132)
*Jonathan Erskine,Taku Yamagata,Raúl Santos-Rodríguez*

Main category: cs.LG

TL;DR: 提出结合概率人群建模和主动学习的框架解决基于偏好的强化学习中人类输入成本高、可用性低问题，在多环境评估，初步结果良好。


<details>
  <summary>Details</summary>
Motivation: 基于偏好的强化学习因可靠人类输入成本高、可用性低，在专家反馈稀缺或错误代价高的领域效果受限。

Method: 提出结合概率人群建模处理嘈杂、多标注者反馈和主动学习优先获取最有信息的反馈的框架，扩展Advise算法支持多训练者、在线估计其可靠性并引入基于熵的查询选择。

Result: 在合成和现实启发的环境中评估，对不确定轨迹反馈训练的智能体在多数任务中学习更快，在血糖控制任务中优于基线。

Conclusion: 所提框架在解决基于偏好的强化学习的人类输入问题上有一定效果。

Abstract: Preference-based reinforcement learning has gained prominence as a strategy
for training agents in environments where the reward signal is difficult to
specify or misaligned with human intent. However, its effectiveness is often
limited by the high cost and low availability of reliable human input,
especially in domains where expert feedback is scarce or errors are costly. To
address this, we propose a novel framework that combines two complementary
strategies: probabilistic crowd modelling to handle noisy, multi-annotator
feedback, and active learning to prioritize feedback on the most informative
agent actions. We extend the Advise algorithm to support multiple trainers,
estimate their reliability online, and incorporate entropy-based query
selection to guide feedback requests. We evaluate our approach in a set of
environments that span both synthetic and real-world-inspired settings,
including 2D games (Taxi, Pacman, Frozen Lake) and a blood glucose control task
for Type 1 Diabetes using the clinically approved UVA/Padova simulator. Our
preliminary results demonstrate that agents trained with feedback on uncertain
trajectories exhibit faster learning in most tasks, and we outperform the
baselines for the blood glucose control task.

</details>


### [165] [Saddle Hierarchy in Dense Associative Memory](https://arxiv.org/abs/2508.19151)
*Robin Thériault,Daniele Tantari*

Main category: cs.LG

TL;DR: 研究基于三层玻尔兹曼机的DAM，推导鞍点方程，提出正则化方案，证明其能学习可解释解，发现小DAM权重对应大DAM不稳定鞍点，实现网络增长算法降低训练成本。


<details>
  <summary>Details</summary>
Motivation: DAM模型对对抗样本鲁棒且与先进机器学习范式相关，有研究价值。

Method: 对基于三层玻尔兹曼机的DAM进行统计力学分析，推导鞍点方程，提出正则化方案，实施网络增长算法。

Result: 提出的正则化方案使训练更稳定，DAM能学习可解释解，发现小DAM权重与大DAM鞍点关系，网络增长算法降低训练成本。

Conclusion: 所研究的DAM在训练稳定性、学习可解释解和降低计算成本方面有良好表现。

Abstract: Dense associative memory (DAM) models have been attracting renewed attention
since they were shown to be robust to adversarial examples and closely related
to state-of-the-art machine learning paradigms, such as the attention
mechanisms in transformers and generative diffusion models. We study a DAM
built upon a three-layer Boltzmann machine with Potts hidden units, which
represent data clusters and classes. Through a statistical mechanics analysis,
we derive saddle-point equations that characterize both the stationary points
of DAMs trained on real data and the fixed points of DAMs trained on synthetic
data within a teacher-student framework. Based on these results, we propose a
novel regularization scheme that makes training significantly more stable.
Moreover, we show empirically that our DAM learns interpretable solutions to
both supervised and unsupervised classification problems. Pushing our
theoretical analysis further, we find that the weights learned by relatively
small DAMs correspond to unstable saddle points in larger DAMs. We implement a
network-growing algorithm that leverages this saddle-point hierarchy to
drastically reduce the computational cost of training dense associative memory.

</details>


### [166] [Get Global Guarantees: On the Probabilistic Nature of Perturbation Robustness](https://arxiv.org/abs/2508.19183)
*Wenchuan Mu,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 本文针对安全关键深度学习应用中现有鲁棒性评估方法的局限，提出基于假设检验的tower robustness指标进行预部署评估，展现了其优势与适用性。


<details>
  <summary>Details</summary>
Motivation: 现有预部署鲁棒性评估方法在计算成本和测量精度间存在权衡，限制了实用性。

Method: 对现有鲁棒性定义和评估方法进行综合比较分析，提出基于假设检验的tower robustness指标。

Result: 广泛的比较评估显示了所提方法的优势和适用性。

Conclusion: 所提方法有助于系统理解和提升安全关键深度学习应用中模型的鲁棒性。

Abstract: In safety-critical deep learning applications, robustness measures the
ability of neural models that handle imperceptible perturbations in input data,
which may lead to potential safety hazards. Existing pre-deployment robustness
assessment methods typically suffer from significant trade-offs between
computational cost and measurement precision, limiting their practical utility.
To address these limitations, this paper conducts a comprehensive comparative
analysis of existing robustness definitions and associated assessment
methodologies. We propose tower robustness to evaluate robustness, which is a
novel, practical metric based on hypothesis testing to quantitatively evaluate
probabilistic robustness, enabling more rigorous and efficient pre-deployment
assessments. Our extensive comparative evaluation illustrates the advantages
and applicability of our proposed approach, thereby advancing the systematic
understanding and enhancement of model robustness in safety-critical deep
learning applications.

</details>


### [167] [Emotions as Ambiguity-aware Ordinal Representations](https://arxiv.org/abs/2508.19193)
*Jingyao Wu,Matthew Barthet,David Melhart,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: 本文引入模糊感知的序数情感表示框架，在两个情感语料库上评估，结果显示该框架在无界标签上优于传统模型，在有界轨迹上捕捉相对变化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有连续情感识别方法要么忽略情感模糊性，要么将其视为独立静态变量，存在研究空白。

Method: 引入模糊感知的序数情感表示框架，通过变化率建模情感模糊性，并在两个情感语料库上评估。

Result: 序数表示在无界标签上优于传统模糊感知模型，获得最高CCC和SDA分数；在有界轨迹上SDA表现出色。

Conclusion: 该框架在建模情感轨迹动态和捕捉相对变化方面有效。

Abstract: Emotions are inherently ambiguous and dynamic phenomena, yet existing
continuous emotion recognition approaches either ignore their ambiguity or
treat ambiguity as an independent and static variable over time. Motivated by
this gap in the literature, in this paper we introduce \emph{ambiguity-aware
ordinal} emotion representations, a novel framework that captures both the
ambiguity present in emotion annotation and the inherent temporal dynamics of
emotional traces. Specifically, we propose approaches that model emotion
ambiguity through its rate of change. We evaluate our framework on two
affective corpora -- RECOLA and GameVibe -- testing our proposed approaches on
both bounded (arousal, valence) and unbounded (engagement) continuous traces.
Our results demonstrate that ordinal representations outperform conventional
ambiguity-aware models on unbounded labels, achieving the highest Concordance
Correlation Coefficient (CCC) and Signed Differential Agreement (SDA) scores,
highlighting their effectiveness in modeling the traces' dynamics. For bounded
traces, ordinal representations excel in SDA, revealing their superior ability
to capture relative changes of annotated emotion traces.

</details>


### [168] [Predicting the Order of Upcoming Tokens Improves Language Modeling](https://arxiv.org/abs/2508.19228)
*Zayd M. K. Zuhri,Erland Hilman Fuadi,Alham Fikri Aji*

Main category: cs.LG

TL;DR: 提出Token Order Prediction (TOP)辅助目标改进语言模型训练，在八个NLP基准测试中表现优于NTP和MTP。


<details>
  <summary>Details</summary>
Motivation: Multi - Token Prediction (MTP)作为辅助目标改进语言模型训练效果不稳定，在标准NLP基准测试中表现不佳。

Method: 提出Token Order Prediction (TOP)，使用学习排序损失训练模型对即将到来的标记按接近程度排序，仅需一个额外的未嵌入层。用NTP、MTP和TOP目标预训练340M、1.8B和7B参数的模型。

Result: 在八个标准NLP基准测试中，TOP总体上优于NTP和MTP。

Conclusion: TOP在语言模型训练中是比MTP更好的辅助目标，能有效提升模型性能。

Abstract: Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to
improve next-token prediction (NTP) in language model training but shows
inconsistent improvements, underperforming in standard NLP benchmarks. We argue
that MTP's exact future token prediction is too difficult as an auxiliary loss.
Instead, we propose Token Order Prediction (TOP), which trains models to order
upcoming tokens by their proximity using a learning-to-rank loss. TOP requires
only a single additional unembedding layer compared to MTP's multiple
transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using
NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show
that TOP overall outperforms both NTP and MTP even at scale. Our code is
available at https://github.com/zaydzuhri/token-order-prediction

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [169] [Leveraging Evolutionary Surrogate-Assisted Prescription in Multi-Objective Chlorination Control Systems](https://arxiv.org/abs/2508.19173)
*Rivaaj Monsia,Olivier Francon,Daniel Young,Risto Miikkulainen*

Main category: cs.NE

TL;DR: 介绍进化代理辅助处方（ESP）概念并给出其在IJCAI - 2025首届饮用水氯化挑战中训练现实世界智能体的初步结果


<details>
  <summary>Details</summary>
Motivation: Project Resilience团队旨在将AI应用于现实世界问题，通过此研究探索ESP在实际场景的应用

Method: 未提及

Result: 给出ESP在训练现实世界智能体的初步结果

Conclusion: 未提及

Abstract: This short, written report introduces the idea of Evolutionary
Surrogate-Assisted Prescription (ESP) and presents preliminary results on its
potential use in training real-world agents as a part of the 1st AI for
Drinking Water Chlorination Challenge at IJCAI-2025. This work was done by a
team from Project Resilience, an organization interested in bridging AI to
real-world problems.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [170] [Exact Persistent Stochastic Non-Interference](https://arxiv.org/abs/2508.19110)
*Carla Piazza,Riccardo Romanello,Sabina Rossi*

Main category: cs.PF

TL;DR: 本文从性能导向视角重新审视PSNI，提出基于弱精确等价的Exact PSNI（EPSNI），并证明其具有与PSNI类似性质。


<details>
  <summary>Details</summary>
Motivation: 从性能导向视角重新审视Persistent Stochastic Non - Interference (PSNI) ，以更好地处理随机系统中的非干扰问题。

Method: 引入弱精确等价（weak - exact equivalence），并基于此定义Exact PSNI (EPSNI)。

Result: EPSNI拥有与PSNI相同的基于双模拟和展开式的特征描述，且具有类似的组合性属性。

Conclusion: 弱精确等价为随机系统的非干扰推理提供了可靠基础。

Abstract: Persistent Stochastic Non-Interference (PSNI) was introduced to capture a
quantitative security property in stochastic process algebras, ensuring that a
high-level process does not influence the observable behaviour of a low-level
component, as formalised via lumpable bisimulation. In this work, we revisit
PSNI from a performance-oriented perspective and propose a new characterisation
based on a refined behavioural relation. We introduce \emph{weak-exact
equivalence}, which extends exact equivalence with a relaxed treatment of
internal (\(\tau\)) actions, enabling precise control over quantitative
observables while accommodating unobservable transitions. Based on this, we
define \emph{Exact PSNI} (EPSNI), a variant of PSNI characterised via
weak-exact equivalence. We show that EPSNI admits the same bisimulation-based
and unwinding-style characterisations as PSNI, and enjoys analogous
compositionality properties. These results confirm weak-exact equivalence as a
robust foundation for reasoning about non-interference in stochastic systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [171] [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.SE

TL;DR: 本文介绍CTF - Dojo与CTF - Forge，训练LLM代理在多个基准测试中取得显著提升，证明执行接地训练信号对开发高性能ML代理至关重要。


<details>
  <summary>Details</summary>
Motivation: 可扩展和通用的执行接地环境稀缺，限制了更强大ML代理的训练。

Method: 引入CTF - Dojo，包含658个可重现的CTF挑战；开发CTF - Forge自动化管道，将公共工件快速转化为执行环境；在CTF - Dojo上训练基于LLM的代理。

Result: 在三个基准测试中比强基线有高达11.6%的绝对提升，最佳32B模型Pass@1达31.9%，创开放权重的新SOTA。

Conclusion: 执行接地训练信号对推进高性能ML代理有效且关键，无需依赖昂贵的专有系统。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when
trained within executable runtime environments, notably excelling at software
engineering tasks through verified feedback loops. Yet, scalable and
generalizable execution-grounded environments remain scarce, limiting progress
in training more capable ML agents. We introduce CTF-Dojo, the first
large-scale executable runtime tailored for training LLMs with verifiable
feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style
challenges containerized in Docker with guaranteed reproducibility. To enable
rapid scaling without manual intervention, we develop CTF-Forge, an automated
pipeline that transforms publicly available artifacts into ready-to-use
execution environments in minutes, eliminating weeks of expert configuration
traditionally required. We trained LLM-based agents on just 486 high-quality,
execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute
gains over strong baselines across three competitive benchmarks: InterCode-CTF,
NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,
establishing a new open-weight state-of-the-art that rivals frontier models
like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a
benchmark for executable-agent learning, CTF-Dojo demonstrates that
execution-grounded training signals are not only effective but pivotal in
advancing high-performance ML agents without dependence on costly proprietary
systems.

</details>


### [172] [DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting](https://arxiv.org/abs/2508.18431)
*Kérian Fiter,Louis Malassigné-Onfroy,Bentley Oakes*

Main category: cs.SE

TL;DR: 介绍了用于数字孪生（DT）持续报告的系统工具DTInsight，可增强利益相关者理解。


<details>
  <summary>Details</summary>
Motivation: 数字孪生构建和演化过程中，利益相关者需要工具随时了解系统当前特征和概念架构。

Method: 引入DTInsight工具，具有交互式概念架构可视化、基于本体数据生成特征摘要、集成输出到CI/CD管道报告页面三个关键特性。

Result: 基于符合DT描述框架（DTDF）的DT建模描述，DTInsight能提供最新详细报告。

Conclusion: DTInsight可增强利益相关者对数字孪生的理解。

Abstract: With Digital Twin (DT) construction and evolution occurring over time,
stakeholders require tools to understand the current characteristics and
conceptual architecture of the system at any time. We introduce DTInsight, a
systematic and automated tool and methodology for producing continuous
reporting for DTs. DTInsight offers three key features: (a) an interactive
conceptual architecture visualization of DTs; (b) generation of summaries of DT
characteristics based on ontological data; and (c) integration of these outputs
into a reporting page within a continuous integration and continuous deployment
(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT
Description Framework (DTDF), DTInsight enables up-to-date and detailed reports
for enhanced stakeholder understanding.

</details>


### [173] [Engineering a Digital Twin for the Monitoring and Control of Beer Fermentation Sampling](https://arxiv.org/abs/2508.18452)
*Pierre-Emmanuel Goffi,Raphaël Tremblay,Bentley Oakes*

Main category: cs.SE

TL;DR: 本文介绍啤酒发酵监测安全关键数字孪生（DT）工程经验，提供连续采样，减少91%手动采样时间，给出系统方法与实用方案。


<details>
  <summary>Details</summary>
Motivation: 成功构建交互式工业DT是复杂任务，尤其实现被动监测外的服务，需要探索工程方法。

Method: 采用三阶段工程方法将被动监测系统转变为具有实时控制能力的交互式2型DT，提出多层安全协议、软硬件集成策略和实时同步解决方案。

Result: 构建的DT可提供连续采样，减少91%手动采样时间，能用于七巴压力系统的实时控制。

Conclusion: 安全优先设计、仿真驱动开发和渐进式实施策略至关重要，为开发安全关键双向控制DT的从业者提供了可操作指导。

Abstract: Successfully engineering interactive industrial DTs is a complex task,
especially when implementing services beyond passive monitoring. We present
here an experience report on engineering a safety-critical digital twin (DT)
for beer fermentation monitoring, which provides continual sampling and reduces
manual sampling time by 91%. We document our systematic methodology and
practical solutions for implementing bidirectional DTs in industrial
environments. This includes our three-phase engineering approach that
transforms a passive monitoring system into an interactive Type 2 DT with
real-time control capabilities for pressurized systems operating at seven bar.
We contribute details of multi-layered safety protocols, hardware-software
integration strategies across Arduino controllers and Unity visualization, and
real-time synchronization solutions. We document specific engineering
challenges and solutions spanning interdisciplinary integration, demonstrating
how our use of the constellation reporting framework facilitates cross-domain
collaboration. Key findings include the critical importance of safety-first
design, simulation-driven development, and progressive implementation
strategies. Our work thus provides actionable guidance for practitioners
developing DTs requiring bidirectional control in safety-critical applications.

</details>


### [174] [How do Humans and LLMs Process Confusing Code?](https://arxiv.org/abs/2508.18547)
*Youssef Abdelsalam,Norman Peitek,Anna-Maria Maurer,Mariya Toneva,Sven Apel*

Main category: cs.SE

TL;DR: 研究对比大语言模型（LLM）和人类程序员对代码的理解，发现二者对代码的困惑相似，并据此提出识别代码困惑区域的方法。


<details>
  <summary>Details</summary>
Motivation: LLM与程序员对代码理解的不一致会导致多种问题，研究二者是否被相同类型代码困扰，以指导LLM在软件工程工作流中的集成和改进。

Method: 通过LLM困惑度衡量LLM对代码的理解，用神经生理反应（基于脑电图的注视相关电位）衡量人类程序员的理解，进行实证研究。

Result: LLM困惑度峰值在位置和幅度上与表明人类困惑的神经生理反应相关，说明LLM和人类对代码的困惑相似。

Conclusion: 基于研究结果，设计了一种数据驱动、基于LLM的方法来识别会引起人类程序员困惑的代码区域。

Abstract: Already today, humans and programming assistants based on large language
models (LLMs) collaborate in everyday programming tasks. Clearly, a
misalignment between how LLMs and programmers comprehend code can lead to
misunderstandings, inefficiencies, low code quality, and bugs.
  A key question in this space is whether humans and LLMs are confused by the
same kind of code. This would not only guide our choices of integrating LLMs in
software engineering workflows, but also inform about possible improvements of
LLMs.
  To this end, we conducted an empirical study comparing an LLM to human
programmers comprehending clean and confusing code. We operationalized
comprehension for the LLM by using LLM perplexity, and for human programmers
using neurophysiological responses (in particular, EEG-based fixation-related
potentials).
  We found that LLM perplexity spikes correlate both in terms of location and
amplitude with human neurophysiological responses that indicate confusion. This
result suggests that LLMs and humans are similarly confused about the code.
Based on these findings, we devised a data-driven, LLM-based approach to
identify regions of confusion in code that elicit confusion in human
programmers.

</details>


### [175] [LaQual: A Novel Framework for Automated Evaluation of LLM App Quality](https://arxiv.org/abs/2508.18636)
*Yan Wang,Xinyi Hou,Yanjie Zhao,Weiguo Lin,Haoyu Wang,Junjun Si*

Main category: cs.SE

TL;DR: 现有LLM应用商店排名推荐方法难让用户高效找到优质应用，提出LaQual框架评估应用质量，实验证明其有效且优于基线系统。


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用商店排名和推荐应用的方法多依赖静态指标，用户难高效找到高质量应用。

Method: LaQual框架分三步，先对LLM应用分层标签分类，再用静态指标过滤低质量应用，最后进行动态、场景自适应评估。

Result: 实验显示LaQual自动评分与人类判断高度一致，能减少候选应用池，用户研究表明其在决策信心、比较效率和评估报告感知价值上优于基线系统。

Conclusion: LaQual为现实场景中寻找和推荐高质量LLM应用提供了可扩展、客观且以用户为中心的解决方案。

Abstract: LLM app stores are quickly emerging as platforms that gather a wide range of
intelligent applications based on LLMs, giving users many choices for content
creation, coding support, education, and more. However, the current methods for
ranking and recommending apps in these stores mostly rely on static metrics
like user activity and favorites, which makes it hard for users to efficiently
find high-quality apps. To address these challenges, we propose LaQual, an
automated framework for evaluating the quality of LLM apps. LaQual consists of
three main stages: first, it labels and classifies LLM apps in a hierarchical
way to accurately match them to different scenarios; second, it uses static
indicators, such as time-weighted user engagement and functional capability
metrics, to filter out low-quality apps; and third, it conducts a dynamic,
scenario-adaptive evaluation, where the LLM itself generates scenario-specific
evaluation metrics, scoring rules, and tasks for a thorough quality assessment.
Experiments on a popular LLM app store show that LaQual is effective. Its
automated scores are highly consistent with human judgments (with Spearman's
rho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in
travel planning). By effectively screening, LaQual can reduce the pool of
candidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual
significantly outperforms baseline systems in decision confidence, comparison
efficiency (with average scores of 5.45 compared to 3.30), and the perceived
value of its evaluation reports (4.75 versus 2.25). Overall, these results
demonstrate that LaQual offers a scalable, objective, and user-centered
solution for finding and recommending high-quality LLM apps in real-world use
cases.

</details>


### [176] [Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision](https://arxiv.org/abs/2508.18675)
*Xu Lu,Weisong Sun,Yiran Zhang,Ming Hu,Cong Tian,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: 提出基于需求开发和形式化的多智能体框架ReDeFo用于可靠代码生成，以实现可靠自动生成软件愿景。


<details>
  <summary>Details</summary>
Motivation: 现有仅依赖大语言模型的代码生成方法在代码质量上不足，缺乏系统的需求开发和建模策略，需提升代码生成的可靠性。

Method: 构建名为ReDeFo的多智能体框架，将三个增强了形式化方法知识和技术的智能体融入需求到代码生成流程，用形式化规范弥合自然语言需求和可执行代码的差距。

Result: 框架能够对正确性进行严格推理，发现隐藏错误，并在开发过程中强制实施关键属性。

Conclusion: 框架朝着实现可靠自动生成软件的愿景迈出了有前景的一步。

Abstract: Automated code generation has long been considered the holy grail of software
engineering. The emergence of Large Language Models (LLMs) has catalyzed a
revolutionary breakthrough in this area. However, existing methods that only
rely on LLMs remain inadequate in the quality of generated code, offering no
guarantees of satisfying practical requirements. They lack a systematic
strategy for requirements development and modeling. Recently, LLM-based agents
typically possess powerful abilities and play an essential role in facilitating
the alignment of LLM outputs with user requirements. In this paper, we envision
the first multi-agent framework for reliable code generation based on
\textsc{re}quirements \textsc{de}velopment and \textsc{fo}rmalization, named
\textsc{ReDeFo}. This framework incorporates three agents, highlighting their
augmentation with knowledge and techniques of formal methods, into the
requirements-to-code generation pipeline to strengthen quality assurance. The
core of \textsc{ReDeFo} is the use of formal specifications to bridge the gap
between potentially ambiguous natural language requirements and precise
executable code. \textsc{ReDeFo} enables rigorous reasoning about correctness,
uncovering hidden bugs, and enforcing critical properties throughout the
development process. In general, our framework aims to take a promising step
toward realizing the long-standing vision of reliable, auto-generated software.

</details>


### [177] [LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging](https://arxiv.org/abs/2508.18721)
*Yunrui Pei,Hongshu Wang,Wenjie Zhang,Yun Lin,Weiyu Kong,Jin song Dong*

Main category: cs.SE

TL;DR: 传统方法在查找动态数据依赖时成本高，本文提出RecovSlicing方法，在单轮运行中用部分插桩计算动态数据依赖，评估显示其准确性和召回率表现出色，还能助力回归错误定位。


<details>
  <summary>Details</summary>
Motivation: 传统方法在查找变量动态定义时成本高，对于库中定义的变量插桩昂贵，非确定性程序无法复制运行。

Method: 提出RecovSlicing方法，在单轮运行中使用部分插桩，借助大语言模型从部分记录的跟踪和代码上下文推断程序行为，恢复缺失执行来估计运行时定义，处理运行时值和结构恢复及变量对齐。

Result: 在三个切片基准上对8300个数据依赖进行评估，RecovSlicing的准确率和召回率均超过最佳基线，集成到回归错误定位器中能多找到16%的回归。

Conclusion: RecovSlicing在计算动态数据依赖方面有效，能提升回归错误定位能力。

Abstract: Dynamic data dependency, answering "why a variable has this value?", is
critical for debugging. Given a program step `s` reading a variable `v`,
finding the dynamic definition of `v` is challenging. Traditional methods
require either (1) exhaustive instrumentation of all possible definitions of
`v` in one run or (2) replicating the run to re-examine reads/writes - both
costly. If `v` is defined in a library, instrumentation becomes expensive; for
non-deterministic programs, replication is infeasible.
  We propose RecovSlicing, which computes dynamic data dependency in a single
run with partial instrumentation. We leverage LLMs to infer program behavior
from a partially recorded trace and code context. Given a trace and a slicing
criterion (step `s` and variable `v`), RecovSlicing estimates the runtime
definition of `v` by recovering the missing execution.It also supports implicit
variables, such as those in `list.get(i)`. Technically, RecovSlicing tackles:
(1) recovering runtime values and structures, and (2) aligning recovered
variables with recorded memory to analyze definitions.
  We evaluate RecovSlicing on 8,300 data dependencies across three slicing
benchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution
Slicer. RecovSlicing achieves accuracy of 80.3%, 91.1%, and 98.3%,
outperforming the best baseline (39.0%, 82.0%, 59.9%), and also leads in recall
(91.1%, 91.1%, 98.3% vs. 53.4%, 79.1%, 87.1%). Integrated into a regression bug
localizer, it enables finding 16% more regressions.

</details>


### [178] [Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions](https://arxiv.org/abs/2508.18771)
*Kexin Sun,Hongyu Kuang,Sebastian Baltes,Xin Zhou,He Zhang,Xiaoxing Ma,Guoping Rong,Dong Shao,Christoph Treude*

Main category: cs.SE

TL;DR: 本文对16种流行的基于AI的代码审查工具进行大规模实证研究，分析超22000条审查评论，发现工具有效性差异大，为改进相关系统提供方向。


<details>
  <summary>Details</summary>
Motivation: 尽管基于AI的代码审查工具日益普遍，但对其实际影响了解甚少，因此开展研究。

Method: 对GitHub工作流中16种流行AI代码审查工具进行大规模实证研究，开发两阶段大语言模型辅助框架判断评论是否被处理，用可解释机器学习识别影响因素。

Result: 工具采用率在增长，但有效性差异大，简洁、含代码片段、手动触发的评论，尤其是块级审查工具的评论更易导致代码更改。

Conclusion: 强调精心设计工具的重要性，并为改进基于AI的代码审查系统提供方向。

Abstract: AI-based code review tools automatically review and comment on pull requests
to improve code quality. Despite their growing presence, little is known about
their actual impact. We present a large-scale empirical study of 16 popular
AI-based code review actions for GitHub workflows, analyzing more than 22,000
review comments in 178 repositories. We investigate (1) how these tools are
adopted and configured, (2) whether their comments lead to code changes, and
(3) which factors influence their effectiveness. We develop a two-stage
LLM-assisted framework to determine whether review comments are addressed, and
use interpretable machine learning to identify influencing factors. Our
findings show that, while adoption is growing, effectiveness varies widely.
Comments that are concise, contain code snippets, and are manually triggered,
particularly those from hunk-level review tools, are more likely to result in
code changes. These results highlight the importance of careful tool design and
suggest directions for improving AI-based code review systems.

</details>


### [179] [Dealing with SonarQube Cloud: Initial Results from a Mining Software Repository Study](https://arxiv.org/abs/2508.18816)
*Sabato Nocera,Davide Fucci,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: 本文研究GitHub项目对SonarQube Cloud这一SCA工具的使用和定制情况，发现多数项目依赖预定义配置，但也有部分项目定制配置以满足特定质量目标。


<details>
  <summary>Details</summary>
Motivation: 探究开源项目如何使用和定制流行的静态代码分析工具SonarQube Cloud。

Method: 对通过GitHub Actions与SonarQube Cloud项目关联的GitHub项目进行挖掘研究。

Result: 321个使用SonarQube Cloud的GitHub项目中，81%正确连接；265个可访问项目里，75%使用组织默认质量门，55%用内置质量门，45%定制质量门。常见质量条件符合“Clean as You Code”原则。

Conclusion: 很多项目依赖预定义配置，部分项目定制配置。未来可研究质量门配置与软件实际结果的关联，为不同场景下配置SCA工具提供循证建议。

Abstract: Background: Static Code Analysis (SCA) tools are widely adopted to enforce
code quality standards. However, little is known about how open-source projects
use and customize these tools. Aims: This paper investigates how GitHub
projects use and customize a popular SCA tool, namely SonarQube Cloud. Method:
We conducted a mining study of GitHub projects that are linked through GitHub
Actions to SonarQube Cloud projects. Results: Among 321 GitHub projects using
SonarQube Cloud, 81% of them are correctly connected to SonarQube Cloud
projects, while others exhibit misconfigurations or restricted access. Among
265 accessible SonarQube Cloud projects, 75% use the organization's default
quality gate, i.e., a set of conditions that deployed source code must meet to
pass automated checks. While 55% of the projects use the built-in quality gate
provided by SonarQube Cloud, 45% of them customize their quality gate with
different conditions. Overall, the most common quality conditions align with
SonarQube Cloud's "Clean as You Code" principle and enforce security,
maintainability, reliability, coverage, and a few duplicates on newly added or
modified source code. Conclusions: Many projects rely on predefined
configurations, yet a significant portion customize their configurations to
meet specific quality goals. Building on our initial results, we envision a
future research agenda linking quality gate configurations to actual software
outcomes (e.g., improvement of software security). This would enable
evidence-based recommendations for configuring SCA tools like SonarQube Cloud
in various contexts.

</details>


### [180] [Interleaving Large Language Models for Compiler Testing](https://arxiv.org/abs/2508.18955)
*Yunbo Ni,Shaohua Li*

Main category: cs.SE

TL;DR: 本文提出新颖编译器测试框架LegoFuzz，将测试过程分为离线和在线阶段，在GCC和LLVM中发现66个漏洞，为AI模型用于软件测试开辟新可能。


<details>
  <summary>Details</summary>
Motivation: 当前使用AI模型测试编译器的方法存在生成程序简单、计算成本高的问题。

Method: 提出将测试过程分为离线和在线阶段的框架，离线用LLMs生成代码片段，在线组合代码片段构建测试程序，并实现工具LegoFuzz。

Result: 在GCC和LLVM中发现66个漏洞，近一半是现有基于LLM工具未发现的难寻漏洞。

Conclusion: 该高效设计为AI模型在软件测试中的应用，不限于C编译器，开辟了新的可能性。

Abstract: Testing compilers with AI models, especially large language models (LLMs),
has shown great promise. However, current approaches struggle with two key
problems: The generated programs for testing compilers are often too simple,
and extensive testing with the LLMs is computationally expensive. In this
paper, we propose a novel compiler testing framework that decouples the testing
process into two distinct phases: an offline phase and an online phase. In the
offline phase, we use LLMs to generate a collection of small but feature-rich
code pieces. In the online phase, we reuse these code pieces by strategically
combining them to build high-quality and valid test programs, which are then
used to test compilers.
  We implement this idea in a tool, LegoFuzz, for testing C compilers. The
results are striking: we found 66 bugs in GCC and LLVM, the most widely used C
compilers. Almost half of the bugs are miscompilation bugs, which are serious
and hard-to-find bugs that none of the existing LLM-based tools could find. We
believe this efficient design opens up new possibilities for using AI models in
software testing beyond just C compilers.

</details>


### [181] [GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging](https://arxiv.org/abs/2508.18993)
*Ziyi Ni,Huacan Wang,Shuo Zhang,Shuo Lu,Ziyang He,Wang You,Zhenheng Tang,Yuntao Du,Bill Sun,Hongzhang Liu,Sen Hu,Ronghao Chen,Bo Li,Xin Li,Chen Hu,Binxing Jiao,Daxin Jiang,Pin Lyu*

Main category: cs.SE

TL;DR: 提出GitTaskBench基准测试评估代码代理利用代码仓库解决实际任务的能力，实验显示利用代码仓库解决复杂任务仍具挑战，发布该基准以推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试很少在真实、工作流驱动的场景中评估代码代理，需填补这一空白。

Method: 引入GitTaskBench，包含54个跨7种模式和7个领域的现实任务，每个任务搭配相关仓库和评估工具，提出alpha - value指标量化性能经济收益。

Result: 实验表明利用代码仓库解决复杂任务困难，最佳系统OpenHands+Claude 3.7仅解决48.15%的任务，超半数失败源于环境设置和依赖解析等步骤。

Conclusion: 发布GitTaskBench以推动对仓库感知的代码推理、执行和部署的研究，使代码代理更接近解决复杂的端到端现实任务。

Abstract: Beyond scratch coding, exploiting large-scale code repositories (e.g.,
GitHub) for practical tasks is vital in real-world software development, yet
current benchmarks rarely evaluate code agents in such authentic,
workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a
benchmark designed to systematically assess this capability via 54 realistic
tasks across 7 modalities and 7 domains. Each task pairs a relevant repository
with an automated, human-curated evaluation harness specifying practical
success criteria. Beyond measuring execution and task success, we also propose
the alpha-value metric to quantify the economic benefit of agent performance,
which integrates task success rates, token cost, and average developer
salaries. Experiments across three state-of-the-art agent frameworks with
multiple advanced LLMs show that leveraging code repositories for complex task
solving remains challenging: even the best-performing system, OpenHands+Claude
3.7, solves only 48.15% of tasks. Error analysis attributes over half of
failures to seemingly mundane yet critical steps like environment setup and
dependency resolution, highlighting the need for more robust workflow
management and increased timeout preparedness. By releasing GitTaskBench, we
aim to drive progress and attention toward repository-aware code reasoning,
execution, and deployment -- moving agents closer to solving complex,
end-to-end real-world tasks. The benchmark and code are open-sourced at
https://github.com/QuantaAlpha/GitTaskBench.

</details>


### [182] [A Slice-Based Change Impact Analysis for Regression Test Case Prioritization of Object-Oriented Programs](https://arxiv.org/abs/2508.19056)
*S. Panda,D. Munjal,D. P. Mohapatra*

Main category: cs.SE

TL;DR: 本文提出通过计算面向对象程序受影响部分的受影响组件耦合（ACC）对测试用例进行优先级排序的静态方法，经案例研究证明该方法可行且性能可接受。


<details>
  <summary>Details</summary>
Motivation: 找到合适的测试用例执行顺序以实现尽早发现故障等性能目标，节省回归测试的时间和成本。

Method: 构建受影响切片图（ASG）表示受影响的程序部分，计算ASG节点的ACC值确定其故障倾向，为覆盖高ACC值节点的测试用例分配更高优先级。

Result: 对变异故障的分析表明执行易出错程序部分的测试用例比其他测试用例更早发现故障，七个案例研究结果表明该方法可行且性能可接受。

Conclusion: 所提出的基于计算ACC对测试用例进行优先级排序的静态方法是可行的，相比现有技术有可接受的性能。

Abstract: Test case prioritization focuses on finding a suitable order of execution of
the test cases in a test suite to meet some performance goals like detecting
faults early. It is likely that some test cases execute the program parts that
are more prone to errors and will detect more errors if executed early during
the testing process. Finding an optimal order of execution for the selected
regression test cases saves time and cost of retesting. This paper presents a
static approach to prioritizing the test cases by computing the affected
component coupling (ACC) of the affected parts of object-oriented programs. We
construct a graph named affected slice graph (ASG) to represent these affected
program parts.We determine the fault-proneness of the nodes of ASG by computing
their respective ACC values. We assign higher priority to those test cases that
cover the nodes with higher ACC values. Our analysis with mutation faults shows
that the test cases executing the fault-prone program parts have a higher
chance to reveal faults earlier than other test cases in the test suite. The
result obtained from seven case studies justifies that our approach is feasible
and gives acceptable performance in comparison to some existing techniques.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [183] [Tracing Positional Bias in Financial Decision-Making: Mechanistic Insights from Qwen2.5](https://arxiv.org/abs/2508.18427)
*Fabrizio Dimino,Krati Saxena,Bhaskarjit Sarmah,Stefano Pasquali*

Main category: q-fin.CP

TL;DR: 提出统一框架和基准检测量化金融决策中位置偏差，分析其在Qwen2.5 - instruct模型中的起源及影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融领域应用使高风险决策面临位置偏差风险，且现代模型架构增加了这种风险。

Method: 提出统一框架和基准，对金融真实数据集进行实证分析，通过透明的机制可解释性研究偏差。

Result: 发现位置偏差普遍存在、对规模敏感，在特定场景易重现，揭示了近因和首因效应带来的新风险。

Conclusion: 工作为偏差诊断和缓解提供新方法标准，为大语言模型在金融系统的可靠部署提供指导。

Abstract: The growing adoption of large language models (LLMs) in finance exposes
high-stakes decision-making to subtle, underexamined positional biases. The
complexity and opacity of modern model architectures compound this risk. We
present the first unified framework and benchmark that not only detects and
quantifies positional bias in binary financial decisions but also pinpoints its
mechanistic origins within open-source Qwen2.5-instruct models (1.5B--14B). Our
empirical analysis covers a novel, finance-authentic dataset revealing that
positional bias is pervasive, scale-sensitive, and prone to resurfacing under
nuanced prompt designs and investment scenarios, with recency and primacy
effects revealing new vulnerabilities in risk-laden contexts. Through
transparent mechanistic interpretability, we map how and where bias emerges and
propagates within the models to deliver actionable, generalizable insights
across prompt types and scales. By bridging domain-specific audit with model
interpretability, our work provides a new methodological standard for both
rigorous bias diagnosis and practical mitigation, establishing essential
guidance for responsible and trustworthy deployment of LLMs in financial
systems.

</details>


### [184] [Jump detection in financial asset prices that exhibit U-shape volatility](https://arxiv.org/abs/2508.18876)
*Cecilia Mancini*

Main category: q-fin.CP

TL;DR: 本文介绍用于估计金融资产价格跳跃的Matlab程序，包括估计日内效应及跳跃，在模拟和实证数据上测试。


<details>
  <summary>Details</summary>
Motivation: 开发可利用Mancini（2009）阈值法估计金融资产价格跳跃的Matlab程序。

Method: 先实现Bollerslev和Todorov（2011）方法估计日内效应，再用阈值法递归估计每日波动率和跳跃，检测到跳跃后更新波动率估计。

Result: 在模拟资产价格上测试程序可靠性，可视化苹果公司股票价格的日内效应和跳跃检测结果。

Conclusion: 所开发的Matlab程序可有效估计金融资产价格的日内效应和跳跃情况。

Abstract: We describe a Matlab routine that allows us to estimate the jumps in
financial asset prices using the Threshold (or Truncation) method of Mancini
(2009). The routine is designed for application to five-minute log-returns. The
underlying assumption is that asset prices evolve in time following an Ito
semimartingale with, possibly stochastic, volatility and jumps. A log-return is
likely to contain a jump if its absolute value is larger than a threshold
determined by the maximum increment of the Brownian semimartingale part. The
latter is particularly sensitive to the magnitude of the volatility
coefficient, and from an empirical point of view, volatility levels typically
depend on the time of day (TOD), with volatility being highest at the beginning
and end of the day, while it is low in the middle. The first routine presented
allows for an estimation of the TOD effect, and is an implementation of the
method described in Bollerslev and Todorov (2011). Subsequently, the TOD effect
for the stock Apple Inc. (AAPL) is visualized. The second routine presented is
an implementation of the threshold method for estimating jumps in AAPL prices.
The procedure recursively estimates daily volatility and jumps. In each round,
the threshold depends on the time of the day and is constructed using the
estimate of the daily volatility multiplied by the daytime TOD factor and by
the continuity modulus of the Brownian motion paths. Once the jumps are
detected, the daily volatility estimate is updated using only the log-returns
not containing jumps. Before application to empirical data, the reliability of
the procedure was separately tested on simulated asset prices. The results
obtained on a record of AAPL stock prices are visualized.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [185] [Identifying Risk Variables From ESG Raw Data Using A Hierarchical Variable Selection Algorithm](https://arxiv.org/abs/2508.18679)
*Zhi Chen,Zachary Feinstein,Ionut Florescu*

Main category: q-fin.RM

TL;DR: 研究提出Hierarchical Variable Selection (HVS)算法从ESG原始数据中选变量评估企业风险，表现优于使用预聚合ESG分数的模型和传统变量选择方法。


<details>
  <summary>Details</summary>
Motivation: 研究能否从ESG数据中提取相关变量来评估企业风险。

Method: 提出专门针对ESG数据集的Hierarchical Variable Selection (HVS)算法来选择与风险最相关的变量。

Result: HVS比使用预聚合ESG分数的模型性能显著更高，与传统变量选择方法相比，用更精简的ESG变量集实现了更优的解释力。

Conclusion: HVS算法在从ESG数据中选择变量评估企业风险方面表现出色。

Abstract: Environmental, Social, and Governance (ESG) factors aim to provide
non-financial insights into corporations. In this study, we investigate whether
we can extract relevant ESG variables to assess corporate risk, as measured by
logarithmic volatility. We propose a novel Hierarchical Variable Selection
(HVS) algorithm to identify a parsimonious set of variables from raw data that
are most relevant to risk. HVS is specifically designed for ESG datasets
characterized by a tree structure with significantly more variables than
observations. Our findings demonstrate that HVS achieves significantly higher
performance than models using pre-aggregated ESG scores. Furthermore, when
compared with traditional variable selection methods, HVS achieves superior
explanatory power using a more parsimonious set of ESG variables. We illustrate
the methodology using company data from various sectors of the US economy.

</details>


### [186] [Forecasting Probability Distributions of Financial Returns with Deep Neural Networks](https://arxiv.org/abs/2508.18921)
*Jakub Michańków*

Main category: q-fin.RM

TL;DR: 研究评估用于预测金融回报概率分布的深度神经网络，发现其是传统计量经济模型的可行替代方案。


<details>
  <summary>Details</summary>
Motivation: 评估深度神经网络在预测金融回报概率分布方面的表现，为金融风险评估和投资组合管理提供新方法。

Method: 使用1D卷积神经网络（CNN）和长短期记忆网络（LSTM）预测三种概率分布参数，用自定义负对数似然损失函数优化参数，在六个主要股票指数上用概率评估指标测试模型。

Result: 深度学习模型能提供准确的分布预测，在风险价值估计上与经典GARCH模型有竞争力，LSTM搭配偏态学生t分布表现最佳。

Conclusion: 深度神经网络是传统计量经济模型用于金融风险评估和投资组合管理的可行替代方案。

Abstract: This study evaluates deep neural networks for forecasting probability
distributions of financial returns. 1D convolutional neural networks (CNN) and
Long Short-Term Memory (LSTM) architectures are used to forecast parameters of
three probability distributions: Normal, Student's t, and skewed Student's t.
Using custom negative log-likelihood loss functions, distribution parameters
are optimized directly. The models are tested on six major equity indices (S\&P
500, BOVESPA, DAX, WIG, Nikkei 225, and KOSPI) using probabilistic evaluation
metrics including Log Predictive Score (LPS), Continuous Ranked Probability
Score (CRPS), and Probability Integral Transform (PIT). Results show that deep
learning models provide accurate distributional forecasts and perform
competitively with classical GARCH models for Value-at-Risk estimation. The
LSTM with skewed Student's t distribution performs best across multiple
evaluation criteria, capturing both heavy tails and asymmetry in financial
returns. This work shows that deep neural networks are viable alternatives to
traditional econometric models for financial risk assessment and portfolio
management.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [187] [Combined machine learning for stock selection strategy based on dynamic weighting methods](https://arxiv.org/abs/2508.18592)
*Lin Cai,Zhiyang He,Caiya Zhang*

Main category: q-fin.ST

TL;DR: 本文提出基于组合机器学习算法的选股策略框架，开发两种加权方法预测选股策略收益，实证表明组合策略优于单模型，IC加权更具竞争力，因子筛选可提升策略表现。


<details>
  <summary>Details</summary>
Motivation: 提出新的选股策略框架并评估其有效性。

Method: 开发基于模型评估指标的静态加权和基于信息系数（IC）的动态加权两种方法，对三种代表性机器学习算法进行加权，利用沪深300指数数据进行实证评估。

Result: （1）组合机器学习算法策略的回测收益显著优于单模型方法；（2）基于IC的加权（尤其是IC_Mean）在回测收益和预测性能上比基于评估指标的加权更具竞争力；（3）因子筛选大幅提升了组合机器学习策略的性能。

Conclusion: 组合机器学习算法的选股策略框架有效，IC加权和因子筛选对策略有积极作用。

Abstract: This paper proposes a novel stock selection strategy framework based on
combined machine learning algorithms. Two types of weighting methods for three
representative machine learning algorithms are developed to predict the returns
of the stock selection strategy. One is static weighting based on model
evaluation metrics, the other is dynamic weighting based on Information
Coefficients (IC). Using CSI 300 index data, we empirically evaluate the
strategy' s backtested performance and model predictive accuracy. The main
results are as follows: (1) The strategy by combined machine learning
algorithms significantly outperforms single-model approaches in backtested
returns. (2) IC-based weighting (particularly IC_Mean) demonstrates greater
competitiveness than evaluation-metric-based weighting in both backtested
returns and predictive performance. (3) Factor screening substantially enhances
the performance of combined machine learning strategies.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [188] [Deterministic Coreset Construction via Adaptive Sensitivity Trimming](https://arxiv.org/abs/2508.18340)
*Faruk Alpay,Taylan Alpay*

Main category: stat.ML

TL;DR: 本文为经验风险最小化（ERM）中的确定性核心集构建开发了严格框架，提出ADUWT算法，给出完整分析，实证结果与理论一致，最后提出开放性问题。


<details>
  <summary>Details</summary>
Motivation: 为ERM中的确定性核心集构建开发严格框架。

Method: 提出Adaptive Deterministic Uniform - Weight Trimming (ADUWT)算法，通过去除敏感度边界最低的点并对其余点应用依赖数据的统一权重来构建核心集。

Result: 该方法能对整个假设空间的ERM目标产生统一的(1±ε)相对误差近似，实证结果与理论一致。

Conclusion: 提出关于实例最优预言机、确定性流处理和公平约束ERM的开放性问题。

Abstract: We develop a rigorous framework for deterministic coreset construction in
empirical risk minimization (ERM). Our central contribution is the Adaptive
Deterministic Uniform-Weight Trimming (ADUWT) algorithm, which constructs a
coreset by excising points with the lowest sensitivity bounds and applying a
data-dependent uniform weight to the remainder. The method yields a uniform
$(1\pm\varepsilon)$ relative-error approximation for the ERM objective over the
entire hypothesis space. We provide complete analysis, including (i) a minimax
characterization proving the optimality of the adaptive weight, (ii) an
instance-dependent size analysis in terms of a \emph{Sensitivity Heterogeneity
Index}, and (iii) tractable sensitivity oracles for kernel ridge regression,
regularized logistic regression, and linear SVM. Reproducibility is supported
by precise pseudocode for the algorithm, sensitivity oracles, and evaluation
pipeline. Empirical results align with the theory. We conclude with open
problems on instance-optimal oracles, deterministic streaming, and
fairness-constrained ERM.

</details>


### [189] [Revisiting Follow-the-Perturbed-Leader with Unbounded Perturbations in Bandit Problems](https://arxiv.org/abs/2508.18604)
*Jongyeong Lee,Junya Honda,Shinji Ito,Min-hwan Oh*

Main category: stat.ML

TL;DR: 本文研究FTPL策略，通过无界扰动建立BOBW结果，探讨了其与Tsallis熵联系，还发现不同场景下扰动的特点及局限性。


<details>
  <summary>Details</summary>
Motivation: 推进FTPL的分析基础，解决其在获得BOBW结果方面因分析挑战而受限的问题。

Method: 重新审视经典FTRL - FTPL对偶性，研究无界扰动，结合数值观察分析。

Result: 为FTPL在多种无界扰动下建立BOBW结果，发现两臂场景与多臂场景中对称Fréchet型扰动的不同特性。

Conclusion: FTPL的BOBW结果得到扩展，对设计替代策略有新见解，但将两臂场景结论推广到多臂场景需进一步研究。

Abstract: Follow-the-Regularized-Leader (FTRL) policies have achieved
Best-of-Both-Worlds (BOBW) results in various settings through hybrid
regularizers, whereas analogous results for Follow-the-Perturbed-Leader (FTPL)
remain limited due to inherent analytical challenges. To advance the analytical
foundations of FTPL, we revisit classical FTRL-FTPL duality for unbounded
perturbations and establish BOBW results for FTPL under a broad family of
asymmetric unbounded Fr\'echet-type perturbations, including hybrid
perturbations combining Gumbel-type and Fr\'echet-type tails. These results not
only extend the BOBW results of FTPL but also offer new insights into designing
alternative FTPL policies competitive with hybrid regularization approaches.
Motivated by earlier observations in two-armed bandits, we further investigate
the connection between the $1/2$-Tsallis entropy and a Fr\'echet-type
perturbation. Our numerical observations suggest that it corresponds to a
symmetric Fr\'echet-type perturbation, and based on this, we establish the
first BOBW guarantee for symmetric unbounded perturbations in the two-armed
setting. In contrast, in general multi-armed bandits, we find an instance in
which symmetric Fr\'echet-type perturbations violate the key condition for
standard BOBW analysis, which is a problem not observed with asymmetric or
nonnegative Fr\'echet-type perturbations. Although this example does not rule
out alternative analyses achieving BOBW results, it suggests the limitations of
directly applying the relationship observed in two-armed cases to the general
case and thus emphasizes the need for further investigation to fully understand
the behavior of FTPL in broader settings.

</details>


### [190] [Efficient Best-of-Both-Worlds Algorithms for Contextual Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.18768)
*Mengmeng Li,Philipp Schneider,Jelisaveta Aleksić,Daniel Kuhn*

Main category: stat.ML

TL;DR: 提出首个上下文组合半带问题的两全算法，在不同场景有不同悔值保证，还解决投影步骤瓶颈，加速每轮计算，适合大规模实时应用。


<details>
  <summary>Details</summary>
Motivation: 为上下文组合半带问题设计算法，使其在对抗和随机场景都有良好悔值表现，并解决FTRL框架中投影步骤的实际瓶颈。

Method: 基于带香农熵正则化器的Follow - the - Regularized - Leader (FTRL)框架，利用Karush - Kuhn - Tucker条件将高维凸投影问题转化为单变量求根问题。

Result: 算法达到两全算法的悔值界，且每轮计算有显著加速。

Conclusion: 该组合策略适合大规模实时应用。

Abstract: We introduce the first best-of-both-worlds algorithm for contextual
combinatorial semi-bandits that simultaneously guarantees
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret in the adversarial regime and
$\widetilde{\mathcal{O}}(\ln T)$ regret in the corrupted stochastic regime. Our
approach builds on the Follow-the-Regularized-Leader (FTRL) framework equipped
with a Shannon entropy regularizer, yielding a flexible method that admits
efficient implementations. Beyond regret bounds, we tackle the practical
bottleneck in FTRL (or, equivalently, Online Stochastic Mirror Descent) arising
from the high-dimensional projection step encountered in each round of
interaction. By leveraging the Karush-Kuhn-Tucker conditions, we transform the
$K$-dimensional convex projection problem into a single-variable root-finding
problem, dramatically accelerating each round. Empirical evaluations
demonstrate that this combined strategy not only attains the attractive regret
bounds of best-of-both-worlds algorithms but also delivers substantial
per-round speed-ups, making it well-suited for large-scale, real-time
applications.

</details>


### [191] [Sparse minimum Redundancy Maximum Relevance for feature selection](https://arxiv.org/abs/2508.18901)
*Peter Naylor,Benjamin Poignard,Héctor Climente-González,Makoto Yamada*

Main category: stat.ML

TL;DR: 提出集成特征间与特征 - 目标关系的特征筛选方法，经模拟和真实数据集验证有效，代码开源。


<details>
  <summary>Details</summary>
Motivation: 开发能准确识别无效特征且控制错误发现率的特征筛选方法。

Method: 通过惩罚最小冗余最大相关性（mRMR）程序识别无效特征，引入基于knockoff filter的多阶段程序控制FDR。

Result: 该方法与HSIC - LASSO表现相当，在选择特征数量上更保守，只需设置FDR阈值。

Conclusion: 提出的特征筛选方法有效，可用于特征筛选任务。

Abstract: We propose a feature screening method that integrates both feature-feature
and feature-target relationships. Inactive features are identified via a
penalized minimum Redundancy Maximum Relevance (mRMR) procedure, which is the
continuous version of the classic mRMR penalized by a non-convex regularizer,
and where the parameters estimated as zero coefficients represent the set of
inactive features. We establish the conditions under which zero coefficients
are correctly identified to guarantee accurate recovery of inactive features.
We introduce a multi-stage procedure based on the knockoff filter enabling the
penalized mRMR to discard inactive features while controlling the false
discovery rate (FDR). Our method performs comparably to HSIC-LASSO but is more
conservative in the number of selected features. It only requires setting an
FDR threshold, rather than specifying the number of features to retain. The
effectiveness of the method is illustrated through simulations and real-world
datasets. The code to reproduce this work is available on the following GitHub:
https://github.com/PeterJackNaylor/SmRMR.

</details>


### [192] [Echoes of the past: A unified perspective on fading memory and echo states](https://arxiv.org/abs/2508.19145)
*Juan-Pablo Ortega,Florian Rossmannek*

Main category: stat.ML

TL;DR: 本文旨在统一循环神经网络（RNN）中关于记忆的不同概念，推导其新的含义和等价关系，并为现有结果提供新证明，以加深对RNN的理解。


<details>
  <summary>Details</summary>
Motivation: RNN中关于记忆的不同概念常被混用，其精确关系尚不明确，需要统一和厘清。

Method: 用通用语言统一概念，推导概念间的新含义和等价关系，为现有结果提供替代证明。

Result: 明确了这些概念之间的关系。

Conclusion: 有助于更深入理解RNN及其时间信息处理能力。

Abstract: Recurrent neural networks (RNNs) have become increasingly popular in
information processing tasks involving time series and temporal data. A
fundamental property of RNNs is their ability to create reliable input/output
responses, often linked to how the network handles its memory of the
information it processed. Various notions have been proposed to conceptualize
the behavior of memory in RNNs, including steady states, echo states, state
forgetting, input forgetting, and fading memory. Although these notions are
often used interchangeably, their precise relationships remain unclear. This
work aims to unify these notions in a common language, derive new implications
and equivalences between them, and provide alternative proofs to some existing
results. By clarifying the relationships between these concepts, this research
contributes to a deeper understanding of RNNs and their temporal information
processing capabilities.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [193] [Parallelizing MCMC Across the Sequence Length](https://arxiv.org/abs/2508.18413)
*David M. Zoltowski,Skyler Wu,Xavier Gonzalez,Leo Kozachkov,Scott W. Linderman*

Main category: stat.CO

TL;DR: 本文提出并行评估马尔可夫链蒙特卡罗（MCMC）采样器的算法，可跨链长度并行化多种MCMC采样，还开发了新的并行拟牛顿法，加速了MCMC采样。


<details>
  <summary>Details</summary>
Motivation: 大多数MCMC算法是顺序的，时间复杂度与序列长度线性相关，此前研究聚焦于并行运行独立链，本文探索跨链长度并行评估MCMC采样器的新途径。

Method: 基于非线性递归的并行评估方法，将状态序列表示为定点问题的解，用并行牛顿法求解定点问题，还开发了两种新的并行拟牛顿法。

Result: 展示了该方法可跨序列长度并行化多种MCMC采样，通过几十次并行牛顿迭代可模拟多达数十万的MCMC样本，新方法降低了内存成本和运行时间。

Conclusion: 提出的并行算法在多个示例中加速了MCMC采样，某些情况下比顺序评估快一个数量级以上。

Abstract: Markov chain Monte Carlo (MCMC) methods are foundational algorithms for
Bayesian inference and probabilistic modeling. However, most MCMC algorithms
are inherently sequential and their time complexity scales linearly with the
sequence length. Previous work on adapting MCMC to modern hardware has
therefore focused on running many independent chains in parallel. Here, we take
an alternative approach: we propose algorithms to evaluate MCMC samplers in
parallel across the chain length. To do this, we build on recent methods for
parallel evaluation of nonlinear recursions that formulate the state sequence
as a solution to a fixed-point problem and solve for the fixed-point using a
parallel form of Newton's method. We show how this approach can be used to
parallelize Gibbs, Metropolis-adjusted Langevin, and Hamiltonian Monte Carlo
sampling across the sequence length. In several examples, we demonstrate the
simulation of up to hundreds of thousands of MCMC samples with only tens of
parallel Newton iterations. Additionally, we develop two new parallel
quasi-Newton methods to evaluate nonlinear recursions with lower memory costs
and reduced runtime. We find that the proposed parallel algorithms accelerate
MCMC sampling across multiple examples, in some cases by more than an order of
magnitude compared to sequential evaluation.

</details>


### [194] [VPPE: Application of Scaled Vecchia Approximations to Parallel Partial Emulation](https://arxiv.org/abs/2508.19144)
*Josh Seidman,Elaine T. Spiller*

Main category: stat.CO

TL;DR: 本文引入了Vecchia Parallel Partial Emulation (VPPE)，利用Scaled Vecchia近似处理大规模训练数据集的并行部分仿真，在三个实验中展现出与PPE相当的预测精度，但运行时间大幅缩短。


<details>
  <summary>Details</summary>
Motivation: 计算机模型计算成本高，Gaussian process emulators中的PPE虽能处理向量输出，但拟合时间随训练运行次数增加而快速增长，需改进方法处理大规模训练数据集。

Method: 引入VPPE，在PPE框架内利用Scaled Vecchia近似实现大规模训练数据集的并行部分仿真。

Result: 将VPPE应用于三个计算机实验，与PPE有相当的预测精度，但运行时间大幅减少。

Conclusion: VPPE能在大规模训练数据集上有效进行并行部分仿真，减少运行时间并保持预测精度。

Abstract: Computer models or simulators are widely used across scientific fields, but
are computationally expensive limiting their use to explore possible
scenarios/outcomes. Gaussian process emulators are statistical surrogates that
can rapidly approximate the outputs of computer models at untested inputs and
enable uncertainty quantification studies. The parallel partial emulation (PPE)
was developed to model simulators with vector-valued outputs. While the PPE is
adept at fitting simulator data with multidimensional outputs, the time to fit
the PPE increases quickly as the number of training runs increases. The Scaled
Vecchia approximation, a fast approximation to multivariate Gaussian
likelihoods, makes fitting Gaussian process emulators with large training
datasets tractable. Here we introduce the Vecchia Parallel Partial Emulation
(VPPE) that utilizes the Scaled Vecchia approximation within the PPE framework
to allow for parallel partial emulation with larger training datasets. The VPPE
is applied to three computer experiments, a synthetic data set, a hydrology
model, and a volcanic flow model, yielding comparable predictive accuracy to
the PPE at a fraction of the runtime.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [195] [Technology-assisted Personalized Yoga for Better Health -- Challenges and Outlook](https://arxiv.org/abs/2508.18283)
*Vivek Kumar,Himanshu Sahu,Hari Prabhat Gupta,Biplav Srivastava*

Main category: cs.HC

TL;DR: 本文探讨瑜伽个性化问题挑战，提出初步方法并从多学科计算视角展望解决途径，还以太阳致敬式为例说明。


<details>
  <summary>Details</summary>
Motivation: 为使人们能根据自身独特需求获得瑜伽益处，解决瑜伽个性化问题。

Method: 描述瑜伽个性化问题挑战，提出初步方法，结合多学科计算技术。

Result: 从姿势感知到完整方案矫正推荐，全面审视瑜伽个性化决策支持问题。

Conclusion: 此为首次全面研究瑜伽个性化决策支持问题的论文，通过案例说明解决思路。

Abstract: Yoga is a discipline of physical postures, breathing techniques, and
meditative practices rooted in ancient Indian traditions, now embraced
worldwide for promoting overall well-being and inner balance. The practices are
a large set of items, our term for executable actions like physical poses or
breath exercises, to offer for a person's well-being. However, to get benefits
of Yoga tailored to a person's unique needs, a person needs to (a) discover
their subset from the large and seemingly complex set with inter-dependencies,
(b) continue to follow them with interest adjusted to their changing abilities
and near-term objectives, and (c) as appropriate, adapt to alternative items
based on changing environment and the person's health conditions. In this
vision paper, we describe the challenges for the Yoga personalization problem.
Next, we sketch a preliminary approach and use the experience to provide an
outlook on solving the challenging problem using existing and novel techniques
from a multidisciplinary computing perspective. To the best of our knowledge,
this is the first paper that comprehensively examines decision support issues
around Yoga personalization, from pose sensing to recommendation of corrections
for a complete regimen, and illustrates with a case study of Surya Namaskar --
a set of 12 choreographed poses.

</details>


### [196] [Does Calibration Affect Human Actions?](https://arxiv.org/abs/2508.18317)
*Meir Nizri,Amos Azaria,Chirag Gupta,Noam Hazon*

Main category: cs.HC

TL;DR: 研究校准分类模型对非专家人类决策的影响，发现仅校准不足，前景理论修正对提高决策与预测相关性至关重要，对信任表述无影响。


<details>
  <summary>Details</summary>
Motivation: 探究校准分类模型对非专家人类基于模型预测做决策的影响。

Method: 开展人机交互实验，研究校准对模型信任和决策与预测相关性的影响，基于前景理论对校准分数进行修正并研究其影响。

Result: 校准本身不足，前景理论修正对提高人类决策与模型预测的相关性至关重要，对“是否更信任模型”的回答不受方法影响。

Conclusion: 前景理论修正对提升人类决策与模型预测的相关性很关键，而对主观信任表述无作用。

Abstract: Calibration has been proposed as a way to enhance the reliability and
adoption of machine learning classifiers. We study a particular aspect of this
proposal: how does calibrating a classification model affect the decisions made
by non-expert humans consuming the model's predictions? We perform a
Human-Computer-Interaction (HCI) experiment to ascertain the effect of
calibration on (i) trust in the model, and (ii) the correlation between
decisions and predictions. We also propose further corrections to the reported
calibrated scores based on Kahneman and Tversky's prospect theory from
behavioral economics, and study the effect of these corrections on trust and
decision-making. We find that calibration is not sufficient on its own; the
prospect theory correction is crucial for increasing the correlation between
human decisions and the model's predictions. While this increased correlation
suggests higher trust in the model, responses to ``Do you trust the model
more?" are unaffected by the method used.

</details>


### [197] [Beyond prior knowledge: The predictive role of knowledge-building in Tutor Learning](https://arxiv.org/abs/2508.18545)
*Tasmia Shahriar,Mia Ameen,Aditi Mallavarapu,Shiyan Jiang,Noboru Matsuda*

Main category: cs.HC

TL;DR: 研究探讨知识构建在程序性和概念性学习双向关系中的中介作用，发现知识构建能缩小低先验知识学生与高学习增益的差距。


<details>
  <summary>Details</summary>
Motivation: 学生在以教促学环境中常知识讲述而非知识构建，限制学习，研究知识构建在程序性和概念性学习双向关系中的作用。

Method: 未提及具体研究方法

Result: 程序性和概念性知识存在稳定双向关系，参与知识构建的学生后测成绩更高，不论前测表现。

Conclusion: 知识构建是缩小低先验知识学生与更高概念和程序性学习增益差距的关键机制。

Abstract: When adopting the role of a teacher in learning-by-teaching environments,
students often struggle to engage in knowledge-building activities, such as
providing explanations and addressing misconceptions. Instead, they frequently
default to knowledge-telling behaviors, where they simply dictate what they
already know or what to do without deeper reflection, thereby limiting
learning. Teachable agents, particularly those capable of posing persistent
follow-up questions, have been shown to encourage students (tutors) to shift
from knowledge-telling to knowledge-building and enhance tutor learning. Tutor
learning encompasses two interrelated types of knowledge: conceptual and
procedural knowledge. Research has established a bidirectional relationship
between these knowledge types, where improvements in one reinforce the other.
This study investigates the role of knowledge-building in mediating the
bidirectional relationship between procedural and conceptual learning. Our
findings revealed a stable bidirectional relationship between procedural and
conceptual knowledge, with higher post-test scores observed among students who
engaged in knowledge-building, regardless of their procedural and conceptual
pre-test performance. This suggests that knowledge-building serves as a crucial
mechanism bridging the gap between students with low prior knowledge and higher
conceptual and procedural learning gain.

</details>


### [198] [Long-Term Variability in Physiological-Arousal Relationships for Robust Emotion Estimation](https://arxiv.org/abs/2508.18782)
*Hiroto Sakimura,Takayuki Nagaya,Tomoki Nishi,Tetsuo Kurahashi,Katsunori Kohda,Nobuhiko Muramoto*

Main category: cs.HC

TL;DR: 研究个体数月内生理特征与主观情绪关系是否稳定，发现存在长期变异性，建议定期更新情绪估计模型。


<details>
  <summary>Details</summary>
Motivation: 以往情感估计系统隐含生理特征与主观情感关系稳定的假设，该假设在长时间框架下很少被检验，所以研究个体数月内这种关系是否一致。

Method: 开发自定义测量系统，收集24名参与者两个三个月期间的生理信号和自我报告的情绪状态，用可解释提升机（EBMs）分析。

Result: 用第一期数据训练的模型在第二期测试时准确率下降5%，心率预测较稳定，最低EDA个体间波动大。

Conclusion: 需考虑生理 - 唤醒关系的时间变异性，情绪估计模型应根据观察到的变化趋势定期更新以保持稳健性能。

Abstract: Estimating emotional states from physiological signals is a central topic in
affective computing and psychophysiology. While many emotion estimation systems
implicitly assume a stable relationship between physiological features and
subjective affect, this assumption has rarely been tested over long timeframes.
This study investigates whether such relationships remain consistent across
several months within individuals. We developed a custom measurement system and
constructed a longitudinal dataset by collecting physiological signals --
including blood volume pulse, electrodermal activity (EDA), skin temperature,
and acceleration--along with self-reported emotional states from 24
participants over two three-month periods. Data were collected in naturalistic
working environments, allowing analysis of the relationship between
physiological features and subjective arousal in everyday contexts. We examined
how physiological-arousal relationships evolve over time by using Explainable
Boosting Machines (EBMs) to ensure model interpretability. A model trained on
1st-period data showed a 5\% decrease in accuracy when tested on 2nd-period
data, indicating long-term variability in physiological-arousal associations.
EBM-based comparisons further revealed that while heart rate remained a
relatively stable predictor, minimum EDA exhibited substantial individual-level
fluctuations between periods. While the number of participants is limited,
these findings highlight the need to account for temporal variability in
physiological-arousal relationships and suggest that emotion estimation models
should be periodically updated -- e.g., every five months -- based on observed
shift trends to maintain robust performance over time.

</details>


### [199] [Insights into User Interface Innovations from a Design Thinking Workshop at deRSE25](https://arxiv.org/abs/2508.18784)
*Maximilian Frank,Simon Lund*

Main category: cs.HC

TL;DR: 本文介绍在deRSE25会议设计思维研讨会上为大语言模型开发创新用户界面概念的情况，探讨其对界面开发的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用，但用户界面有限，需开发创新界面。

Method: 举办设计思维研讨会，让参与者识别用例、评估现有界面优缺点并创建新交互概念可视化。采用以用户为中心的设计流程指导界面开发。

Result: 参与者的想法推动了基于白板的UI方法，界面开发持续进行。

Conclusion: 未来大语言模型界面开发应基于以用户为中心的设计原则，注重UI创新。

Abstract: Large Language Models have become widely adopted tools due to their versatile
capabilities, yet their user interfaces remain limited, often following rigid,
linear interaction paradigms. In this paper, we present insights from a design
thinking workshop held at the deRSE25 conference aiming at collaboratively
developing innovative user interface concepts for LLMs. During the workshop,
participants identified common use cases, evaluated the strengths and
shortcomings of current LLM interfaces, and created visualizations of new
interaction concepts emphasizing flexible context management, dynamic
conversation branching, and enhanced mechanisms for user control. We describe
how these participant-generated ideas advanced our own whiteboard-based UI
approach. The ongoing development of this interface is guided by the
human-centered design process - an iterative, user-focused methodology that
emphasizes continuous refinement through user feedback. Broader implications
for future LLM interface development are discussed, advocating for increased
attention to UI innovation grounded in user-centered design principles.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [200] [SkyTrust: Blockchain-Enhanced UAV Security for NTNs with Dynamic Trust and Energy-Aware Consensus](https://arxiv.org/abs/2508.18735)
*Afan Ali,Irfanullah Khan*

Main category: eess.SP

TL;DR: 本文提出DTSAM - EAC机制增强基于无人机的非地面网络（NTNs）安全性，模拟结果显示该机制在多方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 基于无人机的非地面网络因分布式和动态特性易受安全攻击，易受恶意节点影响，需要增强安全性。

Method: 提出DTSAM - EAC机制，整合许可型Hyperledger Fabric区块链和联邦学习进行隐私保护的信任评估，通过加权聚合更新信任评级，采用能量感知共识机制，使用信任加权的联邦学习聚合。

Result: 模拟结果显示该框架信任分数预测准确率达94%，恶意无人机检测率达96%，在隐私、能源效率和可靠性方面优于集中式和静态的基于信任的解决方案。

Conclusion: 该机制符合6G对分布式智能和可持续性的要求，是保护NTNs的节能且可扩展的解决方案。

Abstract: Non-Terrestrial Networks (NTNs) based on Unmanned Aerial Vehicles (UAVs) as
base stations are extremely susceptible to security attacks due to their
distributed and dynamic nature, which makes them vulnerable to rogue nodes. In
this paper, a new Dynamic Trust Score Adjustment Mechanism with Energy-Aware
Consensus (DTSAM-EAC) is proposed to enhance security in UAV-based NTNs. The
proposed framework integrates a permissioned Hyperledger Fabric blockchain with
Federated Learning (FL) to support privacy-preserving trust evaluation. Trust
ratings are updated continuously through weighted aggregation of past trust,
present behavior, and energy contribution, thus making the system adaptive to
changing network conditions. An energy-aware consensus mechanism prioritizes
UAVs with greater available energy for block validation, ensuring efficient use
of resources under resource-constrained environments. FL aggregation with
trust-weighting further increases the resilience of the global trust model.
Simulation results verify the designed framework achieves 94\% trust score
prediction accuracy and 96\% rogue UAV detection rate while outperforming
centralized and static baselines of trust-based solutions on privacy, energy
efficiency, and reliability. It complies with 6G requirements in terms of
distributed intelligence and sustainability and is an energy-efficient and
scalable solution to secure NTNs.

</details>


### [201] [EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding](https://arxiv.org/abs/2508.18785)
*Luqing Luo,Wenjin Gui,Yunfei Liu,Ziyue Zhang,Yunxi Zhang,Fengxiang Wang,Zonghao Guo,Zizhi Ma,Xinzhu Liu,Hanxiang He,Jinhai Li,Xin Qiu,Wupeng Xie,Yangang Sun*

Main category: eess.SP

TL;DR: 提出电磁信号基础模型 EMind，构建统一标准化数据集，设计新方法，在下游任务表现好且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 电磁信号特性使现有通用模型无法直接使用，当前方法缺乏跨任务泛化和迁移效率，缺少高质量大数据集，难以构建通用多任务学习框架。

Method: 引入 EMind 模型，构建首个统一且最大的标准化电磁信号数据集，设计长度自适应多信号打包方法和硬件感知训练策略。

Result: 实验表明 EMind 在许多下游任务中取得强性能和广泛泛化性。

Conclusion: 实现从特定任务模型向电磁智能统一框架的转变。

Abstract: Deep understanding of electromagnetic signals is fundamental to dynamic
spectrum management, intelligent transportation, autonomous driving and
unmanned vehicle perception. The field faces challenges because electromagnetic
signals differ greatly from text and images, showing high heterogeneity, strong
background noise and complex joint time frequency structure, which prevents
existing general models from direct use. Electromagnetic communication and
sensing tasks are diverse, current methods lack cross task generalization and
transfer efficiency, and the scarcity of large high quality datasets blocks the
creation of a truly general multitask learning framework. To overcome these
issue, we introduce EMind, an electromagnetic signals foundation model that
bridges large scale pretraining and the unique nature of this modality. We
build the first unified and largest standardized electromagnetic signal dataset
covering multiple signal types and tasks. By exploiting the physical properties
of electromagnetic signals, we devise a length adaptive multi-signal packing
method and a hardware-aware training strategy that enable efficient use and
representation learning from heterogeneous multi-source signals. Experiments
show that EMind achieves strong performance and broad generalization across
many downstream tasks, moving decisively from task specific models to a unified
framework for electromagnetic intelligence. The code is available at:
https://github.com/GabrielleTse/EMind.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [202] [A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks](https://arxiv.org/abs/2508.18803)
*Jiaqi Wu,Jing Liu,Yang Liu,Lixu Wang,Zehua Wang,Wei Chen,Zijian Tian,Richard Yu,Victor C. M. Leung*

Main category: cs.NI

TL;DR: 物联网设备和AI服务增长促使云边端协同智能（CETCI）成为重要范式，本文介绍其基础架构、使能技术和场景，分析组件与技术，探讨协作范式和学习框架，讨论挑战与趋势。


<details>
  <summary>Details</summary>
Motivation: 物联网设备和AI服务的发展增加了对高效分布式计算架构和网络的需求，推动CETCI成为重要范式，有必要进行全面综述。

Method: 系统分析云边端各层架构组件，研究核心技术，对协作范式进行分类，回顾智能协作学习框架的进展。

Result: 对CETCI范式的基础架构、技术、场景等进行了全面介绍，分析了组件与技术，分类了协作范式，阐述了学习框架，讨论了挑战和趋势。

Conclusion: 分布式计算和通信的集成可解决开放问题，指导构建强大、高效和安全的协作AIoT系统。

Abstract: The proliferation of Internet of things (IoT) devices in smart cities,
transportation, healthcare, and industrial applications, coupled with the
explosive growth of AI-driven services, has increased demands for efficient
distributed computing architectures and networks, driving cloud-edge-terminal
collaborative intelligence (CETCI) as a fundamental paradigm within the
artificial intelligence of things (AIoT) community. With advancements in deep
learning, large language models (LLMs), and edge computing, CETCI has made
significant progress with emerging AIoT applications, moving beyond isolated
layer optimization to deployable collaborative intelligence systems for AIoT
(CISAIOT), a practical research focus in AI, distributed computing, and
communications. This survey describes foundational architectures, enabling
technologies, and scenarios of CETCI paradigms, offering a tutorial-style
review for CISAIOT beginners. We systematically analyze architectural
components spanning cloud, edge, and terminal layers, examining core
technologies including network virtualization, container orchestration, and
software-defined networking, while presenting categorizations of collaboration
paradigms that cover task offloading, resource allocation, and optimization
across heterogeneous infrastructures. Furthermore, we explain intelligent
collaboration learning frameworks by reviewing advances in federated learning,
distributed deep learning, edge-cloud model evolution, and reinforcement
learning-based methods. Finally, we discuss challenges (e.g., scalability,
heterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum
computing, digital twin), highlighting how integration of distributed computing
and communication can address open issues and guide development of robust,
efficient, and secure collaborative AIoT systems.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [203] [The GINN framework: a stochastic QED correspondence for stability and chaos in deep neural networks](https://arxiv.org/abs/2508.18948)
*Rodrigo Carmo Terin*

Main category: hep-th

TL;DR: 提出将深度神经网络映射到具有局部U(1)对称性的量子电动力学的欧几里得随机场论方法，进行数值验证并提出新网络实现及统一计算方法。


<details>
  <summary>Details</summary>
Motivation: 建立深度神经网络与量子电动力学的联系，探索相关理论和应用。

Method: 将神经激活和权重用费米子物质和规范场表示，利用虚拟朗之万时间进行协变规范固定，通过数值模拟验证，采用幅度 - 相位参数化实现规范不变神经网络，用双拷贝副本方法统一计算。

Result: 理论预测通过数值模拟验证，提出规范不变神经网络实现，统一了随机量子电动力学和宽深度神经网络中最大李雅普诺夫指数的计算。

Conclusion: 该欧几里得随机场论方法能有效建立深度神经网络与量子电动力学的映射关系，具有一定理论和应用价值。

Abstract: The development of a Euclidean stochastic field-theoretic approach that maps
deep neural networks (DNNs) to quantum electrodynamics (QED) with local U(1)
symmetry is presented. Neural activations and weights are represented by
fermionic matter and gauge fields, with a fictitious Langevin time enabling
covariant gauge fixing. This mapping identifies the gauge parameter with kernel
design choices in wide DNNs, relating stability thresholds to gauge-dependent
amplification factors. Finite-width fluctuations correspond to loop corrections
in QED. As a proof of concept, we validate the theoretical predictions through
numerical simulations of standard multilayer perceptrons and, in parallel,
propose a gauge-invariant neural network (GINN) implementation using
magnitude--phase parameterization of weights. Finally, a double-copy replica
approach is shown to unify the computation of the largest Lyapunov exponent in
stochastic QED and wide DNNs.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [204] [scI2CL: Effectively Integrating Single-cell Multi-omics by Intra- and Inter-omics Contrastive Learning](https://arxiv.org/abs/2508.18304)
*Wuchao Liu,Han Peng,Wengen Li,Yichao Zhang,Jihong Guan,Shuigeng Zhou*

Main category: q-bio.GN

TL;DR: 本文提出基于内外组学对比学习的单细胞多组学融合框架scI2CL，实验验证其在下游任务中优于现有方法，能有效融合多组学数据并支持分析。


<details>
  <summary>Details</summary>
Motivation: 单细胞多组学数据虽信息丰富，但基于其对细胞相互作用模式进行计算建模和推断仍具挑战。

Method: 提出scI2CL框架，基于内外组学对比学习从互补多组学数据中学习全面且有区分性的细胞表征。

Result: 在四个下游任务的大量实验中，scI2CL在细胞聚类、亚型分类、发育轨迹构建和细胞类型分类上均优于现有方法。

Conclusion: scI2CL能准确表征细胞间跨组学关系，有效融合多组学数据并学习有区分性的细胞表征以支持下游分析任务。

Abstract: Single-cell multi-omics data contain huge information of cellular states, and
analyzing these data can reveal valuable insights into cellular heterogeneity,
diseases, and biological processes. However, as cell differentiation \&
development is a continuous and dynamic process, it remains challenging to
computationally model and infer cell interaction patterns based on single-cell
multi-omics data. This paper presents scI2CL, a new single-cell multi-omics
fusion framework based on intra- and inter-omics contrastive learning, to learn
comprehensive and discriminative cellular representations from complementary
multi-omics data for various downstream tasks. Extensive experiments of four
downstream tasks validate the effectiveness of scI2CL and its superiority over
existing peers. Concretely, in cell clustering, scI2CL surpasses eight
state-of-the-art methods on four widely-used real-world datasets. In cell
subtyping, scI2CL effectively distinguishes three latent monocyte cell
subpopulations, which are not discovered by existing methods. Simultaneously,
scI2CL is the only method that correctly constructs the cell developmental
trajectory from hematopoietic stem and progenitor cells to Memory B cells. In
addition, scI2CL resolves the misclassification of cell types between two
subpopulations of CD4+ T cells, while existing methods fail to precisely
distinguish the mixed cells. In summary, scI2CL can accurately characterize
cross-omics relationships among cells, thus effectively fuses multi-omics data
and learns discriminative cellular representations to support various
downstream analysis tasks.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [205] [A Bag of Tricks for Efficient Implicit Neural Point Clouds](https://arxiv.org/abs/2508.19140)
*Florian Hahlbohm,Linus Franke,Leon Overkämping,Paula Wespe,Susana Castillo,Martin Eisemann,Marcus Magnor*

Main category: cs.GR

TL;DR: 本文提出优化方法提升 Implicit Neural Point Cloud (INPC) 训练和推理性能，不牺牲视觉保真度，优化后训练更快、渲染更快、显存使用减少且图像质量略有提升。


<details>
  <summary>Details</summary>
Motivation: INPC 在渲染时查询神经网络，导致渲染速度慢，限制了其实用性。

Method: 采用改进的光栅化器实现、更有效的采样技术、对用于空洞填充的卷积神经网络进行预训练，推理时将点建模为小高斯分布。

Result: 优化后的 INPC 管道训练速度提升达 25%，渲染速度提升 2 倍，显存使用减少 20%，图像质量略有提升。

Conclusion: 所提出的优化方法有效，且具有广泛适用性。

Abstract: Implicit Neural Point Cloud (INPC) is a recent hybrid representation that
combines the expressiveness of neural fields with the efficiency of point-based
rendering, achieving state-of-the-art image quality in novel view synthesis.
However, as with other high-quality approaches that query neural networks
during rendering, the practical usability of INPC is limited by comparatively
slow rendering. In this work, we present a collection of optimizations that
significantly improve both the training and inference performance of INPC
without sacrificing visual fidelity. The most significant modifications are an
improved rasterizer implementation, more effective sampling techniques, and the
incorporation of pre-training for the convolutional neural network used for
hole-filling. Furthermore, we demonstrate that points can be modeled as small
Gaussians during inference to further improve quality in extrapolated, e.g.,
close-up views of the scene. We design our implementations to be broadly
applicable beyond INPC and systematically evaluate each modification in a
series of experiments. Our optimized INPC pipeline achieves up to 25% faster
training, 2x faster rendering, and 20% reduced VRAM usage paired with slight
image quality improvements.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [206] [The Quasi-Creature and the Uncanny Valley of Agency: A Synthesis of Theory and Evidence on User Interaction with Inconsistent Generative AI](https://arxiv.org/abs/2508.18563)
*Mauricio Manhaes,Christine Miller,Nicholas Schroeder*

Main category: cs.CY

TL;DR: 用户使用大规模生成式AI体验矛盾，论文提出‘准生物’和‘能动性恐怖谷’概念，通过研究揭示AI效率与用户挫败感负相关，框架解释了用户挫败感并对AI设计等有重要意义。


<details>
  <summary>Details</summary>
Motivation: 解释用户使用大规模生成式AI时因体验矛盾而产生强烈挫败感的原因。

Method: 综合人机交互、认知科学和技术哲学知识，开展名为‘Move 78’的混合方法研究（N = 37），研究协作创意任务。

Result: 发现用户感知的AI效率与挫败感之间存在强烈负相关。

Conclusion: 提出的框架能有力解释用户对生成式AI的挫败感，对AI设计、伦理和社会融入有重要影响。

Abstract: The user experience with large-scale generative AI is paradoxical: superhuman
fluency meets absurd failures in common sense and consistency. This paper
argues that the resulting potent frustration is an ontological problem,
stemming from the "Quasi-Creature"-an entity simulating intelligence without
embodiment or genuine understanding. Interaction with this entity precipitates
the "Uncanny Valley of Agency," a framework where user comfort drops when
highly agentic AI proves erratically unreliable. Its failures are perceived as
cognitive breaches, causing profound cognitive dissonance. Synthesizing HCI,
cognitive science, and philosophy of technology, this paper defines the
Quasi-Creature and details the Uncanny Valley of Agency. An illustrative
mixed-methods study ("Move 78," N=37) of a collaborative creative task reveals
a powerful negative correlation between perceived AI efficiency and user
frustration, central to the negative experience. This framework robustly
explains user frustration with generative AI and has significant implications
for the design, ethics, and societal integration of these powerful, alien
technologies.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [207] [A note on Cybenko's Universal Approximation Theorem](https://arxiv.org/abs/2508.18893)
*Kun Wang*

Main category: math.CA

TL;DR: 指出G.Cybenko对通用逼近定理证明中的错误，该错误不易修复且引出测度论问题。


<details>
  <summary>Details</summary>
Motivation: 发现被广泛引用的G.Cybenko对通用逼近定理证明中存在错误。

Method: 未提及

Result: 明确指出证明中的错误，且该错误按原证明思路或许不易修复。

Conclusion: 证明中的错误引出了测度论中的有趣问题。

Abstract: In this short note, we point out a mistake in G.Cybenko's proof of his
version of the universal approximation theorem which has been widely cited.
This mistake might not be easily fixable along the idea of his proof and it
also leads to an interesting question in measure theory.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [208] [Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology](https://arxiv.org/abs/2508.18288)
*Jay L. Cunningham,Adinawa Adjagbodjou,Jeffrey Basoah,Jainaba Jawara,Kowe Kadoma,Aaleyah Lewis*

Main category: eess.AS

TL;DR: 本文对非裔美国英语及其他语言多样社区在自动语音识别及相关技术中的公平、偏见和公平性研究进行文献综述，指出以治理为中心方法的缺口并提出框架。


<details>
  <summary>Details</summary>
Motivation: 研究自动语音识别及相关技术中针对非裔美国英语使用者和其他语言多样社区的公平、偏见和公平性概念及操作化情况。

Method: 对人机交互、机器学习/自然语言处理和社会语言学领域的44篇同行评审出版物进行文献综述。

Result: 确定四个主要研究领域，发现以治理为中心、强调社区能动性等的方法存在关键缺口。

Conclusion: 提出以治理为中心的自动语音识别生命周期框架，为相关人员解决语音AI系统中的语言边缘化问题提供启示。

Abstract: This scoping literature review examines how fairness, bias, and equity are
conceptualized and operationalized in Automatic Speech Recognition (ASR) and
adjacent speech and language technologies (SLT) for African American English
(AAE) speakers and other linguistically diverse communities. Drawing from 44
peer-reviewed publications across Human-Computer Interaction (HCI), Machine
Learning/Natural Language Processing (ML/NLP), and Sociolinguistics, we
identify four major areas of inquiry: (1) how researchers understand
ASR-related harms; (2) inclusive data practices spanning collection, curation,
annotation, and model training; (3) methodological and theoretical approaches
to linguistic inclusion; and (4) emerging practices and design recommendations
for more equitable systems. While technical fairness interventions are growing,
our review highlights a critical gap in governance-centered approaches that
foreground community agency, linguistic justice, and participatory
accountability. We propose a governance-centered ASR lifecycle as an emergent
interdisciplinary framework for responsible ASR development and offer
implications for researchers, practitioners, and policymakers seeking to
address language marginalization in speech AI systems.

</details>


### [209] [EAI-Avatar: Emotion-Aware Interactive Talking Head Generation](https://arxiv.org/abs/2508.18337)
*Haijie Yang,Zhenyu Zhang,Hao Tang,Jianjun Qian,Jian Yang*

Main category: eess.AS

TL;DR: 提出EAI - Avatar框架用于双向交互的情感感知头像生成，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为单向头像动画，少数支持双向交互的方法缺乏精确情感适应能力，限制了实际应用。

Method: 利用大语言模型对话生成能力，设计基于Transformer的头部掩码生成器学习时间一致的运动特征，引入交互式对话树结构表示对话状态转换并提取情感线索。

Result: 通过大量实验证明该方法性能优越、有效。

Conclusion: 所提EAI - Avatar框架在双向交互的情感感知头像生成方面表现良好。

Abstract: Generative models have advanced rapidly, enabling impressive talking head
generation that brings AI to life. However, most existing methods focus solely
on one-way portrait animation. Even the few that support bidirectional
conversational interactions lack precise emotion-adaptive capabilities,
significantly limiting their practical applicability. In this paper, we propose
EAI-Avatar, a novel emotion-aware talking head generation framework for dyadic
interactions. Leveraging the dialogue generation capability of large language
models (LLMs, e.g., GPT-4), our method produces temporally consistent virtual
avatars with rich emotional variations that seamlessly transition between
speaking and listening states. Specifically, we design a Transformer-based head
mask generator that learns temporally consistent motion features in a latent
mask space, capable of generating arbitrary-length, temporally consistent mask
sequences to constrain head motions. Furthermore, we introduce an interactive
talking tree structure to represent dialogue state transitions, where each tree
node contains information such as child/parent/sibling nodes and the current
character's emotional state. By performing reverse-level traversal, we extract
rich historical emotional cues from the current node to guide expression
synthesis. Extensive experiments demonstrate the superior performance and
effectiveness of our method.

</details>


### [210] [Interpolating Speaker Identities in Embedding Space for Data Expansion](https://arxiv.org/abs/2508.19210)
*Tianchi Liu,Ruijie Tao,Qiongqiong Wang,Yidi Jiang,Hardik B. Sailor,Ke Zhang,Jingru Lin,Haizhou Li*

Main category: eess.AS

TL;DR: 提出INSIDE数据扩展方法，通过插值合成新说话人身份数据，实验显示该方法在说话人验证和性别分类任务上有提升且兼容其他增强技术。


<details>
  <summary>Details</summary>
Motivation: 收集大规模多样说话人身份数据昂贵、有挑战且受隐私限制，需新的数据扩展方法。

Method: 从预训练说话人嵌入空间选相邻嵌入对，用球面线性插值计算中间嵌入，输入文本转语音系统生成波形，与原数据集结合训练模型。

Result: 用INSIDE扩展数据训练的模型在说话人验证上相对提升3.06% - 5.24%，在性别分类上相对提升13.44%。

Conclusion: INSIDE有效，适用于说话人验证和性别分类，且兼容其他增强技术，可灵活扩展现有训练流程。

Abstract: The success of deep learning-based speaker verification systems is largely
attributed to access to large-scale and diverse speaker identity data. However,
collecting data from more identities is expensive, challenging, and often
limited by privacy concerns. To address this limitation, we propose INSIDE
(Interpolating Speaker Identities in Embedding Space), a novel data expansion
method that synthesizes new speaker identities by interpolating between
existing speaker embeddings. Specifically, we select pairs of nearby speaker
embeddings from a pretrained speaker embedding space and compute intermediate
embeddings using spherical linear interpolation. These interpolated embeddings
are then fed to a text-to-speech system to generate corresponding speech
waveforms. The resulting data is combined with the original dataset to train
downstream models. Experiments show that models trained with INSIDE-expanded
data outperform those trained only on real data, achieving 3.06\% to 5.24\%
relative improvements. While INSIDE is primarily designed for speaker
verification, we also validate its effectiveness on gender classification,
where it yields a 13.44\% relative improvement. Moreover, INSIDE is compatible
with other augmentation techniques and can serve as a flexible, scalable
addition to existing training pipelines.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [211] [Is attention truly all we need? An empirical study of asset pricing in pretrained RNN sparse and global attention models](https://arxiv.org/abs/2508.19006)
*Shanyan Lai*

Main category: q-fin.PR

TL;DR: 研究预训练RNN注意力模型在美股资产定价中的应用，测试极端市场条件下模型稳定性，Self - att和Sparse - att模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习资产定价有局限，先进注意力模型存在未来数据泄露问题，需新模型用于资产定价研究。

Method: 研究主流注意力机制的预训练RNN注意力模型，在三个不同市场时期测试模型稳定性。

Result: Self - att和Sparse - att模型在获取绝对回报和对冲下行风险上能力强，新冠期间年化索提诺比率分别达2.0和1.80，Sparse - att模型更稳定。

Conclusion: 提出的注意力模型克服传统局限，为未来实证经济研究提供见解。

Abstract: This study investigates the pretrained RNN attention models with the
mainstream attention mechanisms such as additive attention, Luong's three
attentions, global self-attention (Self-att) and sliding window sparse
attention (Sparse-att) for the empirical asset pricing research on top 420
large-cap US stocks. This is the first paper on the large-scale
state-of-the-art (SOTA) attention mechanisms applied in the asset pricing
context. They overcome the limitations of the traditional machine learning (ML)
based asset pricing, such as mis-capturing the temporal dependency and short
memory. Moreover, the enforced causal masks in the attention mechanisms address
the future data leaking issue ignored by the more advanced attention-based
models, such as the classic Transformer. The proposed attention models also
consider the temporal sparsity characteristic of asset pricing data and
mitigate potential overfitting issues by deploying the simplified model
structures. This provides some insights for future empirical economic research.
All models are examined in three periods, which cover pre-COVID-19 (mild
uptrend), COVID-19 (steep uptrend with a large drawdown) and one year
post-COVID-19 (sideways movement with high fluctuations), for testing the
stability of these models under extreme market conditions. The study finds that
in value-weighted portfolio back testing, Model Self-att and Model Sparse-att
exhibit great capabilities in deriving the absolute returns and hedging
downside risks, while they achieve an annualized Sortino ratio of 2.0 and 1.80
respectively in the period with COVID-19. And Model Sparse-att performs more
stably than Model Self-att from the perspective of absolute portfolio returns
with respect to the size of stocks' market capitalization.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [212] [Scalable Fairness Shaping with LLM-Guided Multi-Agent Reinforcement Learning for Peer-to-Peer Electricity Markets](https://arxiv.org/abs/2508.18610)
*Shrenik Jadhav,Birva Sevak,Srijita Das,Akhtar Hussain,Wencong Su,Van-Hai Bui*

Main category: eess.SY

TL;DR: 提出公平感知多智能体强化学习框架FairMarket - RL用于P2P能源交易，实验表明其能促进本地交易、降低成本、保障公平并维持实用性，性能稳健。


<details>
  <summary>Details</summary>
Motivation: 现有市场和强化学习设计强调效率或私利，缺乏实时指导以确保不确定性下的公平结果。

Method: 提出FairMarket - RL框架，用大语言模型（LLM）批评家在部分可观测和离散价格 - 数量行动的连续双拍卖中塑造投标策略，LLM返回公平性分数并集成到奖励中，环境模拟现实负载和光伏曲线并实施硬约束。

Result: 该框架促进本地P2P交易，降低消费者成本，维持参与者公平性和公用事业可行性，敏感性分析显示性能稳健。

Conclusion: 该框架为去中心化电力市场提供了可扩展、由LLM引导的途径，兼具经济效率、社会公平和技术合理性。

Abstract: Peer-to-peer (P2P) energy trading is becoming central to modern distribution
systems as rooftop PV and home energy management systems become pervasive, yet
most existing market and reinforcement learning designs emphasize efficiency or
private profit and offer little real-time guidance to ensure equitable outcomes
under uncertainty. To address this gap, a fairness-aware multiagent
reinforcement learning framework, FairMarket-RL, is proposed in which a large
language model (LLM) critic shapes bidding policies within a continuous double
auction under partial observability and discrete price-quantity actions. After
each trading slot, the LLM returns normalized fairness scores Fairness-to-Grid
(FTG), Fairness-Between-Sellers (FBS), and Fairness-of-Pricing (FPP) that are
integrated into the reward via ramped coefficients and tunable scaling, so that
fairness guidance complements, rather than overwhelms, economic incentives. The
environment models realistic residential load and PV profiles and enforce hard
constraints on prices, physical feasibility, and policy-update stability.
Across a progression of experiments from a small pilot to a larger simulated
community and a mixed-asset real-world dataset, the framework shifts exchanges
toward local P2P trades, lowers consumer costs relative to grid-only
procurement, sustains strong fairness across participants, and preserves
utility viability. Sensitivity analyses over solar availability and aggregate
demand further indicate robust performance, suggesting a scalable, LLM-guided
pathway to decentralized electricity markets that are economically efficient,
socially equitable, and technically sound.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [213] [Do More Suspicious Transaction Reports Lead to More Convictions for Money Laundering?](https://arxiv.org/abs/2508.18932)
*Rasmus Ingemann Tuffveson Jensen,Sebastian Holmby Hansen,Kalle Johannes Rose*

Main category: econ.GN

TL;DR: 本文研究欧盟可疑交易报告与洗钱定罪之间的关系，发现两者遵循次线性幂律，但考虑时间因素后关系可能是虚假的，对国际反洗钱工作和政策有重要意义。


<details>
  <summary>Details</summary>
Motivation: 探究可疑交易报告与洗钱定罪之间的关系，以助于国际反洗钱工作和政策制定。

Method: 使用来自多个机构的公开数据，采用对数变换，拟合混合和固定效应回归模型。

Result: 初始结果显示两者遵循次线性幂律，但考虑时间作为控制变量后，固定效应模型中关系消失。

Conclusion: 可疑交易报告与洗钱定罪的关系可能是虚假而非因果的，不能统计性地预期增加报告能增加定罪。

Abstract: Almost all countries in the world require banks to report suspicious
transactions to national authorities. The reports are known as suspicious
transaction or activity reports (we use the former term) and are intended to
help authorities detect and prosecute money laundering. In this paper, we
investigate the relationship between suspicious transaction reports and
convictions for money laundering in the European Union. We use publicly
available data from Europol, the World Bank, the International Monetary Fund,
and the European Sourcebook of Crime and Criminal Justice Statistics. To
analyze the data, we employ a log-transformation and fit pooled (i.e., ordinary
least squares) and fixed effects regression models. The fixed effects models,
in particular, allow us to control for unobserved country-specific confounders
(e.g., different laws regarding when and how reports should be filed). Initial
results indicate that the number of suspicious transaction reports and
convictions for money laundering in a country follow a sub-linear power law.
Thus, while more reports may lead to more convictions, their marginal effect
decreases with their amount. The relationship is robust to control variables
such as the size of shadow economies and police forces. However, when we
include time as a control, the relationship disappears in the fixed effects
models. This suggests that the relationship is spurious rather than causal,
driven by cross-country differences and a common time trend. In turn, a country
cannot, ceteris paribus and with statistical confidence, expect that an
increase in suspicious transaction reports will drive an increase in
convictions.
  Our results have important implications for international anti-money
laundering efforts and policies. (...)

</details>


### [214] [From Coverage to Consequences: BMI, Health Behaviors, and Self-rated Health After Medicaid Contraction](https://arxiv.org/abs/2508.19155)
*Md Twfiqur Rahman*

Main category: econ.GN

TL;DR: 本文利用田纳西州2005年医疗补助收缩政策，研究失去公共医保对体重和健康行为的影响，发现会导致体重指数上升等，还给出单处理组有限预处理数据下的稳健推断指导。


<details>
  <summary>Details</summary>
Motivation: 研究失去公共健康保险对体重和相关健康行为的影响。

Method: 使用1997 - 2010年行为风险因素监测系统（BRFSS）数据，估计合成双重差分模型。

Result: 改革使田纳西州无子女成年人的体重指数增加0.38点，超重或肥胖患病率提高约4%；报告“健康状况差”的无子女成年人比例增加21%，医疗补助报销的止痛和消炎药使用减少，适度体育活动参与减少。

Conclusion: 未得到妥善管理的健康状况恶化可能是医保覆盖丧失影响体重增加的关键途径；分析为单处理组有限预处理数据下的稳健推断提供了实用指导。

Abstract: Leveraging Tennessee's 2005 Medicaid contraction, I study the impact of
losing public health insurance on body weight and relevant health behaviors.
Using Behavioral Risk Factor Surveillance System (BRFSS) data from 1997 to
2010, I estimate synthetic difference-in-differences models. The estimates
suggest that the reform increased Body Mass Index by 0.38 points and the
overweight or obesity prevalence (BMI$\geq$25) by $\sim$4\% among Tennessean
childless adults. My findings -- a 21\% increase in the share of childless
adults reporting ``poor'' health status (the lowest level on the five-point
scale), a reduction in Medicaid-reimbursed utilization of pain and
anti-inflammatory medications, and a reduction in participation in moderate
physical activities -- suggest that worsening unmanaged health conditions may
be a key pathway through which coverage loss affected weight gain.
Additionally, my analysis offers practical guidance for conducting robust
inference in single treated cluster settings with limited pre-treatment data.

</details>


### [215] [Profit-Aware Graph Framework for Cross-Platform Ride-Sharing: Analyzing Allocation Mechanisms and Efficiency Gains](https://arxiv.org/abs/2508.19192)
*Xin Dong,Jose Ventura,Vikash V. Gayah*

Main category: econ.GN

TL;DR: 研究提出图论框架促进跨平台拼车，评估三种分配方案，Shapley值机制表现最佳，系统效率和服务质量随需求上升有规模经济效应。


<details>
  <summary>Details</summary>
Motivation: 打车平台碎片化阻碍系统效率，需公平可持续的利润分配机制实现跨平台合作以提升效率。

Method: 引入嵌入利润约束的图论框架进行网络优化，通过大规模仿真评估三种分配方案。

Result: Shapley值机制在六项关键指标上始终优于其他方案，系统效率和服务质量随需求增加而提升。

Conclusion: Shapley值机制是更优的跨平台拼车利润分配机制，可从骑手请求图结构理解规模经济及其收益递减。

Abstract: Ride-hailing platforms (e.g., Uber, Lyft) have transformed urban mobility by
enabling ride-sharing, which holds considerable promise for reducing both
travel costs and total vehicle miles traveled (VMT). However, the fragmentation
of these platforms impedes system-wide efficiency by restricting ride-matching
to intra-platform requests. Cross-platform collaboration could unlock
substantial efficiency gains, but its realization hinges on fair and
sustainable profit allocation mechanisms that can align the incentives of
competing platforms. This study introduces a graph-theoretic framework that
embeds profit-aware constraints into network optimization, facilitating
equitable and efficient cross-platform ride-sharing. Within this framework, we
evaluate three allocation schemes -- equal-profit-based, market-share-based,
and Shapley-value-based -- through large-scale simulations. Results show that
the Shapley-value-based mechanism consistently outperforms the alternatives
across six key metrics. Notably, system efficiency and rider service quality
improve with increasing demand, reflecting clear economies of scale. The
observed economies of scale, along with their diminishing returns, can be
understood with the structural evolution of rider-request graphs, where
super-linear edge growth expands feasible matches and sub-linear degree scaling
limits per-rider connectivity.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [216] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

TL;DR: 本文研究大语言模型在多智能体系统中形成信任、抵抗错误信息等能力，提出KAIROS基准，评估多种缓解策略，发现GRPO表现最佳但对社会影响鲁棒性降低。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注从众偏差，本文旨在分析大语言模型在复杂社会动态下形成信任、抵抗错误信息和整合同伴输入等因素，以实现集体智能。

Method: 提出KAIROS基准模拟测验竞赛，控制不同条件，让大语言模型接收历史交互和当前同伴响应；评估提示、监督微调、强化学习GRPO等缓解策略。

Result: GRPO结合多智能体上下文、基于结果的奖励和无约束推理总体表现最佳，但相比基础模型对社会影响的鲁棒性降低。

Conclusion: 通过KAIROS基准研究大语言模型在多智能体系统中的表现，GRPO有较好性能但存在一定问题，代码和数据集已公开。

Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [217] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

TL;DR: 提出backprompting方法生成类生产的标注数据用于健康建议护栏开发，结合稀疏人工聚类技术标注数据，在健康建议识别护栏中测试有改进。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在企业应用带来风险，护栏技术需开发维护健壮的检测器，但获取真实大语言模型输出的生产级标注数据困难。

Method: 提出backprompting方法生成类生产标注数据，结合稀疏人工聚类技术标注，构建平行语料，注入现有数据集生成检测器训练数据。

Result: 在识别大语言模型输出健康建议这一困难且微妙的护栏任务中测试，检测器优于其他方案，参数少400倍时性能比GPT - 4o高3.73%。

Conclusion: 提出的方法能有效生成用于护栏检测器的训练数据，提升检测性能。

Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [218] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

TL;DR: 提出潜隐自一致性（LSC）方法，用可学习的词嵌入选择语义最一致的响应，在多个推理基准上表现优于其他方法，计算开销小且能提供校准良好的置信度估计。


<details>
  <summary>Details</summary>
Motivation: 概率解码使大语言模型输出不一致，现有自一致性方法在短形式或长形式问答上有局限性，需要改进。

Method: 引入LSC，使用可学习的词嵌入选择最语义一致的响应，通过轻量级的摘要词前向生成。

Result: 在6个短形式和5个长形式推理基准上，LSC平均优于SC、USC和WUCS，计算开销可忽略不计，且能提供校准良好的置信度估计。

Conclusion: LSC是一种实用的一致性选择方法，可跨答案格式可靠工作。

Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [219] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 本文质疑基于OOD评估模型泛化能力的假设，发现QA中不同OOD评估数据集质量差异大，强调常用OOD评估的局限性并给出评估建议。


<details>
  <summary>Details</summary>
Motivation: 挑战现有基于OOD评估模型泛化能力的假设，即OOD评估能反映实际部署中可能的失败。

Method: 将OOD评估结果与现有问答模型的特定失败模式进行对比。

Result: 不同QA的OOD评估数据集对模型抗捷径能力的估计质量差异大，部分表现不如简单的分布内评估，原因包括ID+OOD数据集共享虚假捷径及数据集训练和评估质量脱节。

Conclusion: 强调常用基于OOD的泛化评估存在局限性，并提供更稳健评估泛化能力的方法和建议。

Abstract: A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [220] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

TL;DR: 本文分析不同训练方法对大语言模型重排序任务语义理解的影响，用环境与地球科学领域小数据集重排内容并分析可解释性信息。


<details>
  <summary>Details</summary>
Motivation: 大语言模型语义理解能力提升但透明度下降，深入理解其内部运作和解决新系统重排序挑战的需求。

Method: 利用环境与地球科学领域相对小的排序数据集对检索内容进行重排序，并分析可解释性信息。

Result: 发现一些训练方法比其他方法有更好的可解释性。

Conclusion: 需分析不同训练方法对大语言模型重排序任务语义理解的影响，探究模型能否生成更具信息的文本推理以克服透明度和数据有限的挑战。

Abstract: With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [221] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

TL;DR: 提出用多重测试启发的方法检测大语言模型幻觉，实验验证其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易产生幻觉，需有效检测方法。

Method: 将幻觉检测问题转化为假设检验问题，借鉴机器学习模型的分布外检测问题，提出多重测试启发的方法。

Result: 实验结果验证了该方法相对现有方法的鲁棒性。

Conclusion: 所提方法可有效检测大语言模型的幻觉，且具有鲁棒性。

Abstract: While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [222] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
*Colin Klein*

Main category: cs.CL

TL;DR: 探讨大语言模型建模对象，论证其是训练语料模型，提及变压器架构处理格式，结合相关研究说明，认为并非贬损性观点，指出语言是话语机器，人类和大语言模型使用方式不同。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型实际建模的对象，判断其反映人类能力还是训练语料。

Method: 对比人类语言能力依赖的超线性计算格式和变压器架构支持的线性处理格式，依据变压器计算架构不变性进行论证，结合Liu等人关于捷径自动机的研究。

Result: 给出大语言模型是训练语料模型的非贬损性辩护。

Conclusion: 认为此观点并非贬损性，语言是话语机器，人类和大语言模型使用语言的方式不同。

Abstract: What do large language models actually model? Do they tell us something about
human capacities, or are they models of the corpus we've trained them on? I
give a non-deflationary defence of the latter position. Cognitive science tells
us that linguistic capabilities in humans rely supralinear formats for
computation. The transformer architecture, by contrast, supports at best a
linear formats for processing. This argument will rely primarily on certain
invariants of the computational architecture of transformers. I then suggest a
positive story about what transformers are doing, focusing on Liu et al.
(2022)'s intriguing speculations about shortcut automata. I conclude with why I
don't think this is a terribly deflationary story. Language is not (just) a
means for expressing inner state but also a kind of 'discourse machine' that
lets us make new language given appropriate context. We have learned to use
this technology in one way; LLMs have also learned to use it too, but via very
different means.

</details>


### [223] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
*Chenxi Zhou,Pengfei Cao,Jiang Li,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 本文通过实证研究建立任务分层缩放定律，揭示大语言模型知识记忆和利用能力对量化参数的不同敏感性，为开发知识感知量化策略提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对后训练量化（PTQ）如何影响大语言模型知识能力的全面理解，且量化模型的缩放定律常忽略关键的PTQ特定参数和任务特定敏感性。

Method: 进行广泛的实证研究，将大语言模型知识分解为记忆和利用能力，开发统一的定量框架，纳入模型大小、有效位宽、校准集大小和组大小等参数。

Result: 发现知识记忆对有效位宽、校准集大小和模型大小的变化比知识利用更敏感。

Conclusion: 研究结果有助于精细理解PTQ的影响，为开发能更好保留目标认知功能的知识感知量化策略提供指导。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their scale, with post-training quantization (PTQ) emerging as a practical
compression solution. However, a comprehensive understanding of how PTQ
precisely impacts diverse LLM knowledge capabilities remains elusive, and
existing scaling laws for quantized models often overlook crucial PTQ-specific
parameters and task-specific sensitivities. This paper addresses these gaps by
conducting an extensive empirical investigation to establish task-stratified
scaling laws. We disentangle LLM knowledge into memorization and utilization
capabilities and develop a unified quantitative framework that incorporates
model size, effective bit-width, calibration set size, and group size. Our
central finding reveals that knowledge memorization exhibits markedly greater
sensitivity to variations in effective bit-width, calibration set size, and
model size compared to the more robust knowledge utilization. These findings
offer a fine-grained understanding of PTQ's impact and provide guidance for
developing knowledge-aware quantization strategies that can better preserve
targeted cognitive functions.

</details>


### [224] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
*Chenxu Yang,Qingyi Si,Zheng Lin*

Main category: cs.CL

TL;DR: 提出Collaborative Decoding (CoDe)方法打破大语言模型忠实性和表达性的权衡，实验验证其有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型难以在整合知识时兼顾忠实性和表达性，输出存在缺乏知识支持或过于冗长不自然的问题。

Method: 提出CoDe方法，动态整合有和无外部知识生成的输出概率，引入知识感知重排序机制。

Result: CoDe框架在不同大语言模型和评估指标上提升忠实性且不损害表达性。

Conclusion: CoDe框架有效且具有泛化性。

Abstract: Grounding responses in external knowledge represents an effective strategy
for mitigating hallucinations in Large Language Models (LLMs). However, current
LLMs struggle to seamlessly integrate knowledge while simultaneously
maintaining faithfulness (or fidelity) and expressiveness, capabilities that
humans naturally possess. This limitation results in outputs that either lack
support from external knowledge, thereby compromising faithfulness, or appear
overly verbose and unnatural, thus sacrificing expressiveness. In this work, to
break the trade-off between faithfulness and expressiveness, we propose
Collaborative Decoding (CoDe), a novel approach that dynamically integrates
output probabilities generated with and without external knowledge. This
integration is guided by distribution divergence and model confidence, enabling
the selective activation of relevant and reliable expressions from the model's
internal parameters. Furthermore, we introduce a knowledge-aware reranking
mechanism that prevents over-reliance on prior parametric knowledge while
ensuring proper utilization of provided external information. Through
comprehensive experiments, our plug-and-play CoDe framework demonstrates
superior performance in enhancing faithfulness without compromising
expressiveness across diverse LLMs and evaluation metrics, validating both its
effectiveness and generalizability.

</details>


### [225] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 现有多模态思维链提示示例选择方法不佳，本文提出基于因材施教原则的框架，融合两种信号制定采样策略，实验证明该方法有效提升多模态推理。


<details>
  <summary>Details</summary>
Motivation: 现有随机或手动选择示例的多模态思维链提示方法，未考虑模型知识分布和任务内在复杂性，导致模型性能欠佳且不稳定。

Method: 将提示选择重构为提示课程设计问题，整合模型感知难度和内在样本复杂度两种信号，开发难度平衡采样策略。

Result: 在五个具有挑战性的基准测试和多个流行多模态大语言模型上的实验表明，方法带来显著且一致的性能提升，大幅减少随机采样导致的性能差异。

Conclusion: 该方法为增强多模态推理提供了一种有原则且稳健的途径。

Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


### [226] [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)
*Chang Wang,Siyu Yan,Depeng Yuan,Yuqi Chen,Yanhua Huang,Yuanhang Zheng,Shuhao Li,Yinqi Zhang,Kedi Chen,Mingrui Zhu,Ruiwen Xu*

Main category: cs.CL

TL;DR: 提出DIVER框架优化广告标题生成，兼顾质量与多样性，实验证明有效提升ADVV和CTR。


<details>
  <summary>Details</summary>
Motivation: 当前广告标题生成方法多注重质量或CTR，忽略多样性，导致输出同质化。

Method: 设计语义和风格感知的数据生成管道，提出多阶段多目标优化框架，结合监督微调与强化学习。

Result: 在真实工业数据集实验表明DIVER有效平衡质量与多样性，在大规模平台上使ADVV提升4.0%，CTR提升1.4%。

Conclusion: DIVER框架能有效解决广告标题生成中质量与多样性平衡问题，提升广告效果。

Abstract: The generation of ad headlines plays a vital role in modern advertising,
where both quality and diversity are essential to engage a broad range of
audience segments. Current approaches primarily optimize language models for
headline quality or click-through rates (CTR), often overlooking the need for
diversity and resulting in homogeneous outputs. To address this limitation, we
propose DIVER, a novel framework based on large language models (LLMs) that are
jointly optimized for both diversity and quality. We first design a semantic-
and stylistic-aware data generation pipeline that automatically produces
high-quality training pairs with ad content and multiple diverse headlines. To
achieve the goal of generating high-quality and diversified ad headlines within
a single forward pass, we propose a multi-stage multi-objective optimization
framework with supervised fine-tuning (SFT) and reinforcement learning (RL).
Experiments on real-world industrial datasets demonstrate that DIVER
effectively balances quality and diversity. Deployed on a large-scale
content-sharing platform serving hundreds of millions of users, our framework
improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.

</details>


### [227] [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)
*Qiao Liang,Ying Shen,Tiantian Chen,Lin Zhang*

Main category: cs.CL

TL;DR: 本文引入多模态多场景数据集MECAD，并提出M3HG模型用于多模态对话情感原因三元组提取，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对话情感原因三元组提取领域相关数据集稀缺且场景单一，现有方法未显式建模情感和因果上下文，忽略不同层次语义信息融合。

Method: 引入多模态多场景数据集MECAD，提出通过多模态异构图显式捕获情感和因果上下文、有效融合语句间和语句内上下文信息的M3HG模型。

Result: 实验表明M3HG与现有最先进方法相比更有效。

Conclusion: M3HG模型在多模态对话情感原因三元组提取任务中表现良好，相关代码和数据集已开源。

Abstract: Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has
recently gained significant attention in social media analysis, aiming to
extract emotion utterances, cause utterances, and emotion categories
simultaneously. However, the scarcity of related datasets, with only one
published dataset featuring highly uniform dialogue scenarios, hinders model
development in this field. To address this, we introduce MECAD, the first
multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56
TV series spanning a wide range of dialogue contexts. In addition, existing
MECTEC methods fail to explicitly model emotional and causal contexts and
neglect the fusion of semantic information at different levels, leading to
performance degradation. In this paper, we propose M3HG, a novel model that
explicitly captures emotional and causal contexts and effectively fuses
contextual information at both inter- and intra-utterance levels via a
multimodal heterogeneous graph. Extensive experiments demonstrate the
effectiveness of M3HG compared with existing state-of-the-art methods. The
codes and dataset are available at https://github.com/redifinition/M3HG.

</details>


### [228] [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)
*Yilin Li,Xunjian Yin,Yilin Chen,Xiaojun Wan*

Main category: cs.CL

TL;DR: 文章提出基于Rule - Based RL的新框架用于语法错误纠正，在中文数据集实验中取得SOTA表现，凸显用RL引导大模型的优势。


<details>
  <summary>Details</summary>
Motivation: 传统编码器 - 解码器模型虽在语法错误纠正任务有一定成果，但大语言模型在该领域应用不足，且当前研究依赖监督微调限制了模型推理能力。

Method: 提出基于Rule - Based RL的新框架。

Result: 在中文数据集实验中，Rule - Based RL框架取得了SOTA性能，召回率显著提高。

Conclusion: 使用强化学习引导大语言模型具有优势，为语法错误纠正未来发展提供更可控可靠的范式。

Abstract: Grammatical error correction is a significant task in NLP. Traditional
methods based on encoder-decoder models have achieved certain success, but the
application of LLMs in this field is still underexplored. Current research
predominantly relies on supervised fine-tuning to train LLMs to directly
generate the corrected sentence, which limits the model's powerful reasoning
ability. To address this limitation, we propose a novel framework based on
Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL
framework achieves \textbf{state-of-the-art }performance, with a notable
increase in \textbf{recall}. This result clearly highlights the advantages of
using RL to steer LLMs, offering a more controllable and reliable paradigm for
future development in GEC.

</details>


### [229] [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 提出基于进化算法的自动提示方法ReflectivePrompt，在33个数据集测试中表现优于当前方法。


<details>
  <summary>Details</summary>
Motivation: 随着提示工程发展，需更优自动提示方法为大语言模型选择优化提示。

Method: 采用反射进化方法，在交叉和精英变异前进行短期和长期反射操作，积累进化知识并更新。

Result: 在33个数据集测试，使用t-lite-instruct-0.1和gemma3-27b-it模型，平均指标显著提升，如在BBH上比EvoPrompt高28%。

Conclusion: ReflectivePrompt是基于进化算法的自动提示中最有效解决方案之一。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which has been gaining popularity with the rapid advancement
of prompt engineering, driven by extensive research in the field of large
language models (LLMs). This paper presents ReflectivePrompt - a novel
autoprompting method based on evolutionary algorithms that employs a reflective
evolution approach for more precise and comprehensive search of optimal
prompts. ReflectivePrompt utilizes short-term and long-term reflection
operations before crossover and elitist mutation to enhance the quality of the
modifications they introduce. This method allows for the accumulation of
knowledge obtained throughout the evolution process and updates it at each
epoch based on the current population. ReflectivePrompt was tested on 33
datasets for classification and text generation tasks using open-access large
language models: t-lite-instruct-0.1 and gemma3-27b-it. The method
demonstrates, on average, a significant improvement (e.g., 28% on BBH compared
to EvoPrompt) in metrics relative to current state-of-the-art approaches,
thereby establishing itself as one of the most effective solutions in
evolutionary algorithm-based autoprompting.

</details>


### [230] [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)
*Yibo Li,Miao Xiong,Jiaying Wu,Bryan Hooi*

Main category: cs.CL

TL;DR: 提出ConfTuner微调方法校准大语言模型置信度，提升校准效果并促进可信LLM系统发展。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在高风险领域存在过度自信问题，现有校准方法效果和泛化性有限。

Method: 引入ConfTuner，基于新的损失函数tokenized Brier score进行微调，无需真实置信度分数或代理置信度估计。

Result: ConfTuner在不同推理任务上改善校准效果，可泛化到GPT - 4o，更好校准的置信度能促进下游自校正和模型级联。

Conclusion: ConfTuner是一种简单高效的微调方法，有助于推动可信LLM系统的发展。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains
such as science, law, and healthcare, where accurate expressions of uncertainty
are essential for reliability and trust. However, current LLMs are often
observed to generate incorrect answers with high confidence, a phenomenon known
as "overconfidence". Recent efforts have focused on calibrating LLMs'
verbalized confidence: i.e., their expressions of confidence in text form, such
as "I am 80% confident that...". Existing approaches either rely on prompt
engineering or fine-tuning with heuristically generated uncertainty estimates,
both of which have limited effectiveness and generalizability. Motivated by the
notion of proper scoring rules for calibration in classical machine learning
models, we introduce ConfTuner, a simple and efficient fine-tuning method that
introduces minimal overhead and does not require ground-truth confidence scores
or proxy confidence estimates. ConfTuner relies on a new loss function,
tokenized Brier score, which we theoretically prove to be a proper scoring
rule, intuitively meaning that it "correctly incentivizes the model to report
its true probability of being correct". ConfTuner improves calibration across
diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our
results further show that better-calibrated confidence enables downstream gains
in self-correction and model cascade, advancing the development of trustworthy
LLM systems. The code is available at
https://github.com/liushiliushi/ConfTuner.

</details>


### [231] [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)
*Hung Ming Liu*

Main category: cs.CL

TL;DR: 提出让神经网络模型发展AI母语的框架，有互补训练目标和顺序专业化策略，实验证明有竞争力的准确性和可验证推理痕迹。


<details>
  <summary>Details</summary>
Motivation: 使神经网络模型同时具备直观推理、组合符号链和内在可解释性。

Method: 将推理直接嵌入模型表示，引入互补训练目标，采用顺序专业化策略。

Result: 在AI任务实验中展现出有竞争力的准确性和可验证的推理痕迹。

Conclusion: AI母语可作为神经网络模型可解释性、直觉和符号推理的统一机制。

Abstract: We present a framework where neural models develop an AI Mother Tongue, a
native symbolic language that simultaneously supports intuitive reasoning,
compositional symbol chains, and inherent interpretability. Unlike post-hoc
explanation methods, our approach embeds reasoning directly into the model's
representations: symbols capture meaningful semantic patterns, chains trace
decision paths, and gated induction mechanisms guide selective focus, yielding
transparent yet flexible reasoning. We introduce complementary training
objectives to enhance symbol purity and decision sparsity, and employ a
sequential specialization strategy to first build broad symbolic competence and
then refine intuitive judgments. Experiments on AI tasks demonstrate
competitive accuracy alongside verifiable reasoning traces, showing that AI
Mother Tongue can serve as a unified mechanism for interpretability, intuition,
and symbolic reasoning in neural models.

</details>


### [232] [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 本文提出基于大语言模型的自动提示方法DistillPrompt，在文本分类和生成任务上测试，结果显示比现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型领域研究推动提示工程发展，自动提示方法受关注，需更好的自动提示方法。

Method: 提出DistillPrompt方法，利用训练数据将特定任务信息多阶段集成到提示中，运用蒸馏、压缩和聚合操作探索提示空间。

Result: 在文本分类和生成任务不同数据集上测试，使用t - lite - instruct - 0.1语言模型，关键指标比现有方法有显著平均提升，如比Grips在全数据集提升20.12%。

Conclusion: DistillPrompt是自动提示中最有效的非梯度方法之一。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which is gaining popularity due to the rapid development of
prompt engineering driven by extensive research in the field of large language
models (LLMs). This paper presents DistillPrompt -- a novel autoprompting
method based on large language models that employs a multi-stage integration of
task-specific information into prompts using training data. DistillPrompt
utilizes distillation, compression, and aggregation operations to explore the
prompt space more thoroughly. The method was tested on different datasets for
text classification and generation tasks using the t-lite-instruct-0.1 language
model. The results demonstrate a significant average improvement (e.g., 20.12%
across the entire dataset compared to Grips) in key metrics over existing
methods in the field, establishing DistillPrompt as one of the most effective
non-gradient approaches in autoprompting.

</details>


### [233] [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)
*Ilias Driouich,Hongliu Cao,Eoin Thomas*

Main category: cs.CL

TL;DR: 本文介绍一种用于RAG评估的合成QA数据集生成的多智能体框架，实验表明评估集在多样性上表现好且有强隐私保护能力，为RAG系统评估提供新途径。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统评估主要关注性能指标，对评估数据集设计和质量关注不足，而其对评估至关重要。

Method: 引入多智能体框架，包括利用聚类技术的多样性智能体、检测和屏蔽敏感信息的隐私智能体、合成QA对的QA策展智能体。

Result: 评估集在多样性上优于基线方法，在特定领域数据集上实现强大的隐私屏蔽。

Conclusion: 为RAG系统评估提供了实用且符合道德的途径，为未来符合AI法规和标准的改进奠定基础。

Abstract: Retrieval-augmented generation (RAG) systems improve large language model
outputs by incorporating external knowledge, enabling more informed and
context-aware responses. However, the effectiveness and trustworthiness of
these systems critically depends on how they are evaluated, particularly on
whether the evaluation process captures real-world constraints like protecting
sensitive information. While current evaluation efforts for RAG systems have
primarily focused on the development of performance metrics, far less attention
has been given to the design and quality of the underlying evaluation datasets,
despite their pivotal role in enabling meaningful, reliable assessments. In
this work, we introduce a novel multi-agent framework for generating synthetic
QA datasets for RAG evaluation that prioritize semantic diversity and privacy
preservation. Our approach involves: (1) a Diversity agent leveraging
clustering techniques to maximize topical coverage and semantic variability,
(2) a Privacy Agent that detects and mask sensitive information across multiple
domains and (3) a QA curation agent that synthesizes private and diverse QA
pairs suitable as ground truth for RAG evaluation. Extensive experiments
demonstrate that our evaluation sets outperform baseline methods in diversity
and achieve robust privacy masking on domain-specific datasets. This work
offers a practical and ethically aligned pathway toward safer, more
comprehensive RAG system evaluation, laying the foundation for future
enhancements aligned with evolving AI regulations and compliance standards.

</details>


### [234] [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)
*Ziyue Li,Yuan Chang,Gaihong Yu,Xiaoqiu Le*

Main category: cs.CL

TL;DR: 大语言模型代理在复杂长程规划任务中表现不佳，本文提出HiPlan分层规划框架解决此问题，实验表明其效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理在复杂长程规划场景中存在缺乏宏观指导和执行监督不足的问题。

Method: 提出HiPlan分层规划框架，将复杂任务分解为里程碑行动指南和逐步提示，离线阶段构建里程碑库，执行阶段动态调整轨迹段。

Result: 在两个具有挑战性的基准测试中HiPlan显著优于强基线，消融实验验证分层组件互补优势。

Conclusion: HiPlan框架能有效提升大语言模型代理的决策能力。

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in decision-making tasks, but struggle significantly with complex,
long-horizon planning scenarios. This arises from their lack of macroscopic
guidance, causing disorientation and failures in complex tasks, as well as
insufficient continuous oversight during execution, rendering them unresponsive
to environmental changes and prone to deviations. To tackle these challenges,
we introduce HiPlan, a hierarchical planning framework that provides adaptive
global-local guidance to boost LLM-based agents'decision-making. HiPlan
decomposes complex tasks into milestone action guides for general direction and
step-wise hints for detailed actions. During the offline phase, we construct a
milestone library from expert demonstrations, enabling structured experience
reuse by retrieving semantically similar tasks and milestones. In the execution
phase, trajectory segments from past milestones are dynamically adapted to
generate step-wise hints that align current observations with the milestone
objectives, bridging gaps and correcting deviations. Extensive experiments
across two challenging benchmarks demonstrate that HiPlan substantially
outperforms strong baselines, and ablation studies validate the complementary
benefits of its hierarchical components.

</details>


### [235] [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)
*Zhiliang Peng,Jianwei Yu,Wenhui Wang,Yaoyao Chang,Yutao Sun,Li Dong,Yi Zhu,Weijiang Xu,Hangbo Bao,Zehua Wang,Shaohan Huang,Yan Xia,Furu Wei*

Main category: cs.CL

TL;DR: 介绍了VibeVoice模型，通过下一个令牌扩散合成多说话者长语音，其分词器提升压缩率和效率，能合成90分钟长语音且表现出色。


<details>
  <summary>Details</summary>
Motivation: 设计一个能合成多说话者长语音的模型。

Method: 采用下一个令牌扩散方法，引入新型连续语音分词器。

Result: VibeVoice能合成最长90分钟、最多4个说话者的长语音，捕捉真实对话氛围，超越开源和专有对话模型。

Conclusion: VibeVoice模型在多说话者长语音合成方面表现优秀，其分词器提升了数据压缩和处理效率。

Abstract: This report presents VibeVoice, a novel model designed to synthesize
long-form speech with multiple speakers by employing next-token diffusion,
which is a unified method for modeling continuous data by autoregressively
generating latent vectors via diffusion. To enable this, we introduce a novel
continuous speech tokenizer that, when compared to the popular Encodec model,
improves data compression by 80 times while maintaining comparable performance.
The tokenizer effectively preserves audio fidelity while significantly boosting
computational efficiency for processing long sequences. Thus, VibeVoice can
synthesize long-form speech for up to 90 minutes (in a 64K context window
length) with a maximum of 4 speakers, capturing the authentic conversational
``vibe'' and surpassing open-source and proprietary dialogue models.

</details>


### [236] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
*Jiaqi Chen,Yanzhe Zhang,Yutong Zhang,Yijia Shao,Diyi Yang*

Main category: cs.CL

TL;DR: 论文提出语言模型生成式界面范式，评估显示其优于传统聊天界面，明确用户偏好原因。


<details>
  <summary>Details</summary>
Motivation: 多数大语言模型系统受限于线性请求 - 响应格式，在多轮、信息密集和探索性任务中交互效率低。

Method: 提出生成式界面范式，利用特定结构表示和迭代细化将用户查询转化为特定任务 UI；引入多维评估框架对比生成式界面和传统聊天界面。

Result: 生成式界面始终优于对话式界面，超 70% 情况下人类更喜欢生成式界面。

Conclusion: 研究明确了用户何时以及为何偏好生成式界面，为未来人机交互发展铺平道路。

Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots,
and consultants, capable of supporting a wide range of tasks through natural
conversation. However, most systems remain constrained by a linear
request-response format that often makes interactions inefficient in
multi-turn, information-dense, and exploratory tasks. To address these
limitations, we propose Generative Interfaces for Language Models, a paradigm
in which LLMs respond to user queries by proactively generating user interfaces
(UIs) that enable more adaptive and interactive engagement. Our framework
leverages structured interface-specific representations and iterative
refinements to translate user queries into task-specific UIs. For systematic
evaluation, we introduce a multidimensional assessment framework that compares
generative interfaces with traditional chat-based ones across diverse tasks,
interaction patterns, and query types, capturing functional, interactive, and
emotional aspects of user experience. Results show that generative interfaces
consistently outperform conversational ones, with humans preferring them in
over 70% of cases. These findings clarify when and why users favor generative
interfaces, paving the way for future advancements in human-AI interaction.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [237] [H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems](https://arxiv.org/abs/2508.18295)
*Huangyu Dai,Lingtao Mao,Ben Chen,Zihan Wang,Zihan Liang,Ying Han,Chenyi Lei,Han Li*

Main category: cs.SD

TL;DR: 本文提出一种新的热词定制系统，能提升热词后召回率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型处理大规模热词时识别率会随热词数量增加而大幅下降，需要改进热词定制系统。

Method: 引入热词预检索模块（H - PRM），通过测量热词与语音片段的声学相似度识别最相关热词候选，可集成到传统模型，也通过基于提示的方法融入音频大语言模型。

Result: 经过广泛测试，H - PRM能显著提升热词后召回率（PRR），表现优于现有方法。

Conclusion: H - PRM为自动语音识别中的热词定制展示了新方向。

Abstract: Hotword customization is crucial in ASR to enhance the accuracy of
domain-specific terms. It has been primarily driven by the advancements in
traditional models and Audio large language models (LLMs). However, existing
models often struggle with large-scale hotwords, as the recognition rate drops
dramatically with the number of hotwords increasing. In this paper, we
introduce a novel hotword customization system that utilizes a hotword
pre-retrieval module (H-PRM) to identify the most relevant hotword candidate by
measuring the acoustic similarity between the hotwords and the speech segment.
This plug-and-play solution can be easily integrated into traditional models
such as SeACo-Paraformer, significantly enhancing hotwords post-recall rate
(PRR). Additionally, we incorporate H-PRM into Audio LLMs through a
prompt-based approach, enabling seamless customization of hotwords. Extensive
testing validates that H-PRM can outperform existing methods, showing a new
direction for hotword customization in ASR.

</details>


### [238] [SwiftF0: Fast and Accurate Monophonic Pitch Detection](https://arxiv.org/abs/2508.18440)
*Lars Nieradzik*

Main category: cs.SD

TL;DR: 提出轻量级模型SwiftF0用于单音音高估计，引入SpeechSynth数据集、统一指标及基准套件，性能优且适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 解决嘈杂环境及资源受限设备上准确实时单音音高估计的难题，以及语音语料中缺乏准确真实音高的问题。

Method: 在多样数据集上训练SwiftF0并进行数据增强，引入SpeechSynth数据集，提出统一评估指标。

Result: SwiftF0在10 dB SNR下谐波均值达91.80%，超CREPE 12个百分点，参数少且运行快；SpeechSynth数据集助力模型训练评估。

Conclusion: SwiftF0性能优异，适合实时高效部署，相关数据集、指标和基准套件有助于音高估计研究。

Abstract: Accurate and real-time monophonic pitch estimation in noisy conditions,
particularly on resource-constrained devices, remains an open challenge in
audio processing. We present \emph{SwiftF0}, a novel, lightweight neural model
that sets a new state-of-the-art for monophonic pitch estimation. Through
training on diverse speech, music, and synthetic datasets with extensive data
augmentation, SwiftF0 achieves robust generalization across acoustic domains
while maintaining computational efficiency. SwiftF0 achieves a 91.80\% harmonic
mean (HM) at 10 dB SNR, outperforming baselines like CREPE by over 12
percentage points and degrading by only 2.3 points from clean audio. SwiftF0
requires only 95,842 parameters and runs approximately 42x faster than CREPE on
CPU, making it ideal for efficient, real-time deployment. To address the
critical lack of perfectly accurate ground truth pitch in speech corpora (which
typically rely on algorithmic estimators or laryngograph signals), we introduce
\emph{SpeechSynth}. This synthetic speech dataset, generated by a phoneme-level
TTS model, provides exact, on-demand ground-truth pitch curves, enabling more
robust model training and evaluation. Furthermore, we propose a unified metric,
combining six complementary performance measures for comprehensive and reliable
pitch evaluation, and release an open-source pitch benchmark suite. A live demo
of SwiftF0 is available at https://swift-f0.github.io/, the source code at
https://github.com/lars76/swift-f0, and the benchmark framework at
https://github.com/lars76/pitch-benchmark.

</details>


### [239] [Cross-Learning Fine-Tuning Strategy for Dysarthric Speech Recognition Via CDSD database](https://arxiv.org/abs/2508.18732)
*Qing Xiao,Yingshan Peng,PeiPei Zhang*

Main category: cs.SD

TL;DR: 传统方法针对每位患者微调预训练的ASR模型，实验表明多说话人微调能提升个体语音模式识别效果，降低WER。


<details>
  <summary>Details</summary>
Motivation: 解决构音障碍语音识别中因严重程度变化和与正常语音差异带来的挑战。

Method: 采用多说话人微调（同时对多个构音障碍说话人）的策略。

Result: 多说话人微调策略能通过学习更广泛的病理特征增强泛化能力，减轻特定说话人过拟合，降低对每位患者数据的依赖，相比单说话人微调，目标说话人准确率提高，WER最多降低13.15%。

Conclusion: 多说话人微调策略在构音障碍语音识别中优于单说话人微调策略。

Abstract: Dysarthric speech recognition faces challenges from severity variations and
disparities relative to normal speech. Conventional approaches individually
fine-tune ASR models pre-trained on normal speech per patient to prevent
feature conflicts. Counter-intuitively, experiments reveal that multi-speaker
fine-tuning (simultaneously on multiple dysarthric speakers) improves
recognition of individual speech patterns. This strategy enhances
generalization via broader pathological feature learning, mitigates
speaker-specific overfitting, reduces per-patient data dependence, and improves
target-speaker accuracy - achieving up to 13.15% lower WER versus
single-speaker fine-tuning.

</details>


### [240] [SegReConcat: A Data Augmentation Method for Voice Anonymization Attack](https://arxiv.org/abs/2508.18907)
*Ridwan Arefeen,Xiaoxiao Miao,Rong Tong,Aik Beng Ng,Simon See*

Main category: cs.SD

TL;DR: 提出SegReConcat数据增强方法用于攻击者侧增强自动说话人验证系统，在VoicePrivacy Attacker Challenge 2024框架中评估，在七个匿名化系统中的五个上提高了解匿名化能力。


<details>
  <summary>Details</summary>
Motivation: 语音匿名化后仍存在残留说话人线索，带来隐私风险。

Method: 提出SegReConcat方法，在词级对匿名化语音进行分割，使用随机或基于相似度的策略重新排列片段以破坏长期上下文线索，并与原始话语拼接，使攻击者从多个角度学习源说话人特征。

Result: 在VoicePrivacy Attacker Challenge 2024框架中对七个匿名化系统进行评估，SegReConcat在五个系统上提高了解匿名化能力。

Conclusion: SegReConcat方法能有效增强自动说话人验证系统的解匿名化能力。

Abstract: Anonymization of voice seeks to conceal the identity of the speaker while
maintaining the utility of speech data. However, residual speaker cues often
persist, which pose privacy risks. We propose SegReConcat, a data augmentation
method for attacker-side enhancement of automatic speaker verification systems.
SegReConcat segments anonymized speech at the word level, rearranges segments
using random or similarity-based strategies to disrupt long-term contextual
cues, and concatenates them with the original utterance, allowing an attacker
to learn source speaker traits from multiple perspectives. The proposed method
has been evaluated in the VoicePrivacy Attacker Challenge 2024 framework across
seven anonymization systems, SegReConcat improves de-anonymization on five out
of seven systems.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [241] [Lightweight posterior construction for gravitational-wave catalogs with the Kolmogorov-Arnold network](https://arxiv.org/abs/2508.18698)
*Wenshuai Liu,Yiming Dong,Ziming Wang,Lijing Shao*

Main category: gr-qc

TL;DR: 本文探索用Kolmogorov - Arnold网络（KAN）构建高效可解释的神经密度估计器，用于引力波目录轻量级后验构建，提出KAN神经密度估计器，能压缩数据，有望促进数据存储和传输。


<details>
  <summary>Details</summary>
Motivation: 在引力波数据分析中，需要高效可解释的神经密度估计器进行轻量级后验构建，以促进数据存储和传输，用于后续分析。

Method: 使用KAN网络，用可学习样条替换传统激活函数，构建KAN神经密度估计器，压缩引力波后验样本，从网络权重中提取解析表达式。

Result: KAN在相关科学任务中具有更好的可解释性、更高的准确性和参数效率，能将兆字节级样本压缩为几十千字节的模型权重，可快速再生高保真后验样本。

Conclusion: 提出的轻量级后验构建策略有望促进用户级数据存储和传输，为下一代引力波探测器的大量事件分析铺平道路。

Abstract: Neural density estimation has seen widespread applications in the
gravitational-wave (GW) data analysis, which enables real-time parameter
estimation for compact binary coalescences and enhances rapid inference for
subsequent analysis such as population inference. In this work, we explore the
application of using the Kolmogorov-Arnold network (KAN) to construct efficient
and interpretable neural density estimators for lightweight posterior
construction of GW catalogs. By replacing conventional activation functions
with learnable splines, KAN achieves superior interpretability, higher
accuracy, and greater parameter efficiency on related scientific tasks.
Leveraging this feature, we propose a KAN-based neural density estimator, which
ingests megabyte-scale GW posterior samples and compresses them into model
weights of tens of kilobytes. Subsequently, analytic expressions requiring only
several kilobytes can be further distilled from these neural network weights
with minimal accuracy trade-off. In practice, GW posterior samples with
fidelity can be regenerated rapidly using the model weights or analytic
expressions for subsequent analysis. Our lightweight posterior construction
strategy is expected to facilitate user-level data storage and transmission,
paving a path for efficient analysis of numerous GW events in the
next-generation GW detectors.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [242] [Private Quantum Database](https://arxiv.org/abs/2508.19055)
*Giancarlo Gatti,Rihan Hai*

Main category: quant-ph

TL;DR: 本文提出一种量子数据库，利用量子力学保护用户和数据隐私，还设想了适用于早期部署的混合架构。


<details>
  <summary>Details</summary>
Motivation: 传统系统在用户隐私和数据隐私保护上存在不足，量子数据库可提供经典系统无法比拟的隐私保障。

Method: 将关系表编码为基于互偏基的量子随机访问码序列，传输有限数量的量子态，通过单次破坏性测量重建所选元组。

Result: 能够在无需可信硬件或重量级加密的情况下同时保护数据隐私和用户隐私。

Conclusion: 提出的量子数据库可保护隐私，且设想的混合架构能与当前嘈杂中等规模量子设备的局限性兼容。

Abstract: Quantum databases open an exciting new frontier in data management by
offering privacy guarantees that classical systems cannot match. Traditional
engines tackle user privacy, which hides the records being queried, or data
privacy, which prevents a user from learning more than she has queried. We
propose a quantum database that protects both by leveraging quantum mechanics:
when the user measures her chosen basis, the superposition collapses and the
unqueried rows become physically inaccessible. We encode relational tables as a
sequence of Quantum Random Access Codes (QRACs) over mutually unbiased bases
(MUBs), transmit a bounded number of quantum states, and let a single,
destructive measurement reconstruct only the selected tuple. This allows us to
preserve data privacy and user privacy at once without trusted hardware or
heavyweight cryptography. Moreover, we envision a novel hybrid
quantum-classical architecture ready for early deployment, which ensures
compatibility with the limitations of today's Noisy Intermediate-Scale Quantum
devices.

</details>


### [243] [Architecting Distributed Quantum Computers: Design Insights from Resource Estimation](https://arxiv.org/abs/2508.19160)
*Dmitry Filippov,Peter Yang,Prakash Murali*

Main category: quant-ph

TL;DR: 本文提出新的资源估算框架来分析分布式量子计算系统，表明其资源需求可行且有良好硬件实现前景。


<details>
  <summary>Details</summary>
Motivation: 现有单片设备架构扩展受限，资源估算主要针对单片系统，需填补分布式系统资源估算的空白并解答架构设计问题。

Method: 开发新的资源估算框架，对分布式执行栈关键组件建模，分析不同硬件配置下实用量子算法的性能。

Result: 分布式架构资源需求可行，对于45K qubits的节点大小，物理 qubits 数量平均比单片架构高1.4倍，执行时间高4倍，但硬件实现前景更有利。

Conclusion: 关于纠缠生成率、节点大小和架构的见解可为未来系统设计提供参考。

Abstract: To enable practically useful quantum computing, we require hundreds to
thousands of logical qubits (collections of physical qubits with error
correction). Current monolithic device architectures have scaling limits beyond
few tens of logical qubits. To scale up, we require architectures that
orchestrate several monolithic devices into a distributed quantum computing
system. Currently, resource estimation, which is crucial for determining
hardware needs and bottlenecks, focuses exclusively on monolithic systems. Our
work fills this gap and answers key architectural design questions about
distributed systems, including the impact of distribution on application
resource needs, the organization of qubits across nodes and the requirements of
entanglement distillation (quantum network). To answer these questions, we
develop a novel resource estimation framework that models the key components of
the distributed execution stack. We analyse the performance of practical
quantum algorithms on various hardware configurations, spanning different qubit
speeds, entanglement generation rates and distillation protocols. We show that
distributed architectures have practically feasible resource requirements; for
a node size of 45K qubits, distributed systems need on average 1.4X higher
number of physical qubits and 4X higher execution time compared to monolithic
architectures, but with more favourable hardware implementation prospects. Our
insights on entanglement generation rates, node sizes and architecture have the
potential to inform system designs in the coming years.

</details>


### [244] [Vectorized Attention with Learnable Encoding for Quantum Transformer](https://arxiv.org/abs/2508.18464)
*Ziqing Guo,Ziwen Pan,Alex Khan,Jan Balewski*

Main category: quant-ph

TL;DR: 提出矢量化量子变压器（VQT）模型，支持理想掩码注意力矩阵计算和高效训练，在量子电路模拟有对比结果，在NLP任务有竞争力，为量子计算端到端机器学习提供新架构。


<details>
  <summary>Details</summary>
Motivation: 当前量子变压器依赖深度参数化量子电路，易受QPU噪声影响，阻碍实际性能。

Method: 提出VQT模型，通过量子近似模拟支持理想掩码注意力矩阵计算，利用矢量化非线性量子编码器进行高效训练。

Result: 展示了在IBM和IonQ量子电路模拟的准确性对比，在IBM最先进高保真Kingston QPU的自然语言处理基准任务中有有竞争力的结果。

Conclusion: 噪声中规模量子友好的VQT方法为量子计算端到端机器学习解锁了新架构。

Abstract: Vectorized quantum block encoding provides a way to embed classical data into
Hilbert space, offering a pathway for quantum models, such as Quantum
Transformers (QT), that replace classical self-attention with quantum circuit
simulations to operate more efficiently. Current QTs rely on deep parameterized
quantum circuits (PQCs), rendering them vulnerable to QPU noise, and thus
hindering their practical performance. In this paper, we propose the Vectorized
Quantum Transformer (VQT), a model that supports ideal masked attention matrix
computation through quantum approximation simulation and efficient training via
vectorized nonlinear quantum encoder, yielding shot-efficient and gradient-free
quantum circuit simulation (QCS) and reduced classical sampling overhead. In
addition, we demonstrate an accuracy comparison for IBM and IonQ in quantum
circuit simulation and competitive results in benchmarking natural language
processing tasks on IBM state-of-the-art and high-fidelity Kingston QPU. Our
noise intermediate-scale quantum friendly VQT approach unlocks a novel
architecture for end-to-end machine learning in quantum computing.

</details>


### [245] [Universal Dynamics with Globally Controlled Analog Quantum Simulators](https://arxiv.org/abs/2508.19075)
*Hong-Ye Hu,Abigail McClain Gomez,Liyuan Chen,Aaron Trowbridge,Andy J. Goldschmidt,Zachary Manchester,Frederic T. Chong,Arthur Jaffe,Susanne F. Yelin*

Main category: quant-ph

TL;DR: 本文探讨全局控制模拟量子模拟器实现通用量子动力学问题，建立通用条件，引入新控制技术，实验验证其可行性，为量子模拟开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 尽管全局控制模拟量子模拟器有进展，但该系统在全局控制下实现通用量子动力学的程度这一理论问题未解决。

Method: 建立仅用全局脉冲控制实现通用量子计算的充要条件，将框架扩展到费米子和玻色子系统，引入直接量子最优控制技术。

Result: 实验实现非阻塞区域三体相互作用，展示里德堡原子阵列拓扑动力学，测量显示对称保护拓扑边缘模式的动力学特征。

Conclusion: 工作为超越原生硬件哈密顿量的量子模拟开辟新途径，推动全局控制模拟平台的量子信息处理发展。

Abstract: Analog quantum simulators with global control fields have emerged as powerful
platforms for exploring complex quantum phenomena. Recent breakthroughs, such
as the coherent control of thousands of atoms, highlight the growing potential
for quantum applications at scale. Despite these advances, a fundamental
theoretical question remains unresolved: to what extent can such systems
realize universal quantum dynamics under global control? Here we establish a
necessary and sufficient condition for universal quantum computation using only
global pulse control, proving that a broad class of analog quantum simulators
is, in fact, universal. We further extend this framework to fermionic and
bosonic systems, including modern platforms such as ultracold atoms in optical
superlattices. Crucially, to connect the theoretical possibility with
experimental reality, we introduce a new control technique into the experiment
- direct quantum optimal control. This method enables the synthesis of complex
effective Hamiltonians and allows us to incorporate realistic hardware
constraints. To show its practical power, we experimentally engineer three-body
interactions outside the blockade regime and demonstrate topological dynamics
on a Rydberg atom array. Using the new control framework, we overcome key
experimental challenges, including hardware limitations and atom position
fluctuations in the non-blockade regime, by identifying smooth, short-duration
pulses that achieve high-fidelity dynamics. Experimental measurements reveal
dynamical signatures of symmetry-protected-topological edge modes, confirming
both the expressivity and feasibility of our approach. Our work opens a new
avenue for quantum simulation beyond native hardware Hamiltonians, enabling the
engineering of effective multi-body interactions and advancing the frontier of
quantum information processing with globally-controlled analog platforms.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [246] [From Prediction to Simulation: AlphaFold 3 as a Differentiable Framework for Structural Biology](https://arxiv.org/abs/2508.18446)
*Alireza Abbaszadeh,Armita Shahlaee*

Main category: q-bio.BM

TL;DR: AlphaFold 3在计算生物学领域有变革性进展，改进蛋白质结构预测，实现范式转变。


<details>
  <summary>Details</summary>
Motivation: 提升蛋白质结构预测的准确性和泛化能力，弥合传统静态结构建模与动态分子模拟的差距。

Method: 采用新颖的多尺度变压器架构、生物信息交叉注意力机制和几何感知优化策略，将蛋白质折叠预测重构为可微过程。

Result: 显著提高了不同蛋白质家族预测的准确性和泛化能力，超越先前方法。

Conclusion: AlphaFold 3是将深度学习与基于物理的分子相结合的基础框架。

Abstract: AlphaFold 3 represents a transformative advancement in computational biology,
enhancing protein structure prediction through novel multi-scale transformer
architectures, biologically informed cross-attention mechanisms, and
geometry-aware optimization strategies. These innovations dramatically improve
predictive accuracy and generalization across diverse protein families,
surpassing previous methods. Crucially, AlphaFold 3 embodies a paradigm shift
toward differentiable simulation, bridging traditional static structural
modeling with dynamic molecular simulations. By reframing protein folding
predictions as a differentiable process, AlphaFold 3 serves as a foundational
framework for integrating deep learning with physics-based molecular

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [247] [Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning](https://arxiv.org/abs/2508.18397)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: 文章对离线强化学习中数据整理策略进行大规模对比研究，评估不同临界性加权方案，结果表明数据整理方法优于基线，基于模型不确定性的数据驱动整理安全提升显著，还指出不同时间尺度加权的优缺点。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中使用标准均匀数据采样训练自动驾驶规划策略时，因数据极端不平衡导致策略脆弱和不安全的问题。

Method: 开展系统、大规模的对比研究，研究六种不同的临界性加权方案，分为启发式、基于不确定性和基于行为三类，在两个时间尺度评估，训练七个目标条件保守Q学习代理并在Waymax模拟器评估。

Result: 所有数据整理方法显著优于基线，基于模型不确定性的数据驱动整理安全提升最大，碰撞率降低近三倍；时间步级加权在反应性安全上表现出色，场景级加权改善长期规划。

Conclusion: 为离线强化学习中的数据整理提供综合框架，强调智能、非均匀采样对构建安全可靠自主代理的重要性。

Abstract: Offline Reinforcement Learning (RL) presents a promising paradigm for
training autonomous vehicle (AV) planning policies from large-scale, real-world
driving logs. However, the extreme data imbalance in these logs, where mundane
scenarios vastly outnumber rare "long-tail" events, leads to brittle and unsafe
policies when using standard uniform data sampling. In this work, we address
this challenge through a systematic, large-scale comparative study of data
curation strategies designed to focus the learning process on information-rich
samples. We investigate six distinct criticality weighting schemes which are
categorized into three families: heuristic-based, uncertainty-based, and
behavior-based. These are evaluated at two temporal scales, the individual
timestep and the complete scenario. We train seven goal-conditioned
Conservative Q-Learning (CQL) agents with a state-of-the-art, attention-based
architecture and evaluate them in the high-fidelity Waymax simulator. Our
results demonstrate that all data curation methods significantly outperform the
baseline. Notably, data-driven curation using model uncertainty as a signal
achieves the most significant safety improvements, reducing the collision rate
by nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear
trade-off where timestep-level weighting excels at reactive safety while
scenario-level weighting improves long-horizon planning. Our work provides a
comprehensive framework for data curation in Offline RL and underscores that
intelligent, non-uniform sampling is a critical component for building safe and
reliable autonomous agents.

</details>


### [248] [AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot](https://arxiv.org/abs/2508.18694)
*Jaehwan Jeong,Tuan-Anh Vu,Mohammad Jony,Shahab Ahmad,Md. Mukhlesur Rahman,Sangpil Kim,M. Khalid Jawed*

Main category: cs.RO

TL;DR: 提出AgriChrono平台和多模态数据集，解决现有农业数据集不能反映真实农田动态特性的问题，在该数据集上对3D重建模型进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有精准农业数据集在静态或受控环境收集，缺乏传感器多样性和时间跨度，导致模型应用于真实场景时缺乏鲁棒性和泛化性。

Method: 构建AgriChrono机器人数据收集平台，集成多传感器，可远程、时间同步采集多种数据，支持长期数据收集。

Result: 在AgriChrono数据集上对一系列先进3D重建模型进行基准测试，凸显真实农田环境重建的困难。

Conclusion: AgriChrono数据集作为一种研究资源，有助于提升模型在动态条件下的泛化能力，代码和数据集已公开。

Abstract: Existing datasets for precision agriculture have primarily been collected in
static or controlled environments such as indoor labs or greenhouses, often
with limited sensor diversity and restricted temporal span. These conditions
fail to reflect the dynamic nature of real farmland, including illumination
changes, crop growth variation, and natural disturbances. As a result, models
trained on such data often lack robustness and generalization when applied to
real-world field scenarios. In this paper, we present AgriChrono, a novel
robotic data collection platform and multi-modal dataset designed to capture
the dynamic conditions of real-world agricultural environments. Our platform
integrates multiple sensors and enables remote, time-synchronized acquisition
of RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable
long-term data collection across varying illumination and crop growth stages.
We benchmark a range of state-of-the-art 3D reconstruction models on the
AgriChrono dataset, highlighting the difficulty of reconstruction in real-world
field environments and demonstrating its value as a research asset for
advancing model generalization under dynamic conditions. The code and dataset
are publicly available at: https://github.com/StructuresComp/agri-chrono

</details>


### [249] [Learning Real-World Acrobatic Flight from Human Preferences](https://arxiv.org/abs/2508.18817)
*Colin Merk,Ismail Geles,Jiaxu Xing,Angel Romero,Giorgia Ramponi,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 本文探索基于偏好的强化学习（PbRL）用于敏捷无人机控制，提出REC方法改进奖励学习目标，在模拟和现实中验证其有效性，强调PbRL捕捉复杂目标的能力。


<details>
  <summary>Details</summary>
Motivation: Acrobatic flight问题复杂，传统手动设计奖励函数难以应对，需探索PbRL用于敏捷无人机控制。

Method: 基于Preference PPO，提出Reward Ensemble under Confidence (REC)改进奖励学习目标。

Result: 方法达到88.4%的塑形奖励性能，高于标准Preference PPO的55.2%；成功将策略从模拟转移到现实无人机；在MuJoCo环境验证模型适用性；手动设计奖励与人类偏好仅60.7%一致。

Conclusion: PbRL在物理和模拟领域捕捉复杂、以人类为中心的目标方面有效。

Abstract: Preference-based reinforcement learning (PbRL) enables agents to learn
control policies without requiring manually designed reward functions, making
it well-suited for tasks where objectives are difficult to formalize or
inherently subjective. Acrobatic flight poses a particularly challenging
problem due to its complex dynamics, rapid movements, and the importance of
precise execution. In this work, we explore the use of PbRL for agile drone
control, focusing on the execution of dynamic maneuvers such as powerloops.
Building on Preference-based Proximal Policy Optimization (Preference PPO), we
propose Reward Ensemble under Confidence (REC), an extension to the reward
learning objective that improves preference modeling and learning stability.
Our method achieves 88.4% of the shaped reward performance, compared to 55.2%
with standard Preference PPO. We train policies in simulation and successfully
transfer them to real-world drones, demonstrating multiple acrobatic maneuvers
where human preferences emphasize stylistic qualities of motion. Furthermore,
we demonstrate the applicability of our probabilistic reward model in a
representative MuJoCo environment for continuous control. Finally, we highlight
the limitations of manually designed rewards, observing only 60.7% agreement
with human preferences. These results underscore the effectiveness of PbRL in
capturing complex, human-centered objectives across both physical and simulated
domains.

</details>


### [250] [From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity](https://arxiv.org/abs/2508.19172)
*Luca Grillotti,Lisa Coiffard,Oscar Pang,Maxence Faldor,Antoine Cully*

Main category: cs.RO

TL;DR: 提出URSA方法扩展QDAC，使机器人在现实世界自主发现和掌握多样技能，实验证明其有效性并建立新框架。


<details>
  <summary>Details</summary>
Motivation: 现有自主技能发现方法需手动定义技能空间和调优启发式，限制现实应用，且在物理硬件上学习有挑战。

Method: 提出Unsupervised Real - world Skill Acquisition (URSA)，扩展QDAC。

Result: 在模拟和现实世界中让Unitree A1四足机器人发现多样运动技能，在下游任务如损伤适应中表现优于基线。

Conclusion: 建立了新的现实世界机器人学习框架，减少人工干预，向更自主和适应性强的机器人系统迈进。

Abstract: Autonomous skill discovery aims to enable robots to acquire diverse behaviors
without explicit supervision. Learning such behaviors directly on physical
hardware remains challenging due to safety and data efficiency constraints.
Existing methods, including Quality-Diversity Actor-Critic (QDAC), require
manually defined skill spaces and carefully tuned heuristics, limiting
real-world applicability. We propose Unsupervised Real-world Skill Acquisition
(URSA), an extension of QDAC that enables robots to autonomously discover and
master diverse, high-performing skills directly in the real world. We
demonstrate that URSA successfully discovers diverse locomotion skills on a
Unitree A1 quadruped in both simulation and the real world. Our approach
supports both heuristic-driven skill discovery and fully unsupervised settings.
We also show that the learned skill repertoire can be reused for downstream
tasks such as real-world damage adaptation, where URSA outperforms all
baselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.
Our results establish a new framework for real-world robot learning that
enables continuous skill discovery with limited human intervention,
representing a significant step toward more autonomous and adaptable robotic
systems. Demonstration videos are available at
http://adaptive-intelligent-robotics.github.io/URSA .

</details>


### [251] [Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation](https://arxiv.org/abs/2508.19199)
*Alex LaGrassa,Zixuan Huang,Dmitry Berenson,Oliver Kroemer*

Main category: cs.RO

TL;DR: 本文提出自动生成特定任务、空间自适应动力学模型的方法，在树木操作任务中提高规划速度，为新任务生成高效模型提供路径。


<details>
  <summary>Details</summary>
Motivation: 在高维空间（如涉及可变形物体）进行高效规划，需要计算上可行且表达能力足够的动力学模型。

Method: 提出基于扩散的模型生成器，根据规划查询的起始和目标点云预测区域模型分辨率，采用两阶段过程收集学习数据。

Result: 在树木操作任务中，该方法使规划速度翻倍，与使用全分辨率模型相比，任务性能仅有小幅下降。

Conclusion: 此方法为利用先前规划和控制数据为新任务生成计算高效且表达能力足够的动力学模型指明了方向。

Abstract: Efficient planning in high-dimensional spaces, such as those involving
deformable objects, requires computationally tractable yet sufficiently
expressive dynamics models. This paper introduces a method that automatically
generates task-specific, spatially adaptive dynamics models by learning which
regions of the object require high-resolution modeling to achieve good task
performance for a given planning query. Task performance depends on the complex
interplay between the dynamics model, world dynamics, control, and task
requirements. Our proposed diffusion-based model generator predicts per-region
model resolutions based on start and goal pointclouds that define the planning
query. To efficiently collect the data for learning this mapping, a two-stage
process optimizes resolution using predictive dynamics as a prior before
directly optimizing using closed-loop performance. On a tree-manipulation task,
our method doubles planning speed with only a small decrease in task
performance over using a full-resolution model. This approach informs a path
towards using previous planning and control data to generate computationally
efficient yet sufficiently expressive dynamics models for new tasks.

</details>


### [252] [An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees](https://arxiv.org/abs/2508.19074)
*ZhenDong Chen,ZhanShang Nie,ShiXing Wan,JunYi Li,YongTian Cheng,Shuai Zhao*

Main category: cs.RO

TL;DR: 提出自然-机器人语言翻译框架NRTrans，为生成的控制程序提供正确性验证，并通过反馈微调提高大语言模型程序生成性能，实验表明其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型生成机器人控制程序的方法因模型不一致和任务复杂度高，常导致生成代码有大量编程错误，尤其在应用轻量级大语言模型时效果不佳。

Method: 提出机器人技能语言RSL抽象控制程序细节，构建RSL编译器和调试器验证LLM生成的RSL程序，并提供错误反馈进行微调。

Result: NRTrans在多种大语言模型和任务下优于现有方法，轻量级大语言模型也能达到高成功率。

Conclusion: 该框架能为LLM生成的程序提供正确性保证，显著提高基于LLM的机器人应用的有效性。

Abstract: The Large Language Models (LLM) are increasingly being deployed in robotics
to generate robot control programs for specific user tasks, enabling embodied
intelligence. Existing methods primarily focus on LLM training and prompt
design that utilize LLMs to generate executable programs directly from user
tasks in natural language. However, due to the inconsistency of the LLMs and
the high complexity of the tasks, such best-effort approaches often lead to
tremendous programming errors in the generated code, which significantly
undermines the effectiveness especially when the light-weight LLMs are applied.
This paper introduces a natural-robotic language translation framework that (i)
provides correctness verification for generated control programs and (ii)
enhances the performance of LLMs in program generation via feedback-based
fine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is
proposed to abstract away from the intricate details of the control programs,
bridging the natural language tasks with the underlying robot skills. Then, the
RSL compiler and debugger are constructed to verify RSL programs generated by
the LLM and provide error feedback to the LLM for refining the outputs until
being verified by the compiler. This provides correctness guarantees for the
LLM-generated programs before being offloaded to the robots for execution,
significantly enhancing the effectiveness of LLM-powered robotic applications.
Experiments demonstrate NRTrans outperforms the existing method under a range
of LLMs and tasks, and achieves a high success rate for light-weight LLMs.

</details>


### [253] [ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments](https://arxiv.org/abs/2508.19131)
*Shreya Gummadi,Mateus V. Gasparino,Gianluca Capezzuto,Marcelo Becker,Girish Chowdhary*

Main category: cs.RO

TL;DR: 提出ZeST方法利用大语言模型实时创建可通行性地图，避免机器人处于危险中并加速导航系统开发，实验显示比其他方法更安全。


<details>
  <summary>Details</summary>
Motivation: 传统生成训练预测模型数据集的方法会让机器人处于危险环境，有设备和安全风险。

Method: 提出ZeST方法，利用大语言模型的视觉推理能力实时创建可通行性地图。

Result: 在室内外环境导航实验中，该方法比其他先进方法更安全，能持续到达最终目标。

Conclusion: ZeST方法可进行零样本可通行性预测，降低真实数据收集风险，加速高级导航系统开发，是经济且可扩展的解决方案。

Abstract: The advancement of robotics and autonomous navigation systems hinges on the
ability to accurately predict terrain traversability. Traditional methods for
generating datasets to train these prediction models often involve putting
robots into potentially hazardous environments, posing risks to equipment and
safety. To solve this problem, we present ZeST, a novel approach leveraging
visual reasoning capabilities of Large Language Models (LLMs) to create a
traversability map in real-time without exposing robots to danger. Our approach
not only performs zero-shot traversability and mitigates the risks associated
with real-world data collection but also accelerates the development of
advanced navigation systems, offering a cost-effective and scalable solution.
To support our findings, we present navigation results, in both controlled
indoor and unstructured outdoor environments. As shown in the experiments, our
method provides safer navigation when compared to other state-of-the-art
methods, constantly reaching the final goal.

</details>


### [254] [Uncertainty-Resilient Active Intention Recognition for Robotic Assistants](https://arxiv.org/abs/2508.19150)
*Juan Carlos Saborío,Marc Vinci,Oscar Lima,Sebastian Stock,Lennart Niecksch,Martin Günther,Alexander Sung,Joachim Hertzberg,Martin Atzmüller*

Main category: cs.RO

TL;DR: 论文提出一个框架解决人类意图识别中不确定结果和感知误差问题，且在物理机器人上测试成功。


<details>
  <summary>Details</summary>
Motivation: 现有机器人助理行为问题解决方式存在局限，未解决人类意图识别中不确定结果和感知误差的挑战。

Method: 提出一个能适应不确定性和传感器噪声的框架，将实时传感器数据与多种规划器结合，以意图识别POMDP为核心处理不确定性下的合作规划与行动。

Result: 集成框架在物理机器人上成功测试，取得了有前景的结果。

Conclusion: 所提出的框架能有效应对人类意图识别中的不确定性问题。

Abstract: Purposeful behavior in robotic assistants requires the integration of
multiple components and technological advances. Often, the problem is reduced
to recognizing explicit prompts, which limits autonomy, or is oversimplified
through assumptions such as near-perfect information. We argue that a critical
gap remains unaddressed -- specifically, the challenge of reasoning about the
uncertain outcomes and perception errors inherent to human intention
recognition. In response, we present a framework designed to be resilient to
uncertainty and sensor noise, integrating real-time sensor data with a
combination of planners. Centered around an intention-recognition POMDP, our
approach addresses cooperative planning and acting under uncertainty. Our
integrated framework has been successfully tested on a physical robot with
promising results.

</details>


### [255] [Real-Time Model Checking for Closed-Loop Robot Reactive Planning](https://arxiv.org/abs/2508.19186)
*Christopher Chandler,Bernd Porr,Giulia Lafratta,Alice Miller*

Main category: cs.RO

TL;DR: 提出模型检查新应用，在低功耗设备实时实现自主机器人多步规划与避障，性能优于单步规划反应式智能体。


<details>
  <summary>Details</summary>
Motivation: 实现自主机器人实时多步规划和避障。

Method: 开发专用模型检查算法，基于生物智能体知识和注意力现场生成规划；链式临时控制系统应对环境干扰；对2D LiDAR数据进行离散化；用深度优先搜索模型检查进行多步规划。

Result: 经验结果和非正式证明表明，模型检查可用于创建高效的局部避障多步规划，性能优于单步规划反应式智能体。

Conclusion: 该方法为自动驾驶车辆安全、可靠、可解释规划开发提供了案例。

Abstract: We present a new application of model checking which achieves real-time
multi-step planning and obstacle avoidance on a real autonomous robot. We have
developed a small, purpose-built model checking algorithm which generates plans
in situ based on "core" knowledge and attention as found in biological agents.
This is achieved in real-time using no pre-computed data on a low-powered
device. Our approach is based on chaining temporary control systems which are
spawned to counteract disturbances in the local environment that disrupt an
autonomous agent from its preferred action (or resting state). A novel
discretization of 2D LiDAR data sensitive to bounded variations in the local
environment is used. Multi-step planning using model checking by forward
depth-first search is applied to cul-de-sac and playground scenarios. Both
empirical results and informal proofs of two fundamental properties of our
approach demonstrate that model checking can be used to create efficient
multi-step plans for local obstacle avoidance, improving on the performance of
a reactive agent which can only plan one step. Our approach is an instructional
case study for the development of safe, reliable and explainable planning in
the context of autonomous vehicles.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [256] [An 8- and 12-bit block AES cipher](https://arxiv.org/abs/2508.18485)
*Peter T. Breuer*

Main category: cs.CR

TL;DR: 文档记录了8或12位块的AES（Rijndael）密码及Java源代码。


<details>
  <summary>Details</summary>
Motivation: 该密码不常见、难寻且有讲解价值，值得记录。

Method: 未提及。

Result: 记录了8或12位块的AES（Rijndael）密码及Java源代码。

Conclusion: 未提及。

Abstract: Because it is so unusual, or hard to find, or expository, a truly tiny 8- or
12-bit block AES (Rijndael) cipher is documented here, along with Java source
code.

</details>


### [257] [A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs](https://arxiv.org/abs/2508.18439)
*Anders Mølmen Høst,Pierre Lison,Leon Moonen*

Main category: cs.CR

TL;DR: 本文介绍了自动化方法TRIAGE，用大语言模型将CVE映射到ATT&CK知识库技术，评估表明该方法能提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞数据库缺乏CVE现实影响信息，手动关联CVE和TTPs困难，需自动化支持。

Method: 提出TRIAGE方法，结合两种基于大语言模型的模块，将规则推理与数据驱动推理相结合。

Result: 上下文学习优于单个映射方法，混合方法提高了利用技术的召回率，GPT - 4o - mini表现优于Llama3.3 - 70B。

Conclusion: 大语言模型可自动预测网络安全漏洞影响，TRIAGE使CVE到ATT&CK的映射过程更高效。

Abstract: Vulnerability databases, such as the National Vulnerability Database (NVD),
offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but
often lack information on their real-world impact, such as the tactics,
techniques, and procedures (TTPs) that adversaries may use to exploit the
vulnerability. However, manually linking CVEs to their corresponding TTPs is a
challenging and time-consuming task, and the high volume of new vulnerabilities
published annually makes automated support desirable.
  This paper introduces TRIAGE, a two-pronged automated approach that uses
Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK
knowledge base. We first prompt an LLM with instructions based on MITRE's CVE
Mapping Methodology to predict an initial list of techniques. This list is then
combined with the results from a second LLM-based module that uses in-context
learning to map a CVE to relevant techniques. This hybrid approach
strategically combines rule-based reasoning with data-driven inference. Our
evaluation reveals that in-context learning outperforms the individual mapping
methods, and the hybrid approach improves recall of exploitation techniques. We
also find that GPT-4o-mini performs better than Llama3.3-70B on this task.
Overall, our results show that LLMs can be used to automatically predict the
impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping
CVEs to ATT&CK more efficient.
  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language
models, automated mapping.

</details>


### [258] [An Efficient Lightweight Blockchain for Decentralized IoT](https://arxiv.org/abs/2508.19219)
*Faezeh Dehghan Tarzjani,Mostafa Salehi*

Main category: cs.CR

TL;DR: 针对物联网发展挑战，提出用虚拟化和集群化构建高效轻量级区块链，引入基于WBS的PoA算法，仿真显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 物联网发展受集中化和大规模网络挑战，传统区块链共识算法不适用物联网设备资源有限情况，PoA的TBS需优化。

Method: 使用虚拟化和集群化构建高效轻量级区块链，引入基于WBS方法的新型PoA算法。

Result: 仿真显示相比TBS，所提WBS方法降低能耗和响应时间，提高吞吐量。

Conclusion: 所提方法能解决现有PoA算法问题，可用于去中心化物联网架构。

Abstract: The Internet of Things (IoT) is applied in various fields, and the number of
physical devices connected to the IoT is increasingly growing. There are
significant challenges to the IoT's growth and development, mainly due to the
centralized nature and large-scale IoT networks. The emphasis on the
decentralization of IoT's architecture can overcome challenges to IoT's
capabilities. A promising decentralized platform for IoT is blockchain. Owing
to IoT devices' limited resources, traditional consensus algorithms such as PoW
and PoS in the blockchain are computationally expensive. Therefore, the PoA
consensus algorithm is proposed in the blockchain consensus network for IoT.
The PoA selects the validator as Turn-based selection (TBS) that needs
optimization and faces system reliability, energy consumption, latency, and low
scalability. We propose an efficient, lightweight blockchain for decentralizing
IoT architecture by using virtualization and clustering to increase
productivity and scalability to address these issues. We also introduce a novel
PoA based on the Weight-Based-Selection (WBS) method for validators to validate
transactions and add them to the blockchain. By simulation, we evaluated the
performance of our proposed WBS method as opposed to TBS. The results show
reduced energy consumption, and response time, and increased throughput.

</details>


### [259] [Collaborative Intelligence: Topic Modelling of Large Language Model use in Live Cybersecurity Operations](https://arxiv.org/abs/2508.18488)
*Martin Lochner,Keegan Keplinger*

Main category: cs.CR

TL;DR: 研究安全运营中心（SOC）人员在实时安全操作中使用大语言模型（LLM）的主题建模，发现他们主要用LLM理解复杂文本，可据此开发协作工具。


<details>
  <summary>Details</summary>
Motivation: 了解SOC专家自愿使用LLM的情况，探究他们如何将LLM融入工作。

Method: 收集10个月SOC操作员通过内部HTTP聊天应用访问GPT - 4的数据，先后用BERTopic模型和新的主题建模工作流进行主题建模。

Result: 两种建模方法均显示SOC操作员主要用LLM理解复杂文本，此类用例约占LLM使用的40%。

Conclusion: SOC操作员有利用LLM的自然倾向，可设计协作式LLM工具支持和增强其工作流程，还能开发支持SOC任务流的工作流程。

Abstract: Objective: This work describes the topic modelling of Security Operations
Centre (SOC) use of a large language model (LLM), during live security
operations. The goal is to better understand how these specialists voluntarily
use this tool.
  Background: Human-automation teams have been extensively studied, but
transformer-based language models have sparked a new wave of collaboration. SOC
personnel at a major cybersecurity provider used an LLM to support live
security operations. This study examines how these specialists incorporated the
LLM into their work.
  Method: Our data set is the result of 10 months of SOC operators accessing
GPT-4 over an internally deployed HTTP-based chat application. We performed two
topic modelling exercises, first using the established BERTopic model
(Grootendorst, 2022), and second, using a novel topic modeling workflow.
  Results: Both the BERTopic analysis and novel modelling approach revealed
that SOC operators primarily used the LLM to facilitate their understanding of
complex text strings. Variations on this use-case accounted for ~40% of SOC LLM
usage.
  Conclusion: SOC operators are required to rapidly interpret complex commands
and similar information. Their natural tendency to leverage LLMs to support
this activity indicates that their workflow can be supported and augmented by
designing collaborative LLM tools for use in the SOC.
  Application: This work can aid in creating next-generation tools for Security
Operations Centres. By understanding common use-cases, we can develop workflows
supporting SOC task flow. One example is a right-click context menu for
executing a command line analysis LLM call directly in the SOC environment.

</details>


### [260] [PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality](https://arxiv.org/abs/2508.18649)
*Nanxi Li,Zhengyue Zhao,Chaowei Xiao*

Main category: cs.CR

TL;DR: 提出PRISM框架保障视觉语言模型安全，评估显示有效且能保持模型效用，代码等开源。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型安全防护方法存在过度防御损害效用或浅层对齐无法检测复杂威胁的问题。

Method: 引入类似系统2的PRISM框架，包含数据集PRISM - CoT和通过蒙特卡罗树搜索生成的PRISM - DPO，利用直接偏好优化细化推理。

Result: 在多个测试集上攻击成功率低，对自适应攻击有强鲁棒性，能有效泛化到分布外挑战，同时保持或提升模型效用。

Conclusion: PRISM框架在保障视觉语言模型安全方面有效，能在保证安全的同时维持模型效用。

Abstract: Safeguarding vision-language models (VLMs) is a critical challenge, as
existing methods often suffer from over-defense, which harms utility, or rely
on shallow alignment, failing to detect complex threats that require deep
reasoning. To this end, we introduce PRISM (Principled Reasoning for Integrated
Safety in Multimodality), a system2-like framework that aligns VLMs by
embedding a structured, safety-aware reasoning process. Our framework consists
of two key components: PRISM-CoT, a dataset that teaches safety-aware
chain-of-thought reasoning, and PRISM-DPO, generated via Monte Carlo Tree
Search (MCTS) to further refine this reasoning through Direct Preference
Optimization to help obtain a delicate safety boundary. Comprehensive
evaluations demonstrate PRISM's effectiveness, achieving remarkably low attack
success rates including 0.15% on JailbreakV-28K for Qwen2-VL and 90%
improvement over the previous best method on VLBreak for LLaVA-1.5. PRISM also
exhibits strong robustness against adaptive attacks, significantly increasing
computational costs for adversaries, and generalizes effectively to
out-of-distribution challenges, reducing attack success rates to just 8.70% on
the challenging multi-image MIS benchmark. Remarkably, this robust defense is
achieved while preserving, and in some cases enhancing, model utility. To
promote reproducibility, we have made our code, data, and model weights
available at https://github.com/SaFoLab-WISC/PRISM.

</details>


### [261] [FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation](https://arxiv.org/abs/2508.18684)
*Shaswata Mitra,Azim Bazarov,Martin Duclos,Sudip Mittal,Aritran Piplai,Md Rayhanur Rahman,Edward Zieglar,Shahram Rahimi*

Main category: cs.CR

TL;DR: 介绍FALCON框架利用LLM实时从CTI数据生成可部署的IDS规则并评估，实验证明其在自动规则生成方面表现出色，验证了LLM驱动数据挖掘用于实时网络威胁缓解的可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于签名的IDS因网络威胁不断演变需频繁更新规则，导致部署延迟和安全准备能力下降，需要新方法。

Method: 引入FALCON自主代理框架，利用CTI数据实时生成IDS规则，并使用内置多阶段验证器评估，针对网络和主机两种环境构建规则数据集。

Result: FALCON在自动规则生成方面表现出色，平均准确率达95%，多安全分析师定性评估的评分者间一致性达84%。

Conclusion: LLM驱动的数据挖掘用于实时网络威胁缓解是可行且有效的。

Abstract: Signature-based Intrusion Detection Systems (IDS) detect malicious activities
by matching network or host activity against predefined rules. These rules are
derived from extensive Cyber Threat Intelligence (CTI), which includes attack
signatures and behavioral patterns obtained through automated tools and manual
threat analysis, such as sandboxing. The CTI is then transformed into
actionable rules for the IDS engine, enabling real-time detection and
prevention. However, the constant evolution of cyber threats necessitates
frequent rule updates, which delay deployment time and weaken overall security
readiness. Recent advancements in agentic systems powered by Large Language
Models (LLMs) offer the potential for autonomous IDS rule generation with
internal evaluation. We introduce FALCON, an autonomous agentic framework that
generates deployable IDS rules from CTI data in real-time and evaluates them
using built-in multi-phased validators. To demonstrate versatility, we target
both network (Snort) and host-based (YARA) mediums and construct a
comprehensive dataset of IDS rules with their corresponding CTIs. Our
evaluations indicate FALCON excels in automatic rule generation, with an
average of 95% accuracy validated by qualitative evaluation with 84%
inter-rater agreement among multiple cybersecurity analysts across all metrics.
These results underscore the feasibility and effectiveness of LLM-driven data
mining for real-time cyber threat mitigation.

</details>


### [262] [Attackers Strike Back? Not Anymore -- An Ensemble of RL Defenders Awakens for APT Detection](https://arxiv.org/abs/2508.19072)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.CR

TL;DR: 本文提出结合深度学习、强化学习和主动学习的APT检测框架，解决现有系统静态、无法适应攻击策略变化的问题。


<details>
  <summary>Details</summary>
Motivation: APTs对现代数字基础设施威胁大，传统检测系统难以应对，现有检测系统存在静态、无法适应攻击策略变化的问题。

Method: 将自编码器用于潜在行为编码，结合基于强化学习的多智能体防御者，当智能体决策不确定时触发主动学习循环，通过加权的集成投票机制得出最终预测。

Result: 未提及。

Conclusion: 未提及。

Abstract: Advanced Persistent Threats (APTs) represent a growing menace to modern
digital infrastructure. Unlike traditional cyberattacks, APTs are stealthy,
adaptive, and long-lasting, often bypassing signature-based detection systems.
This paper introduces a novel framework for APT detection that unites deep
learning, reinforcement learning (RL), and active learning into a cohesive,
adaptive defense system. Our system combines auto-encoders for latent
behavioral encoding with a multi-agent ensemble of RL-based defenders, each
trained to distinguish between benign and malicious process behaviors. We
identify a critical challenge in existing detection systems: their static
nature and inability to adapt to evolving attack strategies. To this end, our
architecture includes multiple RL agents (Q-Learning, PPO, DQN, adversarial
defenders), each analyzing latent vectors generated by an auto-encoder. When
any agent is uncertain about its decision, the system triggers an active
learning loop to simulate expert feedback, thus refining decision boundaries.
An ensemble voting mechanism, weighted by each agent's performance, ensures
robust final predictions.

</details>


### [263] [SecureV2X: An Efficient and Privacy-Preserving System for Vehicle-to-Everything (V2X) Applications](https://arxiv.org/abs/2508.19115)
*Joshua Lee,Ali Arastehfard,Weiran Liu,Xuegang Ban,Yuan Hong*

Main category: cs.CR

TL;DR: 随着自动驾驶和V2X技术发展，其机器学习应用存在数据隐私问题，本文提出SecureV2X系统，在两个应用中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决V2X系统中机器学习应用带来的数据隐私问题。

Method: 提出SecureV2X，一个可扩展的多智能体系统用于服务器和车辆间的安全神经网络推理。

Result: 在瞌睡检测中比其他安全系统快9.4倍，计算轮数少143倍，通信量少16.6倍；在闯红灯检测的目标检测任务中运行时间比现有基准快近100倍。

Conclusion: SecureV2X性能强，能有效扩展以同时支持大量安全计算交互。

Abstract: Autonomous driving and V2X technologies have developed rapidly in the past
decade, leading to improved safety and efficiency in modern transportation.
These systems interact with extensive networks of vehicles, roadside
infrastructure, and cloud resources to support their machine learning
capabilities. However, the widespread use of machine learning in V2X systems
raises issues over the privacy of the data involved. This is particularly
concerning for smart-transit and driver safety applications which can
implicitly reveal user locations or explicitly disclose medical data such as
EEG signals. To resolve these issues, we propose SecureV2X, a scalable,
multi-agent system for secure neural network inferences deployed between the
server and each vehicle. Under this setting, we study two multi-agent V2X
applications: secure drowsiness detection, and secure red-light violation
detection. Our system achieves strong performance relative to baselines, and
scales efficiently to support a large number of secure computation interactions
simultaneously. For instance, SecureV2X is $9.4 \times$ faster, requires
$143\times$ fewer computational rounds, and involves $16.6\times$ less
communication on drowsiness detection compared to other secure systems.
Moreover, it achieves a runtime nearly $100\times$ faster than state-of-the-art
benchmarks in object detection tasks for red light violation detection.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [264] [An Analytical Approach to Privacy and Performance Trade-Offs in Healthcare Data Sharing](https://arxiv.org/abs/2508.18513)
*Yusi Wei,Hande Y. Benson,Muge Capan*

Main category: stat.AP

TL;DR: 研究探讨医疗数据共享中隐私保护与数据效用的平衡，评估三种匿名化方法，MO - OBAM效果最佳并为医疗组织提供建议。


<details>
  <summary>Details</summary>
Motivation: 医疗数据二次使用对研究和创新重要，但引发患者隐私担忧，需平衡隐私保护和数据效用。

Method: 使用2013 - 2015年成年住院患者数据集预测败血症情况，识别易受隐私攻击的子群体，评估$k$-匿名、Zheng等人的方法和MO - OBAM三种匿名化方法。

Result: $k$-匿名保护有限，Zheng等人的方法和MO - OBAM隐私保护更强，MO - OBAM效用结果最佳，与原始数据集相比精度和召回率仅变化2%。

Conclusion: 为医疗组织提供负责任的数据共享建议，强调需采用保护弱势群体又不牺牲数据驱动模型性能的匿名化方法。

Abstract: The secondary use of healthcare data is vital for research and clinical
innovation, but it raises concerns about patient privacy. This study
investigates how to balance privacy preservation and data utility in healthcare
data sharing, considering the perspectives of both data providers and data
users. Using a dataset of adult patients hospitalized between 2013 and 2015, we
predict whether sepsis was present at admission or developed during the
hospital stay. We identify sub-populations, such as older adults, frequently
hospitalized patients, and racial minorities, that are especially vulnerable to
privacy attacks due to their unique combinations of demographic and healthcare
utilization attributes. These groups are also critical for machine learning
(ML) model performance. We evaluate three anonymization methods-$k$-anonymity,
the technique by Zheng et al., and the MO-OBAM model-based on their ability to
reduce re-identification risk while maintaining ML utility. Results show that
$k$-anonymity offers limited protection. The methods of Zheng et al. and
MO-OBAM provide stronger privacy safeguards, with MO-OBAM yielding the best
utility outcomes: only a 2% change in precision and recall compared to the
original dataset. This work provides actionable insights for healthcare
organizations on how to share data responsibly. It highlights the need for
anonymization methods that protect vulnerable populations without sacrificing
the performance of data-driven models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [265] [Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches](https://arxiv.org/abs/2508.18293)
*M. Salman Shaukat,Yannik Käckenmeister,Sebastian Bader,Thomas Kirste*

Main category: cs.CV

TL;DR: 文章探讨无真实训练数据下实现可靠水下3D目标检测，对比两种免训练检测范式，模板匹配法表现更优，挑战了水下深度学习依赖数据的传统观念。


<details>
  <summary>Details</summary>
Motivation: 传统水下3D目标检测方法在恶劣声学环境和数据稀缺情况下表现不佳，获取真实标注声纳数据成本高、操作复杂，因此研究无真实训练数据下的可靠检测方法。

Method: 开发并比较两种免训练检测范式，一是基于物理的声纳模拟管道生成合成训练数据用于神经网络，二是基于模型的模板匹配系统利用目标物体几何先验。

Result: 神经网络在模拟场景中mAP达98%，在真实数据中降至40%；模板匹配法在真实数据中mAP维持在83%，无需训练且对噪声和环境变化有强鲁棒性。

Conclusion: 挑战了水下领域深度学习依赖数据的传统观念，建立了免训练水下3D检测的大规模基准，为数据稀缺环境下的相关应用开辟了新可能。

Abstract: Underwater 3D object detection remains one of the most challenging frontiers
in computer vision, where traditional approaches struggle with the harsh
acoustic environment and scarcity of training data. While deep learning has
revolutionized terrestrial 3D detection, its application underwater faces a
critical bottleneck: obtaining sufficient annotated sonar data is prohibitively
expensive and logistically complex, often requiring specialized vessels, expert
surveyors, and favorable weather conditions. This work addresses a fundamental
question: Can we achieve reliable underwater 3D object detection without
real-world training data? We tackle this challenge by developing and comparing
two paradigms for training-free detection of artificial structures in multibeam
echo-sounder point clouds. Our dual approach combines a physics-based sonar
simulation pipeline that generates synthetic training data for state-of-the-art
neural networks, with a robust model-based template matching system that
leverages geometric priors of target objects. Evaluation on real bathymetry
surveys from the Baltic Sea reveals surprising insights: while neural networks
trained on synthetic data achieve 98% mean Average Precision (mAP) on simulated
scenes, they drop to 40% mAP on real sonar data due to domain shift.
Conversely, our template matching approach maintains 83% mAP on real data
without requiring any training, demonstrating remarkable robustness to acoustic
noise and environmental variations. Our findings challenge conventional wisdom
about data-hungry deep learning in underwater domains and establish the first
large-scale benchmark for training-free underwater 3D detection. This work
opens new possibilities for autonomous underwater vehicle navigation, marine
archaeology, and offshore infrastructure monitoring in data-scarce environments
where traditional machine learning approaches fail.

</details>


### [266] [MobileDenseAttn:A Dual-Stream Architecture for Accurate and Interpretable Brain Tumor Detection](https://arxiv.org/abs/2508.18294)
*Shudipta Banik,Muna Das,Trapa Banik,Md. Ehsanul Haque*

Main category: cs.CV

TL;DR: 提出MobileDenseAttn模型用于MRI脑肿瘤检测，经验证有高准确率、稳定F1分数，比基线模型有优势且可解释性强，有望成临床实用工具。


<details>
  <summary>Details</summary>
Motivation: 当前脑肿瘤检测方法存在泛化性差、计算效率低、不可解释和缺乏透明度等问题。

Method: 引入MobileDenseAttn，融合MobileNetV2和DenseNet201双数据流，采用特征级融合，在增强数据集上训练，用GradCAM实现可视化解释。

Result: 在严格5折交叉验证下，训练准确率99.75%，测试准确率98.35%，F1分数稳定为0.9835，比基线模型有优势，GradCAM热图可定位肿瘤区域。

Conclusion: MobileDenseAttn是高效、高性能、可解释的模型，很可能成为现实中脑肿瘤识别的临床实用工具。

Abstract: The detection of brain tumor in MRI is an important aspect of ensuring timely
diagnostics and treatment; however, manual analysis is commonly long and
error-prone. Current approaches are not universal because they have limited
generalization to heterogeneous tumors, are computationally inefficient, are
not interpretable, and lack transparency, thus limiting trustworthiness. To
overcome these issues, we introduce MobileDenseAttn, a fusion model of dual
streams of MobileNetV2 and DenseNet201 that can help gradually improve the
feature representation scale, computing efficiency, and visual explanations via
GradCAM. Our model uses feature level fusion and is trained on an augmented
dataset of 6,020 MRI scans representing glioma, meningioma, pituitary tumors,
and normal samples. Measured under strict 5-fold cross-validation protocols,
MobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of
98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). The
extensive validation shows the stability of the model, and the comparative
analysis proves that it is a great advancement over the baseline models (VGG19,
DenseNet201, MobileNetV2) with a +3.67% accuracy increase and a 39.3% decrease
in training time compared to VGG19. The GradCAM heatmaps clearly show
tumor-affected areas, offering clinically significant localization and
improving interpretability. These findings position MobileDenseAttn as an
efficient, high performance, interpretable model with a high probability of
becoming a clinically practical tool in identifying brain tumors in the real
world.

</details>


### [267] [Can VLMs Recall Factual Associations From Visual References?](https://arxiv.org/abs/2508.18297)
*Dhananjay Ashok,Ashutosh Chaubey,Hirona J. Arai,Jonathan May,Jesse Thomason*

Main category: cs.CV

TL;DR: 研究发现视觉语言模型（VLMs）多模态接地存在系统缺陷，可通过探测内部状态识别不可靠响应，用于视觉问答任务有效果并给出未来方向建议。


<details>
  <summary>Details</summary>
Motivation: 发现视觉语言模型在多模态接地方面存在的问题，推动语言接地研究。

Method: 进行对照研究，分析模型内部状态，利用探针探测不可靠响应。

Result: VLMs在视觉参考时回忆事实知识能力减半，探针标记不可靠响应准确率超92%，用于视觉问答任务可增加覆盖度、降低错误风险。

Conclusion: 解决该可检测的系统缺陷是语言接地重要途径，并给出未来研究方向建议。

Abstract: Through a controlled study, we identify a systematic deficiency in the
multimodal grounding of Vision Language Models (VLMs). While VLMs can recall
factual associations when provided a textual reference to an entity; their
ability to do so is significantly diminished when the reference is visual
instead. Forcing VLMs to rely on image representations of an entity halves
their ability to recall factual knowledge, suggesting that VLMs struggle to
link their internal knowledge of an entity with its image representation. We
show that such linking failures are correlated with the expression of distinct
patterns in model internal states, and that probes on these internal states
achieve over 92% accuracy at flagging cases where the VLM response is
unreliable. These probes can be applied, without retraining, to identify when a
VLM will fail to correctly answer a question that requires an understanding of
multimodal input. When used to facilitate selective prediction on a visual
question answering task, the probes increase coverage by 7.87% (absolute) while
also reducing the risk of error by 0.9% (absolute). Addressing the systematic,
detectable deficiency is an important avenue in language grounding, and we
provide informed recommendations for future directions.

</details>


### [268] [Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight and Custom Architectures with the AerialWaste Dataset](https://arxiv.org/abs/2508.18315)
*Nowshin Sharmily,Rusab Sarmun,Muhammad E. H. Chowdhury,Mir Hamidul Hussain,Saad Bin Abul Kashem,Molla E Majid,Amith Khandakar*

Main category: cs.CV

TL;DR: 本文围绕非法垃圾填埋场检测展开，用AerialWaste数据集训练轻量级模型，组合成集成模型实现高准确率分类。


<details>
  <summary>Details</summary>
Motivation: 非法垃圾填埋场危害大，人工识别困难，且缺乏优质公开数据集，需借助深度学习技术检测。

Method: 使用AerialWaste数据集，训练Mobilenetv2等轻量级深度学习模型，组合最佳模型形成集成模型。

Result: 使用集成和融合技术，对数据集进行二元分类，准确率92.33%，精度92.67%，灵敏度92.33%，F1分数92.41%，特异性92.71%。

Conclusion: 轻量级模型能减少过拟合，集成模型在非法垃圾填埋场检测中有良好表现。

Abstract: Illegal landfills are posing as a hazardous threat to people all over the
world. Due to the arduous nature of manually identifying the location of
landfill, many landfills go unnoticed by authorities and later cause dangerous
harm to people and environment. Deep learning can play a significant role in
identifying these landfills while saving valuable time, manpower and resources.
Despite being a burning concern, good quality publicly released datasets for
illegal landfill detection are hard to find due to security concerns. However,
AerialWaste Dataset is a large collection of 10434 images of Lombardy region of
Italy. The images are of varying qualities, collected from three different
sources: AGEA Orthophotos, WorldView-3, and Google Earth. The dataset contains
professionally curated, diverse and high-quality images which makes it
particularly suitable for scalable and impactful research. As we trained
several models to compare results, we found complex and heavy models to be
prone to overfitting and memorizing training data instead of learning patterns.
Therefore, we chose lightweight simpler models which could leverage general
features from the dataset. In this study, Mobilenetv2, Googlenet, Densenet,
MobileVit and other lightweight deep learning models were used to train and
validate the dataset as they achieved significant success with less
overfitting. As we saw substantial improvement in the performance using some of
these models, we combined the best performing models and came up with an
ensemble model. With the help of ensemble and fusion technique, binary
classification could be performed on this dataset with 92.33% accuracy, 92.67%
precision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity.

</details>


### [269] [Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning](https://arxiv.org/abs/2508.18322)
*Jiangfeng Sun,Sihao He,Zhonghong Ou,Meina Song*

Main category: cs.CV

TL;DR: 提出SSU框架解决多模态情感分析中现有方法问题，在两数据集上获SOTA表现且降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法忽视模态特定结构依赖和语义不对齐，限制了质量、可解释性和鲁棒性。

Method: 提出SSU框架，动态构建模态特定图，引入语义锚点和多视图对比学习目标。

Result: 在CMU - MOSI和CMU - MOSEI数据集上达到SOTA性能，显著降低计算开销。

Conclusion: SSU框架能有效解决现有多模态融合方法的问题，具有可解释性，能捕捉细微情感模式。

Abstract: Multimodal sentiment analysis (MSA) aims to infer emotional states by
effectively integrating textual, acoustic, and visual modalities. Despite
notable progress, existing multimodal fusion methods often neglect
modality-specific structural dependencies and semantic misalignment, limiting
their quality, interpretability, and robustness. To address these challenges,
we propose a novel framework called the Structural-Semantic Unifier (SSU),
which systematically integrates modality-specific structural information and
cross-modal semantic grounding for enhanced multimodal representations.
Specifically, SSU dynamically constructs modality-specific graphs by leveraging
linguistic syntax for text and a lightweight, text-guided attention mechanism
for acoustic and visual modalities, thus capturing detailed intra-modal
relationships and semantic interactions. We further introduce a semantic
anchor, derived from global textual semantics, that serves as a cross-modal
alignment hub, effectively harmonizing heterogeneous semantic spaces across
modalities. Additionally, we develop a multiview contrastive learning objective
that promotes discriminability, semantic consistency, and structural coherence
across intra- and inter-modal views. Extensive evaluations on two widely used
benchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistently
achieves state-of-the-art performance while significantly reducing
computational overhead compared to prior methods. Comprehensive qualitative
analyses further validate SSU's interpretability and its ability to capture
nuanced emotional patterns through semantically grounded interactions.

</details>


### [270] [CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological Visual Question Answering](https://arxiv.org/abs/2508.18430)
*Aranya Saha,Tanvir Ahmed Khan,Ismam Nur Swapnil,Mohammad Ariful Haque*

Main category: cs.CV

TL;DR: 提出CLARIFY框架用于皮肤病视觉问答，结合专家和通才组件，实验显示能提升诊断准确率并减少计算资源需求和延迟。


<details>
  <summary>Details</summary>
Motivation: 通用视觉语言模型用于医疗任务时存在专业诊断准确率受限和推理成本高的问题，需改进。

Method: 引入CLARIFY框架，结合轻量级领域训练图像分类器（专家）和压缩对话式VLM（通才），专家预测指导通才推理，用知识图谱检索模块增强协同。

Result: 在皮肤病数据集实验中，CLARIFY比基线诊断准确率提升18%，VRAM需求和延迟至少降低20%和5%。

Conclusion: 专家 - 通才系统为构建轻量级、可信和临床可行的AI系统提供实用强大范式。

Abstract: Vision-language models (VLMs) have shown significant potential for medical
tasks; however, their general-purpose nature can limit specialized diagnostic
accuracy, and their large size poses substantial inference costs for real-world
clinical deployment. To address these challenges, we introduce CLARIFY, a
Specialist-Generalist framework for dermatological visual question answering
(VQA). CLARIFY combines two components: (i) a lightweight, domain-trained image
classifier (the Specialist) that provides fast and highly accurate diagnostic
predictions, and (ii) a powerful yet compressed conversational VLM (the
Generalist) that generates natural language explanations to user queries. In
our framework, the Specialist's predictions directly guide the Generalist's
reasoning, focusing it on the correct diagnostic path. This synergy is further
enhanced by a knowledge graph-based retrieval module, which grounds the
Generalist's responses in factual dermatological knowledge, ensuring both
accuracy and reliability. This hierarchical design not only reduces diagnostic
errors but also significantly improves computational efficiency. Experiments on
our curated multimodal dermatology dataset demonstrate that CLARIFY achieves an
18\% improvement in diagnostic accuracy over the strongest baseline, a
fine-tuned, uncompressed single-line VLM, while reducing the average VRAM
requirement and latency by at least 20\% and 5\%, respectively. These results
indicate that a Specialist-Generalist system provides a practical and powerful
paradigm for building lightweight, trustworthy, and clinically viable AI
systems.

</details>


### [271] [SAT-SKYLINES: 3D Building Generation from Satellite Imagery and Coarse Geometric Priors](https://arxiv.org/abs/2508.18531)
*Zhangyu Jin,Andrew Feng*

Main category: cs.CV

TL;DR: 提出SatSkylines 3D建筑生成方法，利用卫星图像和粗略几何先验，还开发Skylines - 50K数据集，评估显示模型有效且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的3D生成方法缺乏几何指导难以从卫星图像恢复准确建筑结构，3D细化方法依赖高细节体素输入，从简单先验难以产生满意结果。

Method: 对从插值的含噪粗略先验到详细几何的变换进行建模，开发Skylines - 50K数据集。

Result: 广泛评估表明模型有效且有强泛化能力。

Conclusion: 所提出的SatSkylines方法能解决现有3D建筑生成问题，具有良好性能。

Abstract: We present SatSkylines, a 3D building generation approach that takes
satellite imagery and coarse geometric priors. Without proper geometric
guidance, existing image-based 3D generation methods struggle to recover
accurate building structures from the top-down views of satellite images alone.
On the other hand, 3D detailization methods tend to rely heavily on highly
detailed voxel inputs and fail to produce satisfying results from simple priors
such as cuboids. To address these issues, our key idea is to model the
transformation from interpolated noisy coarse priors to detailed geometries,
enabling flexible geometric control without additional computational cost. We
have further developed Skylines-50K, a large-scale dataset of over 50,000
unique and stylized 3D building assets in order to support the generations of
detailed building models. Extensive evaluations indicate the effectiveness of
our model and strong generalization ability.

</details>


### [272] [Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling](https://arxiv.org/abs/2508.18463)
*Md. Rashid Shahriar Khan,Md. Abrar Hasan,Mohammod Tareq Aziz Justice*

Main category: cs.CV

TL;DR: 提出上下文感知零样本异常检测框架，结合TimeSformer、DPC和CLIP，用InfoNCE和CPC损失联合训练，有上下文门控机制，可泛化到复杂环境。代码开源。


<details>
  <summary>Details</summary>
Motivation: 监控视频中异常检测因不可预测和上下文依赖而具有挑战性，需在不接触异常示例下识别异常事件。

Method: 采用混合架构，结合TimeSformer、DPC和CLIP，用InfoNCE和CPC损失联合训练，有上下文门控机制。

Result: 系统能在复杂环境中泛化到先前未见过的行为。

Conclusion: 该框架弥合了零样本异常检测中时间推理和语义上下文之间的差距。

Abstract: Detecting anomalies in surveillance footage is inherently challenging due to
their unpredictable and context-dependent nature. This work introduces a novel
context-aware zero-shot anomaly detection framework that identifies abnormal
events without exposure to anomaly examples during training. The proposed
hybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal
dynamics and semantic context. TimeSformer serves as the vision backbone to
extract rich spatial-temporal features, while DPC forecasts future
representations to identify temporal deviations. Furthermore, a CLIP-based
semantic stream enables concept-level anomaly detection through
context-specific text prompts. These components are jointly trained using
InfoNCE and CPC losses, aligning visual inputs with their temporal and semantic
representations. A context-gating mechanism further enhances decision-making by
modulating predictions with scene-aware cues or global video features. By
integrating predictive modeling with vision-language understanding, the system
can generalize to previously unseen behaviors in complex environments. This
framework bridges the gap between temporal reasoning and semantic context in
zero-shot anomaly detection for surveillance. The code for this research has
been made available at
https://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance.

</details>


### [273] [ROSE: Remove Objects with Side Effects in Videos](https://arxiv.org/abs/2508.18633)
*Chenxuan Miao,Yutong Feng,Jianshu Zeng,Zixiang Gao,Hantang Liu,Yunfeng Yan,Donglian Qi,Xi Chen,Bin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出ROSE框架用于视频对象及其副作用去除，利用3D渲染引擎生成合成数据，构建自动数据准备管道，基于扩散变压器实现视频修复模型，还推出ROSE - Bench基准，实验显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有视频对象去除方法在处理对象副作用（如阴影、反射等）时，因缺乏成对视频数据监督而效果不佳，需要新方法解决。

Method: 利用3D渲染引擎生成合成数据，构建全自动数据准备管道；基于扩散变压器实现视频修复模型，通过输入全量视频定位相关区域进行擦除，引入额外监督预测受影响区域；推出ROSE - Bench基准进行评估。

Result: ROSE在去除视频对象及其副作用方面比现有视频对象擦除模型性能更优，且能很好地泛化到真实视频场景。

Conclusion: ROSE框架在视频对象及其副作用去除任务上具有有效性和优越性。

Abstract: Video object removal has achieved advanced performance due to the recent
success of video generative models. However, when addressing the side effects
of objects, e.g., their shadows and reflections, existing works struggle to
eliminate these effects for the scarcity of paired video data as supervision.
This paper presents ROSE, termed Remove Objects with Side Effects, a framework
that systematically studies the object's effects on environment, which can be
categorized into five common cases: shadows, reflections, light, translucency
and mirror. Given the challenges of curating paired videos exhibiting the
aforementioned effects, we leverage a 3D rendering engine for synthetic data
generation. We carefully construct a fully-automatic pipeline for data
preparation, which simulates a large-scale paired dataset with diverse scenes,
objects, shooting angles, and camera trajectories. ROSE is implemented as an
video inpainting model built on diffusion transformer. To localize all
object-correlated areas, the entire video is fed into the model for
reference-based erasing. Moreover, additional supervision is introduced to
explicitly predict the areas affected by side effects, which can be revealed
through the differential mask between the paired videos. To fully investigate
the model performance on various side effect removal, we presents a new
benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five
special side effects for comprehensive evaluation. Experimental results
demonstrate that ROSE achieves superior performance compared to existing video
object erasing models and generalizes well to real-world video scenarios. The
project page is https://rose2025-inpaint.github.io/.

</details>


### [274] [Clustering-based Feature Representation Learning for Oracle Bone Inscriptions Detection](https://arxiv.org/abs/2508.18641)
*Ye Tao,Xinran Fu,Honglin Pang,Xi Yang,Chuntao Li*

Main category: cs.CV

TL;DR: 本文提出基于聚类的特征空间表示学习方法用于甲骨文拓片图像自动检测，实验显示该方法提升了主流检测框架性能。


<details>
  <summary>Details</summary>
Motivation: 甲骨文拓片图像因噪声、裂缝等退化因素，传统检测网络效果不佳，需新方法解决。

Method: 利用甲骨文文字库数据集作为先验知识，通过基于聚类的表示学习增强检测网络特征提取，引入基于聚类结果的损失函数优化特征表示并融入总网络损失。

Result: 在两个甲骨文检测数据集上用三种主流检测框架实验，所有框架性能显著提升。

Conclusion: 所提基于聚类的特征空间表示学习方法有效提升了甲骨文拓片图像自动检测性能。

Abstract: Oracle Bone Inscriptions (OBIs), play a crucial role in understanding ancient
Chinese civilization. The automated detection of OBIs from rubbing images
represents a fundamental yet challenging task in digital archaeology, primarily
due to various degradation factors including noise and cracks that limit the
effectiveness of conventional detection networks. To address these challenges,
we propose a novel clustering-based feature space representation learning
method. Our approach uniquely leverages the Oracle Bones Character (OBC) font
library dataset as prior knowledge to enhance feature extraction in the
detection network through clustering-based representation learning. The method
incorporates a specialized loss function derived from clustering results to
optimize feature representation, which is then integrated into the total
network loss. We validate the effectiveness of our method by conducting
experiments on two OBIs detection dataset using three mainstream detection
frameworks: Faster R-CNN, DETR, and Sparse R-CNN. Through extensive
experimentation, all frameworks demonstrate significant performance
improvements.

</details>


### [275] [Improving Noise Robust Audio-Visual Speech Recognition via Router-Gated Cross-Modal Feature Fusion](https://arxiv.org/abs/2508.18734)
*DongHoon Lim,YoungChae Kim,Dong-Hyun Kim,Da-Hee Yang,Joon-Hyuk Chang*

Main category: cs.CV

TL;DR: 提出一种新的AVSR框架，在LRS3实验中降低了字错率，提升了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有系统难以估计音频可靠性和动态调整模态依赖，在嘈杂环境中实现鲁棒的视听语音识别仍具挑战。

Method: 提出路由器门控跨模态特征融合框架，基于令牌级声学损坏分数自适应重新加权音频和视觉特征，通过门控交叉注意力机制降低不可靠音频令牌权重并增强视觉线索。

Result: 在LRS3上实验，与AV - HuBERT相比，字错率相对降低16.51 - 42.67%，消融实验证实路由器和门控机制有助于提升鲁棒性。

Conclusion: 所提出的框架能有效提升AVSR在嘈杂环境中的鲁棒性。

Abstract: Robust audio-visual speech recognition (AVSR) in noisy environments remains
challenging, as existing systems struggle to estimate audio reliability and
dynamically adjust modality reliance. We propose router-gated cross-modal
feature fusion, a novel AVSR framework that adaptively reweights audio and
visual features based on token-level acoustic corruption scores. Using an
audio-visual feature fusion-based router, our method down-weights unreliable
audio tokens and reinforces visual cues through gated cross-attention in each
decoder layer. This enables the model to pivot toward the visual modality when
audio quality deteriorates. Experiments on LRS3 demonstrate that our approach
achieves an 16.51-42.67% relative reduction in word error rate compared to
AV-HuBERT. Ablation studies confirm that both the router and gating mechanism
contribute to improved robustness under real-world acoustic noise.

</details>


### [276] [Are All Marine Species Created Equal? Performance Disparities in Underwater Object Detection](https://arxiv.org/abs/2508.18729)
*Melanie Wille,Tobias Fischer,Scarlett Raine*

Main category: cs.CV

TL;DR: 本文探讨水下物体检测中特定类别表现差异的因素及提升表现不佳海洋物种检测效果的方法，分析发现定位中前景背景区分是难题，分类存在特征难题，给出数据分布建议并指出应聚焦算法改进。


<details>
  <summary>Details</summary>
Motivation: 水下物体检测有独特挑战，不同物种检测效果不同且原因不明，旨在解决影响特定类别表现差异的因素和如何提升表现不佳海洋物种检测效果的问题。

Method: 操作DUO数据集，将目标检测任务分为定位和分类，用YOLO11和TIDE进行定位分析，开展分类实验。

Result: 定位分析显示前景背景区分是难题，与数据量无关；分类实验表明即使数据平衡仍有精度差距，存在特征难题。

Conclusion: 优先精度时推荐不平衡分布，优先召回时推荐平衡分布；提升表现不佳类别应聚焦算法改进，尤其是定位模块，公开代码和数据集。

Abstract: Underwater object detection is critical for monitoring marine ecosystems but
poses unique challenges, including degraded image quality, imbalanced class
distribution, and distinct visual characteristics. Not every species is
detected equally well, yet underlying causes remain unclear. We address two key
research questions: 1) What factors beyond data quantity drive class-specific
performance disparities? 2) How can we systematically improve detection of
under-performing marine species? We manipulate the DUO dataset to separate the
object detection task into localization and classification and investigate the
under-performance of the scallop class. Localization analysis using YOLO11 and
TIDE finds that foreground-background discrimination is the most problematic
stage regardless of data quantity. Classification experiments reveal persistent
precision gaps even with balanced data, indicating intrinsic feature-based
challenges beyond data scarcity and inter-class dependencies. We recommend
imbalanced distributions when prioritizing precision, and balanced
distributions when prioritizing recall. Improving under-performing classes
should focus on algorithmic advances, especially within localization modules.
We publicly release our code and datasets.

</details>


### [277] [PseudoMapTrainer: Learning Online Mapping without HD Maps](https://arxiv.org/abs/2508.18788)
*Christian Löwens,Thorben Funke,Jingchao Xie,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: 提出PseudoMapTrainer用于在线地图构建，用无标签数据生成伪标签训练模型，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有在线地图模型训练依赖昂贵且地理多样性不足的高清地图，影响泛化性。

Method: 通过高斯溅射和预训练2D分割网络从多相机图像重建路面生成伪标签，引入掩码感知分配算法和损失函数处理部分掩码伪标签，还可半监督预训练模型。

Result: 实现无真实地图训练在线地图模型，伪标签可用于半监督预训练。

Conclusion: PseudoMapTrainer是一种有效的在线地图构建新方法，能利用无标签数据，减少对真实地图的依赖。

Abstract: Online mapping models show remarkable results in predicting vectorized maps
from multi-view camera images only. However, all existing approaches still rely
on ground-truth high-definition maps during training, which are expensive to
obtain and often not geographically diverse enough for reliable generalization.
In this work, we propose PseudoMapTrainer, a novel approach to online mapping
that uses pseudo-labels generated from unlabeled sensor data. We derive those
pseudo-labels by reconstructing the road surface from multi-camera imagery
using Gaussian splatting and semantics of a pre-trained 2D segmentation
network. In addition, we introduce a mask-aware assignment algorithm and loss
function to handle partially masked pseudo-labels, allowing for the first time
the training of online mapping models without any ground-truth maps.
Furthermore, our pseudo-labels can be effectively used to pre-train an online
model in a semi-supervised manner to leverage large-scale unlabeled
crowdsourced data. The code is available at
github.com/boschresearch/PseudoMapTrainer.

</details>


### [278] [Interpretable Decision-Making for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.18898)
*Mona Mirzaie,Bodo Rosenhahn*

Main category: cs.CV

TL;DR: 本文提出自动驾驶中增强模型可解释性并优化控制命令的方法，经实验验证该方法有效且表现出色。


<details>
  <summary>Details</summary>
Motivation: 端到端方法在自动驾驶决策解释上有挑战，尤其是复杂城市场景，因深度神经网络非线性决策边界难以掌握AI决策逻辑，需增强可解释性。

Method: 提出促进模型可解释性的损失函数，生成稀疏和局部特征图，进行特征提取步骤的消融研究。

Result: 方法提升了可解释性，减少违规，得到安全高性能驾驶模型，单目非集成模型在CARLA Leaderboard上表现超越顶尖方法。

Conclusion: 所提方法能有效增强自动驾驶模型可解释性，优化控制命令，实现更安全高效驾驶。

Abstract: Trustworthy AI is mandatory for the broad deployment of autonomous vehicles.
Although end-to-end approaches derive control commands directly from raw data,
interpreting these decisions remains challenging, especially in complex urban
scenarios. This is mainly attributed to very deep neural networks with
non-linear decision boundaries, making it challenging to grasp the logic behind
AI-driven decisions. This paper presents a method to enhance interpretability
while optimizing control commands in autonomous driving. To address this, we
propose loss functions that promote the interpretability of our model by
generating sparse and localized feature maps. The feature activations allow us
to explain which image regions contribute to the predicted control command. We
conduct comprehensive ablation studies on the feature extraction step and
validate our method on the CARLA benchmarks. We also demonstrate that our
approach improves interpretability, which correlates with reducing infractions,
yielding a safer, high-performance driving model. Notably, our monocular,
non-ensemble model surpasses the top-performing approaches from the CARLA
Leaderboard by achieving lower infraction scores and the highest route
completion rate, all while ensuring interpretability.

</details>


### [279] [Enhancing compact convolutional transformers with super attention](https://arxiv.org/abs/2508.18960)
*Simpenzwe Honore Leandre,Natenaile Asmamaw Shiferaw,Dillip Rout*

Main category: cs.CV

TL;DR: 提出采用token混合、序列池化和卷积分词器的视觉模型，在CIFAR100基准测试中表现优异且推理高效，代码开源。


<details>
  <summary>Details</summary>
Motivation: 在固定上下文长度任务中实现最先进性能和高效推理。

Method: 采用token混合、序列池化和卷积分词器构建视觉模型。

Result: 在CIFAR100基准测试中，top 1%和top 5%验证准确率显著提升，比SDPA变压器更高效，模型规模仅为60%，训练稳定性高，不依赖数据增强等技术。

Conclusion: 所提出的视觉模型能在固定上下文长度任务中取得良好效果，具有高效和稳定的特点。

Abstract: In this paper, we propose a vision model that adopts token mixing,
sequence-pooling, and convolutional tokenizers to achieve state-of-the-art
performance and efficient inference in fixed context-length tasks. In the
CIFAR100 benchmark, our model significantly improves the baseline of the top 1%
and top 5% validation accuracy from 36.50% to 46.29% and 66.33% to 76.31%,
while being more efficient than the Scaled Dot Product Attention (SDPA)
transformers when the context length is less than the embedding dimension and
only 60% the size. In addition, the architecture demonstrates high training
stability and does not rely on techniques such as data augmentation like mixup,
positional embeddings, or learning rate scheduling. We make our code available
on Github.

</details>


### [280] [USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning](https://arxiv.org/abs/2508.18966)
*Shaojin Wu,Mengqi Huang,Yufeng Cheng,Wenxu Wu,Jiahe Tian,Yiming Luo,Fei Ding,Qian He*

Main category: cs.CV

TL;DR: 本文提出统一风格-主题优化定制模型USO，统一风格和主题驱动生成任务，实验表明其在主题一致性和风格相似性上达开源模型最优。


<details>
  <summary>Details</summary>
Motivation: 现有文献将风格驱动和主题驱动生成视为分离任务，存在明显对立，需统一二者目标。

Method: 构建大规模三元组数据集，引入解耦学习方案，结合风格奖励学习范式SRL，发布联合评估基准USO - Bench。

Result: USO在主题一致性和风格相似性两方面达到开源模型的最先进性能。

Conclusion: 提出的USO模型能有效统一风格和主题驱动生成任务，性能优越。

Abstract: Existing literature typically treats style-driven and subject-driven
generation as two disjoint tasks: the former prioritizes stylistic similarity,
whereas the latter insists on subject consistency, resulting in an apparent
antagonism. We argue that both objectives can be unified under a single
framework because they ultimately concern the disentanglement and
re-composition of content and style, a long-standing theme in style-driven
research. To this end, we present USO, a Unified Style-Subject Optimized
customization model. First, we construct a large-scale triplet dataset
consisting of content images, style images, and their corresponding stylized
content images. Second, we introduce a disentangled learning scheme that
simultaneously aligns style features and disentangles content from style
through two complementary objectives, style-alignment training and
content-style disentanglement training. Third, we incorporate a style
reward-learning paradigm denoted as SRL to further enhance the model's
performance. Finally, we release USO-Bench, the first benchmark that jointly
evaluates style similarity and subject fidelity across multiple metrics.
Extensive experiments demonstrate that USO achieves state-of-the-art
performance among open-source models along both dimensions of subject
consistency and style similarity. Code and model:
https://github.com/bytedance/USO

</details>


### [281] [GReAT: leveraging geometric artery data to improve wall shear stress assessment](https://arxiv.org/abs/2508.19030)
*Julian Suk,Jolanda J. Wentzel,Patryk Rygiel,Joost Daemen,Daniel Rueckert,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 本文探讨用3D血管几何模型大数据集助力冠状动脉壁切应力评估，创建自监督目标，展示几何表征可提升有限数据下的分割效果。


<details>
  <summary>Details</summary>
Motivation: 利用大数据进行患者护理在心血管健康等领域有前景，但训练机器学习模型的数据稀缺，且利用学习表征改善血流动力学生物标志物评估在冠状动脉方面研究不足。

Method: 使用含8449个形状的3D血管几何模型大数据集，通过计算热核签名创建自监督目标。

Result: 即使在有限数据上训练，从该数据集学习到的几何表征也能促进冠状动脉壁切应力区域的分割。

Conclusion: 3D血管几何模型大数据集可助力冠状动脉模型的壁切应力评估。

Abstract: Leveraging big data for patient care is promising in many medical fields such
as cardiovascular health. For example, hemodynamic biomarkers like wall shear
stress could be assessed from patient-specific medical images via machine
learning algorithms, bypassing the need for time-intensive computational fluid
simulation. However, it is extremely challenging to amass large-enough datasets
to effectively train such models. We could address this data scarcity by means
of self-supervised pre-training and foundations models given large datasets of
geometric artery models. In the context of coronary arteries, leveraging
learned representations to improve hemodynamic biomarker assessment has not yet
been well studied. In this work, we address this gap by investigating whether a
large dataset (8449 shapes) consisting of geometric models of 3D blood vessels
can benefit wall shear stress assessment in coronary artery models from a
small-scale clinical trial (49 patients). We create a self-supervised target
for the 3D blood vessels by computing the heat kernel signature, a quantity
obtained via Laplacian eigenvectors, which captures the very essence of the
shapes. We show how geometric representations learned from this datasets can
boost segmentation of coronary arteries into regions of low, mid and high
(time-averaged) wall shear stress even when trained on limited data.

</details>


### [282] [Learning Binary Sampling Patterns for Single-Pixel Imaging using Bilevel Optimisation](https://arxiv.org/abs/2508.19068)
*Serban C. Tudosie,Alexander Denker,Zeljko Kereta,Simon Arridge*

Main category: cs.CV

TL;DR: 提出用于单像素成像的双层优化方法学习特定任务的二进制照明模式，在数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 为单像素荧光显微镜等应用优化照明模式，以提高单像素成像中物体重建性能。

Method: 采用双层优化方法，用直通估计器处理二进制模式优化的不可微性，在双层公式中利用总深度变分正则化器。

Result: 在CytoImageNet显微镜数据集上，学习到的模式在高度欠采样情况下重建性能优于基线方法。

Conclusion: 所提出的双层优化方法能有效学习特定任务的二进制照明模式，提升单像素成像重建效果。

Abstract: Single-Pixel Imaging enables reconstructing objects using a single detector
through sequential illuminations with structured light patterns. We propose a
bilevel optimisation method for learning task-specific, binary illumination
patterns, optimised for applications like single-pixel fluorescence microscopy.
We address the non-differentiable nature of binary pattern optimisation using
the Straight-Through Estimator and leveraging a Total Deep Variation
regulariser in the bilevel formulation. We demonstrate our method on the
CytoImageNet microscopy dataset and show that learned patterns achieve superior
reconstruction performance compared to baseline methods, especially in highly
undersampled regimes.

</details>


### [283] [The point is the mask: scaling coral reef segmentation with weak supervision](https://arxiv.org/abs/2508.18958)
*Matteo Contini,Victor Illien,Sylvain Poulain,Serge Bernard,Julien Barde,Sylvain Bonhommeau,Alexis Joly*

Main category: cs.CV

TL;DR: 提出多尺度弱监督语义分割框架，实现用无人机图像进行大规模珊瑚礁映射，是高分辨率珊瑚礁监测的可扩展且经济高效方法。


<details>
  <summary>Details</summary>
Motivation: 大规模监测珊瑚礁有挑战，无人机图像分辨率有限，获取像素级标注成本高，限制深度学习分割方法扩展性。

Method: 提出多尺度弱监督语义分割框架，将水下图像的精细生态信息转移到航空数据，结合基于分类的监督、空间插值和自蒸馏技术。

Result: 该方法有效，可实现珊瑚形态类型的大面积分割，且能灵活整合新类别。

Conclusion: 本研究提供了一种可扩展、经济高效的高分辨率珊瑚礁监测方法，结合了低成本数据收集、弱监督深度学习和多尺度遥感。

Abstract: Monitoring coral reefs at large spatial scales remains an open challenge,
essential for assessing ecosystem health and informing conservation efforts.
While drone-based aerial imagery offers broad spatial coverage, its limited
resolution makes it difficult to reliably distinguish fine-scale classes, such
as coral morphotypes. At the same time, obtaining pixel-level annotations over
large spatial extents is costly and labor-intensive, limiting the scalability
of deep learning-based segmentation methods for aerial imagery. We present a
multi-scale weakly supervised semantic segmentation framework that addresses
this challenge by transferring fine-scale ecological information from
underwater imagery to aerial data. Our method enables large-scale coral reef
mapping from drone imagery with minimal manual annotation, combining
classification-based supervision, spatial interpolation and self-distillation
techniques. We demonstrate the efficacy of the approach, enabling large-area
segmentation of coral morphotypes and demonstrating flexibility for integrating
new classes. This study presents a scalable, cost-effective methodology for
high-resolution reef monitoring, combining low-cost data collection, weakly
supervised deep learning and multi-scale remote sensing.

</details>


### [284] [RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation](https://arxiv.org/abs/2508.19003)
*Siyuan You,Guozheng Xu,Pengwei Zhou,Qiwen Jin,Jian Yao,Li Li*

Main category: cs.CV

TL;DR: 提出名为RoofSeg的端到端边缘感知变压器网络用于从LiDAR点云分割屋顶平面，设计EAMM模块和新损失函数提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的屋顶平面分割方法存在非端到端、边缘特征判别性低、未充分考虑平面几何特征约束训练等问题。

Method: 开发RoofSeg网络，利用变压器编解码器框架结合可学习平面查询分层预测平面实例掩码，设计EAMM模块，提出自适应加权策略和新的平面几何损失。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Roof plane segmentation is one of the key procedures for reconstructing
three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from
airborne light detection and ranging (LiDAR) point clouds. The majority of
current approaches for roof plane segmentation rely on the manually designed or
learned features followed by some specifically designed geometric clustering
strategies. Because the learned features are more powerful than the manually
designed features, the deep learning-based approaches usually perform better
than the traditional approaches. However, the current deep learning-based
approaches have three unsolved problems. The first is that most of them are not
truly end-to-end, the plane segmentation results may be not optimal. The second
is that the point feature discriminability near the edges is relatively low,
leading to inaccurate planar edges. The third is that the planar geometric
characteristics are not sufficiently considered to constrain the network
training. To solve these issues, a novel edge-aware transformer-based network,
named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds
in a truly end-to-end manner. In the RoofSeg, we leverage a transformer
encoder-decoder-based framework to hierarchically predict the plane instance
masks with the use of a set of learnable plane queries. To further improve the
segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module
(EAMM) that sufficiently incorporates planar geometric prior of edges to
enhance its discriminability for plane instance mask refinement. In addition,
we propose an adaptive weighting strategy in the mask loss to reduce the
influence of misclassified points, and also propose a new plane geometric loss
to constrain the network training.

</details>


### [285] [Few-Shot Connectivity-Aware Text Line Segmentation in Historical Documents](https://arxiv.org/abs/2508.19162)
*Rafael Sterzinger,Tingyu Lin,Robert Sablatnig*

Main category: cs.CV

TL;DR: 本文探讨历史文档文本行分割，提出轻量级架构与拓扑感知损失函数结合的方法，在小数据集上表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 深度学习自动进行文本行分割需大量标注数据，历史文档标注数据稀缺且标注成本高，因此探索少样本学习方向。

Method: 将轻量级UNet++与连通性感知损失函数配对，在每份手稿仅三个标注页面提取的小补丁上训练。

Result: 在U - DIADS - TL数据集上显著提升性能，识别准确率提高200%，行交并比提高75%，F - 分数与DIVA - HisDB基线检测任务竞赛获胜者相当或更优。

Conclusion: 小而简单的架构与拓扑感知损失函数结合在少样本学习的文本行分割任务中更准确且数据效率更高，证明了该方法的有效性。

Abstract: A foundational task for the digital analysis of documents is text line
segmentation. However, automating this process with deep learning models is
challenging because it requires large, annotated datasets that are often
unavailable for historical documents. Additionally, the annotation process is a
labor- and cost-intensive task that requires expert knowledge, which makes
few-shot learning a promising direction for reducing data requirements. In this
work, we demonstrate that small and simple architectures, coupled with a
topology-aware loss function, are more accurate and data-efficient than more
complex alternatives. We pair a lightweight UNet++ with a connectivity-aware
loss, initially developed for neuron morphology, which explicitly penalizes
structural errors like line fragmentation and unintended line merges. To
increase our limited data, we train on small patches extracted from a mere
three annotated pages per manuscript. Our methodology significantly improves
upon the current state-of-the-art on the U-DIADS-TL dataset, with a 200%
increase in Recognition Accuracy and a 75% increase in Line Intersection over
Union. Our method also achieves an F-Measure score on par with or even
exceeding that of the competition winner of the DIVA-HisDB baseline detection
task, all while requiring only three annotated pages, exemplifying the efficacy
of our approach. Our implementation is publicly available at:
https://github.com/RafaelSterzinger/acpr_few_shot_hist.

</details>


### [286] [No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes](https://arxiv.org/abs/2508.19060)
*Blaž Rolih,Matic Fučka,Danijel Skočaj*

Main category: cs.CV

TL;DR: 针对表面缺陷检测现有方法不足，提出SuperSimpleNet模型，能适应四种监督场景，性能优且速度快，有望解决制造业挑战。


<details>
  <summary>Details</summary>
Motivation: 现有表面缺陷检测方法无法满足工业对高性能、效率和适应性的需求，且难以适应多种监督场景。

Method: 基于SimpleNet构建SuperSimpleNet，融入新的合成异常生成过程、增强的分类头和改进的学习程序。

Result: 在四个具有挑战性的基准数据集上表现良好，推理时间低于10ms。

Conclusion: SuperSimpleNet能统一不同监督范式，兼具速度和可靠性，是解决实际制造挑战、弥合学术研究与工业应用差距的有前景一步。

Abstract: Surface defect detection is a critical task across numerous industries, aimed
at efficiently identifying and localising imperfections or irregularities on
manufactured components. While numerous methods have been proposed, many fail
to meet industrial demands for high performance, efficiency, and adaptability.
Existing approaches are often constrained to specific supervision scenarios and
struggle to adapt to the diverse data annotations encountered in real-world
manufacturing processes, such as unsupervised, weakly supervised, mixed
supervision, and fully supervised settings. To address these challenges, we
propose SuperSimpleNet, a highly efficient and adaptable discriminative model
built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel
synthetic anomaly generation process, an enhanced classification head, and an
improved learning procedure, enabling efficient training in all four
supervision scenarios, making it the first model capable of fully leveraging
all available data annotations. SuperSimpleNet sets a new standard for
performance across all scenarios, as demonstrated by its results on four
challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an
inference time below 10 ms. With its ability to unify diverse supervision
paradigms while maintaining outstanding speed and reliability, SuperSimpleNet
represents a promising step forward in addressing real-world manufacturing
challenges and bridging the gap between academic research and industrial
applications. Code: https://github.com/blaz-r/SuperSimpleNet

</details>


### [287] [LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding](https://arxiv.org/abs/2508.19204)
*Julian Ost,Andrea Ramazzina,Amogh Joshi,Maximilian Bömer,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 本文提出一种直接生成大规模3D驾驶场景的方法，结合代理几何生成和分数蒸馏，实现高可控性和逼真的3D场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有神经重建方法场景和轨迹多样性受限，图像或视频扩散模型缺乏几何基础和因果性，需一种能生成准确几何的大规模3D驾驶场景的方法。

Method: 结合代理几何和环境表示的生成与从学习的2D图像先验进行分数蒸馏。

Result: 该方法具有高可控性，能根据地图布局生成逼真且几何一致的复杂驾驶场景的3D场景。

Conclusion: 所提方法能有效弥合现有方法的差距，生成准确几何的大规模3D驾驶场景。

Abstract: Large-scale scene data is essential for training and testing in robot
learning. Neural reconstruction methods have promised the capability of
reconstructing large physically-grounded outdoor scenes from captured sensor
data. However, these methods have baked-in static environments and only allow
for limited scene control -- they are functionally constrained in scene and
trajectory diversity by the captures from which they are reconstructed. In
contrast, generating driving data with recent image or video diffusion models
offers control, however, at the cost of geometry grounding and causality. In
this work, we aim to bridge this gap and present a method that directly
generates large-scale 3D driving scenes with accurate geometry, allowing for
causal novel view synthesis with object permanence and explicit 3D geometry
estimation. The proposed method combines the generation of a proxy geometry and
environment representation with score distillation from learned 2D image
priors. We find that this approach allows for high controllability, enabling
the prompt-guided geometry and high-fidelity texture and structure that can be
conditioned on map layouts -- producing realistic and geometrically consistent
3D generations of complex driving scenes.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [288] [Data-Driven Discovery and Formulation Refines the Quasi-Steady Model of Flapping-Wing Aerodynamics](https://arxiv.org/abs/2508.18703)
*Yu Kamimizu,Hao Liu,Toshiyuki Nakata*

Main category: physics.flu-dyn

TL;DR: 为提升准定常气动模型精度，采用数据驱动方法，识别三个关键机制，降低预测误差，可用于昆虫飞行研究和仿生飞行机器人开发。


<details>
  <summary>Details</summary>
Motivation: 现有昆虫拍动翼非定常气动力评估方法存在高保真模拟成本高、缺乏解释力，准定常理论模型精度低的问题，需提升准定常模型精度。

Method: 采用数据驱动方法，从5000个候选运动学函数中筛选，确定三个关键附加机制的数学表达式。

Result: 将这些机制纳入准定常模型后，在不同雷诺数下均显著降低了以计算流体动力学结果为基准的预测误差。

Conclusion: 数据驱动的准定常模型可实现快速气动分析，是研究昆虫飞行进化适应和开发仿生飞行机器人的实用工具。

Abstract: Insects control unsteady aerodynamic forces on flapping wings to navigate
complex environments. While understanding these forces is vital for biology,
physics, and engineering, existing evaluation methods face trade-offs:
high-fidelity simulations are computationally or experimentally expensive and
lack explanatory power, whereas theoretical models based on quasi-steady
assumptions offer insights but exhibit low accuracy. To overcome these
limitations and thus enhance the accuracy of quasi-steady aerodynamic models,
we applied a data-driven approach involving discovery and formulation of
previously overlooked critical mechanisms. Through selection from 5,000
candidate kinematic functions, we identified mathematical expressions for three
key additional mechanisms -- the effect of advance ratio, effect of spanwise
kinematic velocity, and rotational Wagner effect -- which had been
qualitatively recognized but were not formulated. Incorporating these
mechanisms considerably reduced the prediction errors of the quasi-steady model
using the computational fluid dynamics results as the ground truth, both in
hawkmoth forward flight (at high Reynolds numbers) and fruit fly maneuvers (at
low Reynolds numbers). The data-driven quasi-steady model enables rapid
aerodynamic analysis, serving as a practical tool for understanding
evolutionary adaptations in insect flight and developing bio-inspired flying
robots.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [289] [Temperature-Aware Recurrent Neural Operator for Temperature-Dependent Anisotropic Plasticity in HCP Materials](https://arxiv.org/abs/2508.18806)
*Yannick Hollenweger,Dennis M. Kochman,Burigede Liu*

Main category: cond-mat.mtrl-sci

TL;DR: 本文引入温度感知循环神经算子（TRNO）解决现有神经网络代理模型的不足，应用于多晶镁的塑性响应建模，表现良好并实现多尺度模拟加速。


<details>
  <summary>Details</summary>
Motivation: 现有塑性本构定律的神经网络代理模型存在训练时间长、预测外推性差和处理材料行为简单等问题。

Method: 引入时间分辨率无关的温度感知循环神经算子（TRNO），应用于多晶镁的温度相关塑性响应建模。

Result: TRNO 具有高预测精度，能在不同加载情况、温度和时间分辨率下有效泛化，在训练效率和预测性能上优于传统 GRU 和 LSTM 模型。

Conclusion: TRNO 能解决现有模型的局限，在多尺度模拟中比传统本构模型至少快三个数量级。

Abstract: Neural network surrogate models for constitutive laws in computational
mechanics have been in use for some time. In plasticity, these models often
rely on gated recurrent units (GRUs) or long short-term memory (LSTM) cells,
which excel at capturing path-dependent phenomena. However, they suffer from
long training times and time-resolution-dependent predictions that extrapolate
poorly. Moreover, most existing surrogates for macro- or mesoscopic plasticity
handle only relatively simple material behavior. To overcome these limitations,
we introduce the Temperature-Aware Recurrent Neural Operator (TRNO), a
time-resolution-independent neural architecture. We apply the TRNO to model the
temperature-dependent plastic response of polycrystalline magnesium, which
shows strong plastic anisotropy and thermal sensitivity. The TRNO achieves high
predictive accuracy and generalizes effectively across diverse loading cases,
temperatures, and time resolutions. It also outperforms conventional GRU and
LSTM models in training efficiency and predictive performance. Finally, we
demonstrate multiscale simulations with the TRNO, yielding a speedup of at
least three orders of magnitude over traditional constitutive models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [290] [Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges](https://arxiv.org/abs/2508.18296)
*Edgar Rangel,Fabio Martinez*

Main category: eess.IV

TL;DR: 本文开发协作框架，用中心独立表征知识分割缺血性中风病变，FedAvg模型表现佳且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 中风是全球致死致残重要原因，现有病变分析因多种因素高度可变，计算支持方法有局限性，临床中心样本不足。

Method: 开发基于共享深度中心独立表征知识的协作框架来分割DWI序列中的缺血性中风病变。

Result: FedAvg模型在所有中心上取得较好指标，优于集中式和其他联邦规则，在不同病变类别和分布外中心表现良好。

Conclusion: 所开发的协作框架有效，FedAvg模型具有强泛化性能。

Abstract: Stroke is the second leading cause of death and the third leading cause of
disability worldwide. Clinical guidelines establish diffusion resonance imaging
(DWI, ADC) as the standard for localizing, characterizing, and measuring
infarct volume, enabling treatment support and prognosis. Nonetheless, such
lesion analysis is highly variable due to different patient demographics,
scanner vendors, and expert annotations. Computational support approaches have
been key to helping with the localization and segmentation of lesions. However,
these strategies are dedicated solutions that learn patterns from only one
institution, lacking the variability to generalize geometrical lesions shape
models. Even worse, many clinical centers lack sufficient labeled samples to
adjust these dedicated solutions. This work developed a collaborative framework
for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge
from deep center-independent representations. From 14 emulated healthcare
centers with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \pm
0.24$, AVD of $5.29 \pm 22.74$, ALD of $2.16 \pm 3.60$ and LF1 of $0.70 \pm
0.26$ over all centers, outperforming both the centralized and other federated
rules. Interestingly, the model demonstrated strong generalization properties,
showing uniform performance across different lesion categories and reliable
performance in out-of-distribution centers (with DSC of $0.64 \pm 0.29$ and AVD
of $4.44 \pm 8.74$ without any additional training).

</details>


### [291] [Analise de Desaprendizado de Maquina em Modelos de Classificacao de Imagens Medicas](https://arxiv.org/abs/2508.18509)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: eess.IV

TL;DR: 评估SalUn无学习模型在医学图像分类中的应用，分析数据增强影响，结果显示其表现接近全重训练。


<details>
  <summary>Details</summary>
Motivation: 机器学习无学习技术在医学图像分类中未被探索，旨在将其应用于医学图像分类并评估效果。

Method: 在PathMNIST、OrganAMNIST和BloodMNIST数据集上对SalUn无学习模型进行实验，分析数据增强对无学习质量的影响。

Result: SalUn表现接近全重训练。

Conclusion: SalUn是医学应用中有效的解决方案。

Abstract: Machine unlearning aims to remove private or sensitive data from a
pre-trained model while preserving the model's robustness. Despite recent
advances, this technique has not been explored in medical image classification.
This work evaluates the SalUn unlearning model by conducting experiments on the
PathMNIST, OrganAMNIST, and BloodMNIST datasets. We also analyse the impact of
data augmentation on the quality of unlearning. Results show that SalUn
achieves performance close to full retraining, indicating an efficient solution
for use in medical applications.

</details>


### [292] [A Deep Learning Application for Psoriasis Detection](https://arxiv.org/abs/2508.18528)
*Anna Milani,Fábio S. da Silva,Elloá B. Guedes,Ricardo Rios*

Main category: eess.IV

TL;DR: 本文对比ResNet50、Inception v3和VGG19三种CNN模型对银屑病皮肤病变图像的分类性能，结果表明Inception v3表现良好。


<details>
  <summary>Details</summary>
Motivation: 寻找用于银屑病皮肤病变图像分类的有效CNN模型，辅助银屑病诊断。

Method: 使用从专业平台获取的图像对ResNet50、Inception v3和VGG19模型进行训练和验证，并采用一些技术调整神经网络评估指标。

Result: Inception v3模型在准确率和F1分数方面表现令人满意，为97.5% ± 0.2。

Conclusion: Inception v3可作为支持银屑病诊断的有价值工具。

Abstract: In this paper a comparative study of the performance of three Convolutional
Neural Network models, ResNet50, Inception v3 and VGG19 for classification of
skin images with lesions affected by psoriasis is presented. The images used
for training and validation of the models were obtained from specialized
platforms. Some techniques were used to adjust the evaluation metrics of the
neural networks. The results found suggest the model Inception v3 as a valuable
tool for supporting the diagnosis of psoriasis. This is due to its satisfactory
performance with respect to accuracy and F1-Score (97.5% ${\pm}$ 0.2).

</details>


### [293] [Stress-testing cross-cancer generalizability of 3D nnU-Net for PET-CT tumor segmentation: multi-cohort evaluation with novel oesophageal and lung cancer datasets](https://arxiv.org/abs/2508.18612)
*Soumen Ghosh,Christine Jestin Hannan,Rajat Vashistha,Parveen Kundu,Sandra Brosda,Lauren G. Aoude,James Lonie,Andrew Nathanson,Jessica Ng,Andrew P. Barbour,Viktor Vegh*

Main category: eess.IV

TL;DR: 论文对nnU - Net在PET - CT上进行跨癌症评估，引入新数据集，测试三种训练范式，发现组合训练结果更平衡，强调数据集多样性对鲁棒泛化的重要性。


<details>
  <summary>Details</summary>
Motivation: 在临床PET - CT工作流中，深度学习肿瘤分割需具备鲁棒泛化能力，因解剖部位、扫描仪和患者群体差异大，故开展跨癌症评估。

Method: 引入两个专家标注的全身数据集，对3D nnUNet模型在目标仅训练、仅用公共数据集训练和组合训练三种范式下进行训练和测试。

Result: 目标仅训练模型域内精度高但泛化差；仅公共数据集训练模型泛化好但特定域表现差；组合训练结果最平衡，减少边界误差，提高鲁棒性。

Conclusion: 数据集多样性，尤其是多人口统计、多中心和多癌症整合，是鲁棒泛化的关键，而非模型架构新颖性，临床鲁棒分割应基于数据集多样性。

Abstract: Robust generalization is essential for deploying deep learning based tumor
segmentation in clinical PET-CT workflows, where anatomical sites, scanners,
and patient populations vary widely. This study presents the first cross cancer
evaluation of nnU-Net on PET-CT, introducing two novel, expert-annotated
whole-body datasets. 279 patients with oesophageal cancer (Australian cohort)
and 54 with lung cancer (Indian cohort). These cohorts complement the public
AutoPET dataset and enable systematic stress-testing of cross domain
performance. We trained and tested 3D nnUNet models under three paradigms.
Target only (oesophageal), public only (AutoPET), and combined training. For
the tested sets, the oesophageal only model achieved the best in-domain
accuracy (mean DSC, 57.8) but failed on external Indian lung cohort (mean DSC
less than 3.4), indicating severe overfitting. The public only model
generalized more broadly (mean DSC, 63.5 on AutoPET, 51.6 on Indian lung
cohort) but underperformed in oesophageal Australian cohort (mean DSC, 26.7).
The combined approach provided the most balanced results (mean DSC, lung
(52.9), oesophageal (40.7), AutoPET (60.9)), reducing boundary errors and
improving robustness across all cohorts. These findings demonstrate that
dataset diversity, particularly multi demographic, multi center and multi
cancer integration, outweighs architectural novelty as the key driver of robust
generalization. This work presents the demography based cross cancer deep
learning segmentation evaluation and highlights dataset diversity, rather than
model complexity, as the foundation for clinically robust segmentation.

</details>


### [294] [ModAn-MulSupCon: Modality-and Anatomy-Aware Multi-Label Supervised Contrastive Pretraining for Medical Imaging](https://arxiv.org/abs/2508.18613)
*Eichi Takaya,Ryusei Inamori*

Main category: eess.IV

TL;DR: 提出ModAn - MulSupCon方法利用医学影像元数据预训练，在多任务微调中有较好表现，不同场景有不同优势。


<details>
  <summary>Details</summary>
Motivation: 医学影像专家标注限制大规模监督预训练，普遍存在的元数据未充分利用，需利用元数据学习可迁移表示。

Method: 将图像模态和解剖结构编码为多热向量，用Jaccard加权多标签监督对比损失在miniRIN上预训练ResNet - 18编码器，在三个二分类任务上微调与线性探测评估。

Result: 微调时，ModAn - MulSupCon在MRNet - ACL和Thyroid上AUC最佳，在Breast上排第二；编码器冻结时，SimCLR/ImageNet更优。

Conclusion: 将模态/解剖元数据编码为多标签目标提供实用可扩展预训练信号，微调可行时提升下游准确性，ModAn - MulSupCon适用于标签稀缺场景，SimCLR/ImageNet适用于编码器冻结部署。

Abstract: Background and objective: Expert annotations limit large-scale supervised
pretraining in medical imaging, while ubiquitous metadata (modality, anatomical
region) remain underused. We introduce ModAn-MulSupCon, a modality- and
anatomy-aware multi-label supervised contrastive pretraining method that
leverages such metadata to learn transferable representations.
  Method: Each image's modality and anatomy are encoded as a multi-hot vector.
A ResNet-18 encoder is pretrained on a mini subset of RadImageNet (miniRIN,
16,222 images) with a Jaccard-weighted multi-label supervised contrastive loss,
and then evaluated by fine-tuning and linear probing on three binary
classification tasks--ACL tear (knee MRI), lesion malignancy (breast
ultrasound), and nodule malignancy (thyroid ultrasound).
  Result: With fine-tuning, ModAn-MulSupCon achieved the best AUC on MRNet-ACL
(0.964) and Thyroid (0.763), surpassing all baselines ($p<0.05$), and ranked
second on Breast (0.926) behind SimCLR (0.940; not significant). With the
encoder frozen, SimCLR/ImageNet were superior, indicating that ModAn-MulSupCon
representations benefit most from task adaptation rather than linear
separability.
  Conclusion: Encoding readily available modality/anatomy metadata as
multi-label targets provides a practical, scalable pretraining signal that
improves downstream accuracy when fine-tuning is feasible. ModAn-MulSupCon is a
strong initialization for label-scarce clinical settings, whereas
SimCLR/ImageNet remain preferable for frozen-encoder deployments.

</details>


### [295] [HOTSPOT-YOLO: A Lightweight Deep Learning Attention-Driven Model for Detecting Thermal Anomalies in Drone-Based Solar Photovoltaic Inspections](https://arxiv.org/abs/2508.18912)
*Mahmoud Dhimish*

Main category: eess.IV

TL;DR: 研究开发轻量级AI模型HOTSPOT - YOLO用于光伏系统热异常检测，实验效果好，为大规模光伏检测提供可靠方案。


<details>
  <summary>Details</summary>
Motivation: 确保光伏系统运行效率、降低维护成本，解决检测小而细微热异常的挑战。

Method: 开发集成高效卷积神经网络骨干和注意力机制的轻量级AI模型HOTSPOT - YOLO。

Result: 平均精度均值达90.8%，比基线目标检测模型有显著提升，计算负载降低，在不同环境条件下有鲁棒性。

Conclusion: HOTSPOT - YOLO为大规模光伏检测提供可扩展可靠方案，先进AI技术与工程应用结合革新可再生能源系统自动故障检测。

Abstract: Thermal anomaly detection in solar photovoltaic (PV) systems is essential for
ensuring operational efficiency and reducing maintenance costs. In this study,
we developed and named HOTSPOT-YOLO, a lightweight artificial intelligence (AI)
model that integrates an efficient convolutional neural network backbone and
attention mechanisms to improve object detection. This model is specifically
designed for drone-based thermal inspections of PV systems, addressing the
unique challenges of detecting small and subtle thermal anomalies, such as
hotspots and defective modules, while maintaining real-time performance.
Experimental results demonstrate a mean average precision of 90.8%, reflecting
a significant improvement over baseline object detection models. With a reduced
computational load and robustness under diverse environmental conditions,
HOTSPOT-YOLO offers a scalable and reliable solution for large-scale PV
inspections. This work highlights the integration of advanced AI techniques
with practical engineering applications, revolutionizing automated fault
detection in renewable energy systems.

</details>


### [296] [Random forest-based out-of-distribution detection for robust lung cancer segmentation](https://arxiv.org/abs/2508.19112)
*Aneesh Rangnekar,Harini Veeraraghavan*

Main category: eess.IV

TL;DR: 提出RF - Deep随机森林分类器，利用预训练transformer编码器的深度特征检测OOD扫描，提升癌症分割可靠性，在多数据集测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: Transformer基模型在OOD数据集上癌症分割准确性下降，需要提升癌症分割在ID和OOD场景中的可靠性。

Method: 构建包含Swin Transformer编码器和卷积解码器的分割模型，用SimMIM在大量未标记3D CT扫描上预训练编码器，训练解码器进行肺癌分割；使用RF - Deep利用预训练编码器的深度特征检测OOD扫描。

Result: 在603个3D CT公共数据集上测试，RF - Deep在PE、COVID - 19和腹部CT上检测OOD病例的FPR95分别为18.26%、27.66%和小于0.1%，优于现有OOD方法。

Conclusion: RF - Deep分类器是一种简单有效的方法，可提升ID和OOD场景下癌症分割的可靠性。

Abstract: Accurate detection and segmentation of cancerous lesions from computed
tomography (CT) scans is essential for automated treatment planning and cancer
treatment response assessment. Transformer-based models with self-supervised
pretraining can produce reliably accurate segmentation from in-distribution
(ID) data but degrade when applied to out-of-distribution (OOD) datasets. We
address this challenge with RF-Deep, a random forest classifier that utilizes
deep features from a pretrained transformer encoder of the segmentation model
to detect OOD scans and enhance segmentation reliability. The segmentation
model comprises a Swin Transformer encoder, pretrained with masked image
modeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and
non-cancerous conditions, with a convolution decoder, trained to segment lung
cancers in 317 3D scans. Independent testing was performed on 603 3D CT public
datasets that included one ID dataset and four OOD datasets comprising chest
CTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney
cancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of
18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs,
consistently outperforming established OOD approaches. The RF-Deep classifier
provides a simple and effective approach to enhance reliability of cancer
segmentation in ID and OOD scenarios.

</details>


### [297] [RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration](https://arxiv.org/abs/2508.19154)
*Yan Chen,Yi Wen,Wei Li,Junchao Liu,Yong Guo,Jie Hu,Xinghao Chen*

Main category: eess.IV

TL;DR: 提出RAW域扩散模型RDDM，可直接从传感器RAW数据恢复逼真图像，实验显示其优于sRGB扩散方法。


<details>
  <summary>Details</summary>
Motivation: 近期sRGB域扩散方法在高保真和真实生成间存在困境，处理有损sRGB输入且忽略传感器RAW图像可获取性，导致性能不佳。

Method: 提出RAW域VAE学习最优潜在表示；设计可微的Post Tone Processing模块实现RAW和sRGB空间联合优化；开发可扩展的退化管道合成RAW低质量 - 高质量图像对；设计可配置的多拜耳LoRA模块处理不同RAW模式。

Result: RDDM优于现有sRGB扩散方法，结果保真度更高且伪影更少。

Conclusion: RDDM通过在RAW域直接恢复图像，解决了sRGB域扩散方法的局限，具有更好的性能。

Abstract: We present the RAW domain diffusion model (RDDM), an end-to-end diffusion
model that restores photo-realistic images directly from the sensor RAW data.
While recent sRGB-domain diffusion methods achieve impressive results, they are
caught in a dilemma between high fidelity and realistic generation. As these
models process lossy sRGB inputs and neglect the accessibility of the sensor
RAW images in many scenarios, e.g., in image and video capturing in edge
devices, resulting in sub-optimal performance. RDDM bypasses this limitation by
directly restoring images in the RAW domain, replacing the conventional
two-stage image signal processing (ISP) + IR pipeline. However, a simple
adaptation of pre-trained diffusion models to the RAW domain confronts the
out-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE
(RVAE) learning optimal latent representations, (2) a differentiable Post Tone
Processing (PTP) module enabling joint RAW and sRGB space optimization. To
compensate for the deficiency in the dataset, we develop a scalable degradation
pipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets for
large-scale training. Furthermore, we devise a configurable multi-bayer (CMB)
LoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensive
experiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusion
methods, yielding higher fidelity results with fewer artifacts.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [298] [Huracan: A skillful end-to-end data-driven system for ensemble data assimilation and weather prediction](https://arxiv.org/abs/2508.18486)
*Zekun Ni,Jonathan Weyn,Hang Zhang,Yanfei Xiang,Jiang Bian,Weixin Jin,Kit Thambiratnam,Qi Zhang,Haiyu Dong,Hongyu Sun*

Main category: physics.ao-ph

TL;DR: 提出Huracan观测驱动的天气预报系统，结合数据同化与预报模型，仅依赖观测输入实现高精度预报，表现比肩ECMWF ENS，推动端到端数据驱动天气预报发展。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习数据驱动天气预报模型依赖数值天气预报（NWP）初始条件，限制了预报能力，此前端到端系统预报能力不及NWP。

Method: 提出Huracan系统，结合集成数据同化模型与预报模型，仅以观测数据为输入。

Result: Huracan是首个提供集成初始条件和端到端集成天气预报的系统，在75.4%的变量和提前时间组合上，连续排名概率得分与ECMWF ENS相当或更优。

Conclusion: 该工作是端到端数据驱动天气预报的重大进展，为改进和革新业务天气预报提供了机会。

Abstract: Over the past few years, machine learning-based data-driven weather
prediction has been transforming operational weather forecasting by providing
more accurate forecasts while using a mere fraction of computing power compared
to traditional numerical weather prediction (NWP). However, those models still
rely on initial conditions from NWP, putting an upper limit on their forecast
abilities. A few end-to-end systems have since been proposed, but they have yet
to match the forecast skill of state-of-the-art NWP competitors. In this work,
we propose Huracan, an observation-driven weather forecasting system which
combines an ensemble data assimilation model with a forecast model to produce
highly accurate forecasts relying only on observations as inputs. Huracan is
not only the first to provide ensemble initial conditions and end-to-end
ensemble weather forecasts, but also the first end-to-end system to achieve an
accuracy comparable with that of ECMWF ENS, the state-of-the-art NWP
competitor, despite using a smaller amount of available observation data.
Notably, Huracan matches or exceeds the continuous ranked probability score of
ECMWF ENS on 75.4% of the variable and lead time combinations. Our work is a
major step forward in end-to-end data-driven weather prediction and opens up
opportunities for further improving and revolutionizing operational weather
forecasting.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [299] [Murakkab: Resource-Efficient Agentic Workflow Orchestration in Cloud Platforms](https://arxiv.org/abs/2508.18298)
*Gohar Irfan Chaudhry,Esha Choukse,Haoran Qiu,Íñigo Goiri,Rodrigo Fonseca,Adam Belay,Ricardo Bianchini*

Main category: cs.MA

TL;DR: 现有框架服务智能体工作流效率低，本文提出资源高效的服务系统Murakkab，能降低资源使用和成本。


<details>
  <summary>Details</summary>
Motivation: 当前框架服务智能体工作流效率低下，工作流组件耦合导致资源浪费和服务目标下降。

Method: 引入声明式抽象解耦工作流规范与执行配置，通过轮廓引导优化器和自适应运行时联合管理全栈。

Result: 在不同工作流上评估显示，Murakkab能将GPU使用降低2.8倍、能耗降低3.7倍、成本降低4.3倍，同时维持服务目标。

Conclusion: Murakkab实现了现有框架和云调度器无法达到的跨层优化，能高效服务智能体工作流。

Abstract: Agentic workflows commonly coordinate multiple models and tools with complex
control logic. They are quickly becoming the dominant paradigm for AI
applications. However, serving them remains inefficient with today's
frameworks. The key problem is that they expose workflows as opaque sequences
of model and tool calls that tightly couple agent logic with model and hardware
choices. Often, these workflow components are fragmented across different
entities, preventing systems from reasoning about trade-offs across accuracy,
latency, energy, and cost. This leads to resource waste and degraded
service-level objectives (SLOs).
  We present Murakkab, a resource-efficient serving system for agentic
workflows. Murakkab introduces a declarative abstraction that decouples
workflow specification from execution configuration. A profile-guided optimizer
and adaptive runtime jointly manage the full stack: orchestrating workflow
components, mapping them to models and hardware, and dynamically reconfiguring
execution to satisfy user-defined SLOs. By exposing the internal structure of
agentic workflows, Murakkab enables cross-layer optimization that existing
frameworks and cloud schedulers cannot achieve.
  Our evaluation on diverse workflows shows that \sysname{} reduces GPU usage
by up to 2.8$\times$, energy consumption by 3.7$\times$, and cost by
4.3$\times$ while maintaining SLOs.

</details>


### [300] [Consensus Is All You Need: Gossip-Based Reasoning Among Large Language Models](https://arxiv.org/abs/2508.18292)
*Saksham Arora*

Main category: cs.MA

TL;DR: 借鉴分布式系统中的八卦协议，让大语言模型相互交换答案达成共识，实现多智能体AI推理，结果显示该方法有效且使AI更具协作性和可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型各有优劣，单一模型无法在所有领域表现出色，因此寻求提升模型性能的方法。

Method: 借鉴分布式系统中的八卦协议，让每个大语言模型作为对等网络中的节点，相互分享答案和思考过程以达成集体决策。

Result: “基于八卦的共识”方法实现了强大、有弹性且准确的多智能体AI推理，克服了单个模型的弱点，发挥了集体优势。

Conclusion: 该方法使AI更具协作性和可信度，不再只是黑盒程序。

Abstract: Large language models have advanced rapidly, but no single model excels in
every area -- each has its strengths and weaknesses. Instead of relying on one
model alone, we take inspiration from gossip protocols in distributed systems,
where information is exchanged with peers until they all come to an agreement.
In this setup, models exchange answers and gradually work toward a shared
solution. Each LLM acts as a node in a peer-to-peer network, sharing responses
and thought processes to reach a collective decision. Our results show that
this "gossip-based consensus" leads to robust, resilient, and accurate
multi-agent AI reasoning. It helps overcome the weaknesses of individual models
and brings out their collective strengths. This approach is similar to how
humans build consensus, making AI seem more collaborative and trustworthy
instead of just a black-box program.

</details>


### [301] [Toward Generalized Autonomous Agents: A Neuro-Symbolic AI Framework for Integrating Social and Technical Support in Education](https://arxiv.org/abs/2508.18406)
*Ryan Hare,Ying Tang*

Main category: cs.MA

TL;DR: 本文提出多智能体神经符号框架解决教育挑战，通过案例展示跨领域适应性并展望未来。


<details>
  <summary>Details</summary>
Motivation: 解决如何让学生自主学习的教育挑战，利用LLMs和神经符号系统改进数字学习环境支持。

Method: 提出多智能体神经符号框架，为不同智能体分配教学角色，通过中央教育本体统一。

Result: 通过大学和中学案例研究，证明框架具有跨领域适应性。

Conclusion: 概述关键见解和推进AI驱动学习环境的未来方向。

Abstract: One of the enduring challenges in education is how to empower students to
take ownership of their learning by setting meaningful goals, tracking their
progress, and adapting their strategies when faced with setbacks. Research has
shown that this form of leaner-centered learning is best cultivated through
structured, supportive environments that promote guided practice, scaffolded
inquiry, and collaborative dialogue. In response, educational efforts have
increasingly embraced artificial-intelligence (AI)-powered digital learning
environments, ranging from educational apps and virtual labs to serious games.
Recent advances in large language models (LLMs) and neuro-symbolic systems,
meanwhile, offer a transformative opportunity to reimagine how support is
delivered in digital learning environments. LLMs are enabling socially
interactive learning experiences and scalable, cross-domain learning support
that can adapt instructional strategies across varied subjects and contexts. In
parallel, neuro-symbolic AI provides new avenues for designing these agents
that are not only adaptive but also scalable across domains. Based on these
remarks, this paper presents a multi-agent, neuro-symbolic framework designed
to resolve the aforementioned challenges. The framework assigns distinct
pedagogical roles to specialized agents: an RL-based 'tutor' agent provides
authoritative, non-verbal scaffolding, while a proactive, LLM-powered 'peer'
agent facilitates the social dimensions of learning. While prior work has
explored such agents in isolation, our framework's novelty lies in unifying
them through a central educational ontology. Through case studies in both
college-level and middle school settings, we demonstrate the framework's
adaptability across domains. We conclude by outlining key insights and future
directions for advancing AI-driven learning environments.

</details>


### [302] [Skill-Aligned Fairness in Multi-Agent Learning for Collaboration in Healthcare](https://arxiv.org/abs/2508.18708)
*Promise Osaine Ekpo,Brian La,Thomas Wiener,Saesha Agarwal,Arshia Agrawal,Gonzalo Gonzalez-Pumariega,Lekan P. Molu,Angelique Taylor*

Main category: cs.MA

TL;DR: 本文针对多智能体强化学习公平性问题，提出FairSkillMARL框架和MARLHospital环境，实验表明仅基于工作量的公平性会导致任务 - 技能不匹配。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习公平性常忽略智能体专业技能和现实领域的结构化协调，在医疗领域需要考虑工作量平衡和专业技能匹配。

Method: 提出FairSkillMARL框架，定义公平性为工作量平衡和技能 - 任务匹配的双重目标；引入MARLHospital环境；将FairSkillMARL与四种标准多智能体强化学习方法结合，并与两种最先进的公平性指标进行对比实验。

Result: 仅基于等量工作量的公平性可能导致任务 - 技能不匹配。

Conclusion: 本文为研究异构多智能体系统中的公平性提供了工具和基础，强调努力与专业技能匹配的重要性。

Abstract: Fairness in multi-agent reinforcement learning (MARL) is often framed as a
workload balance problem, overlooking agent expertise and the structured
coordination required in real-world domains. In healthcare, equitable task
allocation requires workload balance or expertise alignment to prevent burnout
and overuse of highly skilled agents. Workload balance refers to distributing
an approximately equal number of subtasks or equalised effort across healthcare
workers, regardless of their expertise. We make two contributions to address
this problem. First, we propose FairSkillMARL, a framework that defines
fairness as the dual objective of workload balance and skill-task alignment.
Second, we introduce MARLHospital, a customizable healthcare-inspired
environment for modeling team compositions and energy-constrained scheduling
impacts on fairness, as no existing simulators are well-suited for this
problem. We conducted experiments to compare FairSkillMARL in conjunction with
four standard MARL methods, and against two state-of-the-art fairness metrics.
Our results suggest that fairness based solely on equal workload might lead to
task-skill mismatches and highlight the need for more robust metrics that
capture skill-task misalignment. Our work provides tools and a foundation for
studying fairness in heterogeneous multi-agent systems where aligning effort
with expertise is critical.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [303] [A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants](https://arxiv.org/abs/2508.18587)
*Barış Bayazıt,Yao Li,Xujie Si*

Main category: cs.PL

TL;DR: 本文对两个成熟Rocq项目进行案例研究，定量和定性分析大语言模型（LLMs）在使用证明助手自动生成证明任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 不清楚大语言模型在使用证明助手进行证明自动化任务中的有效性。

Method: 基于两个成熟Rocq项目（hs - to - coq工具和Verdi）进行案例研究，通过定量和定性分析评估大语言模型生成证明的有效性。

Result: （1）外部依赖和同一源文件中的上下文可显著助力证明生成；（2）大语言模型在小证明上表现出色，也能生成大证明；（3）大语言模型在不同验证项目上表现不同；（4）大语言模型能生成简洁巧妙的证明、将经典技术应用于新定义，但也会犯奇怪错误。

Conclusion: 对大语言模型在使用证明助手进行证明自动化任务中的表现有了多方面的认识，明确其优势与不足。

Abstract: Large language models (LLMs) can potentially help with verification using
proof assistants by automating proofs. However, it is unclear how effective
LLMs are in this task. In this paper, we perform a case study based on two
mature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the
effectiveness of LLMs in generating proofs by both quantitative and qualitative
analysis. Our study finds that: (1) external dependencies and context in the
same source file can significantly help proof generation; (2) LLMs perform
great on small proofs but can also generate large proofs; (3) LLMs perform
differently on different verification projects; and (4) LLMs can generate
concise and smart proofs, apply classical techniques to new definitions, but
can also make odd mistakes.

</details>
