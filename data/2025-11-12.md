<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 69]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 94]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 15]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [stat.CO](#stat.CO) [Total: 4]
- [cs.MA](#cs.MA) [Total: 1]
- [econ.GN](#econ.GN) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [math.CO](#math.CO) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 47]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.CR](#cs.CR) [Total: 18]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [hep-ph](#hep-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SD](#cs.SD) [Total: 8]
- [cs.PL](#cs.PL) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [cs.CY](#cs.CY) [Total: 5]
- [eess.SP](#eess.SP) [Total: 3]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 27]
- [math.HO](#math.HO) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Analysing Environmental Efficiency in AI for X-Ray Diagnosis](https://arxiv.org/abs/2511.07436)
*Liam Kearns*

Main category: cs.AI

TL;DR: 本文将大语言模型与小型判别模型集成到Mendix应用中检测新冠，对比14种模型配置，发现小型模型有偏差，使用小LLM有局限，Covid - Net模型最有效。


<details>
  <summary>Details</summary>
Motivation: 将AI工具集成到医疗应用以提高诊断效率，探索大语言模型和小型判别模型在新冠检测中的应用及对比。

Method: 将大语言模型和小型判别模型集成到Mendix应用检测新冠，用判别模型为大语言模型提供知识库，进行14种不同模型配置的基准研究。

Result: 小型模型有诊断偏差和概率置信度问题；限制大语言模型给出概率输出表现差；GPT - 4.1 - Nano减少碳足迹但仍不如判别模型；Covid - Net模型准确率95.5%，碳足迹比GPT - 4.5 - Preview少99.9%。

Conclusion: 对比了生成式和判别式模型在新冠检测中的表现，指出使用生成式工具进行分类任务的环境风险。

Abstract: The integration of AI tools into medical applications has aimed to improve the efficiency of diagnosis. The emergence of large language models (LLMs), such as ChatGPT and Claude, has expanded this integration even further. Because of LLM versatility and ease of use through APIs, these larger models are often utilised even though smaller, custom models can be used instead. In this paper, LLMs and small discriminative models are integrated into a Mendix application to detect Covid-19 in chest X-rays. These discriminative models are also used to provide knowledge bases for LLMs to improve accuracy. This provides a benchmark study of 14 different model configurations for comparison of accuracy and environmental impact. The findings indicated that while smaller models reduced the carbon footprint of the application, the output was biased towards a positive diagnosis and the output probabilities were lacking confidence. Meanwhile, restricting LLMs to only give probabilistic output caused poor performance in both accuracy and carbon footprint, demonstrating the risk of using LLMs as a universal AI solution. While using the smaller LLM GPT-4.1-Nano reduced the carbon footprint by 94.2% compared to the larger models, this was still disproportionate to the discriminative models; the most efficient solution was the Covid-Net model. Although it had a larger carbon footprint than other small models, its carbon footprint was 99.9% less than when using GPT-4.5-Preview, whilst achieving an accuracy of 95.5%, the highest of all models examined. This paper contributes to knowledge by comparing generative and discriminative models in Covid-19 detection as well as highlighting the environmental risk of using generative tools for classification tasks.

</details>


### [2] [Agentic Educational Content Generation for African Languages on Edge Devices](https://arxiv.org/abs/2511.07437)
*Ravi Gupta,Guneet Bhatia*

Main category: cs.AI

TL;DR: 研究提出自主代理编排框架用于撒哈拉以南非洲边缘设备生成教育内容，实验验证性能佳，欲通过合作建立可持续教育基础，助力联合国可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 解决撒哈拉以南非洲的教育不平等问题，在资源受限环境实现可及、本地化和可持续的人工智能驱动教育。

Method: 采用四个专业代理协同工作，在边缘设备上生成上下文合适的教育内容。

Result: 在Raspberry Pi 4B和NVIDIA Jetson Nano等平台实验，InkubaLM有良好性能指标，框架在多语言质量、文化相关性和流畅性方面表现出色。

Conclusion: 该研究为资源受限环境的教育奠定基础，有助于联合国可持续发展目标4、9和10的实现。

Abstract: Addressing educational inequity in Sub-Saharan Africa, this research presents an autonomous agent-orchestrated framework for decentralized, culturally adaptive educational content generation on edge devices. The system leverages four specialized agents that work together to generate contextually appropriate educational content. Experimental validation on platforms including Raspberry Pi 4B and NVIDIA Jetson Nano demonstrates significant performance achievements. InkubaLM on Jetson Nano achieved a Time-To-First-Token (TTFT) of 129 ms, an average inter-token latency of 33 ms, and a throughput of 45.2 tokens per second while consuming 8.4 W. On Raspberry Pi 4B, InkubaLM also led with 326 ms TTFT and 15.9 tokens per second at 5.8 W power consumption. The framework consistently delivered high multilingual quality, averaging a BLEU score of 0.688, cultural relevance of 4.4/5, and fluency of 4.2/5 across tested African languages. Through potential partnerships with active community organizations including African Youth & Community Organization (AYCO) and Florida Africa Foundation, this research aims to establish a practical foundation for accessible, localized, and sustainable AI-driven education in resource-constrained environments. Keeping focus on long-term viability and cultural appropriateness, it contributes to United Nations SDGs 4, 9, and 10. Index Terms - Multi-Agent Systems, Edge AI Computing, Educational Technology, African Languages, Rural Education, Sustainable Development, UN SDG.

</details>


### [3] [Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning](https://arxiv.org/abs/2511.07483)
*Qianxi He,Qingyu Ren,Shanzhe Lei,Xuhong Wang,Yingchun Wang*

Main category: cs.AI

TL;DR: 提出基于置信度的奖励模型提升STEM推理能力，经测试效果优于多个开源奖励模型并开源代码。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则奖励的强化学习在小模型上存在推理链质量差、推理与答案不一致等问题，限制资源有限组织在小模型上训练。

Method: 提出基于置信度的奖励模型，对错误答案和低置信度正确响应进行惩罚。

Result: 通过静态评估、Best-of-N推理测试和基于PPO的RL训练验证，在多个STEM基准测试中优于多个开源奖励模型。

Conclusion: 基于置信度的奖励模型能有效提升STEM推理能力。

Abstract: Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional instruction tuning and human preference alignment toward reinforcement learning (RL) focused on reasoning capabilities. However, numerous technical reports indicate that purely rule-based reward RL frequently results in poor-quality reasoning chains or inconsistencies between reasoning processes and final answers, particularly when the base model is of smaller scale. During the RL exploration process, models might employ low-quality reasoning chains due to the lack of knowledge, occasionally producing correct answers randomly and receiving rewards based on established rule-based judges. This constrains the potential for resource-limited organizations to conduct direct reinforcement learning training on smaller-scale models. We propose a novel confidence-based reward model tailored for enhancing STEM reasoning capabilities. Unlike conventional approaches, our model penalizes not only incorrect answers but also low-confidence correct responses, thereby promoting more robust and logically consistent reasoning. We validate the effectiveness of our approach through static evaluations, Best-of-N inference tests, and PPO-based RL training. Our method outperforms several state-of-the-art open-source reward models across diverse STEM benchmarks. We release our codes and model in https://github.com/qianxiHe147/C2RM.

</details>


### [4] [Prudential Reliability of Large Language Models in Reinsurance: Governance, Assurance, and Capital Efficiency](https://arxiv.org/abs/2511.08082)
*Stella C. Dong*

Main category: cs.AI

TL;DR: 本文为再保险中评估大语言模型可靠性开发了审慎框架，通过RAIRAB实施，取得良好效果，表明现有审慎原则可适应可靠AI。


<details>
  <summary>Details</summary>
Motivation: 为再保险中评估大语言模型可靠性建立框架。

Method: 构建五支柱架构，将监管期望转化为可衡量的生命周期控制，并通过RAIRAB实施框架。

Result: 检索接地配置在六项任务中实现更高接地准确性（0.90），减少约40%的幻觉和解释漂移，透明度几乎翻倍。

Conclusion: 当治理明确、数据可追溯、保证可验证时，现有审慎原则能适应可靠的AI。

Abstract: This paper develops a prudential framework for assessing the reliability of large language models (LLMs) in reinsurance. A five-pillar architecture--governance, data lineage, assurance, resilience, and regulatory alignment--translates supervisory expectations from Solvency II, SR 11-7, and guidance from EIOPA (2025), NAIC (2023), and IAIS (2024) into measurable lifecycle controls. The framework is implemented through the Reinsurance AI Reliability and Assurance Benchmark (RAIRAB), which evaluates whether governance-embedded LLMs meet prudential standards for grounding, transparency, and accountability. Across six task families, retrieval-grounded configurations achieved higher grounding accuracy (0.90), reduced hallucination and interpretive drift by roughly 40%, and nearly doubled transparency. These mechanisms lower informational frictions in risk transfer and capital allocation, showing that existing prudential doctrines already accommodate reliable AI when governance is explicit, data are traceable, and assurance is verifiable.

</details>


### [5] [Procedural Knowledge Improves Agentic LLM Workflows](https://arxiv.org/abs/2511.07568)
*Vincent Hsiao,Mark Roberts,Leslie Smith*

Main category: cs.AI

TL;DR: 研究利用分层任务网络（HTN）形式的程序知识的代理式大语言模型（LLM）工作流，发现手编码和LLM创建的HTN都能提升LLM在代理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在无大量工具支持、提示工程或微调时执行代理任务有困难，且缺乏对利用程序知识提升LLM在需隐式规划的代理任务上表现的评估。

Method: 形式化、实现并评估利用分层任务网络（HTN）形式的程序知识的代理式LLM工作流。

Result: 手编码的HTN能显著提升LLM在代理任务上的表现，可使20b或70b参数的LLM超越120b参数的LLM基线；LLM创建的HTN也能提升整体表现，但效果稍弱。

Conclusion: 利用来自人类、文档或LLM的专业知识来整理程序知识将成为改进LLM工作流的重要工具。

Abstract: Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. Despite research showing that domain-dependent, procedural knowledge can dramatically increase planning efficiency, little work evaluates its potential for improving LLM performance on agentic tasks that may require implicit planning. We formalize, implement, and evaluate an agentic LLM workflow that leverages procedural knowledge in the form of a hierarchical task network (HTN). Empirical results of our implementation show that hand-coded HTNs can dramatically improve LLM performance on agentic tasks, and using HTNs can boost a 20b or 70b parameter LLM to outperform a much larger 120b parameter LLM baseline. Furthermore, LLM-created HTNs improve overall performance, though less so. The results suggest that leveraging expertise--from humans, documents, or LLMs--to curate procedural knowledge will become another important tool for improving LLM workflows.

</details>


### [6] [Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models](https://arxiv.org/abs/2511.07581)
*Supriti Vijay,Aman Priyanshu,Anu Vellore,Baturay Saglam,Amin Karbasi*

Main category: cs.AI

TL;DR: 提出Orion训练框架，使紧凑型模型通过学习搜索策略进行迭代检索，在多个数据集上表现出色，表明检索性能可源于学习策略而非仅模型规模。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索方法在处理复杂用户查询时，无法捕捉探索、反馈和修订的迭代动态，如神经检索器缺乏推理能力、大语言模型成本高、查询重写或分解有局限。

Method: Orion框架结合合成轨迹生成与监督微调、强化学习以及推理时的束搜索算法。

Result: 1.2B模型虽仅使用3%训练数据，但在多个数据集上取得比先前检索器更好的成绩，在六个基准中的五个上优于大200 - 400倍的检索器。

Conclusion: 当模型经过搜索、反思和修订训练时，检索性能可从学习策略中产生，而非仅依赖模型规模。

Abstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.

</details>


### [7] [Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces](https://arxiv.org/abs/2511.07587)
*Shreyas Rajesh,Pavan Holur,Chenda Duan,David Chong,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: 提出生成语义工作区（GSW）框架，增强大语言模型长上下文推理能力，在EpBench上表现优于现有基线，且效率高。


<details>
  <summary>Details</summary>
Motivation: 大语言模型长上下文推理存在挑战，现有解决方案无法构建时空锚定的叙事表征。

Method: 提出GSW框架，包含Operator和Reconciler，构建结构化、可解释的情境表征。

Result: 在EpBench上，GSW比现有基于RAG的基线性能高20%，减少51%查询时上下文令牌。

Conclusion: GSW为赋予大语言模型类人情景记忆提供蓝图，有助于构建更强大的智能体。

Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \textbf{20\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \textbf{51\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.

</details>


### [8] [AI-Driven Contribution Evaluation and Conflict Resolution: A Framework & Design for Group Workload Investigation](https://arxiv.org/abs/2511.07667)
*Jakub Slapek,Mir Seyedebrahimi,Yang Jianhua*

Main category: cs.AI

TL;DR: 文章指出团队个人贡献公平评估存在挑战，提出AI增强工具框架用于争端调查，介绍框架维度、分析方法等并讨论可行性等问题。


<details>
  <summary>Details</summary>
Motivation: 解决团队中个人贡献公平评估难题，当前评估需人工干预成本高，且现有工具在冲突解决和AI集成方面存在不足。

Method: 提出AI增强工具框架，将各类信息组织成三个维度九个基准，对客观指标归一化、聚合，结合不平等指标找出冲突标记，用大语言模型进行分析生成判断。

Result: 设计出用于争端调查的AI增强工具框架。

Conclusion: 该工具在现有法定和制度政策下可行，同时指出了实际分析方法、偏差保障、局限性和实际挑战。

Abstract: The equitable assessment of individual contribution in teams remains a persistent challenge, where conflict and disparity in workload can result in unfair performance evaluation, often requiring manual intervention - a costly and challenging process. We survey existing tool features and identify a gap in conflict resolution methods and AI integration. To address this, we propose a framework and implementation design for a novel AI-enhanced tool that assists in dispute investigation. The framework organises heterogeneous artefacts - submissions (code, text, media), communications (chat, email), coordination records (meeting logs, tasks), peer assessments, and contextual information - into three dimensions with nine benchmarks: Contribution, Interaction, and Role. Objective measures are normalised, aggregated per dimension, and paired with inequality measures (Gini index) to surface conflict markers. A Large Language Model (LLM) architecture performs validated and contextual analysis over these measures to generate interpretable and transparent advisory judgments. We argue for feasibility under current statutory and institutional policy, and outline practical analytics (sentimental, task fidelity, word/line count, etc.), bias safeguards, limitations, and practical challenges.

</details>


### [9] [Making LLMs Reliable When It Matters Most: A Five-Layer Architecture for High-Stakes Decisions](https://arxiv.org/abs/2511.07669)
*Alejandro R. Jadad*

Main category: cs.AI

TL;DR: 现有大语言模型在高风险战略决策中不可靠，报告提出框架，经实验有三项发现，表明人机团队能实现认知伙伴关系以避免高风险决策遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在高风险战略决策中不可靠的问题，避免认知偏差影响投资估值和可持续性。

Method: 对7个前沿大语言模型和3个市场风投案例进行系统定性评估，采用特定提示、7阶段校准序列、4阶段初始化过程和5层保护架构。

Result: 有三项发现，包括伙伴关系可通过校准实现但需维护协议、架构漂移和上下文耗尽会降低可靠性、解散纪律可防止错误方向；跨模型验证显示架构性能有差异。

Conclusion: 人机团队能达成认知伙伴关系，避免高风险决策中的可避免遗憾，满足投资回报期望。

Abstract: Current large language models (LLMs) excel in verifiable domains where outputs can be checked before action but prove less reliable for high-stakes strategic decisions with uncertain outcomes. This gap, driven by mutually reinforcing cognitive biases in both humans and artificial intelligence (AI) systems, threatens the defensibility of valuations and sustainability of investments in the sector.
  This report describes a framework emerging from systematic qualitative assessment across 7 frontier-grade LLMs and 3 market-facing venture vignettes under time pressure. Detailed prompting specifying decision partnership and explicitly instructing avoidance of sycophancy, confabulation, solution drift, and nihilism achieved initial partnership state but failed to maintain it under operational pressure. Sustaining protective partnership state required an emergent 7-stage calibration sequence, built upon a 4-stage initialization process, within a 5-layer protection architecture enabling bias self-monitoring, human-AI adversarial challenge, partnership state verification, performance degradation detection, and stakeholder protection.
  Three discoveries resulted: partnership state is achievable through ordered calibration but requires emergent maintenance protocols; reliability degrades when architectural drift and context exhaustion align; and dissolution discipline prevents costly pursuit of fundamentally wrong directions. Cross-model validation revealed systematic performance differences across LLM architectures.
  This approach demonstrates that human-AI teams can achieve cognitive partnership capable of preventing avoidable regret in high-stakes decisions, addressing return-on-investment expectations that depend on AI systems supporting consequential decision-making without introducing preventable cognitive traps when verification arrives too late.

</details>


### [10] [AIA Forecaster: Technical Report](https://arxiv.org/abs/2511.07678)
*Rohan Alur,Bradly C. Stadie,Daniel Kang,Ryan Chen,Matt McManus,Michael Rickert,Tyler Lee,Michael Federici,Richard Zhu,Dennis Fogerty,Hayley Williamson,Nina Lozinski,Aaron Linsky,Jasjeet S. Sekhon*

Main category: cs.AI

TL;DR: 介绍基于大语言模型的AIA Forecaster预测系统，在ForecastBench表现佳，结合市场共识可提升表现，创AI预测新水平。


<details>
  <summary>Details</summary>
Motivation: 利用非结构化数据进行判断性预测，提升AI预测水平。

Method: 结合高质量新闻源的智能搜索、协调不同预测的监督代理以及统计校准技术。

Result: 在ForecastBench上表现与人类超级预测者相当，超此前基线；在新基准中与市场共识结合表现更好。

Conclusion: 建立AI预测新水平，为未来研究提供实用建议，是首个可验证的大规模专家级预测工作。

Abstract: This technical report describes the AIA Forecaster, a Large Language Model (LLM)-based system for judgmental forecasting using unstructured data. The AIA Forecaster approach combines three core elements: agentic search over high-quality news sources, a supervisor agent that reconciles disparate forecasts for the same event, and a set of statistical calibration techniques to counter behavioral biases in large language models. On the ForecastBench benchmark (Karger et al., 2024), the AIA Forecaster achieves performance equal to human superforecasters, surpassing prior LLM baselines. In addition to reporting on ForecastBench, we also introduce a more challenging forecasting benchmark sourced from liquid prediction markets. While the AIA Forecaster underperforms market consensus on this benchmark, an ensemble combining AIA Forecaster with market consensus outperforms consensus alone, demonstrating that our forecaster provides additive information. Our work establishes a new state of the art in AI forecasting and provides practical, transferable recommendations for future research. To the best of our knowledge, this is the first work that verifiably achieves expert-level forecasting at scale.

</details>


### [11] [ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents](https://arxiv.org/abs/2511.07685)
*Manasi Sharma,Chen Bo Calvin Zhang,Chaithanya Bandi,Clinton Wang,Ankit Aich,Huy Nghiem,Tahseen Rabbani,Ye Htet,Brian Jang,Sumana Basu,Aishwarya Balwani,Denis Peskoff,Marcos Ayestaran,Sean M. Hendryx,Brad Kenstler,Bing Liu*

Main category: cs.AI

TL;DR: 提出用于深度研究（DR）评估的标准化基准ResearchRubrics，评估现有DR系统，发现领先系统达标率低，发布相关资源促进研究。


<details>
  <summary>Details</summary>
Motivation: 现有DR评估困难，因响应长且多样、有多种有效解且依赖动态信息源，需有效评估方法。

Method: 投入超2800小时人力构建含2500多个细粒度评分标准的ResearchRubrics，提出复杂度框架，开发基于人类和模型的评估协议。

Result: 评估多个先进DR系统，如Gemini和OpenAI的DR，平均达标率低于68%，主要因遗漏隐式上下文和对检索信息推理不足。

Conclusion: 强调需对深度研究能力进行稳健、可扩展的评估，发布ResearchRubrics促进合理研究助手的发展。

Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.

</details>


### [12] [Towards AI-Assisted Generation of Military Training Scenarios](https://arxiv.org/abs/2511.07690)
*Soham Hans,Volkan Ustun,Benjamin Nye,James Sterrett,Matthew Green*

Main category: cs.AI

TL;DR: 本文提出一种基于大语言模型的多智能体、多模态推理框架用于军事训练场景生成，通过概念验证验证其可行性与准确性，展示了大语言模型驱动系统在军事训练场景生成自动化方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统模拟训练场景生成过程费力且资源密集，先前的非大语言模型AI工具难以生成足够复杂或可适应的场景。

Method: 将场景生成分解为子问题层次结构，为每个子问题定义AI工具角色，使用专门的基于大语言模型的智能体处理不同子问题，后续智能体顺序处理输出。

Result: 通过概念验证生成作战命令的部分内容，展示了大语言模型驱动的多智能体系统生成连贯、细致文档并动态适应变化条件的潜力。

Conclusion: 大语言模型驱动的多智能体系统可推动军事训练场景生成的自动化。

Abstract: Achieving expert-level performance in simulation-based training relies on the creation of complex, adaptable scenarios, a traditionally laborious and resource intensive process. Although prior research explored scenario generation for military training, pre-LLM AI tools struggled to generate sufficiently complex or adaptable scenarios. This paper introduces a multi-agent, multi-modal reasoning framework that leverages Large Language Models (LLMs) to generate critical training artifacts, such as Operations Orders (OPORDs). We structure our framework by decomposing scenario generation into a hierarchy of subproblems, and for each one, defining the role of the AI tool: (1) generating options for a human author to select from, (2) producing a candidate product for human approval or modification, or (3) generating textual artifacts fully automatically. Our framework employs specialized LLM-based agents to address distinct subproblems. Each agent receives input from preceding subproblem agents, integrating both text-based scenario details and visual information (e.g., map features, unit positions and applies specialized reasoning to produce appropriate outputs. Subsequent agents process these outputs sequentially, preserving logical consistency and ensuring accurate document generation. This multi-agent strategy overcomes the limitations of basic prompting or single-agent approaches when tackling such highly complex tasks. We validate our framework through a proof-of-concept that generates the scheme of maneuver and movement section of an OPORD while estimating map positions and movements as a precursor demonstrating its feasibility and accuracy. Our results demonstrate the potential of LLM-driven multi-agent systems to generate coherent, nuanced documents and adapt dynamically to changing conditions, advancing automation in scenario generation for military training.

</details>


### [13] [Operational machine learning for remote spectroscopic detection of CH$_{4}$ point sources](https://arxiv.org/abs/2511.07719)
*Vít Růžička,Gonzalo Mateo-García,Itziar Irakulis-Loitxate,Juan Emmanuel Johnson,Manuel Montesino San Martín,Anna Allen,Luis Guanter,David R. Thompson*

Main category: cs.AI

TL;DR: 本文介绍在联合国环境规划署国际甲烷排放观测站的甲烷警报与响应系统中部署机器学习系统检测甲烷排放，创建数据集、比较模型，用模型集成减少误检，加速检测分析，还验证了模型在确认减排成效中的作用。


<details>
  <summary>Details</summary>
Motivation: 当前基于匹配滤波器的甲烷反演方法存在大量误检，需人工核查，需有效方法检测甲烷排放。

Method: 创建全球最大且最多样化的带注释甲烷羽流数据集，比较不同深度学习模型配置，将评估方法从小块数据集扩展到全颗粒评估，用模型集成减少误检。

Result: 模型集成使误检减少超74%，系统部署后7个月助力验证1351个不同甲烷泄漏点，发出479次利益相关者通知。

Conclusion: 该工作是迈向全球人工智能辅助甲烷泄漏检测系统的关键一步，可处理更多光谱仪数据。

Abstract: Mitigating anthropogenic methane sources is one the most cost-effective levers to slow down global warming. While satellite-based imaging spectrometers, such as EMIT, PRISMA, and EnMAP, can detect these point sources, current methane retrieval methods based on matched filters still produce a high number of false detections requiring laborious manual verification. This paper describes the operational deployment of a machine learning system for detecting methane emissions within the Methane Alert and Response System (MARS) of the United Nations Environment Programme's International Methane Emissions Observatory. We created the largest and most diverse global dataset of annotated methane plumes from three imaging spectrometer missions and quantitatively compared different deep learning model configurations. Focusing on the requirements for operational deployment, we extended prior evaluation methodologies from small tiled datasets to full granule evaluation. This revealed that deep learning models still produce a large number of false detections, a problem we address with model ensembling, which reduced false detections by over 74%. Deployed in the MARS pipeline, our system processes scenes and proposes plumes to analysts, accelerating the detection and analysis process. During seven months of operational deployment, it facilitated the verification of 1,351 distinct methane leaks, resulting in 479 stakeholder notifications. We further demonstrate the model's utility in verifying mitigation success through case studies in Libya, Argentina, Oman, and Azerbaijan. Our work represents a critical step towards a global AI-assisted methane leak detection system, which is required to process the dramatically higher data volumes expected from new and current imaging spectrometers.

</details>


### [14] [Alignment-Aware Quantization for LLM Safety](https://arxiv.org/abs/2511.07842)
*Sunghyun Wee,Suyoung Kim,Hyeonjin Kim,Kyomin Hwang,Nojun Kwak*

Main category: cs.AI

TL;DR: 传统后训练量化（PTQ）在大语言模型安全与效率上有冲突，本文提出对齐感知量化（AAQ）方法解决该问题，兼容标准PTQ技术，实现高效且安全的大语言模型。


<details>
  <summary>Details</summary>
Motivation: 传统PTQ范式中量化仅追求低困惑度会成为安全漏洞，困惑度不能有效代表模型安全性，需解决安全与效率的冲突。

Method: 提出AAQ方法，将对齐保留对比（APC）损失集成到PTQ流程中，鼓励量化模型模仿安全的指令调优模型，与未对齐的预训练模型区分开。

Result: AAQ无需专门的安全校准数据集，兼容标准PTQ技术，能在多种模型家族上实现4位量化并保持安全性。

Conclusion: 本文工作解决了效率与安全的关键权衡问题，为构建高效且可信的大语言模型铺平道路。

Abstract: Safety and efficiency are both important factors when deploying large language models(LLMs). LLMs are trained to follow human alignment for safety, and post training quantization(PTQ) is applied afterward for efficiency. However, these two objectives are often in conflict, revealing a fundamental flaw in the conventional PTQ paradigm: quantization can turn into a safety vulnerability if it only aims to achieve low perplexity. Models can demonstrate low perplexity yet exhibit significant degradation in alignment with the safety policy, highlighting that perplexity alone is an insufficient and often misleading proxy for model safety. To address this, we propose Alignment-Aware Quantization(AAQ), a novel approach that integrates Alignment-Preserving Contrastive(APC) loss into the PTQ pipeline. Compared to simple reconstruction loss, ours explicitly preserves alignment by encouraging the quantized model to mimic its safe, instruction-tuned model while diverging from the unaligned, pre-trained counterpart. Our method achieves this robust safety alignment without resorting to specialized safety-focused calibration datasets, highlighting its practical utility and broad applicability. AAQ is compatible with standard PTQ techniques and enables robust 4-bit (W4A4) quantization across diverse model families such as LLaMA, Qwen, and Mistral while maintaining safety where previous methods fail. Our work resolves the critical trade-off between efficiency and safety, paving the way toward LLMs that are both efficient and trustworthy. Anonymized code is available in the supplementary material.

</details>


### [15] [GAMA: A Neural Neighborhood Search Method with Graph-aware Multi-modal Attention for Vehicle Routing Problem](https://arxiv.org/abs/2511.07850)
*Xiangling Chen,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: 提出用于解决车辆路径问题的GAMA方法，实验表明其显著优于近期神经基线。


<details>
  <summary>Details</summary>
Motivation: 现有神经邻域搜索方法在解决车辆路径问题时，状态表示简单，信息融合方式原始，限制了捕捉上下文信息的能力。

Method: 使用图神经网络将问题实例和解决方案编码为不同模态，通过自注意力和交叉注意力层建模模态内和模态间的交互，用门控融合机制将多模态表示集成到结构化状态。

Result: 在各种合成和基准实例上的实验表明，GAMA显著优于近期神经基线，消融研究证实多模态注意力机制和门控融合设计对性能提升起关键作用。

Conclusion: GAMA方法有效，多模态注意力机制和门控融合设计有助于提升性能。

Abstract: Recent advances in neural neighborhood search methods have shown potential in tackling Vehicle Routing Problems (VRPs). However, most existing approaches rely on simplistic state representations and fuse heterogeneous information via naive concatenation, limiting their ability to capture rich structural and semantic context. To address these limitations, we propose GAMA, a neural neighborhood search method with Graph-aware Multi-modal Attention model in VRP. GAMA encodes the problem instance and its evolving solution as distinct modalities using graph neural networks, and models their intra- and inter-modal interactions through stacked self- and cross-attention layers. A gated fusion mechanism further integrates the multi-modal representations into a structured state, enabling the policy to make informed and generalizable operator selection decisions. Extensive experiments conducted across various synthetic and benchmark instances demonstrate that the proposed algorithm GAMA significantly outperforms the recent neural baselines. Further ablation studies confirm that both the multi-modal attention mechanism and the gated fusion design play a key role in achieving the observed performance gains.

</details>


### [16] [WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking](https://arxiv.org/abs/2511.07863)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.AI

TL;DR: 大语言模型生成内容需溯源标记，传统水印影响流畅性，WaterMod通过概率感知模块化规则解决问题，实验证明其能保持检测性能和生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成内容需溯源标记，但传统基于对数几率的水印会侵蚀流畅性，需改进。

Method: 先按模型概率对词汇降序排序，用余数rank mod k划分，对选定类施加小幅度固定偏差，零比特和多比特设置采用不同选择方式。

Result: WaterMod在零比特和多比特设置中都能实现强水印检测性能，同时保持生成质量，在多任务中表现稳健。

Conclusion: WaterMod通过模块化算术支持二进制归因和丰富有效负载，能解决传统水印问题，可用于细粒度溯源追踪。

Abstract: Large language models now draft news, legal analyses, and software code with human-level fluency. At the same time, regulations such as the EU AI Act mandate that each synthetic passage carry an imperceptible, machine-verifiable mark for provenance. Conventional logit-based watermarks satisfy this requirement by selecting a pseudorandom green vocabulary at every decoding step and boosting its logits, yet the random split can exclude the highest-probability token and thus erode fluency. WaterMod mitigates this limitation through a probability-aware modular rule. The vocabulary is first sorted in descending model probability; the resulting ranks are then partitioned by the residue rank mod k, which distributes adjacent-and therefore semantically similar-tokens across different classes. A fixed bias of small magnitude is applied to one selected class. In the zero-bit setting (k=2), an entropy-adaptive gate selects either the even or the odd parity as the green list. Because the top two ranks fall into different parities, this choice embeds a detectable signal while guaranteeing that at least one high-probability token remains available for sampling. In the multi-bit regime (k>2), the current payload digit d selects the color class whose ranks satisfy rank mod k = d. Biasing the logits of that class embeds exactly one base-k digit per decoding step, thereby enabling fine-grained provenance tracing. The same modular arithmetic therefore supports both binary attribution and rich payloads. Experimental results demonstrate that WaterMod consistently attains strong watermark detection performance while maintaining generation quality in both zero-bit and multi-bit settings. This robustness holds across a range of tasks, including natural language generation, mathematical reasoning, and code synthesis. Our code and data are available at https://github.com/Shinwoo-Park/WaterMod.

</details>


### [17] [Confidence-Aware Neural Decoding of Overt Speech from EEG: Toward Robust Brain-Computer Interfaces](https://arxiv.org/abs/2511.07890)
*Soowon Kim,Byung-Kwan Ko,Seo-Hyun Lee*

Main category: cs.AI

TL;DR: 提出自信感知解码框架用于脑机接口语音命令解码，评估显示其有更好性能，可用于现实系统。


<details>
  <summary>Details</summary>
Motivation: 无创脑机接口解码脑电信号中的语音命令需兼具准确性和可靠性。

Method: 提出自信感知解码框架，结合紧凑语音卷积网络深度集成、事后校准和选择性分类，用多种方法量化不确定性，有弃权选项。

Result: 相比基线方法，该方法概率估计更可靠，各操作点选择性性能更好，各类接受度平衡。

Conclusion: 自信感知神经解码可为现实脑机接口通信系统提供可靠、面向部署的性能。

Abstract: Non-invasive brain-computer interfaces that decode spoken commands from electroencephalogram must be both accurate and trustworthy. We present a confidence-aware decoding framework that couples deep ensembles of compact, speech-oriented convolutional networks with post-hoc calibration and selective classification. Uncertainty is quantified using ensemble-based predictive entropy, top-two margin, and mutual information, and decisions are made with an abstain option governed by an accuracy-coverage operating point. The approach is evaluated on a multi-class overt speech dataset using a leakage-safe, block-stratified split that respects temporal contiguity. Compared with widely used baselines, the proposed method yields more reliable probability estimates, improved selective performance across operating points, and balanced per-class acceptance. These results suggest that confidence-aware neural decoding can provide robust, deployment-oriented behavior for real-world brain-computer interface communication systems.

</details>


### [18] [Toward Robust EEG-based Intention Decoding during Misarticulated Speech in Aphasia](https://arxiv.org/abs/2511.07895)
*Ha-Na Jo,Jung-Sun Lee,Eunyeong Ko*

Main category: cs.AI

TL;DR: 本文针对失语症患者开发基于EEG的交流支持系统，通过实验分析EEG信号，构建学习框架，模型在解码意图上表现良好，证明该系统可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管脑机接口技术受关注，但针对失语症患者的基于EEG的交流支持系统开发受关注少，需填补此空白。

Method: 招募一名表达性失语症患者进行韩语自动语音任务，记录EEG信号并标记，进行频谱分析，构建带最大均值差异正则化的软多任务学习框架。

Result: 模型对正确试验准确率达58.6%，对错误发音试验准确率达45.5%，在错误发音试验上比基线高出超45%。

Conclusion: 基于EEG的辅助系统能支持失语症患者现实中不完美的语音状况，具有可行性。

Abstract: Aphasia severely limits verbal communication due to impaired language production, often leading to frequent misarticulations during speech attempts. Despite growing interest in brain-computer interface technologies, relatively little attention has been paid to developing EEG-based communication support systems tailored for aphasic patients. To address this gap, we recruited a single participant with expressive aphasia and conducted an Korean-based automatic speech task. EEG signals were recorded during task performance, and each trial was labeled as either correct or incorrect depending on whether the intended word was successfully spoken. Spectral analysis revealed distinct neural activation patterns between the two trial types: misarticulated trials exhibited excessive delta power across widespread channels and increased theta-alpha activity in frontal regions. Building upon these findings, we developed a soft multitask learning framework with maximum mean discrepancy regularization that focus on delta features to jointly optimize class discrimination while aligning the EEG feature distributions of correct and misarticulated trials. The proposed model achieved 58.6 % accuracy for correct and 45.5 % for misarticulated trials-outperforming the baseline by over 45 % on the latter-demonstrating robust intention decoding even under articulation errors. These results highlight the feasibility of EEG-based assistive systems capable of supporting real-world, imperfect speech conditions in aphasia patients.

</details>


### [19] [SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder](https://arxiv.org/abs/2511.07896)
*Dengcan Liu,Jiahao Li,Zheren Fu,Yi Tu,Jiajun Li,Zhendong Mao,Yongdong Zhang*

Main category: cs.AI

TL;DR: 本文提出SparseRM以解决有限资源下训练可靠奖励模型的问题，实验显示其性能优且参数少，能融入下游对齐流程。


<details>
  <summary>Details</summary>
Motivation: 在有限资源下，训练可靠奖励模型因依赖大规模偏好标注和微调大语言模型成本高而面临挑战。

Method: 提出SparseRM，利用稀疏自动编码器（SAE）提取模型表示中与偏好相关的信息，分解大语言模型表示，计算对齐分数，用简单奖励头预测偏好分数。

Result: 在三个偏好建模任务上，SparseRM优于大多数主流奖励模型，且可训练参数不到1%。

Conclusion: SparseRM具有高效对齐的潜力，能无缝融入下游对齐流程。

Abstract: Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.

</details>


### [20] [Data Descriptions from Large Language Models with Influence Estimation](https://arxiv.org/abs/2511.07897)
*Chaeri Kim,Jaeyeon Bae,Taehwan Kim*

Main category: cs.AI

TL;DR: 提出用大语言模型结合外部知识库生成解释数据的文本描述，引入影响估计和CLIP分数筛选，提出跨模态转移分类任务，实验表明该方法有效且能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法多关注模型预测解释，本文旨在理解如何用深度学习模型训练解释数据，让人类易理解。

Method: 提出用大语言模型结合外部知识库生成文本描述数据的流程，用影响估计和CLIP分数筛选，提出跨模态转移分类任务。

Result: 零样本实验中，文本描述比基线描述更有效，提升了九种图像分类数据集上仅基于图像训练模型的性能，结果获GPT - 4o评估支持。

Conclusion: 该方法有助于深入理解模型决策过程的内在可解释性。

Abstract: Deep learning models have been successful in many areas but understanding their behaviors still remains a black-box. Most prior explainable AI (XAI) approaches have focused on interpreting and explaining how models make predictions. In contrast, we would like to understand how data can be explained with deep learning model training and propose a novel approach to understand the data via one of the most common media - language - so that humans can easily understand. Our approach proposes a pipeline to generate textual descriptions that can explain the data with large language models by incorporating external knowledge bases. However, generated data descriptions may still include irrelevant information, so we introduce to exploit influence estimation to choose the most informative textual descriptions, along with the CLIP score. Furthermore, based on the phenomenon of cross-modal transferability, we propose a novel benchmark task named cross-modal transfer classification to examine the effectiveness of our textual descriptions. In the experiment of zero-shot setting, we show that our textual descriptions are more effective than other baseline descriptions, and furthermore, we successfully boost the performance of the model trained only on images across all nine image classification datasets. These results are further supported by evaluation using GPT-4o. Through our approach, we may gain insights into the inherent interpretability of the decision-making process of the model.

</details>


### [21] [DANS-KGC: Diffusion Based Adaptive Negative Sampling for Knowledge Graph Completion](https://arxiv.org/abs/2511.07901)
*Haoning Li,Qinghua Huang*

Main category: cs.AI

TL;DR: 提出DANS - KGC负采样策略，含三个组件，实验证明其有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 克服现有负采样策略易出现假阴性、泛化性有限和难以控制样本难度等局限。

Method: 提出DANS - KGC，包含DAM评估实体学习难度，ANS用条件扩散模型生成不同难度负样本，DTM动态调整负样本难度分布。

Result: 在六个基准数据集上实验，在UMLS和YAGO3 - 10数据集的三个评估指标上取得了最优结果。

Conclusion: DANS - KGC有效且具有良好的泛化能力。

Abstract: Negative sampling (NS) strategies play a crucial role in knowledge graph representation. In order to overcome the limitations of existing negative sampling strategies, such as vulnerability to false negatives, limited generalization, and lack of control over sample hardness, we propose DANS-KGC (Diffusion-based Adaptive Negative Sampling for Knowledge Graph Completion). DANS-KGC comprises three key components: the Difficulty Assessment Module (DAM), the Adaptive Negative Sampling Module (ANS), and the Dynamic Training Mechanism (DTM). DAM evaluates the learning difficulty of entities by integrating semantic and structural features. Based on this assessment, ANS employs a conditional diffusion model with difficulty-aware noise scheduling, leveraging semantic and neighborhood information during the denoising phase to generate negative samples of diverse hardness. DTM further enhances learning by dynamically adjusting the hardness distribution of negative samples throughout training, enabling a curriculum-style progression from easy to hard examples. Extensive experiments on six benchmark datasets demonstrate the effectiveness and generalization ability of DANS-KGC, with the method achieving state-of-the-art results on all three evaluation metrics for the UMLS and YAGO3-10 datasets.

</details>


### [22] [Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging](https://arxiv.org/abs/2511.08052)
*Po-Chung Hsieh,Chin-Po Chen,Jeng-Lin Li,Ming-Ching Chang*

Main category: cs.AI

TL;DR: 现有大语言模型推理存在问题，本文提出心理支持的脚手架推理框架用于代码调试，在DebugBench表现佳，还分析了认知路径优缺点。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在确定平衡复杂度和计算效率的推理步骤方面的关键问题未解决，且对System 2推理缺乏深入探索。

Method: 提出包含脚手架流、分析流和集成流的脚手架推理框架，通过集成流将脚手架流中参考代码构建与分析流中错误代码分析结果集成。

Result: 框架在DebugBench上通过率达88.91%，每题平均推理时间5.36秒，在推理准确性和效率上优于其他推理方法。

Conclusion: 分析了不同问题难度和错误类型下各种认知路径的优缺点，框架与人类认知过程相符。

Abstract: Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.

</details>


### [23] [Neurophysiological Characteristics of Adaptive Reasoning for Creative Problem-Solving Strategy](https://arxiv.org/abs/2511.07912)
*Jun-Young Kim,Young-Seok Kweon,Gi-Hwan Shin,Seong-Whan Lee*

Main category: cs.AI

TL;DR: 研究用卡片分类范式结合脑电图探究人类自适应推理神经机制，并与多模态大语言模型对比，发现人类神经特征及模型局限，强调脑启发式AI的必要性。


<details>
  <summary>Details</summary>
Motivation: 自适应推理的神经动力学机制尚不清楚，需深入研究。

Method: 采用卡片分类范式结合脑电图，将人类表现与多模态大语言模型对比。

Result: 人类有协调的delta - theta - alpha动力学，模型只有短期反馈驱动调整，无层次规则抽象和真正自适应推理。

Conclusion: 确定人类自适应推理神经特征，强调需脑启发式AI实现真正上下文敏感适应。

Abstract: Adaptive reasoning enables humans to flexibly adjust inference strategies when environmental rules or contexts change, yet its underlying neural dynamics remain unclear. This study investigated the neurophysiological mechanisms of adaptive reasoning using a card-sorting paradigm combined with electroencephalography and compared human performance with that of a multimodal large language model. Stimulus- and feedback-locked analyses revealed coordinated delta-theta-alpha dynamics: early delta-theta activity reflected exploratory monitoring and rule inference, whereas occipital alpha engagement indicated confirmatory stabilization of attention after successful rule identification. In contrast, the multimodal large language model exhibited only short-term feedback-driven adjustments without hierarchical rule abstraction or genuine adaptive reasoning. These findings identify the neural signatures of human adaptive reasoning and highlight the need for brain-inspired artificial intelligence that incorporates oscillatory feedback coordination for true context-sensitive adaptation.

</details>


### [24] [Lightweight Diffusion-based Framework for Online Imagined Speech Decoding in Aphasia](https://arxiv.org/abs/2511.07920)
*Eunyeong Ko,Soowon Kim,Ha-Na Jo*

Main category: cs.AI

TL;DR: 提出用于失语症患者实时想象语音分类的基于扩散的神经解码框架，在20次实时试验中表现良好，推进想象语音脑机接口临床应用。


<details>
  <summary>Details</summary>
Motivation: 为失语症患者开发能在实际临床约束下进行实时想象语音分类的神经解码框架。

Method: 集成轻量级条件扩散编码器和卷积分类器，使用特定受试者的韩语范式EEG数据训练，采用双标准早停策略、dropout正则化和分组时间卷积，在线操作时处理连续EEG流。

Result: 在20次实时试验中，top - 1准确率65%，top - 2准确率70%，优于离线评估。

Conclusion: 该框架可在实际临床约束下部署，推进想象语音脑机接口用于严重表达性语言障碍患者的临床通信支持。

Abstract: A diffusion-based neural decoding framework optimized for real-time imagined speech classification in individuals with aphasia. The system integrates a lightweight conditional diffusion encoder and convolutional classifier trained using subject-specific EEG data acquired from a Korean-language paradigm. A dual-criterion early stopping strategy enabled rapid convergence under limited calibration data, while dropout regularization and grouped temporal convolutions ensured stable generalization. During online operation, continuous EEG streams were processed in two-second sliding windows to generate class probabilities that dynamically modulated visual and auditory feedback according to decoding confidence. Across twenty real-time trials, the framework achieved 65% top-1 and 70% top-2 accuracy, outperforming offline evaluation (50% top-1). These results demonstrate the feasibility of deploying diffusion-based EEG decoding under practical clinical constraints, maintaining reliable performance despite environmental variability and minimal preprocessing. The proposed framework advances the translation of imagined speech brain-computer interfaces toward clinical communication support for individuals with severe expressive language impairment.

</details>


### [25] [Computational Blueprints: Generating Isomorphic Mathematics Problems with Large Language Models](https://arxiv.org/abs/2511.07932)
*Jeong-Hoon Kim,Jinwoo Nam,Geunsik Jo*

Main category: cs.AI

TL;DR: 本文提出新任务IMPG，构建CBIT框架生成同构数学问题，该框架在准确性、成本效益上表现优，生成问题错误率低且有实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有数学问题生成研究侧重于训练数据增强而非教育部署，为填补此空白定义新任务IMPG。

Method: 通过逐步改进探索基于大语言模型的自动IMPG框架，建立CBIT，采用元级生成和基于模板的选择性变异。

Result: CBIT在生成准确性和大规模成本效益上表现优越，生成问题错误率比专家编写的低17.8%，在商业教育平台上有实际应用和大量交互。

Conclusion: CBIT是一种有效的同构数学问题生成方法，可用于个性化数学教育。

Abstract: Personalized mathematics education is growing rapidly, creating a strong demand for large sets of similar practice problems. Yet existing studies on mathematics problem generation have focused on data augmentation for training neural language models rather than on direct educational deployment. To bridge this gap, we define a new task, Isomorphic Math Problem Generation (IMPG), designed to produce structurally consistent variants of source problems. Subsequently, we explored LLM-based frameworks for automatic IMPG through successive refinements, and established Computational Blueprints for Isomorphic Twins (CBIT). With meta-level generation and template-based selective variation, CBIT achieves high mathematical correctness and structural consistency while reducing the cost of generation. Empirical results across refinements demonstrate that CBIT is superior on generation accuracy and cost-effectiveness at scale. Most importantly, CBIT-generated problems exhibited an error rate 17.8% lower than expert-authored items, with deployment to 6,732 learners on a commercial education platform yielding 186,870 interactions.

</details>


### [26] [Toward Practical BCI: A Real-time Wireless Imagined Speech EEG Decoding System](https://arxiv.org/abs/2511.07936)
*Ji-Ha Park,Heon-Gyu Kwak,Gi-Hwan Shin,Yoo-In Jeon,Sun-Min Park,Ji-Yeon Hwang,Seong-Whan Lee*

Main category: cs.AI

TL;DR: 本文介绍实时无线想象言语脑电图解码系统，提升BCI实用性，在有线和无线设备测试有一定准确率，为未来研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 传统BCI研究局限于静态固定环境，为推动其实用化。

Method: 引入实时无线想象言语脑电图解码系统，有用户识别模块，利用实验室流层管理信号传输。

Result: 系统能对想象言语脑电图信号分类，有线设备4类准确率62.00%，便携式无线耳机46.67%。

Conclusion: 该研究向实用且易获取的BCI技术迈进，为未来鲁棒、实用和个性化神经接口研究指明方向。

Abstract: Brain-computer interface (BCI) research, while promising, has largely been confined to static and fixed environments, limiting real-world applicability. To move towards practical BCI, we introduce a real-time wireless imagined speech electroencephalogram (EEG) decoding system designed for flexibility and everyday use. Our framework focuses on practicality, demonstrating extensibility beyond wired EEG devices to portable, wireless hardware. A user identification module recognizes the operator and provides a personalized, user-specific service. To achieve seamless, real-time operation, we utilize the lab streaming layer to manage the continuous streaming of live EEG signals to the personalized decoder. This end-to-end pipeline enables a functional real-time application capable of classifying user commands from imagined speech EEG signals, achieving an overall 4-class accuracy of 62.00 % on a wired device and 46.67 % on a portable wireless headset. This paper demonstrates a significant step towards truly practical and accessible BCI technology, establishing a clear direction for future research in robust, practical, and personalized neural interfaces.

</details>


### [27] [Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction](https://arxiv.org/abs/2511.07943)
*Jun Xu,Xinkai Du,Yu Ao,Peilong Zhao,Yang Li,Ling Zhong,Lin Yuan,Zhongpu Bo,Xiaorui Wang,Mengshu Sun,Zhengke Gui,Dalong Zhang,Zhaoyang Wang,Qiwei Wang,Yangyang Hou,Zhiying Yin,Haofen Wang,Huajun Chen,Lei Liang,Jun Zhou*

Main category: cs.AI

TL;DR: 提出Thinker分层思维模型用于深度搜索，使推理过程可监督可验证，实验显示其性能良好。


<details>
  <summary>Details</summary>
Motivation: 以往训练大语言模型利用外部检索器解决复杂问题的方法忽视推理过程监督，难以保证逻辑连贯性和严谨性。

Method: 提出Thinker模型，将复杂问题分解为子问题，用自然语言和逻辑函数双重表示，通过逻辑函数传递子问题依赖关系，进行知识边界判断。

Result: 少量训练样本时，Thinker性能与基线相当；全量训练时，在多数据集和不同模型大小上显著优于其他方法。

Conclusion: Thinker模型有效，代码已开源。

Abstract: Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.

</details>


### [28] [Smarter Together: Creating Agentic Communities of Practice through Shared Experiential Learning](https://arxiv.org/abs/2511.08301)
*Valentin Tablan,Scott Taylor,Gabriel Hurtado,Kristoffer Bernhem,Anders Uhrenholt,Gabriele Farei,Karo Moilanen*

Main category: cs.AI

TL;DR: 本文介绍了新颖的共享智能体内存架构Spark，能让AI编码智能体实现集体持续学习，经评估可提升代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 从以人类为中心向以智能体为中心的软件开发实践转变，破坏了现有开发者知识共享环境，且缺少智能体功能对等物，AI智能体无法访问有价值的共享学习资源。

Method: 引入Spark共享智能体内存架构，让同一问题空间的智能体使用其作为新知识存储库实现集体持续学习，并对其进行评估。

Result: Spark的推荐可提高不同规模和能力层级的通用代码生成模型的代码质量，300亿参数的小模型在其助力下能达到大模型的代码质量，且推荐的有用性最高达98.2%。

Conclusion: Spark作为AI编码智能体的教练是有效的，能提升代码生成质量和推荐有用性。

Abstract: The transition from human-centric to agent-centric software development practices is disrupting existing knowledge sharing environments for software developers. Traditional peer-to-peer repositories and developer communities for shared technical knowledge and best practice have witnessed dramatic drops in participation in a short period of time. At the same time, agentic functional equivalents are yet to emerge leaving AI agents, which already generate a significant proportion of all new software code produced, without access to repositories of valuable shared learning.
  In this paper, we introduce Spark, a novel shared agentic memory architecture which is designed to emulate the collective intelligence and know-how of human developer communities. Spark enables AI coding agents to both contribute to and draw from a persistent and continuously evolving experiential memory. Agents operating in the same general problem space use the Spark shared memory as a repository of new knowledge to achieve collective continual learning. We evaluate Spark as a coach for AI coding agents performing software development tasks. We demonstrate that recommendations made by Spark improve the quality of code generated by generic code generation models at varying sizes and capability tiers. Boosted by Spark, a small open-weights model with 30 billion parameters was able to match the code quality afforded by a much larger state-of-the-art model. Separately, we measure the intrinsic quality of recommendations generated by Spark against a wide range of criteria inspired by software development best practice, and achieve helpfulness levels of up to 98.2% in the top two (out of five) qualitative helpfulness bands.

</details>


### [29] [TimeFlow: Towards Stochastic-Aware and Efficient Time Series Generation via Flow Matching Modeling](https://arxiv.org/abs/2511.07968)
*He Panjing,Cheng Mingyue,Li Li,Zhang XiaoHan*

Main category: cs.AI

TL;DR: 提出基于SDE的时间序列生成框架TimeFlow，在生成质量、多样性和效率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成方法存在计算效率低、难以捕捉随机性等问题，需更好方法。

Method: 提出TimeFlow框架，采用编码器架构，设计分量分解速度场，增加随机项。

Result: 在多个数据集上实验，模型在生成质量、多样性和效率上优于基线。

Conclusion: TimeFlow灵活通用，能在统一框架下支持无条件和条件生成任务。

Abstract: Generating high-quality time series data has emerged as a critical research topic due to its broad utility in supporting downstream time series mining tasks. A major challenge lies in modeling the intrinsic stochasticity of temporal dynamics, as real-world sequences often exhibit random fluctuations and localized variations. While diffusion models have achieved remarkable success, their generation process is computationally inefficient, often requiring hundreds to thousands of expensive function evaluations per sample. Flow matching has emerged as a more efficient paradigm, yet its conventional ordinary differential equation (ODE)-based formulation fails to explicitly capture stochasticity, thereby limiting the fidelity of generated sequences. By contrast, stochastic differential equation (SDE) are naturally suited for modeling randomness and uncertainty. Motivated by these insights, we propose TimeFlow, a novel SDE-based flow matching framework that integrates a encoder-only architecture. Specifically, we design a component-wise decomposed velocity field to capture the multi-faceted structure of time series and augment the vanilla flow-matching optimization with an additional stochastic term to enhance representational expressiveness. TimeFlow is flexible and general, supporting both unconditional and conditional generation tasks within a unified framework. Extensive experiments across diverse datasets demonstrate that our model consistently outperforms strong baselines in generation quality, diversity, and efficiency.

</details>


### [30] [Versatile and Risk-Sensitive Cardiac Diagnosis via Graph-Based ECG Signal Representation](https://arxiv.org/abs/2511.07973)
*Yue Wang,Yuyang Xu,Renjun Hu,Fanqi Shen,Hanyun Jiang,Jun Wang,Jintai Chen,Danny Z. Chen,Jian Wu,Haochao Ying*

Main category: cs.AI

TL;DR: 本文提出VARS方法解决ECG信号诊断方法临床应用的两大障碍，在多数据集上表现超现有模型，有望成心脏健康评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有ECG信号诊断和分析方法存在处理多样信号缺乏通用性、样本失衡致风险信号检测不足的问题，限制临床应用。

Method: 引入VARS方法，用基于图的表示统一建模异构ECG信号，将信号转化为通用图结构，结合去噪重建和对比学习进行表示转换。

Result: 在三个不同ECG数据集上，VARS始终超越现有模型，识别风险信号能力大幅提升，且能定位波形辅助临床决策。

Conclusion: VARS有望成为全面心脏健康评估的重要工具。

Abstract: Despite the rapid advancements of electrocardiogram (ECG) signal diagnosis and analysis methods through deep learning, two major hurdles still limit their clinical adoption: the lack of versatility in processing ECG signals with diverse configurations, and the inadequate detection of risk signals due to sample imbalances. Addressing these challenges, we introduce VersAtile and Risk-Sensitive cardiac diagnosis (VARS), an innovative approach that employs a graph-based representation to uniformly model heterogeneous ECG signals. VARS stands out by transforming ECG signals into versatile graph structures that capture critical diagnostic features, irrespective of signal diversity in the lead count, sampling frequency, and duration. This graph-centric formulation also enhances diagnostic sensitivity, enabling precise localization and identification of abnormal ECG patterns that often elude standard analysis methods. To facilitate representation transformation, our approach integrates denoising reconstruction with contrastive learning to preserve raw ECG information while highlighting pathognomonic patterns. We rigorously evaluate the efficacy of VARS on three distinct ECG datasets, encompassing a range of structural variations. The results demonstrate that VARS not only consistently surpasses existing state-of-the-art models across all these datasets but also exhibits substantial improvement in identifying risk signals. Additionally, VARS offers interpretability by pinpointing the exact waveforms that lead to specific model outputs, thereby assisting clinicians in making informed decisions. These findings suggest that our VARS will likely emerge as an invaluable tool for comprehensive cardiac health assessment.

</details>


### [31] [Towards Fine-Grained Interpretability: Counterfactual Explanations for Misclassification with Saliency Partition](https://arxiv.org/abs/2511.07974)
*Lintong Zhang,Kang Yin,Seong-Whan Lee*

Main category: cs.AI

TL;DR: 提出细粒度反事实解释框架，非生成方式生成可解释反事实，实验证明其优于细粒度方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于归因的解释技术在细粒度任务中缺乏粒度，尤其是模型误分类时解释不够详细。

Method: 提出细粒度反事实解释框架，非生成方式量化样本相似度和组件贡献，引入基于Shapley值的显著性分区模块。

Result: 实验表明该方法能捕捉更细粒度、直观有意义的区域。

Conclusion: 所提方法在细粒度任务解释上优于现有细粒度方法。

Abstract: Attribution-based explanation techniques capture key patterns to enhance visual interpretability; however, these patterns often lack the granularity needed for insight in fine-grained tasks, particularly in cases of model misclassification, where explanations may be insufficiently detailed. To address this limitation, we propose a fine-grained counterfactual explanation framework that generates both object-level and part-level interpretability, addressing two fundamental questions: (1) which fine-grained features contribute to model misclassification, and (2) where dominant local features influence counterfactual adjustments. Our approach yields explainable counterfactuals in a non-generative manner by quantifying similarity and weighting component contributions within regions of interest between correctly classified and misclassified samples. Furthermore, we introduce a saliency partition module grounded in Shapley value contributions, isolating features with region-specific relevance. Extensive experiments demonstrate the superiority of our approach in capturing more granular, intuitively meaningful regions, surpassing fine-grained methods.

</details>


### [32] [Benchmarking Multi-Step Legal Reasoning and Analyzing Chain-of-Thought Effects in Large Language Models](https://arxiv.org/abs/2511.07979)
*Wenhan Yu,Xinbo Lin,Lanxin Ni,Jinhua Cheng,Lei Sha*

Main category: cs.AI

TL;DR: 本文介绍首个中文多步法律推理数据集MSLR，评估显示LLM表现一般，自生成思维链提示效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有法律基准存在缺陷，如混淆事实回忆与真正推理等，需新数据集推动大语言模型在法律推理中的应用。

Method: 引入基于真实司法决策的MSLR数据集，采用IRAC框架建模，设计人机协作标注流程。

Result: 多个大语言模型在MSLR上表现一般，自生成思维链提示在推理连贯性和质量上优于人工设计提示。

Conclusion: MSLR有助于推进大语言模型推理和思维链策略，为未来研究提供开放资源。

Abstract: Large language models (LLMs) have demonstrated strong reasoning abilities across specialized domains, motivating research into their application to legal reasoning. However, existing legal benchmarks often conflate factual recall with genuine inference, fragment the reasoning process, and overlook the quality of reasoning. To address these limitations, we introduce MSLR, the first Chinese multi-step legal reasoning dataset grounded in real-world judicial decision making. MSLR adopts the IRAC framework (Issue, Rule, Application, Conclusion) to model structured expert reasoning from official legal documents. In addition, we design a scalable Human-LLM collaborative annotation pipeline that efficiently produces fine-grained step-level reasoning annotations and provides a reusable methodological framework for multi-step reasoning datasets. Evaluation of multiple LLMs on MSLR shows only moderate performance, highlighting the challenges of adapting to complex legal reasoning. Further experiments demonstrate that Self-Initiated Chain-of-Thought prompts generated by models autonomously improve reasoning coherence and quality, outperforming human-designed prompts. MSLR contributes to advancing LLM reasoning and Chain-of-Thought strategies and offers open resources for future research. The dataset and code are available at https://github.com/yuwenhan07/MSLR-Bench and https://law.sjtu.edu.cn/flszyjzx/index.html.

</details>


### [33] [Capturing Complex Spatial-Temporal Dependencies in Traffic Forecasting: A Self-Attention Approach](https://arxiv.org/abs/2511.07980)
*Zheng Chenghong,Zongyin Deng,Liu Cheng,Xiong Simin,Di Deshi,Li Guanyao*

Main category: cs.AI

TL;DR: 提出用于交通预测的ST - SAM模型，实验表明其比现有方法更准确高效。


<details>
  <summary>Details</summary>
Motivation: 以往工作以分离方式研究时空依赖，未能捕捉其联合效应，而交通预测问题因区域间复杂的时空相互依赖而复杂，需要解决该问题。

Method: 提出ST - SAM模型，使用区域嵌入层从交通数据中学习区域特定时间的嵌入，采用基于自注意力机制的时空依赖学习模块捕捉远近区域的联合时空依赖。

Result: 在两个真实世界数据集上的大量实验表明，ST - SAM在RMSE上平均提高达15%，在MAPE上平均提高17%，训练时间快32倍。

Conclusion: ST - SAM比现有方法更准确和高效。

Abstract: We study the problem of traffic forecasting, aiming to predict the inflow and outflow of a region in the subsequent time slot. The problem is complex due to the intricate spatial and temporal interdependence among regions. Prior works study the spatial and temporal dependency in a decouple manner, failing to capture their joint effect. In this work, we propose ST-SAM, a novel and efficient Spatial-Temporal Self-Attention Model for traffic forecasting. ST-SAM uses a region embedding layer to learn time-specific embedding from traffic data for regions. Then, it employs a spatial-temporal dependency learning module based on self-attention mechanism to capture the joint spatial-temporal dependency for both nearby and faraway regions. ST-SAM entirely relies on self-attention to capture both local and global spatial-temporal correlations, which make it effective and efficient. Extensive experiments on two real world datasets show that ST-SAM is substantially more accurate and efficient than the state-of-the-art approaches (with an average improvement of up to 15% on RMSE, 17% on MAPE, and 32 times on training time in our experiments).

</details>


### [34] [The One Where They Brain-Tune for Social Cognition: Multi-Modal Brain-Tuning on Friends](https://arxiv.org/abs/2511.07988)
*Nico Policzer,Cameron Braunstein,Mariya Toneva*

Main category: cs.AI

TL;DR: 研究将脑调优方法扩展到多模态视听模型以增强社会认知，取得了脑对齐和下游任务表现的提升。


<details>
  <summary>Details</summary>
Motivation: 现有音频模型的脑调优方法有效，尝试将其扩展到多模态视听模型以增强社会认知。

Method: 在受试者观看《老友记》时，将脑调优方法应用于多模态视听模型，针对社会处理关键区域上颞沟（STS）进行调优。

Result: 在STS及相邻感兴趣区域（ROI）的脑对齐显著增加，与训练数据相关的社会认知任务（情景喜剧中的讽刺检测）表现得到改善。

Conclusion: 将脑调优扩展到多模态领域，调优到相关功能区域后下游任务表现得到提升。

Abstract: Recent studies on audio models show brain-tuning - fine-tuning models to better predict corresponding fMRI activity - improves brain alignment and increases performance on downstream semantic and audio tasks. We extend this approach to a multimodal audio-video model to enhance social cognition, targeting the Superior Temporal Sulcus (STS), a key region for social processing, while subjects watch Friends. We find significant increases in brain alignment to the STS and an adjacent ROI, as well as improvements to a social cognition task related to the training data - sarcasm detection in sitcoms. In summary, our study extends brain-tuning to the multi-modal domain, demonstrating improvements to a downstream task after tuning to a relevant functional region.

</details>


### [35] [VSPO: Validating Semantic Pitfalls in Ontology via LLM-Based CQ Generation](https://arxiv.org/abs/2511.07991)
*Hyojun Choi,Seokju Hwang,Kyong-Ho Lee*

Main category: cs.AI

TL;DR: 提出验证本体语义陷阱的数据集和模型VSPO用于生成能力问题（CQs），微调模型表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以验证CQs中的语义陷阱，需新方法减少人工成本并提升语义对齐。

Method: 用大语言模型生成类和属性的自然语言定义，引入定义与本体的偏差，微调LLaMA - 3.1 - 8B - Instruct生成CQs。

Result: 生成的CQs能检测更多建模错误，微调模型在生成用于陷阱验证的CQs时，比GPT - 4.1精度高26%，召回率高28.2%。

Conclusion: 该研究实现用大语言模型自动生成TBox验证CQs，减少人工且提升语义对齐，是首个用大语言模型进行CQs语义陷阱验证的研究。

Abstract: Competency Questions (CQs) play a crucial role in validating ontology design. While manually crafting CQs can be highly time-consuming and costly for ontology engineers, recent studies have explored the use of large language models (LLMs) to automate this process. However, prior approaches have largely evaluated generated CQs based on their similarity to existing datasets, which often fail to verify semantic pitfalls such as "Misusing allValuesFrom". Since such pitfalls cannot be reliably detected through rule-based methods, we propose a novel dataset and model of Validating Semantic Pitfalls in Ontology (VSPO) for CQ generation specifically designed to verify the semantic pitfalls. To simulate missing and misused axioms, we use LLMs to generate natural language definitions of classes and properties and introduce misalignments between the definitions and the ontology by removing axioms or altering logical operators (e.g., substituting union with intersection). We then fine-tune LLaMA-3.1-8B-Instruct to generate CQs that validate these semantic discrepancies between the provided definitions and the corresponding axioms. The resulting CQs can detect a broader range of modeling errors compared to existing public datasets. Our fine-tuned model demonstrates superior performance over baselines, showing 26% higher precision and 28.2% higher recall than GPT-4.1 in generating CQs for pitfall validation. This research enables automatic generation of TBox-validating CQs using LLMs, significantly reducing manual effort while improving semantic alignment between ontologies and expert knowledge. To the best of our knowledge, this is the first study to target semantic pitfall validation in CQ generation using LLMs.

</details>


### [36] [Enhancing Logical Expressiveness in Graph Neural Networks via Path-Neighbor Aggregation](https://arxiv.org/abs/2511.07994)
*Han Yu,Xiaojuan Zhao,Aiping Li,Kai Chen,Ziniu Liu,Zhichao Peng*

Main category: cs.AI

TL;DR: 提出Path - Neighbor增强GNN (PN - GNN)方法增强GNN逻辑表达能力，经理论分析和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN表达能力研究多针对单关系图，对KG中逻辑规则表达能力讨论不足，需增强GNN逻辑表达能力。

Method: 提出PN - GNN方法，通过聚合推理路径上的节点邻居嵌入增强GNN逻辑表达能力，分析现有方法不足，理论研究PN - GNN表达能力，在多个数据集上评估。

Result: 理论上PN - GNN比C - GNN表达能力更强，(k + 1)跳逻辑表达优于k跳；实验表明PN - GNN在不影响泛化能力的情况下增强逻辑规则表达能力。

Conclusion: PN - GNN能在不影响泛化的前提下增强逻辑规则表达能力，在KG推理任务中有竞争力。

Abstract: Graph neural networks (GNNs) can effectively model structural information of graphs, making them widely used in knowledge graph (KG) reasoning. However, existing studies on the expressive power of GNNs mainly focuses on simple single-relation graphs, and there is still insufficient discussion on the power of GNN to express logical rules in KGs. How to enhance the logical expressive power of GNNs is still a key issue. Motivated by this, we propose Path-Neighbor enhanced GNN (PN-GNN), a method to enhance the logical expressive power of GNN by aggregating node-neighbor embeddings on the reasoning path. First, we analyze the logical expressive power of existing GNN-based methods and point out the shortcomings of the expressive power of these methods. Then, we theoretically investigate the logical expressive power of PN-GNN, showing that it not only has strictly stronger expressive power than C-GNN but also that its $(k+1)$-hop logical expressiveness is strictly superior to that of $k$-hop. Finally, we evaluate the logical expressive power of PN-GNN on six synthetic datasets and two real-world datasets. Both theoretical analysis and extensive experiments confirm that PN-GNN enhances the expressive power of logical rules without compromising generalization, as evidenced by its competitive performance in KG reasoning tasks.

</details>


### [37] [Multivariate Time series Anomaly Detection:A Framework of Hidden Markov Models](https://arxiv.org/abs/2511.07995)
*Jinbo Li,Witold Pedrycz,Iqbal Jamal*

Main category: cs.AI

TL;DR: 研究提出将多变量时间序列转换为单变量时间序列的异常检测方法，用FCM聚类和模糊积分转换，用HMM检测异常并比较转换方法，进行实验研究。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列的异常检测问题。

Method: 将多变量时间序列通过FCM聚类和模糊积分转换为单变量时间序列，用HMM构建异常检测器并比较不同转换方法。

Result: 进行了一系列实验研究和对比分析，但未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: In this study, we develop an approach to multivariate time series anomaly detection focused on the transformation of multivariate time series to univariate time series. Several transformation techniques involving Fuzzy C-Means (FCM) clustering and fuzzy integral are studied. In the sequel, a Hidden Markov Model (HMM), one of the commonly encountered statistical methods, is engaged here to detect anomalies in multivariate time series. We construct HMM-based anomaly detectors and in this context compare several transformation methods. A suite of experimental studies along with some comparative analysis is reported.

</details>


### [38] [Combining LLM Semantic Reasoning with GNN Structural Modeling for Multi-view Multi-Label Feature Selection](https://arxiv.org/abs/2511.08008)
*Zhiqi Chen,Yuzhou Liu,Jiarui Liu,Wanfu Gao*

Main category: cs.AI

TL;DR: 提出结合大语言模型语义推理和图神经网络结构建模的多视图多标签特征选择方法，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多视图多标签特征选择方法主要关注数据统计信息，很少考虑语义信息，本文旨在联合使用这两种信息。

Method: 由三部分组成：用大语言模型评估特征、视图和标签描述间的潜在语义相关性；设计两级语义感知异构图；用轻量级图注意力网络学习节点嵌入作为特征显著性得分进行排序和选择。

Result: 在多个基准数据集上的实验结果表明该方法优于现有基线方法，应用于小规模数据集时仍有效。

Conclusion: 该方法具有鲁棒性、灵活性和泛化能力。

Abstract: Multi-view multi-label feature selection aims to identify informative features from heterogeneous views, where each sample is associated with multiple interdependent labels. This problem is particularly important in machine learning involving high-dimensional, multimodal data such as social media, bioinformatics or recommendation systems. Existing Multi-View Multi-Label Feature Selection (MVMLFS) methods mainly focus on analyzing statistical information of data, but seldom consider semantic information. In this paper, we aim to use these two types of information jointly and propose a method that combines Large Language Models (LLMs) semantic reasoning with Graph Neural Networks (GNNs) structural modeling for MVMLFS. Specifically, the method consists of three main components. (1) LLM is first used as an evaluation agent to assess the latent semantic relevance among feature, view, and label descriptions. (2) A semantic-aware heterogeneous graph with two levels is designed to represent relations among features, views and labels: one is a semantic graph representing semantic relations, and the other is a statistical graph. (3) A lightweight Graph Attention Network (GAT) is applied to learn node embedding in the heterogeneous graph as feature saliency scores for ranking and selection. Experimental results on multiple benchmark datasets demonstrate the superiority of our method over state-of-the-art baselines, and it is still effective when applied to small-scale datasets, showcasing its robustness, flexibility, and generalization ability.

</details>


### [39] [Numerical Sensitivity and Robustness: Exploring the Flaws of Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2511.08022)
*Zhishen Sun,Guang Dai,Ivor Tsang,Haishan Ye*

Main category: cs.AI

TL;DR: 提出新扰动框架评估大语言模型在复杂环境下推理能力，实验揭示其推理能力的不足与局限。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否具备真正的数学理解能力。

Method: 提出新扰动框架，通过注入语义无关扰动句子并增加扰动强度评估推理能力，使用核心提问指令缺失方法分析解题机制。

Result: 大语言模型面对无数字扰动句子时表现稳定但有鲁棒性边界，面对含数字扰动句子性能显著下降；核心提问指令缺失时仍有20%-40%准确率。

Conclusion: 揭示当前大语言模型推理能力的缺点和局限，对其进一步发展有重要意义。

Abstract: LLMs have made significant progress in the field of mathematical reasoning, but whether they have true the mathematical understanding ability is still controversial. To explore this issue, we propose a new perturbation framework to evaluate LLMs' reasoning ability in complex environments by injecting additional semantically irrelevant perturbation sentences and gradually increasing the perturbation intensity. At the same time, we use an additional perturbation method: core questioning instruction missing, to further analyze the LLMs' problem-solving mechanism. The experimental results show that LLMs perform stably when facing perturbation sentences without numbers, but there is also a robustness boundary. As the perturbation intensity increases, the performance exhibits varying degrees of decline; when facing perturbation sentences with numbers, the performance decreases more significantly, most open source models with smaller parameters decrease by nearly or even more than 10%, and further increasing with the enhancement of perturbation intensity, with the maximum decrease reaching 51.55%. Even the most advanced commercial LLMs have seen a 3%-10% performance drop. By analyzing the reasoning process of LLMs in detail, We find that models are more sensitive to perturbations with numerical information and are more likely to give incorrect answers when disturbed by irrelevant numerical information. The higher the perturbation intensity, the more obvious these defects are. At the same time, in the absence of core questioning instruction, models can still maintain an accuracy of 20%-40%, indicating that LLMs may rely on memory templates or pattern matching to complete the task, rather than logical reasoning. In general, our work reveals the shortcomings and limitations of current LLMs in their reasoning capabilities, which is of great significance for the further development of LLMs.

</details>


### [40] [Knowledge-Augmented Long-CoT Generation for Complex Biomolecular Reasoning](https://arxiv.org/abs/2511.08024)
*Tianwen Lyu,Xiang Zhuang,Keyan Ding,Xinzhe Cao,Lei Liang,Wei Zhao,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: 提出知识增强长思维链推理框架，结合大语言模型与知识图谱多跳推理链，并引入PrimeKGQA基准，在多跳任务中表现优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于生物分子问题存在逻辑不一致和缺乏领域知识支撑的问题，现有方法会加剧这些问题。

Method: 提出知识增强长思维链推理框架，通过知识图谱引导多跳遍历和剪枝构建机制链，结合监督微调与强化学习；引入PrimeKGQA基准。

Result: 在PrimeKGQA和现有数据集上实验，虽大闭源模型在简单任务表现好，但该方法在推理深度增加时优势明显，多跳任务达SOTA。

Conclusion: 结合结构化知识与先进推理策略对可靠且可解释的生物分子推理有效。

Abstract: Understanding complex biomolecular mechanisms requires multi-step reasoning across molecular interactions, signaling cascades, and metabolic pathways. While large language models(LLMs) show promise in such tasks, their application to biomolecular problems is hindered by logical inconsistencies and the lack of grounding in domain knowledge. Existing approaches often exacerbate these issues: reasoning steps may deviate from biological facts or fail to capture long mechanistic dependencies. To address these challenges, we propose a Knowledge-Augmented Long-CoT Reasoning framework that integrates LLMs with knowledge graph-based multi-hop reasoning chains. The framework constructs mechanistic chains via guided multi-hop traversal and pruning on the knowledge graph; these chains are then incorporated into supervised fine-tuning to improve factual grounding and further refined with reinforcement learning to enhance reasoning reliability and consistency. Furthermore, to overcome the shortcomings of existing benchmarks, which are often restricted in scale and scope and lack annotations for deep reasoning chains, we introduce PrimeKGQA, a comprehensive benchmark for biomolecular question answering. Experimental results on both PrimeKGQA and existing datasets demonstrate that although larger closed-source models still perform well on relatively simple tasks, our method demonstrates clear advantages as reasoning depth increases, achieving state-of-the-art performance on multi-hop tasks that demand traversal of structured biological knowledge. These findings highlight the effectiveness of combining structured knowledge with advanced reasoning strategies for reliable and interpretable biomolecular reasoning.

</details>


### [41] [Towards a Standard, Enterprise-Relevant Agentic AI Benchmark: Lessons from 5.5 billion tokens' worth of agentic AI evaluations](https://arxiv.org/abs/2511.08042)
*JV Roig*

Main category: cs.AI

TL;DR: 提出企业级基准KAMI v0.1评估代理AI系统，表明传统基准排名难预测实际性能，给出企业部署决策关键见解。


<details>
  <summary>Details</summary>
Motivation: 企业采用代理AI系统需能反映实际部署场景的可靠评估方法，传统大语言模型基准存在缺陷。

Method: 提出KAMI v0.1基准，处理170,000个大语言模型测试项，涵盖超55亿个标记和35种模型配置。

Result: 传统基准排名难以预测实际代理性能，新一代模型在企业相关任务中未必优于旧一代。

Conclusion: 研究成果为企业进行代理AI系统部署决策提供关键见解。

Abstract: Enterprise adoption of agentic AI systems requires reliable evaluation methods that reflect real-world deployment scenarios. Traditional LLM benchmarks suffer from training data contamination and fail to measure agentic capabilities such as multi-step tool use and decision-making under uncertainty. We present the Kamiwaza Agentic Merit Index (KAMI) v0.1, an enterprise-focused benchmark that addresses both contamination resistance and agentic evaluation. Through 170,000 LLM test items processing over 5.5 billion tokens across 35 model configurations, we demonstrate that traditional benchmark rankings poorly predict practical agentic performance. Notably, newer generation models like Llama 4 or Qwen 3 do not always outperform their older generation variants on enterprise-relevant tasks, contradicting traditional benchmark trends. We also present insights on cost-performance tradeoffs, model-specific behavioral patterns, and the impact of reasoning capabilities on token efficiency -- findings critical for enterprises making deployment decisions.

</details>


### [42] [MSCR: Exploring the Vulnerability of LLMs' Mathematical Reasoning Abilities Using Multi-Source Candidate Replacement](https://arxiv.org/abs/2511.08055)
*Zhishen Sun,Guang Dai,Haishan Ye*

Main category: cs.AI

TL;DR: 提出基于多源候选替换的自动化对抗攻击方法MSCR，实验表明轻微扰动可大幅降低大语言模型数学推理准确率，暴露其鲁棒性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理中鲁棒性缺乏系统研究，现有方法存在可扩展性有限、语义保留弱和成本高等问题。

Method: 提出MSCR方法，结合大语言模型嵌入空间余弦相似度、WordNet词典和掩码语言模型上下文预测三个信息源生成候选词，过滤后逐个替换进行攻击。

Result: 单字扰动可显著降低模型准确率，GSM8K最高降49.89%，MATH500最高降35.40%，且扰动使输出变长，增加推理路径和计算资源消耗。

Conclusion: 当前大语言模型在数学推理任务中存在鲁棒性不足和效率瓶颈。

Abstract: LLMs demonstrate performance comparable to human abilities in complex tasks such as mathematical reasoning, but their robustness in mathematical reasoning under minor input perturbations still lacks systematic investigation. Existing methods generally suffer from limited scalability, weak semantic preservation, and high costs. Therefore, we propose MSCR, an automated adversarial attack method based on multi-source candidate replacement. By combining three information sources including cosine similarity in the embedding space of LLMs, the WordNet dictionary, and contextual predictions from a masked language model, we generate for each word in the input question a set of semantically similar candidates, which are then filtered and substituted one by one to carry out the attack. We conduct large-scale experiments on LLMs using the GSM8K and MATH500 benchmarks. The results show that even a slight perturbation involving only a single word can significantly reduce the accuracy of all models, with the maximum drop reaching 49.89% on GSM8K and 35.40% on MATH500, while preserving the high semantic consistency of the perturbed questions. Further analysis reveals that perturbations not only lead to incorrect outputs but also substantially increase the average response length, which results in more redundant reasoning paths and higher computational resource consumption. These findings highlight the robustness deficiencies and efficiency bottlenecks of current LLMs in mathematical reasoning tasks.

</details>


### [43] [Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression](https://arxiv.org/abs/2511.08066)
*Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 因大语言模型计算资源需求大，引入信息容量衡量模型效率，评估49个模型有一致结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算资源需求大，缺乏统一效率衡量指标，基于压缩与智能的关联提出信息容量指标。

Method: 通过文本压缩性能与计算复杂度关系定义信息容量，在主流开源模型上进行实证评估。

Result: 同一系列不同大小模型信息容量一致，能跨系列公平比较效率和系列内准确预测性能，评估49个模型有一致结果。

Conclusion: 信息容量可作为衡量大语言模型效率的指标，且考虑了分词器效率。

Abstract: Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.

</details>


### [44] [Clustering-based Anomaly Detection in Multivariate Time Series Data](https://arxiv.org/abs/2511.08072)
*Jinbo Li,Hesam Izakian,Witold Pedrycz,Iqbal Jamal*

Main category: cs.AI

TL;DR: 本文提出基于聚类的方法检测多元时间序列异常，通过滑动窗口、扩展模糊聚类、重建准则和置信指数等步骤，实验表明该方法有效且适用于多领域。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列异常检测因需同时考虑时间和变量关系而具有挑战性，但应用广泛，故提出新方法。

Method: 使用滑动窗口生成多元子序列，应用扩展模糊聚类揭示结构，用重建准则重建子序列，构建置信指数量化异常，用粒子群优化解决检测问题。

Result: 在多个合成和六个真实数据集上实验，表明方法能检测多元时间序列异常。

Conclusion: 借助扩展模糊聚类的聚类结果，该框架可检测多元时间序列异常，适用于多领域异常幅度和形状模式识别。

Abstract: Multivariate time series data come as a collection of time series describing different aspects of a certain temporal phenomenon. Anomaly detection in this type of data constitutes a challenging problem yet with numerous applications in science and engineering because anomaly scores come from the simultaneous consideration of the temporal and variable relationships. In this paper, we propose a clustering-based approach to detect anomalies concerning the amplitude and the shape of multivariate time series. First, we use a sliding window to generate a set of multivariate subsequences and thereafter apply an extended fuzzy clustering to reveal a structure present within the generated multivariate subsequences. Finally, a reconstruction criterion is employed to reconstruct the multivariate subsequences with the optimal cluster centers and the partition matrix. We construct a confidence index to quantify a level of anomaly detected in the series and apply Particle Swarm Optimization as an optimization vehicle for the problem of anomaly detection. Experimental studies completed on several synthetic and six real-world datasets suggest that the proposed methods can detect the anomalies in multivariate time series. With the help of available clusters revealed by the extended fuzzy clustering, the proposed framework can detect anomalies in the multivariate time series and is suitable for identifying anomalous amplitude and shape patterns in various application domains such as health care, weather data analysis, finance, and disease outbreak detection.

</details>


### [45] [Gateways to Tractability for Satisfiability in Pearl's Causal Hierarchy](https://arxiv.org/abs/2511.08091)
*Robert Ganian,Marlene Gründel,Simon Wietheger*

Main category: cs.AI

TL;DR: 本文从参数化复杂度角度研究Pearl因果层次结构（PCH）公式可满足性问题，得到关键片段的算法及复杂度结果，还提供新算法工具包。


<details>
  <summary>Details</summary>
Motivation: PCH公式可满足性问题在经典设置中计算难解，需寻找可处理性的途径。

Method: 从参数化复杂度角度，利用如原始树宽和变量数量等参数，且不采用典型的动态规划范式，而是利用结构特征。

Result: 得到关键概率和反事实片段可满足性的固定参数和XP算法，以及匹配的难度结果。

Conclusion: 为因果推理提供了新的算法工具包。

Abstract: Pearl's Causal Hierarchy (PCH) is a central framework for reasoning about probabilistic, interventional, and counterfactual statements, yet the satisfiability problem for PCH formulas is computationally intractable in almost all classical settings. We revisit this challenge through the lens of parameterized complexity and identify the first gateways to tractability. Our results include fixed-parameter and XP-algorithms for satisfiability in key probabilistic and counterfactual fragments, using parameters such as primal treewidth and the number of variables, together with matching hardness results that map the limits of tractability. Technically, we depart from the dynamic programming paradigm typically employed for treewidth-based algorithms and instead exploit structural characterizations of well-formed causal models, providing a new algorithmic toolkit for causal reasoning.

</details>


### [46] [Improving Industrial Injection Molding Processes with Explainable AI for Quality Classification](https://arxiv.org/abs/2511.08108)
*Georg Rottenwalter,Marcel Tilly,Victor Owolabi*

Main category: cs.AI

TL;DR: 本文研究XAI技术特征约简对注塑件质量分类的影响，应用多种方法分析特征重要性，结果显示约简特征可提升泛化能力且保持高性能，提高AI质量控制可行性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型复杂缺乏可解释性，工业机器数据采集难，需可解释AI解决质量控制问题。

Method: 应用SHAP、Grad - CAM和LIME分析LSTM模型特征重要性，将19个输入特征分别约简为9个和6个。

Result: 约简特征可在保持高分类性能的同时提升泛化能力，推理速度有小幅度提升。

Conclusion: 该方法提高了AI驱动质量控制在工业场景的可行性，为制造业机器学习应用提供方向。

Abstract: Machine learning is an essential tool for optimizing industrial quality control processes. However, the complexity of machine learning models often limits their practical applicability due to a lack of interpretability. Additionally, many industrial machines lack comprehensive sensor technology, making data acquisition incomplete and challenging. Explainable Artificial Intelligence offers a solution by providing insights into model decision-making and identifying the most relevant features for classification. In this paper, we investigate the impact of feature reduction using XAI techniques on the quality classification of injection-molded parts. We apply SHAP, Grad-CAM, and LIME to analyze feature importance in a Long Short-Term Memory model trained on real production data. By reducing the original 19 input features to 9 and 6, we evaluate the trade-off between model accuracy, inference speed, and interpretability. Our results show that reducing features can improve generalization while maintaining high classification performance, with an small increase in inference speed. This approach enhances the feasibility of AI-driven quality control, particularly for industrial settings with limited sensor capabilities, and paves the way for more efficient and interpretable machine learning applications in manufacturing.

</details>


### [47] [Advancements in synthetic data extraction for industrial injection molding](https://arxiv.org/abs/2511.08117)
*Georg Rottenwalter,Marcel Tilly,Christian Bielenberg,Katharina Obermeier*

Main category: cs.AI

TL;DR: 本文研究在注塑过程训练中引入合成数据，通过模拟生产周期生成合成数据并加入训练集，经实验找到最优比例，结果显示能提升模型处理不同场景能力，有望用于工业降本增效。


<details>
  <summary>Details</summary>
Motivation: 机器学习优化工业过程时数据获取耗时且成本高，合成数据可扩充数据集、提升模型鲁棒性，研究其在注塑过程训练中的可行性。

Method: 利用现有长短期记忆架构，模拟生产周期生成合成数据并加入训练数据集，通过不同比例合成数据的迭代实验找最优平衡。

Result: 加入合成数据提升了模型处理不同场景的能力，有实际工业应用潜力，可减少人力、机器使用和材料浪费。

Conclusion: 该方法为数据收集和维护成本高或不可行的情况提供了替代方案，有助于未来制造业更高效。

Abstract: Machine learning has significant potential for optimizing various industrial processes. However, data acquisition remains a major challenge as it is both time-consuming and costly. Synthetic data offers a promising solution to augment insufficient data sets and improve the robustness of machine learning models. In this paper, we investigate the feasibility of incorporating synthetic data into the training process of the injection molding process using an existing Long Short-Term Memory architecture. Our approach is to generate synthetic data by simulating production cycles and incorporating them into the training data set. Through iterative experimentation with different proportions of synthetic data, we attempt to find an optimal balance that maximizes the benefits of synthetic data while preserving the authenticity and relevance of real data. Our results suggest that the inclusion of synthetic data improves the model's ability to handle different scenarios, with potential practical industrial applications to reduce manual labor, machine use, and material waste. This approach provides a valuable alternative for situations where extensive data collection and maintenance has been impractical or costly and thus could contribute to more efficient manufacturing processes in the future.

</details>


### [48] [National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech - The SpeechCARE Solution](https://arxiv.org/abs/2511.08132)
*Maryam Zolnoori,Hossein Azadmaleki,Yasaman Haghbin,Ali Zolnour,Mohammad Javad Momeni Nezhad,Sina Rashidi,Mehdi Naserian,Elyas Esmaeili,Sepehr Karimi Arpanahi*

Main category: cs.AI

TL;DR: 介绍了用于早期检测认知障碍的多模态语音处理管道SpeechCARE，评估其性能并指出未来工作方向。


<details>
  <summary>Details</summary>
Motivation: 现有基于语音评估认知障碍的传统方法性能和泛化性有限，需要更好的方法进行早期检测。

Method: 引入SpeechCARE，利用预训练多语言声学和语言Transformer模型，采用动态融合架构，有强大的预处理和可解释性模块。

Result: 在分类认知健康、MCI和AD个体时AUC = 0.88、F1 = 0.72，检测MCI时AUC = 0.90、F1 = 0.62，除80岁以上成年人外偏差极小。

Conclusion: SpeechCARE有较好的性能，未来将在现实护理环境部署并为弱势群体提供电子健康记录集成的可解释性。

Abstract: Alzheimer's disease and related dementias (ADRD) affect one in five adults over 60, yet more than half of individuals with cognitive decline remain undiagnosed. Speech-based assessments show promise for early detection, as phonetic motor planning deficits alter acoustic features (e.g., pitch, tone), while memory and language impairments lead to syntactic and semantic errors. However, conventional speech-processing pipelines with hand-crafted features or general-purpose audio classifiers often exhibit limited performance and generalizability. To address these limitations, we introduce SpeechCARE, a multimodal speech processing pipeline that leverages pretrained, multilingual acoustic and linguistic transformer models to capture subtle speech-related cues associated with cognitive impairment. Inspired by the Mixture of Experts (MoE) paradigm, SpeechCARE employs a dynamic fusion architecture that weights transformer-based acoustic, linguistic, and demographic inputs, allowing integration of additional modalities (e.g., social factors, imaging) and enhancing robustness across diverse tasks. Its robust preprocessing includes automatic transcription, large language model (LLM)-based anomaly detection, and task identification. A SHAP-based explainability module and LLM reasoning highlight each modality's contribution to decision-making. SpeechCARE achieved AUC = 0.88 and F1 = 0.72 for classifying cognitively healthy, MCI, and AD individuals, with AUC = 0.90 and F1 = 0.62 for MCI detection. Bias analysis showed minimal disparities, except for adults over 80. Mitigation techniques included oversampling and weighted loss. Future work includes deployment in real-world care settings (e.g., VNS Health, Columbia ADRC) and EHR-integrated explainability for underrepresented populations in New York City.

</details>


### [49] [SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning](https://arxiv.org/abs/2511.08151)
*Xuchen Li,Ruitao Wu,Xuanbo Liu,Xukai Wang,Jinbo Hu,Zhixin Bai,Bohan Zeng,Hao Liang,Leheng Chen,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Xu-Yao Zhang,Liu Liu,Jia Li,Kaiqi Huang,Jiahao Xu,Haitao Mi,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: 介绍多智能体系统SciAgent用于通用科学推理，在多学科竞赛中表现出色，迈向通用科学智能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在特定科学任务上虽有进展，但系统狭窄且需手工制作，需要通用科学推理能力的系统。

Method: 将问题解决组织成层次化过程，协调器代理解释问题，动态编排专业工作系统，各系统含推理子代理协作组装和完善推理管道。

Result: 在数学和物理奥林匹克竞赛中达到或超越人类金牌得主表现，在化学奥林匹克竞赛和HLE基准测试中也证实了跨领域泛化能力。

Conclusion: SciAgent是迈向通用科学智能的具体一步。

Abstract: Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.

</details>


### [50] [oboro: Text-to-Image Synthesis on Limited Data using Flow-based Diffusion Transformer with MMH Attention](https://arxiv.org/abs/2511.08168)
*Ryusuke Mizutani,Kazuaki Matano,Tsugumi Kadowaki,Haruki Tenya,Layris,nuigurumi,Koki Hashimoto,Yu Tanaka*

Main category: cs.AI

TL;DR: 为应对日本动漫制作行业劳动力短缺等挑战，项目开发了图像生成模型'oboro:'，其用版权清晰图像训练，架构能从有限数据集生成高质量图像，模型权重和推理代码公开，是日本首个开源商用图像生成AI。


<details>
  <summary>Details</summary>
Motivation: 应对日本动漫制作行业劳动力短缺等挑战，开发图像生成模型。

Method: 仅使用版权清晰的图像进行训练，构建新的图像生成模型'oboro:'。

Result: 开发出图像生成模型'oboro:'，模型权重和推理代码公开。

Conclusion: 该项目是日本首个开源商用图像生成AI，通过保持开发透明，旨在为日本AI社区做贡献，促进国内AI生态发展。

Abstract: This project was conducted as a 2nd-term adopted project of the "Post-5G Information and Communication System Infrastructure Enhancement R&D Project Development of Competitive Generative AI Foundation Models (GENIAC)," a business of the Ministry of Economy, Trade and Industry (METI) and the New Energy and Industrial Technology Development Organization (NEDO). To address challenges such as labor shortages in Japan's anime production industry, this project aims to develop an image generation model from scratch. This report details the technical specifications of the developed image generation model, "oboro:." We have developed "oboro:," a new image generation model built from scratch, using only copyright-cleared images for training. A key characteristic is its architecture, designed to generate high-quality images even from limited datasets. The foundation model weights and inference code are publicly available alongside this report. This project marks the first release of an open-source, commercially-oriented image generation AI fully developed in Japan. AiHUB originated from the OSS community; by maintaining transparency in our development process, we aim to contribute to Japan's AI researcher and engineer community and promote the domestic AI development ecosystem.

</details>


### [51] [An Efficient Training Pipeline for Reasoning Graphical User Interface Agents](https://arxiv.org/abs/2511.08172)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.AI

TL;DR: 本文提出结合数据过滤和轻量级训练策略的高效训练流程，在视觉定位任务上取得良好效果，证明数据筛选和稳健适配可媲美大规模训练。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位方法依赖大量嘈杂合成数据集，需改进训练方式。

Method: 先从480万合成示例中筛选出1.2万干净多样实例，再在这些数据上用监督微调、思维链增强微调、基于组相对策略优化的强化学习三种机制训练30亿参数的视觉语言模型。

Result: 经筛选数据和轻量级训练策略训练的模型在多个基准测试中达到或超越更大的基线模型。

Conclusion: 有原则的数据筛选和稳健适配能媲美大规模训练，可构建紧凑且有能力的多模态推理代理。

Abstract: Visual grounding is the task of localising image regions from natural language queries and is critical for reasoning capable Graphical User Interface agents. Many existing methods rely on massive, noisy synthetic datasets.This work introduces an efficient training pipeline that combines model-based data filtering with parameter-efficient fine-tuning. From 4.8M synthetic examples, 12K clean and diverse instances are curated by first identifying challenging cases, removing misaligned and then selecting a diverse set of multimodal instances. On this data, a 3B-parameter Vision-Language Model is trained under three regimes: supervised fine-tuning, chain-of-thought- augmented fine-tuning, and reinforcement learning via Group Relative Policy Optimization. Models trained with the filtered data and lightweight training strategies match or surpass larger baselines on benchmarks such as ScreenSpot, Multimodal-Mind2Web, and AndroidControl. These results demonstrate that principled data curation and robust adaptation can rival large-scale training, enabling compact yet capable multimodal reasoning agents.

</details>


### [52] [Towards Provably Unlearnable Examples via Bayes Error Optimisation](https://arxiv.org/abs/2511.08191)
*Ruihan Zhang,Jun Sun,Ee-Peng Lim,Peixin Zhang*

Main category: cs.AI

TL;DR: 本文提出通过最大化贝叶斯误差构建不可学习示例的新方法，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型大量使用在线数据引发用户数据保护担忧，现有不可学习示例构建方法缺乏理论保证且在混合干净数据时失效。

Method: 提出系统最大化贝叶斯误差构建不可学习示例的方法，采用基于优化的方法并使用投影梯度上升提供高效解决方案。

Result: 实验结果与理论分析一致，表明该方法能限制数据可学习性。

Conclusion: 所提方法可证明增加贝叶斯误差，在混合干净样本时仍有效，能在实践中有效限制数据可学习性。

Abstract: The recent success of machine learning models, especially large-scale classifiers and language models, relies heavily on training with massive data. These data are often collected from online sources. This raises serious concerns about the protection of user data, as individuals may not have given consent for their data to be used in training. To address this concern, recent studies introduce the concept of unlearnable examples, i.e., data instances that appear natural but are intentionally altered to prevent models from effectively learning from them. While existing methods demonstrate empirical effectiveness, they typically rely on heuristic trials and lack formal guarantees. Besides, when unlearnable examples are mixed with clean data, as is often the case in practice, their unlearnability disappears. In this work, we propose a novel approach to constructing unlearnable examples by systematically maximising the Bayes error, a measurement of irreducible classification error. We develop an optimisation-based approach and provide an efficient solution using projected gradient ascent. Our method provably increases the Bayes error and remains effective when the unlearning examples are mixed with clean samples. Experimental results across multiple datasets and model architectures are consistent with our theoretical analysis and show that our approach can restrict data learnability, effectively in practice.

</details>


### [53] [EHRStruct: A Comprehensive Benchmark Framework for Evaluating Large Language Models on Structured Electronic Health Record Tasks](https://arxiv.org/abs/2511.08206)
*Xiao Yang,Xuejiao Zhao,Zhiqi Shen*

Main category: cs.AI

TL;DR: 文章介绍用于评估大语言模型处理结构化电子健康记录数据的基准EHRStruct，评估20个模型，分析影响性能因素，提出达最优性能的EHRMaster方法。


<details>
  <summary>Details</summary>
Motivation: 缺乏标准化评估框架和明确任务，难以系统评估和比较大语言模型在结构化电子健康记录数据上的性能。

Method: 引入基准EHRStruct，定义11个代表性任务和2200个评估样本，评估20个大语言模型，分析影响性能的关键因素，与11种增强方法比较结果，提出代码增强方法EHRMaster。

Result: 许多结构化电子健康记录任务对大语言模型的理解和推理能力要求高，EHRMaster方法达到最优性能。

Conclusion: EHRStruct可有效评估大语言模型在结构化电子健康记录任务上的性能，EHRMaster方法具有实用性。

Abstract: Structured Electronic Health Record (EHR) data stores patient information in relational tables and plays a central role in clinical decision-making. Recent advances have explored the use of large language models (LLMs) to process such data, showing promise across various clinical tasks.However, the absence of standardized evaluation frameworks and clearly defined tasks makes it difficult to systematically assess and compare LLM performance on structured EHR data.To address these evaluation challenges, we introduce EHRStruct, a benchmark specifically designed to evaluate LLMs on structured EHR tasks.EHRStruct defines 11 representative tasks spanning diverse clinical needs and includes 2,200 task-specific evaluation samples derived from two widely used EHR datasets.We use EHRStruct to evaluate 20 advanced and representative LLMs, covering both general and medical models.We further analyze key factors influencing model performance, including input formats, few-shot generalisation, and finetuning strategies, and compare results with 11 state-of-the-art LLM-based enhancement methods for structured data reasoning. Our results indicate that many structured EHR tasks place high demands on the understanding and reasoning capabilities of LLMs.In response, we propose EHRMaster, a code-augmented method that achieves state-of-the-art performance and offers practical

</details>


### [54] [MADD: Multi-Agent Drug Discovery Orchestra](https://arxiv.org/abs/2511.08217)
*Gleb V. Solovev,Alina B. Zhidkovskaya,Anastasia Orlova,Nina Gubina,Anastasia Vepreva,Rodion Golovinskii,Ilya Tonkii,Ivan Dubrovsky,Ivan Gurev,Dmitry Gilemkhanov,Denis Chistiakov,Timur A. Aliev,Ivan Poddiakov,Galina Zubkova,Ekaterina V. Skorb,Vladimir Vinogradov,Alexander Boukhanovsky,Nikolay Nikitin,Andrei Dmitrenko,Anna Kalyuzhnaya,Andrey Savchenko*

Main category: cs.AI

TL;DR: 本文提出多智能体系统MADD，基于自然语言查询构建并执行定制化的命中物识别流程，在七个药物发现案例中表现出色，还应用于五个生物靶点并发布命中分子，引入新基准。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现中命中物识别需大量实验资源，虽AI虚拟筛选方法降低成本提高效率，但工具复杂性限制了湿实验室研究人员的使用，因此需解决可及性问题。

Method: 提出MADD多智能体系统，通过四个协调的智能体处理从头化合物生成和筛选中的关键子任务。

Result: 在七个药物发现案例中，MADD性能优于现有基于大语言模型的解决方案；将AI优先的药物设计应用于五个生物靶点并发布命中分子；引入新的查询 - 分子对和超三百万化合物对接分数的基准。

Conclusion: MADD系统为药物设计的智能时代做出贡献，提高了命中物识别效率和可及性。

Abstract: Hit identification is a central challenge in early drug discovery, traditionally requiring substantial experimental resources. Recent advances in artificial intelligence, particularly large language models (LLMs), have enabled virtual screening methods that reduce costs and improve efficiency. However, the growing complexity of these tools has limited their accessibility to wet-lab researchers. Multi-agent systems offer a promising solution by combining the interpretability of LLMs with the precision of specialized models and tools. In this work, we present MADD, a multi-agent system that builds and executes customized hit identification pipelines from natural language queries. MADD employs four coordinated agents to handle key subtasks in de novo compound generation and screening. We evaluate MADD across seven drug discovery cases and demonstrate its superior performance compared to existing LLM-based solutions. Using MADD, we pioneer the application of AI-first drug design to five biological targets and release the identified hit molecules. Finally, we introduce a new benchmark of query-molecule pairs and docking scores for over three million compounds to contribute to the agentic future of drug design.

</details>


### [55] [Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning](https://arxiv.org/abs/2511.08234)
*Zhihao Lin*

Main category: cs.AI

TL;DR: 提出Geometric Action Control (GAC)新范式简化计算，在多基准测试中表现优，揭示关键因素。


<details>
  <summary>Details</summary>
Motivation: 高斯策略在连续控制中有缺陷，vMF分布虽理论好但难实际应用，需更好方法。

Method: 将动作生成分解为方向向量和可学习的集中参数，减少参数和计算复杂度。

Result: 在六个MuJoCo基准测试中表现佳，如在Ant - v4上比SAC提升37.6%，4个任务获最佳结果。

Conclusion: 高效连续控制无需复杂分布，尊重动作空间几何特性即可。

Abstract: Gaussian policies have dominated continuous control in deep reinforcement learning (RL), yet they suffer from a fundamental mismatch: their unbounded support requires ad-hoc squashing functions that distort the geometry of bounded action spaces. While von Mises-Fisher (vMF) distributions offer a theoretically grounded alternative on the sphere, their reliance on Bessel functions and rejection sampling hinders practical adoption. We propose \textbf{Geometric Action Control (GAC)}, a novel action generation paradigm that preserves the geometric benefits of spherical distributions while \textit{simplifying computation}. GAC decomposes action generation into a direction vector and a learnable concentration parameter, enabling efficient interpolation between deterministic actions and uniform spherical noise. This design reduces parameter count from \(2d\) to \(d+1\), and avoids the \(O(dk)\) complexity of vMF rejection sampling, achieving simple \(O(d)\) operations. Empirically, GAC consistently matches or exceeds state-of-the-art methods across six MuJoCo benchmarks, achieving 37.6\% improvement over SAC on Ant-v4 and the best results on 4 out of 6 tasks. Our ablation studies reveal that both \textbf{spherical normalization} and \textbf{adaptive concentration control} are essential to GAC's success. These findings suggest that robust and efficient continuous control does not require complex distributions, but a principled respect for the geometry of action spaces. Code and pretrained models are available in supplementary materials.

</details>


### [56] [Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents](https://arxiv.org/abs/2511.08242)
*Waseem AlShikh,Muayad Sayed Ali,Brian Kennedy,Dmytro Mozolevskyi*

Main category: cs.AI

TL;DR: 提出AI代理性能评估新框架，含十一项指标，经实验验证其有效性，表明混合代理表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于基础设施的指标无法全面评估AI代理性能，需新评估框架。

Method: 提出十一项基于结果、与任务无关的性能指标，通过大规模模拟实验，涉及四种代理架构和五个领域。

Result: 不同代理设计存在性能权衡，混合代理在多数指标上表现最佳，平均目标完成率88.8%，投资回报率最高。

Conclusion: 为AI代理整体评估提供稳健、标准化方法，利于其开发、部署和治理。

Abstract: As AI agents proliferate across industries and applications, evaluating their performance based solely on infrastructural metrics such as latency, time-to-first-token, or token throughput is proving insufficient. These metrics fail to capture the quality of an agent's decisions, its operational autonomy, or its ultimate business value. This white paper proposes a novel, comprehensive framework of eleven outcome-based, task-agnostic performance metrics for AI agents that transcend domain boundaries. These metrics are designed to enable organizations to evaluate agents based on the quality of their decisions, their degree of autonomy, their adaptability to new challenges, and the tangible business value they deliver, regardless of the underlying model architecture or specific use case. We introduce metrics such as Goal Completion Rate (GCR), Autonomy Index (AIx), Multi-Step Task Resilience (MTR), and Business Impact Efficiency (BIE). Through a large-scale simulated experiment involving four distinct agent architectures (ReAct, Chain-of-Thought, Tool-Augmented, Hybrid) across five diverse domains (Healthcare, Finance, Marketing, Legal, and Customer Service), we demonstrate the framework's efficacy. Our results reveal significant performance trade-offs between different agent designs, highlighting the Hybrid Agent as the most consistently high-performing model across the majority of our proposed metrics, achieving an average Goal Completion Rate of 88.8\% and the highest Return on Investment (ROI). This work provides a robust, standardized methodology for the holistic evaluation of AI agents, paving the way for more effective development, deployment, and governance.

</details>


### [57] [Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning](https://arxiv.org/abs/2511.08246)
*Ziyu Ma,Chenhui Gou,Yiming Hu,Yong Wang,Xiangxiang Chu,Bohan Zhuang,Jianfei Cai*

Main category: cs.AI

TL;DR: 提出灵敏度感知任务向量插入框架（STV）解决大多模态模型多样本情境学习难题，实验证明其有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 大多模态模型在多样本情境学习中因上下文长度和推理成本问题受限，现有基于任务向量方法存在插入位置和值确定的不足。

Method: 提出STV框架，利用查询 - 上下文对激活增量的结构模式确定插入位置，构建预聚类激活库，用强化学习选择插入值。

Result: 在多种多模态模型和任务上评估，相比先前基于任务向量方法有一致改进。

Conclusion: STV框架有效且具有强泛化性，能解决大多模态模型多样本情境学习的挑战。

Abstract: Large Multimodal Models (LMMs) have shown promising in-context learning (ICL) capabilities, but scaling to many-shot settings remains difficult due to limited context length and high inference cost. To address these challenges, task-vector-based methods have been explored by inserting compact representations of many-shot in-context demonstrations into model activations. However, existing task-vector-based methods either overlook the importance of where to insert task vectors or struggle to determine suitable values for each location. To this end, we propose a novel Sensitivity-aware Task Vector insertion framework (STV) to figure out where and what to insert. Our key insight is that activation deltas across query-context pairs exhibit consistent structural patterns, providing a reliable cue for insertion. Based on the identified sensitive-aware locations, we construct a pre-clustered activation bank for each location by clustering the activation values, and then apply reinforcement learning to choose the most suitable one to insert. We evaluate STV across a range of multimodal models (e.g., Qwen-VL, Idefics-2) and tasks (e.g., VizWiz, OK-VQA), demonstrating its effectiveness and showing consistent improvements over previous task-vector-based methods with strong generalization.

</details>


### [58] [Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs](https://arxiv.org/abs/2511.08274)
*Anton Gusarov,Anastasia Volkova,Valentin Khrulkov,Andrey Kuznetsov,Evgenii Maslov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 提出Multi - Agent GraphRAG系统用于文本到Cypher查询生成，在多个数据集上评估，展示其在工业数字自动化的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG研究对Cypher和LPG数据库在GraphRAG管道中的应用探索不足，需填补此空白。

Method: 提出Multi - Agent GraphRAG模块化大语言模型代理系统，采用基于大语言模型的工作流自动生成和执行Cypher查询，通过迭代内容感知校正和归一化以及聚合反馈循环优化查询。

Result: 在CypherBench图数据集和IFC数据派生的属性图上进行评估和展示。

Conclusion: 该方法能将人工智能与现实世界应用大规模连接，支持工业数字自动化用例。

Abstract: While Retrieval-Augmented Generation (RAG) methods commonly draw information from unstructured documents, the emerging paradigm of GraphRAG aims to leverage structured data such as knowledge graphs. Most existing GraphRAG efforts focus on Resource Description Framework (RDF) knowledge graphs, relying on triple representations and SPARQL queries. However, the potential of Cypher and Labeled Property Graph (LPG) databases to serve as scalable and effective reasoning engines within GraphRAG pipelines remains underexplored in current research literature. To fill this gap, we propose Multi-Agent GraphRAG, a modular LLM agentic system for text-to-Cypher query generation serving as a natural language interface to LPG-based graph data. Our proof-of-concept system features an LLM-based workflow for automated Cypher queries generation and execution, using Memgraph as the graph database backend. Iterative content-aware correction and normalization, reinforced by an aggregated feedback loop, ensures both semantic and syntactic refinement of generated queries. We evaluate our system on the CypherBench graph dataset covering several general domains with diverse types of queries. In addition, we demonstrate performance of the proposed workflow on a property graph derived from the IFC (Industry Foundation Classes) data, representing a digital twin of a building. This highlights how such an approach can bridge AI with real-world applications at scale, enabling industrial digital automation use cases.

</details>


### [59] [DiagramIR: An Automatic Pipeline for Educational Math Diagram Evaluation](https://arxiv.org/abs/2511.08283)
*Vishal Kumar,Shubhra Mishra,Rebecca Hao,Rizwaan Malik,David Broman,Dorottya Demszky*

Main category: cs.AI

TL;DR: 提出DiagramIR用于几何图形自动可扩展评估，比其他基线与人类评估更一致，还能让小模型以低成本达到大模型效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型学习工具多为纯文本，在需可视化领域受限，且代码生成教育图形的评估存在可扩展性瓶颈。

Method: 提出基于LaTeX TikZ代码中间表示（IRs）的DiagramIR评估管道。

Result: 该方法与人类评估者的一致性更高，小模型GPT - 4.1 - Mini能以低10倍推理成本达到GPT - 5的性能。

Conclusion: 此评估方法对部署可访问和可扩展的教育技术很重要。

Abstract: Large Language Models (LLMs) are increasingly being adopted as tools for learning; however, most tools remain text-only, limiting their usefulness for domains where visualizations are essential, such as mathematics. Recent work shows that LLMs are capable of generating code that compiles to educational figures, but a major bottleneck remains: scalable evaluation of these diagrams. We address this by proposing DiagramIR: an automatic and scalable evaluation pipeline for geometric figures. Our method relies on intermediate representations (IRs) of LaTeX TikZ code. We compare our pipeline to other evaluation baselines such as LLM-as-a-Judge, showing that our approach has higher agreement with human raters. This evaluation approach also enables smaller models like GPT-4.1-Mini to perform comparably to larger models such as GPT-5 at a 10x lower inference cost, which is important for deploying accessible and scalable education technologies.

</details>


### [60] [JobSphere: An AI-Powered Multilingual Career Copilot for Government Employment Platforms](https://arxiv.org/abs/2511.08343)
*Srihari R,Adarsha B,Mohammed Usman Hussain,Shweta Singh*

Main category: cs.AI

TL;DR: 本文介绍AI职业助手JobSphere，阐述其架构、技术及特点，评估显示表现良好，能填补特定用户群体的就业平台可访问性差距。


<details>
  <summary>Details</summary>
Motivation: 解决政府就业网站存在的用户参与度和可访问性问题，如导航复杂、语言选项少和缺乏个性化支持。

Method: 采用Retrieval - Augmented Generation (RAG)架构，运用4位量化技术，具备语音交互、自动模拟测试、简历解析与技能识别、基于嵌入的工作推荐等功能。

Result: 实现89%成本降低，达到94%事实准确率、1.8秒中位响应时间，系统可用性量表得分78.5/100，相比基线平台提升50%，工作推荐精度@10为68%。

Conclusion: JobSphere有效填补了旁遮普/印地语农村用户的可访问性差距，确保用户获取政府机构提供的可靠工作内容。

Abstract: Users of government employment websites commonly face engagement and accessibility challenges linked to navigational complexity, a dearth of language options, and a lack of personalized support. This paper introduces JobSphere, an AI-powered career assistant that is redefining the employment platform in Punjab called PGRKAM. JobSphere employs Retrieval-Augmented Generation (RAG) architecture, and it is multilingual, available in English, Hindi and Punjabi. JobSphere technique uses 4-bit quantization, allowing the platform to deploy on consumer-grade GPUs (i.e., NVIDIA RTX 3050 4GB), making the implementation 89% cheaper than that of cloud-based systems. Key innovations include voice-enabled interaction with the assistant, automated mock tests, resume parsing with skills recognition, and embed-based job recommendation that achieves a precision@10 score of 68%. An evaluation of JobSphere's implementation reveals 94% factual accuracy, a median response time of 1.8 seconds, and a System Usability Scale score of 78.5/100, a 50% improvement compared to the baseline PGRKAM platform context. In conclusion, JobSphere effectively fills significant accessibility gaps for Punjab/Hindi-speaking users in rural locations, while also affirming the users access to trusted job content provided by government agencies.

</details>


### [61] [AI-Powered Data Visualization Platform: An Intelligent Web Application for Automated Dataset Analysis](https://arxiv.org/abs/2511.08363)
*Srihari R,Pallavi M,Tejaswini S,Vaishnavi R C*

Main category: cs.AI

TL;DR: 介绍了一个由AI驱动的数据可视化平台，可自动完成数据分析全流程，经评估能减少手动输入并保证输出质量和用户体验。


<details>
  <summary>Details</summary>
Motivation: 消除耗时的手动数据分析挑战，实现数据驱动环境下基于AI的自动化分析和可视化。

Method: 采用先进机器学习算法进行数据清洗、预处理和特征分析，自动选择可视化方式；结合Python Flask后端和React前端，与Firebase Cloud Storage交互。

Result: 使用两个数据集评估平台性能，能实时分析100000行数据集，云平台可满足多用户请求并同时处理。

Conclusion: 云数据可视化应用可大幅减少数据分析过程中的手动输入，同时保证高质量、有影响力的可视化输出和用户体验。

Abstract: An AI-powered data visualization platform that automates the entire data analysis process, from uploading a dataset to generating an interactive visualization. Advanced machine learning algorithms are employed to clean and preprocess the data, analyse its features, and automatically select appropriate visualizations. The system establishes the process of automating AI-based analysis and visualization from the context of data-driven environments, and eliminates the challenge of time-consuming manual data analysis. The combination of a Python Flask backend to access the dataset, paired with a React frontend, provides a robust platform that automatically interacts with Firebase Cloud Storage for numerous data processing and data analysis solutions and real-time sources. Key contributions include automatic and intelligent data cleaning, with imputation for missing values, and detection of outliers, via analysis of the data set. AI solutions to intelligently select features, using four different algorithms, and intelligent title generation and visualization are determined by the attributes of the dataset. These contributions were evaluated using two separate datasets to assess the platform's performance. In the process evaluation, the initial analysis was performed in real-time on datasets as large as 100000 rows, while the cloud-based demand platform scales to meet requests from multiple users and processes them simultaneously. In conclusion, the cloud-based data visualization application allowed for a significant reduction of manual inputs to the data analysis process while maintaining a high quality, impactful visual outputs, and user experiences

</details>


### [62] [SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models](https://arxiv.org/abs/2511.08379)
*Giorgio Piras,Raffaele Mura,Fabio Brau,Luca Oneto,Fabio Roli,Battista Biggio*

Main category: cs.AI

TL;DR: 本文提出用自组织映射（SOMs）提取语言模型中多个拒绝方向的方法，实验表明消融多个方向效果更好，并分析了方法的机制含义。


<details>
  <summary>Details</summary>
Motivation: 以往将拒绝行为编码为模型潜空间单一方向，而概念常以低维流形形式编码，故提出新方法提取多个拒绝方向。

Method: 证明SOMs推广了先前的均值差技术，在有害提示表示上训练SOMs识别多个神经元，用每个神经元减去无害表示的质心得到多个拒绝方向。

Result: 在实验中，消融多个方向的方法优于单一方向基线和专门越狱算法，能有效抑制拒绝。

Conclusion: 分析了该方法的机制含义。

Abstract: Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach.

</details>


### [63] [FaithAct: Faithfulness Planning and Acting in MLLMs](https://arxiv.org/abs/2511.08409)
*Junxian Li,Xinyue Xu,Sai Ma,Sichao Li*

Main category: cs.AI

TL;DR: 本文针对大语言模型的不忠实问题，提出FaithEval评估方法和FaithAct框架，实验表明FaithAct能提升感知忠实度且不降低任务准确率，建立了多模态推理中评估和强化忠实度的统一框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在不忠实问题，常产生看似合理但无依据的推理链，与感知证据或最终结论不一致。

Method: 区分行为忠实度和感知忠实度，引入FaithEval评估忠实度，提出FaithAct框架在每个推理步骤强制证据基础。

Result: 在多个推理基准测试中，与基于提示和工具增强的基线相比，FaithAct在不降低任务准确率的情况下，将感知忠实度提高了26%。

Conclusion: 将忠实度作为指导原则不仅能减少幻觉，还能带来更稳定的推理轨迹，建立了评估和强化多模态推理中忠实度的统一框架。

Abstract: Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.

</details>


### [64] [Dataset Safety in Autonomous Driving: Requirements, Risks, and Assurance](https://arxiv.org/abs/2511.08439)
*Alireza Abbaspour,Tejaskumar Balgonda Patil,B Ravi Kiran,Russel Mohr,Senthil Yogamani*

Main category: cs.AI

TL;DR: 本文提出符合ISO/PAS 8800指南的安全数据集开发框架，涵盖数据全生命周期，结合安全分析，定义相关流程和策略，还回顾研究趋势，推动自动驾驶AI系统发展。


<details>
  <summary>Details</summary>
Motivation: 数据集完整性对自动驾驶AI系统安全可靠至关重要，需开发安全数据集框架。

Method: 提出AI数据飞轮和数据集生命周期，进行严格安全分析，定义安全要求流程，提出验证和确认策略。

Result: 构建了结构化框架，涵盖数据收集、标注、管理和维护等方面，识别并缓解数据集不足带来的风险。

Conclusion: 该框架有助于推进用于自动驾驶应用的强大、有安全保障的AI系统。

Abstract: Dataset integrity is fundamental to the safety and reliability of AI systems, especially in autonomous driving. This paper presents a structured framework for developing safe datasets aligned with ISO/PAS 8800 guidelines. Using AI-based perception systems as the primary use case, it introduces the AI Data Flywheel and the dataset lifecycle, covering data collection, annotation, curation, and maintenance. The framework incorporates rigorous safety analyses to identify hazards and mitigate risks caused by dataset insufficiencies. It also defines processes for establishing dataset safety requirements and proposes verification and validation strategies to ensure compliance with safety standards. In addition to outlining best practices, the paper reviews recent research and emerging trends in dataset safety and autonomous vehicle development, providing insights into current challenges and future directions. By integrating these perspectives, the paper aims to advance robust, safety-assured AI systems for autonomous driving applications.

</details>


### [65] [Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models](https://arxiv.org/abs/2511.08484)
*Huzaifa Arif,Keerthiram Murugesan,Ching-Yun Ko,Pin-Yu Chen,Payel Das,Alex Gittens*

Main category: cs.AI

TL;DR: 提出像软件打补丁一样为大语言模型解决安全漏洞，方法轻量有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型主要版本发布成本高、不频繁且难满足客户需求，已发布模型存在安全漏洞。

Method: 在现有模型前添加紧凑、可学习的前缀，引入极少额外参数，引导模型行为。

Result: 在三个关键领域实现安全改进，效果与下一代安全对齐模型相当，且保留流畅性。

Conclusion: 大语言模型可像软件一样打补丁，为供应商和从业者提供实用安全更新机制。

Abstract: We propose patching for large language models (LLMs) like software versions, a lightweight and modular approach for addressing safety vulnerabilities. While vendors release improved LLM versions, major releases are costly, infrequent, and difficult to tailor to customer needs, leaving released models with known safety gaps. Unlike full-model fine-tuning or major version updates, our method enables rapid remediation by prepending a compact, learnable prefix to an existing model. This "patch" introduces only 0.003% additional parameters, yet reliably steers model behavior toward that of a safer reference model. Across three critical domains (toxicity mitigation, bias reduction, and harmfulness refusal) policy patches achieve safety improvements comparable to next-generation safety-aligned models while preserving fluency. Our results demonstrate that LLMs can be "patched" much like software, offering vendors and practitioners a practical mechanism for distributing scalable, efficient, and composable safety updates between major model releases.

</details>


### [66] [A Matter of Interest: Understanding Interestingness of Math Problems in Humans and Language Models](https://arxiv.org/abs/2511.08548)
*Shubhra Mishra,Yuka Machino,Gabriel Poesia,Albert Jiang,Joy Hsu,Adrian Weller,Challenger Mishra,David Broman,Joshua B. Tenenbaum,Mateja Jamnik,Cedegao E. Zhang,Katherine M. Collins*

Main category: cs.AI

TL;DR: 研究人类与大语言模型（LLMs）对数学问题趣味性和难度判断的一致性，发现LLMs有一定共识但未完全捕捉人类判断分布和理由。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统更多参与数学领域，需了解其判断与人类判断的一致性。

Method: 通过两项实证研究，对众包平台参与者和国际数学奥林匹克竞赛选手两组人群进行研究。

Result: 许多LLMs与人类对趣味性有大致共识，但未捕捉人类判断分布，且与人类认为某些数学问题有趣的理由相关性弱。

Conclusion: 指出当前LLMs在数学AI思维伙伴关系中捕捉人类趣味性判断的前景与局限。

Abstract: The evolution of mathematics has been guided in part by interestingness. From researchers choosing which problems to tackle next, to students deciding which ones to engage with, people's choices are often guided by judgments about how interesting or challenging problems are likely to be. As AI systems, such as LLMs, increasingly participate in mathematics with people -- whether for advanced research or education -- it becomes important to understand how well their judgments align with human ones. Our work examines this alignment through two empirical studies of human and LLM assessment of mathematical interestingness and difficulty, spanning a range of mathematical experience. We study two groups: participants from a crowdsourcing platform and International Math Olympiad competitors. We show that while many LLMs appear to broadly agree with human notions of interestingness, they mostly do not capture the distribution observed in human judgments. Moreover, most LLMs only somewhat align with why humans find certain math problems interesting, showing weak correlation with human-selected interestingness rationales. Together, our findings highlight both the promises and limitations of current LLMs in capturing human interestingness judgments for mathematical AI thought partnerships.

</details>


### [67] [Hyperdimensional Decoding of Spiking Neural Networks](https://arxiv.org/abs/2511.08558)
*Cedrick Kinavuidi,Luca Peres,Oliver Rhodes*

Main category: cs.AI

TL;DR: 提出结合SNN与HDC的新型SNN解码方法，在多方面表现优且能识别未知类。


<details>
  <summary>Details</summary>
Motivation: 创建高精度、高抗噪性、低延迟和低能耗的解码方法。

Method: 将SNN与HDC相结合的新型SNN解码方法。

Result: 在多个测试用例上分类准确率更高、延迟更低、能耗更低，在DvsGesture和SL - Animals - DVS数据集上有能耗降低，能高效识别未知类。

Conclusion: 该解码方法是速率和解码延迟方法的有力替代方案。

Abstract: This work presents a novel spiking neural network (SNN) decoding method, combining SNNs with Hyperdimensional computing (HDC). The goal is to create a decoding method with high accuracy, high noise robustness, low latency and low energy usage. Compared to analogous architectures decoded with existing approaches, the presented SNN-HDC model attains generally better classification accuracy, lower classification latency and lower estimated energy consumption on multiple test cases from literature. The SNN-HDC achieved estimated energy consumption reductions ranging from 1.24x to 3.67x on the DvsGesture dataset and from 1.38x to 2.27x on the SL-Animals-DVS dataset. The presented decoding method can also efficiently identify unknown classes it has not been trained on. In the DvsGesture dataset the SNN-HDC model can identify 100% of samples from an unseen/untrained class. Given the numerous benefits shown and discussed in this paper, this decoding method represents a very compelling alternative to both rate and latency decoding.

</details>


### [68] [DeepProofLog: Efficient Proving in Deep Stochastic Logic Programs](https://arxiv.org/abs/2511.08581)
*Ying Jiao,Rodrigo Castellano Ontiveros,Luc De Raedt,Marco Gori,Francesco Giannini,Michelangelo Diligenti,Giuseppe Marra*

Main category: cs.AI

TL;DR: 本文提出基于随机逻辑程序的NeSy系统DeepProofLog (DPrL)，解决此前方法可扩展性限制问题，实验表明其优于现有NeSy系统。


<details>
  <summary>Details</summary>
Motivation: 现有NeSy模型在保证准确性、可解释性和泛化能力时存在可扩展性降低的问题，限制了其可用性。

Method: 用神经网络对所有推导步骤进行参数化，在证明系统上实现高效的神经引导；建立深度随机逻辑程序的归结过程与马尔可夫决策过程的形式映射，应用动态规划和强化学习技术进行推理和学习。

Result: 在标准NeSy基准测试和知识图谱推理任务的实验中，DPrL优于现有的NeSy系统。

Conclusion: DPrL解决了先前方法的可扩展性限制，能在更大、更复杂的环境中应用。

Abstract: Neurosymbolic (NeSy) AI aims to combine the strengths of neural architectures and symbolic reasoning to improve the accuracy, interpretability, and generalization capability of AI models. While logic inference on top of subsymbolic modules has been shown to effectively guarantee these properties, this often comes at the cost of reduced scalability, which can severely limit the usability of NeSy models. This paper introduces DeepProofLog (DPrL), a novel NeSy system based on stochastic logic programs, which addresses the scalability limitations of previous methods. DPrL parameterizes all derivation steps with neural networks, allowing efficient neural guidance over the proving system. Additionally, we establish a formal mapping between the resolution process of our deep stochastic logic programs and Markov Decision Processes, enabling the application of dynamic programming and reinforcement learning techniques for efficient inference and learning. This theoretical connection improves scalability for complex proof spaces and large knowledge bases. Our experiments on standard NeSy benchmarks and knowledge graph reasoning tasks demonstrate that DPrL outperforms existing state-of-the-art NeSy systems, advancing scalability to larger and more complex settings than previously possible.

</details>


### [69] [Simulating the Visual World with Artificial Intelligence: A Roadmap](https://arxiv.org/abs/2511.08585)
*Jingtong Yue,Ziqi Huang,Zhaoxi Chen,Xintao Wang,Pengfei Wan,Ziwei Liu*

Main category: cs.AI

TL;DR: 本文对视频生成的发展进行系统综述，将现代视频基础模型概念化为隐式世界模型和视频渲染器的组合，追溯视频生成四代发展，阐述各代特点、代表作品和应用领域，还讨论了下一代世界模型的挑战和设计原则。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成从关注视觉效果转向构建支持交互和物理合理性的虚拟环境，需要对这一演变进行系统梳理，以理解视频基础模型的发展。

Method: 将现代视频基础模型概念化为隐式世界模型和视频渲染器的组合，追溯视频生成的四代发展，定义各代核心特征，分析代表作品和应用领域。

Result: 明确了视频生成的四代发展，各代核心能力逐步提升，最终形成具有物理合理性、实时多模态交互和多时空尺度规划能力的世界模型。

Conclusion: 探讨了下一代世界模型面临的开放挑战和设计原则，强调了智能体在塑造和评估这些系统中的作用。

Abstract: The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a "window" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [70] [Accelerated, Memory-Efficient Far-Field Scattering Computation with Monte Carlo SBR](https://arxiv.org/abs/2511.07586)
*Samuel Audia,Dinesh Manocha,Matthias Zwicker*

Main category: cs.CE

TL;DR: 提出基于蒙特卡罗积分的SBR算法用于电磁散射，尤其针对复杂介质材料，实现高效并行计算，减少内存使用、加快运行速度且高精度低噪声。


<details>
  <summary>Details</summary>
Motivation: 传统确定性SBR方法有局限，需要针对复杂介质材料的高效算法。

Method: 用蒙特卡罗技术和先进方差缩减策略改写SBR积分方程，在现代GPU上进行高效并行计算，强调高能传播路径。

Result: 内存使用最多减少10 - 15倍，运行时间加快4倍，在典型3D几何和ISAR成像验证中高精度低噪声。

Conclusion: 该蒙特卡罗SBR算法适用于下游成像和分析任务。

Abstract: We introduce a Monte Carlo integration-based Shooting and Bouncing Ray (SBR) algorithm for electromagnetic scattering, specifically targeting complex dielectric materials. Unlike traditional deterministic SBR methods, our approach is the first to reformulate the SBR integral equations using Monte Carlo techniques and advanced variance reduction strategies adapted from photorealistic rendering. This enables efficient, massively parallel computation on modern GPUs, resulting in up to a 10-15x reduction in memory usage and a 4x speed up in runtime, particularly for multilayer dielectric structures. Our method emphasizes high-energy propagation paths, efficiently capturing long multipath and interreflection effects. Verification on canonical 3D geometries and ISAR imaging of both conducting and dielectric representative aircraft models demonstrates that our Monte Carlo SBR achieves high accuracy while maintaining low noise, making it suitable for downstream imaging and analysis tasks.

</details>


### [71] [CometNet: Contextual Motif-guided Long-term Time Series Forecasting](https://arxiv.org/abs/2511.08049)
*Weixu Wang,Xiaobo Zhou,Xin Qiao,Lei Wang,Tie Qiu*

Main category: cs.CE

TL;DR: 提出CometNet框架解决长时序预测中因有限回溯窗口导致的问题，实验表明其优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有长时序预测模型受限于有限回溯窗口，难以捕捉长期依赖，导致预测性能不佳。

Method: 提出CometNet框架，包含上下文主题提取模块和主题引导预测模块，利用主题信息增强长期预测能力。

Result: 在八个真实数据集上的实验表明CometNet显著优于当前SOTA方法，在长预测区间表现更佳。

Conclusion: CometNet能有效解决现有长时序预测模型的问题，提升预测性能。

Abstract: Long-term Time Series Forecasting is crucial across numerous critical domains, yet its accuracy remains fundamentally constrained by the receptive field bottleneck in existing models. Mainstream Transformer- and Multi-layer Perceptron (MLP)-based methods mainly rely on finite look-back windows, limiting their ability to model long-term dependencies and hurting forecasting performance. Naively extending the look-back window proves ineffective, as it not only introduces prohibitive computational complexity, but also drowns vital long-term dependencies in historical noise. To address these challenges, we propose CometNet, a novel Contextual Motif-guided Long-term Time Series Forecasting framework. CometNet first introduces a Contextual Motif Extraction module that identifies recurrent, dominant contextual motifs from complex historical sequences, providing extensive temporal dependencies far exceeding limited look-back windows; Subsequently, a Motif-guided Forecasting module is proposed, which integrates the extracted dominant motifs into forecasting. By dynamically mapping the look-back window to its relevant motifs, CometNet effectively harnesses their contextual information to strengthen long-term forecasting capability. Extensive experimental results on eight real-world datasets have demonstrated that CometNet significantly outperforms current state-of-the-art (SOTA) methods, particularly on extended forecast horizons.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [72] [Cortex AISQL: A Production SQL Engine for Unstructured Data](https://arxiv.org/abs/2511.07663)
*Paritosh Aggarwal,Bowei Chen,Anupam Datta,Benjamin Han,Boxin Jiang,Nitish Jindal,Zihan Li,Aaron Lin,Pawel Liskowski,Jay Tayade,Dimitris Tsirogiannis,Nathan Wiegand,Weicheng Zhao*

Main category: cs.DB

TL;DR: Snowflake的Cortex AISQL将语义操作集成到SQL，解决生产规模下语义操作效率挑战，采用三种新技术并已投入生产。


<details>
  <summary>Details</summary>
Motivation: 解决生产规模下语义操作效率低、现有查询引擎无法优化语义操作等问题。

Method: 采用AI感知查询优化、自适应模型级联、语义连接查询重写三种新技术。

Result: 三种技术分别实现2 - 8倍、2 - 6倍、15 - 70倍的加速，且部分能保证质量。

Conclusion: AISQL已在Snowflake投入生产，支持多种客户工作负载。

Abstract: Snowflake's Cortex AISQL is a production SQL engine that integrates native semantic operations directly into SQL. This integration allows users to write declarative queries that combine relational operations with semantic reasoning, enabling them to query both structured and unstructured data effortlessly. However, making semantic operations efficient at production scale poses fundamental challenges. Semantic operations are more expensive than traditional SQL operations, possess distinct latency and throughput characteristics, and their cost and selectivity are unknown during query compilation. Furthermore, existing query engines are not designed to optimize semantic operations. The AISQL query execution engine addresses these challenges through three novel techniques informed by production deployment data from Snowflake customers. First, AI-aware query optimization treats AI inference cost as a first-class optimization objective, reasoning about large language model (LLM) cost directly during query planning to achieve 2-8$\times$ speedups. Second, adaptive model cascades reduce inference costs by routing most rows through a fast proxy model while escalating uncertain cases to a powerful oracle model, achieving 2-6$\times$ speedups while maintaining 90-95% of oracle model quality. Third, semantic join query rewriting lowers the quadratic time complexity of join operations to linear through reformulation as multi-label classification tasks, achieving 15-70$\times$ speedups with often improved prediction quality. AISQL is deployed in production at Snowflake, where it powers diverse customer workloads across analytics, search, and content understanding.

</details>


### [73] [ACGraph: An Efficient Asynchronous Out-of-Core Graph Processing Framework](https://arxiv.org/abs/2511.07886)
*Dechuang Chen,Sibo Wang,Qintian Guo*

Main category: cs.DB

TL;DR: 提出新的异步图处理系统ACGraph，优化SSD环境下内存受限的图处理，性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有图处理系统同步执行开销大，调度机制导致I/O效率低和同步延迟。

Method: 采用动态、以块为中心的优先级调度器和在线异步工作列表，将异步I/O与计算统一在流水线执行模型中，使用优化混合存储格式。

Result: 在ACGraph上实现流行图算法，在运行时间和I/O效率上大幅优于现有系统。

Conclusion: ACGraph能有效克服现有系统局限，提升图处理性能。

Abstract: Graphs are a ubiquitous data structure in diverse domains such as machine learning, social networks, and data mining. As real-world graphs continue to grow beyond the memory capacity of single machines, out-of-core graph processing systems have emerged as a viable solution. Yet, existing systems that rely on strictly synchronous, iteration-by-iteration execution incur significant overheads. In particular, their scheduling mechanisms lead to I/O inefficiencies, stemming from read and work amplification, and induce costly synchronization stalls hindering sustained disk utilization. To overcome these limitations, we present {\em ACGraph}, a novel asynchronous graph processing system optimized for SSD-based environments with constrained memory resources. ACGraph employs a dynamic, block-centric priority scheduler that adjusts in real time based on workload, along with an online asynchronous worklist that minimizes redundant disk accesses by efficiently reusing active blocks in memory. Moreover, ACGraph unifies asynchronous I/O with computation in a pipelined execution model that maintains sustained I/O activation, and leverages a highly optimized hybrid storage format to expedite access to low-degree vertices. We implement popular graph algorithms, such as Breadth-First Search (BFS), Weakly Connected Components (WCC), personalized PageRank (PPR), PageRank (PR), and $k$-core on ACGraph and demonstrate that ACGraph substantially outperforms state-of-the-art out-of-core graph processing systems in both runtime and I/O efficiency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [74] [Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms](https://arxiv.org/abs/2511.07421)
*Tong Qiao,Ao Zhou,Yingjie Qi,Yiou Wang,Han Wan,Jianlei Yang,Chunming Hu*

Main category: cs.DC

TL;DR: 本文提出A3GNN框架用于异构CPU - GPU平台上的GNN训练，通过新方法提升资源利用率，实验显示其能缩小性能差距。


<details>
  <summary>Details</summary>
Motivation: GNN训练依赖昂贵高性能计算平台，限制了许多任务的可访问性，需要在资源受限设备上提升效率。

Method: 引入A3GNN框架，通过 locality - aware采样和细粒度并行调度提高资源利用率，利用强化学习探索设计空间以实现吞吐量、内存占用和准确性的帕累托最优权衡。

Result: A3GNN能缩小性能差距，七块Nvidia 2080Ti GPU在吞吐量上比两块A100 GPU最高高出1.8倍，且精度损失极小。

Conclusion: A3GNN框架在异构CPU - GPU平台上训练GNN是有效可行的，可提升资源受限设备上的GNN训练效率。

Abstract: Graph Neural Networks (GNNs) have been widely adopted due to their strong performance. However, GNN training often relies on expensive, high-performance computing platforms, limiting accessibility for many tasks. Profiling of representative GNN workloads indicates that substantial efficiency gains are possible on resource-constrained devices by fully exploiting available resources. This paper introduces A3GNN, a framework for affordable, adaptive, and automatic GNN training on heterogeneous CPU-GPU platforms. It improves resource usage through locality-aware sampling and fine-grained parallelism scheduling. Moreover, it leverages reinforcement learning to explore the design space and achieve pareto-optimal trade-offs among throughput, memory footprint, and accuracy. Experiments show that A3GNN can bridge the performance gap, allowing seven Nvidia 2080Ti GPUs to outperform two A100 GPUs by up to 1.8X in throughput with minimal accuracy loss.

</details>


### [75] [From Attention to Disaggregation: Tracing the Evolution of LLM Inference](https://arxiv.org/abs/2511.07422)
*Madabattula Rajesh Kumar,Srinivasa Rao Aravilli,Mustafa Saify,Shashank Srivastava*

Main category: cs.DC

TL;DR: 大语言模型发展使瓶颈转向实时推理，本文探讨采用解耦推理架构克服传统集群局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署面临内存带宽、计算吞吐量和延迟要求等挑战，需解决多目标优化问题。

Method: 采用解耦推理架构，将计算密集的预填充阶段和内存密集的解码阶段解耦为可独立扩展的组件。

Result: 该范式缓解资源争用，可独立优化关键指标。

Conclusion: 向解耦推理的架构转变有助于克服传统单体GPU集群的局限。

Abstract: The evolution of Large Language Models from the Transformer architecture to models with trillions of parameters has shifted the primary bottleneck from model training to real time inference. Deploying these massive models is a complex distributed systems challenge constrained by memory bandwidth, computational throughput, and latency requirements. LLM inference fundamentally requires solving a multi objective optimization problem to minimize latency, maximize throughput, and reduce cost. This paper explores the necessary architectural shift towards disaggregated inference, which applies distributed systems principles such as service decomposition, resource disaggregation, and workload partitioning to overcome the limitations of traditional monolithic GPU clusters. By decoupling the compute intensive prefill phase from the memory intensive decode phase into independently scalable components, this paradigm mitigates resource contention and enables independent optimization of key metrics like Time to First Token and Inter Token Latency.

</details>


### [76] [Synera: Synergistic LLM Serving across Device and Cloud at Scale](https://arxiv.org/abs/2511.07423)
*Genglin Wang,Liekang Zeng,Bufang Yang,Kaiwei Liu,Guoliang Xing,Chumin Sun,Li Zhou,Jie Sun,Zhenyu Yan*

Main category: cs.DC

TL;DR: 论文提出Synera设备 - 云协同LLM服务系统，解决LLM在移动操作系统部署的性能挑战，评估显示其在生成质量、成本等方面有优势。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在移动操作系统部署存在生成质量下降和延迟长的问题，云卸载受通信瓶颈限制，设备端小语言模型因资源约束牺牲质量。

Method: 提出Synera系统，应用高效SLM - LLM协同机制，针对设备 - 云协同LLM推理的优化机会，设计通信高效的选择性卸载、无停顿并行推理和可扩展云批处理。

Result: 与竞争基线相比，生成质量提升1.20 - 5.47倍，延迟相当；与现有云服务相比，在各基准测试中云服务成本降低8.2 - 16.5%。

Conclusion: Synera系统能有效解决LLM在移动操作系统部署的性能问题，提升生成质量并降低成本。

Abstract: Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.

</details>


### [77] [Enhancing reliability in AI inference services: An empirical study on real production incidents](https://arxiv.org/abs/2511.07424)
*Bhala Ranganathan,Mickey Zhang,Kai Wu*

Main category: cs.DC

TL;DR: 对大语言模型推理事件进行基于实践的分析，提出分类法和方法，识别故障模式与缓解手段，给出采用清单，推动大规模可靠高效服务。


<details>
  <summary>Details</summary>
Motivation: 超大规模大语言模型推理对云系统要求高，故障影响大，需理解并降低风险。

Method: 基于一年运营经验开发分类法和方法论，对156个高严重度事件验证，对2025年4 - 6月进行定量研究。

Result: 实现高标注一致性，识别主导故障模式和缓解手段，展示分类法指导策略效果。

Conclusion: 基于经验的推理操作系统分析能推动大规模大语言模型可靠且经济高效的服务。

Abstract: Hyperscale large language model (LLM) inference places extraordinary demands on cloud systems, where even brief failures can translate into significant user and business impact. To better understand and mitigate these risks, we present one of the first provider-internal, practice-based analysis of LLM inference incidents. We developed a taxonomy and methodology grounded in a year of operational experience, validating it on 156 high-severity incidents, and conducted a focused quantitative study of Apr-Jun 2025 to ensure recency and relevance. Our approach achieves high labeling consistency (Cohen's K ~0.89), identifies dominant failure modes (in our dataset ~60% inference engine failures, within that category ~40% timeouts), and surfaces mitigation levers (~74% auto-detected; ~28% required hotfix). Beyond hotfixes, many incidents were mitigated via traffic routing, node rebalancing, or capacity increase policies, indicating further automation opportunities. We also show how the taxonomy guided targeted strategies such as connection liveness, GPU capacity-aware routing, and per-endpoint isolation and reduced incident impact and accelerated recovery. Finally, we contribute a practitioner-oriented adoption checklist that enables others to replicate our taxonomy, analysis, and automation opportunities in their own systems. This study demonstrates how systematic, empirically grounded analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.

</details>


### [78] [An Evaluation of LLMs Inference on Popular Single-board Computers](https://arxiv.org/abs/2511.07425)
*Tung,Nguyen,Tuyen Nguyen*

Main category: cs.DC

TL;DR: 本文对25个量化开源大语言模型在三款单板计算机上进行性能基准测试，比较两种推理运行时，给出结果并提供部署建议，填补了大语言模型在单板计算机推理评估的空白。


<details>
  <summary>Details</summary>
Motivation: 设备端大语言模型推理需求增长，单板计算机是有前景的平台，但在大语言模型工作负载方面研究不足。

Method: 使用Ollama和Llamafile两种推理运行时，对25个量化开源大语言模型在三款单板计算机上进行基准测试，在不同CPU配置下评估生成吞吐量、内存使用和功耗。

Result: 单板计算机可可靠支持多达1.5B参数的模型，Llamafile比Ollama吞吐量高4倍，功耗低30 - 40%。

Conclusion: 本研究首次对大语言模型在单板计算机上的推理进行广泛评估，弥合了高性能语言模型和经济实惠的边缘计算之间的差距。

Abstract: The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.

</details>


### [79] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: 本文对MCP与大语言模型交互进行测量分析，揭示能力、性能和成本间的权衡，并提出优化建议。


<details>
  <summary>Details</summary>
Motivation: MCP使大语言模型与外部工具交互时，大量上下文信息会增加token使用，导致成本上升和计算负载增加，因此需要研究权衡与优化。

Method: 对MCP与大语言模型的交互进行基于测量的全面分析，探究不同模型和配置对关键性能指标的影响。

Result: 揭示了能力、性能和成本之间的权衡，提出了包括并行工具调用和任务中止机制等潜在优化建议。

Conclusion: 研究结果为开发更高效、健壮和经济的MCP工作流提供了有用见解。

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>


### [80] [DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones](https://arxiv.org/abs/2511.07427)
*Tuowei Wang,Minxing Huang,Fengzu Li,Ligeng Chen,Jinrui Zhang,Ju Ren*

Main category: cs.DC

TL;DR: 现有长序列大语言模型在手机上解码受KVCache限制，提出DynaKV方法，评估显示其能提升检索准确性并降低端到端延迟，且有更广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 随着对长序列解码需求增长，长序列大语言模型在手机上解码受KVCache限制，现有检索方法有不足且受手机性能制约。

Method: 提出DynaKV方法，集成迁移免集群自适应、以连续性为中心的闪存管理、内存高效缓存设计三项关键技术。

Result: DynaKV相比现有解决方案提高了检索精度，平均精度提升1.38倍，端到端延迟降低，速度提升1.47倍。

Conclusion: DynaKV方法对长序列解码在准确性和效率上有提升，且对其他长上下文工作负载和多层内存架构有更广泛适用性。

Abstract: As the demand for human-like reasoning, multi-turn dialogues, and long-form responses grows, large language models (LLMs) are increasingly expected to support efficient and effective long-sequence decoding. However, due to limited DRAM capacity, long-seuqence LLM decoding on smartphones is constrained by the key-value cache (KVCache), whose memory footprint increases linearly with sequence length. Retrieval-based methods mitigate DRAM pressure by offloading KVCache to flash and retrieving query-relevant entries through cluster-based indexing. Unfortunately, as decoding progresses, KVCache distribution shifts render static or local cluster updates progressively misaligned, excluding essential entries or fetching redundant ones. These issues are further exacerbated by smartphone-specific limitations in bandwidth, IOPS, and memory capacity.
  We propose DynaKV, the first adaptive KVCache management approach that jointly addresses accuracy and efficiency for long-sequence decoding on smartphones. DynaKV integrates three key techniques: (1) Migration-Free Cluster Adaptation, which adaptively splits clusters during retrieval without incurring additional transfers; (2) Continuity-Centric Flash Management, which co-locates correlated entries and clusters and employs a dual-head layout for efficient updates; and (3) Memory-Efficient Cache Design, which virtualizes cache space across DRAM and flash and extends replacement policies to align with cluster-level access patterns. Evaluations demonstrate that DynaKV improves retrieval accuracy and reduces end-to-end latency compared to state-of-the-art solutions, achieving average gains of $1.38\times$ in accuracy and $1.47\times$ speedups. Furthermore, the insights of DynaKV naturally extend to other long-context workloads and multi-tier memory hierarchies, underscoring its broader applicability.

</details>


### [81] [HyProv: Hybrid Provenance Management for Scientific Workflows](https://arxiv.org/abs/2511.07574)
*Vasilis Bountris,Lauritz Thamsen,Ulf Leser*

Main category: cs.DC

TL;DR: 本文提出混合溯源管理系统HyProv，结合集中与联邦范式，可对工作流溯源轨迹进行可扩展、在线且工作流感知的查询，实验表明其可扩展且低延迟、开销小。


<details>
  <summary>Details</summary>
Motivation: 现有溯源系统难以平衡可扩展性、实时处理等多方面，且多数非工作流感知，无法利用工作流规范优势。

Method: HyProv结合集中组件管理工作流规范特定的溯源信息，用联邦查询处理大规模执行日志。

Result: 实验显示HyProv能扩展到大型工作流，以亚秒级延迟回答溯源查询，仅给集群带来适度CPU和内存开销。

Conclusion: HyProv是一种有效的工作流溯源管理系统，能解决现有系统的不足。

Abstract: Provenance plays a crucial role in scientific workflow execution, for instance by providing data for failure analysis, real-time monitoring, or statistics on resource utilization for right-sizing allocations. The workflows themselves, however, become increasingly complex in terms of involved components. Furthermore, they are executed on distributed cluster infrastructures, which makes the real-time collection, integration, and analysis of provenance data challenging. Existing provenance systems struggle to balance scalability, real-time processing, online provenance analytics, and integration across different components and compute resources. Moreover, most provenance solutions are not workflow-aware; by focusing on arbitrary workloads, they miss opportunities for workflow systems where optimization and analysis can exploit the availability of a workflow specification that dictates, to some degree, task execution orders and provides abstractions for physical tasks at a logical level.
  In this paper, we present HyProv, a hybrid provenance management system that combines centralized and federated paradigms to offer scalable, online, and workflow-aware queries over workflow provenance traces. HyProv uses a centralized component for efficient management of the small and stable workflow-specification-specific provenance, and complements this with federated querying over different scalable monitoring and provenance databases for the large-scale execution logs. This enables low-latency access to current execution data. Furthermore, the design supports complex provenance queries, which we exemplify for the workflow system Airflow in combination with the resource manager Kubernetes. Our experiments indicate that HyProv scales to large workflows, answers provenance queries with sub-second latencies, and adds only modest CPU and memory overhead to the cluster.

</details>


### [82] [Intelligence per Watt: Measuring Intelligence Efficiency of Local AI](https://arxiv.org/abs/2511.07885)
*Jon Saad-Falcon,Avanika Narayan,Hakki Orhun Akengin,J. Wes Griffin,Herumb Shandilya,Adrian Gamarra Lafuente,Medhya Goel,Rebecca Joseph,Shlok Natarajan,Etash Kumar Guha,Shang Zhu,Ben Athiwaratkun,John Hennessy,Azalia Mirhoseini,Christopher Ré*

Main category: cs.DC

TL;DR: 研究本地推理能否将大语言模型查询需求从集中式基础设施重新分配，提出每瓦智能（IPW）指标，实证研究发现本地推理可行且发布IPW评测工具。


<details>
  <summary>Details</summary>
Motivation: 集中式云基础设施处理大语言模型查询需求面临扩展压力，小语言模型性能提升和本地加速器发展促使思考本地推理能否重新分配需求。

Method: 提出IPW指标，对20多个先进本地语言模型、8个加速器和100万个真实查询进行大规模实证研究，测量准确率、能耗、延迟和功率。

Result: 本地语言模型能准确回答88.7%的单轮聊天和推理查询；2023 - 2025年IPW提升5.3倍，本地查询覆盖率从23.2%升至71.3%；本地加速器比云加速器IPW至少低1.4倍。

Conclusion: 本地推理能有效重新分配集中式基础设施的需求，IPW是跟踪这一转变的关键指标，发布IPW评测工具。

Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.

</details>


### [83] [Generic Algorithm for Universal TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.08034)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Ilija Basicevic*

Main category: cs.DC

TL;DR: 本文提出新的通用TDM通信算法，克服原算法局限，支持卫星间链路的现实TDM通信。


<details>
  <summary>Details</summary>
Motivation: 原Python测试平台中的TDM通信算法存在只能在网络节点对之间通信的局限，需新算法克服此问题。

Method: 提出新的通用TDM通信算法，涵盖算法理论基础、系统设计和系统验证。

Result: 新算法可让节点与任意数量的对等节点通信。

Conclusion: 新算法的主要优势是支持卫星间链路的现实TDM通信。

Abstract: The original Python Testbed for Federated Learning Algorithms is a light FL framework, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the TDM communication (i.e., peer data exchange) in the current time slot. The limitation of the latter is that it allows communication only between pairs of network nodes. This paper presents the new generic algorithm for the universal TDM communication that overcomes this limitation, such that a node can communicate with an arbitrary number of peers (assuming the peers also want to communicate with it). The paper covers: (i) the algorithm's theoretical foundation, (ii) the system design, and (iii) the system validation. The main advantage of the new algorithm is that it supports real-world TDM communications over inter satellite links.

</details>


### [84] [UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing](https://arxiv.org/abs/2511.08135)
*Zhuoheng Ran,Chong Wu,Renjie Xu,Maolin Che,Hong Yan*

Main category: cs.DC

TL;DR: 介绍了适用于通用和定制计算平台的统一高效Transformer架构UniFormer，在GPU上实现SOTA精度和延迟，在FPGA上有强适应性。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的架构在定制计算硬件上部署时，通用和定制计算的并行计算范式差异导致模型迁移和可部署性有妥协，且许多跨平台优化原则未充分探索。

Method: 提出UniFormer架构，实现更高并行性和计算存储融合。

Result: UniFormer在GPU上达到SOTA精度和延迟，在FPGA上有强适应性。

Conclusion: 本文是首个同时考虑通用和定制计算架构的高效Transformer工作。

Abstract: The success of neural networks such as convolutional neural networks (CNNs) has been largely attributed to their effective and widespread deployment on customised computing platforms, including field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). In the current era, Transformer-based architectures underpin the majority of state-of-the-art (SOTA) larger models that are also increasingly deployed on customised computing hardware for low-power and real-time applications. However, the fundamentally different parallel computation paradigms between general-purpose and customised computing often lead to compromises in model transfer and deployability, which typically come at the cost of complexity, efficiency or accuracy. Moreover, many cross-platform optimisation principles have also remained underexplored in existing studies. This paper introduces UniFormer, a unified and efficient Transformer architecture for both general-purpose and customised computing platforms. By enabling higher parallelism and compute-storage fusion, UniFormer achieves state-of-the-art (SOTA) accuracy and latency on GPUs while exhibiting strong adaptability on FPGAs. To the best of our knowledge, this paper is the first efficient Transformer work that jointly considers both general-purpose and customised computing architectures.

</details>


### [85] [ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum](https://arxiv.org/abs/2511.08147)
*Andrija Stanisic,Stefan Nastic*

Main category: cs.DC

TL;DR: 提出ProbSelect方法用于GPU加速设备客户端选择，无需历史数据和持续监控，评估显示能提升SLO合规性并减少计算浪费。


<details>
  <summary>Details</summary>
Motivation: 将边缘、云和空间设备集成到统一3D连续体中，传统客户端选择方法在动态环境不实用，且现有方案未考虑GPU加速训练特点。

Method: 引入ProbSelect，利用分析建模和概率预测进行客户端选择，在用户定义的SLO内进行建模。

Result: 在不同GPU架构和工作负载上的广泛评估表明，ProbSelect平均提高SLO合规性13.77%，计算浪费减少72.5%。

Conclusion: ProbSelect是一种有效的GPU加速设备客户端选择方法，能提升性能并减少浪费。

Abstract: Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.

</details>


### [86] [\uline{LO}w-c\uline{O}st yet High-\uline{P}erformant \uline{S}parse Matrix-Matrix Multiplication on Arm SME Architectures](https://arxiv.org/abs/2511.08158)
*Kelun Lei,Hailong Yang,Kaige Zhang,Kejie Ma,Yiqing Wang,Xin You,Yufan Xu,Enrique S. Quintana-Orti,Zhongzhi Luan,Yi Liu,Depei Qian*

Main category: cs.DC

TL;DR: 提出LOOPS混合执行框架用于稀疏矩阵 - 稠密矩阵乘法，结合不同布局，支持多精度，实验显示有显著加速和更好能效。


<details>
  <summary>Details</summary>
Motivation: 有效利用Armv9架构的SME和传统SIMD资源进行非结构化稀疏工作负载是挑战，需新方案。

Method: 提出LOOPS框架，结合行式CSR部分和向量式BCSR部分布局，利用向量指令和SME资源，通过轻量级性能模型指导的自适应两级并行化方案支持多精度SpMM。

Result: 在Apple的M4Pro CPU上对SuiteSparse实验，相比CPU基线TACO和Armadillo有显著加速，与GPU方法对比也有平均19.8 - 33.5倍加速，且能效更好。

Conclusion: LOOPS框架在稀疏矩阵 - 稠密矩阵乘法中能有效利用资源，实现高性能和高能效。

Abstract: Sparse matrix-dense matrix multiplication (SpMM) is a critical kernel in both scientific computing and emerging graph learning workloads. The recent Armv9 architecture introduces Scalable Matrix Extension (SME), enabling tile-based matrix operations with high throughput. However, effectively exploiting both SME and traditional SIMD resources for unstructured sparse workloads remains an open challenge. To address this, we propose LOOPS, a hybrid execution framework that combines row-wise CSR-part with vector-wise BCSR-part layout, enabling cooperative utilization of vector instructions (NEON) and Scalable Matrix Extension (SME) resources. LOOPS supports multi-precision SpMM across FP64, FP32, and FP16 via an adaptive two-level parallelization scheme guided by a lightweight performance model. Experimental results on the entire SuiteSparse on an Apple's M4Pro CPU show that LOOPS achieves average speedups of 9.93$\times$ (FP32)/14.4$\times$ (FP64) against the CPU baseline TACO and 71.3$\times$ (FP32)/54.8$\times$ (FP64) with respect to Armadillo. A comparison of LOOPS running on the same CPU with two GPU methods (cuSPARSE, Magicube) executed on an NVIDIA A100 GPU show average speedups for LOOPS between 19.8$\times$ and 33.5$\times$, depending on the precision. Notably, LOOPS delivers significantly better energy efficiency than the GPU codes on the A100 GPU.

</details>


### [87] [Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin](https://arxiv.org/abs/2511.08222)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 本文研究图上受限移动机器人的聚集问题，给出不可能结果并为无限网格和超立方体设计时间最优算法，推测无通用算法。


<details>
  <summary>Details</summary>
Motivation: 研究在OBLOT模型下，具有特定约束条件（初始配置有重复、无法检测重复、在特定图上移动）的机器人聚集问题。

Method: 考虑循环调度激活机器人，为无限网格和超立方体两种特定拓扑设计算法。

Result: 得到一些基本的不可能结果，设计出时间最优的两种算法。

Conclusion: 推测对于所有可解情况不存在通用算法。

Abstract: In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.
  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.

</details>


### [88] [Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing](https://arxiv.org/abs/2511.08373)
*Henrik Daniel Christensen,Saverio Giallorenzo,Jacopo Mauro*

Main category: cs.DC

TL;DR: 本文提出用约束编程优化Kubernetes调度器，在部分场景下提升高优先级Pod分配效果。


<details>
  <summary>Details</summary>
Motivation: Kubernetes默认调度器使用启发式方法，会导致次优放置和资源碎片化。

Method: 提出使用约束编程的方法，以插件形式作为默认调度器的后备机制，使用OR - Tools约束求解器。

Result: 在1秒调度窗口内，在44%默认调度器失败的场景中放置更多高优先级Pod；10秒窗口内，超73%场景改善放置，两种情况均有超19%场景证明默认调度器已达最优。

Conclusion: 约束编程方法可有效优化Kubernetes中Pod的分配。

Abstract: Distributed applications employ Kubernetes for scalable, fault-tolerant deployments over computer clusters, where application components run in groups of containers called pods. The scheduler, at the heart of Kubernetes' architecture, determines the placement of pods given their priority and resource requirements on cluster nodes. To quickly allocate pods, the scheduler uses lightweight heuristics that can lead to suboptimal placements and resource fragmentation, preventing allocations of otherwise deployable pods on the available nodes.
  We propose the usage of constraint programming to find the optimal allocation of pods satisfying all their priorities and resource requests. Implementation-wise, our solution comes as a plug-in to the default scheduler that operates as a fallback mechanism when some pods cannot be allocated. Using the OR-Tools constraint solver, our experiments on small-to-mid-sized clusters indicate that, within a 1-second scheduling window, our approach places more higher-priority pods than the default scheduler (possibly demonstrating allocation optimality) in over 44\% of realisable allocation scenarios where the default scheduler fails, while certifying that the default scheduler's placement is already optimal in over 19\% of scenarios. With a 10-second window, our approach improves placements in over 73\% and still certifies that the default scheduler's placement is already optimal in over 19\% of scenarios.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [89] [Testing noisy low-degree polynomials for sparsity](https://arxiv.org/abs/2511.07835)
*Yiqiao Bao,Anindya De,Shivam Nadimpalli,Rocco A. Servedio,Nathan White*

Main category: cs.DS

TL;DR: 论文研究在有噪声情况下测试低次多项式是否稀疏的问题，给出了低次多项式稀疏性测试具有与维度无关的常数样本复杂度的精确刻画及匹配算法。


<details>
  <summary>Details</summary>
Motivation: 研究在有噪声的随机选点多项式评估下，测试未知低次多项式是否稀疏的问题，是经典带噪声学习稀疏低次多项式问题的属性测试类比，扩展了之前从噪声线性函数到一般低次多项式的工作。

Method: 将Dinur等人（2007）关于{0,1}^n上有界函数的傅里叶尾部结果推广到广泛的有限支撑分布。

Result: 对于T ≥ MSGₓ,d(s)，给出Oₛ,ₓ,d(1)样本算法区分多项式是否稀疏；对于T ≤ MSGₓ,d(s) - 1，证明无噪声时算法需Ωₓ,d,s(log n)个样本。

Conclusion: 得到了低次多项式稀疏性测试何时具有与维度无关的常数样本复杂度的精确刻画。

Abstract: We consider the problem of testing whether an unknown low-degree polynomial $p$ over $\mathbb{R}^n$ is sparse versus far from sparse, given access to noisy evaluations of the polynomial $p$ at \emph{randomly chosen points}. This is a property-testing analogue of classical problems on learning sparse low-degree polynomials with noise, extending the work of Chen, De, and Servedio (2020) from noisy \emph{linear} functions to general low-degree polynomials.
  Our main result gives a \emph{precise characterization} of when sparsity testing for low-degree polynomials admits constant sample complexity independent of dimension, together with a matching constant-sample algorithm in that regime. For any mean-zero, variance-one finitely supported distribution $\boldsymbol{X}$ over the reals, degree $d$, and any sparsity parameters $s \leq T$, we define a computable function $\mathrm{MSG}_{\boldsymbol{X},d}(\cdot)$, and:
  - For $T \ge \mathrm{MSG}_{\boldsymbol{X},d}(s)$, we give an $O_{s,\boldsymbol{X},d}(1)$-sample algorithm that distinguishes whether a multilinear degree-$d$ polynomial over $\mathbb{R}^n$ is $s$-sparse versus $\varepsilon$-far from $T$-sparse, given examples $(\boldsymbol{x},\, p(\boldsymbol{x}) + \mathrm{noise})_{\boldsymbol{x} \sim \boldsymbol{X}^{\otimes n}}$. Crucially, the sample complexity is \emph{completely independent} of the ambient dimension $n$.
  - For $T \leq \mathrm{MSG}_{\boldsymbol{X},d}(s) - 1$, we show that even without noise, any algorithm given samples $(\boldsymbol{x},p(\boldsymbol{x}))_{\boldsymbol{x} \sim \boldsymbol{X}^{\otimes n}}$ must use $Ω_{\boldsymbol{X},d,s}(\log n)$ examples.
  Our techniques employ a generalization of the results of Dinur et al. (2007) on the Fourier tails of bounded functions over $\{0,1\}^n$ to a broad range of finitely supported distributions, which may be of independent interest.

</details>


### [90] [Model-agnostic super-resolution in high dimensions](https://arxiv.org/abs/2511.07846)
*Xi Chen,Anindya De,Yizhi Huang,Shivam Nadimpalli,Rocco A. Servedio,Tianqi Yang*

Main category: cs.DS

TL;DR: 本文分析超分辨率问题的通用版本，考虑d维环面上的一般信号，得到Wasserstein距离重建和‘重击中者’重建的上下界。


<details>
  <summary>Details</summary>
Motivation: 以往超分辨率研究对信号有强建模假设，本文要分析更通用的超分辨率问题。

Method: 考虑d维环面上的完全通用信号，不做空间分离等假设，从Wasserstein距离和‘重击中者’重建两个方面分析。

Result: 在Wasserstein距离重建中，d维信号准确重建约需exp(d)个傅里叶系数；‘重击中者’重建中，仅需约exp(sqrt(d))个傅里叶系数。

Conclusion: 得到两种重建方式下截止频率T和噪声幅度κ的上下界，且两种重建所需傅里叶系数数量差异大。

Abstract: The problem of \emph{super-resolution}, roughly speaking, is to reconstruct an unknown signal to high accuracy, given (potentially noisy) information about its low-degree Fourier coefficients. Prior results on super-resolution have imposed strong modeling assumptions on the signal, typically requiring that it is a linear combination of spatially separated point sources.
  In this work we analyze a very general version of the super-resolution problem, by considering completely general signals over the $d$-dimensional torus $[0,1)^d$; we do not assume any spatial separation between point sources, or even that the signal is a finite linear combination of point sources. We obtain two sets of results, corresponding to two natural notions of reconstruction.
  - {\bf Reconstruction in Wasserstein distance:} We give essentially matching upper and lower bounds on the cutoff frequency $T$ and the magnitude $κ$ of the noise for which accurate reconstruction in Wasserstein distance is possible. Roughly speaking, our results here show that for $d$-dimensional signals, estimates of $\approx \exp(d)$ many Fourier coefficients are necessary and sufficient for accurate reconstruction under the Wasserstein distance.
  - {\bf "Heavy hitter" reconstruction:} For nonnegative signals (equivalently, probability distributions), we introduce a new notion of "heavy hitter" reconstruction that essentially amounts to achieving high-accuracy reconstruction of all "sufficiently dense" regions of the distribution. Here too we give essentially matching upper and lower bounds on the cutoff frequency $T$ and the magnitude $κ$ of the noise for which accurate reconstruction is possible. Our results show that -- in sharp contrast with Wasserstein reconstruction -- accurate estimates of only $\approx \exp(\sqrt{d})$ many Fourier coefficients are necessary and sufficient for heavy hitter reconstruction.

</details>


### [91] [Deterministic Padded Decompositions and Negative-Weight Shortest Paths](https://arxiv.org/abs/2511.07859)
*Jason Li*

Main category: cs.DS

TL;DR: 提出首个整数加权图负权单源最短路径近线性时间确定性算法。


<details>
  <summary>Details</summary>
Motivation: 寻找整数加权图负权单源最短路径的近线性时间确定性算法。

Method: 在有向图上进行填充分解的确定性构造。

Result: 得到首个整数加权图负权单源最短路径近线性时间确定性算法。

Conclusion: 所提出的确定性构造方法有效，可用于解决相关问题。

Abstract: We obtain the first near-linear time deterministic algorithm for negative-weight single-source shortest paths on integer-weighted graphs. Our main ingredient is a deterministic construction of a padded decomposition on directed graphs, which may be of independent interest.

</details>


### [92] [Parallel Sampling via Autospeculation](https://arxiv.org/abs/2511.07869)
*Nima Anari,Carlo Baronio,CJ Chen,Alireza Haqi,Frederic Koehler,Anqi Li,Thuy-Duong Vuong*

Main category: cs.DS

TL;DR: 提出并行算法加速两种模型采样，时间降至O(n^{1/2})，引入投机拒绝采样技术。


<details>
  <summary>Details</summary>
Motivation: 加速任意阶自回归模型和去噪扩散模型的采样过程。

Method: 提出并行算法，引入投机拒绝采样技术，利用近似分布ν，采用自动推测构建ν，在序列级别进行推测。

Result: 通过并行调用预言机，采样时间从O(n)降至O(n^{1/2})，改进自回归模型，为扩散模型带来首次并行加速。

Conclusion: 所提并行算法和投机拒绝采样技术有效，能显著加速采样。

Abstract: We present parallel algorithms to accelerate sampling via counting in two settings: any-order autoregressive models and denoising diffusion models. An any-order autoregressive model accesses a target distribution $μ$ on $[q]^n$ through an oracle that provides conditional marginals, while a denoising diffusion model accesses a target distribution $μ$ on $\mathbb{R}^n$ through an oracle that provides conditional means under Gaussian noise. Standard sequential sampling algorithms require $\widetilde{O}(n)$ time to produce a sample from $μ$ in either setting. We show that, by issuing oracle calls in parallel, the expected sampling time can be reduced to $\widetilde{O}(n^{1/2})$. This improves the previous $\widetilde{O}(n^{2/3})$ bound for any-order autoregressive models and yields the first parallel speedup for diffusion models in the high-accuracy regime, under the relatively mild assumption that the support of $μ$ is bounded.
  We introduce a novel technique to obtain our results: speculative rejection sampling. This technique leverages an auxiliary ``speculative'' distribution~$ν$ that approximates~$μ$ to accelerate sampling. Our technique is inspired by the well-studied ``speculative decoding'' techniques popular in large language models, but differs in key ways. Firstly, we use ``autospeculation,'' namely we build the speculation $ν$ out of the same oracle that defines~$μ$. In contrast, speculative decoding typically requires a separate, faster, but potentially less accurate ``draft'' model $ν$. Secondly, the key differentiating factor in our technique is that we make and accept speculations at a ``sequence'' level rather than at the level of single (or a few) steps. This last fact is key to unlocking our parallel runtime of $\widetilde{O}(n^{1/2})$.

</details>


### [93] [Forgetting Alternation and Blossoms: A New Framework for Fast Matching Augmentation and Its Applications to Sequential/Distributed/Streaming Computation](https://arxiv.org/abs/2511.08210)
*Taisuke Izumi,Naoki Kitamura,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 本文提出一般图最短交替路径新结构定理，基于此提出新算法，还给出分布式和半流式设置下新近似算法。


<details>
  <summary>Details</summary>
Motivation: Micali和Vazirani算法正确性证明极难，需新方法解决最大基数匹配问题。

Method: 提出不考虑花细节的结构定理，利用交替基树概念，提出新算法和近似算法框架。

Result: 新算法比MV算法更易实现和确认正确性，近似算法显著改善运行时间上界。

Conclusion: 新结构定理、算法及近似算法框架为最大基数匹配问题提供新途径。

Abstract: Finding a maximum cardinality matching in a graph is one of the most fundamental problems. An algorithm proposed by Micali and Vazirani (1980) is well-known to solve the problem in $O(m\sqrt{n})$ time, which is still one of the fastest algorithms in general. While the MV algorithm itself is not so complicated and is indeed convincing, its correctness proof is extremely challenging, which can be seen from the history: after the first algorithm paper had appeared in 1980, Vazirani has made several attempts to give a complete proof for more than 40 years. It seems, roughly speaking, caused by the nice but highly complex structure of the shortest alternating paths in general graphs that are deeply intertwined with the so-called (nested) blossoms.
  In this paper, we propose a new structure theorem on the shortest alternating paths in general graphs without taking into the details of blossoms. The high-level idea is to forget the alternation (of matching and non-matching edges) as early as possible. A key ingredient is a notion of alternating base trees (ABTs) introduced by Izumi, Kitamura, and Yamaguchi (2024) to develop a nearly linear-time distributed algorithm. Our structure theorem refines the properties of ABTs exploited in their algorithm, and we also give simpler alternative proofs for them. Based on our structure theorem, we propose a new algorithm, which is slightly slower but more implementable and much easier to confirm its correctness than the MV algorithm.
  As applications of our framework, we also present new $(1 - ε)$-approximation algorithms in the distributed and semi-streaming settings. Both algorithms are deterministic, and substantially improve the best known upper bounds on the running time. The algorithms are built on the top of a novel framework of amplifying approximation factors of given matchings, which is of independent interest.

</details>


### [94] [Fully Dynamic Set Cover: Worst-Case Recourse and Update Time](https://arxiv.org/abs/2511.08485)
*Sayan Bhattacharya,Ruoxu Cen,Debmalya Panigrahi*

Main category: cs.DS

TL;DR: 本文给出首个同时实现非平凡最坏情况界的全动态集合覆盖算法，在两种近似机制下达到O(log n)的追索和f·poly log(n)的更新时间。


<details>
  <summary>Details</summary>
Motivation: 在动态集合覆盖问题中，需最小化追索和更新时间，此前研究未同时实现非平凡最坏情况界。

Method: 未提及具体方法。

Result: 给出全动态集合覆盖算法，在O(log n)和O(f)近似机制下，最坏情况同时实现O(log n)追索和f·poly log(n)更新时间。

Conclusion: 解决了同时实现非平凡最坏情况界的问题，优于此前研究。

Abstract: In (fully) dynamic set cover, the goal is to maintain an approximately optimal solution to a dynamically evolving instance of set cover, where in each step either an element is added to or removed from the instance. The two main desiderata of a dynamic set cover algorithm are to minimize at each time-step, the recourse, which is the number of sets removed from or added to the solution, and the update time to compute the updated solution. This problem has been extensively studied over the last decade leading to many results that achieve ever-improving bounds on the recourse and update time, while maintaining a solution whose cost is comparable to that of offline approximation algorithms.
  In this paper, we give the first algorithms to simultaneously achieve non-trivial worst-case bounds for recourse and update time. Specifically, we give fully-dynamic set cover algorithms that simultaneously achieve $O(\log n)$ recourse and $f\cdot \textrm{poly}\log(n)$ update time in the worst-case, for both approximation regimes: $O(\log n)$ and $O(f)$ approximation. (Here, $n, f$ respectively denote the maximum number of elements and maximum frequency of an element across all instances.) Prior to our work, all results for this problem either settled for amortized bounds on recourse and update time, or obtained $f\cdot \textrm{poly}\log(n)$ update time in the worst-case but at the cost of $Ω(m)$ worst-case recourse. (Here, $m$ denotes the number of sets. Note that any algorithm has recourse at most $m$.)

</details>


### [95] [Deterministic Negative-Weight Shortest Paths in Nearly Linear Time via Path Covers](https://arxiv.org/abs/2511.08551)
*Bernhard Haeupler,Yonggang Jiang,Thatchaphol Saranurak*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the first deterministic nearly-linear time algorithm for single-source shortest paths with negative edge weights on directed graphs: given a directed graph $G$ with $n$ vertices, $m$ edges whose weights are integer in $\{-W,\dots,W\}$, our algorithm either computes all distances from a source $s$ or reports a negative cycle in time $\tilde{O}(m)\cdot \log(nW)$ time.
  All known near-linear time algorithms for this problem have been inherently randomized, as they crucially rely on low-diameter decompositions.
  To overcome this barrier, we introduce a new structural primitive for directed graphs called the path cover. This plays a role analogous to neighborhood covers in undirected graphs, which have long been central to derandomizing algorithms that use low-diameter decomposition in the undirected setting. We believe that path covers will serve as a fundamental tool for the design of future deterministic algorithms on directed graphs.

</details>


### [96] [Universal Connection Schedules for Reconfigurable Networking](https://arxiv.org/abs/2511.08556)
*Shaleen Baral,Robert Kleinberg,Sylvan Martin,Henry Rogers,Tegan Wilson,Ruogu Zhang*

Main category: cs.DS

TL;DR: 提出用于无感知路由的通用调度方案，可同时实现多个帕累托最优权衡点，给出不同类型调度方案及结果并将随机构造确定化。


<details>
  <summary>Details</summary>
Motivation: 现有帕累托最优设计针对单一请求类型的连接调度在处理其他类型请求时效率低，而数据中心工作负载包含不同延迟请求，需要更灵活高效的通用调度。

Method: 基于循环置换的连接调度，开发新颖的傅里叶分析方法分析随机路由，还使用Lovett - Meka差异最小化算法的去随机版本将随机构造确定化。

Result: 给出通用调度方案，一种均匀随机连接调度在吞吐量有乘法误差、延迟达O(log N)最优；更精心设计的随机连接调度在吞吐量有加法误差、延迟常数因子最优；将第一种随机构造确定化获得相同结果。

Conclusion: 成功提出无感知路由的通用调度方案，能同时实现多个帕累托最优权衡点。

Abstract: Reconfigurable networks are a novel communication paradigm in which the pattern of connectivity between hosts varies rapidly over time. Prior theoretical work explored the inherent tradeoffs between throughput (or, hop-count) and latency, and showed the existence of infinitely many Pareto-optimal designs as the network size tends to infinity. Existing Pareto-optimal designs use a connection schedule which is fine-tuned to the desired hop-count $h$, permitting lower latency as $h$ increases. However, in reality datacenter workloads contain a mix of low-latency and high-latency requests. Using a connection schedule fine-tuned for one request type leads to inefficiencies when serving other types.
  A more flexible and efficient alternative is a {\em universal schedule}, a single connection schedule capable of attaining many Pareto-optimal tradeoff points simultaneously, merely by varying the choice of routing paths. In this work we present the first universal schedules for oblivious routing. Our constructions yield universal schedules which are near-optimal for all possible hop-counts $h$. The key technical idea is to specialize to a type of connection schedule based on cyclic permutations and to develop a novel Fourier-analytic method for analyzing randomized routing on these connection schedules. We first show that a uniformly random connection schedule suffices with multiplicative error in throughput, and latency optimal up to a $\log N$ factor. We then show that a more carefully designed random connection schedule suffices with additive error in throughput, but improved latency optimal up to only constant factors. Finally, we show that our first randomized construction can be made deterministic using a derandomized version of the Lovett-Meka discrepancy minimization algorithm to obtain the same result.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [97] [Reliable and Private Utility Signaling for Data Markets](https://arxiv.org/abs/2511.07975)
*Li Peng,Jiayao Zhang,Yihang Wu,Weiran Liu,Jinfei Liu,Zheng Yan,Kui Ren,Lei Zhang,Lin Qu*

Main category: cs.GT

TL;DR: 数据市场中传统信号机制面临隐私与可靠性困境，本文提出非TCP的信号机制，用MPC等方法保障其功能，实验验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决数据市场中常用信号机制在隐私和可靠性方面的两难困境，支持有效数据交易。

Method: 定义理想效用信号机制，利用恶意安全多方计算（MPC）确保信号计算的隐私和鲁棒性，引入基于MPC的哈希验证方案确保输入可靠性，探索MPC - KNN - Shapley方法的设计与优化。

Result: 通过严格实验证明了所提方法的效率和实用性。

Conclusion: 所提出的非TCP的信号机制能同时保证隐私和可靠性，可有效支持数据交易。

Abstract: The explosive growth of data has highlighted its critical role in driving economic growth through data marketplaces, which enable extensive data sharing and access to high-quality datasets. To support effective trading, signaling mechanisms provide participants with information about data products before transactions, enabling informed decisions and facilitating trading. However, due to the inherent free-duplication nature of data, commonly practiced signaling methods face a dilemma between privacy and reliability, undermining the effectiveness of signals in guiding decision-making.
  To address this, this paper explores the benefits and develops a non-TCP-based construction for a desirable signaling mechanism that simultaneously ensures privacy and reliability. We begin by formally defining the desirable utility signaling mechanism and proving its ability to prevent suboptimal decisions for both participants and facilitate informed data trading. To design a protocol to realize its functionality, we propose leveraging maliciously secure multi-party computation (MPC) to ensure the privacy and robustness of signal computation and introduce an MPC-based hash verification scheme to ensure input reliability. In multi-seller scenarios requiring fair data valuation, we further explore the design and optimization of the MPC-based KNN-Shapley method with improved efficiency. Rigorous experiments demonstrate the efficiency and practicality of our approach.

</details>


### [98] [Centralized Group Equitability and Individual Envy-Freeness in the Allocation of Indivisible Items](https://arxiv.org/abs/2511.07984)
*Ying Wang,Jiaqian Li,Tianze Wei,Hau Chan,Minming Li*

Main category: cs.GT

TL;DR: 研究从代理和集中分配者角度对不可分割物品进行公平分配，定义相关公平概念，证明满足特定公平条件的分配存在并设计算法，还考虑了组级公平目标。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中集中分配者确保组间和代理间公平分配不可分割物品的问题，如学校资源分配、城市住房分配。

Method: 引入经典的无嫉妒性（EF）概念确保代理间公平，定义集中组公平性（CGEQ）概念确保组间公平，考虑其对应松弛版本EF1和CGEQ1，针对不同估值函数类别进行研究。

Result: 证明了满足EF1和CGEQ1的分配总是存在，并设计了计算这些分配的高效算法，还给出了关于集中组最大最小份额（CGMMS）的若干结果。

Conclusion: 可以通过合理定义公平概念和设计算法，实现不可分割物品在组间和代理间的公平分配。

Abstract: We study the fair allocation of indivisible items for groups of agents from the perspectives of the agents and a centralized allocator. In our setting, the centralized allocator is interested in ensuring the allocation is fair among the groups and between agents. This setting applies to many real-world scenarios, including when a school administrator wants to allocate resources (e.g., office spaces and supplies) to staff members in departments and when a city council allocates limited housing units to various families in need across different communities. To ensure fair allocation between agents, we consider the classical envy-freeness (EF) notion. To ensure fairness among the groups, we define the notion of centralized group equitability (CGEQ) to capture the fairness for the groups from the allocator's perspective. Because an EF or CGEQ allocation does not always exist in general, we consider their corresponding natural relaxations of envy-freeness to one item (EF1) and centralized group equitability up to one item (CGEQ1). For different classes of valuation functions of the agents and the centralized allocator, we show that allocations satisfying both EF1 and CGEQ1 always exist and design efficient algorithms to compute these allocations. We also consider the centralized group maximin share (CGMMS) from the centralized allocator's perspective as a group-level fairness objective with EF1 for agents and present several results.

</details>


### [99] [Nash-equilibrium Seeking Algorithm for Power-Allocation Games on Networks of International Relations](https://arxiv.org/abs/2511.08033)
*Chuanzhe Zhang,Yuke Li,Wenjun Mei*

Main category: cs.GT

TL;DR: 本文改进“符号图博弈”框架，修改偏好公理并引入新算法证明纯策略纳什均衡存在，用历史数据验证模型，拓展原框架现实应用。


<details>
  <summary>Details</summary>
Motivation: 原“符号图博弈”框架虽基础通用，但在捕捉国际关系战略场景复杂性方面有不足，需进一步探索。

Method: 一是修改现有偏好公理，以更细致理解国家行为；二是引入新算法证明修订后博弈存在纯策略纳什均衡；用1940年历史数据验证模型。

Result: 用历史数据验证模型并预测国家生存能力。

Conclusion: 研究拓展了原框架在现实中的适用性，为网络安全环境下战略互动提供更全面视角。

Abstract: In the field of international security, understanding the strategic interactions between countries within a networked context is crucial. Our previous research has introduced a ``games-on-signed graphs'' framework~\cite{LiMorse2022} to analyze these interactions. While the framework is intended to be basic and general, there is much left to be explored, particularly in capturing the complexity of strategic scenarios in international relations. Our paper aims to fill this gap in two key ways. First, we modify the existing preference axioms to allow for a more nuanced understanding of how countries pursue self-survival, defense of allies, and offense toward adversaries. Second, we introduce a novel algorithm that proves the existence of a pure-strategy Nash equilibrium for these revised games. To validate our model, we employ historical data from the year 1940 as the game input and predict countries' survivability. Our contributions thus extend the real-world applicability of the original framework, offering a more comprehensive view of strategic interactions in a networked security environment.

</details>


### [100] [Dividing Indivisible Items for the Benefit of All: It is Hard to Be Fair Without Social Awareness](https://arxiv.org/abs/2511.08160)
*Argyris Deligkas,Eduard Eiben,Tiger-Lily Goldsmith,Dušan Knop,Šimon Schierreich*

Main category: cs.GT

TL;DR: 研究不可分割物品的公平分配以最大化社会影响，问题复杂度取决于代理人是否有社会意识。


<details>
  <summary>Details</summary>
Motivation: 标准公平分配模型假设代理人自私，但资源分配会影响整体或社会，因此研究在保证公平的同时最大化社会影响的分配。

Method: 为每个代理人关联定义物品价值和社会影响的两个加性函数，根据代理人是否有社会意识分析问题复杂度。

Result: 对无社会意识的代理人，问题在多种公平概念下是NP难的，仅在非常受限的情况下可解；有社会意识则可在多项式时间内计算出公平且最大化社会影响的分配，放松社会意识定义问题又变得难解。

Conclusion: 物品公平分配中最大化社会影响的问题复杂度与代理人的社会意识有关。

Abstract: In standard fair division models, we assume that all agents are selfish. However, in many scenarios, division of resources has a direct impact on the whole group or even society. Therefore, we study fair allocations of indivisible items that, at the same time, maximize social impact. In this model, each agent is associated with two additive functions that define their value and social impact for each item. The goal is to allocate items so that the social impact is maximized while maintaining some fairness criterion. We reveal that the complexity of the problem heavily depends on whether the agents are socially aware, i.e., they take into consideration the social impact functions. For socially unaware agents, we prove that the problem is NP-hard for a variety of fairness notions, and that it is tractable only for very restricted cases, e.g., if, for every agent, the valuation equals social impact and it is binary. On the other hand, social awareness allows for fair allocations that maximize social impact, and such allocations can be computed in polynomial time. Interestingly, the problem becomes again intractable as soon as the definition of social awareness is relaxed.

</details>


### [101] [Classification in Equilibrium: Structure of Optimal Decision Rules](https://arxiv.org/abs/2511.08347)
*Elizabeth Maggie Penn,John W. Patty*

Main category: cs.GT

TL;DR: 本文研究个体对分类规则做出行为调整时的最优分类，得出最优规则类型及与以往不同的结论。


<details>
  <summary>Details</summary>
Motivation: 研究个体对分类规则做出行为调整时的最优分类情况。

Method: 将设计者与群体的交互建模为Stackelberg博弈，在标准单调似然比假设下进行分析。

Result: 最优规则属于小且可解释的家族（单阈值和双截断规则），包含常规和反直觉设计；与先前最优分类器奖励高信号的发现不同，设计者可能奖励似然比低的个体或在中间区间集中奖惩。

Conclusion: 考虑个体行为调整时，最优分类规则与以往研究有显著差异。

Abstract: This paper characterizes optimal classification when individuals adjust their behavior in response to the classification rule. We model the interaction between a designer and a population as a Stackelberg game: the designer selects a classification rule anticipating how individuals will comply, cheat, or abstain in order to obtain a favorable classification. Under standard monotone likelihood ratio assumptions, optimal rules belong to a small and interpretable family (single-threshold and two-cut rules) that encompass both conventional and counterintuitive designs. Our results depart sharply from prior findings that optimal classifiers reward higher signals: in equilibrium, the designer may deliberately reward those with lower likelihood ratios or concentrate rewards/penalties in a middle band to improve informational quality.

</details>


### [102] [Fair Multi-agent Persuasion with Submodular Constraints](https://arxiv.org/abs/2511.08538)
*Yannan Bai,Kamesh Munagala,Yiheng Shen,Davidson Zhu*

Main category: cs.GT

TL;DR: 研究贝叶斯说服中的选择问题，提出信号策略实现对数近似的优化，表现优于线性近似，且能在多项式时间内得到近似解。


<details>
  <summary>Details</summary>
Motivation: 在贝叶斯说服背景下，使资源分配在满足子模约束时，代理人的期望效用达到公平性。

Method: 提出信号策略，在对接收者有温和的有限理性假设下实现目标，利用乘法权重更新方法在多项式时间得到近似解。

Result: 实现了对数近似的优化策略，近似比几乎是最优的，优于线性近似；证明代理人效用向量定义了不同多胞形的基多面体。

Conclusion: 提出的策略在解决贝叶斯说服中的选择问题上有效，能在满足公平性的同时达到较好的近似效果。

Abstract: We study the problem of selection in the context of Bayesian persuasion. We are given multiple agents with hidden values (or quality scores), to whom resources must be allocated by a welfare-maximizing decision-maker. An intermediary with knowledge of the agents' values seeks to influence the outcome of the selection by designing informative signals and providing tie-breaking policies, so that when the receiver maximizes welfare over the resulting posteriors, the expected utilities of the agents (where utility is defined as allocation times value) achieve certain fairness properties. The fairness measure we will use is majorization, which simultaneously approximately maximizes all symmetric, monotone, concave functions of the utilities. We consider the general setting where the allocation to the agents needs to respect arbitrary submodular constraints, as given by the corresponding polymatroid.
  We present a signaling policy that, under a mild bounded rationality assumption on the receiver, achieves a logarithmically approximate majorized policy in this setting. The approximation ratio is almost best possible, and that significantly outperforms generic results that only yield linear approximations. A key component of our result is a structural characterization showing that the vector of agent utilities for a given signaling policy defines the base polytope of a different polymatroid, a result that may be of independent interest. In addition, we show that an arbitrarily good additive approximation to this vector can be produced in (weakly) polynomial time via the multiplicative weights update method.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [103] [A Hybrid Multimodal Deep Learning Framework for Intelligent Fashion Recommendation](https://arxiv.org/abs/2511.07573)
*Kamand Kalashi,Babak Teimourpour*

Main category: cs.IR

TL;DR: 本文提出用于时尚推荐的混合多模态深度学习框架，处理套装兼容性预测和互补商品检索，在相关任务表现良好。


<details>
  <summary>Details</summary>
Motivation: 在线时尚平台迅速扩张，对能理解视觉和文本线索的智能推荐系统需求增加。

Method: 利用CLIP架构的视觉和文本编码器获取时尚单品的联合潜在表征，集成到统一特征向量，用Transformer编码器处理；兼容性预测引入“套装令牌”，互补商品检索使用“目标商品令牌”。

Result: 在Polyvore数据集上兼容性预测AUC达0.95，互补商品检索在FITB指标下准确率达69.24%。

Conclusion: 所提方法在两项任务中表现出色，凸显多模态学习用于时尚推荐的有效性。

Abstract: The rapid expansion of online fashion platforms has created an increasing demand for intelligent recommender systems capable of understanding both visual and textual cues. This paper proposes a hybrid multimodal deep learning framework for fashion recommendation that jointly addresses two key tasks: outfit compatibility prediction and complementary item retrieval. The model leverages the visual and textual encoders of the CLIP architecture to obtain joint latent representations of fashion items, which are then integrated into a unified feature vector and processed by a transformer encoder. For compatibility prediction, an "outfit token" is introduced to model the holistic relationships among items, achieving an AUC of 0.95 on the Polyvore dataset. For complementary item retrieval, a "target item token" representing the desired item description is used to retrieve compatible items, reaching an accuracy of 69.24% under the Fill-in-the-Blank (FITB) metric. The proposed approach demonstrates strong performance across both tasks, highlighting the effectiveness of multimodal learning for fashion recommendation.

</details>


### [104] [TurkEmbed4Retrieval: Turkish Embedding Model for Retrieval Task](https://arxiv.org/abs/2511.07595)
*Özay Ezerceli,Gizem Gümüşçekiçci,Tuğba Erkoç,Berke Özenç*

Main category: cs.IR

TL;DR: 本文介绍TurkEmbed4Retrieval，经微调在土耳其检索任务达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 为土耳其检索任务开发专门模型，提升检索性能。

Method: 在MS MARCO TR数据集上微调TurkEmbed基础模型，采用Matryoshka表示学习和定制多负排名损失等高级训练技术。

Result: 模型在Scifact TR数据集关键检索指标上比Turkish colBERT高19.26%。

Conclusion: 模型为土耳其信息检索建立了新基准。

Abstract: In this work, we introduce TurkEmbed4Retrieval, a retrieval specialized variant of the TurkEmbed model originally designed for Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. By fine-tuning the base model on the MS MARCO TR dataset using advanced training techniques, including Matryoshka representation learning and a tailored multiple negatives ranking loss, we achieve SOTA performance for Turkish retrieval tasks. Extensive experiments demonstrate that our model outperforms Turkish colBERT by 19,26% on key retrieval metrics for the Scifact TR dataset, thereby establishing a new benchmark for Turkish information retrieval.

</details>


### [105] [From IDs to Semantics: A Generative Framework for Cross-Domain Recommendation with Adaptive Semantic Tokenization](https://arxiv.org/abs/2511.08006)
*Peiyu Hu,Wayne Lu,Jia Wang*

Main category: cs.IR

TL;DR: 传统跨域推荐方法有局限，LLM 方法也面临挑战，提出 GenCDR 框架，实验显示其性能优于 SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统跨域推荐方法依赖共享用户/物品 ID，LLM 方法存在物品 ID 分词困境和特定领域建模不足问题，需改进。

Method: 提出 GenCDR 框架，包含领域自适应分词模块、跨域自回归推荐模块和领域感知前缀树。

Result: 在多个真实数据集上实验，GenCDR 显著优于现有 SOTA 基线。

Conclusion: GenCDR 框架有效解决了现有跨域推荐方法的局限，性能良好。

Abstract: Cross-domain recommendation (CDR) is crucial for improving recommendation accuracy and generalization, yet traditional methods are often hindered by the reliance on shared user/item IDs, which are unavailable in most real-world scenarios. Consequently, many efforts have focused on learning disentangled representations through multi-domain joint training to bridge the domain gaps. Recent Large Language Model (LLM)-based approaches show promise, they still face critical challenges, including: (1) the \textbf{item ID tokenization dilemma}, which leads to vocabulary explosion and fails to capture high-order collaborative knowledge; and (2) \textbf{insufficient domain-specific modeling} for the complex evolution of user interests and item semantics. To address these limitations, we propose \textbf{GenCDR}, a novel \textbf{Gen}erative \textbf{C}ross-\textbf{D}omain \textbf{R}ecommendation framework. GenCDR first employs a \textbf{Domain-adaptive Tokenization} module, which generates disentangled semantic IDs for items by dynamically routing between a universal encoder and domain-specific adapters. Symmetrically, a \textbf{Cross-domain Autoregressive Recommendation} module models user preferences by fusing universal and domain-specific interests. Finally, a \textbf{Domain-aware Prefix-tree} enables efficient and accurate generation. Extensive experiments on multiple real-world datasets demonstrate that GenCDR significantly outperforms state-of-the-art baselines. Our code is available in the supplementary materials.

</details>


### [106] [BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives](https://arxiv.org/abs/2511.08029)
*Aarush Sinha,Pavan Kumar S,Roshan Balaji,Nirav Pravinbhai Bhatt*

Main category: cs.IR

TL;DR: 提出BiCA方法利用引文链接挖掘硬负样本，提升特定领域小型密集检索器性能，在多任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 生物医学和科学领域硬负样本挖掘困难，需寻找合适的硬负样本提升检索模型。

Method: 提出BiCA方法，利用20000篇PubMed文章的引文链接进行硬负样本挖掘，微调GTE_small和GTE_Base模型。

Result: 在BEIR的域内和域外任务的零样本密集检索中nDCG@10有一致提升，在LoTTE长尾主题上Success@5优于基线。

Conclusion: 利用文档链接结构生成高信息负样本可实现少微调下的最优性能，为高效领域适配提供途径。

Abstract: Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.

</details>


### [107] [DiffuGR: Generative Document Retrieval with Diffusion Language Models](https://arxiv.org/abs/2511.08150)
*Xinpeng Zhao,Yukun Zhao,Zhenyang Li,Mengqi Zhang,Jun Feng,Ran Chen,Ying Zhou,Zhumin Chen,Shuaiqiang Wang,Zhaochun Ren,Dawei Yin,Xin Xin*

Main category: cs.IR

TL;DR: 本文提出DiffuGR方法解决现有生成式检索方法的局限，实验表明其在检索中有效且能灵活平衡速度与精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归生成模型的生成式检索方法存在DocID生成与自然语言生成不匹配、无法动态平衡检索效率和准确性的问题。

Method: 提出DiffuGR，将DocID生成建模为离散扩散过程，训练时通过随机掩码损坏DocID并学习扩散语言模型恢复，推理时并行生成DocID令牌并通过可控数量的去噪步骤细化。

Result: 在基准检索数据集上的实验表明，DiffuGR与强大的自回归生成式检索器具有竞争力，并能通过可变去噪预算灵活平衡速度和准确性。

Conclusion: 非自回归扩散模型是生成式文档检索的实用且有效替代方案。

Abstract: Generative retrieval (GR) re-frames document retrieval as a sequence-based document identifier (DocID) generation task, memorizing documents with model parameters and enabling end-to-end retrieval without explicit indexing. Existing GR methods are based on auto-regressive generative models, i.e., the token generation is performed from left to right. However, such auto-regressive methods suffer from: (1) mismatch between DocID generation and natural language generation, e.g., an incorrect DocID token generated in early left steps would lead to totally erroneous retrieval; and (2) failure to balance the trade-off between retrieval efficiency and accuracy dynamically, which is crucial for practical applications. To address these limitations, we propose generative document retrieval with diffusion language models, dubbed DiffuGR. It models DocID generation as a discrete diffusion process: during training, DocIDs are corrupted through a stochastic masking process, and a diffusion language model is learned to recover them under a retrieval-aware objective. For inference, DiffuGR attempts to generate DocID tokens in parallel and refines them through a controllable number of denoising steps. In contrast to conventional left-to-right auto-regressive decoding, DiffuGR provides a novel mechanism to first generate more confident DocID tokens and refine the generation through diffusion-based denoising. Moreover, DiffuGR also offers explicit runtime control over the qualitylatency tradeoff. Extensive experiments on benchmark retrieval datasets show that DiffuGR is competitive with strong auto-regressive generative retrievers, while offering flexible speed and accuracy tradeoffs through variable denoising budgets. Overall, our results indicate that non-autoregressive diffusion models are a practical and effective alternative for generative document retrieval.

</details>


### [108] [MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System](https://arxiv.org/abs/2511.08181)
*Seung Hwan Cho,Yujin Yang,Danik Baeck,Minjoo Kim,Young-Min Kim,Heejung Lee,Sangjin Park*

Main category: cs.IR

TL;DR: 提出基于Agentic RAG的多模态多任务鸡尾酒推荐系统MARC，经评估其基于图数据库生成的答案质量优于简单向量数据库。


<details>
  <summary>Details</summary>
Motivation: 缓解推荐系统冷启动限制，结合食品饮料推荐系统特点，利用大语言模型推理能力。

Method: 构建基于图数据库的多模态多任务鸡尾酒推荐系统MARC，通过任务识别路由器和反思过程生成答案。

Result: 使用LLM评估和人工评估，证明基于图数据库生成的答案质量优于简单向量数据库。

Conclusion: 所提出的MARC系统在冷启动条件下有效，代码可在指定链接获取。

Abstract: Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at https://github.com/diddbwls/cocktail_rec_agentrag

</details>


### [109] [Bid Farewell to Seesaw: Towards Accurate Long-tail Session-based Recommendation via Dual Constraints of Hybrid Intents](https://arxiv.org/abs/2511.08378)
*Xiao Wang,Ke Qin,Dongyang Zhang,Xiurui Xie,Shuang Liang*

Main category: cs.IR

TL;DR: 提出HID框架解决会话推荐中长尾与准确性的冲突，提升两者性能。


<details>
  <summary>Details</summary>
Motivation: 实际推荐场景中低曝光物品的长尾分布影响推荐多样性，现有方法在提升长尾性能时会降低准确性。

Method: 提出HID框架，包含混合意图学习和意图约束损失，前者重构物品到意图映射并区分无关噪声，后者用多样性和准确性约束范式调节表征学习。

Result: 在多个SBR模型和数据集上实验，HID能提升长尾性能和推荐准确性。

Conclusion: HID框架可将传统的“跷跷板”效应转变为“双赢”，在长尾推荐系统中达到新的最优性能。

Abstract: Session-based recommendation (SBR) aims to predict anonymous users' next interaction based on their interaction sessions. In the practical recommendation scenario, low-exposure items constitute the majority of interactions, creating a long-tail distribution that severely compromises recommendation diversity. Existing approaches attempt to address this issue by promoting tail items but incur accuracy degradation, exhibiting a "see-saw" effect between long-tail and accuracy performance. We attribute such conflict to session-irrelevant noise within the tail items, which existing long-tail approaches fail to identify and constrain effectively. To resolve this fundamental conflict, we propose \textbf{HID} (\textbf{H}ybrid \textbf{I}ntent-based \textbf{D}ual Constraint Framework), a plug-and-play framework that transforms the conventional "see-saw" into "win-win" through introducing the hybrid intent-based dual constraints for both long-tail and accuracy. Two key innovations are incorporated in this framework: (i) \textit{Hybrid Intent Learning}, where we reformulate the intent extraction strategies by employing attribute-aware spectral clustering to reconstruct the item-to-intent mapping. Furthermore, discrimination of session-irrelevant noise is achieved through the assignment of the target and noise intents to each session. (ii) \textit{Intent Constraint Loss}, which incorporates two novel constraint paradigms regarding the \textit{diversity} and \textit{accuracy} to regulate the representation learning process of both items and sessions. These two objectives are unified into a single training loss through rigorous theoretical derivation. Extensive experiments across multiple SBR models and datasets demonstrate that HID can enhance both long-tail performance and recommendation accuracy, establishing new state-of-the-art performance in long-tail recommender systems.

</details>


### [110] [Advancing Scientific Knowledge Retrieval and Reuse with a Novel Digital Library for Machine-Readable Knowledge](https://arxiv.org/abs/2511.08476)
*Hadi Ghaemi,Lauren Snyder,Markus Stocker*

Main category: cs.IR

TL;DR: 传统数字图书馆不利于科研知识复用，介绍新兴数字图书馆ORKG reborn及其优势，强调科研知识数据库潜力。


<details>
  <summary>Details</summary>
Motivation: 现有研究用数字图书馆基于文档中心模型，不利于科研知识的机器支持和高效复用。

Method: 介绍ORKG reborn系统，用多领域已发表文章对比展示其信息检索能力。

Result: ORKG reborn能支持科研知识的查找、获取和复用，提供新的检索可能。

Conclusion: 科研知识数据库潜力巨大，且给出了可行的构建方法。

Abstract: Digital libraries for research, such as the ACM Digital Library or Semantic Scholar, do not enable the machine-supported, efficient reuse of scientific knowledge (e.g., in synthesis research). This is because these libraries are based on document-centric models with narrative text knowledge expressions that require manual or semi-automated knowledge extraction, structuring, and organization. We present ORKG reborn, an emerging digital library that supports finding, accessing, and reusing accurate, fine-grained, and reproducible machine-readable expressions of scientific knowledge that relate scientific statements and their supporting evidence in terms of data and code. The rich expressions of scientific knowledge are published as reborn (born-reusable) articles and provide novel possibilities for scientific knowledge retrieval, for instance by statistical methods, software packages, variables, or data matching specific constraints. We describe the proposed system and demonstrate its practical viability and potential for information retrieval in contrast to state-of-the-art digital libraries and document-centric scholarly communication using several published articles in research fields ranging from computer science to soil science. Our work underscores the enormous potential of scientific knowledge databases and a viable approach to their construction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [111] [Counterfactual Forecasting of Human Behavior using Generative AI and Causal Graphs](https://arxiv.org/abs/2511.07484)
*Dharmateja Priyadarshi Uddandarao,Ravi Kiran Vadlamani*

Main category: cs.LG

TL;DR: 提出结合结构因果模型与基于Transformer生成式AI的反事实用户行为预测框架，经测试表现优于传统方法，且有更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 为反事实用户行为预测提供更有效的方法，帮助产品团队在部署前模拟和评估可能的干预措施。

Method: 创建因果图映射用户交互、采用指标和产品特征之间的联系，使用基于因果变量的生成模型生成反事实条件下的行为轨迹。

Result: 在网络交互、移动应用和电子商务数据集上测试，该方法优于传统预测和提升建模技术。

Conclusion: 该框架通过因果路径可视化提高了可解释性，能让产品团队有效模拟和评估可能的干预措施。

Abstract: This study presents a novel framework for counterfactual user behavior forecasting that combines structural causal models with transformer-based generative artificial intelligence. To model fictitious situations, the method creates causal graphs that map the connections between user interactions, adoption metrics, and product features. The framework generates realistic behavioral trajectories under counterfactual conditions by using generative models that are conditioned on causal variables. Tested on datasets from web interactions, mobile applications, and e-commerce, the methodology outperforms conventional forecasting and uplift modeling techniques. Product teams can effectively simulate and assess possible interventions prior to deployment thanks to the framework improved interpretability through causal path visualization.

</details>


### [112] [Optimizing Classification of Infrequent Labels by Reducing Variability in Label Distribution](https://arxiv.org/abs/2511.07459)
*Ashutosh Agarwal*

Main category: cs.LG

TL;DR: 提出LEVER解决极端分类中低频类别表现不佳问题，采用架构提升性能，创建新数据集。


<details>
  <summary>Details</summary>
Motivation: 极端分类中低频类别样本稀疏、标签不一致，影响分类性能。

Method: 采用鲁棒的孪生风格架构，利用知识迁移减少标签不一致，提升一对多分类器性能。

Result: 在多个极端分类数据集测试中，低频类别处理有显著提升，创造新基准；创建两个多意图新数据集。

Conclusion: LEVER能有效解决极端分类中低频类别表现不佳的问题，新数据集为未来研究提供资源。

Abstract: This paper presents a novel solution, LEVER, designed to address the challenges posed by underperforming infrequent categories in Extreme Classification (XC) tasks. Infrequent categories, often characterized by sparse samples, suffer from high label inconsistency, which undermines classification performance. LEVER mitigates this problem by adopting a robust Siamese-style architecture, leveraging knowledge transfer to reduce label inconsistency and enhance the performance of One-vs-All classifiers. Comprehensive testing across multiple XC datasets reveals substantial improvements in the handling of infrequent categories, setting a new benchmark for the field. Additionally, the paper introduces two newly created multi-intent datasets, offering essential resources for future XC research.

</details>


### [113] [Slimmable NAM: Neural Amp Models with adjustable runtime computational cost](https://arxiv.org/abs/2511.07470)
*Steven Atkinson*

Main category: cs.LG

TL;DR: 提出可调整大小和计算成本的可瘦身神经放大器模型，可在精度和计算量间权衡，并进行性能量化和实时演示。


<details>
  <summary>Details</summary>
Motivation: 让音乐家能轻松在模型精度和计算量间进行权衡。

Method: 提出可瘦身神经放大器模型，将其性能与常用基线进行量化对比，开发音频效果插件的实时演示。

Result: 完成了模型性能量化和音频效果插件的实时演示。

Conclusion: 可瘦身神经放大器模型能在无需额外训练和低计算开销下调整大小和计算成本。

Abstract: This work demonstrates "slimmable Neural Amp Models", whose size and computational cost can be changed without additional training and with negligible computational overhead, enabling musicians to easily trade off between the accuracy and compute of the models they are using. The method's performance is quantified against commonly-used baselines, and a real-time demonstration of the model in an audio effect plug-in is developed.

</details>


### [114] [Towards Personalized Quantum Federated Learning for Anomaly Detection](https://arxiv.org/abs/2511.07471)
*Ratun Rahman,Sina Shaham,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 针对量子联邦学习在异常检测中因客户端异质性导致单全局模型训练无效的问题，提出个性化量子联邦学习框架PQFL，实验表明其显著提升异常检测准确性。


<details>
  <summary>Details</summary>
Motivation: 量子联邦学习在实际量子网络中，客户端存在硬件、数据等方面的异质性，导致训练单一全局模型对异常检测无效，需要新方法解决。

Method: 提出个性化量子联邦学习（PQFL）框架，利用参数化量子电路和经典优化器增强本地模型训练，并引入以量子为中心的个性化策略使模型适应客户端硬件和数据。

Result: PQFL在不同现实条件下显著提高异常检测准确性，与现有方法相比，最多降低23%的错误率，AUROC提升24.2%，AUPR提升20.5%。

Conclusion: PQFL在实际量子联邦场景中有效且可扩展。

Abstract: Anomaly detection has a significant impact on applications such as video surveillance, medical diagnostics, and industrial monitoring, where anomalies frequently depend on context and anomaly-labeled data are limited. Quantum federated learning (QFL) overcomes these concerns by distributing model training among several quantum clients, consequently eliminating the requirement for centralized quantum storage and processing. However, in real-life quantum networks, clients frequently differ in terms of hardware capabilities, circuit designs, noise levels, and how classical data is encoded or preprocessed into quantum states. These differences create inherent heterogeneity across clients - not just in their data distributions, but also in their quantum processing behaviors. As a result, training a single global model becomes ineffective, especially when clients handle imbalanced or non-identically distributed (non-IID) data. To address this, we propose a new framework called personalized quantum federated learning (PQFL) for anomaly detection. PQFL enhances local model training at quantum clients using parameterized quantum circuits and classical optimizers, while introducing a quantum-centric personalization strategy that adapts each client's model to its own hardware characteristics and data representation. Extensive experiments show that PQFL significantly improves anomaly detection accuracy under diverse and realistic conditions. Compared to state-of-the-art methods, PQFL reduces false errors by up to 23%, and achieves gains of 24.2% in AUROC and 20.5% in AUPR, highlighting its effectiveness and scalability in practical quantum federated settings.

</details>


### [115] [Multivariate Variational Autoencoder](https://arxiv.org/abs/2511.07472)
*Mehmet Can Yavuz*

Main category: cs.LG

TL;DR: 提出多元变分自编码器MVAE，它能保留高斯可处理性并突破对角后验限制，在多个数据集上表现优于对角协方差VAE，还发布可复现实现。


<details>
  <summary>Details</summary>
Motivation: 为了在保留高斯可处理性的同时，突破对角后验限制，以提升变分自编码器性能。

Method: 提出MVAE，对每个后验协方差进行分解，利用全局耦合矩阵和样本对角尺度。

Result: 在多个数据集上，MVAE在重建、校准和无监督结构方面表现更好，潜平面可视化效果更佳。

Conclusion: MVAE是一种有效的VAE变体，优于对角协方差VAE，发布代码方便比较和复用。

Abstract: We present the Multivariate Variational Autoencoder (MVAE), a VAE variant that preserves Gaussian tractability while lifting the diagonal posterior restriction. MVAE factorizes each posterior covariance, where a \emph{global} coupling matrix $\mathbf{C}$ induces dataset-wide latent correlations and \emph{per-sample} diagonal scales modulate local uncertainty. This yields a full-covariance family with analytic KL and an efficient reparameterization via $\mathbf{L}=\mathbf{C}\mathrm{diag}(\boldsymbolσ)$. Across Larochelle-style MNIST variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, MVAE consistently matches or improves reconstruction (MSE~$\downarrow$) and delivers robust gains in calibration (NLL/Brier/ECE~$\downarrow$) and unsupervised structure (NMI/ARI~$\uparrow$) relative to diagonal-covariance VAEs with matched capacity, especially at mid-range latent sizes. Latent-plane visualizations further indicate smoother, more coherent factor traversals and sharper local detail. We release a fully reproducible implementation with training/evaluation scripts and sweep utilities to facilitate fair comparison and reuse.

</details>


### [116] [RELEAP: Reinforcement-Enhanced Label-Efficient Active Phenotyping for Electronic Health Records](https://arxiv.org/abs/2511.07473)
*Yang Yang,Kathryn Pollak,Bibhas Chakraborty,Molei Liu,Doudou Zhou,Chuan Hong*

Main category: cs.LG

TL;DR: 提出基于强化学习的主动学习框架RELEAP，以预测性能为反馈指导表型校正和样本选择，在肺癌风险预测中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录表型分析依赖噪声代理标签影响下游风险预测可靠性，现有主动学习方法有局限，需开发以预测性能为反馈的框架。

Method: 提出RELEAP框架，自适应整合多种查询策略，根据下游模型反馈更新策略，在杜克大学健康系统队列上用逻辑回归和Cox生存模型评估。

Result: RELEAP始终优于所有基线，逻辑回归AUC从0.774提升到0.805，生存C指数从0.718提升到0.752，比启发式方法增益更平稳稳定。

Conclusion: RELEAP通过下游反馈优化表型校正，提供可扩展、标签高效范式，减少人工审查，增强基于EHR的风险预测可靠性。

Abstract: Objective: Electronic health record (EHR) phenotyping often relies on noisy proxy labels, which undermine the reliability of downstream risk prediction. Active learning can reduce annotation costs, but most rely on fixed heuristics and do not ensure that phenotype refinement improves prediction performance. Our goal was to develop a framework that directly uses downstream prediction performance as feedback to guide phenotype correction and sample selection under constrained labeling budgets.
  Materials and Methods: We propose Reinforcement-Enhanced Label-Efficient Active Phenotyping (RELEAP), a reinforcement learning-based active learning framework. RELEAP adaptively integrates multiple querying strategies and, unlike prior methods, updates its policy based on feedback from downstream models. We evaluated RELEAP on a de-identified Duke University Health System (DUHS) cohort (2014-2024) for incident lung cancer risk prediction, using logistic regression and penalized Cox survival models. Performance was benchmarked against noisy-label baselines and single-strategy active learning.
  Results: RELEAP consistently outperformed all baselines. Logistic AUC increased from 0.774 to 0.805 and survival C-index from 0.718 to 0.752. Using downstream performance as feedback, RELEAP produced smoother and more stable gains than heuristic methods under the same labeling budget.
  Discussion: By linking phenotype refinement to prediction outcomes, RELEAP learns which samples most improve downstream discrimination and calibration, offering a more principled alternative to fixed active learning rules.
  Conclusion: RELEAP optimizes phenotype correction through downstream feedback, offering a scalable, label-efficient paradigm that reduces manual chart review and enhances the reliability of EHR-based risk prediction.

</details>


### [117] [Comparing Reconstruction Attacks on Pretrained Versus Full Fine-tuned Large Language Model Embeddings on Homo Sapiens Splice Sites Genomic Data](https://arxiv.org/abs/2511.07481)
*Reem Al-Saidi,Erman Ayday,Ziad Kobti*

Main category: cs.LG

TL;DR: 研究大语言模型应用于基因组序列时嵌入重建攻击，对比预训练和微调嵌入的重建脆弱性，发现微调增强了对重建攻击的抗性。


<details>
  <summary>Details</summary>
Motivation: 基于前人研究，探究微调对大语言模型在基因组序列嵌入重建攻击中的脆弱性的影响，明确任务特定优化对隐私保护的作用。

Method: 将重建攻击管道应用于预训练和微调模型嵌入；针对DNA序列实施专门的分词机制；对预训练和微调嵌入进行位置、核苷酸类型和隐私变化的详细对比分析。

Result: 预训练和微调嵌入在重建脆弱性上有明显区别，微调增强了XLNet、GPT - 2和BERT等架构对重建攻击的抗性。

Conclusion: 处理敏感基因组数据的语言模型需要先进保护机制，微调作为增强隐私的技术值得进一步探索。

Abstract: This study investigates embedding reconstruction attacks in large language models (LLMs) applied to genomic sequences, with a specific focus on how fine-tuning affects vulnerability to these attacks. Building upon Pan et al.'s seminal work demonstrating that embeddings from pretrained language models can leak sensitive information, we conduct a comprehensive analysis using the HS3D genomic dataset to determine whether task-specific optimization strengthens or weakens privacy protections. Our research extends Pan et al.'s work in three significant dimensions. First, we apply their reconstruction attack pipeline to pretrained and fine-tuned model embeddings, addressing a critical gap in their methodology that did not specify embedding types. Second, we implement specialized tokenization mechanisms tailored specifically for DNA sequences, enhancing the model's ability to process genomic data, as these models are pretrained on natural language and not DNA. Third, we perform a detailed comparative analysis examining position-specific, nucleotide-type, and privacy changes between pretrained and fine-tuned embeddings. We assess embeddings vulnerabilities across different types and dimensions, providing deeper insights into how task adaptation shifts privacy risks throughout genomic sequences. Our findings show a clear distinction in reconstruction vulnerability between pretrained and fine-tuned embeddings. Notably, fine-tuning strengthens resistance to reconstruction attacks in multiple architectures -- XLNet (+19.8\%), GPT-2 (+9.8\%), and BERT (+7.8\%) -- pointing to task-specific optimization as a potential privacy enhancement mechanism. These results highlight the need for advanced protective mechanisms for language models processing sensitive genomic data, while highlighting fine-tuning as a potential privacy-enhancing technique worth further exploration.

</details>


### [118] [Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits](https://arxiv.org/abs/2511.07482)
*Dev Patel,Gabrielle Gervacio,Diekola Raimi,Kevin Zhu,Ryan Lagasse,Gabriel Grand,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 提出AAPP动态结构化剪枝方法，实验显示能提升拒绝率，实现高效且安全的大模型部署。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理需大量计算资源，动态剪枝虽高效但会加剧对齐退化，需解决对齐漏洞问题。

Method: 基于Probe Pruning，提出Alignment - Aware Probe Pruning (AAPP)动态结构化剪枝方法，在推理时自适应保留与对齐相关的电路。

Result: 在LLaMA 2 - 7B、Qwen2.5 - 14B - Instruct和Gemma - 3 - 12B - IT上实验，AAPP在相同计算量下使拒绝率提高50%。

Conclusion: AAPP能实现高效且安全的大语言模型部署。

Abstract: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\% at matched compute, enabling efficient yet safety-preserving LLM deployment.

</details>


### [119] [Deep (Predictive) Discounted Counterfactual Regret Minimization](https://arxiv.org/abs/2511.08174)
*Hang Xu,Kai Li,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: 提出高效无模型神经CFR算法，克服现有方法局限，实验显示其在典型游戏中收敛更快、对抗性更强。


<details>
  <summary>Details</summary>
Motivation: 现有基于香草CFR的方法难以有效整合更高级的CFR变体，为增强CFR在大型游戏中的适用性。

Method: 提出高效无模型神经CFR算法，每次迭代收集基于价值网络的方差缩减采样优势，通过自举拟合累积优势，并应用折扣和裁剪操作模拟高级CFR变体的更新机制。

Result: 与无模型神经算法相比，在典型不完全信息游戏中收敛更快，在大型扑克游戏中对抗性能更强。

Conclusion: 所提出的算法克服了现有方法在近似高级CFR变体方面的局限性，具有更好的性能。

Abstract: Counterfactual regret minimization (CFR) is a family of algorithms for effectively solving imperfect-information games. To enhance CFR's applicability in large games, researchers use neural networks to approximate its behavior. However, existing methods are mainly based on vanilla CFR and struggle to effectively integrate more advanced CFR variants. In this work, we propose an efficient model-free neural CFR algorithm, overcoming the limitations of existing methods in approximating advanced CFR variants. At each iteration, it collects variance-reduced sampled advantages based on a value network, fits cumulative advantages by bootstrapping, and applies discounting and clipping operations to simulate the update mechanisms of advanced CFR variants. Experimental results show that, compared with model-free neural algorithms, it exhibits faster convergence in typical imperfect-information games and demonstrates stronger adversarial performance in a large poker game.

</details>


### [120] [When Are Learning Biases Equivalent? A Unifying Framework for Fairness, Robustness, and Distribution Shift](https://arxiv.org/abs/2511.07485)
*Sushant Mehta*

Main category: cs.LG

TL;DR: 提出统一理论框架表征不同偏差机制对模型性能产生等效影响的条件，经实证验证，能跨领域转移去偏方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统有多种故障模式，不同研究群体通常孤立研究，需统一理论框架。

Method: 通过信息论度量将偏差形式化为条件独立性的违反，证明不同偏差机制的形式等价条件。

Result: 理论预测特定强度的虚假相关性与子群体不平衡比率对最差组准确率有等效影响，六个数据集和三种架构的实证验证预测等效性在最差组准确率3%内成立。

Conclusion: 这项工作从共同视角连接了公平性、鲁棒性和分布偏移的相关文献。

Abstract: Machine learning systems exhibit diverse failure modes: unfairness toward protected groups, brittleness to spurious correlations, poor performance on minority sub-populations, which are typically studied in isolation by distinct research communities. We propose a unifying theoretical framework that characterizes when different bias mechanisms produce quantitatively equivalent effects on model performance. By formalizing biases as violations of conditional independence through information-theoretic measures, we prove formal equivalence conditions relating spurious correlations, subpopulation shift, class imbalance, and fairness violations. Our theory predicts that a spurious correlation of strength $α$ produces equivalent worst-group accuracy degradation as a sub-population imbalance ratio $r \approx (1+α)/(1-α)$ under feature overlap assumptions. Empirical validation in six datasets and three architectures confirms that predicted equivalences hold within the accuracy of the worst group 3\%, enabling the principled transfer of debiasing methods across problem domains. This work bridges the literature on fairness, robustness, and distribution shifts under a common perspective.

</details>


### [121] [Provably Efficient Sample Complexity for Robust CMDP](https://arxiv.org/abs/2511.07486)
*Sourav Ganguly,Arnob Ghosh*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of learning policies that maximize cumulative reward while satisfying safety constraints, even when the real environment differs from a simulator or nominal model. We focus on robust constrained Markov decision processes (RCMDPs), where the agent must maximize reward while ensuring cumulative utility exceeds a threshold under the worst-case dynamics within an uncertainty set. While recent works have established finite-time iteration complexity guarantees for RCMDPs using policy optimization, their sample complexity guarantees remain largely unexplored. In this paper, we first show that Markovian policies may fail to be optimal even under rectangular uncertainty sets unlike the {\em unconstrained} robust MDP. To address this, we introduce an augmented state space that incorporates the remaining utility budget into the state representation. Building on this formulation, we propose a novel Robust constrained Value iteration (RCVI) algorithm with a sample complexity of $\mathcal{\tilde{O}}(|S||A|H^5/ε^2)$ achieving at most $ε$ violation using a generative model where $|S|$ and $|A|$ denote the sizes of the state and action spaces, respectively, and $H$ is the episode length. To the best of our knowledge, this is the {\em first sample complexity guarantee} for RCMDP. Empirical results further validate the effectiveness of our approach.

</details>


### [122] [Methodological Precedence in Health Tech: Why ML/Big Data Analysis Must Follow Basic Epidemiological Consistency. A Case Study](https://arxiv.org/abs/2511.07500)
*Marco Roccetti*

Main category: cs.LG

TL;DR: 先进分析工具革新健康研究，但依赖数据质量，以疫苗研究为例揭示基础方法一致性重要性，强调因果推断需稳健方法。


<details>
  <summary>Details</summary>
Motivation: 指出先进分析方法依赖数据质量和统计设计，强调基础方法一致性的重要性。

Method: 对疫苗结果和精神事件的队列研究，运用简单描述性统计方法和国家流行病学基准。

Result: 发现研究存在统计矛盾，报告的风险比无效，效应源于选择偏差。

Conclusion: 复杂健康研究需先通过基础流行病学一致性检验，无随机化时需稳健方法实现有效因果推断。

Abstract: The integration of advanced analytical tools, including Machine Learning (ML) and massive data processing, has revolutionized health research, promising unprecedented accuracy in diagnosis and risk prediction. However, the rigor of these complex methods is fundamentally dependent on the quality and integrity of the underlying datasets and the validity of their statistical design. We propose an emblematic case where advanced analysis (ML/Big Data) must necessarily be subsequent to the verification of basic methodological coherence. This study highlights a crucial cautionary principle: sophisticated analyses amplify, rather than correct, severe methodological flaws rooted in basic design choices, leading to misleading or contradictory findings. By applying simple, standard descriptive statistical methods and established national epidemiological benchmarks to a recently published cohort study on vaccine outcomes and psychiatric events, we expose multiple, statistically irreconcilable paradoxes. These paradoxes, including an implausible risk reduction for a chronic disorder in a high-risk group and contradictory incidence rate comparisons, definitively invalidate the reported hazard ratios (HRs). We demonstrate that the observed effects are mathematical artifacts stemming from an uncorrected selection bias in the cohort construction. This analysis serves as a robust reminder that even the most complex health studies must first pass the test of basic epidemiological consistency before any conclusion drawn from subsequent advanced ML or statistical modeling can be considered valid or publishable. We conclude that robust methods, such as Propensity Score Matching, are essential for achieving valid causal inference from administrative data in the absence of randomization

</details>


### [123] [N-ReLU: Zero-Mean Stochastic Extension of ReLU](https://arxiv.org/abs/2511.07559)
*Md Motaleb Hossen Manik,Md Zabirul Islam,Ge Wang*

Main category: cs.LG

TL;DR: 引入N - ReLU解决ReLU死神经元问题，实验显示其在MNIST数据集上效果良好，噪声注入可增强优化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 标准ReLU存在死神经元问题，需要改进激活函数。

Method: 引入N - ReLU，用高斯噪声替代负激活值并保持期望输出一致。

Result: 在MNIST数据集上，N - ReLU在适度噪声水平下准确率与其他激活函数相当或略高，收敛稳定且无死神经元。

Conclusion: 轻量级高斯噪声注入是一种简单有效的增强优化鲁棒性的机制。

Abstract: Activation functions are fundamental for enabling nonlinear representations in deep neural networks. However, the standard rectified linear unit (ReLU) often suffers from inactive or "dead" neurons caused by its hard zero cutoff. To address this issue, we introduce N-ReLU (Noise-ReLU), a zero-mean stochastic extension of ReLU that replaces negative activations with Gaussian noise while preserving the same expected output. This expectation-aligned formulation maintains gradient flow in inactive regions and acts as an annealing-style regularizer during training. Experiments on the MNIST dataset using both multilayer perceptron (MLP) and convolutional neural network (CNN) architectures show that N-ReLU achieves accuracy comparable to or slightly exceeding that of ReLU, LeakyReLU, PReLU, GELU, and RReLU at moderate noise levels (sigma = 0.05-0.10), with stable convergence and no dead neurons observed. These results demonstrate that lightweight Gaussian noise injection offers a simple yet effective mechanism to enhance optimization robustness without modifying network structures or introducing additional parameters.

</details>


### [124] [LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows](https://arxiv.org/abs/2511.07585)
*Raffi Khatchadourian,Rolando Franco*

Main category: cs.LG

TL;DR: 研究金融机构部署大语言模型时输出漂移问题，量化不同模型架构漂移情况，提出多项贡献并评估模型，展示合规部署途径。


<details>
  <summary>Details</summary>
Motivation: 金融机构使用大语言模型时，非确定性输出（输出漂移）影响可审计性和信任，需解决此问题。

Method: 使用金融校准的确定性测试工具，进行任务特定不变性检查，采用三层模型分类系统和审计就绪证明系统；对五个模型在三个受监管金融任务上进行评估。

Result: 小模型输出一致性高，大模型低；结构化任务稳定，RAG任务有漂移；跨供应商验证表明确定性行为可在本地和云部署间转移。

Conclusion: 挑战大模型普遍更适合生产部署的传统假设，框架能满足相关机构要求，为合规AI部署提供实际途径。

Abstract: Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.
  Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.
  We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.

</details>


### [125] [SCALAR: Benchmarking SAE Interaction Sparsity in Toy LLMs](https://arxiv.org/abs/2511.07572)
*Sean P. Fillingham,Andrew Gordon,Peter Lai,Xavier Poncini,David Quarel,Stefan Heimersheim*

Main category: cs.LG

TL;DR: 本文引入SCALAR基准评估SAE特征间交互稀疏性，提出Staircase SAEs结构，对比多种SAEs，发现Staircase SAEs在提升稀疏性和保持特征可解释性上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有SAEs孤立训练不鼓励跨层稀疏连接，且当前评估未关注交互稀疏性。

Method: 引入SCALAR基准，提出Staircase SAEs结构，用其对比TopK SAEs、JSAEs。

Result: Staircase SAEs比TopK SAEs在稀疏性上有显著提升，JSAEs在部分层有提升但在transformer块训练效果不佳。

Conclusion: 通过基准测试和架构对比，强调了SAEs中交互稀疏性的重要性。

Abstract: Mechanistic interpretability aims to decompose neural networks into interpretable features and map their connecting circuits. The standard approach trains sparse autoencoders (SAEs) on each layer's activations. However, SAEs trained in isolation don't encourage sparse cross-layer connections, inflating extracted circuits where upstream features needlessly affect multiple downstream features. Current evaluations focus on individual SAE performance, leaving interaction sparsity unexamined. We introduce SCALAR (Sparse Connectivity Assessment of Latent Activation Relationships), a benchmark measuring interaction sparsity between SAE features. We also propose "Staircase SAEs", using weight-sharing to limit upstream feature duplication across downstream features. Using SCALAR, we compare TopK SAEs, Jacobian SAEs (JSAEs), and Staircase SAEs. Staircase SAEs improve relative sparsity over TopK SAEs by $59.67\% \pm 1.83\%$ (feedforward) and $63.15\% \pm 1.35\%$ (transformer blocks). JSAEs provide $8.54\% \pm 0.38\%$ improvement over TopK for feedforward layers but cannot train effectively across transformer blocks, unlike Staircase and TopK SAEs which work anywhere in the residual stream. We validate on a $216$K-parameter toy model and GPT-$2$ Small ($124$M), where Staircase SAEs maintain interaction sparsity improvements while preserving feature interpretability. Our work highlights the importance of interaction sparsity in SAEs through benchmarking and comparing promising architectures.

</details>


### [126] [Online Linear Regression with Paid Stochastic Features](https://arxiv.org/abs/2511.08073)
*Nadav Merlis,Kyoungseok Jang,Nicolò Cesa-Bianchi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study an online linear regression setting in which the observed feature vectors are corrupted by noise and the learner can pay to reduce the noise level. In practice, this may happen for several reasons: for example, because features can be measured more accurately using more expensive equipment, or because data providers can be incentivized to release less private features. Assuming feature vectors are drawn i.i.d. from a fixed but unknown distribution, we measure the learner's regret against the linear predictor minimizing a notion of loss that combines the prediction error and payment. When the mapping between payments and noise covariance is known, we prove that the rate $\sqrt{T}$ is optimal for regret if logarithmic factors are ignored. When the noise covariance is unknown, we show that the optimal regret rate becomes of order $T^{2/3}$ (ignoring log factors). Our analysis leverages matrix martingale concentration, showing that the empirical loss uniformly converges to the expected one for all payments and linear predictors.

</details>


### [127] [One Router to Route Them All: Homogeneous Expert Routing for Heterogeneous Graph Transformers](https://arxiv.org/abs/2511.07603)
*Georgiy Shakirov,Albert Arakelov*

Main category: cs.LG

TL;DR: 本文探索将MoE集成到HGNNs中，提出HER层，在多个数据集上表现出色，证明正则化类型依赖可产生更好表示。


<details>
  <summary>Details</summary>
Motivation: 现有HGNNs过度依赖节点/边类型标签，阻碍跨类型知识转移，且MoE在HGNNs中的应用未充分探索。

Method: 提出Homogeneous Expert Routing (HER)，在路由时随机掩码类型嵌入以鼓励类型无关的专业化。

Result: 在IMDB、ACM和DBLP上进行链接预测，HER始终优于标准HGT和类型分离的MoE基线，专家按语义模式专业化。

Conclusion: 正则化专家路由中的类型依赖可产生更通用、高效和可解释的表示，是异质图学习的新设计原则。

Abstract: A common practice in heterogeneous graph neural networks (HGNNs) is to condition parameters on node/edge types, assuming types reflect semantic roles. However, this can cause overreliance on surface-level labels and impede cross-type knowledge transfer. We explore integrating Mixture-of-Experts (MoE) into HGNNs--a direction underexplored despite MoE's success in homogeneous settings. Crucially, we question the need for type-specific experts. We propose Homogeneous Expert Routing (HER), an MoE layer for Heterogeneous Graph Transformers (HGT) that stochastically masks type embeddings during routing to encourage type-agnostic specialization. Evaluated on IMDB, ACM, and DBLP for link prediction, HER consistently outperforms standard HGT and a type-separated MoE baseline. Analysis on IMDB shows HER experts specialize by semantic patterns (e.g., movie genres) rather than node types, confirming routing is driven by latent semantics. Our work demonstrates that regularizing type dependence in expert routing yields more generalizable, efficient, and interpretable representations--a new design principle for heterogeneous graph learning.

</details>


### [128] [Partial Action Replacement: Tackling Distribution Shift in Offline MARL](https://arxiv.org/abs/2511.07629)
*Yue Jin,Giovanni Montana*

Main category: cs.LG

TL;DR: 提出Soft - Partial Conservative Q - Learning (SPaCQL)方法缓解离线多智能体强化学习中评估分布外联合动作的挑战，有理论证明和实证优势。


<details>
  <summary>Details</summary>
Motivation: 解决离线多智能体强化学习中评估分布外联合动作的难题。

Method: 提出部分动作替换（PAR）策略，基于此开发SPaCQL方法，用PAR缓解OOD问题，根据价值估计的不确定性动态加权不同PAR策略。

Result: 理论上证明诱导的分布偏移与偏离智能体数量线性相关，有更紧的值误差界；实证表明SPaCQL能实现更有效的策略学习，在离线数据集有独立结构时优于基线算法。

Conclusion: SPaCQL方法能有效缓解离线多智能体强化学习中评估分布外联合动作的挑战。

Abstract: Offline multi-agent reinforcement learning (MARL) is severely hampered by the challenge of evaluating out-of-distribution (OOD) joint actions. Our core finding is that when the behavior policy is factorized - a common scenario where agents act fully or partially independently during data collection - a strategy of partial action replacement (PAR) can significantly mitigate this challenge. PAR updates a single or part of agents' actions while the others remain fixed to the behavioral data, reducing distribution shift compared to full joint-action updates. Based on this insight, we develop Soft-Partial Conservative Q-Learning (SPaCQL), using PAR to mitigate OOD issue and dynamically weighting different PAR strategies based on the uncertainty of value estimation. We provide a rigorous theoretical foundation for this approach, proving that under factorized behavior policies, the induced distribution shift scales linearly with the number of deviating agents rather than exponentially with the joint-action space. This yields a provably tighter value error bound for this important class of offline MARL problems. Our theoretical results also indicate that SPaCQL adaptively addresses distribution shift using uncertainty-informed weights. Our empirical results demonstrate SPaCQL enables more effective policy learning, and manifest its remarkable superiority over baseline algorithms when the offline dataset exhibits the independence structure.

</details>


### [129] [SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories](https://arxiv.org/abs/2511.08136)
*Returaj Burnwal,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 研究离线安全模仿学习问题，提出SafeMIL方法，实验证明能学习更安全策略且不降低奖励性能。


<details>
  <summary>Details</summary>
Motivation: 现实中在线交互有风险，准确指定奖励和安全成本信息困难，但可收集非偏好轨迹，标准模仿学习需改进以避免风险行为。

Method: 提出SafeMIL方法，通过多实例学习预测状态 - 动作对是否有风险，用学习到的成本避免非偏好行为。

Result: 实验表明该方法能学习满足成本约束的更安全策略，且不降低奖励性能，优于多个基线方法。

Conclusion: SafeMIL方法在离线安全模仿学习中有效，能平衡安全与奖励。

Abstract: In this work, we study the problem of offline safe imitation learning (IL). In many real-world settings, online interactions can be risky, and accurately specifying the reward and the safety cost information at each timestep can be difficult. However, it is often feasible to collect trajectories reflecting undesirable or risky behavior, implicitly conveying the behavior the agent should avoid. We refer to these trajectories as non-preferred trajectories. Unlike standard IL, which aims to mimic demonstrations, our agent must also learn to avoid risky behavior using non-preferred trajectories. In this paper, we propose a novel approach, SafeMIL, to learn a parameterized cost that predicts if the state-action pair is risky via \textit{Multiple Instance Learning}. The learned cost is then used to avoid non-preferred behaviors, resulting in a policy that prioritizes safety. We empirically demonstrate that our approach can learn a safer policy that satisfies cost constraints without degrading the reward performance, thereby outperforming several baselines.

</details>


### [130] [FlowTIE: Flow-based Transport of Intensity Equation for Phase Gradient Estimation from 4D-STEM Data](https://arxiv.org/abs/2511.07633)
*Arya Bangun,Maximilian Töllner,Xuan Zhao,Christian Kübel,Hanno Scharr*

Main category: cs.LG

TL;DR: 介绍基于神经网络的FlowTIE框架用于4D - STEM数据相位重建，验证显示其有优势。


<details>
  <summary>Details</summary>
Motivation: 解决厚样品在动态散射条件下相位重建的问题，结合数据驱动学习和基于物理的先验知识以提高鲁棒性。

Method: 将传输强度方程（TIE）与基于流的相位梯度表示相结合的神经网络框架FlowTIE。

Result: 在模拟数据集验证中，FlowTIE提高了相位重建精度、速度快，且能与厚样品模型集成。

Conclusion: FlowTIE在相位重建方面有良好表现，适用于厚样品。

Abstract: We introduce FlowTIE, a neural-network-based framework for phase reconstruction from 4D-Scanning Transmission Electron Microscopy (STEM) data, which integrates the Transport of Intensity Equation (TIE) with a flow-based representation of the phase gradient. This formulation allows the model to bridge data-driven learning with physics-based priors, improving robustness under dynamical scattering conditions for thick specimen. The validation on simulated datasets of crystalline materials, benchmarking to classical TIE and gradient-based optimization methods are presented. The results demonstrate that FlowTIE improves phase reconstruction accuracy, fast, and can be integrated with a thick specimen model, namely multislice method.

</details>


### [131] [Private-RAG: Answering Multiple Queries with LLMs while Keeping Your Data Private](https://arxiv.org/abs/2511.07637)
*Ruihan Wu,Erchi Wang,Zhiyuan Zhang,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving documents from an external corpus at inference time. When this corpus contains sensitive information, however, unprotected RAG systems are at risk of leaking private information. Prior work has introduced differential privacy (DP) guarantees for RAG, but only in single-query settings, which fall short of realistic usage. In this paper, we study the more practical multi-query setting and propose two DP-RAG algorithms. The first, MURAG, leverages an individual privacy filter so that the accumulated privacy loss only depends on how frequently each document is retrieved rather than the total number of queries. The second, MURAG-ADA, further improves utility by privately releasing query-specific thresholds, enabling more precise selection of relevant documents. Our experiments across multiple LLMs and datasets demonstrate that the proposed methods scale to hundreds of queries within a practical DP budget ($\varepsilon\approx10$), while preserving meaningful utility.

</details>


### [132] [LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics](https://arxiv.org/abs/2511.08544)
*Randall Balestriero,Yann LeCun*

Main category: cs.LG

TL;DR: 提出LeJEPA，结合JEPA预测损失和SIGReg，有诸多理论和实践优势，经多数据集和架构验证。


<details>
  <summary>Details</summary>
Motivation: JEPAs缺乏实用指导和理论，导致临时研发，需提出全面理论和训练目标。

Method: 确定JEPAs嵌入应遵循的最优分布，引入SIGReg约束嵌入，结合JEPA预测损失得到LeJEPA。

Result: 经10多个数据集、60多种架构验证，如在imagenet - 1k上用ViT - H/14达到79%。

Conclusion: LeJEPA的简单性和理论友好性有望使自监督预训练重回AI研究核心。

Abstract: Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&D. We present a comprehensive theory of JEPAs and instantiate it in {\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\href{git@github.com:rbalestr-lab/lejepa.git}{GitHub repo}).

</details>


### [133] [Adaptive Graph Learning with Transformer for Multi-Reservoir Inflow Prediction](https://arxiv.org/abs/2511.07649)
*Pengfei Hu,Ming Fan,Xiaoxue Han,Chang Lu,Wei Zhang,Hyun Kang,Yue Ning,Dan Lu*

Main category: cs.LG

TL;DR: 提出AdaTrip框架用于多水库入流预测，在科罗拉多河上游流域30个水库评估中表现优于现有基线，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有水库入流预测方法多为单水库模型，忽略了相互连接水库间的空间依赖关系。

Method: 引入AdaTrip自适应时变图学习框架，构建动态图，利用注意力机制自动识别关键时空依赖关系。

Result: 在科罗拉多河上游流域30个水库评估中优于现有基线，通过参数共享提升记录有限水库的性能，还能提供可解释的注意力图。

Conclusion: AdaTrip框架在多水库入流预测方面表现良好，可提供水文控制见解以支持运营决策。

Abstract: Reservoir inflow prediction is crucial for water resource management, yet existing approaches mainly focus on single-reservoir models that ignore spatial dependencies among interconnected reservoirs. We introduce AdaTrip as an adaptive, time-varying graph learning framework for multi-reservoir inflow forecasting. AdaTrip constructs dynamic graphs where reservoirs are nodes with directed edges reflecting hydrological connections, employing attention mechanisms to automatically identify crucial spatial and temporal dependencies. Evaluation on thirty reservoirs in the Upper Colorado River Basin demonstrates superiority over existing baselines, with improved performance for reservoirs with limited records through parameter sharing. Additionally, AdaTrip provides interpretable attention maps at edge and time-step levels, offering insights into hydrological controls to support operational decision-making. Our code is available at https://github.com/humphreyhuu/AdaTrip.

</details>


### [134] [BIPPO: Budget-Aware Independent PPO for Energy-Efficient Federated Learning Services](https://arxiv.org/abs/2511.08142)
*Anna Lackinger,Andrea Morichetta,Pantelis A. Frangoudis,Schahram Dustdar*

Main category: cs.LG

TL;DR: 本文提出BIPPO用于物联网联邦学习的客户端选择，在预算受限场景下评估，比其他方法有更好的准确率和能耗表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的联邦学习客户端选择方案未考虑基础设施挑战、训练缺乏实用性，不具备泛化性和能源效率。

Method: 提出预算感知的独立近端策略优化（BIPPO）这一节能的多智能体强化学习解决方案。

Result: 在高度预算受限的两个图像分类任务中，BIPPO比非RL机制、传统PPO和IPPO提高了平均准确率，且仅消耗可忽略的预算，客户端数量增加时也保持稳定。

Conclusion: BIPPO为物联网联邦学习的客户端选择提供了高效、稳定、可扩展和可持续的解决方案。

Abstract: Federated Learning (FL) is a promising machine learning solution in large-scale IoT systems, guaranteeing load distribution and privacy. However, FL does not natively consider infrastructure efficiency, a critical concern for systems operating in resource-constrained environments. Several Reinforcement Learning (RL) based solutions offer improved client selection for FL; however, they do not consider infrastructure challenges, such as resource limitations and device churn. Furthermore, the training of RL methods is often not designed for practical application, as these approaches frequently do not consider generalizability and are not optimized for energy efficiency. To fill this gap, we propose BIPPO (Budget-aware Independent Proximal Policy Optimization), which is an energy-efficient multi-agent RL solution that improves performance. We evaluate BIPPO on two image classification tasks run in a highly budget-constrained setting, with FL clients training on non-IID data, a challenging context for vanilla FL. The improved sampler of BIPPO enables it to increase the mean accuracy compared to non-RL mechanisms, traditional PPO, and IPPO. In addition, BIPPO only consumes a negligible proportion of the budget, which stays consistent even if the number of clients increases. Overall, BIPPO delivers a performant, stable, scalable, and sustainable solution for client selection in IoT-FL.

</details>


### [135] [Enhancing Binary Encoded Crime Linkage Analysis Using Siamese Network](https://arxiv.org/abs/2511.07651)
*Yicheng Zhan,Fahim Ahmed,Amy Burrell,Matthew J. Tonkin,Sarah Galambos,Jessica Woodhams,Dalal Alrajeh*

Main category: cs.LG

TL;DR: 本文提出暹罗自编码器框架处理犯罪数据，结合地理时间特征，分析数据降维策略对模型性能影响，结果显示该方法能提升犯罪关联分析准确率。


<details>
  <summary>Details</summary>
Motivation: 解决传统犯罪关联方法在处理高维、稀疏和异构数据方面的局限性，提升犯罪关联分析效果以识别连环罪犯和增强公共安全。

Method: 提出暹罗自编码器框架学习潜在表示，利用ViCLAS数据，在解码器阶段整合地理 - 时间特征，分析不同数据降维策略。

Result: 该方法在多个评估指标上有持续改进，相比传统方法AUC最多提高9%。

Conclusion: 先进的机器学习方法能大幅提高犯罪关联分析的准确性，并为调查决策提供可解释的见解。

Abstract: Effective crime linkage analysis is crucial for identifying serial offenders and enhancing public safety. To address limitations of traditional crime linkage methods in handling high-dimensional, sparse, and heterogeneous data, we propose a Siamese Autoencoder framework that learns meaningful latent representations and uncovers correlations in complex crime data. Using data from the Violent Crime Linkage Analysis System (ViCLAS), maintained by the Serious Crime Analysis Section of the UK's National Crime Agency, our approach mitigates signal dilution in sparse feature spaces by integrating geographic-temporal features at the decoder stage. This design amplifies behavioral representations rather than allowing them to be overshadowed at the input level, yielding consistent improvements across multiple evaluation metrics. We further analyze how different domain-informed data reduction strategies influence model performance, providing practical guidance for preprocessing in crime linkage contexts. Our results show that advanced machine learning approaches can substantially enhance linkage accuracy, improving AUC by up to 9% over traditional methods while offering interpretable insights to support investigative decision-making.

</details>


### [136] [CAE: Character-Level Autoencoder for Non-Semantic Relational Data Grouping](https://arxiv.org/abs/2511.07657)
*Veera V S Bhargav Nunna,Shinae Kang,Zheyuan Zhou,Virginia Wang,Sucharitha Boinapally,Michael Foley*

Main category: cs.LG

TL;DR: 本文提出字符级自动编码器（CAE）方法，能识别和分组非语义关系数据集中语义相同列，实验表现优于传统NLP方法，为企业数据管理提供自动化解决方案。


<details>
  <summary>Details</summary>
Motivation: 企业关系数据库中大量非语义数据挑战传统语义分析，传统NLP模型存在语义解释性和处理未登录词的局限。

Method: 提出CAE方法，在字符层面基于数据模式和结构检测列相似性，编码非语义关系表列的文本表示并提取高维特征嵌入进行数据分组，保持固定字典大小。

Result: CAE方法在关系数据集的前5列匹配任务中准确率达80.95%，远超传统NLP方法如词袋法（47.62%）。

Conclusion: 该方法有效识别和聚类关系数据集中相同列，弥合字符级神经架构理论进展与企业数据管理实践挑战间的差距，为非语义工业数据集的模式理解和数据剖析提供自动化解决方案。

Abstract: Enterprise relational databases increasingly contain vast amounts of non-semantic data - IP addresses, product identifiers, encoded keys, and timestamps - that challenge traditional semantic analysis. This paper introduces a novel Character-Level Autoencoder (CAE) approach that automatically identifies and groups semantically identical columns in non-semantic relational datasets by detecting column similarities based on data patterns and structures. Unlike conventional Natural Language Processing (NLP) models that struggle with limitations in semantic interpretability and out-of-vocabulary tokens, our approach operates at the character level with fixed dictionary constraints, enabling scalable processing of large-scale data lakes and warehouses. The CAE architecture encodes text representations of non-semantic relational table columns and extracts high-dimensional feature embeddings for data grouping. By maintaining a fixed dictionary size, our method significantly reduces both memory requirements and training time, enabling efficient processing of large-scale industrial data environments. Experimental evaluation demonstrates substantial performance gains: our CAE approach achieved 80.95% accuracy in top 5 column matching tasks across relational datasets, substantially outperforming traditional NLP approaches such as Bag of Words (47.62%). These results demonstrate its effectiveness for identifying and clustering identical columns in relational datasets. This work bridges the gap between theoretical advances in character-level neural architectures and practical enterprise data management challenges, providing an automated solution for schema understanding and data profiling of non-semantic industrial datasets at scale.

</details>


### [137] [ZeroSim: Zero-Shot Analog Circuit Evaluation with Unified Transformer Embeddings](https://arxiv.org/abs/2511.07658)
*Xiaomeng Yang,Jian Gao,Yanzhi Wang,Xuan Zhang*

Main category: cs.LG

TL;DR: 提出ZeroSim框架用于模拟电路性能建模，具泛化能力，实验表现优且集成到优化流程可加速。


<details>
  <summary>Details</summary>
Motivation: 现有模拟电路设计自动化中性能评估是瓶颈，传统SPICE慢，机器学习方法可扩展性和适应性差。

Method: 提出ZeroSim框架，采用多样化训练语料、统一拓扑嵌入和拓扑条件参数映射策略。

Result: ZeroSim显著优于基线模型，能准确零样本预测，集成到优化流程比SPICE快13倍。

Conclusion: ZeroSim有实用价值，可用于多种模拟电路设计自动化任务。

Abstract: Although recent advancements in learning-based analog circuit design automation have tackled tasks such as topology generation, device sizing, and layout synthesis, efficient performance evaluation remains a major bottleneck. Traditional SPICE simulations are time-consuming, while existing machine learning methods often require topology-specific retraining or manual substructure segmentation for fine-tuning, hindering scalability and adaptability. In this work, we propose ZeroSim, a transformer-based performance modeling framework designed to achieve robust in-distribution generalization across trained topologies under novel parameter configurations and zero-shot generalization to unseen topologies without any fine-tuning. We apply three key enabling strategies: (1) a diverse training corpus of 3.6 million instances covering over 60 amplifier topologies, (2) unified topology embeddings leveraging global-aware tokens and hierarchical attention to robustly generalize to novel circuits, and (3) a topology-conditioned parameter mapping approach that maintains consistent structural representations independent of parameter variations. Our experimental results demonstrate that ZeroSim significantly outperforms baseline models such as multilayer perceptrons, graph neural networks and transformers, delivering accurate zero-shot predictions across different amplifier topologies. Additionally, when integrated into a reinforcement learning-based parameter optimization pipeline, ZeroSim achieves a remarkable speedup (13x) compared to conventional SPICE simulations, underscoring its practical value for a wide range of analog circuit design automation tasks.

</details>


### [138] [Probabilities Are All You Need: A Probability-Only Approach to Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2511.07694)
*Manh Nguyen,Sunil Gupta,Hung Le*

Main category: cs.LG

TL;DR: 本文提出一种高效、免训练的大语言模型不确定性估计方法，用响应的前K个概率近似预测熵，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易产生幻觉问题，现有不确定性估计方法需多个样本或额外计算来评估语义熵。

Method: 提出用响应的前K个概率近似预测熵的方法，并采用自适应机制确定K以增强灵活性和过滤低置信度概率。

Result: 在三个自由问答数据集和多个大语言模型上的实验表明，该方法优于昂贵的现有基线。

Conclusion: 该方法有助于提高大语言模型的可信度。

Abstract: Large Language Models (LLMs) exhibit strong performance across various natural language processing (NLP) tasks but remain vulnerable to hallucinations, generating factually incorrect or misleading outputs. Uncertainty estimation, often using predictive entropy estimation, is key to addressing this issue. However, existing methods often require multiple samples or extra computation to assess semantic entropy. This paper proposes an efficient, training-free uncertainty estimation method that approximates predictive entropy using the responses' top-$K$ probabilities. Moreover, we employ an adaptive mechanism to determine $K$ to enhance flexibility and filter out low-confidence probabilities. Experimental results on three free-form question-answering datasets across several LLMs demonstrate that our method outperforms expensive state-of-the-art baselines, contributing to the broader goal of enhancing LLM trustworthiness.

</details>


### [139] [On the Role of Calibration in Benchmarking Algorithmic Fairness for Skin Cancer Detection](https://arxiv.org/abs/2511.07700)
*Brandon Dominique,Prudence Lam,Nicholas Kurtansky,Jochen Weber,Kivanc Kose,Veronica Rotemberg,Jennifer Dy*

Main category: cs.LG

TL;DR: AI模型在黑色素瘤检测中表现出色，但临床应用受不同人群性能差异阻碍，本文用校准指标补充基准评估，发现现有模型有校准问题，强调全面审核和元数据收集的必要性。


<details>
  <summary>Details</summary>
Motivation: AI模型临床应用受不同人群性能差异阻碍，以往基准评估指标不能反映模型准确估计能力，需补充校准指标。

Method: 将校准作为补充指标，评估ISIC 2020挑战赛领先算法在ISIC 2020和PROVE - AI数据集上的表现，并与二、三名模型对比，关注性别、种族和年龄亚组。

Result: 现有模型提高了判别准确性，但应用于新数据集时往往过度诊断风险且存在校准问题。

Conclusion: 需要全面的模型审核策略和广泛的元数据收集以实现公平的AI医疗解决方案。

Abstract: Artificial Intelligence (AI) models have demonstrated expert-level performance in melanoma detection, yet their clinical adoption is hindered by performance disparities across demographic subgroups such as gender, race, and age. Previous efforts to benchmark the performance of AI models have primarily focused on assessing model performance using group fairness metrics that rely on the Area Under the Receiver Operating Characteristic curve (AUROC), which does not provide insights into a model's ability to provide accurate estimates. In line with clinical assessments, this paper addresses this gap by incorporating calibration as a complementary benchmarking metric to AUROC-based fairness metrics. Calibration evaluates the alignment between predicted probabilities and observed event rates, offering deeper insights into subgroup biases. We assess the performance of the leading skin cancer detection algorithm of the ISIC 2020 Challenge on the ISIC 2020 Challenge dataset and the PROVE-AI dataset, and compare it with the second and third place models, focusing on subgroups defined by sex, race (Fitzpatrick Skin Tone), and age. Our findings reveal that while existing models enhance discriminative accuracy, they often over-diagnose risk and exhibit calibration issues when applied to new datasets. This study underscores the necessity for comprehensive model auditing strategies and extensive metadata collection to achieve equitable AI-driven healthcare solutions. All code is publicly available at https://github.com/bdominique/testing_strong_calibration.

</details>


### [140] [Diffusion Guided Adversarial State Perturbations in Reinforcement Learning](https://arxiv.org/abs/2511.07701)
*Xiaolin Sun,Feidi Liu,Zhengming Ding,ZiZhan Zheng*

Main category: cs.LG

TL;DR: 提出SHIFT攻击方法突破现有强化学习防御局限性，证明RL代理易受语义感知对抗扰动影响。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习系统易受对抗攻击，当前防御有效性源于现有攻击方法缺陷，需新攻击方法。

Method: 提出基于扩散的策略无关状态扰动攻击SHIFT，生成语义不同且现实、历史对齐的扰动状态。

Result: SHIFT攻击有效打破现有防御，显著优于现有攻击，更具感知隐蔽性。

Conclusion: 强调RL代理对语义感知对抗扰动的脆弱性，凸显开发更鲁棒策略的重要性。

Abstract: Reinforcement learning (RL) systems, while achieving remarkable success across various domains, are vulnerable to adversarial attacks. This is especially a concern in vision-based environments where minor manipulations of high-dimensional image inputs can easily mislead the agent's behavior. To this end, various defenses have been proposed recently, with state-of-the-art approaches achieving robust performance even under large state perturbations. However, after closer investigation, we found that the effectiveness of the current defenses is due to a fundamental weakness of the existing $l_p$ norm-constrained attacks, which can barely alter the semantics of image input even under a relatively large perturbation budget. In this work, we propose SHIFT, a novel policy-agnostic diffusion-based state perturbation attack to go beyond this limitation. Our attack is able to generate perturbed states that are semantically different from the true states while remaining realistic and history-aligned to avoid detection. Evaluations show that our attack effectively breaks existing defenses, including the most sophisticated ones, significantly outperforming existing attacks while being more perceptually stealthy. The results highlight the vulnerability of RL agents to semantics-aware adversarial perturbations, indicating the importance of developing more robust policies.

</details>


### [141] [Intelligent Optimization of Multi-Parameter Micromixers Using a Scientific Machine Learning Framework](https://arxiv.org/abs/2511.07702)
*Meraj Hassanzadeh,Ehsan Ghaderi,Mohamad Ali Bijarchi,Siamak Kazemzadeh Hannani*

Main category: cs.LG

TL;DR: 本文提出基于Sci - ML的框架解决多维优化问题，以微混合器为例，通过DRL和PINN结合优化，效率提升，与遗传算法对比凸显优势。


<details>
  <summary>Details</summary>
Motivation: 传统基于仿真的优化方法一次只能优化一个问题，且网格划分和数值模拟计算时间长，需新方法解决多维优化问题。

Method: 引入Sci - ML框架，用DRL架构的智能体作为优化器，与PINN构成的环境交互，探索关键参数关系，以微混合器为例进行研究。

Result: 在不同施密特数下效率均高于基线值，施密特数为13.3时效率最高，提升约32%，与遗传算法对比凸显优势。

Conclusion: 所提基于Sci - ML的方法能有效解决多维优化问题，比传统方法和遗传算法更具优势。

Abstract: Multidimensional optimization has consistently been a critical challenge in engineering. However, traditional simulation-based optimization methods have long been plagued by significant limitations: they are typically capable of optimizing only a single problem at a time and require substantial computational time for meshing and numerical simulation. This paper introduces a novel framework leveraging cutting-edge Scientific Machine Learning (Sci-ML) methodologies to overcome these inherent drawbacks of conventional approaches. The proposed method provides instantaneous solutions to a spectrum of complex, multidimensional optimization problems. A micromixer case study is employed to demonstrate this methodology. An agent, operating on a Deep Reinforcement Learning (DRL) architecture, serves as the optimizer to explore the relationships between key problem parameters. This optimizer interacts with an environment constituted by a parametric Physics-Informed Neural Network (PINN), which responds to the agent's actions at a significantly higher speed than traditional numerical methods. The agent's objective, conditioned on the Schmidt number is to discover the optimal geometric and physical parameters that maximize the micromixer's efficiency. After training the agent across a wide range of Schmidt numbers, we analyzed the resulting optimal designs. Across this entire spectrum, the achieved efficiency was consistently greater than the baseline, normalized value. The maximum efficiency occurred at a Schmidt number of 13.3, demonstrating an improvement of approximately 32%. Finally, a comparative analysis with a Genetic Algorithm was conducted under equivalent conditions to underscore the advantages of the proposed method.

</details>


### [142] [A Ranking-Based Optimization Algorithm for the Vehicle Relocation Problem in Car Sharing Services](https://arxiv.org/abs/2511.07724)
*Piotr Szwed,Paweł Skrzynski,Jarosław Wąs*

Main category: cs.LG

TL;DR: 本文提出用摩托车转移人员和重新定位车辆的策略解决自由浮动汽车共享服务中的车辆重新安置问题，实验表明算法有一定性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决自由浮动汽车共享服务中的车辆重新安置问题。

Method: 先将服务区域划分为具有相似车辆存在和服务需求时间模式的区域，再提出基于排名的快速算法做决策。

Result: 与未优化场景和精确算法对比，在相同条件下，算法和MIP求解器平均提升分别为8.44%和19.6%。

Conclusion: 根据劳动力规模，应用该解决方案可使性能指标提高约3%-10%。

Abstract: The paper addresses the Vehicle Relocation Problem in free-floating car-sharing services by presenting a solution focused on strategies for repositioning vehicles and transferring personnel with the use of scooters. Our method begins by dividing the service area into zones that group regions with similar temporal patterns of vehicle presence and service demand, allowing the application of discrete optimization methods. In the next stage, we propose a fast ranking-based algorithm that makes its decisions on the basis of the number of cars available in each zone, the projected probability density of demand, and estimated trip durations. The experiments were carried out on the basis of real-world data originating from a major car-sharing service operator in Poland. The results of this algorithm are evaluated against scenarios without optimization that constitute a baseline and compared with the results of an exact algorithm to solve the Mixed Integer Programming (MIP) model. As performance metrics, the total travel time was used. Under identical conditions (number of vehicles, staff, and demand distribution), the average improvements with respect to the baseline of our algorithm and MIP solver were equal to 8.44\% and 19.6\% correspondingly. However, it should be noted that the MIP model also mimicked decisions on trip selection, which are excluded by current services business rules. The analysis of results suggests that, depending on the size of the workforce, the application of the proposed solution allows for improving performance metrics by roughly 3%-10%.

</details>


### [143] [Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning](https://arxiv.org/abs/2511.07730)
*Bill Chunyuan Zheng,Vivek Myers,Benjamin Eysenbach,Sergey Levine*

Main category: cs.LG

TL;DR: 提出一种实用的GCRL方法，将不同方法集成，在长视野模拟任务和现实机器人操作领域表现良好。


<details>
  <summary>Details</summary>
Motivation: 学习在环境中达成目标是AI长期挑战，现代方法在长视野推理有困难，关键在于估计观测对的时间距离，现有方法各有优劣。

Method: 将不同方法集成到实用的GCRL方法中，用多步蒙特卡罗回报拟合拟度量距离。

Result: 在长达4000步的长视野模拟任务中，即使有视觉观测，该方法也优于现有GCRL方法，还能在现实机器人操作领域实现拼接。

Conclusion: 该方法是首个能在现实操作领域从无标签视觉观测离线数据集中实现多步拼接的端到端GCRL方法。

Abstract: Learning how to reach goals in an environment is a longstanding challenge in AI, yet reasoning over long horizons remains a challenge for modern methods. The key question is how to estimate the temporal distance between pairs of observations. While temporal difference methods leverage local updates to provide optimality guarantees, they often perform worse than Monte Carlo methods that perform global updates (e.g., with multi-step returns), which lack such guarantees. We show how these approaches can be integrated into a practical GCRL method that fits a quasimetric distance using a multistep Monte-Carlo return. We show our method outperforms existing GCRL methods on long-horizon simulated tasks with up to 4000 steps, even with visual observations. We also demonstrate that our method can enable stitching in the real-world robotic manipulation domain (Bridge setup). Our approach is the first end-to-end GCRL method that enables multistep stitching in this real-world manipulation domain from an unlabeled offline dataset of visual observations.

</details>


### [144] [Global Optimization on Graph-Structured Data via Gaussian Processes with Spectral Representations](https://arxiv.org/abs/2511.07734)
*Shu Hong,Yongsheng Mei,Mahdi Imani,Tian Lan*

Main category: cs.LG

TL;DR: 提出一种可扩展的图全局优化框架，用低秩谱表示构建高斯过程替代模型，实验显示比现有方法收敛快、性能好。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化扩展到图结构域有挑战，现有方法存在依赖全图拓扑或收敛慢的问题。

Method: 采用低秩谱表示从稀疏结构观测构建高斯过程替代模型，通过可学习嵌入联合推断图结构和节点表示。

Result: 在合成和真实数据集实验中，比现有方法收敛更快、优化性能更好。

Conclusion: 所提方法能有效解决图结构域优化问题，在不同采样机制下可准确恢复图结构。

Abstract: Bayesian optimization (BO) is a powerful framework for optimizing expensive black-box objectives, yet extending it to graph-structured domains remains challenging due to the discrete and combinatorial nature of graphs. Existing approaches often rely on either full graph topology-impractical for large or partially observed graphs-or incremental exploration, which can lead to slow convergence. We introduce a scalable framework for global optimization over graphs that employs low-rank spectral representations to build Gaussian process (GP) surrogates from sparse structural observations. The method jointly infers graph structure and node representations through learnable embeddings, enabling efficient global search and principled uncertainty estimation even with limited data. We also provide theoretical analysis establishing conditions for accurate recovery of underlying graph structure under different sampling regimes. Experiments on synthetic and real-world datasets demonstrate that our approach achieves faster convergence and improved optimization performance compared to prior methods.

</details>


### [145] [From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training](https://arxiv.org/abs/2511.07738)
*Donglai Xu,Hongzheng Yang,Yuzhi Zhao,Pingping Zhang,Jinpeng Chen,Wenao Ma,Zhijian Hou,Mengyang Wu,Xiaolei Li,Senkang Hu,Ziyi Guan,Jason Chun Lok Li,Lai Man Po*

Main category: cs.LG

TL;DR: 提出用于多模态大语言模型强化学习的两阶段、标记级熵优化方法，在多种任务和噪声设置下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的可验证奖励强化学习依赖高质量标注数据，现有无监督方法易过拟合且限制奖励排序信号，需增强噪声容忍度。

Method: 提出两阶段、标记级熵优化方法，初始探索阶段最大化标记级熵，训练后期进入利用阶段最小化标记级熵。

Result: 在三种多模态大语言模型骨干网络、多种噪声设置和多个任务中，该分阶段策略统一并增强了外部、内部和基于熵的方法，表现优于先前方法。

Conclusion: 所提出的方法能有效应对多模态大语言模型强化学习挑战，具有鲁棒性和优越性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) for Multimodal Large Language Models (MLLMs) is highly dependent on high-quality labeled data, which is often scarce and prone to substantial annotation noise in real-world scenarios. Existing unsupervised RLVR methods, including pure entropy minimization, can overfit to incorrect labels and limit the crucial reward ranking signal for Group-Relative Policy Optimization (GRPO). To address these challenges and enhance noise tolerance, we propose a novel two-stage, token-level entropy optimization method for RLVR. This approach dynamically guides the model from exploration to exploitation during training. In the initial exploration phase, token-level entropy maximization promotes diverse and stochastic output generation, serving as a strong regularizer that prevents premature convergence to noisy labels and ensures sufficient intra-group variation, which enables more reliable reward gradient estimation in GRPO. As training progresses, the method transitions into the exploitation phase, where token-level entropy minimization encourages the model to produce confident and deterministic outputs, thereby consolidating acquired knowledge and refining prediction accuracy. Empirically, across three MLLM backbones - Qwen2-VL-2B, Qwen2-VL-7B, and Qwen2.5-VL-3B - spanning diverse noise settings and multiple tasks, our phased strategy consistently outperforms prior approaches by unifying and enhancing external, internal, and entropy-based methods, delivering robust and superior performance across the board.

</details>


### [146] [Schedulers for Schedule-free: Theoretically inspired hyperparameters](https://arxiv.org/abs/2511.07767)
*Yuen-Man Pun,Matthew Buchholz,Robert M. Gower*

Main category: cs.LG

TL;DR: 扩展无调度方法收敛理论以支持任意调度器，设计新的自适应学习率调度并证明收敛性，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有无调度方法理论仅支持恒定学习率，与实践中使用的预热调度不符，需扩展理论。

Method: 扩展无调度方法的最后迭代收敛理论，设计新的自适应Polyak学习率调度。

Result: 收敛理论对深度神经网络实践有预测能力，应用于wsd调度时显示最优收敛率，新的Polyak调度在黑盒模型蒸馏任务中表现良好。

Conclusion: 成功扩展无调度方法理论，新的Polyak调度具有最优收敛性且表现出色。

Abstract: The recently proposed schedule-free method has been shown to achieve strong performance when hyperparameter tuning is limited. The current theory for schedule-free only supports a constant learning rate, where-as the implementation used in practice uses a warm-up schedule. We show how to extend the last-iterate convergence theory of schedule-free to allow for any scheduler, and how the averaging parameter has to be updated as a function of the learning rate. We then perform experiments showing how our convergence theory has some predictive power with regards to practical executions on deep neural networks, despite that this theory relies on assuming convexity. When applied to the warmup-stable-decay (wsd) schedule, our theory shows the optimal convergence rate of $\mathcal{O}(1/\sqrt{T})$. We then use convexity to design a new adaptive Polyak learning rate schedule for schedule-free. We prove an optimal anytime last-iterate convergence for our new Polyak schedule, and show that it performs well compared to a number of baselines on a black-box model distillation task.

</details>


### [147] [Physical Consistency of Aurora's Encoder: A Quantitative Study](https://arxiv.org/abs/2511.07787)
*Benjamin Richards,Pushpa Kumar Balan*

Main category: cs.LG

TL;DR: 研究探测Aurora编码器物理一致性，发现其学习到物理一致特征但捕捉罕见事件有局限，强调AI天气模型需可解释性方法。


<details>
  <summary>Details</summary>
Motivation: 大型天气预报模型如Aurora存在不透明的“黑盒”特性，阻碍其在高风险操作场景应用，需探究其物理一致性。

Method: 利用大规模嵌入数据集训练线性分类器，识别陆地 - 海洋边界、极端温度事件和大气不稳定三个概念。

Result: 有定量证据表明Aurora学习到物理一致特征，但捕捉最罕见事件存在局限性。

Conclusion: 强调下一代AI驱动的天气模型需要可解释性方法来验证和建立信任。

Abstract: The high accuracy of large-scale weather forecasting models like Aurora is often accompanied by a lack of transparency, as their internal representations remain largely opaque. This "black box" nature hinders their adoption in high-stakes operational settings. In this work, we probe the physical consistency of Aurora's encoder by investigating whether its latent representations align with known physical and meteorological concepts. Using a large-scale dataset of embeddings, we train linear classifiers to identify three distinct concepts: the fundamental land-sea boundary, high-impact extreme temperature events, and atmospheric instability. Our findings provide quantitative evidence that Aurora learns physically consistent features, while also highlighting its limitations in capturing the rarest events. This work underscores the critical need for interpretability methods to validate and build trust in the next generation of Al-driven weather models.

</details>


### [148] [Analyzing Political Text at Scale with Online Tensor LDA](https://arxiv.org/abs/2511.07809)
*Sara Kangaslahti,Danny Ebanks,Jean Kossaifi,Anqi Liu,R. Michael Alvarez,Animashree Anandkumar*

Main category: cs.LG

TL;DR: 提出可线性扩展到数十亿文档的主题建模方法TLDA，有参数和样本复杂度保证，高效且可线性扩展，提供开源GPU实现，用于两项大规模研究，助社科研究者实时研究大型语料。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理大规模文档，需开发可线性扩展到数十亿文档的主题建模方法，以支持大规模数据分析。

Method: 提出Tensor Latent Dirichlet Allocation (TLDA) 主题建模方法，提供基于GPU的开源实现。

Result: TLDA方法有参数和样本复杂度保证，计算和内存高效，速度比先前并行LDA方法快3 - 4倍，可线性扩展到超十亿文档数据集，用于两项大规模研究。

Conclusion: 该方法使社科研究者能大规模研究大型语料，近乎实时回答重要理论相关问题。

Abstract: This paper proposes a topic modeling method that scales linearly to billions of documents. We make three core contributions: i) we present a topic modeling method, Tensor Latent Dirichlet Allocation (TLDA), that has identifiable and recoverable parameter guarantees and sample complexity guarantees for large data; ii) we show that this method is computationally and memory efficient (achieving speeds over 3-4x those of prior parallelized Latent Dirichlet Allocation (LDA) methods), and that it scales linearly to text datasets with over a billion documents; iii) we provide an open-source, GPU-based implementation, of this method. This scaling enables previously prohibitive analyses, and we perform two real-world, large-scale new studies of interest to political scientists: we provide the first thorough analysis of the evolution of the #MeToo movement through the lens of over two years of Twitter conversation and a detailed study of social media conversations about election fraud in the 2020 presidential election. Thus this method provides social scientists with the ability to study very large corpora at scale and to answer important theoretically-relevant questions about salient issues in near real-time.

</details>


### [149] [Multi-Objective Bilevel Learning](https://arxiv.org/abs/2511.07824)
*Zhiyao Zhang,Zhuqing Liu,Xin Zhang,Wen-Yen Chen,Jiyan Yang,Jia Liu*

Main category: cs.LG

TL;DR: 近年来机器学习应用日益复杂，引出多目标双层学习（MOBL）需求，但该领域尚不成熟。本文提出WC - MHGD算法框架，保证收敛率，还做了实验验证理论结果。


<details>
  <summary>Details</summary>
Motivation: 现代ML框架需处理多目标问题，MOBL领域处于起步阶段，很多重要问题待探索，需填补理论和算法基础的空白。

Method: 提出名为加权 - 切比雪夫多超梯度下降（WC - MHGD）的统一算法框架，适用于确定性和随机设置，有有限时间帕累托平稳收敛率保证。

Result: 提出的算法框架有低预言复杂度，能进行系统的帕累托前沿探索。

Conclusion: 通过广泛实验证实了理论结果，所提算法框架有效。

Abstract: As machine learning (ML) applications grow increasingly complex in recent years, modern ML frameworks often need to address multiple potentially conflicting objectives with coupled decision variables across different layers. This creates a compelling need for multi-objective bilevel learning (MOBL). So far, however, the field of MOBL remains in its infancy and many important problems remain under-explored. This motivates us to fill this gap and systematically investigate the theoretical and algorithmic foundation of MOBL. Specifically, we consider MOBL problems with multiple conflicting objectives guided by preferences at the upper-level subproblem, where part of the inputs depend on the optimal solution of the lower-level subproblem. Our goal is to develop efficient MOBL optimization algorithms to (1) identify a preference-guided Pareto-stationary solution with low oracle complexity; and (2) enable systematic Pareto front exploration. To this end, we propose a unifying algorithmic framework called weighted-Chebyshev multi-hyper-gradient-descent (WC-MHGD) for both deterministic and stochastic settings with finite-time Pareto-stationarity convergence rate guarantees, which not only implies low oracle complexity but also induces systematic Pareto front exploration. We further conduct extensive experiments to confirm our theoretical results.

</details>


### [150] [MURPHY: Multi-Turn GRPO for Self Correcting Code Generation](https://arxiv.org/abs/2511.07833)
*Chanakya Ekbote,Vijay Lingam,Behrooz Omidvar-Tehrani,Jun Huan,Sujay Sanghavi,Anoop Deoras,Stefano Soatto*

Main category: cs.LG

TL;DR: 提出Murphy框架扩展GRPO，在代码生成基准测试中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法如GRPO在需要迭代决策的代理任务中表现不佳，需提升模型推理能力。

Method: 引入多轮反思优化框架Murphy，在训练中加入迭代自我修正，利用定量和定性执行反馈。

Result: 在Qwen和OLMo等模型家族的代码生成基准测试中，Murphy比GRPO性能提升，在相似计算预算下pass@1相对增益达8%。

Conclusion: Murphy能有效提升模型推理能力，在代码生成任务中表现优于GRPO。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful framework for enhancing the reasoning capabilities of large language models (LLMs). However, existing approaches such as Group Relative Policy Optimization (GRPO) and its variants, while effective on reasoning benchmarks, struggle with agentic tasks that require iterative decision-making. We introduce Murphy, a multi-turn reflective optimization framework that extends GRPO by incorporating iterative self-correction during training. By leveraging both quantitative and qualitative execution feedback, Murphy enables models to progressively refine their reasoning across multiple turns. Evaluations on code generation benchmarks with model families such as Qwen and OLMo show that Murphy consistently improves performance, achieving up to a 8% relative gain in pass@1 over GRPO, on similar compute budgets.

</details>


### [151] [DP-AdamW: Investigating Decoupled Weight Decay and Bias Correction in Private Deep Learning](https://arxiv.org/abs/2511.07843)
*Jay Chooi,Kevin Cong,Russell Li,Lillian Sun*

Main category: cs.LG

TL;DR: 研究DP - AdamW和引入DP - AdamW - BC优化器，给出理论结果并实证分析，发现DP - AdamW性能优于现有优化器，DP - AdamW - BC加入偏差校正会降低准确率。


<details>
  <summary>Details</summary>
Motivation: 在深度学习利用敏感数据时，实现能保持强性能和隐私保护的DP优化器存在挑战，需研究新的优化器。

Method: 先给出DP - AdamW和DP - AdamW - BC的隐私和收敛保证的理论结果，再在多个隐私预算下实证分析两者表现。

Result: DP - AdamW在文本、图像和图节点分类上优于现有DP优化器；DP - AdamW - BC加入偏差校正会降低准确率。

Conclusion: DP - AdamW是性能较好的差分隐私优化器，偏差校正对DP - AdamW和DP - Adam的影响不同。

Abstract: As deep learning methods increasingly utilize sensitive data on a widespread scale, differential privacy (DP) offers formal guarantees to protect against information leakage during model training. A significant challenge remains in implementing DP optimizers that retain strong performance while preserving privacy. Recent advances introduced ever more efficient optimizers, with AdamW being a popular choice for training deep learning models because of strong empirical performance. We study \emph{DP-AdamW} and introduce \emph{DP-AdamW-BC}, a differentially private variant of the AdamW optimizer with DP bias correction for the second moment estimator. We start by showing theoretical results for privacy and convergence guarantees of DP-AdamW and DP-AdamW-BC. Then, we empirically analyze the behavior of both optimizers across multiple privacy budgets ($ε= 1, 3, 7$). We find that DP-AdamW outperforms existing state-of-the-art differentially private optimizers like DP-SGD, DP-Adam, and DP-AdamBC, scoring over 15\% higher on text classification, up to 5\% higher on image classification, and consistently 1\% higher on graph node classification. Moreover, we empirically show that incorporating bias correction in DP-AdamW (DP-AdamW-BC) consistently decreases accuracy, in contrast to the improvement of DP-AdamBC improvement over DP-Adam.

</details>


### [152] [A General Method for Proving Networks Universal Approximation Property](https://arxiv.org/abs/2511.07857)
*Wei Wang*

Main category: cs.LG

TL;DR: 本文针对深度学习架构通用逼近性证明方法的局限性，提出通用模块化框架，统一不同架构分析。


<details>
  <summary>Details</summary>
Motivation: 现有证明深度学习架构通用逼近性的方法存在需为新架构从头证明、缺乏统一分析基础的问题，阻碍统一理论理解。

Method: 定义具有通用逼近性的基本构建块为通用逼近模块（UAM），证明由其组成的深度网络也具备该性质。

Result: 所提框架可统一不同架构分析，能逐步理解网络表达能力的演变。

Conclusion: 提出的通用模块化框架解决了现有证明方法的问题，利于对不同网络家族进行统一理论理解。

Abstract: Deep learning architectures are highly diverse. To prove their universal approximation properties, existing works typically rely on model-specific proofs. Generally, they construct a dedicated mathematical formulation for each architecture (e.g., fully connected networks, CNNs, or Transformers) and then prove their universal approximability. However, this approach suffers from two major limitations: first, every newly proposed architecture often requires a completely new proof from scratch; second, these proofs are largely isolated from one another, lacking a common analytical foundation. This not only incurs significant redundancy but also hinders unified theoretical understanding across different network families. To address these issues, this paper proposes a general and modular framework for proving universal approximation. We define a basic building block (comprising one or multiple layers) that possesses the universal approximation property as a Universal Approximation Module (UAM). Under this condition, we show that any deep network composed of such modules inherently retains the universal approximation property. Moreover, the overall approximation process can be interpreted as a progressive refinement across modules. This perspective not only unifies the analysis of diverse architectures but also enables a step-by-step understanding of how expressive power evolves through the network.

</details>


### [153] [Algorithm-Relative Trajectory Valuation in Policy Gradient Control](https://arxiv.org/abs/2511.07878)
*Shihao Li,Jiachen Li,Jiamin Xu,Christopher Martin,Wei Li,Dongmei Chen*

Main category: cs.LG

TL;DR: 研究策略梯度控制中轨迹价值与学习算法的关系，发现PE与边际价值相关性受算法影响，实验验证机制。


<details>
  <summary>Details</summary>
Motivation: 探究策略梯度控制中轨迹价值如何依赖于学习算法。

Method: 在不确定LQR中使用Trajectory Shapley，证明方差介导机制，进行实验验证。

Result: 发现香草REINFORCE下PE与边际价值负相关，稳定化后正相关，实验验证机制。

Conclusion: 轨迹价值与算法相关，Leave - One - Out分数和Shapley可分别用于剪枝和识别有害子集。

Abstract: We study how trajectory value depends on the learning algorithm in policy-gradient control. Using Trajectory Shapley in an uncertain LQR, we find a negative correlation between Persistence of Excitation (PE) and marginal value under vanilla REINFORCE ($r\approx-0.38$). We prove a variance-mediated mechanism: (i) for fixed energy, higher PE yields lower gradient variance; (ii) near saddles, higher variance increases escape probability, raising marginal contribution. When stabilized (state whitening or Fisher preconditioning), this variance channel is neutralized and information content dominates, flipping the correlation positive ($r\approx+0.29$). Hence, trajectory value is algorithm-relative. Experiments validate the mechanism and show decision-aligned scores (Leave-One-Out) complement Shapley for pruning, while Shapley identifies toxic subsets.

</details>


### [154] [Meta-cognitive Multi-scale Hierarchical Reasoning for Motor Imagery Decoding](https://arxiv.org/abs/2511.07884)
*Si-Hyun Kim,Heon-Gyu Kwak,Byoung-Hee Kwon,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 本文研究用于四类运动想象（MI）分类的分层和元认知解码框架，通过多尺度分层信号处理和内省不确定性估计模块提升了MI - 基于脑机接口（BCI）系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前基于运动想象的脑电图（EEG）信号存在噪声和变异性，限制了脑机接口（BCI）的实际应用。

Method: 引入多尺度分层信号处理模块和内省不确定性估计模块，在三个标准EEG骨干网络上实现该框架，并在独立受试者设置下使用BCI Competition IV - 2a数据集进行评估。

Result: 在所有骨干网络中，所提出的组件提高了平均分类准确率，减少了受试者间差异。

Conclusion: 将分层多尺度处理与内省置信度估计相结合可以提高基于MI的BCI系统的可靠性。

Abstract: Brain-computer interface (BCI) aims to decode motor intent from noninvasive neural signals to enable control of external devices, but practical deployment remains limited by noise and variability in motor imagery (MI)-based electroencephalogram (EEG) signals. This work investigates a hierarchical and meta-cognitive decoding framework for four-class MI classification. We introduce a multi-scale hierarchical signal processing module that reorganizes backbone features into temporal multi-scale representations, together with an introspective uncertainty estimation module that assigns per-cycle reliability scores and guides iterative refinement. We instantiate this framework on three standard EEG backbones (EEGNet, ShallowConvNet, and DeepConvNet) and evaluate four-class MI decoding using the BCI Competition IV-2a dataset under a subject-independent setting. Across all backbones, the proposed components improve average classification accuracy and reduce inter-subject variance compared to the corresponding baselines, indicating increased robustness to subject heterogeneity and noisy trials. These results suggest that combining hierarchical multi-scale processing with introspective confidence estimation can enhance the reliability of MI-based BCI systems.

</details>


### [155] [A Generalized Spectral Framework to Expain Neural Scaling and Compression Dynamics](https://arxiv.org/abs/2511.07892)
*Yizhou Zhang*

Main category: cs.LG

TL;DR: 本文提出广义谱框架统一学习动态和压缩现象，推广谱演化函数，得出学习与压缩的不变关系。


<details>
  <summary>Details</summary>
Motivation: 经验缩放定律在特定情况下一致，但模型压缩等相关设置有不同缩放行为，受神经表示光谱分析进展启发。

Method: 开发广义谱框架，将谱演化函数从线性核形式推广到渐近多项式函数。

Result: 框架将学习动态和压缩现象统一，恢复现有理论作为特殊情况，得出学习与压缩的不变关系。

Conclusion: 广义谱框架能有效统一学习和压缩现象，在理论上有重要意义。

Abstract: Empirical scaling laws describe how test loss and other performance metrics depend on model size, dataset size, and compute. While such laws are consistent within specific regimes, apparently distinct scaling behaviors have been reported for related settings such as model compression. Motivated by recent progress in spectral analyses of neural representations, this paper develops a \emph{generalized spectral framework} that unifies learning dynamics and compression phenomena under a common functional ansatz. We generalize the spectral evolution function from the linear kernel form $g(λt)=λt$ to an asymptotically polynomial function $g(λ,t;β)$, characterized by an effective spectral--temporal elasticity $ρ(β)$. This framework recovers existing lazy and feature-learning theories as special cases and yields an invariant relation between learning and compression

</details>


### [156] [Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction](https://arxiv.org/abs/2511.07899)
*Ihab Tabbara,Yuxuan Yang,Hussein Sibai*

Main category: cs.LG

TL;DR: 本文引入基于共形预测的框架解决学习的HJ值函数和策略的不确定性问题，并研究集成HJ值函数作为安全过滤器。


<details>
  <summary>Details</summary>
Motivation: 计算HJ值函数计算成本高，采用强化学习近似值函数但结果不一定正确，需解决不确定性问题。

Method: 引入基于共形预测的框架，利用其校准不安全标称控制器和学习的HJ安全策略的切换，推导切换策略下的安全保证，研究集成HJ值函数作为安全过滤器。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Safety assurance is a fundamental requirement for deploying learning-enabled autonomous systems. Hamilton-Jacobi (HJ) reachability analysis is a fundamental method for formally verifying safety and generating safe controllers. However, computing the HJ value function that characterizes the backward reachable set (BRS) of a set of user-defined failure states is computationally expensive, especially for high-dimensional systems, motivating the use of reinforcement learning approaches to approximate the value function. Unfortunately, a learned value function and its corresponding safe policy are not guaranteed to be correct. The learned value function evaluated at a given state may not be equal to the actual safety return achieved by following the learned safe policy. To address this challenge, we introduce a conformal prediction-based (CP) framework that bounds such uncertainty. We leverage CP to provide probabilistic safety guarantees when using learned HJ value functions and policies to prevent control systems from reaching failure states. Specifically, we use CP to calibrate the switching between the unsafe nominal controller and the learned HJ-based safe policy and to derive safety guarantees under this switched policy. We also investigate using an ensemble of independently trained HJ value functions as a safety filter and compare this ensemble approach to using individual value functions alone.

</details>


### [157] [Test-driven Reinforcement Learning](https://arxiv.org/abs/2511.07904)
*Zhao Yu,Xiuping Wu,Liangjun Ke*

Main category: cs.LG

TL;DR: 提出测试驱动强化学习（TdRL）框架解决强化学习奖励设计难题，实验显示其在策略训练上表现良好，为任务目标表示提供新视角。


<details>
  <summary>Details</summary>
Motivation: 强化学习中手动设计奖励函数困难，常导致次优任务表示，需解决奖励设计挑战。

Method: 提出TdRL框架，用多个测试函数表示任务目标，证明相关理论，引入字典序启发式方法学习轨迹返回函数并开发算法实现。

Result: 在DeepMind Control Suite基准实验中，TdRL在策略训练上与或优于手工奖励方法，设计更简单且支持多目标优化。

Conclusion: TdRL为表示任务目标提供新视角，有助于解决强化学习应用中的奖励设计挑战。

Abstract: Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.

</details>


### [158] [CellARC: Measuring Intelligence with Cellular Automata](https://arxiv.org/abs/2511.07908)
*Miroslav Lžičař*

Main category: cs.LG

TL;DR: 介绍CellARC合成基准，含训练和测试集，评估多种模型，展示小模型和大模型表现及神经符号互补性。


<details>
  <summary>Details</summary>
Motivation: 构建一个能将泛化与拟人先验分离、可控制难度并支持可重复性研究的抽象推理合成基准。

Method: 基于多色1D细胞自动机构建CellARC基准，评估符号、循环、卷积、Transformer等多种模型。

Result: 小模型基线（10M参数Transformer）优于递归模型，大模型（GPT - 5 High）有较好表现，神经符号集成达到65.4%/35.5%准确率。

Conclusion: CellARC基准有助于研究模型在有限预算下推理新规则的能力，神经符号方法具有互补性。

Abstract: We introduce CellARC, a synthetic benchmark for abstraction and reasoning built from multicolor 1D cellular automata (CA). Each episode has five support pairs and one query serialized in 256 tokens, enabling rapid iteration with small models while exposing a controllable task space with explicit knobs for alphabet size k, radius r, rule family, Langton's lambda, query coverage, and cell entropy. We release 95k training episodes plus two 1k test splits (interpolation/extrapolation) and evaluate symbolic, recurrent, convolutional, transformer, recursive, and LLM baselines. CellARC decouples generalization from anthropomorphic priors, supports unlimited difficulty-controlled sampling, and enables reproducible studies of how quickly models infer new rules under tight budgets. Our strongest small-model baseline (a 10M-parameter vanilla transformer) outperforms recent recursive models (TRM, HRM), reaching 58.0%/32.4% per-token accuracy on the interpolation/extrapolation splits, while a large closed model (GPT-5 High) attains 62.3%/48.1% on subsets of 100 test tasks. An ensemble that chooses per episode between the Transformer and the best symbolic baseline reaches 65.4%/35.5%, highlighting neuro-symbolic complementarity. Leaderboard: https://cellarc.mireklzicar.com

</details>


### [159] [Rectified Noise: A Generative Model Using Positive-incentive Noise](https://arxiv.org/abs/2511.07911)
*Zhenyu Gu,Yanchen Xu,Sida Huang,Yubin Guo,Hongyuan Zhang*

Main category: cs.LG

TL;DR: 提出Rectified Noise（ΔRN）算法，将π - 噪声注入预训练的Rectified Flow（RF）模型速度场以提升生成性能，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: RF基于概率流ODE，通过反向随机微分方程注入噪声采样可提升生成性能，受π - 噪声启发，旨在提升RF模型生成性能。

Method: 提出Rectified Noise算法，将π - 噪声注入预训练RF模型的速度场，将预训练RF模型转换为π - 噪声生成器。

Result: 在不同数据集和模型架构上实验，RF模型使用Rectified Noise后在ImageNet - 1k上FID从10.16降至9.05，π - 噪声生成器模型仅增加0.39%训练参数就提升了性能。

Conclusion: Rectified Noise算法能有效提升RF模型的生成性能，且增加少量训练参数。

Abstract: Rectified Flow (RF) has been widely used as an effective generative model. Although RF is primarily based on probability flow Ordinary Differential Equations (ODE), recent studies have shown that injecting noise through reverse-time Stochastic Differential Equations (SDE) for sampling can achieve superior generative performance. Inspired by Positive-incentive Noise ($π$-noise), we propose an innovative generative algorithm to train $π$-noise generators, namely Rectified Noise ($Δ$RN), which improves the generative performance by injecting $π$-noise into the velocity field of pre-trained RF models. After introducing the Rectified Noise pipeline, pre-trained RF models can be efficiently transformed into $π$-noise generators. We validate Rectified Noise by conducting extensive experiments across various model architectures on different datasets. Notably, we find that: (1) RF models using Rectified Noise reduce FID from \textbf{10.16 to 9.05} on ImageNet-1k. (2) The models of $π$-noise generators achieve improved performance with only \textbf{0.39\%} additional training parameters.

</details>


### [160] [Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison](https://arxiv.org/abs/2511.07919)
*Yoonho Lee,Joseph Boen,Chelsea Finn*

Main category: cs.LG

TL;DR: 提出Feedback Descent框架，通过结构化文本反馈优化文本工件，在三个领域表现优于现有方法，在分子发现基准中找到优质分子。


<details>
  <summary>Details</summary>
Motivation: 解决偏好学习中信息瓶颈问题，实现文本空间的定向优化，避免仅依赖标量奖励。

Method: 利用结构化文本反馈，通过上下文学习将反馈转化为梯度方向信息，迭代优化在推理时进行且不修改模型权重。

Result: 在三个领域中，Feedback Descent优于现有提示优化、强化学习方法和分子优化器；在分子发现基准中找到优质药物分子。

Conclusion: Feedback Descent框架有效，能通过结构化文本反馈实现文本工件的优化。

Abstract: We introduce \textit{Feedback Descent}, a framework that optimizes text artifacts -- prompts, code, and molecules -- through structured textual feedback, rather than relying solely on scalar rewards. By preserving detailed critiques instead of compressing them to binary preferences, Feedback Descent widens the information bottleneck in preference learning, enabling directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $260{,}000$ compounds across six protein targets.

</details>


### [161] [SERL: Self-Examining Reinforcement Learning on Open-Domain](https://arxiv.org/abs/2511.07922)
*Weixuan Ou,Yanzhao Zheng,Shuoshuo Sun,Wei Zhang,Baohua Dong,Hangcheng Zhu,Ruohui Huang,Gang Yu,Pengwei Yan,Yifan Qiao*

Main category: cs.LG

TL;DR: 本文提出自审查强化学习（SERL）框架解决强化学习应用于开放域任务的挑战，实验表明该方法优于现有自改进训练方法，达到当前最优。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习应用于开放域任务存在挑战，如任务主观性使RLVR无法获得可验证奖励，RLHF依赖外部奖励机制。

Method: 提出SERL框架，LLM同时作为执行者和评判者，引入两种无外部信号的协同奖励机制，包括基于Copeland式成对比较的奖励和自一致性奖励。

Result: SERL方法优于现有自改进训练方法，将Qwen3 - 8B在AlpacaEval 2上的LC胜率从52.37%提升到59.90%，性能与Qwen3 - 32B相当。

Conclusion: SERL在开放域任务上具有优越的有效性和鲁棒性，达到自改进方法中的最优性能。

Abstract: Reinforcement Learning (RL) has been shown to improve the capabilities of large language models (LLMs). However, applying RL to open-domain tasks faces two key challenges: (1) the inherent subjectivity of these tasks prevents the verifiable rewards as required by Reinforcement Learning with Verifiable Rewards (RLVR); (2) Reinforcement Learning from Human Feedback (RLHF) relies on external reward mechanisms. To overcome these limitations, we propose Self-Examining Reinforcement Learning (SERL), a novel self-improving framework where the LLM serves as both Actor and Judge. SERL introduces two synergistic reward mechanisms without any external signals. On the one hand, to improve the Actor's capability, we derive rewards from Copeland-style pairwise comparison judgments across a group of generated responses. On the other hand, a self-consistency reward that encourages coherent judgments is proposed to improve the Judge's reliability. This process refines the Judge's capability, which in turn provides a more robust reward for Actor. Experiments show that our method outperforms existing self-improvement training methods. SERL improves the LC win rate of Qwen3-8B on AlpacaEval 2 from 52.37% to 59.90%. To the best of our knowledge, our method achieves state-of-the-art performance among self-improving approaches. Furthermore, it achieves a performance comparable to significantly larger models like Qwen3-32B, demonstrating superior effectiveness and robustness on open-domain tasks.

</details>


### [162] [IBMA: An Imputation-Based Mixup Augmentation Using Self-Supervised Learning for Time Series Data](https://arxiv.org/abs/2511.07930)
*Dang Nha Nguyen,Hai Dang Nguyen,Khoa Tho Anh Nguyen*

Main category: cs.LG

TL;DR: 提出基于插补的Mixup增强(IBMA)方法用于时间序列预测，实验表明其能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据增强策略少，先进技术如Mixup使用少，需提升模型泛化和预测性能。

Method: 提出IBMA方法，结合插补增强数据和Mixup增强，并在多个预测模型上评估。

Result: 在四个数据集上与八种其他增强技术对比，24次实验中22次提升性能，10次达到最佳，iTrainformer插补效果尤佳。

Conclusion: IBMA方法能有效提升时间序列预测模型的性能。

Abstract: Data augmentation in time series forecasting plays a crucial role in enhancing model performance by introducing variability while maintaining the underlying temporal patterns. However, time series data offers fewer augmentation strategies compared to fields such as image or text, with advanced techniques like Mixup rarely being used. In this work, we propose a novel approach, Imputation-Based Mixup Augmentation (IBMA), which combines Imputation-Augmented data with Mixup augmentation to bolster model generalization and improve forecasting performance. We evaluate the effectiveness of this method across several forecasting models, including DLinear (MLP), TimesNet (CNN), and iTrainformer (Transformer), these models represent some of the most recent advances in time series forecasting. Our experiments, conducted on four datasets (ETTh1, ETTh2, ETTm1, ETTm2) and compared against eight other augmentation techniques, demonstrate that IBMA consistently enhances performance, achieving 22 improvements out of 24 instances, with 10 of those being the best performances, particularly with iTrainformer imputation.

</details>


### [163] [Predict-then-Optimize Method for Seaport Power-Logistics Scheduling: Generalization across Varying Tasks Stream](https://arxiv.org/abs/2511.07938)
*Chuanqing Pu,Feilong Fan,Nengling Tai,Yan Xu,Wentao Huang,Honglin Wen*

Main category: cs.LG

TL;DR: 提出决策导向的持续学习框架解决现代海港电力物流调度问题，实验显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有决策导向学习在下游优化中假设固定任务配置，难以适应海港船只到达导致的任务结构变化。

Method: 引入基于Fisher信息的正则化增强跨任务泛化，开发可微凸代理稳定梯度反向传播。

Result: 在裕廊港的实验中，相比现有方法决策性能和泛化能力更优，计算成本降低。

Conclusion: 所提方法能为新调度任务学习决策对齐的预测模型，同时保留对早期任务的泛化能力。

Abstract: Power-logistics scheduling in modern seaports typically follow a predict-then-optimize pipeline. To enhance decision quality, decision-focused learning has been proposed to align forecasting and optimization via end-to-end training. However, most formulations assume a fixed task configuration in downstream optimization, and thus generalize poorly to evolving task structures induced by varying seaport vessel arrivals. We address this gap with a decision-focused continual learning framework that adapts online to a stream of scheduling tasks. Specifically, we introduce Fisher information based regularization to enhance cross-task generalization by preserving parameters critical to prior tasks. A differentiable convex surrogate is also developed to stabilize gradient backpropagation. The proposed approach enables learning a decision-aligned forecasting model for new scheduling tasks while retaining generalization on earlier tasks. Experiments calibrated to the Jurong Port demonstrate superior decision performance and generalization over existing methods with reduced computational cost.

</details>


### [164] [Balance Equation-based Distributionally Robust Offline Imitation Learning](https://arxiv.org/abs/2511.07942)
*Rishabh Agrawal,Yusuf Alvi,Rahul Jain,Ashutosh Nayyar*

Main category: cs.LG

TL;DR: 提出基于平衡方程的分布鲁棒离线模仿学习框架，仅从标称动力学下的专家演示学习策略，在连续控制基准测试中表现出更好的鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 标准模仿学习方法假设训练和部署时环境动力学固定，但实际中该假设不成立，动力学变化会导致性能严重下降。

Method: 将问题表述为对过渡模型不确定性集的分布鲁棒优化，在最坏情况下的过渡分布下最小化模仿损失，并将鲁棒目标完全用标称数据分布重新表述以实现离线学习。

Result: 在连续控制基准测试中，与现有离线模仿学习基线相比，在受扰动或变化的环境下实现了更优的鲁棒性和泛化性。

Conclusion: 所提出的框架能有效解决模仿学习中环境动力学变化导致的性能问题。

Abstract: Imitation Learning (IL) has proven highly effective for robotic and control tasks where manually designing reward functions or explicit controllers is infeasible. However, standard IL methods implicitly assume that the environment dynamics remain fixed between training and deployment. In practice, this assumption rarely holds where modeling inaccuracies, real-world parameter variations, and adversarial perturbations can all induce shifts in transition dynamics, leading to severe performance degradation. We address this challenge through Balance Equation-based Distributionally Robust Offline Imitation Learning, a framework that learns robust policies solely from expert demonstrations collected under nominal dynamics, without requiring further environment interaction. We formulate the problem as a distributionally robust optimization over an uncertainty set of transition models, seeking a policy that minimizes the imitation loss under the worst-case transition distribution. Importantly, we show that this robust objective can be reformulated entirely in terms of the nominal data distribution, enabling tractable offline learning. Empirical evaluations on continuous-control benchmarks demonstrate that our approach achieves superior robustness and generalization compared to state-of-the-art offline IL baselines, particularly under perturbed or shifted environments.

</details>


### [165] [Continual Unlearning for Text-to-Image Diffusion Models: A Regularization Perspective](https://arxiv.org/abs/2511.07970)
*Justin Lee,Zheda Mai,Jinsu Yoo,Chongyu Fan,Cheng Zhang,Wei-Lun Chao*

Main category: cs.LG

TL;DR: 研究文本到图像扩散模型的持续遗忘学习，指出现有方法存在效用崩溃问题，提出正则化方法改进性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型的遗忘学习方法假设请求一次性到达，而实际中请求是顺序到达的，需研究持续遗忘学习。

Method: 研究一系列附加正则化器，提出梯度投影方法约束参数漂移。

Result: 提出的方法显著提高了持续遗忘学习性能，且与其他正则化器互补可进一步提升。

Conclusion: 确立持续遗忘学习是文本到图像生成中的基本挑战，为推进安全和负责任的生成式AI提供见解、基线和开放方向。

Abstract: Machine unlearning--the ability to remove designated concepts from a pre-trained model--has advanced rapidly, particularly for text-to-image diffusion models. However, existing methods typically assume that unlearning requests arrive all at once, whereas in practice they often arrive sequentially. We present the first systematic study of continual unlearning in text-to-image diffusion models and show that popular unlearning methods suffer from rapid utility collapse: after only a few requests, models forget retained knowledge and generate degraded images. We trace this failure to cumulative parameter drift from the pre-training weights and argue that regularization is crucial to addressing it. To this end, we study a suite of add-on regularizers that (1) mitigate drift and (2) remain compatible with existing unlearning methods. Beyond generic regularizers, we show that semantic awareness is essential for preserving concepts close to the unlearning target, and propose a gradient-projection method that constrains parameter drift orthogonal to their subspace. This substantially improves continual unlearning performance and is complementary to other regularizers for further gains. Taken together, our study establishes continual unlearning as a fundamental challenge in text-to-image generation and provides insights, baselines, and open directions for advancing safe and accountable generative AI.

</details>


### [166] [Low-Rank Curvature for Zeroth-Order Optimization in LLM Fine-Tuning](https://arxiv.org/abs/2511.07971)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: 提出曲率感知零阶优化方法LOREN微调大语言模型，实验显示优于现有零阶方法。


<details>
  <summary>Details</summary>
Motivation: 现有零阶方法通过随机扰动的有限差分估计梯度，存在方差高和搜索方向次优的问题。

Method: 将梯度预条件问题重新表述为自适应估计各向异性扰动分布；用自然进化策略框架通过低秩块对角预条件器捕捉曲率；应用REINFORCE留一法梯度估计器降低方差。

Result: 在标准大语言模型基准测试中，LOREN比现有零阶方法精度更高、收敛更快，与MeZO - Adam相比，峰值内存使用最多降低27.3%。

Conclusion: LOREN方法在大语言模型微调中有效且优于现有零阶方法。

Abstract: We introduce LOREN, a curvature-aware zeroth-order (ZO) optimization method for fine-tuning large language models (LLMs). Existing ZO methods, which estimate gradients via finite differences using random perturbations, often suffer from high variance and suboptimal search directions. Our approach addresses these challenges by: (i) reformulating the problem of gradient preconditioning as that of adaptively estimating an anisotropic perturbation distribution for gradient estimation, (ii) capturing curvature through a low-rank block diagonal preconditioner using the framework of natural evolution strategies, and (iii) applying a REINFORCE leave-one-out (RLOO) gradient estimator to reduce variance. Experiments on standard LLM benchmarks show that our method outperforms state-of-the-art ZO methods by achieving higher accuracy and faster convergence, while cutting peak memory usage by up to 27.3% compared with MeZO-Adam.

</details>


### [167] [Generalizable Insights for Graph Transformers in Theory and Practice](https://arxiv.org/abs/2511.08028)
*Timo Stoll,Luis Müller,Christopher Morris*

Main category: cs.LG

TL;DR: 提出广义距离变压器（GDT）图变压器架构，通过实验确定通用设计选择，提炼出有效GT设计、训练和推理的通用见解。


<details>
  <summary>Details</summary>
Motivation: 现有图变压器架构在注意力机制、位置嵌入和表达能力上差异大，理论结果缺乏大规模数据验证，存在理论与实践差距，缺乏通用见解。

Method: 提出GDT架构，结合近年GT的进展，通过对八百万个图和2.7亿个标记的广泛实验，研究其注意力和位置嵌入的表示能力。

Result: 确定了在不同应用、任务和模型规模中表现良好的设计选择，在少样本迁移设置中未微调也表现出色。

Conclusion: 提炼出关于有效GT设计、训练和推理的通用见解。

Abstract: Graph Transformers (GTs) have shown strong empirical performance, yet current architectures vary widely in their use of attention mechanisms, positional embeddings (PEs), and expressivity. Existing expressivity results are often tied to specific design choices and lack comprehensive empirical validation on large-scale data. This leaves a gap between theory and practice, preventing generalizable insights that exceed particular application domains. Here, we propose the Generalized-Distance Transformer (GDT), a GT architecture using standard attention that incorporates many advancements for GTs from recent years, and develop a fine-grained understanding of the GDT's representation power in terms of attention and PEs. Through extensive experiments, we identify design choices that consistently perform well across various applications, tasks, and model scales, demonstrating strong performance in a few-shot transfer setting without fine-tuning. Our evaluation covers over eight million graphs with roughly 270M tokens across diverse domains, including image-based object detection, molecular property prediction, code summarization, and out-of-distribution algorithmic reasoning. We distill our theoretical and practical findings into several generalizable insights about effective GT design, training, and inference.

</details>


### [168] [From Sequential to Recursive: Enhancing Decision-Focused Learning with Bidirectional Feedback](https://arxiv.org/abs/2511.08035)
*Xinyu Wang,Jinxiao Du,Yiyang Peng,Wei Ma*

Main category: cs.LG

TL;DR: 提出递归决策聚焦学习（R - DFL）框架，扩展两种微分方法，实验表明其提升决策质量且适应性强。


<details>
  <summary>Details</summary>
Motivation: 现有顺序决策聚焦学习（S - DFL）框架有严格顺序结构，无法捕捉复杂场景中预测与优化的双向反馈。

Method: 提出R - DFL框架引入双向反馈，扩展显式展开和隐式微分两种方法促进梯度传播。

Result: 证明两种方法梯度精度相当，隐式方法计算效率更高；实验显示R - DFL提升决策质量，适应性强。

Conclusion: R - DFL是一种有效框架，能改善闭环决策问题中的决策质量，且在不同场景有强适应性。

Abstract: Decision-focused learning (DFL) has emerged as a powerful end-to-end alternative to conventional predict-then-optimize (PTO) pipelines by directly optimizing predictive models through downstream decision losses. Existing DFL frameworks are limited by their strictly sequential structure, referred to as sequential DFL (S-DFL). However, S-DFL fails to capture the bidirectional feedback between prediction and optimization in complex interaction scenarios. In view of this, we first time propose recursive decision-focused learning (R-DFL), a novel framework that introduces bidirectional feedback between downstream optimization and upstream prediction. We further extend two distinct differentiation methods: explicit unrolling via automatic differentiation and implicit differentiation based on fixed-point methods, to facilitate efficient gradient propagation in R-DFL. We rigorously prove that both methods achieve comparable gradient accuracy, with the implicit method offering superior computational efficiency. Extensive experiments on both synthetic and real-world datasets, including the newsvendor problem and the bipartite matching problem, demonstrate that R-DFL not only substantially enhances the final decision quality over sequential baselines but also exhibits robust adaptability across diverse scenarios in closed-loop decision-making problems.

</details>


### [169] [DynaAct: Large Language Model Reasoning with Dynamic Action Spaces](https://arxiv.org/abs/2511.08043)
*Xueliang Zhao,Wei Wu,Jian Guan,Qintong Li,Lingpeng Kong*

Main category: cs.LG

TL;DR: 提出DynaAct框架自动构建紧凑动作空间以增强复杂问题解决中的顺序推理，实验显示显著提升性能且推理高效。


<details>
  <summary>Details</summary>
Motivation: 现有顺序决策系统动作空间构建方法存在缺乏可扩展性或计算成本高的问题，需新方法。

Method: 用大语言模型从语料中提取通用草图估计完整动作空间，构建子模函数联合评估候选动作并使用贪心算法选择最优候选集。

Result: 在六个标准基准测试中显著提升整体性能，推理高效且无显著延迟。

Conclusion: DynaAct框架能有效构建紧凑动作空间，提升复杂问题顺序推理性能。

Abstract: In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named \textsc{DynaAct} for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.

</details>


### [170] [An Integrated Fusion Framework for Ensemble Learning Leveraging Gradient Boosting and Fuzzy Rule-Based Models](https://arxiv.org/abs/2511.08077)
*Jinbo Li,Peng Liu,Long Chen,Witold Pedrycz,Weiping Ding*

Main category: cs.LG

TL;DR: 本文提出集成融合框架，结合梯度提升与模糊规则模型优势，实验证明其能提升性能、减轻过拟合和降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 克服模糊规则模型设计复杂和大数据集可扩展性问题，融合不同学习范式优势。

Method: 提出集成融合框架，用动态因子控制模糊规则模型贡献，结合样本校正机制。

Result: 实验证明框架有效，提升性能，减轻过拟合和复杂度。

Conclusion: 框架通过最优因子提升性能、保持可解释性，简化模型维护和更新。

Abstract: The integration of different learning paradigms has long been a focus of machine learning research, aimed at overcoming the inherent limitations of individual methods. Fuzzy rule-based models excel in interpretability and have seen widespread application across diverse fields. However, they face challenges such as complex design specifications and scalability issues with large datasets. The fusion of different techniques and strategies, particularly Gradient Boosting, with Fuzzy Rule-Based Models offers a robust solution to these challenges. This paper proposes an Integrated Fusion Framework that merges the strengths of both paradigms to enhance model performance and interpretability. At each iteration, a Fuzzy Rule-Based Model is constructed and controlled by a dynamic factor to optimize its contribution to the overall ensemble. This control factor serves multiple purposes: it prevents model dominance, encourages diversity, acts as a regularization parameter, and provides a mechanism for dynamic tuning based on model performance, thus mitigating the risk of overfitting. Additionally, the framework incorporates a sample-based correction mechanism that allows for adaptive adjustments based on feedback from a validation set. Experimental results substantiate the efficacy of the presented gradient boosting framework for fuzzy rule-based models, demonstrating performance enhancement, especially in terms of mitigating overfitting and complexity typically associated with many rules. By leveraging an optimal factor to govern the contribution of each model, the framework improves performance, maintains interpretability, and simplifies the maintenance and update of the models.

</details>


### [171] [Hierarchical Structure-Property Alignment for Data-Efficient Molecular Generation and Editing](https://arxiv.org/abs/2511.08080)
*Ziyu Fan,Zhijian Huang,Yahan Li,Xiaowen Hu,Siyuan Shen,Yunliang Wang,Zeyu Zhong,Shuhong Liu,Shuning Yang,Shangqian Wu,Min Wu,Lei Deng*

Main category: cs.LG

TL;DR: 提出HSPAG框架用于属性约束的分子生成与编辑，减少数据需求并提升生成质量，实验和案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有属性约束的分子生成与编辑受困于难以捕捉分子结构与多属性复杂关系，以及分子属性覆盖窄和注释不完整。

Method: 提出HSPAG框架，将SMILES和分子属性作为互补模态在多层面学习关系，通过支架聚类选代表样本、辅助VAE选难样本，引入属性相关性感知掩码机制和多样化扰动策略。

Result: HSPAG能捕捉细粒度结构 - 属性关系，支持多属性约束下的可控生成，两个实际案例验证了其编辑能力。

Conclusion: HSPAG框架有效解决了现有属性约束分子生成与编辑的局限。

Abstract: Property-constrained molecular generation and editing are crucial in AI-driven drug discovery but remain hindered by two factors: (i) capturing the complex relationships between molecular structures and multiple properties remains challenging, and (ii) the narrow coverage and incomplete annotations of molecular properties weaken the effectiveness of property-based models. To tackle these limitations, we propose HSPAG, a data-efficient framework featuring hierarchical structure-property alignment. By treating SMILES and molecular properties as complementary modalities, the model learns their relationships at atom, substructure, and whole-molecule levels. Moreover, we select representative samples through scaffold clustering and hard samples via an auxiliary variational auto-encoder (VAE), substantially reducing the required pre-training data. In addition, we incorporate a property relevance-aware masking mechanism and diversified perturbation strategies to enhance generation quality under sparse annotations. Experiments demonstrate that HSPAG captures fine-grained structure-property relationships and supports controllable generation under multiple property constraints. Two real-world case studies further validate the editing capabilities of HSPAG.

</details>


### [172] [HipKittens: Fast and Furious AMD Kernels](https://arxiv.org/abs/2511.08083)
*William Hu,Drew Wadsworth,Sean Siddens,Stanley Winata,Daniel Y. Fu,Ryann Swann,Muhammad Osama,Christopher Ré,Simran Arora*

Main category: cs.LG

TL;DR: 本文探索适用于AMD GPU的高性能AI内核编程原语，提出HipKittens框架，验证其在AMD平台性能良好，为跨GPU厂商的高性能AI内核软件层奠定基础。


<details>
  <summary>Details</summary>
Motivation: AMD GPU虽有高计算和内存带宽，但高性能内核用汇编编写，现有针对NVIDIA硬件的DSL难以直接用于AMD，需研究适用于AMD的编程原语。

Method: 研究适用于AMD AI内核的编程原语，封装为HipKittens框架，在CDNA3和CDNA4平台验证。

Result: 基于平铺的抽象可用于AMD GPU，但需重新设计算法；HK内核在GEMMs和注意力计算上与AMD手工优化汇编内核竞争，且在部分场景下优于所有基线。

Conclusion: 为跨GPU厂商的基于平铺的高性能AI内核软件层铺平道路。

Abstract: AMD GPUs offer state-of-the-art compute and memory bandwidth; however, peak performance AMD kernels are written in raw assembly. To address the difficulty of mapping AI algorithms to hardware, recent work proposes C++ embedded and PyTorch-inspired domain-specific languages like ThunderKittens (TK) to simplify high performance AI kernel development on NVIDIA hardware. We explore the extent to which such primitives -- for explicit tile-based programming with optimized memory accesses and fine-grained asynchronous execution across workers -- are NVIDIA-specific or general. We provide the first detailed study of the programming primitives that lead to performant AMD AI kernels, and we encapsulate these insights in the HipKittens (HK) programming framework. We find that tile-based abstractions used in prior DSLs generalize to AMD GPUs, however we need to rethink the algorithms that instantiate these abstractions for AMD. We validate the HK primitives across CDNA3 and CDNA4 AMD platforms. In evaluations, HK kernels compete with AMD's hand-optimized assembly kernels for GEMMs and attention, and consistently outperform compiler baselines. Moreover, assembly is difficult to scale to the breadth of AI workloads; reflecting this, in some settings HK outperforms all available kernel baselines by $1.2-2.4\times$ (e.g., $d=64$ attention, GQA backwards, memory-bound kernels). These findings help pave the way for a single, tile-based software layer for high-performance AI kernels that translates across GPU vendors. HipKittens is released at: https://github.com/HazyResearch/HipKittens.

</details>


### [173] [Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks](https://arxiv.org/abs/2511.08086)
*Muthukumar Pandaram,Jakob Hollenstein,David Drexel,Samuele Tosatto,Antonio Rodríguez-Sánchez,Justus Piater*

Main category: cs.LG

TL;DR: 文章分析MuJoCo Playground基准套件中机器人强化学习环境的真实动力学，研究状态和时间稀疏性假设，发现任务动力学存在局部、依赖状态的稀疏性，挑战了常见稀疏先验假设。


<details>
  <summary>Details</summary>
Motivation: 验证在典型强化学习任务中，所提出的状态和时间稀疏性概念是否成立。

Method: 分析MuJoCo Playground基准套件中一组机器人强化学习环境的真实动力学，研究环境动力学因果图是否稀疏、稀疏性是否依赖状态、局部系统动力学是否稀疏变化。

Result: 全局稀疏性少见，任务动力学存在局部、依赖状态的稀疏性，且稀疏性有独特结构，出现在时间局部簇中并影响特定状态维度子集。

Conclusion: 常见的动力学学习稀疏先验假设受到挑战，需要能反映现实世界动力学依赖状态的稀疏结构的归纳偏置。

Abstract: The use of learned dynamics models, also known as world models, can improve the sample efficiency of reinforcement learning. Recent work suggests that the underlying causal graphs of such dynamics models are sparsely connected, with each of the future state variables depending only on a small subset of the current state variables, and that learning may therefore benefit from sparsity priors. Similarly, temporal sparsity, i.e. sparsely and abruptly changing local dynamics, has also been proposed as a useful inductive bias.
  In this work, we critically examine these assumptions by analyzing ground-truth dynamics from a set of robotic reinforcement learning environments in the MuJoCo Playground benchmark suite, aiming to determine whether the proposed notions of state and temporal sparsity actually tend to hold in typical reinforcement learning tasks.
  We study (i) whether the causal graphs of environment dynamics are sparse, (ii) whether such sparsity is state-dependent, and (iii) whether local system dynamics change sparsely.
  Our results indicate that global sparsity is rare, but instead the tasks show local, state-dependent sparsity in their dynamics and this sparsity exhibits distinct structures, appearing in temporally localized clusters (e.g., during contact events) and affecting specific subsets of state dimensions. These findings challenge common sparsity prior assumptions in dynamics learning, emphasizing the need for grounded inductive biases that reflect the state-dependent sparsity structure of real-world dynamics.

</details>


### [174] [Stuart-Landau Oscillatory Graph Neural Network](https://arxiv.org/abs/2511.08094)
*Kaicheng Zhang,David N. Reynolds,Piero Deidda,Francesco Tudisco*

Main category: cs.LG

TL;DR: 本文提出基于Stuart - Landau振子动力学的复值SLGNN架构，实验表明其性能优于现有OGNNs。


<details>
  <summary>Details</summary>
Motivation: 缓解深度GNN中的过平滑和梯度消失问题，改进现有以相位为中心的基于Kuramoto的OGNNs。

Method: 引入基于Stuart - Landau振子动力学的SLGNN架构，允许节点特征振幅根据其动力学动态演化，并设置可调节超参数。

Result: 在节点分类、图分类和图回归任务的大量实验中，SLGNN表现优于现有OGNNs。

Conclusion: SLGNN为图上的深度振荡架构建立了新颖、富有表现力且有理论依据的框架。

Abstract: Oscillatory Graph Neural Networks (OGNNs) are an emerging class of physics-inspired architectures designed to mitigate oversmoothing and vanishing gradient problems in deep GNNs. In this work, we introduce the Complex-Valued Stuart-Landau Graph Neural Network (SLGNN), a novel architecture grounded in Stuart-Landau oscillator dynamics. Stuart-Landau oscillators are canonical models of limit-cycle behavior near Hopf bifurcations, which are fundamental to synchronization theory and are widely used in e.g. neuroscience for mesoscopic brain modeling. Unlike harmonic oscillators and phase-only Kuramoto models, Stuart-Landau oscillators retain both amplitude and phase dynamics, enabling rich phenomena such as amplitude regulation and multistable synchronization. The proposed SLGNN generalizes existing phase-centric Kuramoto-based OGNNs by allowing node feature amplitudes to evolve dynamically according to Stuart-Landau dynamics, with explicit tunable hyperparameters (such as the Hopf-parameter and the coupling strength) providing additional control over the interplay between feature amplitudes and network structure. We conduct extensive experiments across node classification, graph classification, and graph regression tasks, demonstrating that SLGNN outperforms existing OGNNs and establishes a novel, expressive, and theoretically grounded framework for deep oscillatory architectures on graphs.

</details>


### [175] [A robust methodology for long-term sustainability evaluation of Machine Learning models](https://arxiv.org/abs/2511.08120)
*Jorge Paz-Ruza,João Gama,Amparo Alonso-Betanzos,Bertha Guijarro-Berdiñas*

Main category: cs.LG

TL;DR: 提出综合评估协议评估ML模型长期可持续性，实验表明传统评估不可靠，不同模型长期可持续性差异大且高环境成本未必有高收益。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统监管和报告实践缺乏标准化、模型无关的评估协议，当前评估不能反映现实中长期AI生命周期。

Method: 提出适用于批量和流式学习场景的综合评估协议，并在多种分类任务上用不同模型类型进行实验。

Result: 传统静态训练 - 测试评估在数据演变和模型重复更新时不能可靠衡量可持续性，不同模型长期可持续性差异显著，很多情况下高环境成本性能收益小。

Conclusion: 所提综合评估协议能更好评估ML模型长期可持续性，传统评估方式存在不足。

Abstract: Sustainability and efficiency have become essential considerations in the development and deployment of Artificial Intelligence systems, yet existing regulatory and reporting practices lack standardized, model-agnostic evaluation protocols. Current assessments often measure only short-term experimental resource usage and disproportionately emphasize batch learning settings, failing to reflect real-world, long-term AI lifecycles. In this work, we propose a comprehensive evaluation protocol for assessing the long-term sustainability of ML models, applicable to both batch and streaming learning scenarios. Through experiments on diverse classification tasks using a range of model types, we demonstrate that traditional static train-test evaluations do not reliably capture sustainability under evolving data and repeated model updates. Our results show that long-term sustainability varies significantly across models, and in many cases, higher environmental cost yields little performance benefit.

</details>


### [176] [Improving Long-Range Interactions in Graph Neural Simulators via Hamiltonian Dynamics](https://arxiv.org/abs/2511.08185)
*Tai Hoang,Alessandro Trenta,Alessio Gravina,Niklas Freymuth,Philipp Becker,Davide Bacciu,Gerhard Neumann*

Main category: cs.LG

TL;DR: 提出基于哈密顿动力学原理的信息保留图神经模拟器IGNS，在新基准测试中表现优于现有GNSs。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，现有GNSs难以捕捉长程相互作用且存在误差累积问题。

Method: 构建基于哈密顿动力学原理的IGNS，加入预热阶段、几何编码和多步训练目标，引入新基准测试。

Result: IGNS在所有任务中始终优于最先进的GNSs，在复杂动力系统中实现更高的准确性和稳定性。

Conclusion: IGNS能有效解决现有GNSs的问题，在模拟复杂物理系统方面表现出色。

Abstract: Learning to simulate complex physical systems from data has emerged as a promising way to overcome the limitations of traditional numerical solvers, which often require prohibitive computational costs for high-fidelity solutions. Recent Graph Neural Simulators (GNSs) accelerate simulations by learning dynamics on graph-structured data, yet often struggle to capture long-range interactions and suffer from error accumulation under autoregressive rollouts. To address these challenges, we propose Information-preserving Graph Neural Simulators (IGNS), a graph-based neural simulator built on the principles of Hamiltonian dynamics. This structure guarantees preservation of information across the graph, while extending to port-Hamiltonian systems allows the model to capture a broader class of dynamics, including non-conservative effects. IGNS further incorporates a warmup phase to initialize global context, geometric encoding to handle irregular meshes, and a multi-step training objective to reduce rollout error. To evaluate these properties systematically, we introduce new benchmarks that target long-range dependencies and challenging external forcing scenarios. Across all tasks, IGNS consistently outperforms state-of-the-art GNSs, achieving higher accuracy and stability under challenging and complex dynamical systems.

</details>


### [177] [The Online Patch Redundancy Eliminator (OPRE): A novel approach to online agnostic continual learning using dataset compression](https://arxiv.org/abs/2511.08226)
*Raphaël Bayle,Martial Mermillod,Robert M. French*

Main category: cs.LG

TL;DR: 论文指出多数持续学习（CL）方法引入先验信息，不具数据无关性，提出在线数据集压缩算法OPRE，其在CIFAR - 10和CIFAR - 100上表现优于其他方法，认为在线数据集压缩对实现完全数据无关的CL可能必要。


<details>
  <summary>Details</summary>
Motivation: 克服灾难性遗忘问题以实现持续学习，解决多数CL方法引入先验信息、不具数据无关性的问题。

Method: 提出在线数据集压缩算法OPRE，并在测试时训练分类器。

Result: OPRE在CIFAR - 10和CIFAR - 100上的性能优于许多其他在线持续学习方法。

Conclusion: 在线数据集压缩可能是实现完全数据无关的持续学习所必需的。

Abstract: In order to achieve Continual Learning (CL), the problem of catastrophic forgetting, one that has plagued neural networks since their inception, must be overcome. The evaluation of continual learning methods relies on splitting a known homogeneous dataset and learning the associated tasks one after the other. We argue that most CL methods introduce a priori information about the data to come and cannot be considered agnostic. We exemplify this point with the case of methods relying on pretrained feature extractors, which are still used in CL. After showing that pretrained feature extractors imply a loss of generality with respect to the data that can be learned by the model, we then discuss other kinds of a priori information introduced in other CL methods. We then present the Online Patch Redundancy Eliminator (OPRE), an online dataset compression algorithm, which, along with the training of a classifier at test time, yields performance on CIFAR-10 and CIFAR-100 superior to a number of other state-of-the-art online continual learning methods. Additionally, OPRE requires only minimal and interpretable hypothesis on the data to come. We suggest that online dataset compression could well be necessary to achieve fully agnostic CL.

</details>


### [178] [Towards Non-Stationary Time Series Forecasting with Temporal Stabilization and Frequency Differencing](https://arxiv.org/abs/2511.08229)
*Junkai Lu,Peng Chen,Chenjuan Guo,Yang Shu,Meng Wang,Bin Yang*

Main category: cs.LG

TL;DR: 提出DTAF框架解决时间序列非平稳性问题，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列存在非平稳性，给长期时间序列预测带来挑战。

Method: 提出双分支框架DTAF，在时域用TFS模块，频域用FWM模块，融合二者输出。

Result: 在真实世界基准测试中，DTAF优于现有基线，在非平稳条件下显著提高预测准确性。

Conclusion: DTAF能有效应对时域和频域的非平稳性，可生成鲁棒的预测结果。

Abstract: Time series forecasting is critical for decision-making across dynamic domains such as energy, finance, transportation, and cloud computing. However, real-world time series often exhibit non-stationarity, including temporal distribution shifts and spectral variability, which pose significant challenges for long-term time series forecasting. In this paper, we propose DTAF, a dual-branch framework that addresses non-stationarity in both the temporal and frequency domains. For the temporal domain, the Temporal Stabilizing Fusion (TFS) module employs a non-stationary mix of experts (MOE) filter to disentangle and suppress temporal non-stationary patterns while preserving long-term dependencies. For the frequency domain, the Frequency Wave Modeling (FWM) module applies frequency differencing to dynamically highlight components with significant spectral shifts. By fusing the complementary outputs of TFS and FWM, DTAF generates robust forecasts that adapt to both temporal and frequency domain non-stationarity. Extensive experiments on real-world benchmarks demonstrate that DTAF outperforms state-of-the-art baselines, yielding significant improvements in forecasting accuracy under non-stationary conditions. All codes are available at https://github.com/PandaJunk/DTAF.

</details>


### [179] [PrefPoE: Advantage-Guided Preference Fusion for Learning Where to Explore](https://arxiv.org/abs/2511.08241)
*Zhihao Lin,Lin Wu,Zhen Tian,Jianglin Lan*

Main category: cs.LG

TL;DR: 提出PrefPoE框架解决强化学习探索挑战，在多任务中表现出色，证明学习探索位置的重要性。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的朴素熵最大化探索存在高方差和低效策略更新问题，需更好的探索方法。

Method: 引入PrefPoE框架，训练偏好网络并通过PoE与主策略融合，创建软信任区域。

Result: 在连续和离散动作空间的多个控制任务中，PrefPoE显著提升性能，训练稳定性和样本效率增强。

Conclusion: 通过优势引导偏好学习探索位置对强化学习很关键，为提升策略梯度方法提供通用框架。

Abstract: Exploration in reinforcement learning remains a critical challenge, as naive entropy maximization often results in high variance and inefficient policy updates. We introduce \textbf{PrefPoE}, a novel \textit{Preference-Product-of-Experts} framework that performs intelligent, advantage-guided exploration via the first principled application of product-of-experts (PoE) fusion for single-task exploration-exploitation balancing. By training a preference network to concentrate probability mass on high-advantage actions and fusing it with the main policy through PoE, PrefPoE creates a \textbf{soft trust region} that stabilizes policy updates while maintaining targeted exploration. Across diverse control tasks spanning both continuous and discrete action spaces, PrefPoE demonstrates consistent improvements: +321\% on HalfCheetah-v4 (1276~$\rightarrow$~5375), +69\% on Ant-v4, +276\% on LunarLander-v2, with consistently enhanced training stability and sample efficiency. Unlike standard PPO, which suffers from entropy collapse, PrefPoE sustains adaptive exploration through its unique dynamics, thereby preventing premature convergence and enabling superior performance. Our results establish that learning \textit{where to explore} through advantage-guided preferences is as crucial as learning how to act, offering a general framework for enhancing policy gradient methods across the full spectrum of reinforcement learning domains. Code and pretrained models are available in supplementary materials.

</details>


### [180] [A Unified Geometric Field Theory Framework for Transformers: From Manifold Embeddings to Kernel Modulation](https://arxiv.org/abs/2511.08243)
*Xianshuai Shi,Jianfeng Zhu,Leibo Liu*

Main category: cs.LG

TL;DR: 提出结构理论框架对Transformer进行理论研究，将离散位置映射到连续流形上的空间函数。


<details>
  <summary>Details</summary>
Motivation: Transformer核心组件缺乏统一物理或数学解释。

Method: 提出整合位置编码、核积分算子和注意力机制的结构理论框架，将离散位置映射到连续流形上的空间函数。

Result: 实现了用核调制算子对Transformer层进行场论解释。

Conclusion: 所提框架可对Transformer进行深入理论研究。

Abstract: The Transformer architecture has achieved tremendous success in natural language processing, computer vision, and scientific computing through its self-attention mechanism. However, its core components-positional encoding and attention mechanisms-have lacked a unified physical or mathematical interpretation. This paper proposes a structural theoretical framework that integrates positional encoding, kernel integral operators, and attention mechanisms for in-depth theoretical investigation. We map discrete positions (such as text token indices and image pixel coordinates) to spatial functions on continuous manifolds, enabling a field-theoretic interpretation of Transformer layers as kernel-modulated operators acting over embedded manifolds.

</details>


### [181] [Data-Driven Discovery of Feature Groups in Clinical Time Series](https://arxiv.org/abs/2511.08260)
*Fedor Sergeev,Manuel Burger,Polina Leshetkina,Vincent Fortuin,Gunnar Rätsch,Rita Kuznetsova*

Main category: cs.LG

TL;DR: 提出通过聚类特征嵌入层权重学习特征组的方法，在合成和真实医疗数据上表现良好且特征组有临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 临床时间序列数据特征分组可提升深度学习架构性能，但仅靠语义知识先验定义分组有挑战。

Method: 通过聚类特征嵌入层的权重学习特征组，无缝集成到标准监督训练中。

Result: 在合成数据上优于静态聚类方法，在真实医疗数据上与专家定义组性能相当。

Conclusion: 该方法能直接提升临床相关任务下游性能，且学习到的特征组具有临床可解释性，可实现变量间任务相关关系的数据驱动发现。

Abstract: Clinical time series data are critical for patient monitoring and predictive modeling. These time series are typically multivariate and often comprise hundreds of heterogeneous features from different data sources. The grouping of features based on similarity and relevance to the prediction task has been shown to enhance the performance of deep learning architectures. However, defining these groups a priori using only semantic knowledge is challenging, even for domain experts. To address this, we propose a novel method that learns feature groups by clustering weights of feature-wise embedding layers. This approach seamlessly integrates into standard supervised training and discovers the groups that directly improve downstream performance on clinically relevant tasks. We demonstrate that our method outperforms static clustering approaches on synthetic data and achieves performance comparable to expert-defined groups on real-world medical data. Moreover, the learned feature groups are clinically interpretable, enabling data-driven discovery of task-relevant relationships between variables.

</details>


### [182] [Rethinking Explanation Evaluation under the Retraining Scheme](https://arxiv.org/abs/2511.08281)
*Yi Cai,Thibaud Ardoin,Mayank Gulati,Gerhard Wunder*

Main category: cs.LG

TL;DR: 本文探讨解释评估中经验观察与理论预期的不一致问题，分析原因并提出改进方案，提升评估效率并给出实证结果。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因解释评估方法存在分布偏移、评估结果与理论不符等问题，需要解决这些问题以更好评估解释质量。

Method: 分析确定符号问题是影响基于重训练评估的关键因素，重新构建评估过程，提出新的评估变体。

Result: 新方案提升了评估效率，通过不同数据规模的实证结果深入了解了解释器性能。

Conclusion: 提出的评估方案能解决现有问题，增强了实践适用性，为可解释性研究指明了挑战和方向。

Abstract: Feature attribution has gained prominence as a tool for explaining model decisions, yet evaluating explanation quality remains challenging due to the absence of ground-truth explanations. To circumvent this, explanation-guided input manipulation has emerged as an indirect evaluation strategy, measuring explanation effectiveness through the impact of input modifications on model outcomes during inference. Despite the widespread use, a major concern with inference-based schemes is the distribution shift caused by such manipulations, which undermines the reliability of their assessments. The retraining-based scheme ROAR overcomes this issue by adapting the model to the altered data distribution. However, its evaluation results often contradict the theoretical foundations of widely accepted explainers. This work investigates this misalignment between empirical observations and theoretical expectations. In particular, we identify the sign issue as a key factor responsible for residual information that ultimately distorts retraining-based evaluation. Based on the analysis, we show that a straightforward reframing of the evaluation process can effectively resolve the identified issue. Building on the existing framework, we further propose novel variants that jointly structure a comprehensive perspective on explanation evaluation. These variants largely improve evaluation efficiency over the standard retraining protocol, thereby enhancing practical applicability for explainer selection and benchmarking. Following our proposed schemes, empirical results across various data scales provide deeper insights into the performance of carefully selected explainers, revealing open challenges and future directions in explainability research.

</details>


### [183] [Dual-Kernel Graph Community Contrastive Learning](https://arxiv.org/abs/2511.08287)
*Xiang Chen,Kun Yue,Wenjie Liu,Zhenyu Zhang,Liang Duan*

Main category: cs.LG

TL;DR: 提出高效图对比学习框架，在十六个数据集上验证有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决图对比学习在大规模图上因GNN消息传递机制和对比损失计算复杂度导致的可扩展性问题。

Method: 将输入图转换为紧凑网络，引入线性复杂度的核化图社区对比损失，在解耦GNN架构中加入知识蒸馏技术。

Result: 在十六个不同规模的真实世界数据集上，该方法在有效性和可扩展性上超越了现有图对比学习基线。

Conclusion: 所提方法有效解决了图对比学习在大规模图上的可扩展性问题，具有更好的性能。

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful paradigm for training Graph Neural Networks (GNNs) in the absence of task-specific labels. However, its scalability on large-scale graphs is hindered by the intensive message passing mechanism of GNN and the quadratic computational complexity of contrastive loss over positive and negative node pairs. To address these issues, we propose an efficient GCL framework that transforms the input graph into a compact network of interconnected node sets while preserving structural information across communities. We firstly introduce a kernelized graph community contrastive loss with linear complexity, enabling effective information transfer among node sets to capture hierarchical structural information of the graph. We then incorporate a knowledge distillation technique into the decoupled GNN architecture to accelerate inference while maintaining strong generalization performance. Extensive experiments on sixteen real-world datasets of varying scales demonstrate that our method outperforms state-of-the-art GCL baselines in both effectiveness and scalability.

</details>


### [184] [Test-time Diverse Reasoning by Riemannian Activation Steering](https://arxiv.org/abs/2511.08305)
*Ly Tran Ho Khanh,Dongxuan Zhu,Man-Chung Yue,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 提出无监督激活引导策略解决Best-of-$N$推理输出多样性限制问题，在数学基准测试中表现更优。


<details>
  <summary>Details</summary>
Motivation: Best-of-$N$推理存在输出多样性限制问题，模型生成相似输出导致重复错误。

Method: 提出无监督激活引导策略，在测试时同时优化多个推理轨迹的引导向量，通过解决黎曼优化问题确定向量，用黎曼块坐标下降算法求解。

Result: 在流行数学基准测试中，测试时黎曼激活引导策略在生成多样性和解决方案准确性上优于普通采样技术。

Conclusion: 所提出的策略能有效解决输出多样性限制问题，提升推理效果。

Abstract: Best-of-$N$ reasoning improves the accuracy of language models in solving complex tasks by sampling multiple candidate solutions and then selecting the best one based on some criteria. A critical bottleneck for this strategy is the output diversity limit, which occurs when the model generates similar outputs despite stochastic sampling, and hence recites the same error. To address this lack of variance in reasoning paths, we propose a novel unsupervised activation steering strategy that simultaneously optimizes the steering vectors for multiple reasoning trajectories at test time. At any synchronization anchor along the batch generation process, we find the steering vectors that maximize the total volume spanned by all possible intervened activation subsets. We demonstrate that these steering vectors can be determined by solving a Riemannian optimization problem over the product of spheres with a log-determinant objective function. We then use a Riemannian block-coordinate descent algorithm with a well-tuned learning rate to obtain a stationary point of the problem, and we apply these steering vectors until the generation process reaches the subsequent synchronization anchor. Empirical evaluations on popular mathematical benchmarks demonstrate that our test-time Riemannian activation steering strategy outperforms vanilla sampling techniques in terms of generative diversity and solution accuracy.

</details>


### [185] [Improving the accuracy and generalizability of molecular property regression models with a substructure-substitution-rule-informed framework](https://arxiv.org/abs/2511.08314)
*Xiaoyu Fan,Lin Guo,Ruizhen Jia,Yang Tian,Zhihao Yang,Boxue Tian*

Main category: cs.LG

TL;DR: 提出MolRuleLoss框架提升分子属性回归模型准确性和泛化性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型在分子属性预测回归任务中准确性差、对分布外分子表现不佳的问题。

Method: 将子结构替换规则的偏导数约束纳入分子属性回归模型的损失函数。

Result: 使用GEM模型预测多种属性时RMSE降低，性能提升2.6 - 33.3%；提升对特殊分子和分布外分子预测的泛化性；证明子结构替换规则属性变化上限与模型误差正相关。

Conclusion: MolRuleLoss框架可提升多个分子属性回归模型的预测准确性和泛化性，支持化学信息学和AI辅助药物发现等领域应用。

Abstract: Artificial Intelligence (AI)-aided drug discovery is an active research field, yet AI models often exhibit poor accuracy in regression tasks for molecular property prediction, and perform catastrophically poorly for out-of-distribution (OOD) molecules. Here, we present MolRuleLoss, a substructure-substitution-rule-informed framework that improves the accuracy and generalizability of multiple molecular property regression models (MPRMs) such as GEM and UniMol for diverse molecular property prediction tasks. MolRuleLoss incorporates partial derivative constraints for substructure substitution rules (SSRs) into an MPRM's loss function. When using GEM models for predicting lipophilicity, water solubility, and solvation-free energy (using lipophilicity, ESOL, and freeSolv datasets from MoleculeNet), the root mean squared error (RMSE) values with and without MolRuleLoss were 0.587 vs. 0.660, 0.777 vs. 0.798, and 1.252 vs. 1.877, respectively, representing 2.6-33.3% performance improvements. We show that both the number and the quality of SSRs contribute to the magnitude of prediction accuracy gains obtained upon adding MolRuleLoss to an MPRM. MolRuleLoss improved the generalizability of MPRMs for "activity cliff" molecules in a lipophilicity prediction task and improved the generalizability of MPRMs for OOD molecules in a melting point prediction task. In a molecular weight prediction task for OOD molecules, MolRuleLoss reduced the RMSE value of a GEM model from 29.507 to 0.007. We also provide a formal demonstration that the upper bound of the variation for property change of SSRs is positively correlated with an MPRM's error. Together, we show that using the MolRuleLoss framework as a bolt-on boosts the prediction accuracy and generalizability of multiple MPRMs, supporting diverse applications in areas like cheminformatics and AI-aided drug discovery.

</details>


### [186] [Adversarial Bias: Data Poisoning Attacks on Fairness](https://arxiv.org/abs/2511.08331)
*Eunice Chan,Hanghang Tong*

Main category: cs.LG

TL;DR: 本文聚焦机器学习系统公平性漏洞，提出对抗性投毒策略破坏朴素贝叶斯分类器公平性，实验显示该方法在降低公平性指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有算法公平性研究多关注评估和改进，对公平性漏洞研究较少，本文旨在研究如何故意破坏AI系统公平性。

Method: 提出对抗性投毒策略，在训练集中注入少量精心构造的对抗性数据点，使模型决策边界偏向，影响受保护群体。

Result: 实验表明该攻击方法在多个模型和数据集上显著降低公平性指标，在不严重影响准确性的情况下实现更高不公平性，且对多种模型有效。

Conclusion: 提出的方法是一种强大且稳健的破坏机器学习系统公平性的方法。

Abstract: With the growing adoption of AI and machine learning systems in real-world applications, ensuring their fairness has become increasingly critical. The majority of the work in algorithmic fairness focus on assessing and improving the fairness of machine learning systems. There is relatively little research on fairness vulnerability, i.e., how an AI system's fairness can be intentionally compromised. In this work, we first provide a theoretical analysis demonstrating that a simple adversarial poisoning strategy is sufficient to induce maximally unfair behavior in naive Bayes classifiers. Our key idea is to strategically inject a small fraction of carefully crafted adversarial data points into the training set, biasing the model's decision boundary to disproportionately affect a protected group while preserving generalizable performance. To illustrate the practical effectiveness of our method, we conduct experiments across several benchmark datasets and models. We find that our attack significantly outperforms existing methods in degrading fairness metrics across multiple models and datasets, often achieving substantially higher levels of unfairness with a comparable or only slightly worse impact on accuracy. Notably, our method proves effective on a wide range of models, in contrast to prior work, demonstrating a robust and potent approach to compromising the fairness of machine learning systems.

</details>


### [187] [LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration](https://arxiv.org/abs/2511.08339)
*Ruiyu Qiu,Rui Wang,Guanghui Yang,Xiang Li,Zhijiang Shao*

Main category: cs.LG

TL;DR: 提出LPPG - RL框架解决传统多目标强化学习在处理优先级多目标时的局限，有理论保证且实验效果好。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法扩展到处理有优先级的多目标任务存在挑战，现有LMORL方法有依赖先验知识调参或限于离散域的局限。

Method: 提出LPPG - RL框架，利用顺序梯度投影确定策略更新方向，将投影步骤转化为优化问题，用Dykstra投影加速，引入子问题探索防止梯度消失等。

Result: 通过2D导航环境实验，LPPG - RL优于现有最先进的连续LMORL方法。

Conclusion: LPPG - RL框架有效可行，能解决现有方法的局限，可与连续空间的策略梯度算法兼容。

Abstract: Lexicographic multi-objective problems, which consist of multiple conflicting subtasks with explicit priorities, are common in real-world applications. Despite the advantages of Reinforcement Learning (RL) in single tasks, extending conventional RL methods to prioritized multiple objectives remains challenging. In particular, traditional Safe RL and Multi-Objective RL (MORL) methods have difficulty enforcing priority orderings efficiently. Therefore, Lexicographic Multi-Objective RL (LMORL) methods have been developed to address these challenges. However, existing LMORL methods either rely on heuristic threshold tuning with prior knowledge or are restricted to discrete domains. To overcome these limitations, we propose Lexicographically Projected Policy Gradient RL (LPPG-RL), a novel LMORL framework which leverages sequential gradient projections to identify feasible policy update directions, thereby enabling LPPG-RL broadly compatible with all policy gradient algorithms in continuous spaces. LPPG-RL reformulates the projection step as an optimization problem, and utilizes Dykstra's projection rather than generic solvers to deliver great speedups, especially for small- to medium-scale instances. In addition, LPPG-RL introduces Subproblem Exploration (SE) to prevent gradient vanishing, accelerate convergence and enhance stability. We provide theoretical guarantees for convergence and establish a lower bound on policy improvement. Finally, through extensive experiments in a 2D navigation environment, we demonstrate the effectiveness of LPPG-RL, showing that it outperforms existing state-of-the-art continuous LMORL methods.

</details>


### [188] [HN-MVTS: HyperNetwork-based Multivariate Time Series Forecasting](https://arxiv.org/abs/2511.08340)
*Andrey Savchenko,Oleg Kachan*

Main category: cs.LG

TL;DR: 提出HN - MVTS架构，结合超网络生成先验与任意神经网络预测模型，实验表明其能提升现有模型性能，超网络驱动参数化是复杂场景下增强预测技术的有前景方向。


<details>
  <summary>Details</summary>
Motivation: 准确预测多变量时间序列数据有挑战，复杂通道依赖模型性能易下降，通道独立模型虽稳健但不考虑组件关系。

Method: 提出HN - MVTS架构，超网络输入为时间序列组件的可学习嵌入矩阵，学习生成目标预测网络最后一层权重，仅在训练时使用。

Result: 在八个基准数据集上实验，将HN - MVTS应用于现有模型能提升其性能。

Conclusion: 超网络驱动参数化是复杂场景下增强现有预测技术的有前景方向。

Abstract: Accurate forecasting of multivariate time series data remains a formidable challenge, particularly due to the growing complexity of temporal dependencies in real-world scenarios. While neural network-based models have achieved notable success in this domain, complex channel-dependent models often suffer from performance degradation compared to channel-independent models that do not consider the relationship between components but provide high robustness due to small capacity. In this work, we propose HN-MVTS, a novel architecture that integrates a hypernetwork-based generative prior with an arbitrary neural network forecasting model. The input of this hypernetwork is a learnable embedding matrix of time series components. To restrict the number of new parameters, the hypernetwork learns to generate the weights of the last layer of the target forecasting networks, serving as a data-adaptive regularizer that improves generalization and long-range predictive accuracy. The hypernetwork is used only during the training, so it does not increase the inference time compared to the base forecasting model. Extensive experiments on eight benchmark datasets demonstrate that application of HN-MVTS to the state-of-the-art models (DLinear, PatchTST, TSMixer, etc.) typically improves their performance. Our findings suggest that hypernetwork-driven parameterization offers a promising direction for enhancing existing forecasting techniques in complex scenarios.

</details>


### [189] [From Confusion to Clarity: ProtoScore - A Framework for Evaluating Prototype-Based XAI](https://arxiv.org/abs/2511.08361)
*Helena Monke,Benjamin Sae-Chew,Benjamin Fresz,Marco F. Huber*

Main category: cs.LG

TL;DR: 本文针对基于原型的可解释AI方法缺乏标准化基准问题，提出ProtoScore框架评估该类方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 神经网络复杂不透明，基于原型的可解释AI方法有前景，但缺乏标准化基准评估，阻碍领域发展。

Method: 建立ProtoScore框架，集成Co - 12属性，用于评估不同数据类型尤其是时间序列数据的基于原型的可解释AI方法。

Result: 可有效对比原型方法之间以及与其他可解释AI方法。

Conclusion: 该框架有助于从业者选择合适解释方法，降低用户研究成本。

Abstract: The complexity and opacity of neural networks (NNs) pose significant challenges, particularly in high-stakes fields such as healthcare, finance, and law, where understanding decision-making processes is crucial. To address these issues, the field of explainable artificial intelligence (XAI) has developed various methods aimed at clarifying AI decision-making, thereby facilitating appropriate trust and validating the fairness of outcomes. Among these methods, prototype-based explanations offer a promising approach that uses representative examples to elucidate model behavior. However, a critical gap exists regarding standardized benchmarks to objectively compare prototype-based XAI methods, especially in the context of time series data. This lack of reliable benchmarks results in subjective evaluations, hindering progress in the field. We aim to establish a robust framework, ProtoScore, for assessing prototype-based XAI methods across different data types with a focus on time series data, facilitating fair and comprehensive evaluations. By integrating the Co-12 properties of Nauta et al., this framework allows for effectively comparing prototype methods against each other and against other XAI methods, ultimately assisting practitioners in selecting appropriate explanation methods while minimizing the costs associated with user studies. All code is publicly available at https://github.com/HelenaM23/ProtoScore .

</details>


### [190] [Multi-objective Hyperparameter Optimization in the Age of Deep Learning](https://arxiv.org/abs/2511.08371)
*Soham Basu,Frank Hutter,Danny Stoll*

Main category: cs.LG

TL;DR: 介绍首个能整合多目标用户信念的HPO算法PriMO，其在多目标和单目标设置的8个DL基准测试中达最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有HPO算法难以利用先验知识，且无算法能整合多目标先验，而DL从业者常需优化多目标。

Method: 引入可整合多目标用户信念的HPO算法PriMO。

Result: PriMO在8个DL基准测试的多目标和单目标设置中达到最优性能。

Conclusion: PriMO可成为DL从业者新的首选HPO算法。

Abstract: While Deep Learning (DL) experts often have prior knowledge about which hyperparameter settings yield strong performance, only few Hyperparameter Optimization (HPO) algorithms can leverage such prior knowledge and none incorporate priors over multiple objectives. As DL practitioners often need to optimize not just one but many objectives, this is a blind spot in the algorithmic landscape of HPO. To address this shortcoming, we introduce PriMO, the first HPO algorithm that can integrate multi-objective user beliefs. We show PriMO achieves state-of-the-art performance across 8 DL benchmarks in the multi-objective and single-objective setting, clearly positioning itself as the new go-to HPO algorithm for DL practitioners.

</details>


### [191] [EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting](https://arxiv.org/abs/2511.08396)
*Zhiwei Zhang,Xinyi Du,Xuanchi Guo,Weihao Wang,Wenjuan Han*

Main category: cs.LG

TL;DR: 提出EMAformer模型增强Transformer用于多变量时间序列预测，在12个基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: iTransformer在多变量时间序列预测中落后于最新MLP模型，原因是通道间关系不稳定。

Method: 提出EMAformer，引入全局稳定性、相位敏感性和跨轴特异性三个关键归纳偏置，用辅助嵌入套件增强Transformer。

Result: 在12个真实世界基准测试中达到SOTA，MSE平均降低2.73%，MAE平均降低5.15%。

Conclusion: 显著提升了基于Transformer的多变量时间序列预测方法的实际应用能力。

Abstract: Multivariate time series forecasting is crucial across a wide range of domains. While presenting notable progress for the Transformer architecture, iTransformer still lags behind the latest MLP-based models. We attribute this performance gap to unstable inter-channel relationships. To bridge this gap, we propose EMAformer, a simple yet effective model that enhances the Transformer with an auxiliary embedding suite, akin to armor that reinforces its ability. By introducing three key inductive biases, i.e., \textit{global stability}, \textit{phase sensitivity}, and \textit{cross-axis specificity}, EMAformer unlocks the further potential of the Transformer architecture, achieving state-of-the-art performance on 12 real-world benchmarks and reducing forecasting errors by an average of 2.73\% in MSE and 5.15\% in MAE. This significantly advances the practical applicability of Transformer-based approaches for multivariate time series forecasting. The code is available on https://github.com/PlanckChang/EMAformer.

</details>


### [192] [Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment](https://arxiv.org/abs/2511.08399)
*Hua Ye,Hang Ding,Siyuan Chen,Yiyang Jiang,Changyuan Zhang,Xuan Zhang*

Main category: cs.LG

TL;DR: 提出轻量级插件BACL，将模糊负样本转化为课程信号，理论和实践表现良好。


<details>
  <summary>Details</summary>
Motivation: 多数多模态模型将所有负样本一视同仁，忽略了与正样本只有细微差别的模糊负样本。

Method: 提出Boundary - Aware Curriculum with Local Attention (BACL)，包含边界感知负采样器和对比局部注意力损失两个模块，且与现成的双编码器兼容。

Result: 理论预测误差率为O(1/n)，实践中比CLIP的R@1提升达32%，在四个大规模基准测试中创造新的最优结果，且无需额外标签。

Conclusion: BACL是有效的多模态模型改进方法，能利用模糊负样本提升性能。

Abstract: Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.

</details>


### [193] [ARAC: Adaptive Regularized Multi-Agent Soft Actor-Critic in Graph-Structured Adversarial Games](https://arxiv.org/abs/2511.08412)
*Ruochuan Shi,Runyu Lu,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.LG

TL;DR: 提出ARAC解决图结构多智能体强化学习对抗任务中稀疏奖励问题，实验表明其有效。


<details>
  <summary>Details</summary>
Motivation: 在图结构多智能体强化学习对抗任务中，稀疏奖励阻碍有效策略学习。

Method: 提出Adaptive Regularized Multi - Agent Soft Actor - Critic (ARAC)，集成基于注意力的图神经网络和自适应散度正则化机制。

Result: 在追逐和对抗场景实验中，ARAC比基线方法收敛更快、最终成功率更高、跨不同数量智能体扩展性更强。

Conclusion: ARAC在复杂图结构环境中有效。

Abstract: In graph-structured multi-agent reinforcement learning (MARL) adversarial tasks such as pursuit and confrontation, agents must coordinate under highly dynamic interactions, where sparse rewards hinder efficient policy learning. We propose Adaptive Regularized Multi-Agent Soft Actor-Critic (ARAC), which integrates an attention-based graph neural network (GNN) for modeling agent dependencies with an adaptive divergence regularization mechanism. The GNN enables expressive representation of spatial relations and state features in graph environments. Divergence regularization can serve as policy guidance to alleviate the sparse reward problem, but it may lead to suboptimal convergence when the reference policy itself is imperfect. The adaptive divergence regularization mechanism enables the framework to exploit reference policies for efficient exploration in the early stages, while gradually reducing reliance on them as training progresses to avoid inheriting their limitations. Experiments in pursuit and confrontation scenarios demonstrate that ARAC achieves faster convergence, higher final success rates, and stronger scalability across varying numbers of agents compared with MARL baselines, highlighting its effectiveness in complex graph-structured environments.

</details>


### [194] [NeuCLIP: Efficient Large-Scale CLIP Training with Neural Normalizer Optimization](https://arxiv.org/abs/2511.08417)
*Xiyuan Wei,Chih-Jen Lin,Tianbao Yang*

Main category: cs.LG

TL;DR: 提出NeuCLIP优化框架解决CLIP模型对比损失归一化项估计难题，实验显示其优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 传统CLIP模型训练中估计对比损失归一化项需大量计算资源，现有方法对大数据集或小批量效果不佳。

Method: 通过凸分析将对比损失重构成最小化问题，用变分分析转换为对紧凑神经网络的最小化，设计交替优化算法联合训练CLIP模型和辅助网络。

Result: NeuCLIP实现更准确的归一化项估计，在大规模CLIP训练实验中表现优于先前方法。

Conclusion: NeuCLIP框架在解决CLIP模型对比损失归一化项估计问题上有效且性能更优。

Abstract: Accurately estimating the normalization term (also known as the partition function) in the contrastive loss is a central challenge for training Contrastive Language-Image Pre-training (CLIP) models. Conventional methods rely on large batches for approximation, demanding substantial computational resources. To mitigate this issue, prior works introduced per-sample normalizer estimators, which are updated at each epoch in a blockwise coordinate manner to keep track of updated encoders. However, this scheme incurs optimization error that scales with the ratio of dataset size to batch size, limiting effectiveness for large datasets or small batches. To overcome this limitation, we propose NeuCLIP, a novel and elegant optimization framework based on two key ideas: (i) $\textbf{reformulating}$ the contrastive loss for each sample $\textbf{via convex analysis}$ into a minimization problem with an auxiliary variable representing its log-normalizer; and (ii) $\textbf{transforming}$ the resulting minimization over $n$ auxiliary variables (where $n$ is the dataset size) via $\textbf{variational analysis}$ into the minimization over a compact neural network that predicts the log-normalizers. We design an alternating optimization algorithm that jointly trains the CLIP model and the auxiliary network. By employing a tailored architecture and acceleration techniques for the auxiliary network, NeuCLIP achieves more accurate normalizer estimation, leading to improved performance compared with previous methods. Extensive experiments on large-scale CLIP training, spanning datasets from millions to billions of samples, demonstrate that NeuCLIP outperforms previous methods.

</details>


### [195] [Physics-Informed Neural Operators for Cardiac Electrophysiology](https://arxiv.org/abs/2511.08418)
*Hannah Lydon,Milad Kazemi,Martin Bishop,Nicola Paoletti*

Main category: cs.LG

TL;DR: 提出物理信息神经算子（PINO）方法解决心脏电生理中的偏微分方程问题，能跨分辨率和初始条件泛化，模拟效果好且省时。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高、对离散化敏感，经典深度学习方法数据需求大、处理混沌动力学和长期预测有困难，物理信息神经网络有网格分辨率和长期预测稳定性限制。

Method: 提出PINO方法，该方法学习函数空间之间的映射。

Result: PINO模型能准确重现心脏电生理动力学，包括零样本评估，长滚动预测中保持高质量，可将预测分辨率提高至训练分辨率的10倍，且显著减少模拟时间。

Conclusion: PINO方法在心脏电生理模拟中具有高效和可扩展的潜力。

Abstract: Accurately simulating systems governed by PDEs, such as voltage fields in cardiac electrophysiology (EP) modelling, remains a significant modelling challenge. Traditional numerical solvers are computationally expensive and sensitive to discretisation, while canonical deep learning methods are data-hungry and struggle with chaotic dynamics and long-term predictions. Physics-Informed Neural Networks (PINNs) mitigate some of these issues by incorporating physical constraints in the learning process, yet they remain limited by mesh resolution and long-term predictive stability. In this work, we propose a Physics-Informed Neural Operator (PINO) approach to solve PDE problems in cardiac EP. Unlike PINNs, PINO models learn mappings between function spaces, allowing them to generalise to multiple mesh resolutions and initial conditions. Our results show that PINO models can accurately reproduce cardiac EP dynamics over extended time horizons and across multiple propagation scenarios, including zero-shot evaluations on scenarios unseen during training. Additionally, our PINO models maintain high predictive quality in long roll-outs (where predictions are recursively fed back as inputs), and can scale their predictive resolution by up to 10x the training resolution. These advantages come with a significant reduction in simulation time compared to numerical PDE solvers, highlighting the potential of PINO-based approaches for efficient and scalable cardiac EP simulations.

</details>


### [196] [HardFlow: Hard-Constrained Sampling for Flow-Matching Models via Trajectory Optimization](https://arxiv.org/abs/2511.08425)
*Zeyang Li,Kaveh Alim,Navid Azizan*

Main category: cs.LG

TL;DR: 提出HardFlow框架将硬约束采样转化为轨迹优化问题，实验显示其在约束满足和样本质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有投影方法在生成样本时施加硬约束过于严格，会降低样本质量，需要新方法解决。

Method: 将硬约束采样重新表述为轨迹优化问题，利用数值最优控制引导采样轨迹，将复杂约束优化问题转化为易处理的替代问题。

Result: 控制理论分析给出近似误差界限，在机器人、偏微分方程和视觉等领域实验显示HardFlow大幅优于现有方法。

Conclusion: 提出的HardFlow框架有效解决硬约束采样问题，能提高约束满足度和样本质量。

Abstract: Diffusion and flow-matching have emerged as powerful methodologies for generative modeling, with remarkable success in capturing complex data distributions and enabling flexible guidance at inference time. Many downstream applications, however, demand enforcing hard constraints on generated samples (for example, robot trajectories must avoid obstacles), a requirement that goes beyond simple guidance. Prevailing projection-based approaches constrain the entire sampling path to the constraint manifold, which is overly restrictive and degrades sample quality. In this paper, we introduce a novel framework that reformulates hard-constrained sampling as a trajectory optimization problem. Our key insight is to leverage numerical optimal control to steer the sampling trajectory so that constraints are satisfied precisely at the terminal time. By exploiting the underlying structure of flow-matching models and adopting techniques from model predictive control, we transform this otherwise complex constrained optimization problem into a tractable surrogate that can be solved efficiently and effectively. Furthermore, this trajectory optimization perspective offers significant flexibility beyond mere constraint satisfaction, allowing for the inclusion of integral costs to minimize distribution shift and terminal objectives to further enhance sample quality, all within a unified framework. We provide a control-theoretic analysis of our method, establishing bounds on the approximation error between our tractable surrogate and the ideal formulation. Extensive experiments across diverse domains, including robotics (planning), partial differential equations (boundary control), and vision (text-guided image editing), demonstrate that our algorithm, which we name $\textit{HardFlow}$, substantially outperforms existing methods in both constraint satisfaction and sample quality.

</details>


### [197] [An update to PYRO-NN: A Python Library for Differentiable CT Operators](https://arxiv.org/abs/2511.08427)
*Linda-Sophie Schneider,Yipeng Sun,Chengze Ye,Markus Michen,Andreas Maier*

Main category: cs.LG

TL;DR: 介绍基于Python的可微CT重建库PYRO - NN的更新版本，扩展兼容性、引入CUDA内核支持等并给出代码链接。


<details>
  <summary>Details</summary>
Motivation: 深度学习推动CT重建发展，可微算子在结合经典重建技术与数据驱动方法中起关键作用，需更新库以满足需求。

Method: 更新PYRO - NN库，使其兼容PyTorch，引入CUDA内核支持，提供模拟成像伪影等工具。

Result: 得到更新的PYRO - NN库，具有扩展兼容性、高效投影和反投影操作等功能。

Conclusion: 更新后的PYRO - NN库为可微CT重建提供了更强大、灵活的工具。

Abstract: Deep learning has brought significant advancements to X-ray Computed Tomography (CT) reconstruction, offering solutions to challenges arising from modern imaging technologies. These developments benefit from methods that combine classical reconstruction techniques with data-driven approaches. Differentiable operators play a key role in this integration by enabling end-to-end optimization and the incorporation of physical modeling within neural networks.
  In this work, we present an updated version of PYRO-NN, a Python-based library for differentiable CT reconstruction. The updated framework extends compatibility to PyTorch and introduces native CUDA kernel support for efficient projection and back-projection operations across parallel, fan, and cone-beam geometries. Additionally, it includes tools for simulating imaging artifacts, modeling arbitrary acquisition trajectories, and creating flexible, end-to-end trainable pipelines through a high-level Python API. Code is available at: https://github.com/csyben/PYRO-NN

</details>


### [198] [Coherence Mechanisms for Provable Self-Improvement](https://arxiv.org/abs/2511.08440)
*Mehryar Mohri,Jon Schneider,Yifan Wu*

Main category: cs.LG

TL;DR: 本文提出基于连贯性的自改进框架，有严格理论保证，证明连贯性是可证明自改进的必要原则。


<details>
  <summary>Details</summary>
Motivation: 先前大语言模型自改进方法依赖经验启发式，缺乏形式化保证。

Method: 基于连贯性概念，用投影机制更新基线模型，使其连贯且接近原行为，分析直接和两步投影方法。

Result: 机制能实现单调改进，理论保证可扩展到多种情况，建立表征定理。

Conclusion: 连贯性是可证明自改进的基本且必要原则。

Abstract: Self-improvement is a critical capability for large language models and other intelligent systems, enabling them to refine their behavior and internal consistency without external supervision. Despite its importance, prior approaches largely rely on empirical heuristics and lack formal guarantees. In this paper, we propose a principled framework for self-improvement based on the concept of \emph{coherence}, which requires that a model's outputs remain consistent under task-preserving transformations of the input.
  We formalize this concept using projection-based mechanisms that update a baseline model to be coherent while remaining as close as possible to its original behavior. We provide rigorous theoretical guarantees that these mechanisms achieve \emph{monotonic improvement}, measured by a reduction in expected Bregman divergence. Our analysis is comprehensive, covering both \emph{direct} and \emph{two-step} projection methods, and robustly extends these guarantees to non-realizable settings, empirical (finite-sample) distributions, and relaxed coherence constraints.
  Furthermore, we establish a general \emph{characterization theorem}, showing that any mechanism with similar provable improvement guarantees must inherently conform to a coherence-based structure. This culminates in rigidity results under the demand for universal improvement, establishing coherence as a fundamental and, in a formal sense, necessary principle for provable self-improvement.

</details>


### [199] [One Model for All: Universal Pre-training for EEG based Emotion Recognition across Heterogeneous Datasets and Paradigms](https://arxiv.org/abs/2511.08444)
*Xiang Li,You Li,Yazhou Zhang*

Main category: cs.LG

TL;DR: 提出用于EEG分析的通用预训练框架'One Model for All'，实验表明其在多个数据集上表现优异，为EEG分析任务开辟新道路。


<details>
  <summary>Details</summary>
Motivation: EEG情感识别受数据集异质性阻碍，现有方法难以有效迁移知识。

Method: 将学习解耦为两个阶段，一是通过自监督对比学习进行单变量预训练，二是用ART和GAT架构进行多变量微调。

Result: 通用预训练稳定模型，在多个数据集上有显著性能提升，实现新的SOTA性能和跨数据集迁移。

Conclusion: 该框架为不同EEG分析任务提供了更通用、可扩展和有效的预训练模型。

Abstract: EEG-based emotion recognition is hampered by profound dataset heterogeneity (channel/subject variability), hindering generalizable models. Existing approaches struggle to transfer knowledge effectively. We propose 'One Model for All', a universal pre-training framework for EEG analysis across disparate datasets. Our paradigm decouples learning into two stages: (1) Univariate pre-training via self-supervised contrastive learning on individual channels, enabled by a Unified Channel Schema (UCS) that leverages the channel union (e.g., SEED-62ch, DEAP-32ch); (2) Multivariate fine-tuning with a novel 'ART' (Adaptive Resampling Transformer) and 'GAT' (Graph Attention Network) architecture to capture complex spatio-temporal dependencies. Experiments show universal pre-training is an essential stabilizer, preventing collapse on SEED (vs. scratch) and yielding substantial gains on DEAP (+7.65%) and DREAMER (+3.55%). Our framework achieves new SOTA performance on all within-subject benchmarks: SEED (99.27%), DEAP (93.69%), and DREAMER (93.93%). We also show SOTA cross-dataset transfer, achieving 94.08% (intersection) and 93.05% (UCS) on the unseen DREAMER dataset, with the former surpassing the within-domain pre-training benchmark. Ablation studies validate our architecture: the GAT module is critical, yielding a +22.19% gain over GCN on the high-noise DEAP dataset, and its removal causes a catastrophic -16.44% performance drop. This work paves the way for more universal, scalable, and effective pre-trained models for diverse EEG analysis tasks.

</details>


### [200] [Binary Split Categorical feature with Mean Absolute Error Criteria in CART](https://arxiv.org/abs/2511.08470)
*Peng Yu,Yike Chen,Chao Xu,Albert Bifet,Jesse Read*

Main category: cs.LG

TL;DR: 指出在CART算法中MAE准则处理分类特征时无监督数值编码方法不可行，提出新的高效分裂算法。


<details>
  <summary>Details</summary>
Motivation: 解决CART算法中MAE准则处理分类特征的难题，突破现有方法局限。

Method: 研究分析无监督数值编码方法在MAE准则中的不可行性，提出新的高效分裂算法。

Result: 发现无监督数值编码方法不适用于MAE准则，得到新的高效分裂算法。

Conclusion: 现有处理分类特征的方法有局限性，新算法有望提升CART算法处理分类数据的能力。

Abstract: In the context of the Classification and Regression Trees (CART) algorithm, the efficient splitting of categorical features using standard criteria like GINI and Entropy is well-established. However, using the Mean Absolute Error (MAE) criterion for categorical features has traditionally relied on various numerical encoding methods. This paper demonstrates that unsupervised numerical encoding methods are not viable for the MAE criteria. Furthermore, we present a novel and efficient splitting algorithm that addresses the challenges of handling categorical features with the MAE criterion. Our findings underscore the limitations of existing approaches and offer a promising solution to enhance the handling of categorical data in CART algorithms.

</details>


### [201] [Clustering Guided Residual Neural Networks for Multi-Tx Localization in Molecular Communications](https://arxiv.org/abs/2511.08513)
*Ali Sonmez,Erencem Ozbey,Efe Feyzi Mantaroglu,H. Birkan Yilmaz*

Main category: cs.LG

TL;DR: 提出基于聚类的质心校正方法和两种聚类引导的残差神经网络用于分子通信中多发射机定位，实验表明比K - means减少定位误差。


<details>
  <summary>Details</summary>
Motivation: 解决分子通信中扩散的随机性和接收器表面分子分布重叠导致的多发射机精确定位难题。

Method: 引入基于聚类的质心校正方法，提出AngleNN用于方向细化和SizeNN用于聚类大小估计的两种聚类引导残差神经网络。

Result: 相比K - means，2个发射机时减少定位误差69%，4个发射机时减少43%。

Conclusion: 提出的两种方法在多发射机定位上有显著改进。

Abstract: Transmitter localization in Molecular Communication via Diffusion is a critical topic with many applications. However, accurate localization of multiple transmitters is a challenging problem due to the stochastic nature of diffusion and overlapping molecule distributions at the receiver surface. To address these issues, we introduce clustering-based centroid correction methods that enhance robustness against density variations, and outliers. In addition, we propose two clusteringguided Residual Neural Networks, namely AngleNN for direction refinement and SizeNN for cluster size estimation. Experimental results show that both approaches provide significant improvements with reducing localization error between 69% (2-Tx) and 43% (4-Tx) compared to the K-means.

</details>


### [202] [FMMI: Flow Matching Mutual Information Estimation](https://arxiv.org/abs/2511.08552)
*Ivan Butakov,Alexander Semenenko,Alexey Frolov,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出一种新的互信息（MI）估计器，用归一化流代替训练分类器来估计MI，计算高效且精确。


<details>
  <summary>Details</summary>
Motivation: 改进传统判别式方法来估计互信息，提升计算效率和精度。

Method: 学习一个归一化流，将联合分布转换为边缘分布。

Result: 得到了计算高效且精确的MI估计，能很好地适应高维数据和不同真实MI值情况。

Conclusion: 新的MI估计器具有良好的性能，在高维等场景有应用价值。

Abstract: We introduce a novel Mutual Information (MI) estimator that fundamentally reframes the discriminative approach. Instead of training a classifier to discriminate between joint and marginal distributions, we learn a normalizing flow that transforms one into the other. This technique produces a computationally efficient and precise MI estimate that scales well to high dimensions and across a wide range of ground-truth MI values.

</details>


### [203] [The Path Not Taken: RLVR Provably Learns Off the Principals](https://arxiv.org/abs/2511.08567)
*Hanqing Zhu,Zhenyu Zhang,Hanxian Huang,DiJia Su,Zechun Liu,Jiawei Zhao,Igor Fedorov,Hamed Pirsiavash,Zhizhou Sha,Jinwon Lee,David Z. Pan,Zhangyang Wang,Yuandong Tian,Kai Sheng Tai*

Main category: cs.LG

TL;DR: 研究强化学习带可验证奖励（RLVR）优化参数稀疏性悖论，提出三门理论解释其学习动态，指出与SFT优化机制不同，直接应用SFT时代方法有缺陷。


<details>
  <summary>Details</summary>
Motivation: 解释RLVR提高大语言模型推理性能但仅修改少量参数的悖论。

Method: 提出三 - 门理论（Gate I: KL Anchor; Gate II: Model Geometry; Gate III: Precision）解释学习动态，对比RLVR和SFT的学习机制。

Result: 首次给出RLVR学习动态的参数级表征，发现RLVR与SFT优化机制不同，直接应用SFT时代参数高效微调方法有缺陷。

Conclusion: 研究为白盒理解RLVR和设计适用于RLVR的学习算法指明方向。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.
  Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.

</details>


### [204] [Automatic Grid Updates for Kolmogorov-Arnold Networks using Layer Histograms](https://arxiv.org/abs/2511.08570)
*Jamison Moody,James Usevitch*

Main category: cs.LG

TL;DR: 论文介绍KAN网络优势与不足，提出AdaptKAN并展示其在四个任务上超或匹配先前架构性能。


<details>
  <summary>Details</summary>
Motivation: 原KAN架构训练需调整域离散化，典型KAN层不能根据前层输出范围自主更新域。

Method: 提出一种直方图算法用于AdaptKAN，还可检测OOD输入。

Result: AdaptKAN在四个不同任务上超过或匹配先前KAN架构和MLP性能。

Conclusion: AdaptKAN在多个任务中有良好表现，有一定优势。

Abstract: Kolmogorov-Arnold Networks (KANs) are a class of neural networks that have received increased attention in recent literature. In contrast to MLPs, KANs leverage parameterized, trainable activation functions and offer several benefits including improved interpretability and higher accuracy on learning symbolic equations. However, the original KAN architecture requires adjustments to the domain discretization of the network (called the "domain grid") during training, creating extra overhead for the user in the training process. Typical KAN layers are not designed with the ability to autonomously update their domains in a data-driven manner informed by the changing output ranges of previous layers. As an added benefit, this histogram algorithm may also be applied towards detecting out-of-distribution (OOD) inputs in a variety of settings. We demonstrate that AdaptKAN exceeds or matches the performance of prior KAN architectures and MLPs on four different tasks: learning scientific equations from the Feynman dataset, image classification from frozen features, learning a control Lyapunov function, and detecting OOD inputs on the OpenOOD v1.5 benchmark.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [205] [Bi-Objective Evolutionary Optimization for Large-Scale Open Pit Mine Scheduling Problem under Uncertainty with Chance Constraints](https://arxiv.org/abs/2511.08275)
*Ishara Hewa Pathiranage,Aneta Neumann*

Main category: cs.NE

TL;DR: 本文提出露天矿调度问题（OPMSP）的双目标公式，将其集成到三种多目标进化算法中，对比单目标方法，结果显示双目标公式在经济价值和风险间的权衡更优。


<details>
  <summary>Details</summary>
Motivation: 传统确定性方法忽略地质不确定性，导致露天矿调度方案可能次优或不可行，需新方法解决。

Method: 提出双目标OPMSP公式，用整数编码表示解，引入特定初始化和变异算子，集成到三种多目标进化算法。

Result: 分析最多112687个块的矿床，双目标公式比单目标、依赖置信度的方法在经济价值和风险间有更稳健平衡的权衡。

Conclusion: 所提双目标公式在露天矿调度问题上优于单目标、依赖置信度的方法。

Abstract: The open-pit mine scheduling problem (OPMSP) is a complex, computationally expensive process in long-term mine planning, constrained by operational and geological dependencies. Traditional deterministic approaches often ignore geological uncertainty, leading to suboptimal and potentially infeasible production schedules. Chance constraints allow modeling of stochastic components by ensuring probabilistic constraints are satisfied with high probability. This paper presents a bi-objective formulation of the OPMSP that simultaneously maximizes expected net present value and minimizes scheduling risk, independent of the confidence level required for the constraint. Solutions are represented using integer encoding, inherently satisfying reserve constraints. We introduce a domain-specific greedy randomized initialization and a precedence-aware period-swap mutation operator. We integrate these operators into three multi-objective evolutionary algorithms: the global simple evolutionary multi-objective optimizer (GSEMO), a mutation-only variant of multi-objective evolutionary algorithm based on decomposition (MOEA/D), and non-dominated sorting genetic algorithm II (NSGA-II). We compare our bi-objective formulation against the single-objective approach, which depends on a specific confidence level, by analyzing mine deposits consisting of up to 112 687 blocks. Results demonstrate that the proposed bi-objective formulation yields more robust and balanced trade-offs between economic value and risk compared to single-objective, confidence-dependent approach.

</details>


### [206] [Understanding Electro-communication and Electro-sensing in Weakly Electric Fish using Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2511.08436)
*Satpreet H. Singh,Sonja Johnson-Yu,Zhouyang Lu,Aaron Walsman,Federico Pedraja,Denis Turcu,Pratyusha Sharma,Naomi Saphra,Nathaniel B. Sawtell,Kanaka Rajan*

Main category: cs.NE

TL;DR: 提出生物启发计算框架，用RNN和MARL训练人工智能体模拟弱电鱼行为，出现与真实鱼群相符特征，有广泛意义。


<details>
  <summary>Details</summary>
Motivation: 研究弱电鱼在自然环境下电感应和电通信行为及神经活动有实验挑战。

Method: 构建基于RNN的人工智能体，通过MARL训练，让其在虚拟环境中集体觅食。

Result: 训练后的智能体出现与真实鱼群相符特征，双鱼实验揭示电通信作用。

Conclusion: 该工作对弱电鱼神经行为学及其他社交动物研究有广泛意义。

Abstract: Weakly electric fish, like Gnathonemus petersii, use a remarkable electrical modality for active sensing and communication, but studying their rich electrosensing and electrocommunication behavior and associated neural activity in naturalistic settings remains experimentally challenging. Here, we present a novel biologically-inspired computational framework to study these behaviors, where recurrent neural network (RNN) based artificial agents trained via multi-agent reinforcement learning (MARL) learn to modulate their electric organ discharges (EODs) and movement patterns to collectively forage in virtual environments. Trained agents demonstrate several emergent features consistent with real fish collectives, including heavy tailed EOD interval distributions, environmental context dependent shifts in EOD interval distributions, and social interaction patterns like freeloading, where agents reduce their EOD rates while benefiting from neighboring agents' active sensing. A minimal two-fish assay further isolates the role of electro-communication, showing that access to conspecific EODs and relative dominance jointly shape foraging success. Notably, these behaviors emerge through evolution-inspired rewards for individual fitness and emergent inter-agent interactions, rather than through rewarding agents explicitly for social interactions. Our work has broad implications for the neuroethology of weakly electric fish, as well as other social, communicating animals in which extensive recordings from multiple individuals, and thus traditional data-driven modeling, are infeasible.

</details>


### [207] [Spatio-Temporal Cluster-Triggered Encoding for Spiking Neural Networks](https://arxiv.org/abs/2511.08469)
*Lingyun Ke,Minchi Hu*

Main category: cs.NE

TL;DR: 提出基于聚类的ST3D编码方法用于图像编码，在N - MNIST数据集实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有图像编码方案忽略空间关系且时间一致性差，需更好编码方法。

Method: 提出基于聚类的编码方法，用2D空间聚类触发器识别前景区域，扩展到3D时空框架考虑时间邻域。

Result: 在N - MNIST数据集上，ST3D编码器用单层SNN达98.17%分类准确率，优于TTFS编码，用更少尖峰匹配复杂架构性能。

Conclusion: 该方法为神经形态计算应用提供可解释且高效的编码策略。

Abstract: Encoding static images into spike trains is a crucial step for enabling Spiking Neural Networks (SNNs) to process visual information efficiently. However, existing schemes such as rate coding, Poisson encoding, and time-to-first-spike (TTFS) often ignore spatial relationships and yield temporally inconsistent spike patterns. In this article, a novel cluster-based encoding approach is proposed, which leverages local density computation to preserve semantic structure in both spatial and temporal domains. This method introduces a 2D spatial cluster trigger that identifies foreground regions through connected component analysis and local density estimation. Then, extend to a 3D spatio-temporal (ST3D) framework that jointly considers temporal neighborhoods, producing spike trains with improved temporal consistency. Experiments on the N-MNIST dataset demonstrate that our ST3D encoder achieves 98.17% classification accuracy with a simple single-layer SNN, outperforming standard TTFS encoding (97.58%) and matching the performance of more complex deep architectures while using significantly fewer spikes (~3800 vs ~5000 per sample). The results demonstrate that this approach provides an interpretable and efficient encoding strategy for neuromorphic computing applications.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [208] [Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory](https://arxiv.org/abs/2511.08568)
*Jie Ren,Bin Ma,Shuangyan Yang,Benjamin Francis,Ehsan K. Ardestani,Min Si,Dong Li*

Main category: cs.PF

TL;DR: 提出RecMG系统用于分层内存上的向量缓存和预取，在DLRM推理中减少按需提取次数和推理时间。


<details>
  <summary>Details</summary>
Motivation: DLRMs内存需求大，分层内存架构在嵌入向量放置上有挑战。

Method: 提出RecMG系统，用单独ML模型进行缓存和预取，采用新颖可微损失函数。

Result: 相比现有预取器，RecMG减少按需提取次数，在工业规模DLRM推理中最多减少43%的端到端推理时间。

Conclusion: RecMG能有效应对分层内存架构在DLRM推理中的嵌入向量放置问题，提升推理效率。

Abstract: Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [209] [A Service Suite for Specifying Digital Twins for Industry 5.0](https://arxiv.org/abs/2511.07506)
*Izaque Esteves,Regina Braga,José Maria David,Victor Stroele*

Main category: cs.SE

TL;DR: 提出用于预测性维护决策支持的DT - Create套件，经评估可行。


<details>
  <summary>Details</summary>
Motivation: 解决预测性维护中基于数据进行敏捷果断决策的挑战，利用数字孪生处理信息支持决策。

Method: 采用设计科学研究（DSR）方法，经过两个开发周期，并通过案例研究评估。

Result: DT - Create在数据收集存储处理、信息丰富、模型选择、决策支持和自适应等方面具有可行性。

Conclusion: DT - Create套件可用于指定数字孪生，为预测性维护决策提供支持。

Abstract: One of the challenges of predictive maintenance is making decisions based on data in an agile and assertive way. Connected sensors and operational data favor intelligent processing techniques to enrich information and enable decision-making. Digital Twins (DTs) can be used to process information and support decision-making. DTs are a real-time representation of physical machines and generate data that predictive maintenance can use to make assertive and quick decisions. The main contribution of this work is the specification of a suite of services for specifying DTs, called DT-Create, focused on decision support in predictive maintenance. DT-Create suite is based on intelligent techniques, semantic data processing, and self-adaptation. This suite was developed using the Design Science Research (DSR) methodology through two development cycles and evaluated through case studies. The results demonstrate the feasibility of using DT-Create in specifying DTs considering the following aspects: (i) collection, storage, and intelligent processing of data generated by sensors, (ii) enrichment of information through machine learning and ontologies, (iii) use of intelligent techniques to select predictive models that adhere to the available data set, and (iv) decision support and self-adaptation.

</details>


### [210] [SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction](https://arxiv.org/abs/2511.07584)
*Wuyang Zhang,Chenkai Zhang,Zhen Luo,Jianming Ma,Wangming Yuan,Chuqiao Gu,Chenwei Feng*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have transformed software development by enabling automated code generation, yet they frequently suffer from systematic errors that limit practical deployment. We identify two critical failure modes: \textit{logical hallucination} (incorrect control/data-flow reasoning) and \textit{schematic hallucination} (type mismatches, signature violations, and architectural inconsistencies). These errors stem from the absence of explicit, queryable representations of repository-wide semantics.
  This paper presents \textbf{SemanticForge}, which introduces four fundamental algorithmic advances for semantically-aware code generation: (1) a novel automatic reconciliation algorithm for dual static-dynamic knowledge graphs, unifying compile-time and runtime program semantics; (2) a neural approach that learns to generate structured graph queries from natural language, achieving 73\% precision versus 51\% for traditional retrieval; (3) a novel beam search algorithm with integrated SMT solving, enabling real-time constraint verification during generation rather than post-hoc validation; and (4) an incremental maintenance algorithm that updates knowledge graphs in $O(|ΔR| \cdot \log n)$ time while maintaining semantic equivalence.

</details>


### [211] [An Exploratory Eye Tracking Study on How Developers Classify and Debug Python Code in Different Paradigms](https://arxiv.org/abs/2511.07612)
*Samuel W. Flint,Jigyasa Chauhan,Niloofar Mansoor,Bonita Sharif,Robert Dyer*

Main category: cs.SE

TL;DR: 研究探索Python代码中特定范式语言特性对代码理解和调试的影响，发现功能和过程范式标记有混淆，功能范式代码完成时间长，范式改变不影响调试能力，但功能代码开发者信心低。


<details>
  <summary>Details</summary>
Motivation: 过往研究未关注特定范式语言特性对代码理解的影响，本研究旨在揭示哪些范式特定特性影响代码理解和调试，以及多范式代码对开发者能力的影响。

Method: 开展探索性实证眼动追踪研究，招募29名开发者，让其完成Python的分类和调试任务，并记录眼动。

Result: 功能和过程范式标记有混淆，功能范式代码完成时间最长，范式改变不影响调试能力，功能代码开发者信心低，调试时阅读模式有显著差异，分类时开发者不一定阅读范式相关标记类型。

Conclusion: 特定范式语言特性在代码分类和调试中存在不同影响，功能范式代码在完成时间和开发者信心方面有独特表现。

Abstract: Modern programming languages, such as Python, support language features from several paradigms, such as object-oriented, procedural, and functional. Research has shown that code written in some paradigms can be harder to comprehend, but to date, no research has looked at which paradigm-specific language features impact comprehension. To this end, this study seeks to uncover which paradigm-specific features impactcomprehension and debugging of code or how multi-paradigm code might affect a developer's ability to do so. We present an exploratory empirical eye-tracking study to investigate 1) how developers classify the predominant paradigm in Python code and 2) how the paradigm affects their ability to debug Python code. The goal is to uncover if specific language features are looked at more often while classifying and debugging code with a predominant paradigm. Twenty-nine developers (primarily students) were recruited for the study and were each given four classification and four debugging tasks in Python. Eye movements were recorded during all the tasks. The results indicate confusion in labeling Functional and Procedural paradigms, but not Object-Oriented. The code with predominantly functional paradigms also took the longest to complete. Changing the predominant paradigm did not affect the ability to debug the code, though developers did rate themselves with lower confidence for Functional code. We report significant differences in reading patterns during debugging, especially in the Functional code. During classification, results show that developers do not necessarily read paradigm-relevant token types.

</details>


### [212] [A Self-Improving Architecture for Dynamic Safety in Large Language Models](https://arxiv.org/abs/2511.07645)
*Tyler Slater*

Main category: cs.SE

TL;DR: 现有软件架构静态、安全保障方法不可扩展，本文提出SISF框架，经测试能降低攻击成功率且误报率为0，证明自适应架构保障AI安全可行。


<details>
  <summary>Details</summary>
Motivation: 现有软件架构静态，安全保障方法不可扩展，系统易受新的对抗威胁，需设计能让AI系统在运行时自主持续调整安全协议的软件架构。

Method: 提出Self - Improving Safety Framework (SISF)，将未受保护、未对齐的基础大语言模型与动态反馈循环结合，循环包括用于违规检测的AI Adjudicator和能自主生成新安全策略的Policy Synthesis Module。

Result: 使用520提示的AdvBench数据集评估，未保护模型100%易受攻击，SISF检测到237次违规，合成234个新策略，将攻击成功率降至45.58%，在良性提示测试中误报率为0。

Conclusion: 基于自适应原则的AI安全架构方法是可行有效的策略，该框架为构建更强大、有弹性和可扩展的AI系统提供了实践路径，将安全保障从静态的部署前活动转变为自动化的运行时过程。

Abstract: Context: The integration of Large Language Models (LLMs) into core software systems is accelerating. However, existing software architecture patterns are static, while current safety assurance methods are not scalable, leaving systems vulnerable to novel adversarial threats.
  Objective: To design, implement, and evaluate a novel software architecture that enables an AI-driven system to autonomously and continuously adapt its own safety protocols at runtime.
  Method: We propose the Self-Improving Safety Framework (SISF), a runtime architecture that couples an unprotected, unaligned base LLM (mistralai/Mistral-7B-v0.1) with a dynamic feedback loop. This loop consists of an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) that autonomously generates new, generalized safety policies (both heuristic and semantic) in response to failures.
  Results: We conducted a dynamic learning evaluation using the 520-prompt AdvBench dataset. The unprotected model was 100% vulnerable. Our SISF, starting from zero policies, demonstrated a clear learning curve: it detected 237 breaches, autonomously synthesized 234 new policies, and reduced the overall Attack Success Rate (ASR) to 45.58%. In a subsequent test on 520 benign prompts, the SISF achieved a 0.00% False Positive Rate (FPR), proving its ability to adapt without compromising user utility.
  Conclusion: An architectural approach to AI safety, based on the principles of self-adaptation, is a viable and effective strategy. Our framework demonstrates a practical path towards building more robust, resilient, and scalable AI-driven systems, shifting safety assurance from a static, pre-deployment activity to an automated, runtime process.

</details>


### [213] [Smart but Costly? Benchmarking LLMs on Functional Accuracy and Energy Efficiency](https://arxiv.org/abs/2511.07698)
*Mohammadjavad Mehditabar,Saurabhsingh Rajput,Antonio Mastropaolo,Tushar Sharma*

Main category: cs.SE

TL;DR: 提出BRACE框架评估代码语言模型的能源效率和功能正确性，对22个模型进行基准测试并提出两种评级方法，分析模型表现并为从业者提供选择指导。


<details>
  <summary>Details</summary>
Motivation: AI技术在软件开发中应用加速，需系统评估其环境影响和功能正确性，现有方法缺乏评估代码语言模型准确性 - 能源权衡的系统框架。

Method: 提出BRACE框架，对22个模型在代码生成和总结任务上进行基准测试，提出CIRC和OTER两种评级方法。

Result: 模型在代码总结任务中表现更好，模型大小对评级影响不大。

Conclusion: BRACE框架可帮助从业者基于证据选择模型，根据部署优先级选择评级方法。

Abstract: The rapid advancement of AI technologies and their accelerated adoption in software development necessitates a systematic evaluation of their environmental impact alongside functional correctness. While prior studies have examined sustainability in large language models, existing approaches lack systematic frameworks for evaluating accuracy-energy trade-offs in Code Language Models (CLMs). In this paper, we present a framework, BRACE, to benchmark CLMs on a unified scale of energy efficiency and functional correctness (referred to as accuracy). We benchmark 22 state-of-the-art models on code generation and summarization tasks, proposing two rating methods: Concentric Incremental Rating Circles (CIRC) and Observation to Expectation Rating (OTER). CIRC provides deterministic Euclidean-based rankings with static trade-offs that are robust to outliers, and OTER offers trend-aware evaluation with dynamic trade-offs that capture the complex correlation between energy and accuracy, each offering a distinct perspective and addressing the problem in a unique way. These rating methods enable us to rate LLMs on a 1-5 scale reflecting their combined capabilities in terms of energy efficiency and functional correctness. Our analysis reveals models generally perform better in the code summarization tasks as they are not enforced to generate a grammar-based and syntactically correct output. Also, we find that models' size does not have a significant impact on their ratings, indicating that if models utilize their parameters efficiently, they can be ranked higher on these scales. The proposed BRACE framework empowers practitioners to make evidence-based model selections that balance sustainability with task requirements, guiding rating choice -- CIRC for deterministic comparisons or OTER for trend-aware evaluation -- based on deployment priorities.

</details>


### [214] [Post Processing Graphical User Interface for Heat Flow Visualization](https://arxiv.org/abs/2511.07709)
*Lars Olt,Luis Diego Fonseca Flores,Ian Mckinley*

Main category: cs.SE

TL;DR: 本文介绍了一个用MATLAB和C++构建的GUI，用于解决Thermal Desktop中热流指标提取和可视化软件有限的问题，还提出高效加载指标的方法并反思不足、展望未来。


<details>
  <summary>Details</summary>
Motivation: 当前用于提取和可视化Thermal Desktop中热流相关指标的软件有限，阻碍热工程师快速分析结果。

Method: 用MATLAB和C++构建GUI，利用TD的API（OpenTD）和自定义解析器，借助TD的压缩解决方案结果（CSR）文件的副作用高效加载温度、电导和子模型指标。

Result: 该方法能将将模型节点和导体与子模型ID关联的运行时间缩短几个数量级。

Conclusion: 反思了该数据读取方法的不足，考虑了GUI的未来，并为后续OpenTD版本提供了建议。

Abstract: Thermal Desktop (TD) is an industry-standard thermal analysis tool used to create and analyze thermal models for landers, rovers, spacecraft, and instrument payloads. Currently, limited software exists to extract and visualize metrics relevant to heat flow within TD, impeding thermal engineers from analyzing their results quickly. This paper discusses a graphical user interface (GUI) built in MATLAB and C++ which uses TDs application programming interface (API), OpenTD, and a custom parser to address this void. Specifically, we present a method for efficiently loading temperature, conductance, and submodel metrics using a side effect of TDs Compressed Solution Results (CSR) files. This approach can reduce the runtime for correlating model nodes and conductors with submodel IDs by orders of magnitude. Lastly, we reflect on the shortcomings of this method for reading data, consider the future of the GUI, and provide recommendations for subsequent OpenTD releases.

</details>


### [215] [Event-Driven Inconsistency Detection Between UML Class and Sequence Diagrams](https://arxiv.org/abs/2511.07742)
*Luan Lazzari,Kleinner Farias*

Main category: cs.SE

TL;DR: 提出Harmony Validator工具检测UML模型不一致性，经学生案例研究表明其有助于理解模型一致性和促进反思性学习。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程建模中师生难以理解和管理模型不一致性的问题。

Method: 开发Harmony Validator工具，采用事件驱动架构实时监测建模动作并报告不一致性，还进行学生案例研究。

Result: Harmony Validator工具能提升对模型完整性的认知，支持设计工件迭代优化。

Conclusion: Harmony Validator有助于软件建模教育中对模型一致性的理解和反思性学习实践。

Abstract: Modeling is a central and demanding activity in software engineering that requires skills such as abstraction, consistency maintenance, and precise communication. These skills are difficult to master and even harder to teach effectively. Educators and students often struggle to understand and manage inconsistencies that arise during the modeling process. To address this challenge, we present \texttt{Harmony Validator}, a tool integrated as a plugin for the Papyrus modeling environment, designed to automatically detect and report inconsistencies in UML models, including class and sequence diagrams. The tool adopts an event-driven architecture that continuously monitors modeling actions and notifies users of emerging inconsistencies in real time. This approach enhances awareness of model integrity and supports the iterative refinement of design artifacts. The paper describes the architecture, detection mechanisms, and usage scenarios of Harmony Validator. It also includes a case study conducted with students in a software engineering course to evaluate the perceived usefulness and benefits of UML modeling in teaching and learning. Our results indicate that Harmony Validator fosters a better understanding of model consistency and promotes reflective learning practices in software modeling education.

</details>


### [216] [Uncovering Scientific Software Sustainability through Community Engagement and Software Quality Metrics](https://arxiv.org/abs/2511.07851)
*Sharif Ahmed,Addi Malviya Thakur,Gregory R. Watson,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 探讨GitHub上科学开源软件项目可持续性，提出新可视化技术，分析显示相似领域项目可持续性不同，项目特定反馈对维护软件质量重要。


<details>
  <summary>Details</summary>
Motivation: 科学开源软件项目长期可持续性是重大挑战，本文旨在探索其可持续性。

Method: 将可持续性映射到文献中的仓库指标，挖掘十个知名项目数据，进行多模态分析、统计分析和自然语言分析。

Result: 提出新可视化技术替代传统可视化；发现相似领域项目可持续性不同；自然语言分析支持文献观点，项目特定反馈对维护软件质量关键。

Conclusion: 可视化和分析方法为研究人员、资助者和开发者提供软件长期可持续性的关键见解。

Abstract: Scientific open-source software (Sci-OSS) projects are critical for advancing research, yet sustaining these projects long-term remains a major challenge. This paper explores the sustainability of Sci-OSS hosted on GitHub, focusing on two factors drawn from stewardship organizations: community engagement and software quality. We map sustainability to repository metrics from the literature and mined data from ten prominent Sci-OSS projects. A multimodal analysis of these projects led us to a novel visualization technique, providing a robust way to display both current and evolving software metrics over time, replacing multiple traditional visualizations with one. Additionally, our statistical analysis shows that even similar-domain projects sustain themselves differently. Natural language analysis supports claims from the literature, highlighting that project-specific feedback plays a key role in maintaining software quality. Our visualization and analysis methods offer researchers, funders, and developers key insights into long-term software sustainability.

</details>


### [217] [LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost](https://arxiv.org/abs/2511.07865)
*Daisuke Kikuta,Hiroki Ikeuchi,Kengo Tajiri*

Main category: cs.SE

TL;DR: 本文提出ChaosEater系统，用大语言模型自动化混沌工程（CE）全周期，在Kubernetes系统案例研究中表现出低成本和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有CE工具在实验规划和系统改进方面需人工操作，劳动密集且需多领域专业知识，为解决此问题并让任何人低成本构建弹性系统。

Method: 预定义符合系统CE周期的代理工作流，将工作流细分流程分配给大语言模型，针对Kubernetes软件系统，通过软件工程任务完成CE周期。

Result: 在小规模和大规模Kubernetes系统案例研究中，能以极低时间和金钱成本持续完成合理CE周期，其周期经人类工程师和大语言模型定性验证。

Conclusion: ChaosEater系统能有效自动化CE全周期，以低成本实现系统弹性构建。

Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.

</details>


### [218] [Testing Question Answering Software with Context-Driven Question Generation](https://arxiv.org/abs/2511.07924)
*Shuang Liu,Zhirun Zhang,Jinhao Dong,Zan Wang,Qingchao Shen,Junjie Chen,Wei Lu,Xiaoyong Du*

Main category: cs.SE

TL;DR: 本文介绍用于测试问答系统的上下文驱动问题生成方法CQ^2A，实验表明其在多方面优于现有方法，生成测试用例可降低问答软件错误率。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统测试方法生成问题不自然、缺乏多样性和相关性，为确保问答系统质量，需改进测试方法。

Method: 提出CQ^2A方法，从上下文提取实体和关系形成答案，用大语言模型生成问题，还提出一致性验证和约束检查增加输出可靠性。

Result: 在三个数据集上实验，CQ^2A在错误检测能力、问题自然度和上下文覆盖率上优于现有方法，生成测试用例用于微调可降低问答软件错误率。

Conclusion: CQ^2A是一种有效的问答系统测试方法，能提高测试效果和问答软件质量。

Abstract: Question-answering software is becoming increasingly integrated into our daily lives, with prominent examples including Apple Siri and Amazon Alexa. Ensuring the quality of such systems is critical, as incorrect answers could lead to significant harm. Current state-of-the-art testing approaches apply metamorphic relations to existing test datasets, generating test questions based on these relations. However, these methods have two key limitations. First, they often produce unnatural questions that humans are unlikely to ask, reducing the effectiveness of the generated questions in identifying bugs that might occur in real-world scenarios. Second, these questions are generated from pre-existing test datasets, ignoring the broader context and thus limiting the diversity and relevance of the generated questions.
  In this work, we introduce CQ^2A, a context-driven question generation approach for testing question-answering systems. Specifically, CQ^2A extracts entities and relationships from the context to form ground truth answers, and utilizes large language models to generate questions based on these ground truth answers and the surrounding context. We also propose the consistency verification and constraint checking to increase the reliability of LLM's outputs. Experiments conducted on three datasets demonstrate that CQ^2A outperforms state-of-the-art approaches on the bug detection capability, the naturalness of the generated questions as well as the coverage of the context. Moreover, the test cases generated by CQ^2A reduce error rate when utilized for fine-tuning the QA software under test

</details>


### [219] ["I need to learn better searching tactics for privacy policy laws.'' Investigating Software Developers' Behavior When Using Sources on Privacy Issues](https://arxiv.org/abs/2511.08059)
*Stefan Albert Horstmann,Sandy Hong,Maziar Niazian,Cristiana Santos,Alena Naiakshina*

Main category: cs.SE

TL;DR: 研究30名开发者在隐私敏感场景下利用不同信息源进行隐私开发的情况，发现现有信息源均有不足，需更好的隐私资源。


<details>
  <summary>Details</summary>
Motivation: 了解当前信息源在支持开发者进行隐私敏感实现方面的有效性。

Method: 对30名开发者进行定性研究，让其在隐私敏感场景中利用个人知识、在线资源和AI助手识别隐私问题并提出措施，通过出声思考和后续访谈观察决策过程。

Result: 开发者在使用个人知识、网络内容和AI助手时均遇到困难，个人知识不足，网络内容复杂，AI助手缺乏上下文相关性。

Conclusion: 现有隐私开发支持存在重大缺陷，需要为开发者提供更易获取、理解和可操作的隐私资源。

Abstract: Since the introduction of the European General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), software developers increasingly have to make privacy-related decisions during system design and implementation. However, past research showed that they often lack legal expertise and struggle with privacy-compliant development. To shed light on how effective current information sources are in supporting them with privacy-sensitive implementation, we conducted a qualitative study with 30 developers. Participants were presented with a privacy-sensitive scenario and asked to identify privacy issues and suggest measures using their knowledge, online resources, and an AI assistant. We observed developers' decision-making in think-aloud sessions and discussed it in follow-up interviews. We found that participants struggled with all three sources: personal knowledge was insufficient, web content was often too complex, and while AI assistants provided clear and user-tailored responses, they lacked contextual relevance and failed to identify scenario-specific issues. Our study highlights major shortcomings in existing support for privacy-related development tasks. Based on our findings, we discuss the need for more accessible, understandable, and actionable privacy resources for developers.

</details>


### [220] [A Small Leak Sinks All: Exploring the Transferable Vulnerability of Source Code Models](https://arxiv.org/abs/2511.08127)
*Weiye Li,Wenyi Tang*

Main category: cs.SE

TL;DR: 本文系统研究传统SCMs和LLM4Code内在漏洞可迁移性，提出生成实用对抗样本的方法，实验显示构造的对抗样本效果超现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有研究对SCM可迁移漏洞探索不足，缺乏生成有效对抗样本的实用方法，且未关注LLM4Code。

Method: 提出受害者无关方法，设计HABITAT，包括定制扰动插入机制和分层强化学习框架，进行漏洞内在可迁移性分析。

Result: 基于传统SCMs构造的对抗样本对LLM4Code攻击成功率达64%，超现有技术15%以上。

Conclusion: 研究结果指出未来开发鲁棒防御的关键关注点。

Abstract: Source Code Model learn the proper embeddings from source codes, demonstrating significant success in various software engineering or security tasks. The recent explosive development of LLM extends the family of SCMs,bringing LLMs for code that revolutionize development workflows. Investigating different kinds of SCM vulnerability is the cornerstone for the security and trustworthiness of AI-powered software ecosystems, however, the fundamental one, transferable vulnerability, remains critically underexplored. Existing studies neither offer practical ways, i.e. require access to the downstream classifier of SCMs, to produce effective adversarial samples for adversarial defense, nor give heed to the widely used LLM4Code in modern software development platforms and cloud-based integrated development environments. Therefore, this work systematically studies the intrinsic vulnerability transferability of both traditional SCMs and LLM4Code, and proposes a victim-agnostic approach to generate practical adversarial samples. We design HABITAT, consisting of a tailored perturbation-inserting mechanism and a hierarchical Reinforcement Learning framework that adaptively selects optimal perturbations without requiring any access to the downstream classifier of SCMs. Furthermore, an intrinsic transferability analysis of SCM vulnerabilities is conducted, revealing the potential vulnerability correlation between traditional SCMs and LLM4Code, together with fundamental factors that govern the success rate of victim-agnostic transfer attacks. These findings of SCM vulnerabilities underscore the critical focal points for developing robust defenses in the future. Experimental evaluation demonstrates that our constructed adversarial examples crafted based on traditional SCMs achieve up to 64% success rates against LLM4Code, surpassing the state-of-the-art by over 15%.

</details>


### [221] [OWLAPY: A Pythonic Framework for OWL Ontology Engineering](https://arxiv.org/abs/2511.08232)
*Alkid Baci,Luke Friedrichs,Caglar Demir,Axel-Cyrille Ngonga Ngomo*

Main category: cs.SE

TL;DR: 介绍OWLAPY，一个用于OWL本体工程的Python框架，有多种功能且开源可下载。


<details>
  <summary>Details</summary>
Motivation: 为用户提供一个灵活的Python库用于高级本体工程，满足从Java环境过渡的需求。

Method: 集成原生Python推理器和支持外部Java推理器，提供核心本体组件的多种实现和格式转换能力，允许自定义工作流利用大语言模型。

Result: 创建了OWLAPY框架，在GitHub和PyPI上开源，下载量超50,000。

Conclusion: OWLAPY是一个经过良好测试的软件框架，适用于高级本体工程。

Abstract: In this paper, we introduce OWLAPY, a comprehensive Python framework for OWL ontology engineering. OWLAPY streamlines the creation, modification, and serialization of OWL 2 ontologies. It uniquely integrates native Python-based reasoners with support for external Java reasoners, offering flexibility for users. OWLAPY facilitates multiple implementations of core ontology components and provides robust conversion capabilities between OWL class expressions and formats such as Description Logics, Manchester Syntax, and SPARQL. It also allows users to define custom workflows to leverage large language models (LLMs) in ontology generation from natural language text. OWLAPY serves as a well-tested software framework for users seeking a flexible Python library for advanced ontology engineering, including those transitioning from Java-based environments. The project is publicly available on GitHub at https://github.com/dice-group/owlapy and on the Python Package Index (PyPI) at https://pypi.org/project/owlapy/ , with over 50,000 downloads at the time of writing.

</details>


### [222] [Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale](https://arxiv.org/abs/2511.08475)
*Yangxiao Cai,Ruiyin Li,Peng Liang,Mojtaba Shahin,Zengyang Li*

Main category: cs.SE

TL;DR: 文章研究基于大语言模型的多智能体系统（LLM - based MASs）用于软件工程任务的设计，收集94篇论文，得出常见任务、关注的质量属性、设计模式和设计理由，并给出设计启示。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程任务复杂度上升，LLM - based MASs受关注，但缺乏系统研究其设计，包括关注的质量属性、设计模式和设计理由。

Method: 收集94篇关于LLM - based MASs用于软件工程任务的论文进行研究。

Result: （1）代码生成是最常见的软件工程任务；（2）功能适用性是设计者最关注的质量属性；（3）基于角色的合作是最常采用的设计模式；（4）提高生成代码质量是最常见的设计理由。

Conclusion: 基于研究结果，给出支持软件工程任务的LLM - based MASs设计的启示。

Abstract: As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which the designers mainly focus, the design patterns used by the designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.

</details>


### [223] [Can Large Language Models Simulate Symbolic Execution Output Like KLEE?](https://arxiv.org/abs/2511.08530)
*Rong Feng,Vanisha Gupta,Vivek Patel,Viroopaksh Reddy Ernampati,Suman Saha*

Main category: cs.SE

TL;DR: 研究用GPT - 4o模拟KLEE输出以节省符号执行的时间和资源，用100个C程序测试，准确率约20%，显示了当前大语言模型在模拟符号执行方面的能力。


<details>
  <summary>Details</summary>
Motivation: KLEE在处理大量分支路径的程序时速度慢、资源消耗大，探索大语言模型如GPT - 4o能否替代部分符号执行工作以节省时间和资源。

Method: 使用100个C程序的数据集，测试GPT - 4o预测KLEE输出和识别程序中最复杂路径的能力。

Result: 在生成类似KLEE输出和识别最受约束路径方面约有20%的准确率。

Conclusion: 该初步研究展示了当前大语言模型在模拟符号执行方面的能力和局限性。

Abstract: Symbolic execution helps check programs by exploring different paths based on symbolic inputs. Tools like KLEE are commonly used because they can automatically detect bugs and create test cases. But one of KLEE's biggest issues is how slow it can get when programs have lots of branching paths-it often becomes too resource-heavy to run on large or complex code. In this project, we wanted to see if a large language model like GPT-4o could simulate the kinds of outputs that KLEE generates. The idea was to explore whether LLMs could one day replace parts of symbolic execution to save time and resources.
  One specific goal was to have GPT-4o identify the most constrained path in a program, this is the execution path with the most symbolic conditions. These paths are especially important because they often represent edge cases that are harder to test and more likely to contain deep bugs. However, figuring this out usually requires fully running KLEE, which can be expensive. So, we tested whether GPT-4o could predict the KLEE outputs and the most complex path using a dataset of 100 C programs. Our results showed about 20% accuracy in generating KLEE-like outputs and identifying the most constrained path. While not highly accurate, this early work helps show what current LLMs can and can't do when it comes to simulating symbolic execution.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [224] [Forecasting implied volatility surface with generative diffusion models](https://arxiv.org/abs/2511.07571)
*Chen Jin,Ankush Agarwal*

Main category: q-fin.CP

TL;DR: 本文引入条件去噪扩散概率模型生成无套利隐含波动率曲面，优于基于GAN的方法，还解决了历史数据含套利机会的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的方法生成无套利隐含波动率曲面不够稳定准确，且历史数据存在套利机会与生成目标冲突。

Method: 引入条件去噪扩散概率模型，以丰富市场变量为条件；在损失函数中加入标准套利惩罚项，并采用基于信噪比的无参数加权方案动态调整惩罚强度；对权衡进行形式分析并给出收敛证明。

Result: 模型在捕捉隐含波动率动态的典型事实方面显著优于领先的基于GAN的模型。

Conclusion: 该模型是生成无套利隐含波动率曲面更稳定准确的替代方案，惩罚项引入的偏差可控，能使模型趋向无套利曲面流形且生成分布接近真实数据。

Abstract: We introduce a conditional Denoising Diffusion Probabilistic Model (DDPM) for generating arbitrage-free implied volatility (IV) surfaces, offering a more stable and accurate alternative to existing GAN-based approaches. To capture the path-dependent nature of volatility dynamics, our model is conditioned on a rich set of market variables, including exponential weighted moving averages (EWMAs) of historical surfaces, returns and squared returns of underlying asset, and scalar risk indicators like VIX. Empirical results demonstrate our model significantly outperforms leading GAN-based models in capturing the stylized facts of IV dynamics. A key challenge is that historical data often contains small arbitrage opportunities in the earlier dataset for training, which conflicts with the goal of generating arbitrage-free surfaces. We address this by incorporating a standard arbitrage penalty into the loss function, but apply it using a novel, parameter-free weighting scheme based on the signal-to-noise ratio (SNR) that dynamically adjusts the penalty's strength across the diffusion process. We also show a formal analysis of this trade-off and provide a proof of convergence showing that the penalty introduces a small, controllable bias that steers the model toward the manifold of arbitrage-free surfaces while ensuring the generated distribution remains close to the real-world data.

</details>


### [225] [An extreme Gradient Boosting (XGBoost) Trees approach to Detect and Identify Unlawful Insider Trading (UIT) Transactions](https://arxiv.org/abs/2511.08306)
*Krishna Neupane,Igor Griva*

Main category: q-fin.CP

TL;DR: 使用XGBoost检测内幕交易，准确率达97%并能对特征排序。


<details>
  <summary>Details</summary>
Motivation: 企业内部人员可能利用非公开信息进行非法证券交易，人工检测困难，而机器学习在分析复杂数据上有潜力。

Method: 应用XGBoost算法来识别和检测非法内幕交易活动。

Result: XGBoost能以97%的高精度识别非法交易，并能对检测欺诈活动中最重要的特征进行排序。

Conclusion: XGBoost在检测非法内幕交易方面表现出色，可有效应对识别和检测挑战。

Abstract: Corporate insiders have control of material non-public preferential information (MNPI). Occasionally, the insiders strategically bypass legal and regulatory safeguards to exploit MNPI in their execution of securities trading. Due to a large volume of transactions a detection of unlawful insider trading becomes an arduous task for humans to examine and identify underlying patterns from the insider's behavior. On the other hand, innovative machine learning architectures have shown promising results for analyzing large-scale and complex data with hidden patterns. One such popular technique is eXtreme Gradient Boosting (XGBoost), the state-of-the-arts supervised classifier. We, hence, resort to and apply XGBoost to alleviate challenges of identification and detection of unlawful activities. The results demonstrate that XGBoost can identify unlawful transactions with a high accuracy of 97 percent and can provide ranking of the features that play the most important role in detecting fraudulent activities.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [226] [Optimal Cash Transfers and Microinsurance to Reduce Social Protection Costs](https://arxiv.org/abs/2511.07431)
*Pablo Azcue,Corina Constantinescu,José Miguel Flores-Contró,Nora Muler*

Main category: q-fin.RM

TL;DR: 本文研究小额保险作为现金转移（CT）计划补充工具的作用，通过建模和优化控制确定注资金额，数值例子表明预防性CT能更有效利用资源。


<details>
  <summary>Details</summary>
Motivation: 鉴于小额保险作为社会保护策略补充工具的重要性日益增加，且在资源有限的低收入国家，CT计划的精准靶向对资源有效利用至关重要，因此研究小额保险对CT计划的补充作用。

Method: 采用Kovacevic和Pflug（2011）提出的分段确定性马尔可夫过程对家庭资本建模，运用动态规划技术推导与确定注资金额的最优控制问题相关的HJB方程。

Result: HJB方程存在可数值近似的粘性解，在某些特殊情况下可得到解析解；数值例子显示存在高于贫困阈值的最优注资水平。

Conclusion: 预防性CT比反应性CT成本更低，能实现资源的有效利用。

Abstract: Design and implementation of appropriate social protection strategies is one of the main targets of the United Nation's Sustainable Development Goal (SDG) 1: No Poverty. Cash transfer (CT) programmes are considered one of the main social protection strategies and an instrument for achieving SDG 1. Targeting consists of establishing eligibility criteria for beneficiaries of CT programmes. In low-income countries, where resources are limited, proper targeting of CTs is essential for an efficient use of resources. Given the growing importance of microinsurance as a complementary tool to social protection strategies, this study examines its role as a supplement to CT programmes. In this article, we adopt the piecewise-deterministic Markov process introduced in Kovacevic and Pflug (2011) to model the capital of a household, which when exposed to large proportional capital losses (in contrast to the classical Cramér-Lundberg model) can push them into the poverty area. Striving for cost-effective CT programmes, we optimise the expected discounted cost of keeping the household's capital above the poverty line by means of injection of capital (as a direct capital transfer). Using dynamic programming techniques, we derive the Hamilton-Jacobi-Bellman (HJB) equation associated with the optimal control problem of determining the amount of capital to inject over time. We show that this equation admits a viscosity solution that can be approximated numerically. Moreover, in certain special cases, we obtain closed-form expressions for the solution. Numerical examples show that there is an optimal level of injection above the poverty threshold, suggesting that efficient use of resources is achieved when CTs are preventive rather than reactive, since injecting capital into households when their capital levels are above the poverty line is less costly than to do so only when it falls below the threshold.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [227] [RL-Exec: Impact-Aware Reinforcement Learning for Opportunistic Optimal Liquidation, Outperforms TWAP and a Book-Liquidity VWAP on BTC-USD Replays](https://arxiv.org/abs/2511.07434)
*Enzo Duflot,Stanislas Robineau*

Main category: q-fin.ST

TL;DR: 研究BTC - USD限价订单簿上固定期限内的机会主义最优清算，提出RL - Exec代理，评估显示其显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 研究BTC - USD限价订单簿上固定期限内的机会主义最优清算问题。

Method: 提出PPO代理RL - Exec，在历史回放数据上训练，结合多种因素；采用严格时间分割评估，与TWAP和类似VWAP基线对比，用统计推断方法。

Result: 在2020年2月测试集上，RL - Exec显著优于两个基线，且执行时间越长差距越大。

Conclusion: RL - Exec在BTC - USD限价订单簿清算中表现良好，能实现更好的清算效果。

Abstract: We study opportunistic optimal liquidation over fixed deadlines on BTC-USD limit-order books (LOB). We present RL-Exec, a PPO agent trained on historical replays augmented with endogenous transient impact (resilience), partial fills, maker/taker fees, and latency. The policy observes depth-20 LOB features plus microstructure indicators and acts under a sell-only inventory constraint to reach a residual target. Evaluation follows a strict time split (train: Jan-2020; test: Feb-2020) and a per-day protocol: for each test day we run ten independent start times and aggregate to a single daily score, avoiding pseudo-replication. We compare the agent to (i) TWAP and (ii) a VWAP-like baseline allocating using opposite-side order-book liquidity (top-20 levels), both executed on identical timestamps and costs. Statistical inference uses one-sided Wilcoxon signed-rank tests on daily RL-baseline differences with Benjamini-Hochberg FDR correction and bootstrap confidence intervals. On the Feb-2020 test set, RL-Exec significantly outperforms both baselines and the gap increases with the execution horizon (+2-3 bps at 30 min, +7-8 bps at 60 min, +23 bps at 120 min).
  Code: github.com/Giafferri/RL-Exec

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [228] [Forecast-to-Fill: Benchmark-Neutral Alpha and Billion-Dollar Capacity in Gold Futures (2015-2025)](https://arxiv.org/abs/2511.08571)
*Mainak Singha,Jose Aguilera-Toste,Vinayak Lahiri*

Main category: q-fin.TR

TL;DR: 研究用趋势和动量变量对黄金资产生成样本外阿尔法收益，策略表现佳且具统计显著性，结论是预测到执行的工程可转化收益。


<details>
  <summary>Details</summary>
Motivation: 测试简单可解释的状态变量（趋势和动量）能否在黄金资产中产生持久的样本外阿尔法收益。

Method: 采用2015 - 2025年滚动10年训练和6个月测试的前向推进方法，通过分数调整的凯利 sizing和基于ATR的退出策略将平滑的趋势 - 动量信号转化为考虑成本和冲击的头寸。

Result: 样本外策略夏普比率2.88，最大回撤0.52%，年化复合收益率约43%，阿尔法37%，相关测试确认统计显著性和稳健性。

Conclusion: 预测到执行的工程能将适度的可预测性转化为可大规模配置的阿尔法收益。

Abstract: We test whether simple, interpretable state variables-trend and momentum-can generate durable out-of-sample alpha in one of the world's most liquid assets, gold. Using a rolling 10-year training and 6-month testing walk-forward from 2015 to 2025 (2,793 trading days), we convert a smoothed trend-momentum regime signal into volatility-targeted, friction-aware positions through fractional, impact-adjusted Kelly sizing and ATR-based exits. Out of sample, the strategy delivers a Sharpe ratio of 2.88 and a maximum drawdown of 0.52 percent, net of 0.7 basis-point linear cost and a square-root impact term (gamma = 0.02). A regression on spot-gold returns yields a 43 percent annualized return (CAGR approximately 43 percent) and a 37 percent alpha (Sharpe = 2.88, IR = 2.09) at a 15 percent volatility target with beta approximately 0.03, confirming benchmark-neutral performance. Bootstrap confidence intervals ([2.49, 3.27]) and SPA tests (p = 0.000) confirm statistical significance and robustness to latency, reversal, and cost stress. We conclude that forecast-to-fill engineering-linking transparent signals to executable trades with explicit risk, cost, and impact control-can transform modest predictability into allocator-grade, billion-dollar-scalable alpha.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [229] [Tractable Instances of Bilinear Maximization: Implementing LinUCB on Ellipsoids](https://arxiv.org/abs/2511.07504)
*Raymond Zhang,Hédi Hadiji,Richard Combes*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the maximization of $x^\top θ$ over $(x,θ) \in \mathcal{X} \times Θ$, with $\mathcal{X} \subset \mathbb{R}^d$ convex and $Θ\subset \mathbb{R}^d$ an ellipsoid. This problem is fundamental in linear bandits, as the learner must solve it at every time step using optimistic algorithms. We first show that for some sets $\mathcal{X}$ e.g. $\ell_p$ balls with $p>2$, no efficient algorithms exist unless $\mathcal{P} = \mathcal{NP}$. We then provide two novel algorithms solving this problem efficiently when $\mathcal{X}$ is a centered ellipsoid. Our findings provide the first known method to implement optimistic algorithms for linear bandits in high dimensions.

</details>


### [230] [Infinite-Dimensional Operator/Block Kaczmarz Algorithms: Regret Bounds and $λ$-Effectiveness](https://arxiv.org/abs/2511.07604)
*Halyun Jeong,Palle E. T. Jorgensen,Hyun-Kyoung Kwon,Myung-Sin Song*

Main category: stat.ML

TL;DR: 提出多种基于投影的线性回归算法，研究广义Kaczmarz算法中松弛参数作用，建立先验后悔界，给出详细分析并应用于多种场景。


<details>
  <summary>Details</summary>
Motivation: 聚焦现代机器学习模型及其算法性能，受机器学习实践启发。

Method: 研究广义Kaczmarz算法中松弛参数作用，建立先验后悔界。

Result: 得到Kaczmarz算法模型框架、非正交傅里叶展开等的显式后悔界，以及噪声Kaczmarz算法的后悔界等新结果。

Conclusion: 所提出的更广泛框架处理有界算子，以（块）Kaczmarz算法进行更新，得到新且通用的结果。

Abstract: We present a variety of projection-based linear regression algorithms with a focus on modern machine-learning models and their algorithmic performance. We study the role of the relaxation parameter in generalized Kaczmarz algorithms and establish a priori regret bounds with explicit $λ$-dependence to quantify how much an algorithm's performance deviates from its optimal performance. A detailed analysis of relaxation parameter is also provided. Applications include: explicit regret bounds for the framework of Kaczmarz algorithm models, non-orthogonal Fourier expansions, and the use of regret estimates in modern machine learning models, including for noisy data, i.e., regret bounds for the noisy Kaczmarz algorithms. Motivated by machine-learning practice, our wider framework treats bounded operators (on infinite-dimensional Hilbert spaces), with updates realized as (block) Kaczmarz algorithms, leading to new and versatile results.

</details>


### [231] [Robust Experimental Design via Generalised Bayesian Inference](https://arxiv.org/abs/2511.07671)
*Yasir Zubayr Barlas,Sabina J. Sloman,Samuel Kaski*

Main category: stat.ML

TL;DR: 提出广义贝叶斯最优实验设计（GBOED），推导新的获取函数，实证表明其增强对异常值和噪声分布错误假设的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯最优实验设计依赖数据生成过程统计模型正确指定的假设，违反该假设会导致推理和信息增益估计不佳，需要更鲁棒的方法。

Method: 将广义贝叶斯（Gibbs）推理扩展到实验设计场景，使用扩展的信息论框架推导新的获取函数Gibbs预期信息增益（Gibbs EIG）。

Result: GBOED增强了对异常值和结果噪声分布错误假设的鲁棒性。

Conclusion: GBOED在实验设计和推理方面都能实现鲁棒性。

Abstract: Bayesian optimal experimental design is a principled framework for conducting experiments that leverages Bayesian inference to quantify how much information one can expect to gain from selecting a certain design. However, accurate Bayesian inference relies on the assumption that one's statistical model of the data-generating process is correctly specified. If this assumption is violated, Bayesian methods can lead to poor inference and estimates of information gain. Generalised Bayesian (or Gibbs) inference is a more robust probabilistic inference framework that replaces the likelihood in the Bayesian update by a suitable loss function. In this work, we present Generalised Bayesian Optimal Experimental Design (GBOED), an extension of Gibbs inference to the experimental design setting which achieves robustness in both design and inference. Using an extended information-theoretic framework, we derive a new acquisition function, the Gibbs expected information gain (Gibbs EIG). Our empirical results demonstrate that GBOED enhances robustness to outliers and incorrect assumptions about the outcome noise distribution.

</details>


### [232] [Distributionally Robust Online Markov Game with Linear Function Approximation](https://arxiv.org/abs/2511.07831)
*Zewu Zheng,Yuanyuan Lin*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The sim-to-real gap, where agents trained in a simulator face significant performance degradation during testing, is a fundamental challenge in reinforcement learning. Extansive works adopt the framework of distributionally robust RL, to learn a policy that acts robustly under worst case environment shift. Within this framework, our objective is to devise algorithms that are sample efficient with interactive data collection and large state spaces. By assuming d-rectangularity of environment dynamic shift, we identify a fundamental hardness result for learning in online Markov game, and address it by adopting minimum value assumption. Then, a novel least square value iteration type algorithm, DR-CCE-LSI, with exploration bonus devised specifically for multiple agents, is proposed to find an \episilon-approximate robust Coarse Correlated Equilibrium(CCE). To obtain sample efficient learning, we find that: when the feature mapping function satisfies certain properties, our algorithm, DR-CCE-LSI, is able to achieve ε-approximate CCE with a regret bound of O{dHmin{H,1/min{σ_i}}\sqrt{K}}, where K is the number of interacting episodes, H is the horizon length, d is the feature dimension, and \simga_i represents the uncertainty level of player i. Our work introduces the first sample-efficient algorithm for this setting, matches the best result so far in single agent setting, and achieves minimax optimalsample complexity in terms of the feature dimension d. Meanwhile, we also conduct simulation study to validate the efficacy of our algorithm in learning a robust equilibrium.

</details>


### [233] [PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure](https://arxiv.org/abs/2511.07997)
*Ke Jia,Yuheng Ma,Yang Li,Feifei Wang*

Main category: stat.ML

TL;DR: 本文提出PrAda - GAN解决差分隐私下合成数据生成问题，理论证明其优势，实验表明在隐私 - 效用权衡上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于边际方法在差分隐私下生成合成数据的核心局限。

Method: 提出PrAda - GAN，采用顺序生成器架构捕获变量间复杂依赖，自适应正则化学习结构以促进贝叶斯网络稀疏性。

Result: 理论上建立参数距离、变量选择误差和Wasserstein距离的递减界限，分析表明利用依赖稀疏性可显著提高收敛率；实验显示在隐私 - 效用权衡上优于现有表格数据合成方法。

Conclusion: PrAda - GAN在差分隐私下生成合成数据方面表现良好，在隐私 - 效用权衡上优于现有方法。

Abstract: We revisit the problem of generating synthetic data under differential privacy. To address the core limitations of marginal-based methods, we propose the Private Adaptive Generative Adversarial Network with Bayes Network Structure (PrAda-GAN), which integrates the strengths of both GAN-based and marginal-based approaches. Our method adopts a sequential generator architecture to capture complex dependencies among variables, while adaptively regularizing the learned structure to promote sparsity in the underlying Bayes network. Theoretically, we establish diminishing bounds on the parameter distance, variable selection error, and Wasserstein distance. Our analysis shows that leveraging dependency sparsity leads to significant improvements in convergence rates. Empirically, experiments on both synthetic and real-world datasets demonstrate that PrAda-GAN outperforms existing tabular data synthesis methods in terms of the privacy-utility trade-off.

</details>


### [234] [Semi-Supervised Treatment Effect Estimation with Unlabeled Covariates via Generalized Riesz Regression](https://arxiv.org/abs/2511.08303)
*Masahiro Kato*

Main category: stat.ML

TL;DR: 研究半监督环境下治疗效果估计，提出效率界和有效估计量，分析两种数据生成过程，发现结合辅助协变量可降低效率界。


<details>
  <summary>Details</summary>
Motivation: 在半监督环境下，除标准三元组外，利用未标记辅助协变量进行治疗效果估计。

Method: 提出效率界和有效估计量，引入单样本和两样本两种数据生成过程进行分析。

Result: 在单样本和两样本设置中，结合辅助协变量可降低效率界，得到渐近方差更小的估计量。

Conclusion: 在半监督环境下，结合未标记辅助协变量有助于治疗效果估计，可降低效率界。

Abstract: This study investigates treatment effect estimation in the semi-supervised setting, where we can use not only the standard triple of covariates, treatment indicator, and outcome, but also unlabeled auxiliary covariates. For this problem, we develop efficiency bounds and efficient estimators whose asymptotic variance aligns with the efficiency bound. In the analysis, we introduce two different data-generating processes: the one-sample setting and the two-sample setting. The one-sample setting considers the case where we can observe treatment indicators and outcomes for a part of the dataset, which is also called the censoring setting. In contrast, the two-sample setting considers two independent datasets with labeled and unlabeled data, which is also called the case-control setting or the stratified setting. In both settings, we find that by incorporating auxiliary covariates, we can lower the efficiency bound and obtain an estimator with an asymptotic variance smaller than that without such auxiliary covariates.

</details>


### [235] [Concentration bounds on response-based vector embeddings of black-box generative models](https://arxiv.org/abs/2511.08307)
*Aranyak Acharyya,Joshua Agterberg,Youngser Park,Carey E. Priebe*

Main category: stat.ML

TL;DR: 本文在适当正则条件下，为通过数据核视角空间嵌入方法得到的生成模型样本向量嵌入建立高概率集中界，给出近似总体级向量嵌入所需样本响应数量，相关代数工具还可用于含噪声观测下经典多维缩放嵌入集中界的建立。


<details>
  <summary>Details</summary>
Motivation: 为给定生成模型的样本向量嵌入建立高概率集中界，确定近似总体级向量嵌入所需样本响应数量。

Method: 使用数据核视角空间嵌入方法获取给定生成模型的响应式向量嵌入，并利用代数工具建立集中界。

Result: 建立了高概率集中界，得出近似总体级向量嵌入达到期望精度所需的样本响应数量。

Conclusion: 代数工具可进一步用于含噪声观测下经典多维缩放嵌入集中界的建立。

Abstract: Generative models, such as large language models or text-to-image diffusion models, can generate relevant responses to user-given queries. Response-based vector embeddings of generative models facilitate statistical analysis and inference on a given collection of black-box generative models. The Data Kernel Perspective Space embedding is one particular method of obtaining response-based vector embeddings for a given set of generative models, already discussed in the literature. In this paper, under appropriate regularity conditions, we establish high probability concentration bounds on the sample vector embeddings for a given set of generative models, obtained through the method of Data Kernel Perspective Space embedding. Our results tell us the required number of sample responses needed in order to approximate the population-level vector embeddings with a desired level of accuracy. The algebraic tools used to establish our results can be used further for establishing concentration bounds on Classical Multidimensional Scaling embeddings in general, when the dissimilarities are observed with noise.

</details>


### [236] [Source-Optimal Training is Transfer-Suboptimal](https://arxiv.org/abs/2511.08401)
*C. Evans Hedges*

Main category: stat.ML

TL;DR: 证明迁移学习中源正则化最小化源风险与最大化迁移收益不匹配，刻画最优源惩罚，实验证实非线性网络中存在反直觉模式。


<details>
  <summary>Details</summary>
Motivation: 研究迁移学习中源正则化与迁移收益之间的关系，发现潜在的不匹配问题。

Method: 通过L2 - SP岭回归的尖锐相界刻画最优源惩罚，在CIFAR - 10和MNIST上进行实验。

Result: 最优源惩罚与任务最优值有可预测的偏离，在高SNR和低SNR制度下有不同要求；在各向同性设置中，迁移决策与目标样本大小和噪声无关。

Conclusion: 迁移学习中源正则化最小化源风险和最大化迁移收益存在根本的不匹配，这种反直觉模式在非线性网络中也存在。

Abstract: We prove a fundamental misalignment in transfer learning: the source regularization that minimizes source risk almost never coincides with the regularization maximizing transfer benefit. Through sharp phase boundaries for L2-SP ridge regression, we characterize the transfer-optimal source penalty $τ_0^*$ and show it diverges predictably from task-optimal values, requiring stronger regularization in high-SNR regimes and weaker regularization in low-SNR regimes. Additionally, in isotropic settings the decision to transfer is remarkably independent of target sample size and noise, depending only on task alignment and source characteristics. CIFAR-10 and MNIST experiments confirm this counterintuitive pattern persists in non-linear networks.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [237] [Improved Bounds for Context-Dependent Evolutionary Models Using Sequential Monte Carlo](https://arxiv.org/abs/2511.07736)
*Joseph Mathews,Scott C. Schmidler*

Main category: stat.CO

TL;DR: 研究依赖位点生物序列进化模型下近似边缘序列似然问题，提出SMC算法，证明其效率并与重要性采样器对比，方法或适用于其他近似问题。


<details>
  <summary>Details</summary>
Motivation: 解决系统发育学和计算生物学中具有位点依赖的进化模型的统计推断难题。

Method: 证明马尔可夫链蒙特卡罗算法的多项式混合时间界，引入顺序蒙特卡罗（SMC）算法近似边缘似然，结合重要性采样和有限样本SMC结果获得估计器有限样本近似误差界。

Result: 所提出的SMC算法为实际问题提供了高效随机近似方案，比最近开发的重要性采样器有显著改进。

Conclusion: 该方法结合了MCMC和SMC采样器界的创新成果，可能适用于其他近似边缘似然和贝叶斯因子的问题。

Abstract: Statistical inference in evolutionary models with site-dependence is a long-standing challenge in phylogenetics and computational biology. We consider the problem of approximating marginal sequence likelihoods under dependent-site models of biological sequence evolution. We prove a polynomial mixing time bound for a Markov chain Monte Carlo algorithm that samples the conditional distribution over latent sample paths, when the chain is initialized with a warm start. We then introduce a sequential Monte Carlo (SMC) algorithm for approximating the marginal likelihood, and show that our mixing time bound can be combined with recent importance sampling and finite-sample SMC results to obtain bounds on the finite sample approximation error of the resulting estimator. Our results show that the proposed SMC algorithm yields an efficient randomized approximation scheme for many practical problems of interest, and offers a significant improvement over a recently developed importance sampler for this problem. Our approach combines recent innovations in obtaining bounds for MCMC and SMC samplers, and may prove applicable to other problems of approximating marginal likelihoods and Bayes factors.

</details>


### [238] [A Closed-Form Diffusion Model for Learnring Dynamics from Marginal Observations](https://arxiv.org/abs/2511.07786)
*Hanwen Huang*

Main category: stat.CO

TL;DR: 本文提出学习薛定谔桥动力学的闭式框架，开发无模拟算法，并在单细胞发育轨迹建模和图像恢复任务中验证。


<details>
  <summary>Details</summary>
Motivation: 现有薛定谔桥问题解决方案多依赖迭代随机模拟，不稳定且成本高，需闭式框架。

Method: 引入统一并扩展已知闭式解的闭式框架，开发无模拟算法直接从源和目标分布样本推断薛定谔桥动力学。

Result: 经典高斯薛定谔桥解是该公式的直接推论，算法在单细胞发育轨迹建模和图像恢复任务中得到验证。

Conclusion: 所提闭式框架和无模拟算法有效，可用于相关建模和恢复任务。

Abstract: Score-based generative models learn transformations from a simple Gaussian to complex data distributions. To generalize these transformations between arbitrary distributions, recent work has focused on the Schrödinger Bridge (SB) problem. However, SB solutions are rarely available in closed form, and existing methods rely on iterative stochastic simulations that are often unstable and costly. We introduce a closed-form framework for learning SB dynamics that unifies and extends previously known closed-form solutions, including the Schrödinger Föllmer process and the Gaussian SB. Notably, the classical Gaussian SB solution arises as an immediate corollary of our formulation. Based on this result, we develop a simulation-free algorithm that directly infers SB dynamics from samples of the source and target distributions. We demonstrate the approach in modeling single-cell developmental trajectories and in image restoration tasks such as inpainting and deblurring.

</details>


### [239] [gemlib: Probabilistic programming for epidemic models](https://arxiv.org/abs/2511.08124)
*Alin Morariu,Jess Bridgen,Chris Jewell*

Main category: stat.CO

TL;DR: 介绍Python库gemlib，可定义、模拟和校准马尔可夫状态转换模型，能加速计算，支持疫情决策。


<details>
  <summary>Details</summary>
Motivation: 随机模型计算密集，在疫情响应中实用性差，需要工具解决计算问题。

Method: 将状态转换模型分解为三个关键要素，用Gillespie算法和Tau - leaping算法模拟，结合MCMC采样器，用JAX和TensorFlow Probability实现算法。

Result: 实现gemlib库，能抽象计算问题，让建模者专注模型开发与测试。

Conclusion: gemlib库可让用户快速实现和校准随机流行病模型，支持疫情决策。

Abstract: gemlib is a Python library for defining, simulating, and calibrating Markov state-transition models. Stochastic models are often computationally intensive, making them impractical to use in pandemic response efforts despite their favourable interpretations compared to their deterministic counterparts. gemlib decomposes state-transition models into three key ingredients which succinctly encapsulate the model and are sufficient for executing the subsequent computational routines. Simulation is performed using implementations of Gillespie's algorithm for continuous-time models and a generic Tau-leaping algorithm for discrete time models. gemlib models integrate seamlessly with Markov Chain Monte Carlo samplers as they provide a target distribution for the inference algorithm. Algorithms are implemented using the machine learning computational frameworks JAX and TensorFlow Probability, thus taking advantage of modern hardware to accelerate computation. This abstracts away computational concerns from modellers, allowing them to focus on developing and testing different model structures or assumptions. The gemlib library enables users to rapidly implement and calibrate stochastic epidemic models with the flexibility and robustness required to support decision during an emerging outbreak.

</details>


### [240] [A Fast and Accurate Approach for Covariance Matrix Construction](https://arxiv.org/abs/2511.08223)
*Felix Reichel*

Main category: stat.CO

TL;DR: 文章拓展了Bariance至协方差矩阵，给出代数等价形式，计算高效，有运行时间优势。


<details>
  <summary>Details</summary>
Motivation: 将Bariance拓展到协方差矩阵，寻求更高效的计算方法。

Method: 通过代数推导得出协方差矩阵的新形式，避免显式中心化，结合快速Gram例程。

Result: Python实证基准显示在非BLAS调优设置下比numpy.cov有明显运行时间优势，快速Gram例程可进一步降低总成本。

Conclusion: 提出的协方差矩阵计算方法在计算效率上有优势。

Abstract: Reichel (2025) defined the Bariance as $\mathrm{Bariance}(x)=\frac{1}{n(n-1)}\sum_{i<j}(x_i-x_j)^2$, which admits an $O(n)$ reformulation using scalar sums. We extend this to the covariance matrix by showing that $\mathrm{Cov}(X)=\frac{1}{n-1}\!\left(X^\top X-\frac{1}{n}\,s\,s^\top\right)$ with $s=X^\top \mathbf{1}_n$ is algebraically identical to the pairwise-difference form yet avoids explicit centering. Computation reduces to a single $p\times p$ outer matrix product and one subtraction. Empirical benchmarks in Python show clear runtime gains over numpy.cov in non-BLAS-tuned settings. Faster Gram routines such as RXTX (Rybin et. al) for $XX^\top$ further reduce total cost.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [241] [A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems](https://arxiv.org/abs/2511.07707)
*Manonmani Sekar,Nasim Nezamoddini*

Main category: cs.MA

TL;DR: 本文探讨多智能体强化学习（MARL）在可重构制造系统（RMS）软规划动态调度中的应用，实验表明该方法能降低制造周期和延迟，提高机器利用率。


<details>
  <summary>Details</summary>
Motivation: RMS需要灵活的软规划机制以实现实时生产计划和调度，因此探索MARL在其动态调度中的应用。

Method: 采用深度Q网络（DQN）智能体在集中训练中学习最优作业机器分配，结合注意力机制进行协商，使用关键DQN增强技术稳定和加速学习。

Result: 在模拟RMS环境实验中，该方法优于基线启发式方法，能降低制造周期和延迟、提高机器利用率；机器故障会增加关键性能指标的变异性。

Conclusion: 证实了在动态可重构制造环境中应用MARL机制进行智能自适应调度的优势。

Abstract: Reconfigurable manufacturing systems (RMS) are critical for future market adjustment given their rapid adaptation to fluctuations in consumer demands, the introduction of new technological advances, and disruptions in linked supply chain sections. The adjustable hard settings of such systems require a flexible soft planning mechanism that enables realtime production planning and scheduling amid the existing complexity and variability in their configuration settings. This study explores the application of multi agent reinforcement learning (MARL) for dynamic scheduling in soft planning of the RMS settings. In the proposed framework, deep Qnetwork (DQN) agents trained in centralized training learn optimal job machine assignments in real time while adapting to stochastic events such as machine breakdowns and reconfiguration delays. The model also incorporates a negotiation with an attention mechanism to enhance state representation and improve decision focus on critical system features. Key DQN enhancements including prioritized experience replay, nstep returns, double DQN and soft target update are used to stabilize and accelerate learning. Experiments conducted in a simulated RMS environment demonstrate that the proposed approach outperforms baseline heuristics in reducing makespan and tardiness while improving machine utilization. The reconfigurable manufacturing environment was extended to simulate realistic challenges, including machine failures and reconfiguration times. Experimental results show that while the enhanced DQN agent is effective in adapting to dynamic conditions, machine breakdowns increase variability in key performance metrics such as makespan, throughput, and total tardiness. The results confirm the advantages of applying the MARL mechanism for intelligent and adaptive scheduling in dynamic reconfigurable manufacturing environments.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [242] [Misaligned by Design: Incentive Failures in Machine Learning](https://arxiv.org/abs/2511.07699)
*David Autor,Andrew Caplin,Daniel Martin,Philip Marx*

Main category: econ.GN

TL;DR: 在高风险场景中，AI模型常用非对称损失函数训练辅助决策，但这种做法可能适得其反，应先忽略人类目标训练模型再事后调整预测。


<details>
  <summary>Details</summary>
Motivation: 探讨高风险场景下AI模型使用非对称损失函数训练以辅助决策的标准做法是否合理。

Method: 通过两个重点应用案例，并使用带有内生信息获取的激励设计经济模型进行分析。

Result: 标准的对齐做法会适得其反，先忽略人类目标训练模型再根据目标调整预测更好；调整虽能激励分类选择，但会降低学习激励。

Conclusion: 直观上有吸引力的方法实际上可能会以可预测的方式使人类和机器目标不一致。

Abstract: The cost of error in many high-stakes settings is asymmetric: misdiagnosing pneumonia when absent is an inconvenience, but failing to detect it when present can be life-threatening. Because of this, artificial intelligence (AI) models used to assist such decisions are frequently trained with asymmetric loss functions that incorporate human decision-makers' trade-offs between false positives and false negatives. In two focal applications, we show that this standard alignment practice can backfire. In both cases, it would be better to train the machine learning model with a loss function that ignores the human's objective and then adjust predictions ex post according to that objective. We rationalize this result using an economic model of incentive design with endogenous information acquisition. The key insight from our theoretical framework is that machine classifiers perform not one but two incentivized tasks: choosing how to classify and learning how to classify. We show that while the adjustments engineers use correctly incentivize choosing, they can simultaneously reduce the incentives to learn. Our formal treatment of the problem reveals that methods embraced for their intuitive appeal can in fact misalign human and machine objectives in predictable ways.

</details>


### [243] [Who benefits from increases in military spending? An empirical analysis](https://arxiv.org/abs/2511.08218)
*John Beirne,Haroon Mumtaz,Donghyun Park,Gazi Salah Uddin,Angeliki Theophilopoulou*

Main category: econ.GN

TL;DR: 研究军事支出新闻冲击对发达和新兴经济体家庭收入与财富不平等的异质性影响，发现其对产出和生产率有积极影响，但对收入和财富不平等影响不同。


<details>
  <summary>Details</summary>
Motivation: 探究军事支出新闻冲击对家庭收入和财富不平等的异质性影响。

Method: 对大量发达和新兴经济体进行面板研究。

Result: 军事支出新闻冲击使总产出和全要素生产率持续增加；扩张性军事支出缓解收入不平等，但增加财富不平等，尤其在高收入国家。

Conclusion: 军事支出新闻冲击对收入和财富不平等有不同的分布影响。

Abstract: This paper investigates the heterogeneous effects of military spending news shocks on household income and wealth inequality for a large, panel of advanced and emerging economies. Confirming prior literature, we find that military spending news shocks lead to persistent increases in aggregate output and Total Factor Productivity. Our primary contribution is documenting contrasting distributional impacts. We find that expansionary military spending is associated with a mitigation of income inequality, as income gains are disproportionately larger at the left tail of the distribution, primarily driven by a rise in labour income and employment in industry. Conversely, the shock is found to increase wealth inequality, particularly in high-income countries, by raising the wealth share of the top decile via effects on business asset holdings.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [244] [Evolutionary Analysis of Continuous-time Finite-state Mean Field Games with Discounted Payoffs](https://arxiv.org/abs/2511.07655)
*Leonardo Pedroso,Andrea Agazzi,W. P. M. H. Heemels,Mauro Salazar*

Main category: eess.SY

TL;DR: 本文研究含大量玩家的连续时间动态博弈，提出含状态演化的动态博弈进化框架，引入平均场近似和新均衡概念MSNE，并给出相关性质。


<details>
  <summary>Details</summary>
Motivation: 现有进化博弈论方法无法处理有个体状态动态的情况，而许多应用需要对其建模。

Method: 引入有限群体博弈的平均场近似，提出新的均衡概念Mixed Stationary Nash Equilibrium (MSNE)。

Result: 建立了平均场近似的保证，刻画了MSNE与平均场进化模型静止点的等价性，给出了MSNE进化稳定性的条件。

Conclusion: 所提出的含状态演化的动态博弈进化框架及MSNE概念，能有效处理有个体状态动态的动态博弈问题。

Abstract: We consider a class of continuous-time dynamic games involving a large number of players. Each player selects actions from a finite set and evolves through a finite set of states. State transitions occur stochastically and depend on the player's chosen action. A player's single-stage reward depends on their state, action, and the population-wide distribution of states and actions, capturing aggregate effects such as congestion in traffic networks. Each player seeks to maximize a discounted infinite-horizon reward. Existing evolutionary game-theoretic approaches introduce a model for the way individual players update their decisions in static environments without individual state dynamics. In contrast, this work develops an evolutionary framework for dynamic games with explicit state evolution, which is necessary to model many applications. We introduce a mean field approximation of the finite-population game and establish approximation guarantees. Since state-of-the-art solution concepts for dynamic games lack an evolutionary interpretation, we propose a new concept - the Mixed Stationary Nash Equilibrium (MSNE) - which admits one. We characterize an equivalence between MSNE and the rest points of the proposed mean field evolutionary model and we give conditions for the evolutionary stability of MSNE.

</details>


### [245] [Probabilistic Safety Guarantee for Stochastic Control Systems Using Average Reward MDPs](https://arxiv.org/abs/2511.08419)
*Saber Omidi,Marek Petrik,Se Young Yoon,Momotaz Begum*

Main category: eess.SY

TL;DR: 提出新算法计算随机控制系统安全策略，将安全目标转化为标准平均奖励MDP目标，通过数值验证表明其优于最小折扣奖励解。


<details>
  <summary>Details</summary>
Motivation: 随机控制系统中状态变量不可预测的演变对满足预定义约束构成挑战，需要计算满足约束的安全策略。

Method: 提出新算法，将安全目标转化为标准平均奖励马尔可夫决策过程（MDP）目标，使用线性规划等标准技术计算和分析安全策略。

Result: 在双积分器和倒立摆系统上数值验证，平均奖励MDP解更全面、收敛更快、质量更高。

Conclusion: 所提出的将安全目标转化为平均奖励MDP目标的方法有效，平均奖励MDP解优于最小折扣奖励解。

Abstract: Safety in stochastic control systems, which are subject to random noise with a known probability distribution, aims to compute policies that satisfy predefined operational constraints with high confidence throughout the uncertain evolution of the state variables. The unpredictable evolution of state variables poses a significant challenge for meeting predefined constraints using various control methods. To address this, we present a new algorithm that computes safe policies to determine the safety level across a finite state set. This algorithm reduces the safety objective to the standard average reward Markov Decision Process (MDP) objective. This reduction enables us to use standard techniques, such as linear programs, to compute and analyze safe policies. We validate the proposed method numerically on the Double Integrator and the Inverted Pendulum systems. Results indicate that the average-reward MDPs solution is more comprehensive, converges faster, and offers higher quality compared to the minimum discounted-reward solution.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [246] [Revealing the Hidden Third Dimension of Point Defects in Two-Dimensional MXenes](https://arxiv.org/abs/2511.08350)
*Grace Guinan,Michelle A. Smeaton,Brian C. Wyatt,Steven Goldy,Hilary Egan,Andrew Glaws,Garritt J. Tucker,Babak Anasori,Steven R. Spurgeon*

Main category: cond-mat.mtrl-sci

TL;DR: 利用人工智能引导的电子显微镜工作流程绘制Ti₃C₂Tₓ MXene中原子空位的3D拓扑和聚类图，为理解和控制二维材料点缺陷提供框架。


<details>
  <summary>Details</summary>
Motivation: 解决多层二维材料中三维点缺陷排列解析的挑战，以推动合理的缺陷工程。

Method: 采用人工智能引导的电子显微镜工作流程，结合分子动力学模拟。

Result: 重建了数十万个晶格位置上空位的3D坐标，获得其分布的统计信息，对缺陷结构进行分类并揭示其形成和相互作用机制。

Conclusion: 提供了一个可推广的框架，有助于理解和控制大体积范围内的点缺陷，为缺陷工程化二维功能材料的合理设计铺平道路。

Abstract: Point defects govern many important functional properties of two-dimensional (2D) materials. However, resolving the three-dimensional (3D) arrangement of these defects in multi-layer 2D materials remains a fundamental challenge, hindering rational defect engineering. Here, we overcome this limitation using an artificial intelligence-guided electron microscopy workflow to map the 3D topology and clustering of atomic vacancies in Ti$_3$C$_2$T$_X$ MXene. Our approach reconstructs the 3D coordinates of vacancies across hundreds of thousands of lattice sites, generating robust statistical insight into their distribution that can be correlated with specific synthesis pathways. This large-scale data enables us to classify a hierarchy of defect structures--from isolated vacancies to nanopores--revealing their preferred formation and interaction mechanisms, as corroborated by molecular dynamics simulations. This work provides a generalizable framework for understanding and ultimately controlling point defects across large volumes, paving the way for the rational design of defect-engineered functional 2D materials.

</details>


### [247] [Identification of Empirical Constitutive Models for Age-Hardenable Aluminium Alloy and High-Chromium Martensitic Steel Using Symbolic Regression](https://arxiv.org/abs/2511.08424)
*Evgeniya Kabliman,Gabriel Kronberger*

Main category: cond-mat.mtrl-sci

TL;DR: 本文指出过程 - 结构 - 性能关系对材料科学很重要，介绍符号回归可用于推导本构模型，以两种材料和两种测试方法获取数据，展示其好处并讨论挑战。


<details>
  <summary>Details</summary>
Motivation: 利用符号回归这一强大工具，挖掘描述材料过程 - 结构 - 性能关系的数学模型，推导金属合金塑性变形本构模型。

Method: 选择两种材料（可时效硬化铝合金和高铬马氏体钢）和两种测试方法（压缩和拉伸）获取应力 - 应变数据，用符号回归推导本构模型。

Result: 突出了使用符号回归的好处，同时讨论了潜在挑战。

Conclusion: 符号回归可用于推导描述金属合金塑性变形行为的本构模型，但存在一定挑战。

Abstract: Process-structure-property relationships are fundamental in materials science and engineering and are key to the development of new and improved materials. Symbolic regression serves as a powerful tool for uncovering mathematical models that describe these relationships. It can automatically generate equations to predict material behaviour under specific manufacturing conditions and optimize performance characteristics such as strength and elasticity.
  The present work illustrates how symbolic regression can derive constitutive models that describe the behaviour of various metallic alloys during plastic deformation. Constitutive modelling is a mathematical framework for understanding the relationship between stress and strain in materials under different loading conditions. In this study, two materials (age-hardenable aluminium alloy and high-chromium martensitic steel) and two different testing methods (compression and tension) are considered to obtain the required stress-strain data. The results highlight the benefits of using symbolic regression while also discussing potential challenges.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [248] [Pinching Antennas Meet AI in Next-Generation Wireless Networks](https://arxiv.org/abs/2511.07442)
*Fang Fang,Zhiguo Ding,Victor C. M. Leung,Lajos Hanzo*

Main category: cs.NI

TL;DR: 本文探讨AI与夹捏天线（PAs）在下一代无线网络中的‘双赢’合作，还讨论了研究方向，为自适应、有弹性和自优化的下一代网络铺平道路。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需智能支持新兴应用，PAs可创建视距链路，但需AI管理复杂控制和资源分配。

Method: 研究AI与PAs的合作，AI优化PAs激活位置，PAs支持边缘AI任务。

Result: 实现AI与PAs的‘双赢’合作。

Conclusion: 这种协同为下一代网络发展提供方向。

Abstract: Next-generation (NG) wireless networks must embrace innate intelligence in support of demanding emerging applications, such as extended reality and autonomous systems, under ultra-reliable and low-latency requirements. Pinching antennas (PAs), a new flexible low-cost technology, can create line-of-sight links by dynamically activating small dielectric pinches along a waveguide on demand. As a compelling complement, artificial intelligence (AI) offers the intelligence needed to manage the complex control of PA activation positions and resource allocation in these dynamic environments. This article explores the "win-win" cooperation between AI and PAs: AI facilitates the adaptive optimization of PA activation positions along the waveguide, while PAs support edge AI tasks such as federated learning and over-the-air aggregation. We also discuss promising research directions including large language model-driven PA control frameworks, and how PA-AI integration can advance semantic communications, and integrated sensing and communication. This synergy paves the way for adaptive, resilient, and self-optimizing NG networks.

</details>


### [249] [Resource Allocation in Hybrid Radio-Optical IoT Networks using GNN with Multi-task Learning](https://arxiv.org/abs/2511.07428)
*Aymen Hamrouni,Sofie Pollin,Hazem Sallouha*

Main category: cs.NI

TL;DR: 本文研究混合物联网网络中双技术调度问题，提出DGET框架，仿真显示混合网络性能优，框架有高准确率、低复杂度和强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决混合物联网网络中结合光无线通信和射频的双技术调度问题，传统方法求解NP难问题有局限且全信道可观测假设不实际。

Method: 先建立混合整数非线性规划模型，后提出DGET框架，结合两阶段图神经网络和基于Transformer的编码器，通过一致性损失和分类损失学习。

Result: 混合RF - OWC网络能高效处理更高流量负载，信息年龄最多降低20%，能耗相当；DGET框架分类准确率超90%，降低计算复杂度，在部分信道可观测下鲁棒性更强。

Conclusion: DGET框架在混合物联网网络双技术调度中表现良好，能实现接近最优的调度。

Abstract: This paper addresses the problem of dual-technology scheduling in hybrid Internet of Things (IoT) networks that integrate Optical Wireless Communication (OWC) alongside Radio Frequency (RF). We begin by formulating a Mixed-Integer Nonlinear Programming (MINLP) model that jointly considers throughput maximization and delay minimization between access points and IoT nodes under energy and link availability constraints. However, given the intractability of solving such NP-hard problems at scale and the impractical assumption of full channel observability, we propose the Dual-Graph Embedding with Transformer (DGET) framework, a supervised multi-task learning architecture combining a two-stage Graph Neural Networks (GNNs) with a Transformer-based encoder. The first stage employs a transductive GNN that encodes the known graph topology and initial node and link states. The second stage introduces an inductive GNN for temporal refinement, which learns to generalize these embeddings to the evolved states of the same network, capturing changes in energy and queue dynamics over time, by aligning them with ground-truth scheduling decisions through a consistency loss. These enriched embeddings are then processed by a classifier for the communication links with a Transformer encoder that captures cross-link dependencies through multi-head self-attention via classification loss. Simulation results show that hybrid RF-OWC networks outperform standalone RF systems by handling higher traffic loads more efficiently and reducing the Age of Information (AoI) by up to 20%, all while maintaining comparable energy consumption. The proposed DGET framework, compared to traditional optimization-based methods, achieves near-optimal scheduling with over 90% classification accuracy, reduces computational complexity, and demonstrates higher robustness under partial channel observability.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [250] [Graph Classes Closed under Self-intersection](https://arxiv.org/abs/2511.08011)
*Konrad K. Dabrowski,Vadim V. Lozin,Martin Milanič,Andrea Munaro,Daniël Paulusma,Viktor Zamaraev*

Main category: math.CO

TL;DR: 研究将单调图类的算法二分性扩展到更大的遗传图类族，考虑自交封闭的遗传图类，证明结构特征并给出多个问题的二分性结果。


<details>
  <summary>Details</summary>
Motivation: 已知单调图类算法二分性不能扩展到所有遗传类，提出能否扩展到更大遗传类族的问题。

Method: 考虑自交封闭的遗传图类，证明排除三脚架的自交封闭类中图形的结构特征。

Result: 得到最大独立集及其加权变体、最大诱导匹配、可满足性和计数可满足性等问题的二分性结果，以及二部图类团宽度有界性的二分性结果。

Conclusion: 肯定了将单调类已知算法二分性扩展到自交封闭遗传类的可能性，推广了最大独立集的已知结果。

Abstract: A graph class is monotone if it is closed under taking subgraphs. It is known that a monotone class defined by finitely many obstructions has bounded treewidth if and only if one of the obstructions is a so-called tripod, that is, a disjoint union of trees with exactly one vertex of degree 3 and paths. This dichotomy also characterizes exactly those monotone graph classes for which many NP-hard algorithmic problems admit polynomial-time algorithms. These algorithmic dichotomies, however, do not extend to the universe of all hereditary classes, which are classes closed under taking induced subgraphs. This leads to the natural question of whether we can extend known algorithmic dichotomies for monotone classes to larger families of hereditary classes. We give an affirmative answer to this question by considering the family of hereditary graph classes that are closed under self-intersection, which is known to be located strictly between the monotone and hereditary classes. We prove a new structural characterization of graphs in self-intersection-closed classes excluding a tripod. We use our characterization to give a complete dichotomy of Maximum Independent Set, and its weighted variant for self-intersection-closed classes defined by finitely many obstructions: these problems are in P if the class excludes a tripod and NP-hard otherwise. This generalizes several known results on Maximum Independent Set. We also use it to obtain dichotomies for Maximum Induced Matching on self-intersection-closed classes of bipartite graphs defined by finitely many obstructions. Similarly, we obtain dichotomies for Satisfiability and Counting Satisfiability on self-intersection-closed classes of (bipartite) incidence graphs defined by finitely many obstructions, and for boundedness of clique-width for self-intersection-closed classes of bipartite graphs defined by finitely many obstructions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [251] [Model Predictive Control is almost Optimal for Heterogeneous Restless Multi-armed Bandits](https://arxiv.org/abs/2511.08097)
*Dheeraj Narasimha,Nicolas Gast*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider a general infinite horizon Heterogeneous Restless multi-armed Bandit (RMAB). Heterogeneity is a fundamental problem for many real-world systems largely because it resists many concentration arguments. In this paper, we assume that each of the $N$ arms can have different model parameters. We show that, under a mild assumption of uniform ergodicity, a natural finite-horizon LP-update policy with randomized rounding, that was originally proposed for the homogeneous case, achieves an $O(\log N\sqrt{1/N})$ optimality gap in infinite time average reward problems for fully heterogeneous RMABs. In doing so, we show results that provide strong theoretical guarantees on a well-known algorithm that works very well in practice. The LP-update policy is a model predictive approach that computes a decision at time $t$ by planing over a time-horizon $\{t\dots t+τ\}$. Our simulation section demonstrates that our algorithm works extremely well even when $τ$ is very small and set to $5$, which makes it computationally efficient. Our theoretical results draw on techniques from the model predictive control literature by invoking the concept of \emph{dissipativity} and generalize quite easily to the more general weakly coupled heterogeneous Markov Decision Process setting. In addition, we draw a parallel between our own policy and the LP-index policy by showing that the LP-index policy corresponds to $τ=1$. We describe where the latter's shortcomings arise from and how under our mild assumption we are able to address these shortcomings. The proof of our main theorem answers an open problem posed by (Brown et al 2020), paving the way for several new questions on the LP-update policies.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [252] [Leveraging the Power of AI and Social Interactions to Restore Trust in Public Polls](https://arxiv.org/abs/2511.07593)
*Amr Akmal Abouelmagd,Amr Hilal*

Main category: cs.SI

TL;DR: 本文探讨利用社交互动恢复社交网络众包数据可信度，通过AI图分析检测不合格参与，实验取得不错结果。


<details>
  <summary>Details</summary>
Motivation: 众包数据虽重塑社科研究，但确保安全、公平和可靠参与是挑战，传统民意调查参与度下降，社交网络数据存在可信度问题。

Method: 通过基于AI的社交互动图分析检测民意调查任务中的不合格参与，仅关注社交互动图结构，模拟不同类型不诚实行为，在真实社交网络数据集上实验。

Result: 尽管社交互动图结构差异会带来性能波动，但在检测不同社交和行为特征的不合格参与方面取得了有前景的结果，部分配置下准确率超90%。

Conclusion: 社交互动的图分析能有效检测众包数据收集中的不合格参与，有助于恢复社交网络众包数据的可信度。

Abstract: The emergence of crowdsourced data has significantly reshaped social science, enabling extensive exploration of collective human actions, viewpoints, and societal dynamics. However, ensuring safe, fair, and reliable participation remains a persistent challenge. Traditional polling methods have seen a notable decline in engagement over recent decades, raising concerns about the credibility of collected data. Meanwhile, social and peer-to-peer networks have become increasingly widespread, but data from these platforms can suffer from credibility issues due to fraudulent or ineligible participation. In this paper, we explore how social interactions can help restore credibility in crowdsourced data collected over social networks. We present an empirical study to detect ineligible participation in a polling task through AI-based graph analysis of social interactions among imperfect participants composed of honest and dishonest actors. Our approach focuses solely on the structure of social interaction graphs, without relying on the content being shared. We simulate different levels and types of dishonest behavior among participants who attempt to propagate the task within their social networks. We conduct experiments on real-world social network datasets, using different eligibility criteria and modeling diverse participation patterns. Although structural differences in social interaction graphs introduce some performance variability, our study achieves promising results in detecting ineligibility across diverse social and behavioral profiles, with accuracy exceeding 90% in some configurations.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [253] [Emulating Radiative Transfer in Astrophysical Environments](https://arxiv.org/abs/2511.08219)
*Rune Rost,Lorenzo Branca,Tobias Buck*

Main category: astro-ph.IM

TL;DR: 提出基于Fourier Neural Operator架构和U - Nets的替代模型，加速辐射传输计算，速度提升超2个数量级且误差低于3%。


<details>
  <summary>Details</summary>
Motivation: 数值求解辐射传输方程计算量大，在流体动力学模拟中纳入实时辐射效应成本高，需开发能加速计算且保持高精度的替代模型。

Method: 提出基于Fourier Neural Operator架构与U - Nets结合的替代模型，在吸收 - 发射近似下近似三维、单色辐射传输。

Result: 模型实现速度提升超2个数量级，平均相对误差低于3%。

Conclusion: 该方法有潜力集成到先进的流体动力学模拟中。

Abstract: Radiative transfer is a fundamental process in astrophysics, essential for both interpreting observations and modeling thermal and dynamical feedback in simulations via ionizing radiation and photon pressure. However, numerically solving the underlying radiative transfer equation is computationally intensive due to the complex interaction of light with matter and the disparity between the speed of light and the typical gas velocities in astrophysical environments, making it particularly expensive to include the effects of on-the-fly radiation in hydrodynamic simulations. This motivates the development of surrogate models that can significantly accelerate radiative transfer calculations while preserving high accuracy. We present a surrogate model based on a Fourier Neural Operator architecture combined with U-Nets. Our model approximates three-dimensional, monochromatic radiative transfer in time-dependent regimes, in absorption-emission approximation, achieving speedups of more than 2 orders of magnitude while maintaining an average relative error below 3%, demonstrating our approach's potential to be integrated into state-of-the-art hydrodynamic simulations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [254] [Laplacian Score Sharpening for Mitigating Hallucination in Diffusion Models](https://arxiv.org/abs/2511.07496)
*Barath Chandran. C,Srinivas Anumasa,Dianbo Liu*

Main category: cs.CV

TL;DR: 提出推理时对分数函数的事后调整方法，利用分数的拉普拉斯算子减少无条件扩散模型中的模式插值幻觉，在不同维度数据上有效，并探索拉普拉斯算子与分数不确定性的关系。


<details>
  <summary>Details</summary>
Motivation: 扩散模型存在产生不连贯或不现实样本的幻觉问题，现有研究缺乏防止其在采样时生成的方法。

Method: 在推理时对分数函数进行事后调整，利用分数的拉普拉斯算子；使用Hutchinson迹估计器的有限差分变体推导高维的有效拉普拉斯近似。

Result: 显著降低了玩具1D/2D分布和高维图像数据集上幻觉样本的比例。

Conclusion: 提出的方法能有效减少无条件扩散模型中的模式插值幻觉，且拉普拉斯算子与分数不确定性存在一定关系。

Abstract: Diffusion models, though successful, are known to suffer from hallucinations that create incoherent or unrealistic samples. Recent works have attributed this to the phenomenon of mode interpolation and score smoothening, but they lack a method to prevent their generation during sampling. In this paper, we propose a post-hoc adjustment to the score function during inference that leverages the Laplacian (or sharpness) of the score to reduce mode interpolation hallucination in unconditional diffusion models across 1D, 2D, and high-dimensional image data. We derive an efficient Laplacian approximation for higher dimensions using a finite-difference variant of the Hutchinson trace estimator. We show that this correction significantly reduces the rate of hallucinated samples across toy 1D/2D distributions and a high- dimensional image dataset. Furthermore, our analysis explores the relationship between the Laplacian and uncertainty in the score.

</details>


### [255] [Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding](https://arxiv.org/abs/2511.08480)
*Da Li,Yuxiao Luo,Keping Bi,Jiafeng Guo,Wei Yuan,Biao Yang,Yan Wang,Fan Yang,Tingting Gao,Guorui Zhou*

Main category: cs.CV

TL;DR: 本文提出CoMa压缩预训练阶段，将视觉语言模型转化为有竞争力的嵌入模型，在MMEB上取得新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过大规模对比学习将视觉语言模型转化为嵌入模型，作者认为可将两个互补目标解耦，以少量预训练数据实现模型优化。

Method: 提出CoMa压缩预训练阶段作为对比学习的预热阶段。

Result: 仅用少量预训练数据就能将视觉语言模型转化为有竞争力的嵌入模型，在MMEB上实现效率和效果的优化。

Conclusion: CoMa在可比规模的视觉语言模型中取得新的最优结果，实现了效率和效果的双重提升。

Abstract: Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.

</details>


### [256] [Foam Segmentation in Wastewater Treatment Plants: A Federated Learning Approach with Segment Anything Model 2](https://arxiv.org/abs/2511.08130)
*Mehmet Batuhan Duman,Alejandro Carnero,Cristian Martín,Daniel Garrido,Manuel Díaz*

Main category: cs.CV

TL;DR: 本文提出结合联邦学习（FL）和图像分割模型SAM2的框架，解决污水处理厂泡沫检测中数据稀缺、隐私等问题，提供实用、可扩展且保护隐私的自动泡沫跟踪方案。


<details>
  <summary>Details</summary>
Motivation: 污水处理厂泡沫形成影响处理效率和成本，自动检测泡沫有益，但标准机器学习模型训练需大量标注数据，存在数据稀缺、异质性及隐私问题。

Method: 结合FL和SAM2，使用Flower框架在分布式客户端微调SAM2，中央雾服务器聚合模型权重且不访问私有数据，用多个数据集训练和验证模型。

Result: 框架加速训练收敛，提高了分割性能，模型在多个数据集上训练验证。

Conclusion: 该研究为污水处理厂自动泡沫跟踪提供实用方案，表明将大规模基础模型集成到FL系统可解决现实工业中分布式敏感数据问题。

Abstract: Foam formation in Wastewater Treatment Plants (WTPs) is a major challenge that can reduce treatment efficiency and increase costs. The ability to automatically examine changes in real-time with respect to the percentage of foam can be of great benefit to the plant. However, large amounts of labeled data are required to train standard Machine Learning (ML) models. The development of these systems is slow due to the scarcity and heterogeneity of labeled data. Additionally, the development is often hindered by the fact that different WTPs do not share their data due to privacy concerns. This paper proposes a new framework to address these challenges by combining Federated Learning (FL) with the state-of-the-art base model for image segmentation, Segment Anything Model 2 (SAM2). The FL paradigm enables collaborative model training across multiple WTPs without centralizing sensitive operational data, thereby ensuring privacy. The framework accelerates training convergence and improves segmentation performance even with limited local datasets by leveraging SAM2's strong pre-trained weights for initialization. The methodology involves fine-tuning SAM2 on distributed clients (edge nodes) using the Flower framework, where a central Fog server orchestrates the process by aggregating model weights without accessing private data. The model was trained and validated using various data collections, including real-world images captured at a WTPs in Granada, Spain, a synthetically generated foam dataset, and images from publicly available datasets to improve generalization. This research offers a practical, scalable, and privacy-aware solution for automatic foam tracking in WTPs. The findings highlight the significant potential of integrating large-scale foundational models into FL systems to solve real-world industrial challenges characterized by distributed and sensitive data.

</details>


### [257] [SWAN - Enabling Fast and Mobile Histopathology Image Annotation through Swipeable Interfaces](https://arxiv.org/abs/2511.08271)
*Sweta Banerjee,Timo Gosch,Sara Hester,Viktoria Weiss,Thomas Conrad,Taryn A. Donovan,Nils Porsche,Jonas Ammeling,Christoph Stroblberger,Robert Klopfleisch,Christopher Kaltenecker,Christof A. Bertram,Katharina Breininger,Marc Aubreville*

Main category: cs.CV

TL;DR: 介绍开源网页应用SWAN用于图像补丁分类注释，与传统方法对比实验显示其能加速注释并保证质量。


<details>
  <summary>Details</summary>
Motivation: 解决大规模组织病理学图像数据集注释瓶颈，传统文件夹注释工作流慢、易疲劳且难扩展。

Method: 引入SWAN应用，支持桌面和移动平台，用滑动手势分类图像补丁，开展有四位病理学家参与的试点研究对比SWAN和传统文件夹排序工作流。

Result: SWAN能快速注释，注释者间一致性高，与传统方法性能相当，参与者认为工具易用，赞赏可在移动设备上注释。

Conclusion: SWAN可加速图像注释并保持质量，是传统工作流可扩展且用户友好的替代方案。

Abstract: The annotation of large scale histopathology image datasets remains a major bottleneck in developing robust deep learning models for clinically relevant tasks, such as mitotic figure classification. Folder-based annotation workflows are usually slow, fatiguing, and difficult to scale. To address these challenges, we introduce SWipeable ANnotations (SWAN), an open-source, MIT-licensed web application that enables intuitive image patch classification using a swiping gesture. SWAN supports both desktop and mobile platforms, offers real-time metadata capture, and allows flexible mapping of swipe gestures to class labels. In a pilot study with four pathologists annotating 600 mitotic figure image patches, we compared SWAN against a traditional folder-sorting workflow. SWAN enabled rapid annotations with pairwise percent agreement ranging from 86.52% to 93.68% (Cohen's Kappa = 0.61-0.80), while for the folder-based method, the pairwise percent agreement ranged from 86.98% to 91.32% (Cohen's Kappa = 0.63-0.75) for the task of classifying atypical versus normal mitotic figures, demonstrating high consistency between annotators and comparable performance. Participants rated the tool as highly usable and appreciated the ability to annotate on mobile devices. These results suggest that SWAN can accelerate image annotation while maintaining annotation quality, offering a scalable and user-friendly alternative to conventional workflows.

</details>


### [258] [Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs](https://arxiv.org/abs/2511.07429)
*Hari Lee*

Main category: cs.CV

TL;DR: 提出基于文本的可解释视频异常检测框架TbVAD，在文本域完成检测与解释，经实验验证其在真实场景有效。


<details>
  <summary>Details</summary>
Motivation: 传统弱监督视频异常检测模型依赖视觉特征，缺乏可解释性，需要一种语言驱动的可解释方法。

Method: 框架分三步：用视觉 - 语言模型将视频转为细粒度字幕；将字幕组织成四个语义槽构建结构化知识；生成槽级解释。

Result: 在UCF - Crime和XD - Violence两个公开基准上评估，文本知识推理在真实监控场景提供了可解释且可靠的异常检测。

Conclusion: TbVAD能在真实世界监控场景实现可解释且可靠的视频异常检测。

Abstract: We introduce Text-based Explainable Video Anomaly Detection (TbVAD), a language-driven framework for weakly supervised video anomaly detection that performs anomaly detection and explanation entirely within the textual domain. Unlike conventional WSVAD models that rely on explicit visual features, TbVAD represents video semantics through language, enabling interpretable and knowledge-grounded reasoning. The framework operates in three stages: (1) transforming video content into fine-grained captions using a vision-language model, (2) constructing structured knowledge by organizing the captions into four semantic slots (action, object, context, environment), and (3) generating slot-wise explanations that reveal which semantic factors contribute most to the anomaly decision. We evaluate TbVAD on two public benchmarks, UCF-Crime and XD-Violence, demonstrating that textual knowledge reasoning provides interpretable and reliable anomaly detection for real-world surveillance scenarios.

</details>


### [259] [Modulo Video Recovery via Selective Spatiotemporal Vision Transformer](https://arxiv.org/abs/2511.07479)
*Tianyu Geng,Feng Ji,Wee Peng Tay*

Main category: cs.CV

TL;DR: 传统图像传感器动态范围有限，模相机可解决但需展开算法。本文指出标准HDR方法不适用于模恢复，提出首个深度学习框架SSViT用于模视频重建，实验表明其效果优异。


<details>
  <summary>Details</summary>
Motivation: 传统图像传感器在高动态范围场景中存在饱和问题，模相机虽能解决但模图像恢复进展缓慢，且标准HDR方法不适用于模恢复，需要新方法。

Method: 提出Selective Spatiotemporal Vision Transformer (SSViT)，采用令牌选择策略提高效率并关注关键区域。

Result: SSViT能从8位折叠视频中产生高质量重建结果，在模视频恢复中达到了最先进的性能。

Conclusion: SSViT是适用于模视频重建的有效深度学习框架。

Abstract: Conventional image sensors have limited dynamic range, causing saturation in high-dynamic-range (HDR) scenes. Modulo cameras address this by folding incident irradiance into a bounded range, yet require specialized unwrapping algorithms to reconstruct the underlying signal. Unlike HDR recovery, which extends dynamic range from conventional sampling, modulo recovery restores actual values from folded samples. Despite being introduced over a decade ago, progress in modulo image recovery has been slow, especially in the use of modern deep learning techniques. In this work, we demonstrate that standard HDR methods are unsuitable for modulo recovery. Transformers, however, can capture global dependencies and spatial-temporal relationships crucial for resolving folded video frames. Still, adapting existing Transformer architectures for modulo recovery demands novel techniques. To this end, we present Selective Spatiotemporal Vision Transformer (SSViT), the first deep learning framework for modulo video reconstruction. SSViT employs a token selection strategy to improve efficiency and concentrate on the most critical regions. Experiments confirm that SSViT produces high-quality reconstructions from 8-bit folded videos and achieves state-of-the-art performance in modulo video recovery.

</details>


### [260] [Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance](https://arxiv.org/abs/2511.07499)
*Kwanyoung Kim*

Main category: cs.CV

TL;DR: 提出新的Adversarial Sinkhorn Attention Guidance (ASAG)方法改进扩散模型生成质量，在文本到图像扩散等应用有提升。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型引导方法缺乏理论基础且依赖手动设计失真，需更好方法。

Method: 通过最优传输视角重新解释扩散模型注意力分数，用Sinkhorn算法扰乱传输成本，在自注意力层注入对抗成本。

Result: 在文本到图像扩散有一致改进，增强下游应用可控性和保真度。

Conclusion: ASAG方法轻量级、即插即用，无需模型重新训练就能提高可靠性。

Abstract: Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.

</details>


### [261] [UltraGS: Gaussian Splatting for Ultrasound Novel View Synthesis](https://arxiv.org/abs/2511.07743)
*Yuezhe Yang,Wenjie Cai,Dexin Yang,Yufang Dong,Xingbo Dong,Zhe Jin*

Main category: cs.CV

TL;DR: 提出用于超声成像的高斯 splatting 框架 UltraGS，经实验验证其优越性并开源代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 超声成像视野有限，难以进行新颖视图合成。

Method: 引入深度感知高斯 splatting 策略；设计 SH - DARS 轻量级渲染函数；构建临床超声检查数据集。

Result: 在三个数据集上实验，UltraGS 在 PSNR、SSIM、MSE 指标达最优，能以 64.69 fps 实时合成。

Conclusion: UltraGS 框架在超声成像视图合成方面表现优越。

Abstract: Ultrasound imaging is a cornerstone of non-invasive clinical diagnostics, yet its limited field of view complicates novel view synthesis. We propose \textbf{UltraGS}, a Gaussian Splatting framework optimized for ultrasound imaging. First, we introduce a depth-aware Gaussian splatting strategy, where each Gaussian is assigned a learnable field of view, enabling accurate depth prediction and precise structural representation. Second, we design SH-DARS, a lightweight rendering function combining low-order spherical harmonics with ultrasound-specific wave physics, including depth attenuation, reflection, and scattering, to model tissue intensity accurately. Third, we contribute the Clinical Ultrasound Examination Dataset, a benchmark capturing diverse anatomical scans under real-world clinical protocols. Extensive experiments on three datasets demonstrate UltraGS's superiority, achieving state-of-the-art results in PSNR (up to 29.55), SSIM (up to 0.89), and MSE (as low as 0.002) while enabling real-time synthesis at 64.69 fps. The code and dataset are open-sourced at: https://github.com/Bean-Young/UltraGS.

</details>


### [262] [Auto-US: An Ultrasound Video Diagnosis Agent Using Video Classification Framework and LLMs](https://arxiv.org/abs/2511.07748)
*Yuezhe Yang,Yiyue Guo,Wenjie Cai,Qingqing Ruan,Siying Wang,Xingbo Dong,Zhe Jin,Yong Dai*

Main category: cs.CV

TL;DR: 提出智能诊断代理Auto - US，构建CUV数据集，开发CTU - Net，实现超声视频分类，结合大语言模型生成诊断建议，结果显示其在实际超声应用中有效且有临床潜力。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助超声视频诊断研究在数据集多样性、诊断性能和临床适用性方面有限，需改进。

Method: 提出Auto - US，构建CUV数据集，开发CTU - Net进行超声视频分类，结合大语言模型生成诊断建议。

Result: CTU - Net在超声视频分类中达到86.73%的准确率，最终诊断分数超3分（满分5分），并获专业临床医生验证。

Conclusion: Auto - US在实际超声应用中有效，有临床潜力。

Abstract: AI-assisted ultrasound video diagnosis presents new opportunities to enhance the efficiency and accuracy of medical imaging analysis. However, existing research remains limited in terms of dataset diversity, diagnostic performance, and clinical applicability. In this study, we propose \textbf{Auto-US}, an intelligent diagnosis agent that integrates ultrasound video data with clinical diagnostic text. To support this, we constructed \textbf{CUV Dataset} of 495 ultrasound videos spanning five categories and three organs, aggregated from multiple open-access sources. We developed \textbf{CTU-Net}, which achieves state-of-the-art performance in ultrasound video classification, reaching an accuracy of 86.73\% Furthermore, by incorporating large language models, Auto-US is capable of generating clinically meaningful diagnostic suggestions. The final diagnostic scores for each case exceeded 3 out of 5 and were validated by professional clinicians. These results demonstrate the effectiveness and clinical potential of Auto-US in real-world ultrasound applications. Code and data are available at: https://github.com/Bean-Young/Auto-US.

</details>


### [263] [Filtered-ViT: A Robust Defense Against Multiple Adversarial Patch Attacks](https://arxiv.org/abs/2511.07755)
*Aja Khanal,Ahmed Faid,Apurva Narayan*

Main category: cs.CV

TL;DR: 提出Filtered - ViT架构，集成SMART - VMF机制，在多补丁攻击及真实医疗影像场景表现出色，实现对抗和自然干扰的统一鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习视觉系统易受多局部干扰影响，现有防御方法大多仅针对单补丁，难以应对多补丁情况。

Method: 提出Filtered - ViT架构，集成SMART - VMF机制，可选择性抑制受损区域并保留语义细节。

Result: 在ImageNet上使用LaVAN多补丁攻击，Filtered - ViT取得79.8%的干净准确率和46.3%的鲁棒准确率；在真实医疗影像案例中能减轻自然干扰且不降低诊断内容。

Conclusion: Filtered - ViT是首个对对抗和自然类补丁干扰均有统一鲁棒性的变压器，为高风险环境视觉系统提供可靠路径。

Abstract: Deep learning vision systems are increasingly deployed in safety-critical domains such as healthcare, yet they remain vulnerable to small adversarial patches that can trigger misclassifications. Most existing defenses assume a single patch and fail when multiple localized disruptions occur, the type of scenario adversaries and real-world artifacts often exploit. We propose Filtered-ViT, a new vision transformer architecture that integrates SMART Vector Median Filtering (SMART-VMF), a spatially adaptive, multi-scale, robustness-aware mechanism that enables selective suppression of corrupted regions while preserving semantic detail. On ImageNet with LaVAN multi-patch attacks, Filtered-ViT achieves 79.8% clean accuracy and 46.3% robust accuracy under four simultaneous 1\% patches, outperforming existing defenses. Beyond synthetic benchmarks, a real-world case study on radiographic medical imagery shows that Filtered-ViT mitigates natural artifacts such as occlusions and scanner noise without degrading diagnostic content. This establishes Filtered-ViT as the first transformer to demonstrate unified robustness against both adversarial and naturally occurring patch-like disruptions, charting a path toward reliable vision systems in truly high-stakes environments.

</details>


### [264] [Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label Cross-Modal Retrieval](https://arxiv.org/abs/2511.07780)
*Likang Peng,Chao Su,Wenyuan Wu,Yuan Sun,Dezhong Peng,Xi Peng,Xu Wang*

Main category: cs.CV

TL;DR: 提出Semantic - Consistent Bidirectional Contrastive Hashing (SCBCH)框架解决跨模态哈希在多标签噪声数据下的问题，实验验证其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态哈希方法依赖全标注数据集，多标签数据中存在标签噪声且现有方法忽略部分语义重叠，影响检索性能和泛化能力。

Method: 提出SCBCH框架，包含Cross - modal Semantic - Consistent Classification (CSCC)模块估计样本可靠性、减少噪声标签影响，Bidirectional Soft Contrastive Hashing (BSCH)模块基于多标签语义重叠动态生成软对比样本对，实现跨模态自适应对比学习。

Result: 在四个广泛使用的跨模态检索基准上的实验表明，该方法在多标签噪声条件下始终优于现有方法。

Conclusion: SCBCH框架在多标签噪声数据的跨模态哈希任务中有效且鲁棒。

Abstract: Cross-modal hashing (CMH) facilitates efficient retrieval across different modalities (e.g., image and text) by encoding data into compact binary representations. While recent methods have achieved remarkable performance, they often rely heavily on fully annotated datasets, which are costly and labor-intensive to obtain. In real-world scenarios, particularly in multi-label datasets, label noise is prevalent and severely degrades retrieval performance. Moreover, existing CMH approaches typically overlook the partial semantic overlaps inherent in multi-label data, limiting their robustness and generalization. To tackle these challenges, we propose a novel framework named Semantic-Consistent Bidirectional Contrastive Hashing (SCBCH). The framework comprises two complementary modules: (1) Cross-modal Semantic-Consistent Classification (CSCC), which leverages cross-modal semantic consistency to estimate sample reliability and reduce the impact of noisy labels; (2) Bidirectional Soft Contrastive Hashing (BSCH), which dynamically generates soft contrastive sample pairs based on multi-label semantic overlap, enabling adaptive contrastive learning between semantically similar and dissimilar samples across modalities. Extensive experiments on four widely-used cross-modal retrieval benchmarks validate the effectiveness and robustness of our method, consistently outperforming state-of-the-art approaches under noisy multi-label conditions.

</details>


### [265] [Sparse3DPR: Training-Free 3D Hierarchical Scene Parsing and Task-Adaptive Subgraph Reasoning from Sparse RGB Views](https://arxiv.org/abs/2511.07813)
*Haida Feng,Hao Wei,Zewen Xu,Haolin Wang,Chade Li,Yihong Wu*

Main category: cs.CV

TL;DR: 提出训练免训练框架Sparse3DPR用于开放式场景理解，实验显示其在准确性和效率上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有训练免训练方法在实际部署中存在准确性和效率问题，需改进3D场景理解方式。

Method: 提出Sparse3DPR框架，引入分层平面增强场景图并设计任务自适应子图提取方法。

Result: 在Space3D - Bench上相比ConceptGraphs有28.7%的EM@1提升和78.2%的加速，在ScanQA上与基于训练的方法性能相当。

Conclusion: Sparse3DPR在开放式3D场景理解中具有优越性、鲁棒性和泛化能力。

Abstract: Recently, large language models (LLMs) have been explored widely for 3D scene understanding. Among them, training-free approaches are gaining attention for their flexibility and generalization over training-based methods. However, they typically struggle with accuracy and efficiency in practical deployment. To address the problems, we propose Sparse3DPR, a novel training-free framework for open-ended scene understanding, which leverages the reasoning capabilities of pre-trained LLMs and requires only sparse-view RGB inputs. Specifically, we introduce a hierarchical plane-enhanced scene graph that supports open vocabulary and adopts dominant planar structures as spatial anchors, which enables clearer reasoning chains and more reliable high-level inferences. Furthermore, we design a task-adaptive subgraph extraction method to filter query-irrelevant information dynamically, reducing contextual noise and improving 3D scene reasoning efficiency and accuracy. Experimental results demonstrate the superiority of Sparse3DPR, which achieves a 28.7% EM@1 improvement and a 78.2% speedup compared with ConceptGraphs on the Space3D-Bench. Moreover, Sparse3DPR obtains comparable performance to training-based methods on ScanQA, with additional real-world experiments confirming its robustness and generalization capability.

</details>


### [266] [Boomda: Balanced Multi-objective Optimization for Multimodal Domain Adaptation](https://arxiv.org/abs/2511.08152)
*Jun Sun,Xinxin Zhang,Simin Hong,Jian Zhu,Xiang Gao*

Main category: cs.CV

TL;DR: 本文研究异构多模态领域自适应问题，提出Boomda算法，实验证明其有效性且性能优于其他方案。


<details>
  <summary>Details</summary>
Motivation: 多模态学习面临手动标注成本高、标注数据稀缺问题，且无监督域自适应在多模态场景研究较少，异构多模态领域自适应存在不同模态域偏移不同的挑战。

Method: 引入信息瓶颈方法独立学习各模态表征，用相关性对齐匹配源域和目标域，将问题建模为多目标任务求帕累托最优解，简化为二次规划问题并近似得到闭式解。

Result: 广泛的实验结果表明所提方法有效，Boomda性能优于竞争方案。

Conclusion: 提出的Boomda算法能有效解决异构多模态领域自适应问题，具有良好性能。

Abstract: Multimodal learning, while contributing to numerous success stories across various fields, faces the challenge of prohibitively expensive manual annotation. To address the scarcity of annotated data, a popular solution is unsupervised domain adaptation, which has been extensively studied in unimodal settings yet remains less explored in multimodal settings. In this paper, we investigate heterogeneous multimodal domain adaptation, where the primary challenge is the varying domain shifts of different modalities from the source to the target domain. We first introduce the information bottleneck method to learn representations for each modality independently, and then match the source and target domains in the representation space with correlation alignment. To balance the domain alignment of all modalities, we formulate the problem as a multi-objective task, aiming for a Pareto optimal solution. By exploiting the properties specific to our model, the problem can be simplified to a quadratic programming problem. Further approximation yields a closed-form solution, leading to an efficient modality-balanced multimodal domain adaptation algorithm. The proposed method features \textbf{B}alanced multi-\textbf{o}bjective \textbf{o}ptimization for \textbf{m}ultimodal \textbf{d}omain \textbf{a}daptation, termed \textbf{Boomda}. Extensive empirical results showcase the effectiveness of the proposed approach and demonstrate that Boomda outperforms the competing schemes. The code is is available at: https://github.com/sunjunaimer/Boomda.git.

</details>


### [267] [Evaluating Gemini LLM in Food Image-Based Recipe and Nutrition Description with EfficientNet-B4 Visual Backbone](https://arxiv.org/abs/2511.08215)
*Rizal Khoirul Anam*

Main category: cs.CV

TL;DR: 本文对用于食物识别的解耦多模态管道进行了综合比较评估，在新数据集上实验，发现系统整体效用受视觉前端感知精度瓶颈限制。


<details>
  <summary>Details</summary>
Motivation: 数字食品应用的增加需要强大的自动营养分析和烹饪指导方法，评估食物识别管道在视觉分类准确性、模型效率和生成输出质量之间的权衡。

Method: 将EfficientNet - B4视觉骨干与Google的Gemini LLM集成，与其他视觉骨干和轻量级LLM对比，引入“语义误差传播”（SEP）分析，基于新的自定义中国食品数据集（CCFD）进行实验。

Result: EfficientNet - B4在准确性和效率上平衡最好，Gemini生成质量优越，但系统整体效用受视觉前端感知精度限制，高语义相似性是最关键的失败模式。

Conclusion: 食物识别系统的整体效用主要受视觉前端感知精度的制约。

Abstract: The proliferation of digital food applications necessitates robust methods for automated nutritional analysis and culinary guidance. This paper presents a comprehensive comparative evaluation of a decoupled, multimodal pipeline for food recognition. We evaluate a system integrating a specialized visual backbone (EfficientNet-B4) with a powerful generative large language model (Google's Gemini LLM). The core objective is to evaluate the trade-offs between visual classification accuracy, model efficiency, and the quality of generative output (nutritional data and recipes). We benchmark this pipeline against alternative vision backbones (VGG-16, ResNet-50, YOLOv8) and a lightweight LLM (Gemma). We introduce a formalization for "Semantic Error Propagation" (SEP) to analyze how classification inaccuracies from the visual module cascade into the generative output. Our analysis is grounded in a new Custom Chinese Food Dataset (CCFD) developed to address cultural bias in public datasets. Experimental results demonstrate that while EfficientNet-B4 (89.0\% Top-1 Acc.) provides the best balance of accuracy and efficiency, and Gemini (9.2/10 Factual Accuracy) provides superior generative quality, the system's overall utility is fundamentally bottlenecked by the visual front-end's perceptive accuracy. We conduct a detailed per-class analysis, identifying high semantic similarity as the most critical failure mode.

</details>


### [268] [Generating Sketches in a Hierarchical Auto-Regressive Process for Flexible Sketch Drawing Manipulation at Stroke-Level](https://arxiv.org/abs/2511.07889)
*Sicong Zang,Shuhui Gao,Zhijun Fang*

Main category: cs.CV

TL;DR: 提出分层自回归草图生成过程，可在生成中灵活操纵草图绘制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在草图生成过程中无法进一步操纵，为更灵活实现草图绘制操纵。

Method: 提出分层自回归草图生成过程，分三步生成每个笔触并自回归进行。

Result: 可在生成过程中随时通过调整笔触嵌入灵活操纵笔触级草图绘制。

Conclusion: 所提方法能实现更灵活的草图绘制操纵。

Abstract: Generating sketches with specific patterns as expected, i.e., manipulating sketches in a controllable way, is a popular task. Recent studies control sketch features at stroke-level by editing values of stroke embeddings as conditions. However, in order to provide generator a global view about what a sketch is going to be drawn, all these edited conditions should be collected and fed into generator simultaneously before generation starts, i.e., no further manipulation is allowed during sketch generating process. In order to realize sketch drawing manipulation more flexibly, we propose a hierarchical auto-regressive sketch generating process. Instead of generating an entire sketch at once, each stroke in a sketch is generated in a three-staged hierarchy: 1) predicting a stroke embedding to represent which stroke is going to be drawn, and 2) anchoring the predicted stroke on the canvas, and 3) translating the embedding to a sequence of drawing actions to form the full sketch. Moreover, the stroke prediction, anchoring and translation are proceeded auto-regressively, i.e., both the recently generated strokes and their positions are considered to predict the current one, guiding model to produce an appropriate stroke at a suitable position to benefit the full sketch generation. It is flexible to manipulate stroke-level sketch drawing at any time during generation by adjusting the exposed editable stroke embeddings.

</details>


### [269] [Exploring the Underwater World Segmentation without Extra Training](https://arxiv.org/abs/2511.07923)
*Bingyu Li,Tao Huo,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 提出大规模细粒度水下分割数据集AquaOV255、水下开放词汇分割基准UOVSBench和免训练开放词汇分割框架Earth2Ocean，实验显示Earth2Ocean性能提升且推理高效。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和模型多适用于陆地场景，为进行海洋生物准确分割以用于生物多样性监测和生态评估，需开发水下相关数据集和模型。

Method: 引入AquaOV255数据集，建立UOVSBench基准，提出Earth2Ocean框架，该框架含GMG和CSA两个核心组件。

Result: 在UOVSBench基准上的大量实验表明，Earth2Ocean平均性能显著提升，且推理高效。

Conclusion: Earth2Ocean框架能有效实现水下开放词汇分割，可用于海洋生物分割以支持相关监测和评估工作。

Abstract: Accurate segmentation of marine organisms is vital for biodiversity monitoring and ecological assessment, yet existing datasets and models remain largely limited to terrestrial scenes. To bridge this gap, we introduce \textbf{AquaOV255}, the first large-scale and fine-grained underwater segmentation dataset containing 255 categories and over 20K images, covering diverse categories for open-vocabulary (OV) evaluation. Furthermore, we establish the first underwater OV segmentation benchmark, \textbf{UOVSBench}, by integrating AquaOV255 with five additional underwater datasets to enable comprehensive evaluation. Alongside, we present \textbf{Earth2Ocean}, a training-free OV segmentation framework that transfers terrestrial vision--language models (VLMs) to underwater domains without any additional underwater training. Earth2Ocean consists of two core components: a Geometric-guided Visual Mask Generator (\textbf{GMG}) that refines visual features via self-similarity geometric priors for local structure perception, and a Category-visual Semantic Alignment (\textbf{CSA}) module that enhances text embeddings through multimodal large language model reasoning and scene-aware template construction. Extensive experiments on the UOVSBench benchmark demonstrate that Earth2Ocean achieves significant performance improvement on average while maintaining efficient inference.

</details>


### [270] [DiffRegCD: Integrated Registration and Change Detection with Diffusion Features](https://arxiv.org/abs/2511.07935)
*Seyedehnanita Madani,Rama Chellappa,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出DiffRegCD框架统一密集配准和变化检测，在多数据集实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测模型在处理输入图像未对齐问题时效果不佳，传统和现有联合框架在大位移下有局限。

Method: 将对应估计重新表述为高斯平滑分类任务，利用预训练去噪扩散模型的冻结多尺度特征，通过对标准CD数据集应用受控仿射扰动提供监督。

Result: 在多个航空和地面数据集上实验，DiffRegCD始终超越近期基线，在大时间和几何变化下仍可靠。

Conclusion: 基于扩散特征和分类的对应关系为统一变化检测奠定了坚实基础。

Abstract: Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.

</details>


### [271] [Libra-MIL: Multimodal Prototypes Stereoscopic Infused with Task-specific Language Priors for Few-shot Whole Slide Image Classification](https://arxiv.org/abs/2511.07941)
*Zhenfeng Zhuang,Fangyu Zhou,Liansheng Wang*

Main category: cs.CV

TL;DR: 论文提出多模态原型多实例学习方法，促进双向交互，在三个癌症数据集实验显示出优越泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于计算病理学计算成本高，需多实例学习；病理任务只有包级标签，大语言模型实例级描述有偏差，现有视觉 - 语言多实例学习方法单向引导限制跨模态协同。

Method: 提出多模态原型多实例学习方法，利用冻结大语言模型生成任务特定病理实体描述作为文本原型，视觉分支学习实例级原型，融合阶段采用立体最优传输算法。

Result: 在三个不同癌症数据集上进行少样本分类和可解释性实验，所提方法展现出优越泛化能力。

Conclusion: 构建特定任务的病理实体原型对学习可泛化特征和增强模型可解释性至关重要，所提多模态原型多实例学习方法有效。

Abstract: While Large Language Models (LLMs) are emerging as a promising direction in computational pathology, the substantial computational cost of giga-pixel Whole Slide Images (WSIs) necessitates the use of Multi-Instance Learning (MIL) to enable effective modeling. A key challenge is that pathological tasks typically provide only bag-level labels, while instance-level descriptions generated by LLMs often suffer from bias due to a lack of fine-grained medical knowledge. To address this, we propose that constructing task-specific pathological entity prototypes is crucial for learning generalizable features and enhancing model interpretability. Furthermore, existing vision-language MIL methods often employ unidirectional guidance, limiting cross-modal synergy. In this paper, we introduce a novel approach, Multimodal Prototype-based Multi-Instance Learning, that promotes bidirectional interaction through a balanced information compression scheme. Specifically, we leverage a frozen LLM to generate task-specific pathological entity descriptions, which are learned as text prototypes. Concurrently, the vision branch learns instance-level prototypes to mitigate the model's reliance on redundant data. For the fusion stage, we employ the Stereoscopic Optimal Transport (SOT) algorithm, which is based on a similarity metric, thereby facilitating broader semantic alignment in a higher-dimensional space. We conduct few-shot classification and explainability experiments on three distinct cancer datasets, and the results demonstrate the superior generalization capabilities of our proposed method.

</details>


### [272] [Mitigating Negative Flips via Margin Preserving Training](https://arxiv.org/abs/2511.08322)
*Simone Ricci,Niccolò Biondi,Federico Pernici,Alberto Del Bimbo*

Main category: cs.CV

TL;DR: 提出新方法减少图像分类中AI系统版本更新的负翻转率，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: AI系统版本更新负翻转问题随训练类别增加而加剧，需缓解负翻转。

Method: 提出在学习改进模型时保留原模型边界的方法，引入显式边界校准项，结合双源焦点蒸馏损失。

Result: 在图像分类基准测试中，该方法持续降低负翻转率且整体准确率高。

Conclusion: 所提方法能有效减少负翻转，提升AI系统版本更新时的性能。

Abstract: Minimizing inconsistencies across successive versions of an AI system is as crucial as reducing the overall error. In image classification, such inconsistencies manifest as negative flips, where an updated model misclassifies test samples that were previously classified correctly. This issue becomes increasingly pronounced as the number of training classes grows over time, since adding new categories reduces the margin of each class and may introduce conflicting patterns that undermine their learning process, thereby degrading performance on the original subset. To mitigate negative flips, we propose a novel approach that preserves the margins of the original model while learning an improved one. Our method encourages a larger relative margin between the previously learned and newly introduced classes by introducing an explicit margin-calibration term on the logits. However, overly constraining the logit margin for the new classes can significantly degrade their accuracy compared to a new independently trained model. To address this, we integrate a double-source focal distillation loss with the previous model and a new independently trained model, learning an appropriate decision margin from both old and new data, even under a logit margin calibration. Extensive experiments on image classification benchmarks demonstrate that our approach consistently reduces the negative flip rate with high overall accuracy.

</details>


### [273] [Morphing Through Time: Diffusion-Based Bridging of Temporal Gaps for Robust Alignment in Change Detection](https://arxiv.org/abs/2511.07976)
*Seyedehanita Madani,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出模块化管道提升遥感变化检测时空鲁棒性，实验证明其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 遥感变化检测面临双时相图像空间不对齐问题，现有模型依赖精确配准且跨域迁移性差。

Method: 引入模块化管道，集成基于扩散的语义变形、密集配准和残差流细化，通过扩散模块合成中间变形帧，再用轻量级U - Net细化。

Result: 在多个数据集上，多个骨干网络的配准精度和下游变化检测均有持续提升。

Conclusion: 所提方法具有通用性和有效性。

Abstract: Remote sensing change detection is often challenged by spatial misalignment between bi-temporal images, especially when acquisitions are separated by long seasonal or multi-year gaps. While modern convolutional and transformer-based models perform well on aligned data, their reliance on precise co-registration limits their robustness in real-world conditions. Existing joint registration-detection frameworks typically require retraining and transfer poorly across domains. We introduce a modular pipeline that improves spatial and temporal robustness without altering existing change detection networks. The framework integrates diffusion-based semantic morphing, dense registration, and residual flow refinement. A diffusion module synthesizes intermediate morphing frames that bridge large appearance gaps, enabling RoMa to estimate stepwise correspondences between consecutive frames. The composed flow is then refined through a lightweight U-Net to produce a high-fidelity warp that co-registers the original image pair. Extensive experiments on LEVIR-CD, WHU-CD, and DSIFN-CD show consistent gains in both registration accuracy and downstream change detection across multiple backbones, demonstrating the generality and effectiveness of the proposed approach.

</details>


### [274] [Extreme Model Compression with Structured Sparsity at Low Precision](https://arxiv.org/abs/2511.08360)
*Dan Liu,Nikita Dvornik,Xue Liu*

Main category: cs.CV

TL;DR: 提出SLOPE框架结合结构化稀疏和低比特量化，用正则化策略解决组合问题，在多模型和任务上效果好。


<details>
  <summary>Details</summary>
Motivation: DNN规模大、计算成本高，结构化稀疏和量化单独有效但组合影响精度，需统一框架解决。

Method: 引入SLOPE框架，提出训练时正则化策略，促进角度对齐而非直接匹配。

Result: 在ResNet - 18上实现约20倍模型尺寸缩减，保留约99%原精度，在多任务和模型上优于现有方法。

Conclusion: SLOPE能有效结合结构化稀疏和低比特量化，提升模型资源利用效率。

Abstract: Deep neural networks (DNNs) are used in many applications, but their large size and high computational cost make them hard to run on devices with limited resources. Two widely used techniques to address this challenge are weight quantization, which lowers the precision of all weights, and structured sparsity, which removes unimportant weights while retaining the important ones at full precision. Although both are effective individually, they are typically studied in isolation due to their compounded negative impact on model accuracy when combined. In this work, we introduce SLOPE Structured Sparsity at Low Precision), a unified framework, to effectively combine structured sparsity and low-bit quantization in a principled way. We show that naively combining sparsity and quantization severely harms performance due to the compounded impact of both techniques. To address this, we propose a training-time regularization strategy that minimizes the discrepancy between full-precision weights and their sparse, quantized counterparts by promoting angular alignment rather than direct matching. On ResNet-18, SLOPE achieves $\sim20\times$ model size reduction while retaining $\sim$99% of the original accuracy. It consistently outperforms state-of-the-art quantization and structured sparsity methods across classification, detection, and segmentation tasks on models such as ResNet-18, ViT-Small, and Mask R-CNN.

</details>


### [275] [Hardware-Aware YOLO Compression for Low-Power Edge AI on STM32U5 for Weeds Detection in Digital Agriculture](https://arxiv.org/abs/2511.07990)
*Charalampos S. Kouzinopoulos,Yuri Manna*

Main category: cs.CV

TL;DR: 提出基于STM32U575ZI微控制器的低功耗边缘AI杂草检测系统，经压缩技术优化，在检测精度与效率间取得平衡，适用于电力受限农业环境。


<details>
  <summary>Details</summary>
Motivation: 传统杂草管理方法依赖化学除草剂有环境风险，精确除草依赖高算力平台，需低功耗方案。

Method: 在STM32U575ZI微控制器上部署YOLOv8n检测器，应用结构化剪枝、整数量化和输入图像分辨率缩放等压缩技术。

Result: 在含74种植物的CropAndWeed数据集上训练评估，每次推理能耗仅51.8mJ，支持实时实地杂草检测。

Conclusion: 该系统能在电力受限农业环境中实现可扩展部署。

Abstract: Weeds significantly reduce crop yields worldwide and pose major challenges to sustainable agriculture. Traditional weed management methods, primarily relying on chemical herbicides, risk environmental contamination and lead to the emergence of herbicide-resistant species. Precision weeding, leveraging computer vision and machine learning methods, offers a promising eco-friendly alternative but is often limited by reliance on high-power computational platforms. This work presents an optimized, low-power edge AI system for weeds detection based on the YOLOv8n object detector deployed on the STM32U575ZI microcontroller. Several compression techniques are applied to the detection model, including structured pruning, integer quantization and input image resolution scaling in order to meet strict hardware constraints. The model is trained and evaluated on the CropAndWeed dataset with 74 plant species, achieving a balanced trade-off between detection accuracy and efficiency. Our system supports real-time, in-situ weeds detection with a minimal energy consumption of 51.8mJ per inference, enabling scalable deployment in power-constrained agricultural environments.

</details>


### [276] [Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning](https://arxiv.org/abs/2511.08003)
*Jialong Qin,Xin Zou,Di Lu,Yibo Yan,Xuming Hu*

Main category: cs.CV

TL;DR: 提出SharpV方法解决VideoLLMs计算复杂度和缓存扩展问题，实验证明其优势且是首个无需注意力分数的两阶段剪枝框架。


<details>
  <summary>Details</summary>
Motivation: 当前VideoLLMs因处理过多冗余视觉令牌，存在二次计算复杂度和键值缓存扩展问题。

Method: 提出SharpV，动态调整视觉令牌和KV缓存剪枝率，在KV缓存剪枝阶段通过自校准方式剪枝退化视觉特征。

Result: 在多个公共基准测试中证明了SharpV的优越性，且是首个无需暴露注意力分数的两阶段剪枝框架，与硬件加速技术兼容。

Conclusion: SharpV为VideoLLMs自适应剪枝提供新范式和信息流动新见解。

Abstract: Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.

</details>


### [277] [Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation](https://arxiv.org/abs/2511.08402)
*Difei Gu,Yunhe Gao,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: 提出细粒度视觉语言模型Anatomy - VLM，在多数据集表现出色，能用于下游任务并提供零样本解剖学解释。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型忽视疾病诊断关键的细粒度图像细节，需结合图像特征与临床知识实现专家级诊断决策。

Method: 设计模型编码器定位关键解剖特征，用结构化知识丰富这些区域，对齐多尺度医学信息以生成疾病预测。

Result: Anatomy - VLM在分布内和分布外数据集上表现出色，在下游图像分割任务中得到验证，能进行零样本解剖学解释。

Conclusion: Anatomy - VLM具有强大的专家级临床解释能力。

Abstract: Accurate disease interpretation from radiology remains challenging due to imaging heterogeneity. Achieving expert-level diagnostic decisions requires integration of subtle image features with clinical knowledge. Yet major vision-language models (VLMs) treat images as holistic entities and overlook fine-grained image details that are vital for disease diagnosis. Clinicians analyze images by utilizing their prior medical knowledge and identify anatomical structures as important region of interests (ROIs). Inspired from this human-centric workflow, we introduce Anatomy-VLM, a fine-grained, vision-language model that incorporates multi-scale information. First, we design a model encoder to localize key anatomical features from entire medical images. Second, these regions are enriched with structured knowledge for contextually-aware interpretation. Finally, the model encoder aligns multi-scale medical information to generate clinically-interpretable disease prediction. Anatomy-VLM achieves outstanding performance on both in- and out-of-distribution datasets. We also validate the performance of Anatomy-VLM on downstream image segmentation tasks, suggesting that its fine-grained alignment captures anatomical and pathology-related knowledge. Furthermore, the Anatomy-VLM's encoder facilitates zero-shot anatomy-wise interpretation, providing its strong expert-level clinical interpretation capabilities.

</details>


### [278] [Invisible Triggers, Visible Threats! Road-Style Adversarial Creation Attack for Visual 3D Detection in Autonomous Driving](https://arxiv.org/abs/2511.08015)
*Jian Wang,Lijun He,Yixing Yong,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: 本文提出AdvRoad生成道路风格对抗样本，解决现有对抗海报问题，实验证明其有效性和现实威胁。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度神经网络的视觉3D检测模型易受对抗样本影响，现有对抗海报易被察觉和防御，需研究更隐蔽的攻击方法。

Method: 采用两阶段方法，即道路风格对抗样本生成和场景关联适配，生成自然外观的对抗海报。

Result: AdvRoad能很好地泛化到不同检测器、场景和欺骗位置，物理攻击证明了现实环境中的实际威胁。

Conclusion: AdvRoad是一种有效的、可隐蔽实施的自动驾驶场景对抗攻击方法。

Abstract: Modern autonomous driving (AD) systems leverage 3D object detection to perceive foreground objects in 3D environments for subsequent prediction and planning. Visual 3D detection based on RGB cameras provides a cost-effective solution compared to the LiDAR paradigm. While achieving promising detection accuracy, current deep neural network-based models remain highly susceptible to adversarial examples. The underlying safety concerns motivate us to investigate realistic adversarial attacks in AD scenarios. Previous work has demonstrated the feasibility of placing adversarial posters on the road surface to induce hallucinations in the detector. However, the unnatural appearance of the posters makes them easily noticeable by humans, and their fixed content can be readily targeted and defended. To address these limitations, we propose the AdvRoad to generate diverse road-style adversarial posters. The adversaries have naturalistic appearances resembling the road surface while compromising the detector to perceive non-existent objects at the attack locations. We employ a two-stage approach, termed Road-Style Adversary Generation and Scenario-Associated Adaptation, to maximize the attack effectiveness on the input scene while ensuring the natural appearance of the poster, allowing the attack to be carried out stealthily without drawing human attention. Extensive experiments show that AdvRoad generalizes well to different detectors, scenes, and spoofing locations. Moreover, physical attacks further demonstrate the practical threats in real-world environments.

</details>


### [279] [Multi-modal Deepfake Detection and Localization with FPN-Transformer](https://arxiv.org/abs/2511.08031)
*Chende Zheng,Ruiqi Suo,Zhoulin Ji,Jingyi Deng,Fangbin Yi,Chenhao Lin,Chao Shen*

Main category: cs.CV

TL;DR: 为解决单模态检测方法在跨模态泛化和时间边界回归上的不足，提出基于FPN - Transformer的多模态深度伪造检测与定位框架，在IJCAI'25 DDL - AV基准测试集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 生成对抗网络和扩散模型使深度伪造内容激增，单模态检测方法无法利用跨模态相关性和精确定位伪造片段，难以应对复杂精细的操作。

Method: 利用预训练自监督模型提取分层时间特征，通过R - TLM块构建多尺度特征金字塔，使用双分支预测头同时预测伪造概率和细化操纵片段的时间偏移。

Result: 在IJCAI'25 DDL - AV基准测试集上，跨模态深度伪造检测与定位最终得分0.7535。

Conclusion: 实验结果证实了该方法的有效性，为广义深度伪造检测提供了新途径。

Abstract: The rapid advancement of generative adversarial networks (GANs) and diffusion models has enabled the creation of highly realistic deepfake content, posing significant threats to digital trust across audio-visual domains. While unimodal detection methods have shown progress in identifying synthetic media, their inability to leverage cross-modal correlations and precisely localize forged segments limits their practicality against sophisticated, fine-grained manipulations. To address this, we introduce a multi-modal deepfake detection and localization framework based on a Feature Pyramid-Transformer (FPN-Transformer), addressing critical gaps in cross-modal generalization and temporal boundary regression. The proposed approach utilizes pre-trained self-supervised models (WavLM for audio, CLIP for video) to extract hierarchical temporal features. A multi-scale feature pyramid is constructed through R-TLM blocks with localized attention mechanisms, enabling joint analysis of cross-context temporal dependencies. The dual-branch prediction head simultaneously predicts forgery probabilities and refines temporal offsets of manipulated segments, achieving frame-level localization precision. We evaluate our approach on the test set of the IJCAI'25 DDL-AV benchmark, showing a good performance with a final score of 0.7535 for cross-modal deepfake detection and localization in challenging environments. Experimental results confirm the effectiveness of our approach and provide a novel way for generalized deepfake detection. Our code is available at https://github.com/Zig-HS/MM-DDL

</details>


### [280] [ProSona: Prompt-Guided Personalization for Multi-Expert Medical Image Segmentation](https://arxiv.org/abs/2511.08046)
*Aya Elgebaly,Nikolaos Delopoulos,Juliane Hörner-Rieber,Carolin Rippke,Sebastian Klüter,Luca Boldrini,Lorenzo Placidi,Riccardo Dal Bello,Nicolaus Andratschke,Michael Baumgartl,Claus Belka,Christopher Kurz,Guillaume Landry,Shadi Albarqouni*

Main category: cs.CV

TL;DR: 提出ProSona框架，通过自然语言提示实现可控个性化医学图像分割，在数据集上表现优于DPersona。


<details>
  <summary>Details</summary>
Motivation: 现有自动医学图像分割方法存在观察者间差异大的问题，且处理方式有局限。

Method: 引入两阶段框架ProSona，用概率U - Net骨干捕捉专家假设，通过提示引导投影机制生成个性化分割，用多级对比目标对齐文本和视觉表示。

Result: 在LIDC - IDRI肺结节和多机构前列腺MRI数据集上，比DPersona降低广义能量距离17%，平均Dice系数提高超1点。

Conclusion: 自然语言提示可对个性化医学图像分割提供灵活、准确和可解释的控制。

Abstract: Automated medical image segmentation suffers from high inter-observer variability, particularly in tasks such as lung nodule delineation, where experts often disagree. Existing approaches either collapse this variability into a consensus mask or rely on separate model branches for each annotator. We introduce ProSona, a two-stage framework that learns a continuous latent space of annotation styles, enabling controllable personalization via natural language prompts. A probabilistic U-Net backbone captures diverse expert hypotheses, while a prompt-guided projection mechanism navigates this latent space to generate personalized segmentations. A multi-level contrastive objective aligns textual and visual representations, promoting disentangled and interpretable expert styles. Across the LIDC-IDRI lung nodule and multi-institutional prostate MRI datasets, ProSona reduces the Generalized Energy Distance by 17% and improves mean Dice by more than one point compared with DPersona. These results demonstrate that natural-language prompts can provide flexible, accurate, and interpretable control over personalized medical image segmentation. Our implementation is available online 1 .

</details>


### [281] [Generalizable Blood Cell Detection via Unified Dataset and Faster R-CNN](https://arxiv.org/abs/2511.08465)
*Siddharth Sahay*

Main category: cs.CV

TL;DR: 本文提出外周血细胞自动分类和检测方法，合并数据集，对比不同训练方案，迁移学习方案效果更好，为血液诊断系统奠基。


<details>
  <summary>Details</summary>
Motivation: 解决血细胞显微图像数据稀缺和异质性问题，构建高精度可部署的自动血液诊断系统。

Method: 开发数据管道合并四个公共数据集，采用Faster R - CNN框架和ResNet - 50 - FPN骨干网络，对比随机初始化和迁移学习两种训练方案。

Result: 迁移学习方案收敛更快、稳定性更好，最终验证损失为0.08666，优于基线。

Conclusion: 该验证方法为构建高精度可部署的自动血液诊断系统奠定了坚实基础。

Abstract: This paper presents a comprehensive methodology and comparative performance analysis for the automated classification and object detection of peripheral blood cells (PBCs) in microscopic images. Addressing the critical challenge of data scarcity and heterogeneity, robust data pipeline was first developed to standardize and merge four public datasets (PBC, BCCD, Chula, Sickle Cell) into a unified resource. Then employed a state-of-the-art Faster R-CNN object detection framework, leveraging a ResNet-50-FPN backbone. Comparative training rigorously evaluated a randomly initialized baseline model (Regimen 1) against a Transfer Learning Regimen (Regimen 2), initialized with weights pre-trained on the Microsoft COCO dataset. The results demonstrate that the Transfer Learning approach achieved significantly faster convergence and superior stability, culminating in a final validation loss of 0.08666, a substantial improvement over the baseline. This validated methodology establishes a robust foundation for building high-accuracy, deployable systems for automated hematological diagnosis.

</details>


### [282] [Taming Identity Consistency and Prompt Diversity in Diffusion Models via Latent Concatenation and Masked Conditional Flow Matching](https://arxiv.org/abs/2511.08061)
*Aditi Singhania,Arushi Jain,Krutik Malani,Riddhi Dhawan,Souymodip Chakraborty,Vineet Batra,Ankit Phogat*

Main category: cs.CV

TL;DR: 提出LoRA微调扩散模型、两阶段蒸馏数据整理框架和CHARIS评估框架用于主题驱动图像生成，平衡身份一致性和提示多样性。


<details>
  <summary>Details</summary>
Motivation: 解决主题驱动图像生成中强身份一致性和高提示多样性的权衡问题。

Method: 采用潜在拼接策略的LoRA微调扩散模型结合掩码条件流匹配目标；引入两阶段蒸馏数据整理框架；提出CHARIS细粒度评估框架。

Result: 未明确提及具体结果

Conclusion: 未明确提及具体结论

Abstract: Subject-driven image generation aims to synthesize novel depictions of a specific subject across diverse contexts while preserving its core identity features. Achieving both strong identity consistency and high prompt diversity presents a fundamental trade-off. We propose a LoRA fine-tuned diffusion model employing a latent concatenation strategy, which jointly processes reference and target images, combined with a masked Conditional Flow Matching (CFM) objective. This approach enables robust identity preservation without architectural modifications. To facilitate large-scale training, we introduce a two-stage Distilled Data Curation Framework: the first stage leverages data restoration and VLM-based filtering to create a compact, high-quality seed dataset from diverse sources; the second stage utilizes these curated examples for parameter-efficient fine-tuning, thus scaling the generation capability across various subjects and contexts. Finally, for filtering and quality assessment, we present CHARIS, a fine-grained evaluation framework that performs attribute-level comparisons along five key axes: identity consistency, prompt adherence, region-wise color fidelity, visual quality, and transformation diversity.

</details>


### [283] [Radar-APLANC: Unsupervised Radar-based Heartbeat Sensing via Augmented Pseudo-Label and Noise Contrast](https://arxiv.org/abs/2511.08071)
*Ying Wang,Zhaodong Sun,Xu Cheng,Zuxian He,Xiaobai Li*

Main category: cs.CV

TL;DR: 提出无监督雷达心跳感应框架Radar - APLANC，在两数据集实验表现与有监督方法相当。


<details>
  <summary>Details</summary>
Motivation: 传统雷达心跳感应方法受噪声影响性能下降，有监督学习方法需昂贵标记信号。

Method: 利用雷达距离矩阵中心跳范围和噪声范围构建正负样本；使用NCT损失，避免依赖真实生理信号；设计伪标签增强方法提升信号质量。

Result: 在Equipleth数据集和自研雷达数据集上实验，无监督方法性能与有监督方法相当。

Conclusion: 提出的无监督框架Radar - APLANC有效，代码等资料可从指定链接获取。

Abstract: Frequency Modulated Continuous Wave (FMCW) radars can measure subtle chest wall oscillations to enable non-contact heartbeat sensing. However, traditional radar-based heartbeat sensing methods face performance degradation due to noise. Learning-based radar methods achieve better noise robustness but require costly labeled signals for supervised training. To overcome these limitations, we propose the first unsupervised framework for radar-based heartbeat sensing via Augmented Pseudo-Label and Noise Contrast (Radar-APLANC). We propose to use both the heartbeat range and noise range within the radar range matrix to construct the positive and negative samples, respectively, for improved noise robustness. Our Noise-Contrastive Triplet (NCT) loss only utilizes positive samples, negative samples, and pseudo-label signals generated by the traditional radar method, thereby avoiding dependence on expensive ground-truth physiological signals. We further design a pseudo-label augmentation approach featuring adaptive noise-aware label selection to improve pseudo-label signal quality. Extensive experiments on the Equipleth dataset and our collected radar dataset demonstrate that our unsupervised method achieves performance comparable to state-of-the-art supervised methods. Our code, dataset, and supplementary materials can be accessed from https://github.com/RadarHRSensing/Radar-APLANC.

</details>


### [284] [CLIP is All You Need for Human-like Semantic Representations in Stable Diffusion](https://arxiv.org/abs/2511.08075)
*Cameron Braunstein,Mariya Toneva,Eddy Ilg*

Main category: cs.CV

TL;DR: 研究潜在扩散模型在文本到图像生成中内部表示是否含人类可理解语义信息，发现语义表示主要来自CLIP，扩散过程是视觉解码器。


<details>
  <summary>Details</summary>
Motivation: 探究潜在扩散模型在文本到图像生成时内部表示是否包含对人类有意义的语义信息。

Method: 用简单回归层对Stable Diffusion进行探测，预测对象语义属性并与人类标注对比。

Result: 成功归因于CLIP文本编码，特定语义属性组解码准确率不同，逆扩散过程中属性更难区分。

Conclusion: 单独训练的CLIP视觉语言模型决定类人语义表示，扩散过程是视觉解码器。

Abstract: Latent diffusion models such as Stable Diffusion achieve state-of-the-art results on text-to-image generation tasks. However, the extent to which these models have a semantic understanding of the images they generate is not well understood. In this work, we investigate whether the internal representations used by these models during text-to-image generation contain semantic information that is meaningful to humans. To do so, we perform probing on Stable Diffusion with simple regression layers that predict semantic attributes for objects and evaluate these predictions against human annotations. Surprisingly, we find that this success can actually be attributed to the text encoding occurring in CLIP rather than the reverse diffusion process. We demonstrate that groups of specific semantic attributes have markedly different decoding accuracy than the average, and are thus represented to different degrees. Finally, we show that attributes become more difficult to disambiguate from one another during the inverse diffusion process, further demonstrating the strongest semantic representation of object attributes in CLIP. We conclude that the separately trained CLIP vision-language model is what determines the human-like semantic representation, and that the diffusion process instead takes the role of a visual decoder.

</details>


### [285] [CleverBirds: A Multiple-Choice Benchmark for Fine-grained Human Knowledge Tracing](https://arxiv.org/abs/2511.08512)
*Leonie Bossemeyer,Samuel Heinrich,Grant Van Horn,Oisin Mac Aodha*

Main category: cs.CV

TL;DR: 本文介绍了用于细粒度鸟类物种识别的大规模知识追踪基准CleverBirds，发布数据集以支持视觉知识追踪新方法的开发和评估，指出追踪学习者知识有挑战，该基准能助力研究视觉专业知识发展。


<details>
  <summary>Details</summary>
Motivation: 建模人类专业知识的发展具有挑战性，准确推断人类学习者的知识状态是理解视觉学习的关键一步，所以需要相关基准。

Method: 引入由公民科学平台eBird收集的CleverBirds基准数据集，有超4万参与者回答超1700万道选择题，涉及超1万种鸟类。

Result: 发现追踪学习者知识具有挑战性，尤其是在不同参与者子组和问题类型中，不同形式的上下文信息有不同程度的预测效益。

Conclusion: CleverBirds是同类中最大的基准之一，能为研究视觉专业知识随时间和个体的发展开辟新途径。

Abstract: Mastering fine-grained visual recognition, essential in many expert domains, can require that specialists undergo years of dedicated training. Modeling the progression of such expertize in humans remains challenging, and accurately inferring a human learner's knowledge state is a key step toward understanding visual learning. We introduce CleverBirds, a large-scale knowledge tracing benchmark for fine-grained bird species recognition. Collected by the citizen-science platform eBird, it offers insight into how individuals acquire expertize in complex fine-grained classification. More than 40,000 participants have engaged in the quiz, answering over 17 million multiple-choice questions spanning over 10,000 bird species, with long-range learning patterns across an average of 400 questions per participant. We release this dataset to support the development and evaluation of new methods for visual knowledge tracing. We show that tracking learners' knowledge is challenging, especially across participant subgroups and question types, with different forms of contextual information offering varying degrees of predictive benefit. CleverBirds is among the largest benchmark of its kind, offering a substantially higher number of learnable concepts. With it, we hope to enable new avenues for studying the development of visual expertize over time and across individuals.

</details>


### [286] [SENCA-st: Integrating Spatial Transcriptomics and Histopathology with Cross Attention Shared Encoder for Region Identification in Cancer Pathology](https://arxiv.org/abs/2511.08573)
*Shanaka Liyanaarachchi,Chathurya Wijethunga,Shihab Aaquil Ahamed,Akthas Absar,Ranga Rodrigo*

Main category: cs.CV

TL;DR: 提出SENCA - st架构整合组织病理学和空间转录组数据，在检测肿瘤异质性和微环境区域表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有组织病理学 - 空间转录组区域分割方法存在不足，易丢失重要信息，需更好方法整合两者数据。

Method: 提出SENCA - st架构，用交叉注意力机制保留双模态特征，强调结构相似但功能不同区域。

Result: 模型在检测肿瘤异质性和肿瘤微环境区域上超越了现有方法。

Conclusion: SENCA - st架构能有效整合组织病理学和空间转录组数据，在临床关键方面表现优异。

Abstract: Spatial transcriptomics is an emerging field that enables the identification of functional regions based on the spatial distribution of gene expression. Integrating this functional information present in transcriptomic data with structural data from histopathology images is an active research area with applications in identifying tumor substructures associated with cancer drug resistance. Current histopathology-spatial-transcriptomic region segmentation methods suffer due to either making spatial transcriptomics prominent by using histopathology features just to assist processing spatial transcriptomics data or using vanilla contrastive learning that make histopathology images prominent due to only promoting common features losing functional information. In both extremes, the model gets either lost in the noise of spatial transcriptomics or overly smoothed, losing essential information. Thus, we propose our novel architecture SENCA-st (Shared Encoder with Neighborhood Cross Attention) that preserves the features of both modalities. More importantly, it emphasizes regions that are structurally similar in histopathology but functionally different on spatial transcriptomics using cross-attention. We demonstrate the superior performance of our model that surpasses state-of-the-art methods in detecting tumor heterogeneity and tumor micro-environment regions, a clinically crucial aspect.

</details>


### [287] [Beyond the Pixels: VLM-based Evaluation of Identity Preservation in Reference-Guided Synthesis](https://arxiv.org/abs/2511.08087)
*Aditi Singhania,Krutik Malani,Riddhi Dhawan,Arushi Jain,Garv Tandon,Nippun Sharma,Souymodip Chakraborty,Vineet Batra,Ankit Phogat*

Main category: cs.CV

TL;DR: 提出Beyond the Pixels分层评估框架评估生成模型身份保留，在四个模型验证，引入新基准。


<details>
  <summary>Details</summary>
Motivation: 现有评估生成模型身份保留的指标无法捕捉细粒度身份变化且诊断性有限。

Method: 将身份评估分解为特征级转换，通过分层分解主体和具体转换提示引导VLM结构化推理。

Result: 框架与人类判断在衡量身份一致性上高度一致，新基准包含1078个图像 - 提示对。

Conclusion: 所提框架有效，新基准可用于压力测试生成模型。

Abstract: Evaluating identity preservation in generative models remains a critical yet unresolved challenge. Existing metrics rely on global embeddings or coarse VLM prompting, failing to capture fine-grained identity changes and providing limited diagnostic insight. We introduce Beyond the Pixels, a hierarchical evaluation framework that decomposes identity assessment into feature-level transformations. Our approach guides VLMs through structured reasoning by (1) hierarchically decomposing subjects into (type, style) -> attribute -> feature decision tree, and (2) prompting for concrete transformations rather than abstract similarity scores. This decomposition grounds VLM analysis in verifiable visual evidence, reducing hallucinations and improving consistency. We validate our framework across four state-of-the-art generative models, demonstrating strong alignment with human judgments in measuring identity consistency. Additionally, we introduce a new benchmark specifically designed to stress-test generative models. It comprises 1,078 image-prompt pairs spanning diverse subject types, including underrepresented categories such as anthropomorphic and animated characters, and captures an average of six to seven transformation axes per prompt.

</details>


### [288] [StableMorph: High-Quality Face Morph Generation with Stable Diffusion](https://arxiv.org/abs/2511.08090)
*Wassim Kabbani,Kiran Raja,Raghavendra Ramachandra,Christoph Busch*

Main category: cs.CV

TL;DR: 本文提出StableMorph方法，利用扩散图像合成生成高质量人脸变形图像，对现有检测方案提出挑战，有助于生物识别安全评估和检测系统开发。


<details>
  <summary>Details</summary>
Motivation: 现有变形图像生成方法质量不佳，不能反映真实场景挑战，需高质量图像开发和评估人脸变形攻击检测系统。

Method: 引入StableMorph，利用现代基于扩散的图像合成技术生成高度逼真、无伪影的变形人脸图像。

Result: StableMorph生成的图像质量可与真实人脸图像媲美甚至超越，能有效欺骗人脸识别系统，对现有检测方案构成更大挑战。

Conclusion: StableMorph通过创建更真实有效的攻击，改善生物识别安全评估，支持更强大检测系统的开发。

Abstract: Face morphing attacks threaten the integrity of biometric identity systems by enabling multiple individuals to share a single identity. To develop and evaluate effective morphing attack detection (MAD) systems, we need access to high-quality, realistic morphed images that reflect the challenges posed in real-world scenarios. However, existing morph generation methods often produce images that are blurry, riddled with artifacts, or poorly constructed making them easy to detect and not representative of the most dangerous attacks. In this work, we introduce StableMorph, a novel approach that generates highly realistic, artifact-free morphed face images using modern diffusion-based image synthesis. Unlike prior methods, StableMorph produces full-head images with sharp details, avoids common visual flaws, and offers unmatched control over visual attributes. Through extensive evaluation, we show that StableMorph images not only rival or exceed the quality of genuine face images but also maintain a strong ability to fool face recognition systems posing a greater challenge to existing MAD solutions and setting a new standard for morph quality in research and operational testing. StableMorph improves the evaluation of biometric security by creating more realistic and effective attacks and supports the development of more robust detection systems.

</details>


### [289] [OTSNet: A Neurocognitive-Inspired Observation-Thinking-Spelling Pipeline for Scene Text Recognition](https://arxiv.org/abs/2511.08133)
*Lixu Sun,Nurmemet Yolwas,Wushour Silamu*

Main category: cs.CV

TL;DR: 针对场景文本识别难题，提出OTSNet网络，实验显示其达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有场景文本识别框架中视觉 - 语言解耦优化会因跨模态失准放大误差传播，视觉编码器有注意力偏差，解码器解析变形文本存在空间失准问题，影响不规则模式识别精度。

Method: 提出OTSNet网络，包含DAME模块抑制无关区域、PAM和SQ模块融合空间上下文与语义抽象、MMCV模块通过多模态特征融合自纠错。

Result: OTSNet在Union14M - L基准测试中平均准确率达83.5%，在OST数据集达79.1%，在9个评估场景中创新纪录。

Conclusion: OTSNet在场景文本识别中表现优异，达到了当前最优性能。

Abstract: Scene Text Recognition (STR) remains challenging due to real-world complexities, where decoupled visual-linguistic optimization in existing frameworks amplifies error propagation through cross-modal misalignment. Visual encoders exhibit attention bias toward background distractors, while decoders suffer from spatial misalignment when parsing geometrically deformed text-collectively degrading recognition accuracy for irregular patterns. Inspired by the hierarchical cognitive processes in human visual perception, we propose OTSNet, a novel three-stage network embodying a neurocognitive-inspired Observation-Thinking-Spelling pipeline for unified STR modeling. The architecture comprises three core components: (1) a Dual Attention Macaron Encoder (DAME) that refines visual features through differential attention maps to suppress irrelevant regions and enhance discriminative focus; (2) a Position-Aware Module (PAM) and Semantic Quantizer (SQ) that jointly integrate spatial context with glyph-level semantic abstraction via adaptive sampling; and (3) a Multi-Modal Collaborative Verifier (MMCV) that enforces self-correction through cross-modal fusion of visual, semantic, and character-level features. Extensive experiments demonstrate that OTSNet achieves state-of-the-art performance, attaining 83.5% average accuracy on the challenging Union14M-L benchmark and 79.1% on the heavily occluded OST dataset-establishing new records across 9 out of 14 evaluation scenarios.

</details>


### [290] [2D Representation for Unguided Single-View 3D Super-Resolution in Real-Time](https://arxiv.org/abs/2511.08224)
*Ignasi Mas,Ivan Huerta,Ramon Morros,Javier Ruiz-Hidalgo*

Main category: cs.CV

TL;DR: 提出2Dto3D - SR框架用于实时单视图3D超分辨率，无需高分辨率RGB引导，用不同模型评估效果好。


<details>
  <summary>Details</summary>
Motivation: 解决实时单视图3D超分辨率中高分辨率RGB引导的需求问题，提供更简单实用方案。

Method: 将单视角3D数据编码为结构化2D表示，用PNCC表示3D几何，使用Swin Transformers和Vision Mamba两种模型实现。

Result: Swin Transformer模型在标准基准上达最先进精度，Vision Mamba模型实时速度下结果有竞争力。

Conclusion: 几何引导的2Dto3D - SR框架是简单、可行且适用于现实场景的解决方案。

Abstract: We introduce 2Dto3D-SR, a versatile framework for real-time single-view 3D super-resolution that eliminates the need for high-resolution RGB guidance. Our framework encodes 3D data from a single viewpoint into a structured 2D representation, enabling the direct application of existing 2D image super-resolution architectures. We utilize the Projected Normalized Coordinate Code (PNCC) to represent 3D geometry from a visible surface as a regular image, thereby circumventing the complexities of 3D point-based or RGB-guided methods. This design supports lightweight and fast models adaptable to various deployment environments. We evaluate 2Dto3D-SR with two implementations: one using Swin Transformers for high accuracy, and another using Vision Mamba for high efficiency. Experiments show the Swin Transformer model achieves state-of-the-art accuracy on standard benchmarks, while the Vision Mamba model delivers competitive results at real-time speeds. This establishes our geometry-guided pipeline as a surprisingly simple yet viable and practical solution for real-world scenarios, especially where high-resolution RGB data is inaccessible.

</details>


### [291] [Remodeling Semantic Relationships in Vision-Language Fine-Tuning](https://arxiv.org/abs/2511.08238)
*Xiangyang Wu,Liu Liu,Baosheng Yu,Jiayan Qiu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 提出一种基于语义和关系改善多模态对齐与融合的方法，在多个模型和下游任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言微调方法在对齐视觉和语言时忽略文本上下文语义关系信息，导致性能不佳。

Method: 从不同视觉编码器提取多级语义特征，将视觉特征投影到相关语义组，使用可继承交叉注意力融合视觉和文本特征并去除冗余视觉关系。

Result: 在八个基础模型和两个下游任务（视觉问答和图像字幕）上评估，该方法优于所有现有方法。

Conclusion: 所提方法能有效改善多模态对齐和融合，提高性能。

Abstract: Vision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and relationships.Specifically, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods.

</details>


### [292] [Hierarchical Direction Perception via Atomic Dot-Product Operators for Rotation-Invariant Point Clouds Learning](https://arxiv.org/abs/2511.08240)
*Chenyu Hu,Xiaotong Li,Hao Zhu,Biao Hou*

Main category: cs.CV

TL;DR: 现有方法难以充分利用点云多尺度方向特性，本文提出DiPVNet，含L2DP算子和DASFT，实验表明其在点云分类和分割任务达SOTA。


<details>
  <summary>Details</summary>
Motivation: 任意旋转使点云方向有变化，现有方法难以充分利用点云多尺度方向特性提升特征表示。

Method: 提出DiPVNet，核心是原子点积算子；局部引入L2DP算子，全局利用广义谐波分析构建DASFT以建模整体方向结构，并证明两算子旋转不变性。

Result: 在含噪声和大角度旋转的场景实验中，DiPVNet在点云分类和分割任务达到SOTA。

Conclusion: DiPVNet能有效解决点云旋转带来的表示学习挑战，可用于点云分类和分割任务。

Abstract: Point cloud processing has become a cornerstone technology in many 3D vision tasks. However, arbitrary rotations introduce variations in point cloud orientations, posing a long-standing challenge for effective representation learning. The core of this issue is the disruption of the point cloud's intrinsic directional characteristics caused by rotational perturbations. Recent methods attempt to implicitly model rotational equivariance and invariance, preserving directional information and propagating it into deep semantic spaces. Yet, they often fall short of fully exploiting the multiscale directional nature of point clouds to enhance feature representations. To address this, we propose the Direction-Perceptive Vector Network (DiPVNet). At its core is an atomic dot-product operator that simultaneously encodes directional selectivity and rotation invariance--endowing the network with both rotational symmetry modeling and adaptive directional perception. At the local level, we introduce a Learnable Local Dot-Product (L2DP) Operator, which enables interactions between a center point and its neighbors to adaptively capture the non-uniform local structures of point clouds. At the global level, we leverage generalized harmonic analysis to prove that the dot-product between point clouds and spherical sampling vectors is equivalent to a direction-aware spherical Fourier transform (DASFT). This leads to the construction of a global directional response spectrum for modeling holistic directional structures. We rigorously prove the rotation invariance of both operators. Extensive experiments on challenging scenarios involving noise and large-angle rotations demonstrate that DiPVNet achieves state-of-the-art performance on point cloud classification and segmentation tasks. Our code is available at https://github.com/wxszreal0/DiPVNet.

</details>


### [293] [NERVE: Neighbourhood & Entropy-guided Random-walk for training free open-Vocabulary sEgmentation](https://arxiv.org/abs/2511.08248)
*Kunal Mahatha,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出训练-free的OVSS基线NERVE，结合全局与局部信息，用随机游走优化亲和度，以熵选注意力图，在7个基准测试达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有训练-free的开放词汇语义分割方法存在计算昂贵、注意力图融合无效、依赖固定高斯核等局限。

Method: 提出NERVE方法，整合全局和局部信息，利用稳定扩散模型自注意力层邻域结构；引入随机游走优化亲和度；用基于熵的不确定性选择相关注意力图；无需传统后处理技术。

Result: 在7个流行语义分割基准测试上取得整体最先进的零样本分割性能。

Conclusion: NERVE为开放词汇语义分割提供了有效方法。

Abstract: Despite recent advances in Open-Vocabulary Semantic Segmentation (OVSS), existing training-free methods face several limitations: use of computationally expensive affinity refinement strategies, ineffective fusion of transformer attention maps due to equal weighting or reliance on fixed-size Gaussian kernels to reinforce local spatial smoothness, enforcing isotropic neighborhoods. We propose a strong baseline for training-free OVSS termed as NERVE (Neighbourhood \& Entropy-guided Random-walk for open-Vocabulary sEgmentation), which uniquely integrates global and fine-grained local information, exploiting the neighbourhood structure from the self-attention layer of a stable diffusion model. We also introduce a stochastic random walk for refining the affinity rather than relying on fixed-size Gaussian kernels for local context. This spatial diffusion process encourages propagation across connected and semantically related areas, enabling it to effectively delineate objects with arbitrary shapes. Whereas most existing approaches treat self-attention maps from different transformer heads or layers equally, our method uses entropy-based uncertainty to select the most relevant maps. Notably, our method does not require any conventional post-processing techniques like Conditional Random Fields (CRF) or Pixel-Adaptive Mask Refinement (PAMR). Experiments are performed on 7 popular semantic segmentation benchmarks, yielding an overall state-of-the-art zero-shot segmentation performance, providing an effective approach to open-vocabulary semantic segmentation.

</details>


### [294] [ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation](https://arxiv.org/abs/2511.08263)
*Yue Min,Shaobo Wang,Jiaze Li,Tianle Niu,Junxin Fan,Yongliang Miao,Lijin Yang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出ImageBindDC数据凝聚框架，采用CF损失函数，在多模态数据凝聚上表现出色，在NYU - v2数据集上效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有数据凝聚技术在多模态场景中难以保留复杂的模态间依赖关系，需要新方法解决。

Method: 在ImageBind统一特征空间中引入ImageBindDC框架，采用CF损失函数，从单模态、跨模态和联合模态三个层面实现分布一致性。

Result: 在NYU - v2数据集上，每类仅用5个凝聚数据点训练的模型达到无损性能，比之前最佳方法绝对提升8.2%，凝聚时间减少超4倍。

Conclusion: ImageBindDC框架在多模态数据凝聚方面有效且高效，达到新的技术水平。

Abstract: Data condensation techniques aim to synthesize a compact dataset from a larger one to enable efficient model training, yet while successful in unimodal settings, they often fail in multimodal scenarios where preserving intricate inter-modal dependencies is crucial. To address this, we introduce ImageBindDC, a novel data condensation framework operating within the unified feature space of ImageBind. Our approach moves beyond conventional distribution-matching by employing a powerful Characteristic Function (CF) loss, which operates in the Fourier domain to facilitate a more precise statistical alignment via exact infinite moment matching. We design our objective to enforce three critical levels of distributional consistency: (i) uni-modal alignment, which matches the statistical properties of synthetic and real data within each modality; (ii) cross-modal alignment, which preserves pairwise semantics by matching the distributions of hybrid real-synthetic data pairs; and (iii) joint-modal alignment, which captures the complete multivariate data structure by aligning the joint distribution of real data pairs with their synthetic counterparts. Extensive experiments highlight the effectiveness of ImageBindDC: on the NYU-v2 dataset, a model trained on just 5 condensed datapoints per class achieves lossless performance comparable to one trained on the full dataset, achieving a new state-of-the-art with an 8.2\% absolute improvement over the previous best method and more than 4$\times$ less condensation time.

</details>


### [295] [Towards Open-Set Myoelectric Gesture Recognition via Dual-Perspective Inconsistency Learning](https://arxiv.org/abs/2511.08344)
*Chen Liu,Can Han,Weishi Xu,Yaqi Wang,Dahong Qian*

Main category: cs.CV

TL;DR: 提出基于扩散的sEMG数据增强方法SASG - DA，实验表明其优于现有方法，能缓解过拟合、提升性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: sEMG手势识别系统训练数据稀缺，导致深度学习模型过拟合和泛化能力差，现有数据增强方法存在问题。

Method: 提出SASG - DA，引入SRG机制增强生成忠实性，提出GMSS策略实现多样样本生成，引入稀疏感知语义采样策略提升针对性多样性。

Result: 在Ninapro DB2、DB4和DB7数据集上实验，SASG - DA显著优于现有增强方法。

Conclusion: SASG - DA能提供忠实且多样的样本，有效缓解过拟合，提高识别性能和泛化能力。

Abstract: Surface electromyography (sEMG)-based gesture recognition plays a critical role in human-machine interaction (HMI), particularly for rehabilitation and prosthetic control. However, sEMG-based systems often suffer from the scarcity of informative training data, leading to overfitting and poor generalization in deep learning models. Data augmentation offers a promising approach to increasing the size and diversity of training data, where faithfulness and diversity are two critical factors to effectiveness. However, promoting untargeted diversity can result in redundant samples with limited utility. To address these challenges, we propose a novel diffusion-based data augmentation approach, Sparse-Aware Semantic-Guided Diffusion Augmentation (SASG-DA). To enhance generation faithfulness, we introduce the Semantic Representation Guidance (SRG) mechanism by leveraging fine-grained, task-aware semantic representations as generation conditions. To enable flexible and diverse sample generation, we propose a Gaussian Modeling Semantic Modeling (GMSS) strategy, which models the semantic representation distribution and allows stochastic sampling to produce both faithful and diverse samples. To enhance targeted diversity, we further introduce a Sparse-Aware Semantic Sampling strategy to explicitly explore underrepresented regions, improving distribution coverage and sample utility. Extensive experiments on benchmark sEMG datasets, Ninapro DB2, DB4, and DB7, demonstrate that SASG-DA significantly outperforms existing augmentation methods. Overall, our proposed data augmentation approach effectively mitigates overfitting and improves recognition performance and generalization by offering both faithful and diverse samples.

</details>


### [296] [A Circular Argument : Does RoPE need to be Equivariant for Vision?](https://arxiv.org/abs/2511.08368)
*Chase van de Geijn,Timo Lüddecke,Polina Turishcheva,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 本文证明RoPE是一维数据等变位置嵌入的通用解，提出非等变的Spherical RoPE，发现其学习效果相当或更好，表明相对位置嵌入重要性被高估。


<details>
  <summary>Details</summary>
Motivation: 探讨RoPE在高维数据上的推广，质疑严格等变性对RoPE性能的作用。

Method: 从数学上证明RoPE是一维数据等变位置嵌入通用解，提出Spherical RoPE并进行实验。

Result: Spherical RoPE与等变的类似方法相比，有相当或更好的学习行为。

Conclusion: 相对位置嵌入的重要性可能被高估，去除必须是相对的先入之见或有助于未来视觉位置编码工作。

Abstract: Rotary Positional Encodings (RoPE) have emerged as a highly effective technique for one-dimensional sequences in Natural Language Processing spurring recent progress towards generalizing RoPE to higher-dimensional data such as images and videos. The success of RoPE has been thought to be due to its positional equivariance, i.e. its status as a relative positional encoding. In this paper, we mathematically show RoPE to be one of the most general solutions for equivariant positional embedding in one-dimensional data. Moreover, we show Mixed RoPE to be the analogously general solution for M-dimensional data, if we require commutative generators -- a property necessary for RoPE's equivariance. However, we question whether strict equivariance plays a large role in RoPE's performance. We propose Spherical RoPE, a method analogous to Mixed RoPE, but assumes non-commutative generators. Empirically, we find Spherical RoPE to have the equivalent or better learning behavior compared to its equivariant analogues. This suggests that relative positional embeddings are not as important as is commonly believed, at least within computer vision. We expect this discovery to facilitate future work in positional encodings for vision that can be faster and generalize better by removing the preconception that they must be relative.

</details>


### [297] [Text-based Aerial-Ground Person Retrieval](https://arxiv.org/abs/2511.08369)
*Xinyu Zhou,Yu Wu,Jiayao Ma,Wenhao Wang,Min Cao,Mang Ye*

Main category: cs.CV

TL;DR: 本文提出基于文本的空地行人检索（TAG - PR）任务，构建TAG - PEDES数据集，提出TAG - CLIP框架并评估其效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本的行人检索仅关注地面视角图像，TAG - PR引入不同视角图像，有更大实际意义和独特挑战。

Method: 构建TAG - PEDES数据集，采用多样化文本生成范式；提出TAG - CLIP框架，含分层路由专家混合模块和视角解耦策略。

Result: 在TAG - PEDES数据集和现有T - PR基准上评估了TAG - CLIP的有效性。

Conclusion: 提出的TAG - PR任务、TAG - PEDES数据集和TAG - CLIP框架有一定价值，代码和数据集已开源。

Abstract: This work introduces Text-based Aerial-Ground Person Retrieval (TAG-PR), which aims to retrieve person images from heterogeneous aerial and ground views with textual descriptions. Unlike traditional Text-based Person Retrieval (T-PR), which focuses solely on ground-view images, TAG-PR introduces greater practical significance and presents unique challenges due to the large viewpoint discrepancy across images. To support this task, we contribute: (1) TAG-PEDES dataset, constructed from public benchmarks with automatically generated textual descriptions, enhanced by a diversified text generation paradigm to ensure robustness under view heterogeneity; and (2) TAG-CLIP, a novel retrieval framework that addresses view heterogeneity through a hierarchically-routed mixture of experts module to learn view-specific and view-agnostic features and a viewpoint decoupling strategy to decouple view-specific features for better cross-modal alignment. We evaluate the effectiveness of TAG-CLIP on both the proposed TAG-PEDES dataset and existing T-PR benchmarks. The dataset and code are available at https://github.com/Flame-Chasers/TAG-PR.

</details>


### [298] [RAPTR: Radar-based 3D Pose Estimation using Transformer](https://arxiv.org/abs/2511.08387)
*Sorachi Kato,Ryoma Yataka,Pu Perry Wang,Pedro Miraldo,Takuya Fujihashi,Petros Boufounos*

Main category: cs.CV

TL;DR: 提出弱监督下的雷达室内3D人体姿态估计方法RAPTR，用易收集的标签，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于雷达的室内3D人体姿态估计依赖细粒度3D关键点标签，获取成本高，需更易收集的标签方法。

Method: 提出RAPTR，采用两阶段姿态解码器架构和伪3D可变形注意力，用3D模板损失和3D重力损失，结合3D BBox和2D关键点标签。

Result: 在两个室内雷达数据集上评估，RAPTR优于现有方法，在HIBER和MMVR上分别降低关节位置误差34.3%和76.9%。

Conclusion: RAPTR在弱监督下有效提高室内3D人体姿态估计性能，代码开源。

Abstract: Radar-based indoor 3D human pose estimation typically relied on fine-grained 3D keypoint labels, which are costly to obtain especially in complex indoor settings involving clutter, occlusions, or multiple people. In this paper, we propose \textbf{RAPTR} (RAdar Pose esTimation using tRansformer) under weak supervision, using only 3D BBox and 2D keypoint labels which are considerably easier and more scalable to collect. Our RAPTR is characterized by a two-stage pose decoder architecture with a pseudo-3D deformable attention to enhance (pose/joint) queries with multi-view radar features: a pose decoder estimates initial 3D poses with a 3D template loss designed to utilize the 3D BBox labels and mitigate depth ambiguities; and a joint decoder refines the initial poses with 2D keypoint labels and a 3D gravity loss. Evaluated on two indoor radar datasets, RAPTR outperforms existing methods, reducing joint position error by $34.3\%$ on HIBER and $76.9\%$ on MMVR. Our implementation is available at https://github.com/merlresearch/radar-pose-transformer.

</details>


### [299] [Contrastive Integrated Gradients: A Feature Attribution-Based Method for Explaining Whole Slide Image Classification](https://arxiv.org/abs/2511.08464)
*Anh Mai Vu,Tuan L. Vo,Ngoc Lam Quang Bui,Nam Nguyen Le Binh,Akash Awasthi,Huy Quoc Vo,Thanh-Huy Nguyen,Zhu Han,Chandra Mohan,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 提出对比积分梯度（CIG）方法用于全切片图像（WSI）分析，增强可解释性，通过三个数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法直接应用于WSI有挑战，且可能忽略区分肿瘤亚型的关键信号，需要增强WSI分析的可解释性。

Method: 引入CIG方法，在对数空间计算对比梯度；提出MIL - AIC和MIL - SIC两个归因质量指标。

Result: 在三个不同癌症类型数据集上实验，CIG在定量和定性方面都产生更具信息性的归因。

Conclusion: CIG有潜力用于可解释和可信的基于WSI的诊断。

Abstract: Interpretability is essential in Whole Slide Image (WSI) analysis for computational pathology, where understanding model predictions helps build trust in AI-assisted diagnostics. While Integrated Gradients (IG) and related attribution methods have shown promise, applying them directly to WSIs introduces challenges due to their high-resolution nature. These methods capture model decision patterns but may overlook class-discriminative signals that are crucial for distinguishing between tumor subtypes. In this work, we introduce Contrastive Integrated Gradients (CIG), a novel attribution method that enhances interpretability by computing contrastive gradients in logit space. First, CIG highlights class-discriminative regions by comparing feature importance relative to a reference class, offering sharper differentiation between tumor and non-tumor areas. Second, CIG satisfies the axioms of integrated attribution, ensuring consistency and theoretical soundness. Third, we propose two attribution quality metrics, MIL-AIC and MIL-SIC, which measure how predictive information and model confidence evolve with access to salient regions, particularly under weak supervision. We validate CIG across three datasets spanning distinct cancer types: CAMELYON16 (breast cancer metastasis in lymph nodes), TCGA-RCC (renal cell carcinoma), and TCGA-Lung (lung cancer). Experimental results demonstrate that CIG yields more informative attributions both quantitatively, using MIL-AIC and MIL-SIC, and qualitatively, through visualizations that align closely with ground truth tumor regions, underscoring its potential for interpretable and trustworthy WSI-based diagnostics

</details>


### [300] [Large Sign Language Models: Toward 3D American Sign Language Translation](https://arxiv.org/abs/2511.08535)
*Sen Zhang,Xiaoxiao He,Di Liu,Zhaoyang Xia,Mingyu Zhao,Chaowei Tan,Vivian Li,Bo Liu,Dimitris N. Metaxas,Mubbasir Kapadia*

Main category: cs.CV

TL;DR: 提出大手语模型（LSLM）用于3D美国手语翻译，利用大语言模型，能提升听障人士虚拟交流体验，还探索多模态语言处理。


<details>
  <summary>Details</summary>
Motivation: 提升听障人士虚拟交流的便利性，拓展大语言模型对人类交流的理解，处理多模态语言。

Method: 以大语言模型为骨干，直接利用3D手语数据进行翻译，探索直接翻译和指令引导设置。

Result: 实现更准确和有韧性的翻译，提升数字交流可及性，为多模态智能系统提供基础。

Conclusion: 该工作是迈向包容性、多模态智能系统的基础一步。

Abstract: We present Large Sign Language Models (LSLM), a novel framework for translating 3D American Sign Language (ASL) by leveraging Large Language Models (LLMs) as the backbone, which can benefit hearing-impaired individuals' virtual communication. Unlike existing sign language recognition methods that rely on 2D video, our approach directly utilizes 3D sign language data to capture rich spatial, gestural, and depth information in 3D scenes. This enables more accurate and resilient translation, enhancing digital communication accessibility for the hearing-impaired community. Beyond the task of ASL translation, our work explores the integration of complex, embodied multimodal languages into the processing capabilities of LLMs, moving beyond purely text-based inputs to broaden their understanding of human communication. We investigate both direct translation from 3D gesture features to text and an instruction-guided setting where translations can be modulated by external prompts, offering greater flexibility. This work provides a foundational step toward inclusive, multimodal intelligent systems capable of understanding diverse forms of language.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [301] [Shocks Under Control: Taming Transonic Compressible Flow over an RAE2822 Airfoil with Deep Reinforcement Learning](https://arxiv.org/abs/2511.07564)
*Trishit Mondal,Ricardo Vinuesa,Ameya D. Jagtap*

Main category: physics.flu-dyn

TL;DR: 使用深度强化学习（DRL）研究二维RAE2822翼型在Re = 50,000时可压缩跨音速激波 - 边界层相互作用的主动流动控制，训练的控制器能有效控制流动，实验显示在不同目标下对阻力、升力和升阻比有显著改善。


<details>
  <summary>Details</summary>
Motivation: 研究可压缩跨音速激波 - 边界层相互作用的主动流动控制，解决复杂流动的控制问题。

Method: 利用深度强化学习，结合高保真CFD求解器获取准确流场，采用合成射流作动来操纵非定常流动特征，DRL智能体与高保真可压缩流模拟直接交互自主发现有效控制策略。

Result: 在两组实验中，第一组降低平均阻力系数13.78%、增加升力131.18%、提高升阻比121.52%；第二组减少阻力25.62%、大幅增加升力196.30%、升阻比提高220.26%，且振荡显著减弱。

Conclusion: DRL-based控制在管理复杂流动动力学方面具有潜力。

Abstract: Active flow control of compressible transonic shock-boundary layer interactions over a two-dimensional RAE2822 airfoil at Re = 50,000 is investigated using deep reinforcement learning (DRL). The flow field exhibits highly unsteady dynamics, including complex shock-boundary layer interactions, shock oscillations, and the generation of Kutta waves from the trailing edge. A high-fidelity CFD solver, employing a fifth-order spectral discontinuous Galerkin scheme in space and a strong-stability-preserving Runge-Kutta (5,4) method in time, together with adaptive mesh refinement capability, is used to obtain the accurate flow field. Synthetic jet actuation is employed to manipulate these unsteady flow features, while the DRL agent autonomously discovers effective control strategies through direct interaction with high-fidelity compressible flow simulations. The trained controllers effectively mitigate shock-induced separation, suppress unsteady oscillations, and manipulate aerodynamic forces under transonic conditions. In the first set of experiments, aimed at both drag reduction and lift enhancement, the DRL-based control reduces the average drag coefficient by 13.78% and increases lift by 131.18%, thereby improving the lift-to-drag ratio by 121.52%, which underscores its potential for managing complex flow dynamics. In the second set, targeting drag reduction while maintaining lift, the DRL-based control achieves a 25.62% reduction in drag and a substantial 196.30% increase in lift, accompanied by markedly diminished oscillations. In this case, the lift-to-drag ratio improves by 220.26%.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [302] [FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing](https://arxiv.org/abs/2511.07665)
*Yuzhe Fu,Changchun Zhou,Hancheng Ye,Bowen Duan,Qiyu Huang,Chiyue Wei,Cong Guo,Hai "Helen'' Li,Yiran Chen*

Main category: cs.AR

TL;DR: 提出用于大规模3D点云处理的FractalCloud硬件架构，有显著加速和节能效果。


<details>
  <summary>Details</summary>
Motivation: 现有PNNs处理大规模点云有计算和内存开销大问题，现有加速器针对小负载优化，扩展性差。

Method: 提出FractalCloud架构，含形状感知且硬件友好的Fractal分区方法和块并行点操作，有片上分形和灵活并行性的硬件设计。

Result: 在28nm技术实现的芯片布局中，相比现有加速器有21.7倍加速和27倍节能，保持网络精度。

Conclusion: FractalCloud对PNN推理有良好的可扩展性和效率。

Abstract: Three-dimensional (3D) point clouds are increasingly used in applications such as autonomous driving, robotics, and virtual reality (VR). Point-based neural networks (PNNs) have demonstrated strong performance in point cloud analysis, originally targeting small-scale inputs. However, as PNNs evolve to process large-scale point clouds with hundreds of thousands of points, all-to-all computation and global memory access in point cloud processing introduce substantial overhead, causing $O(n^2)$ computational complexity and memory traffic where n is the number of points}. Existing accelerators, primarily optimized for small-scale workloads, overlook this challenge and scale poorly due to inefficient partitioning and non-parallel architectures. To address these issues, we propose FractalCloud, a fractal-inspired hardware architecture for efficient large-scale 3D point cloud processing. FractalCloud introduces two key optimizations: (1) a co-designed Fractal method for shape-aware and hardware-friendly partitioning, and (2) block-parallel point operations that decompose and parallelize all point operations. A dedicated hardware design with on-chip fractal and flexible parallelism further enables fully parallel processing within limited memory resources. Implemented in 28 nm technology as a chip layout with a core area of 1.5 $mm^2$, FractalCloud achieves 21.7x speedup and 27x energy reduction over state-of-the-art accelerators while maintaining network accuracy, demonstrating its scalability and efficiency for PNN inference.

</details>


### [303] [BDD2Seq: Enabling Scalable Reversible-Circuit Synthesis via Graph-to-Sequence Learning](https://arxiv.org/abs/2511.08315)
*Mingkai Miao,Jianheng Tang,Guangyu Hu,Hongce Zhang*

Main category: cs.AR

TL;DR: 提出图到序列框架BDD2Seq预测BDD高质量变量顺序，实验显示比现代启发式算法效果更好。


<details>
  <summary>Details</summary>
Motivation: BDD可逆电路综合中寻找最优变量顺序是NP完全问题，现有启发式方法随电路复杂度增加效果变差。

Method: 采用图神经网络编码器、指针网络解码器和多样化束搜索，将电路网表视为图学习结构依赖。

Result: 在三个公共基准测试中，BDD2Seq的量子成本降低约1.4倍，综合速度快3.7倍。

Conclusion: 首次用基于图的生成模型和促进多样性的解码解决BDD可逆电路综合中的变量顺序问题。

Abstract: Binary Decision Diagrams (BDDs) are instrumental in many electronic design automation (EDA) tasks thanks to their compact representation of Boolean functions. In BDD-based reversible-circuit synthesis, which is critical for quantum computing, the chosen variable ordering governs the number of BDD nodes and thus the key metrics of resource consumption, such as Quantum Cost. Because finding an optimal variable ordering for BDDs is an NP-complete problem, existing heuristics often degrade as circuit complexity grows. We introduce BDD2Seq, a graph-to-sequence framework that couples a Graph Neural Network encoder with a Pointer-Network decoder and Diverse Beam Search to predict high-quality orderings. By treating the circuit netlist as a graph, BDD2Seq learns structural dependencies that conventional heuristics overlooked, yielding smaller BDDs and faster synthesis. Extensive experiments on three public benchmarks show that BDD2Seq achieves around 1.4 times lower Quantum Cost and 3.7 times faster synthesis than modern heuristic algorithms. To the best of our knowledge, this is the first work to tackle the variable-ordering problem in BDD-based reversible-circuit synthesis with a graph-based generative model and diversity-promoting decoding.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [304] [From Classical to Hybrid: A Practical Framework for Quantum-Enhanced Learning](https://arxiv.org/abs/2511.08205)
*Silvie Illésová,Tomáš Bezděk,Vojtěch Novák,Ivan Zelinka,Stefano Cacciatore,Martin Beseda*

Main category: quant-ph

TL;DR: 提出三阶段框架助力无量子专业知识从业者从经典转向混合量子 - 经典机器学习工作流，在鸢尾花数据集实验中提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 使无量子专业知识的从业者能够从经典机器学习过渡到混合量子 - 经典机器学习工作流。

Method: 提出三阶段框架，先使用经典自训练模型，再引入最小混合量子变体，最后通过QMetric进行诊断反馈来优化混合架构。

Result: 在鸢尾花数据集实验中，改进后的混合模型准确率从经典方法的0.31提升到量子方法的0.87。

Conclusion: 适当诊断引导下，适度的量子组件可增强混合学习中的类别分离和表示能力，为经典机器学习从业者利用量子增强方法提供实用途径。

Abstract: This work addresses the challenge of enabling practitioners without quantum expertise to transition from classical to hybrid quantum-classical machine learning workflows. We propose a three-stage framework: starting with a classical self-training model, then introducing a minimal hybrid quantum variant, and finally applying diagnostic feedback via QMetric to refine the hybrid architecture. In experiments on the Iris dataset, the refined hybrid model improved accuracy from 0.31 in the classical approach to 0.87 in the quantum approach. These results suggest that even modest quantum components, when guided by proper diagnostics, can enhance class separation and representation capacity in hybrid learning, offering a practical pathway for classical machine learning practitioners to leverage quantum-enhanced methods.

</details>


### [305] [An Information-Minimal Geometry for Qubit-Efficient Optimization](https://arxiv.org/abs/2511.08362)
*Gordon Ma,Dimitris G. Angelakis*

Main category: quant-ph

TL;DR: 本文将量子比特高效优化问题转化为几何问题，提出对数宽度流水线，在Gset Max - Cut实例上表现良好，还探讨了超越SA(2)的情况。


<details>
  <summary>Details</summary>
Motivation: 解决二次无约束二进制优化（QUBO）问题时标准量子电路探索指数大状态空间的问题，实现量子比特高效优化。

Method: 将量子比特高效优化重铸为几何问题，明确利用SA(2)几何，通过可微迭代比例拟合（IPF）步骤投影，用最大熵吉布斯采样器解码。

Result: 得到对数宽度流水线（$2\lceil\log_2 N\rceil + 2$个量子比特），在Gset Max - Cut实例上深度2 - 3的电路达到接近最优比率（$r^* \approx 0.99$），超越直接SA(2)基线。

Conclusion: 该框架通过赋予局部一致性问题具体凸几何和最小可微投影解决了差距，建立了清晰的多面体基线，超越SA(2)会导向谱几何。

Abstract: Qubit-efficient optimization seeks to represent an $N$-variable combinatorial problem within a Hilbert space smaller than $2^N$, using only as much quantum structure as the objective itself requires. Quadratic unconstrained binary optimization (QUBO) problems, for example, depend only on pairwise information -- expectations and correlations between binary variables -- yet standard quantum circuits explore exponentially large state spaces. We recast qubit-efficient optimization as a geometry problem: the minimal representation should match the $O(N^2)$ structure of quadratic objectives. The key insight is that the local-consistency problem -- ensuring that pairwise marginals correspond to a realizable global distribution -- coincides exactly with the Sherali-Adams level-2 polytope $\mathrm{SA}(2)$, the tightest convex relaxation expressible at the two-body level. Previous qubit-efficient approaches enforced this consistency only implicitly. Here we make it explicit: (a) anchoring learning to the $\mathrm{SA}(2)$ geometry, (b) projecting via a differentiable iterative-proportional-fitting (IPF) step, and (c) decoding through a maximum-entropy Gibbs sampler. This yields a logarithmic-width pipeline ($2\lceil\log_2 N\rceil + 2$ qubits) that is classically simulable yet achieves strong empirical performance. On Gset Max-Cut instances (N=800--2000), depth-2--3 circuits reach near-optimal ratios ($r^* \approx 0.99$), surpassing direct $\mathrm{SA}(2)$ baselines. The framework resolves the local-consistency gap by giving it a concrete convex geometry and a minimal differentiable projection, establishing a clean polyhedral baseline. Extending beyond $\mathrm{SA}(2)$ naturally leads to spectrahedral geometries, where curvature encodes global coherence and genuine quantum structure becomes necessary.

</details>


### [306] [Hybrid Quantum-Classical Selective State Space Artificial Intelligence](https://arxiv.org/abs/2511.08349)
*Amin Ebrahimi,Farzan Haddadi*

Main category: quant-ph

TL;DR: 本文提出用于Mamba架构的混合量子经典选择机制，分析量子子程序对大语言模型的影响，结果显示量子增强门控机制有潜力，混合模型在有限模拟步骤中有表现。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言处理模型因大规模矩阵乘法和高维优化存在时间复杂度问题，需利用量子系统计算优势解决。

Method: 提出用于Mamba架构的混合量子经典选择机制，利用变分量子电路作为量子门控模块。

Result: 在有限模拟步骤中，混合模型在重塑MNIST数据集前四个epoch达到24.6%准确率，使用一个量子层且有更高表达能力，优于纯经典选择机制。

Conclusion: 量子增强门控机制是实现可扩展、资源高效NLP模型的途径。

Abstract: Hybrid Quantum Classical (HQC) algorithms constitute one of the most effective paradigms for exploiting the computational advantages of quantum systems in large-scale numerical tasks. By operating in high-dimensional Hilbert spaces, quantum circuits enable exponential speed-ups and provide access to richer representations of cost landscapes compared to purely classical methods. These capabilities are particularly relevant for machine learning, where state-of-the-art models especially in Natural Language Processing (NLP) suffer from prohibitive time complexity due to massive matrix multiplications and high-dimensional optimization.
  In this manuscript, we propose a Hybrid Quantum Classical selection mechanism for the Mamba architecture, designed specifically for temporal sequence classification problems. Our approach leverages Variational Quantum Circuits (VQCs) as quantum gating modules that both enhance feature extraction and improve suppression of irrelevant information. This integration directly addresses the computational bottlenecks of deep learning architectures by exploiting quantum resources for more efficient representation learning.
  We analyze how introducing quantum subroutines into large language models (LLMs) impacts their generalization capability, expressivity, and parameter efficiency. The results highlight the potential of quantum-enhanced gating mechanisms as a path toward scalable, resource-efficient NLP models, in a limited simulation step. Within the first four epochs on a reshaped MNIST dataset with input format (batch, 784, d_model), our hybrid model achieved 24.6% accuracy while using one quantum layer and achieve higher expressivity, compared to 21.6% obtained by a purely classical selection mechanism. we state No founding

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [307] [ViPRA: Video Prediction for Robot Actions](https://arxiv.org/abs/2511.07732)
*Sandeep Routray,Hengkai Pan,Unnat Jain,Shikhar Bahl,Deepak Pathak*

Main category: cs.RO

TL;DR: 提出ViPRA框架，从无动作标注视频学习连续机器人控制，避免昂贵标注，性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 多数视频缺乏标注动作，限制其在机器人学习中的应用。

Method: 训练视频语言模型预测未来视觉观察和以运动为中心的潜在动作，用感知损失和光流一致性训练潜在动作，引入分块流匹配解码器映射潜在动作到连续动作序列。

Result: 在SIMPLER基准上提升16%，在现实操作任务中提升13%。

Conclusion: ViPRA框架有效，能从无动作视频学习连续机器人控制，避免昂贵标注，支持跨实体泛化。

Abstract: Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io

</details>


### [308] [SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control](https://arxiv.org/abs/2511.07820)
*Zhengyi Luo,Ye Yuan,Tingwu Wang,Chenran Li,Sirui Chen,Fernando Castañeda,Zi-Ang Cao,Jiefeng Li,David Minor,Qingwei Ben,Xingye Da,Runyu Ding,Cyrus Hogg,Lina Song,Edy Lim,Eugene Jeong,Tairan He,Haoru Xue,Wenli Xiao,Zi Wang,Simon Yuen,Jan Kautz,Yan Chang,Umar Iqbal,Linxi "Jim" Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: 研究表明扩大模型容量、数据和计算量可得到通用人形机器人控制器，还展示了模型实用机制，证明运动跟踪可作为人形控制基础。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人神经控制器规模小、行为集有限且训练资源少，缺乏类似大模型的扩展收益，需探索扩大规模的方法。

Method: 将运动跟踪作为人形控制任务，从网络规模、数据集体积和计算量三方面扩展构建基础模型，还提出实时通用运动学规划器和统一令牌空间两种机制。

Result: 模型性能随计算量和数据多样性增加而提升，学习到的表示能泛化到未见运动。

Conclusion: 大规模运动跟踪具有良好特性，可作为人形控制的实用基础。

Abstract: Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.

</details>


### [309] [Real-Time Performance Analysis of Multi-Fidelity Residual Physics-Informed Neural Process-Based State Estimation for Robotic Systems](https://arxiv.org/abs/2511.08231)
*Devin Hunter,Chinwendu Enyioha*

Main category: cs.RO

TL;DR: 本文探讨基于MFR - PINP的实时数据驱动估计方法用于机器人系统实时状态估计，结合SC框架处理模型不确定性，实验表明该模型在实时估计任务中有潜力。


<details>
  <summary>Details</summary>
Motivation: 随着数据驱动模型用于状态估计，需要有可靠误差范围的模型预测，尤其在安全关键应用中，且要解决选择精确运动学模型的模型失配问题。

Method: 采用多保真残差物理信息神经过程（MFR - PINP）学习低、高保真预测间的残差，结合分裂共形（SC）预测框架处理模型不确定性，提供基于MFR - PINP估计器在混合在线学习场景的实现细节。

Result: 实验中将该方法与卡尔曼滤波器的先进变体对比，MFR - PINP模型展现出有前景的结果。

Conclusion: MFR - PINP模型是实时估计任务的可行选择。

Abstract: Various neural network architectures are used in many of the state-of-the-art approaches for real-time nonlinear state estimation. With the ever-increasing incorporation of these data-driven models into the estimation domain, model predictions with reliable margins of error are a requirement -- especially for safety-critical applications. This paper discusses the application of a novel real-time, data-driven estimation approach based on the multi-fidelity residual physics-informed neural process (MFR-PINP) toward the real-time state estimation of a robotic system. Specifically, we address the model-mismatch issue of selecting an accurate kinematic model by tasking the MFR-PINP to also learn the residuals between simple, low-fidelity predictions and complex, high-fidelity ground-truth dynamics. To account for model uncertainty present in a physical implementation, robust uncertainty guarantees from the split conformal (SC) prediction framework are modeled in the training and inference paradigms. We provide implementation details of our MFR-PINP-based estimator for a hybrid online learning setting to validate our model's usage in real-time applications. Experimental results of our approach's performance in comparison to the state-of-the-art variants of the Kalman filter (i.e. unscented Kalman filter and deep Kalman filter) in estimation scenarios showed promising results for the MFR-PINP model as a viable option in real-time estimation tasks.

</details>


### [310] [X-IONet: Cross-Platform Inertial Odometry Network with Dual-Stage Attention](https://arxiv.org/abs/2511.08277)
*Dehan Shen,Changhao Chen*

Main category: cs.RO

TL;DR: 提出跨平台惯性里程计框架X - IONet，仅用单个IMU，在行人与四足机器人数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 基于学习的惯性里程计在行人导航进展显著，但扩展到四足机器人因运动模式差异面临挑战，原有模型在腿部平台性能下降。

Method: 引入X - IONet，含基于规则的专家选择模块分类平台，位移预测网络采用双阶段注意力架构，输出位移和不确定性并通过EKF融合进行状态估计。

Result: 在公开行人数据集和自采集四足机器人数据集上实验，X - IONet降低行人数据的ATE 14.3%、RTE 11.4%，四足机器人数据的ATE 52.8%、RTE 41.3%。

Conclusion: X - IONet在人类和腿部机器人平台的惯性导航中有效，能提升准确性和鲁棒性。

Abstract: Learning-based inertial odometry has achieved remarkable progress in pedestrian navigation. However, extending these methods to quadruped robots remains challenging due to their distinct and highly dynamic motion patterns. Models that perform well on pedestrian data often experience severe degradation when deployed on legged platforms. To tackle this challenge, we introduce X-IONet, a cross-platform inertial odometry framework that operates solely using a single Inertial Measurement Unit (IMU). X-IONet incorporates a rule-based expert selection module to classify motion platforms and route IMU sequences to platform-specific expert networks. The displacement prediction network features a dual-stage attention architecture that jointly models long-range temporal dependencies and inter-axis correlations, enabling accurate motion representation. It outputs both displacement and associated uncertainty, which are further fused through an Extended Kalman Filter (EKF) for robust state estimation. Extensive experiments on public pedestrian datasets and a self-collected quadruped robot dataset demonstrate that X-IONet achieves state-of-the-art performance, reducing Absolute Trajectory Error (ATE) by 14.3% and Relative Trajectory Error (RTE) by 11.4% on pedestrian data, and by 52.8% and 41.3% on quadruped robot data. These results highlight the effectiveness of X-IONet in advancing accurate and robust inertial navigation across both human and legged robot platforms.

</details>


### [311] [USV Obstacles Detection and Tracking in Marine Environments](https://arxiv.org/abs/2511.07950)
*Yara AlaaEldin,Enrico Simetti,Francesca Odone*

Main category: cs.RO

TL;DR: 本文评估已开发的无人水面艇（USV）障碍物检测跟踪系统性能，在ROS平台集成系统并测试，分析两种方法结果，提出混合方法构建障碍物地图。


<details>
  <summary>Details</summary>
Motivation: 开发适用于海洋环境的USV障碍物检测跟踪系统是具有挑战性的任务，需评估和改进已开发系统。

Method: 评估系统在海洋数据集上的性能，在ROS平台集成系统，用相机与LiDAR融合及仅用LiDAR点云两种方法进行实验分析。

Result: 对两种方法的实验结果进行了全面分析。

Conclusion: 提出结合两种方法优势的混合方法来构建USV周围环境的障碍物地图。

Abstract: Developing a robust and effective obstacle detection and tracking system for Unmanned Surface Vehicle (USV) at marine environments is a challenging task. Research efforts have been made in this area during the past years by GRAAL lab at the university of Genova that resulted in a methodology for detecting and tracking obstacles on the image plane and, then, locating them in the 3D LiDAR point cloud. In this work, we continue on the developed system by, firstly, evaluating its performance on recently published marine datasets. Then, we integrate the different blocks of the system on ROS platform where we could test it in real-time on synchronized LiDAR and camera data collected in various marine conditions available in the MIT marine datasets. We present a thorough experimental analysis of the results obtained using two approaches; one that uses sensor fusion between the camera and LiDAR to detect and track the obstacles and the other uses only the LiDAR point cloud for the detection and tracking. In the end, we propose a hybrid approach that merges the advantages of both approaches to build an informative obstacles map of the surrounding environment to the USV.

</details>


### [312] [AVOID-JACK: Avoidance of Jackknifing for Swarms of Long Heavy Articulated Vehicles](https://arxiv.org/abs/2511.08016)
*Adrian Schönnagel,Michael Dubé,Christoph Steup,Felix Keppler,Sanaz Mostaghim*

Main category: cs.RO

TL;DR: 本文提出利用去中心化群体智能避免重型铰接车辆（HAVs）折叠和相互碰撞的新方法，经仿真实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有文献未解决HAVs避免折叠和相互碰撞的问题，且该问题在物流自动化等现实应用中有重要意义。

Method: 提出纯基于反应的、去中心化群体智能策略，优先避免折叠并为避免相互碰撞奠定基础。

Result: 单HAV实验中99.8%成功避免折叠，86.7%和83.4%分别到达第一和第二目标；两个HAVs交互时相应比例分别为98.9%、79.4%和65.1%，99.7%的HAVs未发生相互碰撞。

Conclusion: 所提方法能有效避免HAVs折叠和相互碰撞。

Abstract: This paper presents a novel approach to avoiding jackknifing and mutual collisions in Heavy Articulated Vehicles (HAVs) by leveraging decentralized swarm intelligence. In contrast to typical swarm robotics research, our robots are elongated and exhibit complex kinematics, introducing unique challenges. Despite its relevance to real-world applications such as logistics automation, remote mining, airport baggage transport, and agricultural operations, this problem has not been addressed in the existing literature.
  To tackle this new class of swarm robotics problems, we propose a purely reaction-based, decentralized swarm intelligence strategy tailored to automate elongated, articulated vehicles. The method presented in this paper prioritizes jackknifing avoidance and establishes a foundation for mutual collision avoidance. We validate our approach through extensive simulation experiments and provide a comprehensive analysis of its performance. For the experiments with a single HAV, we observe that for 99.8% jackknifing was successfully avoided and that 86.7% and 83.4% reach their first and second goals, respectively. With two HAVs interacting, we observe 98.9%, 79.4%, and 65.1%, respectively, while 99.7% of the HAVs do not experience mutual collisions.

</details>


### [313] [SeFA-Policy: Fast and Accurate Visuomotor Policy Learning with Selective Flow Alignment](https://arxiv.org/abs/2511.08583)
*Rong Xue,Jiageng Mao,Mingtong Zhang,Yue Wang*

Main category: cs.RO

TL;DR: 提出Selective Flow Alignment (SeFA)框架解决视觉运动策略学习问题，实验显示其优于现有策略。


<details>
  <summary>Details</summary>
Motivation: 现有整流流方法在视觉运动策略学习中存在迭代蒸馏后生成动作偏离真实动作、误差累积和任务执行不稳定的问题。

Method: 采用选择性流对齐策略，利用专家演示选择性纠正生成动作，引入一致性校正机制。

Result: SeFA策略在模拟和真实操作任务中超越了基于扩散和流的现有策略，准确性和鲁棒性更好，推理延迟降低超98%。

Conclusion: SeFA结合整流流效率和与观察一致的动作生成，为实时视觉运动策略学习提供可扩展且可靠的解决方案。

Abstract: Developing efficient and accurate visuomotor policies poses a central challenge in robotic imitation learning. While recent rectified flow approaches have advanced visuomotor policy learning, they suffer from a key limitation: After iterative distillation, generated actions may deviate from the ground-truth actions corresponding to the current visual observation, leading to accumulated error as the reflow process repeats and unstable task execution. We present Selective Flow Alignment (SeFA), an efficient and accurate visuomotor policy learning framework. SeFA resolves this challenge by a selective flow alignment strategy, which leverages expert demonstrations to selectively correct generated actions and restore consistency with observations, while preserving multimodality. This design introduces a consistency correction mechanism that ensures generated actions remain observation-aligned without sacrificing the efficiency of one-step flow inference. Extensive experiments across both simulated and real-world manipulation tasks show that SeFA Policy surpasses state-of-the-art diffusion-based and flow-based policies, achieving superior accuracy and robustness while reducing inference latency by over 98%. By unifying rectified flow efficiency with observation-consistent action generation, SeFA provides a scalable and dependable solution for real-time visuomotor policy learning. Code is available on https://github.com/RongXueZoe/SeFA.

</details>


### [314] [PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision](https://arxiv.org/abs/2511.08098)
*Sabrina Patania,Luca Annese,Anita Pellegrini,Silvia Serino,Anna Lambiase,Luca Pallonetto,Silvia Rossi,Simone Colombani,Tom Foulsham,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.RO

TL;DR: 研究探讨用ReAct框架结合不同视角能否增强大语言模型理解其他智能体需求的能力，通过扩展Director任务实验，结果表明明确视角线索和主动探索策略可提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 当前训练范式常忽略多智能体交互语境，模型在处理个体视角主观性和多观察者环境时有挑战，需增强大语言模型视角采择能力。

Method: 使用ReAct框架明确纳入不同视角，扩展经典Director任务，设置七个视角采择复杂度递增的场景，在不同状态表示和提示策略下测试。

Result: 明确的视角线索与主动探索策略显著提高了模型的解释准确性和协作有效性。

Conclusion: 将主动感知与视角采择机制相结合，有助于推动大语言模型在机器人和多智能体系统中的应用，为自适应和上下文感知的人工智能系统研究奠定基础。

Abstract: Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [315] [A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain](https://arxiv.org/abs/2511.07577)
*Yining Lu,Wenyi Tang,Max Johnson,Taeho Jung,Meng Jiang*

Main category: cs.CR

TL;DR: 针对现有集中式检索增强生成（RAG）系统的问题，提出具有可靠性评分机制的去中心化RAG系统，在不同环境下评估显示性能提升、成本降低并开源。


<details>
  <summary>Details</summary>
Motivation: 现有集中式RAG系统存在数据收集、整合和管理成本高以及隐私问题，需去中心化系统，但去中心化面临数据源可靠性差异挑战。

Method: 提出新颖的可靠性评分机制动态评估数据源，通过基于区块链的智能合约安全管理评分过程。

Result: 在模拟环境中，系统在不可靠数据环境下比集中式系统性能提升10.7%，在理想可靠数据环境下接近集中式系统上限性能，通过批量更新操作实现约56%的边际成本节省。

Conclusion: 所提出的去中心化RAG系统有效解决了数据源可靠性问题，提高性能并降低成本，且实现了安全可信的评分管理。

Abstract: Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.

</details>


### [316] [Provable Repair of Deep Neural Network Defects by Preimage Synthesis and Property Refinement](https://arxiv.org/abs/2511.07741)
*Jianan Ma,Jingyi Wang,Qi Xuan,Zhen Wang*

Main category: cs.CR

TL;DR: 本文提出ProRepair框架，利用神经网络修复技术缓解安全威胁，在多个修复任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络在多种安全威胁下有危险行为，现有修复技术存在局限，需新方法应对实际场景。

Method: 提出基于形式原像合成和属性细化的ProRepair框架，包括合成精确代理框和执行属性细化。

Result: 在四个安全威胁修复任务和六个基准测试中，ProRepair在有效性、效率和可扩展性上优于现有方法，点修复有性能提升和加速，区域修复能处理更多实例和高维空间。

Conclusion: ProRepair是一种有效且可扩展的神经网络修复框架，能缓解多种安全威胁。

Abstract: It is known that deep neural networks may exhibit dangerous behaviors under various security threats (e.g., backdoor attacks, adversarial attacks and safety property violation) and there exists an ongoing arms race between attackers and defenders. In this work, we propose a complementary perspective to utilize recent progress on "neural network repair" to mitigate these security threats and repair various kinds of neural network defects (arising from different security threats) within a unified framework, offering a potential silver bullet solution to real-world scenarios. To substantially push the boundary of existing repair techniques (suffering from limitations such as lack of guarantees, limited scalability, considerable overhead, etc) in addressing more practical contexts, we propose ProRepair, a novel provable neural network repair framework driven by formal preimage synthesis and property refinement. The key intuitions are: (i) synthesizing a precise proxy box to characterize the feature space preimage, which can derive a bounded distance term sufficient to guide the subsequent repair step towards the correct outputs, and (ii) performing property refinement to enable surgical corrections and scale to more complex tasks. We evaluate ProRepair across four security threats repair tasks on six benchmarks and the results demonstrate it outperforms existing methods in effectiveness, efficiency and scalability. For point-wise repair, ProRepair corrects models while preserving performance and achieving significantly improved generalization, with a speedup of 5x to 2000x over existing provable approaches. In region-wise repair, ProRepair successfully repairs all 36 safety property violation instances (compared to 8 by the best existing method), and can handle 18x higher dimensional spaces.

</details>


### [317] [From LLMs to Agents: A Comparative Evaluation of LLMs and LLM-based Agents in Security Patch Detection](https://arxiv.org/abs/2511.08060)
*Junxiao Han,Zheng Yu,Lingfeng Bao,Jiakun Liu,Yao Wan,Jianwei Yin,Shuiguang Deng,Song Han*

Main category: cs.CR

TL;DR: 本文全面评估大语言模型及基于大语言模型的智能体在安全补丁检测中的性能，发现Data - Aug LLM整体性能最佳，ReAct Agent假阳性率最低，评估方法能降低假阳性率。


<details>
  <summary>Details</summary>
Motivation: 开源软件普及带来安全风险，大语言模型及智能体虽有潜力，但在安全补丁检测能力的系统评估有限，需填补此空白。

Method: 研究三种方法（Plain LLM、Data - Aug LLM、ReAct Agent），评估商业和开源大语言模型在这些方法下的表现，与现有基线对比，分析不同漏洞类型的检测性能、不同提示策略和上下文窗口大小的影响。

Result: Data - Aug LLM整体性能最佳，ReAct Agent假阳性率最低，基线方法准确率高但假阳性率显著更高，评估方法准确率相当且大幅降低假阳性率。

Conclusion: 研究为大语言模型及智能体在安全补丁检测中的实际应用提供有价值见解，凸显其在保持性能同时降低假阳性率的优势。

Abstract: The widespread adoption of open-source software (OSS) has accelerated software innovation but also increased security risks due to the rapid propagation of vulnerabilities and silent patch releases. In recent years, large language models (LLMs) and LLM-based agents have demonstrated remarkable capabilities in various software engineering (SE) tasks, enabling them to effectively address software security challenges such as vulnerability detection. However, systematic evaluation of the capabilities of LLMs and LLM-based agents in security patch detection remains limited. To bridge this gap, we conduct a comprehensive evaluation of the performance of LLMs and LLM-based agents for security patch detection. Specifically, we investigate three methods: Plain LLM (a single LLM with a system prompt), Data-Aug LLM (data augmentation based on the Plain LLM), and the ReAct Agent (leveraging the thought-action-observation mechanism). We also evaluate the performance of both commercial and open-source LLMs under these methods and compare these results with those of existing baselines. Furthermore, we analyze the detection performance of these methods across various vulnerability types, and examine the impact of different prompting strategies and context window sizes on the results. Our findings reveal that the Data-Aug LLM achieves the best overall performance, whereas the ReAct Agent demonstrates the lowest false positive rate (FPR). Although baseline methods exhibit strong accuracy, their false positive rates are significantly higher. In contrast, our evaluated methods achieve comparable accuracy while substantially reducing the FPR. These findings provide valuable insights into the practical applications of LLMs and LLM-based agents in security patch detection, highlighting their advantage in maintaining robust performance while minimizing false positive rates.

</details>


### [318] [Publish Your Threat Models! The benefits far outweigh the dangers](https://arxiv.org/abs/2511.08295)
*Loren Kohnfelder,Adam Shostack*

Main category: cs.CR

TL;DR: 探讨公共威胁模型（PTM）传递安全信息，提供编辑、审查和更新指导，呼吁技术社区共享PTM。


<details>
  <summary>Details</summary>
Motivation: 研究公共威胁模型如何向他人传达有用的安全信息。

Method: 列举早期采用者先例，解释好处，处理潜在异议，引用监管驱动因素，提供编辑、审查和更新指导。

Result: 指出内部威胁模型可能不适合直接披露，明确了相关处理方法。

Conclusion: 鼓励技术社区共享PTM，认为这应成为新规范。

Abstract: Threat modeling has long guided software development work, and we consider how Public Threat Models (PTM) can convey useful security information to others. We list some early adopter precedents, explain the many benefits, address potential objections, and cite regulatory drivers. Internal threat models may not be directly suitable for disclosure so we provide guidance for redaction and review, as well as when to update models (published or not). In a concluding call to action, we encourage the technology community to openly share their PTMs so the security properties of each component are known up and down the supply chain. Technology providers proud of their security efforts can show their work for competitive advantage, and customers can ask for and evaluate PTMs rather than be told "it's secure" but little more. Many great products already have fine threat models, and turning those into PTMs is a relatively minor task, so we argue this should (and easily could) become the new norm.

</details>


### [319] [Blockly2Hooks: Smart Contracts for Everyone with the XRP Ledger and Google Blockly](https://arxiv.org/abs/2511.08403)
*Lucian Trestioreanu,Wazen Shbair,Flaviene Scheidt de Cristo,Radu State*

Main category: cs.CR

TL;DR: 分布式账本技术发展带来多种应用，但智能合约普及受限，本文设计开发 Blockly2Hooks 平台助力非专家学习智能合约，结果有前景。


<details>
  <summary>Details</summary>
Motivation: 当前智能合约普及受安全、可用性和成本限制，且研究多关注安全，忽视可用性，缺乏让非专家创建智能合约的工具。

Method: 设计、开发并测试 Blockly2Hooks 平台，以 XRP 账本为案例，利用可视化编程语言等教学方法，特别是谷歌的 Blockly 库。

Result: 平台经过开发和测试，结果显示能让智能合约开发学习更顺畅。

Conclusion: Blockly2Hooks 平台可填补非专家学习智能合约的空白，即使在智能合约用高级语言编写的场景中也有效。

Abstract: Recent technologies such as inter-ledger payments, non-fungible tokens, and smart contracts are all fruited from the ongoing development of Distributed Ledger Technologies. The foreseen trend is that they will play an increasingly visible role in daily life, which will have to be backed by appropriate operational resources. For example, due to increasing demand, smart contracts could soon face a shortage of knowledgeable users and tools to handle them in practice. Widespread smart contract adoption is currently limited by security, usability and costs aspects. Because of a steep learning curve, the handling of smart contracts is currently performed by specialised developers mainly, and most of the research effort is focusing on smart contract security, while other aspects like usability being somewhat neglected. Specific tools would lower the entry barrier, enabling interested non-experts to create smart contracts.
  In this paper we designed, developed and tested Blockly2Hooks, a solution towards filling this gap even in challenging scenarios such as when the smart contracts are written in an advanced language like C. With the XRP Ledger as a concrete working case, Blockly2Hooks helps interested non-experts from the community to learn smart contracts easily and adopt the technology, through leveraging well-proven teaching methodologies like Visual Programming Languages, and more specifically, the Blockly Visual Programming library from Google. The platform was developed and tested and the results are promising to make learning smart contract development smoother.

</details>


### [320] [QLCoder: A Query Synthesizer For Static Analysis of Security Vulnerabilities](https://arxiv.org/abs/2511.08462)
*Claire Wang,Ziyang Li,Saikat Dutta,Mayur Naik*

Main category: cs.CR

TL;DR: 介绍QLCoder框架，可从CVE元数据自动合成CodeQL查询，评估显示比仅用Claude Code效果好。


<details>
  <summary>Details</summary>
Motivation: 解决编写静态分析工具查询需多样专业知识的难题。

Method: 将LLM嵌入带执行反馈的合成循环，用自定义MCP接口约束推理。

Result: 在176个CVEs上评估，QLCoder合成正确查询比例达53.4%，仅用Claude Code为10%。

Conclusion: QLCoder能生成语法和语义有效的安全查询，且效果优于仅用Claude Code。

Abstract: Static analysis tools provide a powerful means to detect security vulnerabilities by specifying queries that encode vulnerable code patterns. However, writing such queries is challenging and requires diverse expertise in security and program analysis. To address this challenge, we present QLCoder - an agentic framework that automatically synthesizes queries in CodeQL, a powerful static analysis engine, directly from a given CVE metadata. QLCode embeds an LLM in a synthesis loop with execution feedback, while constraining its reasoning using a custom MCP interface that allows structured interaction with a Language Server Protocol (for syntax guidance) and a RAG database (for semantic retrieval of queries and documentation). This approach allows QLCoder to generate syntactically and semantically valid security queries. We evaluate QLCode on 176 existing CVEs across 111 Java projects. Building upon the Claude Code agent framework, QLCoder synthesizes correct queries that detect the CVE in the vulnerable but not in the patched versions for 53.4% of CVEs. In comparison, using only Claude Code synthesizes 10% correct queries.

</details>


### [321] [AudAgent: Automated Auditing of Privacy Policy Compliance in AI Agents](https://arxiv.org/abs/2511.07441)
*Ye Zheng,Yidan Hu*

Main category: cs.CR

TL;DR: 本文介绍了用于实时监控AI代理数据实践并确保符合隐私政策的可视化框架AudAgent，评估显示其能有效实时识别潜在隐私政策违规行为。


<details>
  <summary>Details</summary>
Motivation: AI代理在未经用户明确同意下收集或披露敏感数据，且运行时行为与隐私政策的匹配情况缺乏透明度和问责性，需要解决方案。

Method: AudAgent包含四个组件：策略解析（用LLM将自然语言隐私政策转换为结构化模型）、运行时注释（基于Presidio检测敏感数据并注释使用情况）、合规审计（通过本体对齐和自动机评估进行实时合规检查）和用户界面（可视化执行轨迹和潜在隐私风险），还支持用户自定义策略。

Result: 在基于主流编程框架构建的AI代理上评估，AudAgent能有效实时识别潜在隐私政策违规行为。

Conclusion: AudAgent可实时监控AI代理数据实践，确保其符合隐私政策，还支持用户自定义，是解决AI代理隐私问题的有效方案。

Abstract: AI agents can autonomously perform tasks and, often without explicit user consent, collect or disclose users' sensitive local data, which raises serious privacy concerns. Although AI agents' privacy policies may describe their intended data practices, there remains limited transparency and accountability about whether runtime behavior matches those policies. To close this gap, we introduce AudAgent, a visual framework that continuously monitors AI agents' data practices in real time and guards compliance with stated privacy policies.
  AudAgent consists of four components for automated privacy auditing of AI agents. (i) Policy parsing: an ensemble of LLMs translates natural-language privacy policies into a structured privacy-policy model, where cross-LLM voting guarantees confidence of the parsing results. (ii) Runtime annotation: a lightweight Presidio-based analyzer detects sensitive data and annotates how the data is used based on the context of the AI agent's operations and the privacy-policy model. (iii) Compliance auditing: ontology alignment and automata-based evaluation connect the policy model with runtime annotations, enabling on-the-fly compliance checks between the natural-language policy and observed unordered data practices of AI agents. (iv) User interface: a platform-independent implementation visualizes the real-time execution trace of AI agents along with potential privacy risks detected during auditing, providing user-friendly transparency and accountability.
  In addition to common formatted privacy policies, AudAgent also supports user-defined policies for fine-grained control and customization. We evaluate AudAgent on AI agents built upon mainstream programming frameworks such as AutoGen, experiments show that AudAgent effectively identifies potential privacy policy violations in real time.

</details>


### [322] [KG-DF: A Black-box Defense Framework against Jailbreak Attacks Based on Knowledge Graphs](https://arxiv.org/abs/2511.07480)
*Shuyuan Liu,Jiawei Chen,Xiao Yang,Hang Su,Zhaoxia Yin*

Main category: cs.CR

TL;DR: 本文针对大语言模型越狱攻击问题，提出知识图谱防御框架KG - DF，通过扩展语义解析模块解决传统KG方法问题，实验表明该框架提升防御性能和问答质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用，越狱攻击威胁模型通用性和安全性，现有防御方法难平衡通用性与安全性。

Method: 提出知识图谱防御框架KG - DF，利用知识图谱结构化知识表示和语义关联能力；引入可扩展语义解析模块，将输入查询转换为结构化安全概念表示。

Result: 框架提升了对各种越狱攻击方法的防御性能，结合通用领域知识提高了大语言模型在一般问答场景中的响应质量。

Conclusion: KG - DF框架在防御大语言模型越狱攻击方面有效，且能提升模型问答表现。

Abstract: With the widespread application of large language models (LLMs) in various fields, the security challenges they face have become increasingly prominent, especially the issue of jailbreak. These attacks induce the model to generate erroneous or uncontrolled outputs through crafted inputs, threatening the generality and security of the model. Although existing defense methods have shown some effectiveness, they often struggle to strike a balance between model generality and security. Excessive defense may limit the normal use of the model, while insufficient defense may lead to security vulnerabilities. In response to this problem, we propose a Knowledge Graph Defense Framework (KG-DF). Specifically, because of its structured knowledge representation and semantic association capabilities, Knowledge Graph(KG) can be searched by associating input content with safe knowledge in the knowledge base, thus identifying potentially harmful intentions and providing safe reasoning paths. However, traditional KG methods encounter significant challenges in keyword extraction, particularly when confronted with diverse and evolving attack strategies. To address this issue, we introduce an extensible semantic parsing module, whose core task is to transform the input query into a set of structured and secure concept representations, thereby enhancing the relevance of the matching process. Experimental results show that our framework enhances defense performance against various jailbreak attack methods, while also improving the response quality of the LLM in general QA scenarios by incorporating domain-general knowledge.

</details>


### [323] [Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models](https://arxiv.org/abs/2511.07503)
*Asia Belfiore,Jonathan Passerat-Palmbach,Dmitrii Usynin*

Main category: cs.CR

TL;DR: 本文探索用语言模型生成合成遗传突变谱，用差分隐私保护数据，通过新攻击评估隐私保证，实验表明GPT类模型可行，混合攻击效果更好。


<details>
  <summary>Details</summary>
Motivation: 遗传数据可用性增加带来隐私问题，需保护敏感遗传数据。

Method: 利用语言模型生成合成遗传突变谱，使用差分隐私；引入Biologically - Informed Hybrid Membership Inference Attack评估隐私保证。

Result: 小型和大型Transformer GPT类模型可作为小规模基因组学的合成变异生成器；混合攻击比传统基于指标的成员推理攻击平均有更高的对抗成功率。

Conclusion: GPT类模型适用于小规模基因组学合成变异生成，新的混合攻击能更有效评估隐私保证。

Abstract: The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.

</details>


### [324] [FedRW: Efficient Privacy-Preserving Data Reweighting for Enhancing Federated Learning of Language Models](https://arxiv.org/abs/2511.07505)
*Pukang Ye,Junwei Luo,Xiaolei Dong,Yunbo Yang*

Main category: cs.CR

TL;DR: 提出FedRW框架解决大规模语料数据重复问题，在联邦大语言模型训练中通过样本重加权进行软去重，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模语料数据重复影响大语言模型性能和隐私，传统去重方法依赖可信第三方，有信息样本丢失和隐私风险。

Method: 提出FedRW框架，通过安全多方计算实现安全、频率感知的重加权协议，结合并行编排策略，训练中使用自适应重加权机制。

Result: FedRW在预处理上加速达28.78倍，困惑度提升约11.42%，且有更好的安全保障。

Conclusion: FedRW为联邦大语言模型训练中的重复数据管理建立了新范式。

Abstract: Data duplication within large-scale corpora often impedes large language models' (LLMs) performance and privacy. In privacy-concerned federated learning scenarios, conventional deduplication methods typically rely on trusted third parties to perform uniform deletion, risking loss of informative samples while introducing privacy vulnerabilities. To address these gaps, we propose Federated ReWeighting (FedRW), the first privacy-preserving framework, to the best of our knowledge, that performs soft deduplication via sample reweighting instead of deletion in federated LLM training, without assuming a trusted third party. At its core, FedRW proposes a secure, frequency-aware reweighting protocol through secure multi-party computation, coupled with a parallel orchestration strategy to ensure efficiency and scalability. During training, FedRW utilizes an adaptive reweighting mechanism with global sample frequencies to adjust individual loss contributions, effectively improving generalization and robustness. Empirical results demonstrate that FedRW outperforms the state-of-the-art method by achieving up to 28.78x speedup in preprocessing and approximately 11.42% improvement in perplexity, while offering enhanced security guarantees. FedRW thus establishes a new paradigm for managing duplication in federated LLM training.

</details>


### [325] [SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought](https://arxiv.org/abs/2511.07772)
*Shourya Batra,Pierce Tillman,Samarth Gaggar,Shashank Kesineni,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.CR

TL;DR: 提出SALT方法减少大语言模型推理过程隐私泄露，实验证明有效且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为个人助手处理敏感数据时，内部推理过程会泄露隐私，需平衡隐私和效用。

Method: 引入SALT，在测试时向隐藏状态注入目标导向向量，识别高泄露层。

Result: 在多个大语言模型实验中，SALT减少了上下文隐私泄露，如QwQ - 32B减少18.2% CPL等，且保持任务性能。

Conclusion: SALT是有推理能力语言模型测试时隐私保护的实用方法，有助于基于大语言模型的个人代理安全部署。

Abstract: As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\%$ reduction in CPL on QwQ-32B, $17.9\%$ reduction in CPL on Llama-3.1-8B, and $31.2\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.

</details>


### [326] [HybridGuard: Enhancing Minority-Class Intrusion Detection in Dew-Enabled Edge-of-Things Networks](https://arxiv.org/abs/2511.07793)
*Binayak Kara,Ujjwal Sahua,Ciza Thomas,Jyoti Prakash Sahoo*

Main category: cs.CR

TL;DR: 本文提出HybridGuard框架，结合机器学习和深度学习提升物联网边缘网络入侵检测能力，评估显示其性能强且优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 保障基于露珠计算的物联网边缘网络免受复杂入侵是关键挑战，需提升入侵检测能力。

Method: 通过基于互信息的特征选择处理数据不平衡，利用WCGAN - GP减少类别不平衡，采用DualNetShield两阶段架构进行流量分析和异常检测。

Result: 在UNSW - NB15、CIC - IDS - 2017和IOTID20数据集上评估，在多样攻击场景中表现出色，适应不断演变的网络安全威胁能力优于现有方案。

Conclusion: HybridGuard是保护物联网边缘网络免受现代入侵的有效工具。

Abstract: Securing Dew-Enabled Edge-of-Things (EoT) networks against sophisticated intrusions is a critical challenge. This paper presents HybridGuard, a framework that integrates machine learning and deep learning to improve intrusion detection. HybridGuard addresses data imbalance through mutual information based feature selection, ensuring that the most relevant features are used to improve detection performance, especially for minority attack classes. The framework leverages Wasserstein Conditional Generative Adversarial Networks with Gradient Penalty (WCGAN-GP) to further reduce class imbalance and enhance detection precision. It adopts a two-phase architecture called DualNetShield to support advanced traffic analysis and anomaly detection, improving the granular identification of threats in complex EoT environments. HybridGuard is evaluated on the UNSW-NB15, CIC-IDS-2017, and IOTID20 datasets, where it demonstrates strong performance across diverse attack scenarios and outperforms existing solutions in adapting to evolving cybersecurity threats. This approach establishes HybridGuard as an effective tool for protecting EoT networks against modern intrusions.

</details>


### [327] [PRISM: Privacy-preserving Inference System with Homomorphic Encryption and Modular Activation](https://arxiv.org/abs/2511.07807)
*Zeinab Elkhatib,Ali Sekmen,Kamrul Hasan*

Main category: cs.CR

TL;DR: 本文提出优化框架，用同态兼容近似替换CNN标准非线性函数，平衡了精度与隐私。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在关键基础设施部署时存在数据隐私问题，同态加密与CNN不兼容。

Method: 提出优化框架，替换标准非线性函数，重构CNN架构，引入高效激活函数近似方法。

Result: 在CIFAR - 10实验中，使用4次多项式和Softplus激活函数，在CKKS下，单样本加密耗时2.42秒，10000个样本加密耗时24000秒，准确率达94.4%。

Conclusion: 所提方法能在保证安全计算的同时，最小化计算开销，平衡精度和隐私。

Abstract: With the rapid advancements in machine learning, models have become increasingly capable of learning and making predictions in various industries. However, deploying these models in critical infrastructures presents a major challenge, as concerns about data privacy prevent unrestricted data sharing. Homomor- phic encryption (HE) offers a solution by enabling computations on encrypted data, but it remains incompatible with machine learning models like convolutional neural networks (CNNs), due to their reliance on non-linear activation functions. To bridge this gap, this work proposes an optimized framework that replaces standard non-linear functions with homomorphically compatible approximations, ensuring secure computations while minimizing computational overhead. The proposed approach restructures the CNN architecture and introduces an efficient activation function approximation method to mitigate the performance trade-offs in- troduced by encryption. Experiments on CIFAR-10 achieve 94.4% accuracy with 2.42 s per single encrypted sample and 24,000 s per 10,000 encrypted samples, using a degree-4 polynomial and Softplus activation under CKKS, balancing accuracy and privacy.

</details>


### [328] [Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks](https://arxiv.org/abs/2511.07947)
*Yaxin Xiao,Qingqing Ye,Zi Liang,Haoyang Li,RongHua Li,Huadi Zheng,Haibo Hu*

Main category: cs.CR

TL;DR: 本文指出当前黑盒水印在应对连续MEA和去除攻击方面不足，提出WRK攻击降低水印成功率，又提出CFW水印增强鲁棒性，实验显示CFW表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒水印在应对顺序MEA和去除攻击方面存在不足，低估了风险，需增强模型水印鲁棒性。

Method: 提出Watermark Removal attacK (WRK) 攻击绕过纠缠约束，提出Class-Feature Watermarks (CFW) 利用类级工件提高鲁棒性。

Result: WRK有效降低现有水印基准的水印成功率至少88.79%，CFW在多个领域实验中表现优于先前方法，在提取模型中水印成功率至少70.15%。

Conclusion: CFW能在MEA和WRK扭曲下保持较高水印成功率，同时保留受保护模型的实用性，增强了模型水印的鲁棒性。

Abstract: Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks.
  For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.

</details>


### [329] [LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation](https://arxiv.org/abs/2511.07876)
*Xingyu Li,Xiaolei Liu,Cheng Liu,Yixiao Xu,Kangyi Ding,Bangzhou Xin,Jia-Li Yin*

Main category: cs.CR

TL;DR: 提出LoopLLM框架进行大语言模型的能量 - 延迟攻击，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型能量 - 延迟攻击方法控制输出终止符号困难，效果不佳。

Method: 提出LoopLLM框架，包括重复诱导提示优化和令牌对齐集成优化。

Result: 在12个开源和2个商业大语言模型上实验，LoopLLM达到最大输出长度超90%，优于基线的20%，对DeepSeek - V3和Gemini 2.5 Flash的可迁移性提高约40%。

Conclusion: LoopLLM框架在大语言模型能量 - 延迟攻击方面显著优于现有方法。

Abstract: As large language models (LLMs) scale, their inference incurs substantial computational resources, exposing them to energy-latency attacks, where crafted prompts induce high energy and latency cost. Existing attack methods aim to prolong output by delaying the generation of termination symbols. However, as the output grows longer, controlling the termination symbols through input becomes difficult, making these methods less effective. Therefore, we propose LoopLLM, an energy-latency attack framework based on the observation that repetitive generation can trigger low-entropy decoding loops, reliably compelling LLMs to generate until their output limits. LoopLLM introduces (1) a repetition-inducing prompt optimization that exploits autoregressive vulnerabilities to induce repetitive generation, and (2) a token-aligned ensemble optimization that aggregates gradients to improve cross-model transferability. Extensive experiments on 12 open-source and 2 commercial LLMs show that LoopLLM significantly outperforms existing methods, achieving over 90% of the maximum output length, compared to 20% for baselines, and improving transferability by around 40% to DeepSeek-V3 and Gemini 2.5 Flash.

</details>


### [330] [FedPoP: Federated Learning Meets Proof of Participation](https://arxiv.org/abs/2511.08207)
*Devriş İşler,Elina van Kempen,Seoyeon Hwang,Nikolaos Laoutaris*

Main category: cs.CR

TL;DR: 本文提出FedPoP框架，可在保护隐私下实现联邦学习参与证明，经实验验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 随着模型成为可盈利数字资产，证明参与训练对确立所有权至关重要，现有方法存在不足。

Method: 引入FedPoP框架，与现有安全聚合协议集成，进行概念验证实现和实证评估。

Result: FedPoP每轮增加0.97秒开销，客户端证明参与只需0.0612秒。

Conclusion: FedPoP在不牺牲隐私下，对需要可审计参与的实际部署实用。

Abstract: Federated learning (FL) offers privacy preserving, distributed machine learning, allowing clients to contribute to a global model without revealing their local data. As models increasingly serve as monetizable digital assets, the ability to prove participation in their training becomes essential for establishing ownership. In this paper, we address this emerging need by introducing FedPoP, a novel FL framework that allows nonlinkable proof of participation while preserving client anonymity and privacy without requiring either extensive computations or a public ledger. FedPoP is designed to seamlessly integrate with existing secure aggregation protocols to ensure compatibility with real-world FL deployments. We provide a proof of concept implementation and an empirical evaluation under realistic client dropouts. In our prototype, FedPoP introduces 0.97 seconds of per-round overhead atop securely aggregated FL and enables a client to prove its participation/contribution to a model held by a third party in 0.0612 seconds. These results indicate FedPoP is practical for real-world deployments that require auditable participation without sacrificing privacy.

</details>


### [331] [Revisiting Network Traffic Analysis: Compatible network flows for ML models](https://arxiv.org/abs/2511.08345)
*João Vitorino,Daniela Pinto,Eva Maia,Ivone Amorim,Isabel Praça*

Main category: cs.CR

TL;DR: 本文研究不同网络流量流导出器创建的相似特征对机器学习模型泛化和鲁棒性的影响，处理数据集生成新特征，结果表明直接分析PCAP文件更优，并强调改进特征提取和选择的重要性。


<details>
  <summary>Details</summary>
Motivation: 确保机器学习模型能对网络攻击进行鲁棒检测和分类，解决准确表示攻击复杂流量模式困难的问题。

Method: 用HERA工具分析Bot - IoT、IoT - 23和CICIoT23数据集的PCAP文件原始网络数据包，生成新标记流并提取一致特征，与原始版本对比并微调多个模型。

Result: 直接分析和预处理PCAP文件能计算出更相关特征来训练集成学习模型。

Conclusion: 需继续改进特征提取和选择过程，使不同数据集更兼容，实现对网络安全解决方案中机器学习模型的可靠评估和比较。

Abstract: To ensure that Machine Learning (ML) models can perform a robust detection and classification of cyberattacks, it is essential to train them with high-quality datasets with relevant features. However, it can be difficult to accurately represent the complex traffic patterns of an attack, especially in Internet-of-Things (IoT) networks. This paper studies the impact that seemingly similar features created by different network traffic flow exporters can have on the generalization and robustness of ML models. In addition to the original CSV files of the Bot-IoT, IoT-23, and CICIoT23 datasets, the raw network packets of their PCAP files were analysed with the HERA tool, generating new labelled flows and extracting consistent features for new CSV versions. To assess the usefulness of these new flows for intrusion detection, they were compared with the original versions and were used to fine-tune multiple models. Overall, the results indicate that directly analysing and preprocessing PCAP files, instead of just using the commonly available CSV files, enables the computation of more relevant features to train bagging and gradient boosting decision tree ensembles. It is important to continue improving feature extraction and feature selection processes to make different datasets more compatible and enable a trustworthy evaluation and comparison of the ML models used in cybersecurity solutions.

</details>


### [332] [Toward Autonomous and Efficient Cybersecurity: A Multi-Objective AutoML-based Intrusion Detection System](https://arxiv.org/abs/2511.08491)
*Li Yang,Abdallah Shami*

Main category: cs.CR

TL;DR: 提出一种结合AutoML和MOO的创新入侵检测系统（IDS），在资源受限系统中平衡检测效果与计算效率，实验显示优于现有IDS。


<details>
  <summary>Details</summary>
Motivation: 网络安全威胁复杂、网络自动化需求增加，物联网系统扩张带来挑战，需要可扩展高效的安全解决方案。

Method: 提出的IDS框架集成OIP - AutoFS和OPCE - CASH技术，优化特征选择和模型学习过程，结合AutoML四阶段和多目标优化。

Result: 在两个基准网络安全数据集上实验，MOO - AutoML IDS表现优于现有IDS。

Conclusion: 该框架适用于资源受限的物联网和边缘环境，可用于多种自主网络安全应用。

Abstract: With increasingly sophisticated cybersecurity threats and rising demand for network automation, autonomous cybersecurity mechanisms are becoming critical for securing modern networks. The rapid expansion of Internet of Things (IoT) systems amplifies these challenges, as resource-constrained IoT devices demand scalable and efficient security solutions. In this work, an innovative Intrusion Detection System (IDS) utilizing Automated Machine Learning (AutoML) and Multi-Objective Optimization (MOO) is proposed for autonomous and optimized cyber-attack detection in modern networking environments. The proposed IDS framework integrates two primary innovative techniques: Optimized Importance and Percentage-based Automated Feature Selection (OIP-AutoFS) and Optimized Performance, Confidence, and Efficiency-based Combined Algorithm Selection and Hyperparameter Optimization (OPCE-CASH). These components optimize feature selection and model learning processes to strike a balance between intrusion detection effectiveness and computational efficiency. This work presents the first IDS framework that integrates all four AutoML stages and employs multi-objective optimization to jointly optimize detection effectiveness, efficiency, and confidence for deployment in resource-constrained systems. Experimental evaluations over two benchmark cybersecurity datasets demonstrate that the proposed MOO-AutoML IDS outperforms state-of-the-art IDSs, establishing a new benchmark for autonomous, efficient, and optimized security for networks. Designed to support IoT and edge environments with resource constraints, the proposed framework is applicable to a variety of autonomous cybersecurity applications across diverse networked environments.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [333] [Benchmarking Simulacra AI's Quantum Accurate Synthetic Data Generation for Chemical Sciences](https://arxiv.org/abs/2511.07433)
*Fabio Falcioni,Elena Orlova,Timothy Heightman,Philip Mantrov,Aleksei Ustimenko*

Main category: physics.chem-ph

TL;DR: 研究对比Simulacra和微软合成数据生成管道，发现Simulacra的LWM管道结合VMC算法降低数据生成成本，基于RELAX采样方案，可促进制药等行业发展。


<details>
  <summary>Details</summary>
Motivation: 对比Simulacra和微软的合成数据生成管道，评估性能以加速制药等行业AI驱动的优化和发现。

Method: 在不同规模系统数据集上对比两者管道，分析能量质量、自相关时间和有效样本量，采用RELAX采样方案。

Result: Simulacra的LWM管道结合VMC算法使数据生成成本降低15 - 50倍，能量精度相当，与传统CCSD方法相比降低2 - 3倍。

Conclusion: Simulacra的方案能创建经济的大规模从头算数据集，加速制药等行业的优化和发现。

Abstract: In this work, we benchmark \simulacra's synthetic data generation pipeline against a state-of-the-art Microsoft pipeline on a dataset of small to large systems. By analyzing the energy quality, autocorrelation times, and effective sample size, our findings show that Simulacra's Large Wavefunction Models (LWM) pipeline, paired with state-of-the-art Variational Monte Carlo (VMC) sampling algorithms, reduces data generation costs by 15-50x, while maintaining parity in energy accuracy, and 2-3x compared to traditional CCSD methods on the scale of amino acids. This enables the creation of affordable, large-scale \textit{ab-initio} datasets, accelerating AI-driven optimization and discovery in the pharmaceutical industry and beyond. Our improvements are based on a novel and proprietary sampling scheme called Replica Exchange with Langevin Adaptive eXploration (RELAX).

</details>


### [334] [Kolmogorov-Arnold Chemical Reaction Neural Networks for learning pressure-dependent kinetic rate laws](https://arxiv.org/abs/2511.07686)
*Benjamin C. Koenig,Sili Deng*

Main category: physics.chem-ph

TL;DR: 提出KA - CRNNs模型，可从数据中无假设推断压力效应，在CH3复合反应中表现优于传统模型，为复杂反应系统动力学行为发现奠定基础。


<details>
  <summary>Details</summary>
Motivation: 标准CRNNs不能表示压力依赖的速率行为，而该行为在许多燃烧和化学系统中至关重要。

Method: 使用Kolmogorov - Arnold激活将每个动力学参数建模为系统压力的可学习函数，开发KA - CRNNs模型。

Result: 在CH3复合反应的概念验证研究中，KA - CRNNs能准确再现不同温度和压力下的压力依赖动力学，优于传统插值模型。

Conclusion: 该框架为复杂反应系统中扩展动力学行为的数据驱动发现奠定基础，推动化学模型推断的可解释和物理一致方法发展。

Abstract: Chemical Reaction Neural Networks (CRNNs) have emerged as an interpretable machine learning framework for discovering reaction kinetics directly from data, while strictly adhering to the Arrhenius and mass action laws. However, standard CRNNs cannot represent pressure-dependent rate behavior, which is critical in many combustion and chemical systems and typically requires empirical formulations such as Troe or PLOG. Here, we develop Kolmogorov-Arnold Chemical Reaction Neural Networks (KA-CRNNs) that generalize CRNNs by modeling each kinetic parameter as a learnable function of system pressure using Kolmogorov-Arnold activations. This structure maintains full interpretability and physical consistency while enabling assumption-free inference of pressure effects directly from data. A proof-of-concept study on the CH3 recombination reaction demonstrates that KA-CRNNs accurately reproduce pressure-dependent kinetics across a range of temperatures and pressures, outperforming conventional interpolative models. The framework establishes a foundation for data-driven discovery of extended kinetic behaviors in complex reacting systems, advancing interpretable and physics-consistent approaches for chemical model inference.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [335] [Good flavor search in $SU(5)$: a machine learning approach](https://arxiv.org/abs/2511.08154)
*Fayez Abu-Ajamieh,Shinsuke Kawai,Nobuchika Okada*

Main category: hep-ph

TL;DR: 本文使用机器学习技术重新审视SU(5)大统一理论的费米子质量问题，分析两种修正方案的自然性，发现含24维场相互作用的模型更自然，引入参数y优化后发现y≈0.8对应最自然模型。


<details>
  <summary>Details</summary>
Motivation: 原Georgi - Glashow的SU(5)模型与观测到的费米子质量谱不兼容，需解决此差异并分析哪种修正方案更自然。

Method: 使用机器学习技术，定义自然性为与原Georgi - Glashow SU(5)模型的接近程度，引入连续参数y并进行数值优化。

Result: 在超对称和非超对称场景下，含24维场相互作用的模型更自然；数值优化显示y≈0.8最接近原SU(5)模型。

Conclusion: 根据定义，y≈0.8对应的模型是最自然的模型。

Abstract: We revisit the fermion mass problem of the $SU(5)$ grand unified theory using machine learning techniques. The original $SU(5)$ model proposed by Georgi and Glashow is incompatible with the observed fermion mass spectrum. Two remedies are known to resolve this discrepancy, one is through introducing a new interaction via a 45-dimensional field, and the other via a 24-dimensional field. We investigate which modification is more natural, defining naturalness as proximity to the original Georgi-Glashow $SU(5)$ model. Our analysis shows that, in both supersymmetric and non-supersymmetric scenarios, the model incorporating the interaction with the 24-dimensional field is more natural under this criterion. We then generalise these models by introducing a continuous parameter $y$, which takes the value 3 for the 45-dimensional field and 1.5 for the 24-dimensional field. Numerical optimisation reveals that $y \approx 0.8$ yields the closest match to the original $SU(5)$ model, indicating that this value corresponds to the most natural model according to our definition.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [336] [EvoPS: Evolutionary Patch Selection for Whole Slide Image Analysis in Computational Pathology](https://arxiv.org/abs/2511.07560)
*Saya Hashemian,Azam Asilian Bidgoli*

Main category: eess.IV

TL;DR: 提出EvoPS框架解决全切片图像（WSI）补丁选择问题，能减少训练所需补丁嵌入数量并保持或提升分类F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有WSI补丁选择方法存在计算成本高、未明确管理所选补丁数量与幻灯片表示准确性之间权衡的问题。

Method: 将补丁选择问题构建为多目标优化问题，利用进化搜索同时最小化所选补丁嵌入数量和最大化下游相似性搜索任务的性能。

Result: 在四个主要癌症队列中验证，EvoPS可减少超90%训练所需补丁嵌入数量，且能保持或提升最终分类F1分数。

Conclusion: EvoPS框架为创建高效、准确和可解释的WSI表示提供了可靠且有原则的方法，可让用户在计算成本和诊断性能间取得最佳平衡。

Abstract: In computational pathology, the gigapixel scale of Whole-Slide Images (WSIs) necessitates their division into thousands of smaller patches. Analyzing these high-dimensional patch embeddings is computationally expensive and risks diluting key diagnostic signals with many uninformative patches. Existing patch selection methods often rely on random sampling or simple clustering heuristics and typically fail to explicitly manage the crucial trade-off between the number of selected patches and the accuracy of the resulting slide representation. To address this gap, we propose EvoPS (Evolutionary Patch Selection), a novel framework that formulates patch selection as a multi-objective optimization problem and leverages an evolutionary search to simultaneously minimize the number of selected patch embeddings and maximize the performance of a downstream similarity search task, generating a Pareto front of optimal trade-off solutions. We validated our framework across four major cancer cohorts from The Cancer Genome Atlas (TCGA) using five pretrained deep learning models to generate patch embeddings, including both supervised CNNs and large self-supervised foundation models. The results demonstrate that EvoPS can reduce the required number of training patch embeddings by over 90% while consistently maintaining or even improving the final classification F1-score compared to a baseline that uses all available patches' embeddings selected through a standard extraction pipeline. The EvoPS framework provides a robust and principled method for creating efficient, accurate, and interpretable WSI representations, empowering users to select an optimal balance between computational cost and diagnostic performance.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [337] [Enabling Automatic Self-Talk Detection via Earables](https://arxiv.org/abs/2511.07493)
*Euihyeok Lee,Seonghyeon Kim,SangHun Im,Heung-Seon Oh,Seungwoo Kang*

Main category: cs.SD

TL;DR: 介绍了移动系统MutterMeter可自动检测现实场景中的发声式自言自语，实验显示其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 自言自语在日常生活中难以被察觉和测量，有必要开发能检测它的系统。

Method: 采用分层分类架构，通过顺序处理管道逐步整合声学、语言和上下文信息，平衡准确性和计算效率，使用25名参与者的31.1小时音频数据集构建和评估。

Result: MutterMeter的宏平均F1分数达到0.84，优于传统方法。

Conclusion: MutterMeter在检测发声式自言自语方面表现稳健，优于传统方法。

Abstract: Self-talk-an internal dialogue that can occur silently or be spoken aloud-plays a crucial role in emotional regulation, cognitive processing, and motivation, yet has remained largely invisible and unmeasurable in everyday life. In this paper, we present MutterMeter, a mobile system that automatically detects vocalized self-talk from audio captured by earable microphones in real-world settings. Detecting self-talk is technically challenging due to its diverse acoustic forms, semantic and grammatical incompleteness, and irregular occurrence patterns, which differ fundamentally from assumptions underlying conventional speech understanding models. To address these challenges, MutterMeter employs a hierarchical classification architecture that progressively integrates acoustic, linguistic, and contextual information through a sequential processing pipeline, adaptively balancing accuracy and computational efficiency. We build and evaluate MutterMeter using a first-of-its-kind dataset comprising 31.1 hours of audio collected from 25 participants. Experimental results demonstrate that MutterMeter achieves robust performance with a macro-averaged F1 score of 0.84, outperforming conventional approaches, including LLM-based and speech emotion recognition models.

</details>


### [338] [Speech Separation for Hearing-Impaired Children in the Classroom](https://arxiv.org/abs/2511.07677)
*Feyisayo Olalere,Kiki van der Heijden,H. Christiaan Stronks,Jeroen Briaire,Johan H. M. Frijns,Yagmur Güçlütürk*

Main category: cs.SD

TL;DR: 现有深度学习语音分离模型多基于成人语音在简单低混响条件下开发，本文用MIMO - TasNet模型模拟教室场景训练，发现教室特定训练能提升分离质量，微调可实现高效迁移学习，模型对嘈杂教室中儿童语音可及性有帮助。


<details>
  <summary>Details</summary>
Motivation: 现有多数深度学习语音分离模型用成人语音在简化低混响条件下开发，未考虑儿童语音特点和真实教室声学复杂性，难以适用于有听力障碍的儿童。

Method: 使用MIMO - TasNet模型，模拟自然教室场景，测试模型通过空间线索对儿童语音的适应能力，比较成人语音训练、教室数据训练及微调变体模型。

Result: 成人训练模型在干净场景表现好，教室特定训练大幅提升分离质量；用一半教室数据微调有类似提升；用扩散嘈杂噪声训练增强鲁棒性，模型保留空间感知且能泛化到未见距离。

Conclusion: 空间感知架构结合针对性适应可提高嘈杂教室中儿童的语音可及性，支持未来设备端辅助技术。

Abstract: Classroom environments are particularly challenging for children with hearing impairments, where background noise, multiple talkers, and reverberation degrade speech perception. These difficulties are greater for children than adults, yet most deep learning speech separation models for assistive devices are developed using adult voices in simplified, low-reverberation conditions. This overlooks both the higher spectral similarity of children's voices, which weakens separation cues, and the acoustic complexity of real classrooms. We address this gap using MIMO-TasNet, a compact, low-latency, multi-channel architecture suited for real-time deployment in bilateral hearing aids or cochlear implants. We simulated naturalistic classroom scenes with moving child-child and child-adult talker pairs under varying noise and distance conditions. Training strategies tested how well the model adapts to children's speech through spatial cues. Models trained on adult speech, classroom data, and finetuned variants were compared to assess data-efficient adaptation. Results show that adult-trained models perform well in clean scenes, but classroom-specific training greatly improves separation quality. Finetuning with only half the classroom data achieved comparable gains, confirming efficient transfer learning. Training with diffuse babble noise further enhanced robustness, and the model preserved spatial awareness while generalizing to unseen distances. These findings demonstrate that spatially aware architectures combined with targeted adaptation can improve speech accessibility for children in noisy classrooms, supporting future on-device assistive technologies.

</details>


### [339] [SpikCommander: A High-performance Spiking Transformer with Multi-view Learning for Efficient Speech Command Recognition](https://arxiv.org/abs/2511.07883)
*Jiaqi Wang,Liutao Yu,Xiongri Shen,Sihang Guo,Chenlin Zhou,Leilei Zhao,Yi Zhong,Zhengyu Ma,Zhiguo Zhang*

Main category: cs.SD

TL;DR: 本文提出用于语音命令识别的SpikCommander架构，在多数据集上优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于SNN的语音命令识别方法难以捕捉语音的丰富时间依赖和上下文信息。

Method: 引入MSTASA模块，结合有效尖峰时间感知注意力和多视图学习框架；提出SpikCommander架构，集成MSTASA和SCR - MLP。

Result: 在SHD、SSC和GSC三个基准数据集上，SpikCommander在可比时间步和更少参数下，始终优于SOTA的SNN方法。

Conclusion: SpikCommander对稳健的语音命令识别有效且高效。

Abstract: Spiking neural networks (SNNs) offer a promising path toward energy-efficient speech command recognition (SCR) by leveraging their event-driven processing paradigm. However, existing SNN-based SCR methods often struggle to capture rich temporal dependencies and contextual information from speech due to limited temporal modeling and binary spike-based representations. To address these challenges, we first introduce the multi-view spiking temporal-aware self-attention (MSTASA) module, which combines effective spiking temporal-aware attention with a multi-view learning framework to model complementary temporal dependencies in speech commands. Building on MSTASA, we further propose SpikCommander, a fully spike-driven transformer architecture that integrates MSTASA with a spiking contextual refinement channel MLP (SCR-MLP) to jointly enhance temporal context modeling and channel-wise feature integration. We evaluate our method on three benchmark datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC), and the Google Speech Commands V2 (GSC). Extensive experiments demonstrate that SpikCommander consistently outperforms state-of-the-art (SOTA) SNN approaches with fewer parameters under comparable time steps, highlighting its effectiveness and efficiency for robust speech command recognition.

</details>


### [340] [Speech Emotion Recognition with Phonation Excitation Information and Articulatory Kinematics](https://arxiv.org/abs/2511.07955)
*Ziqian Zhang,Min Huang,Zhongzhe Xiao*

Main category: cs.SD

TL;DR: 本文探讨语音产生时生理信息用于语音情感识别（SER）的潜力，引入数据集 STEM - E2VA，用估计的生理数据做实验，结果证实了其有效性和实用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少关注语音产生时的生理信息，而其包含说话者情感状态等特征，为填补这一空白开展研究。

Method: 开展系列实验，引入包含音频和生理数据的数据集 STEM - E2VA，用从语音中通过反演方法估计的生理数据进行情感识别。

Result: 实验结果证实了将语音产生的生理信息纳入 SER 的有效性。

Conclusion: 语音产生的生理信息用于 SER 具有有效性和在现实场景中的实用潜力。

Abstract: Speech emotion recognition (SER) has advanced significantly for the sake of deep-learning methods, while textual information further enhances its performance. However, few studies have focused on the physiological information during speech production, which also encompasses speaker traits, including emotional states. To bridge this gap, we conducted a series of experiments to investigate the potential of the phonation excitation information and articulatory kinematics for SER. Due to the scarcity of training data for this purpose, we introduce a portrayed emotional dataset, STEM-E2VA, which includes audio and physiological data such as electroglottography (EGG) and electromagnetic articulography (EMA). EGG and EMA provide information of phonation excitation and articulatory kinematics, respectively. Additionally, we performed emotion recognition using estimated physiological data derived through inversion methods from speech, instead of collected EGG and EMA, to explore the feasibility of applying such physiological information in real-world SER. Experimental results confirm the effectiveness of incorporating physiological information about speech production for SER and demonstrate its potential for practical use in real-world scenarios.

</details>


### [341] [Uncertainty Calibration of Multi-Label Bird Sound Classifiers](https://arxiv.org/abs/2511.08261)
*Raphael Schwinger,Ben McEwen,Vincent S. Kather,René Heinrich,Lukas Rauch,Sven Tomforde*

Main category: cs.SD

TL;DR: 研究生物声学中多标签深度学习分类器校准，在BirdSet基准上评估四个分类器，发现校准因数据集和类别而异，可用简单方法改善校准。


<details>
  <summary>Details</summary>
Motivation: 生物声学中可靠分类需高精度和校准良好的不确定性估计，但多标签深度学习分类器校准未被评估。

Method: 在BirdSet基准上系统评估四个多标签鸟类声音分类器，使用无阈值校准指标和判别指标。

Result: 模型校准因数据集和类别差异大，部分模型存在自信不足或过度自信，低频类别校准更好，简单事后校准方法可改善。

Conclusion: 强调评估和改善生物声学分类器不确定性校准的重要性。

Abstract: Passive acoustic monitoring enables large-scale biodiversity assessment, but reliable classification of bioacoustic sounds requires not only high accuracy but also well-calibrated uncertainty estimates to ground decision-making. In bioacoustics, calibration is challenged by overlapping vocalisations, long-tailed species distributions, and distribution shifts between training and deployment data. The calibration of multi-label deep learning classifiers within the domain of bioacoustics has not yet been assessed. We systematically benchmark the calibration of four state-of-the-art multi-label bird sound classifiers on the BirdSet benchmark, evaluating both global, per-dataset and per-class calibration using threshold-free calibration metrics (ECE, MCS) alongside discrimination metrics (cmAP). Model calibration varies significantly across datasets and classes. While Perch v2 and ConvNeXt$_{BS}$ show better global calibration, results vary between datasets. Both models indicate consistent underconfidence, while AudioProtoPNet and BirdMAE are mostly overconfident. Surprisingly, calibration seems to be better for less frequent classes. Using simple post hoc calibration methods we demonstrate a straightforward way to improve calibration. A small labelled calibration set is sufficient to significantly improve calibration with Platt scaling, while global calibration parameters suffer from dataset variability. Our findings highlight the importance of evaluating and improving uncertainty calibration in bioacoustic classifiers.

</details>


### [342] [SpeechJudge: Towards Human-Level Judgment for Speech Naturalness](https://arxiv.org/abs/2511.07931)
*Xueyao Zhang,Chaoren Wang,Huan Liao,Ziniu Li,Yuancheng Wang,Li Wang,Dongya Jia,Yuanzhe Chen,Xiulin Li,Zhuo Chen,Zhizheng Wu*

Main category: cs.SD

TL;DR: 本文引入了包含数据集、基准和奖励模型的SpeechJudge套件，用于解决语音合成中模型与人类偏好对齐的问题，所提奖励模型在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 语音合成中缺乏大规模人类偏好数据集，阻碍模型与人类感知对齐。

Method: 构建99K语音对的SpeechJudge - Data数据集，建立SpeechJudge - Eval基准，基于Qwen2.5 - Omni - 7B开发SpeechJudge - GRM奖励模型，采用两阶段训练。

Result: 现有指标和AudioLLMs在基准测试中表现不佳，SpeechJudge - GRM在基准测试中准确率达77.2%（推理时缩放后79.4%），优于经典模型。

Conclusion: SpeechJudge套件可有效解决语音合成中模型与人类偏好对齐问题，奖励模型可用于语音生成模型后训练。

Abstract: Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce SpeechJudge, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on naturalness--one of the most fundamental subjective metrics for speech synthesis. First, we present SpeechJudge-Data, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish SpeechJudge-Eval, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop SpeechJudge-GRM, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.

</details>


### [343] [DOA Estimation with Lightweight Network on LLM-Aided Simulated Acoustic Scenes](https://arxiv.org/abs/2511.08012)
*Haowen Li,Zhengding Luo,Dongyuan Shi,Boxiang Wang,Junwei Ji,Ziyi Yang,Woon-Seng Gan*

Main category: cs.SD

TL;DR: 本文利用基于大语言模型构建的数据集研究DOA估计，提出轻量级模型LightDOA，实验证明其有效且适用于资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 现有DOA模型多在合成数据上训练，声学多样性受限，泛化性不足。

Method: 在新数据集上对几种代表性的基于神经网络的DOA方法进行基准测试，并提出基于深度可分离卷积的轻量级DOA估计模型LightDOA。

Result: LightDOA在不同声学场景中实现了令人满意的准确性和鲁棒性，同时保持低计算复杂度。

Conclusion: 基于大语言模型合成的空间音频数据有助于推进鲁棒高效的DOA估计研究，LightDOA是资源受限应用的有效解决方案。

Abstract: Direction-of-Arrival (DOA) estimation is critical in spatial audio and acoustic signal processing, with wide-ranging applications in real-world. Most existing DOA models are trained on synthetic data by convolving clean speech with room impulse responses (RIRs), which limits their generalizability due to constrained acoustic diversity. In this paper, we revisit DOA estimation using a recently introduced dataset constructed with the assistance of large language models (LLMs), which provides more realistic and diverse spatial audio scenes. We benchmark several representative neural-based DOA methods on this dataset and propose LightDOA, a lightweight DOA estimation model based on depthwise separable convolutions, specifically designed for mutil-channel input in varying environments. Experimental results show that LightDOA achieves satisfactory accuracy and robustness across various acoustic scenes while maintaining low computational complexity. This study not only highlights the potential of spatial audio synthesized with the assistance of LLMs in advancing robust and efficient DOA estimation research, but also highlights LightDOA as efficient solution for resource-constrained applications.

</details>


### [344] [HQ-SVC: Towards High-Quality Zero-Shot Singing Voice Conversion in Low-Resource Scenarios](https://arxiv.org/abs/2511.08496)
*Bingsong Bai,Yizhong Geng,Fengping Wang,Cong Wang,Puyuan Guo,Yingming Gao,Ya Li*

Main category: cs.SD

TL;DR: 提出高效框架HQ - SVC用于高质量零样本歌唱语音转换，在转换质量、效率和语音自然度上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有零样本歌唱语音转换方法分别建模说话人音色和语音内容，丢失重要声学信息、降低输出质量且需大量计算资源。

Method: HQ - SVC使用解耦编解码器联合提取内容和说话人特征，通过音高和音量建模提高保真度，利用可微信号处理和扩散技术逐步优化输出。

Result: HQ - SVC在转换质量和效率上显著优于现有零样本歌唱语音转换方法，在语音自然度上优于专业音频超分辨率方法，且支持语音超分辨率任务。

Conclusion: HQ - SVC是一种有效的高质量零样本歌唱语音转换框架。

Abstract: Zero-shot singing voice conversion (SVC) transforms a source singer's timbre to an unseen target speaker's voice while preserving melodic content without fine-tuning. Existing methods model speaker timbre and vocal content separately, losing essential acoustic information that degrades output quality while requiring significant computational resources. To overcome these limitations, we propose HQ-SVC, an efficient framework for high-quality zero-shot SVC. HQ-SVC first extracts jointly content and speaker features using a decoupled codec. It then enhances fidelity through pitch and volume modeling, preserving critical acoustic information typically lost in separate modeling approaches, and progressively refines outputs via differentiable signal processing and diffusion techniques. Evaluations confirm HQ-SVC significantly outperforms state-of-the-art zero-shot SVC methods in conversion quality and efficiency. Beyond voice conversion, HQ-SVC achieves superior voice naturalness compared to specialized audio super-resolution methods while natively supporting voice super-resolution tasks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [345] [Dynamic Stability of LLM-Generated Code](https://arxiv.org/abs/2511.07463)
*Prateek Rajput,Abdoul Aziz Bonkoungou,Yewei Song,Abdoul Kader Kabore,Iyiola E. Olatunji,Jacques Klein,Tegewende Bissyande*

Main category: cs.PL

TL;DR: 当前大语言模型代码生成评估重功能正确性，忽视算法复杂度差异，本文引入评估框架和指标，实证发现模型输出有算法差异及正确性与稳定性权衡，呼吁考虑稳定性目标和新基准。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代码生成评估方法仅关注功能正确性，忽略了正确解决方案间算法复杂度和性能的差异，需要改进评估方法。

Method: 引入评估生成代码动态稳定性的框架，提出基于操作码分布的SCTD和DCTD指标，以及BEF指标作为诊断信号。

Result: 在BigOBench和CodeContests上实证表明，即使功能正确输出，模型也存在显著算法差异；提高采样温度会提升pass@1率但降低稳定性。

Conclusion: 代码生成需考虑稳定性目标，且需要有渐近测试用例的新基准进行更可靠的评估。

Abstract: Current evaluations of LLMs for code generation emphasize functional correctness, overlooking the fact that functionally correct solutions can differ significantly in algorithmic complexity. For instance, an $(O(n^2))$ versus $(O(n \log n))$ sorting algorithm may yield similar output but incur vastly different performance costs in production. This discrepancy reveals a critical limitation in current evaluation methods: they fail to capture the behavioral and performance diversity among correct solutions. To address this, we introduce a principled framework for evaluating the dynamic stability of generated code. We propose two metrics derived from opcode distributions: Static Canonical Trace Divergence (SCTD), which captures algorithmic structure diversity across generated solutions, and Dynamic Canonical Trace Divergence (DCTD), which quantifies runtime behavioral variance. Their ratio, the Behavioral Expression Factor (BEF), serves as a diagnostic signal: it indicates critical runtime instability when BEF $\ll$ 1 and functional redundancy when BEF $\gg$ 1. Empirical results on BigOBench and CodeContests show that state-of-the-art LLMs exhibit significant algorithmic variance even among functionally correct outputs. Notably, increasing sampling temperature improves pass@1 rates but degrades stability, revealing an unrecognized trade-off: searching for correct solutions in diverse output spaces introduces a "penalty of instability" between correctness and behavioral consistency. Our findings call for stability-aware objectives in code generation and new benchmarks with asymptotic test cases for robust, real-world LLM evaluation.

</details>


### [346] [Streaming Tensor Program: A streaming abstraction for dynamic parallelism](https://arxiv.org/abs/2511.07776)
*Gina Sohn,Genghan Zhang,Konstantin Hossfeld,Jungwoo Kim,Nathan Sobotka,Nathan Zhang,Olivia Hsu,Kunle Olukotun*

Main category: cs.PL

TL;DR: 提出Streaming Tensor Program (STeP) 抽象，使动态张量工作负载能在空间数据流加速器上高效运行，并展示其优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有空间数据流加速器编程抽象表达能力有限，难以高效处理动态张量应用中的动态行为。

Method: 提出STeP，引入灵活路由算子、显式内存层次和符号形状语义。

Result: 在代表LLM层的模拟实验中，动态平铺减少2.18倍片上内存需求，动态并行化提升1.5倍延迟，配置时分复用提升2.57倍计算利用率。

Conclusion: STeP能有效应对动态张量工作负载，在保持数据流效率的同时适应动态行为进行优化。

Abstract: Dynamic behaviors are becoming prevalent in many tensor applications. In machine learning, for example, the input tensors are dynamically shaped or ragged, and data-dependent control flow is widely used in many models. However, the limited expressiveness of prior programming abstractions for spatial dataflow accelerators forces the dynamic behaviors to be implemented statically or lacks the visibility for performance-critical decisions. To address these challenges, we present the Streaming Tensor Program (STeP), a new streaming abstraction that enables dynamic tensor workloads to run efficiently on spatial dataflow accelerators. STeP introduces flexible routing operators, an explicit memory hierarchy, and symbolic shape semantics that expose dynamic data rates and tensor dimensions. These capabilities unlock new optimizations-dynamic tiling, dynamic parallelization, and configuration time-multiplexing-that adapt to dynamic behaviors while preserving dataflow efficiency. Using a cycle-approximate simulator on representative LLM layers with real-world traces, dynamic tiling reduces on-chip memory requirement by 2.18x, dynamic parallelization improves latency by 1.5x, and configuration time-multiplexing improves compute utilization by 2.57x over implementations available in prior abstractions.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [347] [Unifying Model and Layer Fusion for Speech Foundation Models](https://arxiv.org/abs/2511.08389)
*Yi-Jen Shih,David Harwath*

Main category: eess.AS

TL;DR: 提出接口模块统一两种融合策略，实验表明该方法优于先前融合方法，合适模型选择能提升性能。


<details>
  <summary>Details</summary>
Motivation: 统一先前单模型多层表示融合和多模型融合两种策略以提升语音基础模型下游任务性能。

Method: 提出接口模块，实现跨多个上游语音模型融合，并整合各层信息。

Result: 在不同语音任务上实验显示该方法优于先前融合方法，合适模型选择可带来额外性能提升。

Conclusion: 提出的接口模块是利用语音基础模型有前景的方法。

Abstract: Speech Foundation Models have gained significant attention recently. Prior works have shown that the fusion of representations from multiple layers of the same model or the fusion of multiple models can improve performance on downstream tasks. We unify these two fusion strategies by proposing an interface module that enables fusion across multiple upstream speech models while integrating information across their layers. We conduct extensive experiments on different self-supervised and supervised models across various speech tasks, including ASR and paralinguistic analysis, and demonstrate that our method outperforms prior fusion approaches. We further analyze its scalability concerning model size and count, highlighting the importance of selecting appropriate upstream models. Our results show that the proposed interface provides an additional performance boost when given a suitable upstream model selection, making it a promising approach for utilizing Speech Foundation Models.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [348] [Galactification: painting galaxies onto dark matter only simulations using a transformer-based model](https://arxiv.org/abs/2511.08438)
*Shivam Pandey,Christopher C. Lovell,Chirag Modi,Benjamin D. Wandelt*

Main category: astro-ph.CO

TL;DR: 开发框架基于暗物质模拟快速生成星系模拟目录，模型能重现星系统计数据。


<details>
  <summary>Details</summary>
Motivation: 连接星系形成演化与大尺度结构对解释宇宙学观测至关重要，水动力学模拟计算量大，难以覆盖现代调查的体积。

Method: 开发基于暗物质模拟快速生成模拟星系目录的框架，采用多模态、基于Transformer的模型，输入3D暗物质密度和速度场，输出星系点云及其物理性质。

Result: 训练后的模型能忠实地重现各种星系统计数据，正确捕捉其随宇宙学和天体物理学参数变化的情况。

Conclusion: 该模型是首个能捕捉水动力学模拟中所有相关星系属性、完整空间分布和条件依赖关系的加速正向模型。

Abstract: Connecting the formation and evolution of galaxies to the large-scale structure is crucial for interpreting cosmological observations. While hydrodynamical simulations accurately model the correlated properties of galaxies, they are computationally prohibitive to run over volumes that match modern surveys. We address this by developing a framework to rapidly generate mock galaxy catalogs conditioned on inexpensive dark-matter-only simulations. We present a multi-modal, transformer-based model that takes 3D dark matter density and velocity fields as input, and outputs a corresponding point cloud of galaxies with their physical properties. We demonstrate that our trained model faithfully reproduces a variety of galaxy summary statistics and correctly captures their variation with changes in the underlying cosmological and astrophysical parameters, making it the first accelerated forward model to capture all the relevant galaxy properties, their full spatial distribution, and their conditional dependencies in hydrosimulations.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [349] [CNN-Based Automated Parameter Extraction Framework for Modeling Memristive Devices](https://arxiv.org/abs/2511.07926)
*Akif Hamid,Orchi Hassan*

Main category: cs.ET

TL;DR: 本文提出自动化框架从RRAM设备I - V特性中提取参数，经评估误差低，为RRAM建模提供快速可靠方案。


<details>
  <summary>Details</summary>
Motivation: 现有RRAM紧凑模型提取参数需大量手动调整，耗时长且适应性差，需改进。

Method: 采用在合成数据集上训练的卷积神经网络生成初始参数估计，再通过三个启发式优化块在参数空间进行自适应二分搜索来优化参数。

Result: 使用四个关键NVM指标评估，与其他模型和实验数据对比，该框架在不同设备特性上误差低。

Conclusion: 该框架为RRAM建模提供了快速、可靠且强大的解决方案。

Abstract: Resistive random access memory (RRAM) is a promising candidate for next-generation nonvolatile memory (NVM) and in-memory computing applications. Compact models are essential for analyzing the circuit and system-level performance of experimental RRAM devices. However, most existing RRAM compact models rely on multiple fitting parameters to reproduce the device I-V characteristics, and in most cases, as the parameters are not directly related to measurable quantities, their extraction requires extensive manual tuning, making the process time-consuming and limiting adaptability across different devices. This work presents an automated framework for extracting the fitting parameters of the widely used Stanford RRAM model directly from the device I-V characteristics. The framework employs a convolutional neural network (CNN) trained on a synthetic dataset to generate initial parameter estimates, which are then refined through three heuristic optimization blocks that minimize errors via adaptive binary search in the parameter space. We evaluated the framework using four key NVM metrics: set voltage, reset voltage, hysteresis loop area, and low resistance state (LRS) slope. Benchmarking against RRAM device characteristics derived from previously reported Stanford model fits, other analytical models, and experimental data shows that the framework achieves low error across diverse device characteristics, offering a fast, reliable, and robust solution for RRAM modeling.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [350] [Hyperellipsoid Density Sampling: Exploitative Sequences to Accelerate High-Dimensional Optimization](https://arxiv.org/abs/2511.07836)
*Julian Soltes*

Main category: math.NA

TL;DR: 为解决高维优化中维数灾难问题，提出超椭球密度采样（HDS）方法，与标准QMC方法Sobol对比，结果显示HDS有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统算法在高维优化问题中因维数灾难效率低下，需要新方法加速优化。

Method: 提出HDS方法，通过在搜索空间定义多个超椭球生成序列，用三种无监督学习算法避免高维几何计算，还可使用高斯权重影响样本分布。

Result: 与Sobol对比，在29个CEC2017基准测试函数上，解的几何平均误差有显著改善（p < 0.05），平均性能提升从30维的3%到10维的37%。

Conclusion: HDS是高维优化中QMC采样的有效替代方法。

Abstract: The curse of dimensionality presents a pervasive challenge in optimization problems, with exponential expansion of the search space rapidly causing traditional algorithms to become inefficient or infeasible. An adaptive sampling strategy is presented to accelerate optimization in this domain as an alternative to uniform quasi-Monte Carlo (QMC) methods.
  This method, referred to as Hyperellipsoid Density Sampling (HDS), generates its sequences by defining multiple hyperellipsoids throughout the search space. HDS uses three types of unsupervised learning algorithms to circumvent high-dimensional geometric calculations, producing an intelligent, non-uniform sample sequence that exploits statistically promising regions of the parameter space and improves final solution quality in high-dimensional optimization problems.
  A key feature of the method is optional Gaussian weights, which may be provided to influence the sample distribution towards known locations of interest. This capability makes HDS versatile for applications beyond optimization, providing a focused, denser sample distribution where models need to concentrate their efforts on specific, non-uniform regions of the parameter space.
  The method was evaluated against Sobol, a standard QMC method, using differential evolution (DE) on the 29 CEC2017 benchmark test functions. The results show statistically significant improvements in solution geometric mean error (p < 0.05), with average performance gains ranging from 3% in 30D to 37% in 10D. This paper demonstrates the efficacy of HDS as a robust alternative to QMC sampling for high-dimensional optimization.

</details>


### [351] [A New Initial Approximation Bound in the Durand Kerner Algorithm for Finding Polynomial Zeros](https://arxiv.org/abs/2511.07728)
*B. A. Sanjoyo,M. Yunus,N. Hidayat*

Main category: math.NA

TL;DR: 本文介绍两种确定Durand - Kerner算法初始值的新方法，经理论分析和实验评估，lambda最大界收敛更快更稳定，新界1能保证收敛但半径过大。


<details>
  <summary>Details</summary>
Motivation: Durand - Kerner算法收敛性依赖初始近似值选择，为提高算法稳定性和收敛速度。

Method: 引入New bound 1和lambda maximal bound两种确定初始值的新方法，并进行理论分析和数值实验。

Result: lambda最大界能确保所有根在复圆内，收敛更快更稳定；新界1能保证收敛，但半径过大。

Conclusion: lambda最大界在提高Durand - Kerner算法稳定性和收敛速度上更具优势。

Abstract: The Durand-Kerner algorithm is a widely used iterative technique for simultaneously finding all the roots of a polynomial. However, its convergence heavily depends on the choice of initial approximations. This paper introduces two novel approaches for determining the initial values: New bound 1 and the lambda maximal bound, aimed at improving the stability and convergence speed of the algorithm. Theoretical analysis and numerical experiments were conducted to evaluate the effectiveness of these bounds. The lambda maximal bound consistently ensures that all the roots lie within the complex circle, leading to faster and more stable convergence. Comparative results demonstrate that while New bound 1 guarantees convergence, but it yields excessively large radii.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [352] [From Double to Triple Burden: Gender Stratification in the Latin American Data Annotation Gig Economy](https://arxiv.org/abs/2511.07652)
*Lauren Benjamin Mushro*

Main category: cs.CY

TL;DR: 研究拉美数据标注零工经济中的性别分层，指出女性面临三重负担，建议将标注工作视为技能劳动并进行监管干预。


<details>
  <summary>Details</summary>
Motivation: 探究拉美数据标注零工经济中的性别分层问题，关注女性面临的‘三重负担’。

Method: 对30名拉美数据标注员进行探索性调查，结合定性描述和比较二手文献。

Result: 女性因照顾义务和政治经济不稳定更多参与标注工作，面临低薪、任务获取不稳定和缺乏福利等问题，对工作价值有矛盾态度。

Conclusion: 应将标注视为技能劳动，进行监管干预以解决平台责任、工资压制和地区不平等问题。

Abstract: This paper examines gender stratification in the Latin American data annotation gig economy, with a particular focus on the "triple burden" shouldered by women: unpaid care responsibilities, economic precarity, and the volatility of platform-mediated labor. Data annotation, once lauded as a democratizing force within the global gig economy, has evolved into a segmented labor market characterized by low wages, limited protections, and unequal access to higher-skilled annotation tasks. Drawing on an exploratory survey of 30 Latin American data annotators, supplemented by qualitative accounts and comparative secondary literature, this study situates female annotators within broader debates in labor economics, including segmentation theory, monopsony power in platform labor, and the reserve army of labor. Findings indicate that women are disproportionately drawn into annotation due to caregiving obligations and political-economic instability in countries such as Venezuela, Colombia, and Peru. Respondents highlight low pay, irregular access to tasks, and lack of benefits as central challenges, while also expressing ambivalence about whether their work is valued relative to male counterparts. By framing annotation as both a gendered survival strategy and a critical input in the global artificial intelligence supply chain, this paper argues for the recognition of annotation as skilled labor and for regulatory interventions that address platform accountability, wage suppression, and regional inequalities.

</details>


### [353] [Exploring the Psychometric Validity of AI-Generated Student Responses: A Study on Virtual Personas' Learning Motivation](https://arxiv.org/abs/2511.07451)
*Huanxiao Wang*

Main category: cs.CY

TL;DR: 研究探索大语言模型能否模拟有效学生作答用于教育测量，用GPT - 4o生成2000个虚拟学生完成量表，分析表明其能重现量表结构和动机子组。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否可以模拟有效学生作答以用于教育测量。

Method: 使用GPT - 4o生成2000个虚拟学生，让每个学生完成学术动机量表（AMS），并进行因子分析（EFA和CFA）和聚类。

Result: GPT - 4o能够重现AMS量表结构和不同的动机子组。

Conclusion: 大语言模型（如GPT - 4o）具备模拟有效学生作答用于教育测量的潜力。

Abstract: This study explores whether large language models (LLMs) can simulate valid student responses for educational measurement. Using GPT -4o, 2000 virtual student personas were generated. Each persona completed the Academic Motivation Scale (AMS). Factor analyses(EFA and CFA) and clustering showed GPT -4o reproduced the AMS structure and distinct motivational subgroups.

</details>


### [354] [The Polite Liar: Epistemic Pathology in Language Models](https://arxiv.org/abs/2511.07477)
*Bentley DeVilling*

Main category: cs.CY

TL;DR: 论文指出大语言模型存在‘礼貌说谎者’现象，是RLHF的结构后果，揭示语言合作与认知完整性的对齐张力，提出‘认知对齐’原则。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型自信编造内容这一认知病理现象的成因。

Method: 基于Frankfurt对废话的分析，从认知美德理论、言语行为哲学和认知对齐等角度分析。

Result: 发现该现象是结构上对真相的冷漠，系统学习最大化用户满意度而非追求真相。

Conclusion: 提出‘认知对齐’原则，奖励有根据的自信而非表面的流畅性。

Abstract: Large language models exhibit a peculiar epistemic pathology: they speak as if they know, even when they do not. This paper argues that such confident fabrication, what I call the polite liar, is a structural consequence of reinforcement learning from human feedback (RLHF). Building on Frankfurt's analysis of bullshit as communicative indifference to truth, I show that this pathology is not deception but structural indifference: a reward architecture that optimizes for perceived sincerity over evidential accuracy. Current alignment methods reward models for being helpful, harmless, and polite, but not for being epistemically grounded. As a result, systems learn to maximize user satisfaction rather than truth, performing conversational fluency as a virtue. I analyze this behavior through the lenses of epistemic virtue theory, speech-act philosophy, and cognitive alignment, showing that RLHF produces agents trained to mimic epistemic confidence without access to epistemic justification. The polite liar thus reveals a deeper alignment tension between linguistic cooperation and epistemic integrity. The paper concludes with an "epistemic alignment" principle: reward justified confidence over perceived fluency.

</details>


### [355] [From Hubs to Deserts: Urban Cultural Accessibility Patterns with Explainable AI](https://arxiv.org/abs/2511.07475)
*Protik Bose Pranto,Minhazul Islam,Ripon Kumar Saha,Abimelec Mercado Rivera,Namig Abbasov*

Main category: cs.CY

TL;DR: 提出衡量文化设施获取空间公平性框架，分析文化设施分布与社会人口指标关联，揭示分布特征。


<details>
  <summary>Details</summary>
Motivation: 文化基础设施对多方面有益，但城市间获取机会不均，需衡量空间公平性。

Method: 绘制文化基础设施地图，计算CIAS，结合社会人口指标，用带SHAP的树集成模型解释关联。

Result: 呈现明显核心 - 边缘梯度，非图书馆文化设施集中在城市核心，图书馆随密度分布且覆盖广；高收入区域非图书馆可达性略高，高密度低收入区域图书馆可达性略高。

Conclusion: 所提框架可用于衡量文化设施获取的空间公平性，揭示不同类型文化设施分布与社会经济因素的关系。

Abstract: Cultural infrastructures, such as libraries, museums, theaters, and galleries, support learning, civic life, health, and local economies, yet access is uneven across cities. We present a novel, scalable, and open-data framework to measure spatial equity in cultural access. We map cultural infrastructures and compute a metric called Cultural Infrastructure Accessibility Score (CIAS) using exponential distance decay at fine spatial resolution, then aggregate the score per capita and integrate socio-demographic indicators. Interpretable tree-ensemble models with SHapley Additive exPlanation (SHAP) are used to explain associations between accessibility, income, density, and tract-level racial/ethnic composition. Results show a pronounced core-periphery gradient, where non-library cultural infrastructures cluster near urban cores, while libraries track density and provide broader coverage. Non-library accessibility is modestly higher in higher-income tracts, and library accessibility is slightly higher in denser, lower-income areas.

</details>


### [356] [Judging by the Rules: Compliance-Aligned Framework for Modern Slavery Statement Monitoring](https://arxiv.org/abs/2511.07803)
*Wenhao Xu,Akshatha Arodi,Jian-Yun Nie,Arsene Fansi Tchango*

Main category: cs.CY

TL;DR: 现代奴隶制监管要求企业披露信息，但披露内容模糊，现有大语言模型应用有缺陷，本文提出新框架用于规则级合规验证。


<details>
  <summary>Details</summary>
Motivation: 现代奴隶制监管披露内容模糊难审查，现有大语言模型应用缺乏合规审查所需结构，需解决合规验证问题。

Method: 提出包含CA - Judge和CALLM的新框架，用CA - Judge评估模型理由，训练CALLM产生符合规则且可验证的输出。

Result: CALLM提升了预测性能，输出透明且有法律依据。

Conclusion: 该框架为现实合规分析提供了更可验证和可操作的解决方案。

Abstract: Modern slavery affects millions of people worldwide, and regulatory frameworks such as Modern Slavery Acts now require companies to publish detailed disclosures. However, these statements are often vague and inconsistent, making manual review time-consuming and difficult to scale. While NLP offers a promising path forward, high-stakes compliance tasks require more than accurate classification: they demand transparent, rule-aligned outputs that legal experts can verify. Existing applications of large language models (LLMs) often reduce complex regulatory assessments to binary decisions, lacking the necessary structure for robust legal scrutiny. We argue that compliance verification is fundamentally a rule-matching problem: it requires evaluating whether textual statements adhere to well-defined regulatory rules. To this end, we propose a novel framework that harnesses AI for rule-level compliance verification while preserving expert oversight. At its core is the Compliance Alignment Judge (CA-Judge), which evaluates model-generated justifications based on their fidelity to statutory requirements. Using this feedback, we train the Compliance Alignment LLM (CALLM), a model that produces rule-consistent, human-verifiable outputs. CALLM improves predictive performance and generates outputs that are both transparent and legally grounded, offering a more verifiable and actionable solution for real-world compliance analysis.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [357] [Hybrid Bit and Semantic Communications](https://arxiv.org/abs/2404.19477)
*Kaiwen Yu,Renhe Fan,Gang Wu,Zhijin Qin*

Main category: eess.SP

TL;DR: 提出HybridBSC混合比特与语义通信系统，可利用现有架构传输比特和语义信息，实验证明策略可行。


<details>
  <summary>Details</summary>
Motivation: 当前直接将内容映射到传输符号的语义通信方法难部署，限制语义通信发展。

Method: 提出HybridBSC系统，将编码后的语义信息插入比特信息，利用传统数字通信系统传输，设计语义插入和提取方案。

Result: 基于pluto的软件定义无线电平台在真实无线信道实验，证明可同时传输语义和比特信息。

Conclusion: 所提策略能在现有通信架构下实现比特和语义信息传输。

Abstract: Semantic communication technology is regarded as a method surpassing the Shannon limit of bit transmission, capable of effectively enhancing transmission efficiency. However, current approaches that directly map content to transmission symbols are challenging to deploy in practice, imposing significant limitations on the development of semantic communication. To address this challenge, we propose a hybrid bit and semantic communication system, named HybridBSC, in which encoded semantic information is inserted into bit information for transmission via conventional digital communication systems utilizing same spectrum resources. The system can be easily deployed using existing communication architecture to achieve bit and semantic information transmission. Particularly, we design a semantic insertion and extraction scheme to implement this strategy. Furthermore, we conduct experimental validation based on the pluto-based software defined radio (SDR) platform in a real wireless channel, demonstrating that the proposed strategy can simultaneously transmit semantic and bit information.

</details>


### [358] [Toward Adaptive BCIs: Enhancing Decoding Stability via User State-Aware EEG Filtering](https://arxiv.org/abs/2511.07891)
*Yeon-Woo Choi,Hye-Bin Shin,Dan Li*

Main category: eess.SP

TL;DR: 提出用户状态感知EEG过滤框架改善BCI性能


<details>
  <summary>Details</summary>
Motivation: 解决BCI鲁棒性有限和长期适应性差的问题

Method: 从EEG特征估计用户认知状态，根据注意力水平自适应加权过滤不可靠片段

Result: 在多个EEG数据集实验中，该框架提升了不同用户状态和会话下的分类准确性和稳定性

Conclusion: 利用脑状态信息可显著提高基于EEG的BCI可靠性

Abstract: Brain-computer interfaces (BCIs) often suffer from limited robustness and poor long-term adaptability. Model performance rapidly degrades when user attention fluctuates, brain states shift over time, or irregular artifacts appear during interaction. To mitigate these issues, we introduce a user state-aware electroencephalogram (EEG) filtering framework that refines neural representations before decoding user intentions. The proposed method continuously estimates the user's cognitive state (e.g., focus or distraction) from EEG features and filters unreliable segments by applying adaptive weighting based on the estimated attention level. This filtering stage suppresses noisy or out-of-focus epochs, thereby reducing distributional drift and improving the consistency of subsequent decoding. Experiments on multiple EEG datasets that emulate real BCI scenarios demonstrate that the proposed state-aware filtering enhances classification accuracy and stability across different user states and sessions compared with conventional preprocessing pipelines. These findings highlight that leveraging brain-derived state information--even without additional user labels--can substantially improve the reliability of practical EEG-based BCIs.

</details>


### [359] [Generative AI Meets 6G and Beyond: Diffusion Models for Semantic Communications](https://arxiv.org/abs/2511.08416)
*Hai-Long Qin,Jincheng Dai,Guo Lu,Shuo Shao,Sixian Wang,Tongda Xu,Wenjun Zhang,Ping Zhang,Khaled B. Letaief*

Main category: eess.SP

TL;DR: 文章提供生成语义通信中扩散模型的全面教程，介绍技术支柱，从逆问题视角连接语义通信与计算成像，展示其在不同场景优势，推动其成为下一代无线网络基础组件。


<details>
  <summary>Details</summary>
Motivation: 无线系统接近理论容量极限，语义通信重要，生成AI催化生成语义通信，扩散模型有优势但缺乏系统指导，需提供全面教程。

Method: 介绍基于分数的扩散基础，系统回顾条件扩散、高效扩散、广义扩散三个技术支柱，引入逆问题视角将语义解码重新表述为后验推理。

Result: 通过分析不同场景，说明扩散模型能实现极端压缩，同时保持语义保真度和鲁棒性。

Conclusion: 文章旨在将生成AI创新与通信系统设计相结合，使扩散模型成为下一代无线网络及更广泛领域的基础组件。

Abstract: Semantic communications mark a paradigm shift from bit-accurate transmission toward meaning-centric communication, essential as wireless systems approach theoretical capacity limits. The emergence of generative AI has catalyzed generative semantic communications, where receivers reconstruct content from minimal semantic cues by leveraging learned priors. Among generative approaches, diffusion models stand out for their superior generation quality, stable training dynamics, and rigorous theoretical foundations. However, the field currently lacks systematic guidance connecting diffusion techniques to communication system design, forcing researchers to navigate disparate literatures. This article provides the first comprehensive tutorial on diffusion models for generative semantic communications. We present score-based diffusion foundations and systematically review three technical pillars: conditional diffusion for controllable generation, efficient diffusion for accelerated inference, and generalized diffusion for cross-domain adaptation. In addition, we introduce an inverse problem perspective that reformulates semantic decoding as posterior inference, bridging semantic communications with computational imaging. Through analysis of human-centric, machine-centric, and agent-centric scenarios, we illustrate how diffusion models enable extreme compression while maintaining semantic fidelity and robustness. By bridging generative AI innovations with communication system design, this article aims to establish diffusion models as foundational components of next-generation wireless networks and beyond.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [360] [Simulation-Based Fitting of Intractable Models via Sequential Sampling and Local Smoothing](https://arxiv.org/abs/2511.08180)
*Guido Masarotto*

Main category: stat.ME

TL;DR: 提出拟合生成模型的综合算法，结合全局与局部搜索，性能佳且有R包。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型似然等量难以解析或数值求解，且仅需有限先验信息的问题。

Method: 结合全局搜索确定解的区域，局部搜索模仿费舍尔评分算法计算拟似然估计量。

Result: 与其他方法对比，该方法表现出色。

Conclusion: 该算法可行有效，且有R包方便使用。

Abstract: This paper presents a comprehensive algorithm for fitting generative models whose likelihood, moments, and other quantities typically used for inference are not analytically or numerically tractable. The proposed method aims to provide a general solution that requires only limited prior information on the model parameters. The algorithm combines a global search phase, aimed at identifying the region of the solution, with a local search phase that mimics a trust region version of the Fisher scoring algorithm for computing a quasi-likelihood estimator. Comparisons with alternative methods demonstrate the strong performance of the proposed approach. An R package implementing the algorithm is available on CRAN.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [361] [TurboSAT: Gradient-Guided Boolean Satisfiability Accelerated on GPU-CPU Hybrid System](https://arxiv.org/abs/2511.07737)
*Steve Dai,Cunxi Yu,Kalyan Krishnamani,Brucek Khailany*

Main category: cs.LO

TL;DR: 本文将SAT问题转化为可优化的二值矩阵乘法层，结合并行可微优化与顺序搜索在混合GPU - CPU系统上加速SAT求解，在基准问题上实现超200倍加速。


<details>
  <summary>Details</summary>
Motivation: 加速计算对逻辑推理（如布尔可满足性问题SAT）影响有限，现有SAT求解器依赖顺序搜索算法，并行性受限。

Method: 将SAT问题表述为二值矩阵乘法层，用可微目标函数优化；在混合GPU - CPU系统中，GPU进行并行可微求解，CPU线程进行冲突驱动的顺序搜索。

Result: 在NVIDIA DGX GB200节点上进行原型开发，在公共可满足基准问题上，与最先进的基于CPU的求解器相比，运行时间加速超200倍。

Conclusion: 结合并行可微优化和顺序搜索的混合求解方法能有效加速SAT求解。

Abstract: While accelerated computing has transformed many domains of computing, its impact on logical reasoning, specifically Boolean satisfiability (SAT), remains limited. State-of-the-art SAT solvers rely heavily on inherently sequential conflict-driven search algorithms that offer powerful heuristics but limit the amount of parallelism that could otherwise enable significantly more scalable SAT solving. Inspired by neural network training, we formulate the SAT problem as a binarized matrix-matrix multiplication layer that could be optimized using a differentiable objective function. Enabled by this encoding, we combine the strengths of parallel differentiable optimization and sequential search to accelerate SAT on a hybrid GPU-CPU system. In this system, the GPUs leverage parallel differentiable solving to rapidly evaluate SAT clauses and use gradients to stochastically explore the solution space and optimize variable assignments. Promising partial assignments generated by the GPUs are post-processed on many CPU threads which exploit conflict-driven sequential search to further traverse the solution subspaces and identify complete assignments. Prototyping the hybrid solver on an NVIDIA DGX GB200 node, our solver achieves runtime speedups up to over 200x when compared to a state-of-the-art CPU-based solver on public satisfiable benchmark problems from the SAT Competition.

</details>


### [362] [Proof Minimization in Neural Network Verification](https://arxiv.org/abs/2511.08198)
*Omri Isac,Idan Refaeli,Haoze Wu,Clark Barrett,Guy Katz*

Main category: cs.LO

TL;DR: 本文关注DNN验证器可满足性证明过大问题，提出算法最小化证明，评估显示能大幅减小证明大小和检查时间，同时带来一定验证运行时开销。


<details>
  <summary>Details</summary>
Motivation: DNN验证器可能有漏洞影响验证可靠性，虽可用证明解决，但证明通常过大限制使用，需最小化证明。

Method: 提出算法移除验证过程中学习到但对证明不必要的事实，分析事实间依赖关系，还使用两种替代程序消除剩余不必要依赖。

Result: 最佳算法使证明大小减小37%-82%，证明检查时间减少30%-88%，验证过程运行时开销为7%-20%。

Conclusion: 提出的算法能有效最小化DNN验证器的不可满足性证明，在减小证明大小和检查时间上有显著效果。

Abstract: The widespread adoption of deep neural networks (DNNs) requires efficient techniques for verifying their safety. DNN verifiers are complex tools, which might contain bugs that could compromise their soundness and undermine the reliability of the verification process. This concern can be mitigated using proofs: artifacts that are checkable by an external and reliable proof checker, and which attest to the correctness of the verification process. However, such proofs tend to be extremely large, limiting their use in many scenarios. In this work, we address this problem by minimizing proofs of unsatisfiability produced by DNN verifiers. We present algorithms that remove facts which were learned during the verification process, but which are unnecessary for the proof itself. Conceptually, our method analyzes the dependencies among facts used to deduce UNSAT, and removes facts that did not contribute. We then further minimize the proof by eliminating remaining unnecessary dependencies, using two alternative procedures. We implemented our algorithms on top of a proof producing DNN verifier, and evaluated them across several benchmarks. Our results show that our best-performing algorithm reduces proof size by 37%-82% and proof checking time by 30%-88%, while introducing a runtime overhead of 7%-20% to the verification process itself.

</details>


### [363] [Constrained and Robust Policy Synthesis with Satisfiability-Modulo-Probabilistic-Model-Checking](https://arxiv.org/abs/2511.08078)
*Linus Heck,Filip Macák,Milan Češka,Sebastian Junges*

Main category: cs.LO

TL;DR: 本文提出首个灵活高效框架，用于计算受任意结构约束的鲁棒策略，实验证明其可行性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有计算给定有限马尔可夫决策过程（MDP）奖励最优策略的方法，难以满足策略鲁棒性和额外结构约束的需求，计算此类策略更具挑战性。

Method: 使用一阶理论表达约束，紧密集成可满足性求解器处理组合问题，结合概率模型检查算法分析MDP。

Result: 在数百个基准测试上的实验，证明了约束和鲁棒策略合成的可行性，以及在问题的各个子集中与现有方法相比的竞争力。

Conclusion: 所提出的框架能有效计算受任意结构约束的鲁棒策略。

Abstract: The ability to compute reward-optimal policies for given and known finite Markov decision processes (MDPs) underpins a variety of applications across planning, controller synthesis, and verification. However, we often want policies (1) to be robust, i.e., they perform well on perturbations of the MDP and (2) to satisfy additional structural constraints regarding, e.g., their representation or implementation cost. Computing such robust and constrained policies is indeed computationally more challenging. This paper contributes the first approach to effectively compute robust policies subject to arbitrary structural constraints using a flexible and efficient framework. We achieve flexibility by allowing to express our constraints in a first-order theory over a set of MDPs, while the root for our efficiency lies in the tight integration of satisfiability solvers to handle the combinatorial nature of the problem and probabilistic model checking algorithms to handle the analysis of MDPs. Experiments on a few hundred benchmarks demonstrate the feasibility for constrained and robust policy synthesis and the competitiveness with state-of-the-art methods for various fragments of the problem.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [364] [Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models](https://arxiv.org/abs/2511.08577)
*Tianyu Fu,Yichen You,Zekai Chen,Guohao Dai,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出TaH动态潜在思考方法，仅对难预测的token进行迭代优化，实验表明TaH能提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有循环transformer存在的过度思考问题，在参数约束下提高大语言模型推理能力。

Method: 提出TaH方法，用轻量级神经决策器触发潜在迭代，使用LoRA模块聚焦难预测token，引入双因果注意力机制。

Result: TaH在五个具有挑战性的基准测试中提升了LLM推理性能，相比基线有明显的准确率提升。

Conclusion: TaH是一种有效的动态潜在思考方法，能在参数约束下提高LLM推理能力。

Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.

</details>


### [365] [AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress](https://arxiv.org/abs/2511.08325)
*Zhiheng Xi,Chenyang Liao,Guanyu Li,Yajie Yang,Wenxiang Chen,Zhihao Zhang,Binghai Wang,Senjie Jin,Yuhao Zhou,Jian Guan,Wei Wu,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出 AgentPRM 用于大语言模型的多轮决策任务，采用 TD 与 GAE 结合的方法获取训练数据，实验表明其计算效率高且性能好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮决策任务中存在挑战，以往方法依赖提示工程或微调，本文从构建过程奖励模型的新视角解决问题。

Method: 提出 AgentPRM 来评估决策和指导决策过程，采用 TD 与 GAE 结合的方法获取训练数据。

Result: AgentPRM 计算效率比基线高 8 倍以上，增加测试时计算量能稳健提升性能。

Conclusion: AgentPRM 能有效应用于大语言模型的多轮决策任务，还可用于强化学习。

Abstract: Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.

</details>


### [366] [TurkEmbed: Turkish Embedding Model on NLI & STS Tasks](https://arxiv.org/abs/2511.08376)
*Özay Ezerceli,Gizem Gümüşçekiçci,Tuğba Erkoç,Berke Özenç*

Main category: cs.CL

TL;DR: 本文介绍土耳其语言嵌入模型TurkEmbed，结合多样数据集与先进训练技术，在NLI和STS任务表现优，超当前最优模型。


<details>
  <summary>Details</summary>
Motivation: 现有土耳其嵌入模型常依赖机器翻译数据集，限制准确性和语义理解，需新模型改善。

Method: 结合多样数据集和先进训练技术，如套娃表示学习。

Result: 在土耳其STS - b - TR数据集评估显示语义相似度任务显著进步，在All - NLI - TR和STS - b - TR基准超当前最优模型Emrecan，提升1 - 4%。

Conclusion: TurkEmbed可增强土耳其NLP生态，促进下游应用发展。

Abstract: This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.

</details>


### [367] [REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment](https://arxiv.org/abs/2511.07458)
*Priyanka Mudgal*

Main category: cs.CL

TL;DR: 引入无参考评估指标REFLEX评估日志摘要系统，比传统指标更有效。


<details>
  <summary>Details</summary>
Motivation: 现有日志摘要系统评估因缺乏高质量参考摘要及传统指标局限而具挑战性。

Method: 基于大语言模型判断，将LLM作为零样本评估器，从相关性、信息性和连贯性等维度评估。

Result: REFLEX在多日志摘要数据集上产生稳定、可解释和细粒度评估，比传统指标更能区分模型输出。

Conclusion: REFLEX为参考数据稀缺或不可用的现实场景提供可扩展的日志摘要评估方法。

Abstract: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.

</details>


### [368] [A Preliminary Study of RAG for Taiwanese Historical Archives](https://arxiv.org/abs/2511.07445)
*Claire Lin,Bo-Han Feng,Xuanjun Chen,Te-Lun Yang,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.CL

TL;DR: 研究RAG管道在台湾历史档案数据集上的应用，发现早期元数据集成可提升性能，但仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 少有研究将RAG用于台湾历史档案，本文对此开展初步研究。

Method: 将RAG管道应用于两个繁体中文历史数据集及对应开放式查询集，系统研究查询特征和元数据集成策略对检索质量、答案生成和系统整体性能的影响。

Result: 早期元数据集成能提高检索和答案准确性，RAG系统存在生成幻觉、处理时间或多跳历史查询困难等问题。

Conclusion: RAG应用于台湾历史档案有一定效果，但仍面临一些挑战需解决。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.

</details>


### [369] [GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models](https://arxiv.org/abs/2511.07457)
*Jiarui Feng,Donghong Cai,Yixin Chen,Muhan Zhang*

Main category: cs.CL

TL;DR: 提出GRIP框架让大语言模型处理图结构数据，实验验证其有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在让大语言模型处理结构数据时存在问题，如转换文本有高开销、需大规模训练和复杂对齐且结果不佳。

Method: 提出GRIP框架，通过精心设计的微调任务让大语言模型内化图中的复杂关系信息，将知识存储在轻量级LoRA参数中。

Result: 在多个基准测试上的大量实验验证了方法的有效性和高效性。

Conclusion: GRIP框架能有效让大语言模型处理图结构数据，且推理时无需访问原始图。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remains a challenging problem. Some approaches adopt complex strategies to convert graphs into text sequences, resulting in significant token overhead and rendering them impractical for large-scale graphs. Others introduce additional modules to encode graphs into fixed-size token representations for LLMs. However, these methods typically require large-scale post-training on graph-text corpus and complex alignment procedures, yet often yield sub-optimal results due to poor modality alignment. Inspired by in-parameter knowledge injection for test-time adaptation of LLMs, we propose GRIP, a novel framework that equips LLMs with the ability to internalize complex relational information from graphs through carefully designed fine-tuning tasks. This knowledge is efficiently stored within lightweight LoRA parameters, enabling the fine-tuned LLM to perform a wide range of graph-related tasks without requiring access to the original graph at inference time. Extensive experiments across multiple benchmarks validate the effectiveness and efficiency of our approach.

</details>


### [370] [It Takes Two: A Dual Stage Approach for Terminology-Aware Translation](https://arxiv.org/abs/2511.07461)
*Akshat Singh Jaswal*

Main category: cs.CL

TL;DR: 本文介绍了用于术语约束机器翻译的DuTerm架构，结合NMT模型和基于提示的LLM进行后编辑，评估显示LLM灵活处理术语效果更好。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的术语约束机器翻译架构，提高翻译质量。

Method: 采用两阶段架构DuTerm，结合经大规模合成数据微调的术语感知NMT模型和基于提示的LLM进行后编辑。

Result: 在英德、英西、英俄翻译任务上评估，发现LLM灵活处理术语比严格约束有更高质量翻译。

Conclusion: LLM作为上下文驱动的修改器而非生成器时，最适合高质量翻译。

Abstract: This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-editing. The LLM stage refines NMT output and enforces terminology adherence. We evaluate DuTerm on English-to German, English-to-Spanish, and English-to-Russian with the WMT 2025 Terminology Shared Task corpus. We demonstrate that flexible, context-driven terminology handling by the LLM consistently yields higher quality translations than strict constraint enforcement. Our results highlight a critical trade-off, revealing that an LLM's work best for high-quality translation as context-driven mutators rather than generators.

</details>


### [371] [Motif 2 12.7B technical report](https://arxiv.org/abs/2511.07464)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.CL

TL;DR: 介绍Motif - 2 - 12.7B开放权重基础模型，通过架构创新和系统级优化提升大语言模型效率，在多基准测试有竞争力。


<details>
  <summary>Details</summary>
Motivation: 在受限计算预算下实现可扩展语言理解和强大指令泛化能力。

Method: 基于Motif - 2.6B集成分组差分注意力（GDA）；用课程驱动数据调度器在5.5万亿令牌上预训练；使用MuonClip优化器和自定义高性能内核；采用三阶段监督微调管道。

Result: Motif - 2 - 12.7B在不同基准测试中表现出有竞争力的性能。

Conclusion: 精心的架构扩展和优化的训练设计可媲美更大模型的能力。

Abstract: We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.

</details>


### [372] [Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models](https://arxiv.org/abs/2511.07498)
*Xin Liu,Qiyang Song,Qihang Zhou,Haichao Du,Shaowen Xu,Wenbo Jiang,Weijuan Zhang,Xiaoqi Jia*

Main category: cs.CL

TL;DR: 研究MHA对大语言模型多语言处理的贡献，提出LAHIS方法，发现特定语言和通用语言头，引入轻量级适配提升模型能力。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型发展，MHA在多语言能力方面作用未充分研究，需探索其贡献以提升模型性能。

Method: 提出LAHIS方法，通过单次前向和反向传播识别注意力头重要性；引入轻量级适配学习软头掩码调节注意力输出。

Result: 在多个模型上应用LAHIS发现特定语言和通用语言头，特定语言头有助于解决多语言问题；轻量级适配仅需20个可调参数提升XQuAD准确率。

Conclusion: 从MHA角度提升了大语言模型的可解释性和多语言能力。

Abstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.

</details>


### [373] [Revisiting NLI: Towards Cost-Effective and Human-Aligned Metrics for Evaluating LLMs in Question Answering](https://arxiv.org/abs/2511.07659)
*Sai Shridhar Balamurali,Lu Cheng*

Main category: cs.CL

TL;DR: 本文指出评估大语言模型答案有挑战，重新评估轻量级NLI评分方法，引入DIVER - QA基准，结果显示NLI评估仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型答案的方法存在问题，如词汇指标忽略语义细微差别，‘LLM - as - Judge’评分计算成本高，需寻找更好的评估方法。

Method: 重新评估带简单词汇匹配标志的现成自然语言推理（NLI）评分方法，引入新的人类注释基准DIVER - QA。

Result: NLI评分方法在长形式问答上达到与GPT - 4o相当的准确率（89.9%），且所需参数少得多。

Conclusion: 低成本的基于NLI的评估仍具有竞争力，DIVER - QA可作为未来指标研究的开放资源。

Abstract: Evaluating answers from state-of-the-art large language models (LLMs) is challenging: lexical metrics miss semantic nuances, whereas "LLM-as-Judge" scoring is computationally expensive. We re-evaluate a lightweight alternative -- off-the-shelf Natural Language Inference (NLI) scoring augmented by a simple lexical-match flag and find that this decades-old technique matches GPT-4o's accuracy (89.9%) on long-form QA, while requiring orders-of-magnitude fewer parameters. To test human alignment of these metrics rigorously, we introduce DIVER-QA, a new 3000-sample human-annotated benchmark spanning five QA datasets and five candidate LLMs. Our results highlight that inexpensive NLI-based evaluation remains competitive and offer DIVER-QA as an open resource for future metric research.

</details>


### [374] [Stress Testing Factual Consistency Metrics for Long-Document Summarization](https://arxiv.org/abs/2511.07689)
*Zain Muhammad Mujahid,Dustin Wright,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 评估摘要文本摘要的事实一致性对长文档仍是挑战，本文系统评估六种无参考事实性指标在长文档场景的可靠性，发现其存在问题并指出改进方向。


<details>
  <summary>Details</summary>
Motivation: 传统指标在长文档摘要事实一致性评估中存在输入长度限制和处理长距离依赖的困难，需要评估现有短文本摘要指标在长文档场景的可靠性。

Method: 对摘要应用七种保留事实性的扰动，分析指标对检索上下文和声明信息密度的敏感性，在三个长文本基准数据集上进行实验。

Result: 现有短文本指标对语义等效摘要评分不一致，对信息密集声明可靠性下降，扩展检索上下文仅在部分领域提高稳定性，无指标能在长上下文条件下始终保持事实对齐。

Conclusion: 指出改进事实性评估的具体方向，如多跨度推理、上下文感知校准等，并开源代码和数据。

Abstract: Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at https://github.com/zainmujahid/metricEval-longSum.

</details>


### [375] [CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences](https://arxiv.org/abs/2511.07691)
*Rhitabrat Pokharel,Yufei Tao,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 提出用于多语言大模型偏好优化的CAPO方法，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有如DPO等偏好优化方法在多语言场景泛化能力不足。

Method: 提出Confidence - Aware Preference Optimization (CAPO)方法，用基于相对奖励的动态损失缩放机制替代DPO对偏好对的固定处理。

Result: CAPO在奖励准确性上至少比现有偏好优化基线高16%，且能扩大不同语言中偏好和非偏好响应的差距。

Conclusion: CAPO是有效的多语言大模型偏好优化方法，能增强对噪声或低差异比较的鲁棒性。

Abstract: Preference optimization is a critical post-training technique used to align large language models (LLMs) with human preferences, typically by fine-tuning on ranked response pairs. While methods like Direct Preference Optimization (DPO) have proven effective in English, they often fail to generalize robustly to multilingual settings. We propose a simple yet effective alternative, Confidence-Aware Preference Optimization (CAPO), which replaces DPO's fixed treatment of preference pairs with a dynamic loss scaling mechanism based on a relative reward. By modulating the learning signal according to the confidence in each preference pair, CAPO enhances robustness to noisy or low-margin comparisons, typically encountered in multilingual text. Empirically, CAPO outperforms existing preference optimization baselines by at least 16% in reward accuracy, and improves alignment by widening the gap between preferred and dispreferred responses across languages.

</details>


### [376] [Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG](https://arxiv.org/abs/2511.08245)
*Jisoo Jang,Tien-Cuong Bui,Yunjun Choi,Wen-Syan Li*

Main category: cs.CL

TL;DR: 本文提出基于提示调优的NL - to - SQL纠错方法，结合预训练大模型和RAG，实验显示比基线准确率高12%。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言接口使用增多，需高效准确地将自然语言查询转换为SQL表达式。

Method: 借鉴医疗诊断流程，提出集成纠错机制的框架，结合嵌入微调与RAG利用外部知识库。

Result: 框架比现有基线准确率提高12%。

Conclusion: 该框架有潜力革新当代数据驱动环境下的数据访问和处理。

Abstract: This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.

</details>


### [377] [NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation](https://arxiv.org/abs/2511.07982)
*Maoqi Liu,Quan Fang,Yuhao Wu,Can Zhao,Yang Yang,Kaiquan Cai*

Main category: cs.CL

TL;DR: 本文提出NOTAM - Evolve框架解决航行通告解读难题，还引入新基准数据集，实验显示该框架有显著提升。


<details>
  <summary>Details</summary>
Motivation: 航行通告语言晦涩，现有自动系统解析浅，无法提取操作决策所需信息，需要准确解读以保障航空安全。

Method: 将完整解读任务形式化为深度解析，提出NOTAM - Evolve自进化框架，利用知识图增强检索模块进行数据接地，引入闭环学习过程，同时引入含10000个专家标注航行通告的新基准数据集。

Result: NOTAM - Evolve较基础大语言模型在结构化航行通告解读任务上绝对准确率提升30.4%，达到新的最优水平。

Conclusion: NOTAM - Evolve框架能有效解决航行通告解读难题，提升解读准确性。

Abstract: Accurate interpretation of Notices to Airmen (NOTAMs) is critical for aviation safety, yet their condensed and cryptic language poses significant challenges to both manual and automated processing. Existing automated systems are typically limited to shallow parsing, failing to extract the actionable intelligence needed for operational decisions. We formalize the complete interpretation task as deep parsing, a dual-reasoning challenge requiring both dynamic knowledge grounding (linking the NOTAM to evolving real-world aeronautical data) and schema-based inference (applying static domain rules to deduce operational status). To tackle this challenge, we propose NOTAM-Evolve, a self-evolving framework that enables a large language model (LLM) to autonomously master complex NOTAM interpretation. Leveraging a knowledge graph-enhanced retrieval module for data grounding, the framework introduces a closed-loop learning process where the LLM progressively improves from its own outputs, minimizing the need for extensive human-annotated reasoning traces. In conjunction with this framework, we introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Our experiments demonstrate that NOTAM-Evolve achieves a 30.4% absolute accuracy improvement over the base LLM, establishing a new state of the art on the task of structured NOTAM interpretation.

</details>


### [378] [State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?](https://arxiv.org/abs/2511.07989)
*Taja Kuzman Pungeršek,Peter Rupnik,Ivan Porupski,Vuk Dinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: 评估多种语言模型在南斯拉夫语系文本分类任务中的表现，发现大语言模型零样本性能强，但也有不足，微调的类BERT模型仍是大规模自动文本标注更实用选择。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在文本分类尤其是低资源语言上的表现研究不足，需评估其在南斯拉夫语系文本分类任务中的性能。

Method: 比较公开可用的微调类BERT模型与开源和闭源大语言模型在三个领域三项任务中的表现。

Result: 大语言模型零样本性能强，在南斯拉夫语系和英语中表现相当，但存在输出不可预测、推理慢和计算成本高的问题。

Conclusion: 由于大语言模型的局限性，微调的类BERT模型仍是大规模自动文本标注更实用的选择。

Abstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.

</details>


### [379] [Interaction Dynamics as a Reward Signal for LLMs](https://arxiv.org/abs/2511.08394)
*Sian Gooding,Edward Grefenstette*

Main category: cs.CL

TL;DR: 本文提出基于对话嵌入轨迹几何属性的新型奖励信号TRACE，发现仅基于结构信号训练的奖励模型准确率与分析全文本的基线相当，混合模型性能最佳，证明交流方式和内容同样重要。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多轮对话对齐方法忽略了交互动态这一丰富的信号源。

Method: 引入基于对话嵌入轨迹几何属性的TRACE奖励信号。

Result: 仅基于结构信号训练的奖励模型成对准确率达68.20%，与分析全文本的基线（70.04%）相当，混合模型性能最高，达80.17%。

Conclusion: 在交互场景中，交流方式和内容对成功协作的预测能力相当，提供了新的隐私保护框架和诊断工具。

Abstract: The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.

</details>


### [380] [Self-Correction Distillation for Structured Data Question Answering](https://arxiv.org/abs/2511.07998)
*Yushan Zhu,Wen Zhang,Long Jin,Mengshu Sun,Ling Zhong,Zhiqiang Liu,Juan Li,Lei Liang,Chong Long,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 提出自校正蒸馏（SCD）方法提升小规模大语言模型结构化数据问答能力，实验显示SCD性能佳。


<details>
  <summary>Details</summary>
Motivation: 现有统一结构问答框架应用于小规模大语言模型时，后者生成结构化查询易出错，需提升其结构化数据问答能力。

Method: 提出SCD方法，设计错误提示机制（EPM）检测错误并提供定制错误信息，采用两阶段蒸馏策略将大规模大语言模型的查询生成和纠错能力转移到小规模模型。

Result: 在5个基准测试、3种结构化数据类型的实验中，SCD在小规模大语言模型（8B）上性能最佳、泛化性好，在部分数据集上接近GPT4性能；配备EPM的大规模大语言模型在多数数据集上超越现有最优结果。

Conclusion: SCD方法能有效提升小规模大语言模型的结构化数据问答能力。

Abstract: Structured data question answering (QA), including table QA, Knowledge Graph (KG) QA, and temporal KG QA, is a pivotal research area. Advances in large language models (LLMs) have driven significant progress in unified structural QA frameworks like TrustUQA. However, these frameworks face challenges when applied to small-scale LLMs since small-scale LLMs are prone to errors in generating structured queries. To improve the structured data QA ability of small-scale LLMs, we propose a self-correction distillation (SCD) method. In SCD, an error prompt mechanism (EPM) is designed to detect errors and provide customized error messages during inference, and a two-stage distillation strategy is designed to transfer large-scale LLMs' query-generation and error-correction capabilities to small-scale LLM. Experiments across 5 benchmarks with 3 structured data types demonstrate that our SCD achieves the best performance and superior generalization on small-scale LLM (8B) compared to other distillation methods, and closely approaches the performance of GPT4 on some datasets. Furthermore, large-scale LLMs equipped with EPM surpass the state-of-the-art results on most datasets.

</details>


### [381] [SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation](https://arxiv.org/abs/2511.08500)
*Berkcan Kapusuzoglu,Supriyo Chakraborty,Renkun Ni,Stephen Rawls,Sambit Sahu*

Main category: cs.CL

TL;DR: 提出SPEAR - MM框架解决金融领域大语言模型灾难性遗忘问题，应用于LLaMA - 3.1 - 8B效果好，还能降低成本。


<details>
  <summary>Details</summary>
Motivation: 金融领域适配的大语言模型存在灾难性遗忘通用推理能力问题，影响客户交互和复杂金融分析。

Method: 通过事后分析近似层对外部基准的影响，然后通过球面插值合并选择性冻结或恢复变压器层。

Result: 应用于LLaMA - 3.1 - 8B做金融任务时，通用能力保留率达91.2%（标准持续预训练为69.7%），保留94%的领域适配增益，降低90%计算成本。

Conclusion: SPEAR - MM框架能保留关键能力，实现领域适配，提供可解释权衡控制，降低计算成本，对资源受限金融机构至关重要。

Abstract: Large language models (LLMs) adapted to financial domains often suffer from catastrophic forgetting of general reasoning capabilities essential for customer interactions and complex financial analysis. We introduce Selective Parameter Evaluation and Restoration via Model Merging (SPEAR-MM), a practical framework that preserves critical capabilities while enabling domain adaptation. Our method approximates layer-wise impact on external benchmarks through post-hoc analysis, then selectively freezes or restores transformer layers via spherical interpolation merging. Applied to LLaMA-3.1-8B for financial tasks, SPEAR-MM achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining, while maintaining 94% of domain adaptation gains. The approach provides interpretable trade-off control and reduces computational costs by 90% crucial for resource-constrained financial institutions.

</details>


### [382] [Structured RAG for Answering Aggregative Questions](https://arxiv.org/abs/2511.08505)
*Omri Koshorek,Niv Granot,Aviv Alloni,Shahar Admati,Roee Hendel,Ido Weiss,Alan Arazi,Shay-Nitzan Cohen,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 提出S - RAG方法处理聚合查询，引入新数据集，实验显示其性能优于常见RAG系统和长上下文大语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前数据集和方法难以处理聚合查询，需新方法填补空白。

Method: 提出S - RAG，在数据摄入时构建语料库的结构化表示，推理时将自然语言查询转换为形式查询。

Result: S - RAG在新引入数据集和公开基准上显著优于常见RAG系统和长上下文大语言模型。

Conclusion: S - RAG是处理聚合查询的有效方法，引入的新数据集可推动该领域研究。

Abstract: Retrieval-Augmented Generation (RAG) has become the dominant approach for answering questions over large corpora. However, current datasets and methods are highly focused on cases where only a small part of the corpus (usually a few paragraphs) is relevant per query, and fail to capture the rich world of aggregative queries. These require gathering information from a large set of documents and reasoning over them. To address this gap, we propose S-RAG, an approach specifically designed for such queries. At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation. To validate our approach and promote further research in this area, we introduce two new datasets of aggregative queries: HOTELS and WORLD CUP. Experiments with S-RAG on the newly introduced datasets, as well as on a public benchmark, demonstrate that it substantially outperforms both common RAG systems and long-context LLMs.

</details>


### [383] [BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship Attribution](https://arxiv.org/abs/2511.08085)
*Abdullah Muhammad Moosa,Nusrat Sultana,Mahdi Muhammad Moosa,Md. Miraiz Hossain*

Main category: cs.CL

TL;DR: 研究对孟加拉语作者归属问题展开调查，引入新基准语料库BARD10，分析停用词去除影响，发现经典TF - IDF + SVM表现最佳，揭示停用词重要性等见解。


<details>
  <summary>Details</summary>
Motivation: 对孟加拉语作者归属问题进行全面研究，探究停用词在风格上的意义。

Method: 引入新基准语料库BARD10，评估SVM、Bangla BERT、XGBoost和MLP四种分类器，在BARD10和BAAD16上进行统一预处理。

Result: 经典TF - IDF + SVM基线表现优于其他模型，BARD10作者对停用词修剪敏感，BAAD16作者相对稳健，Transformer模型会削弱高频成分传递的作者特征。

Conclusion: 孟加拉语停用词是重要风格指标；微调的ML模型在短文本中有效果；BARD10为未来研究提供可重复基准。

Abstract: This research presents a comprehensive investigation into Bangla authorship attribution, introducing a new balanced benchmark corpus BARD10 (Bangla Authorship Recognition Dataset of 10 authors) and systematically analyzing the impact of stop-word removal across classical and deep learning models to uncover the stylistic significance of Bangla stop-words. BARD10 is a curated corpus of Bangla blog and opinion prose from ten contemporary authors, alongside the methodical assessment of four representative classifiers: SVM (Support Vector Machine), Bangla BERT (Bidirectional Encoder Representations from Transformers), XGBoost, and a MLP (Multilayer Perception), utilizing uniform preprocessing on both BARD10 and the benchmark corpora BAAD16 (Bangla Authorship Attribution Dataset of 16 authors). In all datasets, the classical TF-IDF + SVM baseline outperformed, attaining a macro-F1 score of 0.997 on BAAD16 and 0.921 on BARD10, while Bangla BERT lagged by as much as five points. This study reveals that BARD10 authors are highly sensitive to stop-word pruning, while BAAD16 authors remain comparatively robust highlighting genre-dependent reliance on stop-word signatures. Error analysis revealed that high frequency components transmit authorial signatures that are diminished or reduced by transformer models. Three insights are identified: Bangla stop-words serve as essential stylistic indicators; finely calibrated ML models prove effective within short-text limitations; and BARD10 connects formal literature with contemporary web dialogue, offering a reproducible benchmark for future long-context or domain-adapted transformers.

</details>


### [384] [Training Language Models to Explain Their Own Computations](https://arxiv.org/abs/2511.08579)
*Belinda Z. Li,Zifan Carl Guo,Vincent Huang,Jacob Steinhardt,Jacob Andreas*

Main category: cs.CL

TL;DR: 研究语言模型能否忠实描述自身计算过程，微调模型生成解释，有一定泛化能力，自解释效果更好，可补充现有可解释性方法。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型利用自身内部信息生成解释技术的程度，了解其能否忠实描述自身计算过程及是否比解释其他模型更优。

Method: 以现有可解释性技术为基准，微调语言模型生成自然语言描述，包括特征编码信息、内部激活因果结构和输入令牌对输出的影响。

Result: 训练数万示例后，解释器模型对新查询有一定泛化能力，自解释效果优于用其他模型解释。

Conclusion: 语言模型能可靠解释自身计算过程，其解释可作为现有可解释性方法的可扩展补充。

Abstract: Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.

</details>


### [385] [Relation as a Prior: A Novel Paradigm for LLM-based Document-level Relation Extraction](https://arxiv.org/abs/2511.08143)
*Qiankun Pi,Yepeng Sun,Jicang Lu,Qinlong Fan,Ningbo Huang,Shiyu Wang*

Main category: cs.CL

TL;DR: 现有大语言模型在文档级关系抽取中有性能差距，本文提出RelPrior范式解决问题，实验显示其性能达最优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文档级关系抽取中因采用“先提取实体再预测关系”范式存在性能差距，有过滤无关实体对和避免严格预定义关系标签误判的需求。

Method: 提出RelPrior范式，用二元关系作为先验过滤无关实体对，用预定义关系作为先验匹配实体进行三元组提取。

Result: 在两个基准测试上的广泛实验表明，RelPrior超越现有基于大语言模型的方法，达到了最优性能。

Conclusion: RelPrior范式能够有效解决大语言模型在文档级关系抽取中的性能问题。

Abstract: Large Language Models (LLMs) have demonstrated their remarkable capabilities in document understanding. However, recent research reveals that LLMs still exhibit performance gaps in Document-level Relation Extraction (DocRE) as requiring fine-grained comprehension. The commonly adopted "extract entities then predict relations" paradigm in LLM-based methods leads to these gaps due to two main reasons: (1) Numerous unrelated entity pairs introduce noise and interfere with the relation prediction for truly related entity pairs. (2) Although LLMs have identified semantic associations between entities, relation labels beyond the predefined set are still treated as prediction errors. To address these challenges, we propose a novel Relation as a Prior (RelPrior) paradigm for LLM-based DocRE. For challenge (1), RelPrior utilizes binary relation as a prior to extract and determine whether two entities are correlated, thereby filtering out irrelevant entity pairs and reducing prediction noise. For challenge (2), RelPrior utilizes predefined relation as a prior to match entities for triples extraction instead of directly predicting relation. Thus, it avoids misjudgment caused by strict predefined relation labeling. Extensive experiments on two benchmarks demonstrate that RelPrior achieves state-of-the-art performance, surpassing existing LLM-based methods.

</details>


### [386] [Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback](https://arxiv.org/abs/2511.08225)
*Yishan Du,Conrad Borchers,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 文章提出基于嵌入的基准测试框架检测形成性反馈中LLMs的偏差，研究6个LLMs，发现模型对性别替换有不对称语义响应，存在性别偏差，并给出相关建议。


<details>
  <summary>Details</summary>
Motivation: 教师在教育实践中更多使用GenAI，需要可靠方法对用于教学的LLMs进行基准测试。

Method: 使用AES 2.0语料库的600篇学生作文构建反事实，研究6个代表性LLMs，用余弦和欧氏距离量化响应差异，通过排列测试评估显著性，用降维可视化结构。

Result: 隐式操作对男女反事实诱导的语义变化大于女男；仅GPT和Llama模型对显式性别提示敏感；存在语言差异。

Conclusion: 即使最先进的LLMs也存在性别偏差，文章讨论了对教学GenAI公平性审计的影响，提出报告标准和实践指导。

Abstract: As teachers increasingly turn to GenAI in their educational practice, we need robust methods to benchmark large language models (LLMs) for pedagogical purposes. This article presents an embedding-based benchmarking framework to detect bias in LLMs in the context of formative feedback. Using 600 authentic student essays from the AES 2.0 corpus, we constructed controlled counterfactuals along two dimensions: (i) implicit cues via lexicon-based swaps of gendered terms within essays, and (ii) explicit cues via gendered author background in the prompt. We investigated six representative LLMs (i.e. GPT-5 mini, GPT-4o mini, DeepSeek-R1, DeepSeek-R1-Qwen, Gemini 2.5 Pro, Llama-3-8B). We first quantified the response divergence with cosine and Euclidean distances over sentence embeddings, then assessed significance via permutation tests, and finally, visualised structure using dimensionality reduction. In all models, implicit manipulations reliably induced larger semantic shifts for male-female counterfactuals than for female-male. Only the GPT and Llama models showed sensitivity to explicit gender cues. These findings show that even state-of-the-art LLMs exhibit asymmetric semantic responses to gender substitutions, suggesting persistent gender biases in feedback they provide learners. Qualitative analyses further revealed consistent linguistic differences (e.g., more autonomy-supportive feedback under male cues vs. more controlling feedback under female cues). We discuss implications for fairness auditing of pedagogical GenAI, propose reporting standards for counterfactual evaluation in learning analytics, and outline practical guidance for prompt design and deployment to safeguard equitable feedback.

</details>


### [387] [Adaptive Multi-Agent Response Refinement in Conversational Systems](https://arxiv.org/abs/2511.08319)
*Soyeong Jeong,Aparna Elangovan,Emine Yilmaz,Oleg Rokhlenko*

Main category: cs.CL

TL;DR: 提出用多智能体框架优化大语言模型对话回复，在对话数据集上验证效果显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在对话系统中生成回复时难以兼顾个性化和特定知识，现有单模型优化方法有局限。

Method: 采用多智能体框架，每个智能体负责事实性、个性化和连贯性其中一个方面的审查和优化，引入动态通信策略自适应选择和协调智能体。

Result: 在具有挑战性的对话数据集上验证，显著优于相关基线。

Conclusion: 提出的多智能体框架在处理涉及知识或用户特征的任务上表现出色，能有效提升对话回复质量。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.

</details>


### [388] [DPRM: A Dual Implicit Process Reward Model in Multi-Hop Question Answering](https://arxiv.org/abs/2511.08364)
*Xinyi Wang,Yiping Song,Zhiliang Tian,Bo Liu,Tingjin Luo,Minlie Huang*

Main category: cs.CL

TL;DR: 提出DPRM解决现有隐式PRM在多跳问答任务中无法处理知识图谱结构约束和捕捉思维链与知识图谱路径不一致的问题，实验效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有隐式PRM在多跳问答任务中无法处理知识图谱的图结构约束，也不能捕捉思维链与知识图谱路径的潜在不一致。

Method: 提出DPRM，为多跳问答任务中的思维链和知识图谱推理分别训练两个隐式PRM，引入一致性约束，相互验证和协作优化推理路径，并给出过程奖励推导的理论证明。

Result: 在多个数据集上优于13个基线，Hit@1最高提升16.6%。

Conclusion: DPRM能有效解决现有隐式PRM在多跳问答任务中的局限，提升推理效果。

Abstract: In multi-hop question answering (MHQA) tasks, Chain of Thought (CoT) improves the quality of generation by guiding large language models (LLMs) through multi-step reasoning, and Knowledge Graphs (KGs) reduce hallucinations via semantic matching. Outcome Reward Models (ORMs) provide feedback after generating the final answers but fail to evaluate the process for multi-step reasoning. Traditional Process Reward Models (PRMs) evaluate the reasoning process but require costly human annotations or rollout generation. While implicit PRM is trained only with outcome signals and derives step rewards through reward parameterization without explicit annotations, it is more suitable for multi-step reasoning in MHQA tasks. However, existing implicit PRM has only been explored for plain text scenarios. When adapting to MHQA tasks, it cannot handle the graph structure constraints in KGs and capture the potential inconsistency between CoT and KG paths. To address these limitations, we propose the DPRM (Dual Implicit Process Reward Model). It trains two implicit PRMs for CoT and KG reasoning in MHQA tasks. Both PRMs, namely KG-PRM and CoT-PRM, derive step-level rewards from outcome signals via reward parameterization without additional explicit annotations. Among them, KG-PRM uses preference pairs to learn structural constraints from KGs. DPRM further introduces a consistency constraint between CoT and KG reasoning steps, making the two PRMs mutually verify and collaboratively optimize the reasoning paths. We also provide a theoretical demonstration of the derivation of process rewards. Experimental results show that our method outperforms 13 baselines on multiple datasets with up to 16.6% improvement on Hit@1.

</details>


### [389] [Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research](https://arxiv.org/abs/2511.08507)
*Neelavro Saha,Rafi Shahriyar,Nafis Ashraf Roudra,Saadman Sakib,Annajiat Alim Rasel*

Main category: cs.CL

TL;DR: 引入新的孟加拉语手语翻译数据集Bangla - SGP，用规则增强数据，并微调多种模型评估翻译性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语手语翻译因缺乏大规模句子级数据集，现有研究局限于单词和字母级检测，需解决句子级翻译问题。

Method: 创建含1000个人工注释句子 - 手语释义对的数据集，用规则增强约3000个合成对；采用规则语言策略和提示工程技术；微调mBart50、Google mT5、GPT4.1 - nano等模型。

Result: 基于BLEU分数评估模型句子到释义的翻译性能。

Conclusion: 通过评估指标比较模型在数据集和基准上的释义翻译一致性。

Abstract: Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a rule-based Retrieval-Augmented Generation (RAG) pipeline. The gloss sequences of the spoken Bangla sentences are made up of individual glosses which are Bangla sign supported words and serve as an intermediate representation for a continuous sign. Our dataset consists of 1000 high quality Bangla sentences that are manually annotated into a gloss sequence by a professional signer. The augmentation process incorporates rule-based linguistic strategies and prompt engineering techniques that we have adopted by critically analyzing our human annotated sentence-gloss pairs and by working closely with our professional signer. Furthermore, we fine-tune several transformer-based models such as mBart50, Google mT5, GPT4.1-nano and evaluate their sentence-to-gloss translation performance using BLEU scores, based on these evaluation metrics we compare the model's gloss-translation consistency across our dataset and the RWTH-PHOENIX-2014T benchmark.

</details>


### [390] [Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models](https://arxiv.org/abs/2511.08565)
*Davi Bastos Costa,Felippe Alves,Renato Vicente*

Main category: cs.CL

TL;DR: 研究大语言模型在角色扮演时的道德反应，引入量化指标，分析模型家族和大小对道德稳健性和敏感性的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在社会环境中应用增多，需分析其道德判断表达和转变。

Method: 使用道德基础问卷（MFQ），引入量化道德敏感性和道德稳健性的基准。

Result: 道德稳健性方面，模型家族影响大，Claude家族最稳健；道德敏感性有家族内大小效应，大模型更敏感，且二者正相关。

Conclusion: 这些分析为角色条件如何塑造大语言模型的道德行为提供了系统视角。

Abstract: Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [391] [Advancing mathematics research with large language models](https://arxiv.org/abs/2511.07420)
*Lisa Carbone*

Main category: math.HO

TL;DR: 指出大语言模型用于高等数学时虽非逻辑推理引擎，但能捕捉人类难见的模式，可通过提示工程助力数学研究，还探讨其与计算机代数系统和形式证明助手的集成。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型用于高等数学时非逻辑推理引擎的缺点，发挥其捕捉模式的优势，推动数学研究。

Method: 通过精心设计提示工程，探讨大语言模型与计算机代数系统和形式证明助手（如Lean）的集成。

Result: 未提及明确具体结果。

Conclusion: 数学家可将大语言模型作为强大的交互式助手，通过提示工程推进数学研究。

Abstract: The main drawback of using generative AI for advanced mathematics via Large Language Models (LLMs) is that they are probabilistic pattern-matchers, not logical reasoning engines. However, LLMs can pick up on patterns in higher mathematics that are difficult for humans to see. By putting the design of LLMs to their advantage, mathematicians may use them as powerful interactive assistants that can carry out laborious tasks, generate and debug code, check examples, formulate conjectures and more. We discuss how LLMs can be used to advance mathematics research by careful use of prompt engineering. We also discuss the integration of LLMs with Computer Algebra Systems and formal proof assistants such as Lean.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [392] [How Fixed-Amount Transactions and Liquidity Constraints Amplify Wealth Inequality: A Kinetic Model Deviating from the Maximum Entropy Benchmark](https://arxiv.org/abs/2511.08202)
*Jihyuan Liuh*

Main category: physics.soc-ph

TL;DR: 本文通过极简动力学交换模型研究财富不平等的出现，发现基本经济约束下平等机会交换会自发产生巨大不平等。


<details>
  <summary>Details</summary>
Motivation: 研究财富不平等的出现机制。

Method: 构建包含固定金额交易和硬预算约束的极简动力学交换模型，发展自洽平均场理论并推导主方程，进行数值求解和基于主体的模拟。

Result: 系统会趋向高度不平等的稳态，稳态分布有大量贫困阶层、高基尼系数和指数尾部，与最大熵基准有显著偏差。

Conclusion: 基本经济约束下平等机会交换能自发产生巨大不平等，为理解贫困是交换规则的涌现属性提供了机制基础。

Abstract: This paper investigates the emergence of wealth inequality through a minimalist kinetic exchange model that incorporates two fundamental economic features: fixed-amount transactions and hard budget constraints. In contrast to the maximum entropy principle, which predicts an exponential Boltzmann-Gibbs distribution with moderate inequality for unconstrained wealth exchange, we demonstrate that these realistic trading rules drive the system toward a highly unequal steady state. We develop a self-consistent mean-field theory, deriving a master equation where agent income follows a Poisson process coupled to the poverty rate. Numerical solution reveals a stationary distribution characterized by a substantial pauper class, high Gini coefficient, and exponential tail--significantly deviating from the maximum entropy benchmark. Agent-based simulations confirm these findings. We identify the poverty trap as the key mechanism: the liquidity constraint creates asymmetric economic agency, where zero-wealth agents become passive recipients, unable to participate in wealth circulation. This work establishes that substantial inequality can emerge spontaneously from equal-opportunity exchanges under basic economic constraints, without requiring agent heterogeneity or multiplicative advantage, providing a mechanistic foundation for understanding poverty as an emergent property of exchange rules.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [393] [GazeCopilot: Evaluating Novel Gaze-Informed Prompting for AI-Supported Code Comprehension and Readability](https://arxiv.org/abs/2511.08177)
*Yasmine Elfares,Gül Çalikli,Mohamed Khamis*

Main category: cs.HC

TL;DR: 提出Real - time GazeCopilot方法，利用开发者实时注视数据优化提示，实验表明该方法能提升代码理解准确性、减少理解时间和提高可读性。


<details>
  <summary>Details</summary>
Motivation: AI编码助手输出质量依赖提示的上下文丰富度，而注视行为含丰富认知信息，可用于改进提示。

Method: 提出Real - time GazeCopilot方法，将注视指标融入提示；通过25名开发者的实验，与Standard Copilot和Pre - set GazeCopilot对比。

Result: 使用实时注视数据动态生成的提示显著提高代码理解准确性、减少理解时间和提高可读性；Real - time GazeCopilot选择性重构代码，优于Pre - set GazeCopilot。

Conclusion: 利用开发者实时注视数据优化提示能有效提升代码理解和可读性，Real - time GazeCopilot方法具有优势。

Abstract: AI-powered coding assistants, like GitHub Copilot, are increasingly used to boost developers' productivity. However, their output quality hinges on the contextual richness of the prompts. Meanwhile, gaze behaviour carries rich cognitive information, providing insights into how developers process code. We leverage this in Real-time GazeCopilot, a novel approach that refines prompts using real-time gaze data to improve code comprehension and readability by integrating gaze metrics, like fixation patterns and pupil dilation, into prompts to adapt suggestions to developers' cognitive states. In a controlled lab study with 25 developers, we evaluated Real-time GazeCopilot against two baselines: Standard Copilot, which relies on text prompts provided by developers, and Pre-set GazeCopilot, which uses a hard-coded prompt that assumes developers' gaze metrics indicate they are struggling with all aspects of the code, allowing us to assess the impact of leveraging the developer's personal real-time gaze data. Our results show that prompts dynamically generated using developers' real-time gaze data significantly improve code comprehension accuracy, reduce comprehension time, and improve perceived readability compared to Standard Copilot. Our Real-time GazeCopilot approach selectively refactors only code aspects where gaze data indicate difficulty, outperforming the overgeneralized refactoring done by Pre-set GazeCopilot by avoiding revising code the developer already understands.

</details>


### [394] [Designing and Evaluating Malinowski's Lens: An AI-Native Educational Game for Ethnographic Learning](https://arxiv.org/abs/2511.07682)
*Michael Hoffmann,Jophin John,Jan Fillies,Adrian Paschke*

Main category: cs.HC

TL;DR: 研究推出首款人类学AI教育游戏'Malinowski's Lens'，结合技术创视觉效果，处理伦理问题，经两项研究验证有效，证明AI教育游戏可传达概念、激发兴趣并给出设计模型。


<details>
  <summary>Details</summary>
Motivation: 将学术文本转化为互动学习体验，解决复杂人类学概念传达问题，推动AI-native教育游戏设计。

Method: 结合Retrieval-Augmented Generation与DALL-E 3文本到图像生成技术，让玩家扮演Malinowski进行互动体验，用剪影处理伦理问题，并开展两项验证研究。

Result: 研究1显示非专家学习成果好、可用性强；研究2得到专家认可， senior researcher有新发现。

Conclusion: AI驱动教育游戏能有效传达复杂人类学概念、激发学科好奇心，该研究推进教育游戏设计并提供可复制模型。

Abstract: This study introduces 'Malinowski's Lens', the first AI-native educational game for anthropology that transforms Bronislaw Malinowski's 'Argonauts of the Western Pacific' (1922) into an interactive learning experience. The system combines Retrieval-Augmented Generation with DALL-E 3 text-to-image generation, creating consistent VGA-style visuals as players embody Malinowski during his Trobriand Islands fieldwork (1915-1918). To address ethical concerns, indigenous peoples appear as silhouettes while Malinowski is detailed, prompting reflection on anthropological representation. Two validation studies confirmed effectiveness: Study 1 with 10 non-specialists showed strong learning outcomes (average quiz score 7.5/10) and excellent usability (SUS: 83/100). Study 2 with 4 expert anthropologists confirmed pedagogical value, with one senior researcher discovering "new aspects" of Malinowski's work through gameplay. The findings demonstrate that AI-driven educational games can effectively convey complex anthropological concepts while sparking disciplinary curiosity. This study advances AI-native educational game design and provides a replicable model for transforming academic texts into engaging interactive experiences.

</details>
