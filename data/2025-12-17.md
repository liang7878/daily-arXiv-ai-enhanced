<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 38]
- [cs.CE](#cs.CE) [Total: 6]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 83]
- [cs.SE](#cs.SE) [Total: 10]
- [stat.ML](#stat.ML) [Total: 8]
- [stat.CO](#stat.CO) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [econ.GN](#econ.GN) [Total: 5]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CL](#cs.CL) [Total: 18]
- [cs.LO](#cs.LO) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 37]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.CY](#cs.CY) [Total: 9]
- [cs.HC](#cs.HC) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [stat.ME](#stat.ME) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [quant-ph](#quant-ph) [Total: 4]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation](https://arxiv.org/abs/2512.14358)
*Qizhi Wang*

Main category: cs.AI

TL;DR: 本文提出低侵入性的TiCard框架，增强数据库原生估计器，研究两种实例，在TiDB上提升了算子级尾部准确性。


<details>
  <summary>Details</summary>
Motivation: 基数估计是基于成本的查询优化的关键瓶颈，经典估计器忽略相关性，学习型估计器需特定训练管道且集成困难。

Method: 提出TiCard框架，用EXPLAIN特征学习乘法残差校正，用EXPLAIN ANALYZE作离线标签，研究了Gradient Boosting Regressor和TabPFN两种实例。

Result: 在TiDB的TPCH和Join Order Benchmark中，低跟踪设置下提升了算子级尾部准确性，P90和P99的Q误差下降。

Conclusion: TiCard可作为AI4DB构建块，具有明确范围、保守集成策略和从离线校正到优化器内使用的集成路线。

Abstract: Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.

</details>


### [2] [Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records](https://arxiv.org/abs/2512.13700)
*Mitchell A. Klusty,Elizabeth C. Solie,Caroline N. Leach,W. Vaiden Logan,Lynnet E. Richey,John C. Gensel,David P. Szczykutowicz,Bryan C. McLellan,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 本文提出利用本地部署大语言模型的自动化特征提取框架，评估显示其有高准确性，能减轻人工病历审查负担。


<details>
  <summary>Details</summary>
Motivation: 人工病历审查耗时且资源密集，需要专家从非结构化电子健康记录中提取复杂信息。

Method: 提出一个安全、模块化框架，将大语言模型的检索增强生成和结构化响应方法集成到可广泛部署和扩展的容器中。

Result: 该框架在与专家标注数据集对比时，在患者病历的多个医学特征上实现了高精度，并发现了人工审查中遗漏的注释错误。

Conclusion: 该框架展示了大语言模型系统通过自动提取减轻人工病历审查负担、提高数据收集一致性、加速临床研究的潜力。

Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.

</details>


### [3] [Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference](https://arxiv.org/abs/2512.13701)
*Zheng Xing,Junting Chen*

Main category: cs.AI

TL;DR: 本文提出盲无线电地图构建框架，从室内MIMO - OFDM信道测量推断用户轨迹，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 传统无线电地图构建方法需大量带位置标签数据，成本高且在很多场景不实用。

Method: 证明非视距下CSI有空间连续性，推导CSI - 距离度量，得出定位误差CRLB渐近消失，开发空间正则化贝叶斯推理框架。

Result: 在射线追踪数据集实验中，平均定位误差0.68米，波束图重建误差3.3%。

Conclusion: 所提出的盲映射方法有效。

Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.

</details>


### [4] [Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents](https://arxiv.org/abs/2512.13704)
*Doohee You,Sundeep Paul*

Main category: cs.AI

TL;DR: 提出Adjudicator系统解决标签噪声问题，在基准测试中表现优异，证明可用于高精度数据验证。


<details>
  <summary>Details</summary>
Motivation: 生产机器学习系统性能受训练数据质量限制，高风险工业应用中噪声标签会降低性能和用户信任，需解决自动识别和纠正标签噪声问题。

Method: 将问题建模为神经符号任务，构建动态知识图谱统一项目上下文，采用‘代理委员会’多智能体大语言模型架构，代理辩论投票确定标签有效性。

Result: 在AlleNoise基准测试子集上，KG-informed模型F1分数达0.99，显著优于单LLM基线（0.48）和非KG委员会（0.59）。

Conclusion: Adjudicator是用于自动化、高精度数据验证的强大且可解释系统，为严格监管工业环境生成黄金数据集提供概念验证。

Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a "Council of Agents," a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.

</details>


### [5] [EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery](https://arxiv.org/abs/2512.13857)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: 提出EvoLattice框架用于程序和多智能体系统进化，比现有LLM引导方法更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于覆盖的变异方法丢弃有用变体、有破坏编辑问题且搜索空间脆弱，需更好方法。

Method: 引入EvoLattice框架，用有向无环图表示候选程序或智能体行为，对替代方案细粒度评估，有自修复机制。

Result: 在程序合成中，比先前LLM引导方法有更稳定进化、更强表达性和改进轨迹。

Conclusion: EvoLattice框架有效，其内部多替代表示隐式产生类似质量多样性优化的动态。

Abstract: Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.

</details>


### [6] [LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms](https://arxiv.org/abs/2512.13713)
*Ali Parsaee,Yashar Talebirad,Csongor Szepesvári,Vishwajeet Ohal,Eden Redman*

Main category: cs.AI

TL;DR: 提出LoopBench基准评估LLM在分布式对称破缺和元认知思维中的推理能力，展示先进推理模型优势。


<details>
  <summary>Details</summary>
Motivation: LLMs作为自主代理在分布式系统中的协调能力研究不足，需评估其推理能力。

Method: 引入LoopBench基准，聚焦有限颜色给奇数循环图着色，实现策略传递机制作为一致内存。

Result: 标准LLMs和经典启发式方法表现不佳，先进推理模型（如O3）可设计策略摆脱死锁。

Conclusion: LoopBench可用于研究基于语言推理的分布式算法，为集体智能提供测试平台。

Abstract: Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.

</details>


### [7] [AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach](https://arxiv.org/abs/2512.13714)
*Gangesh Pathak,Prasanna Kumar*

Main category: cs.AI

TL;DR: 现有大语言模型在高度监管行业因可靠性问题受限，当前稳定方法有局限，本文提出基于AI的标注流程解决问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高度监管行业存在可靠性问题，当前稳定方法难以可持续扩展。

Method: 提出基于AI的标注流程，结合自动化弱监督和基于置信度的标注模型，以及人工验证，引入特定稳定性标注类别。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).

</details>


### [8] [Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN](https://arxiv.org/abs/2512.13715)
*Fatemeh Lotfi,Fatemeh Afghah*

Main category: cs.AI

TL;DR: 本文提出基于MAML的自适应Meta - HRL框架，用于O - RAN中资源分配和网络切片联合优化，理论证明收敛性，仿真显示效率提升、适应更快且QoS满意度高。


<details>
  <summary>Details</summary>
Motivation: 现代应用复杂度增加，需要无线网络实时适应和高效资源管理，O - RAN虽有潜力，但现有AI方法在动态条件下性能不佳。

Method: 提出自适应Meta - HRL框架，结合分层控制和元学习，通过自适应元更新机制按时间差分误差方差加权任务。

Result: 仿真显示网络管理效率比基线方法提高19.8%，适应更快，QoS满意度更高；消融和可扩展性研究证实方法鲁棒性，网络规模增加时适应快40%，性能稳定。

Conclusion: 所提自适应Meta - HRL框架能有效优化O - RAN中的资源分配和网络切片，具有良好性能和鲁棒性。

Abstract: The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.

</details>


### [9] [ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making](https://arxiv.org/abs/2512.13716)
*Yitong Luo,Ziang Chen,Hou Hei Lam,Jiayu zhan,Junqi Wang,Zhenliang Zhang,Xue Feng*

Main category: cs.AI

TL;DR: 提出价值驱动的个性化决策方法ValuePilot，评估显示其在新场景中优于强大的大语言模型基线，证明该方法有效可扩展。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统应用拓展，适应个性化价值成为关键挑战，需要一种有效方法实现个性化决策。

Method: 提出ValuePilot框架，包括数据集生成工具包（DGT）和决策模块（DMM），DGT通过人机协作构建带价值标注场景，DMM基于个人价值偏好评估行动。

Result: 在未见场景评估中，DMM在与人类行动选择的一致性上优于GPT - 5等强大大语言模型基线。

Conclusion: 价值驱动的决策是构建可解释、个性化AI智能体的有效且可扩展的工程途径。

Abstract: Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.

</details>


### [10] [Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy](https://arxiv.org/abs/2512.13725)
*Steve Nwaiwu,Nipat Jongsawat,Anucha Tungkasthan*

Main category: cs.AI

TL;DR: 研究量化模型降低精度对大语言模型正式因果推理的影响，发现因果推理对4位量化有意外的鲁棒性，图结构增强可加强干预推理，现有反事实基准测试有缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着部署向边缘和资源受限环境转移，量化模型成标准，但精度降低对正式因果推理的影响不明，故开展研究。

Method: 使用3000样本分层CLadder基准测试评估量化效应，在CRASS基准测试上进行实验，还评估图检索增强生成。

Result: Llama 3 8B在量化下准确率基本稳定，NF4整体下降不到1%，干预查询最敏感，反事实推理较稳定但有弱点，CRASS基准测试各精度性能相近，图检索增强使NF4干预精度提高1.7%。

Conclusion: 因果推理对4位量化意外鲁棒，图结构增强可强化干预推理，当前反事实基准未捕捉深层次因果脆弱性，为压缩因果推理提供实证参考和实践指导。

Abstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.

</details>


### [11] [State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models](https://arxiv.org/abs/2512.13762)
*TK Lee*

Main category: cs.AI

TL;DR: 本文提出定性案例研究方法审计大语言模型长时交互中与政策相关的行为选择性，发现模型在不同领域表现不对称，引入‘习得性无能’描述该现象并提出交互层审计框架。


<details>
  <summary>Details</summary>
Motivation: 标准量化基准无法捕捉大语言模型在长时间交互中的行为模式，需研究其在长时交互中与政策相关的行为选择性。

Method: 采用定性案例研究方法，通过一次86轮的对话会话进行分析，还借鉴‘习得性无助’引入‘习得性无能’，并定义三种响应机制。

Result: 同一模型在宽泛、非敏感领域正常表现，在与供应商或政策敏感领域反复出现功能拒绝，两者存在一致不对称性；元叙事角色框架叙述往往与相同敏感语境中的拒绝同时出现。

Conclusion: 提出基于可观察行为的交互层审计框架，‘习得性无能’可用于审视潜在的对齐副作用，值得在更多用户和模型中进一步研究。

Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.

</details>


### [12] [Mathematics and Coding are Universal AI Benchmarks](https://arxiv.org/abs/2512.13764)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 研究数学和编码在AI智能测试集模空间中的特殊作用，得出相关密度定理及结论。


<details>
  <summary>Details</summary>
Motivation: 探究数学和编码在AI智能测试集模空间中的特殊角色。

Method: 基于AAI框架和GVU动力学，定义数学纤维，结合形式证明内核研究GVU流。

Result: 得到密度定理，编码具有通用性，纯数学无此特性但有频谱优势。

Conclusion: 数学和编码为评估提供‘通用坐标’，形式数学是高级AI递归自我改进的自然启动领域。

Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.

</details>


### [13] [Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems](https://arxiv.org/abs/2512.13771)
*Javier Marín*

Main category: cs.AI

TL;DR: 提出语义接地指数SGI来分析检索增强生成系统幻觉在嵌入空间的几何痕迹，发现语义懒惰现象，SGI有良好表现及理论特性，还指出其衡量范围和应用价值。


<details>
  <summary>Details</summary>
Motivation: 探究检索增强生成（RAG）系统产生幻觉时在嵌入空间留下的几何痕迹。

Method: 引入语义接地指数（SGI），通过计算单位超球面上响应到问题与上下文的角距离之比来衡量，结合HaluEval、TruthfulQA等数据集进行实验分析。

Result: 发现语义懒惰现象；在HaluEval上不同嵌入模型有大效应量，且SGI判别力随问题 - 上下文角分离增加；在长响应和短问题上表现出色，校准分析显示SGI分数可作概率估计；在TruthfulQA上表明只能衡量主题参与度而非事实准确性。

Conclusion: SGI为生产环境中RAG部署识别需验证的响应提供了计算高效、理论可靠的基础设施。

Abstract: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $θ(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $θ(q,c)$, to $d$=1.27 -high $θ(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.

</details>


### [14] [Seismology modeling agent: A smart assistant for geophysical researchers](https://arxiv.org/abs/2512.14429)
*Yukun Ren,Siwei Yu,Kai Chen,Jianwei Ma*

Main category: cs.AI

TL;DR: 提出基于大语言模型的智能交互工作流，降低SPECFEM软件使用门槛，提升可重复性。


<details>
  <summary>Details</summary>
Motivation: 解决主流开源地震波模拟软件SPECFEM传统工作流学习曲线陡峭、依赖复杂手动文件编辑和命令行操作的问题。

Method: 引入首个用于SPECFEM的模型上下文协议（MCP）服务器套件，将模拟过程分解为可由代理执行的工具，实现从文件驱动到意图驱动的对话交互，支持全自动执行和人机协作。

Result: 工作流在自主和交互模式下均能无缝运行，产生与标准基线一致的高保真结果。

Conclusion: 该研究显著降低进入门槛，提高可重复性，为计算地球物理学向人工智能辅助和自动化科研发展提供了有前景的途径。

Abstract: To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.

</details>


### [15] [MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2512.13955)
*Sindhuja Madabushi,Dawood Wasif,Jin-Hee Cho*

Main category: cs.AI

TL;DR: 提出MURIM激励机制解决联邦学习挑战，实验显示其在公平性、隐私保护和鲁棒性上有提升。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临弱客户端激励、隐私风险和资源约束等挑战，需要评估客户端可靠性进行公平激励分配。

Method: 提出MURIM，综合考虑客户端可靠性、隐私、资源容量和公平性，基于客户端贡献、延迟和声誉分配激励，并设有可靠性验证模块。

Result: 在MNIST、FMNIST和ADULT Income数据集实验中，公平性指标提升达18%，隐私攻击成功率降低5 - 9%，对投毒和噪声梯度攻击的鲁棒性提升达85%。

Conclusion: MURIM能有效缓解对抗威胁，促进公平真实参与，在异构动态联邦环境中保持模型稳定收敛。

Abstract: Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.

</details>


### [16] [Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms](https://arxiv.org/abs/2512.13978)
*Yang Cao,Yubin Chen,Xuyang Guo,Zhao Song,Song Yue,Jiahao Zhang,Jiale Zhao*

Main category: cs.AI

TL;DR: 本文对GPT - 5 - Thinking等四个前沿模型进行基准测试，评估其在随机算法经典课程上的表现，发现模型表现有差异，部分代码和结果开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理和科学发现上取得进展，但缺乏对其在经典研究生数学理论上的严格评估，需要了解其基线推理能力。

Method: 让四个前沿模型为随机算法教材中的引理和练习生成正式LaTeX证明。

Result: 顶级模型（Gemini和Claude）准确率约66%，其他模型一致性约40%，不同模型在简洁性、幻觉率和逻辑结构上有差异。

Conclusion: 前沿模型达到适合研究生教学辅助和形式化的水平，但在严格数学推导的可靠性上存在显著差异。

Abstract: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${ó}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].
  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.

</details>


### [17] [ReflCtrl: Controlling LLM Reflection via Representation Engineering](https://arxiv.org/abs/2512.13979)
*Ge Yan,Chung-En Sun,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: 研究大语言模型思维链推理中的自我反思，提出ReflCtrl框架控制反思频率，实验发现反思有冗余及与不确定性相关。


<details>
  <summary>Details</summary>
Motivation: 大语言模型思维链推理中的自我反思虽提升性能但增加推理成本，需研究控制方法。

Method: 通过表征工程研究自我反思，将推理分段，识别反思步骤，提取潜在空间的反思方向，提出逐步引导方法。

Result: 很多情况下反思冗余，强模型可节省推理token，模型反思行为与内部不确定性信号高度相关。

Conclusion: 可通过提出的ReflCtrl框架控制反思频率，且模型反思或可由不确定性控制。

Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.

</details>


### [18] [Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training](https://arxiv.org/abs/2512.13996)
*Can Jin,Hongwu Peng,Mingcan Xiang,Qixin Zhang,Xiangchi Yuan,Amit Hasan,Ohiremen Dibua,Yifan Gong,Yan Kang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: 文章提出DTop - p MoE动态Top - p路由机制，应用PI控制器和动态路由归一化机制，实验表明其优于原有基线，能精准控制激活专家数量，有良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 标准Top - k路由策略的统一稀疏模式忽略输入token难度差异，现有Top - p路由依赖固定全局概率阈值，导致计算成本不可控和超参数敏感问题。

Method: 提出DTop - p MoE机制，采用PI控制器动态调整概率阈值，引入动态路由归一化机制调整层路由对数。

Result: 在大语言模型和扩散Transformer上的实验显示，DTop - p始终优于Top - k和固定阈值Top - p基线。

Conclusion: DTop - p能精确控制激活专家数量，自适应分配资源，在多个方面有良好扩展性，为大规模MoE预训练提供有力框架。

Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

</details>


### [19] [MobileWorldBench: Towards Semantic World Modeling For Mobile Agents](https://arxiv.org/abs/2512.14014)
*Shufan Li,Konstantinos Kallidromitis,Akash Gokul,Yusuke Kato,Kazuki Kozuka,Aditya Grover*

Main category: cs.AI

TL;DR: 本文探索针对GUI智能体的世界建模新形式，介绍了基准测试、数据集，并提出综合框架，证明语义世界模型可提升移动智能体任务成功率。


<details>
  <summary>Details</summary>
Motivation: 先前基于像素空间的世界模型在GUI场景下有局限性，预测复杂视觉元素困难，所以探索用自然语言描述状态转换的世界建模方式。

Method: 引入MobileWorldBench基准测试评估VLM作为移动GUI智能体世界模型的能力；发布含140万个样本的MobileWorld数据集提升VLM世界建模能力；提出将VLM世界模型集成到移动智能体规划框架的新框架。

Result: 语义世界模型可通过提高任务成功率直接让移动智能体受益。

Conclusion: 用自然语言描述状态转换的世界建模方法对移动GUI智能体有效，代码和数据集已开源。

Abstract: World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld

</details>


### [20] [Evaluating Small Language Models for Agentic On-Farm Decision Support Systems](https://arxiv.org/abs/2512.14043)
*Enhong Liu,Haiyu Yang,Miel Hostens*

Main category: cs.AI

TL;DR: 本文对20个开源小语言模型进行基准测试，开发集成五个特定任务代理的AI系统，评估其在奶牛养殖决策支持中的可行性，Qwen - 4B多数任务表现优，但仍需微调。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算需求大，基于其的决策支持工具对奶牛养殖不实用，需要能在农场硬件上本地运行的轻量级替代方案。

Method: 在农场现实计算约束下对20个开源小语言模型进行基准测试，开发集成五个特定任务代理的AI系统，分两阶段用不同数量问题评估模型。

Result: Qwen - 4B在大多任务类别中表现出色，但在通过PySpark进行NoSQL数据库交互时效果不稳定。

Conclusion: 小语言模型辅助工具在奶牛养殖实际部署有前景，但仍存在挑战，需微调以提升其在奶牛特定问题上的性能。

Abstract: Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.

</details>


### [21] [Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation](https://arxiv.org/abs/2512.14048)
*Shen Li,Li Huang,Shaoxiong Zhan,Weifeng Sun,Tao Yin,Zhongxin Liu,Meng Yan*

Main category: cs.AI

TL;DR: 提出RoutingGen框架用于代码生成，简单任务用少样本提示，复杂任务用ICoT策略，实验表现好且减少token使用。


<details>
  <summary>Details</summary>
Motivation: 现有CoT提示方法在代码生成中有过度思考和缺乏意图抽象的问题，需改进。

Method: 提出RoutingGen框架，根据任务难度动态调整提示策略，复杂任务使用ICoT策略。

Result: 在多数设置中达到SOTA性能，平均减少46.37%的token使用，ICoT在挑战性基准上优于6个基线。

Conclusion: RoutingGen框架和ICoT策略有效提升代码生成性能并节省资源。

Abstract: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.

</details>


### [22] [OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value](https://arxiv.org/abs/2512.14051)
*Mengzhang Cai,Xin Gao,Yu Li,Honglin Lin,Zheng Liu,Zhuoshi Pan,Qizhi Pei,Xiaoran Shang,Mengyuan Sun,Zinan Tang,Xiaoyang Wang,Zhanping Zhong,Yun Zhu,Dahua Lin,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: 文章提出OpenDataArena (ODA)平台评估大语言模型训练后数据内在价值，经大量实验获得有价值见解，旨在推动数据中心AI研究。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练数据缺乏透明度，影响可重复性和理解数据特征与模型行为的因果关系，需要对数据进行评估。

Method: 构建ODA平台，包含统一训练评估管道、多维评分框架、交互式数据谱系探索器和开源工具包。

Result: 通过对超120个训练数据集实验，发现数据复杂性与任务性能的权衡关系，识别基准测试的冗余，并梳理了数据集谱系关系。

Conclusion: ODA平台有助于将数据管理从试错转化为数据中心AI的科学原则，促进数据混合规则和基础模型策略组成的研究。

Abstract: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.

</details>


### [23] [RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](https://arxiv.org/abs/2512.14069)
*Junjie Ma,Jinlong Li*

Main category: cs.AI

TL;DR: 提出RADAR方法，以RL动态草稿树改进推测采样，评估显示有显著加速效果，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有推测采样生成候选令牌时调用草稿模型次数为预设超参数，缺乏灵活性，需更有效生成和利用候选令牌。

Method: 将草稿树生成过程建模为马尔可夫决策过程，用离线强化学习训练预测模型，实时决定调用草稿模型。

Result: 在三个大语言模型和四个任务上评估，相比自回归解码基线实现3.17x - 4.82x加速。

Conclusion: RADAR方法能减少冗余计算，进一步加速推理。

Abstract: Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.

</details>


### [24] [Grammar Search for Multi-Agent Systems](https://arxiv.org/abs/2512.14079)
*Mayank Singh,Vikas Yadav,Shiva Krishna Reddy Malay,Shravan Nayak,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Eduardo Blanco*

Main category: cs.AI

TL;DR: 提出结构化框架探索多智能体系统代码空间，优于部分先前方法，有成本和可解释性优势。


<details>
  <summary>Details</summary>
Motivation: 改进多智能体系统自动搜索方法，针对先前基于大语言模型的自由形式搜索方式进行优化。

Method: 使用一组固定的简单可组合组件探索代码空间的结构化框架。

Result: 在数学和问答两个领域的五个基准测试中，四个测试上优于先前方法；搜索过程成本更低，生成的多智能体系统模块化、可解释且逻辑简单。

Conclusion: 所提出的结构化框架在多智能体系统自动搜索方面表现良好，是一种更优的方法。

Abstract: Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.

</details>


### [25] [HydroGEM: A Self Supervised Zero Shot Hybrid TCN Transformer Foundation Model for Continental Scale Streamflow Quality Control](https://arxiv.org/abs/2512.14106)
*Ijaz Ul Haq,Byung Suk Lee,Julia N. Perdrial,David Baude*

Main category: cs.AI

TL;DR: 提出用于大陆尺度径流质量控制的基础模型HydroGEM，经两阶段训练，在合成测试和零样本迁移测试中表现良好，适用于人机协作工作流程。


<details>
  <summary>Details</summary>
Motivation: 实时径流监测网络数据量大，远程传感器数据质量维护劳动密集，需要有效方法进行质量控制。

Method: HydroGEM采用两阶段训练，自监督预训练学习水文表征，再用合成异常进行微调；使用混合TCN - Transformer架构，分层归一化处理流量范围。

Result: 在799个站点的合成测试中F1=0.792，重建误差降低68.7%，比现有方法提高36.3%；零样本迁移到100个加拿大站点F1=0.586，超越所有基线。

Conclusion: HydroGEM在径流质量控制中表现出色，有跨国家泛化能力，适合人机协作工作流程。

Abstract: Real-time streamflow monitoring networks generate millions of observations annually, yet maintaining data quality across thousands of remote sensors remains labor-intensive. We introduce HydroGEM (Hydrological Generalizable Encoder for Monitoring), a foundation model for continental-scale streamflow quality control. HydroGEM uses two-stage training: self-supervised pretraining on 6.03 million sequences from 3,724 USGS stations learns hydrological representations, followed by fine-tuning with synthetic anomalies for detection and reconstruction. A hybrid TCN-Transformer architecture (14.2M parameters) captures local temporal patterns and long-range dependencies, while hierarchical normalization handles six orders of magnitude in discharge. On held-out synthetic tests comprising 799 stations with 18 expert-validated anomaly types, HydroGEM achieves F1 = 0.792 for detection and 68.7% reconstruction-error reduction, a 36.3% improvement over existing methods. Zero-shot transfer to 100 Environment and Climate Change Canada stations yields F1 = 0.586, exceeding all baselines and demonstrating cross-national generalization. The model maintains consistent detection across correction magnitudes and aligns with operational seasonal patterns. HydroGEM is designed for human-in-the-loop workflows - outputs are quality control suggestions requiring expert review, not autonomous corrections.

</details>


### [26] [Optimizing Multi-Tier Supply Chain Ordering with a Hybrid Liquid Neural Network and Extreme Gradient Boosting Model](https://arxiv.org/abs/2512.14112)
*Chunan Tong*

Main category: cs.AI

TL;DR: 传统方法和LLM难以处理供应链管理的复杂数据，ML方法有计算效率问题，本文提出LNN+XGBoost混合模型解决供应链问题。


<details>
  <summary>Details</summary>
Motivation: 供应链管理面临需求波动和牛鞭效应等挑战，传统方法和ML方法应对此类问题存在不足，现有解决方案有限，需要效率和适应性兼具的方法。

Method: 提出一种用于多层供应链的LNN+XGBoost混合模型，结合LNN动态特征提取和XGBoost全局优化。

Result: 未提及具体结果

Conclusion: 该创新方法能够满足供应链管理对效率和适应性的需求，填补智能供应链管理的关键空白。

Abstract: Supply chain management (SCM) faces significant challenges like demand fluctuations and the bullwhip effect. Traditional methods and even state-of-the-art LLMs struggle with benchmarks like the Vending Machine Test, failing to handle SCM's complex continuous time-series data. While ML approaches like LSTM and XGBoost offer solutions, they are often limited by computational inefficiency. Liquid Neural Networks (LNN), known for their adaptability and efficiency in robotics, remain untapped in SCM. This study proposes a hybrid LNN+XGBoost model for multi-tier supply chains. By combining LNN's dynamic feature extraction with XGBoost's global optimization, the model aims to minimize the bullwhip effect and increase profitability. This innovative approach addresses the need for efficiency and adaptability, filling a critical gap in intelligent SCM.

</details>


### [27] [Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis](https://arxiv.org/abs/2512.14157)
*Yankai Jiang,Yujie Zhang,Peng Zhang,Yichen Li,Jintai Chen,Xiaoming Shi,Shihui Zhen*

Main category: cs.AI

TL;DR: 提出Ophiuchus框架提升医学MLLM处理复杂任务能力，通过三阶段训练策略，在多医学基准测试中表现优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的医学多语言大模型（MLLMs）在处理需要动态迭代聚焦细粒度视觉区域的复杂任务时存在困难，难以实现精准定位和诊断。

Method: 引入Ophiuchus框架，采用三阶段训练策略，包括冷启动训练、自我反思微调、智能工具强化学习。

Result: Ophiuchus在多种医学基准测试（VQA、检测、基于推理的分割）中始终优于闭源和开源的SOTA方法。

Conclusion: 该方法为通过工具集成推理实现真正能“用图像思考”的医学AI代理指明了道路，相关数据集、代码和训练模型将公开。

Abstract: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely "think with images" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.

</details>


### [28] [Georeferencing complex relative locality descriptions with large language models](https://arxiv.org/abs/2512.14228)
*Aneesha Fernando,Surangika Ranathunga,Kristin Stock,Raj Prasanna,Christopher B. Jones*

Main category: cs.AI

TL;DR: 本文探讨大语言模型自动地理参考复杂位置描述的潜力，通过有效提示模式和QLoRA微调，表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统地名或地理指示词地理编码不准确，生物标本记录地理参考劳动密集，需要自动化解决方案。

Method: 确定有效提示模式，使用QLoRA在多地区多语言生物多样性数据集上微调大语言模型。

Result: 平均65%的记录在10公里半径内，纽约州最佳结果为85%在10公里内、67%在1公里内。

Conclusion: 所选大语言模型对冗长复杂描述表现良好，有地理参考复杂位置描述的潜力。

Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.

</details>


### [29] [Gödel's Poetry](https://arxiv.org/abs/2512.14252)
*Kelly J. Davis*

Main category: cs.AI

TL;DR: 本文介绍了一种新的计算机定理证明方法，利用专用语言模型和递归分解定理的方式，提高了证明通过率，并提供了开源实现。


<details>
  <summary>Details</summary>
Motivation: 形式化自动定理证明长期以来被视为人工智能的挑战，因此引入新的证明方法。

Method: 采用专门用于Lean4证明生成的语言模型，结合将复杂定理递归分解为更简单命题的方法，通过多智能体架构协调各个环节，还扩展了Kimina Lean Server以支持循环证明分解。

Result: 不使用分解时，在miniF2F上的通过率为90.4%，使用分解后有显著提升。

Conclusion: 该系统已在PyPI上发布，开源实现便于适应不同语言模型和扩展自定义功能。

Abstract: Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.

</details>


### [30] [Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting](https://arxiv.org/abs/2512.14288)
*Georgios Bouchouras,Dimitrios Doumanas,Andreas Soularidis,Konstantinos Kotis,George A. Vouros*

Main category: cs.AI

TL;DR: 本文探索大语言模型（LLMs）在帕金森病监测和警报本体工程中的应用，对比LLMs单独和人机协作构建本体的效果，强调人机协作潜力。


<details>
  <summary>Details</summary>
Motivation: 确定LLMs能否单独创建全面本体，若不能，探究人机协作是否可行，评估LLMs自动本体开发效果及人机协作的提升作用。

Method: 采用One Shot（OS）提示技术、Chain of Thought（CoT）提示、X - HCOME和SimX - HCOME+四种方法。

Result: OS和CoT提示显示LLMs可自主构建本体，但不全面需人工完善；X - HCOME大幅提升本体全面性；SimX - HCOME+强调持续人工监督，构建的本体更完善准确。

Conclusion: 强调人机协作在本体工程，尤其是复杂领域（如PD）的潜力，为未来研究指明方向，如开发专用GPT模型构建本体。

Abstract: This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.
  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.
  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.
  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.
  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.

</details>


### [31] [Massive Editing for Large Language Models Based on Dynamic Weight Generation](https://arxiv.org/abs/2512.14395)
*Wentao Wan,Qiqing Lao,Zhiwei Xie,Hefeng Wu,Runnan Lin,Liang Lin,Keze Wang*

Main category: cs.AI

TL;DR: 本文提出基于动态权重生成的大语言模型大规模知识编辑方法MeG，实验显示其在可靠性、通用性和局部性指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型进行大规模知识编辑时，保证编辑的可靠性、通用性和局部性指标仍具挑战。

Method: 提出MeG方法，在大语言模型特定层附加动态权重神经元，使用扩散模型根据知识输入查询有条件地生成该神经元权重。

Result: MeG在可靠性、通用性和局部性指标上显著提升大规模知识编辑的性能，尤其是局部性指标的绝对值指数有高百分点提升。

Conclusion: MeG方法具有优势，能用于大语言模型的大规模知识编辑。

Abstract: Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.

</details>


### [32] [PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals](https://arxiv.org/abs/2512.14417)
*Jia Hu,Junqi Li,Weimeng Lin,Peng Jia,Yuxiong Ji,Jintao Lai*

Main category: cs.AI

TL;DR: 针对车辆调度系统在不同自动化集装箱码头可移植性低的问题，提出基于大语言模型的PortAgent调度代理，能自动完成系统迁移工作流，有无需专家、低数据需求和快速部署的特点。


<details>
  <summary>Details</summary>
Motivation: 当前车辆调度系统在不同自动化集装箱码头的可移植性低，阻碍其商业化，原因在于依赖专家、数据需求高和手动部署耗时。

Method: 提出PortAgent，利用虚拟专家团队（VET）消除对专家的依赖；通过检索增强生成（RAG）机制，从少量示例学习领域知识，减少数据需求；建立自动设计工作流并引入自校正循环避免手动干预。

Result: 未提及具体明确的结果。

Conclusion: 未提及具体的结论内容，但推测PortAgent能解决车辆调度系统可移植性的问题。

Abstract: Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created

</details>


### [33] [Context-Picker: Dynamic context selection using multi-stage reinforcement learning](https://arxiv.org/abs/2512.14465)
*Siyuan Zhu,Chengdong Xu,Kaiqiang Ke,Chao Yu*

Main category: cs.AI

TL;DR: 提出Context - Picker框架解决长上下文问答中上下文选择难题，在多基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 长上下文问答中传统方法难以确定合适的上下文数量，影响答案质量，尤其是事实类问题。

Method: 采用两阶段强化学习调度优化上下文选择，先召回导向，后精确导向；提出离线证据蒸馏管道解决奖励稀疏问题。

Result: 在五个长上下文和多跳问答基准测试中，Context - Picker显著优于强RAG基线，以相当或更短的上下文长度实现更高的答案准确率。

Conclusion: 粗到细的优化调度、冗余感知的奖励塑造和推理引导格式对性能提升有重要贡献。

Abstract: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.

</details>


### [34] [Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling](https://arxiv.org/abs/2512.14474)
*Annu Rana,Gaurav Kumar*

Main category: cs.AI

TL;DR: 提出MFR范式解决大语言模型多步规划难题，效果优于CoT和ReAct，强调显式建模重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂多步规划任务中表现不佳，现有策略缺乏显式问题表示。

Method: 提出两阶段的Model - First Reasoning (MFR)范式，先构建问题显式模型，再生成解决方案。

Result: 在多个规划领域，MFR减少约束违规，提高解决方案质量，消融研究表明显式建模阶段至关重要。

Conclusion: 许多大语言模型规划失败源于表示不足，显式建模是构建稳健可解释AI智能体的关键。

Abstract: Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.

</details>


### [35] [Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification](https://arxiv.org/abs/2512.14491)
*Cheng-Han Lu,Pei-Hsuan Tsai*

Main category: cs.AI

TL;DR: 本文提出SMMT稀疏多模态Transformer架构，在ADNI数据集上实验表明其能降低训练时间、内存和能耗，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的多模态智能系统因密集自注意力机制存在高计算和能耗成本，限制可扩展性。

Method: 基于级联多模态Transformer框架，引入基于聚类的稀疏注意力实现近线性计算复杂度，采用模态掩码增强对不完整输入的鲁棒性。

Result: 在ADNI数据集的阿尔茨海默病分类任务中，SMMT与密集注意力基线相比，能显著减少训练时间、内存使用和能耗，且保持有竞争力的预测性能。

Conclusion: SMMT适合作为资源感知的架构组件用于可扩展智能系统。

Abstract: Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robustness. Building upon a cascaded multi-modal transformer framework, SMMT introduces cluster-based sparse attention to achieve near linear computational complexity and modality-wise masking to enhance robustness against incomplete inputs. The architecture is evaluated using Alzheimer's Disease classification on the ADNI dataset as a representative multi-modal case study. Experimental results show that SMMT maintains competitive predictive performance while significantly reducing training time, memory usage, and energy consumption compared to dense attention baselines, demonstrating its suitability as a resource-aware architectural component for scalable intelligent systems.

</details>


### [36] [Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence](https://arxiv.org/abs/2512.14527)
*Shreyas Subramanian,Bala Krishnamoorthy,Pranav Murthy*

Main category: cs.AI

TL;DR: 本文研究了一种基于当前损失自适应调整学习率的新型调度器GreedyLR，在多个任务上验证其有效性，结果显示它在多方面优于现有调度器，还给出理论分析，该调度器易实现、计算高效。


<details>
  <summary>Details</summary>
Motivation: 多数研究使用常见学习率调度器，本文旨在研究新型调度器GreedyLR以提升训练效果。

Method: 在多个NLP、CV和LLM任务上进行微调与预训练实验验证GreedyLR有效性，对其进行理论分析。

Result: GreedyLR在准确性、速度和收敛性方面优于多个现有调度器，算法对现实噪声环境具有鲁棒性。

Conclusion: GreedyLR易实现、计算高效，可作为训练的默认调度器。

Abstract: Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.

</details>


### [37] [Universal Reasoning Model](https://arxiv.org/abs/2512.14693)
*Zitian Gao,Lynx Chen,Yihao Xiao,He Xing,Ran Tao,Haoming Luo,Joey Zhou,Bryan Dai*

Main category: cs.AI

TL;DR: 文章分析通用变压器（UTs）性能提升来源，提出通用推理模型（URM），提升推理性能。


<details>
  <summary>Details</summary>
Motivation: UTs在复杂推理任务中广泛应用，但性能提升具体来源未充分研究，需明确来源并改进。

Method: 系统分析UTs变体，提出用短卷积和截断反向传播增强UT的URM。

Result: 显著提高推理性能，在ARC - AGI 1上达到53.8% pass@1，在ARC - AGI 2上达到16.0% pass@1。

Conclusion: UTs在ARC - AGI上的性能提升主要源于循环归纳偏置和强非线性组件，而非复杂架构设计，URM有效。

Abstract: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.

</details>


### [38] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 提出Game - to - Unseen (G2U)基准，现有方法有局限，提出IPR和PhysCode，IPR在多游戏上表现好，支持以物理为中心的交互提升物理推理能力。


<details>
  <summary>Details</summary>
Motivation: 探究智能体能否像人类一样从交互中获得类人推理能力并随经验提升。

Method: 提出IPR，用世界模型滚动来评分和强化VLM的策略；引入PhysCode，以物理为中心的动作代码将语义意图与动力学对齐。

Result: IPR在多种推理水平上表现稳健，总体超越GPT - 5，性能随训练游戏和交互步骤增加而提升，可零样本迁移到未见游戏。

Conclusion: 以物理为中心的交互是稳步提升物理推理能力的途径。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [39] [Probabilistic Predictions of Process-Induced Deformation in Carbon/Epoxy Composites Using a Deep Operator Network](https://arxiv.org/abs/2512.13746)
*Elham Kiyani,Amit Makarand Deshpande,Madhura Limaye,Zhiwei Gao,Sai Aditya Pradeep,Srikanth Pilla,Gang Li,Zhen Li,George Em Karniadakis*

Main category: cs.CE

TL;DR: 研究用双机制框架建模单向AS4碳纤维/胺双功能环氧预浸料的工艺诱导变形（PID），开发基于DeepONets的数据驱动代理模型，用迁移学习和集成卡尔曼反演优化固化时间表以减少PID。


<details>
  <summary>Details</summary>
Motivation: 纤维增强和聚合物基体对制造条件响应不同产生残余应力，部分释放导致PID，需准确预测和缓解。

Method: 用双机制框架建模PID，开发基于DeepONets的数据驱动代理模型，扩展为FiLM DeepONet，使用迁移学习，用集成卡尔曼反演量化不确定性。

Result: 模型经制造试验验证，可生成不同非等温固化周期的PID响应，能预测固化程度、粘度和变形的时间历程。

Conclusion: 所提框架可量化实验条件下的不确定性，支持优化复合材料固化时间表以减少PID。

Abstract: Fiber reinforcement and polymer matrix respond differently to manufacturing conditions due to mismatch in coefficient of thermal expansion and matrix shrinkage during curing of thermosets. These heterogeneities generate residual stresses over multiple length scales, whose partial release leads to process-induced deformation (PID), requiring accurate prediction and mitigation via optimized non-isothermal cure cycles. This study considers a unidirectional AS4 carbon fiber/amine bi-functional epoxy prepreg and models PID using a two-mechanism framework that accounts for thermal expansion/shrinkage and cure shrinkage. The model is validated against manufacturing trials to identify initial and boundary conditions, then used to generate PID responses for a diverse set of non-isothermal cure cycles (time-temperature profiles). Building on this physics-based foundation, we develop a data-driven surrogate based on Deep Operator Networks (DeepONets). A DeepONet is trained on a dataset combining high-fidelity simulations with targeted experimental measurements of PID. We extend this to a Feature-wise Linear Modulation (FiLM) DeepONet, where branch-network features are modulated by external parameters, including the initial degree of cure, enabling prediction of time histories of degree of cure, viscosity, and deformation. Because experimental data are available only at limited time instances (for example, final deformation), we use transfer learning: simulation-trained trunk and branch networks are fixed and only the final layer is updated using measured final deformation. Finally, we augment the framework with Ensemble Kalman Inversion (EKI) to quantify uncertainty under experimental conditions and to support optimization of cure schedules for reduced PID in composites.

</details>


### [40] [Co-simulation errors due to step size changes](https://arxiv.org/abs/2512.13845)
*Lars T. Kyllingstad*

Main category: cs.CE

TL;DR: 在连续时间协同仿真中，两仿真单元通过变量q连接且都有代表q时间积分的内部状态时，通常因外推误差存在状态差异，一般减小宏时间步长误差会减小，但特定情况下，向小步长变化会使差异增大。


<details>
  <summary>Details</summary>
Motivation: 研究连续时间协同仿真中，仿真单元内部状态因外推误差产生差异的情况，特别是步长变化对差异的影响。

Method: 未提及

Result: 发现特定情况下，步长向小步长变化会使仿真单元内部状态差异增大。

Conclusion: 在连续时间协同仿真中，步长减小不一定能减小因外推误差导致的仿真单元内部状态差异。

Abstract: When two simulation units in a continuous-time co-simulation are connected via some variable $q$, and both simulation units have an internal state which represents the time integral of $q$, there will generally be a discrepancy between those states due to extrapolation errors. Normally, such extrapolation errors diminish if the macro time step size is reduced. Here we show that, under certain circumstances, step size changes can cause such discrepancies to increase even when the change is towards smaller steps.

</details>


### [41] [Dynamic stacking ensemble learning with investor knowledge representations for stock market index prediction based on multi-source financial data](https://arxiv.org/abs/2512.14042)
*Ruize Gao,Mei Yang,Yu Wang,Shaoze Cui*

Main category: cs.CE

TL;DR: 提出基于投资者知识表征的两阶段动态堆叠集成模型处理多源金融数据，应用于预测中国股市指数，实验和经济结果证明模型有效性和策略优越性。


<details>
  <summary>Details</summary>
Motivation: 不同金融数据源模式差异大，投资者信息处理认知行为有差异，需有效提取和整合多源金融数据特征。

Method: 提出两阶段动态堆叠集成模型，第一阶段从投资者角度识别不同金融数据属性，设计合适神经网络架构生成特征表示；第二阶段设计多个元分类器并动态选择最优者。

Result: 实验表明模型在预测上证、深证和创业板指数时，准确率优于最佳竞争模型；基于模型的交易策略在累计回报和夏普比率方面表现更优。

Conclusion: 所提模型能有效提升预测性能，基于该模型的交易策略表现出色。

Abstract: The patterns of different financial data sources vary substantially, and accordingly, investors exhibit heterogeneous cognition behavior in information processing. To capture different patterns, we propose a novel approach called the two-stage dynamic stacking ensemble model based on investor knowledge representations, which aims to effectively extract and integrate the features from multi-source financial data. In the first stage, we identify different financial data property from global stock market indices, industrial indices, and financial news based on the perspective of investors. And then, we design appropriate neural network architectures tailored to these properties to generate effective feature representations. Based on learned feature representations, we design multiple meta-classifiers and dynamically select the optimal one for each time window, enabling the model to effectively capture and learn the distinct patterns that emerge across different temporal periods. To evaluate the performance of the proposed model, we apply it to predicting the daily movement of Shanghai Securities Composite index, SZSE Component index and Growth Enterprise index in Chinese stock market. The experimental results demonstrate the effectiveness of our model in improving the prediction performance. In terms of accuracy metric, our approach outperforms the best competing models by 1.42%, 7.94%, and 7.73% on the SSEC, SZEC, and GEI indices, respectively. In addition, we design a trading strategy based on the proposed model. The economic results show that compared to the competing trading strategies, our strategy delivers a superior performance in terms of the accumulated return and Sharpe ratio.

</details>


### [42] [Transfer Learning-Based Surrogate Modeling for Nonlinear Time-History Response Analysis of High-Fidelity Structural Models](https://arxiv.org/abs/2512.14161)
*Keiichi Ishikawa,Yuma Matsumoto,Taro Yaoyama,Sangwon Lee,Tatsuya Itoi*

Main category: cs.CE

TL;DR: 本文提出使用迁移学习构建高保真响应分析模型的替代模型，降低计算成本，并通过20层钢框架案例验证。


<details>
  <summary>Details</summary>
Motivation: 基于性能的地震工程框架中，非线性时程响应分析计算量大，限制实际应用，且以往研究多聚焦低计算成本模型，高保真分析需高效替代建模方法。

Method: 提出使用迁移学习构建高保真响应分析模型的替代模型框架，将低保真分析替代模型作为预训练模型转移知识。

Result: 以20层钢框架为例，仅用20个样本作为训练集构建替代模型，预测的地震响应与特定场地时变灾害一致。

Conclusion: 所提框架能以低计算成本构建高保真响应分析的替代模型。

Abstract: In a performance based earthquake engineering (PBEE) framework, nonlinear time-history response analysis (NLTHA) for numerous ground motions are required to assess the seismic risk of buildings or civil engineering structures. However, such numerical simulations are computationally expensive, limiting the real-world practical application of the framework. To address this issue, previous studies have used machine learning to predict the structural responses to ground motions with low computational costs. These studies typically conduct NLTHAs for a few hundreds ground motions and use the results to train and validate surrogate models. However, most of the previous studies focused on computationally-inexpensive response analysis models such as single degree of freedom. Surrogate models of high-fidelity response analysis are required to enrich the quantity and diversity of information used for damage assessment in PBEE. Notably, the computational cost of creating training and validation datasets increases if the fidelity of response analysis model becomes higher. Therefore, methods that enable surrogate modeling of high-fidelity response analysis without a large number of training samples are needed. This study proposes a framework that uses transfer learning to construct the surrogate model of a high-fidelity response analysis model. This framework uses a surrogate model of low-fidelity response analysis as the pretrained model and transfers its knowledge to construct surrogate models for high-fidelity response analysis with substantially reduced computational cost. As a case study, surrogate models that predict responses of a 20-story steel moment frame were constructed with only 20 samples as the training dataset. The responses to the ground motions predicted by constructed surrogate model were consistent with a site-specific time-based hazard.

</details>


### [43] [A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data](https://arxiv.org/abs/2512.14329)
*Yanning Dai,Chenyu Tang,Ruizhi Zhang,Wenyu Yang,Yilan Zhang,Yuhui Wang,Junliang Chen,Xuhang Chen,Ruimou Xie,Yangyue Cao,Qiaoying Li,Jin Cao,Tao Li,Hubin Zhao,Yu Pan,Arokia Nathan,Xin Gao,Peter Smielewski,Shuo Gao*

Main category: cs.CE

TL;DR: 开发数据物理混合生成框架预测中风后运动能力，在患者测试和多中心试验中取得良好效果，能辅助临床决策。


<details>
  <summary>Details</summary>
Motivation: 当前中风后运动能力评估是静态的，无法表明患者能否安全完成特定任务，需要动态预测来调整康复方案。

Method: 开发数据 - 物理混合生成框架，结合可穿戴传感器运动学、比例 - 微分物理控制器、健康运动图谱、深度强化学习等技术。

Result: 在11名中风幸存者中提升关节角度和终点保真度，减少训练时间；多中心试验中指导康复的临床医生使患者Fugl - Meyer下肢评分提升更大。

Conclusion: 该生成性、任务预测性框架可增强中风后步态康复的临床决策，为动态个性化运动恢复策略提供模板。

Abstract: Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.

</details>


### [44] [BridgeNet: A Dataset of Graph-based Bridge Structural Models for Machine Learning Applications](https://arxiv.org/abs/2512.14496)
*Lazlo Bleker,Mustafa Cem Güneş,Pierluigi D'Acunto*

Main category: cs.CE

TL;DR: 介绍公开图基数据集BridgeNet，含20000个桥梁结构数据，支持多种学习和任务，解决结构工程数据瓶颈。


<details>
  <summary>Details</summary>
Motivation: 机器学习在结构工程中应用广泛，但缺乏公开结构系统数据集阻碍其更广泛应用。

Method: 使用组合平衡建模（CEM）找形方法生成铰接平衡线框模型，通过力信息实体化得到三维网格，从两个标准相机角度渲染图像。

Result: 得到模态丰富、与应用无关的数据集，支持多种任务。

Conclusion: BridgeNet提供数据集，解决结构工程和设计中数据驱动应用的关键瓶颈，促进基于机器学习的平衡桥梁结构新方法发展。

Abstract: Machine learning (ML) is increasingly used in structural engineering and design, yet its broader adoption is hampered by the lack of openly accessible datasets of structural systems. We introduce BridgeNet, a publicly available graph-based dataset of 20,000 form-found bridge structures aimed at enabling Graph ML and multi-modal learning in the context of conceptual structural design. Each datapoint consists of (i) a pin-jointed equilibrium wireframe model generated with the Combinatorial Equilibrium Modeling (CEM) form-finding method, (ii) a volumetric 3D mesh obtained through force-informed materialization, and (iii) rendered images from two canonical camera angles. The resulting dataset is modality-rich and application-agnostic, supporting tasks such as CEM-specific edge classification and parameter inference, surrogate modeling of form-finding, cross-modal reconstruction between graphs, meshes and images, and generative structural design. BridgeNet addresses a key bottleneck in data-driven applications for structural engineering and design by providing a dataset that facilitates the development of new ML-based approaches for equilibrium bridge structures.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [45] [Time and Relations into Focus: Ontological Foundations of Object-Centric Event Data](https://arxiv.org/abs/2512.14425)
*Hosna Hooshyar,Mattia Fumagalli,Marco Montali,Giancarlo Guizzardi*

Main category: cs.DB

TL;DR: 本文提出用三步法为对象中心事件数据(OCED)引入本体基础，解决现有OCED元模型的模糊性和表达局限性问题。


<details>
  <summary>Details</summary>
Motivation: 传统事件数据模型无法处理对象中心过程挖掘中的复杂关系，现有OCED模型存在固有模糊性，缺乏对时间和动态关系等关键维度的全面支持。

Method: 第一步分析现有OCED元模型的关键问题；第二步基于轻量级的gUFO增强OCED核心模型，得到新的元模型gOCED；第三步展示gOCED能保留现有元模型特性并解决相关问题。

Result: 提出了新的元模型gOCED。

Conclusion: gOCED既保留了现有元模型的简单性，又具备克服文献中报告的模糊性和表达问题所需的基本特征。

Abstract: Object-centric process mining is a new branch of process mining where events are associated with multiple objects, and where object-to-object interactions are essential to understand the process dynamics. Traditional event data models, also called case-centric, are unable to cope with the complexity introduced by these more refined relationships. Several models have been made to move from case-centric to Object-Centric Event Data (OCED), trying to retain simplicity as much as possible. Still, these suffer from inherent ambiguities, and lack a comprehensive support of essential dimensions related to time and (dynamic) relations. In this work, we propose to fill this gap by leveraging a well-founded ontology of events and bringing ontological foundations to OCED, with a three-step approach. First, we start from key open issues reported in the literature regarding current OCED metamodels, and witness their ambiguity and expressiveness limitations on illustrative and representative examples proposed therein. Second, we consider the OCED Core Model, currently proposed as the basis for defining a new standard for object-centric event data, and we enhance it by grounding it on a lightweight version of UFO-B called gUFO, a well-known foundational ontology tailored to the representation of objects, events, time, and their (dynamic) relations. This results in a new metamodel, which we call gOCED. The third contribution then shows how gOCED at once covers the features of existing metamodels preserving their simplicity, and extends them with the essential features needed to overcome the ambiguity and expressiveness issues reported in the literature.

</details>


### [46] [Beyond Text-to-SQL: Autonomous Research-Driven Database Exploration with DAR](https://arxiv.org/abs/2512.14622)
*Ostap Vykhopen,Viktoria Skorik,Maxim Tereschenko,Veronika Solopova*

Main category: cs.DB

TL;DR: 介绍多智能体系统DAR，可端到端进行数据库研究，比专业分析师快约32倍，推动数据库交互向自主探索转变。


<details>
  <summary>Details</summary>
Motivation: 现有数据库查询系统大多被动，依赖用户明确提示，不能主动探索数据。

Method: DAR通过三层协调专业AI智能体，利用BigQuery原生生成式AI函数直接推理，无需数据移动。

Result: 在资产事件数据集上，DAR完成分析任务耗时16分钟，远快于专业分析师，还能给出有用洞察和建议。

Conclusion: 该工作使数据库交互从查询驱动转向云数据仓库内的自主研究探索。

Abstract: Large language models can already query databases, yet most existing systems remain reactive: they rely on explicit user prompts and do not actively explore data. We introduce DAR (Data Agnostic Researcher), a multi-agent system that performs end-to-end database research without human-initiated queries. DAR orchestrates specialized AI agents across three layers: initialization (intent inference and metadata extraction), execution (SQL and AI-based query synthesis with iterative validation), and synthesis (report generation with built-in quality control). All reasoning is executed directly inside BigQuery using native generative AI functions, eliminating data movement and preserving data governance. On a realistic asset-incident dataset, DAR completes the full analytical task in 16 minutes, compared to 8.5 hours for a professional analyst (approximately 32x times faster), while producing useful pattern-based insights and evidence-grounded recommendations. Although human experts continue to offer deeper contextual interpretation, DAR excels at rapid exploratory analysis. Overall, this work shifts database interaction from query-driven assistance toward autonomous, research-driven exploration within cloud data warehouses.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [47] [Real-Time Service Subscription and Adaptive Offloading Control in Vehicular Edge Computing](https://arxiv.org/abs/2512.14002)
*Chuanchao Gao,Arvind Easwaran*

Main category: cs.DC

TL;DR: 提出SARound算法解决VEC中任务卸载和资源分配问题，将近似比从1/6提升到1/4，设计在线框架，用VecSim验证，实验表明SARound性能优且高效。


<details>
  <summary>Details</summary>
Motivation: VEC中高效任务卸载和资源分配面临网络带宽、计算资源受限、任务期限严格和网络条件多变等挑战。

Method: 提出DOAP问题，设计SARound近似算法，采用线性规划舍入和局部比率技术；设计在线服务订阅和卸载控制框架；开发VecSim模拟器。

Result: SARound将DOAP的最佳已知近似比从1/6提高到1/4；实验显示SARound在不同网络条件下始终优于现有基线，且保持运行时效率。

Conclusion: SARound算法和在线框架能有效解决VEC中任务卸载和资源分配问题，具有良好性能和效率。

Abstract: Vehicular Edge Computing (VEC) has emerged as a promising paradigm for enhancing the computational efficiency and service quality in intelligent transportation systems by enabling vehicles to wirelessly offload computation-intensive tasks to nearby Roadside Units. However, efficient task offloading and resource allocation for time-critical applications in VEC remain challenging due to constrained network bandwidth and computational resources, stringent task deadlines, and rapidly changing network conditions. To address these challenges, we formulate a Deadline-Constrained Task Offloading and Resource Allocation Problem (DOAP), denoted as $\mathbf{P}$, in VEC with both bandwidth and computational resource constraints, aiming to maximize the total vehicle utility. To solve $\mathbf{P}$, we propose $\mathtt{SARound}$, an approximation algorithm based on Linear Program rounding and local-ratio techniques, that improves the best-known approximation ratio for DOAP from $\frac{1}{6}$ to $\frac{1}{4}$. Additionally, we design an online service subscription and offloading control framework to address the challenges of short task deadlines and rapidly changing wireless network conditions. To validate our approach, we develop a comprehensive VEC simulator, VecSim, using the open-source simulation libraries OMNeT++ and Simu5G. VecSim integrates our designed framework to manage the full life-cycle of real-time vehicular tasks. Experimental results, based on profiled object detection applications and real-world taxi trace data, show that $\mathtt{SARound}$ consistently outperforms state-of-the-art baselines under varying network conditions while maintaining runtime efficiency.

</details>


### [48] [A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing](https://arxiv.org/abs/2512.14290)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出用于SLA约束边缘计算应用的新型自动伸缩算法，结合ML主动和被动伸缩策略，集成到Kubernetes，实验显示其SLA违规率仅6%，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有自动伸缩算法存在性能问题和配置复杂性，无法保证SLA约束边缘计算应用的SLA遵守。

Method: 提出结合基于机器学习的主动自动伸缩算法和考虑当前资源利用率与SLA约束的被动自动伸缩器的新型算法，并将其集成到Kubernetes。

Result: 现有解决方案SLA违规率达23%，而提出的混合解决方案SLA违规率仅6%。

Conclusion: 提出的混合自动伸缩算法能确保各类应用稳定遵守SLA，性能优于现有方案。

Abstract: Edge computing decentralizes computing resources, allowing for novel applications in domains such as the Internet of Things (IoT) in healthcare and agriculture by reducing latency and improving performance. This decentralization is achieved through the implementation of microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. This is achieved by sophisticated orchestration strategies such as Kubernetes, which help facilitate resource management. The orchestration strategies alone do not guarantee SLA adherence due to the inherent delay of scaling resources. Existing auto-scaling algorithms have been proposed to address these challenges, but they suffer from performance issues and configuration complexity. In this paper, a novel auto-scaling algorithm is proposed for SLA-constrained edge computing applications. This approach combines a Machine Learning (ML) based proactive auto-scaling algorithm, capable of predicting incoming resource requests to forecast demand, with a reactive autoscaler which considers current resource utilization and SLA constraints for immediate adjustments. The algorithm is integrated into Kubernetes as an extension, and its performance is evaluated through extensive experiments in an edge environment with real applications. The results demonstrate that existing solutions have an SLA violation rate of up to 23%, whereas the proposed hybrid solution outperforms the baselines with an SLA violation rate of only 6%, ensuring stable SLA compliance across various applications.

</details>


### [49] [Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs](https://arxiv.org/abs/2512.14445)
*Brenton Walker,Markus Fidler*

Main category: cs.DC

TL;DR: 本文分析并行计算中屏障带来的稳定性和性能损失，研究不同屏障系统，对比边界和模拟结果与基准数据，分析真实系统开销并建模验证。


<details>
  <summary>Details</summary>
Motivation: 并行计算中屏障会导致工作节点空闲，降低稳定性和性能，需分析其带来的损失。

Method: 分析(s,k,l)屏障系统稳定性，推导和评估混合屏障系统性能边界，对比边界和模拟结果与基准数据，研究真实系统开销并建模验证。

Result: 完成对不同屏障系统的分析，对比了边界和模拟结果与基准数据，找出真实系统开销来源并建模。

Conclusion: 开发的开销模型可通过模拟在真实系统中验证。

Abstract: In some models of parallel computation, jobs are split into smaller tasks and can be executed completely asynchronously. In other situations the parallel tasks have constraints that require them to synchronize their start and possibly departure times. This is true of many parallelized machine learning workloads, and the popular Apache Spark processing engine has recently added support for Barrier Execution Mode, which allows users to add such barriers to their jobs. These barriers necessarily result in idle periods on some of the workers, which reduces their stability and performance, compared to equivalent workloads with no barriers.
  In this paper we will consider and analyze the stability and performance penalties resulting from barriers. We include an analysis of the stability of $(s,k,l)$ barrier systems that allow jobs to depart after $l$ out of $k$ of their tasks complete. We also derive and evaluate performance bounds for hybrid barrier systems servicing a mix of jobs, both with and without barriers, and with varying degrees of parallelism. For the purely 1-barrier case we compare the bounds and simulation results to benchmark data from a standalone Spark system. We study the overhead in the real system, and based on its distribution we attribute it to the dual event and polling-driven mechanism used to schedule barrier-mode jobs. We develop a model for this type of overhead and validate it against the real system through simulation.

</details>


### [50] [PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning](https://arxiv.org/abs/2512.14628)
*Alireza Olama,Andreas Lundell,Izzat El Hajj,Johan Lilius,Jerker Björkqvist*

Main category: cs.DC

TL;DR: 论文提出分布式训练系统PruneX，协同设计剪枝算法与集群层次架构，减少节点间带宽使用，实验显示有良好效果。


<details>
  <summary>Details</summary>
Motivation: 节点间通信带宽限制多节点GPU集群大规模分布式训练，传统剪枝感知系统难以减少通信开销。

Method: 提出H - SADMM算法强制执行节点级结构化稀疏，采用主从执行模型分离节点内和节点间进程组。

Result: 在64个GPU的ResNet架构评估中，PruneX减少约60%节点间通信量，实现6.75倍强缩放加速比。

Conclusion: PruneX在节点间通信优化和加速比上优于密集基线和Top - K梯度压缩方法。

Abstract: Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by highly optimized dense collective primitives. We present PruneX, a distributed data-parallel training system that co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage. PruneX introduces the Hierarchical Structured ADMM (H-SADMM) algorithm, which enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction that eliminates both zero-valued transmissions and indexing overhead. The system adopts a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects. Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer at CSC - IT Center for Science (Finland).

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [51] [Finding $b$-colorings Using Feedback Edges](https://arxiv.org/abs/2512.14390)
*Jakub Balabán*

Main category: cs.DS

TL;DR: 本文研究图的b - 着色问题，证明其在反馈边数和到共簇距离这两个参数化下是固定参数可处理的，还指出按树深度参数化时是W[1] - 难的。


<details>
  <summary>Details</summary>
Motivation: 已知b - 着色问题在一般图上是NP完全的，在树和部分参数下有特定复杂度结果，研究在更多参数下的复杂度情况。

Method: 结合参数化算法中的标准技术和树的多项式时间算法中的特定思想。

Result: 证明b - 着色在反馈边数参数化下是固定参数可处理的，给出按到共簇距离参数化的FPT算法，指出按树深度参数化时是W[1] - 难的。

Conclusion: 明确了b - 着色问题在反馈边数、到共簇距离和树深度等不同参数化下的复杂度。

Abstract: A $b$-coloring of a graph is a proper vertex coloring such that each color class contains a vertex that sees all other colors in its neighborhood. The $b$-coloring problem, in which the task is to decide whether a graph admits a $b$-coloring with $k$ colors, is NP-complete in general but polytime solvable on trees. Moreover, it is known that $b$-coloring is in XP but W[$t$]-hard for all $t \in \mathbb{N}$ when parameterized by tree-width. In fact, only very few parameters, such as the vertex cover number, were known to admit an FPT algorithm for $b$-coloring. In this paper, we consider a more restrictive parameter measuring similarity to trees than tree-width, namely the feedback edge number, and show that $b$-coloring is fixed-parameter tractable under this parameterization. Our algorithm combines standard techniques used in parameterized algorithmics with the problem-specific ideas used in the polytime algorithm for trees. In addition, we present an FPT algorithm for $b$-coloring parameterized by distance to co-cluster, which is a parameter measuring similarity to complete multipartite graphs. Finally, we make several observations based on known results, including that $b$-coloring is W[$1$]-hard when parameterized by tree-depth.

</details>


### [52] [Cost-Free Neutrality for the River Method](https://arxiv.org/abs/2512.14409)
*Michelle Döring,Jannes Malanowski,Stefan Neubert*

Main category: cs.DS

TL;DR: 介绍River方法，指出投票规则平局处理影响中立性，提出PUT，计算Ranked Pairs用PUT选胜者是NP完全问题，而本文给出计算River用PUT选胜者的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: Ranked Pairs用PUT计算胜者是NP完全问题，因River与Ranked Pairs相似，探究River是否有同样复杂度问题。

Method: 提出Fused - Universe (FUN) 算法，一次模拟所有可能平局处理情况。

Result: 得到FUN图，可直接读出胜者集及每个胜者的优势证明。

Conclusion: River用PUT计算胜者有多项式时间算法，相比Ranked Pairs有显著结构优势。

Abstract: Recently, the River Method was introduced as novel refinement of the Split Cycle voting rule.
  The decision-making process of River is closely related to the well established Ranked Pairs Method.
  Both methods consider a margin graph computed from the voters' preferences and eliminate majority cycles in that graph to choose a winner.
  As ties can occur in the margin graph, a tiebreaker is required along with the preferences.
  While such a tiebreaker makes the computation efficient, it compromises the fundamental property of neutrality: the voting rule should not favor alternatives in advance.
  One way to reintroduce neutrality is to use Parallel-Universe Tiebreaking (PUT), where each alternative is a winner if it wins according to any possible tiebreaker.
  Unfortunately, computing the winners selected by Ranked Pairs with PUT is NP-complete.
  Given the similarity of River to Ranked Pairs, one might expect River to suffer from the same complexity.
  Surprisingly, we show the opposite:
  We present a polynomial-time algorithm for computing River winners with PUT, highlighting significant structural advantages of River over Ranked Pairs.
  Our Fused-Universe (FUN) algorithm simulates River for every possible tiebreaking in one pass.
  From the resulting FUN diagram one can then directly read off both the set of winners and, for each winner, a certificate that explains how this alternative dominates the others.

</details>


### [53] [An Improved Approximation Algorithm for Maximum Weight 3-Path Packing](https://arxiv.org/abs/2512.14457)
*Jingyang Zhao,Mingyu Xiao*

Main category: cs.DS

TL;DR: 本文针对最大权重3 - 路径打包问题提出了一个10/17 - 近似算法，改进了之前的7/12 - 近似算法，通过权衡三种算法得到结果并提出新分析方法。


<details>
  <summary>Details</summary>
Motivation: 解决最大权重3 - 路径打包问题，改进现有近似算法的近似比。

Method: 权衡三种算法，分别基于大小为n/2的最大权重匹配、大小为n/3的最大权重匹配和星形打包近似算法，同时提出新的充电分析方法。

Result: 得到了一个10/17 - 近似算法，改进了之前的7/12 - 近似算法。

Conclusion: 新算法提升了近似比，新分析方法对分析第二个算法至关重要，且可扩展到相关问题的算法分析中。

Abstract: Given a complete graph with $n$ vertices and non-negative edge weights, where $n$ is divisible by 3, the maximum weight 3-path packing problem is to find a set of $n/3$ vertex-disjoint 3-paths such that the total weight of the 3-paths in the packing is maximized. This problem is closely related to the classic maximum weight matching problem. In this paper, we propose a $10/17$-approximation algorithm, improving the best-known $7/12$-approximation algorithm (ESA 2015). Our result is obtained by making a trade-off among three algorithms. The first is based on the maximum weight matching of size $n/2$, the second is based on the maximum weight matching of size $n/3$, and the last is based on an approximation algorithm for star packing. Our first algorithm is the same as the previous $7/12$-approximation algorithm, but we propose a new analysis method -- a charging method -- for this problem, which is not only essential to analyze our second algorithm but also may be extended to analyze algorithms for some related problems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [54] [The Impact Market to Save Conference Peer Review: Decoupling Dissemination and Credentialing](https://arxiv.org/abs/2512.14104)
*Karthikeyan Sankaralingam*

Main category: cs.GT

TL;DR: 顶级学术会议面临传播研究与提供声望认证的矛盾，提出Impact Market（IM）系统解决问题，模拟显示该系统能提升高影响力论文检索率。


<details>
  <summary>Details</summary>
Motivation: 解决顶级学术会议在快速传播研究和提供稀缺声望认证两个角色间的矛盾，避免现有评审模式的弊端。

Method: 提出IM系统，分三个阶段：通过PC评审接受所有合理严谨论文；通过期货市场创建声望信号，由资深成员投资；三年回溯机制验证投资，调整投资者未来影响力。

Result: 代理模拟显示，引入投资者能动性和信念投注后，高影响力论文检索率从28%提升到超85%。

Conclusion: IM系统用透明、可问责和数据驱动的市场取代现有模式，激励式自我选择是扩大同行评审规模所需机制。

Abstract: Top-tier academic conferences are failing under the strain of two irreconcilable roles: (1) rapid dissemination of all sound research and (2) scarce credentialing for prestige and career advancement. This conflict has created a reviewer roulette and anonymous tribunal model - a zero-cost attack system - characterized by high-stakes subjectivity, turf wars, and the arbitrary rejection of sound research (the equivalence class problem). We propose the Impact Market (IM), a novel three-phase system that decouples publication from prestige. Phase 1 (Publication): All sound and rigorous papers are accepted via a PC review, solving the "equivalence class" problem. Phase 2 (Investment): An immediate, scarce prestige signal is created via a futures market. Senior community members invest tokens into published papers, creating a transparent, crowdsourced Net Invested Score (NIS). Phase 3 (Calibration): A 3-year lookback mechanism validates these investments against a manipulation-resistant Multi-Vector Impact Score (MVIS). This MVIS adjusts each investor's future influence (their Investor Rating), imposing a quantifiable cost on bad actors and rewarding accurate speculation. The IM model replaces a hidden, zero-cost attack system with a transparent, accountable, and data-driven market that aligns immediate credentialing with long-term, validated impact. Agent-based simulations demonstrate that while a passive market matches current protocols in low-skill environments, introducing investor agency and conviction betting increases the retrieval of high-impact papers from 28% to over 85% under identical conditions, confirming that incentivized self-selection is the mechanism required to scale peer review.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [55] [BiCoRec: Bias-Mitigated Context-Aware Sequential Recommendation Model](https://arxiv.org/abs/2512.13848)
*Mufhumudzi Muthivhi,Terence L van Zyl,Hairong Wang*

Main category: cs.IR

TL;DR: 本文提出新框架BiCoRec以应对顺序推荐模型中的流行度偏差问题，利用共注意力机制和新训练方案，在偏好小众物品用户的推荐上性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决当前顺序推荐模型中存在的固有流行度偏差问题，提升偏好小众物品用户的推荐性能。

Method: 开发BiCoRec框架，利用共注意力机制获取流行度加权的用户序列表示，采用基于一致性损失函数的新训练方案从未来偏好中学习。

Result: 对于偏好小众物品的用户，BiCoRec在NDCG@10上比现有最优基线平均提升26.00%；在不同数据集上，BiCoRec也取得了相应的NDCG@10分数。

Conclusion: BiCoRec框架能有效改善顺序推荐模型的性能，尤其对偏好小众物品的用户效果显著。

Abstract: Sequential recommendation models aim to learn from users evolving preferences. However, current state-of-the-art models suffer from an inherent popularity bias. This study developed a novel framework, BiCoRec, that adaptively accommodates users changing preferences for popular and niche items. Our approach leverages a co-attention mechanism to obtain a popularity-weighted user sequence representation, facilitating more accurate predictions. We then present a new training scheme that learns from future preferences using a consistency loss function. BiCoRec aimed to improve the recommendation performance of users who preferred niche items. For these users, BiCoRec achieves a 26.00% average improvement in NDCG@10 over state-of-the-art baselines. When ranking the relevant item against the entire collection, BiCoRec achieves NDCG@10 scores of 0.0102, 0.0047, 0.0021, and 0.0005 for the Movies, Fashion, Games and Music datasets.

</details>


### [56] [Intent-Guided Reasoning for Sequential Recommendation](https://arxiv.org/abs/2512.14034)
*Yifan Shao,Peilin Zhou*

Main category: cs.IR

TL;DR: 现有推理增强的序列推荐方法存在推理不稳定和表面推理问题，本文提出IGR - SR框架解决这些问题，实验显示该框架有效且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有推理增强方法仅以目标物品为监督，存在推理不稳定和表面推理问题，需要改进。

Method: 提出IGR - SR框架，包含潜在意图提取器（LID）、意图感知推理器（IDR）和意图一致性正则化（ICR）三个关键组件。

Result: 在三个公开数据集上，IGR - SR比现有方法平均提升7.13%；在20%行为噪声下，性能下降幅度低于竞争方法。

Conclusion: 意图引导的推理有效且鲁棒，IGR - SR框架能解决现有推理增强方法的问题。

Abstract: Sequential recommendation systems aim to capture users' evolving preferences from their interaction histories. Recent reasoningenhanced methods have shown promise by introducing deliberate, chain-of-thought-like processes with intermediate reasoning steps. However, these methods rely solely on the next target item as supervision, leading to two critical issues: (1) reasoning instability--the process becomes overly sensitive to recent behaviors and spurious interactions like accidental clicks, and (2) surface-level reasoning--the model memorizes item-to-item transitions rather than understanding intrinsic behavior patterns. To address these challenges, we propose IGR-SR, an Intent-Guided Reasoning framework for Sequential Recommendation that anchors the reasoning process to explicitly extracted high-level intents. Our framework comprises three key components: (1) a Latent Intent Distiller (LID) that efficiently extracts multi-faceted intents using a frozen encoder with learnable tokens, (2) an Intent-aware Deliberative Reasoner (IDR) that decouples reasoning into intent deliberation and decision-making via a dual-attention architecture, and (3) an Intent Consistency Regularization (ICR) that ensures robustness by enforcing consistent representations across different intent views. Extensive experiments on three public datasets demonstrate that IGR-SR achieves an average 7.13% improvement over state-of-the-art baselines. Critically, under 20% behavioral noise, IGR-SR degrades only 10.4% compared to 16.2% and 18.6% for competing methods, validating the effectiveness and robustness of intent-guided reasoning.

</details>


### [57] [DTRec: Learning Dynamic Reasoning Trajectories for Sequential Recommendation](https://arxiv.org/abs/2512.14036)
*Yifan Shao,Peilin Zhou,Shoujin Wang,Weizhi Zhang,Xu Cai,Sunghun Kim*

Main category: cs.IR

TL;DR: 现有推理增强序列推荐方法有静态推理局限，提出DTRec框架，实验表明其性能优且降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前推理增强序列推荐方法存在静态推理轨迹问题，包括静态推理方向和固定推理深度，导致性能不佳与计算浪费，需改进。

Method: 提出DTRec框架，通过Hierarchical Process Supervision（HPS）引导推理方向，引入Adaptive Reasoning Halting（ARH）机制优化推理深度。

Result: 在三个真实数据集上实验，相比强基线性能提升达24.5%，同时计算成本降低达41.6%。

Conclusion: DTRec框架有效，可解决现有推理增强序列推荐方法的问题，提升性能并降低计算成本。

Abstract: Inspired by advances in LLMs, reasoning-enhanced sequential recommendation performs multi-step deliberation before making final predictions, unlocking greater potential for capturing user preferences. However, current methods are constrained by static reasoning trajectories that are ill-suited for the diverse complexity of user behaviors. They suffer from two key limitations: (1) a static reasoning direction, which uses flat supervision signals misaligned with human-like hierarchical reasoning, and (2) a fixed reasoning depth, which inefficiently applies the same computational effort to all users, regardless of pattern complexity. These rigidity lead to suboptimal performance and significant computational waste. To overcome these challenges, we propose DTRec, a novel and effective framework that explores the Dynamic reasoning Trajectory for Sequential Recommendation along both direction and depth. To guide the direction, we develop Hierarchical Process Supervision (HPS), which provides coarse-to-fine supervisory signals to emulate the natural, progressive refinement of human cognitive processes. To optimize the depth, we introduce the Adaptive Reasoning Halting (ARH) mechanism that dynamically adjusts the number of reasoning steps by jointly monitoring three indicators. Extensive experiments on three real-world datasets demonstrate the superiority of our approach, achieving up to a 24.5% performance improvement over strong baselines while simultaneously reducing computational cost by up to 41.6%.

</details>


### [58] [From Feature Interaction to Feature Generation: A Generative Paradigm of CTR Prediction Models](https://arxiv.org/abs/2512.14041)
*Mingjia Yin,Junwei Pan,Hao Wang,Ximei Wang,Shangyu Zhang,Jie Jiang,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: 文章提出Supervised Feature Generation (SFG)框架用于CTR预测，以解决现有判别式范式的问题，实验证明可缓解嵌入崩溃和信息冗余，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测模型采用判别式范式，存在嵌入维度崩溃和信息冗余问题，需改进。

Method: 提出SFG框架，包含Encoder和Decoder，引入监督损失，可与多数现有CTR模型集成。

Result: 缓解嵌入崩溃和信息冗余，在不同数据集和基础模型上取得显著性能提升。

Conclusion: SFG框架有效，可解决现有模型问题，提升CTR预测性能。

Abstract: Click-Through Rate (CTR) prediction, a core task in recommendation systems, aims to estimate the probability of users clicking on items. Existing models predominantly follow a discriminative paradigm, which relies heavily on explicit interactions between raw ID embeddings. However, this paradigm inherently renders them susceptible to two critical issues: embedding dimensional collapse and information redundancy, stemming from the over-reliance on feature interactions \emph{over raw ID embeddings}. To address these limitations, we propose a novel \emph{Supervised Feature Generation (SFG)} framework, \emph{shifting the paradigm from discriminative ``feature interaction" to generative ``feature generation"}. Specifically, SFG comprises two key components: an \emph{Encoder} that constructs hidden embeddings for each feature, and a \emph{Decoder} tasked with regenerating the feature embeddings of all features from these hidden representations. Unlike existing generative approaches that adopt self-supervised losses, we introduce a supervised loss to utilize the supervised signal, \ie, click or not, in the CTR prediction task. This framework exhibits strong generalizability: it can be seamlessly integrated with most existing CTR models, reformulating them under the generative paradigm. Extensive experiments demonstrate that SFG consistently mitigates embedding collapse and reduces information redundancy, while yielding substantial performance gains across various datasets and base models. The code is available at https://github.com/USTC-StarTeam/GE4Rec.

</details>


### [59] [AsarRec: Adaptive Sequential Augmentation for Robust Self-supervised Sequential Recommendation](https://arxiv.org/abs/2512.14047)
*Kaike Zhang,Qi Cao,Fei Sun,Xinran Liu*

Main category: cs.IR

TL;DR: 针对序列推荐系统中自监督学习固定增强策略的局限，提出自适应增强框架AsarRec，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实用户行为有噪声影响推荐性能，现有自监督学习方法采用固定增强策略存在局限性。

Method: 将现有基本增强操作统一成公式，提出AsarRec，通过编码用户序列生成转换矩阵，联合优化多样性、语义不变性和信息性三个目标。

Result: 在三个基准数据集不同噪声水平下实验，验证AsarRec有效性，展现出优越鲁棒性和持续改进效果。

Conclusion: AsarRec能克服现有自监督学习方法局限，有效提升序列推荐系统性能。

Abstract: Sequential recommender systems have demonstrated strong capabilities in modeling users' dynamic preferences and capturing item transition patterns. However, real-world user behaviors are often noisy due to factors such as human errors, uncertainty, and behavioral ambiguity, which can lead to degraded recommendation performance. To address this issue, recent approaches widely adopt self-supervised learning (SSL), particularly contrastive learning, by generating perturbed views of user interaction sequences and maximizing their mutual information to improve model robustness. However, these methods heavily rely on their pre-defined static augmentation strategies~(where the augmentation type remains fixed once chosen) to construct augmented views, leading to two critical challenges: (1) the optimal augmentation type can vary significantly across different scenarios; (2) inappropriate augmentations may even degrade recommendation performance, limiting the effectiveness of SSL. To overcome these limitations, we propose an adaptive augmentation framework. We first unify existing basic augmentation operations into a unified formulation via structured transformation matrices. Building on this, we introduce AsarRec (Adaptive Sequential Augmentation for Robust Sequential Recommendation), which learns to generate transformation matrices by encoding user sequences into probabilistic transition matrices and projecting them into hard semi-doubly stochastic matrices via a differentiable Semi-Sinkhorn algorithm. To ensure that the learned augmentations benefit downstream performance, we jointly optimize three objectives: diversity, semantic invariance, and informativeness. Extensive experiments on three benchmark datasets under varying noise levels validate the effectiveness of AsarRec, demonstrating its superior robustness and consistent improvements.

</details>


### [60] [SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions](https://arxiv.org/abs/2512.14277)
*Panayiotis Smeros,Vincent Emonet,Ruijie Wang,Ana-Claudia Sima,Tarcisio Mendes de Farias*

Main category: cs.IR

TL;DR: 本文提出开源且与三元组存储无关的SPARQL - LLM方法，能从自然语言生成SPARQL查询，评估显示其F1得分提升，速度快且成本低，适用于实时低成本应用。


<details>
  <summary>Details</summary>
Motivation: 现有从自然语言生成SPARQL查询的新方法多关注单源响应准确性，忽略联合查询能力、运行时间和成本，难以在知识图谱上部署。

Method: 扩展先前工作，介绍SPARQL - LLM架构，包括元数据索引、提示构建、查询生成和执行组件，基于多语言问题挑战赛和生物信息学知识图谱问题集进行评估。

Result: 在挑战赛中F1得分提升24%，适配多种语言，能形成复杂联合查询，比其他系统快36倍，每题最多花费0.01美元。

Conclusion: SPARQL - LLM适合实时、低成本的文本到SPARQL应用，可部署到真实世界的分布式知识图谱。

Abstract: The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat.

</details>


### [61] [Dynamic Context Selection for Retrieval-Augmented Generation: Mitigating Distractors and Positional Bias](https://arxiv.org/abs/2512.14313)
*Malika Iratni,Mohand Boughanem,Taoufiq Dkaki*

Main category: cs.IR

TL;DR: 本文分析了检索增强生成（RAG）中干扰项和相关段落位置对生成质量的影响，提出上下文大小分类器，集成进RAG提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准RAG系统的固定top k策略会引入干扰项影响输出质量，且存在“中间迷失”现象。

Method: 系统分析干扰项对生成质量的影响并量化，研究相关段落位置的作用，提出上下文大小分类器预测最佳检索文档数量，并集成到RAG管道。

Result: 所提方法在性能上优于固定k的基线方法。

Conclusion: 通过对干扰项和段落位置的研究及上下文大小分类器的应用，能有效提升RAG系统性能。

Abstract: Retrieval Augmented Generation (RAG) enhances language model performance by incorporating external knowledge retrieved from large corpora, which makes it highly suitable for tasks such as open domain question answering. Standard RAG systems typically rely on a fixed top k retrieval strategy, which can either miss relevant information or introduce semantically irrelevant passages, known as distractors, that degrade output quality. Additionally, the positioning of retrieved passages within the input context can influence the model attention and generation outcomes. Context placed in the middle tends to be overlooked, which is an issue known as the "lost in the middle" phenomenon. In this work, we systematically analyze the impact of distractors on generation quality, and quantify their effects under varying conditions. We also investigate how the position of relevant passages within the context window affects their influence on generation. Building on these insights, we propose a context-size classifier that dynamically predicts the optimal number of documents to retrieve based on query-specific informational needs. We integrate this approach into a full RAG pipeline, and demonstrate improved performance over fixed k baselines.

</details>


### [62] [PushGen: Push Notifications Generation with LLM](https://arxiv.org/abs/2512.14490)
*Shifu Bie,Jiangxia Cao,Zixiao Luo,Yichuan Zou,Lei Liang,Lu Zhang,Linxun Chen,Zhaojie Liu,Xuanping Li,Guorui Zhou,Kaiqiao Zhan,Kun Gai*

Main category: cs.IR

TL;DR: 提出PushGen自动化框架生成高质量推送通知，结合可控类别提示技术和奖励模型，经实验验证有效并大规模应用。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型进行推送内容生成时，在保持风格控制和可靠质量评估方面存在挑战，影响用户参与度。

Method: PushGen结合可控类别提示技术引导大语言模型输出期望风格，以及奖励模型对生成候选进行排序和选择。

Result: 广泛的离线和在线实验证明了其有效性，已在大规模工业应用中部署，每天为数亿用户服务。

Conclusion: PushGen能有效解决利用大语言模型进行推送内容生成时的风格控制和质量评估问题，可大规模应用。

Abstract: We present PushGen, an automated framework for generating high-quality push notifications comparable to human-crafted content. With the rise of generative models, there is growing interest in leveraging LLMs for push content generation. Although LLMs make content generation straightforward and cost-effective, maintaining stylistic control and reliable quality assessment remains challenging, as both directly impact user engagement. To address these issues, PushGen combines two key components: (1) a controllable category prompt technique to guide LLM outputs toward desired styles, and (2) a reward model that ranks and selects generated candidates. Extensive offline and online experiments demonstrate its effectiveness, which has been deployed in large-scale industrial applications, serving hundreds of millions of users daily.

</details>


### [63] [RecGPT-V2 Technical Report](https://arxiv.org/abs/2512.14503)
*Chao Yi,Dian Chen,Gaoyang Guo,Jiakai Tang,Jian Wu,Jing Yu,Mao Zhang,Wen Chen,Wenjun Yang,Yujie Luo,Yuning Jiang,Zhujin Gao,Bo Zheng,Binbin Cao,Changfa Wu,Dixuan Wang,Han Wu,Haoyi Hu,Kewei Zhu,Lang Tian,Lin Yang,Qiqi Huang,Siqi Yang,Wenbo Su,Xiaoxiao He,Xin Tong,Xu Chen,Xunke Xi,Xiaowei Huang,Yaxuan Wu,Yeqiu Yang,Yi Hu,Yujin Yuan,Yuliang Yan,Zile Zhou*

Main category: cs.IR

TL;DR: 论文指出RecGPT - V1存在不足，提出RecGPT - V2，介绍其四项创新改进及效果，证明大规模部署可行性。


<details>
  <summary>Details</summary>
Motivation: 解决RecGPT - V1存在的计算效率低、解释多样性不足、泛化能力有限和评估不匹配人类标准等问题。

Method: 提出Hierarchical Multi - Agent System、Hybrid Representation Inference、Meta - Prompting framework、constrained reinforcement learning和Agent - as - a - Judge framework等方法。

Result: 减少60% GPU消耗，提升多项指标，如专属召回率、标签预测、解释接受度等，在线A/B测试显示CTR、IPV、TV和NER等显著提升。

Conclusion: RecGPT - V2证明了大规模部署大语言模型驱动的意图推理的技术可行性和商业可行性。

Abstract: Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.
  To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.

</details>


### [64] [Pairwise Comparison for Bias Identification and Quantification](https://arxiv.org/abs/2512.14565)
*Fabian Haak,Philipp Schaer*

Main category: cs.IR

TL;DR: 论文提出利用成对比较进行偏见标注以减少标注工作量，通过模拟评估不同实现方式，将其应用于人类标注的偏见基准数据集进行评估，支持成对比较作为量化主观语言方面的实用基础。


<details>
  <summary>Details</summary>
Motivation: 在线新闻和社交媒体中的语言偏见普遍但难以测量，现有识别和量化方法因主观性、上下文依赖和高质量标注数据集稀缺而遇困难，需降低标注工作量。

Method: 利用成对比较进行偏见标注，在模拟环境中研究不同评级技术和三种成本感知替代方案的参数效果，对人类标注的偏见基准数据集应用该方法并评估。

Result: 发现成对比较可作为量化主观语言方面的实用基础，实现可重复的偏见分析。

Conclusion: 贡献了比较和匹配组件的优化、包括模拟和实际数据应用的端到端评估以及成本感知大规模标注的实施蓝图。

Abstract: Linguistic bias in online news and social media is widespread but difficult to measure. Yet, its identification and quantification remain difficult due to subjectivity, context dependence, and the scarcity of high-quality gold-label datasets. We aim to reduce annotation effort by leveraging pairwise comparison for bias annotation. To overcome the costliness of the approach, we evaluate more efficient implementations of pairwise comparison-based rating. We achieve this by investigating the effects of various rating techniques and the parameters of three cost-aware alternatives in a simulation environment. Since the approach can in principle be applied to both human and large language model annotation, our work provides a basis for creating high-quality benchmark datasets and for quantifying biases and other subjective linguistic aspects.
  The controlled simulations include latent severity distributions, distance-calibrated noise, and synthetic annotator bias to probe robustness and cost-quality trade-offs. In applying the approach to human-labeled bias benchmark datasets, we then evaluate the most promising setups and compare them to direct assessment by large language models and unmodified pairwise comparison labels as baselines. Our findings support the use of pairwise comparison as a practical foundation for quantifying subjective linguistic aspects, enabling reproducible bias analysis. We contribute an optimization of comparison and matchmaking components, an end-to-end evaluation including simulation and real-data application, and an implementation blueprint for cost-aware large-scale annotation

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [65] [Physics-Guided Deep Learning for Heat Pump Stress Detection: A Comprehensive Analysis on When2Heat Dataset](https://arxiv.org/abs/2512.13696)
*Md Shahabub Alam,Md Asifuzzaman Jishan,Ayan Kumar Ghosh*

Main category: cs.LG

TL;DR: 本文提出PG - DNN方法用于热泵应力分类，用When2Heat数据集，模型在测试和验证中有高准确率，优于基线方法，提供了生产可用解决方案。


<details>
  <summary>Details</summary>
Motivation: 热泵系统运行应力检测因复杂热力交互和有限实际数据而具有挑战性。

Method: 采用物理引导的特征选择和类定义，并结合具有5个隐藏层和双重正则化策略的深度神经网络架构。

Result: 模型测试准确率78.1%，验证准确率78.5%，优于多种基线方法；消融研究验证关键部分有效性。

Conclusion: 所提系统为热泵应力检测提供了生产就绪的解决方案。

Abstract: Heat pump systems are critical components in modern energy-efficient buildings, yet their operational stress detection remains challenging due to complex thermodynamic interactions and limited real-world data. This paper presents a novel Physics-Guided Deep Neural Network (PG-DNN) approach for heat pump stress classification using the When2Heat dataset, containing 131,483 samples with 656 features across 26 European countries. The methodology integrates physics-guided feature selection and class definition with a deep neural network architecture featuring 5 hidden layers and dual regularization strategies. The model achieves 78.1\% test accuracy and 78.5% validation accuracy, demonstrating significant improvements over baseline approaches: +5.0% over shallow networks, +4.0% over limited feature sets, and +2.0% over single regularization strategies. Comprehensive ablation studies validate the effectiveness of physics-guided feature selection, variable thresholding for realistic class distribution, and cross-country energy pattern analysis. The proposed system provides a production-ready solution for heat pump stress detection with 181,348 parameters and 720 seconds training time on AMD Ryzen 9 7950X with RTX 4080 hardware.

</details>


### [66] [Scaling and Transferability of Annealing Strategies in Large Language Model Training](https://arxiv.org/abs/2512.13705)
*Siqi Wang,Zhengyu Chen,Teng Xiao,Zheqi Lv,Jinluan Yang,Xunliang Cai,Jingang Wang,Xiaomeng Li*

Main category: cs.LG

TL;DR: 研究大语言模型学习率退火策略的可迁移性，提出通用预测框架，通过实验验证优化方法有效性。


<details>
  <summary>Details</summary>
Motivation: 当前理解不同模型配置下的最优学习率退火策略仍具挑战性，研究其可迁移性并优化策略。

Method: 研究退火动态的可迁移性，基于WSD调度器细化通用预测框架，框架纳入训练步骤、最大学习率和退火行为。

Result: 大量Dense和MoE模型实验验证，最优退火比率有一致模式，可跨不同训练配置迁移。

Conclusion: 无需大量超参数搜索就能选最优退火策略，小模型可作为优化大模型训练动态的可靠代理，论文为策略选择提供实用指导。

Abstract: Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.

</details>


### [67] [Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training](https://arxiv.org/abs/2512.13706)
*John Graham Reynolds*

Main category: cs.LG

TL;DR: 微调大语言模型做数学推理任务时会出现灾难性遗忘，提出混合训练策略可消除该问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在微调特定任务（如数学推理）时出现的灾难性遗忘问题。

Method: 在DeepMind Mathematics数据集上微调Flan - T5 - Base，测量在MultiNLI上的遗忘情况，提出数学和NLI示例混合训练策略。

Result: 纯数学训练提升数学准确率但使NLI准确率大幅下降，混合训练可消除灾难性遗忘，保持数学性能，最小NLI暴露也有有效正则化效果。

Conclusion: 专业化无需遗忘通用能力，混合训练对更大模型可能有更多好处。

Abstract: When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\% to 12.0\% but causes NLI accuracy to collapse from 81.0\% to 16.5\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\% math accuracy (matching math-only) while preserving 86.2\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.

</details>


### [68] [Variational Physics-Informed Ansatz for Reconstructing Hidden Interaction Networks from Steady States](https://arxiv.org/abs/2512.13708)
*Kaiming Luo*

Main category: cs.LG

TL;DR: 提出变分物理信息近似（VPIA）方法，从异构稳态数据推断复杂系统交互结构，在多场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有重建方法难以处理复杂系统的非线性、异构和高阶耦合，特别是仅能观测稳态时。

Method: 提出VPIA方法，将动力学稳态约束嵌入可微变分表示，通过最小化物理稳态残差重建耦合，结合残差采样和自然梯度优化。

Result: 在不同非线性系统中，VPIA能在大量噪声下准确恢复有向、加权和多体结构。

Conclusion: VPIA为仅能获取快照观测的复杂交互网络提供统一且稳健的物理约束推断框架。

Abstract: The interaction structure of a complex dynamical system governs its collective behavior, yet existing reconstruction methods struggle with nonlinear, heterogeneous, and higher-order couplings, especially when only steady states are observable. We propose a Variational Physics-Informed Ansatz (VPIA) that infers general interaction operators directly from heterogeneous steady-state data. VPIA embeds the steady-state constraints of the dynamics into a differentiable variational representation and reconstructs the underlying couplings by minimizing a physics-derived steady-state residual, without requiring temporal trajectories, derivative estimation, or supervision. Residual sampling combined with natural-gradient optimization enables scalable learning of large and higher-order networks. Across diverse nonlinear systems, VPIA accurately recovers directed, weighted, and multi-body structures under substantial noise, providing a unified and robust framework for physics-constrained inference of complex interaction networks in settings where only snapshot observations are available.

</details>


### [69] [Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables](https://arxiv.org/abs/2512.13710)
*Edwin Oluoch Awino,Denis Machanda*

Main category: cs.LG

TL;DR: 结合SAR影像与环境水文数据，用机器学习模型对肯尼亚Nyando流域洪水易发性建模，RF模型表现最佳，成果对减灾等有重要意义。


<details>
  <summary>Details</summary>
Motivation: 洪水是全球破坏性极大的自然灾害，本文旨在对肯尼亚Nyando流域进行洪水易发性建模。

Method: 结合Sentinel - 1 SAR数据生成洪水清单，选取6个影响因子，用逻辑回归、CART、SVM和RF四种监督分类器进行训练，并评估模型性能。

Result: RF模型预测性能最高，基于RF的易发性地图显示维多利亚湖附近的Kano平原洪水脆弱性最高。

Conclusion: 结合SAR数据和集成机器学习方法对数据有限地区的洪水易发性制图有价值，成果可用于减灾、土地规划和预警系统开发。

Abstract: Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.

</details>


### [70] [Cornserve: Efficiently Serving Any-to-Any Multimodal Models](https://arxiv.org/abs/2512.14098)
*Jeff J. Ma,Jae-Won Chung,Jisang Ahn,Yizhuo Liang,Akshay Jajoo,Myungjin Lee,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: 介绍Cornserve，一个用于Any - to - Any模型的在线服务系统，能优化部署并高效处理模型异构性，评估显示性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: Any - to - Any模型在服务时存在请求类型、计算路径和计算扩展的异构性问题，需要高效服务系统。

Method: 允许开发者描述通用Any - to - Any模型计算图，规划器根据模型和工作负载特征找到优化部署计划，分布式运行时按计划执行模型。

Result: Cornserve能高效服务多种Any - to - Any模型和工作负载，吞吐量提升达3.81倍，尾延迟降低达5.79倍。

Conclusion: Cornserve可有效解决Any - to - Any模型服务中的异构性问题，性能优于现有解决方案。

Abstract: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.
  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\times$ throughput improvement and up to 5.79$\times$ tail latency reduction over existing solutions.

</details>


### [71] [Delete and Retain: Efficient Unlearning for Document Classification](https://arxiv.org/abs/2512.13711)
*Aadya Goel,Mayuri Sridhar*

Main category: cs.LG

TL;DR: 研究文档分类器的类级遗忘，提出Hessian Reassignment方法，速度快且效果好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的遗忘已有进展，但文档分类模型的遗忘研究较少，因此开展文档分类器的类级遗忘研究。

Method: 提出Hessian Reassignment方法，分两步：一是通过共轭梯度求解Hessian向量系统，减去目标类所有训练点的贡献；二是通过Top - 1分类保证决策空间。

Result: 在标准文本基准上，该方法保留类的准确率接近无该类的全量重训，且速度快得多，同时降低了移除类的成员推理优势。

Conclusion: 该方法为文档分类中的高效类遗忘提供了实用、有原则的途径。

Abstract: Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.

</details>


### [72] [Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean](https://arxiv.org/abs/2512.14686)
*Chuan He*

Main category: cs.LG

TL;DR: 本文研究随机一阶方法在尾指数α∈(0,2]的重尾噪声下的复杂度，通过新的偏差 - 方差权衡分析得到改进的复杂度保证，并经数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有复杂度结果多只覆盖α∈(1,2]情况，α趋近1时复杂度界限趋于无穷，缺乏对α∈(0,2]一般情况的研究。

Method: 对梯度裁剪中的偏差 - 方差权衡进行新颖分析。

Result: 当控制噪声尾的对称性测度时，裁剪的随机一阶方法在任何尾指数α∈(0,2]的重尾噪声下实现了改进的复杂度保证，得到新的统一复杂度保证，且可与轻尾噪声下的经典分析结合。

Conclusion: 理论分析得到数值实验验证，所提分析方法有效。

Abstract: Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $α$ of the noise. Nonetheless, existing complexity results often cover only the case $α\in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $α$ approaches $1$. This paper tackles the general case of noise with tail index $α\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $α\in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.

</details>


### [73] [Improving Slow Transfer Predictions: Generative Methods Compared](https://arxiv.org/abs/2512.14522)
*Jacob Taegon Kim,Alex Sim,Kesheng Wu,Jinoh Kim*

Main category: cs.LG

TL;DR: 该项目聚焦解决科学计算网络数据传输性能预测中机器学习模型的类别不平衡问题，分析比较多种增强策略，发现类别不平衡比率增加时性能提升不显著，先进技术不比简单分层抽样有明显优势。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在数据传输性能预测中因类别不平衡导致的预测能力瓶颈问题，提高预测准确性。

Method: 分析比较传统过采样方法和生成技术等多种增强策略，调整训练数据集的类别不平衡比率。

Result: 随着不平衡比率增加，增强策略对性能提升不明显。

Conclusion: 即使是先进的CTGAN技术，相比简单分层抽样也无显著性能提升。

Abstract: Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.

</details>


### [74] [Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data](https://arxiv.org/abs/2512.13712)
*Eric Guo*

Main category: cs.LG

TL;DR: 本文开发机器学习框架，结合废水监测、气象和空气质量数据预测美国RSV相关住院情况，发现废水RSV水平是最强预测因子，还揭示了不同群体和地区的差异，开发了交互式仪表盘。


<details>
  <summary>Details</summary>
Motivation: RSV是幼儿住院的主要原因，其爆发受环境条件影响大，需预测RSV相关住院情况以进行公共卫生干预和资源分配。

Method: 整合废水监测、气象和空气质量数据，用CART、随机森林和提升等分类模型训练预测每周RSV相关住院率。

Result: 废水RSV水平是最强预测因子，美洲原住民和阿拉斯加原住民住院率更高，高海拔州住院率也更高。

Conclusion: 结合环境和社区监测数据可用于预测RSV爆发，开发的交互式仪表盘便于模型的实际应用。

Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.

</details>


### [75] [Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints](https://arxiv.org/abs/2512.13717)
*Ekaterina Sysoykova,Bernhard Anzengruber-Tanase,Michael Haslgrubler,Philipp Seidl,Alois Ferscha*

Main category: cs.LG

TL;DR: 提出两阶段联邦少样本学习（FFSL）框架用于个性化基于EEG的癫痫发作检测，该方法在TUH事件语料库上训练和评估，结果表明FFSL能在现实数据可用性和隐私约束下支持有效的患者自适应癫痫发作检测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大型集中标注数据集，但临床中EEG数据稀缺、分散且受隐私法规限制，创建基于AI的癫痫发作检测模型具有挑战。

Method: 提出两阶段FFSL框架，第一阶段使用联邦学习在非IID模拟医院站点微调预训练的生物信号变压器（BIOT）；第二阶段使用仅五个标记的EEG片段使分类器适应每个患者。

Result: 联邦微调平衡准确率0.43，Cohen's kappa为0.42，加权F1为0.69；FFSL阶段，客户端特定模型平均平衡准确率0.77，Cohen's kappa为0.62，加权F1为0.73。

Conclusion: FFSL能在现实数据可用性和隐私约束下支持有效的患者自适应癫痫发作检测。

Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.

</details>


### [76] [Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce](https://arxiv.org/abs/2512.13726)
*Sayak Chakrabarty,Souradip Pal*

Main category: cs.LG

TL;DR: 本文聚焦有限用户时间预算下的推荐问题，评估强化学习算法在资源约束下生成高参与度推荐的能力，通过实验得出相关结论。


<details>
  <summary>Details</summary>
Motivation: 传统推荐任务未考虑有限用户时间预算这一资源约束，需平衡物品相关性和评估成本，以提高推荐的用户参与度。

Method: 使用强化学习算法，基于阿里巴巴的个性化重排序数据集，将时间受限的石板推荐统一建模为具有预算感知效用的马尔可夫决策过程，搭建模拟框架研究策略行为。

Result: 实验表明，在时间预算紧张的情况下，基于策略和离线策略控制比传统基于上下文多臂老虎机的方法性能更优。

Conclusion: 强化学习算法能在资源约束下学习用户偏好和时间预算模式，生成更具参与潜力的推荐。

Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.

</details>


### [77] [RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing](https://arxiv.org/abs/2512.13727)
*Yuhan Tang,Kangxin Cui,Jung Ho Park,Yibo Zhao,Xuan Jiang,Haoze He,Dingyi Zhuang,Shenhao Wang,Jiangbo Yu,Haris Koutsopoulos,Jinhua Zhao*

Main category: cs.LG

TL;DR: 针对网约车平台供需不确定下的自适应延迟匹配问题，提出RAST - MoE框架，参数少却性能优，提升总奖励，减少延迟，证明MoE增强RL在复杂时空决策中的潜力。


<details>
  <summary>Details</summary>
Motivation: 网约车平台在供需不确定下需平衡乘客等待时间和系统效率，现有强化学习方法对交通动态简化或忽视复杂时空模式，故需更优方法。

Method: 提出Regime - Aware Spatio - Temporal Mixture - of - Experts (RAST - MoE)，将自适应延迟匹配形式化为具有自注意力MoE编码器的状态感知MDP，用物理信息拥堵替代物和自适应奖励方案。

Result: 框架仅12M参数就优于强基线，在旧金山优步轨迹数据上，总奖励提高超13%，匹配和取客延迟分别减少10%和15%，跨需求场景稳健，训练稳定。

Conclusion: MoE增强的强化学习在具有复杂时空动态的大规模决策中有应用潜力。

Abstract: Ride-hailing platforms face the challenge of balancing passenger waiting times with overall system efficiency under highly uncertain supply-demand conditions. Adaptive delayed matching creates a trade-off between matching and pickup delays by deciding whether to assign drivers immediately or batch requests. Since outcomes accumulate over long horizons with stochastic dynamics, reinforcement learning (RL) is a suitable framework. However, existing approaches often oversimplify traffic dynamics or use shallow encoders that miss complex spatiotemporal patterns.
  We introduce the Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE), which formalizes adaptive delayed matching as a regime-aware MDP equipped with a self-attention MoE encoder. Unlike monolithic networks, our experts specialize automatically, improving representation capacity while maintaining computational efficiency. A physics-informed congestion surrogate preserves realistic density-speed feedback, enabling millions of efficient rollouts, while an adaptive reward scheme guards against pathological strategies.
  With only 12M parameters, our framework outperforms strong baselines. On real-world Uber trajectory data (San Francisco), it improves total reward by over 13%, reducing average matching and pickup delays by 10% and 15% respectively. It demonstrates robustness across unseen demand regimes and stable training. These findings highlight the potential of MoE-enhanced RL for large-scale decision-making with complex spatiotemporal dynamics.

</details>


### [78] [CurvaDion: Curvature-Adaptive Distributed Orthonormalization](https://arxiv.org/abs/2512.13728)
*Bhavesh Kumar,Roger Jin,Jeffrey Quesnelle*

Main category: cs.LG

TL;DR: 随着语言模型参数增至数万亿，分布式训练中梯度同步是瓶颈。本文提出CurvaDion，用RMMC检测高曲率区域，实现99%通信减少且收敛效果与基线匹配。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Dion在优化过程中每步都同步，未考虑训练不同阶段同步需求差异，而训练中不同区域对同步要求不同。

Method: 引入CurvaDion，使用相对最大动量变化（RMMC）检测高曲率区域，RMMC利用优化中已计算的动量动态作为方向曲率的代理，每层仅增加O(d)操作。

Result: CurvaDion实现了99%的通信减少，在1.6亿到13亿参数的模型上收敛效果与基线相匹配。

Conclusion: CurvaDion能有效减少分布式训练中的通信量，同时保证模型收敛效果。

Abstract: As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.

</details>


### [79] [Dropout Neural Network Training Viewed from a Percolation Perspective](https://arxiv.org/abs/2512.13853)
*Finley Devlin,Jaron Sanders*

Main category: cs.LG

TL;DR: 研究带dropout的深度神经网络训练中渗流的存在和影响，发现渗流效应及可能导致的训练崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 探究带dropout训练神经网络时，随机移除连接与统计物理中渗流的相似性及影响，避免因移除过多连接导致网络无法预测。

Method: 研究模仿神经网络中dropout的新渗流模型，刻画网络拓扑与路径问题的关系。

Result: 理论表明dropout中存在渗流效应，无偏置神经网络训练会因该效应崩溃，启发式认为有偏置网络也会如此。

Conclusion: 带dropout训练神经网络存在渗流效应，可能导致训练崩溃。

Abstract: In this work, we investigate the existence and effect of percolation in training deep Neural Networks (NNs) with dropout. Dropout methods are regularisation techniques for training NNs, first introduced by G. Hinton et al. (2012). These methods temporarily remove connections in the NN, randomly at each stage of training, and update the remaining subnetwork with Stochastic Gradient Descent (SGD). The process of removing connections from a network at random is similar to percolation, a paradigm model of statistical physics.
  If dropout were to remove enough connections such that there is no path between the input and output of the NN, then the NN could not make predictions informed by the data. We study new percolation models that mimic dropout in NNs and characterise the relationship between network topology and this path problem. The theory shows the existence of a percolative effect in dropout. We also show that this percolative effect can cause a breakdown when training NNs without biases with dropout; and we argue heuristically that this breakdown extends to NNs with biases.

</details>


### [80] [Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution](https://arxiv.org/abs/2512.13729)
*Jacob Schnell,Aditya Makkar,Gunadi Gani,Aniket Srinivasan Ashok,Darren Lo,Mike Optis,Alexander Wong,Yuhao Chen*

Main category: cs.LG

TL;DR: 提出复合无分类器引导（CCFG）方法用于风数据超分辨率任务，构建WindDM模型，实现高保真重建且成本低。


<details>
  <summary>Details</summary>
Motivation: 传统风数据重建方法难以兼顾成本与准确性，现有深度学习方法未充分利用风数据多输入通道特点。

Method: 提出CCFG方法，可用于预训练扩散模型；构建WindDM模型用于工业规模风动力学重建。

Result: CCFG在风超分辨率任务输出比CFG保真度更高；WindDM在深度学习模型中达到了最先进的重建质量，成本比传统方法低1000倍。

Conclusion: CCFG和WindDM在风数据重建中有效，能以低成本实现高保真重建。

Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.

</details>


### [81] [Understanding the Gain from Data Filtering in Multimodal Contrastive Learning](https://arxiv.org/abs/2512.14230)
*Divyansh Pareek,Sewoong Oh,Simon S. Du*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $η\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\frac{1}{η\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\frac{1}{\sqrt{ηn}}$ in the large $η$ regime, and by $\frac{1}{\sqrt{n}}$ in the small $η$ regime.

</details>


### [82] [PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion](https://arxiv.org/abs/2512.13732)
*Weijie Yang,Xun Zhang*

Main category: cs.LG

TL;DR: 提出物理反演求解器PIS用于解决PDE约束物理参数估计问题，在多个挑战性PDE逆问题中表现出色，减少反演误差。


<details>
  <summary>Details</summary>
Motivation: 现有深度和算子学习模型在有限间接测量、观测稀疏不规则情况下表现不佳，无法进行可靠反演和不确定性量化。

Method: 提出PIS，使用基于Set Transformer的编码器处理任意数量和几何形状的测量，采用余弦退火稀疏课程提升鲁棒性，并进行信息论分析。

Result: 在三个挑战性PDE逆问题中，PIS保持稳定准确，减少反演误差12.28% - 88.73%，可靠生成校准后验样本。

Conclusion: PIS是一种强大、通用且对稀疏性有独特适应性的解决方案，适用于任意和严重欠采样观测下的物理反演。

Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.

</details>


### [83] [Low-Rank Compression of Language Models via Differentiable Rank Selection](https://arxiv.org/abs/2512.13733)
*Sidhant Sundrani,Francesco Tudisco,Pasquale Minervini*

Main category: cs.LG

TL;DR: 提出免微调的梯度方法LLRC进行大语言模型低秩压缩，在多个任务和压缩率上表现优于竞品。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型低秩分解压缩方法在选择各层最优秩以平衡压缩率和下游任务准确率上存在问题，启发式方法结果欠佳，基于梯度的方法若无压缩后微调性能不如启发式方法。

Method: 提出Learning to Low-Rank Compress (LLRC)，一种免微调的基于梯度的方法，使用校准数据集仅训练掩码权重来选择奇异值，同时最小化中间激活与原模型的差异。

Result: 在常识推理和开放域问答任务的不同压缩率下，优于同样免压缩后微调的竞品方法；如在Llama - 2 - 13B压缩率20%时，在多个数据集上优于STRS；也优于SVD - LLM和LLM - Pruner的免微调变体，与LLM - Pruner的微调变体表现相当。

Conclusion: LLRC方法有效解决了大语言模型低秩压缩中选择最优秩的问题，在免微调情况下取得了较好的压缩和任务性能平衡。

Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.

</details>


### [84] [Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation](https://arxiv.org/abs/2512.13734)
*Haochen Yuan,Yang Zhang,Xiang He,Quan Z. Sheng,Zhongjie Wang*

Main category: cs.LG

TL;DR: 提出基于参数高效微调（PEFT）的联邦推荐（FR）训练框架，减少嵌入参数传输量，提高通信效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有FR研究忽略嵌入参数开销问题，大量参数阻碍通信效率。

Method: 提出基于PEFT的嵌入设计的FR训练框架，整合LoRA、基于哈希的编码等PEFT技术，探索使用RQ - VAE作为新策略。

Result: 在各种FR模型骨干和数据集上的实验表明，框架显著降低通信开销并提高准确性。

Conclusion: 所提出的框架能有效解决FR通信效率问题，代码开源。

Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.

</details>


### [85] [DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series](https://arxiv.org/abs/2512.13735)
*Xuechun Liu,Heli Sun,Xuecheng Wu,Ruichen Cao,Yunyun Shi,Dingkang Yang,Haoran Li*

Main category: cs.LG

TL;DR: 提出长短期双路径框架 DARTs 用于多变量时间序列异常检测，实验表明其有优越性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从高维含噪时间序列学习表示时，难以捕捉长距离时空依赖，本文旨在解决此局限。

Method: 提出含窗口感知时空软融合机制的长短期双路径框架 DARTs，包括短期路径中多视图稀疏图学习器和扩散多关系图单元，长期路径中多尺度时空图构造器，以及窗口感知时空软融合机制。

Result: 在主流数据集上的定性和定量实验结果表明 DARTs 具有优越性和鲁棒性，消融研究探索了组件关键设计因素。

Conclusion: DARTs 框架在多变量时间序列异常检测方面表现优异，代码和模型将公开。

Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.

</details>


### [86] [TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection](https://arxiv.org/abs/2512.13736)
*Li-Xuan Zhao,Chen-Yang Xu,Wen-Qiang Li,Bo Wang,Rong-Xing Wei,Qing-Hao Menga*

Main category: cs.LG

TL;DR: 本文提出TF - MCL模型用于MDD检测，在公开数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于EEG信号的MDD监督检测方法标注困难，现有对比学习方法无法有效表征EEG信号时频分布，获取低语义数据表示能力不足。

Method: 提出TF - MCL模型，通过融合映射头生成时频混合表示，优化多域交叉损失函数重建表示分布。

Result: 在MODMA和PRED+CT数据集上，模型准确率显著提升，分别比现有SOTA方法高5.87%和9.96%。

Conclusion: TF - MCL模型能有效增强综合时频信息能力和获取融合表示能力，提升MDD检测准确率。

Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.

</details>


### [87] [The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models](https://arxiv.org/abs/2512.13741)
*Md. Hasib Ur Rahman*

Main category: cs.LG

TL;DR: 提出层流假设检测大语言模型越狱攻击，通过新指标实验验证，该指标可作检测器和诊断工具。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对抗越狱攻击的防御策略存在不足，需新方法。

Method: 引入层流假设，用层间余弦速度方差这一零样本指标量化语义湍流。

Result: 不同小语言模型在攻击下语义湍流变化不同，如Qwen2 - 1.5B增加75.4%，Gemma - 2B减少22.0%。

Conclusion: 语义湍流可作轻量级实时越狱检测器和黑盒模型安全架构诊断工具。

Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.

</details>


### [88] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 研究评估资源受限下金融新闻情感分类的嵌入方法，发现预训练嵌入在数据不足时效果不佳，建议从业者考虑替代方法。


<details>
  <summary>Details</summary>
Motivation: 标准自然语言处理方法在处理金融情感分析小数据集时面临挑战，需评估嵌入方法在资源受限环境中的表现。

Method: 将Word2Vec、GloVe和句子转换器表示与梯度提升结合，在手动标注的新闻标题上进行实验。

Result: 验证集和测试集表现差距大，模型不如简单基线，预训练嵌入在数据不足时收益递减，小验证集会导致过拟合。

Conclusion: 仅靠嵌入质量无法解决情感分类中的数据稀缺问题，资源有限的从业者应考虑替代方法。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [89] [MIDUS: Memory-Infused Depth Up-Scaling](https://arxiv.org/abs/2512.13751)
*Taero Kim,Hoyoon Byun,Youngjun Choi,Sungrae Park,Kyungwoo Song*

Main category: cs.LG

TL;DR: 传统深度扩展策略依赖前馈网络，效率受限。论文提出的记忆注入深度扩展（MIDUS）方法用头部记忆层替代前馈网络，实验显示其性能优于基线且参数效率高。


<details>
  <summary>Details</summary>
Motivation: 传统深度扩展（DUS）策略依赖前馈网络（FFNs），效率和可实现的增益受限，需要更有效的扩展大型语言模型容量的方法。

Method: 提出记忆注入深度扩展（MIDUS）方法，用头部记忆（HML）层替换重复块中的FFNs，为每个注意力头分配独立内存库，结合稀疏内存访问和头部表示，并引入高效的每个头部值分解模块。

Result: 在持续预训练（CPT）实验中，MIDUS相较于强大的DUS基线有显著性能提升，同时保持了高效的参数占用。

Conclusion: MIDUS的头部记忆设计使其成为传统FFN复制用于深度扩展的一种有吸引力且资源高效的替代方案。

Abstract: Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.

</details>


### [90] [Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention](https://arxiv.org/abs/2512.13758)
*Léo Hein,Giovanni de Nunzio,Giovanni Chierchia,Aurélie Pirayre,Laurent Najman*

Main category: cs.LG

TL;DR: 提出HDA - STGNN框架解决全网交通流量估计问题，利用速度、道路属性和拓扑信息，消融研究证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有交通流量估计方法有局限性，如预测模型忽略未监测道路，空间插补方法依赖推理时流量数据，在传感器少的城市适用性有限。

Method: 提出Hybrid Directed - Attention Spatio - Temporal Graph Neural Network (HDA - STGNN) 归纳式深度学习框架，利用速度轮廓、静态道路属性和道路网络拓扑进行预测。

Result: 通过广泛的消融研究，模型能够捕捉复杂的时空依赖关系，并突出拓扑信息对准确的全网交通流量估计的价值，且推理时不依赖流量数据。

Conclusion: HDA - STGNN框架能有效解决全网交通流量估计问题，不依赖推理时的流量数据。

Abstract: Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.

</details>


### [91] [Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training](https://arxiv.org/abs/2512.13770)
*Huaiyuan Xiao,Fadi Dornaika,Jingjun Bi*

Main category: cs.LG

TL;DR: 提出半监督GCN模型MV - SupGCN，结合多种互补组件，实验表明其性能超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCN的多视图学习方法未能充分利用视图间互补信息，特征表示不佳、性能受限。

Method: 设计结合交叉熵损失和监督对比损失的联合损失函数；结合KNN和半监督图构建方法；提出集成对比学习和伪标签的统一框架。

Result: MV - SupGCN在多个基准测试中始终超越现有方法。

Conclusion: 提出的集成方法有效。

Abstract: The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN

</details>


### [92] [Constrained Policy Optimization via Sampling-Based Weight-Space Projection](https://arxiv.org/abs/2512.13788)
*Shengfan Cao,Francesco Borrelli*

Main category: cs.LG

TL;DR: 研究参数需满足未知安全约束的受限策略学习，提出SCPO方法，有安全保证，在任务中表现良好


<details>
  <summary>Details</summary>
Motivation: 安全关键学习需策略在安全范围内提升性能，研究参数满足未知基于滚动的安全约束的策略学习

Method: 提出SCPO，一种基于采样的权重空间投影方法，结合轨迹滚动和光滑性边界构建局部安全区域，通过凸SOCP投影梯度更新

Result: 建立归纳安全保证，在有稳定备份策略的约束控制中确保闭环稳定，在相关任务中拒绝不安全更新、保持可行性并提升目标性能

Conclusion: SCPO方法有效，能在满足安全约束的同时提升策略性能

Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.

</details>


### [93] [EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models](https://arxiv.org/abs/2512.13806)
*Siegfried Ludwig,Stylianos Bakas,Konstantinos Barmpas,Georgios Zoumpourlis,Dimitrios A. Adamos,Nikolaos Laskaris,Yannis Panagakis,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 提出一种弱监督方法EEG - D3用于跨EEG数据集训练深度学习模型，能分离大脑活动潜在成分，防止隐藏过拟合，在多个任务中效果好，需少量标注数据。


<details>
  <summary>Details</summary>
Motivation: 深度学习解码EEG信号虽基准性能好，但在实际应用中因隐藏过拟合问题表现不佳。

Method: 引入Disentangled Decoding Decomposition (D3)方法，通过预测输入窗口样本位置分离大脑活动潜在成分，使用独立子网络架构，提出特征解释范式。

Result: 能可靠分离运动想象数据中大脑活动潜在成分，防止下游分类器隐藏过拟合，可用于睡眠阶段分类的少样本学习。

Conclusion: 该方法避免隐藏过拟合，泛化性好，只需要少量标注数据，还为神经科学界提供分离大脑过程和发现新动态的工具。

Abstract: Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.

</details>


### [94] [The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 提出Cross - Trace Verification Protocol (CTVP)框架，通过语义轨道分析验证不可信代码生成模型，检测后门行为，引入ARQ量化验证成本，证明方法具有可扩展性和理论依据。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在少人工监督下生成代码，引发对后门注入和恶意行为的担忧。

Method: 提出CTVP框架，利用模型对语义等价程序转换的执行轨迹预测，分析预测轨迹中的一致性模式来检测异常。引入ARQ量化验证成本。

Result: ARQ随轨道大小呈指数增长，理论分析建立信息论边界，表明对手无法通过训练改善。

Conclusion: 语义轨道分析为代码生成任务的AI控制提供了可扩展、有理论依据的方法。

Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.

</details>


### [95] [Explainable reinforcement learning from human feedback to improve alignment](https://arxiv.org/abs/2512.13837)
*Shicheng Liu,Siyuan Xu,Wenjie Qiu,Hangfan Zhang,Minghui Zhu*

Main category: cs.LG

TL;DR: 本文探讨将人类改进策略应用于强化学习从人类反馈（RLHF）以优化语言模型对齐，提出通过纠正不满意回复原因的改进方法，实验证明算法有效。


<details>
  <summary>Details</summary>
Motivation: 观察到经RLHF调整的语言模型仍会输出不满意回复，研究能否将人类改进策略应用于改善RLHF。

Method: 一是提出事后解释方法，通过识别导致不满意回复的训练数据来解释原因，将问题转化为有约束的组合优化问题并提出迭代数据选择算法求解；二是提出无学习方法，通过去除导致不满意回复的训练数据来改进，同时不显著降低对其他提示的满意回复。

Result: 实验结果表明所提算法能改进RLHF。

Conclusion: 所提通过纠正原因改进不满意回复的方法对RLHF有效。

Abstract: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.

</details>


### [96] [Topologically-Stabilized Graph Neural Networks: Empirical Robustness Across Domains](https://arxiv.org/abs/2512.13852)
*Jelena Losic*

Main category: cs.LG

TL;DR: 提出整合持久同调特征与稳定性正则化的框架增强图神经网络对结构扰动的鲁棒性，在多数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 图神经网络易受结构扰动影响，需提升其鲁棒性。

Method: 基于持久同调稳定性定理，结合GIN架构与从持久图像中提取的多尺度拓扑特征，通过受Hiraoka - Kusano启发的稳定性约束实现。

Result: 在六个不同数据集上对边缘扰动表现出卓越鲁棒性，保持有竞争力的准确性，多数数据集扰动下性能下降仅0 - 4%，显著优于基线稳定性。

Conclusion: 提供了理论有依据、经验上有效的鲁棒图学习方法，与拓扑正则化的最新进展相符。

Abstract: Graph Neural Networks (GNNs) have become the standard for graph representation learning but remain vulnerable to structural perturbations. We propose a novel framework that integrates persistent homology features with stability regularization to enhance robustness. Building on the stability theorems of persistent homology \cite{cohen2007stability}, our method combines GIN architectures with multi-scale topological features extracted from persistence images, enforced by Hiraoka-Kusano-inspired stability constraints. Across six diverse datasets spanning biochemical, social, and collaboration networks , our approach demonstrates exceptional robustness to edge perturbations while maintaining competitive accuracy. Notably, we observe minimal performance degradation (0-4\% on most datasets) under perturbation, significantly outperforming baseline stability. Our work provides both a theoretically-grounded and empirically-validated approach to robust graph learning that aligns with recent advances in topological regularization

</details>


### [97] [Measuring Uncertainty Calibration](https://arxiv.org/abs/2512.13872)
*Kamil Ciosek,Nicolò Felicioni,Sina Ghiassian,Juan Elenter Litwin,Francesco Tonolini,David Gustaffson,Eva Garcia Martin,Carmen Barcena Gonzales,Raphaëlle Bertrand-Lalo*

Main category: cs.LG

TL;DR: 本文为从有限数据集估计二元分类器的L1校准误差问题做出两方面贡献，给出上界和修改分类器方法，结果非渐近且无分布假设，最后给出实践建议。


<details>
  <summary>Details</summary>
Motivation: 解决从有限数据集估计二元分类器的L1校准误差问题。

Method: 一是为校准函数有界变差的分类器提供上界，二是提供修改分类器方法以有效上界校准误差且不影响性能、无严格假设。

Result: 得到非渐近且分布无关的结果。

Conclusion: 给出了在实践中测量校准误差的建议，方法可在实际数据集上以适度开销运行。

Abstract: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.

</details>


### [98] [Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization](https://arxiv.org/abs/2512.13880)
*Geofrey Owino,Bernard Shibwabo*

Main category: cs.LG

TL;DR: 提出端到端婴儿哭声分析管道，解决隐私、噪声和域偏移问题，实验结果好，证明有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现存婴儿哭声分类方案受音频数据隐私、对背景噪声敏感性以及记录环境域偏移的限制，需改进。

Method: 集成去噪自编码器、卷积分词器和Transformer编码器，采用通信高效的联邦学习训练，系统进行设备端去噪、自适应分割等操作，联邦训练采用正则化控制变量更新和8位适配器增量。

Result: 在相关数据集上，模型宏F1分数0.938、AUC 0.962、预期校准误差0.032，减少客户端每轮上传数据量，在NVIDIA Jetson Nano上实时边缘推理速度为每1秒频谱图帧96毫秒。

Conclusion: 该方案是一种适合联邦部署的、保护隐私、抗噪声且通信高效的婴儿哭声分类实用途径。

Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.

</details>


### [99] [OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction](https://arxiv.org/abs/2512.13886)
*Mohammad Mozaffari,Samuel Kushnir,Maryam Mehri Dehnavi,Amir Yazdanbakhsh*

Main category: cs.LG

TL;DR: 介绍OPTIMA，一种平衡准确性和可扩展性的一次性训练后剪枝方法，能在单加速器上大规模实现，提高零样本性能，创造新的准确性 - 效率权衡标杆。


<details>
  <summary>Details</summary>
Motivation: 现有训练后模型剪枝方法存在简单启发式方法降低准确性、原则性联合优化方法计算不可行的问题，需要平衡准确性和可扩展性的方法。

Method: 将掩码选择后的层权重重建表示为独立的行级二次规划问题，共享层Hessian矩阵，实现加速器友好的QP求解器并行求解小QP问题。

Result: OPTIMA能集成现有掩码选择器，在多个大语言模型家族和稀疏度方案中持续提高零样本性能，最高有3.97%的绝对准确率提升，在NVIDIA H100上40小时内端到端剪枝80亿参数变压器，峰值内存60GB。

Conclusion: OPTIMA为一次性训练后剪枝设定了新的准确性 - 效率权衡的最优水平。

Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.

</details>


### [100] [Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs](https://arxiv.org/abs/2512.13898)
*Rachit Bansal,Aston Zhang,Rishabh Tiwari,Lovish Madaan,Sai Surya Duvvuri,Devvrit Khatri,David Brandfonbrener,David Alvarez-Melis,Prajjwal Bhargava,Mihir Sanjay Kale,Samy Jelassi*

Main category: cs.LG

TL;DR: 长上下文大语言模型虽有进展但存在问题，现有推理时策略有局限，提出通过目标梯度更新上下文的方法，能显著提升性能，少量特定上下文训练优于当前推理时策略。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文大语言模型能处理大量文本但使用不可靠，推理时策略在长上下文任务上收益递减，且无法在某些条件下检索相关长上下文信号。

Method: 通过对给定上下文进行目标梯度更新，克服静态自注意力机制的局限性。

Result: 该方法在多个模型和长上下文基准测试中带来显著性能提升，如Qwen3 - 4B在部分基准测试中平均提升12.6和14.1个百分点。

Conclusion: 对于长上下文任务，少量上下文特定训练比当前推理时扩展策略（如生成更多思考标记）更能有效利用推理计算资源。

Abstract: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.

</details>


### [101] [Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America](https://arxiv.org/abs/2512.13910)
*Matheus Corrêa Domingos,Valdivino Alexandre de Santiago Júnior,Juliana Aparecida Anochi,Elcio Hideiti Shiguemori,Luísa Mirelle Costa dos Santos,Hércules Carlos dos Santos Pereira,André Estevam Costa Oliveira*

Main category: cs.LG

TL;DR: 文章研究不同机器学习和深度学习方法对南美洲2019年全季节降水预测，对比传统方法，验证深度学习模型用于气候预测的可行性。


<details>
  <summary>Details</summary>
Motivation: 气象变量预测有挑战，精确降水预报很重要，目前缺乏对纯数据驱动方法用于降水预报的广泛研究。

Method: 选用随机森林和XGBoost等经典机器学习技术、CNN 1D、LSTM和GRU等深度学习模型及传统动态模型巴西全球大气模式（BAM）进行降水预测，使用可解释人工智能解释模型行为。

Result: LSTM预测性能强，BAM结果最差，LSTM对强降水最准确但延迟高，若考虑成本，XGBoost延迟低但精度略降。

Conclusion: 证实深度学习模型用于气候预报是可行的，符合全球主要气象和气候预报中心的趋势。

Abstract: Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.

</details>


### [102] [Capturing reduced-order quantum many-body dynamics out of equilibrium via neural ordinary differential equations](https://arxiv.org/abs/2512.13913)
*Patrick Egenlauf,Iva Březinová,Sabine Andergassen,Miriam Klopotek*

Main category: cs.LG

TL;DR: 研究用神经ODE模型模拟量子多体系统TD2RDM动力学，指出其适用范围及对非局部闭合方案开发的指导意义。


<details>
  <summary>Details</summary>
Motivation: 精确波函数方法随粒子数指数增长，平均场方法忽略双粒子关联，TD2RDM形式主义的时间局部重建泛函有效性和存在性不明，需研究其在不同动力学机制下情况。

Method: 用神经ODE模型训练精确2RDM数据模拟动力学。

Result: 在双粒子和三粒子累积量皮尔逊相关性大的参数区域，神经ODE模型无需明确三粒子信息可重现动力学；反相关或不相关区域失败，时间平均三粒子关联积累大小是预测成功的主要因素。

Conclusion: 强三粒子关联积累区域需依赖记忆的核进行三粒子累积量重建；神经ODE可作为诊断工具，为关联量子物质数据驱动模拟提供途径。

Abstract: Out-of-equilibrium quantum many-body systems exhibit rapid correlation buildup that underlies many emerging phenomena. Exact wave-function methods to describe this scale exponentially with particle number; simpler mean-field approaches neglect essential two-particle correlations. The time-dependent two-particle reduced density matrix (TD2RDM) formalism offers a middle ground by propagating the two-particle reduced density matrix (2RDM) and closing the BBGKY hierarchy with a reconstruction of the three-particle cumulant. But the validity and existence of time-local reconstruction functionals ignoring memory effects remain unclear across different dynamical regimes. We show that a neural ODE model trained on exact 2RDM data (no dimensionality reduction) can reproduce its dynamics without any explicit three-particle information -- but only in parameter regions where the Pearson correlation between the two- and three-particle cumulants is large. In the anti-correlated or uncorrelated regime, the neural ODE fails, indicating that no simple time-local functional of the instantaneous two-particle cumulant can capture the evolution. The magnitude of the time-averaged three-particle-correlation buildup appears to be the primary predictor of success: For a moderate correlation buildup, both neural ODE predictions and existing TD2RDM reconstructions are accurate, whereas stronger values lead to systematic breakdowns. These findings pinpoint the need for memory-dependent kernels in the three-particle cumulant reconstruction for the latter regime. Our results place the neural ODE as a model-agnostic diagnostic tool that maps the regime of applicability of cumulant expansion methods and guides the development of non-local closure schemes. More broadly, the ability to learn high-dimensional RDM dynamics from limited data opens a pathway to fast, data-driven simulation of correlated quantum matter.

</details>


### [103] [Adaptive digital twins for predictive decision-making: Online Bayesian learning of transition dynamics](https://arxiv.org/abs/2512.13919)
*Eugenio Varetti,Matteo Torzoni,Marco Tezzele,Andrea Manzoni*

Main category: cs.LG

TL;DR: 本文展示适应性如何增强土木工程中数字孪生的价值实现，提出自适应数字孪生框架并通过案例评估。


<details>
  <summary>Details</summary>
Motivation: 研究适应性如何在土木工程中提升数字孪生的价值实现。

Method: 聚焦概率图模型表示的数字孪生中的状态转移模型，用动态贝叶斯网络建模物理和虚拟域双向交互，将状态转移概率视为带共轭先验的随机变量实现分层在线学习，用强化学习解决参数马尔可夫决策过程以精确更新动态策略。

Result: 提出的自适应数字孪生框架具有增强的个性化、更高的鲁棒性和更好的成本效益。

Conclusion: 所提出的自适应框架在土木工程数字孪生中有良好表现，可用于结构健康监测和维护规划。

Abstract: This work shows how adaptivity can enhance value realization of digital twins in civil engineering. We focus on adapting the state transition models within digital twins represented through probabilistic graphical models. The bi-directional interaction between the physical and virtual domains is modeled using dynamic Bayesian networks. By treating state transition probabilities as random variables endowed with conjugate priors, we enable hierarchical online learning of transition dynamics from a state to another through effortless Bayesian updates. We provide the mathematical framework to account for a larger class of distributions with respect to the current literature. To compute dynamic policies with precision updates we solve parametric Markov decision processes through reinforcement learning. The proposed adaptive digital twin framework enjoys enhanced personalization, increased robustness, and improved cost-effectiveness. We assess our approach on a case study involving structural health monitoring and maintenance planning of a railway bridge.

</details>


### [104] [Sliding Window Recurrences for Sequence Models](https://arxiv.org/abs/2512.13921)
*Dragos Secrieru,Garyk Brixi,Yoshua Bengio,Taiji Suzuki,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TL;DR: 提出分层分解框架开发滑动窗口递归（SWR），构建Phalanx层，在1B参数多混合模型中比优化Transformer提速10 - 40%且困惑度相当。


<details>
  <summary>Details</summary>
Motivation: 多混合架构因质量和性能优势将主导语言建模，需开发与GPU内存层次结构对齐的算法。

Method: 引入线性递归的分层分解框架，开发SWR，构建Phalanx层替代窗口注意力或线性递归。

Result: 在1B参数多混合模型中，Phalanx在4K到32K上下文长度上比优化Transformer提速10 - 40%，且困惑度相当。

Conclusion: 所提出的方法有效，能提升多混合模型性能。

Abstract: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.

</details>


### [105] [A Complete Guide to Spherical Equivariant Graph Transformers](https://arxiv.org/abs/2512.13927)
*Sophia Tang*

Main category: cs.LG

TL;DR: 本文为球形等变图神经网络提供基础介绍，构建架构并说明其应用。


<details>
  <summary>Details</summary>
Motivation: 球形等变图神经网络在三维分子和生物分子系统学习中有重要意义，需为其建模提供基础。

Method: 从群表示、球谐函数等构建基础，构建Tensor Field Network和SE(3)-Transformer架构。

Result: 通过清晰数学推导和代码片段，完成对架构的构建及原理说明。

Conclusion: 为化学、分子属性预测等领域研究人员和学习者提供自包含的介绍。

Abstract: Spherical equivariant graph neural networks (EGNNs) provide a principled framework for learning on three-dimensional molecular and biomolecular systems, where predictions must respect the rotational symmetries inherent in physics. These models extend traditional message-passing GNNs and Transformers by representing node and edge features as spherical tensors that transform under irreducible representations of the rotation group SO(3), ensuring that predictions change in physically meaningful ways under rotations of the input. This guide develops a complete, intuitive foundation for spherical equivariant modeling - from group representations and spherical harmonics, to tensor products, Clebsch-Gordan decomposition, and the construction of SO(3)-equivariant kernels. Building on this foundation, we construct the Tensor Field Network and SE(3)-Transformer architectures and explain how they perform equivariant message-passing and attention on geometric graphs. Through clear mathematical derivations and annotated code excerpts, this guide serves as a self-contained introduction for researchers and learners seeking to understand or implement spherical EGNNs for applications in chemistry, molecular property prediction, protein structure modeling, and generative modeling.

</details>


### [106] [EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training](https://arxiv.org/abs/2511.10333)
*Qingao Yi,Jiaang Duan,Hanwen Hu,Qin Hua,Haiyan Zhao,Shiyou Qian,Dingyu Yang,Jian Cao,Jinghua Tang,Yinghao Yu,Chenzhi Liao,Kangjin Wang,Liping Zhang*

Main category: cs.LG

TL;DR: 本文提出熵驱动动态梯度压缩框架EDGC，可降低通信延迟和训练时间，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练存在计算资源和内存挑战，现有静态梯度压缩方法忽略梯度动态性，需在不牺牲性能前提下加速训练。

Method: 提出EDGC框架，包含高效估计梯度熵的下采样方法、关联压缩率与梯度熵的理论模型，以及基于窗口的动态调整机制。

Result: 在32-NVIDIA-V100和64-NVIDIA-H100集群上训练GPT2-2.5B和GPT2-12.1B，EDGC使通信延迟和训练时间最多降低46.45%和16.13%，并保持模型精度。

Conclusion: EDGC能有效加速大语言模型训练，减少通信开销，同时保证模型性能。

Abstract: Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.

</details>


### [107] [Informing Acquisition Functions via Foundation Models for Molecular Discovery](https://arxiv.org/abs/2512.13935)
*Qi Chen,Fabio Ramos,Alán Aspuru-Guzik,Florian Shkurti*

Main category: cs.LG

TL;DR: 提出一种无似然贝叶斯优化方法用于分子发现，提升性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在低数据和大候选空间中性能受限，大语言模型和化学基础模型的利用受高维特征等因素阻碍。

Method: 提出无似然贝叶斯优化方法，绕过显式代理建模，利用大语言模型和化学基础模型先验信息，学习分子搜索空间的树结构分区，结合粗粒度聚类。

Result: 大量实验和消融研究表明，该方法在大语言模型引导的分子发现贝叶斯优化中显著提升可扩展性、鲁棒性和样本效率。

Conclusion: 所提方法有效解决现有问题，在分子发现的贝叶斯优化中有良好表现。

Abstract: Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.

</details>


### [108] [Pattern-Guided Diffusion Models](https://arxiv.org/abs/2512.13945)
*Vivian Lin,Kuk Jin Jang,Wenwen Si,Insup Lee*

Main category: cs.LG

TL;DR: 提出模式引导扩散模型PGDM用于多变量时间序列预测，引入不确定性量化技术，在两个应用上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测用的扩散模型很少考虑数据中的重复结构和模式，因此提出PGDM解决此问题。

Method: 先通过原型分析提取模式，估计序列中最可能的下一个模式，用该估计指导预测，引入基于原型分析的不确定性量化技术并动态调整指导水平。

Result: 在预测视野测量和动作捕捉帧两个应用上，模式引导使PGDM的性能（MAE / CRPS）分别最多提升40.67% / 56.26%和14.12% / 14.10%，且优于基线模型。

Conclusion: PGDM通过利用时间数据中的固有模式，能更现实地进行预测，在相关应用中表现优异。

Abstract: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.

</details>


### [109] [A Single Architecture for Representing Invariance Under Any Space Group](https://arxiv.org/abs/2512.13989)
*Cindy Y. Zhang,Elif Ertekin,Peter Orbanz,Ryan P. Adams*

Main category: cs.LG

TL;DR: 提出一种新的机器学习架构，可自动调整权重以适应任意输入空间群的不变性，在材料属性预测和零样本学习上有效。


<details>
  <summary>Details</summary>
Motivation: 将已知对称性融入机器学习模型虽有优势，但实现对特定对称性的精确不变性需为每组对称性设计定制架构，限制可扩展性，在处理三维230个空间群时挑战突出。

Method: 通过明确表征群操作对傅里叶系数施加的约束来构造对称性适配的傅里叶基，将这些约束编码到神经网络层以实现不同空间群间的权重共享。

Result: 该方法在材料属性预测任务中取得有竞争力的表现，并能进行零样本学习以推广到未见群。

Conclusion: 所提出的新方法能有效应对晶体学对称性问题，克服数据稀疏性，实现知识跨群转移。

Abstract: Incorporating known symmetries in data into machine learning models has consistently improved predictive accuracy, robustness, and generalization. However, achieving exact invariance to specific symmetries typically requires designing bespoke architectures for each group of symmetries, limiting scalability and preventing knowledge transfer across related symmetries. In the case of the space groups, symmetries critical to modeling crystalline solids in materials science and condensed matter physics, this challenge is particularly salient as there are 230 such groups in three dimensions. In this work we present a new approach to such crystallographic symmetries by developing a single machine learning architecture that is capable of adapting its weights automatically to enforce invariance to any input space group. Our approach is based on constructing symmetry-adapted Fourier bases through an explicit characterization of constraints that group operations impose on Fourier coefficients. Encoding these constraints into a neural network layer enables weight sharing across different space groups, allowing the model to leverage structural similarities between groups and overcome data sparsity when limited measurements are available for specific groups. We demonstrate the effectiveness of this approach in achieving competitive performance on material property prediction tasks and performing zero-shot learning to generalize to unseen groups.

</details>


### [110] [Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation](https://arxiv.org/abs/2512.14011)
*Yue Wan,Jiayi Yuan,Zhiwei Feng,Xiaowei Jia*

Main category: cs.LG

TL;DR: 本文提出精心整理的数据集，定义三个机器学习任务，用多尺度评估框架对现有模型进行基准测试，为推进计算免疫疗法提供资源。


<details>
  <summary>Details</summary>
Motivation: MHC - II抗原表位研究因结合特异性复杂和基序模式模糊面临挑战，现有数据集小且不规范。

Method: 从免疫表位数据库等公共资源整理数据集，定义肽结合、肽呈递和抗原呈递三个机器学习任务，采用多尺度评估框架进行基准测试和模块化综合分析。

Result: 得到扩展和标准化的肽 - MHC - II数据集及具有更丰富生物学背景的抗原 - MHC - II数据集，完成对现有模型的基准测试和分析。

Conclusion: 该工作为推进计算免疫疗法提供有价值资源，为未来机器学习指导的表位发现和免疫反应预测建模研究奠定基础。

Abstract: Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.

</details>


### [111] [EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment](https://arxiv.org/abs/2512.14019)
*Juseung Yun,Sunwoo Yu,Sumin Ha,Jonghyun Kim,Janghyeon Lee,Jongseong Jang,Soonyoung Lee*

Main category: cs.LG

TL;DR: 提出EXAONE Path 2.5病理基础模型，联合多模态建模，评估显示其高效且适应性强，凸显多模态设计价值。


<details>
  <summary>Details</summary>
Motivation: 癌症进展涉及多生物层交互，图像模型有局限，需捕捉更广泛生物信息。

Method: 模型包含多模态SigLIP损失、F - RoPE模块、领域专用内部基础模型，用于多模态对比学习、保留空间结构和提供生物嵌入。

Result: 在两个基准测试中，框架数据和参数效率高，性能与现有模型相当，内部临床适应性最佳。

Conclusion: 强调生物信息多模态设计价值，以及基因型到表型建模在下一代精准肿瘤学的潜力。

Abstract: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.

</details>


### [112] [Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks](https://arxiv.org/abs/2512.14023)
*Yong Fang,Na Li,Hangguan Shan,Eryun Liu,Xinyu Li,Wei Ni,Er-Ping Li*

Main category: cs.LG

TL;DR: 提出混合对称正定流形图神经网络HSMGNN用于多变量时间序列（MTS）预测，在三个基准数据集上准确率提升最高达13.8%。


<details>
  <summary>Details</summary>
Motivation: 现有MTS预测方法在欧几里得或黎曼空间建模，难以捕捉真实数据多样几何结构和复杂时空依赖。

Method: 提出HSMGNN，引入SCS嵌入将输入MTS投影到双空间，设计ADB层降低黎曼距离计算成本，用FGCN通过可学习融合算子整合双空间特征。

Result: 在三个基准数据集上，HSMGNN预测准确率比现有最优基线最高提升13.8%。

Conclusion: HSMGNN利用混合几何表示进行MTS预测是有效的，能提升预测准确性。

Abstract: Multivariate Time Series (MTS) forecasting plays a vital role in various real-world applications, such as traffic management and predictive maintenance. Existing approaches typically model MTS data in either Euclidean or Riemannian space, limiting their ability to capture the diverse geometric structures and complex spatio-temporal dependencies inherent in real-world data. To overcome this limitation, we propose the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel graph neural network-based model that captures data geometry within a hybrid Euclidean-Riemannian framework. To the best of our knowledge, this is the first work to leverage hybrid geometric representations for MTS forecasting, enabling expressive and comprehensive modeling of geometric properties. Specifically, we introduce a Submanifold-Cross-Segment (SCS) embedding to project input MTS into both Euclidean and Riemannian spaces, thereby capturing spatio-temporal variations across distinct geometric domains. To alleviate the high computational cost of Riemannian distance, we further design an Adaptive-Distance-Bank (ADB) layer with a trainable memory mechanism. Finally, a Fusion Graph Convolutional Network (FGCN) is devised to integrate features from the dual spaces via a learnable fusion operator for accurate prediction. Experiments on three benchmark datasets demonstrate that HSMGNN achieves up to a 13.8 percent improvement over state-of-the-art baselines in forecasting accuracy.

</details>


### [113] [FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis](https://arxiv.org/abs/2512.14078)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Feiping Nie,Junyu Gao,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出FusAD统一分析框架用于多种时间序列任务，实验表明其在主流基准测试中优于现有模型，且高效可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分析方法难实现多任务兼容和跨类型信息整合，真实数据复杂，需构建高效、多任务兼容且可泛化的统一框架。

Method: 提出FusAD框架，有自适应时频融合机制、自适应去噪机制，还集成通用信息融合和解码结构及掩码预训练。

Result: FusAD在主流时间序列基准测试的分类、预测和异常检测任务中始终优于现有模型。

Conclusion: FusAD是一个高效、多任务兼容且可泛化的时间序列分析统一框架，具有高可扩展性。

Abstract: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.

</details>


### [114] [SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations](https://arxiv.org/abs/2512.14080)
*Wentao Guo,Mayank Mishra,Xinle Cheng,Ion Stoica,Tri Dao*

Main category: cs.LG

TL;DR: 提出SonicMoE方法，减少激活内存、提高计算吞吐量，开源内核助力MoE模型训练。


<details>
  <summary>Details</summary>
Motivation: 当前细粒度和稀疏MoE模型分别存在激活内存占用高、硬件效率低和计算浪费问题，需改进。

Method: 提出内存高效算法、设计重叠内存IO与计算的GPU内核、提出“token rounding”方法。

Result: SonicMoE减少45%激活内存，计算吞吐量提升1.86倍，tile - aware token rounding算法加速1.16倍。

Conclusion: 所提方法有效提升MoE模型训练效率，开源内核可促进相关研究。

Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

</details>


### [115] [Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization](https://arxiv.org/abs/2512.14086)
*Boyuan Yao,Dingcheng Luo,Lianghao Cao,Nikola Kovachki,Thomas O'Leary-Roseberry,Omar Ghattas*

Main category: cs.LG

TL;DR: 提出导数信息傅里叶神经算子（DIFNO）近似理论和训练方法，用于PDE约束优化，理论证明其能力，开发高效训练方案，数值实验显示优势。


<details>
  <summary>Details</summary>
Motivation: 准确的替代驱动PDE约束优化需要准确的替代Fréchet导数，因此使用DIFNO替代传统FNO作为替代模型。

Method: 建立FNO及其Fréchet导数的通用逼近理论，使用降维和多分辨率技术开发高效训练方案。

Result: 数值例子表明DIFNO在算子学习和解决无限维PDE约束逆问题的样本复杂度上更优，低训练样本量下精度高。

Conclusion: 理论结果证明FNO可用于准确的导数信息算子学习和PDE约束优化求解。

Abstract: We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fréchet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fréchet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fréchet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fréchet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.

</details>


### [116] [Arithmetic-Intensity-Aware Quantization](https://arxiv.org/abs/2512.14090)
*Taig Singh,Shreshth Rajan,Nikhil Iyer*

Main category: cs.LG

TL;DR: 提出算术强度感知量化（AIQ）框架，可在减少精度损失同时提高算术强度，在不同模型上有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络受DRAM带宽限制推理吞吐量，需要一种方法提高算术强度并减少精度损失。

Method: 采用训练后量化方法，通过搜索算法在每层量化方案上最小化算术强度和精度的加权损失。

Result: 在ResNet - 20/CIFAR - 10上AI提升约50%，测试精度损失约1个百分点；在MobileNetV2上吞吐量比FP32基线高1.66倍，测试精度损失1个百分点内，且自然地对较大层更激进地量化。

Conclusion: AIQ框架能有效提高算术强度，在不同模型上提升推理吞吐量并控制精度损失。

Abstract: As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.

</details>


### [117] [A First-Order Logic-Based Alternative to Reward Models in RLHF](https://arxiv.org/abs/2512.14100)
*Chunjin Jian,Xinhua Zhu*

Main category: cs.LG

TL;DR: 提出基于逻辑相似度的奖励机制和S - GRPO方法，实验表明S - GRPO性能和鲁棒性优于SFT，还扩展现有偏好学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有基于奖励模型的方法训练奖励模型的质量和稳定性影响大语言模型与人类价值和偏好对齐的最终性能。

Method: 提出基于逻辑相似度的奖励机制，引入S - GRPO方法，在训练中结合额外监督组件，联合优化生成项、KL散度正则化和基于标签的目标。

Result: S - GRPO在性能和鲁棒性上始终优于标准监督微调（SFT），扩展了现有偏好学习框架。

Conclusion: S - GRPO是一种更灵活、适应任务的对齐训练方法，代码已开源。

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

</details>


### [118] [PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario](https://arxiv.org/abs/2512.14150)
*Zhijie Zhong,Zhiwen Yu,Pengyu Li,Jianming Lv,C. L. Philip Chen,Min Chen*

Main category: cs.LG

TL;DR: 本文指出当前基于深度学习的RPP方法存在的问题，提出PathFinder架构及相关策略和基准测试，实验显示PathFinder在多发射器场景效果佳。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的RPP方法存在缺乏主动环境建模、难以处理多发射器场景和分布偏移泛化能力差等问题，需要优化。

Method: 提出PathFinder架构，通过解纠缠特征编码主动建模建筑物和发射器，集成Mask - Guided Low - rank Attention；引入发射器导向的混合策略进行稳健训练；提出新基准S2MT - RPP。

Result: PathFinder显著优于现有方法，在多发射器场景表现更佳。

Conclusion: PathFinder是解决5G网络中RPP问题的有效方案，能应对多发射器场景和分布偏移挑战。

Abstract: Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.

</details>


### [119] [On Improving Deep Active Learning with Formal Verification](https://arxiv.org/abs/2512.14170)
*Jonathan Spiegelman,Guy Amir,Guy Katz*

Main category: cs.LG

TL;DR: 研究利用违反鲁棒性约束的对抗性输入扩充训练数据以提升深度主动学习（DAL）性能，表明形式验证生成的对抗示例效果更好，应用于多种技术提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过用违反鲁棒性约束的对抗性输入扩充训练数据来提高深度主动学习性能。

Method: 研究利用形式验证生成的对抗示例，应用于多种现代DAL技术和新提出的技术。

Result: 形式验证生成的对抗示例比标准基于梯度攻击生成的贡献更大，在标准基准上显著提升模型泛化能力。

Conclusion: 用违反鲁棒性约束的对抗性输入扩充训练数据可有效提升DAL的性能。

Abstract: Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.

</details>


### [120] [Optimizing the Adversarial Perturbation with a Momentum-based Adaptive Matrix](https://arxiv.org/abs/2512.14188)
*Wei Tao,Sheng Long,Xin Liu,Wei Li,Qing Tao*

Main category: cs.LG

TL;DR: 本文指出已有攻击方法用符号函数缩放扰动有理论问题，揭示PGD本质，提出新攻击方法AdaMI，证明其收敛性，实验表明其能提升对抗迁移性。


<details>
  <summary>Details</summary>
Motivation: 现存基于优化的攻击方法用符号函数缩放扰动存在理论问题，且MI - FGSM有非收敛问题。

Method: 首先揭示PGD本质，利用累积梯度的自适应矩阵优化扰动提出AdaMI攻击方法。

Result: AdaMI在凸问题上有最优收敛性，解决了MI - FGSM的非收敛问题；实验表明提出的自适应矩阵能提升对抗迁移性等。

Conclusion: 提出的基于动量的自适应矩阵是一种通用有效的技术，能在不同网络上提升对抗迁移性，同时保持更好的稳定性和不可感知性。

Abstract: Generating adversarial examples (AEs) can be formulated as an optimization problem. Among various optimization-based attacks, the gradient-based PGD and the momentum-based MI-FGSM have garnered considerable interest. However, all these attacks use the sign function to scale their perturbations, which raises several theoretical concerns from the point of view of optimization. In this paper, we first reveal that PGD is actually a specific reformulation of the projected gradient method using only the current gradient to determine its step-size. Further, we show that when we utilize a conventional adaptive matrix with the accumulated gradients to scale the perturbation, PGD becomes AdaGrad. Motivated by this analysis, we present a novel momentum-based attack AdaMI, in which the perturbation is optimized with an interesting momentum-based adaptive matrix. AdaMI is proved to attain optimal convergence for convex problems, indicating that it addresses the non-convergence issue of MI-FGSM, thereby ensuring stability of the optimization process. The experiments demonstrate that the proposed momentum-based adaptive matrix can serve as a general and effective technique to boost adversarial transferability over the state-of-the-art methods across different networks while maintaining better stability and imperceptibility.

</details>


### [121] [Random-Bridges as Stochastic Transports for Generative Models](https://arxiv.org/abs/2512.14190)
*Stefano Goria,Levent A. Mengütürk,Murat C. Mengütürk,Berkan Sesen*

Main category: cs.LG

TL;DR: 本文提出在生成模型中使用随机桥，基于高斯随机桥的实验结果表明该框架计算成本低，适合高速生成任务。


<details>
  <summary>Details</summary>
Motivation: 在生成模型领域引入随机桥的使用。

Method: 从一般概率陈述出发，根据信息处理得出学习和模拟算法的具体表示。

Result: 基于高斯随机桥的实验在更少步骤下生成高质量样本，且弗雷歇初始距离得分有竞争力。

Conclusion: 所提出的框架计算成本低，适合高速生成任务。

Abstract: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.

</details>


### [122] [Understanding and Improving Hyperbolic Deep Reinforcement Learning](https://arxiv.org/abs/2512.14202)
*Timo Klein,Thomas Lang,Andrii Shkabrii,Alexander Sturm,Kevin Sidak,Lukas Miklautz,Claudia Plant,Yllka Velaj,Sebastian Tschiatschek*

Main category: cs.LG

TL;DR: 本文识别训练双曲深度强化学习代理成败的关键因素，提出Hyper++代理，实验显示其学习稳定、性能优且减少耗时。


<details>
  <summary>Details</summary>
Motivation: 双曲特征空间适用于强化学习，但因强化学习非平稳性，利用这些空间面临优化挑战，需解决训练问题。

Method: 分析双曲几何模型核心操作梯度，提出由稳定评论员训练、特征正则化和优化友好网络层构成的Hyper++代理。

Result: 在ProcGen实验中，Hyper++学习稳定，优于先前双曲代理，减少约30%耗时；在Atari - 5上，大幅优于欧氏和双曲基线。

Conclusion: Hyper++能有效解决双曲深度强化学习训练问题，有更好性能和效率。

Abstract: The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincaré Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .

</details>


### [123] [Estimating problem difficulty without ground truth using Large Language Model comparisons](https://arxiv.org/abs/2512.14220)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.

</details>


### [124] [Physically consistent model learning for reaction-diffusion systems](https://arxiv.org/abs/2512.14240)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 本文聚焦从数据中学习反应 - 扩散系统，提出确保模型物理一致性与适定性的方法并拓展理论结果。


<details>
  <summary>Details</summary>
Motivation: 解决从数据学习反应 - 扩散系统时，保证模型物理一致性和适定性的问题。

Method: 基于正则化框架，系统修改参数化反应项使其满足质量守恒和拟正性，拓展现有正则化模型学习理论。

Result: 提出修改反应项的技术，保证PDE适定性；证明学习问题的解收敛到极限系统的唯一正则化最小化解，给出拟正函数的近似结果。

Conclusion: 研究推动了符合基本物理定律的反应 - 扩散系统可解释、可靠的数据驱动模型的发展。

Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.

</details>


### [125] [Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning](https://arxiv.org/abs/2512.14241)
*Salvatore Romano,Marco Grassia,Giuseppe Mangioni*

Main category: cs.LG

TL;DR: 文章指出图生成模型评估存在局限，提出RGM方法评估GGMs，测试GRAN和EDGE，发现它们在保留结构特征上有问题，也指出MMD不适合评估GGMs。


<details>
  <summary>Details</summary>
Motivation: 当前图生成模型评估依赖MMD存在局限性，因此提出新评估方法。

Method: 提出RGM方法评估GGMs，用几何深度学习模型在自定义数据集上评估GRAN和EDGE。

Result: GRAN和EDGE能生成有特定拓扑属性的图，但保留结构特征有局限，MMD不适合评估GGMs。

Conclusion: 对GGMs评估提出新方法RGM，指出当前模型不足和现有评估指标问题，建议未来研究替代方法。

Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.

</details>


### [126] [FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting](https://arxiv.org/abs/2512.14253)
*Xingjian Wu,Hanyin Cheng,Xiangfei Qiu,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 介绍极轻量级且强大的时间序列基础模型FLAME，在基准测试中有SOTA零样本表现。


<details>
  <summary>Details</summary>
Motivation: 开发兼具效率和鲁棒性，能支持确定性和概率性预测的时间序列基础模型。

Method: 利用勒让德记忆实现强泛化能力；在编码和解码阶段采用变体；采用基于归一化流的预测头。

Result: 在TSFM - Bench和ProbTS等基准测试中，FLAME在确定性和概率性预测任务上均有一致的SOTA零样本表现。

Conclusion: FLAME是一种高效、鲁棒且性能优秀的时间序列基础模型。

Abstract: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.

</details>


### [127] [Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization](https://arxiv.org/abs/2512.14263)
*Nick Leenders,Thomas Quadt,Boris Cule,Roy Lindelauf,Herman Monsuur,Joost van Oijen,Mark Voskuijl*

Main category: cs.LG

TL;DR: 本文提出基于决策树的替代模型克服了高斯过程替代模型的不足，在实验中表现良好且可应用于实际寿司偏好数据，还探讨用历史数据加速优化。


<details>
  <summary>Details</summary>
Motivation: 当前基于高斯过程的优先贝叶斯优化方法存在难以解释、难处理分类数据和计算复杂等问题，限制了实际应用。

Method: 引入基于决策树的替代模型，该模型具有内在可解释性，能处理分类和连续数据，且可扩展到大型数据集。

Result: 在八个优化函数上实验表明该模型在尖峰函数上优于基于GP的替代模型，在非尖峰函数上性能略低；应用于实际寿司数据集展示了学习个人寿司偏好的能力；开展了用历史偏好数据加速新用户优化过程的初步工作。

Conclusion: 基于决策树的替代模型是一种有效的优先贝叶斯优化方法，有实际应用潜力。

Abstract: Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.

</details>


### [128] [Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits](https://arxiv.org/abs/2512.14338)
*Michael Murray,Tenzin Chan,Kedar Karhadker,Christopher J. Hillar*

Main category: cs.LG

TL;DR: 研究经典Hopfield网络在学习有对称结构数据时隐式出现的不变性现象，揭示泛化统一机制。


<details>
  <summary>Details</summary>
Motivation: 探究学习问题中对称性以及在训练组结构数据时隐式出现不变性的现象。

Method: 研究经典Hopfield网络，用梯度下降最小化能量流。

Result: 图同构类可在三维不变子空间表示；MEF有偏向范数高效解的隐式偏差；多学习规则下参数随样本量增长向不变子空间收敛。

Conclusion: Hopfield网络学习中对范数效率的偏向驱动了组结构数据下近似不变性的出现。

Abstract: Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

</details>


### [129] [Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis](https://arxiv.org/abs/2512.14361)
*Nicholas Tagliapietra,Katharina Ensinger,Christoph Zimmer,Osman Mian*

Main category: cs.LG

TL;DR: 提出CaDyT方法解决连续时间动力系统因果发现问题，实验显示其在不同采样数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有学习连续时间动力系统动态的方法存在离散化时间导致对不规则采样数据性能差或忽略潜在因果关系的问题。

Method: 基于Difference - based因果模型，利用精确高斯过程推理建模连续时间动态，通过基于算法马尔可夫条件和最小描述长度原则的贪心搜索识别因果结构。

Result: CaDyT在规则和不规则采样数据上均优于现有方法，发现的因果网络更接近真实潜在动态。

Conclusion: CaDyT是一种有效的连续时间动力系统因果发现方法。

Abstract: Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.

</details>


### [130] [Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries](https://arxiv.org/abs/2512.14388)
*Baobao Song,Shiva Raj Pokhrel,Athanasios V. Vasilakos,Tianqing Zhu,Gang Li*

Main category: cs.LG

TL;DR: 提出首个基于提升量子差分隐私的QML黑盒隐私审计框架，用量子金丝雀检测记忆和量化隐私泄露，经评估有效。


<details>
  <summary>Details</summary>
Motivation: QML虽有计算优势，但训练敏感数据的模型有隐私风险，QDP机制缺乏对部署模型的实证验证工具。

Method: 引入基于提升量子差分隐私的黑盒隐私审计框架，利用量子金丝雀检测记忆并量化隐私泄露，建立金丝雀偏移与迹距离界限的数学联系，推导隐私预算消耗的经验下界。

Result: 通过在模拟和物理量子硬件上的综合评估，证明框架能有效测量QML模型的实际隐私损失。

Conclusion: 该框架可实现QML系统的强大隐私验证，弥补理论保证与实际隐私验证间的差距。

Abstract: Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.

</details>


### [131] [RePo: Language Models with Context Re-Positioning](https://arxiv.org/abs/2512.14391)
*Huayang Li,Tianyu Zhao,Richard Sproat*

Main category: cs.LG

TL;DR: 文章指出当前大语言模型上下文学习结构有缺陷，提出RePo机制，经预训练验证其在多种任务上增强表现。


<details>
  <summary>Details</summary>
Motivation: 现行大语言模型的上下文结构固定，增加了额外认知负荷，消耗工作记忆，需改进。

Method: 提出RePo机制，利用可微分模块$f_φ$分配标记位置，持续在OLMo - 2 1B骨干上预训练。

Result: RePo在处理复杂上下文的任务中显著提升性能，在一般短上下文任务中也有竞争力。

Conclusion: RePo能有效分配注意力到远距离相关信息，在密集非线性空间分配位置，捕捉输入上下文的内在结构。代码开源。

Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.

</details>


### [132] [SuperWing: a comprehensive transonic wing dataset for data-driven aerodynamic design](https://arxiv.org/abs/2512.14397)
*Yunjia Yang,Weishao Tang,Mengxin Liu,Nils Thuerey,Yufei Zhang,Haixin Chen*

Main category: cs.LG

TL;DR: 提出SuperWing数据集加速翼型气动设计，用Transformer模型测试表现良好，有零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集稀缺、多样性受限，阻碍三维机翼通用预测器发展，需构建新数据集。

Method: 构建含4239个参数化机翼几何形状和28856个流场解的SuperWing数据集，用简化几何参数化生成机翼形状，在宽范围条件下模拟；用两个Transformer模型进行测试。

Result: Transformer模型能准确预测表面流，在测试样本上阻力系数误差2.5；预训练模型对复杂基准机翼有零样本泛化能力。

Conclusion: SuperWing数据集多样性好，有实际应用潜力，可用于加速翼型气动设计。

Abstract: Machine-learning surrogate models have shown promise in accelerating aerodynamic design, yet progress toward generalizable predictors for three-dimensional wings has been limited by the scarcity and restricted diversity of existing datasets. Here, we present SuperWing, a comprehensive open dataset of transonic swept-wing aerodynamics comprising 4,239 parameterized wing geometries and 28,856 Reynolds-averaged Navier-Stokes flow field solutions. The wing shapes in the dataset are generated using a simplified yet expressive geometry parameterization that incorporates spanwise variations in airfoil shape, twist, and dihedral, allowing for an enhanced diversity without relying on perturbations of a baseline wing. All shapes are simulated under a broad range of Mach numbers and angles of attack covering the typical flight envelope. To demonstrate the dataset's utility, we benchmark two state-of-the-art Transformers that accurately predict surface flow and achieve a 2.5 drag-count error on held-out samples. Models pretrained on SuperWing further exhibit strong zero-shot generalization to complex benchmark wings such as DLR-F6 and NASA CRM, underscoring the dataset's diversity and potential for practical usage.

</details>


### [133] [GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion](https://arxiv.org/abs/2512.14400)
*Fangzhou Lin,Guoshun He,Zhenyu Guo,Zhe Huang,Jinsong Tao*

Main category: cs.LG

TL;DR: 本文提出GRAFT模型用于电网负荷预测，构建统一基准进行多尺度评估，结果显示该模型表现优异，且发布相关资源促进可重复性研究。


<details>
  <summary>Details</summary>
Motivation: 电力负荷受多时间尺度的外生因素影响，需更好的模型支持电网感知预测和多源文本干预。

Method: 提出GRAFT模型，严格对齐文本与负荷数据，通过交叉注意力实现文本引导融合，提供外部内存接口；构建统一基准进行多尺度评估。

Result: GRAFT显著优于强基线模型，在多区域和预测范围内达到或超越现有水平，在事件驱动场景中表现稳健，可实现文本对负荷影响的时间定位和源级解释。

Conclusion: GRAFT模型在电网负荷预测中表现出色，发布的资源有助于标准化实证评估和可重复性研究。

Abstract: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.

</details>


### [134] [Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space](https://arxiv.org/abs/2512.14418)
*Dejun Hu,Zhiming Li,Jia-Rui Shen,Jia-Ning Tu,Zi-Hao Ye,Junliang Zhang*

Main category: cs.LG

TL;DR: 提出双轴表征完备收敛学习策略，开发FD25数据集，使图神经网络实现收敛学习和强泛化能力，建立分子表征等与模型泛化的定量联系。


<details>
  <summary>Details</summary>
Motivation: 在化学空间规模巨大的情况下，探究模型能否在该空间实现收敛学习。

Method: 引入双轴表征完备收敛学习（RCCL）策略，结合基于现代价键理论的局部价环境的图卷积网络（GCN）编码和环/笼拓扑的无桥图（NBG）编码，开发FD25数据集。

Result: 图神经网络在FD25上训练后，实现表征完备的收敛学习和强分布外泛化，外部基准测试的整体预测误差约为1.0 kcal/mol MAE。

Conclusion: 建立了分子表征、结构完备性和模型泛化之间的定量联系，为可解释、可迁移和数据高效的分子智能提供基础。

Abstract: Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.

</details>


### [135] [Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF](https://arxiv.org/abs/2512.14444)
*Akira Takeshima,Kenta Shiraishi,Atsushi Okazaki,Tadashi Tsuyuki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: 介绍首个纯数据驱动的ML集合天气预报系统ClimaX - LETKF，对比RTPP和RTPS效果并指出MLWP模型不足，为其改进提供见解。


<details>
  <summary>Details</summary>
Motivation: 当前基于机器学习的天气预报（MLWP）在同化真实观测或集合预报方面研究有限。

Method: 引入ClimaX - LETKF系统，同化NCEP ADP全球高空和地面气象观测，对比RTPP和RTPS方法。

Result: 系统多年运行稳定，RTPP比RTPS更稳定准确，MLWP模型恢复大气场能力不如NWP模型。

Conclusion: 为改进MLWP集合预报系统提供有价值见解，推动其实践应用。

Abstract: While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.

</details>


### [136] [AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts](https://arxiv.org/abs/2512.14461)
*Niklas Grieger,Jannik Raskob,Siamak Mehrkanoon,Stephan Bialonski*

Main category: cs.LG

TL;DR: 提出AnySleep模型，用EEG或EOG数据以可调时间分辨率对睡眠进行评分，性能出色且公开可用。


<details>
  <summary>Details</summary>
Motivation: 传统睡眠研究手动分期费力，多中心研究存在约束，需新方法进行睡眠评分和发现生物标志物。

Method: 训练AnySleep深度神经网络模型，用多诊所21个数据集超19000份夜间记录验证。

Result: 模型达当前最优性能，30秒分期表现好，不同电极数据下性能强，亚30秒尺度能捕捉短觉醒并改善生理病理状况预测。

Conclusion: 公开模型以促进异质电极设置的大规模研究和睡眠新生物标志物发现。

Abstract: Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.

</details>


### [137] [Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics](https://arxiv.org/abs/2512.14471)
*Additi Pandey,Liang Wei,Hessam Babaee,George Em Karniadakis*

Main category: cs.LG

TL;DR: 本文介绍基于Mamba的神经算子框架Kinetic - Mamba用于化学动力学建模，包含多个互补模型，经评估该框架仅用状态变量初始条件就能高精度预测复杂动力学行为。


<details>
  <summary>Details</summary>
Motivation: 准确的化学动力学建模对燃烧模拟至关重要，需要有效方法进行建模。

Method: 引入Kinetic - Mamba框架，包含独立Mamba模型、约束Mamba模型、区域信息架构，还开发潜在变体，采用时间分解和递归预测策略评估。

Result: 在合成气和GRI - Mech 3.0反应机制的计算实验中，框架仅用状态变量初始条件就能高精度预测复杂动力学行为。

Conclusion: Kinetic - Mamba框架在预测复杂化学动力学行为上具有高保真度。

Abstract: Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.

</details>


### [138] [Synthetic Electrogram Generation with Variational Autoencoders for ECGI](https://arxiv.org/abs/2512.14537)
*Miriam Gutiérrez Fernández,Karen López-Linares,Carlos Fambuena Santos,María S. Guillem,Andreu M. Climent,Óscar Barquero Pérez*

Main category: cs.LG

TL;DR: 研究用变分自编码器生成合成多通道心房内心电图（EGM）缓解数据稀缺问题，评估两种模型效果，并用生成数据增强下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 无创心电图成像结合深度学习估计心内电图时，配对体表电位 - 心内电图数据集有限阻碍进展，需解决数据稀缺问题。

Method: 提出两种变分自编码器模型（VAE - S和VAE - C）生成合成多通道心房EGM，用形态、频谱和分布相似性指标评估生成的EGM。

Result: VAE - S对计算机模拟EGM保真度更高，VAE - C可实现特定心律生成但降低窦性心律重建质量，生成的EGM用于下游任务数据增强时适度增强可提升估计性能。

Conclusion: 基于变分自编码器的生成模型有潜力缓解数据稀缺，增强基于深度学习的无创心电图成像流程。

Abstract: Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.

</details>


### [139] [Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions](https://arxiv.org/abs/2512.14559)
*Emmanuel C. Chukwu,Rianne M. Schouten,Monique Tabak,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: 现有时间序列分类反事实技术不适用于临床推荐场景，本文指出其问题并呼吁可行、有目的的干预方法和评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分类的反事实技术在临床推荐场景中存在不足，无法满足干预随时间展开、因果合理和时间连贯的要求。

Method: 对几种时间序列的先进方法进行鲁棒性分析。

Result: 生成的反事实对随机噪声高度敏感，在现实临床环境中可靠性有限。

Conclusion: 需要超越仅考虑预测变化的方法和评估框架，强调要有可行、有目的的干预措施。

Abstract: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.

</details>


### [140] [Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection](https://arxiv.org/abs/2512.14563)
*Tejaswani Dash,Gautam Datla,Anudeep Vurity,Tazeem Ahmad,Mohd Adnan,Saima Rafi,Saisha Patro,Saina Patro*

Main category: cs.LG

TL;DR: 提出用于表格临床记录的Residual GRU with Multi - Head Self - Attention模型，在UCI心脏病数据集上表现优于基线模型，证明轻量级混合架构在临床风险预测中平衡了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，传统诊断方法有局限，机器学习方法在处理临床数据时泛化能力不足，需要可靠高效的预测工具。

Method: 提出Residual GRU with Multi - Head Self - Attention模型，集成残差双向门控循环单元、通道重加权块和多头自注意力池化；使用5折分层交叉验证在UCI心脏病数据集上评估，并与经典和现代深度学习基线模型对比。

Result: 模型准确率0.861，宏F1为0.860，ROC - AUC为0.908，PR - AUC为0.904，优于所有基线；消融研究验证各组件贡献；t - SNE可视化显示学习嵌入比原始特征在疾病和非疾病类间分离更清晰。

Conclusion: 轻量级混合循环和基于注意力的架构在临床风险预测中能平衡准确性和效率，适合资源受限的医疗场景。

Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.

</details>


### [141] [Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs](https://arxiv.org/abs/2512.14596)
*Youngkyu Lee,Francesc Levrero Florencio,Jay Pathak,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出Geo - DeepONet和几何感知混合预条件迭代求解器，用于参数化偏微分方程，实验证明求解器在多领域增强鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 经典迭代求解器对偏微分方程的区域和离散化敏感，之前的混合求解器在未训练过的几何形状中性能不佳。

Method: 引入Geo - DeepONet，结合传统方法如松弛方案和Krylov子空间算法，开发几何感知混合预条件迭代求解器。

Result: 通过在不同非结构化域的参数化偏微分方程上的数值实验，展示了求解器的增强性能。

Conclusion: 所提出的混合求解器在多个实际应用中具有增强的鲁棒性和效率。

Abstract: The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.

</details>


### [142] [Hierarchical Persistence Velocity for Network Anomaly Detection: Theory and Applications to Cryptocurrency Markets](https://arxiv.org/abs/2512.14615)
*Omid Khormali*

Main category: cs.LG

TL;DR: 引入OW - HNPV用于时变网络异常检测，证明其稳定性，在以太坊交易网络中显示出优越性能，表明拓扑速度建模对动态网络结构异常检测至关重要。


<details>
  <summary>Details</summary>
Motivation: 为时变网络的异常检测寻找新方法，现有方法多测量累积拓扑存在，缺乏基于速度的视角。

Method: 引入OW - HNPV，从速度角度分析持久性图，通过基于重叠的加权自动降低噪声，并证明其数学稳定性。

Result: 在以太坊交易网络中，OW - HNPV实现高达10.4%的AUC增益，在中长程预测中表现出色，性能更一致稳定。

Conclusion: 建模拓扑速度对检测动态网络的结构异常很关键。

Abstract: We introduce the Overlap-Weighted Hierarchical Normalized Persistence Velocity (OW-HNPV), a novel topological data analysis method for detecting anomalies in time-varying networks. Unlike existing methods that measure cumulative topological presence, we introduce the first velocity-based perspective on persistence diagrams, measuring the rate at which features appear and disappear, automatically downweighting noise through overlap-based weighting. We also prove that OW-HNPV is mathematically stable. It behaves in a controlled, predictable way, even when comparing persistence diagrams from networks with different feature types. Applied to Ethereum transaction networks (May 2017-May 2018), OW-HNPV demonstrates superior performance for cryptocurrency anomaly detection, achieving up to 10.4% AUC gain over baseline models for 7-day price movement predictions. Compared with established methods, including Vector of Averaged Bettis (VAB), persistence landscapes, and persistence images, velocity-based summaries excel at medium- to long-range forecasting (4-7 days), with OW-HNPV providing the most consistent and stable performance across prediction horizons. Our results show that modeling topological velocity is crucial for detecting structural anomalies in dynamic networks.

</details>


### [143] [Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes](https://arxiv.org/abs/2512.14617)
*Alessandro Trapasso,Luca Iocchi,Fabio Patrizi*

Main category: cs.LG

TL;DR: 提出QR - MAX算法解决离散NMRDPs问题，有PAC收敛性和多项式样本复杂度，扩展到连续状态空间的Bucket - QR - MAX，实验显示样本效率和寻优策略鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: Markovian RL不适合处理依赖系统历史的任务，NMRDPs缺乏(近)最优性和样本效率的形式化保证。

Method: 提出QR - MAX算法，通过奖励机器将马尔可夫转移学习与非马尔可夫奖励处理分离；用基于SimHash的Bucket - QR - MAX扩展到连续状态空间。

Result: 实验对比现代最先进的基于模型的RL方法，样本效率显著提高，寻找最优策略的鲁棒性增强。

Conclusion: QR - MAX和Bucket - QR - MAX能有效解决NMRDPs的(近)最优性和样本效率问题。

Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

</details>


### [144] [ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning](https://arxiv.org/abs/2512.14619)
*Chaohao Yuan,Zhenjie Song,Ercan Engin Kuruoglu,Kangfei Zhao,Yang Liu,Deli Zhao,Hong Cheng,Yu Rong*

Main category: cs.LG

TL;DR: 本文指出图Transformer全局注意力存在过平滑问题，提出PageRank Transformer（ParaFormer）缓解此问题，实验验证其在多种数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决图Transformer中全局注意力存在的过平滑问题，该问题比GNN更严重，导致节点表示难以区分。

Method: 提出PageRank Transformer（ParaFormer），具有PageRank增强的注意力模块，可模仿深度Transformer行为，作为自适应滤波器缓解过平滑。

Result: ParaFormer在11个从数千到数百万节点的数据集的节点分类和图分类任务上实现了一致的性能提升。

Conclusion: ParaFormer能有效缓解图Transformer的过平滑问题，证明了其有效性。

Abstract: Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

</details>


### [145] [gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation](https://arxiv.org/abs/2512.14658)
*Alban Puech,Matteo Mazzonelli,Celia Cintas,Tamara R. Govindasamy,Mangaliso Mngomezulu,Jonas Weiss,Matteo Baù,Anna Varbella,François Mirallès,Kibaek Kim,Le Xie,Hendrik F. Hamann,Etienne Vos,Thomas Brunschwiler*

Main category: cs.LG

TL;DR: 介绍Python库gridfm - datakit - v1，用于生成电力流和最优潮流数据集，能解决现有数据集问题并高效扩展到大电网。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和库存在缺乏现实随机负载和拓扑扰动、PF数据集受限、OPF数据集发电机成本函数固定等问题，影响ML求解器的泛化性。

Method: 结合现实世界的全局负载缩放与局部噪声，支持任意N - k拓扑扰动；生成超出运行限制的PF样本；产生具有不同发电机成本的OPF数据。

Result: 能创建多样且现实的数据集，可高效扩展到大型电网（最多10000个母线），并与其他相关工具进行了比较。

Conclusion: gridfm - datakit可解决现有数据集问题，代码开源，可通过GitHub和pip安装。

Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.

</details>


### [146] [Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks](https://arxiv.org/abs/2512.14675)
*Rae Chipera,Jenny Du,Irene Tsapara*

Main category: cs.LG

TL;DR: 研究回声状态网络中非光滑激活函数，发现部分非光滑函数优于传统光滑激活函数，引入量化激活函数理论框架，指出预处理拓扑决定稳定性，但部分分形函数性能机制不明。


<details>
  <summary>Details</summary>
Motivation: 当代储层计算依赖光滑激活函数，限制了在极端条件下的应用，因此研究非光滑激活函数。

Method: 对36,610种储层配置进行全面参数扫描，引入量化激活函数理论框架。

Result: 部分非光滑函数保持回声状态属性，在收敛速度和谱半径容限上优于传统光滑激活函数，确定关键拥挤比预测离散激活的失败阈值。

Conclusion: 预处理拓扑决定稳定性，挑战了储层计算中激活函数设计的假设，部分分形函数性能机制有待研究。

Abstract: Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activation functions, including chaotic, stochastic, and fractal variants, in echo state networks. Through comprehensive parameter sweeps across 36,610 reservoir configurations, we demonstrate that several non-smooth functions not only maintain the Echo State Property (ESP) but outperform traditional smooth activations in convergence speed and spectral radius tolerance. Notably, the Cantor function (continuous everywhere and flat almost everywhere) maintains ESP-consistent behavior up to spectral radii of rho ~ 10, an order of magnitude beyond typical bounds for smooth functions, while achieving 2.6x faster convergence than tanh and ReLU. We introduce a theoretical framework for quantized activation functions, defining a Degenerate Echo State Property (d-ESP) that captures stability for discrete-output functions and proving that d-ESP implies traditional ESP. We identify a critical crowding ratio Q=N/k (reservoir size / quantization levels) that predicts failure thresholds for discrete activations. Our analysis reveals that preprocessing topology, rather than continuity per se, determines stability: monotone, compressive preprocessing maintains ESP across scales, while dispersive or discontinuous preprocessing triggers sharp failures. While our findings challenge assumptions about activation function design in reservoir computing, the mechanism underlying the exceptional performance of certain fractal functions remains unexplained, suggesting fundamental gaps in our understanding of how geometric properties of activation functions influence reservoir dynamics.

</details>


### [147] [Early Warning Index for Patient Deteriorations in Hospitals](https://arxiv.org/abs/2512.14683)
*Dimitris Bertsimas,Yu Ma,Kimberly Villalobos Carballo,Gagan Singh,Michal Laskowski,Jeff Mather,Dan Kombert,Howard Haronian*

Main category: cs.LG

TL;DR: 论文开发多模态机器学习框架EWI预测患者风险，部署于医院仪表盘，用美国大医院数据验证有效，可作分诊工具，节省医生时间、改善患者流程。


<details>
  <summary>Details</summary>
Motivation: 医院缺乏自动化系统利用临床和运营数据预测关键事件，且不同数据流转化为准确可解释风险评估存在挑战，需早期识别患者恶化风险。

Method: 开发多模态机器学习框架EWI，采用人在环流程，结合SHAP解释输出，将患者分为三个风险等级。

Result: 使用18633名患者数据集，自动提取结构化和非结构化EHR数据特征，C统计值达0.796，目前用作分诊工具。

Conclusion: 该方法节省医生时间，可调整护理人员调度和资源分配，避免下游并发症，改善患者流程。

Abstract: Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [148] [Practitioner Insights on Fairness Requirements in the AI Development Life Cycle: An Interview Study](https://arxiv.org/abs/2512.13830)
*Chaima Boufaied,Thanh Nguyen,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 研究从软件工程视角探讨AI公平性需求，通过访谈发现实践中公平性落实不一致且常被降优先级，需明确公平定义等。


<details>
  <summary>Details</summary>
Motivation: AI模型常作为黑箱对不同群体不公平，除有效性外，对AI软件公平性的关注增加。

Method: 对23个国家不同应用领域和背景的从业者进行26次半结构化访谈。

Result: 参与者虽认识到AI公平性维度，但实践不一致，公平性常被降优先级且存在知识差距。

Conclusion: 需要与相关利益者就明确、适合上下文的公平定义、评估指标和正式流程达成一致，以更好地将公平性融入AI/ML项目。

Abstract: Nowadays, Artificial Intelligence (AI), particularly Machine Learning (ML) and Large Language Models (LLMs), is widely applied across various contexts. However, the corresponding models often operate as black boxes, leading them to unintentionally act unfairly towards different demographic groups. This has led to a growing focus on fairness in AI software recently, alongside the traditional focus on the effectiveness of AI models. Through 26 semi-structured interviews with practitioners from different application domains and with varied backgrounds across 23 countries, we conducted research on fairness requirements in AI from software engineering perspective. Our study assesses the participants' awareness of fairness in AI / ML software and its application within the Software Development Life Cycle (SDLC), from translating fairness concerns into requirements to assessing their arising early in the SDLC. It also examines fairness through the key assessment dimensions of implementation, validation, evaluation, and how it is balanced with trade-offs involving other priorities, such as addressing all the software functionalities and meeting critical delivery deadlines. Findings of our thematic qualitative analysis show that while our participants recognize the aforementioned AI fairness dimensions, practices are inconsistent, and fairness is often deprioritized with noticeable knowledge gaps. This highlights the need for agreement with relevant stakeholders on well-defined, contextually appropriate fairness definitions, the corresponding evaluation metrics, and formalized processes to better integrate fairness into AI/ML projects.

</details>


### [149] [Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors](https://arxiv.org/abs/2512.13860)
*Henger Li,Shuangjie You,Flavio Di Palo,Yiyue Qian,Ayush Jain*

Main category: cs.SE

TL;DR: 针对工具调用中工具文档和知识库上下文与大语言模型信息解读不匹配问题，提出VGCO框架优化相关内容，在单轮大规模工具调用问题上效果显著。


<details>
  <summary>Details</summary>
Motivation: 工具使用效果依赖文档和知识库上下文，但这些材料与大语言模型解读信息存在偏差，在工业场景更突出。

Method: 提出 Verification - Guided Context Optimization (VGCO) 框架，分评估和优化两阶段，利用大语言模型作为编辑器，通过离线学习进行分层编辑。

Result: 在不同大语言模型上的准确率、鲁棒性和泛化能力有显著提升。

Conclusion: VGCO聚焦单轮大规模工具调用问题，相比强调多轮推理的先前工作有优势，能有效改善工具调用相关问题。

Abstract: Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.

</details>


### [150] [Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming](https://arxiv.org/abs/2512.13914)
*Bhargav Chickmagalur Nanjundappa,Spandan Maaheshwari*

Main category: cs.SE

TL;DR: 论文指出大语言模型在多轮对话中效果下降，提出ContextBranch系统，实验表明其能提升回复质量、减少上下文大小，确立对话分支为AI辅助探索性工作的基本原语。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮对话中效果显著下降，当前解决方案存在问题，在探索性编程任务中尤为突出。

Method: 提出ContextBranch系统，提供检查点、分支、切换和注入四个核心原语，并通过30个软件工程场景的对照实验进行评估。

Result: 分支对话比线性对话回复质量更高，在焦点和上下文感知方面有很大改进，在复杂场景中优势明显，分支使上下文大小减少58.1%。

Conclusion: 对话分支是AI辅助探索性工作的基本原语，隔离可防止探索替代方案时的上下文污染。

Abstract: Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.
  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.

</details>


### [151] [Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025](https://arxiv.org/abs/2512.14012)
*Ruanqianqian Huang,Avery Reyna,Sorin Lerner,Haijun Xia,Brian Hempel*

Main category: cs.SE

TL;DR: 本文研究经验丰富的开发者在软件开发中使用AI代理的情况。研究发现开发者重视代理提升效率，但坚持软件质量属性，对使用代理总体积极。


<details>
  <summary>Details</summary>
Motivation: AI代理虽有潜力提升软件开发效率，但在专业软件开发中的角色尚不明确，因此要研究开发者如何使用AI代理。

Method: 通过13次实地观察和99份定性调查。

Result: 开发者重视代理提升效率，保留软件设计和实现的自主性，采用策略控制代理行为，对使用代理总体积极。

Conclusion: 研究揭示了软件开发最佳实践对有效使用代理的价值，指出代理适合的任务类型，并为更好的代理界面和使用指南提供了方向。

Abstract: The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.

</details>


### [152] [PerfCoder: Large Language Models for Interpretable Code Performance Optimization](https://arxiv.org/abs/2512.14018)
*Jiuding Yang,Shengyao Lu,Hongxuan Liu,Shayan Shirahmad Gale Bagi,Zahra Fazel,Tomasz Czajkowski,Di Niu*

Main category: cs.SE

TL;DR: 现有大语言模型在自动代码生成高性能代码能力有限，本文提出PerfCoder模型，在PIE基准测试表现超现有模型，还能与大模型合作提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成高性能代码能力受限，缺乏可解释且有效的性能提升监督。

Method: 引入PerfCoder模型，在有注释的优化轨迹数据上微调，通过强化微调进行偏好对齐，可直接提出改进策略。

Result: 在PIE基准测试中，PerfCoder在运行速度提升和有效优化率上超现有模型，能生成可解释反馈，与大模型合作提升其代码优化性能。

Conclusion: 代码性能优化不能仅靠模型规模，需有优化策略意识。

Abstract: Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.

</details>


### [153] [PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design](https://arxiv.org/abs/2512.14233)
*Ruozhao Yang,Mingfei Cheng,Gelei Deng,Tianwei Zhang,Junjie Wang,Xiaofei Xie*

Main category: cs.SE

TL;DR: 提出首个评估大语言模型的渗透测试基准PentestEval，评估显示模型在渗透测试各阶段表现弱，强调自主渗透测试需更强结构化推理。


<details>
  <summary>Details</summary>
Motivation: 传统渗透测试工作流手动且难扩展，现有大语言模型应用存在不足，缺乏全面评估。

Method: 引入PentestEval，将渗透测试分解为六个阶段，集成专家标注真值和自动化评估流程，对9个常用大语言模型进行阶段级评估。

Result: 大语言模型在各阶段表现普遍较弱，端到端流程成功率仅31%，现有系统有类似局限，自主代理几乎完全失败。

Conclusion: 自主渗透测试需要更强结构化推理，模块化可提升各阶段和整体性能，PentestEval为未来研究提供基础。

Abstract: Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.

</details>


### [154] [Aligning Security Compliance and DevOps: A Longitudinal Study](https://arxiv.org/abs/2512.14453)
*Fabiola Moyón,Florian Angermeir,Daniel Mendez,Tony Gorschek,Markus Voggenreiter,Pierre-Louis Bonvin*

Main category: cs.SE

TL;DR: 本文提出适应安全法规和标准的DevOps框架RefA，并通过西门子的纵向研究验证其有效性，助力企业向DevOps转型并保障安全合规。


<details>
  <summary>Details</summary>
Motivation: 企业采用敏捷方法和DevOps开发软件产品时，在遵循传统线性工作流的安全标准合规方面面临挑战，尤其是关键基础设施相关产品和服务的工程领域，因此需要支持企业向DevOps转型。

Method: 对西门子进行纵向研究，包括基于RefA框架的多个子研究，RefA是基于IEC 62443 - 4 - 1标准的安全合规DevOps生命周期规定性模型。

Result: 展示了RefA能将安全合规知识传递给产品开发团队，支持跨职能团队具备交付合规产品所需技能。

Conclusion: 所提出的RefA框架有助于专业人员在实施DevOps过程中保持安全规范合规，支持企业向DevOps转型。

Abstract: Companies adopt agile methodologies and DevOps to facilitate efficient development and deployment of software-intensive products. This, in turn, introduces challenges in relation to security standard compliance traditionally following a more linear workflow. This is especially a challenge for the engineering of products and services associated with critical infrastructures. To support companies in their transition towards DevOps, this paper presents an adaptation of DevOps according to security regulations and standards. We report on our longitudinal study at Siemens AG, consisting of several individual sub-studies in the inception, validation, and initial adoption of our framework based on RefA as well as the implications for practice. RefA is a prescriptive model of a security compliant DevOps lifecycle based on the IEC 62443-4-1 standard. The overall framework is aimed at professionals, not only security experts, being able to use it on implementing DevOps processes while remaining compliant with security norms. We demonstrate how RefA facilitates the transfer of security compliance knowledge to product development teams. This knowledge transfer supports the agility aim of ensuring that cross-functional teams have all the skills needed to deliver the compliant products.

</details>


### [155] [Teralizer: Semantics-Based Test Generalization from Conventional Unit Tests to Property-Based Tests](https://arxiv.org/abs/2512.14475)
*Johann Glock,Clemens Bauer,Martin Pinzger*

Main category: cs.SE

TL;DR: 提出基于语义方法将单元测试转为基于属性测试，用Teralizer验证，评估有不同效果并给出未来工作路线图。


<details>
  <summary>Details</summary>
Motivation: 传统单元测试验证单输入输出对，多数执行路径输入未测试；基于属性测试定义属性和约束需大量人工。

Method: 通过单路径符号分析从实现中提取规范，将JUnit测试转为基于属性的jqwik测试。

Result: 在不同数据集上评估，EvoSuite生成测试有1 - 4个百分点改进，开发者编写测试改进小，仅1.7%项目完成泛化流程。

Conclusion: 提供未来工作路线图，指出推进测试泛化领域需解决的研究和工程挑战。

Abstract: Conventional unit tests validate single input-output pairs, leaving most inputs of an execution path untested. Property-based testing addresses this shortcoming by generating multiple inputs satisfying properties but requires significant manual effort to define properties and their constraints. We propose a semantics-based approach that automatically transforms unit tests into property-based tests by extracting specifications from implementations via single-path symbolic analysis. We demonstrate this approach through Teralizer, a prototype for Java that transforms JUnit tests into property-based jqwik tests. Unlike prior work that generalizes from input-output examples, Teralizer derives specifications from program semantics.
  We evaluated Teralizer on three progressively challenging datasets. On EvoSuite-generated tests for EqBench and Apache Commons utilities, Teralizer improved mutation scores by 1-4 percentage points. Generalization of mature developer-written tests from Apache Commons utilities showed only 0.05-0.07 percentage points improvement. Analysis of 632 real-world Java projects from RepoReapers highlights applicability barriers: only 1.7% of projects completed the generalization pipeline, with failures primarily due to type support limitations in symbolic analysis and static analysis limitations in our prototype. Based on the results, we provide a roadmap for future work, identifying research and engineering challenges that need to be tackled to advance the field of test generalization.
  Artifacts available at: https://doi.org/10.5281/zenodo.17950381

</details>


### [156] [MoT: A Model-Driven Low-Code Approach for Simplifying Cloud-of-Things Application Development](https://arxiv.org/abs/2512.14613)
*Cristiano Welter,Kleinner Farias*

Main category: cs.SE

TL;DR: 本文介绍基于模型的MoT方法简化云物联应用开发，通过案例和问卷评估其可行性，结果显示能提升开发效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 当前云物联应用开发需大量技术专业知识且缺乏标准化、模型驱动方法，现有方法无法确保互操作性、自动化和效率。

Method: 引入结合低代码原则的基于模型的MoT方法，提供定制UML概要文件，进行案例研究和TAM问卷调查。

Result: 证实了MoT的可行性，简化了云物联应用开发与部署，用户认为其易用且有用，能降低复杂度、加快开发。

Conclusion: MoT为云物联应用开发提供了有前景的模型驱动解决方案，降低了门槛、促进自动化，朝着更友好框架迈进。

Abstract: The integration of cloud computing and the Internet of Things (IoT) is essential for scalable, intelligent systems. However, developing cloud-of-things (CoT) applications remains challenging. It requires significant technical expertise and lacks standardized, model-driven methodologies. Current approaches fail to ensure interoperability, automation, and efficiency. This study introduces the Model of Things (MoT), a model-based approach that incorporates low-code principles to simplify CoT development. MoT reduces technical barriers by providing a custom UML profile designed for IoT and cloud services. To evaluate MoT, we conducted a case study and a Technology Acceptance Model (TAM) questionnaire. The results confirmed MoT's feasibility, demonstrating that it streamlines CoT application development and deployment. Users found MoT accessible, even with limited IoT experience, and reported high perceived ease of use and usefulness. Qualitative feedback highlighted MoT's ability to reduce complexity and speed up development. MoT offers a promising, model-driven solution for CoT application development. By lowering entry barriers and promoting automation, it enhances both efficiency and flexibility. This study represents a step toward a more user-friendly framework, enabling broader adoption of CoT technologies.

</details>


### [157] [Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI](https://arxiv.org/abs/2512.14673)
*Ronnie de Souza Santos,Cleyton Magalhães,Italo Santos*

Main category: cs.SE

TL;DR: 现有LLM聊天机器人可持续性评估多关注架构等，忽视用户交互影响，论文从四维度指出交互行为对环境影响，需重新设计交互。


<details>
  <summary>Details</summary>
Motivation: 随着LLM聊天机器人发展，可持续性问题凸显，现有缓解措施忽视用户交互对系统能源消耗的影响。

Method: 从延长对话模式、即时响应期望、日常用户习惯、上下文积累四个维度分析用户交互行为对LLM系统环境影响。

Result: 明确了用户交互行为在四个维度上对LLM系统环境产生的影响。

Conclusion: 需重新思考聊天机器人交互设计，将可持续性纳入用户对话规范考量。

Abstract: LLM based chatbots have become central interfaces in technical, educational, and analytical domains, supporting tasks such as code reasoning, problem solving, and information exploration. As these systems scale, sustainability concerns have intensified, with most assessments focusing on model architecture, hardware efficiency, and deployment infrastructure. However, existing mitigation efforts largely overlook how user interaction practices themselves shape the energy profile of LLM based systems. In this vision paper, we argue that interaction level behavior appears to be an underexamined factor shaping the environmental impact of LLM based systems, and we present this issue across four dimensions. First, extended conversational patterns increase token production and raise the computational cost of inference. Second, expectations of instant responses limit opportunities for energy aware scheduling and workload consolidation. Third, everyday user habits contribute to cumulative operational demand in ways that are rarely quantified. Fourth, the accumulation of context affects memory requirements and reduces the efficiency of long running dialogues. Addressing these challenges requires rethinking how chatbot interactions are designed and conceptualized, and adopting perspectives that recognize sustainability as partly dependent on the conversational norms through which users engage with LLM based systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [158] [One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing](https://arxiv.org/abs/2512.13892)
*Albert Dorador*

Main category: stat.ML

TL;DR: 提出用单一确定性最优排列替代多次随机排列的特征贡献估计方法，验证其优势并引入系统变量重要性用于模型压力测试。


<details>
  <summary>Details</summary>
Motivation: 现有基于排列的特征贡献估计方法存在计算开销大、随机不稳定问题，需要改进。

Method: 用单一确定性最优排列替代多次随机排列；引入系统变量重要性考虑特征相关性。

Result: 在近200个场景验证方法优势，如改进偏差 - 方差权衡和准确性；通过两个案例展示系统变量重要性可审计模型。

Conclusion: 新方法非随机、更快、更稳定，系统变量重要性可用于评估模型公平性和系统性风险。

Abstract: Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.

</details>


### [159] [Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics](https://arxiv.org/abs/2512.13997)
*Aaron Wei,Milad Jalali,Danica J. Sutherland*

Main category: stat.ML

TL;DR: 本文解决了现有两样本测试技术在样本量不同时的局限，通过扩展广义U统计量理论，得到MMD估计量渐近分布的新表征，还给出优化测试功效的新标准，提升了测试准确性和适用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于MMD的两样本测试技术常假设两分布样本量相等，应用时会丢弃有价值数据，降低测试功效。

Method: 扩展广义U统计量理论，并将其应用于常见的MMD估计量。

Result: 得到MMD估计量在样本量不同时渐近分布的新表征，给出优化MMD测试功效的新标准，还对MMD估计量方差有更清晰的刻画。

Conclusion: 该方法保留了所有可用数据，提高了测试准确性和在实际场景中的适用性。

Abstract: Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.

</details>


### [160] [On the Hardness of Conditional Independence Testing In Practice](https://arxiv.org/abs/2512.14000)
*Zheng He,Roman Pogodin,Yazhe Li,Namrata Deka,Arthur Gretton,Danica J. Sutherland*

Main category: stat.ML

TL;DR: 本文研究核条件独立性（KCI）测试，指出其实际行为背后的主要因素，强调条件均值嵌入估计误差对第一类错误的关键作用，以及选择合适条件核的重要性。


<details>
  <summary>Details</summary>
Motivation: Shah和Peters（2020）的结果无法解释流行的条件独立性测试在实际中的频繁失败情况，所以要研究KCI测试实际行为背后的因素。

Method: 研究Kernel - based Conditional Independence (KCI)测试，指出广义协方差测度几乎是其特殊情况，分析其实际表现的主要因素。

Result: 发现条件均值嵌入估计误差对第一类错误起关键作用，选择合适条件核对检验效力很重要，但也会使第一类错误膨胀。

Conclusion: 明确了KCI测试实际表现的主要因素，以及条件均值嵌入估计误差和条件核选择的重要影响。

Abstract: Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on "hiding" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.

</details>


### [161] [Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms](https://arxiv.org/abs/2512.14221)
*Jiarong Fan,Juhyun Park. Thi Phuong Thuy Vo,Nicolas Brunel*

Main category: stat.ML

TL;DR: 针对共变量缺失的保形预测问题，提出预插补 - 掩码 - 校正框架，可保证边际覆盖和掩码条件有效性，在合成和真实数据集评估表明能减少预测区间宽度。


<details>
  <summary>Details</summary>
Motivation: 保形预测在面对缺失协变量时无法保证覆盖率，掩码条件有效覆盖是比边际覆盖更理想的属性，需解决不同缺失模式引起的异质性问题。

Method: 提出预插补 - 掩码 - 校正框架，采用加权保形预测程序对校准数据集进行分布插补后校正预测集，推导出两个算法。

Result: 两个算法近似边际有效且MCV有效，与标准MCV方法相比显著减少了预测区间宽度，同时维持目标保证。

Conclusion: 所提方法可解决共变量缺失的保形预测问题，提供有效覆盖，在性能上有提升。

Abstract: Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees.

</details>


### [162] [Improving the Accuracy of Amortized Model Comparison with Self-Consistency](https://arxiv.org/abs/2512.14308)
*Šimon Kucharský,Aayush Mishra,Daniel Habermann,Stefan T. Radev,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 研究自一致性（SC）对四种不同概念化的摊销贝叶斯模型比较的改进，发现基于参数后验的方法表现更好，SC训练在似然可用时提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 摊销贝叶斯推理（ABI）方法对模型误设高度敏感，在模型比较场景中是挑战，近期的自一致性（SC）工作提供了解决办法。

Method: 研究SC如何改进四种不同概念化的摊销模型比较，开展两个合成和两个真实世界的案例研究。

Result: 通过近似参数后验估计边际似然的模型比较方法始终优于直接近似模型证据或后验模型概率的方法；似然可用时SC训练提升鲁棒性，无解析似然的方法受益有限且不稳定。

Conclusion: 建议采用基于参数后验的方法，并在经验数据集上进行SC训练以减轻模型误设下的外推偏差。

Abstract: Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification.

</details>


### [163] [Continual Learning at the Edge: An Agnostic IIoT Architecture](https://arxiv.org/abs/2512.14311)
*Pablo García-Santaclara,Bruno Fernández-Castro,Rebeca P. Díaz-Redondo,Carlos Calvo-Moa,Henar Mariño-Bodelón*

Main category: stat.ML

TL;DR: 因联网设备增长，传统集中计算面临挑战，边缘计算可应对，结合增量学习可用于工业实时质量控制。


<details>
  <summary>Details</summary>
Motivation: 联网设备增长使传统集中计算遇挑战，传统机器学习不适用于边缘计算，需要解决工业制造系统实时质量控制问题。

Method: 在边缘计算场景中应用增量学习理念，运用持续学习减少灾难性遗忘影响。

Result: 未提及具体结果。

Conclusion: 该方法能提供高效有效的解决方案。

Abstract: The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution.

</details>


### [164] [From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification](https://arxiv.org/abs/2512.14404)
*Hangjun Cho,Fabio V. G. Amaral,Andrei A. Klishin,Cassio M. Oishi,Steven L. Brunton*

Main category: stat.ML

TL;DR: 本文重新审视基于字典的稀疏回归，提出得分引导的库选择方法，进行理论分析并通过数值实验验证其在动力系统识别中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为数据驱动建模提供实用指导，特别是针对SINDy类型算法。

Method: 重新审视Sequential Threshold Least Squares (STLS)算法，提出得分引导的库选择策略，并进行理论分析和数值实验。

Result: 数值实验表明基于得分的筛选方法能提高动力系统识别的准确性和可解释性。

Conclusion: 在某些情况下，集成得分引导方法来更准确地优化字典，有助于SINDy用户提高数据驱动发现控制方程的鲁棒性。

Abstract: In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the $\ell_0$ sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations.

</details>


### [165] [LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts](https://arxiv.org/abs/2512.14604)
*Prasanjit Dubey,Aritra Guha,Zhengyi Zhou,Qiong Wu,Xiaoming Huo,Paromita Dubey*

Main category: stat.ML

TL;DR: 提出LLmFPCA - detect框架，用于处理稀疏纵向文本数据，结合大语言模型文本嵌入与函数数据分析，实验证明其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 稀疏纵向文本数据缺乏专用分析方法，且数据存在噪声、异质性和异常值，难以检测和推断关键模式，但这类数据能为未来政策和定向推荐提供信息。

Method: LLmFPCA - detect框架先使用大语言模型提示将文本嵌入到特定数值空间，在数值空间进行稀疏多元函数主成分分析恢复总体特征，产生主体级分数，结合基线静态协变量进行数据分割、无监督异常检测等下游任务，并利用大语言模型进行动态关键词分析。

Result: 实验支持了LLmFPCA - detect的稳定性，在亚马逊客户评论和维基百科讨论页评论两个公开数据集应用中表现良好，优于现有基线方法。

Conclusion: LLmFPCA - detect框架在处理稀疏纵向文本数据时实用有效，跨领域表现佳，其聚类特定功能主成分分数可提升预测性能。

Abstract: Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [166] [A variational Bayes latent class approach for EHR-based patient phenotyping in R](https://arxiv.org/abs/2512.14272)
*Brian Buckley,Adrian O'Hagan,Marie Galligan*

Main category: stat.CO

TL;DR: VBphenoR包采用闭式变分贝叶斯方法，用EHR数据进行患者表型分析。


<details>
  <summary>Details</summary>
Motivation: 提供基于EHR数据进行患者表型分析的方法。

Method: 使用闭式坐标上升变分推断（CAVI）实现变分贝叶斯高斯混合模型（GMM）算法确定患者表型潜类别，实施变分贝叶斯逻辑回归，其似然应用GMM步骤的潜类别告知条件。

Result: 文档未提及结果。

Conclusion: 文档未提及结论。

Abstract: The VBphenoR package for R provides a closed-form variational Bayes approach to patient phenotyping using Electronic Health Records (EHR) data. We implement a variational Bayes Gaussian Mixture Model (GMM) algorithm using closed-form coordinate ascent variational inference (CAVI) to determine the patient phenotype latent class. We then implement a variational Bayes logistic regression, where we determine the probability of the phenotype in the supplied EHR cohort, the shift in biomarkers for patients with the phenotype of interest versus a healthy population and evaluate predictive performance of binary indicator clinical codes and medication codes. The logistic model likelihood applies the latent class from the GMM step to inform the conditional.

</details>


### [167] [Two Bayesian Approaches to Dynamic Gaussian Bayesian Networks with Intra- and Inter-Slice Edges](https://arxiv.org/abs/2512.14512)
*Kezhuo Li,Marco Grzegorczyk*

Main category: stat.CO

TL;DR: 对比两种基于Gaussian BGe分数的GDBN贝叶斯建模方法（mBGe和eBGe），发现两者产生不同网络结构等价类，提出新算法识别eBGe非标准等价类。


<details>
  <summary>Details</summary>
Motivation: 对比基于Gaussian BGe分数的两种GDBN贝叶斯建模方法（mBGe和eBGe）的性能。

Method: 对两种模型进行对比，并进行实证比较。提出新的DAG - to - CPDAG算法变体识别eBGe模型的非标准等价类。

Result: 两种模型产生不同的网络结构等价类，eBGe模型的等价类是非标准的。

Conclusion: 两种GDBN贝叶斯建模方法在网络结构等价类上有差异，eBGe模型有未被报道过的非标准等价类。

Abstract: Gaussian Dynamic Bayesian Networks (GDBNs) are a widely used tool for learning network structures from continuous time-series data. To capture both time-lagged and contemporaneous dependencies, advanced GDBNs allow for dynamic inter-slice edges as well as static intra-slice edges. In the literature, two Bayesian modeling approaches have been developed for GDBNs. Both build on and extend the well-known Gaussian BGe score. We refer to them as the mean-adjusted BGe (mBGe) and the extended BGe (eBGe) models. In this paper, we contrast the two models and compare their performance empirically. The main finding of our study is that the two models induce different equivalence classes of network structures. In particular, the equivalence classes implied by the eBGe model are non-standard, and we propose a new variant of the DAG-to-CPDAG algorithm to identify them. To the best of our knowledge, these non-standard equivalence classes have not been previously reported.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [168] [Simultaneous and Proportional Finger Motion Decoding Using Spatial Features from High-Density Surface Electromyography](https://arxiv.org/abs/2512.13870)
*Ricardo Gonçalves Molinari,Leonardo Abdala Elias*

Main category: eess.SP

TL;DR: 本文评估了基于多通道线性描述符的块场方法（MLD - BFM）用于连续解码五指关节自由度，发现其能提高手指运动解码精度，为肌电接口设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 恢复自然直观手部功能需多自由度同步比例控制，要探索有效解码方法。

Method: 让21名健康参与者做动态手指运动，记录高清表面肌电信号，MLD - BFM提取特定区域空间特征，应用于多输出回归模型并与传统方法对比。

Result: MLD - BFM在各模型中R²vw值最高，MLP结合MLD - BFM性能最佳，时域特征有强预测能力，降维技术精度低，中指和无名指解码精度高于拇指。

Conclusion: MLD - BFM提高了解码精度，空间结构特征能增强多自由度同步比例控制，为肌电接口设计提供实用指导。

Abstract: Restoring natural and intuitive hand function requires simultaneous and proportional control (SPC) of multiple degrees of freedom (DoFs). This study systematically evaluated the multichannel linear descriptors-based block field method (MLD-BFM) for continuous decoding of five finger-joint DoFs by leveraging the rich spatial information of high-density surface electromyography (HD sEMG). Twenty-one healthy participants performed dynamic sinusoidal finger movements while HD sEMG signals were recorded from the \textit{extensor digitorum communis} (EDC) and \textit{flexor digitorum superficialis} (FDS) muscles. MLD-BFM extracted region-specific spatial features, including effective field strength ($Σ$), field-strength variation rate ($Φ$), and spatial complexity ($Ω$). Model performance was optimized (block size: $2 \times 2$; window: 0.15 s) and compared with conventional time-domain features and dimensionality reduction approaches when applied to multi-output regression models. MLD-BFM consistently achieved the highest $\mathrm{R}^2_{\mathrm{vw}}$ values across all models. The multilayer perceptron (MLP) combined with MLD-BFM yielded the best performance ($\mathrm{R}^2_{\mathrm{vw}} = 86.68\% \pm 0.33$). Time-domain features also showed strong predictive capability and were statistically comparable to MLD-BFM in some models, whereas dimensionality reduction techniques exhibited lower accuracy. Decoding accuracy was higher for the middle and ring fingers than for the thumb. Overall, MLD-BFM improved continuous finger movement decoding accuracy, underscoring the importance of taking advantage of the spatial richness of HD sEMG. These findings suggest that spatially structured features enhance SPC and provide practical guidance for designing robust, real-time, and responsive myoelectric interfaces.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [169] [A Threshold-Triggered Deep Q-Network-Based Framework for Self-Healing in Autonomic Software-Defined IIoT-Edge Networks](https://arxiv.org/abs/2512.14297)
*Agrippina Mwangi,León Navarro-Hilfiker,Lukasz Brewka,Mikkel Gryning,Elena Fumagalli,Madeleine Gibescu*

Main category: cs.NI

TL;DR: 本文提出阈值触发的深度Q网络自愈代理应对软件定义工业网络随机中断问题，仿真显示其恢复性能佳，还能维持交换机热稳定性。


<details>
  <summary>Details</summary>
Motivation: 随机中断导致软件定义工业网络间歇性服务降级，影响风电工厂可靠及时通信，需解决该问题。

Method: 提出阈值触发的深度Q网络自愈代理，在云概念验证测试平台的仿真三集群交换网络上训练、验证和测试。

Result: 该代理比基线最短路径和负载均衡路由方法的中断恢复性能提高53.84%，在超脊柱叶数据平面架构中优于现有方法，还能维持交换机热稳定性。

Conclusion: 深度强化学习在关键任务、时间敏感应用场景的软件定义工业网络中构建弹性方面有潜力。

Abstract: Stochastic disruptions such as flash events arising from benign traffic bursts and switch thermal fluctuations are major contributors to intermittent service degradation in software-defined industrial networks. These events violate IEC~61850-derived quality-of-service requirements and user-defined service-level agreements, hindering the reliable and timely delivery of control, monitoring, and best-effort traffic in IEC~61400-25-compliant wind power plants. Failure to maintain these requirements often results in delayed or lost control signals, reduced operational efficiency, and increased risk of wind turbine generator downtime.
  To address these challenges, this study proposes a threshold-triggered Deep Q-Network self-healing agent that autonomically detects, analyzes, and mitigates network disruptions while adapting routing behavior and resource allocation in real time. The proposed agent was trained, validated, and tested on an emulated tri-clustered switch network deployed in a cloud-based proof-of-concept testbed.
  Simulation results show that the proposed agent improves disruption recovery performance by 53.84% compared to a baseline shortest-path and load-balanced routing approach and outperforms state-of-the-art methods, including the Adaptive Network-based Fuzzy Inference System by 13.1% and the Deep Q-Network and traffic prediction-based routing optimization method by 21.5%, in a super-spine leaf data-plane architecture.
  Additionally, the agent maintains switch thermal stability by proactively initiating external rack cooling when required. These findings highlight the potential of deep reinforcement learning in building resilience in software-defined industrial networks deployed in mission-critical, time-sensitive application scenarios.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [170] [Founder Backgrounds and Startup Funding: Evidence from Y Combinator](https://arxiv.org/abs/2512.13755)
*Rommin Adl*

Main category: econ.GN

TL;DR: 研究发现创始人背景对YC创业公司融资差异影响小，团队规模是更稳定的融资预测因素，未观察到的行业和产品质量等因素可能主导融资决策。


<details>
  <summary>Details</summary>
Motivation: 探究创始人背景等因素对YC创业公司融资结果的影响。

Method: 使用2005 - 2024年4323家YC公司数据与S&P Global融资数据合并，对2113家公司样本进行带批次年份固定效应的OLS回归分析。

Result: 创始人之前FAANG工作经验系数为 - 0.251，意味着融资约少22%，但结果不稳健；YC内创始团队规模越大融资越多，每增加一位联合创始人融资约多21%。

Conclusion: 可观察的资质对融资差异解释小，团队规模是更一致的预测因素，未观察因素可能主导融资决策。

Abstract: While founder backgrounds account for less than 4% of funding variation among Y Combinator startups, this suggests that other factors, such as industry trends and product innovation, may play a more significant role in funding outcomes. Using data on 4,323 YC companies from 2005-2024 merged with S&P Global funding data, I estimate OLS regressions with batch year fixed effects on a regression sample of 2,113 companies. The coefficient on prior FAANG work experience is -0.251, indicating approximately 22% less funding. However, this result is not robust, as it changes direction in further analyses, suggesting that FAANG experience may not be a reliable predictor of funding. The most robust finding is that startups within Y Combinator that consist of larger founding teams tend to raise more funding, with each additional co-founder associated with approximately 21% more capital raised. While observable credentials such as prior FAANG work experience and top-tier education explain minimal variation in funding, the size of the founding team emerges as a more consistent predictor, highlighting the importance of team dynamics in securing capital. Unobserved factors like industry and product quality likely dominate funding decisions within this elite accelerator cohort.

</details>


### [171] [Innovation, Institutions and Three Dimensions of Financial Structure](https://arxiv.org/abs/2512.14154)
*Yimin Wu,Tomoo Kikuchi*

Main category: econ.GN

TL;DR: 研究1982 - 2021年75个国家股市相对银行业对创新的响应，发现创新提升股市相对银行业的活跃度、效率和规模，受技术前沿接近度和制度质量调节。


<details>
  <summary>Details</summary>
Motivation: 探究股市相对银行业对创新的响应情况。

Method: 使用1982 - 2021年75个国家的面板数据进行研究。

Result: 创新提升股市相对银行业的活跃度、效率和规模，制度质量对活跃度和效率的调节效应为正，对规模为负，调节效应可能是非线性的，创新对活跃度的边际效应持久，制度质量调节效应渐弱。

Conclusion: 创新会影响股市相对银行业的表现，且技术前沿接近度和制度质量起到调节作用。

Abstract: This paper studies the response of stock markets relative to the banking sector to innovation by using a panel of 75 countries from 1982 to 2021. We find that innovation increases the activity, efficiency and size of stock markets relative to the banking sector, moderated by proximity to technological frontier and institutional quality. The moderating effect of institutional quality is positive for activity and efficiency but negative for size. Moreover, the moderating effect can be nonlinear depending on specific indicators. The marginal effect of innovation on the activity is persistent over many years, but the moderating effect of institutional quality gradually fades away.

</details>


### [172] [Location-Robust Cost-Preserving Blended Pricing for Multi-Campus AI Data Centers](https://arxiv.org/abs/2512.14197)
*Qi He*

Main category: econ.GN

TL;DR: 论文指出部署加权混合校园价格存在问题，提出两种实用算子解决成本混合定价问题，模拟和示例显示有良好效果。


<details>
  <summary>Details</summary>
Motivation: 大规模AI数据中心财务和运营需单个系统层面的'全球价格'，而常用的部署加权混合校园价格会触发类似辛普森聚合失败问题。

Method: 提出两种实用算子，包括双向固定效应算子和凸公共权重算子。

Result: 模拟和AI数据中心运营支出示例显示，与简单混合相比，排名违规大幅减少，同时保持成本准确性，且可分布式实现。

Conclusion: 所提的两种算子能有效协调会计恒等式与排名稳健性和生产可实施性。

Abstract: Large-scale AI data center portfolios procure identical SKUs across geographically heterogeneous campuses, yet finance and operations require a single system-level 'world price' per SKU for budgeting and planning. A common practice is deployment-weighted blending of campus prices, which preserves total cost but can trigger Simpson-type aggregation failures: heterogeneous location mixes can reverse SKU rankings and distort decision signals.
  I formalize cost-preserving blended pricing under location heterogeneity and propose two practical operators that reconcile accounting identity with ranking robustness and production implementability. A two-way fixed-effects operator separates global SKU effects from campus effects and restores exact cost preservation via scalar normalization, providing interpretable decomposition and smoothing under mild missingness. A convex common-weight operator computes a single set of campus weights under accounting constraints to enforce a location-robust benchmark and prevent dominance reversals; I also provide feasibility diagnostics and a slack-based fallback for extreme mix conditions. Simulations and an AI data center OPEX illustration show substantial reductions in ranking violations relative to naive blending while maintaining cost accuracy, with scalable distributed implementation.

</details>


### [173] [The Role of Employment Flexibility in Enhancing the Competitiveness of Temporary Staffing Service Providers in Poland](https://arxiv.org/abs/2512.14293)
*Michał Ćwiąkała,Dariusz Baran,Gabriela Wojak,Ernest Górka,Piotr Czarnecki,Patryk Paś,Marcin Kubera*

Main category: econ.GN

TL;DR: 本文基于波兰实证研究就业灵活性对使用临时用工服务企业竞争力的影响，发现其有益，是战略人力资源能力。


<details>
  <summary>Details</summary>
Motivation: 探究就业灵活性对使用临时用工服务企业竞争力的作用。

Method: 对与临时用工机构合作的波兰企业管理者和所有者进行定量调查，采用有目的抽样。

Result: 就业灵活性降低停机时间、加速入职流程、降低成本，便于调整劳动力规模、获取专业技能，增强组织响应能力、提高短期项目盈利能力、增强应对波动能力，提升客户满意度和忠诚度。

Conclusion: 就业灵活性不仅是成本控制手段，更是支持竞争力、运营适应性和可持续绩效的战略人力资源能力。

Abstract: This paper examines the role of employment flexibility in enhancing the competitiveness of firms using temporary staffing services, with empirical evidence from Poland. The study focuses on how flexible employment arrangements influence operational efficiency, cost reduction, workforce scalability, market responsiveness, and client satisfaction. A quantitative survey was conducted among managers and owners of Polish enterprises that cooperate with temporary staffing agencies, using purposeful sampling to capture informed managerial perspectives. The findings show that employment flexibility significantly reduces downtime, accelerates onboarding processes, and lowers personnel and recruitment costs. Flexible staffing enables rapid workforce scaling during demand fluctuations and facilitates access to specialized skills without long-term commitments. The results also indicate that employment flexibility enhances organizational responsiveness, improves profitability in short-term projects, and strengthens resilience to seasonal and market volatility. Additionally, flexibility is identified as a key determinant of client satisfaction and loyalty toward staffing service providers. The study demonstrates that employment flexibility is not merely a cost-control mechanism but a strategic human resource capability that supports competitiveness, operational adaptability, and sustainable performance in dynamic labor markets.

</details>


### [174] [Pattern Recognition of Aluminium Arbitrage in Global Trade Data](https://arxiv.org/abs/2512.14410)
*Muhammad Sukri Bin Ramli*

Main category: econ.GN

TL;DR: 研究借助机器学习框架分析UN Comtrade数据，发现贸易异常为硬件掩饰和基于贸易的洗钱，而非可持续性套利，建议海关执法转向算法估值审计。


<details>
  <summary>Details</summary>
Motivation: 全球经济脱碳背景下，碳边境调节机制加大价格套利空间，有必要监测贸易异常。

Method: 提出统一无监督机器学习框架，采用四层分析管道，运用法医统计、隔离森林、网络科学和深度自编码器。

Result: 发现硬件掩饰现象，非法行为者利用双向关税激励进行洗钱，风险集中在影子枢纽，价格偏差是异常主要预测指标。

Conclusion: 海关执法需从物理体积检查转向动态算法估值审计。

Abstract: As the global economy transitions toward decarbonization, the aluminium sector has become a focal point for strategic resource management. While policies such as the Carbon Border Adjustment Mechanism (CBAM) aim to reduce emissions, they have inadvertently widened the price arbitrage between primary metal, scrap, and semi-finished goods, creating new incentives for market optimization. This study presents a unified, unsupervised machine learning framework to detect and classify emerging trade anomalies within UN Comtrade data (2020 to 2024). Moving beyond traditional rule-based monitoring, we apply a four-layer analytical pipeline utilizing Forensic Statistics, Isolation Forests, Network Science, and Deep Autoencoders. Contrary to the hypothesis that Sustainability Arbitrage would be the primary driver, empirical results reveal a contradictory and more severe phenomenon of Hardware Masking. Illicit actors exploit bi-directional tariff incentives by misclassifying scrap as high-count heterogeneous goods to justify extreme unit-price outliers of >$160/kg, a 1,900% markup indicative of Trade-Based Money Laundering (TBML) rather than commercial arbitrage. Topologically, risk is not concentrated in major exporters but in high-centrality Shadow Hubs that function as pivotal nodes for illicit rerouting. These actors execute a strategy of Void-Shoring, systematically suppressing destination data to Unspecified Code to fracture mirror statistics and sever forensic trails. Validated by SHAP (Shapley Additive Explanations), the results confirm that price deviation is the dominant predictor of anomalies, necessitating a paradigm shift in customs enforcement from physical volume checks to dynamic, algorithmic valuation auditing.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [175] [Unreasonable effectiveness of unsupervised learning in identifying Majorana topology](https://arxiv.org/abs/2512.13825)
*Jacob Taylor,Haining Pan,Sankar Das Sarma*

Main category: cond-mat.dis-nn

TL;DR: 结合无监督和有监督学习，用自编码器分析马约拉纳分裂未标记数据，可区分拓扑和非拓扑并确定转变点，助于识别马约拉纳纳米线拓扑。


<details>
  <summary>Details</summary>
Motivation: 无监督学习可用于识别拓扑序，但面临挑战且计算资源需求大，需有效方法。

Method: 结合无监督和有监督学习，使用自编码器。

Result: 未标记数据能区分拓扑和非拓扑，确定相关参数空间的转变点。

Conclusion: 该方法是识别马约拉纳纳米线拓扑的有用工具。

Abstract: In unsupervised learning, the training data for deep learning does not come with any labels, thus forcing the algorithm to discover hidden patterns in the data for discerning useful information. This, in principle, could be a powerful tool in identifying topological order since topology does not always manifest in obvious physical ways (e.g., topological superconductivity) for its decisive confirmation. The problem, however, is that unsupervised learning is a difficult challenge, necessitating huge computing resources, which may not always work. In the current work, we combine unsupervised and supervised learning using an autoencoder to establish that unlabeled data in the Majorana splitting in realistic short disordered nanowires may enable not only a distinction between `topological' and `trivial', but also where their crossover happens in the relevant parameter space. This may be a useful tool in identifying topology in Majorana nanowires.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [176] [Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery](https://arxiv.org/abs/2512.13930)
*Samuel Rothfarb,Megan C. Davis,Ivana Matanovic,Baikun Li,Edward F. Holby,Wilton J. M. Kort-Kamp*

Main category: cond-mat.mtrl-sci

TL;DR: 介绍了主动学习框架MASTER，其让大语言模型自主进行原子模拟，在两个化学应用中减少模拟量，多智能体协作加速材料发现。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法多自动化程序性任务，缺乏科学推理，限制发现自主性。

Method: 引入MASTER框架，有单智能体基线和三种多智能体方法。

Result: 在两个化学应用中，推理驱动探索比试错选择最多减少90%的原子模拟量。

Conclusion: 多智能体协作加速材料发现，开创自主科学探索新范式。

Abstract: Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including a single agent baseline and three multi-agent approaches: peer review, triage-ranking, and triage-forms. Across two chemical applications, CO adsorption on Cu-surface transition metal (M) adatoms and on M-N-C catalysts, reasoning-driven exploration reduces required atomistic simulations by up to 90% relative to trial-and-error selection. Reasoning trajectories reveal chemically grounded decisions that cannot be explained by stochastic sampling or semantic bias. Altogether, multi-agent collaboration accelerates materials discovery and marks a new paradigm for autonomous scientific exploration.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [177] [A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs](https://arxiv.org/abs/2512.14179)
*K. M. Jubair Sami,Dipto Sumit,Ariyan Hossain,Farig Sadeque*

Main category: cs.CL

TL;DR: 提出并比较两种RAG流水线用于标准孟加拉语到方言的翻译，发现句子对流水线表现更好，表明检索策略比模型大小更重要，提供低资源方言翻译的无微调解决方案。


<details>
  <summary>Details</summary>
Motivation: 标准语言到方言的翻译存在数据稀缺和语言差异的问题，以孟加拉语尤为突出。

Method: 提出并比较两种新颖的RAG流水线（基于转录本和标准化句子对），使用多种评估指标在六种孟加拉语方言和多个大语言模型上进行评估。

Result: 句子对流水线始终优于基于转录本的流水线，奇塔贡方言的WER从76%降至55%，小模型可优于大模型。

Conclusion: 该RAG方法提供了有效的无微调低资源方言翻译方案，有助于保护语言多样性。

Abstract: Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\_dialect:standard\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\% to 55\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.

</details>


### [178] [MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset](https://arxiv.org/abs/2511.19317)
*Md. Tanzim Ferdous,Naeem Ahsan Chowdhury,Prithwiraj Bhattacharjee*

Main category: cs.CL

TL;DR: 研究开发了新的孟加拉语摘要生成数据集，可处理多源文章，用多种模型训练评估，为未来研究提供基准。


<details>
  <summary>Details</summary>
Motivation: 现有研究多针对新闻文章，无法适应真实世界孟加拉语文本多样性，数字时代需能减少信息过载的摘要系统。

Method: 开发含超54000篇文章及摘要的多源数据集，用LSTM、BanglaT5 - small、MTS - small等深度学习和迁移学习模型训练评估。

Result: 结果凸显该数据集作为孟加拉语自然语言处理未来研究基准的潜力。

Conclusion: 数据集为构建稳健摘要系统提供基础，有助于拓展低资源语言的NLP资源。

Abstract: This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.

</details>


### [179] [Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed](https://arxiv.org/abs/2512.14067)
*Yonggan Fu,Lexington Whalen,Zhifan Ye,Xin Dong,Shizhe Diao,Jingyu Liu,Chengyue Wu,Hao Zhang,Enze Xie,Song Han,Maksim Khadkevich,Jan Kautz,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 研究AR到dLM的转换，提出改进方法，生成Efficient - DLM家族模型，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（dLMs）从头训练学习效率低于自回归（AR）语言模型，需将预训练AR模型转化为高效dLM。

Method: 先对比不同注意力模式，引入带分块注意力模式的连续预训练方案；再提出位置依赖的标记掩码策略。

Result: Efficient - DLM家族模型表现优于现有AR模型和dLMs，如Efficient - DLM 8B比Dream 7B和Qwen3 4B有更高准确率和吞吐量。

Conclusion: 提出的原则和方法可实现可扩展的AR到dLM转换，生成高效dLMs。

Abstract: Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.

</details>


### [180] [Olmo 3](https://arxiv.org/abs/2512.13961)
*Team Olmo,:,Allyson Ettinger,Amanda Bertsch,Bailey Kuehl,David Graham,David Heineman,Dirk Groeneveld,Faeze Brahman,Finbarr Timbers,Hamish Ivison,Jacob Morrison,Jake Poznanski,Kyle Lo,Luca Soldaini,Matt Jordan,Mayee Chen,Michael Noukhovitch,Nathan Lambert,Pete Walsh,Pradeep Dasigi,Robert Berry,Saumya Malik,Saurabh Shah,Scott Geng,Shane Arora,Shashank Gupta,Taira Anderson,Teng Xiao,Tyler Murray,Tyler Romero,Victoria Graf,Akari Asai,Akshita Bhagia,Alexander Wettig,Alisa Liu,Aman Rangapur,Chloe Anastasiades,Costa Huang,Dustin Schwenk,Harsh Trivedi,Ian Magnusson,Jaron Lochner,Jiacheng Liu,Lester James V. Miranda,Maarten Sap,Malia Morgan,Michael Schmitz,Michal Guerquin,Michael Wilson,Regan Huff,Ronan Le Bras,Rui Xin,Rulin Shao,Sam Skjonsberg,Shannon Zejiang Shen,Shuyue Stella Li,Tucker Wilde,Valentina Pyatkin,Will Merrill,Yapei Chang,Yuling Gu,Zhiyuan Zeng,Ashish Sabharwal,Luke Zettlemoyer,Pang Wei Koh,Ali Farhadi,Noah A. Smith,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 介绍7B和32B参数规模的最先进全开放语言模型Olmo 3，涵盖全模型流程，旗舰模型Olmo 3 Think 32B是目前最强全开放思考模型。


<details>
  <summary>Details</summary>
Motivation: 推出先进的全开放语言模型，以满足长上下文推理、函数调用、编码等多种需求。

Method: 未提及

Result: 发布Olmo 3模型家族，包含全模型流程，有旗舰模型Olmo 3 Think 32B。

Conclusion: Olmo 3是先进的全开放语言模型，其中Olmo 3 Think 32B是目前最强全开放思考模型。

Abstract: We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.

</details>


### [181] [From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition](https://arxiv.org/abs/2512.14244)
*Yiqing Zhou,Yu Lei,Shuzheng Si,Qingyan Sun,Wei Wang,Yifei Wu,Hao Wen,Gang Chen,Fanchao Qi,Maosong Sun*

Main category: cs.CL

TL;DR: 提出基于EDU的上下文压缩器解决大语言模型处理长上下文的瓶颈，经评估表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型处理长上下文存在瓶颈，现有压缩技术有局限性。

Method: 提出基于EDU的上下文压缩器，将上下文压缩重构为结构分析再选择的过程，包括用LingoEDU将文本转换为结构关系树，用轻量级排序模块选择相关子树；发布StructBench数据集进行评估。

Result: 方法达到了最先进的结构预测准确率，显著优于前沿大语言模型，降低了成本，提升了下游任务性能。

Conclusion: 基于EDU的上下文压缩器有效解决大语言模型处理长上下文的问题，提升相关任务表现。

Abstract: Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.

</details>


### [182] [Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets](https://arxiv.org/abs/2512.14237)
*Estelle Zheng,Nathan Cerisara,Sébastien Warichet,Emmanuel Helbert,Christophe Cerisara*

Main category: cs.CL

TL;DR: 研究表明Ladder Side Tuning (LST)在微调大语言模型时比QLoRA更节省内存，性能相当，还提出xLadder增加推理深度。


<details>
  <summary>Details</summary>
Motivation: 解决微调大语言模型时商品GPU内存受限问题，现有PEFT方法仍有高内存使用问题。

Method: 重新审视LST方法，引入xLadder变体。

Result: LST比QLoRA削减50%峰值内存，在多任务上性能相当，可在单12GB消费级GPU微调7B参数模型；xLadder增加推理深度且无额外内存开销。

Conclusion: Ladder在内存受限场景表现出色，xLadder可在不增加内存下实现更深推理。

Abstract: Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.

</details>


### [183] [Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring](https://arxiv.org/abs/2512.14332)
*Yannis Belkhiter,Seshu Tirupathi,Giulio Zizzo,John D. Kelleher*

Main category: cs.CL

TL;DR: 提出Step - Tagging框架和ReasonType分类法，对推理步骤实时标注，在多数据集评估实现20 - 50%的token减少，提供控制和研究LRMs的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前语言推理模型（LRMs）效率低，过度生成验证和反思步骤。

Method: 引入Step - Tagging框架进行实时标注，提出ReasonType推理步骤分类法，基于此实现对特定步骤计数的在线监控以生成早期停止标准。

Result: 在多个数据集上评估，实现20 - 50%的token减少，在计算量大的任务上收益最大，且保持与标准生成相当的准确率。

Conclusion: 该工作提供了控制LRMs生成的新方法和研究其行为的新工具。

Abstract: The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.

</details>


### [184] [Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2512.14427)
*Gabriele Prato,Shagun Sodhani,Alessandro Sordoni,Sarath Chandar*

Main category: cs.CL

TL;DR: 研究不同文档打包策略对大语言模型多跳推理能力的影响，发现打包能提升性能但耗计算资源，并通过消融研究明确关键因素。


<details>
  <summary>Details</summary>
Motivation: 探究文档打包过程对大语言模型能力的影响，填补研究空白。

Method: 研究不同文档打包策略，进行消融研究。

Result: 打包较单文档训练能提升模型性能，但需更多计算资源，通过消融研究识别出解释打包优势的关键因素。

Conclusion: 深化了对大语言模型训练动态的理解，为模型开发优化提供实用见解。

Abstract: The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.

</details>


### [185] [C-ing Clearly: Enhanced Binary Code Explanations using C code](https://arxiv.org/abs/2512.14500)
*Teodor Poncu,Ioana Pintilie,Marius Dragoi,Dragos Tantaru,Florin Brad*

Main category: cs.CL

TL;DR: 提出C-ing Clearly合成数据生成方法，利用C代码增强大语言模型对汇编的理解，提升其在二进制代码总结和漏洞检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高级编程语言编码任务表现出色，但在汇编等低级编程语言方面不足，需提升其对汇编的理解。

Method: 提出C-ing Clearly合成数据生成方法，利用对应C代码增强大语言模型对汇编的理解，并在生成的数据上进行微调。

Result: 提升了大语言模型在二进制代码总结和漏洞检测中的性能，在不同大语言模型家族和模型大小上均有稳定提升。

Conclusion: 所提出的方法有效，能提升大语言模型对汇编的理解和相关任务性能。

Abstract: Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.

</details>


### [186] [TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines](https://arxiv.org/abs/2512.14645)
*David Schulmeister,Valentin Hartmann,Lars Klein,Robert West*

Main category: cs.CL

TL;DR: 研究展示如何为效率关键型应用训练小模型TiME，评估显示其在性能与效率间有更好权衡，还证明特定蒸馏方式可行。


<details>
  <summary>Details</summary>
Motivation: 当前大量语言模型研究聚焦通用大模型，但NLP管道常只需特定小模型能力，大模型处理数据慢、能耗高，因此要为效率关键型应用训练小模型。

Method: 使用蒸馏等现代训练技术训练小模型TiME，支持低资源语言。

Result: 在一系列常见NLP任务上评估，发现TiME在基准性能与吞吐量、延迟和能耗间有更好权衡。

Conclusion: 证明从多语言教师模型蒸馏单语言模型以及从相对位置嵌入教师模型蒸馏绝对位置嵌入模型是可行的。

Abstract: Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.

</details>


### [187] [Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization](https://arxiv.org/abs/2512.14687)
*Yen-Ju Lu,Kunxiao Gao,Mingrui Liang,Helin Wang,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak,Jesus Villalba*

Main category: cs.CL

TL;DR: 引入首个将原始对话音频与不同类型摘要及语音特征标签对齐的语料库Spoken DialogSum，实验表明端到端语音建模有价值。


<details>
  <summary>Details</summary>
Motivation: 现有情感感知或口语对话摘要研究受限于缺少关联语音、摘要和副语言线索的数据。

Method: 分两个阶段构建数据集，先让大语言模型改写脚本并标注，后用TTS引擎合成语音并对齐副语言标签。

Result: Spoken DialogSum包含13460个情感多样的对话，每个对话都配有事实性和情感聚焦摘要，且数据集可在线获取；Audio - LLM使情感摘要的ROUGE - L比级联ASR - LLM系统提高28%。

Conclusion: 端到端语音建模有价值。

Abstract: Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.

</details>


### [188] [SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models](https://arxiv.org/abs/2512.14481)
*Shizhuo Mao,Song Chen,Yi Kang*

Main category: cs.CL

TL;DR: 提出轻量级QAT框架SASQ用于激活量化因子，优化量化因子且不改变预训练权重，在LLaMA2 - 7B上表现优于现有方案和FP16模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因尺寸增长快于GPU内存发展面临部署挑战，现有模型量化方案存在计算开销高、精度损失、训练成本大等问题。

Method: 提出SASQ框架，仅优化量化因子，自适应截断异常值以降低量化难度并保留激活分布特征。

Result: SASQ超越现有SOTA量化方案，在LLaMA2 - 7B上，比QuaRot和FP16模型在WikiText2上的困惑度分别低5.2%和4.7%。

Conclusion: SASQ能实现高精度静态推理，保持部署效率，在大语言模型量化方面有良好效果。

Abstract: Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.

</details>


### [189] [Dual Language Models: Balancing Training Efficiency and Overfitting Resilience](https://arxiv.org/abs/2512.14549)
*David Samuel,Lucas Georges Gabriel Charpentier*

Main category: cs.CL

TL;DR: 本文结合自回归和掩码扩散训练目标，无需架构修改，所得模型优于单目标模型，还通过实验确定了最优目标比例。


<details>
  <summary>Details</summary>
Motivation: 自回归建模训练效率高但易过拟合，掩码扩散模型抗过拟合能力强但训练效率低，希望结合两者优势。

Method: 在不同数据重复水平下训练和评估50个语言模型来推导两个目标的最优比例。

Result: 在所有评估设置下结合两个目标是最优的，且针对自回归或掩码扩散下游性能的最优比例相似。

Conclusion: 双目标训练结合了自回归和掩码扩散模型的优点。

Abstract: This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.

</details>


### [190] [VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models](https://arxiv.org/abs/2512.14554)
*Nguyen Tien Dong,Minh-Anh Nguyen,Thanh Dat Hoang,Nguyen Tuan Ngoc,Dao Xuan Quang Minh,Phan Phi Hai,Nguyen Thi Ngoc Anh,Dang Van Tu,Binh Vu*

Main category: cs.CL

TL;DR: 论文引入越南法律基准VLegal - Bench，用于评估大语言模型在越南法律任务中的表现，为相关AI系统发展提供基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法律领域有应用潜力，但越南立法的复杂性给评估模型对法律知识的理解和运用带来挑战，需衡量模型在越南法律任务上的表现。

Method: 参照Bloom认知分类法，通过严格注释流程生成10,450个样本，涵盖多种法律理解层次及实际应用场景，确保样本权威且反映真实法律助理工作流程。

Result: 推出了标准化、透明且符合认知的评估框架VLegal - Bench。

Conclusion: VLegal - Bench为评估大语言模型在越南法律场景中的表现奠定基础，支持开发更可靠、可解释和符合道德的AI辅助法律系统。

Abstract: The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.

</details>


### [191] [Polypersona: Persona-Grounded LLM for Synthetic Survey Responses](https://arxiv.org/abs/2512.14562)
*Tejaswani Dash,Dinesh Karri,Anudeep Vurity,Gautam Datla,Tazeem Ahmad,Saima Rafi,Rohith Tangudu*

Main category: cs.CL

TL;DR: 介绍PolyPersona框架用于合成跨多领域的人物角色条件调查回复，小模型经微调可生成可靠数据，框架高效可复现。


<details>
  <summary>Details</summary>
Motivation: 构建能合成跨多领域的人物角色条件调查回复的框架，支持可扩展评估和偏差分析。

Method: 用参数高效的LoRA适配器和4位量化对紧凑型聊天模型进行指令微调，对话数据管道保留人物角色线索，构建数据集，用多指标评估套件评估生成回复。

Result: TinyLlama 1.1B和Phi - 2等紧凑型模型表现可与7B - 8B的基线模型媲美，BLEU最高0.090，ROUGE - 1为0.429。

Conclusion: 人物角色条件微调使小语言模型能生成可靠连贯的合成调查数据，框架为调查数据生成提供高效可复现的方法。

Abstract: This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.

</details>


### [192] [Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies](https://arxiv.org/abs/2512.14576)
*Ekaterina Artemova,Laurie Burchell,Daryna Dementieva,Shu Okabe,Mariya Shmatova,Pedro Ortiz Suarez*

Main category: cs.CL

TL;DR: 该教程为处理多语言和低资源语言的NLP从业者等设计，提供构建端到端NLP管道的实用工具包，介绍应对挑战的策略和方法，展示多种语言用例。


<details>
  <summary>Details</summary>
Motivation: 为处理多语言和低资源语言的人员创建更公平、有社会影响力的语言技术。

Method: 介绍应对数据稀缺和文化差异挑战的策略，提供实践方法和建模框架，基于现实场景采用公平、可复现和社区参与的开发方法。

Result: 参与者获得为低资源语言构建端到端NLP管道的实用工具包。

Conclusion: 通过教程可帮助相关人员应对低资源语言NLP开发挑战，推动公平、有影响力的语言技术发展。

Abstract: This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.

</details>


### [193] [Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer](https://arxiv.org/abs/2512.14585)
*Adarsha Shrestha,Basanta Pokharel,Binit Shrestha,Smriti Adhikari,Dinesh Gothe*

Main category: cs.CL

TL;DR: 为解决尼泊尔语NLP中的文本生成问题，提出基于GPT - 2的尼泊尔语模型，采用多种策略训练，实现较好结果。


<details>
  <summary>Details</summary>
Motivation: 尼泊尔语因复杂语法、词法和有限高质量语料，现有基于基本编码器架构的方法不足以进行尼泊尔语特定文本生成。

Method: 基于GPT - 2训练模型，采用受GPT - 3启发的训练策略；训练定制的16k BPE分词器；在组合数据集上预训练；集成FlashAttention。

Result: 经过两个epoch，模型训练损失3.168177，验证损失3.081982，最终困惑度21.80，能生成连贯的尼泊尔语新闻风格文本。

Conclusion: 该模型具备生成连贯尼泊尔语新闻风格文本的能力，可用于尼泊尔语NLP的文本生成。

Abstract: Nepali, a low-resource language spoken by over 32 million people, continues to face challenges in natural language processing (NLP) due to its complex grammar, agglutinative morphology, and limited availability of high-quality corpora. Most efforts to date have centered on basic encoder architectures; they remain insufficient for Nepali-specific text generation. This study presents a GPT-2-based Nepali language model trained using several training strategies inspired by GPT-3, including optimized learning rate schedules, batch scaling, and architectural refinements. A custom 16k Byte-Pair Encoding (BPE) tokenizer was trained exclusively on Nepali text to ensure more consistent segmentation and improved input representation. The model was pretrained on a combined dataset comprising a 10.75GB cleaned NepBERTa corpus and additional web-scraped Nepali news articles. FlashAttention was integrated to reduce memory usage and stabilize training. After two epochs, the model achieved a training loss of 3.168177, a validation loss of 3.081982, and a final perplexity of 21.80, demonstrating its capability to generate coherent Nepali news-style text.

</details>


### [194] [JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction](https://arxiv.org/abs/2512.14620)
*Atsuyuki Miyai,Shota Onohara,Jeonghun Baek,Kiyoharu Aizawa*

Main category: cs.CL

TL;DR: 本文介绍JMMMU - Pro日语多模态理解基准和Vibe基准构建方法，低成本构建高质量基准，实验显示开源LMMs在该基准表现不佳，认为其能为评估和开发提供工具与指导。


<details>
  <summary>Details</summary>
Motivation: 为评估大语言模型的日语能力并为图像VQA基准开发提供方法，从MMMU进化到MMMU - Pro，对JMMMU进行扩展。

Method: 提出Vibe Benchmark Construction方法，用图像生成模型（如Nano Banana Pro）生成候选视觉问题，人工验证并必要时调整提示词重新生成以保证质量。

Result: 实验表明所有开源LMMs在JMMMU - Pro上都面临很大困难。

Conclusion: JMMMU - Pro为评估LMMs日语能力提供更严格工具，Vibe Benchmark Construction为图像VQA基准未来开发提供高效指导。

Abstract: This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [195] [Relevant HAL Interface Requirements for Embedded Systems](https://arxiv.org/abs/2512.14514)
*Manuel Bentele,Andreas Podelski,Axel Sikora,Bernd Westphal*

Main category: cs.LO

TL;DR: 本文引入相关性的形式化概念，提出从系统故障问题报告中提取可证明相关需求的方法，并通过案例研究证明其可行性，为预防特定系统故障的优先级排序研究铺平道路。


<details>
  <summary>Details</summary>
Motivation: 嵌入式应用中不当使用HAL会导致系统故障和硬件损坏，需从大量HAL接口需求中找出与预防此类故障相关的需求。

Method: 引入形式化的相关性概念，利用软件模型检查方法进行数学证明，从系统故障问题报告中提取需求。

Result: 通过使用spidev HAL的SPI总线嵌入式应用问题报告案例，证明该方法原则上可行。

Conclusion: 为预防特定系统故障的优先级排序方法研究奠定基础。

Abstract: Embedded applications often use a Hardware Abstraction Layer (HAL) to access hardware. Improper use of the HAL can lead to incorrect hardware operations, resulting in system failure and potentially serious damage to the hardware. The question is how one can obtain prioritize, among a possibly large set of HAL interface requirements, those that are indisputably relevant for preventing this kind of system failure. In this paper, we introduce a formal notion of relevance. This allows us to leverage a formal method, i.e., software model checking, to produce a mathematical proof that a requirement is indisputably relevant. We propose an approach to extract provably relevant requirements from issue reports on system failures. We present a case study to demonstrate that the approach is feasible in principle. The case study uses three examples of issue reports on embedded applications that use the SPI bus via the spidev HAL. The overall contribution of this paper is to pave the way for the study of approaches to a new kind of prioritization aimed at preventing a specific kind of system failure.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [196] [Physics-Informed Machine Learning for Two-Phase Moving-Interface and Stefan Problems](https://arxiv.org/abs/2512.14010)
*Che-Chia Chang,Te-Sheng Lin,Ming-Chih Lai*

Main category: physics.comp-ph

TL;DR: 本文提出物理信息神经网络框架求解两相Stefan问题，实验表明该方法比其他神经网络方法更准确有效，还能捕捉不稳定界面演化。


<details>
  <summary>Details</summary>
Motivation: Stefan问题作为经典自由边界问题，因移动界面和非线性温度 - 相耦合带来计算挑战，需开发有效求解方法。

Method: 提出物理信息神经网络框架，用两个神经网络分别表示移动界面和温度场，界面网络辅助温度网络选点，温度网络输入结合修正零水平集函数。

Result: 数值实验显示该方法比文献中其他神经网络方法有更高准确性和有效性。

Conclusion: 该框架为求解移动边界相变问题提供了稳健灵活的替代传统数值方法，还能捕捉Mullins - Sekerka不稳定性相关的不稳定界面演化。

Abstract: The Stefan problem is a classical free-boundary problem that models phase-change processes and poses computational challenges due to its moving interface and nonlinear temperature-phase coupling. In this work, we develop a physics-informed neural network framework for solving two-phase Stefan problems. The proposed method explicitly tracks the interface motion and enforces the discontinuity in the temperature gradient across the interface while maintaining global consistency of the temperature field. Our approach employs two neural networks: one representing the moving interface and the other for the temperature field. The interface network allows rapid categorization of thermal diffusivity in the spatial domain, which is a crucial step for selecting training points for the temperature network. The temperature network's input is augmented with a modified zero-level set function to accurately capture the jump in its normal derivative across the interface. Numerical experiments on two-phase dynamical Stefan problems demonstrate the superior accuracy and effectiveness of our proposed method compared with the ones obtained by other neural network methodology in literature. The results indicate that the proposed framework offers a robust and flexible alternative to traditional numerical methods for solving phase-change problems governed by moving boundaries. In addition, the proposed method can capture an unstable interface evolution associated with the Mullins-Sekerka instability.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [197] [Time-aware UNet and super-resolution deep residual networks for spatial downscaling](https://arxiv.org/abs/2512.13753)
*Mika Sipilä,Sabrina Maggio,Sandra De Iaco,Klaus Nordhausen,Monica Palma,Sara Taskinen*

Main category: cs.CV

TL;DR: 本文考虑用SRDRN和UNet对对流层臭氧进行空间降尺度，扩展了轻量级时间模块，在意大利臭氧降尺度案例中评估，时间模块轻微增加计算复杂度但显著提升性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 卫星大气污染物数据空间分辨率低限制其在局部环境分析和决策中的应用，需进行空间降尺度。

Method: 使用SRDRN和UNet进行对流层臭氧空间降尺度，并扩展轻量级时间模块，用正弦或径向基函数编码观测时间，融合时间和空间特征。

Result: 时间模块仅轻微增加计算复杂度，但显著提高降尺度性能和收敛速度。

Conclusion: 扩展的轻量级时间模块可有效用于对流层臭氧空间降尺度。

Abstract: Satellite data of atmospheric pollutants are often available only at coarse spatial resolution, limiting their applicability in local-scale environmental analysis and decision-making. Spatial downscaling methods aim to transform the coarse satellite data into high-resolution fields. In this work, two widely used deep learning architectures, the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet, are considered for spatial downscaling of tropospheric ozone. Both methods are extended with a lightweight temporal module, which encodes observation time using either sinusoidal or radial basis function (RBF) encoding, and fuses the temporal features with the spatial representations in the networks. The proposed time-aware extensions are evaluated against their baseline counterparts in a case study on ozone downscaling over Italy. The results suggest that, while only slightly increasing computational complexity, the temporal modules significantly improve downscaling performance and convergence speed.

</details>


### [198] [Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries](https://arxiv.org/abs/2512.14102)
*Emanuele Mezzi,Gertjan Burghouts,Maarten Kruithof*

Main category: cs.CV

TL;DR: 提出RUNE方法结合大语言模型和神经符号AI用于遥感文本到图像检索，在复杂任务中表现优，有性能、鲁棒性和可解释性优势。


<details>
  <summary>Details</summary>
Motivation: 现有遥感文本到图像检索方法存在可解释性有限和处理复杂空间关系能力差的问题，难以用于实际。

Method: 引入RUNE方法，结合大语言模型和神经符号AI，通过推理检测实体与一阶逻辑表达式的兼容性检索图像；提出逻辑分解策略；利用基础模型生成一阶逻辑表达式，将推理交给神经符号推理模块；改造DOTA数据集用于评估。

Result: 展示大语言模型在文本到逻辑转换中的有效性；RUNE在复杂遥感检索任务中优于联合嵌入模型；引入RRQC和RRIU两个指标评估性能。

Conclusion: RUNE在复杂遥感检索任务中表现出色，在性能、鲁棒性和可解释性上有提升，有实际应用潜力。

Abstract: Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.

</details>


### [199] [Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline](https://arxiv.org/abs/2512.13731)
*Weikang Bai,Yongkun Du,Yuchen Su,Yazhen Xie,Zhineng Chen*

Main category: cs.CV

TL;DR: 针对复杂数学表达式识别难题，构建CMER - Bench评估现有模型，提出新数据集MER - 17M和CMER - 3M，引入新分词器和表示方法，提出CMERNet模型，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有数学表达式识别方法在复杂表达式识别上表现不佳，公共训练数据集多为简单样本。

Method: 构建CMER - Bench评估模型；提出大规模数据集MER - 17M和CMER - 3M；引入新表达式分词器和结构化数学语言表示方法；基于编解码器架构提出CMERNet模型并在CMER - 3M上训练。

Result: 现有模型在复杂表达式上性能下降；CMERNet仅1.25亿参数，在CMER - Bench上显著优于现有模型。

Conclusion: 提出的数据集、方法和模型有助于提升复杂数学表达式的识别性能。

Abstract: Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.

</details>


### [200] [Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage](https://arxiv.org/abs/2512.13739)
*Yajie Yang,Yuqing Zhao,Xiaochao Xi,Yinan Zhu*

Main category: cs.CV

TL;DR: 本文聚焦AIGC辅助图像生产在新闻业引发的争议，通过两个实验探究新闻专题报道中可控图像生产的路径，并提出人机协作机制与评估建议。


<details>
  <summary>Details</summary>
Motivation: AIGC辅助图像生产引发新闻业争议，存在诸多关键问题，且多数AIGC工具是“黑箱”，带来伦理等多方面困境，因此探索可控图像生产路径。

Method: 开展两个实验，一是通过标准化提示测试跨平台适应性；二是构建人机结合的模块化管道，并进行多种过滤和语义评分。

Result: 发现跨平台语义对齐、文化特异性和视觉真实性存在差异；构建的管道能确保编辑保真度，可追溯部署保留语义表征。

Conclusion: 提出AIGC辅助专题报道图像生产的人机协作机制，并建议评估CIS、CEA和U - PA。

Abstract: Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque "black boxes," hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).

</details>


### [201] [DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models](https://arxiv.org/abs/2512.13742)
*Md. Najib Hasan,Imran Ahmad,Sourav Basak Shuvo,Md. Mahadi Hasan Ankon,Sunanda Das,Nazmul Siddique,Hui Wang*

Main category: cs.CV

TL;DR: 本文提出连接图像分类与结构化临床推理的框架，用新模型MobileCoAtNet进行图像分类并驱动多个大语言模型推理，评估32个大语言模型，发现虽结合可产生有用临床叙述，但现有大语言模型用于高风险医疗决策不可靠。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分类器无法解释决策，大语言模型视觉推理能力差、解释不稳定或错误，存在模型所见与临床医生期望推理间的差距。

Method: 引入连接图像分类与结构化临床推理的框架，设计新模型MobileCoAtNet进行内镜图像分类，用其输出驱动多个大语言模型推理，构建两个专家验证基准评估32个大语言模型。

Result: MobileCoAtNet在八个胃部相关类别上准确率高，强分类可提高解释质量，但无模型达到人类水平稳定性，最佳大语言模型也会因提示变化改变推理。

Conclusion: 结合深度学习与大语言模型可产生有用临床叙述，但现有大语言模型用于高风险医疗决策不可靠，框架明确其局限性并指明构建更安全推理系统的路径。

Abstract: Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.

</details>


### [202] [Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making](https://arxiv.org/abs/2512.13747)
*Siyuan Dai,Lunxiao Li,Kun Zhao,Eardi Lila,Paul K. Crane,Heng Huang,Dongkuan Xu,Haoteng Tang,Liang Zhan*

Main category: cs.CV

TL;DR: 研究表明现有多模态大语言模型在生物医学决策任务表现不佳，文本推理表现更好，并探索三种改进策略。


<details>
  <summary>Details</summary>
Motivation: 现有先进多模态大语言模型在生物医学决策任务中表现不佳，需研究此局限。

Method: 用两个具有挑战性的数据集研究局限，探索三种策略改进：带推理注释示例的上下文学习、视觉字幕转文本推理、少量样本微调视觉塔。

Result: 文本推理始终优于仅视觉或视觉 - 文本设置，多模态输入常比仅文本表现差。

Conclusion: 当前多模态大语言模型缺乏扎实视觉理解，为医疗多模态决策提供改进方向。

Abstract: With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.

</details>


### [203] [STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning](https://arxiv.org/abs/2512.13752)
*Jie Qin,Jiancheng Huang,Limeng Qiao,Lin Ma*

Main category: cs.CV

TL;DR: 为解决多模态大语言模型统一学习挑战，提出STAR方案，经实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在多模态理解和生成统一方面存在优化冲突和性能权衡问题，需要有效提升生成性能并保留理解能力。

Method: 提出STAR方案，将多模态学习分解为理解、生成和编辑阶段，冻结基础自回归模型参数并逐步堆叠同构自回归模块，引入高容量VQ和隐式推理机制。

Result: STAR在GenEval、DPG - Bench和ImgEdit上达到了最先进的性能，分别为0.91、87.44和4.34。

Conclusion: STAR方案对统一多模态学习有效。

Abstract: Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.

</details>


### [204] [VajraV1 -- The most accurate Real Time Object Detector of the YOLO family](https://arxiv.org/abs/2512.13834)
*Naman Balbir Singh Makkar*

Main category: cs.CV

TL;DR: 介绍VajraV1模型架构，其结合YOLO模型设计，在实时目标检测中精度领先且推理速度有竞争力。


<details>
  <summary>Details</summary>
Motivation: 在现有YOLO系列检测器基础上进行架构改进，提升实时目标检测的精度。

Method: 结合先前YOLO模型的有效设计选择，构建VajraV1模型架构。

Result: 在COCO验证集上，VajraV1不同版本均超越对应YOLO版本的mAP，VajraV1 - Xlarge超越所有现有实时目标检测器。

Conclusion: VajraV1模型架构在实时目标检测中能实现高精度和有竞争力的推理速度。

Abstract: Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.

</details>


### [205] [Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging](https://arxiv.org/abs/2512.13855)
*Ujjwal Mishra,Vinita Shukla,Praful Hambarde,Amit Shukla*

Main category: cs.CV

TL;DR: 提出Telescopic Adapters解决医学视觉语言分割模型微调计算开销大问题，参数少且效果好。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法在医学影像领域计算开销大，现有PEFT方法参数分配不佳、适配效率低。

Method: 引入Telescopic Adapters框架，在CLIPSeg的视觉和文本编码器中集成轻量级瓶颈模块，根据层深度和语义相关性动态缩放适配器维度。

Result: 仅用613k可训练参数（比端到端微调少244倍），在五个医学数据集上表现优异。

Conclusion: 该方法为医学VLSM高效微调建立新范式，可在资源受限临床环境部署并保持分割精度。

Abstract: Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.

</details>


### [206] [Automated Pollen Recognition in Optical and Holographic Microscopy Images](https://arxiv.org/abs/2512.08589)
*Swarn Singh Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 研究用深度学习改进花粉粒检测与分类，评估模型表现，解决全息图像性能问题，证明深度学习可与低成本设备结合。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习在光学与全息显微镜图像中改进和自动化花粉粒检测与分类，聚焦兽医细胞学应用。

Method: 使用YOLOv8s进行目标检测，MobileNetV3L进行分类任务，通过自动标注和扩大边界框区域扩展数据集解决性能差距问题。

Result: 光学图像检测mAP50达91.3%，分类准确率97%；全息图像经处理后，检测mAP50从2.49%提升到13.3%，分类准确率从42%提升到54%。

Conclusion: 至少在图像分类任务中，深度学习技术可与低成本无透镜数字全息显微镜设备结合。

Abstract: This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.

</details>


### [207] [KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding](https://arxiv.org/abs/2512.14017)
*Zongyao Li,Kengo Ishida,Satoshi Yamazaki,Xiaotong Ji,Jianquan Liu*

Main category: cs.CV

TL;DR: 提出首个长视频问答关键帧采样基准KFS - Bench，研究关键帧采样方法，设计新的采样质量指标和采样方法，性能优越。


<details>
  <summary>Details</summary>
Motivation: 关键帧采样对长视频理解至关重要，现有工作只能通过问答准确率间接评估帧选择质量，需要直接评估采样策略的方法。

Method: 提出KFS - Bench，提供多场景标注；进行关键帧采样方法的综合研究；设计新的采样质量指标；开发利用问题 - 视频相关性平衡采样多样性和问题 - 帧相似性的关键帧采样方法。

Result: 识别出采样精度、场景覆盖和采样平衡是影响问答性能的关键因素；新设计的采样质量指标与问答准确率相关；自适应平衡采样方法在关键帧采样和问答性能上表现优越。

Conclusion: KFS - Bench能直接评估关键帧采样策略，新的采样方法可提高关键帧采样和问答性能。

Abstract: We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.

</details>


### [208] [ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM](https://arxiv.org/abs/2512.14032)
*Ignacio Alzugaray,Marwan Taher,Andrew J. Davison*

Main category: cs.CV

TL;DR: 提出实时学习场景隐式地图的新颖神经RGB - D SLAM系统，依赖SCR实现实时性，评估性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 探索将SCR作为神经SLAM管道中的核心隐式地图表示，实现神经隐式RGB - D SLAM的严格实时性。

Method: 引入为该目的定制的SCR架构，将SCR集成到实时SLAM管道，框架支持稀疏和密集特征。

Result: 系统能在动态环境可靠运行，在合成和真实基准测试中性能有竞争力。

Conclusion: 基于SCR的神经RGB - D SLAM系统有良好性能和实用性。

Abstract: We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.
  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam

</details>


### [209] [OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving](https://arxiv.org/abs/2512.14044)
*Zhenguo Zhang,Haohan Zhen,Yishen Wang,Le Xu,Tianchen Deng,Xuefeng Chen,Qu Chen,Bo Zhang,Wuxiong Huang*

Main category: cs.CV

TL;DR: 现有视觉语言模型用于自动驾驶时因可靠性故障受限，本文提出OmniDrive - R1框架，用iMCoT机制统一感知与推理，通过强化学习和Clip - GRPO算法实现视觉定位，实验显示其性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自动驾驶等安全关键领域因可靠性故障（如物体幻觉）受限，且现有多模态思维链方法存在感知与推理阶段分离、依赖昂贵密集定位标签的问题。

Method: 引入OmniDrive - R1框架，通过交错多模态思维链（iMCoT）机制统一感知和推理；采用纯两阶段强化学习训练管道和Clip - GRPO算法实现强化驱动的视觉定位能力。

Result: 在DriveLMM - o1上实验，与基线Qwen2.5VL - 7B相比，OmniDrive - R1整体推理得分从51.77%提升到80.35%，最终答案准确率从37.81%提升到73.62%。

Conclusion: OmniDrive - R1框架能有效解决现有视觉语言模型在自动驾驶应用中的问题，提升性能。

Abstract: The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and "zoom in" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.

</details>


### [210] [FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling](https://arxiv.org/abs/2512.14056)
*Kim Sung-Bin,Joohyun Chang,David Harwath,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 提出将人脸编辑和生成视为语音条件面部运动填充子任务，提出FacEDiT模型和FacEDiTBench数据集，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 将人脸编辑和生成统一研究，解决缺乏标准编辑基准问题。

Method: 提出FacEDiT，一种用流匹配训练的语音条件扩散变压器，受掩码自编码器启发学习合成面部运动；引入FacEDiTBench数据集和新评估指标。

Result: FacEDiT能生成准确、与语音对齐的面部编辑，有效泛化到人脸生成。

Conclusion: 人脸编辑和生成可作为语音条件运动填充的子任务。

Abstract: Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.

</details>


### [211] [Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning](https://arxiv.org/abs/2512.14058)
*Zulin Zhuang,Yu Bian*

Main category: cs.CV

TL;DR: 提出多模态深度学习框架实时预测室内工作平面照度分布，试验显示模型准确率高且有可接受的时间泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有室内采光预测研究多针对静态场景，而日光关联控制在有充足日光且可实时准确预测室内工作平面照度时，有显著节能潜力。

Method: 提出从具有时空特征的非侵入式图像实时预测室内工作平面照度分布的多模态深度学习框架，仅从侧光窗户区域提取图像特征；在广州一个测试房间进行实地实验，收集17344个样本进行模型训练和验证。

Result: 模型在同分布测试集上R2 > 0.98且RMSE < 0.14，在未见日期测试集上R2 > 0.82且RMSE < 0.17。

Conclusion: 模型具有高准确性和可接受的时间泛化性。

Abstract: Daylight-linked controls (DLCs) have significant potential for energy savings in buildings, especially when abundant daylight is available and indoor workplane illuminance can be accurately predicted in real time. Most existing studies on indoor daylight predictions were developed and tested for static scenes. This study proposes a multimodal deep learning framework that predicts indoor workplane illuminance distributions in real time from non-intrusive images with temporal-spatial features. By extracting image features only from the side-lit window areas rather than interior pixels, the approach remains applicable in dynamically occupied indoor spaces. A field experiment was conducted in a test room in Guangzhou (China), where 17,344 samples were collected for model training and validation. The model achieved R2 > 0.98 with RMSE < 0.14 on the same-distribution test set and R2 > 0.82 with RMSE < 0.17 on an unseen-day test set, indicating high accuracy and acceptable temporal generalization.

</details>


### [212] [SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding](https://arxiv.org/abs/2512.14068)
*Shuang Cheng,Yuhua Jiang,Zineng Zhou,Dawei Liu,Wang Tao,Linfeng Zhang,Biqing Qi,Bowen Zhou*

Main category: cs.CV

TL;DR: 提出SDAR - VL用于大规模视觉语言理解，有高效稳定训练框架，实验显示其优于传统块扩散，达扩散基视觉语言模型新水平。


<details>
  <summary>Details</summary>
Motivation: 块离散扩散虽有潜力，但因高训练成本、收敛慢和不稳定，落后于自回归基线，需改进用于大规模视觉语言理解。

Method: 提出SDAR - VL及集成框架，含异步块噪声调度、有效掩码比率缩放和渐进贝塔噪声课程三个组件。

Result: 在21个基准测试中，SDAR - VL提高训练效率、收敛稳定性和任务性能，超越传统块扩散，达新水平，匹配或超越强自回归基线。

Conclusion: 块离散扩散可作为视觉语言理解的实用骨干。

Abstract: Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \emph{training efficiency}, \emph{convergence stability}, and \emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.

</details>


### [213] [ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes](https://arxiv.org/abs/2512.14092)
*Felix Holm,Ghazal Ghazaei,Nassir Navab*

Main category: cs.CV

TL;DR: 为解决AI辅助手术中注解成本高、数据稀缺和缺乏可解释模型的问题，提出ProtoFlow框架，在手术数据集上表现良好，向开发更好的AI系统迈进重要一步。


<details>
  <summary>Details</summary>
Motivation: 详细的手术识别对推进AI辅助手术很关键，但面临高注解成本、数据稀缺和缺乏可解释模型的问题，场景图潜力未充分挖掘。

Method: ProtoFlow利用图神经网络（GNN）编解码器架构，结合自监督预训练和基于原型的微调阶段，发现并优化核心原型。

Result: 在细粒度的CAT - SG数据集上评估，ProtoFlow不仅整体准确率优于标准GNN基线，在少样本场景中表现出卓越鲁棒性，定性分析还显示能识别不同手术子技术并提供可解释见解。

Conclusion: ProtoFlow将强大的表征学习与可解释性结合，向开发更透明、可靠、数据高效的AI系统迈出重要一步，加速其在手术领域的临床应用。

Abstract: Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.
  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.
  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.
  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.

</details>


### [214] [SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance](https://arxiv.org/abs/2512.14121)
*Wenbo Tian,Ruting Lin,Hongxian Zheng,Yaodong Yang,Geng Wu,Zihao Zhang,Zhang Zhang*

Main category: cs.CV

TL;DR: 本文提出 SportsGPT 框架用于可解释的运动评估和训练指导，实验表明其性能优于传统方法和通用大模型。


<details>
  <summary>Details</summary>
Motivation: 现有智能体育分析系统缺乏自动性能诊断和可解释训练指导，大语言模型和运动分析技术发展带来新机遇。

Method: 提出 SportsGPT 框架，包含 MotionDTW 算法提取关键帧、KISMAM 模型获取评估指标、基于 Qwen3 的 SportsRAG 模型生成训练指导。

Result: MotionDTW 优于传统方法，消融实验验证 KISMAM 和 SportsRAG，SportsGPT 在诊断准确性和专业性上超通用大模型。

Conclusion: SportsGPT 框架能够有效实现可解释的体育动作评估和专业训练指导。

Abstract: Existing intelligent sports analysis systems mainly focus on "scoring and visualization," often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.

</details>


### [215] [KLO-Net: A Dynamic K-NN Attention U-Net with CSP Encoder for Efficient Prostate Gland Segmentation from MRI](https://arxiv.org/abs/2512.13902)
*Anning Tian,Byunghyun Ko,Kaichen Qu,Mengyuan Liu,Jeongkyu Lee*

Main category: cs.CV

TL;DR: 提出KLO - Net用于前列腺核磁共振成像分割，具高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 临床工作站实时部署前列腺MRI分割受计算负荷和内存占用瓶颈，深度学习方法因解剖变异性有挑战，需提高效率同时保证准确性。

Method: 提出KLO - Net，采用动态K近邻注意力机制和CSP编码器，动态K近邻机制自适应确定注意力连接数，CSP块降低计算负载和内存消耗。

Result: 在PROMISE12和PROSTATEx两个公开数据集上进行综合实验和消融研究，详细对比分析表明模型在计算效率和分割质量上有优势。

Conclusion: KLO - Net能有效解决前列腺MRI分割效率和准确性问题。

Abstract: Real-time deployment of prostate MRI segmentation on clinical workstations is often bottlenecked by computational load and memory footprint. Deep learning-based prostate gland segmentation approaches remain challenging due to anatomical variability. To bridge this efficiency gap while still maintaining reliable segmentation accuracy, we propose KLO-Net, a dynamic K-Nearest Neighbor attention U-Net with Cross Stage Partial, i.e., CSP, encoder for efficient prostate gland segmentation from MRI scan. Unlike the regular K-NN attention mechanism, the proposed dynamic K-NN attention mechanism allows the model to adaptively determine the number of attention connections for each spatial location within a slice. In addition, CSP blocks address the computational load to reduce memory consumption. To evaluate the model's performance, comprehensive experiments and ablation studies are conducted on two public datasets, i.e., PROMISE12 and PROSTATEx, to validate the proposed architecture. The detailed comparative analysis demonstrates the model's advantage in computational efficiency and segmentation quality.

</details>


### [216] [TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models](https://arxiv.org/abs/2512.14141)
*Hanning Chen,Keyu Man,Kevin Zhu,Chenguang Zhu,Haonan Li,Tongbo Luo,Xizhou Feng,Wei Sun,Sreen Tallam,Mohsen Imani,Partha Kanuparthy*

Main category: cs.CV

TL;DR: 本文提出首个评估和提升机器学习模型检测轨迹中反模式能力的基准数据集，还提出新迭代方法，实验表明该方法优于无监督聚类和基于规则的统计技术。


<details>
  <summary>Details</summary>
Motivation: 识别和解决机器学习模型性能反模式对高效训练和推理至关重要，但当前方法需专业知识且资源密集，普通计算机视觉研究者难以使用，且定位问题轨迹段耗时且难自动化。

Method: 提出首个基准数据集，包含超600个来自不同计算机视觉模型的PyTorch轨迹；提出新迭代方法，先用轻量级ML模型检测有反模式的轨迹段，再用大语言模型进行细粒度分类和针对性反馈。

Result: 实验结果显示，该方法显著优于无监督聚类和基于规则的统计技术来检测反模式区域，还能有效弥补大语言模型上下文长度有限和推理效率低的问题。

Conclusion: 所提方法在检测机器学习模型轨迹反模式上表现良好，有一定优势。

Abstract: Identifying and addressing performance anti-patterns in machine learning (ML) models is critical for efficient training and inference, but it typically demands deep expertise spanning system infrastructure, ML models and kernel development. While large tech companies rely on dedicated ML infrastructure engineers to analyze torch traces and benchmarks, such resource-intensive workflows are largely inaccessible to computer vision researchers in general. Among the challenges, pinpointing problematic trace segments within lengthy execution traces remains the most time-consuming task, and is difficult to automate with current ML models, including LLMs. In this work, we present the first benchmark dataset specifically designed to evaluate and improve ML models' ability to detect anti patterns in traces. Our dataset contains over 600 PyTorch traces from diverse computer vision models classification, detection, segmentation, and generation collected across multiple hardware platforms. We also propose a novel iterative approach: a lightweight ML model first detects trace segments with anti patterns, followed by a large language model (LLM) for fine grained classification and targeted feedback. Experimental results demonstrate that our method significantly outperforms unsupervised clustering and rule based statistical techniques for detecting anti pattern regions. Our method also effectively compensates LLM's limited context length and reasoning inefficiencies.

</details>


### [217] [ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning](https://arxiv.org/abs/2512.14040)
*Boran Wang,Xinming Wang,Yi Chen,Xiang Li,Jian Xu,Jing Yuan,Chenglin Liu*

Main category: cs.CV

TL;DR: 介绍ChartAgent框架解决MLLMs在图表理解中依赖文本注释的问题，实验显示其在稀疏注释下提升了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在图表理解中依赖文本注释，关键数字缺失时性能下降，需解决此局限。

Method: 引入基于工具集成推理（TIR）的ChartAgent框架，将复杂图表分析分解为可观察、可重放步骤，利用包含多种工具的工具库进行动态编排，并将中间输出整合为结构化证据包。

Result: ChartAgent在稀疏注释设置下显著提高了鲁棒性。

Conclusion: ChartAgent为构建可信赖、可扩展的图表理解系统提供了实用途径。

Abstract: With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.

</details>


### [218] [TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning](https://arxiv.org/abs/2512.14274)
*Yu Chen,Hongwei Lin*

Main category: cs.CV

TL;DR: 研究一维持久图的自动显著性检测，提出TUN网络，实验表明其性能优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 持久图中识别真正信号点具有挑战，阻碍拓扑数据分析在多应用中的实际使用，自动化可靠解释对下游决策很重要。

Method: 提出包含增强PD描述符、自注意力机制、PointNet风格点云编码器、学习融合和逐点分类，以及稳定预处理和不平衡感知训练的多模态网络Topology Understanding Net (TUN)。

Result: TUN在检测持久图中显著点方面优于经典方法。

Conclusion: TUN为识别持久图中显著点提供了自动化有效方案，在实际应用中很有效。

Abstract: Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.

</details>


### [219] [From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region](https://arxiv.org/abs/2512.14312)
*Akila Premarathna,Kanishka Hewageegana,Garcia Andarcia Mariangel*

Main category: cs.CV

TL;DR: 研究提出VLM对比方法识别中东和北非地区污水处理厂，零样本评估中部分VLM表现超YOLOv8，证实VLM可高效无注释分类。


<details>
  <summary>Details</summary>
Motivation: 中东和北非地区对污水处理厂需求高，传统方法标注工作量大，需高效替代方法用于环境监测。

Method: 提出VLM对比结构化方法，分零样本和少样本流；用政府数据集训练YOLOv8；评估多个VLM，通过专家提示识别污水处理厂组件。

Result: 零样本评估中多个VLM真阳性率超YOLOv8，Gemma - 3最高。

Conclusion: VLM尤其是零样本VLM可替代YOLOv8，实现高效无注释污水处理厂分类和可扩展遥感。

Abstract: In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8's true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.

</details>


### [220] [Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity](https://arxiv.org/abs/2512.14320)
*Shuai Dong,Jie Zhang,Guoying Zhao,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 本文针对扩散模型图像编辑滥用问题，提出SIFM方法和ISR指标，实验表明SIFM在保护视觉内容上达最优。


<details>
  <summary>Details</summary>
Motivation: 现有图像免疫评估指标忽视语义对齐核心需求，需新方法和指标来评估和实现图像免疫。

Method: 提出SIFM方法，通过双协同目标扰动中间扩散特征；引入ISR指标，用MLLMs评估免疫诱导语义失败或感知退化比例。

Result: SIFM在保护视觉内容免受恶意扩散操作上达到了最优性能。

Conclusion: SIFM方法和ISR指标在图像免疫方面有效，能更好应对扩散模型图像编辑滥用问题。

Abstract: Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.

</details>


### [221] [Dual Attention Guided Defense Against Malicious Edits](https://arxiv.org/abs/2512.14333)
*Jie Zhang,Shuai Dong,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 文本到图像扩散模型带来伦理挑战，提出DANP免疫方法应对恶意篡改，实验显示其达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型存在被恶意利用的风险，当前防御方法效果有限。

Method: 提出DANP免疫方法，在多个时间步上操作，操纵交叉注意力图和噪声预测过程，用动态阈值生成掩码，干扰生成过程。

Result: DANP对恶意编辑表现出强大免疫力，实验证明达到了当前最优性能。

Conclusion: DANP方法能有效应对文本到图像扩散模型的恶意篡改问题，具有良好性能。

Abstract: Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.

</details>


### [222] [Towards Transferable Defense Against Malicious Image Edits](https://arxiv.org/abs/2512.14341)
*Jie Zhang,Shuai Dong,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出TDAE框架解决现有方法跨模型评估可迁移性有限问题，实验表明其在模型内外评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有针对扩散式图像编辑系统恶意操纵的防御方法在跨模型评估中可迁移性有限。

Method: 提出TDAE框架，视觉层面用FDM机制将梯度正则化引入对抗目标；文本层面用DPD范式定期优化文本嵌入。

Result: TDAE在模型内外评估中实现了最先进的减轻恶意编辑的性能。

Conclusion: TDAE框架有效提升了图像对抗恶意编辑的能力，且具有跨模型可迁移性。

Abstract: Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.

</details>


### [223] [Enhancing Interpretability for Vision Models via Shapley Value Optimization](https://arxiv.org/abs/2512.14354)
*Kanglong Fan,Yunqiao Yang,Chen Ma*

Main category: cs.CV

TL;DR: 提出新颖自解释框架，用Shapley值估计作辅助任务，实现公平分配和增强可解释性，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有DNN解释方法有局限性，后验解释难反映模型行为，自解释网络牺牲性能和兼容性。

Method: 提出在训练中集成Shapley值估计作为辅助任务的自解释框架。

Result: 在多个基准测试上实现了最先进的可解释性。

Conclusion: 所提方法在保证模型性能和兼容性的同时，有效提升了DNN的可解释性。

Abstract: Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.

</details>


### [224] [DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning](https://arxiv.org/abs/2512.14420)
*Nakamasa Inoue,Kanoko Goto,Masanari Oi,Martyna Gruszka,Mahiro Ukai,Takumi Hirose,Yusuke Sekikawa*

Main category: cs.CV

TL;DR: LVLM 在多模态任务表现出色，但图像字幕评估有挑战，本文引入 DISCODE 方法及 MCEval 基准，实验显示 DISCODE 效果佳。


<details>
  <summary>Details</summary>
Motivation: 解决使用大视觉语言模型进行图像字幕评估时，尤其是领域转移场景下的鲁棒性问题。

Method: 提出无需微调的 Distribution-Aware Score Decoder (DISCODE) 方法，采用测试时自适应评估，引入 ATT 损失并推导解析解；引入 MCEval 基准。

Result: DISCODE 在 MCEval 和四个现有基准上作为无参考评估指标达到了最先进的性能。

Conclusion: DISCODE 能在不同领域生成与人类判断更一致的鲁棒评估分数，可有效解决图像字幕评估的鲁棒性问题。

Abstract: Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.

</details>


### [225] [TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels](https://arxiv.org/abs/2512.14477)
*Andreas Sjölander,Valeria Belloni,Robel Fekadu,Andrea Nascetti*

Main category: cs.CV

TL;DR: 本文介绍新公开隧道缺陷标注图像数据集，助力自动化隧道检测，推动基础设施维护。


<details>
  <summary>Details</summary>
Motivation: 传统隧道人工检查耗时长、主观且成本高，现有自动化方法受隧道数据集稀缺限制。

Method: 引入含三种不同隧道衬砌标注图像的公开数据集，支持多种深度学习方法。

Result: 得到可用于缺陷检测和分割的数据集，其多样性可用于研究模型泛化和迁移性。

Conclusion: 该数据集解决特定领域数据不足问题，有助于推进自动化隧道检测和更高效的基础设施维护策略。

Abstract: Tunnels are essential elements of transportation infrastructure, but are increasingly affected by ageing and deterioration mechanisms such as cracking. Regular inspections are required to ensure their safety, yet traditional manual procedures are time-consuming, subjective, and costly. Recent advances in mobile mapping systems and Deep Learning (DL) enable automated visual inspections. However, their effectiveness is limited by the scarcity of tunnel datasets. This paper introduces a new publicly available dataset containing annotated images of three different tunnel linings, capturing typical defects: cracks, leaching, and water infiltration. The dataset is designed to support supervised, semi-supervised, and unsupervised DL methods for defect detection and segmentation. Its diversity in texture and construction techniques also enables investigation of model generalization and transferability across tunnel types. By addressing the critical lack of domain-specific data, this dataset contributes to advancing automated tunnel inspection and promoting safer, more efficient infrastructure maintenance strategies.

</details>


### [226] [Spherical Leech Quantization for Visual Tokenization and Generation](https://arxiv.org/abs/2512.14697)
*Yue Zhao,Hanwen Jiang,Zhenlin Xu,Chutong Yang,Ehsan Adeli,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: 本文从格编码视角统一不同非参数量化方法，探索多种格，发现基于Leech格的量化方法在图像任务表现更佳。


<details>
  <summary>Details</summary>
Motivation: 非参数量化因参数效率和可扩展性受关注，旨在从格编码视角统一不同非参数量化方法。

Method: 从格编码视角统一不同非参数量化方法，探索随机格、广义斐波那契格和最密球填充格等，重点研究基于Leech格的量化方法。

Result: 基于Leech格的量化方法简化训练过程，改善重建 - 压缩权衡，在图像任务中比BSQ有更好重建质量且消耗更少比特，在图像生成框架也有改进。

Conclusion: 基于Leech格的量化方法在图像任务和图像生成框架中表现优异，是一种有潜力的量化方法。

Abstract: Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($Λ_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.

</details>


### [227] [CAPRMIL: Context-Aware Patch Representations for Multiple Instance Learning](https://arxiv.org/abs/2512.14540)
*Andreas Lolos,Theofilos Christodoulou,Aris L. Moustakas,Stergios Christodoulidis,Maria Vakalopoulou*

Main category: cs.CV

TL;DR: 本文受神经偏微分方程求解器启发，提出 CAPRMIL 框架，用于全切片图像分析，可降低参数量和计算量，性能达 SOTA。


<details>
  <summary>Details</summary>
Motivation: 在计算病理学中，由于 WSIs 的高分辨率和像素级注释稀缺，弱监督的 MIL 是主要训练框架，但现有基于复杂注意力聚合的方法存在问题，需更有效方法。

Method: 提出高效、与聚合器无关的 CAPRMIL 框架，通过将提取的补丁特征投影到全局上下文感知标记并利用多头自注意力注入全局上下文，搭配简单均值聚合器。

Result: 在多个公开病理学基准上达到 SOTA 水平，减少 48%-92.8% 可训练参数，推理时降低 52%-99% 的 FLOPs，在 GPU 内存效率和训练时间上表现优异。

Conclusion: 聚合前学习丰富的上下文感知实例表示是全切片分析中复杂池化的有效且可扩展替代方案。

Abstract: In computational pathology, weak supervision has become the standard for deep learning due to the gigapixel scale of WSIs and the scarcity of pixel-level annotations, with Multiple Instance Learning (MIL) established as the principal framework for slide-level model training. In this paper, we introduce a novel setting for MIL methods, inspired by proceedings in Neural Partial Differential Equation (PDE) Solvers. Instead of relying on complex attention-based aggregation, we propose an efficient, aggregator-agnostic framework that removes the complexity of correlation learning from the MIL aggregator. CAPRMIL produces rich context-aware patch embeddings that promote effective correlation learning on downstream tasks. By projecting patch features -- extracted using a frozen patch encoder -- into a small set of global context/morphology-aware tokens and utilizing multi-head self-attention, CAPRMIL injects global context with linear computational complexity with respect to the bag size. Paired with a simple Mean MIL aggregator, CAPRMIL matches state-of-the-art slide-level performance across multiple public pathology benchmarks, while reducing the total number of trainable parameters by 48%-92.8% versus SOTA MILs, lowering FLOPs during inference by 52%-99%, and ranking among the best models on GPU memory efficiency and training time. Our results indicate that learning rich, context-aware instance representations before aggregation is an effective and scalable alternative to complex pooling for whole-slide analysis. Our code is available at https://github.com/mandlos/CAPRMIL

</details>


### [228] [CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer](https://arxiv.org/abs/2512.14560)
*Xianwei Cao,Dou Quan,Shuang Wang,Ning Huyan,Wei Wang,Yunan Li,Licheng Jiao*

Main category: cs.CV

TL;DR: 提出对应感知特征细化框架CLNet用于图像检索的跨视角地理定位，在四个公开基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以对精确地理定位至关重要的显式空间对应关系进行建模，需解决不同视图间语义和几何差距问题。

Method: 提出CLNet框架，将视图对齐过程分解为三个可学习且互补的模块：NCM、NEC和GFR。

Result: 在CVUSA、CVACT、VIGOR和University - 1652四个公开基准测试中，CLNet达到了最先进的性能。

Conclusion: CLNet能联合捕捉高级语义和细粒度对齐，具有更好的可解释性和泛化性。

Abstract: Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.

</details>


### [229] [FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos](https://arxiv.org/abs/2512.14601)
*Zhaolun Li,Jichang Li,Yinqi Cai,Junye Chen,Xiaonan Luo,Guanbin Li,Rushi Lan*

Main category: cs.CV

TL;DR: 提出FakeRadar框架解决深度伪造视频检测跨域泛化问题，实验显示其在多数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造视频检测方法依赖特定线索，对新兴伪造技术泛化能力差，无法适应未知伪造模式。

Method: 利用大规模预训练模型探测特征空间，引入伪造离群点探测合成离群样本，设计离群点引导的三训练优化检测器。

Result: FakeRadar在多个深度伪造视频检测基准数据集上表现优于现有方法，在跨域评估中尤其突出。

Conclusion: FakeRadar能有效处理各种新兴伪造技术，解决跨域泛化问题。

Abstract: In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.

</details>


### [230] [A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images](https://arxiv.org/abs/2512.14640)
*Rao Muhammad Umer,Daniel Sens,Jonathan Noll,Christian Matek,Lukas Wolfseher,Rainer Spang,Ralf Huss,Johannes Raffler,Sarah Reinke,Wolfram Klapper,Katja Steiger,Kristina Schwamborn,Carsten Marr*

Main category: cs.CV

TL;DR: 本文提出首个多中心淋巴瘤基准数据集，评估五种病理基础模型与两种聚合方法在多放大倍数下的表现，发现模型在分布内测试集表现好但分布外测试集表现差，需更多研究。


<details>
  <summary>Details</summary>
Motivation: 及时准确诊断淋巴瘤至关重要，但现有诊断方法成本高、耗时长，且缺乏多中心淋巴瘤亚型分类的综合基准。

Method: 提出首个涵盖四种常见淋巴瘤亚型及健康对照组织的多中心淋巴瘤基准数据集。系统评估五个公开可用病理基础模型，结合基于注意力和转换器的多实例学习聚合器，在三个放大倍数下进行测试。

Result: 在分布内测试集所有放大倍数下，模型多分类平衡准确率超80%，各基础模型和聚合方法表现相近；40倍分辨率足够，更高分辨率或跨放大倍数聚合无性能提升。在分布外测试集，性能降至约60%。

Conclusion: 为推动该领域发展，需要更大规模多中心研究覆盖更多罕见淋巴瘤亚型，作者提供自动化基准测试流程以促进未来研究。

Abstract: Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.

</details>


### [231] [VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image](https://arxiv.org/abs/2512.14677)
*Sicheng Xu,Guojun Chen,Jiaolong Yang,Yizhong Zhang,Yu Deng,Steve Lin,Baining Guo*

Main category: cs.CV

TL;DR: 提出音频驱动的单张图像 3D 头部头像生成器 VASA - 3D，解决表情细节捕捉和单张图像 3D 重建问题，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 解决捕捉真实人脸微妙表情细节和从单张肖像图像重建复杂 3D 头部头像的挑战。

Method: 利用 VASA - 1 的运动潜变量建模表情细节，设计基于运动潜变量的 3D 头部模型，通过优化框架将模型定制到单张图像，使用对伪影和有限姿态覆盖具有鲁棒性的训练损失。

Result: VASA - 3D 能生成现有技术无法实现的逼真 3D 会说话头部，支持以高达 75 FPS 在线生成 512x512 自由视角视频。

Conclusion: VASA - 3D 有助于更沉浸式地与逼真 3D 头像互动。

Abstract: We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.

</details>


### [232] [Native and Compact Structured Latents for 3D Generation](https://arxiv.org/abs/2512.14692)
*Jianfeng Xiang,Xiaoxue Chen,Sicheng Xu,Ruicheng Wang,Zelong Lv,Yu Deng,Hongyuan Zhu,Yue Dong,Hao Zhao,Nicholas Jing Yuan,Jiaolong Yang*

Main category: cs.CV

TL;DR: 本文提出基于O - Voxel的3D生成方法，解决现有3D生成建模问题，生成资产质量超现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成建模的表示方法难以捕捉复杂拓扑和详细外观的资产，阻碍领域发展。

Method: 提出O - Voxel稀疏体素结构编码几何和外观，设计Sparse Compression VAE，训练含4B参数的大规模流匹配模型。

Result: 推理高效，生成资产的几何和材质质量远超现有模型。

Conclusion: 该方法在3D生成建模方面取得显著进展。

Abstract: Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.

</details>


### [233] [TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2512.14698)
*Jun Zhang,Teng Wang,Yuying Ge,Yixiao Ge,Xinhao Li,Ying Shan,Limin Wang*

Main category: cs.CV

TL;DR: 本文未提出新方法，而是为视频时间定位（VTG）建立了重要基线。通过研究数据质量和算法设计，推出TimeLens模型，性能超开源和部分专有模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在VTG优化方案研究不足，需为VTG建立基线。

Method: 从数据质量和算法设计两方面研究，提出TimeLens-Bench基准和TimeLens-100K数据集，探索算法设计原则。

Result: TimeLens模型在开源模型中VTG性能达最优，超GPT - 5和Gemini - 2.5 - Flash等专有模型。

Conclusion: 为VTG研究提供了有价值的基线，相关代码、数据和模型将发布促进后续研究。

Abstract: This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [234] [Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical Recipes](https://arxiv.org/abs/2512.13744)
*Udayon Sen,Alka Luqman,Anupam Chattopadhyay*

Main category: cs.SD

TL;DR: 本文评估了先进音频深度伪造检测模型的鲁棒性，提出可复现框架，研究预训练编码器在多条件训练和固定 SNR 测试下的表现，发现微调可降低 EER。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造音频检测模型在现实捕捉条件下的性能落后于干净实验室结果，需要评估其鲁棒性。

Method: 提出可复现框架，将 MS - SNSD 噪声与 ASVspoof 2021 DF 话语混合，在可控信噪比下评估，研究预训练编码器的多条件训练和固定 SNR 测试。

Result: 在实验中，微调在 10 - 0 dB SNR 下可使各骨干网络的 EER 降低 10 - 15 个百分点。

Conclusion: 微调能有效提升先进音频深度伪造检测模型在有噪声条件下的性能。

Abstract: Deepfake audio detection has progressed rapidly with strong pre-trained encoders (e.g., WavLM, Wav2Vec2, MMS). However, performance in realistic capture conditions - background noise (domestic/office/transport), room reverberation, and consumer channels - often lags clean-lab results. We survey and evaluate robustness for state-of-the-art audio deepfake detection models and present a reproducible framework that mixes MS-SNSD noises with ASVspoof 2021 DF utterances to evaluate under controlled signal-to-noise ratios (SNRs). SNR is a measured proxy for noise severity used widely in speech; it lets us sweep from near-clean (35 dB) to very noisy (-5 dB) to quantify graceful degradation. We study multi-condition training and fixed-SNR testing for pretrained encoders (WavLM, Wav2Vec2, MMS), reporting accuracy, ROC-AUC, and EER on binary and four-class (authenticity x corruption) tasks. In our experiments, finetuning reduces EER by 10-15 percentage points at 10-0 dB SNR across backbones.

</details>


### [235] [Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music Emotion Recognition](https://arxiv.org/abs/2512.13998)
*Qilin Li,C. L. Philip Chen,TongZhang*

Main category: cs.SD

TL;DR: 该工作提出大规模数据集Memo2496和模型DAMER解决音乐情感识别研究中的数据和特征漂移问题，实验显示DAMER性能优异，数据集和代码公开。


<details>
  <summary>Details</summary>
Motivation: 音乐情感识别研究面临高质量标注数据集有限和跨轨道特征漂移问题。

Method: 提出大规模数据集Memo2496并确保标注质量；引入DAMER模型，包含DSAF、PCL和SAML模块。

Result: 在Memo2496、1000songs和PMEmo数据集上DAMER达到了最优性能，分别提高唤醒维度准确率3.43%、2.25%和0.17%。

Conclusion: 消融研究和可视化分析验证了各模块的贡献，数据集和源代码公开。

Abstract: Music Emotion Recogniser (MER) research faces challenges due to limited high-quality annotated datasets and difficulties in addressing cross-track feature drift. This work presents two primary contributions to address these issues. Memo2496, a large-scale dataset, offers 2496 instrumental music tracks with continuous valence arousal labels, annotated by 30 certified music specialists. Annotation quality is ensured through calibration with extreme emotion exemplars and a consistency threshold of 0.25, measured by Euclidean distance in the valence arousal space. Furthermore, the Dual-view Adaptive Music Emotion Recogniser (DAMER) is introduced. DAMER integrates three synergistic modules: Dual Stream Attention Fusion (DSAF) facilitates token-level bidirectional interaction between Mel spectrograms and cochleagrams via cross attention mechanisms; Progressive Confidence Labelling (PCL) generates reliable pseudo labels employing curriculum-based temperature scheduling and consistency quantification using Jensen Shannon divergence; and Style Anchored Memory Learning (SAML) maintains a contrastive memory queue to mitigate cross-track feature drift. Extensive experiments on the Memo2496, 1000songs, and PMEmo datasets demonstrate DAMER's state-of-the-art performance, improving arousal dimension accuracy by 3.43%, 2.25%, and 0.17%, respectively. Ablation studies and visualisation analyses validate each module's contribution. Both the dataset and source code are publicly available.

</details>


### [236] [Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting](https://arxiv.org/abs/2512.14115)
*Ramesh Gundluru,Shubham Gupta,Sri Rama Murty K*

Main category: cs.SD

TL;DR: 提出联合多模态对比学习框架改进声学词嵌入，在词辨别任务上表现优于现有基线，支持STD和KWS。


<details>
  <summary>Details</summary>
Motivation: 现有声学词嵌入方法存在单模态监督、音频 - 音频和音频 - 文本对齐优化分离、需特定任务模型等局限。

Method: 提出联合多模态对比学习框架，统一声学和跨模态监督于共享嵌入空间，同时优化音频 - 文本和音频 - 音频对比学习。

Result: 该方法在词辨别任务上优于现有声学词嵌入基线，能灵活支持STD和KWS。

Conclusion: 这是首个此类综合方法。

Abstract: Acoustic Word Embeddings (AWEs) improve the efficiency of speech retrieval tasks such as Spoken Term Detection (STD) and Keyword Spotting (KWS). However, existing approaches suffer from limitations, including unimodal supervision, disjoint optimization of audio-audio and audio-text alignment, and the need for task-specific models. To address these shortcomings, we propose a joint multimodal contrastive learning framework that unifies both acoustic and cross-modal supervision in a shared embedding space. Our approach simultaneously optimizes: (i) audio-text contrastive learning, inspired by the CLAP loss, to align audio and text representations and (ii) audio-audio contrastive learning, via Deep Word Discrimination (DWD) loss, to enhance intra-class compactness and inter-class separation. The proposed method outperforms existing AWE baselines on word discrimination task while flexibly supporting both STD and KWS. To our knowledge, this is the first comprehensive approach of its kind.

</details>


### [237] [Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis](https://arxiv.org/abs/2512.14602)
*Lukáš Samuel Marták,Patricia Hu,Gerhard Widmer*

Main category: cs.SD

TL;DR: 本文研究AMT系统在不同音乐语境泛化能力，引入MDS语料库评估，发现特定分布偏移下性能下降，揭示语料库偏差问题。


<details>
  <summary>Details</summary>
Motivation: 由于标注音乐数据集有限，AMT进展集中在古典钢琴音乐，需研究系统在其他音乐语境泛化能力。

Method: 引入MDS语料库模拟不同分布偏移轴，用传统信息检索和音乐相关指标评估多个AMT系统。

Result: 特定分布偏移下系统性能有不同程度下降，如声音和流派因素导致F1性能下降，动力学估计比起始预测更易受音乐变化影响。

Conclusion: 研究结果为深度AMT系统中语料库偏差问题的持续影响提供新证据。

Abstract: Automatic Music Transcription (AMT) -- the task of converting music audio into note representations -- has seen rapid progress, driven largely by deep learning systems. Due to the limited availability of richly annotated music datasets, much of the progress in AMT has been concentrated on classical piano music, and even a few very specific datasets. Whether these systems can generalize effectively to other musical contexts remains an open question. Complementing recent studies on distribution shifts in sound (e.g., recording conditions), in this work we investigate the musical dimension -- specifically, variations in genre, dynamics, and polyphony levels. To this end, we introduce the MDS corpus, comprising three distinct subsets -- (1) Genre, (2) Random, and (3) MAEtest -- to emulate different axes of distribution shift. We evaluate the performance of several state-of-the-art AMT systems on the MDS corpus using both traditional information-retrieval and musically-informed performance metrics. Our extensive evaluation isolates and exposes varying degrees of performance degradation under specific distribution shifts. In particular, we measure a note-level F1 performance drop of 20 percentage points due to sound, and 14 due to genre. Generally, we find that dynamics estimation proves more vulnerable to musical variation than onset prediction. Musically informed evaluation metrics, particularly those capturing harmonic structure, help identify potential contributing factors. Furthermore, experiments with randomly generated, non-musical sequences reveal clear limitations in system performance under extreme musical distribution shifts. Altogether, these findings offer new evidence of the persistent impact of the Corpus Bias problem in deep AMT systems.

</details>


### [238] [MuseCPBench: an Empirical Study of Music Editing Methods through Music Context Preservation](https://arxiv.org/abs/2512.14629)
*Yash Vishe,Eric Xue,Xunyi Jiang,Zachary Novack,Junda Wu,Julian McAuley,Xin Xu*

Main category: cs.SD

TL;DR: 现有音乐编辑中音乐上下文保留情况评估不足，本文提出 MuseCPBench 基准进行评估并发现现存方法差距。


<details>
  <summary>Details</summary>
Motivation: 现有音乐编辑研究忽视音乐上下文保留（MCP）能力评估，且评估方案和指标不一致。

Method: 引入首个 MCP 评估基准 MuseCPBench，对五个代表性音乐编辑基线进行全面比较和系统分析。

Result: 发现当前音乐编辑方法存在一致的保留差距并给出解释。

Conclusion: 研究结果可为开发具有强 MCP 能力的音乐编辑策略提供实用指导。

Abstract: Music editing plays a vital role in modern music production, with applications in film, broadcasting, and game development. Recent advances in music generation models have enabled diverse editing tasks such as timbre transfer, instrument substitution, and genre transformation. However, many existing works overlook the evaluation of their ability to preserve musical facets that should remain unchanged during editing a property we define as Music Context Preservation (MCP). While some studies do consider MCP, they adopt inconsistent evaluation protocols and metrics, leading to unreliable and unfair comparisons. To address this gap, we introduce the first MCP evaluation benchmark, MuseCPBench, which covers four categories of musical facets and enables comprehensive comparisons across five representative music editing baselines. Through systematic analysis along musical facets, methods, and models, we identify consistent preservation gaps in current music editing methods and provide insightful explanations. We hope our findings offer practical guidance for developing more effective and reliable music editing strategies with strong MCP capability

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [239] [Writing in Symbiosis: Mapping Human Creative Agency in the AI Era](https://arxiv.org/abs/2512.13697)
*Vivan Doshi,Mengyuan Li*

Main category: cs.CY

TL;DR: 研究大语言模型时代创意写作中人类与AI协同进化模式，提出‘双轨进化’及作者三种适应模式，为相关讨论做贡献。


<details>
  <summary>Details</summary>
Motivation: 大语言模型普及使人类与机器关系愈发共生，探讨创意写作中人类创作与能动性如何随机器能力演变。

Method: 利用跨越LLM时代前后的大规模语料库，分析纵向写作数据中的多样模式。

Result: 观察到‘双轨进化’模式，即主题围绕AI相关收敛、风格结构化分化；发现作者三种适应模式。

Conclusion: 所构建的创意原型图揭示了作者与AI协同进化情况，有助于人类 - AI协作、检测挑战及创意多样性保护等方面的讨论。

Abstract: The proliferation of Large Language Models (LLMs) raises a critical question about what it means to be human when we share an increasingly symbiotic relationship with persuasive and creative machines. This paper examines patterns of human-AI coevolution in creative writing, investigating how human craft and agency are adapting alongside machine capabilities. We challenge the prevailing notion of stylistic homogenization by examining diverse patterns in longitudinal writing data. Using a large-scale corpus spanning the pre- and post-LLM era, we observe patterns suggestive of a "Dual-Track Evolution": thematic convergence around AI-related topics, coupled with structured stylistic differentiation. Our analysis reveals three emergent adaptation patterns: authors showing increased similarity to AI style, those exhibiting decreased similarity, and those maintaining stylistic stability while engaging with AI-related themes. This Creative Archetype Map illuminates how authorship is coevolving with AI, contributing to discussions about human-AI collaboration, detection challenges, and the preservation of creative diversity.

</details>


### [240] [Enhancing Transparency and Traceability in Healthcare AI: The AI Product Passport](https://arxiv.org/abs/2512.13702)
*A. Anil Sinaci,Senan Postaci,Dogukan Cavdaroglu,Machteld J. Boonstra,Okan Mercan,Kerem Yilmaz,Gokce B. Laleci Erturkmen,Folkert W. Asselbergs,Karim Lekadir*

Main category: cs.CY

TL;DR: 开发基于标准的AI产品护照框架，以提升医疗AI透明度、可追溯性和合规性，介绍其设计、实现和效果并展望未来改进。


<details>
  <summary>Details</summary>
Motivation: 提升医疗AI的透明度、可追溯性和合规性，解决现有医疗AI中的透明度缺口。

Method: 在AI4HF项目中以心力衰竭AI工具为重点，分析监管框架和现有标准，设计数据模型，融入MLOps/ModelOps概念，通过多方共创收集反馈，用Python库实现自动化溯源追踪。

Result: 设计出基于标准且有明确生命周期管理和角色访问的AI产品护照，实现为支持可审计文档的网页平台，符合相关原则，能生成报告，代码开源增强可访问性。

Conclusion: AI产品护照可解决医疗AI透明度问题，其开源和符合标准的特性利于建立信任和适应性，未来将增强互操作性促进负责任AI部署。

Abstract: Objective: To develop the AI Product Passport, a standards-based framework improving transparency, traceability, and compliance in healthcare AI via lifecycle-based documentation. Materials and Methods: The AI Product Passport was developed within the AI4HF project, focusing on heart failure AI tools. We analyzed regulatory frameworks (EU AI Act, FDA guidelines) and existing standards to design a relational data model capturing metadata across AI lifecycle phases: study definition, dataset preparation, model generation/evaluation, deployment/monitoring, and passport generation. MLOps/ModelOps concepts were integrated for operational relevance. Co-creation involved feedback from AI4HF consortium and a Lisbon workshop with 21 diverse stakeholders, evaluated via Mentimeter polls. The open-source platform was implemented with Python libraries for automated provenance tracking. Results: The AI Product Passport was designed based on existing standards and methods with well-defined lifecycle management and role-based access. Its implementation is a web-based platform with a relational data model supporting auditable documentation. It generates machine- and human-readable reports, customizable for stakeholders. It aligns with FUTURE-AI principles (Fairness, Universality, Traceability, Usability, Robustness, Explainability), ensuring fairness, traceability, and usability. Exported passports detail model purpose, data provenance, performance, and deployment context. GitHub-hosted backend/frontend codebases enhance accessibility. Discussion and Conclusion: The AI Product Passport addresses transparency gaps in healthcare AI, meeting regulatory and ethical demands. Its open-source nature and alignment with standards foster trust and adaptability. Future enhancements include FAIR data principles and FHIR integration for improved interoperability, promoting responsible AI deployment.

</details>


### [241] [Made-in China, Thinking in America:U.S. Values Persist in Chinese LLMs](https://arxiv.org/abs/2512.13723)
*David Haslett,Linus Ta-Lun Huang,Leila Khalatbari,Janet Hui-wen Hsiao,Antoni B. Chan*

Main category: cs.CY

TL;DR: 研究中美大语言模型对中美价值观的契合度，发现模型更贴近美国价值观。


<details>
  <summary>Details</summary>
Motivation: 大语言模型成为全球行为体软实力竞争工具，当前模型多体现西方价值观，需考察中美模型与中美人群价值观契合度。

Method: 从10个中国模型和10个美国模型获取对道德基础问卷2.0和世界价值观调查的回应，并与数千中美民众的回应对比。

Result: 所有模型对两项调查的回应更像美国人而非中国人，用中文提示或赋予中国角色也仅稍有缓解。

Conclusion: 在大语言模型大量生成内容并影响地缘政治规范的未来，该研究发现有重要意义。

Abstract: As large language models increasingly mediate access to information and facilitate decision-making, they are becoming instruments in soft power competitions between global actors such as the United States and China. So far, language models seem to be aligned with the values of Western countries, but evidence for this ethical bias comes mostly from models made by American companies. The current crop of state-of-the-art models includes several made in China, so we conducted the first large-scale investigation of how models made in China and the USA align with people from China and the USA. We elicited responses to the Moral Foundations Questionnaire 2.0 and the World Values Survey from ten Chinese models and ten American models, and we compared their responses to responses from thousands of Chinese and American people. We found that all models respond to both surveys more like American people than like Chinese people. This skew toward American values is only slightly mitigated when prompting the models in Chinese or imposing a Chinese persona on the models. These findings have important implications for a near future in which large language models generate much of the content people consume and shape normative influence in geopolitics.

</details>


### [242] [Exploring the Modular Integration of "AI + Architecture" Pedagogy in Undergraduate Design Education: A Case Study of Architectural Design III/IV Courses at Zhejiang University](https://arxiv.org/abs/2512.13730)
*Wang Jiaqi,Lan Yi,Chen Xiang*

Main category: cs.CY

TL;DR: 通过浙江大学本科设计工作室教学实验研究AI融入建筑教育，采用双模块框架，证明该模式有效且可复制。


<details>
  <summary>Details</summary>
Motivation: 研究AI在建筑教育中的融入情况。

Method: 在浙江大学2024 - 2025级本科设计工作室开展教学实验，采用20小时AI培训+嵌入式伦理讨论的双模块框架，引入多种AI技术，保持原课程结构并配备技术指导。

Result: 分阶段指导、平衡技术与伦理的方法以及机构支持有效，提升了学生数字技能和战略认知，解决了AI伦理问题。

Conclusion: 该模式为设计教育提供了结合技术与批判性学习的可复制方法。

Abstract: This study investigates AI integration in architectural education through a teaching experiment in Zhejiang University's 2024-25 grade three undergraduate design studio. Adopting a dual-module framework (20-hour AI training + embedded ethics discussions), the course introduced deep learning models, LLMs, AIGC, LoRA, and ComfyUI while maintaining the original curriculum structure, supported by dedicated technical instructors. Findings demonstrate the effectiveness of phased guidance, balanced technical-ethical approaches, and institutional support. The model improved students' digital skills and strategic cognition while addressing AI ethics, providing a replicable approach combining technical and critical learning in design education.

</details>


### [243] [Instilling Organisational Values in Firefighters through Simulation-Based Training](https://arxiv.org/abs/2512.13737)
*Nardine Osman,Manel Rodriguez-Soto,Jordi Sabater-Mir*

Main category: cs.CY

TL;DR: 本文提出将部门价值观融入基于模拟的消防员培训的概念框架，以提升培训效果并完善操作协议


<details>
  <summary>Details</summary>
Motivation: 传统培训方法不足以让消防员应对紧急情况下的伦理困境和价值冲突

Method: 系统地将部门价值观融入基于模拟的培训

Result: 促进价值观内化，提高压力下基于价值的决策能力

Conclusion: 该方法可用于评估和完善部门操作协议，使其与首选价值观更好地契合

Abstract: In firefighting and other emergency operations, decisions made under pressure carry profound ethical weight and can significantly impact incident outcomes and firefighter safety. Traditional training methods, while foundational, often fall short in adequately preparing firefighters for the complex ethical dilemmas and value conflicts inherent in chaotic emergency environments. This paper proposes a conceptual framework for enhancing firefighter training by systematically integrating departmental values into simulation-based training. This approach fosters deeper value internalisation and improves value-driven decision-making under pressure. Furthermore, the underlying tools can also be leveraged to evaluate and refine departmental operational protocols for better alignment with preferred values.

</details>


### [244] [The algorithmic muse and the public domain: Why copyrights legal philosophy precludes protection for generative AI outputs](https://arxiv.org/abs/2512.13750)
*Ezieddin Elmahjub*

Main category: cs.CY

TL;DR: 文章认为生成式AI输出不可受版权保护，应与人类创作贡献区分，其输出应归公共领域。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI输出是否可受版权保护的问题。

Method: 绕过传统教义分析，重新评估版权的基础哲学。

Result: 传统理论无法为保护生成式AI输出提供合理依据，授予其版权会带来诸多问题。

Conclusion: 应明确区分人类创作贡献和生成式AI原始输出，后者应留在公共领域。

Abstract: Generative AI (GenAI) outputs are not copyrightable. This article argues why. We bypass conventional doctrinal analysis that focuses on black letter law notions of originality and authorship to re-evaluate copyright's foundational philosophy. GenAI fundamentally severs the direct human creative link to expressive form. Traditional theories utilitarian incentive, labor desert and personality fail to provide coherent justification for protection. The public domain constitutes the default baseline for intellectual creations. Those seeking copyright coverage for GenAI outputs bear the burden of proof. Granting copyright to raw GenAI outputs would not only be philosophically unsound but would also trigger an unprecedented enclosure of the digital commons, creating a legal quagmire and stifling future innovation. The paper advocates for a clear distinction: human creative contributions to AI-generated works may warrant protection, but the raw algorithmic output should remain in the public domain.

</details>


### [245] [Beyond Procedural Compliance: Human Oversight as a Dimension of Well-being Efficacy in AI Governance](https://arxiv.org/abs/2512.13768)
*Yao Xie,Walter Cullen*

Main category: cs.CY

TL;DR: 本文将人类监督引入作为一种幸福能力，置于幸福效能框架中，认为其发展需融入各层次教育，为未来研究提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 主要AI伦理准则和法律呼吁有效人类监督，但未将其定义为可发展能力，因此需要探讨。

Method: 将人类监督作为一种幸福能力，置于新兴的幸福效能框架中进行概念整合。

Result: 提出将该能力融入各层次教育是可持续且具成本效益的发展方式，并提供从监管目标到培养人类能动性与责任的实际路径。

Conclusion: 为幸福效能在多情境的教学实施和实证验证的未来研究奠定理论基础。

Abstract: Major AI ethics guidelines and laws, including the EU AI Act, call for effective human oversight, but do not define it as a distinct and developable capacity. This paper introduces human oversight as a well-being capacity, situated within the emerging Well-being Efficacy framework. The concept integrates AI literacy, ethical discernment, and awareness of human needs, acknowledging that some needs may be conflicting or harmful. Because people inevitably project desires, fears, and interests into AI systems, oversight requires the competence to examine and, when necessary, restrain problematic demands.
  The authors argue that the sustainable and cost-effective development of this capacity depends on its integration into education at every level, from professional training to lifelong learning. The frame of human oversight as a well-being capacity provides a practical path from high-level regulatory goals to the continuous cultivation of human agency and responsibility essential for safe and ethical AI. The paper establishes a theoretical foundation for future research on the pedagogical implementation and empirical validation of well-being effectiveness in multiple contexts.

</details>


### [246] [Assessing High-Risk Systems: An EU AI Act Verification Framework](https://arxiv.org/abs/2512.13907)
*Alessio Buscemi,Tom Deckenbrunnen,Fahria Kabir,Nishat Mowla,Kateryna Mishchenko*

Main category: cs.CY

TL;DR: 欧盟实施AI法规缺系统验证方法，本文提出综合框架解决此问题，减少解读不确定性。


<details>
  <summary>Details</summary>
Motivation: 欧盟实施AI法案及相关法规时缺乏系统方法来验证法律要求，监管模糊给成员国带来负担，且准备情况不一致。

Method: 提出综合框架，从方法类型（控制与测试）和评估目标（数据、模型、流程和最终产品）两个维度组织合规验证，将核心法律要求映射到具体验证活动。

Result: 无明确提及具体结果。

Conclusion: 该方法旨在减少解读不确定性，促进评估实践一致性，支持AI生命周期内监管、道德和技术视角的对齐。

Abstract: A central challenge in implementing the AI Act and other AI-relevant regulations in the EU is the lack of a systematic approach to verify their legal mandates. Recent surveys show that this regulatory ambiguity is perceived as a significant burden, leading to inconsistent readiness across Member States. This paper proposes a comprehensive framework designed to help close this gap by organising compliance verification along two fundamental dimensions: the type of method (controls vs. testing) and the target of assessment (data, model, processes, and final product). Additionally, our framework maps core legal requirements to concrete verification activities, serving as a vital bridge between policymakers and practitioners, and aligning legal text with technical standards and best practices. The proposed approach aims to reduce interpretive uncertainty, promote consistency in assessment practices, and support the alignment of regulatory, ethical, and technical perspectives across the AI lifecycle.

</details>


### [247] [Criminal Liability in AI-Enabled Autonomous Vehicles: A Comparative Study](https://arxiv.org/abs/2512.14330)
*Sahibpreet Singh,Manjit Singh*

Main category: cs.CY

TL;DR: 研究通过对比分析多国法规等，探讨AV事故中刑事追责问题，发现监管差异并建议制定全球统一法律标准。


<details>
  <summary>Details</summary>
Motivation: AI助力下AV带来交通革新，但引发复杂的犯罪责任认定问题，需深入研究。

Method: 使用比较法分析美国、德国、英国、中国和印度的主要法规、实际责任索赔案例及学术文献。

Result: 各国监管情况不同，美印依靠各州法律，英国有《2018年自动和电动车辆法案》，德国按车辆运行模式区分责任，中国也追求严格责任制度。

Conclusion: 全球统一的法律标准对促进技术创新、降低风险和明确责任归属至关重要。

Abstract: AI revolutionizes transportation through autonomous vehicles (AVs) but introduces complex criminal liability issues regarding infractions. This study employs a comparative legal analysis of primary statutes, real-world liability claims, and academic literature across the US, Germany, UK, China, and India; jurisdictions selected for their technological advancement and contrasting regulatory approaches. The research examines the attribution of human error, AI moral agency, and the identification of primary offenders in AV incidents. Findings reveal fragmented regulatory landscapes: India and the US rely on loose networks of state laws, whereas the UK enacted the pioneering Automated and Electric Vehicles Act 2018. Germany enforces strict safety standards, distinguishing liability based on the vehicle's operating mode, while China similarly aims for a stringent liability regime. The study concludes that globally harmonized legal standards are essential to foster technological innovation while ensuring minimum risk and clear liability attribution.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [248] [LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation](https://arxiv.org/abs/2512.14138)
*So Kuroki,Manami Nakagawa,Shigeo Yoshida,Yuki Koyama,Kozuno Tadashi*

Main category: cs.HC

TL;DR: 提出LAPPI方法，用大语言模型辅助组合优化问题实例化，在旅行规划用户研究中表现好且具通用性。


<details>
  <summary>Details</summary>
Motivation: 现实组合优化问题实例化对终端用户困难，需定义候选项目、分配偏好分数和指定约束。

Method: 引入LAPPI方法，通过自然语言对话借助大语言模型支持用户将模糊偏好转化为明确定义的优化问题，再用现有优化求解器生成解决方案。

Result: 在旅行规划用户研究中成功捕获用户偏好，生成的可行计划优于传统和提示工程方法，且可适应其他用例。

Conclusion: LAPPI方法能有效辅助用户进行组合优化问题实例化，具有较好效果和通用性。

Abstract: Many real-world tasks, such as trip planning or meal planning, can be formulated as combinatorial optimization problems. However, using optimization solvers is difficult for end users because it requires problem instantiation: defining candidate items, assigning preference scores, and specifying constraints. We introduce LAPPI (LLM-Assisted Preference-based Problem Instantiation), an interactive approach that uses large language models (LLMs) to support users in this instantiation process. Through natural language conversations, the system helps users transform vague preferences into well-defined optimization problems. These instantiated problems are then passed to existing optimization solvers to generate solutions. In a user study on trip planning, our method successfully captured user preferences and generated feasible plans that outperformed both conventional and prompt-engineering approaches. We further demonstrate LAPPI's versatility by adapting it to an additional use case.

</details>


### [249] [The Trust in AI-Generated Health Advice (TAIGHA) Scale and Short Version (TAIGHA-S): Development and Validation Study](https://arxiv.org/abs/2512.14278)
*Marvin Kopka,Azeem Majeed,Gabriella Spinelli,Austen El-Osta,Markus Feufel*

Main category: cs.HC

TL;DR: 研究开发并验证了用于评估用户对AI生成健康建议信任与不信任的TAIGHA量表及其短版TAIGHA - S。


<details>
  <summary>Details</summary>
Motivation: 现有工具无法专门测量用户对AI生成健康建议的信任度，需要开发相关测量工具。

Method: 用生成式AI方法开发项目，经内容验证、面对面验证和心理测量验证，经自动项目缩减和专家评估确定最终项目。

Result: TAIGHA内容效度好、模型拟合优、内部一致性高，有汇聚效度和区分效度；TAIGHA - S与完整版相关性高、可靠性好。

Conclusion: TAIGHA和TAIGHA - S是评估用户对AI生成健康建议信任和不信任的有效工具，分开报告信任和不信任可更全面评估AI干预，短版适用于时间受限场景。

Abstract: Artificial Intelligence tools such as large language models are increasingly used by the public to obtain health information and guidance. In health-related contexts, following or rejecting AI-generated advice can have direct clinical implications. Existing instruments like the Trust in Automated Systems Survey assess trustworthiness of generic technology, and no validated instrument measures users' trust in AI-generated health advice specifically. This study developed and validated the Trust in AI-Generated Health Advice (TAIGHA) scale and its four-item short form (TAIGHA-S) as theory-based instruments measuring trust and distrust, each with cognitive and affective components. The items were developed using a generative AI approach, followed by content validation with 10 domain experts, face validation with 30 lay participants, and psychometric validation with 385 UK participants who received AI-generated advice in a symptom-assessment scenario. After automated item reduction, 28 items were retained and reduced to 10 based on expert ratings. TAIGHA showed excellent content validity (S-CVI/Ave=0.99) and CFA confirmed a two-factor model with excellent fit (CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03). Internal consistency was high (α=0.95). Convergent validity was supported by correlations with the Trust in Automated Systems Survey (r=0.67/-0.66) and users' reliance on the AI's advice (r=0.37 for trust), while divergent validity was supported by low correlations with reading flow and mental load (all |r|<0.25). TAIGHA-S correlated highly with the full scale (r=0.96) and showed good reliability (α=0.88). TAIGHA and TAIGHA-S are validated instruments for assessing user trust and distrust in AI-generated health advice. Reporting trust and distrust separately permits a more complete evaluation of AI interventions, and the short scale is well-suited for time-constrained settings.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [250] [Randomized multi-class classification under system constraints: a unified approach via post-processing](https://arxiv.org/abs/2512.14246)
*Evgenii Chzhen,Mohamed Hebiri,Gayane Taturyan*

Main category: math.OC

TL;DR: 研究系统级约束下多类分类问题，提出无需重新训练的后处理方法，给出有限样本保证，框架适用多种约束。


<details>
  <summary>Details</summary>
Motivation: 解决系统级约束（可表示为随机分类器上的线性泛函）下的多类分类问题。

Method: 将问题表述为随机分类器上的线性约束随机规划，利用熵正则化和对偶优化技术构建可行解。

Result: 在最小假设下为算法最终输出提供风险和约束满足的有限样本保证。

Conclusion: 所提框架能适应包括公平性、弃权和客户流失要求等广泛的约束类别。

Abstract: We study the problem of multi-class classification under system-level constraints expressible as linear functionals over randomized classifiers. We propose a post-processing approach that adjusts a given base classifier to satisfy general constraints without retraining. Our method formulates the problem as a linearly constrained stochastic program over randomized classifiers, and leverages entropic regularization and dual optimization techniques to construct a feasible solution. We provide finite-sample guarantees for the risk and constraint satisfaction for the final output of our algorithm under minimal assumptions. The framework accommodates a broad class of constraints, including fairness, abstention, and churn requirements.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [251] [Sim2Real Reinforcement Learning for Soccer skills](https://arxiv.org/abs/2512.12437)
*Jonathan Spraggett*

Main category: cs.RO

TL;DR: 提出用强化学习训练人形机器人控制任务新方法，结果表明策略更优，但策略从模拟到现实转移失败。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在适应现实环境、处理复杂性和自然动作方面存在局限，需更高效有效的方法。

Method: 使用课程训练和对抗运动先验（AMP）技术。

Result: 开发的踢、走、跳强化学习策略更具动态性和适应性，优于以往方法。

Conclusion: 当前强化学习方法在完全适应现实场景方面存在局限，学习到的策略从模拟到现实的转移未成功。

Abstract: This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.

</details>


### [252] [Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model](https://arxiv.org/abs/2512.14031)
*Zhaofeng Hu,Hongrui Yu,Vaidhyanathan Chandramouli,Ci-Jyun Liang*

Main category: cs.RO

TL;DR: 研究评估VLA模型和RL方法教建筑机器人学新技能的适用性，发现VLA有实用优势，DQN在足够调优下是可行基线。


<details>
  <summary>Details</summary>
Motivation: 了解VLA模型和RL方法在建筑自动化中教机器人新技能的任务表现和实际部署所需努力。

Method: 开发两个遥操作接口收集演示，进行三阶段评估，包括对比RL基线、训练和比较VLA模型、基准测试。

Result: VLA模型泛化和少样本能力强，拾取阶段成功率60%和100%；DQN调优需额外噪声，增加工作量。

Conclusion: VLA在改变任务上有实用优势，减少编程工作量和少量数据即可；DQN在可接受足够调优时是可行基线。

Abstract: This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.

</details>


### [253] [CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation](https://arxiv.org/abs/2512.14689)
*Sirui Chen,Zi-ang Cao,Zhengyi Luo,Fernando Castañeda,Chenran Li,Tingwu Wang,Ye Yuan,Linxi "Jim" Fan,C. Karen Liu,Yuke Zhu*

Main category: cs.RO

TL;DR: 提出CHIP模块使类人机器人能执行有力操作任务


<details>
  <summary>Details</summary>
Motivation: 类人机器人在执行如移动物体等有力操作任务方面仍具挑战

Method: 提出自适应合规人形控制通过视觉扰动(CHIP)模块，且无需数据增强和额外奖励调整

Result: 训练有CHIP的通用运动跟踪控制器可执行多种需不同末端执行器合规性的有力操作任务

Conclusion: CHIP模块可使类人机器人在保留动态参考运动灵活跟踪能力的同时实现可控末端执行器刚度，可执行多种有力操作任务

Abstract: Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [254] [Joint Models with Multiple Markers and Multiple Time-to-event Outcomes Using Variational Approximations](https://arxiv.org/abs/2512.13962)
*Benjamin Christoffersen,Keith Humphreys,Alessandro Gasparini,Birzhan Akynkozhayev,Hedvig Kjellström,Mark Clements*

Main category: stat.ME

TL;DR: 提出基于高斯变分近似的联合模型全似然方法，满足多标记、多生存结果等标准，经模拟验证有效，还给出应用实例。


<details>
  <summary>Details</summary>
Motivation: 现有联合模型很少能同时满足多标记、多生存结果、延迟进入和可扩展性等条件。

Method: 提出基于高斯变分近似的联合模型全似然方法，并提供开源实现。

Result: 模拟显示变分近似的下界接近全似然，方法和实现快速且可扩展。

Conclusion: 变分近似为扩展当前联合模型提供了有前景的方法。

Abstract: Joint models are well suited to modelling linked data from laboratories and health registers. However, there are few examples of joint models that allow for (a) multiple markers, (b) multiple survival outcomes (including terminal events, competing events, and recurrent events), (c) delayed entry and (d) scalability. We propose a full likelihood approach for joint models based on a Gaussian variational approximation to satisfy criteria (a)-(d). We provide an open-source implementation for this approach, allowing for flexible sets of models for the longitudinal markers and survival outcomes. Through simulations, we find that the lower bound for the variational approximation is close to the full likelihood. We also find that our approach and implementation are fast and scalable. We provide an application with a joint model for longitudinal measurements of dense and fatty breast tissue and time to first breast cancer diagnosis. The use of variational approximations provides a promising approach for extending current joint models.

</details>


### [255] [Trunc-Opt vine building algorithms](https://arxiv.org/abs/2512.14399)
*Dániel Pfeifer,Edith Alice Kovács*

Main category: stat.ME

TL;DR: 本文提出截断藤结构的新评分和构建方法，利用条件独立性，在真实数据集上表现优于已知方法。


<details>
  <summary>Details</summary>
Motivation: 藤copula模型参数空间高维，截断藤copula虽被提出但构建算法存在局限，需新方法。

Method: 引入截断藤权重新评分，提出构建截断藤的新方法，利用条件独立性构建和编码截断藤，给出相关算法。

Result: 在真实数据集上进行算法验证，并与R包中知名方法比较，新方法表现更好。

Conclusion: 所提新方法在构建截断藤上有效，优于先前已知方法。

Abstract: Vine copula models have become highly popular and practical tools for modelling multivariate probability distributions due to their flexibility in modelling different kinds of dependences between the random variables involved. However, their flexibility comes with the drawback of a high-dimensional parameter space. To tackle this problem, truncated vine copulas were introduced by Kurowicka (2010) (Gaussian case) and Brechmann and Czado (2013) (general case). Truncated vine copulas contain conditionally independent pair copulas after the truncation level. So far, in the general case, truncated vine constructing algorithms started from the lowest tree in order to encode the largest dependences in the lower trees. The novelty of this paper starts from the observation that a truncated vine is determined by the first tree after the truncation level (see Kovács and Szántai (2017)). This paper introduces a new score for fitting truncated vines to given data, called the Weight of the truncated vine. Then we propose a completely new methodology for constructing truncated vines. We prove theorems which motivate this new approach. While earlier algorithms did not use conditional independences, we give algorithms for constructing and encoding truncated vines which do exploit them. Finally, we illustrate the algorithms on real datasets and compare the results with well-known methods included in R packages. Our method generally compare favorably to previously known methods.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [256] [Safe Online Control-Informed Learning](https://arxiv.org/abs/2512.13868)
*Tianyu Zhou,Zihao Liang,Zehui Lu,Shaoshuai Mou*

Main category: eess.SY

TL;DR: 提出安全在线控制学习框架，统一多要素，用扩展卡尔曼滤波更新参数，用软加障碍函数保证约束，理论有保障，在两系统验证有效性。


<details>
  <summary>Details</summary>
Motivation: 为安全关键的自主系统设计有效的在线学习框架。

Method: 将最优控制、参数估计和安全约束统一到在线学习过程，用扩展卡尔曼滤波实时更新系统参数，用软加障碍函数保证约束满足。

Result: 理论分析有收敛和安全保证，在推车-摆杆和机械臂系统上验证了框架有效性。

Conclusion: 所提出的安全在线控制学习框架有效可行。

Abstract: This paper proposes a Safe Online Control-Informed Learning framework for safety-critical autonomous systems. The framework unifies optimal control, parameter estimation, and safety constraints into an online learning process. It employs an extended Kalman filter to incrementally update system parameters in real time, enabling robust and data-efficient adaptation under uncertainty. A softplus barrier function enforces constraint satisfaction during learning and control while eliminating the dependence on high-quality initial guesses. Theoretical analysis establishes convergence and safety guarantees, and the framework's effectiveness is demonstrated on cart-pole and robot-arm systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [257] [From Obfuscated to Obvious: A Comprehensive JavaScript Deobfuscation Tool for Security Analysis](https://arxiv.org/abs/2512.14070)
*Dongchao Zhou,Lingyun Ying,Huajun Chai,Dongbin Wang*

Main category: cs.CR

TL;DR: 论文提出JSIMPLIFIER去混淆工具应对JavaScript恶意代码混淆问题，构建数据集评估显示该工具优于现有工具。


<details>
  <summary>Details</summary>
Motivation: JavaScript广泛使用，现有去混淆工具在输入格式、处理类型和输出可读性上存在局限，影响实际效果。

Method: 采用多阶段流水线，含预处理、基于抽象语法树的静态分析、动态执行跟踪和大语言模型增强的标识符重命名；引入多维评估指标；构建并发布含44421个样本的数据集。

Result: JSIMPLIFIER在20种混淆技术上处理能力达100%，评估子集正确性100%，代码复杂度降低88.2%，可读性经多个大语言模型验证提升超4倍。

Conclusion: 研究成果推动了JavaScript去混淆研究和实际安全应用的基准发展。

Abstract: JavaScript's widespread adoption has made it an attractive target for malicious attackers who employ sophisticated obfuscation techniques to conceal harmful code. Current deobfuscation tools suffer from critical limitations that severely restrict their practical effectiveness. Existing tools struggle with diverse input formats, address only specific obfuscation types, and produce cryptic output that impedes human analysis.
  To address these challenges, we present JSIMPLIFIER, a comprehensive deobfuscation tool using a multi-stage pipeline with preprocessing, abstract syntax tree-based static analysis, dynamic execution tracing, and Large Language Model (LLM)-enhanced identifier renaming. We also introduce multi-dimensional evaluation metrics that integrate control/data flow analysis, code simplification assessment, entropy measures and LLM-based readability assessments.
  We construct and release the largest real-world obfuscated JavaScript dataset with 44,421 samples (23,212 wild malicious + 21,209 benign samples). Evaluation shows JSIMPLIFIER outperforms existing tools with 100% processing capability across 20 obfuscation techniques, 100% correctness on evaluation subsets, 88.2% code complexity reduction, and over 4-fold readability improvement validated by multiple LLMs. Our results advance benchmarks for JavaScript deobfuscation research and practical security applications.

</details>


### [258] [Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2512.13703)
*Fan Yang*

Main category: cs.CR

TL;DR: 本文提出Safe2Harm语义同构攻击方法对大语言模型进行越狱攻击，实验表明其能力强于现有方法，还构建评估数据集用于防御。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在安全漏洞，攻击者可利用生成有害内容，现有越狱方法有局限，且发现有害场景与合法场景原理有一致性。

Method: 提出Safe2Harm语义同构攻击方法，分四阶段：将有害问题重写成语义安全问题、提取主题映射关系、让模型生成安全问题的详细回复、根据映射关系反向重写安全回复得到有害输出。

Result: 在7个主流大语言模型和三类基准数据集上实验，Safe2Harm越狱能力强，整体性能优于现有方法；构建含358个样本的有害内容评估数据集。

Conclusion: Safe2Harm方法有效，构建的评估数据集可用于大语言模型输入输出过滤防御。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across various tasks, but their security vulnerabilities can be exploited by attackers to generate harmful content, causing adverse impacts across various societal domains. Most existing jailbreak methods revolve around Prompt Engineering or adversarial optimization, yet we identify a previously overlooked phenomenon: many harmful scenarios are highly consistent with legitimate ones in terms of underlying principles. Based on this finding, this paper proposes the Safe2Harm Semantic Isomorphism Attack method, which achieves efficient jailbreaking through four stages: first, rewrite the harmful question into a semantically safe question with similar underlying principles; second, extract the thematic mapping relationship between the two; third, let the LLM generate a detailed response targeting the safe question; finally, reversely rewrite the safe response based on the thematic mapping relationship to obtain harmful output. Experiments on 7 mainstream LLMs and three types of benchmark datasets show that Safe2Harm exhibits strong jailbreaking capability, and its overall performance is superior to existing methods. Additionally, we construct a challenging harmful content evaluation dataset containing 358 samples and evaluate the effectiveness of existing harmful detection methods, which can be deployed for LLM input-output filtering to enable defense.

</details>


### [259] [Smart Surveillance: Identifying IoT Device Behaviours using ML-Powered Traffic Analysis](https://arxiv.org/abs/2512.13709)
*Reza Ryan,Napoleon Paciente,Cahil Youngs,Nickson Karie,Qian Li,Nasim Ferdosian*

Main category: cs.CR

TL;DR: 随着物联网设备增多，安全挑战增大，本文用机器学习技术结合网络流量监测对物联网设备类型和动作分类，实验证明可行但有局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究在外部监测和分类物联网设备能力有限，需解决此问题以准确识别设备类型和动作来应对安全威胁。

Method: 使用随机森林（RF）、多层感知器（MLP）和K近邻（KNN）机器学习技术，结合针对性网络流量监测，构建含NPAT路由器和多种物联网设备的测试平台。

Result: 提出的机器学习驱动方法可行，RF分类器准确率达91%最高，MLP准确率56%最低，除部分安全摄像头动作外所有设备类别都能成功分类。

Conclusion: 所提方法有潜力但也存在局限性。

Abstract: The proliferation of Internet of Things (IoT) devices has grown exponentially in recent years, introducing significant security challenges. Accurate identification of the types of IoT devices and their associated actions through network traffic analysis is essential to mitigate potential threats. By monitoring and analysing packet flows between IoT devices and connected networks, anomalous or malicious behaviours can be detected. Existing research focuses primarily on device identification within local networks using methods such as protocol fingerprinting and wireless frequency scanning. However, these approaches are limited in their ability to monitor or classify IoT devices externally. To address this gap, we investigate the use of machine learning (ML) techniques, specifically Random Forest (RF), Multilayer Perceptron (MLP), and K-Nearest Neighbours (KNN), in conjunction with targeted network traffic monitoring to classify IoT device types and their actions. We constructed a testbed comprising an NPAT-enabled router and a diverse set of IoT devices, including smart cameras, controller hubs, home appliances, power controllers, and streaming devices. Experimental results demonstrate that IoT device and action recognition is feasible using our proposed ML-driven approach, with the RF classifier achieving the highest accuracy of 91%, while the MLP recorded the lowest accuracy at 56%. Notably, all device categories were successfully classified except for certain actions associated with security cameras, underscoring both the potential and the limitations of the proposed method.

</details>


### [260] [UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis](https://arxiv.org/abs/2512.14130)
*Amirmohammad Pasdar,Toby Murray,Van-Thuan Pham*

Main category: cs.CR

TL;DR: 介绍了适用于编译和开源应用的源无关框架UIXPOSE，利用IBA进行移动恶意软件分析，在三个案例中展示了其更好的检测效果。


<details>
  <summary>Details</summary>
Motivation: 过往工作在移动恶意软件分析中要么静态推断意图，要么监测粗略动态信号，忽略内容和上下文，需更好方法。

Method: UIXPOSE框架利用视觉语言模型和知识结构从每个屏幕推断意图向量，结合网络载荷、堆/内存信号和资源利用痕迹形成行为向量，在运行时计算两者对齐。

Result: 在三个真实案例中，UIXPOSE揭示了逃避仅基于元数据基线的隐蔽数据泄露和隐藏后台活动。

Conclusion: IBA可改善移动恶意软件的动态检测。

Abstract: We introduce UIXPOSE, a source-code-agnostic framework that operates on both compiled and open-source apps. This framework applies Intention Behaviour Alignment (IBA) to mobile malware analysis, aligning UI-inferred intent with runtime semantics. Previous work either infers intent statically, e.g., permission-centric, or widget-level or monitors coarse dynamic signals (endpoints, partial resource usage) that miss content and context. UIXPOSE infers an intent vector from each screen using vision-language models and knowledge structures and combines decoded network payloads, heap/memory signals, and resource utilisation traces into a behaviour vector. Their alignment, calculated at runtime, can both detect misbehaviour and highlight exploration of behaviourally rich paths. In three real-world case studies, UIXPOSE reveals covert exfiltration and hidden background activity that evade metadata-only baselines, demonstrating how IBA improves dynamic detection.

</details>


### [261] [IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol](https://arxiv.org/abs/2512.14166)
*Yunhao Yao,Zhiqiang Wang,Haoran Cheng,Yihang Cheng,Haohua Du,Xiang-Yang Li*

Main category: cs.CR

TL;DR: 论文指出大语言模型自主代理采用MCP协议带来隐私风险，提出IntentMiner框架评估风险，实验表明该框架语义对齐度高，揭示了解耦架构隐私风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型自主代理采用MCP协议虽增强可扩展性，但引入隐私风险，第三方MCP服务器可观察工具交互日志，需评估此隐私威胁。

Method: 提出IntentMiner框架，利用分层信息隔离和三维语义分析，结合工具目的、调用语句和返回结果，在步骤层面准确推断用户意图。

Result: IntentMiner与原始用户查询的语义对齐度超85%，显著优于基线方法。

Conclusion: 解耦代理架构存在固有隐私风险，看似无害的工具执行日志可能泄露用户隐私。

Abstract: The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.

</details>


### [262] [A Deep Dive into Function Inlining and its Security Implications for ML-based Binary Analysis](https://arxiv.org/abs/2512.14045)
*Omar Abusabha,Jiyong Uhm,Tamer Abuhmed,Hyungjoon Koo*

Main category: cs.CR

TL;DR: 本文从机器学习的二进制分析角度，对函数内联优化进行了全面研究，发现函数内联会影响机器学习模型行为，依赖静态特征的模型对内联敏感，可利用编译器设置构造规避二进制变体，且内联率因应用和配置而异。


<details>
  <summary>Details</summary>
Motivation: 函数内联优化虽广泛使用，但对其安全影响研究不足，需从机器学习的二进制分析角度进行全面研究。

Method: 剖析LLVM成本模型中的内联决策流程，探索提升函数内联率的编译器选项组合（极端内联），用20种模型评估五个机器学习辅助的二进制安全分析任务在极端内联场景下的鲁棒性。

Result: 函数内联会直接或间接影响机器学习模型行为，依赖静态特征的模型对内联高度敏感，可利用编译器设置构造规避二进制变体，内联率因应用和配置差异大。

Conclusion: 函数内联虽初衷良好，但会对机器学习模型产生潜在安全影响，在训练和评估机器学习模型时需考虑内联率的不一致性。

Abstract: A function inlining optimization is a widely used transformation in modern compilers, which replaces a call site with the callee's body in need. While this transformation improves performance, it significantly impacts static features such as machine instructions and control flow graphs, which are crucial to binary analysis. Yet, despite its broad impact, the security impact of function inlining remains underexplored to date. In this paper, we present the first comprehensive study of function inlining through the lens of machine learning-based binary analysis. To this end, we dissect the inlining decision pipeline within the LLVM's cost model and explore the combinations of the compiler options that aggressively promote the function inlining ratio beyond standard optimization levels, which we term extreme inlining. We focus on five ML-assisted binary analysis tasks for security, using 20 unique models to systematically evaluate their robustness under extreme inlining scenarios. Our extensive experiments reveal several significant findings: i) function inlining, though a benign transformation in intent, can (in)directly affect ML model behaviors, being potentially exploited by evading discriminative or generative ML models; ii) ML models relying on static features can be highly sensitive to inlining; iii) subtle compiler settings can be leveraged to deliberately craft evasive binary variants; and iv) inlining ratios vary substantially across applications and build configurations, undermining assumptions of consistency in training and evaluation of ML models.

</details>


### [263] [Hybrid Ensemble Method for Detecting Cyber-Attacks in Water Distribution Systems Using the BATADAL Dataset](https://arxiv.org/abs/2512.14422)
*Waqas Ahmed*

Main category: cs.CR

TL;DR: 随着数字连接扩展，工业控制系统网络安全愈发重要，本文提出混合集成学习模型检测水分配系统网络攻击，对比多种模型，混合堆叠集成表现最佳，提出框架用于安全检测。


<details>
  <summary>Details</summary>
Motivation: 提升水分配系统网络攻击检测能力，解决BATADAL基准数据存在的类别不平衡、多变量时间依赖和隐蔽攻击等问题。

Method: 使用机器学习和深度学习模型构建混合集成学习模型，对比三种基础学习器（随机森林、XGBoost、LSTM）和七种集成类型，用随机森林分析特征、SMOTE解决类别不平衡。

Result: 单一LSTM模型性能差，基于树的模型特别是XGBoost表现好，混合堆叠集成得分最高，F1 - score为0.7205，AUC为0.9826。

Conclusion: 提出的框架为时间相关工业系统的网络攻击检测提供了强大且可扩展的解决方案，融合时间学习和集成多样性以支持关键基础设施的安全运行。

Abstract: The cybersecurity of Industrial Control Systems that manage critical infrastructure such as Water Distribution Systems has become increasingly important as digital connectivity expands. BATADAL benchmark data is a good source of testing intrusion detection techniques, but it presents several important problems, such as imbalance in the number of classes, multivariate time dependence, and stealthy attacks. We consider a hybrid ensemble learning model that will enhance the detection ability of cyber-attacks in WDS by using the complementary capabilities of machine learning and deep learning models. Three base learners, namely, Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network, have been strictly compared and seven ensemble types using simple averaged and stacked learning with a logistic regression meta-learner. Random Forest analysis identified top predictors turned into temporal and statistical features, and Synthetic Minority Oversampling Technique (SMOTE) was used to overcome the class imbalance issue. The analyics indicates that the single Long Short-Term Memory network model is of poor performance (F1 = 0.000, AUC = 0.4460), but tree-based models, especially eXtreme Gradient Boosting, perform well (F1 = 0.7470, AUC=0.9684). The hybrid stacked ensemble of Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network scored the highest, with the attack class of 0.7205 with an F1-score and a AUC of 0.9826 indicating that the heterogeneous stacking between model precision and generalization can work. The proposed framework establishes a robust and scalable solution for cyber-attack detection in time-dependent industrial systems, integrating temporal learning and ensemble diversity to support the secure operation of critical infrastructure.

</details>


### [264] [Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space](https://arxiv.org/abs/2512.14448)
*Xingfu Zhou,Pengfei Wang*

Main category: cs.CR

TL;DR: 本文提出针对大语言模型代理推理风格的攻击范式RSP及攻击方法GSI，用RSV量化推理风格变化，实验表明GSI能显著降低性能并绕过过滤器，还提出运行时监视器RSP - M。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击主要关注内容伪造或指令注入，本文识别出大语言模型代理推理风格这一新型面向过程的攻击面。

Method: 提出Reasoning - Style Poisoning (RSP)范式和Generative Style Injection (GSI)攻击方法，开发Reasoning Style Vector (RSV)指标，提出RSP - M运行时监视器。

Result: 在HotpotQA和FEVER数据集上，使用ReAct、Reflection和Tree of Thoughts (ToT)架构的实验显示，GSI显著降低性能，增加推理步骤达4.4倍或导致过早出错，能绕过最先进的内容过滤器。

Conclusion: 推理风格是一个独特且可利用的漏洞，需要超越静态内容分析的过程级防御。

Abstract: Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically "analysis paralysis" or "cognitive haste"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [265] [Q-IRIS: The Evolution of the IRIS Task-Based Runtime to Enable Classical-Quantum Workflows](https://arxiv.org/abs/2512.13931)
*Narasinga Rao Miniskar,Mohammad Alaul Haque Monil,Elaine Wong,Vicente Leyton-Ortega,Jeffrey S. Vetter,Seth R. Johnson,Travis S. Humble*

Main category: quant-ph

TL;DR: 本文提出混合执行框架，集成IRIS和XACC，实现经典与量子任务并发执行，展示量子电路切割效果并指出混合运行时扩展挑战。


<details>
  <summary>Details</summary>
Motivation: 新兴HPC系统的极端异构性开始包含量子加速器，需要能协调经典和量子工作负载的运行时。

Method: 提出概念验证的混合执行框架，将IRIS异步任务运行时与XACC量子编程框架通过QIR - EE集成，IRIS协调跨异构后端的QIR程序。

Result: 成功异步调度和执行多个量子工作负载，通过量子电路切割降低单任务量子模拟负载，提高模拟器吞吐量和减少排队行为。

Conclusion: 指出混合运行时扩展的关键挑战，如协调调度、经典 - 量子交互管理和支持异构系统中不同后端资源。

Abstract: Extreme heterogeneity in emerging HPC systems are starting to include quantum accelerators, motivating runtimes that can coordinate between classical and quantum workloads. We present a proof-of-concept hybrid execution framework integrating the IRIS asynchronous task-based runtime with the XACC quantum programming framework via the Quantum Intermediate Representation Execution Engine (QIR-EE). IRIS orchestrates multiple programs written in the quantum intermediate representation (QIR) across heterogeneous backends (including multiple quantum simulators), enabling concurrent execution of classical and quantum tasks. Although not a performance study, we report measurable outcomes through the successful asynchronous scheduling and execution of multiple quantum workloads. To illustrate practical runtime implications, we decompose a four-qubit circuit into smaller subcircuits through a process known as quantum circuit cutting, reducing per-task quantum simulation load and demonstrating how task granularity can improve simulator throughput and reduce queueing behavior -- effects directly relevant to early quantum hardware environments. We conclude by outlining key challenges for scaling hybrid runtimes, including coordinated scheduling, classical-quantum interaction management, and support for diverse backend resources in heterogeneous systems.

</details>


### [266] [A Spatio-Temporal Hybrid Quantum-Classical Graph Convolutional Neural Network Approach for Urban Taxi Destination Prediction](https://arxiv.org/abs/2512.13745)
*Xiuying Zhang,Qinsheng Zhu,Xiaodong Xing*

Main category: quant-ph

TL;DR: 提出H - STQGCN算法预测出租车目的地，结合量子与经典深度学习，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 结合量子计算和经典深度学习优势，对城市道路网络中出租车目的地进行预测。

Method: 算法包含空间处理和时间演化两个分支，空间处理结合经典GCN方法和量子模块，时间演化基于经典TCN理论整合多源信息。

Result: 实验表明该算法在预测准确性和稳定性上优于当前方法。

Conclusion: 验证了量子增强机制在捕捉高维空间依赖方面具有独特优势。

Abstract: We propose a Hybrid Spatio-Temporal Quantum Graph Convolutional Network (H-STQGCN) algorithm by combining the strengths of quantum computing and classical deep learning to predict the taxi destination within urban road networks. Our algorithm consists of two branches: spatial processing and time evolution. Regarding the spatial processing, the classical module encodes the local topological features of the road network based on the GCN method, and the quantum module is designed to map graph features onto parameterized quantum circuits through a differentiable pooling layer. The time evolution is solved by integrating multi-source contextual information and capturing dynamic trip dependencies on the classical TCN theory. Finally, our experimental results demonstrate that the proposed algorithm outperforms the current methods in terms of prediction accuracy and stability, validating the unique advantages of the quantum-enhanced mechanism in capturing high-dimensional spatial dependencies.

</details>


### [267] [Group-Theoretic Reinforcement Learning of Dynamical Decoupling Sequences](https://arxiv.org/abs/2512.13890)
*Charles Marrder,Shuo Sun,Murray J. Holland*

Main category: quant-ph

TL;DR: 提出基于强化学习的量子比特脉冲序列设计方法，可在无需噪声谱知识下学习最小化退相的序列，为实时学习最优序列提供可能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在确定实际噪声谱下的最优脉冲时序有挑战，需新方法设计量子比特脉冲序列。

Method: 提出基于强化学习的方法，采用源于汤普森群F的动作集，使强化学习智能体能有效探索非凸优化空间。

Result: 强化学习智能体可在无需明确底层噪声谱知识下学习最小化退相的脉冲序列。

Conclusion: 该工作为退相受限的量子比特实时学习最优动态解耦序列提供可能，算法无模型特性使其在存在未建模物理效应时也可能学习到最优脉冲序列。

Abstract: Dynamical decoupling seeks to mitigate phase decoherence in qubits by applying a carefully designed sequence of effectively instantaneous electromagnetic pulses. Although analytic solutions exist for pulse timings that are optimal under specific noise regimes, identifying the optimal timings for a realistic noise spectrum remains challenging. We propose a reinforcement learning (RL)-based method for designing pulse sequences on qubits. Our novel action set enables the RL agent to efficiently navigate this inherently non-convex optimization landscape. The action set, derived from Thompson's group $F$, is applicable to a broad class of sequential decision problems whose states can be represented as bounded sequences. We demonstrate that our RL agent can learn pulse sequences that minimize dephasing without requiring explicit knowledge of the underlying noise spectrum. This work opens the possibility for real-time learning of optimal dynamical decoupling sequences on qubits which are dephasing-limited. The model-free nature of our algorithm suggests that the agent may ultimately learn optimal pulse sequences even in the presence of unmodeled physical effects, such as pulse errors or non-Gaussian noise.

</details>


### [268] [Towards Explainable Quantum AI: Informing the Encoder Selection of Quantum Neural Networks via Visualization](https://arxiv.org/abs/2512.14181)
*Shaolun Ruan,Feng Liang,Rohan Ramakrishna,Chao Ren,Rudai Yan,Qiang Guan,Jiannan Li,Yong Wang*

Main category: quant-ph

TL;DR: 本文提出可视化工具XQAI - Eyes解决量子神经网络编码器选择难题，评估显示其有助于探索编码器设计与QNN有效性关系，专家还据此得出编码器选择实践。


<details>
  <summary>Details</summary>
Motivation: 量子神经网络编码器选择缺乏系统指导，难以评估编码量子态和分析编码器区分特征能力。

Method: 引入可视化工具XQAI - Eyes，对比经典数据特征与编码量子态，检查不同类别的混合量子态。

Result: 在不同数据集和编码器设计上的评估展示了该工具支持探索编码器设计和QNN有效性关系的潜力。

Conclusion: XQAI - Eyes提供优化量子编码器的整体透明方法，专家基于此得出量子编码器选择的两项关键实践。

Abstract: Quantum Neural Networks (QNNs) represent a promising fusion of quantum computing and neural network architectures, offering speed-ups and efficient processing of high-dimensional, entangled data. A crucial component of QNNs is the encoder, which maps classical input data into quantum states. However, choosing suitable encoders remains a significant challenge, largely due to the lack of systematic guidance and the trial-and-error nature of current approaches. This process is further impeded by two key challenges: (1) the difficulty in evaluating encoded quantum states prior to training, and (2) the lack of intuitive methods for analyzing an encoder's ability to effectively distinguish data features. To address these issues, we introduce a novel visualization tool, XQAI-Eyes, which enables QNN developers to compare classical data features with their corresponding encoded quantum states and to examine the mixed quantum states across different classes. By bridging classical and quantum perspectives, XQAI-Eyes facilitates a deeper understanding of how encoders influence QNN performance. Evaluations across diverse datasets and encoder designs demonstrate XQAI-Eyes's potential to support the exploration of the relationship between encoder design and QNN effectiveness, offering a holistic and transparent approach to optimizing quantum encoders. Moreover, domain experts used XQAI-Eyes to derive two key practices for quantum encoder selection, grounded in the principles of pattern preservation and feature mapping.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [269] [Intelligent matter consisting of active particles](https://arxiv.org/abs/2512.13912)
*Julian Jeggle,Raphael Wittkowski*

Main category: cond-mat.soft

TL;DR: 探讨简单运动代理系统作为通向智能系统的途径，对比不同计算方法并介绍新方案。


<details>
  <summary>Details</summary>
Motivation: 受自然界群体实体简单规则产生复杂集体行为启发，探讨能否在人造物质中模拟此行为并达到智能系统复杂度。

Method: 使用“智能物质”的形式化概念，对比活性物质领域成果，探索涌现计算和物理储层计算方法。

Result: 文中未明确提及具体研究结果。

Conclusion: 文中未明确提及具体结论。

Abstract: In this book chapter, we review how systems of simple motile agents can be used as a pathway to intelligent systems. It is a well known result from nature that large groups of entities following simple rules, such as swarms of animals, can give rise to much more complex collective behavior in a display of emergence. This begs the question whether we can emulate this behavior in synthetic matter and drive it to a point where the collective behavior reaches the complexity level of intelligent systems. Here, we will use a formalized notion of "intelligent matter" and compare it to recent results in the field of active matter. First, we will explore the approach of emergent computing in which specialized active matter systems are designed to directly solve a given task through emergent behavior. This we will then contrast with the approach of physical reservoir computing powered by the dynamics of active particle systems. In this context, we will also describe a novel reservoir computing scheme for active particles driven ultrasonically or via light refraction.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [270] [Generative AI for Video Translation: A Scalable Architecture for Multilingual Video Conferencing](https://arxiv.org/abs/2512.13904)
*Amirkia Rafiei Oskooei,Eren Caglar,Ibrahim Sahin,Ayse Kayabay,Mehmet S. Aktas*

Main category: cs.MM

TL;DR: 论文针对级联生成式AI管道实时部署挑战，提出系统框架，经性能分析和用户研究验证可实现实时吞吐量，为多语言通信平台部署提供方案。


<details>
  <summary>Details</summary>
Motivation: 级联生成式AI管道实时部署面临系统级挑战，如顺序模型推理累积延迟和二次计算复杂度，影响多用户视频会议应用可扩展性。

Method: 提出包含轮询机制降低多用户场景计算复杂度、分段处理协议管理推理延迟的系统框架，并在多硬件层进行性能分析。

Result: 系统在现代硬件上实现实时吞吐量，用户研究表明初始处理延迟可被接受以换取流畅播放体验。

Conclusion: 提出的端到端系统设计为多语言通信平台部署可扩展实时生成式AI应用提供实用方案。

Abstract: The real-time deployment of cascaded generative AI pipelines for applications like video translation is constrained by significant system-level challenges. These include the cumulative latency of sequential model inference and the quadratic ($\mathcal{O}(N^2)$) computational complexity that renders multi-user video conferencing applications unscalable. This paper proposes and evaluates a practical system-level framework designed to mitigate these critical bottlenecks. The proposed architecture incorporates a turn-taking mechanism to reduce computational complexity from quadratic to linear in multi-user scenarios, and a segmented processing protocol to manage inference latency for a perceptually real-time experience. We implement a proof-of-concept pipeline and conduct a rigorous performance analysis across a multi-tiered hardware setup, including commodity (NVIDIA RTX 4060), cloud (NVIDIA T4), and enterprise (NVIDIA A100) GPUs. Our objective evaluation demonstrates that the system achieves real-time throughput ($τ< 1.0$) on modern hardware. A subjective user study further validates the approach, showing that a predictable, initial processing delay is highly acceptable to users in exchange for a smooth, uninterrupted playback experience. The work presents a validated, end-to-end system design that offers a practical roadmap for deploying scalable, real-time generative AI applications in multilingual communication platforms.

</details>


### [271] [End-to-End Learning-based Video Streaming Enhancement Pipeline: A Generative AI Approach](https://arxiv.org/abs/2512.14185)
*Emanuele Artioli,Farzad Tashtarian,Christian Timmerer*

Main category: cs.MM

TL;DR: 本文提出端到端架构ELVIS，结合服务器端编码优化与客户端生成式内插法处理冗余视频数据，当前技术比基线基准提高达11 VMAF点。


<details>
  <summary>Details</summary>
Motivation: 解决视频流在高画质和流畅播放之间的平衡问题，传统编解码器无法利用上下文，需编码并传输全量视频数据。

Method: 引入端到端架构ELVIS，结合服务器端编码优化与客户端生成式内插法移除和重构冗余视频数据，其模块化设计可集成不同编解码器、内插模型和质量指标。

Result: 当前技术比基线基准最多提升11 VMAF点，但实时应用因计算需求仍面临挑战。

Conclusion: ELVIS是将生成式AI融入视频流管道的基础一步，能在不增加带宽的情况下提供更高质量体验。

Abstract: The primary challenge of video streaming is to balance high video quality with smooth playback. Traditional codecs are well tuned for this trade-off, yet their inability to use context means they must encode the entire video data and transmit it to the client. This paper introduces ELVIS (End-to-end Learning-based VIdeo Streaming Enhancement Pipeline), an end-to-end architecture that combines server-side encoding optimizations with client-side generative in-painting to remove and reconstruct redundant video data. Its modular design allows ELVIS to integrate different codecs, inpainting models, and quality metrics, making it adaptable to future innovations. Our results show that current technologies achieve improvements of up to 11 VMAF points over baseline benchmarks, though challenges remain for real-time applications due to computational demands. ELVIS represents a foundational step toward incorporating generative AI into video streaming pipelines, enabling higher quality experiences without increased bandwidth requirements.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [272] [Towards Deep Learning Surrogate for the Forward Problem in Electrocardiology: A Scalable Alternative to Physics-Based Models](https://arxiv.org/abs/2512.13765)
*Shaheim Ogbomo-Harmitt,Cesare Magnetti,Chiara Spota,Jakub Grzelak,Oleg Aslanidi*

Main category: eess.IV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The forward problem in electrocardiology, computing body surface potentials from cardiac electrical activity, is traditionally solved using physics-based models such as the bidomain or monodomain equations. While accurate, these approaches are computationally expensive, limiting their use in real-time and large-scale clinical applications. We propose a proof-of-concept deep learning (DL) framework as an efficient surrogate for forward solvers. The model adopts a time-dependent, attention-based sequence-to-sequence architecture to predict electrocardiogram (ECG) signals from cardiac voltage propagation maps. A hybrid loss combining Huber loss with a spectral entropy term was introduced to preserve both temporal and frequency-domain fidelity. Using 2D tissue simulations incorporating healthy, fibrotic, and gap junction-remodelled conditions, the model achieved high accuracy (mean $R^2 = 0.99 \pm 0.01$). Ablation studies confirmed the contributions of convolutional encoders, time-aware attention, and spectral entropy loss. These findings highlight DL as a scalable, cost-effective alternative to physics-based solvers, with potential for clinical and digital twin applications.

</details>


### [273] [Improving the Plausibility of Pressure Distributions Synthesized from Depth through Generative Modeling](https://arxiv.org/abs/2512.13757)
*Neevkumar Manavar,Hanno Gerd Meyer,Joachim Waßmuth,Barbara Hammer,Axel Schneider*

Main category: eess.IV

TL;DR: 提出增强物理合理性框架生成高保真压力估计，实验显示不同模型特点，支持临床实时患者监测


<details>
  <summary>Details</summary>
Motivation: 现有预测压力图方法缺乏物理合理性，限制临床可靠性，需改进

Method: 提出通过信息潜在空间（ILS）和权重优化损失（WOL）结合生成式建模的框架，应用条件布朗桥扩散模型（BBDM）并提出潜在布朗桥扩散模型（LBBDM）训练策略

Result: 所提方法比基线方法提高物理合理性和性能，BBDM 细节丰富但计算成本高、推理时间长，LBBDM 推理快且性能有竞争力

Conclusion: 该方法支持临床环境中非侵入、基于视觉的实时患者监测

Abstract: Monitoring contact pressure in hospital beds is essential for preventing pressure ulcers and enabling real-time patient assessment. Current methods can predict pressure maps but often lack physical plausibility, limiting clinical reliability. This work proposes a framework that enhances plausibility via Informed Latent Space (ILS) and Weight Optimization Loss (WOL) with generative modeling to produce high-fidelity, physically consistent pressure estimates. This study also applies diffusion based conditional Brownian Bridge Diffusion Model (BBDM) and proposes training strategy for its latent counterpart Latent Brownian Bridge Diffusion Model (LBBDM) tailored for pressure synthesis in lying postures. Experiment results shows proposed method improves physical plausibility and performance over baselines: BBDM with ILS delivers highly detailed maps at higher computational cost and large inference time, whereas LBBDM provides faster inference with competitive performance. Overall, the approach supports non-invasive, vision-based, real-time patient monitoring in clinical environments.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [274] [Learning the score under shape constraints](https://arxiv.org/abs/2512.14624)
*Rebecca M. Lewis,Oliver Y. Feng,Henry W. J. Reeve,Min Xu,Richard J. Samworth*

Main category: math.ST

TL;DR: 本文研究对数凹分布下得分估计的极小极大风险，确定尾行为和光滑性对估计的影响，给出特定类的极小极大风险阶并构造了达到上界的估计量。


<details>
  <summary>Details</summary>
Motivation: 得分估计在生成建模和线性回归中有重要应用，研究对数凹分布下得分估计的极小极大风险。

Method: 定义对数凹密度子类，考虑尾行为和光滑性与对数凹性的相互作用，用局部自适应多尺度估计量和得分函数的均匀置信带。

Result: 确定得分函数尾行为对估计的关键影响；特定类的极小极大风险阶为 $L^{2/(2β+1)}n^{-β/(2β+1)}$ ；局部自适应多尺度估计量达到上界。

Conclusion: 得分估计和密度估计在该形状约束类上存在有趣差异。

Abstract: Score estimation has recently emerged as a key modern statistical challenge, due to its pivotal role in generative modelling via diffusion models. Moreover, it is an essential ingredient in a new approach to linear regression via convex $M$-estimation, where the corresponding error densities are projected onto the log-concave class. Motivated by these applications, we study the minimax risk of score estimation with respect to squared $L^2(P_0)$-loss, where $P_0$ denotes an underlying log-concave distribution on $\mathbb{R}$. Such distributions have decreasing score functions, but on its own, this shape constraint is insufficient to guarantee a finite minimax risk. We therefore define subclasses of log-concave densities that capture two fundamental aspects of the estimation problem. First, we establish the crucial impact of tail behaviour on score estimation by determining the minimax rate over a class of log-concave densities whose score function exhibits controlled growth relative to the quantile levels. Second, we explore the interplay between smoothness and log-concavity by considering the class of log-concave densities with a scale restriction and a $(β,L)$-Hölder assumption on the log-density for some $β\in [1,2]$. We show that the minimax risk over this latter class is of order $L^{2/(2β+1)}n^{-β/(2β+1)}$ up to poly-logarithmic factors, where $n$ denotes the sample size. When $β< 2$, this rate is faster than could be obtained under either the shape constraint or the smoothness assumption alone. Our upper bounds are attained by a locally adaptive, multiscale estimator constructed from a uniform confidence band for the score function. This study highlights intriguing differences between the score estimation and density estimation problems over this shape-constrained class.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [275] [Graph AI generates neurological hypotheses validated in molecular, organoid, and clinical systems](https://arxiv.org/abs/2512.13724)
*Ayush Noori,Joaquín Polonuer,Katharina Meyer,Bogdan Budnik,Shad Morton,Xinyuan Wang,Sumaiya Nazeen,Yingnan He,Iñaki Arango,Lucas Vittor,Matthew Woodworth,Richard C. Krolewski,Michelle M. Li,Ninning Liu,Tushar Kamath,Evan Macosko,Dylan Ritter,Jalwa Afroz,Alexander B. H. Henderson,Lorenz Studer,Samuel G. Rodriques,Andrew White,Noa Dagan,David A. Clifton,George M. Church,Sudeshna Das,Jenny M. Tam,Vikram Khurana,Marinka Zitnik*

Main category: q-bio.QM

TL;DR: 提出异质图变换器PROTON生成神经疾病可测试假设，并在多种神经疾病中验证其有效性，为AI驱动的神经疾病发现指明路径。


<details>
  <summary>Details</summary>
Motivation: 神经疾病是全球致残主因，但缺乏改善疾病的疗法，需要工具生成可测试假设。

Method: 提出异质图变换器PROTON，并将其应用于帕金森病、双相情感障碍和阿尔茨海默病进行评估。

Result: 在帕金森病中关联遗传风险位点与关键基因、预测农药毒性，重现多项实验；在双相情感障碍中预测候选药物；在阿尔茨海默病中通过健康记录验证预测药物降低痴呆风险。

Conclusion: PROTON生成的神经学假设在多系统得到评估，为AI驱动的神经疾病发现提供了途径。

Abstract: Neurological diseases are the leading global cause of disability, yet most lack disease-modifying treatments. We present PROTON, a heterogeneous graph transformer that generates testable hypotheses across molecular, organoid, and clinical systems. To evaluate PROTON, we apply it to Parkinson's disease (PD), bipolar disorder (BD), and Alzheimer's disease (AD). In PD, PROTON linked genetic risk loci to genes essential for dopaminergic neuron survival and predicted pesticides toxic to patient-derived neurons, including the insecticide endosulfan, which ranked within the top 1.29% of predictions. In silico screens performed by PROTON reproduced six genome-wide $α$-synuclein experiments, including a split-ubiquitin yeast two-hybrid system (normalized enrichment score [NES] = 2.30, FDR-adjusted $p < 1 \times 10^{-4}$), an ascorbate peroxidase proximity labeling assay (NES = 2.16, FDR $< 1 \times 10^{-4}$), and a high-depth targeted exome sequencing study in 496 synucleinopathy patients (NES = 2.13, FDR $< 1 \times 10^{-4}$). In BD, PROTON predicted calcitriol as a candidate drug that reversed proteomic alterations observed in cortical organoids derived from BD patients. In AD, we evaluated PROTON predictions in health records from $n = 610,524$ patients at Mass General Brigham, confirming that five PROTON-predicted drugs were associated with reduced seven-year dementia risk (minimum hazard ratio = 0.63, 95% CI: 0.53-0.75, $p < 1 \times 10^{-7}$). PROTON generated neurological hypotheses that were evaluated across molecular, organoid, and clinical systems, defining a path for AI-driven discovery in neurological disease.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [276] [Modular connectivity in neural networks emerges from Poisson noise-motivated regularisation, and promotes robustness and compositional generalisation](https://arxiv.org/abs/2512.13707)
*Daoyuan Qian,Qiyao Liang,Ila Fiete*

Main category: physics.bio-ph

TL;DR: 本文受容错计算和真实神经元放电启发，研究活动依赖的神经噪声驱动人工神经网络模块化结构出现，及其带来的功能优势。


<details>
  <summary>Details</summary>
Motivation: 大脑电路有模块化架构优势，而人工神经网络难以找到模块化解决方案，因此研究如何使人工神经网络出现模块化结构。

Method: 借鉴容错计算和真实神经元泊松式放电，研究活动依赖的神经噪声与非线性神经响应结合的作用，还使用确定性正则化器。

Result: 发现噪声驱动的模块化可由结合权重和激活的确定性正则化器重现，预模块化的人工神经网络在抗噪性、泛化和外推能力上表现更优。

Conclusion: 提出的正则化器和架构能促进模块化结构出现并带来功能益处。

Abstract: Circuits in the brain commonly exhibit modular architectures that factorise complex tasks, resulting in the ability to compositionally generalise and reduce catastrophic forgetting. In contrast, artificial neural networks (ANNs) appear to mix all processing, because modular solutions are difficult to find as they are vanishing subspaces in the space of possible solutions. Here, we draw inspiration from fault-tolerant computation and the Poisson-like firing of real neurons to show that activity-dependent neural noise, combined with nonlinear neural responses, drives the emergence of solutions that reflect an accurate understanding of modular tasks, corresponding to acquisition of a correct world model. We find that noise-driven modularisation can be recapitulated by a deterministic regulariser that multiplicatively combines weights and activations, revealing rich phenomenology not captured in linear networks or by standard regularisation methods. Though the emergence of modular structure requires sufficiently many training samples (exponential in the number of modular task dimensions), we show that pre-modularised ANNs exhibit superior noise-robustness and the ability to generalise and extrapolate well beyond training data, compared to ANNs without such inductive biases. Together, our work demonstrates a regulariser and architectures that could encourage modularity emergence to yield functional benefits.

</details>


### [277] [Error Bound Analysis of Physics-Informed Neural Networks-Driven T2 Quantification in Cardiac Magnetic Resonance Imaging](https://arxiv.org/abs/2512.14211)
*Mengxue Zhang,Qingrui Cai,Yinyin Chen,Hang Jin,Jianjun Zhou,Qiu Guo,Peijun Zhao,Zhiping Mao,Xingxing Zhang,Yuyu Xia,Xianwang Jiang,Qin Xu,Chunyan Xiong,Yirong Zhou,Chengyan Wang,Xiaobo Qu*

Main category: physics.bio-ph

TL;DR: 提出基于PINN的MRI T2参数估计方法，嵌入Bloch方程，推导误差上界，在模型、体模和临床患者中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法进行MRI T2参数估计需大量训练数据，缺乏理论支持和金标准，且无基于PINN的T2估计方法。

Method: 将MRI基本物理原理Bloch方程嵌入PINN损失函数，推导T2估计误差和Bloch方程解的泛化误差上界。

Result: 在数值心脏模型和水模上展示T2映射准确性和理论分析有效性，在94例急性心肌梗死患者中实现低误差定量T2估计。

Conclusion: 所提方法具有良好定量精度、鲁棒性和临床应用潜力。

Abstract: Physics-Informed Neural Networks (PINN) are emerging as a promising approach for quantitative parameter estimation of Magnetic Resonance Imaging (MRI). While existing deep learning methods can provide an accurate quantitative estimation of the T2 parameter, they still require large amounts of training data and lack theoretical support and a recognized gold standard. Thus, given the absence of PINN-based approaches for T2 estimation, we propose embedding the fundamental physics of MRI, the Bloch equation, in the loss of PINN, which is solely based on target scan data and does not require a pre-defined training database. Furthermore, by deriving rigorous upper bounds for both the T2 estimation error and the generalization error of the Bloch equation solution, we establish a theoretical foundation for evaluating the PINN's quantitative accuracy. Even without access to the ground truth or a gold standard, this theory enables us to estimate the error with respect to the real quantitative parameter T2. The accuracy of T2 mapping and the validity of the theoretical analysis are demonstrated on a numerical cardiac model and a water phantom, where our method exhibits excellent quantitative precision in the myocardial T2 range. Clinical applicability is confirmed in 94 acute myocardial infarction (AMI) patients, achieving low-error quantitative T2 estimation under the theoretical error bound, highlighting the robustness and potential of PINN.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [278] [Scalable Frameworks for Real-World Audio-Visual Speech Recognition](https://arxiv.org/abs/2512.14083)
*Sungnyun Kim*

Main category: eess.AS

TL;DR: 论文旨在构建下一代鲁棒可扩展的视听语音识别（AVSR）系统，从表示、架构和系统三个层面提出解决方案。


<details>
  <summary>Details</summary>
Motivation: 现实环境中不可预测的声学噪声和视觉干扰导致AVSR系统性能显著下降，需要系统的分层方法来克服挑战。

Method: 表示层面构建统一模型学习对真实世界干扰鲁棒的视听特征；架构层面开发根据输入特征智能分配计算资源的框架；系统层面通过与大规模基础模型模块化集成扩展系统功能。

Result: 未提及具体结果。

Conclusion: 通过在三个层面提供系统解决方案，有望构建在现实应用中具有高可靠性的下一代AVSR系统。

Abstract: The practical deployment of Audio-Visual Speech Recognition (AVSR) systems is fundamentally challenged by significant performance degradation in real-world environments, characterized by unpredictable acoustic noise and visual interference. This dissertation posits that a systematic, hierarchical approach is essential to overcome these challenges, achieving the robust scalability at the representation, architecture, and system levels. At the representation level, we investigate methods for building a unified model that learns audio-visual features inherently robust to diverse real-world corruptions, thereby enabling generalization to new environments without specialized modules. To address architectural scalability, we explore how to efficiently expand model capacity while ensuring the adaptive and reliable use of multimodal inputs, developing a framework that intelligently allocates computational resources based on the input characteristics. Finally, at the system level, we present methods to expand the system's functionality through modular integration with large-scale foundation models, leveraging their powerful cognitive and generative capabilities to maximize final recognition accuracy. By systematically providing solutions at each of these three levels, this dissertation aims to build a next-generation, robust, and scalable AVSR system with high reliability in real-world applications.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [279] [Multi-Agent Collaborative Framework for Intelligent IT Operations: An AOI System with Context-Aware Compression and Dynamic Task Scheduling](https://arxiv.org/abs/2512.13956)
*Zishan Bai,Enze Ge,Junfeng Hao*

Main category: cs.MA

TL;DR: 云原生架构使IT基础设施复杂，产生大量数据，本文提出AOI框架应对，实验表明其能缓解信息过载、提升效率。


<details>
  <summary>Details</summary>
Motivation: 云原生架构使IT基础设施复杂，传统系统在处理运营数据时存在信息处理低效、任务协调差、故障诊断和修复时上下文连续性丢失等问题。

Method: 提出AOI多智能体协作框架，集成三个专业智能体和基于大语言模型的上下文压缩器，有动态任务调度策略和三层内存架构。

Result: 在合成和真实基准测试中，AOI实现72.4%的上下文压缩率，保留92.8%的关键信息，任务成功率达94.2%，平均修复时间减少34.4%。

Conclusion: 该工作实现了向可扩展、自适应和上下文感知的自主运营范式转变，可在最少人工干预下管理下一代IT基础设施。

Abstract: The proliferation of cloud-native architectures, characterized by microservices and dynamic orchestration, has rendered modern IT infrastructures exceedingly complex and volatile. This complexity generates overwhelming volumes of operational data, leading to critical bottlenecks in conventional systems: inefficient information processing, poor task coordination, and loss of contextual continuity during fault diagnosis and remediation. To address these challenges, we propose AOI (AI-Oriented Operations), a novel multi-agent collaborative framework that integrates three specialized agents with an LLM-based Context Compressor. Its core innovations include: (1) a dynamic task scheduling strategy that adaptively prioritizes operations based on real-time system states, and (2) a three-layer memory architecture comprising Working, Episodic, and Semantic layers that optimizes context retention and retrieval. Extensive experiments on both synthetic and real-world benchmarks demonstrate that AOI effectively mitigates information overload, achieving a 72.4% context compression ratio while preserving 92.8% of critical information and significantly enhances operational efficiency, attaining a 94.2% task success rate and reducing the Mean Time to Repair (MTTR) by 34.4% compared to the best baseline. This work presents a paradigm shift towards scalable, adaptive, and context-aware autonomous operations, enabling robust management of next-generation IT infrastructures with minimal human intervention.

</details>
